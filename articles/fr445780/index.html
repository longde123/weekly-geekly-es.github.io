<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö≥ ‚§¥Ô∏è ü§∞üèª M√©thodes d'ensemble. Extrait du livre üëáüèø üíπ üçã</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Khabrozhiteli, nous avons remis √† l'imprimerie un nouveau livre , Machine Learning: Algorithms for Business . Voici un extrait sur les m√©thod...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√©thodes d'ensemble. Extrait du livre</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/445780/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/vk/fr/zn/vkfrzn9ctkjsd2wjx8puqifp980.jpeg" alt="image"></a> <br><br>  Bonjour, Khabrozhiteli, nous avons remis √† l'imprimerie un nouveau livre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">, Machine Learning: Algorithms for Business</a> .  Voici un extrait sur les m√©thodes d'ensemble, son but est d'expliquer ce qui les rend efficaces, et comment √©viter les erreurs courantes qui conduisent √† leur mauvaise utilisation en finance. <br><a name="habracut"></a><br><h3>  6.2.  Trois sources d'erreur </h3><br>  Les mod√®les MO souffrent g√©n√©ralement de trois <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">erreurs</a> . <br><br>  1. Biais: cette erreur est caus√©e par des hypoth√®ses irr√©alistes.  Lorsque le biais est √©lev√©, cela signifie que l'algorithme MO n'a pas pu reconna√Ætre les relations importantes entre les traits et les r√©sultats.  Dans cette situation, il est dit que l'algorithme est "non approuv√©". <br><br>  2. Dispersion: cette erreur est caus√©e par la sensibilit√© √† de petits changements dans le sous-ensemble d'entra√Ænement.  Lorsque la variance est √©lev√©e, cela signifie que l'algorithme est sur-ajust√© au sous-ensemble d'apprentissage, et donc m√™me des changements minimes dans le sous-ensemble d'apprentissage peuvent produire des pr√©dictions tr√®s diff√©rentes.  Au lieu de mod√©liser des mod√®les g√©n√©raux dans un sous-ensemble d'apprentissage, l'algorithme prend par erreur du bruit pour le signal. <br><br>  3. Bruit: cette erreur est caus√©e par la dispersion des valeurs observ√©es, telles que des changements impr√©visibles ou des erreurs de mesure.  Il s'agit d'une erreur fatale qui ne peut √™tre expliqu√©e par aucun mod√®le. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bc/8g/ag/bc8gaguh6e1w07vd3aoqcqcz4m0.png" alt="image"></div><br>  Une m√©thode d'ensemble est une m√©thode qui combine de nombreux √©l√®ves faibles, qui sont bas√©s sur le m√™me algorithme d'apprentissage, dans le but de cr√©er un √©l√®ve (plus fort) dont les performances sont meilleures que celles de chacun des √©l√®ves.  Les techniques d'ensemble aident √† r√©duire le biais et / ou la dispersion. <br><br><h3>  6.3.  Agr√©gation d'amor√ßage </h3><br>  L'ensachage (agr√©gation), ou agr√©gation d'√©chantillons bootstrap, est un moyen efficace de r√©duire la variance des pr√©visions.  Il fonctionne comme suit: premi√®rement, il est n√©cessaire de g√©n√©rer N sous-ensembles d'apprentissage de donn√©es en utilisant un √©chantillonnage al√©atoire avec retour.  Deuxi√®mement, adapter N √©valuateurs, un pour chaque sous-ensemble de formation.  Ces √©valuateurs sont ajust√©s ind√©pendamment les uns des autres, par cons√©quent, les mod√®les peuvent √™tre ajust√©s en parall√®le.  Troisi√®mement, la pr√©vision d'ensemble est une moyenne arithm√©tique simple des pr√©visions individuelles de N mod√®les.  Dans le cas des variables cat√©gorielles, la probabilit√© qu'une observation appartienne √† une classe est d√©termin√©e par la part des √©valuateurs qui classent cette observation comme membre de cette classe (par vote majoritaire, c'est-√†-dire par vote majoritaire).  Lorsque l'√©valuateur de base peut faire des pr√©dictions avec la probabilit√© de pr√©diction, le classificateur ensach√© peut obtenir la valeur moyenne des probabilit√©s. <br><br>  Si vous utilisez la classe baggingClassifier de la biblioth√®que sklearn pour calculer la pr√©cision des non-paquets, vous devez conna√Ætre ce d√©faut: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/scikit-learn/scikitlearn/issues/8933</a> .  Une solution consiste √† renommer les √©tiquettes dans un ordre s√©quentiel entier. <br><br><h3>  6.3.1.  R√©duction de la dispersion </h3><br>  Le principal avantage de l'ensachage est qu'il r√©duit la variance des pr√©visions, contribuant ainsi √† r√©soudre le probl√®me du sur-ajustement.  La variance de la pr√©diction ensach√©e (œÜi [c]) est fonction du nombre d'√©valuateurs ensach√©s (N), de la variance moyenne de la pr√©diction effectu√©e par un √©valuateur (œÉÃÑ) et de la corr√©lation moyenne entre leurs pr√©dictions (œÅÃÑ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pq/ly/9i/pqly9iyqdqu5m8ty8zahuoeomve.png" alt="image"></div><br>  le bootstraping s√©quentiel (chapitre 4) consiste √† pr√©lever des √©chantillons aussi ind√©pendants que possible, r√©duisant ainsi œÅÃÑ, ce qui devrait r√©duire la dispersion des classificateurs ensach√©s.  Dans la fig.  6.1, nous avons trac√© le diagramme de l'√©cart-type de la pr√©diction ensach√©e en fonction de N ‚àà [5, 30], œÅÃÑ ‚àà [0, 1] et œÉÃÑ = 1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/na/bm/8z/nabm8zbq6st62mg82lmhac45y0i.png" alt="image"></div><br><h3>  6.3.2.  Pr√©cision am√©lior√©e </h3><br>  Consid√©rons un classificateur ensach√©, qui fait des pr√©dictions sur k classes par un vote majoritaire parmi N classificateurs ind√©pendants.  Nous pouvons d√©signer des pr√©dictions comme {0,1}, o√π 1 signifie une pr√©diction correcte.  La pr√©cision du classificateur est la probabilit√© p de marquer la pr√©diction comme 1. En moyenne, nous obtenons Np pr√©dictions marqu√©es comme 1 avec une variance de Np (1 - p).  Le vote majoritaire fait la bonne pr√©diction lorsque la classe la plus pr√©visible est observ√©e.  Par exemple, pour N = 10 et k = 3, le classificateur ensach√© a fait la bonne pr√©diction lorsqu'il a √©t√© observ√© <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g-/oq/oi/g-oqoilmsmjgpaukoouor90ndbo.png" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/wk/cl/wxwkcl97h4cnn1n-yx8ljmdj14g.png" alt="image"></div><br>  Listing 6.1.  La justesse du classificateur en sac <br><br><pre><code class="plaintext hljs">from scipy.misc import comb N,p,k=100,1./3,3. p_=0 for i in xrange(0,int(N/k)+1): p_+=comb(N,i)*p**i*(1-p)**(Ni) print p,1-p_</code> </pre> <br>  C'est un argument fort en faveur de l'ensachage de tout classifieur dans le cas g√©n√©ral, lorsque les capacit√©s de calcul le permettent.  Cependant, contrairement au boosting, l'ensachage ne peut pas am√©liorer la pr√©cision des classificateurs faibles: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/fz/i8/9nfzi86m4gvaqru13iicjf1q0bg.png" alt="image"></div><br>  Pour une analyse d√©taill√©e de ce sujet, il est conseill√© au lecteur de se reporter au th√©or√®me du jury Condorcet.  Bien que ce th√©or√®me ait √©t√© obtenu aux fins du vote majoritaire en science politique, le probl√®me abord√© par ce th√©or√®me a des caract√©ristiques communes avec celui d√©crit ci-dessus. <br><br><h3>  6.3.3.  Redondance des observations </h3><br>  Au chapitre 4, nous avons examin√© l'une des raisons pour lesquelles les observations financi√®res ne peuvent pas √™tre consid√©r√©es comme r√©parties √©galement et ind√©pendantes les unes des autres.  Des observations excessives ont deux effets n√©fastes sur l'ensachage.  Premi√®rement, les √©chantillons pr√©lev√©s avec retour sont plus susceptibles d'√™tre presque identiques, m√™me s'ils n'ont pas d'observations communes.  Cela fait <img src="https://habrastorage.org/webt/sx/i4/u0/sxi4u0afpswnvbe5nzxexnp_k80.png" alt="image">  et l'ensachage ne r√©duira pas l'√©cart, quel que soit N. Par exemple, si chaque cas √† t est marqu√© conform√©ment √† un rendement financier entre t et t + 100, nous devons s√©lectionner 1% des cas par √©valuateur en sac, mais pas plus.  Dans le chapitre 4, section 4.5, trois solutions alternatives sont recommand√©es, dont l'une d√©finissait max_samples = out ['tW']. Mean () dans l'impl√©mentation de la classe de classificateur ensach√© dans la biblioth√®que sklearn.  Une autre (meilleure) solution √©tait l'application de la m√©thode de s√©lection s√©quentielle de bootstrap. <br><br>  Le deuxi√®me effet n√©faste de la redondance d'observation est que la pr√©cision des paquets suppl√©mentaires sera gonfl√©e.  Cela est d√ª au fait que l'√©chantillonnage al√©atoire avec √©chantillonnage renvoie aux √©chantillons du sous-ensemble de formation qui sont tr√®s similaires √† ceux en dehors du package.  Dans ce cas, la validation crois√©e de bloc k stratifi√©e correcte sans m√©lange avant la division montrera une pr√©cision beaucoup moins grande sur le sous-ensemble de test que celle qui a √©t√© √©valu√©e en dehors du package.  Pour cette raison, lors de l'utilisation de cette classe de biblioth√®que sklearn, il est recommand√© de d√©finir stratifiedKFold (n_splits = k, shuffle = False), de recouper le classificateur ensach√© et d'ignorer les r√©sultats de la pr√©cision des non-paquets.  Un k faible est pr√©f√©rable √† un k √©lev√©, car une sur-division placera √† nouveau des motifs dans le sous-ensemble de test qui sont trop similaires √† ceux utilis√©s dans le sous-ensemble d'apprentissage. <br><br><h3>  6.4.  For√™t al√©atoire </h3><br>  Les arbres de d√©cision sont bien connus en ce qu'ils ont tendance √† sur-ajuster, ce qui augmente la variance des pr√©visions.  Afin de r√©soudre ce probl√®me, une m√©thode de for√™t al√©atoire (RF) a √©t√© d√©velopp√©e pour g√©n√©rer des pr√©visions d'ensemble avec une variance plus faible. <br><br>  Une for√™t al√©atoire pr√©sente des similitudes communes avec l'ensachage dans le sens de la formation ind√©pendante d'√©valuateurs individuels sur des sous-ensembles de donn√©es bootstrap√©s.  La principale diff√©rence avec l'ensachage est qu'un deuxi√®me niveau de caract√®re al√©atoire est int√©gr√© dans les for√™ts al√©atoires: lors de l'optimisation de chaque fragmentation nodale, seul un sous-√©chantillon al√©atoire (sans retour) d'attributs sera √©valu√© afin de d√©corr√©ler davantage les √©valuateurs. <br><br>  Comme l'ensachage, une for√™t al√©atoire r√©duit la variance des pr√©visions sans surajustement (rappelez-vous que jusqu'√†).  Le deuxi√®me avantage est qu'une for√™t al√©atoire √©value l'importance des attributs, dont nous discuterons en d√©tail au chapitre 8. Le troisi√®me avantage est qu'une for√™t al√©atoire fournit des estimations de la pr√©cision hors emballage, mais dans les applications financi√®res, elles sont susceptibles d'√™tre gonfl√©es (comme d√©crit dans Section 6.3.3).  Mais comme pour l'ensachage, une for√™t al√©atoire ne pr√©sentera pas n√©cessairement un biais plus faible que les arbres de d√©cision individuels. <br><br>  Si un grand nombre d'√©chantillons est redondant (pas √©galement r√©partis et ind√©pendants les uns des autres), il y aura toujours un r√©ajustement: l'√©chantillonnage al√©atoire avec retour construira un grand nombre d'arbres presque identiques (), o√π chaque arbre de d√©cision est surajust√© (un inconv√©nient en raison de la notori√©t√© des arbres de d√©cision). .  Contrairement √† l'ensachage, une for√™t al√©atoire d√©finit toujours la taille des √©chantillons de bootstrap en fonction de la taille du sous-ensemble de donn√©es d'apprentissage.  Voyons comment nous pouvons r√©soudre ce probl√®me de r√©-ajustement des for√™ts al√©atoires dans la biblioth√®que sklearn.  √Ä des fins d'illustration, je ferai r√©f√©rence aux classes de biblioth√®que sklearn;  cependant, ces solutions peuvent √™tre appliqu√©es √† n'importe quelle impl√©mentation: <br><br>  1. D√©finissez le param√®tre max_features sur une valeur inf√©rieure pour obtenir une diff√©rence entre les arbres. <br><br>  2. Arr√™t anticip√©: d√©finissez le param√®tre de r√©gularisation min_weight_fraction_leaf sur une valeur suffisamment grande (par exemple, 5%) pour que la pr√©cision hors paquet converge vers la correction hors √©chantillon (k-block). <br><br>  3. Utilisez l'√©valuateur BaggingClassifier sur l'√©valuateur de base DecisionTreeClassifier, o√π max_samples est d√©fini sur l'unicit√© moyenne (avgU) entre les √©chantillons. <br><br><ul><li>  clf = DecisionTreeClassifier (crit√®re = 'entropie', max_features = 'auto', class_weight = 'sym√©trique') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  4. Utilisez l'√©valuateur BaggingClassifier sur l'√©valuateur de base RandomForestClassifier, o√π max_samples est d√©fini sur l'unicit√© moyenne (avgU) entre les √©chantillons. <br><br><ul><li>  clf = RandomForestClassifier (n_estimators = 1, crit√®re = 'entropie', bootstrap = faux, class_weight = 'balanc√©_sous-√©chantillon') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  5. Modifiez la classe de for√™t al√©atoire pour remplacer les amorces standard par des amorces s√©quentielles. <br><br>  Pour r√©sumer, l'extrait 6.2 montre trois fa√ßons alternatives de configurer une for√™t al√©atoire en utilisant diff√©rentes classes. <br><br>  Listing 6.2.  Trois fa√ßons de configurer une for√™t al√©atoire <br><br><pre> <code class="plaintext hljs">clf0=RandomForestClassifier(n_estimators=1000, class_weight='balanced_ subsample', criterion='entropy') clf1=DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced') clf1=BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU) clf2=RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample') clf2=BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)</code> </pre> <br>  Lors de l'ajustement d'arbres de d√©cision, la rotation de l'espace d'entit√© dans la direction co√Øncidant avec les axes, en r√®gle g√©n√©rale, r√©duit le nombre de niveaux n√©cessaires pour l'arbre.  Pour cette raison, je sugg√®re que vous ajustiez un arbre al√©atoire sur la PCA des attributs, car cela peut acc√©l√©rer les calculs et r√©duire l√©g√®rement le r√©ajustement (plus √† ce sujet au chapitre 8).  De plus, comme d√©crit au chapitre 4, section 4.8, l'argument class_weight = 'balanc√©_sous-√©chantillon' aidera √† emp√™cher les arbres de classer incorrectement les classes minoritaires. <br><br><h3>  6.5.  Boost </h3><br>  Kearns et Valiant [1989] ont √©t√© parmi les premiers √† demander si des √©valuateurs faibles pouvaient √™tre combin√©s afin de r√©aliser la r√©alisation d'un √©valuateur tr√®s pr√©cis.  Peu de temps apr√®s, Schapire [1990] a montr√© une r√©ponse affirmative √† cette question en utilisant une proc√©dure que nous appelons aujourd'hui boosting (boosting, boosting, amplification).  D'une mani√®re g√©n√©rale, cela fonctionne comme suit: tout d'abord, g√©n√©rer un sous-ensemble d'apprentissage par s√©lection al√©atoire avec retour en fonction de certains poids d'√©chantillon (initialis√©s par des poids uniformes).  Deuxi√®mement, adapter un √©valuateur √† l'aide de ce sous-ensemble de formation.  Troisi√®mement, si un √©valuateur unique atteint une pr√©cision d√©passant le seuil d'acceptabilit√© (par exemple, dans un classificateur binaire, il est de 50% pour que le classificateur fonctionne mieux que la diseuse de bonne aventure), alors l'√©valuateur reste, sinon il est rejet√©.  Quatri√®mement, accordez plus de poids aux observations mal class√©es et moins de poids aux observations correctement class√©es.  Cinqui√®mement, r√©p√©tez les √©tapes pr√©c√©dentes jusqu'√† ce que N √©valuateurs soient re√ßus.  Sixi√®mement, la pr√©vision d'ensemble est la moyenne pond√©r√©e des pr√©visions individuelles de N mod√®les, o√π les poids sont d√©termin√©s par la pr√©cision des √©valuateurs individuels.  Il existe un certain nombre d'algorithmes boost√©s, dont le boosting adaptatif AdaBoost est l'un des plus populaires (Geron [2017]).  La figure 6.3 r√©sume le flux de d√©cision dans l'impl√©mentation standard de l'algorithme AdaBoost. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1d/u-/cr/1du-crm0gpkjgvy7lk45jj9f9ig.png" alt="image"></div><br><h3>  6.6.  Bagging vs Finance Boosting </h3><br>  D'apr√®s la description ci-dessus, plusieurs aspects rendent le boosting compl√®tement diff√©rent de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensachage</a> : <br><br><ul><li>  L'ajustement des classificateurs individuels est effectu√© s√©quentiellement. </li><li>  Les classificateurs pauvres sont rejet√©s. </li><li>  √Ä chaque it√©ration, les observations sont pond√©r√©es diff√©remment. </li></ul><br>  La pr√©vision d'ensemble est la moyenne pond√©r√©e des √©l√®ves individuels. <br><br>  Le principal avantage de la stimulation est qu'elle r√©duit √† la fois la variance et le biais dans les pr√©visions.  N√©anmoins, une correction du biais se produit en raison d'un risque accru de sur-ajustement.  On peut affirmer que dans les applications financi√®res, l'ensachage est g√©n√©ralement pr√©f√©rable √† l'augmentation.  L'ensachage r√©sout le probl√®me de sur-ajustement, tandis que le boosting r√©sout le probl√®me de sur-ajustement.  Le surapprentissage est souvent un probl√®me plus grave que le sous-ajustement, car l'ajustement trop serr√© de l'algorithme MO aux donn√©es financi√®res n'est pas du tout difficile en raison du faible rapport signal / bruit.  De plus, l'ensachage peut √™tre parall√©lis√©, tandis que le boost n√©cessite g√©n√©ralement une ex√©cution s√©quentielle. <br><br><h3>  6.7.  Ensachage pour l'√©volutivit√© </h3><br>  Comme vous le savez, certains algorithmes MO populaires ne se mettent pas tr√®s bien √† l'√©chelle en fonction de la taille de l'√©chantillon.  La m√©thode des machines √† vecteurs de support (SVM) en est un excellent exemple.  Si vous essayez de faire correspondre l'√©valuateur SVM √† plus d'un million d'observations, la convergence de l'algorithme peut prendre un certain temps.  Et m√™me apr√®s sa convergence, rien ne garantit que la solution est un optimum global ou qu'elle ne sera pas r√©align√©e. <br><br>  Une approche pratique consiste √† construire un algorithme ensach√© o√π l'√©valuateur de base appartient √† une classe qui ne s'adapte pas bien √† la taille de l'√©chantillon, comme SVM.  En d√©finissant cet √©valuateur de base, nous introduisons une condition stricte pour un arr√™t pr√©coce.  Par exemple, dans l'impl√©mentation des machines √† vecteurs de support (SVM) dans la biblioth√®que sklearn, vous pouvez d√©finir une valeur faible pour le param√®tre max_iter, par exemple, les it√©rations 1E5.  La valeur par d√©faut est max_iter = -1, ce qui indique √† l'√©valuateur de continuer l'it√©ration jusqu'√† ce que les erreurs tombent en dessous du niveau de tol√©rance.  D'un autre c√¥t√©, vous pouvez augmenter le niveau de tol√©rance avec le param√®tre tol, qui est par d√©faut tol = iE-3.  Chacune de ces deux options entra√Ænera un arr√™t anticip√©.  Vous pouvez arr√™ter rapidement d'autres algorithmes en utilisant des param√®tres √©quivalents, tels que le nombre de niveaux dans une for√™t al√©atoire (max_depth) ou la fraction pond√©r√©e minimale de la somme totale des poids (tous les √©chantillons d'entr√©e) devant se trouver sur un n≈ìud feuille (min_weight_fraction_leaf). <br><br>  √âtant donn√© que les algorithmes ensach√©s peuvent √™tre parall√©lis√©s, nous transformons une grande t√¢che s√©quentielle en une s√©rie de plus petits qui sont ex√©cut√©s simultan√©ment.  Bien s√ªr, un arr√™t pr√©coce augmentera la variance des r√©sultats des √©valuateurs de base individuels;  cependant, cette augmentation peut √™tre plus que compens√©e par la diminution de la variance associ√©e √† l'algorithme ensach√©.  Vous pouvez contr√¥ler cette r√©duction en ajoutant de nouveaux √©valuateurs de base ind√©pendants.  Utilis√© de cette mani√®re, l'ensachage vous permet d'obtenir des estimations rapides et robustes sur de tr√®s grands ensembles de donn√©es. <br><br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  25% de r√©duction pour les livres de pr√©commande Khabrozhiteley sur un coupon - <b>Machine Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr445780/">https://habr.com/ru/post/fr445780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr445762/index.html">Les math√©matiques sont-elles logiques ou pourquoi les th√©ories axiomatiques sont-elles paradoxales</a></li>
<li><a href="../fr445764/index.html">Ma fa√ßon de cr√©er des composants principaux dans la figure</a></li>
<li><a href="../fr445766/index.html">√Ä propos du centre de donn√©es en toute honn√™tet√©: comment nous avons r√©solu le probl√®me de la poussi√®re dans les salles de serveurs du centre de donn√©es</a></li>
<li><a href="../fr445772/index.html">Syst√®me de paiement rapide ou l'impossible est possible</a></li>
<li><a href="../fr445778/index.html">10 nouveaux cours gratuits sur les services cognitifs et Azure</a></li>
<li><a href="../fr445782/index.html">Une s√©lection de tournevis geek et d'outils polyvalents inhabituels de Leatherman √† Xiaomi</a></li>
<li><a href="../fr445784/index.html">Croissance professionnelle des employ√©s - qu'est-ce que c'est et pourquoi est-ce n√©cessaire: nous communiquons avec Dodo Pizza, Icons8 et Evil Martians</a></li>
<li><a href="../fr445786/index.html">Cryptographie en Java. Classe KeyStore</a></li>
<li><a href="../fr445788/index.html">Vid√©osurveillance en nuage √† faire soi-m√™me: nouvelles fonctionnalit√©s du SDK Web Ivideon</a></li>
<li><a href="../fr445792/index.html">Comment nous d√©veloppons la documentation dans un projet Embox ouvert</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>