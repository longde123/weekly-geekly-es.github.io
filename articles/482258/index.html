<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游낄 游븸游낗 郊윒잺 C칩mo funcionan las redes neuronales y por qu칠 comenzaron a traer mucho dinero 游꿨 游븺游낖 游눊游</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las redes neuronales han crecido de un estado de curiosidad acad칠mica a una industria masiva. 


 Durante la 칰ltima d칠cada, las computadoras han mejor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C칩mo funcionan las redes neuronales y por qu칠 comenzaron a traer mucho dinero</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482258/"><h3>  Las redes neuronales han crecido de un estado de curiosidad acad칠mica a una industria masiva. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/21a/fd3/36a21afd35a805d95a2a67b2ec52080a.jpg"><br><br>  Durante la 칰ltima d칠cada, las computadoras han mejorado significativamente su capacidad de comprender el mundo que las rodea.  El software para equipos fotogr치ficos reconoce autom치ticamente los rostros de las personas.  Los tel칠fonos inteligentes convierten la voz en texto.  Los robomobiles reconocen objetos en el camino y evitan colisiones con ellos. <br><br>  En el coraz칩n de todos estos avances est치 la tecnolog칤a de inteligencia artificial (IA) llamada aprendizaje profundo (GO).  GO se basa en redes neuronales (NS), estructuras de datos inspiradas en redes compuestas de neuronas biol칩gicas.  Los NS est치n organizados en capas, y las entradas de una capa est치n conectadas a las salidas de la vecina. <br><br>  Los inform치ticos han estado experimentando con NS desde la d칠cada de 1950.  Sin embargo, la base de la vasta industria de GO de hoy se estableci칩 por dos avances importantes: uno ocurri칩 en 1986, el segundo en 2012. El avance de 2012, la revoluci칩n de GO, se asoci칩 con el descubrimiento de que el uso de NS con una gran cantidad de capas nos permitir치 mejorar significativamente su eficiencia.  El descubrimiento fue facilitado por los crecientes vol칰menes de datos y potencia inform치tica. <br><a name="habracut"></a><br>  En este art칤culo, le presentaremos el mundo de la Asamblea Nacional.  Explicaremos qu칠 es NS, c칩mo funcionan y de d칩nde provienen.  Y estudiaremos por qu칠, a pesar de muchas d칠cadas de investigaci칩n previa, los NS se convirtieron en algo realmente 칰til solo en 2012. <br><br><h2>  Las redes neuronales aparecieron en la d칠cada de 1950. </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/770/c2e/327/770c2e3276b0e875a99025f4887dda36.jpg"><br>  <i>Frank Rosenblatt est치 trabajando en su perceptr칩n, uno de los primeros modelos de NS</i> <br><br>  La idea de la Asamblea Nacional es bastante antigua, al menos seg칰n los est치ndares de la inform치tica.  En 1957, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt,</a> de la Universidad de Cornell, public칩 un <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">informe que</a> describe un concepto de NS temprano llamado perceptr칩n.  En 1958, con el apoyo de la Marina de los EE. UU., Cre칩 un sistema primitivo capaz de analizar 20x20 p칤xeles y reconocer formas geom칠tricas simples. <br><br>  El objetivo principal de Rosenblatt no era crear un sistema pr치ctico de clasificaci칩n de im치genes.  Trat칩 de entender c칩mo funciona el cerebro humano, creando sistemas inform치ticos organizados a su imagen.  Sin embargo, este concepto ha generado un entusiasmo excesivo por parte de terceros. <br><br>  "Hoy, la Marina de los EE. UU. Ha revelado al mundo el germen de una computadora electr칩nica, que se espera que pueda caminar, hablar, ver, escribir, reproducirse y ser consciente de su existencia", escribi칩 el New York Times. <br><br>  De hecho, cada neurona en el NS es solo una funci칩n matem치tica.  Cada neurona calcula la suma ponderada de los datos de entrada: cuanto mayor es el peso de entrada, m치s fuertemente afectan estos datos de entrada a la salida de la neurona.  Luego, la suma ponderada se alimenta a la funci칩n de "activaci칩n" no lineal; en este paso, los NS pueden simular fen칩menos no lineales complejos. <br><br>  Las habilidades de los primeros perceptrones con los que Rosenblatt experiment칩, y NS en general, provienen de su capacidad de "aprender" con ejemplos.  Los NS se entrenan ajustando los pesos de entrada de las neuronas en funci칩n de los resultados de la red con los datos de entrada seleccionados, por ejemplo.  Si la red clasifica correctamente la imagen, los pesos que contribuyen a la respuesta correcta aumentan, mientras que otros disminuyen.  Si la red est치 mal, los pesos se ajustan en la otra direcci칩n. <br><br>  Tal procedimiento permiti칩 a los NS tempranos "aprender" de una manera que recordaba el comportamiento del sistema nervioso humano.  La exageraci칩n que rodea este enfoque no se detuvo en la d칠cada de 1960.  Sin embargo, el <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_(%25D0%25BA%25D0%25BD%25D0%25B8%25D0%25B3%25D0%25B0)">influyente libro de</a> 1969 de los autores de los cient칤ficos inform치ticos Marvin Minsky y Seymour Papert mostr칩 que estos primeros NA tienen limitaciones significativas. <br><br>  Los primeros NS de Rosenblatt ten칤an solo una o dos capas entrenadas.  Minsky y Papert demostraron que tales NS son matem치ticamente incapaces de modelar fen칩menos complejos del mundo real. <br><br>  En principio, las NS m치s profundas eran m치s capaces.  Sin embargo, tal NS sobrecargar칤a esos recursos inform치ticos miserables que ten칤an las computadoras en ese momento.  Los algoritmos de <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2%25D0%25BE%25D1%2581%25D1%2585%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC_%25D0%25BA_%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B5">b칰squeda ascendente</a> m치s simples utilizados en los primeros NS no escalaban para NS m치s profundos. <br><br>  Como resultado, la Asamblea Nacional perdi칩 todo el apoyo en los a침os setenta y principios de los ochenta, fue parte de la era del "invierno de la IA". <br><br><h2>  Algoritmo innovador </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/970/c5b/807/970c5b8074a5b4516be251bd4b9a31b0.jpg"><br>  <i>Mi propia red neuronal basada en "equipo blando" cree que la probabilidad de tener un hot dog en esta foto es 1. 춰Nos haremos ricos!</i> <br><br>  La suerte volvi칩 a recurrir al NS gracias al famoso <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">trabajo de</a> 1986, que introdujo el concepto de propagaci칩n hacia atr치s, un m칠todo pr치ctico para ense침ar NS. <br><br>  Suponga que trabaja como programador en una compa침칤a de software imaginaria y se le ha indicado que cree una aplicaci칩n que determine si hay un hot dog en la imagen.  Comienza a trabajar con un NS inicializado al azar, que toma una imagen de entrada y genera un valor de 0 a 1, donde 1 significa "hot dog" y 0 significa "no hot dog". <br><br>  Para entrenar a la red, recopila miles de im치genes, debajo de cada una de las cuales hay una etiqueta que indica si hay un hot dog en esta imagen.  Le das de comer la primera imagen, y hay un hot dog en ella, en la red neuronal.  Da un valor de salida de 0.07, lo que significa "no hot dog".  Esta es la respuesta incorrecta;  la red deber칤a haber devuelto una respuesta cercana a 1. <br><br>  El objetivo del algoritmo de retropropagaci칩n es ajustar los pesos de entrada para que la red produzca un valor m치s alto si se le vuelve a dar esta imagen, y, preferiblemente, otras im치genes donde hay perros calientes.  Para esto, el algoritmo de retropropagaci칩n comienza examinando las neuronas de entrada de la capa de salida.  Cada valor tiene una variable de peso.  El algoritmo de retropropagaci칩n ajusta cada peso en una direcci칩n tal que el NS da un valor m치s alto.  Cuanto mayor sea el valor de entrada, m치s aumenta su peso. <br><br>  Hasta ahora, estoy describiendo el ascenso m치s simple a la cima familiar para los investigadores en la d칠cada de 1960.  El avance de la retropropagaci칩n fue el siguiente paso: el algoritmo utiliza derivadas parciales para distribuir la "falla" de la salida incorrecta entre las entradas de las neuronas.  El algoritmo calcula c칩mo un peque침o cambio en cada valor de entrada afectar치 la salida final de una neurona, y si este cambio acercar치 el resultado a la respuesta correcta, o viceversa. <br><br>  El resultado es un conjunto de valores de error para cada neurona en la capa anterior; de hecho, una se침al que eval칰a si el valor de cada neurona es demasiado grande o demasiado peque침o.  Luego, el algoritmo repite el proceso de ajuste para nuevas neuronas desde la segunda capa [desde el final].  Cambia ligeramente los pesos de entrada de cada neurona para acercar la red a la respuesta correcta. <br><br>  Luego, el algoritmo nuevamente utiliza derivadas parciales para calcular c칩mo el valor de cada entrada de la capa anterior afect칩 los errores de salida de esta capa, y propaga estos errores nuevamente a la capa anterior, donde el proceso se repite nuevamente. <br><br>  Este es solo un modelo simplificado de retropropagaci칩n.  Si necesita detalles matem치ticos detallados, le recomiendo el libro de Michael Nielsen sobre este tema [ <a href="https://habr.com/ru/post/456738/">y tenemos su traducci칩n</a> / aprox.  transl.].  Para nuestros prop칩sitos, es suficiente que la distribuci칩n inversa cambie radicalmente el rango de NS entrenado.  Las personas ya no estaban limitadas a redes simples con una o dos capas.  Podr칤an crear redes con cinco, diez o cincuenta capas, y estas redes podr칤an tener una estructura interna arbitrariamente compleja. <br><br>  La invenci칩n de la propagaci칩n hacia atr치s lanz칩 el segundo auge de la Asamblea Nacional, que comenz칩 a producir resultados pr치cticos.  En 1998, un grupo de investigadores de AT&amp;T mostr칩 c칩mo las redes neuronales se pueden usar para reconocer n칰meros escritos a mano, lo que permiti칩 automatizar el procesamiento de cheques. <br><br>  "El mensaje principal de este trabajo es que podemos crear sistemas mejorados para reconocer patrones, confiando m치s en el aprendizaje autom치tico y menos en la heur칤stica desarrollada manualmente", escribieron los autores. <br><br>  Y, sin embargo, en esta fase, los NS fueron solo una de las muchas tecnolog칤as a disposici칩n de los investigadores de aprendizaje autom치tico.  Cuando estudi칠 en un curso de IA en el instituto en 2008, las redes neuronales eran solo uno de los nueve algoritmos MO, de los cuales pod칤amos elegir la opci칩n adecuada para la tarea.  Sin embargo, GO ya se estaba preparando para eclipsar el resto de la tecnolog칤a. <br><br>  Big data demuestra el poder del aprendizaje profundo <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br>  <i>Relajaci칩n detectada.</i>  <i>Posibilidad de la playa 1.0.</i>  <i>Comenzamos el procedimiento de usar Mai Tai.</i> <br><br>  La retropropagaci칩n facilit칩 el proceso de c치lculo de NS, pero las redes m치s profundas a칰n necesitaban m치s recursos inform치ticos que las peque침as.  Los resultados de los estudios realizados en los a침os 1990 y 2000 a menudo mostraron que era posible obtener cada vez menos beneficios de una complicaci칩n adicional de la NS. <br><br>  Luego, el famoso trabajo de 2012 cambi칩 el pensamiento de la gente, que describi칩 el NS bajo el nombre de AlexNet, llamado as칤 por el investigador l칤der Alex Krizhevsky.  Al igual que las redes m치s profundas, podr칤an proporcionar una eficiencia innovadora, pero solo en combinaci칩n con una gran cantidad de energ칤a de la computadora y una gran cantidad de datos. <br><br>  AlexNet ha desarrollado un tr칤o de inform치ticos de la Universidad de Toronto para participar en la competencia cient칤fica ImageNet.  Los organizadores del concurso recopilaron un mill칩n de im치genes en Internet, cada una de las cuales fue etiquetada y asignada a una de las miles de categor칤as de objetos, por ejemplo, "cereza", "portacontenedores" o "leopardo".  Se pidi칩 a los investigadores de IA que entrenaran sus programas de MO en partes de estas im치genes, y luego trataran de poner las etiquetas correctas para otras im치genes que el software no hab칤a encontrado antes.  El software tuvo que seleccionar cinco posibles etiquetas para cada imagen, y el intento se consider칩 exitoso si una de ellas coincid칤a con la real. <br><br>  Esta fue una tarea dif칤cil, y hasta 2012 los resultados no fueron muy buenos.  Para el ganador de 2011, la tasa de error fue del 25%. <br><br>  En 2012, el equipo de AlexNet super칩 a todos los competidores al dar respuestas con un 15% de errores.  Para el competidor m치s cercano, esta cifra fue del 26%. <br><br>  Investigadores de Toronto combinaron varias t칠cnicas para lograr resultados innovadores.  Uno de ellos fue el uso de <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">neurosis convolucionales</a> (SNS).  De hecho, el SNA, por as칤 decirlo, entrena peque침as redes neuronales, cuyos datos de entrada son cuadrados con un lado de 7 a 11 p칤xeles, y luego los "superpone" en una imagen m치s grande. <br><br>  "Es como si tomas una plantilla peque침a o una plantilla y tratas de compararla con cada punto de la imagen", nos dijo el investigador de IA Jie Tan el a침o pasado.  - 쯊ienes una plantilla de un perro, y la pegas a la imagen y ves si hay un perro all칤?  Si no, mueva la plantilla.  Y as칤, para toda la imagen.  Y no importa d칩nde aparezca el perro en la imagen.  La plantilla coincidir치 con ella.  Cada subsecci칩n de red no debe convertirse en un clasificador de perro separado ". <br><br>  Otro factor clave de 칠xito para AlexNet ha sido el uso de tarjetas gr치ficas para acelerar el proceso de aprendizaje.  Las tarjetas gr치ficas tienen un poder de procesamiento paralelo, muy adecuado para la inform치tica repetitiva necesaria para entrenar una red neuronal.  Al transferir la carga de la inform치tica a un par de GPU, la Nvidia GTX 580, con 3 GB de memoria cada una, los investigadores pudieron desarrollar y entrenar una red extremadamente grande y compleja.  AlexNet ten칤a ocho capas entrenables, 650,000 neuronas y 60 millones de par치metros. <br><br>  Finalmente, el 칠xito de AlexNet tambi칠n fue asegurado por el gran tama침o de la base de datos de im치genes de capacitaci칩n de ImageNet: un mill칩n de piezas.  Se necesitan muchas im치genes para ajustar 60 millones de par치metros.  Para lograr una victoria decisiva, AlexNet fue ayudado por una combinaci칩n de una red compleja y un gran conjunto de datos. <br><br>  Me pregunto por qu칠 tal avance no ocurri칩 antes: <br><br><ul><li>  El par de GPU de grado de consumo utilizado por los investigadores de AlexNet estaba lejos de ser el dispositivo inform치tico m치s poderoso para 2012.  Cinco e incluso diez a침os antes de eso, hab칤a computadoras m치s potentes.  Adem치s, la tecnolog칤a para acelerar el aprendizaje de NS mediante tarjetas gr치ficas se conoce desde al menos 2004. </li><li>  La base de un mill칩n de im치genes era inusualmente grande para ense침ar algoritmos de MO en 2012, sin embargo, recopilar dichos datos no era una tecnolog칤a nueva para ese a침o.  Un equipo de investigaci칩n bien financiado podr칤a reunir f치cilmente una base de datos de este tama침o cinco o diez a침os antes. </li><li>  Los algoritmos principales utilizados en AlexNet no eran nuevos.  El algoritmo de retropropagaci칩n para 2012 ya exist칤a durante aproximadamente un cuarto de siglo.  Las ideas clave relacionadas con las redes neuronales convolucionales se desarrollaron en los a침os ochenta y noventa. </li></ul><br>  Por lo tanto, cada uno de los elementos de 칠xito de AlexNet exist칤a por separado mucho antes de que ocurriera el avance.  Obviamente, a nadie se le ocurri칩 combinarlos, en su mayor parte porque nadie sab칤a cu치n poderosa ser칤a esta combinaci칩n. <br><br>  El aumento de la profundidad del NS pr치cticamente no mejor칩 la eficiencia de su trabajo si no usaban conjuntos de datos de entrenamiento lo suficientemente grandes.  Y expandir el conjunto de datos no mejor칩 el rendimiento de las redes peque침as.  Para ver el aumento de la eficiencia, necesit치bamos redes m치s profundas y conjuntos de datos m치s grandes, adem치s de una potencia inform치tica considerable que nos permitiera llevar a cabo el proceso de capacitaci칩n en un per칤odo de tiempo razonable.  El equipo de AlexNet fue el primero en reunir los tres elementos en un solo programa. <br><br><h2>  El boom del aprendizaje profundo </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/ef0/fb4/922/ef0fb4922c3164251c722134b84460e3.jpg"><br><br>  Muchas personas notaron la demostraci칩n de todo el poder del NS profundo, proporcionado por una cantidad suficiente de datos de capacitaci칩n, tanto entre cient칤ficos, investigadores como entre representantes de la industria. <br><br>  El primer concurso de ImageNet para cambiar.  Hasta 2012, la mayor칤a de los concursantes usaban tecnolog칤as distintas al aprendizaje profundo.  En la competencia de 2013, como escribieron los patrocinadores, "la mayor칤a" de los concursantes usaron GO. <br><br>  El porcentaje de errores entre los ganadores disminuy칩 gradualmente, de un impresionante 16% en AlexNet en 2012 a 2.3% en 2017: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ba/95e/b4b/9ba95eb4baee6580ea97ac76347a02e4.png"><br><br>  La revoluci칩n GO se extendi칩 r치pidamente por toda la industria.  En 2013, Google adquiri칩 una startup formada por los autores de AlexNet y utiliz칩 su tecnolog칤a como base para la funci칩n de b칰squeda de im치genes en Google Photos.  Para 2014, Facebook promocionaba su propio software que reconoce im치genes usando GO.  Apple ha estado utilizando GO para el reconocimiento facial en iOS desde al menos 2016. <br><br>  GO tambi칠n subyace a la reciente mejora en la tecnolog칤a de reconocimiento de voz.  Siri de Apple, Alexa de Amazon, Cortana de Microsoft y el asistente de Google usan GO, ya sea para comprender las palabras de una persona, o para generar una voz m치s natural, o ambas. <br><br>  En los 칰ltimos a침os, ha surgido una tendencia autosostenida en la industria, en la que el aumento de la potencia inform치tica, el volumen de datos y la profundidad de la red se apoyan mutuamente.  El equipo de AlexNet us칩 la GPU porque ofrec칤an computaci칩n paralela a un precio razonable.  Pero en los 칰ltimos a침os, cada vez m치s empresas han comenzado a desarrollar sus propios chips, dise침ados espec칤ficamente para su uso en el campo de MO. <br><br>  Google anunci칩 el lanzamiento del chip Tensor Processing Unit espec칤ficamente dise침ado para el NS en 2016. En el mismo a침o, Nvidia anunci칩 el lanzamiento de una nueva GPU llamada Tesla P100, optimizada para el NS.  Intel respondi칩 a la llamada con su chip AI en 2017. En 2018, Amazon anunci칩 el lanzamiento de su propio chip AI, que puede usarse como parte de los servicios en la nube de la compa침칤a.  Incluso se dice que Microsoft est치 trabajando en su chip AI. <br><br>  Los fabricantes de tel칠fonos inteligentes tambi칠n est치n trabajando en chips que permitir치n que los dispositivos m칩viles realicen m치s c칩mputo utilizando NS localmente, sin tener que cargar datos a los servidores.  Tal computaci칩n en dispositivos reduce la latencia y mejora la privacidad. <br><br>  Incluso Tesla entr칩 en este juego con fichas especiales.  Este a침o, Tesla mostr칩 una nueva y poderosa computadora, optimizada para calcular NS.  Tesla lo nombr칩 Full Self-Driving Computer y lo present칩 como un momento clave en la estrategia de la compa침칤a para convertir la flota de Tesla en veh칤culos rob칩ticos. <br><br>  La disponibilidad de capacidades inform치ticas optimizadas para IA ha generado una solicitud de los datos necesarios para entrenar NS cada vez m치s complejas.  Esta din치mica es m치s evidente en el sector robom칩vil, donde las empresas recopilan datos sobre millones de kil칩metros de carreteras reales.  Tesla puede recopilar estos datos autom치ticamente de los autom칩viles de los usuarios, y sus competidores, Waymo y Cruise, pagaron a los conductores que conduc칤an sus autom칩viles en v칤as p칰blicas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/80a/3be/2eb80a3be8fd038777829faff752e8bf.jpg"><br><br>  La solicitud de datos ofrece una ventaja a las grandes compa침칤as en l칤nea que ya tienen acceso a grandes vol칰menes de datos de usuarios. <br><br>  El aprendizaje profundo ha conquistado tantas 치reas diferentes debido a su extrema flexibilidad.  Las d칠cadas de prueba y error han permitido a los investigadores desarrollar los bloques de construcci칩n b치sicos para las tareas m치s comunes en el campo de MO, como las redes de convoluci칩n para el reconocimiento eficiente de im치genes.  Sin embargo, si tiene una red de alto nivel adecuada para el esquema y suficientes datos, entonces el proceso de capacitaci칩n ser치 simple.  Los NS profundos pueden reconocer una gama excepcionalmente amplia de patrones complejos sin la gu칤a especial de los desarrolladores humanos. <br><br>  Hay limitaciones, por supuesto.  Por ejemplo, algunas personas se entregaron a la idea de entrenar robom칩viles con la ayuda de solo GO, es decir, alimentar im치genes recibidas de una c치mara, una red neuronal y recibir instrucciones de ella para girar el volante y el pedal.  Soy esc칠ptico de este enfoque.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La Asamblea Nacional a칰n no ha demostrado la capacidad de llevar a cabo un razonamiento l칩gico complejo, que se requiere para comprender ciertas condiciones que surgen en el camino. </font><font style="vertical-align: inherit;">Adem치s, los NS son "cajas negras", cuyo flujo de trabajo es pr치cticamente invisible. </font><font style="vertical-align: inherit;">Ser칤a dif칤cil evaluar y confirmar la seguridad de dicho sistema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, GO permiti칩 dar saltos muy amplios en una gama inesperadamente grande de aplicaciones. </font><font style="vertical-align: inherit;">En los pr칩ximos a침os, uno puede esperar el pr칩ximo progreso en esta 치rea.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/482258/">https://habr.com/ru/post/482258/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../482248/index.html">Compartimos nuestra experiencia de c칩mo se muestran los SSD en RAID y qu칠 nivel de matriz es m치s rentable</a></li>
<li><a href="../482250/index.html">Una m치quina de estado simple para VueJS</a></li>
<li><a href="../482252/index.html">Inodoro autom치tico para gatos - continuaci칩n</a></li>
<li><a href="../482254/index.html">Experimento VonmoTrade. Parte 3: Libro de warrants. Procesamiento y almacenamiento de informaci칩n comercial.</a></li>
<li><a href="../482256/index.html">La IA y el futuro del trabajo: perspectivas de empleo en el futuro cercano</a></li>
<li><a href="../482260/index.html">TelegramBot. La funcionalidad b치sica. Pegatinas y emoticones. (Parte 3)</a></li>
<li><a href="../482262/index.html">C칩mo iniciar sesi칩n en Talend Open Studio</a></li>
<li><a href="../482264/index.html">Brasil, magia oscura, Mortal Kombat, Marte y 15,000 personas. Resultados del a침o de Ontiko</a></li>
<li><a href="../482268/index.html">Megaestructuras del futuro: la esfera Dyson, el motor estelar y la "bomba de agujero negro"</a></li>
<li><a href="../482272/index.html">Elecci칩n de un almac칠n de datos para Prometheus: Thanos vs VictoriaMetrics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>