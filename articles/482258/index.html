<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üè© üßëüèΩ ‚ñ∂Ô∏è C√≥mo funcionan las redes neuronales y por qu√© comenzaron a traer mucho dinero üé∂ üßîüèº üíáüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las redes neuronales han crecido de un estado de curiosidad acad√©mica a una industria masiva. 


 Durante la √∫ltima d√©cada, las computadoras han mejor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo funcionan las redes neuronales y por qu√© comenzaron a traer mucho dinero</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482258/"><h3>  Las redes neuronales han crecido de un estado de curiosidad acad√©mica a una industria masiva. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/21a/fd3/36a21afd35a805d95a2a67b2ec52080a.jpg"><br><br>  Durante la √∫ltima d√©cada, las computadoras han mejorado significativamente su capacidad de comprender el mundo que las rodea.  El software para equipos fotogr√°ficos reconoce autom√°ticamente los rostros de las personas.  Los tel√©fonos inteligentes convierten la voz en texto.  Los robomobiles reconocen objetos en el camino y evitan colisiones con ellos. <br><br>  En el coraz√≥n de todos estos avances est√° la tecnolog√≠a de inteligencia artificial (IA) llamada aprendizaje profundo (GO).  GO se basa en redes neuronales (NS), estructuras de datos inspiradas en redes compuestas de neuronas biol√≥gicas.  Los NS est√°n organizados en capas, y las entradas de una capa est√°n conectadas a las salidas de la vecina. <br><br>  Los inform√°ticos han estado experimentando con NS desde la d√©cada de 1950.  Sin embargo, la base de la vasta industria de GO de hoy se estableci√≥ por dos avances importantes: uno ocurri√≥ en 1986, el segundo en 2012. El avance de 2012, la revoluci√≥n de GO, se asoci√≥ con el descubrimiento de que el uso de NS con una gran cantidad de capas nos permitir√° mejorar significativamente su eficiencia.  El descubrimiento fue facilitado por los crecientes vol√∫menes de datos y potencia inform√°tica. <br><a name="habracut"></a><br>  En este art√≠culo, le presentaremos el mundo de la Asamblea Nacional.  Explicaremos qu√© es NS, c√≥mo funcionan y de d√≥nde provienen.  Y estudiaremos por qu√©, a pesar de muchas d√©cadas de investigaci√≥n previa, los NS se convirtieron en algo realmente √∫til solo en 2012. <br><br><h2>  Las redes neuronales aparecieron en la d√©cada de 1950. </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/770/c2e/327/770c2e3276b0e875a99025f4887dda36.jpg"><br>  <i>Frank Rosenblatt est√° trabajando en su perceptr√≥n, uno de los primeros modelos de NS</i> <br><br>  La idea de la Asamblea Nacional es bastante antigua, al menos seg√∫n los est√°ndares de la inform√°tica.  En 1957, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt,</a> de la Universidad de Cornell, public√≥ un <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">informe que</a> describe un concepto de NS temprano llamado perceptr√≥n.  En 1958, con el apoyo de la Marina de los EE. UU., Cre√≥ un sistema primitivo capaz de analizar 20x20 p√≠xeles y reconocer formas geom√©tricas simples. <br><br>  El objetivo principal de Rosenblatt no era crear un sistema pr√°ctico de clasificaci√≥n de im√°genes.  Trat√≥ de entender c√≥mo funciona el cerebro humano, creando sistemas inform√°ticos organizados a su imagen.  Sin embargo, este concepto ha generado un entusiasmo excesivo por parte de terceros. <br><br>  "Hoy, la Marina de los EE. UU. Ha revelado al mundo el germen de una computadora electr√≥nica, que se espera que pueda caminar, hablar, ver, escribir, reproducirse y ser consciente de su existencia", escribi√≥ el New York Times. <br><br>  De hecho, cada neurona en el NS es solo una funci√≥n matem√°tica.  Cada neurona calcula la suma ponderada de los datos de entrada: cuanto mayor es el peso de entrada, m√°s fuertemente afectan estos datos de entrada a la salida de la neurona.  Luego, la suma ponderada se alimenta a la funci√≥n de "activaci√≥n" no lineal; en este paso, los NS pueden simular fen√≥menos no lineales complejos. <br><br>  Las habilidades de los primeros perceptrones con los que Rosenblatt experiment√≥, y NS en general, provienen de su capacidad de "aprender" con ejemplos.  Los NS se entrenan ajustando los pesos de entrada de las neuronas en funci√≥n de los resultados de la red con los datos de entrada seleccionados, por ejemplo.  Si la red clasifica correctamente la imagen, los pesos que contribuyen a la respuesta correcta aumentan, mientras que otros disminuyen.  Si la red est√° mal, los pesos se ajustan en la otra direcci√≥n. <br><br>  Tal procedimiento permiti√≥ a los NS tempranos "aprender" de una manera que recordaba el comportamiento del sistema nervioso humano.  La exageraci√≥n que rodea este enfoque no se detuvo en la d√©cada de 1960.  Sin embargo, el <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_(%25D0%25BA%25D0%25BD%25D0%25B8%25D0%25B3%25D0%25B0)">influyente libro de</a> 1969 de los autores de los cient√≠ficos inform√°ticos Marvin Minsky y Seymour Papert mostr√≥ que estos primeros NA tienen limitaciones significativas. <br><br>  Los primeros NS de Rosenblatt ten√≠an solo una o dos capas entrenadas.  Minsky y Papert demostraron que tales NS son matem√°ticamente incapaces de modelar fen√≥menos complejos del mundo real. <br><br>  En principio, las NS m√°s profundas eran m√°s capaces.  Sin embargo, tal NS sobrecargar√≠a esos recursos inform√°ticos miserables que ten√≠an las computadoras en ese momento.  Los algoritmos de <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2%25D0%25BE%25D1%2581%25D1%2585%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC_%25D0%25BA_%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B5">b√∫squeda ascendente</a> m√°s simples utilizados en los primeros NS no escalaban para NS m√°s profundos. <br><br>  Como resultado, la Asamblea Nacional perdi√≥ todo el apoyo en los a√±os setenta y principios de los ochenta, fue parte de la era del "invierno de la IA". <br><br><h2>  Algoritmo innovador </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/970/c5b/807/970c5b8074a5b4516be251bd4b9a31b0.jpg"><br>  <i>Mi propia red neuronal basada en "equipo blando" cree que la probabilidad de tener un hot dog en esta foto es 1. ¬°Nos haremos ricos!</i> <br><br>  La suerte volvi√≥ a recurrir al NS gracias al famoso <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">trabajo de</a> 1986, que introdujo el concepto de propagaci√≥n hacia atr√°s, un m√©todo pr√°ctico para ense√±ar NS. <br><br>  Suponga que trabaja como programador en una compa√±√≠a de software imaginaria y se le ha indicado que cree una aplicaci√≥n que determine si hay un hot dog en la imagen.  Comienza a trabajar con un NS inicializado al azar, que toma una imagen de entrada y genera un valor de 0 a 1, donde 1 significa "hot dog" y 0 significa "no hot dog". <br><br>  Para entrenar a la red, recopila miles de im√°genes, debajo de cada una de las cuales hay una etiqueta que indica si hay un hot dog en esta imagen.  Le das de comer la primera imagen, y hay un hot dog en ella, en la red neuronal.  Da un valor de salida de 0.07, lo que significa "no hot dog".  Esta es la respuesta incorrecta;  la red deber√≠a haber devuelto una respuesta cercana a 1. <br><br>  El objetivo del algoritmo de retropropagaci√≥n es ajustar los pesos de entrada para que la red produzca un valor m√°s alto si se le vuelve a dar esta imagen, y, preferiblemente, otras im√°genes donde hay perros calientes.  Para esto, el algoritmo de retropropagaci√≥n comienza examinando las neuronas de entrada de la capa de salida.  Cada valor tiene una variable de peso.  El algoritmo de retropropagaci√≥n ajusta cada peso en una direcci√≥n tal que el NS da un valor m√°s alto.  Cuanto mayor sea el valor de entrada, m√°s aumenta su peso. <br><br>  Hasta ahora, estoy describiendo el ascenso m√°s simple a la cima familiar para los investigadores en la d√©cada de 1960.  El avance de la retropropagaci√≥n fue el siguiente paso: el algoritmo utiliza derivadas parciales para distribuir la "falla" de la salida incorrecta entre las entradas de las neuronas.  El algoritmo calcula c√≥mo un peque√±o cambio en cada valor de entrada afectar√° la salida final de una neurona, y si este cambio acercar√° el resultado a la respuesta correcta, o viceversa. <br><br>  El resultado es un conjunto de valores de error para cada neurona en la capa anterior; de hecho, una se√±al que eval√∫a si el valor de cada neurona es demasiado grande o demasiado peque√±o.  Luego, el algoritmo repite el proceso de ajuste para nuevas neuronas desde la segunda capa [desde el final].  Cambia ligeramente los pesos de entrada de cada neurona para acercar la red a la respuesta correcta. <br><br>  Luego, el algoritmo nuevamente utiliza derivadas parciales para calcular c√≥mo el valor de cada entrada de la capa anterior afect√≥ los errores de salida de esta capa, y propaga estos errores nuevamente a la capa anterior, donde el proceso se repite nuevamente. <br><br>  Este es solo un modelo simplificado de retropropagaci√≥n.  Si necesita detalles matem√°ticos detallados, le recomiendo el libro de Michael Nielsen sobre este tema [ <a href="https://habr.com/ru/post/456738/">y tenemos su traducci√≥n</a> / aprox.  transl.].  Para nuestros prop√≥sitos, es suficiente que la distribuci√≥n inversa cambie radicalmente el rango de NS entrenado.  Las personas ya no estaban limitadas a redes simples con una o dos capas.  Podr√≠an crear redes con cinco, diez o cincuenta capas, y estas redes podr√≠an tener una estructura interna arbitrariamente compleja. <br><br>  La invenci√≥n de la propagaci√≥n hacia atr√°s lanz√≥ el segundo auge de la Asamblea Nacional, que comenz√≥ a producir resultados pr√°cticos.  En 1998, un grupo de investigadores de AT&amp;T mostr√≥ c√≥mo las redes neuronales se pueden usar para reconocer n√∫meros escritos a mano, lo que permiti√≥ automatizar el procesamiento de cheques. <br><br>  "El mensaje principal de este trabajo es que podemos crear sistemas mejorados para reconocer patrones, confiando m√°s en el aprendizaje autom√°tico y menos en la heur√≠stica desarrollada manualmente", escribieron los autores. <br><br>  Y, sin embargo, en esta fase, los NS fueron solo una de las muchas tecnolog√≠as a disposici√≥n de los investigadores de aprendizaje autom√°tico.  Cuando estudi√© en un curso de IA en el instituto en 2008, las redes neuronales eran solo uno de los nueve algoritmos MO, de los cuales pod√≠amos elegir la opci√≥n adecuada para la tarea.  Sin embargo, GO ya se estaba preparando para eclipsar el resto de la tecnolog√≠a. <br><br>  Big data demuestra el poder del aprendizaje profundo <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br>  <i>Relajaci√≥n detectada.</i>  <i>Posibilidad de la playa 1.0.</i>  <i>Comenzamos el procedimiento de usar Mai Tai.</i> <br><br>  La retropropagaci√≥n facilit√≥ el proceso de c√°lculo de NS, pero las redes m√°s profundas a√∫n necesitaban m√°s recursos inform√°ticos que las peque√±as.  Los resultados de los estudios realizados en los a√±os 1990 y 2000 a menudo mostraron que era posible obtener cada vez menos beneficios de una complicaci√≥n adicional de la NS. <br><br>  Luego, el famoso trabajo de 2012 cambi√≥ el pensamiento de la gente, que describi√≥ el NS bajo el nombre de AlexNet, llamado as√≠ por el investigador l√≠der Alex Krizhevsky.  Al igual que las redes m√°s profundas, podr√≠an proporcionar una eficiencia innovadora, pero solo en combinaci√≥n con una gran cantidad de energ√≠a de la computadora y una gran cantidad de datos. <br><br>  AlexNet ha desarrollado un tr√≠o de inform√°ticos de la Universidad de Toronto para participar en la competencia cient√≠fica ImageNet.  Los organizadores del concurso recopilaron un mill√≥n de im√°genes en Internet, cada una de las cuales fue etiquetada y asignada a una de las miles de categor√≠as de objetos, por ejemplo, "cereza", "portacontenedores" o "leopardo".  Se pidi√≥ a los investigadores de IA que entrenaran sus programas de MO en partes de estas im√°genes, y luego trataran de poner las etiquetas correctas para otras im√°genes que el software no hab√≠a encontrado antes.  El software tuvo que seleccionar cinco posibles etiquetas para cada imagen, y el intento se consider√≥ exitoso si una de ellas coincid√≠a con la real. <br><br>  Esta fue una tarea dif√≠cil, y hasta 2012 los resultados no fueron muy buenos.  Para el ganador de 2011, la tasa de error fue del 25%. <br><br>  En 2012, el equipo de AlexNet super√≥ a todos los competidores al dar respuestas con un 15% de errores.  Para el competidor m√°s cercano, esta cifra fue del 26%. <br><br>  Investigadores de Toronto combinaron varias t√©cnicas para lograr resultados innovadores.  Uno de ellos fue el uso de <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">neurosis convolucionales</a> (SNS).  De hecho, el SNA, por as√≠ decirlo, entrena peque√±as redes neuronales, cuyos datos de entrada son cuadrados con un lado de 7 a 11 p√≠xeles, y luego los "superpone" en una imagen m√°s grande. <br><br>  "Es como si tomas una plantilla peque√±a o una plantilla y tratas de compararla con cada punto de la imagen", nos dijo el investigador de IA Jie Tan el a√±o pasado.  - ¬øTienes una plantilla de un perro, y la pegas a la imagen y ves si hay un perro all√≠?  Si no, mueva la plantilla.  Y as√≠, para toda la imagen.  Y no importa d√≥nde aparezca el perro en la imagen.  La plantilla coincidir√° con ella.  Cada subsecci√≥n de red no debe convertirse en un clasificador de perro separado ". <br><br>  Otro factor clave de √©xito para AlexNet ha sido el uso de tarjetas gr√°ficas para acelerar el proceso de aprendizaje.  Las tarjetas gr√°ficas tienen un poder de procesamiento paralelo, muy adecuado para la inform√°tica repetitiva necesaria para entrenar una red neuronal.  Al transferir la carga de la inform√°tica a un par de GPU, la Nvidia GTX 580, con 3 GB de memoria cada una, los investigadores pudieron desarrollar y entrenar una red extremadamente grande y compleja.  AlexNet ten√≠a ocho capas entrenables, 650,000 neuronas y 60 millones de par√°metros. <br><br>  Finalmente, el √©xito de AlexNet tambi√©n fue asegurado por el gran tama√±o de la base de datos de im√°genes de capacitaci√≥n de ImageNet: un mill√≥n de piezas.  Se necesitan muchas im√°genes para ajustar 60 millones de par√°metros.  Para lograr una victoria decisiva, AlexNet fue ayudado por una combinaci√≥n de una red compleja y un gran conjunto de datos. <br><br>  Me pregunto por qu√© tal avance no ocurri√≥ antes: <br><br><ul><li>  El par de GPU de grado de consumo utilizado por los investigadores de AlexNet estaba lejos de ser el dispositivo inform√°tico m√°s poderoso para 2012.  Cinco e incluso diez a√±os antes de eso, hab√≠a computadoras m√°s potentes.  Adem√°s, la tecnolog√≠a para acelerar el aprendizaje de NS mediante tarjetas gr√°ficas se conoce desde al menos 2004. </li><li>  La base de un mill√≥n de im√°genes era inusualmente grande para ense√±ar algoritmos de MO en 2012, sin embargo, recopilar dichos datos no era una tecnolog√≠a nueva para ese a√±o.  Un equipo de investigaci√≥n bien financiado podr√≠a reunir f√°cilmente una base de datos de este tama√±o cinco o diez a√±os antes. </li><li>  Los algoritmos principales utilizados en AlexNet no eran nuevos.  El algoritmo de retropropagaci√≥n para 2012 ya exist√≠a durante aproximadamente un cuarto de siglo.  Las ideas clave relacionadas con las redes neuronales convolucionales se desarrollaron en los a√±os ochenta y noventa. </li></ul><br>  Por lo tanto, cada uno de los elementos de √©xito de AlexNet exist√≠a por separado mucho antes de que ocurriera el avance.  Obviamente, a nadie se le ocurri√≥ combinarlos, en su mayor parte porque nadie sab√≠a cu√°n poderosa ser√≠a esta combinaci√≥n. <br><br>  El aumento de la profundidad del NS pr√°cticamente no mejor√≥ la eficiencia de su trabajo si no usaban conjuntos de datos de entrenamiento lo suficientemente grandes.  Y expandir el conjunto de datos no mejor√≥ el rendimiento de las redes peque√±as.  Para ver el aumento de la eficiencia, necesit√°bamos redes m√°s profundas y conjuntos de datos m√°s grandes, adem√°s de una potencia inform√°tica considerable que nos permitiera llevar a cabo el proceso de capacitaci√≥n en un per√≠odo de tiempo razonable.  El equipo de AlexNet fue el primero en reunir los tres elementos en un solo programa. <br><br><h2>  El boom del aprendizaje profundo </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/ef0/fb4/922/ef0fb4922c3164251c722134b84460e3.jpg"><br><br>  Muchas personas notaron la demostraci√≥n de todo el poder del NS profundo, proporcionado por una cantidad suficiente de datos de capacitaci√≥n, tanto entre cient√≠ficos, investigadores como entre representantes de la industria. <br><br>  El primer concurso de ImageNet para cambiar.  Hasta 2012, la mayor√≠a de los concursantes usaban tecnolog√≠as distintas al aprendizaje profundo.  En la competencia de 2013, como escribieron los patrocinadores, "la mayor√≠a" de los concursantes usaron GO. <br><br>  El porcentaje de errores entre los ganadores disminuy√≥ gradualmente, de un impresionante 16% en AlexNet en 2012 a 2.3% en 2017: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ba/95e/b4b/9ba95eb4baee6580ea97ac76347a02e4.png"><br><br>  La revoluci√≥n GO se extendi√≥ r√°pidamente por toda la industria.  En 2013, Google adquiri√≥ una startup formada por los autores de AlexNet y utiliz√≥ su tecnolog√≠a como base para la funci√≥n de b√∫squeda de im√°genes en Google Photos.  Para 2014, Facebook promocionaba su propio software que reconoce im√°genes usando GO.  Apple ha estado utilizando GO para el reconocimiento facial en iOS desde al menos 2016. <br><br>  GO tambi√©n subyace a la reciente mejora en la tecnolog√≠a de reconocimiento de voz.  Siri de Apple, Alexa de Amazon, Cortana de Microsoft y el asistente de Google usan GO, ya sea para comprender las palabras de una persona, o para generar una voz m√°s natural, o ambas. <br><br>  En los √∫ltimos a√±os, ha surgido una tendencia autosostenida en la industria, en la que el aumento de la potencia inform√°tica, el volumen de datos y la profundidad de la red se apoyan mutuamente.  El equipo de AlexNet us√≥ la GPU porque ofrec√≠an computaci√≥n paralela a un precio razonable.  Pero en los √∫ltimos a√±os, cada vez m√°s empresas han comenzado a desarrollar sus propios chips, dise√±ados espec√≠ficamente para su uso en el campo de MO. <br><br>  Google anunci√≥ el lanzamiento del chip Tensor Processing Unit espec√≠ficamente dise√±ado para el NS en 2016. En el mismo a√±o, Nvidia anunci√≥ el lanzamiento de una nueva GPU llamada Tesla P100, optimizada para el NS.  Intel respondi√≥ a la llamada con su chip AI en 2017. En 2018, Amazon anunci√≥ el lanzamiento de su propio chip AI, que puede usarse como parte de los servicios en la nube de la compa√±√≠a.  Incluso se dice que Microsoft est√° trabajando en su chip AI. <br><br>  Los fabricantes de tel√©fonos inteligentes tambi√©n est√°n trabajando en chips que permitir√°n que los dispositivos m√≥viles realicen m√°s c√≥mputo utilizando NS localmente, sin tener que cargar datos a los servidores.  Tal computaci√≥n en dispositivos reduce la latencia y mejora la privacidad. <br><br>  Incluso Tesla entr√≥ en este juego con fichas especiales.  Este a√±o, Tesla mostr√≥ una nueva y poderosa computadora, optimizada para calcular NS.  Tesla lo nombr√≥ Full Self-Driving Computer y lo present√≥ como un momento clave en la estrategia de la compa√±√≠a para convertir la flota de Tesla en veh√≠culos rob√≥ticos. <br><br>  La disponibilidad de capacidades inform√°ticas optimizadas para IA ha generado una solicitud de los datos necesarios para entrenar NS cada vez m√°s complejas.  Esta din√°mica es m√°s evidente en el sector robom√≥vil, donde las empresas recopilan datos sobre millones de kil√≥metros de carreteras reales.  Tesla puede recopilar estos datos autom√°ticamente de los autom√≥viles de los usuarios, y sus competidores, Waymo y Cruise, pagaron a los conductores que conduc√≠an sus autom√≥viles en v√≠as p√∫blicas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/80a/3be/2eb80a3be8fd038777829faff752e8bf.jpg"><br><br>  La solicitud de datos ofrece una ventaja a las grandes compa√±√≠as en l√≠nea que ya tienen acceso a grandes vol√∫menes de datos de usuarios. <br><br>  El aprendizaje profundo ha conquistado tantas √°reas diferentes debido a su extrema flexibilidad.  Las d√©cadas de prueba y error han permitido a los investigadores desarrollar los bloques de construcci√≥n b√°sicos para las tareas m√°s comunes en el campo de MO, como las redes de convoluci√≥n para el reconocimiento eficiente de im√°genes.  Sin embargo, si tiene una red de alto nivel adecuada para el esquema y suficientes datos, entonces el proceso de capacitaci√≥n ser√° simple.  Los NS profundos pueden reconocer una gama excepcionalmente amplia de patrones complejos sin la gu√≠a especial de los desarrolladores humanos. <br><br>  Hay limitaciones, por supuesto.  Por ejemplo, algunas personas se entregaron a la idea de entrenar robom√≥viles con la ayuda de solo GO, es decir, alimentar im√°genes recibidas de una c√°mara, una red neuronal y recibir instrucciones de ella para girar el volante y el pedal.  Soy esc√©ptico de este enfoque.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La Asamblea Nacional a√∫n no ha demostrado la capacidad de llevar a cabo un razonamiento l√≥gico complejo, que se requiere para comprender ciertas condiciones que surgen en el camino. </font><font style="vertical-align: inherit;">Adem√°s, los NS son "cajas negras", cuyo flujo de trabajo es pr√°cticamente invisible. </font><font style="vertical-align: inherit;">Ser√≠a dif√≠cil evaluar y confirmar la seguridad de dicho sistema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, GO permiti√≥ dar saltos muy amplios en una gama inesperadamente grande de aplicaciones. </font><font style="vertical-align: inherit;">En los pr√≥ximos a√±os, uno puede esperar el pr√≥ximo progreso en esta √°rea.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/482258/">https://habr.com/ru/post/482258/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../482248/index.html">Compartimos nuestra experiencia de c√≥mo se muestran los SSD en RAID y qu√© nivel de matriz es m√°s rentable</a></li>
<li><a href="../482250/index.html">Una m√°quina de estado simple para VueJS</a></li>
<li><a href="../482252/index.html">Inodoro autom√°tico para gatos - continuaci√≥n</a></li>
<li><a href="../482254/index.html">Experimento VonmoTrade. Parte 3: Libro de warrants. Procesamiento y almacenamiento de informaci√≥n comercial.</a></li>
<li><a href="../482256/index.html">La IA y el futuro del trabajo: perspectivas de empleo en el futuro cercano</a></li>
<li><a href="../482260/index.html">TelegramBot. La funcionalidad b√°sica. Pegatinas y emoticones. (Parte 3)</a></li>
<li><a href="../482262/index.html">C√≥mo iniciar sesi√≥n en Talend Open Studio</a></li>
<li><a href="../482264/index.html">Brasil, magia oscura, Mortal Kombat, Marte y 15,000 personas. Resultados del a√±o de Ontiko</a></li>
<li><a href="../482268/index.html">Megaestructuras del futuro: la esfera Dyson, el motor estelar y la "bomba de agujero negro"</a></li>
<li><a href="../482272/index.html">Elecci√≥n de un almac√©n de datos para Prometheus: Thanos vs VictoriaMetrics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>