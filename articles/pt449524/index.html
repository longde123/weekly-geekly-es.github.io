<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÖüèª üíá üíÉüèª Distinguir caracteres do lixo: como criar modelos de rede neural robustos em tarefas de OCR ü•É üßïüèº üêÇ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recentemente, no grupo de reconhecimento ABBYY, estamos usando cada vez mais redes neurais em v√°rias tarefas. Muito bem, eles se provaram principalmen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Distinguir caracteres do lixo: como criar modelos de rede neural robustos em tarefas de OCR</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/449524/">  Recentemente, no grupo de reconhecimento ABBYY, estamos usando cada vez mais redes neurais em v√°rias tarefas.  Muito bem, eles se provaram principalmente para tipos complexos de escrita.  Em posts anteriores, falamos sobre como usamos as redes neurais para reconhecer scripts em japon√™s, chin√™s e coreano. <br><br><img src="https://habrastorage.org/webt/nf/p0/ws/nfp0wsz4wap5qwx33ulwimmaid8.png" alt="imagem">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Publique sobre o reconhecimento de caracteres japoneses e chineses</a> <br><img src="https://habrastorage.org/webt/nf/p0/ws/nfp0wsz4wap5qwx33ulwimmaid8.png" alt="imagem">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Post de reconhecimento de caracteres coreanos</a> <br><br>  Nos dois casos, usamos redes neurais para substituir completamente o m√©todo de classifica√ß√£o por um √∫nico s√≠mbolo.  Todas as abordagens inclu√≠am muitas redes diferentes e algumas das tarefas inclu√≠ram a necessidade de trabalhar adequadamente em imagens que n√£o s√£o s√≠mbolos.  O modelo nessas situa√ß√µes deve de alguma forma sinalizar que n√£o somos um s√≠mbolo.  Hoje, falaremos apenas sobre por que isso pode ser necess√°rio em princ√≠pio e sobre abordagens que podem ser usadas para alcan√ßar o efeito desejado. <br><br><h2>  Motiva√ß√£o </h2><br>  Qual √© o problema?  Por que trabalhar em imagens que n√£o s√£o caracteres separados?  Parece que voc√™ pode dividir um fragmento de uma string em caracteres, classific√°-los todos e coletar o resultado disso, como, por exemplo, na figura abaixo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2r/cn/ie/2rcnierxmu_ydwqpxkd_r0lgxaq.png"></div><br><br>  Sim, especificamente neste caso, isso realmente pode ser feito.  Mas, infelizmente, o mundo real √© muito mais complicado e, na pr√°tica, ao reconhecer, voc√™ precisa lidar com distor√ß√µes geom√©tricas, borr√µes, manchas de caf√© e outras dificuldades. <br><a name="habracut"></a><br>  Como resultado, voc√™ frequentemente precisa trabalhar com esses fragmentos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2k/cw/ea/2kcweakgxef1te-opdgk9lpozwq.png"></div><br><br>  Eu acho que √© √≥bvio para todos qual √© o problema.  De acordo com essa imagem de um fragmento, n√£o √© t√£o simples dividi-lo inequivocamente em s√≠mbolos separados para reconhec√™-los individualmente.  Temos que apresentar um conjunto de hip√≥teses sobre onde est√£o os limites entre os personagens e onde est√£o os pr√≥prios personagens.  Para isso, usamos o chamado gr√°fico de divis√£o linear (GLD).  Na figura acima, este gr√°fico √© mostrado na parte inferior: os segmentos verdes s√£o os arcos da GLD constru√≠da, ou seja, as hip√≥teses sobre onde os s√≠mbolos individuais est√£o localizados. <br><br>  Assim, algumas das imagens para as quais o m√≥dulo de reconhecimento de caracteres individuais √© iniciado n√£o s√£o, de fato, caracteres individuais, mas erros de segmenta√ß√£o.  E esse mesmo m√≥dulo deve sinalizar que na frente dele, provavelmente, n√£o √© um s√≠mbolo, retornando baixa confian√ßa para todas as op√ß√µes de reconhecimento.  E se isso n√£o acontecer, no final, a op√ß√£o errada para segmentar esse fragmento por s√≠mbolos pode ser escolhida, o que aumentar√° bastante o n√∫mero de erros de divis√£o linear. <br><br>  Al√©m dos erros de segmenta√ß√£o, o modelo tamb√©m deve ser resistente a lixo a priori da p√°gina.  Por exemplo, aqui essas imagens tamb√©m podem ser enviadas para reconhecer um √∫nico caractere: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ng/m8/wx/ngm8wxkpcel72qdqn8ssfvbpmzw.png"></div><br><br>  Se voc√™ simplesmente classificar essas imagens em caracteres separados, os resultados da classifica√ß√£o cair√£o nos resultados do reconhecimento.  Al√©m disso, de fato, essas imagens s√£o simplesmente artefatos do algoritmo de binariza√ß√£o, nada deve corresponder a elas no resultado final.  Portanto, tamb√©m para eles, voc√™ precisa ser capaz de retornar uma baixa confian√ßa na classifica√ß√£o. <br><br>  Todas as imagens semelhantes: erros de segmenta√ß√£o, lixo a priori, etc.  a seguir seremos chamados exemplos negativos.  Imagens de s√≠mbolos reais ser√£o chamadas de exemplos positivos. <br><br><h2>  O problema da abordagem da rede neural </h2><br>  Agora vamos lembrar como uma rede neural normal funciona para reconhecer caracteres individuais.  Geralmente, esse √© um tipo de camada convolucional e totalmente conectada, com a ajuda da qual √© formado o vetor de probabilidades de pertencer a cada classe em particular a partir da imagem de entrada. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hw/ke/gh/hwkeghalueokk7qtqu3zrxnwhj4.png"></div><br><br>  Al√©m disso, o n√∫mero de classes coincide com o tamanho do alfabeto.  Durante o treinamento da rede neural, imagens de s√≠mbolos reais s√£o exibidas e ensinadas a retornar uma alta probabilidade para a classe correta de s√≠mbolos. <br><br>  E o que acontecer√° se uma rede neural for alimentada com erros de segmenta√ß√£o e lixo a priori?  De fato, puramente teoricamente, tudo pode acontecer, porque a rede n√£o viu essas imagens no processo de aprendizado.  Para algumas imagens, pode ter sorte e a rede retornar√° uma baixa probabilidade para todas as classes.  Mas em alguns casos, a rede pode come√ßar a procurar no lixo na entrada os contornos familiares de um determinado s√≠mbolo, por exemplo, o s√≠mbolo "A" e reconhec√™-lo com uma probabilidade de 0,99. <br><br>  Na pr√°tica, quando trabalhamos, por exemplo, em um modelo de rede neural para escrita japonesa e chinesa, o uso da probabilidade bruta da sa√≠da da rede levou ao aparecimento de um grande n√∫mero de erros de segmenta√ß√£o.  E, apesar do modelo simb√≥lico funcionar muito bem com base em imagens, n√£o foi poss√≠vel integr√°-lo ao algoritmo de reconhecimento total. <br><br>  Algu√©m pode perguntar: por que exatamente com redes neurais esse problema surge?  Por que os classificadores de atributos n√£o funcionaram da mesma maneira, porque tamb√©m estudaram com base em imagens, o que significa que n√£o houve exemplos negativos no processo de aprendizagem? <br><br>  A diferen√ßa fundamental, na minha opini√£o, est√° em como exatamente os sinais se distinguem das imagens de s√≠mbolos.  No caso do classificador usual, a pr√≥pria pessoa prescreve como extra√≠-los, guiada por algum conhecimento do seu dispositivo.  No caso de uma rede neural, a extra√ß√£o de recursos tamb√©m √© uma parte treinada do modelo: eles s√£o configurados para que seja poss√≠vel distinguir da melhor maneira os caracteres de diferentes classes.  E, na pr√°tica, verifica-se que as caracter√≠sticas descritas por uma pessoa s√£o mais resistentes a imagens que n√£o s√£o s√≠mbolos: √© menos prov√°vel que sejam iguais √†s imagens de s√≠mbolos reais, o que significa que um valor menor de confian√ßa pode ser retornado a elas. <br><br><h2>  Aprimorando a estabilidade do modelo com perda central </h2><br>  Porque  o problema, de acordo com nossas suspeitas, era como a rede neural seleciona sinais, decidimos tentar melhorar essa parte em particular, ou seja, aprender a destacar alguns sinais "bons".  No aprendizado profundo, h√° uma se√ß√£o separada dedicada a esse t√≥pico, chamada de "Aprendizado de Representa√ß√£o".  Decidimos tentar v√°rias abordagens bem-sucedidas nessa √°rea.  A maioria das solu√ß√µes foi proposta para o treinamento de representa√ß√µes em problemas de reconhecimento facial. <br><br>  A abordagem descrita no artigo ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma abordagem discriminat√≥ria de aprendizado de recursos para o reconhecimento facial profundo</a> ‚Äù parecia bastante boa.  A principal id√©ia dos autores: adicionar um termo adicional √† fun√ß√£o de perda, o que reduzir√° a dist√¢ncia euclidiana no espa√ßo de fei√ß√£o entre elementos da mesma classe. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5p/g8/yf/5pg8yfnedj_kveiyhikga5l1xok.png"></div><br><br>  Para v√°rios valores do peso desse termo na fun√ß√£o de perda geral, √© poss√≠vel obter v√°rias imagens nos espa√ßos de atributo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mb/mi/y3/mbmiy3dk5uxypa8jaf5sxeuoleu.png"></div><br><br>  Esta figura mostra a distribui√ß√£o dos elementos de uma amostra de teste em um espa√ßo de atributo bidimensional.  O problema de classificar n√∫meros manuscritos √© considerado (amostra MNIST). <br><br>  Uma das propriedades importantes declaradas pelos autores: um aumento na capacidade de generaliza√ß√£o das caracter√≠sticas obtidas para pessoas que n√£o estavam no conjunto de treinamento.  Os rostos de algumas pessoas ainda estavam localizados nas proximidades, e os rostos de pessoas diferentes estavam distantes. <br><br>  Decidimos verificar se uma propriedade semelhante para a sele√ß√£o de caracteres √© preservada.  Nesse caso, eles foram guiados pela seguinte l√≥gica: se no espa√ßo de fei√ß√£o todos os elementos da mesma classe forem agrupados de maneira compacta perto de um ponto, √© menos prov√°vel que sinais de exemplos negativos sejam localizados perto do mesmo ponto.  Portanto, como principal crit√©rio de filtragem, usamos a dist√¢ncia euclidiana ao centro estat√≠stico de uma classe em particular. <br><br>  Para testar a hip√≥tese, realizamos o seguinte experimento: treinamos modelos para reconhecer um pequeno subconjunto de caracteres japoneses de alfabetos sil√°bicos (o chamado kana).  Al√©m da amostra de treinamento, tamb√©m examinamos 3 bases artificiais de exemplos negativos: <br><br><ul><li>  Pares - um conjunto de pares de caracteres europeus </li><li>  Cortes - fragmentos de linhas japonesas cortadas em espa√ßos, n√£o caracteres </li><li>  N√£o kana - outros caracteres do alfabeto japon√™s que n√£o est√£o relacionados ao subconjunto considerado </li></ul><br>  Quer√≠amos comparar a abordagem cl√°ssica com a fun√ß√£o de perda de entropia cruzada e a abordagem com a perda de centro na capacidade de filtrar exemplos negativos.  Os crit√©rios de filtragem para exemplos negativos foram diferentes.  No caso de perda de entropia cruzada, usamos a resposta de rede da √∫ltima camada e, no caso de perda de centro, usamos a dist√¢ncia euclidiana ao centro estat√≠stico da classe no espa√ßo de atributo.  Nos dois casos, escolhemos o limite das estat√≠sticas correspondentes, no qual n√£o mais de 3% dos exemplos positivos da amostra de teste s√£o eliminados e analisamos a propor√ß√£o de exemplos negativos de cada banco de dados que √© eliminado nesse limite. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pe/qm/_m/peqm_m3gvnht-vc_lfspy_bgw0u.png"></div><br>  Como voc√™ pode ver, a abordagem Center Loss realmente faz um trabalho melhor ao filtrar exemplos negativos.  Al√©m disso, em ambos os casos, n√£o tivemos imagens de exemplos negativos no processo de aprendizagem.  Isso √© realmente muito bom, porque, no caso geral, obter uma base representativa de todos os exemplos negativos no problema de OCR n√£o √© uma tarefa f√°cil. <br><br>  Aplicamos essa abordagem ao problema de reconhecer caracteres japoneses (no segundo n√≠vel de um modelo de dois n√≠veis), e o resultado nos agradou: o n√∫mero de erros de divis√£o linear foi reduzido significativamente.  Embora os erros persistissem, eles j√° podiam ser classificados por tipos espec√≠ficos: pares de n√∫meros ou hier√≥glifos com um s√≠mbolo de pontua√ß√£o presa.  Para esses erros, j√° era poss√≠vel formar uma base sint√©tica de exemplos negativos e us√°-la no processo de aprendizado.  Sobre como isso pode ser feito, e ser√° discutido mais adiante. <br><br><h2>  Usando a base de exemplos negativos no treinamento </h2><br>  Se voc√™ tem alguns exemplos negativos, √© tolice n√£o us√°-lo no processo de aprendizagem.  Mas vamos pensar em como isso pode ser feito. <br><br>  Primeiro, considere o esquema mais simples: agrupamos todos os exemplos negativos em uma classe separada e adicionamos outro neur√¥nio √† camada de sa√≠da correspondente a essa classe.  Agora, na sa√≠da, temos uma distribui√ß√£o de probabilidade para a classe <b>N + 1</b> .  E ensinamos a isso a perda entre entropia usual. <br><br>  O crit√©rio de que o exemplo √© negativo pode ser considerado o valor da nova resposta de rede correspondente.  Mas, √†s vezes, caracteres reais de qualidade n√£o muito alta podem ser classificados como exemplos negativos.  √â poss√≠vel tornar a transi√ß√£o entre exemplos positivos e negativos mais suave? <br><br>  Na verdade, voc√™ pode tentar n√£o aumentar o n√∫mero total de sa√≠das, mas simplesmente fazer com que o modelo, ao aprender, retorne respostas baixas para todas as classes ao aplicar exemplos negativos √† entrada.  Para fazer isso, n√£o podemos adicionar explicitamente a sa√≠da <b>N + 1</b> ao modelo, mas simplesmente adicionar o valor de <b>‚Äìmax</b> das respostas de todas as outras classes ao elemento <b>N + 1</b> .  Ent√£o, ao aplicar exemplos negativos √† entrada, a rede tentar√° fazer o m√°ximo poss√≠vel, o que significa que a resposta m√°xima tentar√° fazer o m√≠nimo poss√≠vel. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ud/my/3x/udmy3x4h8eujvz34tvkomt8vlma.png"></div><br><br>  Exatamente esse esquema foi aplicado no primeiro n√≠vel de um modelo de dois n√≠veis para os japoneses em combina√ß√£o com a abordagem de perda central no segundo n√≠vel.  Assim, alguns dos exemplos negativos foram filtrados no primeiro n√≠vel e outros no segundo.  Em conjunto, j√° conseguimos obter uma solu√ß√£o pronta para ser incorporada no algoritmo de reconhecimento geral. <br><br>  Em geral, pode-se perguntar: como usar a base de exemplos negativos na abordagem com Center Loss?  Acontece que, de alguma forma, precisamos adiar os exemplos negativos que est√£o localizados pr√≥ximos aos centros estat√≠sticos das classes no espa√ßo de atributo.  Como colocar essa l√≥gica na fun√ß√£o de perda? <br><br>  Vamos <img src="https://habrastorage.org/webt/mg/y8/to/mgy8toffrnoim1xtgaouwxq23gq.png" alt="imagem">  - sinais de exemplos negativos, e <img src="https://habrastorage.org/webt/nh/y3/pb/nhy3pbb0kc-8nqu1q1yfh1v_oje.png" alt="imagem">  - centros de aulas.  Em seguida, podemos considerar a seguinte adi√ß√£o para a fun√ß√£o de perda: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4u/nk/hb/4unkhblwbiyf8ch7xokv2ekrgje.png"></div><br><br>  Aqui <img src="https://habrastorage.org/webt/tj/pv/f8/tjpvf8kvio44ks3yh4kcj2xrjwi.png" alt="imagem">  - uma certa lacuna permitida entre o centro e os exemplos negativos, dentro dos quais uma penalidade √© aplicada a exemplos negativos. <br><br>  A combina√ß√£o do Center Loss com o aditivo descrito acima, aplicamos com sucesso, por exemplo, alguns classificadores individuais na tarefa de reconhecer caracteres coreanos. <br><br><h2>  Conclus√µes </h2><br>  Em geral, todas as abordagens para filtrar os chamados "exemplos negativos" descritos acima podem ser aplicadas a qualquer problema de classifica√ß√£o quando voc√™ tem uma classe implicitamente altamente desequilibrada em rela√ß√£o ao restante, sem uma boa base de representantes, que, no entanto, precisa ser considerada de alguma forma .  O OCR √© apenas uma tarefa espec√≠fica na qual esse problema √© mais agudo. <br><br>  Naturalmente, todos esses problemas surgem apenas ao usar redes neurais como modelo principal para o reconhecimento de caracteres individuais.  Ao usar o reconhecimento de linha de ponta a ponta como um modelo separado, esse problema n√£o ocorre. <br><br>  <i>Grupo de Novas Tecnologias OCR</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt449524/">https://habr.com/ru/post/pt449524/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt449514/index.html">PNL. O b√°sico. T√©cnicas. Autodesenvolvimento. Parte 2: NER</a></li>
<li><a href="../pt449516/index.html">Preparando-se para o hackathon: como se espremer em um m√°ximo de 48 horas</a></li>
<li><a href="../pt449518/index.html">Sele√ß√£o: 5 servi√ßos √∫teis para escrever artigos em ingl√™s</a></li>
<li><a href="../pt449520/index.html">Como eu ensinei um neur√¥nio em um "dinossauro" a tocar</a></li>
<li><a href="../pt449522/index.html">Pensamentos sobre o Elixir: pr√≥s e contras da ferramenta mais popular para desenvolvedores de alta carga</a></li>
<li><a href="../pt449526/index.html">Apresentando o Tartiflette: uma implementa√ß√£o de c√≥digo-fonte aberto GraphQL para Python 3.6+</a></li>
<li><a href="../pt449528/index.html">Sobre a decomposi√ß√£o da resposta multicanal de um sistema em modos "pseudo-pr√≥prios"</a></li>
<li><a href="../pt449532/index.html">ok.tech: Cassandra meetup</a></li>
<li><a href="../pt449534/index.html">Carro-conceito do SLA: como √© feito na China</a></li>
<li><a href="../pt449536/index.html">iOS Digest No. 4 (5 a 26 de abril)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>