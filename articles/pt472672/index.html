<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üï¥üèª üôÖüèΩ üõë O cabe√ßalho "Leia artigos para voc√™". Julho - setembro 2019 üêâ üê≠ üêé</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Continuamos a publicar resenhas de artigos cient√≠ficos de membros da comunidade Open Data Science no canal #article_essense. Se voc√™ deseja ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O cabe√ßalho "Leia artigos para voc√™". Julho - setembro 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/472672/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Ol√° Habr!  Continuamos a publicar resenhas de artigos cient√≠ficos de membros da comunidade Open Data Science no canal #article_essense.  Se voc√™ deseja receb√™-los antes de todos os outros - participe da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">comunidade</a> ! </p><br><p>  Artigos para hoje: </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Rota√ß√£o de camadas: um indicador surpreendentemente poderoso de generaliza√ß√£o em redes profundas?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">(Universit√© catholique de Louvain, B√©lgica, 2018)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado de transfer√™ncia eficiente em par√¢metros para PNL (Google Research, Jagiellonian University, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RoBERTa: Uma Abordagem de Pr√©-Treino BERT Robustamente Otimizada (Universidade de Washington, Facebook AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">EfficientNet: Repensando o dimensionamento de modelos para redes neurais convolucionais (Google Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Como o c√©rebro transita da percep√ß√£o consciente para a subliminar (EUA, Argentina, Espanha, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Grandes camadas de mem√≥ria com chaves de produto (Facebook AI Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Realmente estamos fazendo muito progresso?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma an√°lise preocupante das recentes abordagens de recomenda√ß√£o neural (Politecnico di Milano, University of Klagenfurt, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Omni-Scale Feature Learning for Person Re-Identification (Universidade de Surrey, Universidade Queen Mary, Samsung AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">A reparameteriza√ß√£o neural melhora a otimiza√ß√£o estrutural (Google Research, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Links para cole√ß√µes anteriores da s√©rie:</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Janeiro - junho 2019</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fevereiro - mar√ßo de 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dezembro de 2017 - janeiro de 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Outubro - novembro de 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Setembro 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Agosto de 2017</a> </li></ul></div></div><br><h3 id="1-layer-rotation-a-surprisingly-powerful-indicator-of-generalization-in-deep-networks">  1. Rota√ß√£o de camada: um indicador surpreendentemente poderoso de generaliza√ß√£o em redes profundas? </h3><br><p>  Autores: Simon Carbonnelle, Christophe De Vleeschouwer (Universit√© catholique de Louvain, B√©lgica, 2018) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí Artigo original</a> <br>  Autor da resenha: Svyatoslav Skoblov (em erro folgado_derivativo) </p><br><img src="https://habrastorage.org/webt/tt/n5/g8/ttn5g8j27-ihyqnwk0rowhg8oie.png" width="500" height="250"><br><p><br>  Neste artigo, os autores chamaram aten√ß√£o para uma observa√ß√£o bastante simples: dist√¢ncia do cosseno entre os pesos da camada durante a inicializa√ß√£o e ap√≥s o treinamento (o processo de aumentar a dist√¢ncia durante o treinamento √© chamado de rota√ß√£o da camada).  Os senhores dizem que, na maioria dos experimentos, as redes que atingiram uma dist√¢ncia de 1 em todas as camadas s√£o consistentemente superiores em precis√£o a outras configura√ß√µes.  O artigo tamb√©m apresenta o algoritmo <strong>Layca</strong> (quantidade controlada em n√≠vel de camada de rota√ß√£o de peso), que permite o uso dessa taxa de aprendizado em camadas para controlar essa mesma rota√ß√£o de camada.  De fato, difere do algoritmo SGD usual pela presen√ßa de proje√ß√£o ortogonal e normaliza√ß√£o.  Uma lista detalhada do algoritmo, juntamente com o esquema de treinamento, pode ser encontrada no artigo. </p><br><p>  A principal id√©ia que os autores deduzem √©: <strong>quanto maiores as rota√ß√µes da camada, melhor o desempenho da generaliza√ß√£o</strong> .  A maior parte do artigo √© um registro de experimentos em que foram estudados v√°rios cen√°rios de treinamento: MNIST, CIFAR-10 / CIFAR-100, pequena ImageNet com arquiteturas diferentes, de uma rede de camada √∫nica √† fam√≠lia ResNet. </p><br><p>  Uma s√©rie de experimentos foi dividida em v√°rias etapas: </p><br><ol><li>  <strong>Vanilla SGD</strong> Verificou- <strong>se</strong> que, no geral, o comportamento das escalas coincide com a hip√≥tese (grandes mudan√ßas na dist√¢ncia correspondiam aos melhores valores m√©tricos), no entanto, tamb√©m foram notados problemas: a rota√ß√£o da camada parou muito antes dos valores desejados;  Tamb√©m foi notada instabilidade na mudan√ßa de dist√¢ncia. </li><li>  <strong>Deteriora√ß√£o de peso SGD + A</strong> diminui√ß√£o da norma de <strong>peso</strong> melhorou muito a imagem do treinamento: a maioria das camadas alcan√ßou a dist√¢ncia m√°xima e o desempenho do teste √© semelhante ao Layca proposto.  A vantagem indubit√°vel do m√©todo do autor √© a falta de um hiperpar√¢metro adicional. </li><li>  <strong>Aquecimentos de LR</strong> Acontece que o aquecimento ajuda a SGD a superar o problema da rota√ß√£o inst√°vel das camadas, no entanto, n√£o afeta o Layca. </li><li>  <strong>M√©todos adaptativos de gradiente</strong> Al√©m da verdade conhecida (que usando esses m√©todos √© mais dif√≠cil alcan√ßar o n√≠vel de generaliza√ß√£o que o decaimento de peso SGD + pode dar), descobriu-se que os efeitos da rota√ß√£o de camadas s√£o muito diferentes: o primeiro aumenta a rota√ß√£o nas √∫ltimas camadas, enquanto o SGD nas camadas iniciais .  Os autores sugerem que essa pode ser a maldade dos m√©todos adaptativos.  E eles sugerem o uso do Layca em conjunto com eles (melhorando a capacidade de generalizar em m√©todos adaptativos e acelerando o aprendizado no SGD). </li></ol><br><p>  O artigo termina com uma tentativa de interpretar o fen√¥meno.  Para fazer isso, os autores treinaram uma rede com 1 camada oculta em uma vers√£o simplificada do MNIST, ap√≥s o que visualizaram neur√¥nios aleat√≥rios, chegando a uma conclus√£o bastante l√≥gica: um maior grau de rota√ß√£o da camada corresponde a um menor efeito de inicializa√ß√£o e melhor estudo de caracter√≠sticas, o que contribui para uma melhor generaliza√ß√£o. </p><br><p>  O <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">c√≥digo do algoritmo implementado (tf / keras)</a> e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">c√≥digo para reproduzir experimentos s√£o</a> carregados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">.</a> </p><br><h3 id="2-parameter-efficient-transfer-learning-for-nlp">  2. Aprendizado de transfer√™ncia eficiente em par√¢metros para a PNL </h3><br><p>  Autores do artigo: Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly (Pesquisa do Google, Universidade Jagiellonian, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Alexey Karnachev (em slack zhirzemli) </p><br><img src="https://habrastorage.org/webt/ka/lg/gp/kalggpbjkmd8zc7ep451lysxpc8.png"><br><p><br>  Aqui, os cavalheiros oferecem uma t√©cnica de ajuste fino simples, por√©m eficaz, para os modelos de PNL (neste caso, BERT).  A id√©ia √© incorporar camadas de aprendizado (adaptadores) diretamente na rede.  Cada uma dessas camadas √© uma rede com um gargalo, que adapta os estados latentes do modelo original a uma tarefa de downstream espec√≠fica.  Os pesos do modelo original, por sua vez, permanecem congelados. </p><br><p>  <strong>Motiva√ß√£o</strong> <br>  Nas condi√ß√µes do treinamento de streaming (ou treinamento quase on-line), onde h√° muitas tarefas de downstream, n√£o quero realmente arquivar o modelo inteiro.  Em primeiro lugar, por um longo per√≠odo de tempo, e em segundo lugar, √© dif√≠cil e, em terceiro lugar, mesmo que seja apertado, o modelo precisa ser armazenado de alguma forma: para despejar ou guardar na mem√≥ria.  E n√£o poderemos reutilizar esse modelo para a seguinte tarefa: cada vez que precisarmos ajustar de uma nova maneira.  Como resultado, podemos tentar adaptar os estados da rede oculta ao problema atual.  Al√©m disso, o modelo original permanece intocado e os adaptadores em si s√£o muito mais amplos que o modelo principal (~ 4% do n√∫mero total de par√¢metros) </p><br><p>  <strong>Implementa√ß√£o</strong> <br>  O problema √© resolvido de uma maneira incrivelmente simples: adicionamos 2 adaptadores a cada camada do modelo.  Antes da normaliza√ß√£o da camada em modelos baseados em transformadores, ocorre a conex√£o de igni√ß√£o: a entrada transformada (estado oculto atual) √© adicionada √† entrada original. </p><br><p>  Existem duas se√ß√µes em cada camada do transformador: uma ap√≥s aten√ß√£o m√∫ltipla e a segunda ap√≥s alimenta√ß√£o para a frente.  Portanto, os estados ocultos dessas se√ß√µes s√£o passados ‚Äã‚Äãadicionalmente pelo adaptador: uma rede rasa com uma camada oculta de 1 gargalo e com sa√≠da na mesma dimens√£o da entrada.  A n√£o-linearidade √© aplicada ao estado do gargalo e a Entrada (conex√£o sem fio) √© adicionada √† sa√≠da.  Acontece que o n√∫mero total de par√¢metros treinados √©: 2md + m + d, em que d √© a dimens√£o do estado oculto do modelo original, m √© o tamanho da camada do adaptador de gargalo.  Acontece que, para o modelo de base BERT (12 camadas, par√¢metros de 110M) e para o tamanho do adaptador, obtemos 4,3% do n√∫mero total de par√¢metros </p><br><p>  <strong>Resultados</strong> <br>  A compara√ß√£o foi feita com o ajuste completo do modelo.  Para todas as tarefas, essa abordagem mostrou uma pequena perda de m√©tricas (em m√©dia, menos de 1 ponto), com o n√∫mero de pesos treinados - 3% do total.  N√£o vou listar as tarefas, existem muitas, h√° um tablet no artigo. </p><br><p>  <strong>Ajuste fino</strong> <br>  Nesse modelo, apenas a parte do adaptador √© ajustada (+ o pr√≥prio classificador de sa√≠da).  Para escalas de adaptadores, eles prop√µem a inicializa√ß√£o por quase identidade.  Assim, um modelo n√£o treinado n√£o alterar√° os estados da rede oculta de maneira alguma, e isso permitir√° j√° no processo de treinamento do modelo decidir quais estados se adaptar√£o √† tarefa e quais permanecer√£o inalterados. </p><br><p>  A taxa de aprendizado recomenda tomar mais do que com a sintonia fina padr√£o do BERT.  Pessoalmente, na minha tarefa, o 1e-04 lr funcionou bem.  Al√©m disso, (j√° pessoalmente, minha observa√ß√£o) durante o processo de ajuste, o modelo quase sempre explode gradientes, portanto, lembre-se de fazer um recorte.  Optimizer - Adam com aquecimento 10% </p><br><p>  <strong>C√≥digo</strong> <br>  O c√≥digo em seu artigo est√° anexado.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Implementa√ß√£o no Tensorflow</a> . <br>  Para o Torch, o autor da revis√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">transferiu pytorch-transformers e adicionou uma camada Adapter</a> (no in√≠cio do arquivo README.md, h√° um pequeno manual de inicializa√ß√£o) </p><br><h3 id="3-roberta-a-robustly-optimized-bert-pretraining-approach">  3. RoBERTa: Uma abordagem de pr√©-forma√ß√£o robusta e otimizada da BERT </h3><br><p>  Autores: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer e Veselin Stoyanov (Universidade de Washington, Facebook AI, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Artem Rodichev (in slack fuckai) </p><br><p>  Aumentou dramaticamente a qualidade dos modelos BERT, primeiro lugar na tabela de classifica√ß√£o GLUE e SOTA em muitas tarefas de PNL.  Eles sugeriram v√°rias maneiras de treinar o modelo BERT da melhor maneira poss√≠vel, sem nenhuma altera√ß√£o na arquitetura do modelo. </p><br><p>  Principais diferen√ßas com o BERT original: </p><br><ol><li>  Aumento da constru√ß√£o de trens 10 vezes, de 16 GB de texto bruto para 160 GB </li><li>  Mascaramento din√¢mico feito para cada amostra </li><li>  Removido o uso da pr√≥xima frase de previs√£o de perda </li><li>  Aumentou o tamanho do mini-lote de 256 amostras para 8k </li><li>  Codifica√ß√£o BPE aprimorada ao converter o banco de dados de Unicode em bytes. </li></ol><br><p>  O melhor modelo final foi treinado em 1024 placas Nvidia V100 (128 servidores DGX-1) por 5 dias. </p><br><p>  <strong>A ess√™ncia da abordagem:</strong> </p><br><p>  <em>Dados.</em>  Al√©m dos shells do Wiki e do BookCorpus (16 GB no total), que ensinaram o BERT original, eles adicionaram mais 3 shells maiores, todos em ingl√™s: </p><br><ol><li>  SS-News 63 milh√µes de not√≠cias em 2,5 anos com 76GB </li><li>  OpenWebText √© a estrutura na qual o OpenAI aprendeu o modelo GPT2.  Estes s√£o artigos rastreados para os quais foram fornecidos links em postagens em um reddit com pelo menos tr√™s atualiza√ß√µes.  Dados de 38GB </li><li>  Hist√≥rias - Caso de 31 GB CommonCrawl Story </li></ol><br><p>  <em>Mascaramento din√¢mico.</em>  No BERT original, 15% dos tokens s√£o mascarados em cada amostra e esses tokens s√£o previstos usando a parte n√£o mascarada da sequ√™ncia.  Uma m√°scara √© gerada para cada amostra uma vez durante o pr√©-processamento e n√£o muda.  Ao mesmo tempo, a mesma amostra no trem pode ocorrer v√°rias vezes, dependendo do n√∫mero de √©pocas no corpo.  A id√©ia do mascaramento din√¢mico √© criar uma nova m√°scara para a sequ√™ncia a cada vez, em vez de usar uma m√°scara fixa no pr√©-processamento. </p><br><p>  <em>Objetivo da pr√≥xima frase de previs√£o.</em>  Vamos apenas cortar esse objetivo e ver se ficou pior?  Melhorou ou tamb√©m permaneceu - nas tarefas SQuAD, MNLI, SST e RACE. </p><br><p>  <em>Aumente o tamanho do mini lote.</em>  Em muitos lugares, em particular na tradu√ß√£o autom√°tica, foi demonstrado que quanto maior o minilote, melhores os resultados finais do trem.  Eles mostraram que se voc√™ aumentar o minibatch de 256 amostras, como no BERT original, para 2k e depois para 8k, a perplexidade na valida√ß√£o diminui e as m√©tricas no MNLI e SST-2 aumentam. </p><br><p>  <em>BPE</em>  O BPE da implementa√ß√£o original do BERT usa caracteres Unicode como base para unidades de subpalavras.  Isso leva ao fato de que, em casos grandes e diversos, uma parte significativa do dicion√°rio ser√° ocupada por caracteres Unicode individuais.  O OpenAI de volta ao GPT2 sugeriu o uso de caracteres n√£o Unicode, mas bytes como base para subpalavras.  Se usarmos um dicion√°rio de 50k BPE, n√£o teremos tokens desconhecidos.  Comparado com o BERT original, o tamanho do modelo cresceu 15 milh√µes de par√¢metros para o modelo base e 20 milh√µes para grandes, ou seja, 5 a 10% a mais. </p><br><p>  <strong>Resultados:</strong> <br>  BERT-large e XLNet-large s√£o usados ‚Äã‚Äãcomo modelos para compara√ß√£o.  O RoBERTa em si √© o mesmo em par√¢metros que o BERT-large e, como resultado, eles conquistaram o primeiro lugar no benchmark GLUE.  Usamos o ajuste de arquivos de tarefa √∫nica, diferentemente de muitas outras abordagens do topo do benchmark GLUE que fazem o ajuste de arquivos de tarefas m√∫ltiplas.  Nas meninas do GLUE, os resultados de modelo √∫nico s√£o comparados, eles obtiveram SOTA em todas as 9 tarefas.  No conjunto de teste, o conjunto de modelos √© comparado, SOTA para 4 de 9 tarefas e a velocidade final da cola.  Em duas vers√µes do SQuAD na rede de desenvolvimento SOTA, no conjunto de teste no n√≠vel XLNet.  Al√©m disso, ao contr√°rio da XLNet, eles n√£o s√£o pegos em pacotes adicionais de controle de qualidade antes de resolver o SQuAD. </p><br><img src="https://habrastorage.org/webt/5x/ue/u9/5xueu9hpmqwowfuf0yn1_zopqxy.png" width="500" height="250"><br><p><br>  Tarefa SOTA on RACE na qual um peda√ßo de texto √© fornecido, uma pergunta sobre este texto e 4 op√ß√µes de resposta onde voc√™ precisa escolher o caminho certo.  Para resolver essa tarefa, eles concatenam o texto, a pergunta e a resposta, executam o BERT, obt√™m uma representa√ß√£o do token CLF, aplicam-se a uma camada totalmente conectada e prev√™em se a resposta est√° correta.  Isso √© feito 4 vezes - para cada uma das op√ß√µes de resposta. </p><br><p>  Publicamos o c√≥digo e o pr√©-treino do modelo <a href="">RoBERTa</a> no <a href="">nabo fairseq</a> .  Voc√™ pode us√°-lo, tudo parece limpo e simples. </p><br><h3 id="4-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks">  4. EfficientNet: Repensando o Escalonamento de Modelos para Redes Neurais Convolucionais </h3><br><p>  Autores: Mingxing Tan, Quoc V. Le (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí Artigo original</a> <br>  Autor do artigo: Alexander Denisenko (na folga Alexander Denisenko) </p><br><img src="https://habrastorage.org/webt/ey/se/0k/eyse0kouanmvflpgz9ev--x1oqm.png" width="500" height="250"><br><p><br>  Eles estudam o dimensionamento (dimensionamento) dos modelos e o equil√≠brio entre eles, a profundidade e a largura (n√∫mero de canais) da rede, bem como a resolu√ß√£o das imagens na grade.  Eles oferecem um novo m√©todo de dimensionamento que dimensiona uniformemente a profundidade / largura / resolu√ß√£o.  Mostre sua efic√°cia no MobileNet e ResNet. </p><br><p>  Eles tamb√©m usam a Pesquisa de arquitetura neural para criar uma nova malha e escal√°-la, obtendo assim uma classe de novos modelos - EfficientNets.  Eles s√£o melhores e muito mais econ√¥micos do que as redes anteriores.  No ImageNet, o EfficientNet-B7 atinge 84,4% de precis√£o no top 1 e 97,1% no top 5, sendo 8,4 vezes menor e 6,1 vezes mais r√°pido em infer√™ncia do que o atual ConvNet, o melhor da categoria.  Ele transfere bem para outros conjuntos de dados - eles obtiveram SOTA em 5 dos 8 conjuntos de dados mais populares. </p><br><p>  <strong>Escala de modelo composto</strong> <br>  A escala √© quando as opera√ß√µes executadas dentro da grade s√£o fixas e apenas a profundidade (n√∫mero de repeti√ß√µes dos mesmos m√≥dulos) d, largura (n√∫mero de canais em convolu√ß√£o) e resolu√ß√£o r s√£o alteradas.  No pager, o dimensionamento √© formulado como um problema de otimiza√ß√£o - queremos a Precis√£o m√°xima (Net (d, w, r)), apesar do fato de n√£o rastrearmos a mem√≥ria e os FLOPS. </p><br><p>  Realizamos experimentos e nos certificamos de que realmente ajuda a dimensionar em profundidade e resolu√ß√£o ao dimensionar em largura.  Com os mesmos FLOPS, obtemos um resultado significativamente melhor no ImageNet (veja a figura acima).  Em geral, isso √© razo√°vel, porque parece que ao aumentar a resolu√ß√£o da imagem da rede, s√£o necess√°rias mais camadas em profundidade para aumentar o campo receptivo e mais canais para capturar todos os padr√µes da imagem com uma resolu√ß√£o mais alta. </p><br><p>  A ess√™ncia da escala composta: tomamos o coeficiente composto phi, que escala uniformemente d, we er com este coeficiente: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="45.48ex" height="2.419ex" viewBox="0 -780.1 19581.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-3D" x="801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="2107" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-6C" x="2637" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="2935" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="3439" y="0"></use><g transform="translate(4015,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="4895" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-69" x="5975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="6320" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-77" x="6765" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-3D" x="7760" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-62" x="9066" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-65" x="9495" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-74" x="9962" y="0"></use><g transform="translate(10323,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="11203" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="11706" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-69" x="12283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="12628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-72" x="13073" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-3D" x="13803" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-67" x="15109" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="15589" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-6D" x="16119" y="0"></use><g transform="translate(16998,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="17877" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="18381" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-69" x="18957" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="19303" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> d = \ alpha ^ \ phi, w = \ beta ^ \ phi, r = \ gama ^ \ phi, </script>  onde <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="19.238ex" height="2.419ex" viewBox="0 -780.1 8282.8 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="7753" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ alpha, \ beta, \ gama </script>  - constantes obtidas de uma pequena exibi√ß√£o na grade de origem. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ phi </script>  - coeficiente que caracteriza a quantidade de recursos computacionais dispon√≠veis. </p><br><p>  <strong>Rede eficiente</strong> <br>  Para criar a grade, utilizamos a pesquisa de arquitetura neural multiobjetivo, precis√£o e FLOPS otimizados com o par√¢metro respons√°vel pela troca entre eles.  Essa pesquisa deu ao EfficientNet-B0.  Em resumo - Conv seguido por v√°rios MBConv, no final de Conv1x1, Pool, FC. </p><br><p>  Em seguida, fa√ßa o dimensionamento em duas etapas: </p><br><ol><li>  Para come√ßar, consertamos <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.152ex" height="2.419ex" viewBox="0 -780.1 3510.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-69" x="1330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-3D" x="1953" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-31" x="3009" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ phi = 1 </script>  , fa√ßa pesquisa em grade para pesquisa <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="19.238ex" height="2.419ex" viewBox="0 -780.1 8282.8 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-61" x="7753" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ alpha, \ beta, \ gama </script>  . </li><li>  Escale a grade usando as f√≥rmulas para d, we er.  Obtive o EffiientNet-B1.  Da mesma forma, aumentar <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhhi9ioRAWmA2b8l67KzTAJtKJBEFw#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ phi </script>  , obtenha o EfficientNet-B2, ... B7. </li></ol><br><p>  Escalado para diferentes ResNet e MobileNet, em todos os lugares recebeu melhorias significativas no ImageNet, o escalonamento composto proporcionou um aumento significativo em compara√ß√£o ao escalonamento em apenas uma dimens√£o.  Tamb√©m conduzimos experimentos com o EfficientNet em oito conjuntos de dados mais populares, em todos os lugares onde obtivemos o SOTA ou um resultado pr√≥ximo a ele com um n√∫mero significativamente menor de par√¢metros. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">C√≥digo</a> </p><br><h3 id="5-how-the-brain-transitions-from-conscious-to-subliminal-perception">  5. Como o c√©rebro transita da percep√ß√£o consciente para a subliminar </h3><br><p>  Autores do artigo: Francesca Arese Lucini, Gino Del Ferraro, Mariano Sigman, Hernan A. Makse (EUA, Argentina, Espanha, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí Artigo original</a> <br>  Autor da resenha: Svyatoslav Skoblov (em erro folgado_derivativo) </p><br><p>  Este artigo √© uma continua√ß√£o e repensar o trabalho de <em>Dehaene, S, Naccache, L, Cohen, L, Le Bihan, D, Mangin, JF, Poline, JB e Rivie`re, D. Mecanismos cerebrais de mascaramento de palavras e priming inconsciente de repeti√ß√µes</em> , em que os autores tentaram considerar os modos de funcionamento cerebral consciente e inconsciente. </p><br><img src="https://habrastorage.org/webt/fi/o4/te/fio4terok3udte6bcmpajzfs_um.png" width="500" height="250"><br><p><br>  <strong>Experi√™ncia:</strong> <br>  Os volunt√°rios recebem figuras (palavras de 4 letras, uma tela em branco ou rabiscos).  Cada um deles √© mostrado por 30 ms, em geral, toda a a√ß√£o dura 5 minutos. </p><br><ol><li>  No modo experimental "consciente", uma tela em branco alterna com palavras, o que permite que uma pessoa perceba conscientemente o texto. </li><li>  No modo "inconsciente", as palavras se alternam com rabiscos, o que interfere de maneira bastante eficaz na percep√ß√£o do texto em um n√≠vel consciente. </li></ol><br><p>  <strong>Dados:</strong> <br>  Durante esta apresenta√ß√£o, os c√©rebros de nossos primatas foram escaneados usando a resson√¢ncia magn√©tica.  No total, os pesquisadores tiveram 15 volunt√°rios, cada um repetiu o experimento 5 vezes, um total de 75 fluxos de fMRI.  Vale ressaltar que a varredura do voxel acabou sendo bastante grande (muito simplificada: o voxel √© um cubo 3D que cont√©m um n√∫mero bastante grande de c√©lulas) - 4x4x4mm. </p><br><p>  <strong>Magia:</strong> <br>  Vamos chamar o n√≥ de voxel ativo do nosso fluxo.  Como o c√©rebro √© um pano de prato modular, introduzimos nele dois tipos de conex√µes: externa e interna (correspondente ao arranjo espacial dos n√≥s).  As conex√µes s√£o montadas de uma maneira interessante: constru√≠mos uma matriz de correla√ß√£o cruzada entre n√≥s e conectamos os n√≥s a uma conex√£o se a correla√ß√£o for maior que algum par√¢metro adapt√°vel lambda.  Este par√¢metro afeta a descarga da nossa rede. </p><br><p>  O ajuste dos par√¢metros √© realizado usando o procedimento de "filtragem".  Se influenciarmos nosso lambda um pouco, as transi√ß√µes n√≠tidas entre as dimens√µes finais da rede se tornar√£o vis√≠veis (ou seja, uma altera√ß√£o de par√¢metro suficientemente pequena corresponde a um grande incremento de tamanho). </p><br><p>  Portanto: as conex√µes internas s√£o ativadas pelo valor lambda-1, que corresponde ao valor lambda antes de uma transi√ß√£o acentuada.  Externo - valor lambda-2 correspondente ao valor lambda imediatamente ap√≥s uma transi√ß√£o acentuada. </p><br><p>  <strong>Magia 2:</strong> <br>  filtragem k-core.  O conceito k-core descreve a conectividade de rede e √© formulado de maneira simples: a sub-rede m√°xima, cujos n√≥s de todos t√™m pelo menos k vizinhos.  Essa sub-rede pode ser obtida pela remo√ß√£o iterativa de n√≥s com menos de k vizinhos.  Como os n√≥s restantes perder√£o vizinhos, o processo continuar√° at√© que n√£o haja nada a ser exclu√≠do.  O que resta √© a rede k-core. </p><br><p>  <strong>Resultados:</strong> <br>  Ao aplicar esta artilharia em nossos c√©rebros, voc√™ pode ver uma s√©rie de caracter√≠sticas muito interessantes. </p><br><ol><li>  O n√∫mero de n√≥s no n√∫cleo k com k pequeno / muito grande √© extremamente grande.  Mas para o k m√©dio, pelo contr√°rio, n√£o √© suficiente.  Na figura, parece uma forma de U, ou seja, essa configura√ß√£o de rede oferece a maior estabilidade do sistema (resist√™ncia a erros locais e globais). </li><li>  <strong>e os</strong> n√≥s <strong>mais importantes</strong> pertencentes ao k-core com k pequeno podem ser vistos em quase qualquer estado da rede.  Mas um n√∫cleo k com k muito grande √© caracter√≠stico apenas para as partes do c√©rebro que est√£o ativas no estado inconsciente, o <em>giro fusiforme e o giro precentral esquerdo</em> .  As mesmas partes do c√≥rtex s√£o mais ativas e em estado consciente. </li></ol><br><p>  Para verificar o resultado, os autores criaram um milh√£o de redes aleat√≥rias baseadas em redes reais, realizando a fia√ß√£o aleat√≥ria, mantendo o grau original dos n√≥s (o mesmo que o grau do v√©rtice no gr√°fico).  Redes reais diferiam das aleat√≥rias por valores muito maiores de k m√°ximo.  Ao mesmo tempo, a forma em U do n√∫mero de n√≥s nos clusters permaneceu percept√≠vel em redes aleat√≥rias, o que levou os autores √† id√©ia de que √© o grau dos n√≥s que √© respons√°vel por esse fen√¥meno. </p><br><p>  <strong>Conclus√µes:</strong> <br> ,  ,   ,              .     ,     ,      ,    -     (     , , ,     ). </p><br><p>  ,   ,         ,       ,        ,         , ,    - . , ,            qualia. </p><br><h3 id="6-large-memory-layers-with-product-keys"> 6. Large Memory Layers with Product Keys </h3><br><p>  : Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv√© J√©gou (Facebook AI Research, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí  </a> <br>  :   (  belerafon) </p><br><img src="https://habrastorage.org/webt/4b/_q/nm/4b_qnmw2tilj5zyscirkrogerrg.png"><br><p><br> ,       key-value        ,   . </p><br><p>     -    attention.    q,     k   v.  q,    k,    ,      value  .  ,       .    ,         .      ,         ,   .     -    q       (, -10).        .      . </p><br><p>    ‚Äî     q   k   .   ,    "Product Keys".      ,         q   ,     .        -10   , ,     O(N)    ""  ,   (sqrt(N)). </p><br><p>         key-value .      ,    (  ,     ). ,   BERT      28  . ,          ,     .  : 12-       2  ,  24-  ,    perplexity     . </p><br><p>            (      self-attention). ,     -         .  ,         multy-head attention.  I.e.    query  ,      value,     .         -. </p><br><p>        , ,        ,    ,    BERT  .      . </p><br><h3 id="7-are-we-really-making-much-progress-a-worrying-analysis-of-recent-neural-recommendation-approaches"> 7. Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches </h3><br><p>  : Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach (Politecnico di Milano, University of Klagenfurt, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí  </a> <br>  :   (  netcitizen) </p><br><img src="https://habrastorage.org/webt/zk/89/wu/zk89wuyudxpggdl6-0tyxe2wwrw.png"><br><p><br>       DL    , ,             . </p><br><p> <strong></strong> <br>             DL       top-n.    DL      KDD, SIGIR, TheWebConf (WWW)  RecSys     : </p><br><ol><li>     </li><li>    -       </li><li>       </li></ol><br><p> <strong></strong> </p><br><ol><li>    7/18 (39%) </li><li>        ‚Äú‚Äù    train/test,     .,   , ,   . </li><li>     (Variational Autoencoders for Collaborative Filtering (Mult-VAE)  ¬±   )     KNN, SVD, PR. </li></ol><br><p> <strong></strong> <br>  DL,       CV, NLP      ,       . </p><br><h3 id="8-omni-scale-feature-learning-for-person-re-identification"> 8. Omni-Scale Feature Learning for Person Re-Identification </h3><br><p>  : Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang (University of Surrey, Queen Mary University, Samsung AI, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí  </a> <br>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> (  graviton) </p><br><p>  Person Re-Identification,    Face Recognition,    ,          .  (Kaiyang Zhou)        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deep-person-reid</a>     ,      (OSNet),          Person Re-Identification.     . </p><br><p> <strong>  </strong> Person Re-Identification: </p><br><img src="https://habrastorage.org/webt/6d/rq/jv/6drqjvwsyg33s_e7zv2t4nov0ts.png" width="500" height="250"><br><p><br> <strong> :</strong> </p><br><ol><li>   conv1x1  deepwise conv3x3   conv3x3   (figure 3). </li><li>  ,      .    ResNeXt         ,     Inception      (figure 4). </li><li>      ‚Äúaggregation gate‚Äù       .  ,    Inception     . </li></ol><br><img src="https://habrastorage.org/webt/ic/tc/9z/ictc9z6mcdkv3o0i23swu_f5zty.png"><br><p><br>  OSNet       , ..      ,    :      (  ,  )   . </p><br><p> <strong>  ReID </strong>  OSNet ( 2  )         (Market: R1 93.6%, mAP 81.0%  OSNet  R1 87.0%, mAP 69.5%  MobileNetV2)          ResNet  DenseNet (Market: R1 94.8%, mAP 84.9%  OSNet  R1 94.8%, mAP 86.0%  ResNet). </p><br><p> Outro desafio √© a <strong>adapta√ß√£o do dom√≠nio</strong> : modelos treinados em um conjunto de dados t√™m baixa qualidade em outro.  A OSNet tamb√©m mostra bons resultados nesse segmento sem o uso de "adapta√ß√£o de dom√≠nio n√£o supervisionada" (usando dados de teste de forma n√£o alocada para uniformizar a distribui√ß√£o dos dados). </p><br><p>  A arquitetura tamb√©m foi testada no ImageNet, onde alcan√ßou precis√£o semelhante com o MobileNetV2 com menos par√¢metros, mas mais opera√ß√µes. </p><br><h3 id="9-neural-reparameterization-improves-structural-optimization">  9. Reparameteriza√ß√£o neural melhora a otimiza√ß√£o estrutural </h3><br><p>  Autores: Stephan Hoyer, Jascha Sohl-Dickstein, Sam Greydanus (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Üí Artigo original</a> <br>  Autor do coment√°rio: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Alexey</a> (em Arech slack) </p><br><img src="https://habrastorage.org/webt/ga/sd/hd/gasdhdqgy5febp9elrdgdhy9nqg.png"><br><p><br>  Na constru√ß√£o e outras tecnologias, h√° tarefas de otimizar a estrutura / topologia de alguma solu√ß√£o.  Grosso modo, esta √© uma resposta do computador para uma pergunta como, por exemplo, como projetar uma ponte / constru√ß√£o / asa de um avi√£o / l√¢mina de turbina / blablabla, para que certas restri√ß√µes sejam cumpridas e a estrutura seja suficientemente forte.  H√° um conjunto de m√©todos de solu√ß√£o "padr√£o" - ele funciona, mas nem sempre tudo √© bom l√°. </p><br><p>  O que esses caras do Google inventaram?  Eles disseram: vamos gerar uma solu√ß√£o por uma rede neural (a parte de upsampling da UNet) e, em seguida, usar um modelo f√≠sico diferenci√°vel, que calcular√° o comportamento de uma solu√ß√£o sob a influ√™ncia de todas as for√ßas e gravidade, calcule a fun√ß√£o objetivo - for√ßa (mais precisamente, o inverso - conformidade) ) designs.  Ent√£o, como tudo √© automaticamente diferenci√°vel, obtemos o gradiente da fun√ß√£o objetivo, que √© empurrada por toda a estrutura de volta aos pesos e √† entrada da rede neural.  Alteramos pesos e entrada e continuamos o ciclo at√© a converg√™ncia para uma solu√ß√£o est√°vel. </p><br><p>  Os resultados acabaram sendo pequenos (em termos de tamanho do espa√ßo de poss√≠veis solu√ß√µes) compar√°veis ‚Äã‚Äãaos m√©todos tradicionais de otimiza√ß√£o de topologias e, para grandes problemas, s√£o visivelmente melhores que os tradicionais (excesso de peso em 99 versus 66 em 116 problemas).  Al√©m disso, as solu√ß√µes resultantes costumam ser significativamente mais tecnol√≥gicas e ideais do que as decis√µes das linhas de base. </p><br><p>  I.e.  na verdade, eles usaram o NS como uma maneira complicada de parametrizar o modelo f√≠sico da estrutura, que implicitamente (devido √† arquitetura do NS) √© capaz de impor algumas restri√ß√µes √∫teis aos valores dos par√¢metros (controlado pela remo√ß√£o do NS do m√©todo e otimiza√ß√£o direta dos valores de pixel). </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">C√≥digo fonte</a> </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma vis√£o geral mais detalhada deste artigo no habr.</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt472672/">https://habr.com/ru/post/pt472672/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt472658/index.html">Quase tudo sobre o futuro HolyJS 2019 Moscow</a></li>
<li><a href="../pt472660/index.html">Como prototipar rapidamente dispositivos e por que isso √© importante. Relat√≥rio Yandex.Taxi</a></li>
<li><a href="../pt472662/index.html">Com quem exportar</a></li>
<li><a href="../pt472668/index.html">Produto pensando. O que √© e como desenvolv√™-lo</a></li>
<li><a href="../pt472670/index.html">Limely outono, Limely inverno ...</a></li>
<li><a href="../pt472674/index.html">Vari√°veis ‚Äã‚Äãde ambiente para projetos Python</a></li>
<li><a href="../pt472676/index.html">Criamos o departamento de Jones para ajudar as equipes principais, usando apenas Slack, Jira e a fita azul</a></li>
<li><a href="../pt472682/index.html">Retardando o envelhecimento com sinergias medicamentosas em C. elegans</a></li>
<li><a href="../pt472684/index.html">Surpresa fsync () PostgreSQL</a></li>
<li><a href="../pt472686/index.html">Est√∫dio de V√≠deo Baseado no i486</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>