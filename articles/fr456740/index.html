<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêØ üëÜ ü§Ωüèº Immersion dans les r√©seaux de neurones convolutifs. Partie 5/1 - 9 üç´ üö≥ üñåÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le cours complet de russe se trouve sur ce lien . 
 Le cours d'anglais original est disponible sur ce lien . 



 De nouvelles conf√©rences sont pr√©vue...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Immersion dans les r√©seaux de neurones convolutifs. Partie 5/1 - 9</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456740/"><p>  Le cours complet de russe se trouve sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . <br>  Le cours d'anglais original est disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . </p><br><p><img src="https://habrastorage.org/webt/1m/hz/qn/1mhzqnwa288uxjmptqhexriiahc.png"><br>  <i>De nouvelles conf√©rences sont pr√©vues tous les 2-3 jours.</i> </p><a name="habracut"></a><br><h1>  Table des mati√®res </h1><br><ol><li>  Entretien avec Sebastian Trun </li><li>  Pr√©sentation </li><li>  Dogs and Cats Dataset </li><li>  Images de diff√©rentes tailles </li><li>  Images couleur.  Partie 1 </li><li>  Images couleur.  2e partie </li><li>  Op√©ration de convolution sur des images couleur </li><li>  L'op√©ration de sous-√©chantillonnage par la valeur maximale dans les images en couleur </li><li>  CoLab: chats et chiens </li><li>  Softmax et sigmo√Øde </li><li>  V√©rifier </li><li>  Extension d'image </li><li>  Exception </li><li>  CoLab: chiens et chats.  R√©p√©tition </li><li>  Autres techniques pour emp√™cher le recyclage </li><li>  Exercices: classification des images en couleur </li><li>  Solution: classification des images couleur </li><li>  R√©sum√© </li></ol><br><h1>  Entretien avec Sebastian Trun </h1><br><p>  - Donc, aujourd'hui, nous sommes ici √† nouveau, avec Sebastian et nous parlerons de recyclage.  Ce sujet est tr√®s int√©ressant pour nous, en particulier dans les parties pratiques du cours actuel sur l'utilisation de TensorFlow. <br>  - Sebastian, avez-vous d√©j√† rencontr√© du sur-ajustement - fini, en forme?  Si vous dites que vous ne l'avez pas rencontr√©, alors je dirai certainement que je ne peux pas vous croire! <br>  - Ainsi, la raison du recyclage est ce que l'on appelle le compromis <strong>biais-variance</strong> (un compromis entre les valeurs du param√®tre de biais et leur propagation).  Un r√©seau de neurones dans lequel un petit nombre de poids n'est pas en mesure d'apprendre un nombre suffisant d'exemples, une situation similaire dans l'apprentissage automatique est appel√©e distorsion. <br>  - Oui. <br>  - Un r√©seau de neurones avec autant de param√®tres peut choisir arbitrairement une solution que vous n'aimez pas, simplement √† cause d'un si grand nombre de ces param√®tres.  Le r√©sultat du choix d'une solution de r√©seau neuronal d√©pend de la variabilit√© des donn√©es sources.  Ainsi, une r√®gle simple peut √™tre formul√©e: plus il y a de param√®tres dans le r√©seau concernant la taille (quantit√©) des donn√©es, plus il est probable d'obtenir une solution al√©atoire au lieu de la bonne.  Par exemple, vous vous demandez: ¬´Qui est l'homme dans cette pi√®ce et qui est la femme?¬ª  Un r√©seau neuronal complexe peut vous dire que, par exemple, tous ceux dont le nom commence par T sont des hommes et ne se recyclent jamais.  Il y a deux solutions.  Le premier utilise un ensemble de donn√©es d'exclusion (une petite quantit√© de l'ensemble d'apprentissage pour valider la pr√©cision du mod√®le).  Vous pouvez prendre les donn√©es, les diviser en deux parties - 90% pour la formation et 10% pour tester et effectuer la soi-disant validation crois√©e, o√π vous v√©rifiez la pr√©cision du mod√®le sur les donn√©es que le r√©seau de neurones n'a pas vues - d√®s que la valeur d'erreur commence cro√Ætre apr√®s un certain cycle de formation - il est temps d'arr√™ter d'apprendre.  La deuxi√®me solution consiste √† introduire des restrictions dans le r√©seau neuronal.  Par exemple, pour limiter les valeurs des param√®tres de d√©placements et de poids, en les rapprochant de plus en plus de z√©ro.  Plus les poids sont limit√©s, moins le mod√®le sera recycl√©. <br>  - Je comprends bien que nous pouvons avoir des ensembles de donn√©es pour la formation et les tests et la validation, non? <br>  - C'est vrai.  Si vous avez un ensemble de donn√©es √† valider, vous devez avoir un ensemble de donn√©es que vous n'avez jamais touch√© ou montr√© √† votre r√©seau neuronal.  Si vous avez montr√© au mod√®le un certain ensemble de donn√©es plusieurs fois, alors, bien s√ªr, le processus de recyclage commencera, ce qui est tr√®s mauvais pour nous. <br>  - Peut-√™tre vous souviendrez-vous des cas les plus int√©ressants lorsque votre mod√®le a √©t√© recycl√©? <br>  - Ah, oui ... il y a eu un tel incident dans ma premi√®re jeunesse quand je d√©veloppais un r√©seau de neurones pour jouer aux √©checs.  C'√©tait en 1993. Ce qui √©tait int√©ressant, c'est qu'√† partir des donn√©es d'√©checs sur lesquelles le r√©seau neuronal √©tait form√©, le r√©seau a rapidement d√©termin√© que si un expert d√©place la reine au centre de l'√©chiquier, il y a 60% de chances de gagner.  Ce qu'elle a commenc√© √† faire √©tait d'ouvrir le ¬´passage¬ª avec un pion et de d√©placer la reine au centre de l'√©chiquier.  C'√©tait une d√©cision si stupide pour tout joueur d'√©checs, qui t√©moignait clairement de la reconversion du mod√®le. <br>  - G√©nial!  Nous avons donc discut√© de plusieurs techniques pour am√©liorer nos mod√®les.  Selon vous, quel est l'aspect le plus sous-estim√© de l'apprentissage en profondeur? <br>  - 90% de votre travail est sous-estim√©, car 90% de votre travail consistera en un nettoyage des donn√©es. <br>  - Ici, je suis compl√®tement d'accord avec toi! <br>  - Comme le montre la pratique, tout ensemble de donn√©es contient une sorte de d√©chets.  Il est tr√®s difficile d'amener les donn√©es au bon type, pour les rendre coh√©rentes, c'est un processus tr√®s long. <br>  - Oui, m√™me si vous travaillez avec des ensembles de donn√©es tels que des images ou des vid√©os, o√π, semble-t-il, toutes les informations sont d√©j√† l√†, √† l'int√©rieur, il y a toujours un besoin de pr√©traiter les images. <br>  - Les seules personnes pour qui les donn√©es sont id√©ales sont les professeurs, car ils ont la possibilit√© de pr√©tendre dans une pr√©sentation dans PowerPoint que tout est comme il se doit et que tout est parfait!  En r√©alit√©, 90% de votre temps sera occup√© par le nettoyage des donn√©es. <br>  - G√©nial.  Alors, d√©couvrons-en plus sur le recyclage et les techniques qui nous permettront d'am√©liorer nos mod√®les d'apprentissage en profondeur. </p><br><h1>  Pr√©sentation </h1><br><p>  - salut!  Et encore une fois, bienvenue au cours! <br>  ¬´Dans la derni√®re le√ßon, nous avons d√©velopp√© un petit r√©seau de neurones convolutionnels pour classer les images des v√™tements en nuances de gris √† partir du jeu de donn√©es FASHION MNIST.  Nous avons vu en pratique que notre petit r√©seau de neurones peut classer les images entrantes avec une pr√©cision assez √©lev√©e.  Cependant, dans le monde r√©el, nous devons travailler avec des images haute r√©solution et diff√©rentes tailles.  L'un des grands avantages du SNA est qu'il peut tout aussi bien fonctionner avec des images en couleur.  Par cons√©quent, nous allons commencer notre le√ßon actuelle en explorant le fonctionnement du SCN avec les images en couleur. <br>  - Plus tard, √† la m√™me fr√©quence, vous construirez un r√©seau neuronal convolutif qui pourra classer des images de chats et de chiens.  En route vers la mise en place d'un r√©seau neuronal convolutif capable de classer les images de chats et de chiens, nous apprendrons √©galement √† utiliser diff√©rentes techniques pour r√©soudre l'un des probl√®mes les plus courants des r√©seaux neuronaux - la reconversion.  Et √† la fin de cette le√ßon, dans la partie pratique, vous d√©velopperez votre propre r√©seau neuronal convolutif pour classer les images en couleur.  Commen√ßons! </p><br><h1>  Ensemble de donn√©es chats et chiens </h1><br><p>  Jusqu'√† ce moment, nous ne travaillions qu'avec des images en niveaux de gris et des tailles 28x28 du jeu de donn√©es FASHION MNIST. </p><br><p><img src="https://habrastorage.org/webt/e4/1o/cb/e41ocb39ngplbr8osyfccvji0mm.png"></p><br><p>  Dans les applications r√©elles, nous sommes oblig√©s de rencontrer des images de diff√©rentes tailles, par exemple, celles illustr√©es ci-dessous: </p><br><p><img src="https://habrastorage.org/webt/xs/oc/u2/xsocu2t1m1ywlkqk5_qdfslglow.png"></p><br><p>  Comme nous l'avons mentionn√© au d√©but de cette le√ßon, dans cette le√ßon, nous d√©velopperons un r√©seau neuronal convolutif qui peut classer les images en couleur des chiens et des chats. </p><br><p>  Pour mettre en ≈ìuvre nos plans, nous utiliserons des images de chats et de chiens du jeu de donn√©es Microsoft Asirra.  Chaque image de cet ensemble de donn√©es est √©tiquet√©e 1 ou 0 s'il y a un chien ou un chat dans l'image, respectivement. </p><br><p><img src="https://habrastorage.org/webt/pn/4e/uf/pn4euf-gaaxlv8_2eakpvntjtkw.png"></p><br><p>  Malgr√© le fait que l'ensemble de donn√©es Microsoft Asirra contient plus de 3 millions d'images balis√©es de chats et de chiens, seulement 25 000 sont accessibles au public.  La formation de notre r√©seau neuronal convolutionnel sur ces 25 000 images prendra beaucoup de temps.  C'est pourquoi nous utiliserons un petit nombre d'images pour former notre r√©seau de neurones convolutionnels √† partir des 25 000 disponibles. </p><br><p>  Notre sous-ensemble d'images de formation comprend 2 000 pi√®ces et 1 000 pi√®ces d'images pour la validation du mod√®le.  Dans l'ensemble de donn√©es de formation, 1 000 images contiennent des chats et les 1 000 autres images contiennent des chiens.  Nous parlerons de l'ensemble de donn√©es √† valider un peu plus loin dans cette partie de la le√ßon. </p><br><p><img src="https://habrastorage.org/webt/oa/wh/dg/oawhdglv3_dtccwky_e1pjwygog.png"></p><br><p>  En travaillant avec cet ensemble de donn√©es, nous rencontrerons deux difficult√©s principales: travailler avec des images de tailles diff√©rentes et travailler avec des images en couleur. </p><br><p>  Commen√ßons √† explorer comment travailler avec des images de diff√©rentes tailles. </p><br><h1>  Images de diff√©rentes tailles </h1><br><p>  Notre premier test sera de r√©soudre le probl√®me du traitement d'images de diff√©rentes tailles.  C'est parce qu'un r√©seau de neurones √† l'entr√©e a besoin de donn√©es de taille fixe. </p><br><p> Par exemple, vous pouvez vous rappeler de nos parties pr√©c√©dentes en utilisant le param√®tre <code>input_shape</code> lors de la cr√©ation d'un calque <code>Flatten</code> : </p><br><p><img src="https://habrastorage.org/webt/v5/tt/hk/v5tthkilik-9reer8owxjpv-x3m.png"></p><br><p>  Avant de transmettre l'image d'un √©l√©ment vestimentaire √† un r√©seau neuronal, nous l'avons convertie en un tableau 1D d'une taille fixe - 28x28 = 784 √©l√©ments (pixels).  √âtant donn√© que les images du jeu de donn√©es Fashion MNIST √©taient de la m√™me taille, le tableau unidimensionnel r√©sultant √©tait de la m√™me taille et comprenait 784 √©l√©ments. </p><br><p>  Cependant, en travaillant avec des images de diff√©rentes tailles (hauteur et largeur) et en les transformant en tableaux unidimensionnels, nous obtenons des tableaux de diff√©rentes tailles. </p><br><p>  √âtant donn√© que les r√©seaux de neurones √† l'entr√©e n√©cessitent des donn√©es de la m√™me taille, il ne suffit pas de se contenter de la conversion en un tableau unidimensionnel de valeurs de pixels. </p><br><p>  Pour r√©soudre les probl√®mes de classification des images, nous avons toujours recours √† l'une des options pour unifier les donn√©es d'entr√©e - r√©duire la taille des images √† des valeurs communes (redimensionnement). </p><br><p><img src="https://habrastorage.org/webt/dt/rt/7f/dtrt7frns4fdinexvf0e3bow5gi.png"></p><br><p>  Dans ce didacticiel, nous recourrons √† redimensionner toutes les images √† des tailles de 150 pixels en hauteur et 150 pixels en largeur.  En convertissant les images en une seule taille, nous garantissons ainsi que l'image de la bonne taille arrivera √† l'entr√©e du r√©seau neuronal et, lorsqu'elle sera transf√©r√©e sur une couche <code>flatten</code> , nous obtiendrons un tableau unidimensionnel de la m√™me taille. </p><br><pre> <code class="python hljs">tf.keras.layers.Flatten(input_shape(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre> <br><p>  En cons√©quence, nous avons obtenu un tableau unidimensionnel compos√© de 150x150 = 22 500 valeurs (pixels). </p><br><p>  Le prochain probl√®me auquel nous serons confront√©s sera celui des images couleur - couleur.  Nous en parlerons dans la prochaine partie. </p><br><h1>  Images couleur.  Partie 1 </h1><br><p>  Afin de comprendre et de comprendre comment les r√©seaux de neurones convolutifs fonctionnent avec les images en couleur, nous devons approfondir le fonctionnement exact du SNA en g√©n√©ral.  Rafra√Æchissons ce que nous savons d√©j√†. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3b5/c0c/687/3b5c0c68738975962dc882b9ab1522b2.png" alt="image"></p><br><p>  Un exemple ci-dessus est une image en niveaux de gris et comment l'ordinateur l'interpr√®te comme un tableau bidimensionnel de valeurs de pixels. </p><br><p>  Un exemple ci-dessous est une image, cette fois une <strong>couleur,</strong> et comment l'ordinateur l'interpr√®te comme un tableau tridimensionnel de valeurs de pixels. </p><br><p><img src="https://habrastorage.org/webt/n-/2u/ic/n-2uicbpmraz7i2sggtwqckexbc.png"></p><br><p>  La hauteur et la largeur du r√©seau 3D seront d√©termin√©es par la hauteur et la largeur de l'image, et la profondeur (profondeur) d√©termine le nombre de canaux de couleur de l'image. </p><br><p>  La plupart des images en couleur peuvent √™tre repr√©sent√©es par trois canaux de couleur - rouge (rouge), vert (vert) et bleu (bleu). </p><br><p><img src="https://habrastorage.org/webt/kn/14/zm/kn14zmbazrhrnbhtweoil9m-ipk.png"></p><br><p>  Les images compos√©es de canaux rouge, vert et bleu sont appel√©es images RVB.  La combinaison de ces trois canaux donne une image couleur.  Dans chacune des images RVB, chaque canal est repr√©sent√© par un r√©seau bidimensionnel distinct de pixels. </p><br><p><img src="https://habrastorage.org/webt/56/h5/g6/56h5g6loe_bu4_oiuu0-vy_unoc.png"></p><br><p>  Puisque le nombre de canaux que nous avons est de trois, nous aurons donc trois tableaux bidimensionnels.  Ainsi, une image couleur compos√©e de 3 canaux de couleur aura la repr√©sentation suivante: </p><br><p><img src="https://habrastorage.org/webt/nn/2r/q6/nn2rq6itb9gz5suamhcl5kvjwtu.png"></p><br><h1>  Images couleur.  2e partie </h1><br><p>  Ainsi, puisque notre image sera d√©sormais compos√©e de 3 couleurs, ce qui signifie que ce sera un tableau tridimensionnel de valeurs de pixels, alors notre code devra √™tre modifi√© en cons√©quence. </p><br><p>  Si vous regardez le code que nous avons utilis√© dans notre derni√®re le√ßon lorsque nous avons r√©solu le probl√®me de la classification des √©l√©ments de v√™tements dans les images, nous pouvons voir que nous avons indiqu√© la dimension des donn√©es d'entr√©e: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Les deux premiers param√®tres du tuple <code>(28,28,1)</code> sont les valeurs de la hauteur et de la largeur de l'image.  Les images du jeu de donn√©es Fashion MNIST mesuraient 28 x 28 pixels.  Le dernier param√®tre du tuple <code>(28,28,1)</code> indique le nombre de canaux de couleur.  Dans le jeu de donn√©es Fashion MNIST, les images √©taient uniquement dans les tons de gris - 1 canal de couleur. </p><br><p>  Maintenant que la t√¢che est devenue un peu plus compliqu√©e et que nos images de chats et de chiens sont devenues de tailles diff√©rentes (mais converties en une seule - 150x150 pixels) et contiennent 3 canaux de couleur, le tuple de valeurs devrait √©galement √™tre diff√©rent: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)))</code> </pre> <br><p>  Dans la partie suivante, nous verrons comment la convolution est calcul√©e en pr√©sence de trois canaux de couleur dans l'image. </p><br><h1>  Op√©ration de convolution sur des images couleur </h1><br><p>  Dans les le√ßons pr√©c√©dentes, nous avons appris √† effectuer une op√©ration de convolution sur des images en niveaux de gris.  Mais comment effectuer une op√©ration de convolution sur des images couleur?  Commen√ßons par r√©p√©ter comment l'op√©ration de convolution est effectu√©e sur les images en niveaux de gris. </p><br><p>  Tout commence par un filtre (core) d'une certaine taille. </p><br><p><img src="https://habrastorage.org/webt/hv/jo/1h/hvjo1h8-b8xxaxnd5yzoj_6cr78.png"></p><br><p>  Le filtre est situ√© sur un pixel d'image sp√©cifique √† convertir, puis chaque valeur de filtre est multipli√©e par la valeur de pixel correspondante dans l'image et toutes ces valeurs sont additionn√©es.  La valeur finale du pixel est d√©finie dans la nouvelle image √† l'endroit o√π se trouvait le pixel original converti.  L'op√©ration est r√©p√©t√©e pour chaque pixel de l'image d'origine. </p><br><p>  Il convient √©galement de rappeler que pendant l'op√©ration de convolution, afin de ne pas perdre d'informations aux bords de l'image, nous pouvons appliquer l'alignement et garnir les bords de l'image avec des z√©ros: </p><br><p><img src="https://habrastorage.org/webt/fc/wa/1s/fcwa1sx3ekaufxf0zzstuzzk7bo.png"></p><br><p>  Voyons maintenant comment effectuer l'op√©ration de convolution sur des images en couleur. </p><br><p>  Tout comme lors de la conversion d'une image en nuances de gris, on commence par choisir la taille du filtre (core) d'une certaine taille. </p><br><p><img src="https://habrastorage.org/webt/ho/s2/8d/hos28dmxwzluclof1hpjtwa8eq0.png"></p><br><p>  La seule diff√©rence maintenant sera que maintenant le filtre lui-m√™me sera en trois dimensions, et la valeur du param√®tre de profondeur sera √©gale √† la valeur du nombre de canaux de couleur dans l'image - 3 (dans notre cas, RVB).  Pour chaque ¬´couche¬ª du canal de couleur, nous appliquerons √©galement l'op√©ration de convolution avec un filtre de la taille s√©lectionn√©e.  Voyons comment ce sera un exemple. </p><br><p><img src="https://habrastorage.org/webt/77/yz/ms/77yzmskwfveucvryfrqnfqk9ti0.png"></p><br><p>  Imaginez que nous avons une image RVB et que nous voulons appliquer l'op√©ration de convolution avec le prochain filtre 3D.  Il convient de noter que notre filtre est compos√© de 3 filtres bidimensionnels.  Par souci de simplicit√©, imaginons que notre image RVB mesure 5x5 pixels. </p><br><p><img src="https://habrastorage.org/webt/eg/46/54/eg4654ymcuktxfo-sybo7c2jw9q.png"></p><br><p>  Rappelons √©galement que chaque canal de couleur est un tableau bidimensionnel de valeurs de couleur de pixel. </p><br><p><img src="https://habrastorage.org/webt/th/sk/9y/thsk9yyqd91koooaqcftklhyq-i.png"></p><br><p>  Comme pour l'op√©ration de convolution sur des images dans des tons de gris, ainsi que sur des images en couleur, nous alignerons et compl√©terons l'image avec des z√©ros sur les bords pour √©viter la perte d'informations aux bordures. </p><br><p><img src="https://habrastorage.org/webt/us/vl/h7/usvlh7czpbmfn2iwbibynpr_aem.png"></p><br><p>  Nous sommes maintenant pr√™ts pour l'op√©ration de convolution! </p><br><p>  Le m√©canisme de convolution pour les images en couleur sera similaire au processus que nous avons effectu√© avec les images en niveaux de gris.  La seule diff√©rence entre les op√©rations effectu√©es sur les images en niveaux de gris et en couleur est que l'op√©ration de convolution doit maintenant √™tre effectu√©e 3 fois pour chaque canal de couleur. </p><br><p><img src="https://habrastorage.org/webt/-8/mq/x5/-8mqx5ehxcqfg_4jttweanhhrhc.png"></p><br><p>  Ensuite, apr√®s avoir effectu√© l'op√©ration de convolution sur chaque canal de couleur, additionnez les trois valeurs obtenues et ajoutez-y 1 (la valeur standard utilis√©e pour effectuer des op√©rations de ce type).  La nouvelle valeur r√©sultante est fix√©e √† la m√™me position dans la nouvelle image, position dans laquelle se trouvait le pixel converti actuel. </p><br><p>  Nous effectuons une op√©ration de conversion similaire (une op√©ration de convolution) pour chaque pixel de notre image d'origine et pour chaque canal de couleur. </p><br><p>  Dans cet exemple particulier, l'image r√©sultante a la m√™me taille en hauteur et en largeur que notre image RVB d'origine. </p><br><p>  Comme vous pouvez le voir, l'application de l'op√©ration de convolution avec un seul filtre 3D entra√Æne une seule valeur de sortie. </p><br><p><img src="https://habrastorage.org/webt/y_/kq/c7/y_kqc7d4p07hq7v-qkfxwakwuwg.png"></p><br><p>  Cependant, lorsque vous travaillez avec des r√©seaux de neurones convolutifs, il est courant d'utiliser plusieurs filtres 3D.  Si nous utilisons plus d'un filtre 3D, le r√©sultat sera plusieurs valeurs de sortie - chaque valeur est le r√©sultat d'un filtre. </p><br><p><img src="https://habrastorage.org/webt/36/ci/01/36ci013hfcfdcdthe23etkcg-_g.png"></p><br><p>  Dans notre exemple ci-dessus, puisque nous utilisons 3 filtres, la repr√©sentation 3D r√©sultante aura une profondeur de 3 - chaque couche correspondra √† la valeur de sortie de la conversion d'un filtre au-dessus de l'image avec tous ses canaux de couleur. </p><br><p>  Si, par exemple, au lieu de 3 filtres, nous d√©cidions d'utiliser 16, la repr√©sentation 3D de sortie contiendrait 16 couches de profondeur. </p><br><p>  Dans le code, nous pouvons contr√¥ler le nombre de filtres cr√©√©s en passant la valeur appropri√©e pour le param√®tre <code>filters</code> : </p><br><pre> <code class="python hljs">tf.keras.layers.Conv2D(filters, kernel_size, ...)</code> </pre> <br><p>  Nous pouvons √©galement sp√©cifier la taille du filtre via le param√®tre <code>kernel_size</code> .  Par exemple, pour cr√©er 3 filtres de taille 3x3, comme c'√©tait le cas dans notre exemple ci-dessus, nous pouvons √©crire le code comme suit: </p><br><pre> <code class="python hljs">tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">3</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), ...)</code> </pre> <br><p>  N'oubliez pas que lors de la formation du r√©seau neuronal convolutionnel, les valeurs des filtres 3D seront mises √† jour pour minimiser la valeur de la fonction de perte. </p><br><p>  Maintenant que nous savons comment effectuer l'op√©ration de convolution sur des images en couleur, il est temps de comprendre comment appliquer l'op√©ration de sous-√©chantillonnage au r√©sultat maximal par la valeur maximale (le m√™me regroupement maximal). </p><br><h1>  L'op√©ration de sous-√©chantillonnage par la valeur maximale dans les images en couleur </h1><br><p>  Voyons maintenant comment effectuer l'op√©ration de sous-√©chantillonnage √† la valeur maximale dans les images en couleur.  En fait, l'op√©ration de sous-√©chantillonnage par la valeur maximale fonctionne de la m√™me mani√®re qu'elle fonctionne avec des images dans des tons de gris avec une l√©g√®re diff√©rence - l'op√©ration de sous-√©chantillonnage doit maintenant √™tre appliqu√©e √† chaque repr√©sentation de sortie que nous avons re√ßue √† la suite de l'application de filtres.  Regardons un exemple. </p><br><p>  Pour simplifier, imaginons que notre vue de sortie ressemble √† ceci: </p><br><p><img src="https://habrastorage.org/webt/b0/-k/u1/b0-ku1roxi7hyplaadecnribfvw.png"></p><br><p>  Comme pr√©c√©demment, nous utiliserons un noyau 2x2 et l'√©tape 2 pour effectuer l'op√©ration de sous-√©chantillonnage √† la valeur maximale.  L'op√©ration de sous-√©chantillonnage par la valeur maximale commence par ¬´l'installation¬ª d'un noyau 2x2 dans le coin sup√©rieur gauche de chaque repr√©sentation de sortie (la repr√©sentation qui a √©t√© obtenue apr√®s l'application de l'op√©ration de convolution). </p><br><p><img src="https://habrastorage.org/webt/sc/hv/68/schv68ab1pzdg-lcazzelhmmvr8.png"></p><br><p>  Nous pouvons maintenant d√©marrer l'op√©ration de sous-√©chantillonnage √† la valeur maximale.  Par exemple, dans notre premi√®re repr√©sentation de sortie, les valeurs suivantes sont tomb√©es dans le noyau 2x2 - 1, 9, 5, 4. Puisque la valeur maximale dans ce noyau est 9, c'est elle qui est envoy√©e √† la nouvelle repr√©sentation de sortie.  Une op√©ration similaire est r√©p√©t√©e pour chaque repr√©sentation d'entr√©e. </p><br><p>  Par cons√©quent, nous devrions obtenir le r√©sultat suivant: </p><br><p><img src="https://habrastorage.org/webt/9v/um/_0/9vum_0x2p4a78inha_xv3rnce8y.png"></p><br><p>  Apr√®s avoir effectu√© l'op√©ration de sous-√©chantillonnage par la valeur maximale, le r√©sultat est 3 tableaux bidimensionnels, chacun √©tant 2 fois plus petit que la repr√©sentation d'entr√©e d'origine. </p><br><p>  Ainsi, dans ce cas particulier, lors de l'ex√©cution de l'op√©ration de sous-√©chantillonnage par la valeur maximale sur la repr√©sentation d'entr√©e tridimensionnelle, nous obtenons une repr√©sentation de sortie tridimensionnelle de la m√™me profondeur, mais avec les valeurs de hauteur et de largeur moiti√© des valeurs initiales. </p><br><p>  C'est donc toute la th√©orie dont nous aurons besoin pour poursuivre nos travaux.  Voyons maintenant comment cela fonctionne dans le code! </p><br><h1>  CoLab: chats et chiens </h1><br><p>  CoLab original en anglais est disponible √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . <br>  CoLab en russe est disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . </p><br><p>  Dans ce tutoriel, nous verrons comment cat√©goriser les images de chats et de chiens.  Nous allons d√©velopper un classificateur d'images en utilisant le mod√®le <code>tf.keras.Sequential</code> et utiliser <code>tf.keras.Sequential</code> pour charger les donn√©es. </p><br><h3 id="idei-kotorye-budut-zatronuty-v-etoy-chasti">  Id√©es √† couvrir dans cette partie: </h3><br><p>  Nous allons acqu√©rir une exp√©rience pratique dans le d√©veloppement d'un classificateur et d√©velopper une compr√©hension intuitive des concepts suivants: </p><br><ol><li>  Cr√©ation d'un mod√®le de flux de donn√©es ( <em>pipelines d'entr√©e de donn√©es</em> ) √† l'aide de la classe <code>tf.keras.preprocessing.image.ImageDataGenerator</code> (Comment travailler efficacement avec des donn√©es sur disque en interaction avec le mod√®le?) </li><li>  Recyclage - qu'est-ce que c'est et comment le d√©terminer? </li></ol><br><p>  <strong>Avant de commencer ...</strong> </p><br><p>  Avant de d√©marrer le code dans l'√©diteur, nous vous recommandons de r√©initialiser tous les param√®tres dans <strong>Runtime -&gt; Tout r√©initialiser</strong> dans le menu sup√©rieur.  Une telle action aidera √† √©viter les probl√®mes de manque de m√©moire, si vous avez travaill√© en parall√®le ou travaillez avec plusieurs √©diteurs. </p><br><h1 id="importirovanie-paketov">  Importer des packages </h1><br><p>  Commen√ßons par importer les packages dont vous avez besoin: </p><br><ul><li>  <code>os</code> - lire des fichiers et des structures de r√©pertoires; </li><li>  <code>numpy</code> - pour certaines op√©rations matricielles en dehors de TensorFlow; </li><li>  <code>matplotlib.pyplot</code> - tra√ßage et affichage d'images √† partir d'un ensemble de donn√©es de test et de validation. </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np</code> </pre> <br><p>  Importer <code>TensorFlow</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><h1 id="zagruzka-dannyh">  Chargement des donn√©es </h1><br><p>  Nous commen√ßons le d√©veloppement de notre classificateur en chargeant un ensemble de donn√©es.  L'ensemble de donn√©es que nous utilisons est une version filtr√©e de l'ensemble de donn√©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dogs vs Cats</a> du service Kaggle (au final, cet ensemble de donn√©es est fourni par Microsoft Research). </p><br><p>  Dans le pass√©, CoLab et moi-m√™me utilisions un ensemble de donn√©es du module de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">donn√©es TensorFlow</a> lui-m√™me, ce qui est extr√™mement pratique pour le travail et les tests.  Dans ce CoLab, cependant, nous utiliserons la classe <code>tf.keras.preprocessing.image.ImageDataGenerator</code> pour lire les donn√©es du disque.  Par cons√©quent, nous devons d'abord t√©l√©charger l'ensemble de donn√©es Dog VS Cats et le d√©compresser. </p><br><pre> <code class="python hljs">_URL = <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'</span></span> zip_dir = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filterted.zip'</span></span>, origin=_URL, extract=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  L'ensemble de donn√©es que nous avons t√©l√©charg√© a la structure suivante: </p><br><pre> <code class="plaintext hljs">cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]</code> </pre> <br><p>  Pour obtenir une liste compl√®te des r√©pertoires, vous pouvez utiliser la commande suivante: </p><br><pre> <code class="python hljs">zip_dir_base = os.path.dirname(zip_dir) !find $zip_dir_base -type d -<span class="hljs-keyword"><span class="hljs-keyword">print</span></span></code> </pre> <br><p>  En cons√©quence, nous obtenons quelque chose de similaire: </p><br><pre> <code class="plaintext hljs">/root/.keras/datasets /root/.keras/datasets/cats_and_dogs_filtered /root/.keras/datasets/cats_and_dogs_filtered/train /root/.keras/datasets/cats_and_dogs_filtered/train/dogs /root/.keras/datasets/cats_and_dogs_filtered/train/cats /root/.keras/datasets/cats_and_dogs_filtered/validation /root/.keras/datasets/cats_and_dogs_filtered/validation/dogs /root/.keras/datasets/cats_and_dogs_filtered/validation/cats</code> </pre> <br><p>  Attribuez maintenant les bons chemins aux r√©pertoires avec les ensembles de donn√©es pour la formation et la validation des variables: </p><br><pre> <code class="python hljs">base_dir = os.path.join(os.path.dirname(zip_dir), <span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filtered'</span></span>) train_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>) validation_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'validation'</span></span>) train_cats_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) train_dogs_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>) validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>)</code> </pre> <br><h4 id="razbiraemsya-s-dannymi-i-ih-strukturoy">  Comprendre les donn√©es et leur structure </h4><br><p>  Voyons combien d'images de chats et de chiens nous avons dans les ensembles de donn√©es de test et de validation (r√©pertoires). </p><br><pre> <code class="python hljs">num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val</code> </pre> <br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_val) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_val) print(<span class="hljs-string"><span class="hljs-string">'--'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_train) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_val)</code> </pre> <br><p>  La sortie du dernier bloc sera la suivante: </p><br><pre> <code class="plaintext hljs">    : 1000     : 1000     : 500     : 500 --      : 2000      : 1000</code> </pre> <br><h1 id="ustanovka-parametrov-modeli">  D√©finition des param√®tres du mod√®le </h1><br><p>  Pour plus de commodit√©, nous placerons l'installation des variables dont nous avons besoin pour le traitement des donn√©es et la formation des mod√®les dans une annonce distincte: </p><br><pre> <code class="python hljs">BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-comment"><span class="hljs-comment">#          IMG_SHAPE = 150 #  150x150      </span></span></code> </pre> <br><h1 id="podgotovka-dannyh">  Pr√©paration des donn√©es </h1><br><p>  Avant que les images puissent √™tre utilis√©es comme entr√©e pour notre r√©seau, elles doivent √™tre converties en tenseurs avec des valeurs √† virgule flottante.  Liste des √©tapes √† suivre pour ce faire: </p><br><ol><li>  Lire des images √† partir du disque </li><li>  D√©coder le contenu de l'image et convertir au format souhait√© en tenant compte du profil RVB </li><li>  Convertir en tenseurs avec des valeurs √† virgule flottante </li><li>  Normaliser les valeurs du tenseur de l'intervalle de 0 √† 255 √† l'intervalle de 0 √† 1, car les r√©seaux de neurones fonctionnent mieux avec de petites valeurs d'entr√©e. </li></ol><br><p>  Heureusement, toutes ces op√©rations peuvent √™tre effectu√©es √† l'aide de la classe <code>tf.keras.preprocessing.image.ImageDataGenerator</code> . </p><br><p>  Nous pouvons faire tout cela en utilisant plusieurs lignes de code: </p><br><pre> <code class="python hljs">train_image_generator = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>) validation_image_generator = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>)</code> </pre> <br><p>  Apr√®s avoir d√©fini des g√©n√©rateurs pour un ensemble de donn√©es de test et de validation, la m√©thode <strong>flow_from_directory chargera</strong> les images du disque, normalisera les donn√©es et redimensionnera les images avec une seule ligne de code: </p><br><pre> <code class="python hljs">train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE,IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>  G√©n√©rateur de donn√©es de validation: </p><br><pre> <code class="python hljs">val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE, directory=validation_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, target_size=(IMG_SHAPE,IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Found 1000 images belonging to 2 classes.</code> </pre> <br><h4 id="vizualiziruem-izobrazheniya-iz-trenirovochnogo-nabora">  Visualisez les images de l'ensemble d'entra√Ænement. </h4><br><p>  Nous pouvons visualiser des images d'un ensemble de donn√©es de formation en utilisant <code>matplotlib</code> : </p><br><pre> <code class="python hljs">sample_training_images, _ = next(train_data_gen)</code> </pre> <br><p>  La fonction <code>next</code> renvoie un bloc d'images de l'ensemble de donn√©es.  Un bloc est un tuple de <em>(plusieurs images, plusieurs √©tiquettes)</em> .  Pour le moment, nous allons supprimer les √©tiquettes, car nous n'en avons pas besoin - nous nous int√©ressons aux images elles-m√™mes. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#        15 def plotImages(images_arr): fig, axes = plt.subplots(1, 5, figsize=(20, 20)) axes = axes.flatten() for img, ax in zip(images_arr, axes): ax.imshow(img) plt.tight_layout() plt.show()</span></span></code> </pre> <br><pre> <code class="python hljs">plotImages(sample_training_images[:<span class="hljs-number"><span class="hljs-number">5</span></span>]) <span class="hljs-comment"><span class="hljs-comment">#   0-4</span></span></code> </pre> <br><p>  Exemple de sortie (2 images au lieu des 5): </p><br><p><img src="https://habrastorage.org/webt/ag/sd/hr/agsdhrbpj3jqtgd-xwbnzoh-7go.png"></p><br><h1 id="sozdanie-modeli">  Cr√©ation de mod√®le </h1><br><h3 id="opisyvaem-model">  Nous d√©crivons le mod√®le </h3><br><p>  Le mod√®le se compose de 4 blocs de convolution, apr√®s chacun desquels il y a un bloc avec une couche de sous-√©chantillon.  Ensuite, nous avons une couche enti√®rement connect√©e avec 512 neurones et une <code>relu</code> activation <code>relu</code> .  Le mod√®le donnera une distribution de probabilit√© pour deux classes - chiens et chats - en utilisant <code>softmax</code> . </p><br><pre> <code class="python hljs">model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(IMG_SHAPE, IMG_SHAPE, <span class="hljs-number"><span class="hljs-number">3</span></span>)), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Flatten(), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ])</code> </pre> <br><h4 id="kompilirovanie-modeli">  Compilation de mod√®les </h4><br><p>  Comme pr√©c√©demment, nous utiliserons l'optimiseur <code>adam</code> .  Nous utilisons <code>sparse_categorical_crossentropy</code> comme fonction de perte.  Nous voulons √©galement surveiller la pr√©cision du mod√®le √† chaque it√©ration de formation, nous passons donc la valeur de <code>accuracy</code> au param√®tre <code>metrics</code> : </p><br><pre> <code class="python hljs">model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><h4 id="predstavlenie-modeli">  Vue mod√®le </h4><br><p>  Jetons un coup d'≈ìil √† la structure de notre mod√®le par niveaux en utilisant la m√©thode de <strong>r√©sum√©</strong> : </p><br><pre> <code class="python hljs">model.summary()</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 148, 148, 32) 896 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 34, 34, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 15, 15, 128) 147584 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 6272) 0 _________________________________________________________________ dense (Dense) (None, 512) 3211776 _________________________________________________________________ dense_1 (Dense) (None, 2) 1026 ================================================================= Total params: 3,453,634 Trainable params: 3,453,634 Non-trainable params: 0</code> </pre> <br><h4 id="trenirovka-modeli">   </h4><br><p>    ! </p><br><p>         ( <code>ImageDataGenerator</code> )    <code>fit_generator</code>     <code>fit</code> : </p><br><pre> <code class="python hljs">EPOCHS = <span class="hljs-number"><span class="hljs-number">100</span></span> history = model.fit_generator( train_data_gen, steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))), epochs=EPOCHS, validation_data=val_data_gen, validation_steps=int(np.ceil(total_val / float(BATCH_SIZE))) )</code> </pre> <br><h4 id="vizualizaciya-rezultatov-trenirovki">    </h4><br><p>       : </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">'./foo.png'</span></span>) plt.show()</code> </pre> <br><p>  Conclusion: </p><br><p><img src="https://habrastorage.org/webt/z5/wn/we/z5wnwe2v8nmxrkwlrpgwgdr38qg.png"></p><br><p>     ,                   70%      (    ). </p><br><p>     .          ,             . </p><br><p> <em> ‚Ä¶   .</em> </p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ... et appel √† l'action standard - inscrivez-vous, mettez un plus et partagez :) </font></font></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">T√©l√©gramme</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456740/">https://habr.com/ru/post/fr456740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456724/index.html">5 fa√ßons extr√™mement simples d'acc√©l√©rer consid√©rablement votre application VueJS</a></li>
<li><a href="../fr456730/index.html">Livre "{You Don't Know JS} Types et constructions grammaticales"</a></li>
<li><a href="../fr456732/index.html">√ätre mentor</a></li>
<li><a href="../fr456736/index.html">Recettes PostgreSQL: cURL: get, post and ... email</a></li>
<li><a href="../fr456738/index.html">R√©seaux de neurones et apprentissage profond, chapitre 1: utiliser les r√©seaux de neurones pour reconna√Ætre les nombres manuscrits</a></li>
<li><a href="../fr456744/index.html">10 probl√®mes que j'ai r√©solus avec des rappels sur mon smartphone</a></li>
<li><a href="../fr456746/index.html">Big data - grande responsabilit√©, gros stress et gros argent</a></li>
<li><a href="../fr456748/index.html">Imprimante thermique 2003 d'un march√© aux puces: que peut-elle faire en 2019?</a></li>
<li><a href="../fr456754/index.html">GitOps: comparaison des m√©thodes Pull et Push</a></li>
<li><a href="../fr456756/index.html">Pourquoi CockroachDB change de licence Open Source</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>