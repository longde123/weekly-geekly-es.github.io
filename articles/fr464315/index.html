<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧐 🍕 🎎 Le film dans lequel il y avait de la terre. Recherche sur Yandex et bref historique de la recherche par sens 👩🏽‍🤝‍👩🏼 🧔 💇</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parfois, les gens se tournent vers Yandex pour trouver un film dont le nom leur est sorti de la tête. Ils décrivent l'intrigue, des scènes mémorables,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le film dans lequel il y avait de la terre. Recherche sur Yandex et bref historique de la recherche par sens</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/464315/">  Parfois, les gens se tournent vers Yandex pour trouver un film dont le nom leur est sorti de la tête.  Ils décrivent l'intrigue, des scènes mémorables, des détails saisissants: par exemple, [quel est le nom du film où un homme choisit une pilule rouge ou bleue].  Nous avons décidé d'étudier les descriptions des films oubliés et de découvrir ce dont les gens se souviennent le plus dans les films. <br><br>  Aujourd'hui, nous partagerons non seulement un lien vers nos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">recherches</a> , mais nous parlerons également brièvement de l'évolution de la recherche sémantique de Yandex.  Vous apprendrez quelles technologies aident la recherche à trouver la réponse même s'il est tout simplement impossible de formuler la demande exacte. <br><br>  Et nous avons également ajouté des curseurs d'énigmes avec des exemples de demandes de personnes réelles - sentez-vous comme un moteur de recherche et essayez de deviner la réponse. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a>  Tous les moteurs de recherche ont commencé par une recherche de mots.  Yandex déjà au départ était capable de prendre en compte la morphologie de la langue russe, mais c'était toujours la même recherche de mots à partir d'une requête sur des pages du réseau.  Nous avons conservé des listes de toutes les pages connues pour chaque mot.  Si la demande contenait une phrase, il suffisait de traverser les listes de mots - voici la réponse.  Cela fonctionnait très bien à l'époque où il y avait peu de sites, et la question du classement n'était pas encore aussi aiguë. <br><br>  Runet s'est développé, les sites sont devenus de plus en plus nombreux.  Deux autres facteurs ont été ajoutés au facteur de croisement de mots.  D'une part, les utilisateurs eux-mêmes nous ont aidés.  Nous avons commencé à réfléchir aux sites et aux requêtes qu'ils choisissaient.  Il n'y a pas de correspondance exacte des mots, mais le site résout-il le problème humain?  Ceci est un signal utile.  En revanche, les liens entre sites qui ont permis d'évaluer la signification des pages sont venus à la rescousse. <br><br>  Trois facteurs sont très peu nombreux.  Surtout quand ils sont souvent essayés par les optimiseurs de moteur de recherche très talentueux.  Mais digérer davantage à la main était difficile.  Et c'est ici qu'a commencé l'ère de l'apprentissage automatique.  En 2009, nous introduisons Matrixnet basé sur le boost de gradient (plus tard, cette technologie a formé la base de la bibliothèque open source plus avancée <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CatBoost</a> ). <br><br>  Depuis lors, il y a eu de plus en plus de facteurs, car nous n'avons plus à rechercher manuellement les relations entre eux.  Une voiture l'a fait pour nous. <br><br>  Pour l'histoire de tous les changements ultérieurs dans la recherche, non seulement le message, mais aussi les livres suffiront, nous allons donc essayer de nous concentrer sur les plus importants. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Le classement n'est pas seulement une comparaison des mots de la requête et de la page depuis longtemps.  Deux exemples. <br><br>  En 2014, nous avons introduit la technologie d'annotation de documents avec des requêtes caractéristiques.  Supposons que dans le passé il y ait eu une demande [une série du Brésil sur le roi de la viande], pour laquelle une bonne réponse est déjà connue.  Ensuite, un autre utilisateur saisit une requête [la série brésilienne dans laquelle il y avait un roi de la viande et un roi du lait], pour laquelle la machine ne connaît pas encore la réponse.  Mais ces requêtes ont de nombreux mots communs.  C'est un signal que la page trouvée sur la première demande peut être pertinente sur la seconde. <br><br>  Un autre exemple.  Prenons des enquêtes [la série brésilienne dans laquelle il y avait un roi de la viande et un roi du lait] et [un héritage fatal en série].  Sur le total, ils n'ont qu'un seul mot - "série", et cela ne suffit pas pour une correspondance explicite des demandes.  Dans ce cas, nous avons commencé à prendre en compte l'historique de la recherche.  Si deux demandes différentes sont demandées sur les mêmes sites lors de l'émission, alors nous pouvons supposer que les demandes sont interchangeables.  Ceci est utile car nous allons maintenant utiliser le texte des deux requêtes pour rechercher des pages plus utiles.  Mais cela ne fonctionne que pour les demandes répétées lorsqu'il existe déjà au moins quelques statistiques.  Que faire des nouvelles demandes? <br><br>  Le manque de statistiques peut être compensé par l'analyse de contenu.  Et dans l'analyse de données homogènes (texte, voix, images), les réseaux de neurones se montrent le mieux.  En 2016, nous avons d'abord parlé à la communauté Habr de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la technologie Palekh</a> , qui est devenue le point de départ d'une utilisation plus large des réseaux de neurones dans la recherche. <br><br>  Nous avons commencé à former le réseau neuronal pour comparer la proximité sémantique (sémantique) du texte de la requête et du titre de la page.  Deux textes sont représentés sous forme de vecteurs dans un espace multidimensionnel de sorte que le cosinus de l'angle entre eux prédit bien la probabilité de choisir une page par une personne, et donc la proximité sémantique.  Cela vous permet d'évaluer la proximité des significations même des textes dans lesquels il n'y a pas d'intersection de mots. <br><br><div class="spoiler">  <b class="spoiler_title">Un exemple d'architecture de couches pour les curieux</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/404/470/082/4044700822614d34976b92f9caa6a38c.png" alt="image"><br></div></div><br>  De la même manière, nous avons commencé à comparer des textes de requête afin d'identifier des liens entre eux.  Un exemple réel sous le capot d'un moteur de recherche: pour une requête [la série américaine sur la façon dont la méthamphétamine est bouillie], c'est le réseau de neurones qui trouve les expressions [signifiant mauvais] et [cassant] comme ayant un sens similaire. <br><br>  Les demandes et les en-têtes sont déjà bons, mais nous n'avons pas renoncé à espérer utiliser les réseaux de neurones dans le texte intégral des pages.  De plus, lorsque nous recevons une demande de l'utilisateur, nous commençons à sélectionner les meilleures pages parmi des millions de pages d'index, mais à Palekh, nous avons utilisé des modèles de réseau neuronal uniquement aux toutes dernières étapes du classement (L3) - pour environ 150 des meilleurs documents.  Cela peut entraîner la perte de bonnes réponses. <br><br><img src="https://habrastorage.org/web/0fa/5fb/280/0fa5fb280cf74efeab59bd9657aaeb00.png" alt="image"><br><br>  La raison en est prévisible - ressources limitées et exigences élevées en termes de vitesse de réponse.  Les limites strictes des calculs sont liées à un simple fait: vous ne pouvez pas forcer l'utilisateur à attendre.  Mais ensuite, nous avons trouvé quelque chose. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  En 2017, nous avons présenté la mise à jour de la recherche Korolev, qui comprenait non seulement l'utilisation élargie des réseaux de neurones, mais également un travail sérieux sur l'architecture pour économiser les ressources.  Plus en détail, avec des diagrammes de couches et d'autres détails que nous avons déjà racontés dans un autre post sur Habré, mais maintenant nous allons rappeler l'essentiel. <br><br>  Au lieu de prendre le titre du document et de calculer son vecteur sémantique lors de l'exécution de la requête, vous pouvez pré-calculer ce vecteur et l'enregistrer dans la base de données de recherche.  En d'autres termes, nous pouvons faire une partie substantielle du travail à l'avance.  Bien sûr, en même temps, nous avions besoin de plus d'espace pour stocker des vecteurs, mais cela nous a fait gagner du temps processeur.  Mais ce n'est pas tout. <br><br><div class="spoiler">  <b class="spoiler_title">Un autre schéma pour les curieux</b> <div class="spoiler_text"><img src="https://habrastorage.org/web/9ff/3a8/06a/9ff3a806a97647299ae6736cf02a7d06.png" alt="image"><br></div></div><br>  Nous avons construit un index supplémentaire.  Il est basé sur l'hypothèse: si vous prenez une liste suffisamment grande des documents les plus pertinents pour chaque mot ou expression pour une requête de plusieurs mots, alors parmi eux il y aura des documents qui seront pertinents en même temps pour tous les mots.  En pratique, cela signifie cela.  Pour tous les mots et paires de mots populaires, un index supplémentaire est formé avec une liste de pages et leur pertinence préliminaire à la requête.  Autrement dit, nous transférons une partie du travail de l'étape L0 à l'étape d'indexation et, à nouveau, nous enregistrons. <br><br>  En conséquence, un changement d'architecture et une redistribution des charges nous ont permis d'utiliser des réseaux de neurones non seulement au stade L3, mais aussi pour L2 et L1.  De plus, la possibilité de former un vecteur à l'avance et avec des exigences de performance moins strictes nous a permis d'utiliser non seulement le titre de la page, mais aussi son texte. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Plus c'est plus.  Au fil du temps, nous avons commencé à utiliser les réseaux de neurones au tout début du classement.  Nous enseignons aux réseaux de neurones à identifier les modèles implicites dans l'ordre des mots et leurs positions relatives.  Et même pour révéler la similitude sémantique des textes dans différentes langues.  Chacun de ces domaines est attiré par un article distinct, et nous essaierons de revenir avec eux dans un proche avenir. <br><br><hr><br>  Aujourd'hui, nous avons rappelé une fois de plus comment les moteurs de recherche apprennent à trouver la réponse dans les conditions d'une vague interrogation et d'un manque d'informations.  La recherche de films par leur description n'est pas seulement un cas particulier de telles demandes, mais aussi un grand sujet de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">recherche</a> .  De là, vous apprendrez: ce qui est le plus retenu par les gens du cinéma, avec quels différents genres et cinématographes de différents pays sont associés, quels mouvements de l'intrigue font une impression particulière. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr464315/">https://habr.com/ru/post/fr464315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr464299/index.html">0, 0, 1, 0, 2, 0, 2, 2, 1, 6, 0, 5, 0, 2, 6, 5, 4, 0, 5, 3, 0, 3, 2, 9, 0, 4, 9, 3, 6, 14, 0, 6, 3, 5, 15, 0, 5, 3, 5 ...</a></li>
<li><a href="../fr464303/index.html">Données de séries chronologiques dans un SGBD relationnel. Extensions TimescaleDB et PipelineDB pour PostgreSQL</a></li>
<li><a href="../fr464305/index.html">Petit, oui. Déballage du pétard microvirtuel</a></li>
<li><a href="../fr464307/index.html">Test d'intégration de microservices sur Scala</a></li>
<li><a href="../fr464309/index.html">Bouton d'appel bricolage. Raspberry Pi, MajorDoMo, Freeswitch et Linphonec</a></li>
<li><a href="../fr464317/index.html">Projet Konbanwa</a></li>
<li><a href="../fr464325/index.html">Comment Scrumban unit le meilleur des méthodologies Kanban et Scrum</a></li>
<li><a href="../fr464327/index.html">Comparaison de l'utilisation de la mémoire des différentes interfaces graphiques de la boîte à outils</a></li>
<li><a href="../fr464331/index.html">Avantages inutiles: synthèse de produits chimiques absorbant les UV à partir de noix de cajou</a></li>
<li><a href="../fr464333/index.html">Suivi du cycle de vie des utilisateurs sans pince ni ruban électrique</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>