<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçü§ù‚Äçüë®üèº üï∫üèø üë®‚Äçüé® Exp√©rience avec Starwind VSAN et EMC ScaleIO (VxFlexOS) + aide-m√©moire pour le stockage mini-entreprise (1 partie) üñ±Ô∏è üò≠ üëçüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parfois, il devient n√©cessaire d'organiser un stockage √† tol√©rance de pannes de petits volumes de stockage jusqu'√† 20 To, mais avec des fonctionnalit√©...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Exp√©rience avec Starwind VSAN et EMC ScaleIO (VxFlexOS) + aide-m√©moire pour le stockage mini-entreprise (1 partie)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454114/"> Parfois, il devient n√©cessaire d'organiser un stockage √† tol√©rance de pannes de petits volumes de stockage jusqu'√† 20 To, mais avec des fonctionnalit√©s d'entreprise - All-Flash, cache SSD, MPIO, HA (Activ-Activ) et tout cela √† un prix abordable.  Les solutions mat√©rielles pr√™tes √† l'emploi avec ces fonctions partent de centaines de t√©raoctets et de prix de 8 signes ou plus en roubles.  Avoir un petit budget de 6-7 caract√®res dans la rivi√®re.  et la n√©cessit√© d'un stockage petit et rapide (mais fiable), depuis 2009, deux versions de syst√®mes de stockage ont √©t√© test√©es et mises en service commercial (La chose courante avec ces syst√®mes est qu'ils sont des syst√®mes tr√®s fiables sans point de d√©faillance unique + vous pouvez les toucher avant l'achat ou ¬´s'en passer¬ª (GRATUIT)). <br><br>  Les personnes int√©ress√©es par cette exp√©rience seront d√©crites ci-dessous: <br><br><ol><li>  Exp√©rience du logiciel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">StarWind Virtual SAN (VSAN)</a> . </li><li>  Comment faire du stockage pour petites entreprises. </li><li>  Historique d'overclocking IOPS (pratique). </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Aide-m√©moire</a> pour le d√©ploiement et l'exploitation des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">syst√®mes</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">stockage EMC ScaleIO (VxFlexOS)</a> (en l'absence de support technique par les sp√©cialistes de ¬´NOT Linux-gourou¬ª) 1 partie. </li></ol><a name="habracut"></a><br><h2>  1. Exp√©rience d'exploitation du logiciel StarWind Virtual SAN (VSAN) </h2><br>  <b>StarWind Virtual SAN (VSAN)</b> - dans la solution Activ-Activ (r√©plication synchrone sur 3 serveurs), en fonctionnement de 2009 √† 2016 dans diff√©rentes √©ditions (Starwind ISCSI SAN HA-3) bas√©e sur des serveurs avec des matrices RAID mat√©rielles. <br><br>  <i>Avantages</i> : <br><br><ul><li>  Facile et rapide, m√™me pas install√© par un professionnel; </li><li>  MPIO sur Ethernet iSCSI; </li><li>  HA (Activ-Activ); </li><li>  Sur les nouveaux serveurs (sous garantie) (avec de nouveaux disques), vous pouvez oublier de conserver le stockage pendant plusieurs ann√©es (les utilisateurs ne remarqueront m√™me pas la panne de deux serveurs sur trois); </li><li>  Volumes de cache RAM et SSD; </li><li>  Rapide Synchronisation rapide pour les pannes de r√©seau mineures. </li></ul><br>  <i>Inconv√©nients</i> : <br><br><ul><li>  Auparavant, il n'y avait qu'une version pour la plate-forme Windows; </li><li>  Avec un fonctionnement √† long terme (plus de 3 ans) - il est difficile de trouver un disque pour remplacer un disque d√©fectueux (hors production) pour r√©parer une matrice RAID (avec des disques h√©t√©rog√®nes, des pannes de matrice peuvent se produire); </li><li>  Une augmentation du nombre d'interfaces r√©seau et des emplacements PCI qu'elles occupent (en plus pour la synchronisation, les cartes r√©seau, les commutateurs); </li><li>  Lors de l'utilisation de LSFS - ¬´journing file system¬ª, arr√™t prolong√© du syst√®me, qui peut √™tre pr√©judiciable lorsque l'onduleur est d√©clench√© lorsque l'alimentation est coup√©e; </li><li>  Une tr√®s longue p√©riode de synchronisation compl√®te avec un grand volume. </li></ul><br>  <i>Peut-√™tre des probl√®mes d√©j√† r√©solus</i> (survenus pr√©c√©demment pendant le fonctionnement dans notre centre de donn√©es): <br><br><ul><li>  Lorsque la matrice RAID s'effondre, le serveur reste visible via le canal de synchronisation et de donn√©es, mais le disque du serveur Windows est hors r√©seau, le journal Starwind est gonfl√© et la m√©moire du serveur est consomm√©e, √† la suite du gel du serveur.  Traitement possible: affectation d'un fichier de contr√¥le et suppression des messages non critiques des param√®tres du journal. </li><li>  Si le commutateur ou les interfaces r√©seau √©chouent, un choix ambigu du serveur h√¥te (parfois c'est arriv√©, le syst√®me ne peut pas comprendre avec qui se synchroniser). </li></ul><br>  <i>Actualit√©s utiles</i> (non encore test√©es): <br>  StarWind Virtual SAN pour vSphere (solution hyperconverg√©e), vous permet d'int√©grer la virtualisation Vmware dans un cluster sans liaison avec les serveurs Windows (bas√©s sur des machines virtuelles Linux). <br><br>  <i><b>R√©sum√©</b></i> : Une solution tol√©rante aux pannes s'il existe un programme de remplacement de serveur mat√©riel normal √† la fin de la garantie et un support technique StarWindSoftWare est disponible. <br><br><h2>  <b>2. Comment faire du stockage pour petites entreprises</b> </h2><br>  <b>√ânonc√© du probl√®me:</b> <br><br>  Cr√©ez un r√©seau de stockage de donn√©es √† faible volume de s√©curit√© avec un total de 4 To-20 To, avec un fonctionnement garanti √† moyen terme sans co√ªts financiers suppl√©mentaires importants. <br><br><ul><li>  Le syst√®me doit √™tre tol√©rant aux pannes (transf√©rer calmement la d√©faillance d'au moins un commutateur, un serveur, des disques et des cartes r√©seau dans le serveur). </li><li>  Utiliser au maximum toutes les ressources du parc mat√©riel disponible de serveurs (serveurs et commutateurs de 3 √† 10 ans). </li><li>  Assurer le fonctionnement des volumes de diff√©rents niveaux: All-Flash et cache HDD + SSD. </li></ul><br>  <b>Donn√©es sources:</b> <br><br><ul><li>  budget limit√©; </li><li>  √©quipement de production il y a 3 √† 10 ans; </li><li>  Sp√©cialistes - Pas Linux-Guru. </li></ul><br>  <b>Calcul des caract√©ristiques</b> <br><br>  Pour √©viter les goulots d'√©tranglement des performances lors de l'utilisation de disques SSD, qui seront coup√©s par rapport √† quelque chose de la cha√Æne mat√©rielle: cartes r√©seau, contr√¥leur RAID (HBA), module d'extension (panier), disques. <br><br>  Il est n√©cessaire au moment de la cr√©ation de fournir, en fonction de leurs caract√©ristiques requises, une certaine configuration d'√©quipement. <br><br>  Vous pouvez bien s√ªr ex√©cuter une configuration avec SSD de cache SAS HDD sur les r√©seaux 1 Gbit / s et les contr√¥leurs 3G, mais le r√©sultat sera 3-7 fois pire que sur les r√©seaux RAID 6 Gbit et 10 Gbit / s (v√©rifi√© par des tests). <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les instructions de r√©glage VxFlexOS</a> d√©crivent des instructions simples pour calculer la bande passante n√©cessaire, bas√©es sur les classements SSD -450 Mo / C et HDD -100 Mo / C, pour l'enregistrement s√©quentiel (par exemple, lorsque le serveur est r√©√©quilibr√© et reconstruit). <br><br><img src="https://habrastorage.org/webt/ia/-h/kl/ia-hklgcn0f0i_t6126vqpondoi.jpeg"><br>  Par exemple: <br><br><ul><li>  (Cache SSD + 3 HDD), nous obtenons ((450 * 1) + (3 * 100)) * 8/1000 = 6 Go </li><li>  (TOUS SSD FLASH) + (Cache SSD + 3 HDD) ((450 * 2) + (3 * 100)) * 8/1000 = 9,6 Go </li></ul><br>  Pour d√©terminer la bande passante r√©seau par IOPS (charge standard sur les serveurs de base de donn√©es et les serveurs virtuels charg√©s), il existe un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tableau indicatif de StariWindSoftware</a> <br><br><img src="https://habrastorage.org/webt/uc/xm/qf/ucxmqfcjkdfvg2qyfzyuofsso6e.jpeg"><br>  <b>Configuration finale</b> : <br><br><ul><li>  Logiciel de stockage, qui peut ne pas combiner des disques en matrices RAID, mais les transf√©rer vers le stockage sous forme de disques s√©par√©s (afin qu'il n'y ait aucun probl√®me √† remplacer les disques apr√®s un certain laps de temps o√π ils √©chouent, mais simplement les s√©lectionner par capacit√©); </li><li>  Serveurs de g√©n√©ration de processeurs e55xx-x56xx et sup√©rieurs, bus pci-express v 2.0 et sup√©rieurs, contr√¥leurs Raid (HBA) 6G-12G avec m√©moire, paniers d'extension pour 6-16 disques; </li><li>  Commutateurs SMB 10G couche 2 (JUMBO FRAME, LACP). </li></ul><br>  <b>M√©thode de solution</b> <br><br>  Aucune option budg√©taire pour un ¬´stockage mat√©riel de petite entreprise¬ª n'a √©t√© trouv√©e pour le petit volume avec les exigences ci-dessus. <br><br>  Nous nous sommes arr√™t√©s sur des solutions logicielles qui vous permettent de profiter du stockage d'entreprise, avec la possibilit√© d'utiliser des serveurs existants, qui dans ce cas ont le droit de mourir de vieillesse sans compromettre le stockage. <br><br><ul><li>  Ceph - pas assez de sp√©cialistes Linux; </li><li>  EMC ScaleIO - pour quelques ann√©es de support technique - vous pouvez vous en tirer avec le personnel existant. </li><li>  (il s'est av√©r√© que les connaissances sous Linux peuvent √™tre minimes, plus sur cela plus tard dans la feuille de triche). </li></ul><br><h2>  3. Historique de l'overclocking IOPS (pratique budg√©taire) </h2><br>  Pour acc√©l√©rer les op√©rations de lecture et d'√©criture dans les syst√®mes de stockage, les p√©riph√©riques SSD suivants ont √©t√© utilis√©s: <br><br>  3.1.  Contr√¥leurs avec fonctionnalit√©s de mise en cache SSD. <br><br>  En 2010, des contr√¥leurs RAID avec des fonctions de mise en cache SSD Adaptec 5445 avec un disque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MaxIQ</a> sont apparus (pour un r√©sultat tangible, vous devez avoir au moins 10% du disque MaxIQ du volume du volume mis en cache), le r√©sultat est mais insignifiant * test√© sur soi-m√™me; <br>  Plus tard, il y avait des contr√¥leurs qui peuvent utiliser un disque SSD arbitraire pour la mise en cache, √† la fois la s√©rie Adaptec Q et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LSI CacheCade</a> (mais les licences y sont distinctes); <br><br>  3.2.  Mise en cache logicielle √† l'aide de disques, tels que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intel DC S3700</a> , qui est vu par le contr√¥leur et l'expandeur des serveurs de marque HP, IBM, FUJI (la plupart des serveurs les reconnaissent avec succ√®s, co√ªteux pour All-Flash, mais pour 10% sur le cache SSD, il est tol√©rable de ne pas les publier sous partenaires d'IBM, HP, FUJI et juste Intel).  * Mais maintenant, il existe des options compatibles moins ch√®res (voir paragraphe 3.5.); <br><br>  3.3.  La mise en cache logicielle √† l'aide de l'adaptateur PCIe-M.2, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Synology M.2 M2D18 SSD</a> , est v√©rifi√©e, elle fonctionne sur des serveurs ordinaires (pas seulement dans Synology), elle est utile lorsque le contr√¥leur RAID et le panier refusent de voir les SSD que le fabricant n'a pas indiqu√©s dans les compatibles (n HP D2700)?  *; <br><br>  3.4.  Disques hybrides <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Seagate EXOS</a>  600 Go Seagate Exos 10E2400 (ST600MM0099) {SAS 12 Gb / s, 10000 tr / min, 256 Mo, 2,5 "}, * v√©rifi√© reconnu par les serveurs HP, IBM, FUJI (alternative aux versions 3.1.-3.3.); <br><br>  3.5.  Disques SSD avec une grande ressource et un prix comparable avec SAS de classe entreprise, <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Crucial Micron 5200 MAX</a> MTFDDAK480TDN-1AT1ZABYY, * v√©rifi√© reconnu par les serveurs HP, IBM, FUJI <br>  (une alternative au remplacement des disques durs par ceux compatibles avec la clause 3.4 et compatibles avec les anciens serveurs SAS: disque dur SAS2.5 "600 Go AL14SEB060N TOSHIBA *, <br>  C10K1800 0B31229 HGST, ST600MM0099 SEAGATE).  Permet √† un budget de passer des volumes HDD + SSD aux volumes 100% Flash.; <br><br><h2>  4. Aide-m√©moire pour le d√©ploiement et le fonctionnement du stockage EMC ScaleIO (VxFlexOS) 1 partie </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stockage EMC ScaleIO (VxFlexOS)</a> <br><br>  Apr√®s avoir test√© la solution avant l'achat, je suis arriv√© √† la conclusion que pour le fonctionnement normal du syst√®me, plus de 3 n≈ìuds sont n√©cessaires (le basculement est instable √† 3), par exemple, prenez une configuration de 8 serveurs (il survivra √† l'√©chec cons√©cutif de 4 serveurs sans perdre de volumes). <br><br>  <i>Partie mat√©riel</i> : <br><br>  FUJI CX2550M1 (E5-2xxx) - 3 pi√®ces.  (Client SDC et serveur SDS du cluster principal de virtualisation du serveur VmWare VSphere + ScaleIO); <br>  Serveurs de g√©n√©ration +5 HP G6 (G7) ou IBM M3 (e55xx-x56xx) - serveurs ScaleIO SDS; <br>  + 2 commutateurs NetGear XS712T-100NES <br><br>  En ex√©cutant le stockage en mode RFCache, j'ai pu overclocker √† 44KIops en utilisant Iometer <br><br><img src="https://habrastorage.org/webt/ro/vg/19/rovg1982umabe29n5vqa6_9erpi.jpeg"><br><br>  Configuration de stockage: <br><br>  Capacit√© brute de 12 To (licence minimale au moment o√π elle √©tait encore vendue en tant que logiciel) <br><br><img src="https://habrastorage.org/webt/vw/os/4e/vwos4eljcmjyrnk8drmccc9vsoa.jpeg"><br><br>  8 serveurs SDS 28 disques <br><br><img src="https://habrastorage.org/webt/gd/-y/5j/gd-y5j3wve2zmum8vexmkbwhmu8.jpeg"><br><br>  Lire le cache RAM 14 Go <br><br><img src="https://habrastorage.org/webt/uo/-j/ck/uo-jck7jbbvfdbjvap9emzkzjvu.jpeg"><br><br>  Lire Flash cashe 1,27 To (RFCashe) <br><br><img src="https://habrastorage.org/webt/2x/y2/km/2xy2kmosvvldbniqfv6sr_6hykw.jpeg"><br><br>  Dans la version interm√©diaire, o√π seuls 3 serveurs 2x10Gb ont des cartes r√©seau, dans les 2 x1Gb restants. <br><br><img src="https://habrastorage.org/webt/vh/k8/xc/vhk8xcff6fqdhzp2347gmgtksuu.jpeg"><br>  On voit clairement que m√™me avec une mise en cache SSD √† 1 Go au lieu de 10 Go, il y a une perte de bande passante SDS trois fois ou plus, avec des supports identiques. <br><br>  Sans mise en cache, si vous consid√©rez selon ces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"standards"</a> alors avec 28 disques durs, <br>  on obtient 28X140 = 3920 IOPS, soit  pour obtenir 44 000 IOPS, il vous faudrait 11 fois plus de disques.  Il est √©conomiquement plus rentable pour les petits volumes, non pour augmenter le nombre de disques mais pour le cache SSD. <br><br>  A la question de savoir pourquoi de telles vitesses avec un petit volume, je r√©pondrai tout de suite! <br><br>  Il existe de telles petites organisations (comme la n√¥tre) dans lesquelles il existe un grand nombre de documents √©lectroniques qui sont trait√©s dans le logiciel pendant une longue p√©riode (chaque registre contr√¥le d'envoyer le logiciel jusqu'√† 1 heure, m√™me dans ce stockage overclock√©).  Toutes les autres options ont d√©j√† √©t√© appliqu√©es pr√©c√©demment (augmentation sur RM-RAM, CPU i5, SSD, 1Gb-NET).  M√™me l'utilisation de bundles SSD + SAS uniquement sur le stockage (sans ALL-Flash jusqu'√† pr√©sent) a permis d'utiliser la plupart des ressources des serveurs de virtualisation, le transfert de machines virtuelles charg√©es vers ScaleIO - a doubl√© la charge sur les processeurs FUJI CX400M1 (stockage pr√©c√©demment retenu). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr454114/">https://habr.com/ru/post/fr454114/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr454100/index.html">Probl√®mes de code courants dans les microservices</a></li>
<li><a href="../fr454102/index.html">Utiliser un oracle al√©atoire sur l'exemple d'une loterie</a></li>
<li><a href="../fr454104/index.html">Services cloud pour jouer sur des PC faibles, pertinents en 2019</a></li>
<li><a href="../fr454110/index.html">D√©veloppement d'une boutique en ligne pour pr√©server la nature du Kamtchatka</a></li>
<li><a href="../fr454112/index.html">Duke Nukem Level Design History (avec Levelord Sketches)</a></li>
<li><a href="../fr454124/index.html">Le livre "Apprendre √† coder en JavaScript"</a></li>
<li><a href="../fr454126/index.html">Des accidents quotidiens √† la stabilit√©: Informatica avec 10 yeux d'administrateur</a></li>
<li><a href="../fr454128/index.html">Comment faire deux applications √† partir d'une seule. Exp√©rience Tinkoff Junior</a></li>
<li><a href="../fr454130/index.html">C-V2X avec prise en charge des r√©seaux 5G NR: un nouveau paradigme pour l'√©change de donn√©es entre v√©hicules</a></li>
<li><a href="../fr454132/index.html">Surveillance vid√©o sur Orange Pi Zero - pas cher et pas du tout en col√®re</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>