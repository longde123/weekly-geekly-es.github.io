<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçü§ù‚Äçüë®üèª üíÆ üöã Mehrsprachige Sprachsynthese mit Klonen üóØÔ∏è üëÉüèø üë±üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Obwohl neuronale Netze vor nicht allzu langer Zeit ( zum Beispiel ) f√ºr die Sprachsynthese verwendet wurden, haben sie es bereits geschafft, die klass...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mehrsprachige Sprachsynthese mit Klonen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465941/"><p>  Obwohl neuronale Netze vor nicht allzu langer Zeit ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zum Beispiel</a> ) f√ºr die Sprachsynthese verwendet wurden, haben sie es bereits geschafft, die klassischen Ans√§tze zu √ºberholen, und jedes Jahr treten immer neuere Aufgaben auf. </p><br><p>  Zum Beispiel gab es vor ein paar Monaten eine Implementierung der Sprachsynthese mit Voice Cloning <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Real-Time-Voice-Cloning</a> .  Versuchen wir herauszufinden, woraus es besteht, und realisieren wir unser mehrsprachiges (russisch-englisches) Phonemmodell. </p><br><h2 id="stroenie">  Geb√§ude </h2><br><p><img src="https://habrastorage.org/webt/4b/k7/e_/4bk7e_qeb-qw-clbaxsqs14f7cg.png"></p><br><p>  Unser Modell wird aus vier neuronalen Netzen bestehen.  Der erste konvertiert den Text in Phoneme (g2p), der zweite konvertiert die Sprache, die wir klonen m√∂chten, in einen Vektor von Zeichen (Zahlen).  Das dritte wird Mel-Spektrogramme basierend auf den Ausgaben der ersten beiden synthetisieren.  Und schlie√ülich wird der vierte Ton von Spektrogrammen empfangen. </p><a name="habracut"></a><br><h2 id="nabory-dannyh">  Datens√§tze </h2><br><p>  Dieses Modell braucht viel Sprache.  Nachfolgend finden Sie die Grundlagen, die Ihnen dabei helfen werden. </p><br><div class="scrollable-table"><table><thead><tr><th>  Vorname </th><th>  Sprache </th><th>  Link </th><th>  Kommentare </th><th>  Mein Link </th><th>  Kommentare </th></tr></thead><tbody><tr><td>  W√∂rterbuch der Phoneme </td><td>  En ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">En</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ru</a> </td><td></td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Kombiniertes russisches und englisches Phonemw√∂rterbuch </td></tr><tr><td>  Libripepeech </td><td>  En </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  300 Stimmen, 360 Stunden reine Rede </td><td></td><td></td></tr><tr><td>  VoxCeleb </td><td>  En </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  7000 Stimmen, viele Stunden schlechten Klangs </td><td></td><td></td></tr><tr><td>  M-AILABS </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  3 Stimmen, 46 Stunden reine Rede </td><td></td><td></td></tr><tr><td>  open_tts, open_stt </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">open_tts</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">open_stt</a> </td><td>  viele Stimmen, viele Stunden schlechten Klangs </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Ich habe 4 Stunden Rede von einem Sprecher gereinigt.  Korrigierte Anmerkung, unterteilt in Segmente bis zu 7 Sekunden </td></tr><tr><td>  Voxforge + H√∂rbuch </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  viele Stimmen, 25h unterschiedlicher Qualit√§t </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Ausgew√§hlte gute Dateien.  Brach in Segmente.  H√∂rb√ºcher aus dem Internet hinzugef√ºgt.  Es stellte sich heraus, dass in wenigen Minuten jeweils 200 Lautsprecher vorhanden waren </td></tr><tr><td>  RUSLAN </td><td>  Ru </td><td>  <a href="">Link</a> </td><td>  Eine Stimme, 40 Stunden reine Sprache </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Bei 16 kHz neu codiert </td></tr><tr><td>  Mozilla </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  50 Stimmen, 30 Stunden normale Qualit√§t </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Bei 16 kHz neu codiert, verschiedene Benutzer in Ordnern verteilt </td></tr><tr><td>  Russische Single </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Eine Stimme, 9 Stunden reine Sprache </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td></td></tr></tbody></table></div><br><h2 id="obrabotka-teksta">  Textverarbeitung </h2><br><p>  Die erste Aufgabe ist die Textverarbeitung.  Stellen Sie sich den Text in der Form vor, in der er weiter gesprochen wird.  Wir werden Zahlen in Worten darstellen und Abk√ºrzungen √∂ffnen.  Lesen Sie mehr im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel √ºber Synthese</a> .  Dies ist eine schwierige Aufgabe. Nehmen wir also an, wir haben bereits Text verarbeitet (er wurde in den obigen Datenbanken verarbeitet). </p><br><p>  Die n√§chste Frage ist, ob Graphem- oder Phonemaufzeichnung verwendet werden soll.  F√ºr eine monophone und einsprachige Stimme eignet sich auch ein Buchstabenmodell.  Wenn Sie mit einem mehrsprachigen mehrsprachigen Modell arbeiten m√∂chten, empfehle ich Ihnen, die Transkription zu verwenden (auch Google). </p><br><h3 id="g2p">  G2p </h3><br><p>  F√ºr die russische Sprache gibt es eine Implementierung namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">russian_g2p</a> .  Es basiert auf den Regeln der russischen Sprache und kommt mit der Aufgabe gut zurecht, hat aber Nachteile.  Nicht alle W√∂rter betonen und sind auch nicht f√ºr ein mehrsprachiges Modell geeignet.  Nehmen Sie daher das f√ºr sie erstellte W√∂rterbuch, f√ºgen Sie das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">W√∂rterbuch f√ºr die englische Sprache hinzu</a> und speisen Sie das neuronale Netzwerk (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> ). </p><br><p>  Bevor Sie das Netzwerk trainieren, sollten Sie sich √ºberlegen, welche Kl√§nge aus verschiedenen Sprachen √§hnlich klingen, und Sie k√∂nnen ein Zeichen f√ºr sie ausw√§hlen, f√ºr das dies unm√∂glich ist.  Je mehr Ger√§usche es gibt, desto schwieriger ist es, das Modell zu lernen, und wenn es zu wenige davon gibt, hat das Modell einen Akzent.  Denken Sie daran, einzelne Zeichen mit betonten Vokalen hervorzuheben.  F√ºr die englische Sprache spielt sekund√§rer Stress eine kleine Rolle, und ich w√ºrde es nicht unterscheiden. </p><br><h2 id="kodirovanie-spikerov">  Lautsprecherkodierung </h2><br><p>  Das Netzwerk √§hnelt der Aufgabe, einen Benutzer per Sprache zu identifizieren.  Am Ausgang erhalten verschiedene Benutzer unterschiedliche Vektoren mit Zahlen.  Ich schlage vor, die Implementierung von CorentinJ selbst zu verwenden, die auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> basiert.  Das Modell ist ein dreischichtiges LSTM mit 768 Knoten, gefolgt von einer vollst√§ndig verbundenen Schicht von 256 Neuronen, was einen Vektor von 256 Zahlen ergibt. </p><br><p>  Die Erfahrung hat gezeigt, dass ein in Englisch geschultes Netzwerk gut Russisch spricht.  Dies vereinfacht das Leben erheblich, da f√ºr das Training viele Daten erforderlich sind.  Ich empfehle, ein bereits geschultes Modell zu nehmen und von VoxCeleb und LibriSpeech auf Englisch umzuschulen sowie alle russischen Sprachen, die Sie finden.  Der Encoder ben√∂tigt keine Textanmerkung von Sprachfragmenten. </p><br><h3 id="trenirovka">  Schulung </h3><br><ol><li> F√ºhren Sie <code>python encoder_preprocess.py &lt;datasets_root&gt;</code> , um die Daten zu verarbeiten </li><li>  F√ºhren Sie "visdom" in einem separaten Terminal aus. </li><li>  F√ºhren Sie <code>python encoder_train.py my_run &lt;datasets_root&gt;</code> , um den Encoder zu trainieren </li></ol><br><h2 id="sintez">  Synthese </h2><br><p>  Fahren wir mit der Synthese fort.  Die Modelle, die ich kenne, erhalten keinen Ton direkt aus dem Text, da dies schwierig ist (zu viele Daten).  Erstens erzeugt der Text Ton in spektraler Form, und erst dann wird das vierte Netzwerk in eine vertraute Stimme √ºbersetzt.  Daher verstehen wir zun√§chst, wie die Spektralform mit der Stimme verbunden ist.  Es ist einfacher, das umgekehrte Problem herauszufinden, wie man ein Spektrogramm aus Ton erh√§lt. </p><br><p>  Der Ton ist in Schritten von 10 ms in Segmente von 25 ms unterteilt (die Standardeinstellung bei den meisten Modellen).  Dann wird unter Verwendung der Fourier-Transformation f√ºr jedes St√ºck das Spektrum berechnet (harmonische Schwingungen, deren Summe das urspr√ºngliche Signal ergibt) und in Form eines Diagramms dargestellt, wobei der vertikale Streifen das Spektrum eines Segments (in der Frequenz) und in der Horizontalen eine Folge von Segmenten (in der Zeit) ist.  Dieser Graph wird als Spektrogramm bezeichnet.  Wenn die Frequenz nichtlinear codiert wird (die unteren Frequenzen sind besser als die oberen), √§ndert sich die vertikale Skala (erforderlich, um die Daten zu reduzieren). Dieser Graph wird dann als Mel-Spektrogramm bezeichnet.  So funktioniert das menschliche Geh√∂r, dass wir bei den niedrigeren Frequenzen eine leichte Abweichung besser h√∂ren als bei den h√∂heren, daher wird die Klangqualit√§t nicht leiden </p><br><p><img src="https://habrastorage.org/webt/dx/mv/fs/dxmvfst1objf8dmdglu7rlbhaiq.jpeg"></p><br><p>  Es gibt mehrere gute Implementierungen der Spektrogrammsynthese wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tacotron 2</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deepvoice 3</a> .  Jedes dieser Modelle verf√ºgt √ºber eigene Implementierungen, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">4</a> .  Wir werden (wie CorentinJ) das Tacotron-Modell von Rayhane-mamah verwenden. </p><br><p><img src="https://habrastorage.org/webt/u9/jm/h0/u9jmh0ldgpelp8qoouinbx9ptfi.png"></p><br><p>  Tacotron basiert auf dem seq2seq-Netzwerk mit einem Aufmerksamkeitsmechanismus.  Lesen Sie die Details im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> . </p><br><h3 id="trenirovka-1">  Schulung </h3><br><p>  Vergessen Sie nicht, utils / symbols.py zu bearbeiten, wenn Sie nicht nur die englische Sprache hparams.p, sondern auch preprocess.py synthetisieren. </p><br><p>  Die Synthese erfordert viel sauberen, gut markierten Klang von verschiedenen Lautsprechern.  Hier hilft eine Fremdsprache nicht weiter. </p><br><ol><li>  F√ºhren Sie <code>python synthesizer_preprocess_audio.py &lt;datasets_root&gt;</code> , um verarbeiteten Sound und Spektrogramme zu erstellen </li><li>  F√ºhren Sie <code>python synthesizer_preprocess_embeds.py &lt;datasets_root&gt;</code> , um den Sound zu codieren (erhalten Sie die Zeichen einer Stimme). </li><li>  F√ºhren Sie <code>python synthesizer_train.py my_run &lt;datasets_root&gt;</code> , um den Synthesizer zu trainieren </li></ol><br><h2 id="vokoder">  Vocoder </h2><br><p>  Jetzt m√ºssen nur noch die Spektrogramme in Ton umgewandelt werden.  Das letzte Netzwerk ist daf√ºr der Vocoder.  Es stellt sich die Frage, ob es m√∂glich ist, mit der inversen Transformation wieder Schall zu erhalten, wenn Spektrogramme aus Schall mit der Fourier-Transformation erhalten werden.  Die Antwort lautet ja und nein.  Die harmonischen Schwingungen, aus denen das urspr√ºngliche Signal besteht, enthalten sowohl Amplitude als auch Phase, und unsere Spektrogramme enthalten nur Informationen √ºber die Amplitude (um Parameter zu reduzieren und mit Spektrogrammen zu arbeiten). Wenn wir also die inverse Fourier-Transformation durchf√ºhren, erhalten wir einen schlechten Klang. </p><br><p>  Um dieses Problem zu l√∂sen, erfanden sie einen schnellen Griffin-Lim-Algorithmus.  Er macht die inverse Fourier-Transformation des Spektrogramms und erh√§lt einen "schlechten" Klang.  Dann wandelt er diesen Ton direkt um und empf√§ngt ein Spektrum, das bereits einige Informationen √ºber die Phase enth√§lt, und die Amplitude √§ndert sich dabei nicht.  Als n√§chstes wird die inverse Transformation erneut durchgef√ºhrt und ein saubererer Klang erhalten.  Leider l√§sst die durch einen solchen Algorithmus erzeugte Sprachqualit√§t zu w√ºnschen √ºbrig. </p><br><p>  Es wurde durch neuronale Vocoder wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveRNN</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveGlow</a> und andere ersetzt.  CorentinJ verwendete das WaveRNN-Modell von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fatchord</a> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2fe/b25/464/2feb25464260ad8a96c983bb85340fee.gif" alt="Bild"></p><br><p>  F√ºr die Datenvorverarbeitung werden zwei Ans√§tze verwendet.  Erhalten Sie entweder Spektrogramme aus Ton (mithilfe der Fourier-Transformation) oder aus Text (mithilfe des Synthesemodells).  Google empfiehlt einen zweiten Ansatz. </p><br><h3 id="trenirovka-2">  Schulung </h3><br><ol><li>  F√ºhren Sie <code>python vocoder_preprocess.py &lt;datasets_root&gt;</code> , um Spektrogramme zu synthetisieren </li><li>  F√ºhren Sie <code>python vocoder_train.py &lt;datasets_root&gt;</code> f√ºr <code>python vocoder_train.py &lt;datasets_root&gt;</code> aus </li></ol><br><h2 id="itogo">  Insgesamt </h2><br><p>  Wir haben ein Modell der mehrsprachigen Sprachsynthese, mit dem eine Stimme geklont werden kann. <br>  F√ºhren Sie die Toolbox aus: <code>python demo_toolbox.py -d &lt;datasets_root&gt;</code> <br>  Beispiele sind hier zu h√∂ren </p><br><div class="oembed">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://soundcloud.com/fn5va3vghrkh/sets/multi-tacotron</a> </div><br><h4 id="sovety-i-vyvody">  Tipps und Schlussfolgerungen </h4><br><ul><li>  Ben√∂tigen Sie viele Daten (&gt; 1000 Stimmen,&gt; 1000 Stunden) </li><li>  Die Betriebsgeschwindigkeit ist nur bei der Synthese von mindestens 4 S√§tzen mit Echtzeit vergleichbar </li><li>  Verwenden Sie f√ºr den Encoder das vorgefertigte Modell f√ºr die englische Sprache, und trainieren Sie es leicht um.  Ihr geht es gut </li><li>  Ein Synthesizer, der auf "saubere" Daten trainiert ist, funktioniert besser, klont jedoch schlechter als einer, der auf einem gr√∂√üeren Volumen trainiert hat, aber schmutzige Daten </li><li>  Das Modell funktioniert nur mit den Daten, an denen ich studiert habe </li></ul><br><p>  Sie k√∂nnen Ihre Stimme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">online mit colab</a> synthetisieren oder meine Implementierung auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github sehen</a> und meine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gewichte</a> herunterladen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465941/">https://habr.com/ru/post/de465941/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465923/index.html">3 Fehler, die Ihr Startup-Leben kosten k√∂nnen</a></li>
<li><a href="../de465927/index.html">Count Scoring de la Fer oder eine Studie zum Kredit-Scoring als Teil der Erweiterung des eigenen Horizonts. Teil 3</a></li>
<li><a href="../de465929/index.html">Infrastruktur-A / B-Experimente in der gro√üen Suche. Yandex-Bericht</a></li>
<li><a href="../de465935/index.html">Professionelle Videokonferenzen sind wie nie zuvor verf√ºgbar. Mind Server - neue Version, neue Preise</a></li>
<li><a href="../de465937/index.html">Technostream: Eine neue Auswahl an Trainingsvideos zu Beginn des Schuljahres</a></li>
<li><a href="../de465943/index.html">Das Einhorn-Startup Bolt wird eine Meisterschaft f√ºr Entwickler mit einem Preis von 350.000 Rubel und der M√∂glichkeit eines Umzugs nach Europa veranstalten</a></li>
<li><a href="../de465945/index.html">Informationen zur Installation und Verwendung von LineageOS 16, F-Droid</a></li>
<li><a href="../de465947/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 30. Cisco-Netzwerkarchitektur und Fehlerbehebung</a></li>
<li><a href="../de465949/index.html">Anwendungen f√ºr E-Books auf dem Android-Betriebssystem. Teil 5. Cloud-Speicher und Player</a></li>
<li><a href="../de465951/index.html">Wir alle brauchen einen Helpdesk</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>