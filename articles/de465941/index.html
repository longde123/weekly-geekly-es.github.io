<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🤝‍👨🏻 💮 🚋 Mehrsprachige Sprachsynthese mit Klonen 🗯️ 👃🏿 👱🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Obwohl neuronale Netze vor nicht allzu langer Zeit ( zum Beispiel ) für die Sprachsynthese verwendet wurden, haben sie es bereits geschafft, die klass...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mehrsprachige Sprachsynthese mit Klonen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465941/"><p>  Obwohl neuronale Netze vor nicht allzu langer Zeit ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zum Beispiel</a> ) für die Sprachsynthese verwendet wurden, haben sie es bereits geschafft, die klassischen Ansätze zu überholen, und jedes Jahr treten immer neuere Aufgaben auf. </p><br><p>  Zum Beispiel gab es vor ein paar Monaten eine Implementierung der Sprachsynthese mit Voice Cloning <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Real-Time-Voice-Cloning</a> .  Versuchen wir herauszufinden, woraus es besteht, und realisieren wir unser mehrsprachiges (russisch-englisches) Phonemmodell. </p><br><h2 id="stroenie">  Gebäude </h2><br><p><img src="https://habrastorage.org/webt/4b/k7/e_/4bk7e_qeb-qw-clbaxsqs14f7cg.png"></p><br><p>  Unser Modell wird aus vier neuronalen Netzen bestehen.  Der erste konvertiert den Text in Phoneme (g2p), der zweite konvertiert die Sprache, die wir klonen möchten, in einen Vektor von Zeichen (Zahlen).  Das dritte wird Mel-Spektrogramme basierend auf den Ausgaben der ersten beiden synthetisieren.  Und schließlich wird der vierte Ton von Spektrogrammen empfangen. </p><a name="habracut"></a><br><h2 id="nabory-dannyh">  Datensätze </h2><br><p>  Dieses Modell braucht viel Sprache.  Nachfolgend finden Sie die Grundlagen, die Ihnen dabei helfen werden. </p><br><div class="scrollable-table"><table><thead><tr><th>  Vorname </th><th>  Sprache </th><th>  Link </th><th>  Kommentare </th><th>  Mein Link </th><th>  Kommentare </th></tr></thead><tbody><tr><td>  Wörterbuch der Phoneme </td><td>  En ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">En</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ru</a> </td><td></td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Kombiniertes russisches und englisches Phonemwörterbuch </td></tr><tr><td>  Libripepeech </td><td>  En </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  300 Stimmen, 360 Stunden reine Rede </td><td></td><td></td></tr><tr><td>  VoxCeleb </td><td>  En </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  7000 Stimmen, viele Stunden schlechten Klangs </td><td></td><td></td></tr><tr><td>  M-AILABS </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  3 Stimmen, 46 Stunden reine Rede </td><td></td><td></td></tr><tr><td>  open_tts, open_stt </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">open_tts</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">open_stt</a> </td><td>  viele Stimmen, viele Stunden schlechten Klangs </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Ich habe 4 Stunden Rede von einem Sprecher gereinigt.  Korrigierte Anmerkung, unterteilt in Segmente bis zu 7 Sekunden </td></tr><tr><td>  Voxforge + Hörbuch </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  viele Stimmen, 25h unterschiedlicher Qualität </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Ausgewählte gute Dateien.  Brach in Segmente.  Hörbücher aus dem Internet hinzugefügt.  Es stellte sich heraus, dass in wenigen Minuten jeweils 200 Lautsprecher vorhanden waren </td></tr><tr><td>  RUSLAN </td><td>  Ru </td><td>  <a href="">Link</a> </td><td>  Eine Stimme, 40 Stunden reine Sprache </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Bei 16 kHz neu codiert </td></tr><tr><td>  Mozilla </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  50 Stimmen, 30 Stunden normale Qualität </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Bei 16 kHz neu codiert, verschiedene Benutzer in Ordnern verteilt </td></tr><tr><td>  Russische Single </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td>  Eine Stimme, 9 Stunden reine Sprache </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> </td><td></td></tr></tbody></table></div><br><h2 id="obrabotka-teksta">  Textverarbeitung </h2><br><p>  Die erste Aufgabe ist die Textverarbeitung.  Stellen Sie sich den Text in der Form vor, in der er weiter gesprochen wird.  Wir werden Zahlen in Worten darstellen und Abkürzungen öffnen.  Lesen Sie mehr im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel über Synthese</a> .  Dies ist eine schwierige Aufgabe. Nehmen wir also an, wir haben bereits Text verarbeitet (er wurde in den obigen Datenbanken verarbeitet). </p><br><p>  Die nächste Frage ist, ob Graphem- oder Phonemaufzeichnung verwendet werden soll.  Für eine monophone und einsprachige Stimme eignet sich auch ein Buchstabenmodell.  Wenn Sie mit einem mehrsprachigen mehrsprachigen Modell arbeiten möchten, empfehle ich Ihnen, die Transkription zu verwenden (auch Google). </p><br><h3 id="g2p">  G2p </h3><br><p>  Für die russische Sprache gibt es eine Implementierung namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">russian_g2p</a> .  Es basiert auf den Regeln der russischen Sprache und kommt mit der Aufgabe gut zurecht, hat aber Nachteile.  Nicht alle Wörter betonen und sind auch nicht für ein mehrsprachiges Modell geeignet.  Nehmen Sie daher das für sie erstellte Wörterbuch, fügen Sie das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wörterbuch für die englische Sprache hinzu</a> und speisen Sie das neuronale Netzwerk (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> ). </p><br><p>  Bevor Sie das Netzwerk trainieren, sollten Sie sich überlegen, welche Klänge aus verschiedenen Sprachen ähnlich klingen, und Sie können ein Zeichen für sie auswählen, für das dies unmöglich ist.  Je mehr Geräusche es gibt, desto schwieriger ist es, das Modell zu lernen, und wenn es zu wenige davon gibt, hat das Modell einen Akzent.  Denken Sie daran, einzelne Zeichen mit betonten Vokalen hervorzuheben.  Für die englische Sprache spielt sekundärer Stress eine kleine Rolle, und ich würde es nicht unterscheiden. </p><br><h2 id="kodirovanie-spikerov">  Lautsprecherkodierung </h2><br><p>  Das Netzwerk ähnelt der Aufgabe, einen Benutzer per Sprache zu identifizieren.  Am Ausgang erhalten verschiedene Benutzer unterschiedliche Vektoren mit Zahlen.  Ich schlage vor, die Implementierung von CorentinJ selbst zu verwenden, die auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> basiert.  Das Modell ist ein dreischichtiges LSTM mit 768 Knoten, gefolgt von einer vollständig verbundenen Schicht von 256 Neuronen, was einen Vektor von 256 Zahlen ergibt. </p><br><p>  Die Erfahrung hat gezeigt, dass ein in Englisch geschultes Netzwerk gut Russisch spricht.  Dies vereinfacht das Leben erheblich, da für das Training viele Daten erforderlich sind.  Ich empfehle, ein bereits geschultes Modell zu nehmen und von VoxCeleb und LibriSpeech auf Englisch umzuschulen sowie alle russischen Sprachen, die Sie finden.  Der Encoder benötigt keine Textanmerkung von Sprachfragmenten. </p><br><h3 id="trenirovka">  Schulung </h3><br><ol><li> Führen Sie <code>python encoder_preprocess.py &lt;datasets_root&gt;</code> , um die Daten zu verarbeiten </li><li>  Führen Sie "visdom" in einem separaten Terminal aus. </li><li>  Führen Sie <code>python encoder_train.py my_run &lt;datasets_root&gt;</code> , um den Encoder zu trainieren </li></ol><br><h2 id="sintez">  Synthese </h2><br><p>  Fahren wir mit der Synthese fort.  Die Modelle, die ich kenne, erhalten keinen Ton direkt aus dem Text, da dies schwierig ist (zu viele Daten).  Erstens erzeugt der Text Ton in spektraler Form, und erst dann wird das vierte Netzwerk in eine vertraute Stimme übersetzt.  Daher verstehen wir zunächst, wie die Spektralform mit der Stimme verbunden ist.  Es ist einfacher, das umgekehrte Problem herauszufinden, wie man ein Spektrogramm aus Ton erhält. </p><br><p>  Der Ton ist in Schritten von 10 ms in Segmente von 25 ms unterteilt (die Standardeinstellung bei den meisten Modellen).  Dann wird unter Verwendung der Fourier-Transformation für jedes Stück das Spektrum berechnet (harmonische Schwingungen, deren Summe das ursprüngliche Signal ergibt) und in Form eines Diagramms dargestellt, wobei der vertikale Streifen das Spektrum eines Segments (in der Frequenz) und in der Horizontalen eine Folge von Segmenten (in der Zeit) ist.  Dieser Graph wird als Spektrogramm bezeichnet.  Wenn die Frequenz nichtlinear codiert wird (die unteren Frequenzen sind besser als die oberen), ändert sich die vertikale Skala (erforderlich, um die Daten zu reduzieren). Dieser Graph wird dann als Mel-Spektrogramm bezeichnet.  So funktioniert das menschliche Gehör, dass wir bei den niedrigeren Frequenzen eine leichte Abweichung besser hören als bei den höheren, daher wird die Klangqualität nicht leiden </p><br><p><img src="https://habrastorage.org/webt/dx/mv/fs/dxmvfst1objf8dmdglu7rlbhaiq.jpeg"></p><br><p>  Es gibt mehrere gute Implementierungen der Spektrogrammsynthese wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tacotron 2</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deepvoice 3</a> .  Jedes dieser Modelle verfügt über eigene Implementierungen, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">4</a> .  Wir werden (wie CorentinJ) das Tacotron-Modell von Rayhane-mamah verwenden. </p><br><p><img src="https://habrastorage.org/webt/u9/jm/h0/u9jmh0ldgpelp8qoouinbx9ptfi.png"></p><br><p>  Tacotron basiert auf dem seq2seq-Netzwerk mit einem Aufmerksamkeitsmechanismus.  Lesen Sie die Details im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> . </p><br><h3 id="trenirovka-1">  Schulung </h3><br><p>  Vergessen Sie nicht, utils / symbols.py zu bearbeiten, wenn Sie nicht nur die englische Sprache hparams.p, sondern auch preprocess.py synthetisieren. </p><br><p>  Die Synthese erfordert viel sauberen, gut markierten Klang von verschiedenen Lautsprechern.  Hier hilft eine Fremdsprache nicht weiter. </p><br><ol><li>  Führen Sie <code>python synthesizer_preprocess_audio.py &lt;datasets_root&gt;</code> , um verarbeiteten Sound und Spektrogramme zu erstellen </li><li>  Führen Sie <code>python synthesizer_preprocess_embeds.py &lt;datasets_root&gt;</code> , um den Sound zu codieren (erhalten Sie die Zeichen einer Stimme). </li><li>  Führen Sie <code>python synthesizer_train.py my_run &lt;datasets_root&gt;</code> , um den Synthesizer zu trainieren </li></ol><br><h2 id="vokoder">  Vocoder </h2><br><p>  Jetzt müssen nur noch die Spektrogramme in Ton umgewandelt werden.  Das letzte Netzwerk ist dafür der Vocoder.  Es stellt sich die Frage, ob es möglich ist, mit der inversen Transformation wieder Schall zu erhalten, wenn Spektrogramme aus Schall mit der Fourier-Transformation erhalten werden.  Die Antwort lautet ja und nein.  Die harmonischen Schwingungen, aus denen das ursprüngliche Signal besteht, enthalten sowohl Amplitude als auch Phase, und unsere Spektrogramme enthalten nur Informationen über die Amplitude (um Parameter zu reduzieren und mit Spektrogrammen zu arbeiten). Wenn wir also die inverse Fourier-Transformation durchführen, erhalten wir einen schlechten Klang. </p><br><p>  Um dieses Problem zu lösen, erfanden sie einen schnellen Griffin-Lim-Algorithmus.  Er macht die inverse Fourier-Transformation des Spektrogramms und erhält einen "schlechten" Klang.  Dann wandelt er diesen Ton direkt um und empfängt ein Spektrum, das bereits einige Informationen über die Phase enthält, und die Amplitude ändert sich dabei nicht.  Als nächstes wird die inverse Transformation erneut durchgeführt und ein saubererer Klang erhalten.  Leider lässt die durch einen solchen Algorithmus erzeugte Sprachqualität zu wünschen übrig. </p><br><p>  Es wurde durch neuronale Vocoder wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveRNN</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveGlow</a> und andere ersetzt.  CorentinJ verwendete das WaveRNN-Modell von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fatchord</a> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2fe/b25/464/2feb25464260ad8a96c983bb85340fee.gif" alt="Bild"></p><br><p>  Für die Datenvorverarbeitung werden zwei Ansätze verwendet.  Erhalten Sie entweder Spektrogramme aus Ton (mithilfe der Fourier-Transformation) oder aus Text (mithilfe des Synthesemodells).  Google empfiehlt einen zweiten Ansatz. </p><br><h3 id="trenirovka-2">  Schulung </h3><br><ol><li>  Führen Sie <code>python vocoder_preprocess.py &lt;datasets_root&gt;</code> , um Spektrogramme zu synthetisieren </li><li>  Führen Sie <code>python vocoder_train.py &lt;datasets_root&gt;</code> für <code>python vocoder_train.py &lt;datasets_root&gt;</code> aus </li></ol><br><h2 id="itogo">  Insgesamt </h2><br><p>  Wir haben ein Modell der mehrsprachigen Sprachsynthese, mit dem eine Stimme geklont werden kann. <br>  Führen Sie die Toolbox aus: <code>python demo_toolbox.py -d &lt;datasets_root&gt;</code> <br>  Beispiele sind hier zu hören </p><br><div class="oembed">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://soundcloud.com/fn5va3vghrkh/sets/multi-tacotron</a> </div><br><h4 id="sovety-i-vyvody">  Tipps und Schlussfolgerungen </h4><br><ul><li>  Benötigen Sie viele Daten (&gt; 1000 Stimmen,&gt; 1000 Stunden) </li><li>  Die Betriebsgeschwindigkeit ist nur bei der Synthese von mindestens 4 Sätzen mit Echtzeit vergleichbar </li><li>  Verwenden Sie für den Encoder das vorgefertigte Modell für die englische Sprache, und trainieren Sie es leicht um.  Ihr geht es gut </li><li>  Ein Synthesizer, der auf "saubere" Daten trainiert ist, funktioniert besser, klont jedoch schlechter als einer, der auf einem größeren Volumen trainiert hat, aber schmutzige Daten </li><li>  Das Modell funktioniert nur mit den Daten, an denen ich studiert habe </li></ul><br><p>  Sie können Ihre Stimme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">online mit colab</a> synthetisieren oder meine Implementierung auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github sehen</a> und meine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gewichte</a> herunterladen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465941/">https://habr.com/ru/post/de465941/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465923/index.html">3 Fehler, die Ihr Startup-Leben kosten können</a></li>
<li><a href="../de465927/index.html">Count Scoring de la Fer oder eine Studie zum Kredit-Scoring als Teil der Erweiterung des eigenen Horizonts. Teil 3</a></li>
<li><a href="../de465929/index.html">Infrastruktur-A / B-Experimente in der großen Suche. Yandex-Bericht</a></li>
<li><a href="../de465935/index.html">Professionelle Videokonferenzen sind wie nie zuvor verfügbar. Mind Server - neue Version, neue Preise</a></li>
<li><a href="../de465937/index.html">Technostream: Eine neue Auswahl an Trainingsvideos zu Beginn des Schuljahres</a></li>
<li><a href="../de465943/index.html">Das Einhorn-Startup Bolt wird eine Meisterschaft für Entwickler mit einem Preis von 350.000 Rubel und der Möglichkeit eines Umzugs nach Europa veranstalten</a></li>
<li><a href="../de465945/index.html">Informationen zur Installation und Verwendung von LineageOS 16, F-Droid</a></li>
<li><a href="../de465947/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 30. Cisco-Netzwerkarchitektur und Fehlerbehebung</a></li>
<li><a href="../de465949/index.html">Anwendungen für E-Books auf dem Android-Betriebssystem. Teil 5. Cloud-Speicher und Player</a></li>
<li><a href="../de465951/index.html">Wir alle brauchen einen Helpdesk</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>