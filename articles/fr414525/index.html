<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•õ üóûÔ∏è üëßüèª Le livre ¬´Effective Spark. Mise √† l'√©chelle et optimisation " ü§õüèø ‚úåüèª üßïüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans cet article, nous examinerons l'acc√®s √† l'API Spark √† partir de divers langages de programmation dans la JVM, ainsi que certains probl√®mes de per...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le livre ¬´Effective Spark. Mise √† l'√©chelle et optimisation "</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/414525/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/g1/uu/lu/g1uulu2edgzcixecswin9lfylnc.jpeg" align="left" alt="image"></a>  Dans cet article, nous examinerons l'acc√®s √† l'API Spark √† partir de divers langages de programmation dans la JVM, ainsi que certains probl√®mes de performances lorsque nous allons au-del√† du langage Scala.  M√™me si vous travaillez en dehors de la JVM, cette section peut √™tre utile, car les langages non JVM d√©pendent souvent de l'API Java et non de l'API Scala. <br><br>  Travailler dans d'autres langages de programmation ne signifie pas toujours que vous devez aller au-del√† de la JVM, et travailler dans la JVM pr√©sente de nombreux avantages en termes de performances - principalement en raison du fait que vous n'avez pas besoin de copier les donn√©es.  Bien qu'il ne soit pas n√©cessaire d'utiliser des biblioth√®ques de liaison ou des adaptateurs sp√©ciaux pour acc√©der √† Spark depuis l'ext√©rieur du langage Scala, l'invocation de code Scala √† partir d'autres langages de programmation peut √™tre difficile.  Le framework Spark prend en charge l'utilisation de Java 8 dans les expressions lambda, et ceux qui utilisent des versions plus anciennes du JDK ont la possibilit√© d'impl√©menter l'interface appropri√©e √† partir du package org.apache.spark.api.java.function.  M√™me dans les cas o√π vous n'avez pas besoin de copier de donn√©es, le travail dans un autre langage de programmation peut avoir des nuances petites mais importantes li√©es aux performances. <br><a name="habracut"></a><br>  Les difficult√©s d'acc√®s √† diverses API Scala sont particuli√®rement prononc√©es lors de l'appel de fonctions avec des balises de classe ou lors de l'utilisation de propri√©t√©s fournies √† l'aide de conversions de types implicites (par exemple, toutes les fonctionnalit√©s des ensembles RDD li√©s aux classes Double et Tuple).  Pour les m√©canismes qui d√©pendent de conversions de types implicites, des classes concr√®tes √©quivalentes sont souvent fournies avec des conversions explicites.  Les balises de classe factices (par exemple, AnyRef) peuvent √™tre transmises √† des fonctions qui d√©pendent des balises de classe (souvent les adaptateurs le font automatiquement.  L'utilisation de classes sp√©cifiques au lieu de conversions de types implicites n'entra√Æne g√©n√©ralement pas de surcharge suppl√©mentaire, mais les balises de classe factices peuvent imposer des restrictions sur certaines optimisations du compilateur. <br><br>  L'API Java n'est pas trop diff√©rente de l'API Scala en termes de propri√©t√©s, seulement occasionnellement certaines fonctionnalit√©s ou API de d√©veloppeur manquent.  D'autres langages de programmation JVM, tels que le langage Clojure avec DSL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Flambo</a> et la biblioth√®que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©tincelante</a> , sont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pris</a> en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">charge √† l'</a> aide de diverses API Java au lieu d'appeler directement l'API Scala.  √âtant donn√© que la plupart des liaisons de langage, m√™me les langages non JVM comme Python et R, passent par l'API <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Java</a> , il sera utile de les g√©rer. <br><br>  Les API Java sont tr√®s similaires aux API Scala, bien qu'elles soient ind√©pendantes des balises de classe et des conversions implicites.  L'absence de ce dernier signifie qu'au lieu de convertir automatiquement les ensembles RDD d'objets Tuple ou doubles en classes sp√©ciales avec des fonctions suppl√©mentaires, vous devez utiliser des fonctions de conversion de type explicite (par exemple, mapToDouble ou mapToPair).  Les fonctions sp√©cifi√©es sont d√©finies uniquement pour les ensembles RDD Java;  heureusement pour la compatibilit√©, ces types sp√©ciaux ne sont que des adaptateurs pour les ensembles Scala RDD.  De plus, ces fonctions sp√©ciales renvoient divers types de donn√©es, tels que JavaDoubleRDD et JavaPairRDD, avec des fonctionnalit√©s fournies par des transformations implicites du langage Scala. <br><br>  Reprenons l'exemple canonique du comptage de mots √† l'aide de l'API Java (exemple 7.1).  √âtant donn√© que l'appel de l'API Scala √† partir de Java peut parfois √™tre une t√¢che ardue, presque toutes les API du framework Java Spark sont impl√©ment√©es dans le langage Scala avec des balises de classe cach√©es et des conversions implicites.  Pour cette raison, les adaptateurs Java sont une couche tr√®s mince, compos√©e en moyenne de seulement quelques lignes de code, et leur r√©√©criture est pratiquement sans effort. <br><br>  Exemple 7.1  Comptage de mots (Java) <br><br><pre><code class="hljs actionscript"><span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> scala.Tuple2;</span></span>  <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaRDD;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaPairRDD </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaSparkContext;</span></span>  <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.util.regex.Pattern;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.util.Arrays;</span></span>  <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">WordCount</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> Pattern pattern = Pattern.compile(<span class="hljs-string"><span class="hljs-string">" "</span></span>);  <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">void</span></span> main(String[] args) throws Exception { JavaSparkContext jsc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JavaSparkContext(); JavaRDD&lt;String&gt; lines = jsc.textFile(args[<span class="hljs-number"><span class="hljs-number">0</span></span>]); JavaRDD&lt;String&gt; words = lines.flatMap(e -&gt; Arrays.asList(                                           pattern.split(e)).iterator()); JavaPairRDD&lt;String, Integer&gt; wordsIntial = words.mapToPair(  e -&gt; <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Tuple2&lt;String, Integer&gt;(e, <span class="hljs-number"><span class="hljs-number">1</span></span>));   } }</code> </pre> <br>  Parfois, vous devrez peut-√™tre convertir des RDD Java en RDD Scala ou vice versa.  Cela est le plus souvent n√©cessaire pour les biblioth√®ques n√©cessitant une entr√©e ou renvoyant des ensembles RDD Scala, mais parfois les propri√©t√©s de base de Spark peuvent ne pas encore √™tre disponibles dans l'API Java.  La conversion de RDD Java en RDD Scala est le moyen le plus simple d'utiliser ces nouvelles fonctionnalit√©s. <br><br>  Si vous devez transf√©rer l'ensemble RDD Java vers la biblioth√®que Scala, qui attend un √©tincelle RDD standard √† l'entr√©e, vous pouvez acc√©der au Scala RDD sous-jacent √† l'aide de la m√©thode rdd ().  Le plus souvent, cela suffit pour transf√©rer le RDD final vers n'importe quelle biblioth√®que Scala souhait√©e;  Parmi les exceptions notables figurent les biblioth√®ques Scala, qui s'appuient sur des conversions implicites de types de jeux de types de contenu ou d'informations de balises de classe dans leur travail.  Dans ce cas, le moyen le plus simple d'acc√©der aux conversions implicites consiste √† √©crire un petit adaptateur dans Scala.  Si les shells Scala ne peuvent pas √™tre utilis√©s, vous pouvez appeler la fonction correspondante de la classe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">JavaConverters</a> et former une balise de classe factice. <br><br>  Pour cr√©er une balise de classe factice, vous pouvez utiliser la m√©thode scala.reflect.ClassTag $ .MODULE $ .AnyRef () ou obtenir la vraie en utilisant scala.reflect.ClassTag $ .MODULE $ .apply (CLASS), comme illustr√© dans les exemples 7.2 et 7.3. <br><br>  Pour convertir de Scala RDD en RDD Java, les informations de balise de classe sont souvent plus importantes que la plupart des biblioth√®ques Spark.  La raison en est que bien que diverses classes JavaRDD fournissent des constructeurs accessibles au public qui prennent Scala RDD comme arguments, elles sont destin√©es √† √™tre appel√©es √† partir du code Scala et n√©cessitent donc des informations sur la balise de classe. <br><br>  Les balises de classe factices sont le plus souvent utilis√©es dans le code g√©n√©rique ou mod√®le, o√π les types exacts sont inconnus au moment de la compilation.  De telles balises suffisent souvent, bien qu'il soit possible de perdre certaines nuances du c√¥t√© du code Scala;  dans de tr√®s rares cas, le code Scala n√©cessite des informations pr√©cises sur les balises de classe.  Dans ce cas, vous devrez utiliser une vraie balise.  Dans la plupart des cas, cela ne n√©cessite pas beaucoup d'efforts et am√©liore les performances, essayez donc d'utiliser ces balises dans la mesure du possible. <br><br>  Exemple 7.2.  Rendre Java / Scala RDD compatible avec une balise de classe factice <br><br><pre> <code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">public</span></span> static JavaPairRDD wrapPairRDDFakeCt( RDD&lt;Tuple2&lt;String, <span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>&gt;&gt; RDD) { //       AnyRef ‚Äî   //        , //        , //        //    ClassTag&lt;<span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>&gt; fake = ClassTag$.MODULE$.AnyRef(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> JavaPairRDD(rdd, fake, fake); }</code> </pre> <br>  Exemple 7.3.  Garantir la compatibilit√© RDD Java / Scala <br><br><pre> <code class="hljs ruby">public static JavaPairRDD wrapPairRDD( RDD&lt;Tuple2&lt;String, Object<span class="hljs-meta"><span class="hljs-meta">&gt;&gt; </span></span>RDD) { <span class="hljs-regexp"><span class="hljs-regexp">//</span></span>    ClassTag&lt;String&gt; strCt = ClassTag$.MODULE$.apply(String.class); ClassTag&lt;Long&gt; longCt = ClassTag$.MODULE$.apply(scala.Long.class); return new JavaPairRDD(rdd, strCt, longCt); }</code> </pre> <br>  Les API Spark SQL et ML pipeline ont √©t√© pour la plupart rendues coh√©rentes en Java et Scala.  Cependant, il existe des fonctions d'assistance sp√©cifiques √† Java et les fonctions Scala √©quivalentes ne sont pas faciles √† appeler.  Voici leurs exemples: diverses fonctions num√©riques, telles que plus, moins, etc., pour la classe Column.  Il est difficile d'appeler leurs √©quivalents surcharg√©s de la langue Scala (+, -).  Au lieu d'utiliser JavaDataFrame et JavaSQLContext, les m√©thodes requises par Java sont disponibles dans SQLContext et les ensembles DataFrame standard.  Cela peut vous d√©router, car certaines des m√©thodes mentionn√©es dans la documentation Java ne peuvent pas √™tre utilis√©es √† partir du code Java, mais dans de tels cas, des fonctions portant les m√™mes noms sont fournies pour appeler √† partir de Java. <br><br>  Les fonctions d√©finies par l'utilisateur (UDF) dans le langage Java, et d'ailleurs, dans la plupart des autres langages √† l'exception de Scala, elles n√©cessitent de sp√©cifier le type de la valeur renvoy√©e par la fonction, car elle ne peut pas √™tre d√©duite de mani√®re logique, semblable √† la fa√ßon dont elle est effectu√©e dans Scala (exemple 7.4) . <br><br>  Exemple 7.4.  Exemple UDF pour Java <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">sqlContext</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.udf</span></span>() <span class="hljs-selector-class"><span class="hljs-selector-class">.register</span></span>("<span class="hljs-selector-tag"><span class="hljs-selector-tag">strlen</span></span>", (<span class="hljs-selector-tag"><span class="hljs-selector-tag">String</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">s</span></span>) <span class="hljs-selector-tag"><span class="hljs-selector-tag">-</span></span>&gt; <span class="hljs-selector-tag"><span class="hljs-selector-tag">s</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.length</span></span>(), <span class="hljs-selector-tag"><span class="hljs-selector-tag">DataTypes</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.StringType</span></span>);</code> </pre> <br>  Bien que les types requis par les API Scala et Java soient diff√©rents, l'encapsulation des types de collection Java ne n√©cessite pas de copie suppl√©mentaire.  Dans le cas des it√©rateurs, la conversion de type requise pour l'adaptateur est effectu√©e de mani√®re retard√©e lors de l'acc√®s aux √©l√©ments, ce qui permet au framework Spark de vider les donn√©es si n√©cessaire (comme expliqu√© dans la section "Effectuer des transformations it√©rateur-it√©rateur √† l'aide de la fonction mapPartitions" √† la page 125).  Ceci est tr√®s important car pour de nombreuses op√©rations simples, le co√ªt de la copie des donn√©es peut √™tre sup√©rieur au co√ªt du calcul lui-m√™me. <br><br><h3>  Au-del√† de Scala et JVM </h3><br>  Si vous ne vous limitez pas √† la JVM, le nombre de langages de programmation disponibles pour le travail augmente consid√©rablement.  Cependant, avec l'architecture Spark actuelle, travailler en dehors de la JVM - en particulier sur les n≈ìuds de travail - peut entra√Æner des augmentations de co√ªts importantes en raison de la copie des donn√©es dans les n≈ìuds de travail entre la JVM et le code de langue cible.  Dans les op√©rations complexes, la part du co√ªt de copie des donn√©es est relativement faible, mais dans les op√©rations simples, elle peut facilement conduire √† un doublement du co√ªt de calcul total. <br><br>  Le premier langage de programmation non JVM directement pris en charge en dehors de Spark est Python, son API et son interface sont devenues le mod√®le sur lequel les impl√©mentations pour d'autres langages de programmation non JVM sont bas√©es. <br><br><h3>  Comment PySpark fonctionne </h3><br>  PySpark se connecte √† JVM Spark en utilisant un m√©lange de canaux sur les travailleurs et Py4J, une biblioth√®que sp√©cialis√©e qui fournit une interaction Python / Java, sur le pilote.  Sous cela, √† premi√®re vue, une architecture simple cache beaucoup de nuances complexes, gr√¢ce auxquelles PySpark fonctionne, comme le montre la Fig.  7.1.  L'un des principaux probl√®mes: m√™me lorsque les donn√©es sont copi√©es d'un travailleur Python vers la JVM, ce n'est pas sous la forme qu'une machine virtuelle peut facilement analyser.  Des efforts particuliers sont requis de la part du travailleur Python et Java pour garantir que la machine virtuelle Java dispose de suffisamment d'informations pour des op√©rations telles que le partitionnement. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/10/ez/wf/10ezwfv-1jvl1gxwsansnexwvj4.png" alt="image"></div><br><h3>  Kits RDD PySpark </h3><br>  Le co√ªt des ressources pour le transfert de donn√©es vers et depuis la JVM, ainsi que pour l'ex√©cution de l'ex√©cuteur Python, est important.  Vous pouvez √©viter de nombreux probl√®mes de performances avec les API PySpark RDD Suite √† l'aide des API DataFrame / Dataset, car les donn√©es restent dans la JVM aussi longtemps que possible. <br><br>  La copie des donn√©es de la JVM vers Python se fait √† l'aide de sockets et d'octets s√©rialis√©s.  Une version plus g√©n√©rale pour interagir avec des programmes dans d'autres langues est disponible via l'interface PipedRDD, dont l'application est pr√©sent√©e dans la sous-section ¬´Utilisation de pipe¬ª. <br><br>  L'organisation de canaux d'√©change de donn√©es (dans les deux sens) pour chaque transformation serait trop co√ªteuse.  Par cons√©quent, PySpark organise (si possible) le pipeline de transformation Python √† l'int√©rieur de l'interpr√©teur Python, encha√Ænant l'op√©ration de filtrage, puis la carte, sur l'it√©rateur d'objet Python √† l'aide de la classe sp√©cialis√©e PipelinedRDD.  M√™me lorsque vous devez m√©langer des donn√©es et que PySpark n'est pas en mesure de cha√Æner les conversions dans la machine virtuelle d'un travailleur individuel, vous pouvez r√©utiliser l'interpr√©teur Python, de sorte que le co√ªt de d√©marrage de l'interpr√©teur ne ralentira pas davantage. <br><br>  Ce n'est qu'une partie du puzzle.  Les PipedRDD classiques fonctionnent avec le type String, qui n'est pas si facile √† m√©langer en raison de l'absence d'une cl√© naturelle.  Dans PySpark, et dans son image et sa similitude dans les biblioth√®ques se liant √† de nombreux autres langages de programmation, un type sp√©cial de PairwiseRDD est utilis√©, o√π la cl√© est un entier long, et sa d√©s√©rialisation est effectu√©e par le code utilisateur dans le langage Scala, destin√© √† l'analyse des valeurs Python.  Le co√ªt de cette d√©s√©rialisation n'est pas trop √©lev√©, mais cela d√©montre que Scala dans le framework Spark consid√®re essentiellement les r√©sultats du code Python comme des tableaux d'octets ¬´opaques¬ª. <br><br>  Pour toute sa simplicit√©, cette approche d'int√©gration fonctionne √©tonnamment bien, et la plupart des op√©rations sur les ensembles Scala RDD sont disponibles en Python.  Dans certains des endroits les plus difficiles du code, les biblioth√®ques sont accessibles, par exemple, MLlib, ainsi que le chargement / enregistrement des donn√©es √† partir de diverses sources. <br><br>  Travailler avec diff√©rents formats de donn√©es impose √©galement ses limites, car une partie importante du code de chargement / enregistrement des donn√©es √† partir du framework Spark est bas√©e sur les interfaces Java Hadoop.  Cela signifie que toutes les donn√©es charg√©es sont d'abord charg√©es dans la JVM, puis d√©plac√©es vers Python. <br><br>  Deux approches sont g√©n√©ralement utilis√©es pour interagir avec MLlib: soit PySpark utilise un type de donn√©es sp√©cialis√© avec des conversions de type Scala, soit l'algorithme est r√©impl√©ment√© en Python.  Ces probl√®mes peuvent √™tre √©vit√©s avec le package Spark ML, qui utilise l'interface DataFrame / Dataset, qui stocke g√©n√©ralement les donn√©es dans la JVM. <br><br><h3>  Kits PySpark DataFrame et Dataset </h3><br>  Les ensembles DataFrame et Dataset n'ont pas beaucoup de probl√®mes de performances avec les API d'ensemble Python RDD car ils stockent les donn√©es dans la JVM aussi longtemps que possible.  Le m√™me test de performances que nous avons effectu√© pour illustrer la sup√©riorit√© des ensembles DataFrame sur les ensembles RDD (voir figure 3.1) montre des diff√©rences significatives lors de l'ex√©cution en Python (figure 7.2). <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d9/mk/tl/d9mktl7qhe3hg8z2e9lnyanlgde.png" alt="image"></div><br>  Pour de nombreuses op√©rations avec des ensembles DataFrame et Dataset, vous n'aurez peut-√™tre pas besoin de d√©placer les donn√©es de la JVM, bien que l'utilisation de diverses expressions lambda UDF, UDAF et Python n√©cessite naturellement de d√©placer certaines donn√©es dans la JVM.  Cela conduit au sch√©ma simplifi√© suivant pour de nombreuses op√©rations, qui ressemble √† celui illustr√© sur la Fig.  7.3. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4e/3q/el/4e3qel6hamrvb5ipzycqh9sftcg.png" alt="image"></div><br><h3>  Acc√®s aux objets Java sous-jacents et au code mixte dans Scala </h3><br>  Une cons√©quence importante de l'architecture PySpark est que de nombreuses classes de framework Spark Python sont en fait des adaptateurs pour traduire les appels du code Python en une forme JVM compr√©hensible. <br><br>  Si vous travaillez avec des d√©veloppeurs Scala / Java et que vous souhaitez interagir avec leur code, √† l'avance, il n'y aura pas d'adaptateurs pour acc√©der √† votre code, mais vous pouvez enregistrer votre UDF Java / Scala et les utiliser √† partir du code Python.  √Ä partir de Spark 2.1, cela peut √™tre fait √† l'aide de la m√©thode registerJavaFunction de l'objet sqlContext. <br><br>  Parfois, ces adaptateurs ne disposent pas de tous les m√©canismes n√©cessaires, et comme Python ne dispose pas d'une solide protection contre les appels de m√©thodes priv√©es, vous pouvez imm√©diatement vous tourner vers la JVM.  La m√™me technique vous permettra d'acc√©der √† votre propre code dans la JVM et, avec peu d'effort, de reconvertir les r√©sultats en objets Python. <br><br>  Dans la sous-section "Grands plans de requ√™te et algorithmes it√©ratifs" √† la p.  91 nous avons not√© l'importance d'utiliser la version JVM des ensembles DataFrame et RDD pour r√©duire le plan de requ√™te.  Il s'agit d'une solution de contournement, car lorsque les plans de requ√™te deviennent trop volumineux pour √™tre trait√©s par l'optimiseur Spark SQL, l'optimiseur SQL, en raison du placement de l'ensemble RDD au milieu, perd la possibilit√© de regarder au-del√† du moment o√π les donn√©es apparaissent dans RDD.  La m√™me chose peut √™tre obtenue √† l'aide des API Python publiques, cependant, de nombreux avantages des ensembles DataFrame seront perdus, car toutes les donn√©es devront aller et venir via les n≈ìuds de travail de Python.  Au lieu de cela, vous pouvez r√©duire le graphique d'origine en continuant √† stocker des donn√©es dans la JVM (comme illustr√© dans l'exemple 7.5). <br><br>  Exemple 7.5  D√©coupage d'un plan de requ√™te volumineux pour un DataFrame √† l'aide de Python <br><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cutLineage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    DataFrame ‚Äî     .. :              &gt;&gt;&gt; df = RDD.toDF() &gt;&gt;&gt; cutDf = cutLineage(df) &gt;&gt;&gt; cutDf.count() 3 """</span></span> jRDD = df._jdf.toJavaRDD() jSchema = df._jdf.schema() jRDD.cache() sqlCtx = df.sql_ctx <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: javaSqlCtx = sqlCtx._jsqlContext <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._ssql_ctx newJavaDF = javaSqlCtx.createDataFrame(jRDD, jSchema) newDF = DataFrame(newJavaDF, sqlCtx) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> newDF</code> </pre> <br>  De mani√®re g√©n√©rale, par convention, la syntaxe _j [nom_abr√©g√©] est utilis√©e pour acc√©der aux versions Java internes de la plupart des objets Python.  Ainsi, par exemple, l'objet SparkContext a _jsc, ce qui vous permet d'obtenir l'objet Java interne SparkContext.  Cela n'est possible que dans le programme du pilote, donc lorsque vous envoyez des objets PySpark aux n≈ìuds de travail, vous ne pourrez pas acc√©der au composant Java interne et la plupart de l'API ne fonctionnera pas. <br><br>  Pour acc√©der √† la classe Spark dans la JVM, qui ne poss√®de pas d'adaptateur Python, vous pouvez utiliser la passerelle Py4J sur le pilote.  L'objet SparkContext contient un lien vers la passerelle dans la propri√©t√© _gateway.  La syntaxe sc._gateway.jvm. [Full_class_name_in_JVM] permettra d'acc√©der √† n'importe quel objet Java. <br><br>  Une technique similaire fonctionnera pour vos propres classes Scala si elles sont organis√©es selon le chemin de classe.  Vous pouvez ajouter des fichiers JAR au chemin de classe √† l'aide de la commande spark-submit avec le param√®tre --jars ou en d√©finissant les propri√©t√©s de configuration spark.driver.extraClassPath.  Exemple 7.6, qui a aid√© √† g√©n√©rer du riz.  7.2, est intentionnellement con√ßu pour g√©n√©rer des donn√©es pour les tests de performances en utilisant le code Scala existant. <br><br>  Exemple 7.6  Appel de classes non Spark-JVM √† l'aide de Py4J <br><br><pre> <code class="hljs pgsql">sc = sqlCtx._sc #  <span class="hljs-keyword"><span class="hljs-keyword">SQL</span></span> Context,   <span class="hljs-number"><span class="hljs-number">2.1</span></span>, <span class="hljs-number"><span class="hljs-number">2.0</span></span>   , #  <span class="hljs-number"><span class="hljs-number">2.0</span></span>, ‚Äî  ,   :p try: try: javaSqlCtx = sqlCtx._jsqlContext <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._ssql_ctx <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._jwrapped jsc = sc._jsc scalasc = jsc.sc() gateway = sc._gateway #  java-,   RDD JVM- # <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span> (<span class="hljs-type"><span class="hljs-type">Int</span></span>, <span class="hljs-type"><span class="hljs-type">Double</span></span>).   RDD  Python   #  RDD  Java (   <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span>),   # ,      . #   Java-RDD  <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span> ‚Äî     #    DataFrame,     #    RDD  <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span>. java_rdd = (gateway.jvm.com.highperformancespark.examples. tools.GenerateScalingData. generateMiniScaleRows(scalasc, <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span>, numCols)) #     <span class="hljs-type"><span class="hljs-type">JSON</span></span>     . #  Python-     Java-. schema = StructType([ StructField("zip", IntegerType()), StructField("fuzzyness", DoubleType())]) #   <span class="hljs-number"><span class="hljs-number">2.1</span></span> /  <span class="hljs-number"><span class="hljs-number">2.1</span></span> try: jschema = javaSqlCtx.parseDataType(<span class="hljs-keyword"><span class="hljs-keyword">schema</span></span>.json()) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: jschema = sqlCtx._jsparkSession.parseDataType(<span class="hljs-keyword"><span class="hljs-keyword">schema</span></span>.json()) #  RDD (Java)  DataFrame (Java) java_dataframe = javaSqlCtx.createDataFrame(java_rdd, jschema) #  DataFrame (Java)  DataFrame (Python) python_dataframe = DataFrame(java_dataframe, sqlCtx) #  DataFrame (Python)   RDD pairRDD = python_dataframe.rdd.map(lambda <span class="hljs-keyword"><span class="hljs-keyword">row</span></span>: (<span class="hljs-keyword"><span class="hljs-keyword">row</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-keyword"><span class="hljs-keyword">row</span></span>[<span class="hljs-number"><span class="hljs-number">1</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (python_dataframe, pairRDD)</code> </pre> <br><br>  Bien que de nombreuses classes Python soient simplement des adaptateurs d'objets Java, tous les objets Java ne peuvent pas √™tre encapsul√©s dans des objets Python, puis utilis√©s dans Spark.  Par exemple, les objets des ensembles RDD PySpark sont repr√©sent√©s comme des cha√Ænes s√©rialis√©es, qui ne peuvent √™tre analys√©es facilement qu'en code Python.  Heureusement, les objets DataFrame sont standardis√©s entre diff√©rents langages de programmation, donc si vous pouvez convertir vos donn√©es en ensembles DataFrame, vous pouvez ensuite les encapsuler dans des objets Python et les utiliser directement comme Python DataFrame, ou convertir un Python DataFrame en RDD de cette m√™me langue. <br><br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  20% de r√©duction sur les coupons pour pulv√©risateurs - <b>Spark</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr414525/">https://habr.com/ru/post/fr414525/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr414513/index.html">27 excellents outils de d√©veloppement Web open source</a></li>
<li><a href="../fr414515/index.html">Le√ßon d'optimisation du serveur d'applications Web</a></li>
<li><a href="../fr414517/index.html">Scientifiques d'Oxford: la probabilit√© que nous soyons seuls dans la partie pr√©visible de l'univers est bien sup√©rieure √† z√©ro</a></li>
<li><a href="../fr414519/index.html">Comment transformer 15 minutes de r√©unions Scrum en salle comble?</a></li>
<li><a href="../fr414523/index.html">Comparaison des quadcopt√®res DJI Mavic Pro et Mavic Air</a></li>
<li><a href="../fr414527/index.html">Ce qui nous attend sur Highload ++ Siberia, √† l'exception des ours peints</a></li>
<li><a href="../fr414531/index.html">Personne ne sait ce qu'il adviendra des achats en ligne √† partir du 1er juillet</a></li>
<li><a href="../fr414535/index.html">Manchester: berceau du d√©couragement, du post-punk et de deux c√©l√®bres clubs de football</a></li>
<li><a href="../fr414537/index.html">Comment nous avons cr√©√© l'un des meilleurs jeux AR au monde sans trafic payant</a></li>
<li><a href="../fr414539/index.html">Les cybercriminels volent de plus en plus les donn√©es personnelles des Russes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>