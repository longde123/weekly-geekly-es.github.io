<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë± üíì üôãüèª Comment nous avons fait une reconnaissance historique dans Cloud Mail.ru, et pourquoi üßìüèº üìü üö¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Avec l'av√®nement des t√©l√©phones portables avec des cam√©ras de haute qualit√©, nous avons commenc√© √† faire de plus en plus de photos et de vid√©os de mom...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons fait une reconnaissance historique dans Cloud Mail.ru, et pourquoi</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/467905/"><img src="https://habrastorage.org/webt/vz/bg/ov/vzbgovf2e5gessaexiyl0rnhvay.jpeg"><br><br>  Avec l'av√®nement des t√©l√©phones portables avec des cam√©ras de haute qualit√©, nous avons commenc√© √† faire de plus en plus de photos et de vid√©os de moments lumineux et m√©morables dans nos vies.  Beaucoup d'entre nous ont des archives photographiques qui remontent √† des d√©cennies et comprennent des milliers d'images, ce qui les rend de plus en plus difficiles √† parcourir.  N'oubliez pas combien de temps il a fallu pour trouver une photo int√©ressante il y a quelques ann√©es √† peine. <br><br>  L'un des objectifs de Mail.ru Cloud est de fournir les moyens les plus pratiques pour acc√©der et rechercher vos propres archives photo et vid√©o.  √Ä cette fin, nous, √† Mail.ru Computer Vision Team, avons cr√©√© et mis en ≈ìuvre des syst√®mes de traitement intelligent des images: recherche par objet, par sc√®ne, par visage, etc.  Une autre technologie spectaculaire est la reconnaissance historique.  Aujourd'hui, je vais vous dire comment nous en avons fait une r√©alit√© gr√¢ce au Deep Learning. <br><a name="habracut"></a><br>  Imaginez la situation: vous revenez de vos vacances avec un tas de photos.  En discutant avec vos amis, vous √™tes invit√© √† montrer une image d'un endroit qui vaut le d√©tour, comme un palais, un ch√¢teau, une pyramide, un temple, un lac, une cascade, une montagne, etc.  Vous vous pr√©cipitez pour faire d√©filer le dossier de votre galerie en essayant d'en trouver un qui est vraiment bon.  Tr√®s probablement, il est perdu parmi des centaines d'images, et vous dites que vous le montrerez plus tard. <br><br>  Nous r√©solvons ce probl√®me en regroupant les photos des utilisateurs dans des albums.  Cela vous permettra de trouver les images dont vous avez besoin en quelques clics.  Nous avons maintenant des albums compil√©s par visage, par objet et par sc√®ne, ainsi que par point de rep√®re. <br><br>  Les photos avec des points de rep√®re sont essentielles car elles capturent souvent les moments forts de notre vie (voyages, par exemple).  Il peut s'agir d'images avec une architecture ou une nature sauvage en arri√®re-plan.  C'est pourquoi nous cherchons √† localiser ces images et √† les rendre facilement accessibles aux utilisateurs. <br><br><h2>  Particularit√©s de la reconnaissance des monuments </h2><br>  Il y a une nuance ici: on ne se contente pas d'enseigner un mod√®le et de le faire reconna√Ætre imm√©diatement des points de rep√®re - il y a un certain nombre de d√©fis. <br><br>  Premi√®rement, nous ne pouvons pas dire clairement ce qu'est r√©ellement un ¬´point de rep√®re¬ª.  Nous ne pouvons pas dire pourquoi un b√¢timent est un point de rep√®re, alors qu'un autre √† c√¥t√© ne l'est pas.  Ce n'est pas un concept formalis√©, ce qui complique la formulation de la t√¢che de reconnaissance. <br><br>  Deuxi√®mement, les points de rep√®re sont incroyablement diversifi√©s.  Il peut s'agir de b√¢timents ayant une valeur historique ou culturelle, comme un temple, un palais ou un ch√¢teau.  Alternativement, il peut s'agir de toutes sortes de monuments.  Ou des caract√©ristiques naturelles: lacs, canyons, cascades, etc.  En outre, il existe un mod√®le unique qui devrait √™tre en mesure de trouver tous ces points de rep√®re. <br><br>  Troisi√®mement, les images avec des rep√®res sont extr√™mement rares.  Selon nos estimations, elles ne repr√©sentent que 1 √† 3% des photos des utilisateurs.  C'est pourquoi nous ne pouvons pas nous permettre de faire des erreurs de reconnaissance, car si nous montrons √† quelqu'un une photographie sans rep√®re, ce sera assez √©vident et provoquera une r√©action ind√©sirable.  Ou, √† l'inverse, imaginez que vous montrez une photo avec un lieu d'int√©r√™t √† New York √† une personne qui n'est jamais all√©e aux √âtats-Unis.  Ainsi, le mod√®le de reconnaissance devrait avoir un FPR faible (taux de faux positifs). <br><br>  Quatri√®mement, environ 50% des utilisateurs, voire plus, d√©sactivent g√©n√©ralement la sauvegarde des donn√©es g√©ographiques.  Nous devons en tenir compte et utiliser uniquement l'image elle-m√™me pour identifier l'emplacement.  Aujourd'hui, la plupart des services capables de g√©rer les rep√®res utilisent d'une mani√®re ou d'une autre les g√©odonn√©es des propri√©t√©s de l'image.  Cependant, nos exigences initiales √©taient plus strictes. <br><br>  Permettez-moi maintenant de vous montrer quelques exemples. <br><br>  Voici trois objets qui se ressemblent, trois cath√©drales gothiques en France.  A gauche se trouve la cath√©drale d'Amiens, celle du milieu est la cath√©drale de Reims, et Notre-Dame de Paris est √† droite. <br><br><img src="https://habrastorage.org/webt/bh/4f/ej/bh4fejtind2ngdha-cgxdgkqf9g.jpeg"><br><br>  M√™me un humain a besoin d'un certain temps pour regarder de pr√®s et voir qu'il s'agit de cath√©drales diff√©rentes, mais le moteur devrait √™tre capable de faire de m√™me, et encore plus rapidement qu'un humain. <br><br>  Voici un autre d√©fi: les trois photos ici pr√©sentent Notre-Dame de Paris prises sous des angles diff√©rents.  Les photos sont assez diff√©rentes, mais elles doivent encore √™tre reconnues et r√©cup√©r√©es. <br><br><img src="https://habrastorage.org/webt/vx/de/wr/vxdewr0j1vdlm8knfq2_z_6n95u.jpeg"><br><br>  Les caract√©ristiques naturelles sont enti√®rement diff√©rentes de l'architecture.  √Ä gauche, C√©sar√©e en Isra√´l, √† droite, Englischer Garten √† Munich. <br><br><img src="https://habrastorage.org/webt/y6/6o/mb/y66ombyco0nzwj3ghhjumufaiz4.jpeg"><br><br>  Ces photos donnent au mod√®le tr√®s peu d'indices √† deviner. <br><br><h2>  Notre m√©thode </h2><br>  Notre m√©thode est enti√®rement bas√©e sur des r√©seaux neuronaux convolutionnels profonds.  La strat√©gie de formation que nous avons choisie √©tait ce qu'on appelle l'apprentissage curriculaire, ce qui signifie l'apprentissage en plusieurs √©tapes.  Pour atteindre une plus grande efficacit√© avec et sans donn√©es g√©ographiques disponibles, nous avons fait une inf√©rence sp√©cifique.  Permettez-moi de vous parler de chaque √©tape plus en d√©tail. <br><br><h2>  Ensemble de donn√©es </h2><br>  Les donn√©es sont le carburant de l'apprentissage automatique.  Tout d'abord, nous avons d√ª rassembler un ensemble de donn√©es pour enseigner le mod√®le. <br><br>  Nous avons divis√© le monde en 4 r√©gions, chacune √©tant utilis√©e √† une √©tape sp√©cifique du processus d'apprentissage.  Ensuite, nous avons s√©lectionn√© des pays dans chaque r√©gion, choisi une liste de villes pour chaque pays et collect√© une banque de photos.  Voici quelques exemples. <br><br><img src="https://habrastorage.org/webt/cm/al/en/cmalenos8kpuchcb7ridv6m5rge.jpeg"><br><br>  Tout d'abord, nous avons tent√© de faire apprendre notre mod√®le √† partir de la base de donn√©es obtenue.  Les r√©sultats ont √©t√© m√©diocres.  Notre analyse a montr√© que les donn√©es √©taient sales.  Il y avait trop de bruit g√™nant la reconnaissance de chaque point de rep√®re.  Qu'allions-nous faire?  Il serait co√ªteux, encombrant et pas trop sage d'examiner manuellement la majeure partie des donn√©es.  Nous avons donc con√ßu un processus de nettoyage automatique de la base de donn√©es o√π la manipulation manuelle n'est utilis√©e qu'en une seule √©tape: nous avons tri√© sur le volet 3 √† 5 photographies de r√©f√©rence pour chaque point de rep√®re qui montraient d√©finitivement l'objet souhait√© sous un angle plus ou moins appropri√©.  Cela fonctionne assez rapidement car la quantit√© de ces donn√©es de r√©f√©rence est faible par rapport √† l'ensemble de la base de donn√©es.  Ensuite, un nettoyage automatique bas√© sur des r√©seaux de neurones convolutionnels profonds est effectu√©. <br><br>  Plus loin, je vais utiliser le terme ¬´int√©gration¬ª par lequel je veux dire ce qui suit.  Nous avons un r√©seau neuronal convolutif.  Nous l'avons form√© pour classer les objets, puis nous avons coup√© la derni√®re couche de classification, s√©lectionn√© quelques images, les avons analys√©es par le r√©seau et obtenu un vecteur num√©rique en sortie.  C'est ce que j'appellerai l'int√©gration. <br><br>  Comme je l'ai d√©j√† dit, nous avons organis√© notre processus d'apprentissage en plusieurs √©tapes correspondant √† des parties de notre base de donn√©es.  Donc, tout d'abord, nous prenons soit le r√©seau neuronal de l'√©tape pr√©c√©dente, soit le r√©seau d'initialisation. <br><br>  Nous avons des photos de r√©f√©rence d'un point de rep√®re, les traitons par le r√©seau et obtenons plusieurs plongements.  Nous pouvons maintenant proc√©der au nettoyage des donn√©es.  Nous prenons toutes les photos de l'ensemble de donn√©es pour le point de rep√®re et les faisons √©galement traiter par le r√©seau.  Nous obtenons quelques plongements et d√©terminons la distance par rapport aux plongements de r√©f√©rence pour chacun.  Ensuite, nous d√©terminons la distance moyenne et, si elle d√©passe un certain seuil qui est un param√®tre de l'algorithme, traitons l'objet comme non-rep√®re.  Si la distance moyenne est inf√©rieure au seuil, nous conservons la photo. <br><br><img src="https://habrastorage.org/webt/cl/s7/xu/cls7xusqgmt6mmx5uoyksvkvg4s.jpeg"><br><br>  En cons√©quence, nous avions une base de donn√©es qui contenait plus de 11 mille monuments de plus de 500 villes dans 70 pays, plus de 2,3 millions de photos.  N'oubliez pas que la majeure partie des photographies n'a aucun rep√®re.  Nous devons le dire √† nos mod√®les d'une mani√®re ou d'une autre.  Pour cette raison, nous avons ajout√© 900 000 photos sans rep√®res √† notre base de donn√©es et form√© notre mod√®le avec l'ensemble de donn√©es r√©sultant. <br><br>  Nous avons introduit un test hors ligne pour mesurer la qualit√© de l'apprentissage.  √âtant donn√© que les points de rep√®re n'apparaissent que dans 1 √† 3% de toutes les photos, nous avons compil√© manuellement un ensemble de 290 images qui montraient un point de rep√®re.  Ces photos √©taient assez diverses et complexes, avec un grand nombre d'objets pris sous diff√©rents angles pour rendre le test aussi difficile que possible pour le mod√®le.  En suivant le m√™me sch√©ma, nous avons choisi 11 000 photographies sans rep√®res, plut√¥t compliqu√©es √©galement, et nous avons essay√© de trouver des objets qui ressemblaient beaucoup aux rep√®res dans notre base de donn√©es. <br><br>  Pour √©valuer la qualit√© de l'apprentissage, nous mesurons la pr√©cision de notre mod√®le √† l'aide de photos avec et sans rep√®res.  Ce sont nos deux principales mesures. <br><br><h2>  Approches existantes </h2><br>  Il existe relativement peu d'informations sur la reconnaissance des points de rep√®re dans la litt√©rature.  La plupart des solutions sont bas√©es sur des fonctionnalit√©s locales.  L'id√©e principale est que nous avons une image de requ√™te et une image de la base de donn√©es.  Les caract√©ristiques locales - points cl√©s - sont trouv√©es puis mises en correspondance.  Si le nombre de correspondances est suffisamment important, nous concluons que nous avons trouv√© un point de rep√®re. <br><br>  Actuellement, la meilleure m√©thode est le DELF (fonctionnalit√©s locales approfondies) propos√© par Google, qui combine des fonctionnalit√©s locales correspondant √† un apprentissage en profondeur.  En ayant une image d'entr√©e trait√©e par le r√©seau convolutionnel, nous obtenons certaines fonctionnalit√©s DELF. <br><br><img src="https://habrastorage.org/webt/i9/-5/g-/i9-5g-dj0fkjpxlgnjpwaadwyec.jpeg"><br><br>  Comment fonctionne la reconnaissance des points de rep√®re?  Nous avons une banque de photos et une image d'entr√©e, et nous voulons savoir si elle montre un point de rep√®re ou non.  En ex√©cutant le r√©seau DELF de toutes les photos, les fonctionnalit√©s correspondantes pour la base de donn√©es et l'image d'entr√©e peuvent √™tre obtenues.  Ensuite, nous effectuons une recherche par la m√©thode du plus proche voisin et obtenons des images candidates avec des caract√©ristiques en sortie.  Nous utilisons la v√©rification g√©om√©trique pour faire correspondre les caract√©ristiques: en cas de succ√®s, nous concluons que l'image montre un point de rep√®re. <br><br><h2>  R√©seau de neurones convolutifs </h2><br>  La pr√©-formation est cruciale pour le Deep Learning.  Nous avons donc utilis√© une base de donn√©es de sc√®nes pour pr√©-former notre r√©seau de neurones.  Pourquoi de cette fa√ßon?  Une sc√®ne est un objet multiple qui comprend un grand nombre d'autres objets.  Landmark est une instance d'une sc√®ne.  En pr√©-entra√Ænant le mod√®le avec une telle base de donn√©es, nous pouvons lui donner une id√©e de certaines fonctionnalit√©s de bas niveau qui peuvent ensuite √™tre g√©n√©ralis√©es pour une reconnaissance r√©ussie des points de rep√®re. <br><br>  Nous avons utilis√© un r√©seau neuronal de la famille des r√©seaux r√©siduels comme mod√®le.  La diff√©rence critique de ces r√©seaux est qu'ils utilisent un bloc r√©siduel qui inclut une connexion par saut qui permet √† un signal de sauter par-dessus des couches avec des poids et de passer librement.  Une telle architecture permet de former des r√©seaux profonds avec un haut niveau de qualit√© et de contr√¥ler l'effet de gradient de fuite, ce qui est essentiel pour la formation. <br><br>  Notre mod√®le est Wide ResNet-50-2, une version de ResNet-50 o√π le nombre de convolutions dans le bloc d'√©tranglement interne est doubl√©. <br><br><img src="https://habrastorage.org/webt/s0/mg/ra/s0mgravn9tobelwyraas7v-umfi.jpeg"><br><br>  Le r√©seau fonctionne tr√®s bien.  Nous l'avons test√© avec notre base de donn√©es de sc√®nes, et voici les r√©sultats: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Mod√®le <br></th><th>  Top 1 err <br></th><th>  Top 5 des erreurs <br></th></tr><tr><td>  ResNet-50 <br></td><td>  46,1% <br></td><td>  15,7% <br></td></tr><tr><td>  ResNet-200 <br></td><td>  42,6% <br></td><td>  12,9% <br></td></tr><tr><td>  SE-ResNext-101 <br></td><td>  42% <br></td><td>  12,1% <br></td></tr><tr><td>  WRN-50-2 (rapide!) <br></td><td>  41,8% <br></td><td>  11,8% <br></td></tr></tbody></table></div><br>  Wide ResNet fonctionnait presque deux fois plus vite que ResNet-200.  Apr√®s tout, c'est la vitesse de rotation qui est cruciale pour la production.  Compte tenu de toutes ces consid√©rations, nous avons choisi Wide ResNet-50-2 comme notre principal r√©seau de neurones. <br><br><h2>  La formation </h2><br>  Nous avons besoin d'une fonction de perte pour former notre r√©seau.  Nous avons d√©cid√© d'utiliser l'approche d'apprentissage m√©trique pour le choisir: un r√©seau de neurones est form√© de sorte que les √©l√©ments de la m√™me classe affluent vers un cluster, tandis que les clusters pour diff√©rentes classes doivent √™tre espac√©s autant que possible.  Pour les points de rep√®re, nous avons utilis√© la perte centrale qui tire les √©l√©ments d'une classe vers un centre.  Une caract√©ristique importante de cette approche est qu'elle ne n√©cessite pas d'√©chantillonnage n√©gatif, ce qui devient une chose assez difficile √† faire √† des √©poques ult√©rieures. <br><br><img src="https://habrastorage.org/webt/ix/xd/if/ixxdifizq_hbvfbz6vloiqc_ppk.jpeg"><br><br>  N'oubliez pas que nous avons n classes de points de rep√®re et une autre classe ¬´non-rep√®re¬ª pour laquelle la perte centrale n'est pas utilis√©e.  Nous supposons qu'un point de rep√®re est un seul et m√™me objet, et qu'il a une structure, il est donc logique de d√©terminer son centre.  Quant au non-rep√®re, il peut se r√©f√©rer √† n'importe quoi, donc cela n'a aucun sens de d√©terminer son centre. <br><br>  Nous avons ensuite mis tout cela ensemble, et il y a notre mod√®le de formation.  Il comprend trois parties principales: <br><br><ul><li>  Large r√©seau neuronal convolutionnel ResNet 50-2 pr√©-form√© avec une base de donn√©es de sc√®nes; </li><li>  Partie d‚Äôincorporation comprenant une couche enti√®rement connect√©e et une couche normalis√©e par lots; </li><li>  Classificateur qui est une couche enti√®rement connect√©e, suivie d'une paire compos√©e de la perte Softmax et de la perte centrale. </li></ul><br><img src="https://habrastorage.org/webt/pt/k0/g_/ptk0g_0cyy3qgfw0wp8d4zqfd3k.jpeg"><br><br>  Comme vous vous en souvenez, notre base de donn√©es est divis√©e en 4 parties par r√©gion.  Nous utilisons ces 4 parties dans un paradigme d'apprentissage curriculaire.  Nous avons un ensemble de donn√©es actuel, et √† chaque √©tape de l'apprentissage, nous ajoutons une autre partie du monde pour obtenir un nouvel ensemble de donn√©es pour la formation. <br><br>  Le mod√®le comprend trois parties et nous utilisons un taux d'apprentissage sp√©cifique pour chacune dans le processus de formation.  Cela est n√©cessaire pour que le r√©seau puisse √† la fois apprendre les points de rep√®re d'une nouvelle partie de l'ensemble de donn√©es que nous avons ajout√©e et se souvenir des donn√©es d√©j√† apprises.  De nombreuses exp√©riences ont prouv√© que cette approche √©tait la plus efficace. <br><br>  Nous avons donc form√© notre mod√®le.  Maintenant, nous devons comprendre comment cela fonctionne.  Utilisons la carte d'activation de classe pour trouver la partie de l'image √† laquelle notre r√©seau de neurones r√©agit le plus facilement.  L'image ci-dessous montre les images d'entr√©e dans la premi√®re ligne, et les m√™mes images superpos√©es avec la carte d'activation de classe du r√©seau que nous avons form√©e √† l'√©tape pr√©c√©dente sont affich√©es dans la deuxi√®me ligne. <br><br><img src="https://habrastorage.org/webt/_e/p6/x0/_ep6x0-7sjfjfkrmyyrxfhlunsq.jpeg"><br><br>  La carte thermique montre quelles parties de l'image sont les plus fr√©quent√©es par le r√©seau.  Comme le montre la carte d'activation de classe, notre r√©seau de neurones a appris avec succ√®s le concept de point de rep√®re. <br><br><h2>  Inf√©rence </h2><br>  Maintenant, nous devons utiliser ces connaissances d'une mani√®re ou d'une autre pour faire avancer les choses.  Puisque nous avons utilis√© la perte de centre pour la formation, dans le cas de l'inf√©rence, il semble tout √† fait logique de d√©terminer √©galement les centro√Ødes pour les points de rep√®re. <br><br>  Pour ce faire, nous prenons une partie des images de l'ensemble d'entra√Ænement pour un monument, disons le Cavalier de bronze √† Saint-P√©tersbourg.  Ensuite, nous les faisons traiter par le r√©seau, obtenons les plongements, calculons la moyenne et d√©rivons un centro√Øde. <br><br><img src="https://habrastorage.org/webt/zy/di/6f/zydi6frte3yxetvtnnkwak2t9y4.jpeg"><br><br>  Cependant, voici une question: combien de centro√Ødes par point de rep√®re il est logique de d√©river?  Au d√©but, il semblait clair et logique de dire: un centro√Øde.  Pas exactement, comme il s'est av√©r√©.  Nous avons d'abord d√©cid√© de faire un seul centro√Øde aussi, et le r√©sultat n'√©tait pas mauvais.  Alors pourquoi plusieurs centro√Ødes? <br><br>  Premi√®rement, les donn√©es dont nous disposons ne sont pas aussi propres.  Bien que nous ayons nettoy√© l'ensemble de donn√©es, nous n'avons supprim√© que les donn√©es √©videntes sur les d√©chets.  Cependant, il pourrait toujours y avoir des images qui ne sont pas manifestement des d√©chets mais qui nuisent au r√©sultat. <br><br>  Par exemple, j'ai un palais d'hiver de classe historique √† Saint-P√©tersbourg.  Je veux en d√©river un centro√Øde.  Cependant, son jeu de donn√©es comprend des photos de la place du Palais et de l'arc du si√®ge g√©n√©ral, car ces objets sont proches les uns des autres.  Si le centro√Øde doit √™tre d√©termin√© pour toutes les images, le r√©sultat ne sera pas aussi stable.  Ce que nous devons faire est de regrouper d'une mani√®re ou d'une autre leurs incorporations d√©riv√©es du r√©seau de neurones, de ne prendre que le centro√Øde qui traite avec le Palais d'Hiver et de faire la moyenne en utilisant les donn√©es r√©sultantes. <br><br><img src="https://habrastorage.org/webt/do/2n/lz/do2nlzvg9awjggqsodeuxn9hz3o.jpeg"><br><br>  Deuxi√®mement, les photographies peuvent avoir √©t√© prises sous diff√©rents angles. <br><br>  Voici un exemple d'un tel comportement illustr√© par le Beffroi de Bruges.  Deux centro√Ødes ont √©t√© d√©riv√©s pour cela.  Dans la rang√©e sup√©rieure de l'image, il y a ces photos qui sont plus proches du premier centro√Øde, et dans la deuxi√®me rang√©e - celles qui sont plus proches du deuxi√®me centro√Øde. <br><br><img src="https://habrastorage.org/webt/34/uo/n5/34uon5ifrysj8l9xlh3wwykzw6o.jpeg"><br><br>  Le premier centro√Øde traite de photographies plus ¬´grandioses¬ª qui ont √©t√© prises √† courte distance au march√© de Bruges.  Le deuxi√®me centro√Øde traite de photographies prises √† distance dans des rues particuli√®res. <br><br>  En fait, en d√©rivant plusieurs centro√Ødes par classe de points de rep√®re, nous pouvons r√©fl√©chir √† l'inf√©rence de diff√©rents angles de cam√©ra pour ce point de rep√®re. <br><br>  Alors, comment obtenir ces ensembles pour d√©river les centro√Ødes?  Nous appliquons un regroupement hi√©rarchique (lien complet) aux ensembles de donn√©es pour chaque point de rep√®re.  Nous l'utilisons pour trouver des clusters valides dont les centro√Ødes doivent √™tre d√©riv√©s.  Par grappes valides, nous entendons celles comprenant au moins 50 photographies √† la suite de grappes.  Les autres clusters sont rejet√©s.  En cons√©quence, nous avons obtenu environ 20% des rep√®res avec plus d'un centro√Øde. <br><br>  Passons maintenant √† l'inf√©rence.  Il est obtenu en deux √©tapes: premi√®rement, nous alimentons l'image d'entr√©e √† notre r√©seau de neurones convolutionnels et obtenons l'int√©gration, puis nous associons l'int√©gration avec les centro√Ødes en utilisant le produit scalaire.  Si les images ont des donn√©es g√©ographiques, nous limitons la recherche aux centro√Ødes, qui se r√©f√®rent aux points de rep√®re situ√©s dans un carr√© de 1 x 1 km de l'emplacement de l'image.  Cela permet une recherche plus pr√©cise et un seuil inf√©rieur pour la correspondance ult√©rieure.  Si la distance r√©sultante d√©passe le seuil qui est un param√®tre de l'algorithme, alors nous concluons qu'une photo a un point de rep√®re avec la valeur maximale du produit scalaire.  Si c'est moins, alors c'est une photo sans rep√®re. <br><br><img src="https://habrastorage.org/webt/mi/pl/os/miplosde7vmgj1ty5qlsjzabc2u.png"><br><br>  Supposons qu'une photo ait un rep√®re.  Si nous avons des donn√©es g√©ographiques, nous les utilisons et obtenons une r√©ponse.  Si les donn√©es g√©ographiques ne sont pas disponibles, nous effectuons une v√©rification suppl√©mentaire.  Lorsque nous nettoyions l'ensemble de donn√©es, nous avons cr√©√© un ensemble d'images de r√©f√©rence pour chaque classe.  Nous pouvons d√©terminer les plongements pour eux, puis obtenir la distance moyenne entre eux et l'incorporation de l'image de requ√™te.  S'il d√©passe un certain seuil, la v√©rification est r√©ussie et nous introduisons des m√©tadonn√©es et obtenons un r√©sultat.  Il est important de noter que nous pouvons ex√©cuter cette proc√©dure pour plusieurs points de rep√®re qui ont √©t√© trouv√©s dans une image. <br><br><img src="https://habrastorage.org/webt/rw/kd/ht/rwkdhtwi78ko9fohfgj2dex-lro.png"><br><br><h2>  R√©sultats des tests </h2><br>  Nous avons compar√© notre mod√®le avec DELF, pour lequel nous avons pris des param√®tres avec lesquels il montrerait les meilleures performances dans notre test.  Les r√©sultats sont presque identiques. <br><br><div class="scrollable-table"><table><tbody><tr><th>  Mod√®le <br></th><th>  Rep√®re <br></th><th>  Non historique <br></th></tr><tr><td>  Notre mod√®le <br></td><td>  80% <br></td><td>  99% <br></td></tr><tr><td>  Delf <br></td><td>  80,1% <br></td><td>  99% <br></td></tr></tbody></table></div><br>  Nous avons ensuite class√© les rep√®res en deux types: fr√©quents (plus de 100 photographies dans la base de donn√©es), qui repr√©sentaient 87% de tous les rep√®res du test, et rares.  Notre mod√®le fonctionne bien avec les plus fr√©quents: 85,3% de pr√©cision.  Avec des points de rep√®re rares, nous avions 46%, ce qui n'√©tait pas mal non plus du tout, ce qui signifie que notre approche fonctionnait assez bien m√™me avec peu de donn√©es. <br><br><div class="scrollable-table"><table><tbody><tr><th>  Tapez <br></th><th>  Pr√©cision <br></th><th>  Part du nombre total <br></th></tr><tr><td>  Fr√©quent <br></td><td>  85,3% <br></td><td>  87% <br></td></tr><tr><td>  Rare <br></td><td>  46% <br></td><td>  13% <br></td></tr></tbody></table></div><br>  Ensuite, nous avons ex√©cut√© un test A / B avec des photos d'utilisateurs.  En cons√©quence, le taux de conversion d'achat d'espace cloud a augment√© de 10%, le taux de conversion de d√©sinstallation d'applications mobiles a diminu√© de 3% et le nombre de vues d'album a augment√© de 13%. <br><br>  Comparons notre vitesse √† celle du DELF.  Avec le GPU, DELF n√©cessite 7 ex√©cutions r√©seau car il utilise 7 √©chelles d'image, tandis que notre approche n'en utilise que 1. Avec le CPU, DELF utilise une recherche plus longue par la m√©thode du plus proche voisin et une tr√®s longue v√©rification g√©om√©trique.  Au final, notre m√©thode √©tait 15 fois plus rapide avec le CPU.  Notre approche montre une vitesse plus √©lev√©e dans les deux cas, ce qui est crucial pour la production. <br><br><h2>  R√©sultats: souvenirs de vacances </h2><br>  Au d√©but de cet article, j'ai mentionn√© une solution pour faire d√©filer et trouver les images de rep√®re souhait√©es.  √áa y est. <br><br><img src="https://habrastorage.org/webt/ps/x_/7x/psx_7x8ptraq4s_tjodrfhi3g6o.jpeg"><br><br>  C'est mon nuage o√π toutes les photos sont class√©es dans des albums.  Il existe des albums ¬´People¬ª, ¬´Objects¬ª et ¬´Attractions¬ª.  Dans l'album Attractions, les points de rep√®re sont class√©s en albums regroup√©s par ville.  Un clic sur Dresdner Zwinger ouvre un album avec des photos de ce monument seulement. <br><br><img src="https://habrastorage.org/webt/sf/qa/bz/sfqabzwvyehdtq9nv4ko85rbnnu.jpeg" width="400"><br><br>  Une fonctionnalit√© pratique: vous pouvez partir en vacances, prendre des photos et les stocker dans votre cloud.  Plus tard, lorsque vous souhaitez les t√©l√©charger sur Instagram ou les partager avec vos amis et votre famille, vous n'aurez pas √† chercher et √† choisir trop longtemps - les photos souhait√©es seront disponibles en quelques clics. <br><br><h2>  Conclusions </h2><br>  Permettez-moi de vous rappeler les principales caract√©ristiques de notre solution. <br><br><ol><li>  Nettoyage de base de donn√©es semi-automatique.  Un peu de travail manuel est n√©cessaire pour la cartographie initiale, puis le r√©seau neuronal fera le reste.  Cela permet de nettoyer rapidement les nouvelles donn√©es et de les utiliser pour recycler le mod√®le. </li><li>  Nous utilisons des r√©seaux de neurones convolutionnels profonds et un apprentissage m√©trique profond qui nous permet d'apprendre efficacement la structure en classe. </li><li>  Nous avons utilis√© l'apprentissage curriculaire, c'est-√†-dire la formation en plusieurs parties, comme paradigme de formation.  Cette approche nous a √©t√© tr√®s utile.  Nous utilisons plusieurs centro√Ødes sur l'inf√©rence, qui permettent d'utiliser des donn√©es plus propres et de trouver diff√©rentes vues des points de rep√®re. </li></ol><br>  Il peut sembler que la reconnaissance d'objets est une t√¢che banale.  Cependant, en explorant les besoins r√©els des utilisateurs, nous d√©couvrons de nouveaux d√©fis comme la reconnaissance des points de rep√®re.  Cette technique permet de dire aux gens quelque chose de nouveau sur le monde en utilisant des r√©seaux de neurones.  C'est tr√®s encourageant et motivant! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr467905/">https://habr.com/ru/post/fr467905/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr467891/index.html">FAQ sur la signature [√©lectronique] du cloud</a></li>
<li><a href="../fr467893/index.html">Juste un autre wrapper Qt pour gRPC et protobuf</a></li>
<li><a href="../fr467895/index.html">Quels mod√®les les r√©seaux de neurones trouvent-ils?</a></li>
<li><a href="../fr467897/index.html">Outils de test automatique, int√©gration de Yandex Mapkit 3, design sympa et approche d'interface utilisateur pilot√©e par le serveur - Annonce Mitap Android</a></li>
<li><a href="../fr467901/index.html">R√©futez quatre st√©r√©otypes sur le langage de programmation Rust</a></li>
<li><a href="../fr467909/index.html">Chat sur iOS: √† l'aide de sockets</a></li>
<li><a href="../fr467913/index.html">Comment am√©liorer le ¬´min√©ral b√¢tard¬ª, ou la nouvelle interface pour le panneau solaire</a></li>
<li><a href="../fr467917/index.html">Mod√®les de gestion</a></li>
<li><a href="../fr467921/index.html">Sortez les stylos poussi√©reux: l'√©criture manuscrite est bonne pour le cerveau</a></li>
<li><a href="../fr467923/index.html">Vous souhaitez donc devenir analyste dans le domaine de la s√©curit√© des r√©seaux ...</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>