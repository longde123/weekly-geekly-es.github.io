<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéì üôÖüèø üíÖüèø ¬øQu√© patrones encuentran las redes neuronales? ü•Ö üë¶üèæ ü¶â</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En esta publicaci√≥n quiero hablar sobre los patrones que las redes neuronales pueden encontrar. Muchas gu√≠as para principiantes se centran en la t√©cni...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øQu√© patrones encuentran las redes neuronales?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467895/">  En esta publicaci√≥n quiero hablar sobre los patrones que las redes neuronales pueden encontrar.  Muchas gu√≠as para principiantes se centran en la t√©cnica de escribir c√≥digo para redes neuronales, mientras que las preguntas de "l√≥gica" (¬øqu√© pueden hacer las redes neuronales? ¬øQu√© arquitecturas son m√°s adecuadas para qu√© tareas y por qu√©?) A menudo permanecen al margen.  Espero que mi publicaci√≥n ayude a los principiantes a comprender mejor las capacidades de las redes neuronales.  Para hacer esto, trataremos de ver c√≥mo hacen frente a algunas tareas modelo.  El c√≥digo de muestra se proporcionar√° en python utilizando la biblioteca keras. <br><br>  <b>Tarea 1.</b> Comencemos con una simple.  Construimos una red neuronal que se aproxima al seno. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, np.pi, n) y = np.sin(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n = <span class="hljs-number"><span class="hljs-number">40</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">6</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">4</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">1000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">4</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=np.pi, num=<span class="hljs-number"><span class="hljs-number">500</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Sinus approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre> <br>  Obtenemos el siguiente cuadro: <br><br><img src="https://habrastorage.org/webt/t3/xv/_o/t3xv_ocq-o9m8yupmxxdrfxqgai.png" width="500" height="500"><br><br>  Como puede ver, la red neuronal hizo frente con √©xito a la tarea de aproximar una funci√≥n simple. <br><a name="habracut"></a><br>  <b>Tarea 2.</b> Veamos c√≥mo la red neuronal har√° frente a una tarea m√°s compleja.  Ingresaremos valores de x uniformemente distribuidos en el intervalo [0, 1], e y se establecer√° aleatoriamente: para x &lt;0.6, y ser√° una variable aleatoria que tomar√° el valor 0 con una probabilidad de 0.75 y 1 con una probabilidad de 0.25 (es decir, un valor aleatorio binomial con p = 0,25).  Para x&gt; 0.6, y ser√° una variable aleatoria que tomar√° el valor 0 con probabilidad 0.3 y el valor 1 con probabilidad 0.7.  Como una funci√≥n optimizada, tomamos el error est√°ndar. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, n) y0 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.25</span></span>) y1 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.7</span></span>) y = np.where(X &lt; <span class="hljs-number"><span class="hljs-number">0.6</span></span>, y0, y1) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n_inputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n_hidden1 = <span class="hljs-number"><span class="hljs-number">100</span></span> n_hidden2 = <span class="hljs-number"><span class="hljs-number">50</span></span> n_outputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n = <span class="hljs-number"><span class="hljs-number">2000</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(n_hidden1, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(n_hidden2, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">200</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">100</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=<span class="hljs-number"><span class="hljs-number">1</span></span>, num=<span class="hljs-number"><span class="hljs-number">100</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Binomial distribution approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre><br>  Obtenemos el siguiente gr√°fico de una red neuronal de funci√≥n aproximada: <br><br><img src="https://habrastorage.org/webt/y_/rp/lo/y_rplovhyaxioena0tb8iueed9m.png" width="500" height="500"><br><br>  Como puede ver, la red neuronal se aproxim√≥ a la expectativa matem√°tica de nuestra variable aleatoria y.  Entonces, las redes neuronales pueden (en principio) aproximar los valores promedio de variables aleatorias que dependen de los par√°metros.  Por ejemplo, podemos esperar que resuelvan el siguiente problema: las personas con ingresos de hasta $ 1,000 est√°n en promedio infelices, y las personas con ingresos superiores a $ 1,000 est√°n en promedio felices;  uno debe aprender a predecir el "nivel de felicidad" dependiendo de los ingresos.  La red neuronal podr√° encontrar la dependencia del nivel promedio de felicidad en los ingresos, a pesar del hecho de que entre las personas con cualquier nivel de ingresos hay tanto felices como infelices. <br><br>  <b>Problema 3.</b> Ahora pasamos a la predicci√≥n de secuencias.  Consideraremos secuencias de 0 y 1 dadas por la siguiente regla: 10 miembros - equiparablemente 0 o 1, y el und√©cimo es igual a 1 si el t√©rmino anterior es 0, e igualmente probable 0 o 1 si el t√©rmino anterior 1. Generaremos tales secuencias de longitud 11 (10 entradas miembros de secuencia y uno, el √∫ltimo, predecimos) y entrenarlos en nuestra red neuronal recurrente.  Y despu√©s del entrenamiento, verifiquemos c√≥mo se las arregla con la predicci√≥n de nuevas secuencias (tambi√©n longitud 11). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(m, n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.binomial(size=(m,n), n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y0 = np.ones(m) y1 = np.random.binomial(size=m, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y = np.where(X[:, n<span class="hljs-number"><span class="hljs-number">-1</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>, y0, y1) X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">50</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">20</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) m_test = <span class="hljs-number"><span class="hljs-number">12</span></span> n_test = <span class="hljs-number"><span class="hljs-number">10</span></span> X_test, y_test = get_X_y(m_test, n_test) y_predicted = model.predict(X_test) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(m_test): print(<span class="hljs-string"><span class="hljs-string">"x_last="</span></span>, X_test[i, n_test<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-string"><span class="hljs-string">"y_predicted="</span></span>, y_predicted[i, <span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br>  Veamos qu√© pron√≥sticos ofrece nuestra red neuronal sobre las secuencias probadas (tendr√° resultados diferentes, ya que existe una coincidencia en la elecci√≥n de secuencias y en el entrenamiento de la red neuronal). <br><br><div class="scrollable-table"><table><tbody><tr><th>  N√∫mero de secuencia </th><th>  Pen√∫ltimo miembro de la secuencia </th><th>  Valor predicho </th></tr><tr><td>  1 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  2 </td><td>  0 0 </td><td>  0,95 </td></tr><tr><td>  3 </td><td>  0 0 </td><td>  0,97 </td></tr><tr><td>  4 4 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  5 5 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  6 6 </td><td>  1 </td><td>  0,45 </td></tr><tr><td>  7 7 </td><td>  0 0 </td><td>  0,94 </td></tr><tr><td>  8 </td><td>  1 </td><td>  0,50 </td></tr><tr><td>  9 9 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  10 </td><td>  1 </td><td>  0,42 </td></tr><tr><td>  11 </td><td>  1 </td><td>  0,44 </td></tr><tr><td>  12 </td><td>  0 0 </td><td>  0,92 </td></tr></tbody></table></div><br><br>  Como puede ver, si el pen√∫ltimo miembro de la secuencia es 0, entonces la red neuronal predice un valor cercano a 1, y si es 1, entonces un valor cercano a 0.5.  Esto est√° cerca del pron√≥stico √≥ptimo.  Un ejemplo similar de "vida" podr√≠a verse as√≠: "si hoy voy al cine, ma√±ana almorzar√© en un restaurante;  si voy al teatro hoy, ma√±ana almorzar√© en cualquier lugar ".  Como hemos visto, una red neuronal puede detectar patrones de este tipo y predecir un viaje a un restaurante al ir al cine (y al cine para predecir "algo intermedio"). <br><br>  <b>Tarea 4.</b> Complicamos la tarea de la red neuronal.  Deje que todo sea como en el ejemplo anterior, solo el und√©cimo miembro de la secuencia estar√° determinado no por el anterior, sino por el segundo miembro de la secuencia (por la misma regla).  No daremos el c√≥digo aqu√≠, ya que pr√°cticamente no difiere del anterior.  Mi experimento mostr√≥ que la red neuronal todav√≠a encuentra un patr√≥n, pero durante m√°s tiempo (tuve que usar 100 √©pocas en lugar de 20 para el entrenamiento). <br>  Por lo tanto, las redes neuronales pueden (nuevamente, en principio, aclarar) detectar dependencias a largo plazo (en nuestro "ejemplo de vida", pueden detectar patrones como "Voy a un restaurante hoy si estuve en una pel√≠cula hace una semana"). <br><br>  <b>Tarea 5.</b> Veamos c√≥mo la red neuronal usa la informaci√≥n disponible para pronosticar. <br>  Para hacer esto, realizaremos capacitaci√≥n sobre secuencias de longitud 4. En total, tendremos 3 secuencias diferentes igualmente probables: <br><br> <code>0, 0, 1, 1 <br> 0, 1, 0, 1 <br> 0, 1, 1, 0</code> <br> <br>  Por lo tanto, despu√©s de la combinaci√≥n inicial de 0, 0, siempre encontramos dos unidades, despu√©s de la combinaci√≥n de 0, 1 es igualmente probable que encontremos 0 o 1, pero sabremos con seguridad el √∫ltimo n√∫mero.  Ahora le pediremos a nuestra red neuronal que devuelva secuencias estableciendo return_sequences = True.  Como las secuencias predichas, tomamos nuestras mismas secuencias desplazadas en un paso y complementadas por cero a la derecha.  Ahora ya podemos suponer lo que suceder√°: en el primer paso, la red neuronal producir√° un n√∫mero cercano a 2/3 (ya que con una probabilidad de 2/3 el segundo t√©rmino es 1), y luego para una combinaci√≥n de 0, 0 producir√° dos n√∫meros cercanos a unidad, pero para 0, 1 primero dar√° un n√∫mero cercano a 0.5, y luego dar√° un n√∫mero cercano a 0 o 1, dependiendo de si obtuvimos la secuencia 0, 1, 0 o 0, 1, 1. Al final de la red neuronal siempre producir√° un n√∫mero cercano a 0. Verificar con el siguiente c√≥digo muestra que nuestras suposiciones son correctas. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) z = np.array([random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n)]) X[z == <span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] y = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) y[:, :<span class="hljs-number"><span class="hljs-number">3</span></span>] = X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>:] X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) y = np.reshape(y, (y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], y.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">20</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">100</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) X_test = np.zeros((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>)) X_test[<span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] X_test = np.reshape(X_test, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) y_predicted = model.predict(X_test) print(y_predicted)</code> </pre><br><br>  A partir de este ejemplo, vemos que la red neuronal puede cambiar din√°micamente el pron√≥stico dependiendo de la informaci√≥n recibida.  Har√≠amos lo mismo, tratando de predecir una cierta secuencia: cuando la informaci√≥n disponible nos permite estimar las probabilidades de resultados en el siguiente paso, predecimos con base en esta informaci√≥n;  pero cuando descubrimos informaci√≥n adicional en el siguiente paso, cambiamos el pron√≥stico dependiendo de ello. <br>  Entonces, si vemos que alguien viene a nosotros desde la oscuridad, entonces decimos "esta es una persona, no lo sabemos con m√°s detalle";  Cuando comenzamos a distinguir el cabello largo en la oscuridad, decimos "esta es probablemente una mujer".  Pero si despu√©s de eso consideramos que una persona tiene bigote, entonces decimos que probablemente sea un hombre (aunque con el pelo largo).  Como hemos visto, una red neuronal act√∫a de manera similar, utilizando la totalidad de la informaci√≥n actualmente disponible para el pron√≥stico. <br><br>  Entonces, observamos ejemplos simples de c√≥mo funcionan las redes neuronales y qu√© patrones pueden encontrar.  En general, vimos que a menudo las redes neuronales se comportan de manera bastante "razonable", haciendo predicciones cercanas a las que har√≠a una persona.  Aunque, debe tenerse en cuenta, para captar patrones simples, necesitan mucha m√°s informaci√≥n que las personas. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467895/">https://habr.com/ru/post/467895/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467883/index.html">Kubernetes 1.16: c√≥mo actualizar y no romper nada</a></li>
<li><a href="../467885/index.html">Producto y minas segmentadas</a></li>
<li><a href="../467887/index.html">Reflexiones c√°usticas realistas</a></li>
<li><a href="../467891/index.html">Preguntas frecuentes sobre la firma en la nube [electr√≥nica]</a></li>
<li><a href="../467893/index.html">Solo otro contenedor Qt para gRPC y protobuf</a></li>
<li><a href="../467897/index.html">Herramientas de prueba autom√°tica, integraci√≥n Yandex Mapkit 3, dise√±o atractivo y enfoque de interfaz de usuario impulsada por el servidor: anuncio de mitap de Android</a></li>
<li><a href="../467903/index.html">Las 20 mejores funciones de navegaci√≥n en IntelliJ IDEA. Parte 1</a></li>
<li><a href="../467905/index.html">C√≥mo hicimos un reconocimiento hist√≥rico en Cloud Mail.ru y por qu√©</a></li>
<li><a href="../467907/index.html">Pros y contras de la subcontrataci√≥n</a></li>
<li><a href="../467913/index.html">C√≥mo mejorar el "mineral bastardo", o la nueva interfaz para el panel solar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>