<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游끡游 游뗻 游똂游낕 N칩mada: problemas y soluciones 游땸 游띴 游꿙</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El primer servicio en Nomad I se lanz칩 en septiembre de 2016. En este momento lo uso como programador y apoyo como administrador de dos clusters Nomad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>N칩mada: problemas y soluciones</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435132/"><p>  El primer servicio en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Nomad</a> I se lanz칩 en septiembre de 2016.  En este momento lo uso como programador y apoyo como administrador de dos clusters Nomad: un "hogar" para mis proyectos personales (6 m치quinas micro-virtuales en Hetzner Cloud y ArubaCloud en 5 centros de datos diferentes en Europa) y el segundo en funcionamiento (alrededor de 40 servidores privados virtuales y f칤sicos en dos centros de datos). </p><br><p>  En el pasado, se ha acumulado bastante experiencia con el entorno de Nomad, en el art칤culo describir칠 los problemas encontrados por Nomad y c칩mo lidiar con ellos. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Yamal Nomad realiza la instancia de entrega continua de su software 춸 National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. El n칰mero de nodos de servidor por centro de datos </h2><br><p>  <strong>Soluci칩n: un nodo de servidor es suficiente para un centro de datos.</strong> </p><br><p>  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci칩n</a> no indica expl칤citamente cu치ntos nodos de servidor se requieren en un centro de datos.  Solo se indica que se necesitan 3-5 nodos por regi칩n, lo cual es l칩gico para el consenso del protocolo de balsa. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  Al principio, plane칠 2-3 nodos de servidor en cada centro de datos para proporcionar redundancia. </p><br><p>  Al usarlo result칩: </p><br><ol><li>  Esto simplemente no es necesario, ya que en el caso de una falla del nodo en el centro de datos, el papel del nodo del servidor para los agentes en este centro de datos ser치 desempe침ado por otros nodos del servidor en la regi칩n. </li><li>  Resulta a칰n peor si el problema 8 no se resuelve.  Cuando el asistente es reelegido, pueden producirse incoherencias y Nomad reiniciar치 parte de los servicios. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Recursos del servidor para el nodo del servidor </h2><br><p>  <strong>Soluci칩n: una peque침a m치quina virtual es suficiente para el nodo del servidor.</strong>  <strong>En el mismo servidor, est치 permitido ejecutar otros servicios que no requieren muchos recursos.</strong> </p><br><p>  El consumo de memoria del demonio Nomad depende del n칰mero de tareas en ejecuci칩n.  Consumo de CPU: basado en la cantidad de tareas y la cantidad de servidores / agentes en la regi칩n (no lineal). </p><br><p>  En nuestro caso: para 300 tareas en ejecuci칩n, el consumo de memoria es de aproximadamente 500 MB para el nodo maestro actual. </p><br><p>  En un cl칰ster de trabajo, una m치quina virtual para un nodo de servidor: 4 CPU, 6 GB de RAM. <br>  Lanzado adicionalmente: Consul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Consenso sobre la falta de centros de datos. </h2><br><p>  <strong>Soluci칩n: creamos tres centros de datos virtuales y tres nodos de servidor para dos centros de datos f칤sicos.</strong> </p><br><p>  El trabajo de Nomad dentro de la regi칩n se basa en el protocolo de la balsa.  Para un funcionamiento correcto, necesita al menos 3 nodos de servidor ubicados en diferentes centros de datos.  Esto permitir치 un funcionamiento correcto con una p칠rdida completa de conectividad de red con uno de los centros de datos. </p><br><p>  Pero solo tenemos dos centros de datos.  Nos comprometemos: seleccionamos un centro de datos, en el que confiamos m치s, y hacemos un nodo de servidor adicional en 칠l.  Hacemos esto mediante la introducci칩n de un centro de datos virtual adicional, que se ubicar치 f칤sicamente en el mismo centro de datos (consulte el subp치rrafo 2 del problema 1). </p><br><p>  <strong>Soluci칩n alternativa: separamos los centros de datos en regiones separadas.</strong> </p><br><p>  Como resultado, los centros de datos funcionan de manera independiente y solo se necesita consenso dentro de un centro de datos.  Dentro de un centro de datos, en este caso es mejor hacer 3 nodos de servidor implementando tres centros de datos virtuales en uno f칤sico. </p><br><p>  Esta opci칩n es menos conveniente para la distribuci칩n de tareas, pero ofrece una garant칤a del 100% de la independencia de los servicios en caso de problemas de red entre centros de datos. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Servidor" y "agente" en el mismo servidor </h2><br><p>  <strong>Soluci칩n: v치lida si tiene un n칰mero limitado de servidores.</strong> </p><br><p>  La documentaci칩n n칩mada dice que hacer esto no es deseable.  Pero si no tiene la oportunidad de asignar m치quinas virtuales separadas para los nodos del servidor, puede colocar el servidor y los nodos de agente en el mismo servidor. </p><br><p>  Ejecutar simult치neamente significa iniciar el daemon Nomad tanto en modo cliente como en modo servidor. </p><br><p>  쯈u칠 amenaza esto?  Con una gran carga en la CPU de este servidor, el nodo del servidor Nomad funcionar치 de manera inestable, se puede perder el consenso y el latido, y los servicios se recargar치n. <br>  Para evitar esto, aumentamos los l칤mites de la descripci칩n del problema No. 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Implementaci칩n de espacios de nombres </h2><br><p>  <strong>Soluci칩n: quiz치s a trav칠s de la organizaci칩n de un centro de datos virtual.</strong> </p><br><p>  A veces necesita ejecutar parte de los servicios en servidores separados. </p><br><p>  La soluci칩n es la primera, simple, pero m치s exigente en recursos.  Dividimos todos los servicios en grupos de acuerdo con su prop칩sito: frontend, back-end, ... Agrega meta atributos a los servidores, prescribe los atributos para ejecutar para todos los servicios. </p><br><p>  La segunda soluci칩n es simple.  Agregamos nuevos servidores, prescribimos meta atributos para ellos, prescribimos estos atributos de lanzamiento a los servicios necesarios, y todos los dem치s servicios prescriben la prohibici칩n de iniciar servidores con este atributo. </p><br><p>  La tercera soluci칩n es complicada.  Creamos un centro de datos virtual: inicie Consul para un nuevo centro de datos, inicie el nodo del servidor Nomad para este centro de datos, sin olvidar el n칰mero de nodos del servidor para esta regi칩n.  Ahora puede ejecutar servicios individuales en este centro de datos virtual dedicado. </p><br><h2 id="6-integraciya-s-vault">  6. Integraci칩n con Vault </h2><br><p>  <strong>Soluci칩n: Evite las dependencias circulares de Nomad &lt;-&gt; Vault.</strong> </p><br><p>  La B칩veda lanzada no deber칤a tener ninguna dependencia de Nomad.  La direcci칩n de la B칩veda registrada en Nomad preferiblemente debe apuntar directamente a la B칩veda, sin capas de equilibradores (pero v치lidos).  La reserva de b칩veda en este caso se puede hacer a trav칠s de DNS - Consul DNS o externa. </p><br><p>  Si los datos de Vault se escriben en los archivos de configuraci칩n de Nomad, Nomad intenta acceder a Vault al inicio.  Si el acceso no es exitoso, Nomad se niega a comenzar. </p><br><p>  Comet칤 un error con una dependencia c칤clica hace mucho tiempo, esto una vez destruy칩 brevemente casi por completo el grupo Nomad.  Vault se lanz칩 correctamente, independientemente de Nomad, pero Nomad mir칩 la direcci칩n de Vault a trav칠s de los equilibradores que se ejecutaban en Nomad.  La reconfiguraci칩n y el reinicio de los nodos del servidor Nomad provocaron un reinicio de los servicios del equilibrador, lo que provoc칩 un error al iniciar los nodos del servidor. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Lanzamiento de importantes servicios estatales. </h2><br><p>  <strong>Soluci칩n: v치lida, pero yo no.</strong> </p><br><p>  쮼s posible ejecutar PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB a trav칠s de Nomad? </p><br><p>  Imagine que tiene un conjunto de servicios importantes, cuyo trabajo est치 vinculado a la mayor칤a de los otros servicios.  Por ejemplo, una base de datos en PostgreSQL / ClickHouse.  O almacenamiento general a corto plazo en Redis Cluster / MongoDB.  O un bus de datos en Redis Cluster / RabbitMQ. </p><br><p>  Todos estos servicios de alguna forma implementan un esquema tolerante a fallas: Stolon / Patroni para PostgreSQL, su propia implementaci칩n de balsa en Redis Cluster, su propia implementaci칩n de cl칰ster en RabbitMQ, MongoDB, ClickHouse. </p><br><p>  S칤, todos estos servicios se pueden iniciar a trav칠s de Nomad con referencia a servidores espec칤ficos, pero 쯣or qu칠? </p><br><p>  Adem치s: facilidad de lanzamiento, un formato de script 칰nico, como otros servicios.  No hay que preocuparse por los guiones ansibles / cualquier otra cosa. </p><br><p>  Menos es un punto adicional de falla, que no ofrece ninguna ventaja.  Personalmente, elimin칠 por completo el cl칰ster Nomad dos veces por varias razones: una vez "en casa", una vez trabajando.  Esto fue en las primeras etapas de la presentaci칩n de Nomad y debido a la negligencia. <br>  Adem치s, Nomad comienza a comportarse mal y reinicia los servicios debido al problema n칰mero 8.  Pero incluso si ese problema se resuelve, el peligro permanece. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. La estabilizaci칩n del trabajo y el servicio se reinicia en una red inestable. </h2><br><p>  <strong>Soluci칩n: use las opciones de ajuste de latidos.</strong> </p><br><p>  De forma predeterminada, Nomad est치 configurado para que cualquier problema de red a corto plazo o carga de CPU provoque la p칠rdida de consenso y la reelecci칩n del asistente o marque el nodo del agente como inaccesible.  Y esto lleva a reinicios espont치neos de servicios y su transferencia a otros nodos. </p><br><p>  Estad칤sticas del cl칰ster "hogar" antes de solucionar el problema: la vida 칰til m치xima del contenedor antes de reiniciar es de aproximadamente 10 d칤as.  Aqu칤, todav칤a tiene la carga de ejecutar el agente y el servidor en el mismo servidor y colocarlo en 5 centros de datos diferentes en Europa, lo que implica una gran carga en la CPU y una red menos estable. </p><br><p>  Estad칤sticas del cl칰ster de trabajo antes de solucionar el problema: la vida 칰til m치xima del contenedor antes de reiniciar es m치s de 2 meses.  Aqu칤 todo es relativamente bueno debido a los servidores separados para los nodos del servidor Nomad y la excelente red entre los centros de datos. </p><br><p>  Valores por defecto </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  A juzgar por el c칩digo: en esta configuraci칩n, los latidos se hacen cada 10 segundos.  Con la p칠rdida de dos latidos, comienza la reelecci칩n del maestro o la transferencia de servicios desde el nodo del agente.  Configuraciones controvertidas, en mi opini칩n.  Los editamos dependiendo de la aplicaci칩n. </p><br><p>  Si tiene todos los servicios ejecut치ndose en varias instancias y son distribuidos por centros de datos, lo m치s probable es que no le importe un largo per칤odo para determinar la inaccesibilidad del servidor (aproximadamente 5 minutos, en el ejemplo a continuaci칩n): hacemos menos frecuente el intervalo de latidos y un per칤odo m치s largo para determinar la inaccesibilidad.  Este es un ejemplo de c칩mo configurar mi cl칰ster de inicio: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  Si tiene una buena conectividad de red, servidores separados para los nodos del servidor, y el per칤odo para determinar la inaccesibilidad del servidor es importante (hay alg칰n servicio ejecut치ndose en una instancia y es importante transferirlo r치pidamente), luego aumente el per칤odo para determinar la inaccesibilidad (heartbeat_grace).  Opcionalmente, puede hacer m치s latidos (disminuyendo min_heartbeat_ttl); esto aumentar치 ligeramente la carga en la CPU.  Ejemplo de configuraci칩n de cl칰ster de trabajo: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Esta configuraci칩n soluciona completamente el problema. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. Inicio de tareas peri칩dicas. </h2><br><p>  <strong>Soluci칩n: Se pueden utilizar los servicios peri칩dicos n칩madas, pero cron es m치s conveniente para el soporte.</strong> </p><br><p>  Nomad tiene la capacidad de lanzar peri칩dicamente el servicio. </p><br><p>  La 칰nica ventaja es la simplicidad de esta configuraci칩n. </p><br><p>  El primer inconveniente es que si el servicio se inicia con frecuencia, ensuciar치 la lista de tareas.  Por ejemplo, al inicio cada 5 minutos, se agregar치n 12 tareas adicionales a la lista cada hora, hasta que se active el GC Nomad, lo que eliminar치 las tareas anteriores. </p><br><p>  El segundo inconveniente: no est치 claro c칩mo configurar adecuadamente la supervisi칩n de dicho servicio.  쮺칩mo entender que un servicio comienza, cumple y hace su trabajo hasta el final? </p><br><p>  Como resultado, para m칤, llegu칠 a la implementaci칩n "cron" de tareas peri칩dicas: </p><br><ol><li>  Puede ser un cron regular en un contenedor que se ejecuta constantemente.  Cron ejecuta peri칩dicamente un cierto script.  Se agrega f치cilmente una comprobaci칩n de estado del script a dicho contenedor, que verifica cualquier indicador que cree un script en ejecuci칩n. </li><li>  Puede ser un contenedor en ejecuci칩n constante, con un servicio en ejecuci칩n constante.  Ya se ha implementado un lanzamiento peri칩dico dentro del servicio.  Se puede agregar f치cilmente un script-healthcheck similar o http-healthcheck a dicho servicio, que verifica el estado inmediatamente por su "interior". </li></ol><br><p>  En el momento en que escribo la mayor parte del tiempo en Go, respectivamente, prefiero la segunda opci칩n con http healthcheck: on Go y el lanzamiento peri칩dico, y http healthcheck'i se agregan con algunas l칤neas de c칩digo. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Prestaci칩n de servicios redundantes. </h2><br><p>  <strong>Soluci칩n: no existe una soluci칩n simple.</strong>  <strong>Hay dos opciones m치s dif칤ciles.</strong> </p><br><p>  El esquema de aprovisionamiento proporcionado por los desarrolladores de Nomad es para admitir la cantidad de servicios en ejecuci칩n.  Dices que el n칩mada "lanzame 5 instancias del servicio" y 칠l las inicia en alg칰n lugar all칤.  No hay control sobre la distribuci칩n.  Las instancias pueden ejecutarse en el mismo servidor. </p><br><p>  Si el servidor falla, las instancias se transfieren a otros servidores.  Mientras se transfieren las instancias, el servicio no funciona.  Esta es una opci칩n de provisi칩n de reserva incorrecta. </p><br><p>  Lo hacemos bien: </p><br><ol><li>  Distribuimos instancias en servidores a trav칠s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">distinct_hosts</a> . </li><li>  Distribuimos instancias en centros de datos.  Desafortunadamente, solo al crear una copia del script del formulario service1, service2 con los mismos contenidos, diferentes nombres y una indicaci칩n del lanzamiento en diferentes centros de datos. </li></ol><br><p>  En Nomad 0.9, aparecer치 una funcionalidad que solucionar치 este problema: ser치 posible distribuir servicios en una proporci칩n porcentual entre servidores y centros de datos. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>Soluci칩n: la interfaz de usuario integrada es terrible, el hashi-ui es hermoso.</strong> </p><br><p>  El cliente de la consola realiza la mayor parte de la funcionalidad requerida, pero a veces desea ver los gr치ficos, presionar los botones ... </p><br><p>  Nomad tiene una interfaz de usuario incorporada.  No es muy conveniente (incluso peor que la consola). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  La 칰nica alternativa que conozco es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hashi-ui</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  De hecho, ahora personalmente necesito el cliente de consola solo para "nomad run".  E incluso esto planea transferir a CI. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Soporte para sobresuscripciones de memoria </h2><br><p>  <strong>Soluci칩n: no.</strong> </p><br><p>  En la versi칩n actual de Nomad, debe especificar un l칤mite de memoria estricto para el servicio.  Si se excede el l칤mite, el servicio ser치 asesinado por OOM Killer. </p><br><p>  La suscripci칩n excesiva es cuando los l칤mites para un servicio se pueden especificar "desde y hacia".  Algunos servicios requieren m치s memoria al inicio que durante el funcionamiento normal.  Algunos servicios pueden consumir m치s memoria de lo habitual por un corto tiempo. </p><br><p>  La elecci칩n de una restricci칩n estricta o suave es un tema de discusi칩n, pero, por ejemplo, Kubernetes permite al programador tomar una decisi칩n.  Desafortunadamente, en las versiones actuales de Nomad no existe tal posibilidad.  Admito que aparecer치 en futuras versiones. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Limpieza del servidor de los servicios Nomad </h2><br><p>  <strong>Soluci칩n:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  A veces "algo sale mal".  En el servidor, mata el nodo del agente y se niega a iniciarse.  O el nodo del agente deja de responder.  O el nodo del agente "pierde" los servicios en este servidor. <br>  Esto a veces sucedi칩 con versiones anteriores de Nomad, ahora esto no sucede, o muy raramente. </p><br><p>  쯈u칠 es lo m치s f치cil en este caso, dado que el servidor de drenaje no producir치 el resultado deseado?  Limpiamos el servidor manualmente: </p><br><ol><li>  Det칠n al agente n칩mada. </li><li>  Desmonta en la montura que crea. </li><li>  Eliminar todos los datos del agente. </li><li>  Eliminamos todos los contenedores filtrando los contenedores de servicio (si los hay). </li><li>  Comenzamos el agente. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. 쮺u치l es la mejor manera de desplegar Nomad? </h2><br><p>  <strong>Soluci칩n: por supuesto, a trav칠s del C칩nsul.</strong> </p><br><p>  El c칩nsul en este caso no es en absoluto una capa adicional, sino un servicio que se adapta org치nicamente a la infraestructura, lo que brinda m치s ventajas que desventajas: DNS, almacenamiento KV, b칰squeda de servicios, monitoreo de la disponibilidad del servicio, la capacidad de intercambiar informaci칩n de manera segura. </p><br><p>  Adem치s, se desarrolla tan f치cilmente como el propio Nomad. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. 쯈u칠 es mejor: n칩mada o kubernetes? </h2><br><p>  <strong>Soluci칩n: depende de ...</strong> </p><br><p>  Anteriormente, a veces ten칤a la idea de comenzar una migraci칩n a Kubernetes; estaba tan molesto por el reinicio peri칩dico y espont치neo de los servicios (vea el problema n칰mero 8).  Pero despu칠s de una soluci칩n completa al problema, puedo decir: Nomad me conviene en este momento. </p><br><p>  Por otro lado: Kubernetes tambi칠n tiene una recarga de servicios semi-espont치nea, cuando el planificador de Kubernetes redistribuye instancias dependiendo de la carga.  Esto no es muy bueno, pero es muy probable que est칠 configurado. </p><br><p>  Ventajas de Nomad: la infraestructura es muy f치cil de implementar, scripts simples, buena documentaci칩n, soporte integrado para Consul / Vault, que a su vez brinda: una soluci칩n simple al problema del almacenamiento de contrase침as, DNS incorporado, helchecks f치ciles de configurar. </p><br><p>  Ventajas de Kubernetes: ahora es un "est치ndar de facto".  Buena documentaci칩n, muchas soluciones listas para usar, con una buena descripci칩n y estandarizaci칩n del lanzamiento. </p><br><p>  Desafortunadamente, no tengo la misma gran experiencia en Kubernetes para responder inequ칤vocamente a la pregunta: qu칠 usar para el nuevo cl칰ster.  Depende de las necesidades planificadas. <br>  Si tiene planeados muchos espacios de nombres (problema n칰mero 5) o sus servicios espec칤ficos consumen mucha memoria al principio, luego lib칠relos (problema n칰mero 12), definitivamente Kubernetes, porque  Estos dos problemas en Nomad no est치n completamente resueltos o son inconvenientes. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es435132/">https://habr.com/ru/post/es435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es435120/index.html">Funciones Lambda en SQL ... pensemos</a></li>
<li><a href="../es435122/index.html">C칩mo se implement칩 la llama en Doom en Playstation</a></li>
<li><a href="../es435124/index.html">Obras maestras de la construcci칩n de columnas mundiales: un monitor-transformador de estudio con un n칰mero variable de bandas</a></li>
<li><a href="../es435126/index.html">Experiencia en la organizaci칩n y realizaci칩n de conferencias corporativas para analistas.</a></li>
<li><a href="../es435128/index.html">Pi-Sonos: un pasatiempo fuera de control</a></li>
<li><a href="../es435134/index.html">Simplifique el trabajo con bases de datos en Qt con QSqlRelationalTableModel</a></li>
<li><a href="../es435136/index.html">Sergey y el m칠todo cient칤fico</a></li>
<li><a href="../es435138/index.html">C칩mo tomar el control de su infraestructura de red. Cap칤tulo tres Seguridad de red. Primera parte</a></li>
<li><a href="../es435142/index.html">Rastreo de aprendizaje utilizando eBPF: una gu칤a y ejemplos</a></li>
<li><a href="../es435144/index.html">Introducci칩n a Spring Boot: creaci칩n de una API REST simple en Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>