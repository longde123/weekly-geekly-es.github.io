<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏂🏿 🙈 🙏🏻 Nómada: problemas y soluciones 😱 🛶 🎦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El primer servicio en Nomad I se lanzó en septiembre de 2016. En este momento lo uso como programador y apoyo como administrador de dos clusters Nomad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nómada: problemas y soluciones</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435132/"><p>  El primer servicio en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Nomad</a> I se lanzó en septiembre de 2016.  En este momento lo uso como programador y apoyo como administrador de dos clusters Nomad: un "hogar" para mis proyectos personales (6 máquinas micro-virtuales en Hetzner Cloud y ArubaCloud en 5 centros de datos diferentes en Europa) y el segundo en funcionamiento (alrededor de 40 servidores privados virtuales y físicos en dos centros de datos). </p><br><p>  En el pasado, se ha acumulado bastante experiencia con el entorno de Nomad, en el artículo describiré los problemas encontrados por Nomad y cómo lidiar con ellos. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Yamal Nomad realiza la instancia de entrega continua de su software © National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. El número de nodos de servidor por centro de datos </h2><br><p>  <strong>Solución: un nodo de servidor es suficiente para un centro de datos.</strong> </p><br><p>  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentación</a> no indica explícitamente cuántos nodos de servidor se requieren en un centro de datos.  Solo se indica que se necesitan 3-5 nodos por región, lo cual es lógico para el consenso del protocolo de balsa. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  Al principio, planeé 2-3 nodos de servidor en cada centro de datos para proporcionar redundancia. </p><br><p>  Al usarlo resultó: </p><br><ol><li>  Esto simplemente no es necesario, ya que en el caso de una falla del nodo en el centro de datos, el papel del nodo del servidor para los agentes en este centro de datos será desempeñado por otros nodos del servidor en la región. </li><li>  Resulta aún peor si el problema 8 no se resuelve.  Cuando el asistente es reelegido, pueden producirse incoherencias y Nomad reiniciará parte de los servicios. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Recursos del servidor para el nodo del servidor </h2><br><p>  <strong>Solución: una pequeña máquina virtual es suficiente para el nodo del servidor.</strong>  <strong>En el mismo servidor, está permitido ejecutar otros servicios que no requieren muchos recursos.</strong> </p><br><p>  El consumo de memoria del demonio Nomad depende del número de tareas en ejecución.  Consumo de CPU: basado en la cantidad de tareas y la cantidad de servidores / agentes en la región (no lineal). </p><br><p>  En nuestro caso: para 300 tareas en ejecución, el consumo de memoria es de aproximadamente 500 MB para el nodo maestro actual. </p><br><p>  En un clúster de trabajo, una máquina virtual para un nodo de servidor: 4 CPU, 6 GB de RAM. <br>  Lanzado adicionalmente: Consul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Consenso sobre la falta de centros de datos. </h2><br><p>  <strong>Solución: creamos tres centros de datos virtuales y tres nodos de servidor para dos centros de datos físicos.</strong> </p><br><p>  El trabajo de Nomad dentro de la región se basa en el protocolo de la balsa.  Para un funcionamiento correcto, necesita al menos 3 nodos de servidor ubicados en diferentes centros de datos.  Esto permitirá un funcionamiento correcto con una pérdida completa de conectividad de red con uno de los centros de datos. </p><br><p>  Pero solo tenemos dos centros de datos.  Nos comprometemos: seleccionamos un centro de datos, en el que confiamos más, y hacemos un nodo de servidor adicional en él.  Hacemos esto mediante la introducción de un centro de datos virtual adicional, que se ubicará físicamente en el mismo centro de datos (consulte el subpárrafo 2 del problema 1). </p><br><p>  <strong>Solución alternativa: separamos los centros de datos en regiones separadas.</strong> </p><br><p>  Como resultado, los centros de datos funcionan de manera independiente y solo se necesita consenso dentro de un centro de datos.  Dentro de un centro de datos, en este caso es mejor hacer 3 nodos de servidor implementando tres centros de datos virtuales en uno físico. </p><br><p>  Esta opción es menos conveniente para la distribución de tareas, pero ofrece una garantía del 100% de la independencia de los servicios en caso de problemas de red entre centros de datos. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Servidor" y "agente" en el mismo servidor </h2><br><p>  <strong>Solución: válida si tiene un número limitado de servidores.</strong> </p><br><p>  La documentación nómada dice que hacer esto no es deseable.  Pero si no tiene la oportunidad de asignar máquinas virtuales separadas para los nodos del servidor, puede colocar el servidor y los nodos de agente en el mismo servidor. </p><br><p>  Ejecutar simultáneamente significa iniciar el daemon Nomad tanto en modo cliente como en modo servidor. </p><br><p>  ¿Qué amenaza esto?  Con una gran carga en la CPU de este servidor, el nodo del servidor Nomad funcionará de manera inestable, se puede perder el consenso y el latido, y los servicios se recargarán. <br>  Para evitar esto, aumentamos los límites de la descripción del problema No. 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Implementación de espacios de nombres </h2><br><p>  <strong>Solución: quizás a través de la organización de un centro de datos virtual.</strong> </p><br><p>  A veces necesita ejecutar parte de los servicios en servidores separados. </p><br><p>  La solución es la primera, simple, pero más exigente en recursos.  Dividimos todos los servicios en grupos de acuerdo con su propósito: frontend, back-end, ... Agrega meta atributos a los servidores, prescribe los atributos para ejecutar para todos los servicios. </p><br><p>  La segunda solución es simple.  Agregamos nuevos servidores, prescribimos meta atributos para ellos, prescribimos estos atributos de lanzamiento a los servicios necesarios, y todos los demás servicios prescriben la prohibición de iniciar servidores con este atributo. </p><br><p>  La tercera solución es complicada.  Creamos un centro de datos virtual: inicie Consul para un nuevo centro de datos, inicie el nodo del servidor Nomad para este centro de datos, sin olvidar el número de nodos del servidor para esta región.  Ahora puede ejecutar servicios individuales en este centro de datos virtual dedicado. </p><br><h2 id="6-integraciya-s-vault">  6. Integración con Vault </h2><br><p>  <strong>Solución: Evite las dependencias circulares de Nomad &lt;-&gt; Vault.</strong> </p><br><p>  La Bóveda lanzada no debería tener ninguna dependencia de Nomad.  La dirección de la Bóveda registrada en Nomad preferiblemente debe apuntar directamente a la Bóveda, sin capas de equilibradores (pero válidos).  La reserva de bóveda en este caso se puede hacer a través de DNS - Consul DNS o externa. </p><br><p>  Si los datos de Vault se escriben en los archivos de configuración de Nomad, Nomad intenta acceder a Vault al inicio.  Si el acceso no es exitoso, Nomad se niega a comenzar. </p><br><p>  Cometí un error con una dependencia cíclica hace mucho tiempo, esto una vez destruyó brevemente casi por completo el grupo Nomad.  Vault se lanzó correctamente, independientemente de Nomad, pero Nomad miró la dirección de Vault a través de los equilibradores que se ejecutaban en Nomad.  La reconfiguración y el reinicio de los nodos del servidor Nomad provocaron un reinicio de los servicios del equilibrador, lo que provocó un error al iniciar los nodos del servidor. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Lanzamiento de importantes servicios estatales. </h2><br><p>  <strong>Solución: válida, pero yo no.</strong> </p><br><p>  ¿Es posible ejecutar PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB a través de Nomad? </p><br><p>  Imagine que tiene un conjunto de servicios importantes, cuyo trabajo está vinculado a la mayoría de los otros servicios.  Por ejemplo, una base de datos en PostgreSQL / ClickHouse.  O almacenamiento general a corto plazo en Redis Cluster / MongoDB.  O un bus de datos en Redis Cluster / RabbitMQ. </p><br><p>  Todos estos servicios de alguna forma implementan un esquema tolerante a fallas: Stolon / Patroni para PostgreSQL, su propia implementación de balsa en Redis Cluster, su propia implementación de clúster en RabbitMQ, MongoDB, ClickHouse. </p><br><p>  Sí, todos estos servicios se pueden iniciar a través de Nomad con referencia a servidores específicos, pero ¿por qué? </p><br><p>  Además: facilidad de lanzamiento, un formato de script único, como otros servicios.  No hay que preocuparse por los guiones ansibles / cualquier otra cosa. </p><br><p>  Menos es un punto adicional de falla, que no ofrece ninguna ventaja.  Personalmente, eliminé por completo el clúster Nomad dos veces por varias razones: una vez "en casa", una vez trabajando.  Esto fue en las primeras etapas de la presentación de Nomad y debido a la negligencia. <br>  Además, Nomad comienza a comportarse mal y reinicia los servicios debido al problema número 8.  Pero incluso si ese problema se resuelve, el peligro permanece. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. La estabilización del trabajo y el servicio se reinicia en una red inestable. </h2><br><p>  <strong>Solución: use las opciones de ajuste de latidos.</strong> </p><br><p>  De forma predeterminada, Nomad está configurado para que cualquier problema de red a corto plazo o carga de CPU provoque la pérdida de consenso y la reelección del asistente o marque el nodo del agente como inaccesible.  Y esto lleva a reinicios espontáneos de servicios y su transferencia a otros nodos. </p><br><p>  Estadísticas del clúster "hogar" antes de solucionar el problema: la vida útil máxima del contenedor antes de reiniciar es de aproximadamente 10 días.  Aquí, todavía tiene la carga de ejecutar el agente y el servidor en el mismo servidor y colocarlo en 5 centros de datos diferentes en Europa, lo que implica una gran carga en la CPU y una red menos estable. </p><br><p>  Estadísticas del clúster de trabajo antes de solucionar el problema: la vida útil máxima del contenedor antes de reiniciar es más de 2 meses.  Aquí todo es relativamente bueno debido a los servidores separados para los nodos del servidor Nomad y la excelente red entre los centros de datos. </p><br><p>  Valores por defecto </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  A juzgar por el código: en esta configuración, los latidos se hacen cada 10 segundos.  Con la pérdida de dos latidos, comienza la reelección del maestro o la transferencia de servicios desde el nodo del agente.  Configuraciones controvertidas, en mi opinión.  Los editamos dependiendo de la aplicación. </p><br><p>  Si tiene todos los servicios ejecutándose en varias instancias y son distribuidos por centros de datos, lo más probable es que no le importe un largo período para determinar la inaccesibilidad del servidor (aproximadamente 5 minutos, en el ejemplo a continuación): hacemos menos frecuente el intervalo de latidos y un período más largo para determinar la inaccesibilidad.  Este es un ejemplo de cómo configurar mi clúster de inicio: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  Si tiene una buena conectividad de red, servidores separados para los nodos del servidor, y el período para determinar la inaccesibilidad del servidor es importante (hay algún servicio ejecutándose en una instancia y es importante transferirlo rápidamente), luego aumente el período para determinar la inaccesibilidad (heartbeat_grace).  Opcionalmente, puede hacer más latidos (disminuyendo min_heartbeat_ttl); esto aumentará ligeramente la carga en la CPU.  Ejemplo de configuración de clúster de trabajo: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Esta configuración soluciona completamente el problema. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. Inicio de tareas periódicas. </h2><br><p>  <strong>Solución: Se pueden utilizar los servicios periódicos nómadas, pero cron es más conveniente para el soporte.</strong> </p><br><p>  Nomad tiene la capacidad de lanzar periódicamente el servicio. </p><br><p>  La única ventaja es la simplicidad de esta configuración. </p><br><p>  El primer inconveniente es que si el servicio se inicia con frecuencia, ensuciará la lista de tareas.  Por ejemplo, al inicio cada 5 minutos, se agregarán 12 tareas adicionales a la lista cada hora, hasta que se active el GC Nomad, lo que eliminará las tareas anteriores. </p><br><p>  El segundo inconveniente: no está claro cómo configurar adecuadamente la supervisión de dicho servicio.  ¿Cómo entender que un servicio comienza, cumple y hace su trabajo hasta el final? </p><br><p>  Como resultado, para mí, llegué a la implementación "cron" de tareas periódicas: </p><br><ol><li>  Puede ser un cron regular en un contenedor que se ejecuta constantemente.  Cron ejecuta periódicamente un cierto script.  Se agrega fácilmente una comprobación de estado del script a dicho contenedor, que verifica cualquier indicador que cree un script en ejecución. </li><li>  Puede ser un contenedor en ejecución constante, con un servicio en ejecución constante.  Ya se ha implementado un lanzamiento periódico dentro del servicio.  Se puede agregar fácilmente un script-healthcheck similar o http-healthcheck a dicho servicio, que verifica el estado inmediatamente por su "interior". </li></ol><br><p>  En el momento en que escribo la mayor parte del tiempo en Go, respectivamente, prefiero la segunda opción con http healthcheck: on Go y el lanzamiento periódico, y http healthcheck'i se agregan con algunas líneas de código. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Prestación de servicios redundantes. </h2><br><p>  <strong>Solución: no existe una solución simple.</strong>  <strong>Hay dos opciones más difíciles.</strong> </p><br><p>  El esquema de aprovisionamiento proporcionado por los desarrolladores de Nomad es para admitir la cantidad de servicios en ejecución.  Dices que el nómada "lanzame 5 instancias del servicio" y él las inicia en algún lugar allí.  No hay control sobre la distribución.  Las instancias pueden ejecutarse en el mismo servidor. </p><br><p>  Si el servidor falla, las instancias se transfieren a otros servidores.  Mientras se transfieren las instancias, el servicio no funciona.  Esta es una opción de provisión de reserva incorrecta. </p><br><p>  Lo hacemos bien: </p><br><ol><li>  Distribuimos instancias en servidores a través de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">distinct_hosts</a> . </li><li>  Distribuimos instancias en centros de datos.  Desafortunadamente, solo al crear una copia del script del formulario service1, service2 con los mismos contenidos, diferentes nombres y una indicación del lanzamiento en diferentes centros de datos. </li></ol><br><p>  En Nomad 0.9, aparecerá una funcionalidad que solucionará este problema: será posible distribuir servicios en una proporción porcentual entre servidores y centros de datos. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>Solución: la interfaz de usuario integrada es terrible, el hashi-ui es hermoso.</strong> </p><br><p>  El cliente de la consola realiza la mayor parte de la funcionalidad requerida, pero a veces desea ver los gráficos, presionar los botones ... </p><br><p>  Nomad tiene una interfaz de usuario incorporada.  No es muy conveniente (incluso peor que la consola). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  La única alternativa que conozco es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hashi-ui</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  De hecho, ahora personalmente necesito el cliente de consola solo para "nomad run".  E incluso esto planea transferir a CI. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Soporte para sobresuscripciones de memoria </h2><br><p>  <strong>Solución: no.</strong> </p><br><p>  En la versión actual de Nomad, debe especificar un límite de memoria estricto para el servicio.  Si se excede el límite, el servicio será asesinado por OOM Killer. </p><br><p>  La suscripción excesiva es cuando los límites para un servicio se pueden especificar "desde y hacia".  Algunos servicios requieren más memoria al inicio que durante el funcionamiento normal.  Algunos servicios pueden consumir más memoria de lo habitual por un corto tiempo. </p><br><p>  La elección de una restricción estricta o suave es un tema de discusión, pero, por ejemplo, Kubernetes permite al programador tomar una decisión.  Desafortunadamente, en las versiones actuales de Nomad no existe tal posibilidad.  Admito que aparecerá en futuras versiones. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Limpieza del servidor de los servicios Nomad </h2><br><p>  <strong>Solución:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  A veces "algo sale mal".  En el servidor, mata el nodo del agente y se niega a iniciarse.  O el nodo del agente deja de responder.  O el nodo del agente "pierde" los servicios en este servidor. <br>  Esto a veces sucedió con versiones anteriores de Nomad, ahora esto no sucede, o muy raramente. </p><br><p>  ¿Qué es lo más fácil en este caso, dado que el servidor de drenaje no producirá el resultado deseado?  Limpiamos el servidor manualmente: </p><br><ol><li>  Detén al agente nómada. </li><li>  Desmonta en la montura que crea. </li><li>  Eliminar todos los datos del agente. </li><li>  Eliminamos todos los contenedores filtrando los contenedores de servicio (si los hay). </li><li>  Comenzamos el agente. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. ¿Cuál es la mejor manera de desplegar Nomad? </h2><br><p>  <strong>Solución: por supuesto, a través del Cónsul.</strong> </p><br><p>  El cónsul en este caso no es en absoluto una capa adicional, sino un servicio que se adapta orgánicamente a la infraestructura, lo que brinda más ventajas que desventajas: DNS, almacenamiento KV, búsqueda de servicios, monitoreo de la disponibilidad del servicio, la capacidad de intercambiar información de manera segura. </p><br><p>  Además, se desarrolla tan fácilmente como el propio Nomad. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. ¿Qué es mejor: nómada o kubernetes? </h2><br><p>  <strong>Solución: depende de ...</strong> </p><br><p>  Anteriormente, a veces tenía la idea de comenzar una migración a Kubernetes; estaba tan molesto por el reinicio periódico y espontáneo de los servicios (vea el problema número 8).  Pero después de una solución completa al problema, puedo decir: Nomad me conviene en este momento. </p><br><p>  Por otro lado: Kubernetes también tiene una recarga de servicios semi-espontánea, cuando el planificador de Kubernetes redistribuye instancias dependiendo de la carga.  Esto no es muy bueno, pero es muy probable que esté configurado. </p><br><p>  Ventajas de Nomad: la infraestructura es muy fácil de implementar, scripts simples, buena documentación, soporte integrado para Consul / Vault, que a su vez brinda: una solución simple al problema del almacenamiento de contraseñas, DNS incorporado, helchecks fáciles de configurar. </p><br><p>  Ventajas de Kubernetes: ahora es un "estándar de facto".  Buena documentación, muchas soluciones listas para usar, con una buena descripción y estandarización del lanzamiento. </p><br><p>  Desafortunadamente, no tengo la misma gran experiencia en Kubernetes para responder inequívocamente a la pregunta: qué usar para el nuevo clúster.  Depende de las necesidades planificadas. <br>  Si tiene planeados muchos espacios de nombres (problema número 5) o sus servicios específicos consumen mucha memoria al principio, luego libérelos (problema número 12), definitivamente Kubernetes, porque  Estos dos problemas en Nomad no están completamente resueltos o son inconvenientes. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es435132/">https://habr.com/ru/post/es435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es435120/index.html">Funciones Lambda en SQL ... pensemos</a></li>
<li><a href="../es435122/index.html">Cómo se implementó la llama en Doom en Playstation</a></li>
<li><a href="../es435124/index.html">Obras maestras de la construcción de columnas mundiales: un monitor-transformador de estudio con un número variable de bandas</a></li>
<li><a href="../es435126/index.html">Experiencia en la organización y realización de conferencias corporativas para analistas.</a></li>
<li><a href="../es435128/index.html">Pi-Sonos: un pasatiempo fuera de control</a></li>
<li><a href="../es435134/index.html">Simplifique el trabajo con bases de datos en Qt con QSqlRelationalTableModel</a></li>
<li><a href="../es435136/index.html">Sergey y el método científico</a></li>
<li><a href="../es435138/index.html">Cómo tomar el control de su infraestructura de red. Capítulo tres Seguridad de red. Primera parte</a></li>
<li><a href="../es435142/index.html">Rastreo de aprendizaje utilizando eBPF: una guía y ejemplos</a></li>
<li><a href="../es435144/index.html">Introducción a Spring Boot: creación de una API REST simple en Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>