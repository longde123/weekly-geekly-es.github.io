<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèß ü§∂üèø üîª O que os pesquisadores da IA ‚Äã‚Äãpensam sobre os poss√≠veis riscos associados a ela üìö üï∫üèΩ üë©‚Äçüíª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fiquei interessado nos riscos associados √† IA em 2007. Naquela √©poca, a rea√ß√£o da maioria das pessoas a esse t√≥pico era mais ou menos assim: "√â muito ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O que os pesquisadores da IA ‚Äã‚Äãpensam sobre os poss√≠veis riscos associados a ela</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/"> Fiquei interessado nos riscos associados √† IA em 2007.  Naquela √©poca, a rea√ß√£o da maioria das pessoas a esse t√≥pico era mais ou menos assim: "√â muito engra√ßado, volte quando algu√©m que n√£o seja idiota da Internet acreditar√° nele". <br><br>  Nos anos que se seguiram, v√°rias figuras extremamente inteligentes e influentes, incluindo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stephen Hawking</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Elon Musk</a> , compartilharam publicamente suas preocupa√ß√µes sobre os riscos da IA, seguidos por centenas de outros intelectuais, de fil√≥sofos de Oxford a cosm√≥logos do MIT e investidores do Vale do Sil√≠cio .  E estamos de volta. <br><br>  Ent√£o a rea√ß√£o mudou para: "Bem, alguns cientistas e empres√°rios podem acreditar nisso, mas √© improv√°vel que sejam verdadeiros especialistas nesse campo e que sejam realmente versados ‚Äã‚Äãna situa√ß√£o". <br><br>  A partir da√≠ vieram declara√ß√µes como o artigo da Popular Science " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bill Gates tem medo da IA, mas os pesquisadores da IA ‚Äã‚Äãdevem saber</a> ": <br><blockquote>  Tendo conversado com pesquisadores de IA - pesquisadores reais, que dificilmente fazem esses sistemas funcionarem, sem mencionar que funcionam bem, fica claro que eles n√£o t√™m medo de que a superintelig√™ncia subitamente se apodere deles, nem agora nem no futuro .  Apesar de todas as hist√≥rias assustadoras contadas por Mask, os pesquisadores n√£o t√™m pressa em construir salas de prote√ß√£o e autodestrui√ß√£o com uma contagem regressiva. </blockquote><a name="habracut"></a><br>  Ou, como escreveram no Fusion.net no artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Obje√ß√£o sobre rob√¥s assassinos de uma pessoa que est√° realmente desenvolvendo IA</a> ": <br><blockquote>  Andrew Angie desenvolve profissionalmente sistemas de IA.  Ele ministrou um curso de IA em Stanford, desenvolveu a IA no Google e depois se mudou para o mecanismo de busca chin√™s Baidu para continuar seu trabalho na vanguarda da aplica√ß√£o da IA ‚Äã‚Äãem problemas do mundo real.  Ent√£o, quando ele ouve sobre como Elon Musk ou Stephen Hawking - pessoas que n√£o est√£o familiarizadas diretamente com a tecnologia moderna - est√£o falando sobre a IA, que pode potencialmente destruir a humanidade, voc√™ quase pode ouvi-lo cobrindo o rosto com as m√£os. </blockquote><br>  Ramez Naam, da Marginal Revolution, repete aproximadamente a mesma coisa no artigo ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O que os pesquisadores pensam sobre os riscos da IA?</a> ‚Äù: <br><blockquote>  Elon Musk, Stephen Hawking e Bill Gates expressaram recentemente preocupa√ß√µes de que o desenvolvimento da IA ‚Äã‚Äãpossa implementar o cen√°rio de ‚Äúassassino da IA‚Äù e potencialmente levar √† extin√ß√£o da humanidade.  Eles n√£o s√£o pesquisadores de IA e, at√© onde eu sei, eles n√£o trabalharam diretamente com IA.  O que os pesquisadores de IA real pensam sobre os riscos da IA? </blockquote><br>  Ele cita as palavras de pesquisadores de IA especialmente selecionados, como os autores de outras hist√≥rias - e depois para, sem mencionar opini√µes diferentes disso. <br><br>  Mas eles existem.  Pesquisadores de IA, incluindo l√≠deres no campo, expressaram ativamente preocupa√ß√µes sobre os riscos da IA ‚Äã‚Äãe al√©m da intelig√™ncia, e desde o in√≠cio.  Come√ßarei listando essas pessoas, apesar da lista de Naam, e depois passarei a explicar por que n√£o considero isso uma "discuss√£o" no sentido cl√°ssico esperado ao listar as estrelas. <br><br>  Os crit√©rios da minha lista s√£o os seguintes: mencionei apenas os pesquisadores de maior prest√≠gio, ou professores de ci√™ncias de bons institutos, com muitas cita√ß√µes de artigos cient√≠ficos, ou cientistas muito respeitados do setor, que trabalham para grandes empresas e t√™m um bom hist√≥rico.  Eles est√£o envolvidos em IA e aprendizado de m√°quina.  Eles t√™m v√°rias declara√ß√µes fortes em apoio a um certo ponto de vista sobre o aparecimento de uma singularidade ou risco grave da IA ‚Äã‚Äãem um futuro pr√≥ximo.  Alguns deles escreveram obras ou livros sobre esse assunto.  Outros simplesmente expressaram seus pensamentos, acreditando que este √© um t√≥pico importante e digno de estudo. <br><br>  Se algu√©m n√£o concordar com a inclus√£o de uma pessoa nesta lista ou acreditar que eu esqueci algo importante, informe-me. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stuart Russell</a> √© professor de ci√™ncia da computa√ß√£o em Berkeley, vencedor do Pr√™mio IJCAI de Computadores e Pensamento, pesquisador da Computer Mechanization Association, pesquisador da Academia Americana de Pesquisa Cient√≠fica Avan√ßada, diretor do Center for Intelligent Systems, vencedor do Pr√™mio Blaise Pascal, etc.  etc.  Co-autor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AI: uma abordagem moderna</a> , um livro cl√°ssico usado em 1.200 universidades em todo o mundo.  Em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">seu site,</a> ele escreve: <br><blockquote>  No campo da IA, 50 anos de pesquisa est√£o em andamento sob a bandeira da suposi√ß√£o de que quanto mais inteligente, melhor.  A preocupa√ß√£o com o benef√≠cio da humanidade deve ser combinada com isso.  O argumento √© simples: <br><br>  1. A IA provavelmente ser√° criada com sucesso. <br>  2. Sucesso ilimitado leva a grandes riscos e grandes benef√≠cios. <br>  3. O que podemos fazer para aumentar as chances de obter benef√≠cios e evitar riscos? <br><br>  Algumas organiza√ß√µes j√° est√£o trabalhando nessas quest√µes, incluindo o Instituto para o Futuro da Humanidade em Oxford, o Centro de Estudos de Riscos Existenciais em Cambridge (CSER), o Instituto de Estudos de Intelig√™ncia de M√°quinas em Berkeley e o Instituto de Vida Futura em Harvard / MIT (FLI).  Estou em conselhos consultivos com CSER e FLI. <br><br>  Assim como os pesquisadores em fus√£o nuclear consideraram o problema de limitar as rea√ß√µes nucleares como um dos problemas mais importantes em seu campo, o desenvolvimento do campo da IA ‚Äã‚Äãinevitavelmente levantar√° quest√µes de controle e seguran√ßa.  Os pesquisadores j√° est√£o come√ßando a levantar quest√µes, desde puramente t√©cnicas (os principais problemas de racionalidade e utilidade etc.) at√© quest√µes amplamente filos√≥ficas. </blockquote><br>  No <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">edge.org,</a> ele descreve um ponto de vista semelhante: <br><blockquote>  Conforme explicado por Steve Omohandro, Nick Bostrom e outros, uma discrep√¢ncia de valores nos sistemas de tomada de decis√£o, cujas possibilidades est√£o em constante crescimento, pode levar a problemas - talvez at√© problemas da escala de extin√ß√£o, se as m√°quinas forem mais capazes do que as pessoas.  Alguns acreditam que n√£o h√° riscos previs√≠veis para a humanidade nos pr√≥ximos s√©culos, talvez esquecendo que a diferen√ßa de tempo entre a afirma√ß√£o confiante de Rutherford de que a energia at√¥mica nunca pode ser extra√≠da e menos de 24 horas decorridas pela inven√ß√£o da rea√ß√£o em cadeia nuclear iniciada por n√™utrons . </blockquote><br>  Ele tamb√©m tentou se tornar um representante dessas id√©ias na comunidade acad√™mica, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">indicando</a> : <br><blockquote>  Acho que as principais pessoas do setor, que nunca expressaram medo antes, pensam que esse problema precisa ser levado muito a s√©rio e, quanto mais cedo o levarmos a s√©rio, melhor. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">David McAllister</a> √© professor e membro s√™nior do Instituto de Tecnologia da Toyota, afiliado √† Universidade de Chicago, que trabalhou anteriormente nas faculdades do MIT e do Instituto Cornell.  Ele √© membro da American AI Association, publicou mais de cem trabalhos, realizou pesquisas nos campos de aprendizado de m√°quina, teoria da programa√ß√£o, tomada de decis√£o autom√°tica, planejamento da IA, lingu√≠stica computacional e teve um grande impacto nos algoritmos do famoso computador de xadrez Deep Blue.  De acordo com um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> da Pittsburgh Tribune Review: <br><blockquote>  David McAllister, professor de Chicago, considera inevit√°vel o surgimento da capacidade de m√°quinas inteligentes totalmente autom√°ticas de projetar e criar vers√µes mais inteligentes de si mesmas, isto √©, o in√≠cio de um evento conhecido como singularidade [tecnol√≥gica].  A singularidade permitir√° que as m√°quinas se tornem infinitamente inteligentes, levando a um "cen√°rio incrivelmente perigoso", diz ele. </blockquote><br>  Em seu blog, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Thoughts on Cars</a> , ele escreve: <br><blockquote>  A maioria dos cientistas da computa√ß√£o se recusa a falar sobre sucessos reais na IA.  Eu acho que seria mais razo√°vel dizer que ningu√©m √© capaz de prever quando uma IA compar√°vel √† mente humana ser√° recebida.  John MacArthy me disse uma vez que quando lhe perguntam sobre quanto tempo a IA em n√≠vel humano ser√° criada, ele responde que ela tem de quinhentos a quinhentos anos.  MacArthy era inteligente.  Dadas as incertezas nessa √°rea, √© razo√°vel considerar o problema da IA ‚Äã‚Äãamig√°vel ... <br><br>  Nos est√°gios iniciais, a IA generalizada estar√° segura.  No entanto, os est√°gios iniciais do OII ser√£o um excelente local de teste para IA como servidor ou outras op√ß√µes amig√°veis ‚Äã‚Äãde IA.  Ben Goertzel tamb√©m anuncia uma abordagem experimental em um bom post em seu blog.  Se a era de OIIs seguros e n√£o muito inteligentes nos espera, teremos tempo para pensar em tempos mais perigosos. </blockquote><br>  Ele foi membro do grupo de especialistas do Painel da AAAI sobre Futuros de IA de Longo Prazo, dedicado √†s perspectivas de longo prazo da IA, presidiu o comit√™ de controle de longo prazo e √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descrito a seguir</a> : <br><blockquote>  Makalister falou comigo sobre a abordagem da "singularidade", um evento em que os computadores se tornam mais inteligentes que as pessoas.  Ele n√£o nomeou a data exata de sua ocorr√™ncia, mas disse que isso poderia acontecer nas pr√≥ximas d√©cadas e, no final, definitivamente aconteceria.  Aqui est√£o seus pontos de vista sobre a singularidade.  Dois eventos significativos ocorrer√£o: intelig√™ncia operacional, na qual podemos conversar facilmente com computadores, e uma rea√ß√£o em cadeia da IA, na qual o computador pode melhorar a si mesmo sem qualquer ajuda e depois repeti-la novamente.  O primeiro evento que notaremos nos sistemas de assist√™ncia autom√°tica que realmente nos ajudar√£o.  Mais tarde, ser√° realmente interessante se comunicar com os computadores.  E, para que os computadores possam fazer tudo o que as pessoas podem fazer, voc√™ deve aguardar o segundo evento. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hans Moravek</a> √© um ex-professor do Instituto de Rob√≥tica da Universidade Carnegie Mellon, nomeado ap√≥s ele o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">paradoxo de Moravec</a> , fundador da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SeeGrid Corporation</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">especializada</a> em sistemas de vis√£o de m√°quina para aplica√ß√µes industriais.  Seu trabalho, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">S√≠ntese de sensores nas redes de certeza de rob√¥s m√≥veis</a> ", foi citado mais de mil vezes, e ele foi convidado a escrever um artigo para a Enciclop√©dia Brit√¢nica de Rob√≥tica, numa √©poca em que artigos em enciclop√©dias eram escritos por especialistas mundiais nesse campo, e n√£o centenas de comentaristas an√¥nimos da Internet. <br><br>  Ele tamb√©m √© o autor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Robot: From a Simple Machine to a Transcendental Mind</a> , que a Amazon descreve da seguinte maneira: <br><blockquote>  Neste emocionante livro, Hans Moravek prev√™ que, em 2040, as m√°quinas se aproximar√£o do n√≠vel intelectual das pessoas e, em 2050, elas v√£o nos superar.  Mas enquanto Moravec prediz o fim de uma era de dom√≠nio humano, sua vis√£o desse evento n√£o √© t√£o sombria.  Ele n√£o est√° isolado de um futuro no qual as m√°quinas governam o mundo, mas o aceita e descreve um ponto de vista surpreendente segundo o qual os rob√¥s inteligentes se tornar√£o nossos descendentes evolutivos.  Moravec acredita que, ao final deste processo, "o vasto ciberespa√ßo se unir√° √† super-mente desumana e lidar√° com assuntos t√£o distantes das pessoas quanto distantes dos seres humanos e bact√©rias". </blockquote><br>  Shane Leg √© co-fundador da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind Technologies</a> , uma startup de IA comprada em 2014 por US $ 500 milh√µes pelo Google.  Ele recebeu seu doutorado no Instituto de IA em homenagem a  Dale Moul, na Su√≠√ßa, e tamb√©m trabalhou na Divis√£o de Neurobiologia Computacional.  Gatsby em Londres.  No final de sua disserta√ß√£o, "superintelig√™ncia de m√°quina", ele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escreve</a> : <br><blockquote>  Se alguma coisa aparecer que possa se aproximar do poder absoluto, ser√° uma m√°quina super-inteligente.  Por defini√ß√£o, ela ser√° capaz de atingir um grande n√∫mero de objetivos em uma ampla variedade de ambientes.  Se nos prepararmos com anteced√™ncia para essa oportunidade, n√£o apenas evitaremos o desastre, como tamb√©m iniciaremos uma era de prosperidade, diferente de tudo que existia antes. </blockquote><br>  Em uma entrevista subseq√ºente, ele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">diz</a> : <br><blockquote>  A IA est√° agora onde a Internet estava em 1988.  As necessidades de aprendizado de m√°quina s√£o necess√°rias em aplicativos especiais (mecanismos de pesquisa como Google, fundos de hedge e bioinform√°tica), e seu n√∫mero est√° aumentando a cada ano.  Penso que em meados da pr√≥xima d√©cada esse processo se tornar√° maci√ßo e percept√≠vel.  O boom da IA ‚Äã‚Äãdeve ocorrer por volta de 2020, seguido por uma d√©cada de r√°pido progresso, possivelmente ap√≥s corre√ß√µes de mercado.  A IA em n√≠vel humano ser√° criada em meados de 2020, embora muitas pessoas n√£o aceitem o in√≠cio deste evento.  Depois disso, os riscos associados √† IA avan√ßada ser√£o colocados em pr√°tica.  N√£o vou dizer sobre a ‚Äúsingularidade‚Äù, mas eles esperam que em algum momento ap√≥s a cria√ß√£o da OII, coisas loucas come√ßar√£o a acontecer.  Est√° entre 2025 e 2040. </blockquote><br>  Ele e seus co-fundadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Demis Khasabis</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mustafa Suleiman</a> assinaram uma peti√ß√£o ao Instituto para a Vida Futura em rela√ß√£o aos riscos de IA, e uma de suas condi√ß√µes para ingressar no Google era que a empresa concorda em organizar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conselho de √©tica em AI</a> para estudar esses problemas. <br><br>  Steve Omohundro √© um ex-professor de ci√™ncia da computa√ß√£o da Universidade de Illinois, o fundador do grupo de vis√£o e treinamento em computa√ß√£o do Centro para o Estudo de Sistemas Complexos e o inventor de v√°rios desenvolvimentos importantes em aprendizado de m√°quina e vis√£o por computador.  Ele trabalhou em rob√¥s que liam os l√°bios, a linguagem de programa√ß√£o paralela StarLisp, algoritmos de aprendizado geom√©trico.  Atualmente, ele lidera o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Self-Aware Systems</a> ", uma equipe de cientistas que trabalha para garantir que as tecnologias inteligentes beneficiem a humanidade".  Seu trabalho, ‚ÄúOs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">princ√≠pios b√°sicos da motiva√ß√£o da IA</a> ‚Äù <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> ajudou a gerar o dom√≠nio da √©tica das m√°quinas, pois observou que sistemas superinteligentes ser√£o direcionados a objetivos potencialmente perigosos.  Ele escreve: <br><blockquote>  Mostramos que todos os sistemas avan√ßados de IA provavelmente t√™m um conjunto de motiva√ß√µes essenciais.  √â imperativo entender essas motiva√ß√µes para criar tecnologias que garantam um futuro positivo para a humanidade.  Yudkovsky pediu uma "IA amig√°vel".  Para fazer isso, precisamos desenvolver uma abordagem cient√≠fica para o ‚Äúdesenvolvimento utilit√°rio‚Äù, que nos permita desenvolver fun√ß√µes socialmente √∫teis que levar√£o √†s seq√º√™ncias que desejamos.  Os r√°pidos avan√ßos no progresso tecnol√≥gico sugerem que esses problemas podem se tornar cr√≠ticos em breve. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Voc√™</a> pode encontrar seus artigos sobre o t√≥pico "IA racional para o bem comum" no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Murray Shanahan</a> √© Ph.D. em ci√™ncia da computa√ß√£o em Cambridge e atualmente √© professor de rob√≥tica cognitiva no Imperial College de Londres.  Ele publicou trabalhos em √°reas como rob√≥tica, l√≥gica, sistemas din√¢micos, neurobiologia computacional e filosofia da mente.  Atualmente, ele est√° trabalhando no livro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Singularidade Tecnol√≥gica</a> , que ser√° publicado em agosto.  A anota√ß√£o promocional da Amazon √© a seguinte: <br><blockquote>  Shanahan descreve os avan√ßos tecnol√≥gicos na IA, ambos feitos sob a influ√™ncia do conhecimento da biologia e desenvolvidos a partir do zero.  Ele explica que quando a IA do n√≠vel humano for criada - uma tarefa teoricamente poss√≠vel, mas dif√≠cil - a transi√ß√£o para a IA superinteligente ser√° muito r√°pida.  O Shanahan reflete sobre o que a exist√™ncia de m√°quinas superinteligentes pode levar a √°reas como personalidade, responsabilidade, direitos e individualidade.  Alguns representantes da IA ‚Äã‚Äãsuperinteligente podem ser criados para o benef√≠cio do homem, outros podem ficar fora de controle.  (Ou seja, Siri ou HAL?) A singularidade representa para a humanidade uma amea√ßa existencial e uma oportunidade existencial para superar suas limita√ß√µes.  Shanahan deixa claro que, se queremos alcan√ßar um resultado melhor, precisamos imaginar as duas possibilidades. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Marcus Hutter</a> √© professor de pesquisa em ci√™ncia da computa√ß√£o na National Australia University.  Antes disso, ele trabalhou no Instituto de IA em homenagem a  Dale Mole, na Su√≠√ßa, e no Instituto Nacional de Inform√°tica e Comunica√ß√µes, na Austr√°lia, e tamb√©m trabalhou em aprendizado estimulado, descobertas bayesianas, teoria da complexidade computacional, teoria das previs√µes indutivas de Salom√£o, vis√£o computacional e perfis gen√©ticos.  Ele tamb√©m escreveu muito sobre singularidade.  No artigo ‚ÄúA <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">intelig√™ncia pode explodir?</a> ‚Äù Ele escreve: <br><blockquote>  Este s√©culo pode testemunhar uma explos√£o tecnol√≥gica, cuja escala merece o nome de singularidade.  O cen√°rio padr√£o √© uma comunidade de indiv√≠duos inteligentes interagindo no mundo virtual, simulados em computadores com recursos de computa√ß√£o que aumentam hiperbolicamente.  Isso √© inevitavelmente acompanhado por uma explos√£o de velocidade, medida pelo tempo f√≠sico, mas n√£o necessariamente por uma explos√£o de intelig√™ncia.  Se o mundo virtual for povoado por indiv√≠duos livres e em intera√ß√£o, a press√£o evolutiva levar√° ao surgimento de indiv√≠duos com intelig√™ncia crescente que competir√£o por recursos de computa√ß√£o.  O ponto final dessa acelera√ß√£o evolutiva da intelig√™ncia pode ser a comunidade dos indiv√≠duos mais inteligentes.  Alguns aspectos dessa comunidade singular podem teoricamente ser estudados com a ajuda de ferramentas cient√≠ficas modernas.  Muito antes do surgimento dessa singularidade, mesmo colocando essa comunidade virtual em nossa imagina√ß√£o, √© poss√≠vel imaginar o surgimento de diferen√ßas, como, por exemplo, uma queda acentuada no valor de um indiv√≠duo, o que pode levar a consequ√™ncias radicais. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">J√ºrgen Schmidhuber</a> √© professor de IA na Universidade de Lugano e ex-professor de rob√≥tica cognitiva na Universidade de Tecnologia de Munique.  Ele desenvolve algumas das redes neurais mais avan√ßadas do mundo, trabalha com rob√≥tica evolucion√°ria e na teoria da complexidade computacional e atua como pesquisador na Academia Europeia de Ci√™ncias e Artes.  Em seu livro " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hip√≥teses de singularidades</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> ele argumenta que "com a continua√ß√£o das tend√™ncias existentes, enfrentaremos uma explos√£o intelectual nas pr√≥ximas d√©cadas".  Quando perguntado diretamente no Reddit AMA sobre os riscos associados √† IA, ele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">respondeu</a> : <br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">encontre um nicho para a sobreviv√™ncia</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ".</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √© professor e membro do Comit√™ iCORE da Universidade de Alberta. </font><font style="vertical-align: inherit;">Ele atua como pesquisador da Association for the Development of AI, co-autor do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">livro mais popular sobre aprendizado estimulado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pioneiro no m√©todo das diferen√ßas de tempo, um dos mais importantes nesse campo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em seu </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">relat√≥rio</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em uma confer√™ncia sobre IA organizada pelo Instituto para o Futuro da Vida, Saton argumentou que "existe uma chance real de que, mesmo com nossas vidas", seja criada uma IA que seja intelectualmente compar√°vel aos seres humanos e acrescentou que essa IA "n√£o nos obedecer√°". para competir e cooperar conosco ", e que" se criarmos escravos super inteligentes, teremos oponentes super inteligentes ". Em conclus√£o, ele disse que "precisamos pensar em mecanismos (sociais, legais, pol√≠ticos, culturais) para garantir o resultado desejado", mas que "inevitavelmente as pessoas comuns se tornar√£o menos importantes". Ele tamb√©m mencionou quest√µes semelhantes na </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apresenta√ß√£o do</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Instituto Gadsby. Tamb√©m no livro de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">existem essas frases: "Richard Saton, um dos maiores especialistas em IA, prev√™ uma explos√£o de intelig√™ncia em algum lugar no meio do s√©culo". </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √© professor de vis√£o de m√°quina no Imperial College London, l√≠der em grupos de vis√£o rob√≥tica e no Laborat√≥rio de Rob√≥tica Dyson e inventor do sistema informatizado de localiza√ß√£o e marca√ß√£o MonoSLAM. Em seu site, ele </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">escreve</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         ¬´ ¬ª (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    ¬´ ¬ª    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> n√£o precisam ser apresentados. Turing inventou os fundamentos matem√°ticos da ci√™ncia computacional e recebeu o nome de uma m√°quina de Turing, completude de Turing e teste de Turing. Goode trabalhou com Turing em Bletchley Park, ajudou a criar um dos primeiros computadores e inventou muitos algoritmos conhecidos, por exemplo, o algoritmo de transforma√ß√£o r√°pida e discreta de Fourier, conhecido como algoritmo FFT. Em seu trabalho, os carros digitais podem pensar? Turing escreve:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos supor que essas m√°quinas possam ser criadas e considerar as conseq√º√™ncias de sua cria√ß√£o. </font><font style="vertical-align: inherit;">Tal ato, sem d√∫vida, ser√° recebido com hostilidade, a menos que tenhamos avan√ßado na toler√¢ncia religiosa desde a √©poca de Galileu. </font><font style="vertical-align: inherit;">A oposi√ß√£o ser√° formada por intelectuais com medo de perder o emprego. </font><font style="vertical-align: inherit;">Mas √© prov√°vel que os intelectuais estejam enganados. </font><font style="vertical-align: inherit;">Ser√° poss√≠vel fazer muitas coisas na tentativa de manter seu intelecto no n√≠vel dos padr√µes estabelecidos pelas m√°quinas, pois ap√≥s o in√≠cio do m√©todo da m√°quina, n√£o leva muito tempo at√© o momento em que as m√°quinas superam nossas capacidades insignificantes. </font><font style="vertical-align: inherit;">Em algum momento, devemos esperar que as m√°quinas assumam o controle.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enquanto trabalhava no Atlas Computer Lab nos anos 60, Goode desenvolveu essa id√©ia em " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Racioc√≠nio para a primeira m√°quina ultra-inteligente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ":</font></font><br><blockquote>   ,  ,       .     ‚Äì     ,       .  ,   ,   ¬´ ¬ª,      . ,    ‚Äì   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Incomoda-me que esta lista possa dar a impress√£o de uma certa disputa entre "crentes" e "c√©ticos" nessa √°rea, durante a qual eles se esmagam em pedacinhos. Mas acho que n√£o. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quando leio artigos sobre c√©ticos, sempre me deparei com dois argumentos. Em primeiro lugar, ainda estamos muito distantes da IA ‚Äã‚Äãdo n√≠vel humano, sem mencionar a superintelig√™ncia, e n√£o h√° uma maneira √≥bvia de alcan√ßar tais alturas. Em segundo lugar, se voc√™ exige proibi√ß√µes de pesquisa em IA, voc√™ √© um idiota. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Concordo plenamente com os dois pontos. Como os l√≠deres do movimento de risco da IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pesquisa entre pesquisadores de IA ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) mostraram que, em m√©dia, d√£o 50% pelo fato de que a IA em n√≠vel humano aparecer√° em 2040 ode e 90% - em 2075. Em m√©dia, 75% deles acreditam que a superintelig√™ncia ("intelig√™ncia da m√°quina, superando seriamente as capacidades") de cada pessoa na maioria das profiss√µes ") aparecer√° dentro de 30 anos ap√≥s o advento do n√≠vel humano AI. E embora a t√©cnica desta pesquisa suscite algumas d√∫vidas, se aceitarmos seus resultados, acontece que a maioria dos pesquisadores de IA concorda que algo que vale a pena se preocupar aparecer√° em uma ou duas gera√ß√µes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas o diretor do Instituto de Intelig√™ncia de M√°quinas, Luke Muelhauser, e o diretor do Instituto para o Futuro da Humanidade, Nick Bostrom, afirmaram que suas previs√µes para o desenvolvimento da IA ‚Äã‚Äãs√£o muito posteriores √†s previs√µes dos cientistas participantes da pesquisa. Se voc√™ estuda</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dados sobre as previs√µes de IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de Stuart Armstrong, pode-se ver que, em geral, as estimativas no momento do aparecimento da IA ‚Äã‚Äãfeitas pelos apoiadores da IA ‚Äã‚Äãn√£o diferem das estimativas feitas pelos c√©ticos da IA. Al√©m disso, a previs√£o de longo prazo nesta tabela pertence ao pr√≥prio Armstrong. No entanto, Armstrong est√° atualmente trabalhando no Instituto para o Futuro da Humanidade, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">chamando a aten√ß√£o para os riscos da IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e a necessidade de pesquisar os objetivos da superintelig√™ncia. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A diferen√ßa entre apoiadores e c√©ticos n√£o est√° em suas avalia√ß√µes de quando devemos esperar o aparecimento da IA ‚Äã‚Äãno n√≠vel humano, mas em quando precisamos come√ßar a nos preparar para isso.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O que nos leva ao segundo ponto. A posi√ß√£o dos c√©ticos, ao que parece, √© que, embora provavelmente devamos enviar algumas pessoas inteligentes para trabalhar em uma avalia√ß√£o preliminar do problema, n√£o h√° absolutamente nenhuma necessidade de entrar em p√¢nico ou proibir o estudo da IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os f√£s de IA insistem que, embora n√£o precisemos entrar em p√¢nico ou banir a pesquisa de IA, provavelmente vale a pena enviar algumas pessoas inteligentes para trabalhar em uma avalia√ß√£o preliminar do problema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun √© sem d√∫vida o c√©tico mais ardente dos riscos de IA. Ele foi abundantemente citado em um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artigo sobre Popular Science</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , em um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">post sobre a Revolu√ß√£o Marginal</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , e tamb√©m falou com o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sobre as "inevit√°veis ‚Äã‚Äãquest√µes da singularidade", que ele mesmo descreve como "estando t√£o distantes que a fic√ß√£o cient√≠fica pode ser escrita sobre elas". </font><font style="vertical-align: inherit;">Mas, quando solicitado a esclarecer sua posi√ß√£o, ele declarou:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk est√° muito preocupado com amea√ßas existenciais √† humanidade (√© por isso que ele constr√≥i foguetes para enviar pessoas para colonizar outros planetas). </font><font style="vertical-align: inherit;">E embora o risco de uma rebeli√£o de IA seja muito pequeno e muito distante do futuro, precisamos pensar sobre isso, desenvolver medidas e regras de precau√ß√£o. </font><font style="vertical-align: inherit;">Assim como o comit√™ de bio√©tica apareceu nas d√©cadas de 1970 e 1980, antes do amplo uso da gen√©tica, precisamos de comit√™s de √©tica em IA. </font><font style="vertical-align: inherit;">Mas, como escreveu Yoshua Benjio, ainda temos tempo de sobra.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz √© outro especialista conhecido como o principal porta-voz do ceticismo e das limita√ß√µes. Seu ponto de vista foi descrito em artigos como "O </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diretor da Microsoft Research acredita que a IA fora de controle n√£o nos matar√°</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " e " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz, da Microsoft, acredita que a IA n√£o deve ter medo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ". Mas aqui est√° o que ele disse em uma entrevista mais longa com a NPR:</font></font><br><blockquote> Keist: Horvitz duvida que os secret√°rios virtuais se transformem em algo que conquistar√° o mundo.  Ele diz que √© esperado que uma pipa evolua para um Boeing 747. Isso significa que ele tira sarro de uma singularidade? <br><br>  Horvitz: N√£o.  Eu acho que houve uma mistura de conceitos, e eu tamb√©m tenho sentimentos confusos. <br><br>  Keist: Em particular, devido a id√©ias como singularidade, Horvits e outros especialistas em IA est√£o cada vez mais tentando lidar com quest√µes √©ticas que podem surgir com IA de alvo restrito nos pr√≥ximos anos.  Eles tamb√©m fazem perguntas mais futuristas.  Por exemplo, como posso fazer um bot√£o de desligamento de emerg√™ncia para um computador que pode mudar sozinho? <br><br>  Horvits: Eu realmente acredito que as apostas s√£o altas o suficiente para gastar tempo e energia buscando ativamente solu√ß√µes, mesmo que a probabilidade de tais eventos seja pequena. </blockquote><br>  Isso geralmente coincide com a posi√ß√£o de muitos dos mais ardentes agitadores de risco de IA.  Com esses amigos, inimigos n√£o s√£o necess√°rios. <br><br>  O artigo da Slate, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">N√£o tenha medo da IA</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> tamb√©m surpreendentemente coloca as coisas na luz certa: <br><blockquote>  Como o pr√≥prio Musk afirma, a solu√ß√£o para o risco de IA reside na colabora√ß√£o s√≥bria e racional de cientistas e legisladores.  No entanto, √© muito dif√≠cil entender como falar sobre "dem√¥nios" pode ajudar a alcan√ßar esse objetivo nobre.  Ela pode at√© impedi-la. <br><br>  Em primeiro lugar, existem enormes buracos na id√©ia do script da Skynet.  Embora os pesquisadores do campo da ci√™ncia da computa√ß√£o acreditem que o racioc√≠nio de Mask "n√£o √© completamente louco", eles ainda est√£o muito longe de um mundo em que o hype sobre a IA disfar√ßa uma realidade um pouco menos da IA ‚Äã‚Äãcom a qual nossos cientistas da computa√ß√£o s√£o confrontados. <br><br>  Ian Lekun, chefe do laborat√≥rio de IA do Facebook, resumiu essa ideia em uma postagem no Google+ em 2013: o hype est√° prejudicando a IA.  Nas √∫ltimas cinco d√©cadas, o hype matou a IA quatro vezes.  Ela precisa ser interrompida. "Lekun e outros t√™m muito medo de exageros. O fracasso em atender √†s altas expectativas impostas pela fic√ß√£o cient√≠fica leva a graves cortes nos or√ßamentos da pesquisa em IA. </blockquote><br>  Os cientistas que trabalham com IA s√£o pessoas inteligentes.  Eles n√£o est√£o interessados ‚Äã‚Äãem cair em armadilhas pol√≠ticas cl√°ssicas, nas quais seriam divididos em campos e acusariam um ao outro de p√¢nico ou avestruzismo.  Aparentemente, eles est√£o tentando encontrar um equil√≠brio entre a necessidade de iniciar um trabalho preliminar relacionado √† amea√ßa que aparece em algum lugar distante e o risco de causar uma sensa√ß√£o t√£o forte que os atingir√°. <br><br>  N√£o quero dizer que n√£o haja diferen√ßa de opini√£o sobre quanto tempo voc√™ precisa para come√ßar a resolver esse problema.  Basicamente, tudo se resume a saber se √© poss√≠vel dizer que "resolveremos o problema quando o encontrarmos" ou esperar uma decolagem t√£o inesperada, devido √† qual tudo ficar√° fora de controle e para a qual, portanto, precisamos nos preparar. com anteced√™ncia.  Vejo menos evid√™ncias do que gostaria que a maioria dos pesquisadores de IA com suas pr√≥prias opini√µes entendam a segunda possibilidade.  O que posso dizer, mesmo que, em um artigo sobre a Revolu√ß√£o Marginal, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">especialista seja citado</a> dizendo que a superintelig√™ncia n√£o representa uma grande amea√ßa, porque "os computadores inteligentes n√£o podem estabelecer metas para si mesmos", embora quem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">l√™ o Bostrom</a> saiba que todo o problema √©. <br><br>  Ainda h√° uma montanha de trabalho a ser feito.  Mas apenas para n√£o selecionar especificamente artigos nos quais "verdadeiros especialistas em IA n√£o se preocupam com a superintelig√™ncia". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt402379/">https://habr.com/ru/post/pt402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt402367/index.html">Como parar de pagar pelo roaming ou Com um n√∫mero em todo o mundo</a></li>
<li><a href="../pt402369/index.html">Como medir a velocidade de uma impressora 3D - seu ponto quente. E n√£o apenas velocidade</a></li>
<li><a href="../pt402373/index.html">O que d√° a "Gen√©tica da Microbiota"</a></li>
<li><a href="../pt402375/index.html">Interruptor CA de 8 canais e 8 kilowatts com medi√ß√£o de consumo. Parte 1</a></li>
<li><a href="../pt402377/index.html">O que seus smartphones pensam sobre o carregamento USB do carro</a></li>
<li><a href="../pt402381/index.html">Como recrutar astronautas</a></li>
<li><a href="../pt402383/index.html">Saw, Shura: como projetamos o aplicativo m√≥vel Mishiko Dog Tracker</a></li>
<li><a href="../pt402385/index.html">Por que voc√™ deve esperar um boom no campo da cria√ß√£o de rob√¥s para instala√ß√µes comerciais</a></li>
<li><a href="../pt402387/index.html">Caneta 3D para impressoras 3D</a></li>
<li><a href="../pt402389/index.html">MPAA e RIAA planejam recuperar dados de discos r√≠gidos com falha no compartilhamento de arquivos Megaupload</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>