<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏧 🤶🏿 🔻 O que os pesquisadores da IA ​​pensam sobre os possíveis riscos associados a ela 📚 🕺🏽 👩‍💻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fiquei interessado nos riscos associados à IA em 2007. Naquela época, a reação da maioria das pessoas a esse tópico era mais ou menos assim: "É muito ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O que os pesquisadores da IA ​​pensam sobre os possíveis riscos associados a ela</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/"> Fiquei interessado nos riscos associados à IA em 2007.  Naquela época, a reação da maioria das pessoas a esse tópico era mais ou menos assim: "É muito engraçado, volte quando alguém que não seja idiota da Internet acreditará nele". <br><br>  Nos anos que se seguiram, várias figuras extremamente inteligentes e influentes, incluindo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stephen Hawking</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Elon Musk</a> , compartilharam publicamente suas preocupações sobre os riscos da IA, seguidos por centenas de outros intelectuais, de filósofos de Oxford a cosmólogos do MIT e investidores do Vale do Silício .  E estamos de volta. <br><br>  Então a reação mudou para: "Bem, alguns cientistas e empresários podem acreditar nisso, mas é improvável que sejam verdadeiros especialistas nesse campo e que sejam realmente versados ​​na situação". <br><br>  A partir daí vieram declarações como o artigo da Popular Science " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bill Gates tem medo da IA, mas os pesquisadores da IA ​​devem saber</a> ": <br><blockquote>  Tendo conversado com pesquisadores de IA - pesquisadores reais, que dificilmente fazem esses sistemas funcionarem, sem mencionar que funcionam bem, fica claro que eles não têm medo de que a superinteligência subitamente se apodere deles, nem agora nem no futuro .  Apesar de todas as histórias assustadoras contadas por Mask, os pesquisadores não têm pressa em construir salas de proteção e autodestruição com uma contagem regressiva. </blockquote><a name="habracut"></a><br>  Ou, como escreveram no Fusion.net no artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Objeção sobre robôs assassinos de uma pessoa que está realmente desenvolvendo IA</a> ": <br><blockquote>  Andrew Angie desenvolve profissionalmente sistemas de IA.  Ele ministrou um curso de IA em Stanford, desenvolveu a IA no Google e depois se mudou para o mecanismo de busca chinês Baidu para continuar seu trabalho na vanguarda da aplicação da IA ​​em problemas do mundo real.  Então, quando ele ouve sobre como Elon Musk ou Stephen Hawking - pessoas que não estão familiarizadas diretamente com a tecnologia moderna - estão falando sobre a IA, que pode potencialmente destruir a humanidade, você quase pode ouvi-lo cobrindo o rosto com as mãos. </blockquote><br>  Ramez Naam, da Marginal Revolution, repete aproximadamente a mesma coisa no artigo “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O que os pesquisadores pensam sobre os riscos da IA?</a> ”: <br><blockquote>  Elon Musk, Stephen Hawking e Bill Gates expressaram recentemente preocupações de que o desenvolvimento da IA ​​possa implementar o cenário de “assassino da IA” e potencialmente levar à extinção da humanidade.  Eles não são pesquisadores de IA e, até onde eu sei, eles não trabalharam diretamente com IA.  O que os pesquisadores de IA real pensam sobre os riscos da IA? </blockquote><br>  Ele cita as palavras de pesquisadores de IA especialmente selecionados, como os autores de outras histórias - e depois para, sem mencionar opiniões diferentes disso. <br><br>  Mas eles existem.  Pesquisadores de IA, incluindo líderes no campo, expressaram ativamente preocupações sobre os riscos da IA ​​e além da inteligência, e desde o início.  Começarei listando essas pessoas, apesar da lista de Naam, e depois passarei a explicar por que não considero isso uma "discussão" no sentido clássico esperado ao listar as estrelas. <br><br>  Os critérios da minha lista são os seguintes: mencionei apenas os pesquisadores de maior prestígio, ou professores de ciências de bons institutos, com muitas citações de artigos científicos, ou cientistas muito respeitados do setor, que trabalham para grandes empresas e têm um bom histórico.  Eles estão envolvidos em IA e aprendizado de máquina.  Eles têm várias declarações fortes em apoio a um certo ponto de vista sobre o aparecimento de uma singularidade ou risco grave da IA ​​em um futuro próximo.  Alguns deles escreveram obras ou livros sobre esse assunto.  Outros simplesmente expressaram seus pensamentos, acreditando que este é um tópico importante e digno de estudo. <br><br>  Se alguém não concordar com a inclusão de uma pessoa nesta lista ou acreditar que eu esqueci algo importante, informe-me. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stuart Russell</a> é professor de ciência da computação em Berkeley, vencedor do Prêmio IJCAI de Computadores e Pensamento, pesquisador da Computer Mechanization Association, pesquisador da Academia Americana de Pesquisa Científica Avançada, diretor do Center for Intelligent Systems, vencedor do Prêmio Blaise Pascal, etc.  etc.  Co-autor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AI: uma abordagem moderna</a> , um livro clássico usado em 1.200 universidades em todo o mundo.  Em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">seu site,</a> ele escreve: <br><blockquote>  No campo da IA, 50 anos de pesquisa estão em andamento sob a bandeira da suposição de que quanto mais inteligente, melhor.  A preocupação com o benefício da humanidade deve ser combinada com isso.  O argumento é simples: <br><br>  1. A IA provavelmente será criada com sucesso. <br>  2. Sucesso ilimitado leva a grandes riscos e grandes benefícios. <br>  3. O que podemos fazer para aumentar as chances de obter benefícios e evitar riscos? <br><br>  Algumas organizações já estão trabalhando nessas questões, incluindo o Instituto para o Futuro da Humanidade em Oxford, o Centro de Estudos de Riscos Existenciais em Cambridge (CSER), o Instituto de Estudos de Inteligência de Máquinas em Berkeley e o Instituto de Vida Futura em Harvard / MIT (FLI).  Estou em conselhos consultivos com CSER e FLI. <br><br>  Assim como os pesquisadores em fusão nuclear consideraram o problema de limitar as reações nucleares como um dos problemas mais importantes em seu campo, o desenvolvimento do campo da IA ​​inevitavelmente levantará questões de controle e segurança.  Os pesquisadores já estão começando a levantar questões, desde puramente técnicas (os principais problemas de racionalidade e utilidade etc.) até questões amplamente filosóficas. </blockquote><br>  No <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">edge.org,</a> ele descreve um ponto de vista semelhante: <br><blockquote>  Conforme explicado por Steve Omohandro, Nick Bostrom e outros, uma discrepância de valores nos sistemas de tomada de decisão, cujas possibilidades estão em constante crescimento, pode levar a problemas - talvez até problemas da escala de extinção, se as máquinas forem mais capazes do que as pessoas.  Alguns acreditam que não há riscos previsíveis para a humanidade nos próximos séculos, talvez esquecendo que a diferença de tempo entre a afirmação confiante de Rutherford de que a energia atômica nunca pode ser extraída e menos de 24 horas decorridas pela invenção da reação em cadeia nuclear iniciada por nêutrons . </blockquote><br>  Ele também tentou se tornar um representante dessas idéias na comunidade acadêmica, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">indicando</a> : <br><blockquote>  Acho que as principais pessoas do setor, que nunca expressaram medo antes, pensam que esse problema precisa ser levado muito a sério e, quanto mais cedo o levarmos a sério, melhor. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">David McAllister</a> é professor e membro sênior do Instituto de Tecnologia da Toyota, afiliado à Universidade de Chicago, que trabalhou anteriormente nas faculdades do MIT e do Instituto Cornell.  Ele é membro da American AI Association, publicou mais de cem trabalhos, realizou pesquisas nos campos de aprendizado de máquina, teoria da programação, tomada de decisão automática, planejamento da IA, linguística computacional e teve um grande impacto nos algoritmos do famoso computador de xadrez Deep Blue.  De acordo com um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> da Pittsburgh Tribune Review: <br><blockquote>  David McAllister, professor de Chicago, considera inevitável o surgimento da capacidade de máquinas inteligentes totalmente automáticas de projetar e criar versões mais inteligentes de si mesmas, isto é, o início de um evento conhecido como singularidade [tecnológica].  A singularidade permitirá que as máquinas se tornem infinitamente inteligentes, levando a um "cenário incrivelmente perigoso", diz ele. </blockquote><br>  Em seu blog, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Thoughts on Cars</a> , ele escreve: <br><blockquote>  A maioria dos cientistas da computação se recusa a falar sobre sucessos reais na IA.  Eu acho que seria mais razoável dizer que ninguém é capaz de prever quando uma IA comparável à mente humana será recebida.  John MacArthy me disse uma vez que quando lhe perguntam sobre quanto tempo a IA em nível humano será criada, ele responde que ela tem de quinhentos a quinhentos anos.  MacArthy era inteligente.  Dadas as incertezas nessa área, é razoável considerar o problema da IA ​​amigável ... <br><br>  Nos estágios iniciais, a IA generalizada estará segura.  No entanto, os estágios iniciais do OII serão um excelente local de teste para IA como servidor ou outras opções amigáveis ​​de IA.  Ben Goertzel também anuncia uma abordagem experimental em um bom post em seu blog.  Se a era de OIIs seguros e não muito inteligentes nos espera, teremos tempo para pensar em tempos mais perigosos. </blockquote><br>  Ele foi membro do grupo de especialistas do Painel da AAAI sobre Futuros de IA de Longo Prazo, dedicado às perspectivas de longo prazo da IA, presidiu o comitê de controle de longo prazo e é <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descrito a seguir</a> : <br><blockquote>  Makalister falou comigo sobre a abordagem da "singularidade", um evento em que os computadores se tornam mais inteligentes que as pessoas.  Ele não nomeou a data exata de sua ocorrência, mas disse que isso poderia acontecer nas próximas décadas e, no final, definitivamente aconteceria.  Aqui estão seus pontos de vista sobre a singularidade.  Dois eventos significativos ocorrerão: inteligência operacional, na qual podemos conversar facilmente com computadores, e uma reação em cadeia da IA, na qual o computador pode melhorar a si mesmo sem qualquer ajuda e depois repeti-la novamente.  O primeiro evento que notaremos nos sistemas de assistência automática que realmente nos ajudarão.  Mais tarde, será realmente interessante se comunicar com os computadores.  E, para que os computadores possam fazer tudo o que as pessoas podem fazer, você deve aguardar o segundo evento. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hans Moravek</a> é um ex-professor do Instituto de Robótica da Universidade Carnegie Mellon, nomeado após ele o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">paradoxo de Moravec</a> , fundador da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SeeGrid Corporation</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">especializada</a> em sistemas de visão de máquina para aplicações industriais.  Seu trabalho, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Síntese de sensores nas redes de certeza de robôs móveis</a> ", foi citado mais de mil vezes, e ele foi convidado a escrever um artigo para a Enciclopédia Britânica de Robótica, numa época em que artigos em enciclopédias eram escritos por especialistas mundiais nesse campo, e não centenas de comentaristas anônimos da Internet. <br><br>  Ele também é o autor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Robot: From a Simple Machine to a Transcendental Mind</a> , que a Amazon descreve da seguinte maneira: <br><blockquote>  Neste emocionante livro, Hans Moravek prevê que, em 2040, as máquinas se aproximarão do nível intelectual das pessoas e, em 2050, elas vão nos superar.  Mas enquanto Moravec prediz o fim de uma era de domínio humano, sua visão desse evento não é tão sombria.  Ele não está isolado de um futuro no qual as máquinas governam o mundo, mas o aceita e descreve um ponto de vista surpreendente segundo o qual os robôs inteligentes se tornarão nossos descendentes evolutivos.  Moravec acredita que, ao final deste processo, "o vasto ciberespaço se unirá à super-mente desumana e lidará com assuntos tão distantes das pessoas quanto distantes dos seres humanos e bactérias". </blockquote><br>  Shane Leg é co-fundador da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind Technologies</a> , uma startup de IA comprada em 2014 por US $ 500 milhões pelo Google.  Ele recebeu seu doutorado no Instituto de IA em homenagem a  Dale Moul, na Suíça, e também trabalhou na Divisão de Neurobiologia Computacional.  Gatsby em Londres.  No final de sua dissertação, "superinteligência de máquina", ele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escreve</a> : <br><blockquote>  Se alguma coisa aparecer que possa se aproximar do poder absoluto, será uma máquina super-inteligente.  Por definição, ela será capaz de atingir um grande número de objetivos em uma ampla variedade de ambientes.  Se nos prepararmos com antecedência para essa oportunidade, não apenas evitaremos o desastre, como também iniciaremos uma era de prosperidade, diferente de tudo que existia antes. </blockquote><br>  Em uma entrevista subseqüente, ele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">diz</a> : <br><blockquote>  A IA está agora onde a Internet estava em 1988.  As necessidades de aprendizado de máquina são necessárias em aplicativos especiais (mecanismos de pesquisa como Google, fundos de hedge e bioinformática), e seu número está aumentando a cada ano.  Penso que em meados da próxima década esse processo se tornará maciço e perceptível.  O boom da IA ​​deve ocorrer por volta de 2020, seguido por uma década de rápido progresso, possivelmente após correções de mercado.  A IA em nível humano será criada em meados de 2020, embora muitas pessoas não aceitem o início deste evento.  Depois disso, os riscos associados à IA avançada serão colocados em prática.  Não vou dizer sobre a “singularidade”, mas eles esperam que em algum momento após a criação da OII, coisas loucas começarão a acontecer.  Está entre 2025 e 2040. </blockquote><br>  Ele e seus co-fundadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Demis Khasabis</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mustafa Suleiman</a> assinaram uma petição ao Instituto para a Vida Futura em relação aos riscos de IA, e uma de suas condições para ingressar no Google era que a empresa concorda em organizar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conselho de ética em AI</a> para estudar esses problemas. <br><br>  Steve Omohundro é um ex-professor de ciência da computação da Universidade de Illinois, o fundador do grupo de visão e treinamento em computação do Centro para o Estudo de Sistemas Complexos e o inventor de vários desenvolvimentos importantes em aprendizado de máquina e visão por computador.  Ele trabalhou em robôs que liam os lábios, a linguagem de programação paralela StarLisp, algoritmos de aprendizado geométrico.  Atualmente, ele lidera o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Self-Aware Systems</a> ", uma equipe de cientistas que trabalha para garantir que as tecnologias inteligentes beneficiem a humanidade".  Seu trabalho, “Os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">princípios básicos da motivação da IA</a> ” <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> ajudou a gerar o domínio da ética das máquinas, pois observou que sistemas superinteligentes serão direcionados a objetivos potencialmente perigosos.  Ele escreve: <br><blockquote>  Mostramos que todos os sistemas avançados de IA provavelmente têm um conjunto de motivações essenciais.  É imperativo entender essas motivações para criar tecnologias que garantam um futuro positivo para a humanidade.  Yudkovsky pediu uma "IA amigável".  Para fazer isso, precisamos desenvolver uma abordagem científica para o “desenvolvimento utilitário”, que nos permita desenvolver funções socialmente úteis que levarão às seqüências que desejamos.  Os rápidos avanços no progresso tecnológico sugerem que esses problemas podem se tornar críticos em breve. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Você</a> pode encontrar seus artigos sobre o tópico "IA racional para o bem comum" no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Murray Shanahan</a> é Ph.D. em ciência da computação em Cambridge e atualmente é professor de robótica cognitiva no Imperial College de Londres.  Ele publicou trabalhos em áreas como robótica, lógica, sistemas dinâmicos, neurobiologia computacional e filosofia da mente.  Atualmente, ele está trabalhando no livro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Singularidade Tecnológica</a> , que será publicado em agosto.  A anotação promocional da Amazon é a seguinte: <br><blockquote>  Shanahan descreve os avanços tecnológicos na IA, ambos feitos sob a influência do conhecimento da biologia e desenvolvidos a partir do zero.  Ele explica que quando a IA do nível humano for criada - uma tarefa teoricamente possível, mas difícil - a transição para a IA superinteligente será muito rápida.  O Shanahan reflete sobre o que a existência de máquinas superinteligentes pode levar a áreas como personalidade, responsabilidade, direitos e individualidade.  Alguns representantes da IA ​​superinteligente podem ser criados para o benefício do homem, outros podem ficar fora de controle.  (Ou seja, Siri ou HAL?) A singularidade representa para a humanidade uma ameaça existencial e uma oportunidade existencial para superar suas limitações.  Shanahan deixa claro que, se queremos alcançar um resultado melhor, precisamos imaginar as duas possibilidades. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Marcus Hutter</a> é professor de pesquisa em ciência da computação na National Australia University.  Antes disso, ele trabalhou no Instituto de IA em homenagem a  Dale Mole, na Suíça, e no Instituto Nacional de Informática e Comunicações, na Austrália, e também trabalhou em aprendizado estimulado, descobertas bayesianas, teoria da complexidade computacional, teoria das previsões indutivas de Salomão, visão computacional e perfis genéticos.  Ele também escreveu muito sobre singularidade.  No artigo “A <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">inteligência pode explodir?</a> ” Ele escreve: <br><blockquote>  Este século pode testemunhar uma explosão tecnológica, cuja escala merece o nome de singularidade.  O cenário padrão é uma comunidade de indivíduos inteligentes interagindo no mundo virtual, simulados em computadores com recursos de computação que aumentam hiperbolicamente.  Isso é inevitavelmente acompanhado por uma explosão de velocidade, medida pelo tempo físico, mas não necessariamente por uma explosão de inteligência.  Se o mundo virtual for povoado por indivíduos livres e em interação, a pressão evolutiva levará ao surgimento de indivíduos com inteligência crescente que competirão por recursos de computação.  O ponto final dessa aceleração evolutiva da inteligência pode ser a comunidade dos indivíduos mais inteligentes.  Alguns aspectos dessa comunidade singular podem teoricamente ser estudados com a ajuda de ferramentas científicas modernas.  Muito antes do surgimento dessa singularidade, mesmo colocando essa comunidade virtual em nossa imaginação, é possível imaginar o surgimento de diferenças, como, por exemplo, uma queda acentuada no valor de um indivíduo, o que pode levar a consequências radicais. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Jürgen Schmidhuber</a> é professor de IA na Universidade de Lugano e ex-professor de robótica cognitiva na Universidade de Tecnologia de Munique.  Ele desenvolve algumas das redes neurais mais avançadas do mundo, trabalha com robótica evolucionária e na teoria da complexidade computacional e atua como pesquisador na Academia Europeia de Ciências e Artes.  Em seu livro " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hipóteses de singularidades</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> ele argumenta que "com a continuação das tendências existentes, enfrentaremos uma explosão intelectual nas próximas décadas".  Quando perguntado diretamente no Reddit AMA sobre os riscos associados à IA, ele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">respondeu</a> : <br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">encontre um nicho para a sobrevivência</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ".</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> é professor e membro do Comitê iCORE da Universidade de Alberta. </font><font style="vertical-align: inherit;">Ele atua como pesquisador da Association for the Development of AI, co-autor do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">livro mais popular sobre aprendizado estimulado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pioneiro no método das diferenças de tempo, um dos mais importantes nesse campo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em seu </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">relatório</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em uma conferência sobre IA organizada pelo Instituto para o Futuro da Vida, Saton argumentou que "existe uma chance real de que, mesmo com nossas vidas", seja criada uma IA que seja intelectualmente comparável aos seres humanos e acrescentou que essa IA "não nos obedecerá". para competir e cooperar conosco ", e que" se criarmos escravos super inteligentes, teremos oponentes super inteligentes ". Em conclusão, ele disse que "precisamos pensar em mecanismos (sociais, legais, políticos, culturais) para garantir o resultado desejado", mas que "inevitavelmente as pessoas comuns se tornarão menos importantes". Ele também mencionou questões semelhantes na </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apresentação do</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Instituto Gadsby. Também no livro de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">existem essas frases: "Richard Saton, um dos maiores especialistas em IA, prevê uma explosão de inteligência em algum lugar no meio do século". </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> é professor de visão de máquina no Imperial College London, líder em grupos de visão robótica e no Laboratório de Robótica Dyson e inventor do sistema informatizado de localização e marcação MonoSLAM. Em seu site, ele </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">escreve</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         « » (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    « »    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> não precisam ser apresentados. Turing inventou os fundamentos matemáticos da ciência computacional e recebeu o nome de uma máquina de Turing, completude de Turing e teste de Turing. Goode trabalhou com Turing em Bletchley Park, ajudou a criar um dos primeiros computadores e inventou muitos algoritmos conhecidos, por exemplo, o algoritmo de transformação rápida e discreta de Fourier, conhecido como algoritmo FFT. Em seu trabalho, os carros digitais podem pensar? Turing escreve:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos supor que essas máquinas possam ser criadas e considerar as conseqüências de sua criação. </font><font style="vertical-align: inherit;">Tal ato, sem dúvida, será recebido com hostilidade, a menos que tenhamos avançado na tolerância religiosa desde a época de Galileu. </font><font style="vertical-align: inherit;">A oposição será formada por intelectuais com medo de perder o emprego. </font><font style="vertical-align: inherit;">Mas é provável que os intelectuais estejam enganados. </font><font style="vertical-align: inherit;">Será possível fazer muitas coisas na tentativa de manter seu intelecto no nível dos padrões estabelecidos pelas máquinas, pois após o início do método da máquina, não leva muito tempo até o momento em que as máquinas superam nossas capacidades insignificantes. </font><font style="vertical-align: inherit;">Em algum momento, devemos esperar que as máquinas assumam o controle.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enquanto trabalhava no Atlas Computer Lab nos anos 60, Goode desenvolveu essa idéia em " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Raciocínio para a primeira máquina ultra-inteligente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ":</font></font><br><blockquote>   ,  ,       .     –     ,       .  ,   ,   « »,      . ,    –   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Incomoda-me que esta lista possa dar a impressão de uma certa disputa entre "crentes" e "céticos" nessa área, durante a qual eles se esmagam em pedacinhos. Mas acho que não. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quando leio artigos sobre céticos, sempre me deparei com dois argumentos. Em primeiro lugar, ainda estamos muito distantes da IA ​​do nível humano, sem mencionar a superinteligência, e não há uma maneira óbvia de alcançar tais alturas. Em segundo lugar, se você exige proibições de pesquisa em IA, você é um idiota. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Concordo plenamente com os dois pontos. Como os líderes do movimento de risco da IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pesquisa entre pesquisadores de IA ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) mostraram que, em média, dão 50% pelo fato de que a IA em nível humano aparecerá em 2040 ode e 90% - em 2075. Em média, 75% deles acreditam que a superinteligência ("inteligência da máquina, superando seriamente as capacidades") de cada pessoa na maioria das profissões ") aparecerá dentro de 30 anos após o advento do nível humano AI. E embora a técnica desta pesquisa suscite algumas dúvidas, se aceitarmos seus resultados, acontece que a maioria dos pesquisadores de IA concorda que algo que vale a pena se preocupar aparecerá em uma ou duas gerações. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas o diretor do Instituto de Inteligência de Máquinas, Luke Muelhauser, e o diretor do Instituto para o Futuro da Humanidade, Nick Bostrom, afirmaram que suas previsões para o desenvolvimento da IA ​​são muito posteriores às previsões dos cientistas participantes da pesquisa. Se você estuda</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dados sobre as previsões de IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de Stuart Armstrong, pode-se ver que, em geral, as estimativas no momento do aparecimento da IA ​​feitas pelos apoiadores da IA ​​não diferem das estimativas feitas pelos céticos da IA. Além disso, a previsão de longo prazo nesta tabela pertence ao próprio Armstrong. No entanto, Armstrong está atualmente trabalhando no Instituto para o Futuro da Humanidade, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">chamando a atenção para os riscos da IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e a necessidade de pesquisar os objetivos da superinteligência. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A diferença entre apoiadores e céticos não está em suas avaliações de quando devemos esperar o aparecimento da IA ​​no nível humano, mas em quando precisamos começar a nos preparar para isso.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O que nos leva ao segundo ponto. A posição dos céticos, ao que parece, é que, embora provavelmente devamos enviar algumas pessoas inteligentes para trabalhar em uma avaliação preliminar do problema, não há absolutamente nenhuma necessidade de entrar em pânico ou proibir o estudo da IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os fãs de IA insistem que, embora não precisemos entrar em pânico ou banir a pesquisa de IA, provavelmente vale a pena enviar algumas pessoas inteligentes para trabalhar em uma avaliação preliminar do problema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun é sem dúvida o cético mais ardente dos riscos de IA. Ele foi abundantemente citado em um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artigo sobre Popular Science</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , em um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">post sobre a Revolução Marginal</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , e também falou com o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sobre as "inevitáveis ​​questões da singularidade", que ele mesmo descreve como "estando tão distantes que a ficção científica pode ser escrita sobre elas". </font><font style="vertical-align: inherit;">Mas, quando solicitado a esclarecer sua posição, ele declarou:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk está muito preocupado com ameaças existenciais à humanidade (é por isso que ele constrói foguetes para enviar pessoas para colonizar outros planetas). </font><font style="vertical-align: inherit;">E embora o risco de uma rebelião de IA seja muito pequeno e muito distante do futuro, precisamos pensar sobre isso, desenvolver medidas e regras de precaução. </font><font style="vertical-align: inherit;">Assim como o comitê de bioética apareceu nas décadas de 1970 e 1980, antes do amplo uso da genética, precisamos de comitês de ética em IA. </font><font style="vertical-align: inherit;">Mas, como escreveu Yoshua Benjio, ainda temos tempo de sobra.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz é outro especialista conhecido como o principal porta-voz do ceticismo e das limitações. Seu ponto de vista foi descrito em artigos como "O </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diretor da Microsoft Research acredita que a IA fora de controle não nos matará</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " e " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz, da Microsoft, acredita que a IA não deve ter medo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ". Mas aqui está o que ele disse em uma entrevista mais longa com a NPR:</font></font><br><blockquote> Keist: Horvitz duvida que os secretários virtuais se transformem em algo que conquistará o mundo.  Ele diz que é esperado que uma pipa evolua para um Boeing 747. Isso significa que ele tira sarro de uma singularidade? <br><br>  Horvitz: Não.  Eu acho que houve uma mistura de conceitos, e eu também tenho sentimentos confusos. <br><br>  Keist: Em particular, devido a idéias como singularidade, Horvits e outros especialistas em IA estão cada vez mais tentando lidar com questões éticas que podem surgir com IA de alvo restrito nos próximos anos.  Eles também fazem perguntas mais futuristas.  Por exemplo, como posso fazer um botão de desligamento de emergência para um computador que pode mudar sozinho? <br><br>  Horvits: Eu realmente acredito que as apostas são altas o suficiente para gastar tempo e energia buscando ativamente soluções, mesmo que a probabilidade de tais eventos seja pequena. </blockquote><br>  Isso geralmente coincide com a posição de muitos dos mais ardentes agitadores de risco de IA.  Com esses amigos, inimigos não são necessários. <br><br>  O artigo da Slate, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Não tenha medo da IA</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> também surpreendentemente coloca as coisas na luz certa: <br><blockquote>  Como o próprio Musk afirma, a solução para o risco de IA reside na colaboração sóbria e racional de cientistas e legisladores.  No entanto, é muito difícil entender como falar sobre "demônios" pode ajudar a alcançar esse objetivo nobre.  Ela pode até impedi-la. <br><br>  Em primeiro lugar, existem enormes buracos na idéia do script da Skynet.  Embora os pesquisadores do campo da ciência da computação acreditem que o raciocínio de Mask "não é completamente louco", eles ainda estão muito longe de um mundo em que o hype sobre a IA disfarça uma realidade um pouco menos da IA ​​com a qual nossos cientistas da computação são confrontados. <br><br>  Ian Lekun, chefe do laboratório de IA do Facebook, resumiu essa ideia em uma postagem no Google+ em 2013: o hype está prejudicando a IA.  Nas últimas cinco décadas, o hype matou a IA quatro vezes.  Ela precisa ser interrompida. "Lekun e outros têm muito medo de exageros. O fracasso em atender às altas expectativas impostas pela ficção científica leva a graves cortes nos orçamentos da pesquisa em IA. </blockquote><br>  Os cientistas que trabalham com IA são pessoas inteligentes.  Eles não estão interessados ​​em cair em armadilhas políticas clássicas, nas quais seriam divididos em campos e acusariam um ao outro de pânico ou avestruzismo.  Aparentemente, eles estão tentando encontrar um equilíbrio entre a necessidade de iniciar um trabalho preliminar relacionado à ameaça que aparece em algum lugar distante e o risco de causar uma sensação tão forte que os atingirá. <br><br>  Não quero dizer que não haja diferença de opinião sobre quanto tempo você precisa para começar a resolver esse problema.  Basicamente, tudo se resume a saber se é possível dizer que "resolveremos o problema quando o encontrarmos" ou esperar uma decolagem tão inesperada, devido à qual tudo ficará fora de controle e para a qual, portanto, precisamos nos preparar. com antecedência.  Vejo menos evidências do que gostaria que a maioria dos pesquisadores de IA com suas próprias opiniões entendam a segunda possibilidade.  O que posso dizer, mesmo que, em um artigo sobre a Revolução Marginal, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">especialista seja citado</a> dizendo que a superinteligência não representa uma grande ameaça, porque "os computadores inteligentes não podem estabelecer metas para si mesmos", embora quem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lê o Bostrom</a> saiba que todo o problema é. <br><br>  Ainda há uma montanha de trabalho a ser feito.  Mas apenas para não selecionar especificamente artigos nos quais "verdadeiros especialistas em IA não se preocupam com a superinteligência". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt402379/">https://habr.com/ru/post/pt402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt402367/index.html">Como parar de pagar pelo roaming ou Com um número em todo o mundo</a></li>
<li><a href="../pt402369/index.html">Como medir a velocidade de uma impressora 3D - seu ponto quente. E não apenas velocidade</a></li>
<li><a href="../pt402373/index.html">O que dá a "Genética da Microbiota"</a></li>
<li><a href="../pt402375/index.html">Interruptor CA de 8 canais e 8 kilowatts com medição de consumo. Parte 1</a></li>
<li><a href="../pt402377/index.html">O que seus smartphones pensam sobre o carregamento USB do carro</a></li>
<li><a href="../pt402381/index.html">Como recrutar astronautas</a></li>
<li><a href="../pt402383/index.html">Saw, Shura: como projetamos o aplicativo móvel Mishiko Dog Tracker</a></li>
<li><a href="../pt402385/index.html">Por que você deve esperar um boom no campo da criação de robôs para instalações comerciais</a></li>
<li><a href="../pt402387/index.html">Caneta 3D para impressoras 3D</a></li>
<li><a href="../pt402389/index.html">MPAA e RIAA planejam recuperar dados de discos rígidos com falha no compartilhamento de arquivos Megaupload</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>