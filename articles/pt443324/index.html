<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå•Ô∏è üèΩ üßÄ Python vs. Scala para Apache Spark - benchmark esperado com resultado inesperado üë± üí™üèª üë©üèø‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O Apache Spark hoje √© talvez a plataforma mais popular para analisar dados de grande volume. Uma contribui√ß√£o consider√°vel para sua popularidade √© fei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python vs. Scala para Apache Spark - benchmark esperado com resultado inesperado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/443324/"><p><img src="https://habrastorage.org/webt/uk/hc/yv/ukhcyvudckoey9ttm1libhqkyv8.jpeg"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Apache Spark</a> hoje √© talvez a plataforma mais popular para analisar dados de grande volume.  Uma contribui√ß√£o consider√°vel para sua popularidade √© feita pela possibilidade de us√°-lo no Python.  Ao mesmo tempo, todos concordam que, dentro da estrutura da API padr√£o, o desempenho do c√≥digo Python e Scala / Java √© compar√°vel, mas n√£o h√° um ponto de vista √∫nico em rela√ß√£o √†s fun√ß√µes definidas pelo usu√°rio (User Defined Function, UDF).  Vamos tentar descobrir como os custos indiretos aumentam nesse caso, usando o exemplo da tarefa de verificar a solu√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SNA Hackathon 2019</a> . </p><a name="habracut"></a><br><p>  Como parte da competi√ß√£o, os participantes resolvem o problema de classificar o feed de not√≠cias de uma rede social e enviar solu√ß√µes na forma de um conjunto de listas classificadas.  Para verificar a qualidade da solu√ß√£o obtida, primeiro, para cada uma das listas carregadas, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AUC</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ROC √©</a> calculada e, em seguida, o valor m√©dio √© exibido.  Observe que voc√™ precisa calcular n√£o um ROC AUC comum, mas pessoal para cada usu√°rio - n√£o h√° um projeto pronto para resolver esse problema, portanto voc√™ ter√° que escrever uma fun√ß√£o especializada.  Uma boa raz√£o para comparar as duas abordagens na pr√°tica. </p><br><p> Como uma plataforma de compara√ß√£o, usaremos um cont√™iner na nuvem com quatro n√∫cleos e o Spark lan√ßado no modo local e trabalharemos com ele atrav√©s do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Apache Zeppelin</a> .  Para comparar a funcionalidade, espelharemos o mesmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">c√≥digo</a> no PySpark e Scala Spark.  [aqui] Vamos come√ßar carregando os dados. </p><br><pre><code class="python hljs">data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) \ .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) \ .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) \ .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><p>  Ao usar a API padr√£o, a identidade quase completa do c√≥digo √© digna de nota, at√© a palavra-chave <code>val</code> .  O tempo de opera√ß√£o n√£o √© significativamente diferente.  Agora vamos tentar determinar a UDF que precisamos. </p><br><pre> <code class="python hljs">parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)], ArrayType(IntegerType())) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores)) auc_udf = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, auc, DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, (x : <span class="hljs-type"><span class="hljs-type">String</span></span>) =&gt; x.slice(<span class="hljs-number"><span class="hljs-number">1</span></span>,x.size - <span class="hljs-number"><span class="hljs-number">1</span></span>).split(<span class="hljs-string"><span class="hljs-string">","</span></span>).map(_.trim.toInt)) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AucAccumulator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">height: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, area: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, negatives: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">val</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">auc_udf</span></span></span><span class="hljs-class"> </span></span>= sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, (byScore: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>], gt: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> byLabel = gt.toSet <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> accumulator = byScore.foldLeft(<span class="hljs-type"><span class="hljs-type">AucAccumulator</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>))((accumulated, current) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (byLabel.contains(current)) { accumulated.copy(height = accumulated.height + <span class="hljs-number"><span class="hljs-number">1</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { accumulated.copy(area = accumulated.area + accumulated.height, negatives = accumulated.negatives + <span class="hljs-number"><span class="hljs-number">1</span></span>) } }) (accumulator.area).toDouble / (accumulator.negatives * accumulator.height) })</code> </pre> <br><p>  Ao implementar uma fun√ß√£o espec√≠fica, fica claro que o Python √© mais conciso, principalmente devido √† capacidade de usar a fun√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">interna de scikit-learn</a> .  No entanto, existem momentos desagrad√°veis ‚Äã‚Äã- voc√™ deve especificar explicitamente o tipo do valor de retorno, enquanto que no Scala √© determinado automaticamente.  Vamos executar a opera√ß√£o: </p><br><pre> <code class="python hljs">toValidate.select(auc_udf(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><pre> <code class="scala hljs">toValidate.select(auc_udf(parse($<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse($<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p>  O c√≥digo parece quase id√™ntico, mas os resultados s√£o desanimadores. </p><br><p><img src="https://habrastorage.org/webt/vc/jw/y_/vcjwy_zwvfbkb_jlw6mimlogdwi.png"></p><br><p>  A implementa√ß√£o no PySpark funcionou um minuto e meio em vez de dois segundos no Scala, ou seja, o <strong>Python ficou 45 vezes mais lento</strong> .  Durante a execu√ß√£o, o topo mostra 4 processos Python ativos em execu√ß√£o a toda velocidade, e isso sugere que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloqueio Global de Int√©rpretes</a> n√£o cria problemas aqui.  Mas!  Talvez o problema esteja na implementa√ß√£o interna do scikit-learn - vamos tentar reproduzir o c√≥digo Python literalmente, sem recorrer a bibliotecas padr√£o. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) auc_udf_modified = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc_modified"</span></span>, auc, DoubleType()) toValidate.select(auc_udf_modified(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/cx/o2/io/cxo2ioxdl18a6djwsmgr6dravhy.png"></p><br><p>  O experimento mostra resultados interessantes.  Por um lado, com essa abordagem, a produtividade foi nivelada, mas, por outro, o laconicismo desapareceu.  Os resultados obtidos podem indicar que, ao trabalhar em Python usando m√≥dulos C ++ adicionais, s√£o exibidas despesas significativas para alternar entre contextos.  Obviamente, h√° uma sobrecarga semelhante ao usar o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">JNI</a> no Java / Scala, no entanto, n√£o tive que lidar com exemplos de degrada√ß√£o 45 vezes ao us√°-los. </p><br><p>  Para uma an√°lise mais detalhada, realizaremos duas experi√™ncias adicionais: usando o Python puro sem Spark para medir a contribui√ß√£o da chamada do pacote e com o aumento do tamanho dos dados no Spark para amortizar a sobrecarga e obter uma compara√ß√£o mais precisa. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sklearn_auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores))</code> </pre> <br><p><img src="https://habrastorage.org/webt/bk/ul/m1/bkulm1xarplv2-f4ilyg5dtxixw.png"></p><br><p>  O experimento com Python local e Pandas confirmou a suposi√ß√£o de sobrecarga significativa ao usar pacotes adicionais - ao usar o scikit-learn, a velocidade diminui em mais de 20 vezes.  No entanto, 20 n√£o √© 45 - vamos tentar "aumentar" os dados e comparar o desempenho do Spark novamente. </p><br><pre> <code class="python hljs">k4 = toValidate.union(toValidate) k8 = k4.union(k4) m1 = k8.union(k8) m2 = m1.union(m1) m4 = m2.union(m2).repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() m4.count()</code> </pre> <br><p><img src="https://habrastorage.org/webt/zp/sf/po/zpsfpoabev7w3_xjcn0vqobwqf4.png"></p><br><p>  A nova compara√ß√£o mostra a vantagem de velocidade de uma implementa√ß√£o Scala sobre Python em 7-8 vezes - 7 segundos versus 55. Finalmente, vamos tentar "o mais r√°pido que est√° em Python" - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">numpy</a> para calcular a soma da matriz: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy numpy_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"numpy_sum"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: float(numpy.sum(x)), DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> my_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"my_sum"</span></span>, (x: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; x.map(_.toDouble).sum)</code> </pre> <br><p><img src="https://habrastorage.org/webt/og/tr/gx/ogtrgx-kkuzx4kmkh4pd5cbyd-k.png"></p><br><p>  Novamente uma desacelera√ß√£o significativa - 5 segundos de Scala versus 80 segundos de Python.  Em resumo, podemos tirar as seguintes conclus√µes: </p><br><ul><li>  Embora o PySpark opere dentro da estrutura da API padr√£o, ele pode ser realmente compar√°vel em velocidade ao Scala. </li><li>  Quando a l√≥gica espec√≠fica aparece na forma de Fun√ß√µes definidas pelo usu√°rio, o desempenho do PySpark diminui visivelmente.  Com informa√ß√µes suficientes, quando o tempo de processamento de um bloco de dados excede v√°rios segundos, a implementa√ß√£o do Python √© de 5 a 10 mais lenta devido √† necessidade de mover dados entre processos e desperdi√ßar recursos na interpreta√ß√£o do Python. </li><li>  Se o uso de fun√ß√µes adicionais implementadas nos m√≥dulos C ++ aparecer, surgem custos adicionais de chamadas e a diferen√ßa entre Python e Scala aumenta de 10 a 50 vezes. </li></ul><br><p>  Como resultado, apesar de todo o charme do Python, seu uso em conjunto com o Spark nem sempre parece justificado.  Se n√£o houver muitos dados para tornar a sobrecarga do Python significativa, voc√™ deve pensar se o Spark √© necess√°rio aqui?  Se houver muitos dados, mas o processamento ocorrer dentro da estrutura da API Spark SQL padr√£o, o Python √© necess√°rio aqui? </p><br><p>  Se houver muitos dados e muitas vezes precisar lidar com tarefas que ultrapassam os limites da API do SQL, para executar a mesma quantidade de trabalho ao usar o PySpark, voc√™ precisar√° aumentar significativamente o cluster.  Por exemplo, para Odnoklassniki, o custo das despesas de capital do cluster Spark aumentaria em muitas centenas de milh√µes de rublos.  E se voc√™ tentar tirar proveito dos recursos avan√ßados das bibliotecas do ecossistema Python, ou seja, o risco de abrandar n√£o √© apenas √†s vezes, mas uma ordem de magnitude. </p><br><p>  Alguma acelera√ß√£o pode ser obtida usando a funcionalidade relativamente nova das fun√ß√µes vetorizadas.  Nesse caso, nenhuma linha √© alimentada na entrada UDF, mas um pacote de v√°rias linhas na forma de um Dataframe do Pandas.  No entanto, o desenvolvimento dessa funcionalidade <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ainda n√£o est√° conclu√≠do</a> e, mesmo nesse caso, a diferen√ßa ser√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">significativa</a> . </p><br><p>  Uma alternativa seria manter uma extensa equipe de engenheiros de dados, capaz de atender rapidamente √†s necessidades dos cientistas de dados com fun√ß√µes adicionais.  Ou mergulhar no mundo do Scala, j√° que n√£o √© t√£o dif√≠cil: muitas das ferramentas necess√°rias j√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">existem</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">programas de treinamento</a> aparecem que v√£o al√©m do PySpark. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt443324/">https://habr.com/ru/post/pt443324/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt443312/index.html">Gerenciamento de mem√≥ria Python</a></li>
<li><a href="../pt443314/index.html">O que √© cordentidade? [Tradu√ß√£o do artigo]</a></li>
<li><a href="../pt443318/index.html">Escrevendo um carregador wasm para Ghidra. Parte 1: Declara√ß√£o do Problema e Configura√ß√£o do Ambiente</a></li>
<li><a href="../pt443320/index.html">Sistema de gerenciamento eletr√¥nico de documentos "Vizier"</a></li>
<li><a href="../pt443322/index.html">GitLab 11.8 lan√ßado com SAST para JavaScript, p√°ginas GitLab para subgrupos e rastreamento de erros</a></li>
<li><a href="../pt443326/index.html">Python e Arduino. Simples, r√°pido e bonito</a></li>
<li><a href="../pt443330/index.html">Semana da Seguran√ßa 11: RSA 2019 e um futuro melhor</a></li>
<li><a href="../pt443332/index.html">Rob√¥ de cozinha</a></li>
<li><a href="../pt443334/index.html">Cara amarela</a></li>
<li><a href="../pt443336/index.html">Trabalhar com a c√¢mera no Flutter</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>