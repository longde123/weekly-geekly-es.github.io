<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèä ‚ôøÔ∏è üçü ¬øC√≥mo decidi√≥ el concurso retro cosacos üîë ‚úäüèº ü§¶üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esta primavera, se celebr√≥ un importante concurso retro de OpenAI, que se dedic√≥ al aprendizaje de refuerzo, meta aprendizaje y, por supuesto, Sonic. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øC√≥mo decidi√≥ el concurso retro cosacos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/421585/"><p>  Esta primavera, se celebr√≥ un importante concurso retro de OpenAI, que se dedic√≥ al aprendizaje de refuerzo, meta aprendizaje y, por supuesto, Sonic.  Nuestro equipo ocup√≥ el cuarto lugar de m√°s de 900 equipos.  El campo de entrenamiento con refuerzo es ligeramente diferente del aprendizaje autom√°tico est√°ndar, y este concurso fue diferente de una competencia t√≠pica de RL.  Pido detalles bajo cat. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sx/pt/0r/sxpt0rdvj5g3xn6eohh6vmusj1o.jpeg" alt="imagen"></div><br><hr><a name="habracut"></a><br><h2 id="tldr">  TL; DR </h2><br><p>  Una l√≠nea de base ajustada adecuadamente no necesita trucos adicionales ... pr√°cticamente. </p><br><h2 id="intro-v-obuchenie-s-podkrepleniem">  Introducci√≥n en el entrenamiento de refuerzo </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/nb/vn/yxnbvndwcscdelo4q_m7zxxhfsu.png" alt="imagen"></div><br><p>  El aprendizaje reforzado es un √°rea que combina la teor√≠a del control √≥ptimo, la teor√≠a de juegos, la psicolog√≠a y la neurobiolog√≠a.  En la pr√°ctica, el aprendizaje por refuerzo se utiliza para resolver problemas de toma de decisiones y buscar estrategias de comportamiento √≥ptimas, o pol√≠ticas que son demasiado complejas para la programaci√≥n "directa".  En este caso, el agente est√° capacitado en el historial de interacciones con el medio ambiente.  El entorno, a su vez, al evaluar las acciones del agente, le proporciona una recompensa (escalar): cuanto mejor sea el comportamiento del agente, mayor ser√° la recompensa.  Como resultado, la mejor pol√≠tica se aprende del agente que ha aprendido a maximizar la recompensa total por todo el tiempo de interacci√≥n con el medio ambiente. </p><br><p>  Como un simple ejemplo, puedes jugar BreakOut.  En este viejo juego de la serie Atari, una persona / agente necesita controlar la plataforma horizontal inferior, golpear la pelota y romper gradualmente todos los bloques superiores con ella.  Cuanto m√°s derribado, mayor es la recompensa.  En consecuencia, lo que ve una persona / agente es una imagen de pantalla y es necesario tomar una decisi√≥n en qu√© direcci√≥n mover la plataforma inferior. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/an/ke/wiankeye_8wb0cxtrhpsfdf5sdm.gif" alt="imagen"></div><br><p>  Si est√° interesado en el tema de la capacitaci√≥n de refuerzo, le aconsejo un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">curso introductorio</a> genial <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de HSE</a> , as√≠ como su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">contraparte de c√≥digo abierto</a> m√°s detallada.  Si quieres algo que puedas leer, pero con ejemplos, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">libro</a> inspirado en estos dos cursos.  Revis√© / complet√© / ayud√© a crear todos estos cursos, y por lo tanto s√© por mi propia experiencia que proporcionan una base excelente. </p><br><h2 id="pro-zadachu">  Sobre la tarea </h2><br><p>  El objetivo principal de esta competencia era conseguir un agente que pudiera jugar bien en la serie de juegos SEGA: Sonic The Hedgehog.  OpenAI reci√©n comenzaba a importar juegos de SEGA a su plataforma para entrenar a agentes de RL y, por lo tanto, decidi√≥ promover un poco este momento.  Incluso el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo fue</a> lanzado con el dispositivo de todo y una descripci√≥n detallada de los m√©todos b√°sicos. </p><br><p>  Los 3 juegos de Sonic fueron compatibles, cada uno con 9 niveles, en los que, al mover una l√°grima llorosa, incluso podr√≠as jugar, recordando tu infancia (despu√©s de comprarlos en Steam primero). </p><br><p>  El estado del entorno (lo que vio el agente) fue la imagen del simulador, una imagen RGB, y como acci√≥n se le pidi√≥ al agente que eligiera qu√© bot√≥n en el joystick virtual presionar: saltar / izquierda / derecha y as√≠ sucesivamente.  El agente recibi√≥ puntos de recompensa, as√≠ como en el juego original, es decir.  para recoger anillos, as√≠ como para la velocidad de pasar el nivel.  De hecho, ten√≠amos un sonic original frente a nosotros, solo que era necesario hacerlo con la ayuda de nuestro agente. </p><br><p>  La competencia se llev√≥ a cabo del 5 de abril al 5 de junio, es decir.  solo 2 meses, lo que parece bastante peque√±o.  Nuestro equipo pudo reunirse e ingresar a la competencia solo en mayo, lo que nos hizo aprender mucho sobre la marcha. </p><br><h2 id="baselines">  L√≠neas de base </h2><br><p>  Como l√≠neas de base, se proporcionaron gu√≠as de capacitaci√≥n completas para la capacitaci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Rainbow</a> (enfoque DQN) y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PPO</a> (enfoque de Gradiente de Pol√≠ticas) en uno de los niveles posibles en Sonic y la presentaci√≥n del agente resultante. </p><br><p>  La versi√≥n Rainbow se bas√≥ en el proyecto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">anyrl</a> poco conocido, pero PPO us√≥ las viejas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">l√≠neas</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">base</a> de OpenAI y nos pareci√≥ mucho m√°s preferible. </p><br><p>  Las l√≠neas de base publicadas difer√≠an de los enfoques descritos en el art√≠culo por su mayor simplicidad y conjuntos m√°s peque√±os de "hacks" para acelerar el aprendizaje.  Por lo tanto, los organizadores arrojaron ideas y establecieron la direcci√≥n, pero la decisi√≥n sobre el uso y la implementaci√≥n de estas ideas se dej√≥ al participante en la competencia. </p><br><p>  Con respecto a las ideas, me gustar√≠a agradecer a OpenAI por su apertura, e individualmente a John Schulman por los consejos, ideas y sugerencias que expres√≥ al comienzo de esta competencia.  Nosotros, como muchos participantes (y a√∫n m√°s reci√©n llegados al mundo de RL), esto nos permiti√≥ enfocarnos mejor en el objetivo principal de la competencia: el meta aprendizaje y la mejora de la generalizaci√≥n de agentes, de lo que hablaremos ahora. </p><br><h2 id="osobennosti-ocenivaniya-resheniy">  Caracter√≠sticas de la evaluaci√≥n de decisiones. </h2><br><p>  Lo m√°s interesante comenz√≥ a la hora de evaluar a los agentes.  En las competencias / puntos de referencia t√≠picos de RL, los algoritmos se prueban en el mismo entorno en el que fueron entrenados, lo que contribuye a algoritmos que son buenos para recordar y tienen muchos hiperpar√°metros.  En el mismo concurso, la prueba del algoritmo se llev√≥ a cabo en los nuevos niveles de Sonic (que nunca se mostraron a nadie), desarrollados por el equipo de OpenAI espec√≠ficamente para este concurso.  La guinda del pastel fue el hecho de que en el proceso de prueba, el agente tambi√©n recibi√≥ una recompensa durante el paso del nivel, lo que hizo posible volver a entrenar directamente en el proceso de prueba.  Sin embargo, en este caso vali√≥ la pena recordar que las pruebas fueron limitadas tanto en tiempo, 24 horas como en ticks del juego, 1 mill√≥n.  Al mismo tiempo, OpenAI apoy√≥ firmemente la creaci√≥n de agentes que pudieran volver a entrenarse r√°pidamente a nuevos niveles.  Como ya se mencion√≥, obtener y estudiar tales soluciones fue el objetivo principal de OpenAI durante esta competencia. </p><br><p>  En el entorno acad√©mico, la direcci√≥n de estudiar pol√≠ticas que pueden adaptarse r√°pidamente a las nuevas condiciones se llama meta aprendizaje, y en los √∫ltimos a√±os se ha estado desarrollando activamente. </p><br><p>  Adem√°s, en contraste con las competencias habituales de kaggle, donde todo el env√≠o se reduce al env√≠o de su archivo de respuestas, en esta competencia (y de hecho en las competencias de RL) el equipo deb√≠a envolver su soluci√≥n en un contenedor acoplable con la API dada, recopilarla y enviarla Imagen del acoplador.  Esto aument√≥ el umbral para ingresar a la competencia, pero hizo que el proceso de decisi√≥n fuera mucho m√°s honesto: los recursos y el tiempo para la imagen del acoplador eran limitados, respectivamente, los algoritmos demasiado pesados ‚Äã‚Äãy / o lentos simplemente no pasaron la selecci√≥n.  Me parece que este enfoque para la evaluaci√≥n es mucho m√°s preferible, ya que permite a los investigadores sin un "grupo dom√©stico de DGX y AWS" competir a la par con los amantes de los modelos 50000 de vidrio.  Espero ver m√°s de este tipo de competencia en el futuro. </p><br><h2 id="komanda">  El equipo </h2><br><p>  Kolesnikov Sergey ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">scitator</a> ) <br>  RL entusiasta.  En el momento de la competencia, un estudiante del Instituto de F√≠sica y Tecnolog√≠a de Mosc√∫, MIPT, escribi√≥ y defendi√≥ un diploma de la competencia NIPS: Learning to Run del a√±o pasado (un art√≠culo sobre el cual tambi√©n debe escribirse). <br>  Senior Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dbrain</a> - Traemos al mundo real concursos listos para producci√≥n con docker y recursos limitados. </p><br><p>  Pavlov Mikhail ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fgvbrt</a> ) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Desarrollador de</a> Investigaci√≥n Senior <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DiphakLab</a> .  Repetidamente particip√≥ y gan√≥ premios en hackatones y competencias de entrenamiento reforzado. </p><br><p>  Sergeev Ilya ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sergeevii123</a> ) <br>  RL entusiasta.  Golpe√© uno de los hackathons RL de Deephack y todo comenz√≥.  Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Avito.ru</a> - visi√≥n por computadora para varios proyectos internos. </p><br><p>  Sorokin Ivan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1ytic</a> ) <br>  Comprometido en el reconocimiento de voz en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">speechpro.ru</a> . </p><br><h2 id="podhody-i-reshenie">  Enfoques y soluci√≥n </h2><br><p>  Despu√©s de una prueba r√°pida de las l√≠neas de base propuestas, nuestra elecci√≥n recay√≥ en el enfoque OpenAI: PPO, como una opci√≥n m√°s formada e interesante para desarrollar nuestra soluci√≥n.  Adem√°s, a juzgar por su art√≠culo para esta competencia, el agente PPO se ocup√≥ un poco mejor de la tarea.  Del mismo art√≠culo, nacieron las primeras mejoras que utilizamos en nuestra soluci√≥n, pero lo primero es lo primero: </p><br><ol><li><p>  Entrenamiento colaborativo de PPO en todos los niveles disponibles </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/oc/lx/iaoclxlyzkgmdgiqkkksiwzcoli.png" alt="imagen"></div><br><p>  La l√≠nea base establecida se entren√≥ en solo uno de los 27 niveles de Sonic disponibles.  Sin embargo, con la ayuda de peque√±as modificaciones, fue posible paralelizar el entrenamiento de una vez a los 27 niveles.  Debido a la mayor diversidad en el entrenamiento, el agente resultante tuvo una generalizaci√≥n mucho mayor y una mejor comprensi√≥n del dispositivo del mundo de Sonic, y por lo tanto logr√≥ un orden de magnitud mejor. </p><br></li><li><p>  Entrenamiento adicional durante las pruebas <br>  Volviendo a la idea principal de la competencia, el metaaprendizaje, era necesario encontrar un enfoque que tuviera la m√°xima generalizaci√≥n y pudiera adaptarse f√°cilmente a los nuevos entornos.  Y para la adaptaci√≥n, fue necesario volver a capacitar al agente existente para el entorno de prueba, lo que, de hecho, se realiz√≥ (en cada nivel de prueba, el agente tom√≥ 1 mill√≥n de pasos, que fue suficiente para adaptarse a un nivel espec√≠fico).  Al final de cada uno de los juegos de prueba, el agente evalu√≥ el premio recibido y optimiz√≥ su pol√≠tica utilizando la historia que acaba de recibir.  Es importante se√±alar aqu√≠ que con este enfoque es importante no olvidar toda su experiencia previa y no degradarse en condiciones espec√≠ficas, lo que, en esencia, es el principal inter√©s del meta aprendizaje, ya que dicho agente pierde inmediatamente toda su capacidad existente para generalizar. </p><br></li><li><p>  Bonos de exploraci√≥n <br>  Profundizando en las condiciones de remuneraci√≥n de un nivel, el agente recibi√≥ una recompensa por avanzar a lo largo de la coordenada x, respectivamente, pod√≠a quedar atrapado en algunos niveles, cuando primero ten√≠a que avanzar y luego retroceder.  Se decidi√≥ hacer una adici√≥n a la recompensa para el agente, la llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">exploraci√≥n basada en el conteo</a> , cuando el agente recibi√≥ una peque√±a recompensa si estaba en un estado en el que a√∫n no se encontraba.  Se implementaron dos tipos de bonificaci√≥n de exploraci√≥n: seg√∫n la imagen y seg√∫n la coordenada x del agente.  El premio basado en la imagen se calcul√≥ de la siguiente manera: para cada ubicaci√≥n de p√≠xeles en la imagen, se cont√≥ cu√°ntas veces se encontr√≥ cada valor para un episodio, el premio fue inversamente proporcional al producto en todas las ubicaciones de p√≠xeles de cu√°ntas veces se encontraron los valores en estos lugares para un episodio.  La recompensa basada en la coordenada x se consider√≥ de manera similar: para cada coordenada x (con cierta precisi√≥n) se cont√≥ cu√°ntas veces estuvo el agente en esta coordenada para el episodio, la recompensa es inversamente proporcional a esta cantidad para la coordenada x actual. </p><br></li><li><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Experimentos mixtos</a> <br>  En ‚Äúense√±ar con un maestro‚Äù recientemente, un m√©todo simple pero efectivo de aumento de datos, el llamado  confusi√≥n  La idea es muy simple: se agrega dos im√°genes de entrada arbitrarias y se asigna una suma ponderada de las etiquetas correspondientes a esta nueva imagen (por ejemplo, 0.7 <em>perro + 0.3</em> gato).  En tareas como la clasificaci√≥n de im√°genes y el reconocimiento de voz, la confusi√≥n muestra buenos resultados.  Por lo tanto, fue interesante probar este m√©todo para RL.  El aumento se realiz√≥ en cada lote grande, que consta de varios episodios.  Las im√°genes de entrada se mezclaron en p√≠xeles, pero con las etiquetas no todo fue tan simple.  Los valores devueltos, valores y neglogpacs se combinaron mediante una suma ponderada, pero la acci√≥n (acciones) se eligi√≥ del ejemplo con el coeficiente m√°ximo.  Tal soluci√≥n no mostr√≥ un aumento tangible (aunque, al parecer, deber√≠a haber un aumento en la generalizaci√≥n), pero no empeor√≥ la l√≠nea de base.  Los gr√°ficos a continuaci√≥n comparan el algoritmo PPO con confusi√≥n (rojo) y sin confusi√≥n (azul): en la parte superior est√° la recompensa durante el entrenamiento, en la parte inferior est√° la duraci√≥n del episodio. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bk/h-/jb/bkh-jbhhzowcl3gpazkbn8mun2a.jpeg" alt="imagen"></div><br></li><li><p>  Selecci√≥n de la mejor pol√≠tica inicial. <br>  Esta mejora fue una de las √∫ltimas e hizo una contribuci√≥n muy significativa al resultado final.  A nivel de capacitaci√≥n, se capacitaron varias pol√≠ticas diferentes con diferentes hiperpar√°metros.  En el nivel de prueba, para los primeros episodios, se prob√≥ cada uno de ellos, y para capacitaci√≥n adicional, se eligi√≥ la pol√≠tica que dio la m√°xima recompensa de prueba para su episodio. </p><br></li></ol><br><h2 id="bloopers">  Bloopers </h2><br><p>  Y ahora sobre la cuesti√≥n de lo que se intent√≥, pero "no vol√≥".  Despu√©s de todo, este no es un nuevo art√≠culo de SOTA para ocultar algo. </p><br><ol><li>  Cambio de arquitectura de red: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">activaci√≥n de SELU</a> , auto-atenci√≥n, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bloques SE</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Neuroevoluci√≥n</a> </li><li>  Creando tus propios niveles de Sonic: todo estaba preparado, pero no hab√≠a suficiente tiempo </li><li>  Meta-entrenamiento a trav√©s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MAML</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">REPTILE</a> </li><li>  Conjunto de varios modelos y capacitaci√≥n adicional durante la prueba de cada modelo usando muestreo de importancia </li></ol><br><h2 id="itogi">  Resumen </h2><br><p>  Despu√©s de 3 semanas desde el final de la competencia, OpenAI public√≥ los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">resultados</a> .  En 11 niveles adicionales creados adicionalmente, nuestro equipo recibi√≥ un honorable 4to lugar, saltando del 8vo en una prueba p√∫blica y superando las l√≠neas oscuras oscuras de OpenAI. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/om/iw/yvomiwj1jmffvmlicjmhe7vjp9y.png" alt="imagen"></div><br><p>  Los principales puntos distintivos que "volaron" en el primer 3ki: </p><br><ol><li>  Sistema de acciones mejorado (cre√≥ sus propios botones, elimin√≥ los adicionales); </li><li>  Investigaci√≥n de estados a trav√©s de hash de la imagen de entrada; </li><li>  M√°s niveles de entrenamiento; </li></ol><br><p>  Adem√°s, quiero se√±alar que en este concurso, adem√°s de ganar, se alent√≥ activamente la descripci√≥n de sus decisiones, as√≠ como los materiales que ayudaron a otros participantes, tambi√©n hubo una nominaci√≥n separada para esto.  Lo cual, nuevamente, aument√≥ el concurso de l√°mparas. </p><br><h2 id="posleslovie">  Ep√≠logo </h2><br><p>  Personalmente, me gust√≥ mucho esta competencia, as√≠ como el tema del meta aprendizaje.  Durante la participaci√≥n, me familiaric√© con una gran lista de art√≠culos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ni</a> siquiera <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">olvid√©</a> algunos de ellos) y aprend√≠ una gran cantidad de enfoques diferentes que espero aplicar en el futuro. </p><br><p>  En la mejor tradici√≥n de participar en la competencia, todo el c√≥digo est√° disponible y publicado en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">github</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es421585/">https://habr.com/ru/post/es421585/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es421573/index.html">Food Design Digest Agosto 2018</a></li>
<li><a href="../es421575/index.html">La confrontaci√≥n entre Yandex y Roskomnadzor madura</a></li>
<li><a href="../es421577/index.html">Se publica un exploit para una vulnerabilidad no cerrada en el Programador de tareas de Windows (traducci√≥n)</a></li>
<li><a href="../es421579/index.html">Organizaci√≥n de la interacci√≥n efectiva de microservicios.</a></li>
<li><a href="../es421583/index.html">¬øD√≥nde ir a la universidad para estudiar para un especialista en TI? + encuesta</a></li>
<li><a href="../es421587/index.html">[Ekaterimburgo, anuncio] Java Mitap - JUG.EKB</a></li>
<li><a href="../es421589/index.html">Metamorfosis: programaci√≥n molecular de la forma.</a></li>
<li><a href="../es421591/index.html">Sistema de presupuesto para video vigilancia inal√°mbrica (Wi-Fi) aut√≥noma (desde bater√≠a)</a></li>
<li><a href="../es421593/index.html">SandboxEscaper / PoC-LPE: ¬øqu√© hay dentro?</a></li>
<li><a href="../es421595/index.html">C√≥mo las personas de TI encuentran trabajo en los EE. UU. Y la UE: los 9 mejores recursos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>