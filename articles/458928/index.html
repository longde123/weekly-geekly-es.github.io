<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéõÔ∏è üë®üèæ‚Äçü§ù‚Äçüë®üèΩ üÖ±Ô∏è XLNet vs BERT ‚ôÄÔ∏è üöã ü¶ì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A finales de junio, un equipo de la Universidad Carnegie Mellon nos mostr√≥ XLNet, presentando de inmediato la publicaci√≥n , el c√≥digo y el modelo term...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>XLNet vs BERT</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/458928/"><img src="https://habrastorage.org/webt/py/g0/es/pyg0es7u25w7xb0cc8z49aczcls.png"><br><br>  A finales de junio, un equipo de la Universidad Carnegie Mellon nos mostr√≥ XLNet, presentando de inmediato la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n</a> , el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo</a> y el modelo terminado ( <a href="">XLNet-Large</a> , con carcasa: 24 capas, 1024-ocultos, 16 cabezas).  Este es un modelo pre-entrenado para resolver varios problemas de procesamiento del lenguaje natural. <br><br>  En la publicaci√≥n, inmediatamente indicaron una comparaci√≥n de su modelo con el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BERT</a> de Google.  Escriben que XLNet es superior a BERT en una gran cantidad de tareas.  Y muestra resultados en 18 tareas de vanguardia. <br><a name="habracut"></a><br><h2>  BERT, XLNet y transformadores </h2><br>  Una de las tendencias recientes en el aprendizaje profundo es el aprendizaje de transferencia.  Entrenamos modelos para resolver problemas simples en una gran cantidad de datos, y luego usamos estos modelos previamente entrenados, pero ya para resolver otros problemas m√°s espec√≠ficos.  BERT y XLNet son redes pre-entrenadas que pueden usarse para resolver problemas de procesamiento del lenguaje natural. <br><br>  Estos modelos desarrollan la idea de los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">transformadores</a> , el enfoque actualmente dominante para construir modelos para trabajar con secuencias.  Muy detallado y con ejemplos de c√≥digo sobre transformadores y el mecanismo de atenci√≥n (mecanismo de atenci√≥n) est√° escrito en el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El transformador anotado</a> . <br><br>  Si observa la tabla de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">clasificaci√≥n de evaluaci√≥n de comprensi√≥n del lenguaje general (GLUE)</a> , desde la parte superior puede ver muchos modelos basados ‚Äã‚Äãen transformadores.  Incluyendo ambos modelos que muestran mejores resultados que los humanos.  Podemos decir que con los transformadores estamos presenciando una mini revoluci√≥n en el procesamiento del lenguaje natural. <br><br><h2>  Desventajas BERT </h2><br>  BERT es un codificador autom√°tico (codificador autom√°tico, AE).  Oculta y estropea algunas palabras en la secuencia e intenta restaurar la secuencia original de palabras del contexto. <br><br>  Esto lleva a desventajas del modelo: <br><br><ul><li>  Cada palabra oculta se predice individualmente.  Perdemos informaci√≥n sobre las posibles relaciones entre palabras enmascaradas.  El art√≠culo proporciona un ejemplo llamado "Nueva York".  Si intentamos predecir independientemente estas palabras en contexto, no tendremos en cuenta la relaci√≥n entre ellas. </li><li>  Inconsistencia entre las fases de entrenamiento del modelo BERT y el uso del modelo BERT pre-entrenado.  Cuando entrenamos el modelo, tenemos palabras ocultas (tokens [MASK]), cuando usamos el modelo pre-entrenado, ya no suministramos tales tokens a la entrada. </li></ul><br>  Y sin embargo, a pesar de estos problemas, BERT mostr√≥ resultados de vanguardia en muchas tareas de procesamiento del lenguaje natural. <br><br><h2>  Caracter√≠sticas de XLNet </h2><br>  XLNet es un modelado de lenguaje autorregresivo, AR LM.  Ella est√° tratando de predecir el siguiente token de la secuencia de los anteriores.  En los modelos cl√°sicos autorregresivos, esta secuencia contextual se toma independientemente de dos direcciones de la cadena original. <br><br>  XLNet generaliza este m√©todo y forma el contexto desde diferentes lugares en la secuencia fuente.  ¬øC√≥mo lo hace?  Toma todas las permutaciones posibles (en teor√≠a) de la secuencia original y predice cada ficha en la secuencia de las anteriores. <br><br>  Aqu√≠ hay un ejemplo del art√≠culo sobre c√≥mo se predice el token x3 de varias permutaciones de la secuencia original. <br><br><img src="https://habrastorage.org/webt/yq/mb/fa/yqmbfas9mcnfkciq6pmew_-4hh8.png"><br><br>  Adem√°s, el contexto no es una bolsa de palabras.  La informaci√≥n sobre el orden inicial de los tokens tambi√©n se proporciona al modelo. <br><br>  Si dibujamos analog√≠as con el BERT, resulta que no enmascaramos las fichas por adelantado, sino que usamos diferentes conjuntos de fichas ocultas para diferentes permutaciones.  Al mismo tiempo, el segundo problema de BERT desaparece: la falta de tokens ocultos cuando se usa el modelo pre-entrenado.  En el caso de XLNet, toda la secuencia, sin m√°scaras, ya est√° ingresada. <br><br>  ¬øDe d√≥nde viene el XL en el nombre?  XL: porque XLNet utiliza el mecanismo de atenci√≥n e ideas del modelo Transformer-XL.  Aunque los lenguajes malvados afirman que XL insin√∫a la cantidad de recursos necesarios para entrenar la red. <br><br><img src="https://habrastorage.org/webt/hs/fb/u-/hsfbu-ufj-9e-me1agkauoa389c.png"><br><br>  Y sobre los recursos.  En Twitter, publicaron el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√°lculo de</a> lo que costar√≠a capacitar a la red con los par√°metros del art√≠culo.  Result√≥ 245,000 d√≥lares.  Es cierto, entonces vino un ingeniero de Google y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">corrigi√≥</a> que el art√≠culo menciona 512 chips de TPU, cuatro de los cuales est√°n en el dispositivo.  Es decir, el costo ya es de 62.440 d√≥lares, o incluso 32.720 d√≥lares, dados los 512 n√∫cleos, que tambi√©n se mencionan en el art√≠culo. <br><br><h2>  XLNet vs BERT </h2><br>  Hasta ahora, solo se ha presentado un modelo pre-entrenado para ingl√©s para el art√≠culo (XLNet-Large, Cased).  Pero el art√≠culo tambi√©n menciona experimentos con modelos m√°s peque√±os.  Y en muchas tareas, los modelos XLNet muestran mejores resultados en comparaci√≥n con modelos BERT similares. <br><br><img src="https://habrastorage.org/webt/ac/p_/th/acp_thyxqwgcuyhvfvkkixhwj_y.png"><br><br>  El advenimiento del BERT y especialmente los modelos pre-entrenados atrajeron mucha atenci√≥n de los investigadores y condujeron a una gran cantidad de trabajos relacionados.  Ahora aqu√≠ est√° XLNet.  Es interesante ver si por alg√∫n tiempo se convertir√° en el est√°ndar de facto en PNL, o viceversa, estimular√° a los investigadores en la b√∫squeda de nuevas arquitecturas y enfoques para procesar el lenguaje natural. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458928/">https://habr.com/ru/post/458928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458918/index.html">Lo que puedes aprender del dise√±o de juegos hiper-casuales</a></li>
<li><a href="../458920/index.html">Conferencia para fan√°ticos de DevOps</a></li>
<li><a href="../458922/index.html">C√≥mo pasar de ESXi a KVM / LXD y no perder la cabeza</a></li>
<li><a href="../458924/index.html">Los accidentes te ayudan a aprender</a></li>
<li><a href="../458926/index.html">La tragedia no viene sola</a></li>
<li><a href="../458930/index.html">C√≥mo los estudiantes de Perm llegaron a la final del campeonato internacional de an√°lisis de datos de Data Mining Cup 2019</a></li>
<li><a href="../458932/index.html">Yota, o c√≥mo puedes averiguarlo todo</a></li>
<li><a href="../458934/index.html">Implementaci√≥n de aplicaciones en m√∫ltiples cl√∫steres de Kubernetes con Helm</a></li>
<li><a href="../458936/index.html">"Es m√°s f√°cil responder que guardar silencio": una gran entrevista con el padre de la memoria transaccional, Maurice Herlichi</a></li>
<li><a href="../458938/index.html">C ++ 20 est√° incluido, se inicia C ++ 23. Resultados de la reuni√≥n en Colonia</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>