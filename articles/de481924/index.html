<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â›¹ğŸ¼ ğŸ» ğŸ–Šï¸ Apache Spark, Lazy Evaluation und mehrseitige SQL-Abfragen ğŸ˜ ğŸŒ¾ ğŸ’Ÿ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das berÃ¼hmte: spark arbeitet mit Datenrahmen, bei denen es sich um Transformationsalgorithmen handelt. Der Algorithmus wird im allerletzten Moment ges...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Spark, Lazy Evaluation und mehrseitige SQL-Abfragen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/alfastrah/blog/481924/"><p>  Das berÃ¼hmte: spark arbeitet mit Datenrahmen, bei denen es sich um Transformationsalgorithmen handelt.  Der Algorithmus wird im allerletzten Moment gestartet, um der Optimierung "mehr Platz" zu geben und sie aufgrund der Optimierung so effizient wie mÃ¶glich auszufÃ¼hren. </p><br><p>  Im Folgenden wird analysiert, wie eine mehrseitige SQL-Abfrage in Atome zerlegt werden kann (ohne EinbuÃŸen bei der Effizienz) und wie die AusfÃ¼hrungszeit der ETL-Pipeline aufgrund dessen erheblich verkÃ¼rzt werden kann. </p><a name="habracut"></a><br><h1 id="lazy-evaluation">  Faule Bewertung </h1><br><p>  Ein interessantes Funktionsmerkmal von spark ist die verzÃ¶gerte Auswertung: Transformationen werden nur ausgefÃ¼hrt, wenn die Aktionen abgeschlossen sind.  So funktioniert es (grob): Die Algorithmen zum Erstellen der Datenrahmen vor der Aktion sind â€zusammengeklebtâ€œ. Der Optimierer erstellt aus seiner Sicht den effizientesten endgÃ¼ltigen Algorithmus, der startet und das Ergebnis liefert (das von der Aktion angeforderte). </p><br><p>  Was hier im Zusammenhang mit unserer PrÃ¤sentation interessant ist: Jede komplexe Abfrage kann ohne Effizienzverlust in â€Atomeâ€œ zerlegt werden.  Lassen Sie uns etwas weiter analysieren. </p><br><h1 id="mnogostranichnyy-sql">  Mehrseitiges SQL </h1><br><p>  Es gibt viele GrÃ¼nde, warum wir "mehrseitige" SQL-Abfragen schreiben, eine der Hauptursachen fÃ¼r die ZurÃ¼ckhaltung, Zwischenobjekte zu erstellen (ZurÃ¼ckhaltung, die durch Effizienzanforderungen gestÃ¼tzt wird).  Das folgende Beispiel zeigt eine relativ komplexe Abfrage (es ist natÃ¼rlich auch sehr einfach, aber fÃ¼r die weitere Darstellung haben wir genug). </p><br><pre><code class="python hljs">qSel = <span class="hljs-string"><span class="hljs-string">""" select con.contract_id as con_contract_id, con.begin_date as con_begin_date, con.product_id as con_product_id, cst.contract_status_type_id as cst_status_type_id, sbj.subject_id as sbj_subject_id, sbj.subject_name as sbj_subject_name, pp.birth_date as pp_birth_date from kasko.contract con join kasko.contract_status cst on cst.contract_status_id = con.contract_status_id join kasko.subject sbj on sbj.subject_id = con.owner_subject_id left join kasko.physical_person pp on pp.subject_id = con.owner_subject_id """</span></span> dfSel = sp.sql(qSel)</code> </pre> <br><p>  Was sehen wir: </p><br><ul><li>  Daten werden aus mehreren Tabellen ausgewÃ¤hlt </li><li>  Es werden verschiedene Join-Typen verwendet </li><li>  AuswÃ¤hlbare Spalten werden nach Select Part, Join Part (und Where Part, aber hier ist es nicht hier - ich habe es der Einfachheit halber entfernt) verteilt. </li></ul><br><p>  Diese Abfrage kann in einfache Abfragen zerlegt werden (z. B. zuerst die Tabellen contract und contract_status verknÃ¼pfen, das Ergebnis in einer temporÃ¤ren Tabelle speichern und dann mit subject kombinieren, das Ergebnis auch in einer temporÃ¤ren Tabelle speichern usw.).  Wenn wir wirklich komplexe Abfragen erstellen, tun wir dies sicherlich, und dann - nach dem Debuggen - sammeln wir all dies in einem mehrseitigen Block. </p><br><p>  Was ist schlecht hier?  Nichts, in der Tat, jeder arbeitet so und ist daran gewÃ¶hnt. </p><br><p>  Aber es gibt Nachteile - oder besser gesagt, was zu verbessern ist - lesen Sie weiter. </p><br><h1 id="tot-zhe-zapros-v-spark">  Die gleiche Abfrage im Funken </h1><br><p>  Wenn Sie spark fÃ¼r die Transformation verwenden, kÃ¶nnen Sie diese Anforderung natÃ¼rlich einfach annehmen und ausfÃ¼hren (und es ist gut, wir werden sie auch ausfÃ¼hren), aber Sie kÃ¶nnen auch in die andere Richtung gehen, probieren wir es aus. </p><br><p>  Zerlegen wir diese "komplexe" Abfrage in "Atome" - elementare Datenrahmen.  Wir erhalten so viele von ihnen wie Tabellen an der Abfrage beteiligt sind (in diesem Fall 4). </p><br><p>  Hier sind sie - "Atome": </p><br><pre> <code class="python hljs">dfCon = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select contract_id as con_contract_id, begin_date as con_begin_date, product_id as con_product_id, owner_subject_id as con_owner_subject_id, contract_status_id as con_contract_status_id from kasko.contract"""</span></span>) dfCStat = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select contract_status_id as cst_status_id, contract_status_type_id as cst_status_type_id from kasko.contract_status"""</span></span>) dfSubj = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select subject_id as sbj_subject_id, subject_type_id as sbj_subject_type_id, subject_name as sbj_subject_name from kasko.subject"""</span></span>) dfPPers = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select subject_id as pp_subject_id, birth_date as pp_birth_date from kasko.physical_person"""</span></span>)</code> </pre> <br><p>  Mit Spark kÃ¶nnen Sie sie mit AusdrÃ¼cken verbinden, die von den tatsÃ¤chlichen â€Atomenâ€œ getrennt sind. </p><br><pre> <code class="python hljs">con_stat = f.col(<span class="hljs-string"><span class="hljs-string">"cst_status_id"</span></span>)==f.col(<span class="hljs-string"><span class="hljs-string">"con_contract_status_id"</span></span>) con_subj_own = f.col(<span class="hljs-string"><span class="hljs-string">"con_owner_subject_id"</span></span>)==f.col(<span class="hljs-string"><span class="hljs-string">"sbj_subject_id"</span></span>) con_ppers_own = f.col(<span class="hljs-string"><span class="hljs-string">"con_owner_subject_id"</span></span>)==f.col(<span class="hljs-string"><span class="hljs-string">"pp_subject_id"</span></span>)</code> </pre> <br><p>  Dann sieht unsere "komplexe Abfrage" so aus: </p><br><pre> <code class="python hljs">dfAtom = dfCon.join(dfCStat,con_stat, <span class="hljs-string"><span class="hljs-string">"inner"</span></span>)\ .join(dfSubj,con_subj_own,<span class="hljs-string"><span class="hljs-string">"inner"</span></span>) \ .join(dfPPers,con_ppers_own, <span class="hljs-string"><span class="hljs-string">"left"</span></span>) \ .drop(<span class="hljs-string"><span class="hljs-string">"con_contract_status_id"</span></span>,<span class="hljs-string"><span class="hljs-string">"sbj_subject_type_id"</span></span>, <span class="hljs-string"><span class="hljs-string">"pp_subject_id"</span></span>,<span class="hljs-string"><span class="hljs-string">"con_owner_subject_id"</span></span>,<span class="hljs-string"><span class="hljs-string">"cst_status_id"</span></span>)</code> </pre> <br><p>  Was ist hier gut?  Auf den ersten Blick ist es nichts, ganz im Gegenteil: Mit "komplexem" SQL kÃ¶nnen Sie verstehen, was passiert, mit unserer "atomaren" Abfrage ist es schwieriger zu verstehen, Sie mÃ¼ssen "Atome" und AusdrÃ¼cke betrachten. </p><br><p>  Vergewissern wir uns zunÃ¤chst, dass diese Abfragen gleichwertig sind - im Jupyter-Buch habe ich PlÃ¤ne fÃ¼r die ErfÃ¼llung beider Abfragen angegeben (die Neugierigen kÃ¶nnen 10 Unterschiede feststellen, aber das Wesentliche - die Gleichwertigkeit - liegt auf der Hand).  Dies ist natÃ¼rlich kein Wunder, es sollte auch so sein (siehe oben fÃ¼r eine verzÃ¶gerte Auswertung und Optimierung). </p><br><p>  Was wir am Ende haben - die "mehrseitige" Anfrage und die "atomare" Anfrage arbeiten mit der gleichen Effizienz (dies ist wichtig, ohne dass weitere Ãœberlegungen teilweise ihre Bedeutung verlieren). </p><br><p>  Nun, lassen Sie uns das Gute in der "atomaren" Art der Erstellung von Abfragen finden. </p><br><p>  Was ist ein "Atom" (elementarer Datenrahmen) ist unser Wissen Ã¼ber eine Teilmenge des Themenbereichs (Teil der relationalen Tabelle).  Durch die Isolierung solcher â€Atomeâ€œ wÃ¤hlen wir automatisch (und vor allem algorithmisch und reproduzierbar) einen wesentlichen Teil des fÃ¼r uns grenzenlosen Objekts aus, das als â€physikalisches Datenmodellâ€œ bezeichnet wird. </p><br><p>  Was ist der Ausdruck, den wir beim Beitritt verwendet haben?  Dies ist auch Wissen Ã¼ber den Themenbereich - auf diese Weise werden (wie im Ausdruck angegeben) die EntitÃ¤ten des Themenbereichs (Tabellen in der Datenbank) miteinander verbunden. </p><br><p>  Ich wiederhole - das ist wichtig - dieses â€Wissenâ€œ (Atome und AusdrÃ¼cke) materialisiert sich im ausfÃ¼hrbaren Code (nicht im Diagramm oder in der verbalen Beschreibung). Dies ist der Code, der jedes Mal ausgefÃ¼hrt wird, wenn die ETL-Pipeline ausgefÃ¼hrt wird (das Beispiel stammt Ã¼brigens aus dem wirklichen Leben). </p><br><p>  Der ausfÃ¼hrbare Code - wie wir ihn von CleanCoder kennen - ist eines von zwei objektiv vorhandenen Artefakten, die behaupten, der "Titel" der Dokumentation zu sein.  Das heiÃŸt, die Verwendung von â€Atomenâ€œ ermÃ¶glicht es uns, in einem so wichtigen Prozess wie der Dokumentation von Daten einen Schritt nach vorne zu machen. </p><br><p>  Was kann man sonst noch in â€AtomizitÃ¤tâ€œ finden? </p><br><h1 id="optimizaciya-konveyerov">  FÃ¶rdereroptimierung </h1><br><p>  Im wirklichen Leben besteht eine ETL-Pipeline aus Dutzenden von Transformationen, die den oben genannten Ã¤hnlich sind. Dies habe ich mir Ã¼brigens nicht vorgestellt.  Tabellen werden in ihnen sehr oft wiederholt (ich habe sie irgendwie in Excel berechnet - einige Tabellen werden in 40% der Abfragen verwendet). </p><br><p>  Was passiert in Bezug auf Effizienz?  Durcheinander - dieselbe Tabelle wird mehrmals aus der Quelle gelesen ... </p><br><p>  Wie kann man es verbessern?  Spark hat einen Mechanismus zum Zwischenspeichern von Datenrahmen - wir kÃ¶nnen explizit angeben, welche Datenrahmen und wie viel im Cache gespeichert werden sollen. </p><br><p>  Dazu mÃ¼ssen wir doppelte Tabellen auswÃ¤hlen und Abfragen so erstellen, dass die Gesamt-Cache-GrÃ¶ÃŸe minimiert wird (da per Definition nicht alle Tabellen hineinpassen, gibt es groÃŸe Datenmengen). </p><br><p>  Kann dies mit mehrseitigen SSQ-Abfragen erfolgen?  Ja, aber ... ein bisschen kompliziert (wir haben dort nicht wirklich Datenrahmen, nur Tabellen, sie kÃ¶nnen auch zwischengespeichert werden - die Spark-Community arbeitet daran). </p><br><p>  Kann dies mit atomaren Abfragen erfolgen?  Ja  Und es ist nicht schwierig, wir mÃ¼ssen nur die â€Atomeâ€œ verallgemeinern - fÃ¼gen Sie die Spalten hinzu, die in allen Abfragen unserer Pipeline verwendet werden.  Wenn Sie darÃ¼ber nachdenken, ist dies unter dem Gesichtspunkt der Dokumentation â€richtigâ€œ: Wenn in einer Abfrage eine Spalte verwendet wird (auch im Where-Teil), ist dies ein Teil der Daten des Themenbereichs, der fÃ¼r uns interessant ist. </p><br><p>  Und dann ist alles einfach - wir zwischenspeichern sich wiederholende Atome (Datenrahmen), wir bilden eine Kette von Transformationen, so dass die Schnittmenge zwischengespeicherter Datenrahmen minimal ist (dies ist Ã¼brigens nicht trivial, aber algorithmisierbar). </p><br><p>  Und wir bekommen den effizientesten FÃ¶rderer komplett â€gratisâ€œ.  Ein nÃ¼tzliches und wichtiges Artefakt ist darÃ¼ber hinaus die â€Vorbereitungâ€œ zur Dokumentation von Daten zum Themenbereich. </p><br><h1 id="robotizaciya-i-avtomatizaciya">  Robotisierung und Automatisierung </h1><br><p>  Atome sind anfÃ¤lliger fÃ¼r die automatische Verarbeitung als "groÃŸes und mÃ¤chtiges SQL" - ihre Struktur ist einfach und klar, Spark parst fÃ¼r uns (wofÃ¼r er sich besonders bedankt), er erstellt auch AbfrageplÃ¤ne und analysiert, mit denen Sie die Reihenfolge der Abfrageverarbeitung automatisch neu ordnen kÃ¶nnen. </p><br><p>  Hier kÃ¶nnen Sie also etwas spielen. </p><br><h1 id="v-zaklyuchenie">  AbschlieÃŸend </h1><br><p>  Vielleicht bin ich zu optimistisch - es scheint mir, dass dieser Pfad (Query Atomization) mehr funktioniert, als zu versuchen, eine Datenquelle nachtrÃ¤glich zu beschreiben.  Ãœbrigens, was ist die Verwendung von "Additiven"? Wir erzielen auÃŸerdem eine Effizienzsteigerung.  Warum betrachte ich den atomaren Ansatz als "funktionierend"?  Es ist Teil des regulÃ¤ren Prozesses, was bedeutet, dass die beschriebenen Artefakte eine echte Chance haben, auf lange Sicht relevant zu sein. </p><br><p>  Ich habe wahrscheinlich etwas verpasst - Hilfe finden (in den Kommentaren)? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de481924/">https://habr.com/ru/post/de481924/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de481910/index.html">Flexible VerticalSwipeBehavior schreiben</a></li>
<li><a href="../de481912/index.html">Ich war eine Woche lang Praktikant bei SRE-engineer. Beobachten Sie mit den Augen eines Software-Ingenieurs</a></li>
<li><a href="../de481914/index.html">Spring Boot vs Spring MVC vs Spring - Wie vergleichen sie?</a></li>
<li><a href="../de481916/index.html">Woran hat sich das Jahr 2019 in der Entwicklung erinnert?</a></li>
<li><a href="../de481922/index.html">Neujahr IMaskjs 6 - Reagieren Sie Native, Pipes, ESM</a></li>
<li><a href="../de481926/index.html">Lernen Sie die neue Veeam Backup for AWS-LÃ¶sung kennen</a></li>
<li><a href="../de481930/index.html">Entwicklungskultur: Wie Leistung und Effizienz bewertet werden</a></li>
<li><a href="../de481932/index.html">Bereitstellung und Datenbanken ohne Ausfallzeiten</a></li>
<li><a href="../de481934/index.html">Analyse: Warum der Kurs der Tesla-Aktie steigt</a></li>
<li><a href="../de481936/index.html">Vor- und Nachteile von A / B-Tests: Erfahrung groÃŸer Unternehmen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>