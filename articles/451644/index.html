<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤸🏻 🎷 🤚🏽 Implementación de aplicaciones en VM, Nomad y Kubernetes 🙋 👸🏽 🍈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos! Mi nombre es Pavel Agaletsky. Trabajo como líder de equipo en un equipo que desarrolla un sistema de entrega Lamoda. En 2018, hablé en l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implementación de aplicaciones en VM, Nomad y Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lamoda/blog/451644/">  Hola a todos!  Mi nombre es Pavel Agaletsky.  Trabajo como líder de equipo en un equipo que desarrolla un sistema de entrega Lamoda.  En 2018, hablé en la conferencia HighLoad ++, y hoy quiero presentar una transcripción de mi informe. <br><br>  Mi tema está dedicado a la experiencia de nuestra empresa en la implementación de sistemas y servicios en diferentes entornos.  Comenzando desde nuestros tiempos prehistóricos, cuando implementamos todos los sistemas en servidores virtuales regulares, terminando con una transición gradual de Nomad a una implementación en Kubernetes.  Te diré por qué hicimos esto y qué problemas tuvimos en el proceso. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/oqrb7dWECSo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br><h1>  Implementar aplicaciones en VM </h1><br>  Para empezar, hace 3 años, todos los sistemas y servicios de la compañía se implementaron en servidores virtuales comunes.  Técnicamente, se organizó para que todo el código de nuestros sistemas se distribuyera y ensamblara utilizando herramientas de compilación automáticas con jenkins.  Con Ansible, estaba implementando nuestro sistema de control de versiones en servidores virtuales.  Además, cada sistema que estaba en nuestra empresa se implementó al menos en 2 servidores: uno de ellos, en la cabeza, el segundo, en la cola.  Estos dos sistemas eran absolutamente idénticos entre sí en todos sus ajustes, potencia, configuración y más.  La única diferencia entre ellos era que la cabeza recibía tráfico de usuarios, mientras que la cola nunca recibía tráfico de usuarios. <br><br>  ¿Por qué se hizo esto? <br><br>  Cuando implementamos nuevas versiones de nuestra aplicación, queríamos ofrecer la posibilidad de un despliegue continuo, es decir, sin consecuencias notables para los usuarios.  Esto se logró debido al hecho de que la próxima versión ensamblada con Ansible se lanzó en cola.  Allí, las personas que participaron en el despliegue pudieron verificar y asegurarse de que todo estaba bien: todas las métricas, secciones y aplicaciones funcionaron;  Se inician los scripts necesarios.  Solo después de que se convencieron de que todo está bien, se cambió el tráfico.  Comenzó a ir al servidor que antes era cola.  Y el que era la cabeza antes, se quedó sin tráfico de usuarios, mientras que con la versión anterior de nuestra aplicación. <br><br>  Por lo tanto, para los usuarios fue perfecto.  Porque la conmutación es simultánea, ya que es solo una conmutación equilibradora.  Es muy fácil volver a la versión anterior simplemente cambiando el balanceador hacia atrás.  También podríamos verificar la capacidad de producción de la aplicación incluso antes de que el tráfico de usuarios llegue a ella, lo cual fue lo suficientemente conveniente. <br><br>  ¿Qué ventajas vimos en todo esto? <br><br><ol><li>  En primer lugar, <b>funciona de manera</b> bastante <b>simple.</b>  Todos entienden cómo funciona este esquema de implementación, porque la mayoría de las personas se han implementado en servidores virtuales comunes. </li><li> Esto es bastante <b>confiable</b> , ya que la tecnología de implementación es simple, probada por miles de empresas.  Millones de servidores se implementan de esa manera.  Es difícil romper algo. </li><li>  Y finalmente, podríamos obtener <b>despliegues atómicos</b> .  Implementaciones que ocurren a los usuarios simultáneamente, sin una etapa notable de cambio entre la versión anterior y la nueva. </li></ol><br>  Pero en esto también vimos varias deficiencias: <br><br><ol><li>  Además del entorno de producción, el entorno de desarrollo, hay otros entornos.  Por ejemplo, qa y preproducción.  En ese momento, teníamos muchos servidores y unos 60 servicios.  Por esta razón, era necesario <b>que cada servicio mantuviera la versión de la</b> máquina virtual <b>que le era relevante</b> .  Además, si desea actualizar bibliotecas o instalar nuevas dependencias, debe hacerlo en todos los entornos.  También era necesario sincronizar la hora en la que iba a implementar la próxima versión nueva de su aplicación con la hora en que los desarrolladores hicieron la configuración de entorno necesaria.  En este caso, es fácil entrar en una situación en la que nuestro entorno será ligeramente diferente a la vez en todos los entornos seguidos.  Por ejemplo, en el entorno de QA habrá algunas versiones de bibliotecas, y en producción, otras, lo que generará problemas. </li><li>  <b>Dificultad para actualizar las dependencias de</b> su aplicación.  No depende de ti, sino del otro equipo.  Es decir, desde el comando devops, que admite el servidor.  Debe establecer una tarea adecuada para ellos y dar una descripción de lo que desea hacer. </li><li>  En ese momento, también queríamos dividir los grandes monolitos grandes que teníamos en pequeños servicios separados, ya que entendimos que habría más y más de ellos.  En ese momento, ya teníamos más de 100 de ellos. Era necesario que cada nuevo servicio creara una nueva máquina virtual separada, que también necesita ser reparada e implementada.  Además, no necesita un automóvil, sino al menos dos.  A esto, el entorno de control de calidad todavía se está agregando.  Esto causa problemas y hace que crear y lanzar nuevos sistemas sea más <b>difícil, costoso y lento para usted.</b> </li></ol><br>  Por lo tanto, decidimos que sería más conveniente pasar de la implementación de máquinas virtuales ordinarias a la implementación de nuestras aplicaciones en el contenedor docker.  Si tiene docker, necesita un sistema que pueda ejecutar la aplicación en el clúster, ya que no puede simplemente levantar el contenedor.  Por lo general, desea realizar un seguimiento de cuántos contenedores se levantan para que se eleven automáticamente.  Por esta razón, tuvimos que elegir un sistema de control. <br><br>  Pensamos durante mucho tiempo sobre cuál se puede tomar.  El hecho es que en ese momento esta pila de implementaciones en servidores virtuales comunes estaba algo desactualizada, ya que no había las últimas versiones de los sistemas operativos allí.  En algún momento, incluso FreeBSD se quedó allí, lo que no era muy conveniente de mantener.  Entendemos que debe migrar a Docker lo más rápido posible.  Nuestros desarrolladores analizaron su experiencia existente con diferentes soluciones y eligieron un sistema como Nomad. <br><br><h1>  Cambiar a nómada </h1><br>  Nomad es un producto de HashiCorp.  También son conocidos por sus otras decisiones: <br><br><img src="https://habrastorage.org/webt/4e/vz/4e/4evz4entvdaauztnu558tb-2z9u.jpeg" alt="imagen"><br><br>  <b>Consul</b> es una herramienta para el descubrimiento de servicios. <br><br>  <b>Terraform</b> es un sistema de administración de servidores que le permite configurarlos a través de una configuración llamada infraestructura como código. <br><br>  <b>Vagrant le</b> permite implementar máquinas virtuales localmente o en la nube a través de archivos de configuración específicos. <br><br>  Nomad en ese momento parecía una solución bastante simple a la que puede cambiar rápidamente sin cambiar toda la infraestructura.  Además, se domina con bastante facilidad.  Por lo tanto, lo elegimos como nuestro sistema de filtro para nuestro contenedor. <br><br>  ¿Qué se necesita para implementar completamente su sistema en Nomad? <br><br><ol><li>  En primer lugar, necesita la <b>imagen acoplable de</b> su aplicación.  Debe compilarlo y colocarlo en el almacenamiento de imágenes de la ventana acoplable.  En nuestro caso, esto es un artefacto, un sistema que le permite insertar varios artefactos de varios tipos.  Puede almacenar archivos, imágenes acoplables, paquetes de compositor PHP, paquetes NPM, etc. </li><li>  También necesita un <b>archivo de configuración</b> que le indique a Nomad qué, dónde y cuánto desea implementar. </li></ol><br>  Cuando hablamos de Nomad, utiliza el lenguaje HCL como un formato de archivo de información, que significa <i>HashiCorp Configuration Language</i> .  Este es un superconjunto de Yaml que le permite describir su servicio en términos de Nomad. <br><br><img src="https://habrastorage.org/webt/vg/q9/vb/vgq9vb_izh4i890ro7giihibz6g.jpeg" alt="imagen"><br><br>  Le permite decir cuántos contenedores desea implementar, desde qué imágenes transferirles varios parámetros durante la implementación.  Por lo tanto, alimenta este archivo Nomad y lanza contenedores en producción de acuerdo con él. <br><br>  En nuestro caso, nos dimos cuenta de que simplemente escribir exactamente los mismos archivos HLC idénticos para cada servicio no sería muy conveniente, porque hay muchos servicios y, a veces, desea actualizarlos.  Sucede que un servicio se implementa no en una instancia, sino en las más diferentes.  Por ejemplo, uno de los sistemas que tenemos en producción tiene más de 100 instancias en la producción.  Se inician desde las mismas imágenes, pero difieren en la configuración y los archivos de configuración. <br><br>  Por lo tanto, decidimos que sería conveniente para nosotros almacenar todos nuestros archivos de configuración para la implementación en un repositorio común.  Por lo tanto, se volvieron observables: eran fáciles de mantener y era posible ver qué sistemas teníamos.  Si es necesario, también es fácil actualizar o cambiar algo.  Agregar un nuevo sistema tampoco es difícil: solo ingrese el archivo de configuración dentro del nuevo directorio.  En su interior están los archivos: service.hcl, que contiene una descripción de nuestro servicio, y algunos archivos env que permiten que este servicio, que se implementa en producción, se configure. <br><br><img src="https://habrastorage.org/webt/-g/l7/8h/-gl78hl7nbmdbhbuacuntgxw4ow.jpeg" alt="imagen"><br><br>  Sin embargo, algunos de nuestros sistemas se implementan en el producto no en una copia, sino en varias a la vez.  Por lo tanto, decidimos que sería conveniente para nosotros no almacenar configuraciones en su forma pura, sino en su forma de plantilla.  Y como lenguaje de plantilla elegimos <i>jinja 2</i> .  En este formato, almacenamos tanto las configuraciones del servicio como los archivos env necesarios para ello. <br><br>  Además, colocamos en el repositorio un despliegue de secuencia de comandos común para todos los proyectos, lo que le permite lanzar e implementar su servicio en producción, en el entorno deseado, en el objetivo deseado.  En el caso en que convertimos nuestra configuración HCL en una plantilla, el archivo HCL, que anteriormente era una configuración Nomad normal, en este caso comenzó a verse un poco diferente. <br><br><img src="https://habrastorage.org/webt/9a/ib/zv/9aibzvsme34ra2eg53bjfbqt1tw.jpeg" alt="imagen"><br><br>  Es decir, reemplazamos algunas variables en el archivo de configuración con inserciones variables, que se toman de los archivos env o de otras fuentes.  Además, pudimos recopilar archivos HL dinámicamente, es decir, podemos usar no solo las inserciones variables habituales.  Dado que jinja admite bucles y condiciones, también puede crear archivos de configuración allí, que varían según dónde implemente exactamente sus aplicaciones. <br><br>  Por ejemplo, desea implementar su servicio en preproducción y en producción.  Suponga que en la preproducción no desea ejecutar scripts de corona, solo desea ver el servicio en un dominio separado para asegurarse de que esté funcionando.  Para cualquiera que implemente un servicio, el proceso parece muy simple y transparente.  Es suficiente para ejecutar el archivo deploy.sh, especifique qué servicio desea implementar y en qué destino.  Por ejemplo, desea implementar un determinado sistema en Rusia, Bielorrusia o Kazajstán.  Para hacer esto, simplemente cambie uno de los parámetros y tendrá el archivo de configuración correcto. <br><br>  Cuando el servicio Nomad ya está implementado en su clúster, se ve así. <br><br><img src="https://habrastorage.org/webt/nu/au/8d/nuau8dqdcwft0boyw57x37wrkcm.jpeg" alt="imagen"><br><br>  Primero, necesita un equilibrador externo que incorpore todo el tráfico de usuarios.  Trabajará junto con Cónsul y descubrirá de él dónde, en qué nodo, en qué dirección IP hay un servicio específico que corresponde a un nombre de dominio particular.  Los servicios en Consul provienen del propio Nomad.  Como se trata de productos de la misma empresa, están bien conectados.  Podemos decir que Nomad listo para usar puede registrar todos los servicios lanzados en él dentro de Consul. <br><br>  Después de que su equilibrador externo descubre a qué servicio es necesario enviar tráfico, lo redirige al contenedor apropiado oa varios contenedores que corresponden a su aplicación.  Naturalmente, también es necesario pensar en la seguridad.  Aunque todos los servicios se ejecutan en las mismas máquinas virtuales en contenedores, esto generalmente requiere la prohibición del acceso gratuito de cualquier servicio a cualquier otro.  Logramos esto a través de la segmentación.  Cada servicio se lanzó en su propia red virtual, en la que se prescribieron reglas de enrutamiento y reglas para permitir / denegar el acceso a otros sistemas y servicios.  Podrían ubicarse tanto dentro de este grupo como fuera de él.  Por ejemplo, si desea evitar que un servicio se conecte a una base de datos específica, esto puede hacerse mediante la segmentación a nivel de red.  Es decir, incluso por error, no puede conectarse accidentalmente desde un entorno de prueba a su base de producción. <br><br>  ¿Cuánto nos costó la transición en términos de recursos humanos? <br><br>  La transición de toda la compañía a Nomad tomó alrededor de 5-6 meses.  Cambiamos sin servicio, pero a un ritmo bastante rápido.  Cada equipo tuvo que crear sus propios contenedores para los servicios. <br><br>  Hemos adoptado un enfoque tal que cada equipo es responsable de las imágenes acopladas de sus sistemas por su cuenta.  Los Devops también proporcionan la infraestructura general necesaria para la implementación, es decir, soporte para el clúster en sí, soporte para el sistema CI, etc.  Y en ese momento teníamos más de 60 sistemas trasladados a Nomad, que resultó en unos 2 mil contenedores. <br><br>  Devops es responsable de la infraestructura general de todo lo relacionado con la implementación, con los servidores.  Y cada equipo de desarrollo, a su vez, es responsable de la implementación de contenedores para su sistema específico, ya que es el equipo que sabe lo que generalmente necesita en un contenedor en particular. <br><br><h1>  Razones para abandonar a Nomad </h1><br>  ¿Qué ventajas obtuvimos al cambiar a implementar usando Nomad y Docker también? <br><br><ol><li>  <b>Proporcionamos las mismas condiciones</b> para todos los entornos.  En una empresa de desarrollo, entorno de control de calidad, preproducción, producción, se utilizan las mismas imágenes de contenedor, con las mismas dependencias.  En consecuencia, prácticamente no tiene posibilidades de que la producción resulte ser diferente de lo que probó anteriormente localmente o en un entorno de prueba. </li><li>  También descubrimos que es bastante <b>fácil agregar un nuevo servicio</b> .  Desde el punto de vista de la implementación, cualquier sistema nuevo se lanza de manera muy simple.  Es suficiente ir al repositorio que almacena las configuraciones, agregar la siguiente configuración para su sistema allí y ya está listo.  Puede implementar su sistema en producción sin esfuerzo adicional de los desarrolladores. </li><li>  Todos <b>los archivos de configuración</b> en un repositorio común <b>resultaron ser monitoreados</b> .  En ese momento, cuando implementamos nuestros sistemas usando servidores virtuales, usamos Ansible, en el que las configuraciones se encuentran en el mismo repositorio.  Sin embargo, para la mayoría de los desarrolladores fue un poco más difícil trabajar con ellos.  Aquí, el volumen de configuraciones y código que necesita agregar para implementar el servicio se ha vuelto mucho más pequeño.  Además para devops es muy fácil arreglarlo o cambiarlo.  En el caso de las transiciones, por ejemplo, en la nueva versión de Nomad, pueden tomar y actualizar masivamente todos los archivos operativos que se encuentran en el mismo lugar. </li></ol><br>  Pero también enfrentamos varias deficiencias: <br><br>  Resultó que no <b>pudimos lograr implementaciones sin problemas</b> en el caso de Nomad.  Al sacar los contenedores de diferentes condiciones, podría resultar que se estaba ejecutando, y Nomad lo percibió como un contenedor listo para aceptar el tráfico.  Esto sucedió incluso antes de que la aplicación en su interior se iniciara.  Por esta razón, el sistema comenzó a producir 500 errores por poco tiempo, porque el tráfico comenzó a ir al contenedor, que aún no está listo para recibirlo. <br><br>  Nos encontramos con algunos <b>errores</b> .  El error más significativo es que Nomad no acepta muy bien un clúster grande si tiene muchos sistemas y contenedores.  Cuando desee poner en servicio uno de los servidores incluidos en el clúster Nomad, existe una alta probabilidad de que el clúster no se sienta muy bien y se desmorone.  Parte de los contenedores puede, por ejemplo, caerse y no elevarse, lo que posteriormente será muy costoso para usted si todos sus sistemas de producción están ubicados en un clúster administrado por Nomad. <br><br>  Por lo tanto, decidimos pensar a dónde ir después.  En ese momento, nos dimos cuenta de lo que queríamos lograr.  A saber: queremos confiabilidad, un poco más de funciones que las que ofrece Nomad y un sistema más maduro y estable. <br><br>  En este sentido, nuestra elección recayó en Kubernetes como la plataforma más popular para el lanzamiento de clústeres.  Especialmente siempre que el tamaño y la cantidad de nuestros contenedores fuera bastante grande.  Para tales propósitos, Kubernetes parecía el sistema más adecuado de los que podíamos ver. <br><br><h1>  Yendo a Kubernetes </h1><br>  Hablaré un poco sobre los conceptos básicos de Kubernetes y cómo se diferencian de Nomad. <br><br><img src="https://habrastorage.org/webt/pv/eh/va/pvehvavusoxszoxum9bsuwyjqbc.jpeg" alt="imagen"><br><br>  En primer lugar, el concepto más básico en Kubernetes es el concepto de pod.  <b>Un pod</b> es un grupo de uno o más contenedores que siempre se ejecutan juntos.  Y parecen funcionar siempre estrictamente en la misma máquina virtual.  Están disponibles entre sí a través de IP 127.0.0.1 en diferentes puertos. <br><br>  Supongamos que tiene una aplicación PHP que consiste en nginx y php-fpm, un circuito clásico.  Lo más probable es que desee que los contenedores nginx y php-fpm estén siempre juntos.  Kubernetes hace esto describiéndolos como una vaina común.  Esto es exactamente lo que no pudimos obtener con la ayuda de Nomad. <br><br>  El segundo concepto es el <b>despliegue</b> .  El hecho es que la cápsula en sí es algo efímero, comienza y desaparece.  Ya sea que desee eliminar primero todos sus contenedores anteriores y luego lanzar nuevas versiones a la vez, o si desea implementarlos gradualmente, este es el concepto del que es responsable la implementación.  Describe cómo despliega sus pods, en cuántos y cómo actualizarlos. <br><br>  El tercer concepto es el <b>servicio</b> .  Su servicio es en realidad su sistema, que recibe algo de tráfico y luego lo dirige a uno o más pods que corresponden a su servicio.  Es decir, le permite decir que todo el tráfico entrante a dicho servicio con dicho nombre debe enviarse a estos pods en particular.  Y al mismo tiempo, le proporciona equilibrio de tráfico.  Es decir, puede ejecutar dos pods de su aplicación, y todo el tráfico entrante se equilibrará de manera uniforme entre los pods relacionados con este servicio. <br><br>  Y el cuarto concepto básico es <b>Ingress</b> .  Este es un servicio que se ejecuta en un clúster de Kubernetes.  Actúa como un equilibrador de carga externo, que acepta todas las solicitudes.  Debido a la API, Kubernetes Ingress puede determinar dónde se deben enviar estas solicitudes.  Y lo hace con mucha flexibilidad.  Puede decir que todas las solicitudes a este host y dicha URL se envían a este servicio.  Y enviamos estas solicitudes a este host y a otra URL a otro servicio. <br><br>  Lo mejor desde el punto de vista de quien desarrolla la aplicación es que puede administrarlo todo usted mismo.  Una vez configurada la configuración de Ingress, puede enviar todo el tráfico que llega a dicha API a contenedores separados registrados, por ejemplo, a Go.  Pero este tráfico que llega al mismo dominio, pero a una URL diferente, debe enviarse a contenedores escritos en PHP, donde hay mucha lógica, pero no son muy rápidos. <br><br>  Si comparamos todos estos conceptos con Nomad, entonces podemos decir que los primeros tres conceptos están todos juntos. Servicio.  Y falta el último concepto en Nomad.  Usamos un equilibrador externo, ya que puede ser haproxy, nginx, nginx +, etc.  En el caso de un cubo, no necesita introducir este concepto adicional por separado.  Sin embargo, si miras a Ingress por dentro, es nginx, o haproxy, o traefik, pero como si estuviera integrado en Kubernetes. <br><br>  Todos los conceptos que he descrito son esencialmente los recursos que existen dentro del clúster de Kubernetes.  Para describirlos en el cubo, se utiliza el formato yaml, que es más legible y familiar que los archivos HCl en el caso de Nomad.  Pero estructuralmente describen en el caso de, por ejemplo, pod lo mismo.  Dicen: quiero desplegar tal y tal vaina allí y allá, con tal y tal imagen, en tal y tal cantidad. <br><br><img src="https://habrastorage.org/webt/2k/ka/53/2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg" alt="imagen"><br><br>  Además, nos dimos cuenta de que no queríamos crear cada recurso individual con nuestras propias manos: implementación, servicios, Ingress y más.  En cambio, queríamos describir cada sistema implementado en términos de Kubernetes durante la implementación para que no tuviéramos que recrear manualmente todas las dependencias de recursos necesarias en el orden correcto.  Helm fue elegido como el sistema que nos permitió hacer esto. <br><br><h1>  Conceptos clave en Helm </h1><br>  Helm es un <b>administrador de paquetes</b> para Kubernetes.  Es muy similar a cómo funcionan los gestores de paquetes en lenguajes de programación.  Le permiten almacenar un servicio que consiste, por ejemplo, en implementación nginx, implementación php-fpm, una configuración para Ingress, configmaps (esta es una entidad que le permite configurar env y otros parámetros para su sistema) en forma de los llamados gráficos.  Al mismo tiempo, Helm <b>corre sobre Kubernetes</b> .  Es decir, este no es un tipo de sistema que se hace a un lado, sino simplemente otro servicio que se ejecuta dentro del cubo.  Interactúa con él a través de su API a través de un comando de consola.  Su conveniencia y encanto es que incluso si el timón se rompe o lo elimina del clúster, sus servicios no desaparecerán, ya que el timón esencialmente solo sirve para iniciar el sistema.  Kubernetes es responsable del tiempo de actividad y el estado de los servicios. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">También nos dimos cuenta de que la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estandarización</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que antes tenía que hacerse de forma independiente mediante la introducción de jinja en nuestras configuraciones, es una de las principales características de helm. Todas las configuraciones que cree para sus sistemas se almacenan en helm en forma de plantillas similares a jinja, pero, de hecho, utilizando la plantilla de idioma Go en la que está escrito helm, como Kubernetes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Helm nos agrega algunos conceptos adicionales. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El cuadro</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es una descripción de su servicio. Otros administradores de paquetes lo llamarían paquete, paquete o algo así. Esto se llama gráfico aquí. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los valores</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> son las variables que desea usar para construir sus configuraciones a partir de plantillas. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lanzamiento</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Cada vez que un servicio que se implementa usando helm recibe una versión incremental de la versión. Helm recuerda cuál era la configuración del servicio en el año anterior, el año anterior al último lanzamiento, y así sucesivamente. Por lo tanto, si necesita retroceder, simplemente ejecute el comando de devolución de llamada helm, que le indica la versión anterior de la versión. Incluso si en el momento de la reversión, la configuración correspondiente en su repositorio no está disponible, Helm aún recuerda lo que era y revierte su sistema al estado que tenía en la versión anterior. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En el caso cuando usamos helm, las configuraciones usuales para Kubernetes también se convierten en plantillas, en las cuales es posible usar variables, funciones, aplicar operadores condicionales. Por lo tanto, puede recopilar la configuración de su servicio según el entorno.</font></font><br><br><img src="https://habrastorage.org/webt/dc/lr/fh/dclrfhbdr29ms0gouz_pvd8xz44.jpeg" alt="imagen"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En la práctica, decidimos hacer un poco diferente de lo que hicimos en el caso de Nomad. Si en Nomad en el mismo repositorio se almacenaban tanto las configuraciones para la implementación como las n variables que se necesitaban para implementar nuestro servicio, aquí decidimos dividirlas en dos repositorios separados. Solo las n variables necesarias para el despliegue se almacenan en el repositorio de despliegue, y las configuraciones o gráficos se almacenan en el repositorio de timón. </font></font><br><br><img src="https://habrastorage.org/webt/2r/lo/yt/2rloytnvlrj6nri8vj7e9-hccg8.jpeg" alt="imagen"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¿Qué nos dio?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A pesar de que no almacenamos datos realmente confidenciales en los archivos de configuración. Por ejemplo, contraseñas de bases de datos. Se almacenan como secretos en Kubernetes, pero, sin embargo, todavía hay algunas cosas allí que no queremos dar acceso a todos en una fila. Por lo tanto, el acceso al repositorio de implementación es más limitado, y el repositorio de helm simplemente contiene una descripción del servicio. Por esta razón, es posible dar acceso a un círculo más grande de personas de manera segura. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dado que no solo tenemos producción, sino también otros entornos, gracias a esta separación, podemos reutilizar nuestros gráficos de timón para implementar servicios no solo en producción, sino también, por ejemplo, en el entorno de control de calidad. Incluso desplegarlos localmente usando </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minikube</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es algo así como ejecutar Kubernetes localmente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dentro de cada repositorio, dejamos una separación en directorios separados para cada servicio. Es decir, dentro de cada directorio hay plantillas relacionadas con el gráfico correspondiente y que describen los recursos que deben implementarse para iniciar nuestro sistema. En el repositorio de despliegue, solo dejamos enves. En este caso, no utilizamos plantillas con jinja, porque el timón mismo proporciona plantillas fuera de la caja, esta es una de sus funciones principales.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dejamos el script de implementación, deploy.sh, que simplifica y estandariza el lanzamiento para la implementación usando helm. </font><font style="vertical-align: inherit;">Por lo tanto, para cualquier persona que quiera implementar, la interfaz de implementación se ve exactamente igual que en el caso de la implementación a través de Nomad. </font><font style="vertical-align: inherit;">El mismo deploy.sh, el nombre de su servicio y dónde desea implementarlo. </font><font style="vertical-align: inherit;">Esto hace que el timón comience dentro. </font><font style="vertical-align: inherit;">Él, a su vez, recopila configuraciones de plantillas, sustituye los archivos de valores necesarios en ellas, luego las implementa y las coloca en Kubernetes.</font></font><br><br><h1>  Conclusiones </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El servicio de Kubernetes parece más complejo que Nomad. </font></font><br><br><img src="https://habrastorage.org/webt/fe/p6/qi/fep6qibp6hhnbsmqifk2jnmg20a.jpeg" alt="imagen"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aquí es donde el tráfico saliente llega a Ingress. Este es solo el controlador frontal, que recibe todas las solicitudes y posteriormente las envía a los servicios correspondientes a los datos de la solicitud. Los define sobre la base de configuraciones, que son parte de la descripción de su aplicación en timón y que los desarrolladores configuran de forma independiente. El servicio envía solicitudes a sus pods, es decir, contenedores específicos, equilibrando el tráfico entrante entre todos los contenedores que pertenecen a este servicio. Bueno, por supuesto, no olvides que no debemos ir desde la seguridad a nivel de red. Por lo tanto, el clúster de Kubernetes opera la segmentación, que se basa en el etiquetado. Todos los servicios tienen ciertas etiquetas, a las cuales los derechos de acceso de los servicios a ciertos recursos externos / internos están unidos dentro o fuera del clúster.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al completar la transición, vimos que Kubernetes tiene todas las características de Nomad, que usamos antes, y también agrega muchas cosas nuevas. Se puede ampliar a través de complementos y, de hecho, a través de tipos de recursos personalizados. Es decir, tiene la oportunidad no solo de usar algo que entra en Kubernetes de forma inmediata, sino de crear su propio recurso y servicio que leerá su recurso. Esto proporciona opciones adicionales para expandir su sistema sin la necesidad de reinstalar Kubernetes y sin la necesidad de cambios.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un ejemplo de esto es Prometheus, que se ejecuta dentro de nuestro clúster de Kubernetes. Para que pueda comenzar a recopilar métricas de un servicio en particular, necesitamos agregar un tipo de recurso adicional, el denominado monitor de servicio, a la descripción del servicio. Prometheus, debido al hecho de que puede leer, al lanzarse en Kubernetes, un tipo personalizado de recursos, automáticamente comienza a recopilar métricas del nuevo sistema. Es bastante conveniente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El primer despliegue que hicimos en Kubernetes fue en marzo de 2018. </font><font style="vertical-align: inherit;">Y durante este tiempo nunca tuvimos problemas con él. </font><font style="vertical-align: inherit;">Funciona de manera estable sin errores importantes. </font><font style="vertical-align: inherit;">Además, podemos ampliarlo aún más. </font><font style="vertical-align: inherit;">Hoy, tenemos suficientes oportunidades, y realmente nos gusta el ritmo de desarrollo de Kubernetes. </font><font style="vertical-align: inherit;">Actualmente, más de 3.000 contenedores se encuentran en Kubernetes. </font><font style="vertical-align: inherit;">El clúster toma varios nodos. </font><font style="vertical-align: inherit;">Al mismo tiempo, tiene servicio, es estable y está muy controlado.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/451644/">https://habr.com/ru/post/451644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451634/index.html">Cómo somos analizados en tiendas y restaurantes.</a></li>
<li><a href="../451636/index.html">Cinco años de esclavitud</a></li>
<li><a href="../451638/index.html">Animación en aplicaciones móviles: prueba de Lottie</a></li>
<li><a href="../451640/index.html">Juego de tronos: creación de infografías sobre asesinatos, sexo, viajes de Poniente y más</a></li>
<li><a href="../451642/index.html">Encontrar un camino entre obstáculos redondos</a></li>
<li><a href="../451646/index.html">La producción del casco de la nave espacial de la Federación ha comenzado</a></li>
<li><a href="../451648/index.html">¿Cómo buscamos un turismo inusual en Rusia y qué tipo de aventuras ocurren generalmente?</a></li>
<li><a href="../451650/index.html">Parte I. Pregúntele a su madre: ¿Cómo comunicarse con los clientes y confirmar la exactitud de su idea de negocio, si todos mienten?</a></li>
<li><a href="../451652/index.html">Parte II Pregúntele a su madre: ¿Cómo comunicarse con los clientes y confirmar la exactitud de su idea de negocio, si todos mienten?</a></li>
<li><a href="../451654/index.html">Nuevo empleado: vivo o muerto</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>