<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüé§ üë∏üèª üç∂ Pr√©dictions de math√©maticiens. Nous analysons les principales m√©thodes de d√©tection des anomalies ‚òÄÔ∏è üßñüèø üõ•Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√Ä l'√©tranger, l'utilisation de l'intelligence artificielle dans l'industrie pour la maintenance pr√©dictive de divers syst√®mes gagne de plus en plus en...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pr√©dictions de math√©maticiens. Nous analysons les principales m√©thodes de d√©tection des anomalies</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lanit/blog/447190/">  √Ä l'√©tranger, l'utilisation de l'intelligence artificielle dans l'industrie pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la maintenance pr√©dictive de</a> divers syst√®mes gagne de plus en plus en popularit√©.  Le but de cette technique est d'identifier les dysfonctionnements dans le fonctionnement du syst√®me pendant la phase de fonctionnement avant qu'il ne tombe en panne pour une r√©ponse rapide. <br><br>  Quelle est la pertinence de cette approche dans notre pays et en Occident?  La conclusion peut √™tre faite, par exemple, sur des articles sur Habr√© et dans Medium.  Il n'y a presque pas d'articles sur Habr√© sur la r√©solution des probl√®mes de maintenance pr√©dictive.  Sur Medium, il y a tout un ensemble.  Ici, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici,</a> il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">est</a> bien d√©crit quels sont les objectifs et les avantages de cette approche. <br><br>  De cet article, vous apprendrez: <br><br><ul><li>  pourquoi cette technique est-elle n√©cessaire </li><li>  quelles approches d'apprentissage automatique sont plus couramment utilis√©es pour la maintenance pr√©dictive, </li><li>  comment j'ai essay√© l'une des astuces avec un exemple simple. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/2a4/888/8452a4888db8d633dd14d426f6b80cbe.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><i>Source</i></a> <br><a name="habracut"></a><br>  Quelles sont les fonctionnalit√©s du service pr√©dictif? <br><br><ul><li>  un processus ma√Ætris√© de travaux de r√©paration, qui est effectu√© selon les besoins, permettant ainsi d'√©conomiser de l'argent, et sans pr√©cipitation, ce qui am√©liore la qualit√© de ces travaux; </li><li>  identification d'un dysfonctionnement sp√©cifique dans le fonctionnement de l'√©quipement (la possibilit√© d'acheter une pi√®ce sp√©cifique pour le remplacement lorsque l'√©quipement fonctionne offre d'√©normes avantages) </li><li>  optimisation du fonctionnement des √©quipements, des charges, etc. </li><li>  r√©duction des co√ªts pour l'arr√™t r√©gulier des √©quipements. </li></ul><br>  Le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prochain article sur Medium</a> d√©crit bien les questions auxquelles il faut r√©pondre afin de comprendre comment aborder cette question dans un cas particulier. <br><br>  Lors de la collecte de donn√©es ou lors du choix de donn√©es pour construire un mod√®le, il est important de r√©pondre √† trois groupes de questions: <br><br><ol><li>  Peut-on pr√©voir tous les probl√®mes du syst√®me?  Quelle pr√©diction est particuli√®rement importante? </li><li>  Qu'est-ce qu'un processus d'√©chec?  Le syst√®me entier cesse-t-il de fonctionner ou le mode de fonctionnement change-t-il seulement?  S'agit-il d'un processus rapide, d'une d√©gradation instantan√©e ou progressive? </li><li>  Les performances du syst√®me refl√®tent-elles correctement ses performances?  Se rapportent-ils √† des parties individuelles du syst√®me ou au syst√®me dans son ensemble? </li></ol><br>  Il est √©galement important de comprendre √† l'avance ce que vous voulez pr√©dire, ce qui est possible de pr√©dire et ce qui ne l'est pas. <br><br>  L'article sur Medium r√©pertorie √©galement les questions qui vous aideront √† d√©terminer votre objectif sp√©cifique: <br><br><ul><li>  Que faut-il pr√©voir?  La dur√©e de vie restante, comportement anormal ou non, la probabilit√© d'√©chec dans les N prochaines heures / jours / semaines? </li><li>  Y a-t-il suffisamment de donn√©es historiques? </li><li>  Sait-on quand le syst√®me a donn√© des relev√©s anormaux et quand non.  Est-il possible de marquer de telles indications? </li><li>  Jusqu'o√π le mod√®le doit-il voir?  Quel est le degr√© d'ind√©pendance des lectures refl√©tant le fonctionnement du syst√®me dans l'intervalle d'une heure / jour / semaine </li><li>  De quoi avez-vous besoin pour optimiser?  Le mod√®le doit-il d√©tecter autant de violations que possible, tout en √©mettant une fausse alarme, ou suffit-il d'attraper plusieurs √©v√©nements sans faux positifs? </li></ul><br>  Il est √† esp√©rer que la situation s'am√©liorera √† l'avenir.  Jusqu'√† pr√©sent, il existe des difficult√©s dans le domaine de la maintenance pr√©dictive: il existe peu d'exemples de dysfonctionnement du syst√®me, ou des moments de dysfonctionnement du syst√®me suffisent, mais ils ne sont pas marqu√©s;  le processus d'√©chec est inconnu. <br><br>  Le principal moyen de surmonter les difficult√©s de la maintenance pr√©dictive consiste √† utiliser <b>des m√©thodes de recherche d'anomalies</b> .  De tels algorithmes ne n√©cessitent pas de balisage pour la formation.  Pour tester et d√©boguer des algorithmes, un balisage sous une forme ou une autre est n√©cessaire.  De telles m√©thodes sont limit√©es en ce qu'elles ne pr√©disent pas une d√©faillance sp√©cifique, mais signalent uniquement une anomalie des indicateurs. <br><br>  Mais ce n'est d√©j√† pas mal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/847/46c/6d7/84746c6d7414722b0a1b7b322000f201.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><i>Source</i></a> <br><br><h2>  Les m√©thodes </h2><br>  Maintenant, je veux parler de certaines caract√©ristiques des approches de d√©tection d'anomalies, puis ensemble, nous testerons les capacit√©s de quelques algorithmes simples dans la pratique. <br><br>  Bien qu'une situation particuli√®re n√©cessite de tester plusieurs algorithmes pour rechercher des anomalies et choisir le meilleur, il est possible de d√©terminer certains avantages et inconv√©nients des principales techniques utilis√©es dans ce domaine. <br><br>  Tout d'abord, il est important de comprendre √† l'avance quel est le pourcentage d'anomalies dans les donn√©es. <br><br>  Si nous parlons d'une variation de l'approche semi-supervis√©e (nous √©tudions uniquement sur des donn√©es ¬´normales¬ª, et nous travaillons (testons) puis sur des donn√©es pr√©sentant des anomalies), alors le choix le plus optimal est <b>la m√©thode du vecteur support √† une classe ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SVM √† une classe</a> )</b> .  Lorsque vous utilisez des fonctions de base radiales comme noyau, cet algorithme construit une surface non lin√©aire autour de l'origine.  Plus les donn√©es d'entra√Ænement sont propres, mieux cela fonctionne. <br><br>  Dans d'autres cas, la n√©cessit√© de conna√Ætre le rapport des points anormaux et "normaux" demeure √©galement - pour d√©terminer le seuil de coupure. <br><br>  Si le nombre d'anomalies dans les donn√©es est sup√©rieur √† 5% et qu'elles sont assez bien s√©parables de l'√©chantillon principal, des m√©thodes de recherche d'anomalies standard peuvent √™tre utilis√©es. <br><br>  Dans ce cas, la <b>m√©thode de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">for√™t d'isolement</a></b> est la plus stable en termes de qualit√©: la <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">for√™t d'isolement est constitu√©e de</a></b> donn√©es randomis√©es.  Une indication plus caract√©ristique est plus susceptible d'aller plus loin, tandis que des indicateurs inhabituels se s√©pareront du reste de l'√©chantillon dans les premi√®res it√©rations. <br><br>  D'autres algorithmes fonctionnent mieux s'ils "cadrent" avec les sp√©cificit√©s des donn√©es. <br><br>  Lorsque les donn√©es ont une distribution normale, la <b>m√©thode de l'enveloppe elliptique</b> convient, en rapprochant les donn√©es avec une distribution normale multidimensionnelle.  Moins le point appartient √† la distribution, plus il est probable qu'il soit anormal. <br><br>  Si les donn√©es sont pr√©sent√©es de telle mani√®re que la position relative des diff√©rents points refl√®te bien leurs diff√©rences, alors les m√©thodes m√©triques semblent √™tre un bon choix: par exemple, <b>k voisins les plus proches, k-√®me voisin le plus proche, ABOD (d√©tection des valeurs aberrantes bas√©es sur l'angle) ou LOF (facteur local des valeurs aberrantes) )</b> <br><br>  Toutes ces m√©thodes sugg√®rent que les ¬´bons¬ª indicateurs sont concentr√©s dans une zone d'espace multidimensionnel.  Si, parmi les k (ou k-√®mes) voisins les plus proches, tout est loin de la cible, alors le point est une anomalie.  Pour ABOD, le raisonnement est similaire: si tous les k points les plus proches sont dans le m√™me secteur d'espace par rapport √† celui consid√©r√©, alors le point est une anomalie.  Pour LOF: si la densit√© locale (pr√©d√©termin√©e pour chaque point par k voisins les plus proches) est inf√©rieure √† celle des k voisins les plus proches, alors le point est une anomalie. <br><br>  Si les donn√©es sont bien regroup√©es, les <b>m√©thodes bas√©es sur l'analyse des clusters</b> sont un bon choix.  Si le point est √©quidistant des centres de plusieurs grappes, il est alors anormal. <br><br>  Si les directions de la plus grande variation de variance sont bien distingu√©es dans les donn√©es, alors il semble √™tre un bon choix pour <b>rechercher des anomalies en se basant sur la m√©thode des composantes principales</b> .  Dans ce cas, les √©carts par rapport √† la valeur moyenne pour n1 (les composants les plus ¬´principaux¬ª) et n2 (le moins ¬´principal¬ª) sont consid√©r√©s comme une mesure d'anomalie. <br><br>  Par exemple, il est sugg√©r√© d'examiner l'ensemble de donn√©es de <b>The Prognostics and Health Management Society (PHM Society)</b> .  Cette organisation √† but non lucratif organise des concours chaque ann√©e.  En 2018, par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">il √©tait n√©cessaire de pr√©voir les erreurs de fonctionnement et le temps avant la panne de l'usine de gravure par faisceau d'ions</a> .  Nous prendrons l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensemble de donn√©es pour 2015</a> .  Il contient les lectures de plusieurs capteurs pour 30 installations (√©chantillon de formation) et il est n√©cessaire de pr√©voir quand et quelle erreur se produira. <br><br>  Je n'ai pas trouv√© les r√©ponses pour l'√©chantillon de test sur le r√©seau, donc nous ne jouerons qu'avec celui de formation. <br><br>  En g√©n√©ral, tous les param√®tres sont similaires, mais diff√®rent, par exemple, par le nombre de composants, le nombre d'anomalies, etc.  Par cons√©quent, l'apprentissage dans les 20 premiers et les tests dans d'autres n'ont pas beaucoup de sens. <br><br>  Nous allons donc choisir l'une des installations, la charger et jeter un ≈ìil √† ces donn√©es.  L'article ne traitera pas de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ing√©nierie des fonctionnalit√©s</a> , donc nous ne nous attarderons pas beaucoup. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.covariance <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EllipticEnvelope <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LocalOutlierFactor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> IsolationForest <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OneClassSVM dfa=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12a.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Component number'</span></span>,<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4ref'</span></span>]) dfa.head(<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/301/77e/2bb/30177e2bb970e82495b18008b44d7ea4.jpg"></div><br>  Comme vous pouvez le voir, il y a sept composants pour chacun desquels il y a des lectures de quatre capteurs qui sont prises toutes les 15 minutes.  S1ref-S4ref dans la description de la comp√©tition sont r√©pertori√©s comme valeurs de r√©f√©rence, mais les valeurs sont tr√®s diff√©rentes des lectures des capteurs.  Afin de ne pas perdre de temps √† r√©fl√©chir √† leur signification, nous les supprimons.  Si vous regardez la distribution des valeurs pour chaque entit√© (S1-S4), il s'av√®re que les distributions sont continues pour S1, S2 et S4 et discr√®tes pour S3.  De plus, si vous regardez la distribution conjointe de S2 et S4, il s'av√®re qu'elles sont inversement proportionnelles. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/69b/82b/cde/69b82bcdecdfca69128d88673b0518b4.jpg"></div><br>  Bien qu'un √©cart par rapport √† une d√©pendance directe puisse indiquer une erreur, nous ne v√©rifierons pas cela, mais supprimerons simplement S4. <br><br>  Encore une fois, nous traitons l'ensemble de donn√©es.  Quittez S1, S2 et S3.  Nous mettons √† l'√©chelle S1 et S2 avec StandardScaler (nous soustrayons la moyenne et divisons par l'√©cart-type), traduisons S3 en OHE (One Hot Encoding).  Nous cousons les lectures de tous les composants d'installation sur une seule ligne.  Total 89 fonctionnalit√©s.  2 * 7 = 14 - lectures S1 et S2 pour 7 composants et 75 valeurs uniques de R3.  Seulement 56 mille lignes de ce type. <br><br>  T√©l√©chargez le fichier contenant des erreurs. <br><br><pre> <code class="python hljs">dfc=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12c.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>, <span class="hljs-string"><span class="hljs-string">'End Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>]) dfc.head()</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ad/419/e50/3ad419e50ac870a71c153b523a22bfed.jpg"></div><br>  Avant d'essayer ces algorithmes sur notre jeu de donn√©es, je me permettrai une autre petite digression.  Vous devez √™tre test√©.  Pour cela, il est propos√© de prendre l'heure de d√©but de l'erreur et l'heure de fin.  Et toutes les indications √† l'int√©rieur de cet intervalle sont consid√©r√©es comme anormales et √† l'ext√©rieur - normales.  Cette approche pr√©sente de nombreux inconv√©nients.  Mais surtout, un comportement anormal se produit tr√®s probablement avant que l'erreur ne soit corrig√©e.  Pour la fid√©lit√©, d√©pla√ßons la fen√™tre des anomalies il y a une demi-heure dans le temps.  Nous √©valuerons la mesure F1, la pr√©cision et le rappel. <br><br>  Le code pour distinguer les caract√©ristiques et d√©terminer la qualit√© du mod√®le: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_and_preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(plant_num)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-comment"><span class="hljs-comment">#      ,       dfa=pd.read_csv('plant_{}a.csv'.format(plant_num),names=['Component number','Time','S1','S2','S3','S4','S1ref','S2ref','S3ref','S4ref'])   dfc=pd.read_csv('plant_{}c.csv'.format(plant_num),names=['Start Time','End Time','Type']).drop(0,axis=0)   N_comp=len(dfa['Component number'].unique())   #  15    dfa['Time']=pd.to_datetime(dfa['Time']).dt.round('15min')   #  6    (  ,    )   dfc=dfc[dfc['Type']!=6]   dfc['Start Time']=pd.to_datetime(dfc['Start Time'])   dfc['End Time']=pd.to_datetime(dfc['End Time'])   #      ,       OHE  3-    dfa=pd.concat([dfa.groupby('Time').nth(i)[['S1','S2','S3']].rename(columns={"S1":"S1_{}".format(i),"S2":"S2_{}".format(i),"S3":"S3_{}".format(i)}) for i in range(N_comp)],axis=1).dropna().reset_index()   for k in range(N_comp):       dfa=pd.concat([dfa.drop('S3_'+str(k),axis=1),pd.get_dummies(dfa['S3_'+str(k)],prefix='S3_'+str(k))],axis=1).reset_index(drop=True)   #          df_train,df_test=train_test_split(dfa,test_size=0.25,shuffle=False)   cols_to_scale=df_train.filter(regex='S[1,2]').columns   scaler=preprocessing.StandardScaler().fit(df_train[cols_to_scale])   df_train[cols_to_scale]=scaler.transform(df_train[cols_to_scale])   df_test[cols_to_scale]=scaler.transform(df_test[cols_to_scale])   return df_train,df_test,dfc #       def get_true_labels(measure_times,dfc,shift_delta):   idxSet=set()   dfc['Start Time']-=pd.Timedelta(minutes=shift_delta)   dfc['End Time']-=pd.Timedelta(minutes=shift_delta)   for idx,mes_time in tqdm_notebook(enumerate(measure_times),total=measure_times.shape[0]):       intersect=np.array(dfc['Start Time']&lt;mes_time).astype(int)*np.array(dfc['End Time']&gt;mes_time).astype(int)       idxs=np.where(intersect)[0]       if idxs.shape[0]:           idxSet.add(idx)   dfc['Start Time']+=pd.Timedelta(minutes=shift_delta)   dfc['End Time']+=pd.Timedelta(minutes=shift_delta)   true_labels=pd.Series(index=measure_times.index)   true_labels.iloc[list(idxSet)]=1   true_labels.fillna(0,inplace=True)   return true_labels #          def check_model(model,df_train,df_test,filt='S[123]'):   model.fit(df_train.drop('Time',axis=1).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))).map({-1:1,1:0})   print('F1 score: {:.3f}'.format(f1_score(df_test['Label'],y_preds)))   print('Precision score: {:.3f}'.format(precision_score(df_test['Label'],y_preds)))   print('Recall score: {:.3f}'.format(recall_score(df_test['Label'],y_preds)))   score = model.decision_function(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))   sns.distplot(score[df_test['Label']==0])   sns.distplot(score[df_test['Label']==1]) df_train,df_test,anomaly_times=load_and_preprocess(12) df_test['Label']=get_true_labels(df_test['Time'],dfc,30)</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/50b/6f2/be9/50b6f2be9c3220853e92461805030e0b.jpg"></div>  <i>R√©sultats des tests pour des algorithmes de recherche d'anomalies simples sur l'ensemble de donn√©es PHM 2015 Data Challenge</i> <br><br>  Retour aux algorithmes.  Essayons One Class SVM (OCSVM), IsolationForest (IF), EllipticEnvelope (EE) et LocalOutlierFactor (LOF) sur nos donn√©es.  Pour commencer, nous ne d√©finirons aucun param√®tre.  Je note que LOF peut fonctionner dans deux modes.  Si novelty = False ne peut rechercher des anomalies que dans l'ensemble d'apprentissage (il n'y a que fit_predict), si True, alors il vise √† rechercher des anomalies en dehors de l'ensemble d'apprentissage (peut s'adapter et pr√©dire s√©par√©ment).  IF a un mode de comportement ancien et nouveau.  Nous utilisons de nouveaux.  Il donne de meilleurs r√©sultats. <br><br>  OCSVM d√©tecte bien les anomalies, mais il y a trop de faux positifs.  Pour d'autres m√©thodes, le r√©sultat est encore pire. <br><br>  Mais supposons que nous connaissions le pourcentage d'anomalies dans les donn√©es.  Dans notre cas, 27%.  OCSVM a nu - l'estimation sup√©rieure pour le pourcentage d'erreurs et la plus faible pour le pourcentage de vecteurs de support.  D'autres m√©thodes de contamination ont un pourcentage d'erreurs de donn√©es.  Dans les m√©thodes IF et LOF, il est d√©termin√© automatiquement, tandis que pour OCSVM et EE, il est d√©fini sur 0,1 par d√©faut.  Essayons de d√©finir la contamination (nu) √† 0,27.  Maintenant le meilleur r√©sultat pour EE. <br><br>  Code de v√©rification des mod√®les: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">check_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model,df_train,df_test,filt=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'S[123]'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   model_type,model = model   model.fit(df_train.drop(<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))).map({<span class="hljs-number"><span class="hljs-number">-1</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>})   print(<span class="hljs-string"><span class="hljs-string">'F1 score for {}: {:.3f}'</span></span>.format(model_type,f1_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Precision score for {}: {:.3f}'</span></span>.format(model_type,precision_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Recall score for {}: {:.3f}'</span></span>.format(model_type,recall_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   score = model.decision_function(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>])   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>])   plt.title(<span class="hljs-string"><span class="hljs-string">'Decision score distribution for {}'</span></span>.format(model_type))   plt.show()</code> </pre> <br>  Il est int√©ressant de regarder la distribution des indicateurs d'anomalie pour diff√©rentes m√©thodes.  On peut voir que LOF ne fonctionne pas bien pour ces donn√©es.  EE a des points que l'algorithme consid√®re comme extr√™mement anormaux.  Cependant, les points normaux y tombent.  IsoFor et OCSVM montrent que le choix du seuil de coupure (contamination / nu) est important, ce qui changera le compromis entre pr√©cision et exhaustivit√©. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a9d/0c8/80f/a9d0c880f36fe37f1de6c9290b8cccc7.png"></div><br>  Il est logique que les lectures des capteurs aient une distribution proche de la normale, proche des valeurs stationnaires.  Si nous avons vraiment un √©chantillon de test √©tiquet√©, et de pr√©f√©rence √©galement un √©chantillon de validation, alors la valeur de contamination peut √™tre teint√©e.  La question suivante est, quelles erreurs sont plus orient√©es: faux positif ou faux n√©gatif? <br><br>  Le r√©sultat LOF est tr√®s faible.  Pas tr√®s impressionnant.  Mais rappelez-vous que les variables OHE vont √† l'entr√©e avec les variables transform√©es par StandardScaler.  Et les distances par d√©faut sont euclidiennes.  Mais si vous ne comptez que les variables selon S1 et S2, la situation est corrig√©e et le r√©sultat est comparable avec d'autres m√©thodes.  Il est toutefois important de comprendre que l'un des param√®tres cl√©s des classificateurs de mesures r√©pertori√©s est le nombre de voisins.  Cela affecte consid√©rablement la qualit√© et doit √™tre r√©gl√©.  La mesure de la distance elle-m√™me serait √©galement int√©ressante √† saisir. <br><br>  Essayez maintenant de combiner les deux mod√®les.  Au d√©but d'un, nous supprimons les anomalies de l'ensemble d'entra√Ænement.  Et puis nous formerons OCSVM sur un ensemble de formation ¬´plus propre¬ª.  Selon les r√©sultats pr√©c√©dents, nous avons observ√© la plus grande exhaustivit√© en EE.  Nous effa√ßons l'√©chantillon de formation via EE, formons OCSVM dessus et obtenons F1 = 0,50, Pr√©cision = 0,34, exhaustivit√© = 0,95.  Pas impressionnant.  Mais nous venons de demander nu = 0,27.  Et les donn√©es dont nous disposons sont plus ou moins ¬´propres¬ª.  Si nous supposons que la pl√©nitude de l'EE sur l'√©chantillon d'apprentissage est la m√™me, alors 5% des erreurs resteront.  Nous nous fixons un tel nu et obtenons F1 = 0,69, Pr√©cision = 0,59, exhaustivit√© = 0,82.  Super.  Il est important de noter que dans d'autres m√©thodes, une telle combinaison ne fonctionnera pas, car elles impliquent que le nombre d'anomalies dans l'ensemble d'apprentissage et le num√©ro de test sont les m√™mes.  Lors de la formation de ces m√©thodes sur un ensemble de donn√©es de formation pur, vous devrez sp√©cifier moins de contamination que dans les donn√©es r√©elles et pas proche de z√©ro, mais il est pr√©f√©rable de le s√©lectionner pour une validation crois√©e. <br><br>  Il est int√©ressant de regarder le r√©sultat de la recherche sur la s√©quence des indications: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/0c5/26d/2a20c526d944b55116f3fcd6af064eab.png"></div><br>  La figure montre un segment des lectures des premier et deuxi√®me capteurs pour 7 composants.  Dans la l√©gende, la couleur des erreurs correspondantes (le d√©but et la fin sont indiqu√©s par des lignes verticales de la m√™me couleur).  Les points indiquent les pr√©dictions: vert - pr√©dictions vraies, rouge - faux positif, violet - faux n√©gatif.  On peut voir sur la figure qu'il est difficile de d√©terminer visuellement le temps d'erreur, et l'algorithme g√®re assez bien cette t√¢che.  Bien qu'il soit important de comprendre que les lectures du troisi√®me capteur ne sont pas donn√©es ici.  De plus, il y a des lectures faussement positives apr√®s la fin de l'erreur.  C'est-√†-dire  l'algorithme voit qu'il existe √©galement des valeurs erron√©es, et nous avons marqu√© cette zone comme sans erreur.  Le c√¥t√© droit de la figure montre la zone avant l'erreur, que nous avons marqu√©e comme erron√©e (une demi-heure avant l'erreur), qui a √©t√© reconnue comme sans erreur, ce qui conduit √† des erreurs de mod√®le faussement n√©gatives.  Au centre de la figure, une pi√®ce coh√©rente est reconnue, reconnue comme une erreur.  La conclusion peut √™tre tir√©e comme suit: lors de la r√©solution du probl√®me de recherche d'anomalies, vous devez interagir √©troitement avec des ing√©nieurs qui comprennent l'essence des syst√®mes dont vous devez pr√©dire la sortie, car la v√©rification des algorithmes utilis√©s sur le balisage ne refl√®te pas pleinement la r√©alit√© et ne simule pas les conditions dans lesquelles ces algorithmes pourraient √™tre utilis√©. <br><br>  Code pour tracer le graphique: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_time_course</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df_test,dfc,y_preds,start,end,vert_shift=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))   cols=df_train.filter(regex=(<span class="hljs-string"><span class="hljs-string">'S[12]'</span></span>)).columns   add=<span class="hljs-number"><span class="hljs-number">0</span></span>   preds_idx=y_preds.iloc[start:end][y_preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   true_idx=df_test.iloc[start:end,:][df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   tp_idx=set(true_idx.values).intersection(set(preds_idx.values))   fn_idx=set(true_idx.values).difference(set(preds_idx.values))   fp_idx=set(preds_idx.values).difference(set(true_idx.values))   xtime=df_test[<span class="hljs-string"><span class="hljs-string">'Time'</span></span>].iloc[start:end]   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> cols:       plt.plot(xtime,df_test[col].iloc[start:end]+add)       plt.scatter(xtime.loc[tp_idx].values,df_test.loc[tp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'green'</span></span>)       plt.scatter(xtime.loc[fn_idx].values,df_test.loc[fn_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'violet'</span></span>)       plt.scatter(xtime.loc[fp_idx].values,df_test.loc[fp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>)       add+=vert_shift   failures=dfc[(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&gt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">0</span></span>])&amp;(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&lt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">-1</span></span>])]   unique_fails=np.sort(failures[<span class="hljs-string"><span class="hljs-string">'Type'</span></span>].unique())   colors=np.array([np.random.rand(<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> unique_fails])   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> failures.index:       c=colors[np.where(unique_fails==failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>]][<span class="hljs-number"><span class="hljs-number">0</span></span>]       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>],color=c)       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'End Time'</span></span>],color=c)   leg=plt.legend(unique_fails)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(unique_fails)):       leg.legendHandles[i].set_color(colors[i])</code> </pre> <br>  Si le pourcentage d'anomalies est inf√©rieur √† 5% et / ou si elles sont mal s√©par√©es des indicateurs ¬´normaux¬ª, les m√©thodes ci-dessus fonctionnent mal et il vaut la peine d'utiliser des algorithmes bas√©s sur des r√©seaux de neurones.  Dans le cas le plus simple, ce serait: <br><br><ul><li>  encodeurs automatiques (une erreur √©lev√©e d'un encodeur automatique form√© signalera une anomalie dans la lecture); </li><li>  r√©seaux r√©currents (apprentissage par s√©quence pour pr√©dire la derni√®re lecture. Si la diff√©rence est grande - le point est anormal). </li></ul><br>  S√©par√©ment, il convient de noter les sp√©cificit√©s du travail avec les s√©ries chronologiques.  Il est important de comprendre que la plupart des algorithmes ci-dessus (√† l'exception des encodeurs automatiques et des for√™ts isolantes) donneront tr√®s probablement une qualit√© inf√©rieure lors de l'ajout de fonctionnalit√©s de d√©calage (lectures des points pr√©c√©dents). <br><br>  Essayons d'ajouter des fonctionnalit√©s de d√©calage dans notre exemple.  La description du concours indique que les valeurs 3 heures avant l'erreur ne sont en aucun cas li√©es √† l'erreur.  Ajoutez ensuite les panneaux en 3 heures.  259 signes au total. <br><br>  En cons√©quence, les r√©sultats pour OCSVM et IsolationForest sont rest√©s presque inchang√©s, tandis que ceux pour Elliptic Envelope et LOF ont chut√©. <br><br>  Pour utiliser les informations sur la dynamique du syst√®me, des auto-encodeurs avec des r√©seaux de neurones r√©currents ou convolutionnels doivent √™tre utilis√©s.  Ou, par exemple, une combinaison d'auto-encodeurs, de compression d'informations et d'approches conventionnelles pour rechercher des anomalies sur la base d'informations compress√©es.  L'approche inverse semble √©galement prometteuse.  D√©pistage primaire des points les plus inhabituels par des algorithmes standard, puis formation de l'auto-encodeur d√©j√† sur des donn√©es plus propres. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/340/9e8/dbc/3409e8dbcc5e9bcac2a1acb08bc7d146.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><i>Source</i></a> <br><br>  Il existe un ensemble de techniques pour travailler avec des s√©ries chronologiques unidimensionnelles.  Tous visent √† pr√©dire les lectures futures, et les points qui s'√©cartent de la pr√©diction sont consid√©r√©s comme des anomalies. <br><br><h2>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mod√®le Holt-Winters</a> </h2><br>  Triple lissage exponentiel, divise la s√©rie en 3 composantes: niveau, tendance et saisonnalit√©.  Par cons√©quent, si la s√©rie est pr√©sent√©e sous cette forme, la m√©thode fonctionne bien.  Facebook Prophet fonctionne sur un principe similaire, mais √©value les composants eux-m√™mes d'une mani√®re diff√©rente.  Plus de d√©tails peuvent √™tre lus, par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br><h2>  S (ARIMA) </h2><br>  Dans cette m√©thode, le mod√®le pr√©dictif est bas√© sur l'autor√©gression et la moyenne mobile.  Si nous parlons de l'expansion de S (ARIMA), cela nous permet d'√©valuer la saisonnalit√©.  En savoir plus sur l'approche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br><h2>  Autres approches de services pr√©dictifs </h2><br>  En ce qui concerne les s√©ries chronologiques et les informations sur les heures d'occurrence des erreurs, vous pouvez appliquer des m√©thodes d'enseignement avec un enseignant.  Outre le besoin de donn√©es balis√©es, dans ce cas, il est important de comprendre que la pr√©diction d'erreur d√©pendra de la nature de l'erreur.  S'il y a beaucoup d'erreurs et de nature diff√©rente, il sera probablement n√©cessaire de pr√©voir chacune s√©par√©ment, ce qui n√©cessitera des donn√©es encore plus √©tiquet√©es, mais les perspectives seront plus attrayantes. <br><br>  Il existe d'autres fa√ßons d'utiliser l'apprentissage automatique dans la maintenance pr√©dictive.  Par exemple, pr√©dire une d√©faillance du syst√®me dans les N prochains jours (t√¢che de classification).  Il est important de comprendre qu'une telle approche n√©cessite que la survenue d'une erreur dans le fonctionnement du syst√®me soit pr√©c√©d√©e d'une p√©riode de d√©gradation (pas forc√©ment progressive).  Dans ce cas, l'approche la plus r√©ussie semble √™tre l'utilisation de r√©seaux de neurones avec des couches convolutionnelles et / ou r√©currentes.  S√©par√©ment, il convient de noter les m√©thodes d'augmentation des s√©ries chronologiques.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deux approches</a> me semblent les plus int√©ressantes et en m√™me temps simples: <br><br><ul><li>  la partie continue de la ligne est s√©lectionn√©e (par exemple, 70% et le reste est supprim√©) et √©tir√©e √† la taille d'origine </li><li>  une partie continue de la rang√©e (par exemple, 20%) est s√©lectionn√©e et √©tir√©e ou compress√©e.  Apr√®s cela, la ligne enti√®re est compress√©e ou √©tir√©e en cons√©quence √† sa taille d'origine. </li></ul><br>  Il existe √©galement une option pour pr√©dire la dur√©e de vie restante du syst√®me (t√¢che de r√©gression).  Ici, nous pouvons distinguer une approche distincte: la pr√©diction n'est pas de la dur√©e de vie, mais des param√®tres de distribution de Weibull. <br><br>  Vous pouvez lire sur la distribution elle-m√™me <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> , et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> sur son utilisation en conjonction avec des maillages r√©currents.  Cette distribution a deux param√®tres Œ± et Œ≤.  Œ± indique quand l'√©v√©nement se produira et Œ≤ indique le degr√© de confiance de l'algorithme.  Bien que l'application de cette approche soit prometteuse, des difficult√©s surviennent dans la formation du r√©seau neuronal dans ce cas, car il est plus facile pour l'algorithme d'√™tre pr√©caire au d√©but que de pr√©dire une dur√©e de vie ad√©quate. <br><br>  S√©par√©ment, il convient de noter la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©gression de Cox</a> .  Il vous permet de pr√©dire la tol√©rance aux pannes du syst√®me pour chaque instant dans le temps apr√®s le diagnostic, en le pr√©sentant comme un produit de deux fonctions.  Une fonction est la d√©gradation du syst√®me, ind√©pendamment de ses param√®tres, c'est-√†-dire  commun √† ces syst√®mes.  Et le second est une d√©pendance exponentielle des param√®tres d'un syst√®me particulier.  Donc, pour une personne, il y a une fonction commune associ√©e au vieillissement, plus ou moins la m√™me pour tout le monde.  Mais la d√©t√©rioration de la sant√© est √©galement associ√©e √† l'√©tat des organes internes, diff√©rent pour chacun. <br><br>  J'esp√®re que vous en savez maintenant un peu plus sur la maintenance pr√©dictive.  Je suis s√ªr que vous aurez des questions concernant les m√©thodes d'apprentissage automatique les plus souvent utilis√©es pour cette technologie.  Je serai heureux de r√©pondre √† chacun d'eux dans les commentaires.  Si vous souhaitez non seulement demander ce qui est √©crit, mais souhaitez faire quelque chose de similaire, notre √©quipe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CleverDATA</a> est toujours ravie de professionnels talentueux et enthousiastes. <br><br><div class="spoiler">  <b class="spoiler_title">Y a-t-il des postes vacants?</b>  <b class="spoiler_title">Bien s√ªr!</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©veloppeur Java (Big Data)</a> </li></ul></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr447190/">https://habr.com/ru/post/fr447190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr447178/index.html">5 opportunit√©s efficaces pour utiliser la technologie d'extraction de processus</a></li>
<li><a href="../fr447180/index.html">Pr√©sentation et comparaison des contr√¥leurs d'entr√©e pour Kubernetes</a></li>
<li><a href="../fr447182/index.html">Syst√®mes d'exploitation: trois pi√®ces faciles. Partie 3: API de processus (traduction)</a></li>
<li><a href="../fr447184/index.html">Qu'est-ce que l'offre d'√©change initiale (IEO) et en quoi est-elle diff√©rente de l'ICO?</a></li>
<li><a href="../fr447186/index.html">Comment lancer un prototype ML en une journ√©e. Signaler Yandex.Taxi</a></li>
<li><a href="../fr447192/index.html">Quel r√¥le la technologie peut-elle jouer dans l'art ancien du m√©lange d'√©pices?</a></li>
<li><a href="../fr447194/index.html">Fonctionnalit√©s de rendu dans Metro: Exodus c raytracing</a></li>
<li><a href="../fr447196/index.html">7. Check Point Getting Started R80.20. Contr√¥le d'acc√®s</a></li>
<li><a href="../fr447198/index.html">Mission lunaire "Bereshit": atterrissage-accident-chute sur la lune</a></li>
<li><a href="../fr447204/index.html">17 avril: Conf√©rence ouverte "Le chemin du d√©veloppeur du jeu: de l'id√©e au lancement" et une ludoth√®que √† la Higher School of Law</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>