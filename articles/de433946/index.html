<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ö üë©üèΩ‚Äçü§ù‚Äçüë©üèº üõÇ Spickzettel f√ºr k√ºnstliche Intelligenz - werfen Sie den √úberschuss weg, lehren Sie die Hauptsache. Trainingssequenz-Verarbeitungstechnik ü§í üëåüèº ü§£</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dies ist der zweite Artikel √ºber die Analyse und Untersuchung von Materialien aus dem Wettbewerb um die Suche nach Schiffen auf See. Aber jetzt werden...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Spickzettel f√ºr k√ºnstliche Intelligenz - werfen Sie den √úberschuss weg, lehren Sie die Hauptsache. Trainingssequenz-Verarbeitungstechnik</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433946/">  Dies ist der zweite Artikel √ºber die Analyse und Untersuchung von Materialien aus dem Wettbewerb um die Suche nach Schiffen auf See.  Aber jetzt werden wir die Eigenschaften von Trainingssequenzen untersuchen.  Versuchen wir, √ºbersch√ºssige Informationen und Redundanz in den Quelldaten zu finden und zu l√∂schen. <br><br><img src="https://habrastorage.org/webt/b4/yk/pz/b4ykpzewv86nxd0szou25c_egzg.jpeg"><br><br>  Dieser Artikel ist auch einfach das Ergebnis von Neugier und m√º√üigem Interesse, nichts davon ist in der Praxis anzutreffen, und f√ºr praktische Aufgaben gibt es fast nichts zum Kopieren und Einf√ºgen.  Dies ist eine kleine Studie √ºber die Eigenschaften der Trainingssequenz. Die Argumentation und der Code des Autors werden vorgestellt. Sie k√∂nnen alles selbst √ºberpr√ºfen / erg√§nzen / √§ndern. <br><br>  Der Kaggle Marine Search Wettbewerb ist vor kurzem beendet.  Airbus schlug vor, Satellitenbilder des Meeres mit und ohne Schiffe zu analysieren.  Insgesamt 192555 Bilder 768x768x3 - sind 340 720 680 960 Bytes, wenn uint8 und dies ist eine riesige Menge an Informationen, und es bestand der vage Verdacht, dass nicht alle Bilder f√ºr das Training des Netzwerks ben√∂tigt werden, und bei dieser Menge an Informationen sind die Wiederholung und Redundanz offensichtlich.  Beim Training eines Netzwerks ist es √ºblich, einige der Daten zu trennen und nicht f√ºr das Training zu verwenden, sondern die Qualit√§t des Trainings zu √ºberpr√ºfen.  Und wenn ein und derselbe Abschnitt des Meeres in zwei verschiedene Bilder und gleichzeitig ein Bild in die Trainingssequenz und das andere in die Verifizierungssequenz fiel, verliert die Verifikation ihre Bedeutung und das Netzwerk wird umgeschult. Wir werden die F√§higkeit des Netzwerks zur Verallgemeinerung von Informationen nicht √ºberpr√ºfen, da die Daten dieselben sind.  Der Kampf gegen dieses Ph√§nomen hat die GPU der Teilnehmer viel Zeit und M√ºhe gekostet.  Wie √ºblich haben es Gewinner und Preistr√§ger nicht eilig, ihren Fans die Geheimnisse der Meisterschaft zu zeigen und den Code zu entwerfen, und es gibt keine M√∂glichkeit, ihn zu studieren und zu lernen, also werden wir die Theorie aufgreifen. <br><a name="habracut"></a><br>  Eine einfache visuelle √úberpr√ºfung ergab, dass es wirklich zu viele Daten gibt, derselbe Abschnitt des Meeres in verschiedene Bilder zerfiel, siehe Beispiele <br><br><img src="https://habrastorage.org/webt/b8/wn/tb/b8wntbyikiqqjafizkmc6okundc.png"><br><br><img src="https://habrastorage.org/webt/ph/xh/g1/phxhg1aaqljhnqiaq28h17fktvo.png"><br><br><img src="https://habrastorage.org/webt/7z/pz/ua/7zpzuaapp2rhjfsxqbqe2jip5jk.png"><br><br><img src="https://habrastorage.org/webt/ed/yx/7c/edyx7cyftluhepdskpo7thftvlm.png"><br><br>  Aus diesem Grund sind wir nicht an realen Daten interessiert, es gibt viele falsche Abh√§ngigkeiten, unn√∂tige Verbindungen zu uns, schlechtes Markup und andere M√§ngel. <br><br>  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Artikel haben</a> wir uns Bilder mit Ellipsen und Rauschen angesehen und werden sie weiter untersuchen.  Der Vorteil dieses Ansatzes besteht darin, dass, wenn Sie ein attraktives Merkmal eines Netzwerks finden, das auf einem beliebigen Satz von Bildern trainiert wurde, nicht klar ist, ob dies eine Netzwerkeigenschaft oder eine Eigenschaft eines Trainingssatzes ist.  Die statistischen Parameter von Sequenzen aus der realen Welt sind unbekannt.  Vor <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">kurzem</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sprach</a> Gro√ümeister <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">Pleskov</a> Pavel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">paske57</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dar√ºber,</a> wie einfach es manchmal ist, eine Segmentierungs- / Klassifizierungsklassifizierung von Bildern zu gewinnen, wenn es gut ist, sich selbst mit den Daten zu befassen, siehe beispielsweise Fotometadaten.  Und es gibt keine Garantie daf√ºr, dass es in den realen Daten keine solchen Abh√§ngigkeiten gibt, die unfreiwillig verbleiben.  Um die Eigenschaften des Netzwerks zu untersuchen, machen wir Bilder mit Ellipsen und Rechtecken und bestimmen den Ort und die Farbe sowie andere Parameter mithilfe eines Zufallszahlengenerators eines Computers (der einen Pseudozufallsgenerator hat, der einen Generator hat, der auf anderen nicht digitalen Algorithmen und physikalischen Eigenschaften der Substanz basiert). Wir werden dies jedoch in diesem Artikel nicht diskutieren. <br><br>  Nehmen Sie also das Meer <i>np.random.sample () * 0.75</i> , wir brauchen keine Wellen, Wind, K√ºsten und andere versteckte Muster und Gesichter.  Die Schiffe / Ellipsen werden ebenfalls in der gleichen Farbe lackiert. Um das Meer vom Boot und von St√∂rungen zu unterscheiden, f√ºgen Sie dem Meer oder dem Boot / St√∂rsender 0,25 hinzu, und sie haben alle die gleiche Form - Ellipsen unterschiedlicher Gr√∂√üe und Ausrichtung.  Die Interferenz wird auch nur Rechtecke der gleichen Farbe wie die Ellipse sein - dies ist wichtig, Information und Interferenz der gleichen Farbe vor dem Hintergrund des Rauschens.  Wir werden nur eine kleine √Ñnderung an der Farbgebung <i>vornehmen</i> und <i>np.random.sample ()</i> f√ºr jedes Bild und f√ºr jede Ellipse / jedes Rechteck <i>ausf√ºhren</i> , d. H.  Weder der Hintergrund noch die Farbe der Ellipse / des Rechtecks ‚Äã‚Äãwerden wiederholt.  Weiter im Text gibt es einen Code des Programms zum Erstellen von Bildern / Masken und ein Beispiel von zehn zuf√§llig ausgew√§hlten Paaren. <br><br>  Nehmen Sie eine sehr verbreitete Version des Netzwerks (Sie k√∂nnen Ihr Lieblingsnetzwerk verwenden) und versuchen Sie, die Redundanz einer gro√üen Trainingssequenz zu identifizieren und anzuzeigen, um zumindest einige qualitative und quantitative Merkmale der Redundanz zu erhalten.  Das hei√üt,  Der Autor glaubt, dass viele Gigabyte an Trainingssequenzen im Wesentlichen redundant sind, es gibt viele unn√∂tige Bilder, es ist nicht erforderlich, Dutzende von GPUs zu laden und unn√∂tige Berechnungen durchzuf√ºhren.  Die Redundanz von Daten manifestiert sich nicht nur und nicht so sehr in der Tatsache, dass dieselben Teile in unterschiedlichen Bildern angezeigt werden, sondern auch in der Redundanz von Informationen in diesen Daten.  Daten k√∂nnen redundant sein, auch wenn sie nicht genau wiederholt werden.  Bitte beachten Sie, dass dies keine strikte Definition von Informationen und deren Angemessenheit oder Redundanz ist.  Wir m√∂chten nur herausfinden, um wie viel Sie den Zug reduzieren k√∂nnen, welche Bilder Sie aus der Trainingssequenz herauswerfen k√∂nnen und wie viele Bilder f√ºr ein akzeptables Training (wir werden die Genauigkeit selbst im Programm festlegen) ausreichen.  Dies ist ein spezifisches Programm, ein spezifischer Datensatz, und es ist m√∂glich, dass auf Ellipsen mit Dreiecken als Hindernis nichts so gut funktioniert wie auf Ellipsen mit Rechtecken (meine Hypothese ist, dass alles gleich und gleich sein wird. Aber wir √ºberpr√ºfen es jetzt nicht , wir f√ºhren keine Analyse durch und beweisen keine Theoreme). <br><br>  Also gegeben: <br><br><ul><li>  Lernsequenz von Bild / Masken-Paaren.  Wir k√∂nnen beliebig viele Bilder / Maskenpaare erzeugen.  Ich werde die Frage sofort beantworten - warum sind Farbe und Hintergrund zuf√§llig?  Ich werde einfach, kurz, klar und umfassend antworten, dass es mir so gut gef√§llt, dass eine zus√§tzliche Einheit in Form einer Grenze nicht ben√∂tigt wird. </li><li>  Das Netzwerk ist ein gew√∂hnliches, gew√∂hnliches U-Netz, leicht modifiziert und wird h√§ufig zur Segmentierung verwendet. </li></ul><br>  Idee zu testen: <br><br><ul><li>  In der konstruierten Sequenz werden wie bei realen Aufgaben Gigabyte an Daten verwendet.  Der Autor ist der Ansicht, dass die Gr√∂√üe der Trainingssequenz nicht so kritisch ist und es nicht viele Daten geben sollte, aber sie sollten ‚Äûviele‚Äú Informationen enthalten.  Eine solche Menge, zehntausend Paar Bilder / Masken, ist nicht erforderlich, und das Netzwerk lernt aus einer viel geringeren Datenmenge. </li></ul><br>  Beginnen wir, w√§hlen Sie 10.000 Paare aus und betrachten Sie sie sorgf√§ltig.  Wir werden das gesamte Wasser und alle unn√∂tigen Teile dieser Trainingssequenz auspressen und alle trockenen R√ºckst√§nde verwenden und in die Praxis umsetzen. <br><br>  Sie k√∂nnen jetzt Ihre Intuition √ºberpr√ºfen und davon ausgehen, wie viele Paare von 10.000 ausreichen, um ein anderes zu trainieren und vorherzusagen, aber auch eine Folge von 10.000 Paaren mit einer Genauigkeit von mehr als 0,98 erstellen.  Schreiben Sie nach dem Vergleich auf ein Blatt Papier. <br><br>  <i>Beachten</i> Sie f√ºr den praktischen Gebrauch, dass sowohl das Meer als auch die Schiffe mit St√∂rungen k√ºnstlich ausgew√§hlt werden. Dies ist <i>np.random.sample ()</i> . <br><br><div class="spoiler">  <b class="spoiler_title">Wir laden Bibliotheken, wir bestimmen die Gr√∂√üe eines Arrays von Bildern</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.draw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ellipse, polygon <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input,Conv2D,Conv2DTranspose,MaxPooling2D,concatenate <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization,Activation,Add,Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.losses <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> binary_crossentropy <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> keras w_size = <span class="hljs-number"><span class="hljs-number">128</span></span> train_num = <span class="hljs-number"><span class="hljs-number">10000</span></span> radius_min = <span class="hljs-number"><span class="hljs-number">10</span></span> radius_max = <span class="hljs-number"><span class="hljs-number">20</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Bestimmen Sie die Verlust- und Genauigkeitsfunktionen</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dice_coef</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> y_true_f = K.flatten(y_true) y_pred = K.cast(y_pred, <span class="hljs-string"><span class="hljs-string">'float32'</span></span>) y_pred_f = K.cast(K.greater(K.flatten(y_pred), <span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-string"><span class="hljs-string">'float32'</span></span>) intersection = y_true_f * y_pred_f score = <span class="hljs-number"><span class="hljs-number">2.</span></span> * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> score <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dice_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> smooth = <span class="hljs-number"><span class="hljs-number">1.</span></span> y_true_f = K.flatten(y_true) y_pred_f = K.flatten(y_pred) intersection = y_true_f * y_pred_f score = (<span class="hljs-number"><span class="hljs-number">2.</span></span> * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> - score <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bce_dice_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_iou_vector</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, B)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Numpy version batch_size = A.shape[0] metric = 0.0 for batch in range(batch_size): t, p = A[batch], B[batch] true = np.sum(t) pred = np.sum(p) # deal with empty mask first if true == 0: metric += (pred == 0) continue # non empty mask case. Union is never empty # hence it is safe to divide by its number of pixels intersection = np.sum(t * p) union = true + pred - intersection iou = intersection / union # iou metrric is a stepwise approximation of the real iou over 0.5 iou = np.floor(max(0, (iou - 0.45)*20)) / 10 metric += iou # teake the average over all images in batch metric /= batch_size return metric def my_iou_metric(label, pred): # Tensorflow version return tf.py_func(get_iou_vector, [label, pred &gt; 0.5], tf.float64) from keras.utils.generic_utils import get_custom_objects get_custom_objects().update({'bce_dice_loss': bce_dice_loss }) get_custom_objects().update({'dice_loss': dice_loss }) get_custom_objects().update({'dice_coef': dice_coef }) get_custom_objects().update({'my_iou_metric': my_iou_metric })</span></span></code> </pre><br></div></div><br>  Wir werden die Metrik aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Artikel verwenden</a> .  Ich erinnere die Leser daran, dass wir die Maske des Pixels vorhersagen werden - dies ist das "Meer" oder "Schiff" - und die Wahrheit oder Falschheit der Vorhersage bewerten werden.  Das hei√üt,  Die folgenden vier Optionen sind m√∂glich: Wir haben richtig vorausgesagt, dass ein Pixel ein ‚ÄûMeer‚Äú ist, richtig vorausgesagt, dass ein Pixel ein ‚ÄûSchiff‚Äú ist, oder einen Fehler bei der Vorhersage eines ‚ÄûMeeres‚Äú oder eines ‚ÄûSchiffs‚Äú gemacht.  Daher sch√§tzen wir f√ºr alle Bilder und alle Pixel die Anzahl aller vier Optionen und berechnen das Ergebnis - dies ist das Ergebnis des Netzwerks.  Und je weniger fehlerhafte Vorhersagen und wahrer, desto genauer das Ergebnis und desto besser das Netzwerk. <br><br>  Nehmen wir f√ºr die Forschung die Option des gut untersuchten U-Netzes, das ein hervorragendes Netzwerk f√ºr die Bildsegmentierung darstellt.  Die nicht so klassische U-Net-Option wurde gew√§hlt, aber die Idee ist dieselbe: Das Netzwerk f√ºhrt eine sehr einfache Operation mit Bildern durch - es reduziert die Bilddimension mit einigen Transformationen Schritt f√ºr Schritt und versucht dann, die Maske aus dem komprimierten Bild wiederherzustellen.  Das hei√üt,  In unserem Fall wird die Bildgr√∂√üe auf 16 x 16 gebracht. Anschlie√üend versuchen wir, die Maske mithilfe von Daten aus allen vorherigen Komprimierungsebenen wiederherzustellen. <br><br>  Wir untersuchen das Netzwerk als ‚ÄûBlack Box‚Äú, wir werden nicht untersuchen, was mit dem Netzwerk im Inneren passiert, wie sich Gewichte √§ndern und wie Gradienten ausgew√§hlt werden - dies ist das Thema einer anderen Studie. <br><br><div class="spoiler">  <b class="spoiler_title">U-Netz mit Bl√∂cken</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convolution_block</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, filters, size, strides=</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">,</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, padding=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'same'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, activation=True)</span></span></span><span class="hljs-function">:</span></span> x = Conv2D(filters, size, strides=strides, padding=padding)(x) x = BatchNormalization()(x) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> activation == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">residual_block</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(blockInput, num_filters=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">16</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(blockInput) x = BatchNormalization()(x) x = convolution_block(x, num_filters, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>) ) x = convolution_block(x, num_filters, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) x = Add()([x, blockInput]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x <span class="hljs-comment"><span class="hljs-comment"># Build model def build_model(input_layer, start_neurons, DropoutRatio = 0.5): conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding="same" )(input_layer) conv1 = residual_block(conv1,start_neurons * 1) conv1 = residual_block(conv1,start_neurons * 1) conv1 = Activation('relu')(conv1) pool1 = MaxPooling2D((2, 2))(conv1) pool1 = Dropout(DropoutRatio/2)(pool1) conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding="same" )(pool1) conv2 = residual_block(conv2,start_neurons * 2) conv2 = residual_block(conv2,start_neurons * 2) conv2 = Activation('relu')(conv2) pool2 = MaxPooling2D((2, 2))(conv2) pool2 = Dropout(DropoutRatio)(pool2) conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding="same")(pool2) conv3 = residual_block(conv3,start_neurons * 4) conv3 = residual_block(conv3,start_neurons * 4) conv3 = Activation('relu')(conv3) pool3 = MaxPooling2D((2, 2))(conv3) pool3 = Dropout(DropoutRatio)(pool3) conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding="same")(pool3) conv4 = residual_block(conv4,start_neurons * 8) conv4 = residual_block(conv4,start_neurons * 8) conv4 = Activation('relu')(conv4) pool4 = MaxPooling2D((2, 2))(conv4) pool4 = Dropout(DropoutRatio)(pool4) # Middle convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding="same")(pool4) convm = residual_block(convm,start_neurons * 16) convm = residual_block(convm,start_neurons * 16) convm = Activation('relu')(convm) deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(convm) uconv4 = concatenate([deconv4, conv4]) uconv4 = Dropout(DropoutRatio)(uconv4) uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding="same")(uconv4) uconv4 = residual_block(uconv4,start_neurons * 8) uconv4 = residual_block(uconv4,start_neurons * 8) uconv4 = Activation('relu')(uconv4) deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding="same")(uconv4) uconv3 = concatenate([deconv3, conv3]) uconv3 = Dropout(DropoutRatio)(uconv3) uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding="same")(uconv3) uconv3 = residual_block(uconv3,start_neurons * 4) uconv3 = residual_block(uconv3,start_neurons * 4) uconv3 = Activation('relu')(uconv3) deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding="same")(uconv3) uconv2 = concatenate([deconv2, conv2]) uconv2 = Dropout(DropoutRatio)(uconv2) uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding="same")(uconv2) uconv2 = residual_block(uconv2,start_neurons * 2) uconv2 = residual_block(uconv2,start_neurons * 2) uconv2 = Activation('relu')(uconv2) deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding="same")(uconv2) uconv1 = concatenate([deconv1, conv1]) uconv1 = Dropout(DropoutRatio)(uconv1) uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding="same")(uconv1) uconv1 = residual_block(uconv1,start_neurons * 1) uconv1 = residual_block(uconv1,start_neurons * 1) uconv1 = Activation('relu')(uconv1) uconv1 = Dropout(DropoutRatio/2)(uconv1) output_layer = Conv2D(1, (1,1), padding="same", activation="sigmoid")(uconv1) return output_layer # model input_layer = Input((w_size, w_size, 3)) output_layer = build_model(input_layer, 16) model = Model(input_layer, output_layer) model.compile(loss=bce_dice_loss, optimizer="adam", metrics=[my_iou_metric]) model.summary()</span></span></code> </pre> <br></div></div><br>  Die Funktion zum Erzeugen von Bild / Masken-Paaren.  Auf einem 128x128-Farbbild, das mit zuf√§lligem Rauschen gef√ºllt ist und zuf√§llig aus zwei Bereichen ausgew√§hlt wurde, entweder 0,0 ... 0,75 oder 0,25..1,0.  Platzieren Sie zuf√§llig eine zuf√§llig ausgerichtete Ellipse im Bild und platzieren Sie ein Rechteck an derselben Stelle.  Wir pr√ºfen, ob sie sich nicht schneiden, und verschieben gegebenenfalls das Rechteck zur Seite.  Jedes Mal berechnen wir die Werte der F√§rbung des Meeres / Bootes neu.  Der Einfachheit halber werden wir die Maske mit dem Bild in einem Array als vierte Farbe platzieren, d.h.  Red.Green.Blue.Mask, es ist einfacher. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_pair</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> img_l = (np.random.sample((w_size, w_size, <span class="hljs-number"><span class="hljs-number">3</span></span>))* <span class="hljs-number"><span class="hljs-number">0.75</span></span>).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) img_h = (np.random.sample((w_size, w_size, <span class="hljs-number"><span class="hljs-number">3</span></span>))* <span class="hljs-number"><span class="hljs-number">0.75</span></span> + <span class="hljs-number"><span class="hljs-number">0.25</span></span>).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) img = np.zeros((w_size, w_size, <span class="hljs-number"><span class="hljs-number">4</span></span>), dtype=<span class="hljs-string"><span class="hljs-string">'float'</span></span>) p = np.random.sample() - <span class="hljs-number"><span class="hljs-number">0.5</span></span> r = np.random.sample()*(w_size<span class="hljs-number"><span class="hljs-number">-2</span></span>*radius_max) + radius_max c = np.random.sample()*(w_size<span class="hljs-number"><span class="hljs-number">-2</span></span>*radius_max) + radius_max r_radius = np.random.sample()*(radius_max-radius_min) + radius_min c_radius = np.random.sample()*(radius_max-radius_min) + radius_min rot = np.random.sample()*<span class="hljs-number"><span class="hljs-number">360</span></span> rr, cc = ellipse( r, c, r_radius, c_radius, rotation=np.deg2rad(rot), shape=img_l.shape ) p1 = np.rint(np.random.sample()* (w_size<span class="hljs-number"><span class="hljs-number">-2</span></span>*radius_max) + radius_max) p2 = np.rint(np.random.sample()* (w_size<span class="hljs-number"><span class="hljs-number">-2</span></span>*radius_max) + radius_max) p3 = np.rint(np.random.sample()* (<span class="hljs-number"><span class="hljs-number">2</span></span>*radius_max - radius_min) + radius_min) p4 = np.rint(np.random.sample()* (<span class="hljs-number"><span class="hljs-number">2</span></span>*radius_max - radius_min) + radius_min) poly = np.array(( (p1, p2), (p1, p2+p4), (p1+p3, p2+p4), (p1+p3, p2), (p1, p2), )) rr_p, cc_p = polygon(poly[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], poly[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], img_l.shape) in_sc_rr = list(set(rr) &amp; set(rr_p)) in_sc_cc = list(set(cc) &amp; set(cc_p)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(in_sc_rr) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(in_sc_cc) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(in_sc_rr) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: _delta_rr = np.max(in_sc_rr) - np.min(in_sc_rr) + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> np.mean(rr_p) &gt; np.mean(in_sc_rr): poly[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] += _delta_rr <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: poly[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] -= _delta_rr <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(in_sc_cc) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: _delta_cc = np.max(in_sc_cc) - np.min(in_sc_cc) + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> np.mean(cc_p) &gt; np.mean(in_sc_cc): poly[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] += _delta_cc <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: poly[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] -= _delta_cc rr_p, cc_p = polygon(poly[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], poly[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], img_l.shape) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> p &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: img[:,:,:<span class="hljs-number"><span class="hljs-number">3</span></span>] = img_l.copy() img[rr, cc,:<span class="hljs-number"><span class="hljs-number">3</span></span>] = img_h[rr, cc] img[rr_p, cc_p,:<span class="hljs-number"><span class="hljs-number">3</span></span>] = img_h[rr_p, cc_p] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: img[:,:,:<span class="hljs-number"><span class="hljs-number">3</span></span>] = img_h.copy() img[rr, cc,:<span class="hljs-number"><span class="hljs-number">3</span></span>] = img_l[rr, cc] img[rr_p, cc_p,:<span class="hljs-number"><span class="hljs-number">3</span></span>] = img_l[rr_p, cc_p] img[:,:,<span class="hljs-number"><span class="hljs-number">3</span></span>] = <span class="hljs-number"><span class="hljs-number">0.</span></span> img[rr, cc,<span class="hljs-number"><span class="hljs-number">3</span></span>] = <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> img</code> </pre><br>  Lassen Sie uns eine Trainingssequenz von Paaren erstellen, siehe Zufall 10 <br><br><pre> <code class="python hljs">_txy = [next_pair() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(train_num)] f_imgs = np.array(_txy)[:,:,:,:<span class="hljs-number"><span class="hljs-number">3</span></span>].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">3</span></span>) f_msks = np.array(_txy)[:,:,:,<span class="hljs-number"><span class="hljs-number">3</span></span>:].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">del</span></span>(_txy) <span class="hljs-comment"><span class="hljs-comment">#    10   fig, axes = plt.subplots(2, 10, figsize=(20, 5)) for k in range(10): kk = np.random.randint(train_num) axes[0,k].set_axis_off() axes[0,k].imshow(f_imgs[kk]) axes[1,k].set_axis_off() axes[1,k].imshow(f_msks[kk].squeeze())</span></span></code> </pre><br><img src="https://habrastorage.org/webt/42/7w/x6/427wx65rkih5858776eoahbgbhw.png"><br><br><h3>  Erster Schritt.  Lassen Sie uns versuchen, auf einem minimalen Satz zu trainieren </h3><br>  Der erste Schritt unseres Experiments ist einfach. Wir versuchen, das Netzwerk so zu trainieren, dass nur 11 erste Bilder vorhergesagt werden. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">10</span></span> val_len = <span class="hljs-number"><span class="hljs-number">11</span></span> precision = <span class="hljs-number"><span class="hljs-number">0.85</span></span> m0_select = np.zeros((f_imgs.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]), dtype=<span class="hljs-string"><span class="hljs-string">'int'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(val_len): m0_select[k] = <span class="hljs-number"><span class="hljs-number">1</span></span> t = tqdm() <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: fit = model.fit(f_imgs[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], f_msks[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], batch_size=batch_size, epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span> ) current_accu = fit.history[<span class="hljs-string"><span class="hljs-string">'my_iou_metric'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] current_loss = fit.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] t.set_description(<span class="hljs-string"><span class="hljs-string">"accuracy {0:6.4f} loss {1:6.4f} "</span></span>.\ format(current_accu, current_loss)) t.update(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> current_accu &gt; precision: <span class="hljs-keyword"><span class="hljs-keyword">break</span></span> t.close()</code> </pre> <br> <code>accuracy 0.8636 loss 0.0666 : : 47it [00:29, 5.82it/s]</code> <br> <br>  Wir haben die ersten 11 aus der Anfangssequenz ausgew√§hlt und das Netzwerk darauf trainiert.  Jetzt spielt es keine Rolle, ob das Netzwerk diese Bilder speziell speichert oder zusammenfasst. Hauptsache, es kann diese 11 Bilder so erkennen, wie wir es brauchen.  Abh√§ngig vom ausgew√§hlten Datensatz und der Genauigkeit kann das Netzwerktraining sehr lange dauern.  Wir haben aber nur wenige Iterationen.  Ich wiederhole, dass es uns jetzt egal ist, wie und was das Netzwerk gelernt oder gelernt hat. Hauptsache, es hat die etablierte Genauigkeit der Vorhersage erreicht. <br><br><h3>  Starten Sie nun das Hauptexperiment </h3><br>  Wir werden neue Bild / Masken-Paare aus der konstruierten Sequenz nehmen und versuchen, sie durch das trainierte Netzwerk auf der bereits ausgew√§hlten Sequenz vorherzusagen.  Am Anfang sind es nur 11 Bild- / Maskenpaare und das Netzwerk ist trainiert, vielleicht nicht sehr richtig.  Wenn in einem neuen Paar die Maske aus dem Bild mit akzeptabler Genauigkeit vorhergesagt wird, verwerfen wir dieses Paar, es enth√§lt keine neuen Informationen f√ºr das Netzwerk, es kennt die Maske bereits und kann sie aus diesem Bild berechnen.  Wenn die Genauigkeit der Vorhersage nicht ausreicht, f√ºgen wir dieses Bild mit einer Maske zu unserer Sequenz hinzu und beginnen, das Netzwerk zu trainieren, bis ein akzeptables Genauigkeitsergebnis f√ºr die ausgew√§hlte Sequenz erzielt wird.  Das hei√üt,  Dieses Bild enth√§lt neue Informationen und wir f√ºgen sie unserer Trainingssequenz hinzu und extrahieren die darin enthaltenen Informationen durch Training. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">50</span></span> t_batch_size = <span class="hljs-number"><span class="hljs-number">1024</span></span> raw_len = val_len t = tqdm(<span class="hljs-number"><span class="hljs-number">-1</span></span>) id_train = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-comment"><span class="hljs-comment">#id_select = 1 while True: t.set_description("Accuracy {0:6.4f} loss {1:6.4f}\ selected img {2:5d} tested img {3:5d} ". format(current_accu, current_loss, val_len, raw_len)) t.update(1) if id_train == 1: fit = model.fit(f_imgs[m0_select&gt;0], f_msks[m0_select&gt;0], batch_size=batch_size, epochs=1, verbose=0 ) current_accu = fit.history['my_iou_metric'][0] current_loss = fit.history['loss'][0] if current_accu &gt; precision: id_train = 0 else: t_pred = model.predict( f_imgs[raw_len: min(raw_len+t_batch_size,f_imgs.shape[0])], batch_size=batch_size ) for kk in range(t_pred.shape[0]): val_iou = get_iou_vector( f_msks[raw_len+kk].reshape(1,w_size,w_size,1), t_pred[kk].reshape(1,w_size,w_size,1) &gt; 0.5) if val_iou &lt; precision*0.95: new_img_test = 1 m0_select[raw_len+kk] = 1 val_len += 1 break raw_len += (kk+1) id_train = 1 if raw_len &gt;= train_num: break t.close()</span></span></code> </pre><br><pre> <code class="bash hljs">Accuracy 0.9830 loss 0.0287 selected img 271 tested img 9949 : : 1563it [14:16, 1.01it/s]</code> </pre> <br>  Hier wird Genauigkeit im Sinne von "Genauigkeit" und nicht als Standard-Keras-Metrik verwendet, und das Unterprogramm "my_iou_metric" wird zur Berechnung der Genauigkeit verwendet.  Es ist sehr interessant, die Genauigkeit und Anzahl der untersuchten und hinzugef√ºgten Bilder zu beobachten.  Zu Beginn werden fast alle Bild / Masken-Paare vom Netzwerk hinzugef√ºgt, und irgendwo um die 70 beginnt es wegzuwerfen.  N√§her an 8000 wirft fast alle Paare. <br><br>  √úberpr√ºfen Sie visuell zuf√§llige Paare, die vom Netzwerk ausgew√§hlt wurden: <br><br><pre> <code class="python hljs">fig, axes = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) t_imgs = f_imgs[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>] t_msks = f_msks[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): kk = np.random.randint(t_msks.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>,k].set_axis_off() axes[<span class="hljs-number"><span class="hljs-number">0</span></span>,k].imshow(t_imgs[kk]) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>,k].set_axis_off() axes[<span class="hljs-number"><span class="hljs-number">1</span></span>,k].imshow(t_msks[kk].squeeze())</code> </pre><br>  Nichts Besonderes oder √úbernat√ºrliches: <br><br><img src="https://habrastorage.org/webt/dq/rr/7r/dqrr7rze9sbmmoepvvjuma-tjxu.png"><br><br>  Dies sind Paare, die vom Netzwerk in verschiedenen Trainingsphasen ausgew√§hlt werden.  Wenn das Netzwerk ein Eingabepaar aus dieser Sequenz erhielt, konnte es die Maske nicht mit der angegebenen Genauigkeit berechnen, und dieses Paar wurde in die Trainingssequenz aufgenommen.  Aber nichts Besonderes, gew√∂hnliche Bilder. <br><br><h3>  √úberpr√ºfung des Ergebnisses und der Richtigkeit </h3><br>  Lassen Sie uns die Qualit√§t des Netzwerktrainingsprogramms √ºberpr√ºfen und sicherstellen, dass die Qualit√§t nicht wesentlich von der Reihenfolge der Anfangssequenz abh√§ngt, f√ºr die wir die Anfangssequenz von Bild / Masken-Paaren mischen, die anderen 11 zuerst und auf die gleiche Weise nehmen, das Netzwerk trainieren und den √úberschuss abschneiden. <br><br><pre> <code class="python hljs">sh = np.arange(train_num) np.random.shuffle(sh) f0_imgs = f_imgs[sh] f0_msks = f_msks[sh] model.compile(loss=bce_dice_loss, optimizer=<span class="hljs-string"><span class="hljs-string">"adam"</span></span>, metrics=[my_iou_metric]) model.summary()</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Trainingscode</b> <div class="spoiler_text"><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">10</span></span> val_len = <span class="hljs-number"><span class="hljs-number">11</span></span> precision = <span class="hljs-number"><span class="hljs-number">0.85</span></span> m0_select = np.zeros((f_imgs.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]), dtype=<span class="hljs-string"><span class="hljs-string">'int'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(val_len): m0_select[k] = <span class="hljs-number"><span class="hljs-number">1</span></span> t = tqdm() <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: fit = model.fit(f0_imgs[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], f0_msks[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], batch_size=batch_size, epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span> ) current_accu = fit.history[<span class="hljs-string"><span class="hljs-string">'my_iou_metric'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] current_loss = fit.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] t.set_description(<span class="hljs-string"><span class="hljs-string">"accuracy {0:6.4f} loss {1:6.4f} "</span></span>.\ format(current_accu, current_loss)) t.update(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> current_accu &gt; precision: <span class="hljs-keyword"><span class="hljs-keyword">break</span></span> t.close()</code> </pre> <br><pre> <code class="bash hljs">accuracy 0.8636 loss 0.0710 : : 249it [01:03, 5.90it/s]</code> </pre> <br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">50</span></span> t_batch_size = <span class="hljs-number"><span class="hljs-number">1024</span></span> raw_len = val_len t = tqdm(<span class="hljs-number"><span class="hljs-number">-1</span></span>) id_train = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-comment"><span class="hljs-comment">#id_select = 1 while True: t.set_description("Accuracy {0:6.4f} loss {1:6.4f}\ selected img {2:5d} tested img {3:5d} ". format(current_accu, current_loss, val_len, raw_len)) t.update(1) if id_train == 1: fit = model.fit(f0_imgs[m0_select&gt;0], f0_msks[m0_select&gt;0], batch_size=batch_size, epochs=1, verbose=0 ) current_accu = fit.history['my_iou_metric'][0] current_loss = fit.history['loss'][0] if current_accu &gt; precision: id_train = 0 else: t_pred = model.predict( f_imgs[raw_len: min(raw_len+t_batch_size,f_imgs.shape[0])], batch_size=batch_size ) for kk in range(t_pred.shape[0]): val_iou = get_iou_vector( f_msks[raw_len+kk].reshape(1,w_size,w_size,1), t_pred[kk].reshape(1,w_size,w_size,1) &gt; 0.5) if val_iou &lt; precision*0.95: new_img_test = 1 m0_select[raw_len+kk] = 1 val_len += 1 break raw_len += (kk+1) id_train = 1 if raw_len &gt;= train_num: break t.close()</span></span></code> </pre><br><pre> <code class="bash hljs">Accuracy 0.9890 loss 0.0224 selected img 408 tested img 9456 : : 1061it [21:13, 2.16s/it]</code> </pre> <br></div></div><br>  Das Ergebnis h√§ngt nicht wesentlich von der Reihenfolge der Paare der urspr√ºnglichen Sequenz ab.  Im vorherigen Fall hat das Netzwerk 271, jetzt 408 gew√§hlt. Wenn Sie es verwechseln, kann das Netzwerk einen anderen Betrag ausw√§hlen.  Wir werden nicht √ºberpr√ºfen, der Autor glaubt, dass es immer wesentlich weniger als 10.000 geben wird. <br><br>  √úberpr√ºfen Sie die Genauigkeit der Netzwerkvorhersage in einer neuen unabh√§ngigen Sequenz <br><br><pre> <code class="python hljs">_txy = [next_pair() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(train_num)] test_imgs = np.array(_txy)[:,:,:,:<span class="hljs-number"><span class="hljs-number">3</span></span>].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">3</span></span>) test_msks = np.array(_txy)[:,:,:,<span class="hljs-number"><span class="hljs-number">3</span></span>:].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">del</span></span>(_txy) test_pred_0 = model.predict(test_imgs) t_val_0 = get_iou_vector(test_msks,test_pred_0) t_val_0</code> </pre> <br><pre> <code class="bash hljs">0.9927799999999938</code> </pre> <br><br><h3>  Zusammenfassung und Schlussfolgerungen </h3><br>  Wir konnten also weniger als drei bis vierhundert aus 10.000 Paaren ausw√§hlen, die Vorhersagegenauigkeit betr√§gt 0,99278, wir haben alle Paare, die mindestens einige n√ºtzliche Informationen enthalten, genommen und den Rest weggeworfen.  Wir haben die statistischen Parameter der Trainingssequenz nicht angepasst, die Wiederholbarkeit von Informationen hinzugef√ºgt usw.  und benutzte √ºberhaupt keine statistischen Methoden.  Wir machen ein Bild, das Informationen enth√§lt, die dem Netzwerk noch unbekannt sind, und dr√ºcken alles aus dem Gewicht des Netzwerks heraus.  Wenn das Netzwerk mindestens einem ‚Äûmysteri√∂sen‚Äú Bild entspricht, wird es im Gesch√§ftsleben verwendet. <br><br>  Insgesamt 271 Bild / Masken-Paare enthalten Informationen zur Vorhersage von 10.000 Paaren mit einer Genauigkeit von mindestens 0,8075 f√ºr jedes Paar, d. H. Die Gesamtgenauigkeit √ºber die gesamte Sequenz ist h√∂her, aber in jedem Bild ist sie nicht kleiner als 0,8075. Wir haben keine Bilder, die wir nicht haben wir k√∂nnen vorhersagen und wir kennen die untere Grenze dieser Vorhersage.  (Hier prahlte der Autor nat√ºrlich, dass der Artikel ohne diese Aussage diese Aussage, etwa 0,8075 oder Beweise, nicht verifiziert, aber h√∂chstwahrscheinlich ist dies wahr) <br><br>  Um das Netzwerk zu trainieren, muss die GPU nicht mit allem geladen werden, was zur Hand ist. Sie k√∂nnen den Kern des Zuges herausziehen und das Netzwerk zu Beginn des Trainings darauf trainieren.  Wenn Sie neue Bilder erhalten, k√∂nnen Sie diejenigen, die das Netzwerk nicht vorhersagen konnte, manuell markieren und zum Kern des Zuges hinzuf√ºgen. Dadurch wird das Netzwerk erneut geschult, um alle Informationen aus den neuen Bildern herauszuholen.  Und es ist nicht notwendig, eine Validierungssequenz herauszugreifen, wir k√∂nnen davon ausgehen, dass alles andere au√üer der ausgew√§hlten eine Validierungssequenz ist. <br><br>  Noch eine mathematisch nicht strenge, aber sehr wichtige Bemerkung.  Man kann mit Sicherheit sagen, dass jedes Bild / Masken-Paar ‚Äûviele‚Äú Informationen enth√§lt.  Jedes Paar enth√§lt ‚Äûviele‚Äú Informationen, obwohl sich die Informationen in den meisten Bild- / Maskenpaaren √ºberschneiden oder wiederholen.  Jedes der 271 Bild / Masken-Paare enth√§lt Informationen, die f√ºr die Vorhersage wesentlich sind, und dieses Paar kann nicht einfach weggeworfen werden. <br><br>  Nun, eine kleine Bemerkung √ºber Falten, viele Experten und Kagglers teilen die Trainingssequenz in Falten auf und trainieren sie separat, wobei sie die auf weitere knifflige Weise erzielten Ergebnisse kombinieren.  In unserem Fall k√∂nnen Sie es auch in Falten unterteilen. Wenn Sie 271 Paare von 10.000 entfernen, k√∂nnen Sie in den verbleibenden Paaren eine neue Wurzelsequenz erstellen, die offensichtlich ein anderes, aber vergleichbares Ergebnis liefert.  Sie k√∂nnen einfach die andere Initiale 11 mischen und nehmen, wie oben gezeigt. <br><br>  Der Artikel enth√§lt einen Code und zeigt, wie U-Net f√ºr die Bildsegmentierung trainiert wird.  Dies ist ein konkretes Beispiel, und in dem Artikel gibt es absichtlich keine Verallgemeinerungen zu anderen Netzwerken, zu anderen Sequenzen, es gibt keine strenge Mathematik, alles wird ‚Äûan den Fingern‚Äú erz√§hlt und gezeigt.  Nur ein Beispiel daf√ºr, wie Sie das Netzwerk lernen und gleichzeitig eine akzeptable Genauigkeit erzielen k√∂nnen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de433946/">https://habr.com/ru/post/de433946/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de433934/index.html">SAFe oder Scaled Agile Framework</a></li>
<li><a href="../de433936/index.html">Auf der Suche nach einem Hightech-Geschenk f√ºr ein Kind? Denken Sie an einen Spielplatz, nicht an einen Laufstall</a></li>
<li><a href="../de433938/index.html">Wie Yandex und Google das Jahr zusammenfassen</a></li>
<li><a href="../de433940/index.html">Wie viel kostet Review im AppStore?</a></li>
<li><a href="../de433944/index.html">Verheerende Ausnahmen</a></li>
<li><a href="../de433948/index.html">So machen Sie die Zahlung bequemer: die Erfahrung eines IaaS-Anbieters</a></li>
<li><a href="../de433952/index.html">10 Gr√ºnde, sich f√ºr eine L√∂sung f√ºr SAP HANA von HPE zu entscheiden. Teil 2</a></li>
<li><a href="../de433954/index.html">Acht Audiotechnologien und Audio-Gadgets, die 2019 in die TECnology Hall of Fame aufgenommen werden</a></li>
<li><a href="../de433956/index.html">Modder haben KI verwendet, um die Textur in Spielen zu verbessern</a></li>
<li><a href="../de433958/index.html">TDD-Anwendungen beim Spring Boot: Arbeiten mit einer Datenbank</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>