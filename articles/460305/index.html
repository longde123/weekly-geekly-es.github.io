<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõï ‚ö±Ô∏è üõå AERODISCO Motor: catastr√≥fico. Parte 2. Metrocluster ü•ô üì∑ üç≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola lectores de Habr! En un art√≠culo anterior, hablamos sobre una herramienta simple de tolerancia a desastres en los sistemas de almacenamiento AERO...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AERODISCO Motor: catastr√≥fico. Parte 2. Metrocluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/460305/"><p><img src="https://habrastorage.org/webt/ft/el/xz/ftelxzp4fmlci1fn9endwmjffew.jpeg"></p><br><p>  Hola lectores de Habr!  En un art√≠culo anterior, hablamos sobre una herramienta simple de tolerancia a desastres en los sistemas de almacenamiento AERODISK ENGINE, sobre la replicaci√≥n.  En este art√≠culo, profundizaremos en un tema m√°s complejo e interesante: el cl√∫ster metropolitano, es decir, un medio de protecci√≥n automatizada ante desastres para dos centros de datos, que permite a los centros de datos trabajar en modo activo-activo.  Lo diremos, mostraremos, romperemos y arreglaremos. </p><a name="habracut"></a><br><h2 id="kak-obychno-v-nachale-teoriya">  Como siempre, al principio de la teor√≠a. </h2><br><p>  Un grupo de metro es un grupo dividido en varios sitios dentro de una ciudad o distrito.  La palabra "cl√∫ster" nos sugiere claramente que el complejo est√° automatizado, es decir, el cambio de nodos del cl√∫ster en caso de fallas se produce autom√°ticamente. </p><br><p>  Aqu√≠ es donde radica la principal diferencia entre el cl√∫ster metropolitano y la replicaci√≥n ordinaria.  Automatizaci√≥n de operaciones.  Es decir, en el caso de ciertos incidentes (falla del centro de datos, canales rotos, etc.), el sistema de almacenamiento realizar√° independientemente las acciones necesarias para mantener la disponibilidad de datos.  Cuando se utilizan r√©plicas regulares, el administrador realiza estas acciones completa o parcialmente de forma manual. </p><br><h3 id="dlya-chego-eto-nuzhno">  ¬øPara qu√© es esto? </h3><br><p> El objetivo principal que los clientes persiguen utilizando una u otra implementaci√≥n del cl√∫ster de metro es minimizar el RTO (Objetivo del tiempo de recuperaci√≥n).  Es decir, minimice el tiempo de recuperaci√≥n de los servicios de TI despu√©s de una falla.  Si usa la replicaci√≥n normal, el tiempo de recuperaci√≥n siempre ser√° m√°s largo que el tiempo de recuperaci√≥n con el cl√∫ster metro.  Por qu√©  Muy simple  El administrador debe estar en el lugar de trabajo y cambiar la replicaci√≥n a mano, y el cl√∫ster de metro lo hace autom√°ticamente. </p><br><p>  Si no tiene un administrador dedicado de guardia que no duerma, coma, fume o se enferme y observe el estado de almacenamiento las 24 horas del d√≠a, no hay forma de garantizar que el administrador estar√° disponible para el cambio manual durante una falla. </p><br><p>  En consecuencia, RTO en ausencia de un grupo de metro o <del>  administrador inmortal nivel 99 </del>  El servicio en servicio del administrador ser√° igual a la suma del tiempo de conmutaci√≥n de todos los sistemas y el per√≠odo m√°ximo de tiempo despu√©s del cual se garantiza que el administrador comenzar√° a trabajar con los sistemas de almacenamiento y sistemas relacionados. </p><br><p>  Por lo tanto, llegamos a la conclusi√≥n obvia de que el cl√∫ster metropolitano debe usarse si el requerimiento de RTO es minutos, no horas o d√≠as, es decir, cuando el departamento de TI debe proporcionar a las empresas tiempo para restablecer el acceso a TI en el caso de la peor ca√≠da del centro de datos -servicios en minutos, o incluso segundos. </p><br><h3 id="kak-eto-rabotaet">  Como funciona </h3><br><p>  En el nivel inferior, el cl√∫ster metropolitano utiliza el mecanismo de replicaci√≥n de datos s√≠ncronos, que describimos en un art√≠culo anterior (ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> ).  Como la replicaci√≥n es sincr√≥nica, los requisitos para ella son apropiados, o m√°s bien: </p><br><ul><li>  fibra como f√≠sica, 10 gigabit Ethernet (o superior); </li><li>  la distancia entre los centros de datos no es m√°s de 40 kil√≥metros; </li><li>  Retardo de canal √≥ptico entre centros de datos (entre sistemas de almacenamiento) hasta 5 milisegundos (√≥ptimamente 2). </li></ul><br><p>  Todos estos requisitos son de naturaleza consultiva, es decir, el cl√∫ster de metro funcionar√° incluso si no se cumplen estos requisitos, pero debe entenderse que las consecuencias del incumplimiento de estos requisitos son iguales a la desaceleraci√≥n de ambos sistemas de almacenamiento en el cl√∫ster de metro. </p><br><p>  Entonces, una r√©plica sincr√≥nica se usa para transferir datos entre sistemas de almacenamiento, y c√≥mo las r√©plicas se cambian autom√°ticamente, y lo m√°s importante, ¬øc√≥mo evitar el cerebro dividido?  Para esto, en el nivel anterior, se utiliza una entidad adicional: el √°rbitro. </p><br><h3 id="kak-rabotaet-arbitr-i-v-chem-ego-zadacha">  ¬øC√≥mo funciona el √°rbitro y cu√°l es su tarea? </h3><br><p>  El √°rbitro es una peque√±a m√°quina virtual, o un cl√∫ster de hardware, que debe ejecutarse en la tercera plataforma (por ejemplo, en la oficina) y proporcionar acceso al almacenamiento a trav√©s de ICMP y SSH.  Despu√©s del lanzamiento, el √°rbitro debe establecer la IP y luego, desde el lado del almacenamiento, indicar su direcci√≥n, m√°s las direcciones de los controladores remotos que participan en el cl√∫ster metropolitano.  Despu√©s de eso, el √°rbitro est√° listo para trabajar. </p><br><p>  El √°rbitro supervisa constantemente todos los sistemas de almacenamiento en el cl√∫ster metropolitano y, si un sistema de almacenamiento no est√° disponible, despu√©s de confirmar que no est√° disponible desde otro miembro del cl√∫ster (uno de los sistemas de almacenamiento "en vivo"), toma la decisi√≥n de iniciar el procedimiento para cambiar las reglas de replicaci√≥n y el mapeo. </p><br><p>  Un punto muy importante.  El √°rbitro siempre debe estar en un sitio diferente de aquellos en los que se encuentra el almacenamiento, es decir, ni en el centro de datos-1, donde se encuentra el almacenamiento 1, ni en el centro de datos-2, donde est√° instalado el almacenamiento 2. </p><br><p>  Por qu√©  Porque la √∫nica forma en que un √°rbitro con la ayuda de uno de los sistemas de almacenamiento sobrevivientes puede determinar de manera inequ√≠voca y precisa la ca√≠da de cualquiera de los dos sitios donde est√°n instalados los sistemas de almacenamiento.  Cualquier otra forma de colocar un √°rbitro puede resultar en un cerebro dividido. </p><br><h3 id="teper-pogruzimsya-v-detali-raboty-arbitra">  Ahora sum√©rgete en los detalles del √°rbitro </h3><br><p>  El √°rbitro ejecuta varios servicios que son interrogados constantemente por todos los controladores de almacenamiento.  Si el resultado de la encuesta difiere del anterior (disponible / inaccesible), se registra en una peque√±a base de datos, que tambi√©n funciona como √°rbitro. </p><br><p>  <strong>Considere la l√≥gica del √°rbitro con m√°s detalle.</strong> </p><br><p>  <u>Paso 1. Determinaci√≥n de la inaccesibilidad.</u>  Una se√±al de evento sobre la falla del sistema de almacenamiento es la ausencia de ping de ambos controladores del mismo sistema de almacenamiento durante 5 segundos. </p><br><p>  <u>Paso 2. Inicie el procedimiento de cambio.</u>  Despu√©s de que el √°rbitro comprende que uno de los sistemas de almacenamiento no est√° disponible, env√≠a una solicitud al sistema de almacenamiento "en vivo" para asegurarse de que el sistema de almacenamiento "muerto" realmente haya muerto. </p><br><p>  Despu√©s de recibir dicha orden del √°rbitro, el segundo sistema de almacenamiento (en vivo) verifica adicionalmente la disponibilidad del primer sistema de almacenamiento ca√≠do y, si no es as√≠, env√≠a al √°rbitro la confirmaci√≥n de sus conjeturas.  El almacenamiento realmente no est√° disponible. </p><br><p>  Despu√©s de recibir dicha confirmaci√≥n, el √°rbitro inicia el procedimiento remoto para cambiar la replicaci√≥n y elevar la asignaci√≥n en aquellas r√©plicas que estaban activas (primarias) en el sistema de almacenamiento descartado, y env√≠a un comando al segundo sistema de almacenamiento para que estas r√©plicas pasen de secundaria a primaria y aumenten la asignaci√≥n.  Bueno, el segundo sistema de almacenamiento, respectivamente, realiza estos procedimientos, despu√©s de lo cual proporciona acceso a los LUN perdidos de s√≠ mismo. </p><br><p>  ¬øPor qu√© necesito verificaci√≥n adicional?  Para qu√≥rum.  Es decir, la mayor√≠a del n√∫mero impar total (3) de miembros del cl√∫ster deber√≠a confirmar la ca√≠da de uno de los nodos del cl√∫ster.  Solo entonces esta decisi√≥n ser√° correcta.  Esto es necesario para evitar cambios err√≥neos y, en consecuencia, cerebro dividido. </p><br><p>  El paso 2 en el tiempo toma aproximadamente 5 a 10 segundos, por lo tanto, teniendo en cuenta el tiempo requerido para determinar la inaccesibilidad (5 segundos), dentro de los 10 a 15 segundos despu√©s del accidente, los LUN con almacenamiento ca√≠do estar√°n disponibles autom√°ticamente para trabajar con almacenamiento en vivo. </p><br><p>  Est√° claro que para evitar desconectar los hosts, tambi√©n debe cuidar la configuraci√≥n correcta de los tiempos de espera en los hosts.  El tiempo de espera recomendado es de al menos 30 segundos.  Esto no permitir√° que el host se desconecte del sistema de almacenamiento durante la transferencia de carga durante un accidente y podr√° garantizar que no haya interrupci√≥n de la entrada-salida. </p><br><blockquote>  Resulta que solo un segundo, si todo est√° bien con el cl√∫ster de metro, ¬øpor qu√© necesita una replicaci√≥n regular? </blockquote><p>  De hecho, no todo es tan simple. </p><br><h3 id="rassmotrim-plyusy-i-minusy-metroklastera">  Considere los pros y los contras del cl√∫ster de metro </h3><br><p>  Entonces, nos dimos cuenta de que las ventajas obvias del cl√∫ster metro en comparaci√≥n con la replicaci√≥n convencional son: </p><br><ul><li>  Automatizaci√≥n completa que proporciona un tiempo de recuperaci√≥n m√≠nimo en caso de desastre; </li><li>  Y eso es todo :-). </li></ul><br><p>  Y ahora, atenci√≥n, contras: </p><br><ul><li>  El costo de la decisi√≥n.  Aunque el cl√∫ster metropolitano en los sistemas Aerodisk no requiere licencias adicionales (se usa la misma licencia que para la r√©plica), el costo de la soluci√≥n seguir√° siendo mayor que el uso de la replicaci√≥n sincr√≥nica.  Ser√° necesario implementar todos los requisitos para la r√©plica s√≠ncrona, m√°s los requisitos para el cl√∫ster metropolitano relacionados con la conmutaci√≥n adicional y el sitio adicional (consulte la planificaci√≥n del cl√∫ster metropolitano); </li><li>  La complejidad de la decisi√≥n.  El cl√∫ster metropolitano es mucho m√°s complejo que una r√©plica normal y requiere mucha m√°s atenci√≥n y trabajo para la planificaci√≥n, configuraci√≥n y documentaci√≥n. </li></ul><br><p>  Al final  <strong>Metro cluster es, por supuesto, una soluci√≥n muy tecnol√≥gica y buena cuando realmente necesita proporcionar RTO en segundos o minutos.</strong>  Pero si no existe tal tarea, y RTO en horas est√° bien para el negocio, entonces no tiene sentido disparar gorriones desde el ca√±√≥n.  La replicaci√≥n habitual de los trabajadores y campesinos es suficiente, porque el cl√∫ster de metro incurrir√° en costos adicionales y complicar√° la infraestructura de TI. </p><br><h2 id="planirovanie-metroklastera">  Planificaci√≥n de cl√∫ster de metro </h2><br><p>  Esta secci√≥n no pretende ser una gu√≠a completa para el dise√±o del cl√∫ster metropolitano, sino que solo muestra las instrucciones principales que deben elaborarse si decide construir dicho sistema.  Por lo tanto, con la implementaci√≥n real del cl√∫ster metro, aseg√∫rese de involucrar al fabricante de los sistemas de almacenamiento (es decir, nosotros) y otros sistemas relacionados para consultas. </p><br><h3 id="ploschadki">  Plataformas </h3><br><p>  Como se indic√≥ anteriormente, se requieren un m√≠nimo de tres sitios para un cl√∫ster de metro.  Dos centros de datos, donde funcionar√°n los sistemas de almacenamiento y sistemas relacionados, as√≠ como una tercera plataforma donde trabajar√° el √°rbitro. </p><br><p>  La distancia recomendada entre los centros de datos no es m√°s de 40 kil√≥metros.  Es muy probable que distancias m√°s grandes causen demoras adicionales, que en el caso de un grupo de metro son altamente indeseables.  Recuerde, los retrasos deben ser de hasta 5 milisegundos, aunque es deseable cumplir con 2. </p><br><p>  Tambi√©n se recomienda verificar las demoras durante el proceso de planificaci√≥n.  Cualquier proveedor m√°s o menos adulto que proporciona fibra entre los centros de datos, un control de calidad puede organizarse con bastante rapidez. </p><br><p>  En cuanto a las demoras ante el √°rbitro (es decir, entre la tercera plataforma y las dos primeras), el umbral de demora recomendado es de hasta 200 milisegundos, es decir, es adecuada una conexi√≥n VPN corporativa regular a trav√©s de Internet. </p><br><h3 id="kommutaciya-i-set">  Conmutaci√≥n y redes </h3><br><p>  A diferencia de un esquema de replicaci√≥n, donde es suficiente para interconectar sistemas de almacenamiento de diferentes sitios, un esquema con un cl√∫ster metropolitano requiere conectar hosts con ambos sistemas de almacenamiento en diferentes sitios.  Para aclarar cu√°l es la diferencia, ambos esquemas se enumeran a continuaci√≥n. </p><br><p><img src="https://habrastorage.org/webt/qq/tg/x-/qqtgx-ze1zjj9fhbb7drzz6zabw.png"></p><br><p><img src="https://habrastorage.org/webt/l3/vs/es/l3vsesenlgm7lalbws0gdqhysuy.png"></p><br><p>  Como puede ver en el diagrama, los hosts en el sitio 1 est√°n mirando tanto SHD1 como SHD 2. Adem√°s, por el contrario, los hosts de la plataforma 2 est√°n mirando SHD 2 y SHD1.  Es decir, cada host ve ambos sistemas de almacenamiento.  Este es un requisito previo para el funcionamiento del cl√∫ster de metro. </p><br><p>  Por supuesto, no es necesario tirar de cada host con un cable √≥ptico a un centro de datos diferente, no habr√° suficientes puertos y cordones.  Todas estas conexiones deben realizarse a trav√©s de conmutadores Ethernet 10G + o FibreChannel 8G + (FC solo para conectar hosts y almacenamiento para IO, el canal de replicaci√≥n actualmente solo est√° disponible a trav√©s de IP (Ethernet 10G +). </p><br><p>  Ahora algunas palabras sobre la topolog√≠a de la red.  Un punto importante es la configuraci√≥n correcta de las subredes.  Debe identificar inmediatamente varias subredes para los siguientes tipos de tr√°fico: </p><br><ul><li>  La subred para la replicaci√≥n sobre la cual se sincronizar√°n los datos entre los sistemas de almacenamiento.  Puede haber varios, en este caso no importa, todo depende de la topolog√≠a de red actual (ya implementada).  Si hay dos de ellos, entonces obviamente el enrutamiento entre ellos debe configurarse; </li><li>  Subredes de almacenamiento a trav√©s de las cuales los hosts acceder√°n a los recursos de almacenamiento (si es iSCSI).  Debe haber una de esas subredes en cada centro de datos; </li><li>  Controle las subredes, es decir, tres subredes enrutables en tres sitios desde los que se realiza la gesti√≥n de almacenamiento, y tambi√©n hay un √°rbitro. </li></ul><br><p>  No consideramos subredes para acceder a los recursos del host aqu√≠, ya que dependen mucho de las tareas. </p><br><p>  La separaci√≥n de tr√°fico diferente en subredes diferentes es extremadamente importante (es especialmente importante separar la r√©plica de E / S), porque si mezcla todo el tr√°fico en una subred "gruesa", este tr√°fico ser√° imposible de administrar, y en las condiciones de dos centros de datos a√∫n puede causar diferentes opciones de colisi√≥n de red.  No profundizaremos mucho en este tema en el marco de este art√≠culo, ya que puede leer sobre la planificaci√≥n de una red extendida entre centros de datos sobre los recursos de los fabricantes de equipos de red, donde se describe con gran detalle. </p><br><h3 id="konfiguraciya-arbitra">  Configuraci√≥n de √°rbitro </h3><br><p>  El √°rbitro debe proporcionar acceso a todas las interfaces de administraci√≥n de almacenamiento a trav√©s de los protocolos ICMP y SSH.  Tambi√©n debe considerar la tolerancia a fallas del √°rbitro.  Hay un matiz. </p><br><p>  La tolerancia a fallos del √°rbitro es muy deseable, pero opcional.  ¬øY qu√© pasa si el √°rbitro se estrella en el momento equivocado? </p><br><ul><li>  El funcionamiento del cl√∫ster metro en modo normal no cambiar√° porque  arbtir no afecta el funcionamiento del cl√∫ster metropolitano en modo normal de ninguna manera (su tarea es cambiar oportunamente la carga entre los centros de datos) </li><li>  Adem√°s, si el √°rbitro por una raz√≥n u otra cae y "despierta" el accidente en el centro de datos, entonces no ocurrir√° ning√∫n cambio, porque no habr√° nadie para dar los comandos necesarios para cambiar y organizar un qu√≥rum.  En este caso, el cl√∫ster metropolitano se convertir√° en un esquema de replicaci√≥n regular, que deber√° cambiarse manualmente durante un desastre, que afectar√° a RTO. </li></ul><br><p>  ¬øQu√© se sigue de esto?  Si realmente necesita garantizar un RTO m√≠nimo, debe garantizar la tolerancia a fallos del √°rbitro.  Hay dos opciones para esto: </p><br><ul><li>  Ejecute una m√°quina virtual con un √°rbitro en un hipervisor de conmutaci√≥n por error, ya que todos los hipervisores adultos admiten la conmutaci√≥n por error; </li><li>  Si est√° en el tercer sitio (en una oficina condicional) <del>  pereza para poner un cl√∫ster normal </del>  Dado que no existe un cl√∫ster de hipervizor existente, hemos proporcionado una versi√≥n de hardware del √°rbitro, que se realiza en una caja de 2U, en la que funcionan dos servidores x-86 ordinarios y que pueden sobrevivir a una falla local. </li></ul><br><p>  Recomendamos encarecidamente que el √°rbitro sea tolerante a fallas a pesar de que el cl√∫ster metropolitano no lo necesita en modo normal.  Pero tanto la teor√≠a como la pr√°ctica muestran que si construyes una infraestructura verdaderamente confiable a prueba de desastres, entonces es mejor ir a lo seguro.  Es mejor protegerse y proteger a su empresa de la "ley de la mezquindad", es decir, de la falla tanto del √°rbitro como de uno de los sitios donde se encuentra el sistema de almacenamiento. </p><br><h3 id="arhitektura-resheniya">  Arquitectura de soluciones </h3><br><p>  Teniendo en cuenta los requisitos anteriores, obtenemos la siguiente arquitectura de soluci√≥n general. </p><br><p><img src="https://habrastorage.org/webt/di/wt/oq/diwtoqu2jdf7ik-nsigr4ypx37s.png"></p><br><p>  Los LUN deben distribuirse uniformemente en dos sitios para evitar una congesti√≥n intensa.  Al mismo tiempo, al dimensionar en ambos centros de datos, es necesario colocar no solo un doble volumen (que es necesario para almacenar datos simult√°neamente en dos sistemas de almacenamiento), sino tambi√©n un doble rendimiento en IOPS y MB / s, para evitar la degradaci√≥n de las aplicaciones en caso de falla de uno de los centros de datos: ov. </p><br><p>  Por separado, observamos que con un enfoque adecuado para el dimensionamiento (es decir, siempre que hayamos proporcionado los l√≠mites superiores adecuados para IOPS y MB / s, as√≠ como los recursos necesarios de CPU y RAM), si uno de los sistemas de almacenamiento falla en el cl√∫ster metropolitano, no habr√° una reducci√≥n seria en el rendimiento bajo trabajo temporal en un sistema de almacenamiento. </p><br><p>  Esto se debe al hecho de que, en las condiciones de dos sitios simult√°neamente, el trabajo de replicaci√≥n sincr√≥nica "consume" la mitad del rendimiento de escritura, ya que cada transacci√≥n debe escribirse en dos sistemas de almacenamiento (similar a RAID-1/10).  Entonces, si uno de los sistemas de almacenamiento falla, el efecto de la replicaci√≥n temporalmente (hasta que el sistema de almacenamiento con fallas aumente) desaparece, y obtenemos un aumento doble en el rendimiento de escritura.  Despu√©s de que los LUN del sistema de almacenamiento fallido se reiniciaron en un sistema de almacenamiento en funcionamiento, este aumento doble desaparece debido a la carga de los LUN de otro sistema de almacenamiento, y volvemos al mismo nivel de rendimiento que ten√≠amos antes de la "ca√≠da", pero solo dentro del marco de una plataforma. </p><br><p>  Con la ayuda de un tama√±o competente, es posible proporcionar condiciones bajo las cuales los usuarios no sentir√°n la falla de un sistema de almacenamiento completo.  Pero, de nuevo, esto requiere un tama√±o muy cuidadoso, por lo que, por cierto, puede contactarnos gratis :-). </p><br><h2 id="nastroyka-metroklastera">  Configuraci√≥n de Metro Cluster </h2><br><p>  Configurar un cl√∫ster de metro es muy similar a configurar una replicaci√≥n regular, que describimos en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior</a> .  Por lo tanto, nos enfocamos solo en las diferencias.  Configuramos un soporte basado en la arquitectura anterior en el laboratorio solo en la versi√≥n m√≠nima: dos sistemas de almacenamiento conectados por Ethernet 10G entre s√≠, dos conmutadores 10G y un host que observa los 10 puertos de almacenamiento a trav√©s de los conmutadores en ambos sistemas de almacenamiento.  El √°rbitro se ejecuta en una m√°quina virtual. </p><br><p><img src="https://habrastorage.org/webt/gy/-v/ci/gy-vci08eyyimjl6lwa2mciot8k.png"></p><br><p>  Al configurar IP virtuales (VIP) para una r√©plica, seleccione el tipo de VIP para el cluster de metro. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ym/aw/89/ymaw89gofhplngagm5nvveg4oe0.png"></a> </p><br><p>  Creamos dos enlaces de replicaci√≥n para dos LUN y los distribuimos en dos sistemas de almacenamiento: LUN TEST Primary para SHD1 (enlace METRO), LUN TEST2 Primary para SHD2 (enlace METRO2). </p><br><p><img src="https://habrastorage.org/webt/rz/7w/6e/rz7w6esdq4bued37xobjg1obcha.jpeg"></p><br><p>  Para ellos, configuramos dos objetivos id√©nticos (en nuestro caso iSCSI, pero FC tambi√©n es compatible, la l√≥gica de configuraci√≥n es la misma). </p><br><p>  SHD1: </p><br><p><img src="https://habrastorage.org/webt/d8/r-/dw/d8r-dw3gglptcowecdjabpewdve.jpeg"></p><br><p>  SHD2: </p><br><p><img src="https://habrastorage.org/webt/lf/ys/_6/lfys_6aohva42tp89ddqcjhakes.jpeg"></p><br><p>  Para las conexiones de replicaci√≥n, hicieron asignaciones en cada sistema de almacenamiento. </p><br><p>  SHD1: </p><br><p><img src="https://habrastorage.org/webt/fw/qe/5q/fwqe5qqhcir_4rlbj3usmea1hgg.jpeg"></p><br><p>  SHD2: </p><br><p><img src="https://habrastorage.org/webt/rn/68/ap/rn68apxwkixmyh3dpv70vo-9pk8.jpeg"></p><br><p>  Multipath configurado y presentado al host. </p><br><p><img src="https://habrastorage.org/webt/gx/ii/wj/gxiiwjndb_fzeywc5hrxdfbqnrk.jpeg"></p><br><p><img src="https://habrastorage.org/webt/c6/df/5j/c6df5jjxnggdqk8wnjfbebb1rty.jpeg"></p><br><h3 id="nastraivaem-arbitra">  Configurar el √°rbitro </h3><br><p>  No necesita hacer nada especial con el √°rbitro, solo debe habilitarlo en la tercera plataforma, configurar una IP y configurar el acceso a ella a trav√©s de ICMP y SSH.  La configuraci√≥n en s√≠ se realiza desde los propios sistemas de almacenamiento.  En este caso, es suficiente configurar el √°rbitro una vez en cualquiera de los controladores de almacenamiento en el cl√∫ster metro, esta configuraci√≥n se distribuir√° a todos los controladores autom√°ticamente. </p><br><p>  En la secci√≥n Replicaci√≥n remota &gt;&gt; Metrocluster (en cualquier controlador) &gt;&gt; bot√≥n Configurar. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/sy/h0/x-/syh0x-gywrz2yu8w9mk0us5mwcg.jpeg"></a> </p><br><p>  Introducimos la IP del √°rbitro, as√≠ como las interfaces de control de los dos controladores del sistema de almacenamiento remoto. </p><br><p><img src="https://habrastorage.org/webt/rs/u2/dt/rsu2dt3tt18k1it_qvmdsdnxldy.jpeg"></p><br><p>  Despu√©s de eso, debe habilitar todos los servicios (el bot√≥n "Reiniciar todo").  En el caso de una reconfiguraci√≥n en el futuro, los servicios deben reiniciarse para que la configuraci√≥n surta efecto. </p><br><p><img src="https://habrastorage.org/webt/kj/sb/7u/kjsb7u9rrzlk6cttl7amr_ebxma.jpeg"></p><br><p>  Verifique que todos los servicios se est√©n ejecutando. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/oh/mw/xl/ohmwxl5jry0dufgfamey0ddxris.jpeg"></a> </p><br><p>  <strong>Esto completa la configuraci√≥n del cl√∫ster de metro.</strong> </p><br><h2 id="krash-test">  Prueba de choque </h2><br><p>  La prueba de choque en nuestro caso ser√° bastante simple y r√°pida, ya que la funcionalidad de replicaci√≥n (cambio, consistencia, etc.) se consider√≥ en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior</a> .  Por lo tanto, para probar la confiabilidad de un cl√∫ster de metro, es suficiente para nosotros verificar la automatizaci√≥n de la detecci√≥n de accidentes, la conmutaci√≥n y la ausencia de p√©rdidas de registro (paradas de E / S). </p><br><p>  Para hacer esto, emulamos la falla completa de uno de los sistemas de almacenamiento apagando f√≠sicamente sus dos controladores, comenzando una copia preliminar de un archivo grande en el LUN, que debe activarse en el otro sistema de almacenamiento. </p><br><p><img src="https://habrastorage.org/webt/f-/lb/fo/f-lbfonzy8n4iawaihumetqtkmo.png"></p><br><p>  Deshabilitar un almacenamiento.  En el segundo sistema de almacenamiento, vemos alertas y mensajes en los registros de que la conexi√≥n con el sistema vecino ha desaparecido.  Si ha configurado alertas para la supervisi√≥n SMTP o SNMP, el administrador recibir√° las alertas correspondientes. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/jo/xv/aejoxv6piwk4fvl-nc1leklbxce.jpeg"></a> </p><br><p>  Exactamente 10 segundos despu√©s (visto en ambas capturas de pantalla), el enlace de replicaci√≥n METRO (el que era Primario en el sistema de almacenamiento bloqueado) se convirti√≥ autom√°ticamente en Primario en el sistema de almacenamiento en ejecuci√≥n.  Usando el mapeo existente, LUN TEST permaneci√≥ disponible para el host, la grabaci√≥n se hundi√≥ un poco (dentro del 10 por ciento prometido), pero no se interrumpi√≥. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/v_/gj/d9/v_gjd9sgoi5hwrgjdkwxwtdomeu.jpeg"></a> </p><br><p><img src="https://habrastorage.org/webt/ih/lq/rg/ihlqrgv7dq-4fv6dlnhair6nib4.jpeg"></p><br><p>  <strong>Prueba completada con √©xito.</strong> </p><br><h2 id="podvodim-itog">  Para resumir </h2><br><p>  La implementaci√≥n actual del metrocluster en los sistemas de almacenamiento de la serie N del motor AERODISK permite resolver por completo los problemas en los que es necesario eliminar o minimizar el tiempo de inactividad de los servicios de TI y garantizar su funcionamiento 24/7/365 con una mano de obra m√≠nima. </p><br><p>  , ,    ,      ‚Ä¶       ,      ,    .      ,         ,        ,         . </p><br><p> ,   . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/460305/">https://habr.com/ru/post/460305/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../460287/index.html">Visualizaci√≥n de noticias runet</a></li>
<li><a href="../460291/index.html">Problemas de procesamiento por lotes de solicitudes y sus soluciones (parte 1)</a></li>
<li><a href="../460295/index.html">¬øQu√© significa inseguro en Rust?</a></li>
<li><a href="../460297/index.html">WeakRef - propuesta para agregar al est√°ndar ECMAScript</a></li>
<li><a href="../460301/index.html">L√°mparas LED de alta potencia de nueva generaci√≥n.</a></li>
<li><a href="../460307/index.html">Experiencia de modelado del equipo de Computer Vision Mail.ru</a></li>
<li><a href="../460311/index.html">Hora de una nueva teor√≠a del dinero.</a></li>
<li><a href="../460313/index.html">¬øLas diferentes canciones de √©xito tienen algo en com√∫n?</a></li>
<li><a href="../460319/index.html">Caza de inspectores espaciales</a></li>
<li><a href="../460321/index.html">Galer√≠a de los mejores cuadernos ML y Data Science</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>