<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçÅ üå•Ô∏è üèóÔ∏è Agentes de aprendizado de m√°quina na Unity üòö üòπ üåí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este artigo sobre agentes de aprendizado de m√°quina do Unity foi escrito por Michael Lanham, um inovador t√©cnico, desenvolvedor ativo do Unity, consul...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Agentes de aprendizado de m√°quina na Unity</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454612/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/6fe/055/9aa6fe055c20cde3642bb0f0782f62d3.jpg" alt="imagem"></div><br>  <em>Este artigo sobre agentes de aprendizado de m√°quina do Unity foi escrito por Michael Lanham, um inovador t√©cnico, desenvolvedor ativo do Unity, consultor, gerente e autor de muitos jogos, projetos gr√°ficos e livros do Unity.</em> <br><br>  Os desenvolvedores do Unity implementaram o suporte ao aprendizado de m√°quina e, em particular, ao aprendizado por refor√ßo para a cria√ß√£o de SDKs de DDR (Deep Enhancement Learning) para desenvolvedores de jogos e simula√ß√£o.  Felizmente, a equipe da Unity, liderada por Danny Lange, implementou com sucesso um mecanismo DRL confi√°vel e moderno capaz de fornecer resultados impressionantes.  O Unity usa o modelo de otimiza√ß√£o de pol√≠tica proximal (PPO) como base do mecanismo DRL;  esse modelo √© muito mais complexo e pode diferir em alguns aspectos. <br><br>  Neste artigo, apresentarei as ferramentas e SDKs para a cria√ß√£o de agentes de DRL em jogos e simula√ß√µes.  Apesar da novidade e do poder dessa ferramenta, √© f√°cil de usar e possui ferramentas auxiliares que permitem aprender conceitos de aprendizado de m√°quina em movimento.  Para trabalhar com o tutorial, voc√™ precisa instalar o mecanismo do Unity. <br><a name="habracut"></a><br><h2>  Instalar agentes ML </h2><br>  Nesta se√ß√£o, falarei brevemente sobre as etapas que devem ser tomadas para instalar o SDK do ML-Agents.  Este material ainda est√° na vers√£o beta e pode variar de vers√£o para vers√£o.  Siga estes passos: <br><br><ol><li>  Instale o Git no computador;  Funciona a partir da linha de comando.  O Git √© um sistema de gerenciamento de c√≥digo-fonte muito popular e h√° muitos recursos na Internet sobre a instala√ß√£o e o uso do Git em v√°rias plataformas.  Ap√≥s instalar o Git, verifique se ele funciona criando um clone de qualquer reposit√≥rio. </li><li>  Abra um prompt de comando ou shell regular.  Os usu√°rios do Windows podem abrir a janela do Anaconda. </li><li>  V√° para a pasta de trabalho onde deseja colocar seu novo c√≥digo e digite o seguinte comando (os usu√°rios do Windows podem selecionar C: \ ML-Agents): <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents </pre></li><li>  Ent√£o voc√™ clona o reposit√≥rio ml-agents no seu computador e cria uma nova pasta com o mesmo nome.  Voc√™ tamb√©m pode adicionar um n√∫mero de vers√£o ao nome da pasta.  A unidade, como quase todo o mundo da intelig√™ncia artificial, est√° mudando constantemente, pelo menos por enquanto.  Isso significa que novas mudan√ßas est√£o aparecendo constantemente.  No momento da reda√ß√£o, estamos clonando o reposit√≥rio na pasta ml-agents.6: <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents ml-agents.6 </pre></li><li>  Crie um novo ambiente virtual para ml-agents e especifique a vers√£o 3.6, assim: <br><br><pre>  #Windows 
 conda create -n ml-agents python = 3.6
 
 #Mac
 Use a documenta√ß√£o para o seu ambiente preferido </pre></li><li>  Ative seu ambiente novamente com o Anaconda: <br><br><pre>  ativar agentes ml </pre></li><li>  Instale o TensorFlow.  No Anaconda, isso pode ser feito com o seguinte comando: <br><br><pre>  pip install tensorflow == 1.7.1 </pre></li><li>  Instale pacotes Python.  No Anaconda, digite o seguinte: <br><br><pre><code class="plaintext hljs">cd ML-Agents #from root folder cd ml-agents or cd ml-agents.6 #for example cd ml-agents pip install -e . or pip3 install -e .</code> </pre> </li><li>  Ent√£o voc√™ instala todos os pacotes necess√°rios do SDK do Agents;  isso pode levar alguns minutos.  N√£o feche a janela, ela ser√° √∫til em breve. </li></ol><br>  Por isso, instalamos e configuramos o Unity Python SDK para agentes ML.  Na pr√≥xima se√ß√£o, aprenderemos como configurar e treinar um dos muitos ambientes fornecidos pelo Unity. <br><br><h2>  Treinamento de agentes </h2><br>  Agora podemos come√ßar a trabalhar imediatamente e explorar exemplos que usam o DRL (Deep Refor√ßo Learning).  Felizmente, existem v√°rios exemplos no kit de ferramentas do novo agente para demonstrar a pot√™ncia do mecanismo.  Abra o Unity ou o Unity Hub e siga estas etapas: <br><br><ol><li>  Clique no bot√£o Abrir projeto na parte superior da caixa de di√°logo Projeto. </li><li>  Localize e abra a pasta do projeto UnitySDK, conforme mostrado na captura de tela: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/371/706/016/3717060168f78fccd271c064f0e055ce.png"></div><br>  <i>Abra o projeto do SDK do Unity</i> </li><li>  Aguarde o carregamento do projeto e abra a janela Projeto na parte inferior do editor.  Se uma janela abrir solicitando a atualiza√ß√£o do projeto, selecione sim ou continue.  Atualmente, todo o c√≥digo do agente √© compat√≠vel com vers√µes anteriores. </li><li>  Localize e abra a cena do GridWorld, conforme mostrado na captura de tela: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0aa/6bd/ab5/0aa6bdab5b8d07cdf414e59255862c05.png"></div><br>  <em>Abrindo um exemplo de uma cena do GridWorld</em> </li><li>  Selecione o objeto GridAcademy na janela Hierarquia. </li><li>  V√° para a janela Inspetor e, ao lado do campo C√©rebros, clique no √≠cone para abrir a caixa de di√°logo Sele√ß√£o do c√©rebro: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08c/cda/126/08ccda1263b74d131c70cff42edae92c.png"></div></li><li>  Selecione o c√©rebro do GridWorldPlayer.  Esse c√©rebro pertence ao jogador, ou seja, o jogador (voc√™) pode controlar o jogo. </li><li>  Clique no bot√£o Reproduzir na parte superior do editor e observe o ambiente.  Como o jogo est√° agora configurado para controlar o jogador, voc√™ pode usar as teclas WASD para mover o cubo.  A tarefa √© mover o cubo azul para o s√≠mbolo verde +, evitando o X vermelho. </li></ol><br>  Sinta-se confort√°vel no jogo.  Observe que o jogo s√≥ funciona por um determinado per√≠odo de tempo e n√£o √© baseado em turnos.  Na pr√≥xima se√ß√£o, aprenderemos como executar este exemplo com o agente DRL. <br><br><h2>  O que h√° no c√©rebro? </h2><br>  Um dos aspectos surpreendentes da plataforma ML-Agents √© a capacidade de mudar r√°pida e facilmente do gerenciamento de jogadores para o gerenciamento de IA / agente.  Para isso, o Unity usa o conceito de "c√©rebro".  O c√©rebro pode ser controlado pelo jogador ou pelo agente (c√©rebro de aprendizado).  O mais surpreendente √© que voc√™ pode montar o jogo e test√°-lo como jogador, e depois entreg√°-lo sob o controle de um agente da RL.  Gra√ßas a isso, qualquer jogo escrito com um pouco de esfor√ßo pode ser feito para ser controlado usando a IA. <br><br>  O processo de configurar e iniciar o treinamento do agente RL no Unity √© bastante simples.  O Unity usa Python externo para construir um modelo do c√©rebro que aprende.  O uso do Python faz muito sentido, porque j√° existem v√°rias bibliotecas de aprendizado profundo (DL) criadas em torno dele.  Para treinar o agente no GridWorld, execute as seguintes etapas: <br><br><ol><li>  Selecione GridAcademy novamente e selecione o c√©rebro do GridWorldLearning no campo Brains em vez de GridWorldPlayer: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/869/fc8/b42/869fc8b42b64d8b3ccc785eb9a6e765e.png"></div><br>  <em>Mudando para o uso do GridWorldLearning Brain</em> </li><li>  Marque a caixa de controle √† direita.  Este par√¢metro simples relata que o c√©rebro pode ser controlado externamente.  Esta op√ß√£o deve estar ativada. </li><li>  Selecione o objeto trueAgent na janela Hierarquia e, na janela Inspetor, altere a propriedade Brain no componente Grid Agent para o c√©rebro GridWorldLearning: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/ff3/ed2/832ff3ed2bc17c16cc5ed823d10a7156.png"></div><br>  <em>GridWorldLearning trabalho cerebral para agente</em> </li><li>  Neste exemplo, precisamos que a Academy e o Agent usem o mesmo c√©rebro do GridWorldLearning.  Alterne para a janela Anaconda ou Python e selecione a pasta ML-Agents / ml-agents. </li><li>  Execute o seguinte comando em uma janela Anaconda ou Python usando o ambiente virtual ml-agents: <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = firstRun --train </pre></li><li>  Isso iniciar√° o modelo de treinamento do Unity PPO e um agente de exemplo com a configura√ß√£o especificada.  Em um determinado momento, a janela do prompt de comando solicitar√° que voc√™ inicie o editor do Unity com o ambiente carregado. </li><li>  Clique em Reproduzir no editor do Unity para iniciar o ambiente GridWorld.  Logo depois, voc√™ ver√° o treinamento do agente e a sa√≠da para a janela de script Python: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/583/cc2/fd9/583cc2fd94c49e4a1039d9af64312f68.png"></div><br>  <em>Executando o GridWorld no Modo de Aprendizagem</em> </li><li>  Observe que o script mlagents-learn √© um c√≥digo Python que cria um modelo RL para executar um agente.  Como voc√™ pode ver na sa√≠da do script, existem v√°rios par√¢metros (hiperpar√¢metros) que precisam ser configurados. </li><li>  Deixe o agente aprender alguns milhares de itera√ß√µes e observe a rapidez com que aprende.  O modelo interno usado aqui chamado PPO provou ser um modelo de aprendizado muito eficaz para muitas tarefas diferentes e √© muito adequado para o desenvolvimento de jogos.  Com equipamentos suficientemente potentes, um agente pode aprender idealmente em menos de uma hora. </li></ol><br>  Deixe o agente aprender mais e explorar outras maneiras de acompanhar o processo de aprendizado do agente, conforme apresentado na pr√≥xima se√ß√£o. <br><br><h2>  Monitorando o aprendizado com o TensorBoard </h2><br>  Treinar um agente usando o modelo RL ou qualquer modelo de DL geralmente √© uma tarefa assustadora e requer aten√ß√£o aos detalhes.  Felizmente, o TensorFlow possui um conjunto de ferramentas de gr√°ficos chamado TensorBoard que voc√™ pode usar para monitorar seu processo de aprendizado.  Siga estas etapas para iniciar o TensorBoard: <br><br><ol><li>  Abra uma janela Anaconda ou Python.  Ative o ambiente virtual ml-agents.  N√£o feche a janela na qual o modelo de treinamento est√° sendo executado;  precisamos que continue. </li><li>  V√° para a pasta ML-Agents / ml-agents e execute o seguinte comando: <br><br><pre>  tensorboard --logdir = resumos </pre></li><li>  Ent√£o, lan√ßamos o TensorBoard em nosso pr√≥prio servidor da web incorporado.  Voc√™ pode carregar a p√°gina usando o URL mostrado ap√≥s o comando anterior. </li><li>  Digite o URL para o TensorBoard conforme mostrado na janela ou digite localhost: 6006 ou nome da m√°quina: 6006 no navegador.  Ap√≥s cerca de uma hora, voc√™ ver√° algo parecido com isto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/619/b62/964/619b62964a88d9ee4b7635073799caf8.png"></div><br>  <em>Janela Gr√°fico do TensorBoard</em> </li><li>  A captura de tela anterior mostra gr√°ficos, cada um dos quais exibe um aspecto separado do treinamento.  Para entender como nosso agente √© treinado, voc√™ precisa lidar com cada um desses gr√°ficos; portanto, analisaremos a sa√≠da de cada se√ß√£o: </li></ol><br><ul><li>  Ambiente: esta se√ß√£o mostra como o agente se manifesta no ambiente como um todo.  Abaixo est√° uma vis√£o mais detalhada dos gr√°ficos com a tend√™ncia preferida: </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/596/956/bc15969569f3d959bc550c2ee629ac3d.png"></div><br>  <em>Uma imagem detalhada dos gr√°ficos da se√ß√£o Ambiente</em> <br><br><ul><li>  Recompensa cumulativa: Essa √© a recompensa total que maximiza o agente.  Geralmente √© necess√°rio que aumente, mas por algum motivo pode diminuir.  √â sempre melhor maximizar as recompensas entre 1 e -1.  Se as recompensas do cronograma ultrapassarem esse intervalo, isso tamb√©m precisar√° ser corrigido. </li><li>  Dura√ß√£o do epis√≥dio: se esse valor diminui, geralmente √© um bom sinal.  Por fim, quanto mais curtos os epis√≥dios, mais treinamento.  No entanto, lembre-se de que, se necess√°rio, a dura√ß√£o dos epis√≥dios pode aumentar, portanto a imagem pode ser diferente. </li><li>  Li√ß√£o: esse gr√°fico deixa claro em qual li√ß√£o o agente est√°;  Destina-se √† aprendizagem curricular. </li><li>  Perdas: Esta se√ß√£o mostra gr√°ficos que representam as perdas ou custos calculados para a pol√≠tica e o valor.  Abaixo est√° uma captura de tela desta se√ß√£o com setas apontando para as configura√ß√µes ideais: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aaa/8c5/3f6/aaa8c53f67533a8d349b62d216c15a1b.png"></div><br>  <em>Perdas e treinamento preferencial</em> </li></ul><br><ul><li>  Perda de pol√≠tica: Este gr√°fico determina a quantidade de altera√ß√£o de pol√≠tica ao longo do tempo.  A pol√≠tica √© um elemento que define a√ß√µes e, no caso geral, esse cronograma deve tender para baixo, mostrando que a pol√≠tica est√° tomando melhores decis√µes. </li><li>  Perda de valor: √© a perda m√©dia da fun√ß√£o de valor.  Em ess√™ncia, ele modela o qu√£o bem o agente prev√™ o valor do seu pr√≥ximo estado.  Inicialmente, esse valor deve aumentar e, ap√≥s a estabiliza√ß√£o da remunera√ß√£o, deve diminuir. </li><li>  Pol√≠tica: para avaliar a qualidade das a√ß√µes no OPP, √© utilizado o conceito de pol√≠tica, n√£o o modelo.  A captura de tela abaixo mostra os gr√°ficos de pol√≠ticas e a tend√™ncia preferida: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/0ce/2c0/8450ce2c0e0a95bb39e92fe698dbee7c.png"></div><br>  <i>Gr√°ficos de pol√≠ticas e tend√™ncias preferidas</i> </li><li>  Entropia: Este gr√°fico mostra a magnitude do agente de pesquisa.  Esse valor precisa ser reduzido, porque o agente aprende mais sobre o meio ambiente e precisa de menos pesquisas. </li><li>  Taxa de aprendizado: nesse caso, esse valor deve diminuir gradualmente linearmente. </li><li>  Estimativa do valor: este √© o valor m√©dio visitado por todos os estados do agente.  Para refletir o aumento do conhecimento de um agente, esse valor deve crescer e depois se estabilizar. </li></ul><br>  6. Deixe o agente em execu√ß√£o at√© concluir e n√£o feche o TensorBoard. <br>  7. Volte √† janela Anaconda / Python que treinou o c√©rebro e execute este comando: <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = secondRun --train </pre><br>  8. Voc√™ ser√° solicitado novamente a clicar em Reproduzir no editor;  ent√£o fa√ßa isso.  Deixe o agente come√ßar o treinamento e realizar v√°rias sess√µes.  No processo, observe a janela TensorBoard e observe como o secondRun √© exibido nos gr√°ficos.  Voc√™ pode deixar esse agente funcionar at√© a conclus√£o, mas pode interromp√™-lo, se desejar. <br><br>  Nas vers√µes anteriores do ML-Agents, voc√™ precisava primeiro criar o execut√°vel do Unity como um ambiente de aprendizado para o jogo e depois execut√°-lo.  O c√©rebro externo de Python deveria ter funcionado da mesma maneira.  Esse m√©todo dificultava a depura√ß√£o de problemas no c√≥digo ou no jogo.  Na nova t√©cnica, todas essas dificuldades foram eliminadas. <br><br>  Agora que vimos como √© f√°cil configurar e treinar um agente, vamos para a pr√≥xima se√ß√£o, na qual aprendemos como executar um agente sem o c√©rebro externo do Python e execut√°-lo diretamente no Unity. <br><br><h2>  Lan√ßamento do agente </h2><br>  O treinamento em Python √© √≥timo, mas voc√™ n√£o pode us√°-lo em um jogo real.  Idealmente, gostar√≠amos de criar um gr√°fico TensorFlow e us√°-lo no Unity.  Felizmente, foi criada a biblioteca TensorFlowSharp que permite ao .NET usar gr√°ficos TensorFlow.  Isso nos permite criar modelos TFModels offline e injet√°-los posteriormente no jogo.  Infelizmente, s√≥ podemos usar modelos treinados, mas n√£o trein√°-los dessa maneira, pelo menos ainda n√£o. <br><br>  Vamos ver como isso funciona, usando o exemplo do gr√°fico que acabamos de treinar para o ambiente GridWorld;  use-o como um c√©rebro interno no Unity.  Siga as etapas na se√ß√£o a seguir para configurar e usar seu c√©rebro interno: <br><br><ol><li>  Fa√ßa o download do plugin TFSharp <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="external nofollow">aqui</a> </li><li>  No menu do editor, selecione Ativos |  Pacote de importa√ß√£o |  Pacote personalizado ... </li><li>  Encontre o pacote de ativos que voc√™ acabou de baixar e use as caixas de di√°logo de importa√ß√£o para carregar o plug-in no projeto. </li><li>  No menu, selecione Editar |  Configura√ß√µes do projeto.  A janela Configura√ß√µes √© aberta (apareceu na vers√£o 2018.3) </li><li>  Encontre os caracteres de Definir s√≠mbolos de script nas op√ß√µes do Player e altere o texto para ENABLE_TENSORFLOW, al√©m de ativar o c√≥digo n√£o seguro de permiss√£o, conforme mostrado na captura de tela: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/ecf/c31/79decfc310a7f6cf38b33d1998c14c1f.png"></div><br>  <em>Definindo o sinalizador ENABLE_TENSORFLOW</em> </li><li>  Encontre o objeto GridWorldAcademy na janela Hierarquia e verifique se ele usa Brains |  GridWorldLearning.  Desative a op√ß√£o Controle na se√ß√£o Brains do script Grid Academy. </li><li>  Encontre o c√©rebro do GridWorldLearning na pasta Assets / Examples / GridWorld / Brains e verifique se o par√¢metro Model na janela Inspector est√° definido, conforme mostrado na captura de tela: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/7c8/fef/fb47c8fefd97d4e855e8a2c029e4da5d.png"></div><br>  <em>Tarefa modelo para o c√©rebro</em> </li><li>  O GridWorldLearning j√° deve estar definido como modelo.  Neste exemplo, usamos o TFModel que acompanha o exemplo GridWorld. </li><li>  Clique em Reproduzir para iniciar o editor e ver como o agente gerencia o cubo. </li></ol><br>  Agora estamos lan√ßando o ambiente pr√©-treinado da Unity.  Na pr√≥xima se√ß√£o, aprenderemos como usar o c√©rebro que treinamos na se√ß√£o anterior. <br><br><h2>  Carga cerebral treinada </h2><br>  Todos os exemplos do Unity possuem c√©rebros pr√©-treinados que podem ser usados ‚Äã‚Äãpara estudar exemplos.  Obviamente, queremos poder carregar nossos pr√≥prios gr√°ficos TF no Unity e execut√°-los.  Para carregar um gr√°fico treinado, siga estas etapas: <br><br><ol><li>  V√° para a pasta ML-Agents / ml-agents / models / firstRun-0.  Dentro desta pasta est√° o arquivo GridWorldLearning.bytes.  Arraste esse arquivo para a pasta Project / Assets / ML-Agents / Examples / GridWorld / TFModels dentro do editor do Unity: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f51/955/ef5/f51955ef56aae53e7946378048b9e13e.png"></div><br>  <em>Arrastando um gr√°fico de bytes para o Unity</em> </li><li>  Portanto, importamos o gr√°fico para o projeto do Unity como um recurso e o renomeamos para GridWorldLearning 1. O mecanismo faz isso porque o modelo padr√£o j√° tem o mesmo nome. </li><li>  Localize o GridWorldLearning na pasta brain, selecione-o na janela Inspector e arraste o novo modelo GridWorldLearning 1 para o campo Model dos par√¢metros Brain Parameters: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/784/8a3/9b7/7848a39b79b02ffb11d968f835054295.png"></div><br>  <em>Carregando o c√©rebro no campo Modelo de Gr√°fico</em> </li><li>  Nesse est√°gio, n√£o precisamos alterar nenhum outro par√¢metro, mas preste aten√ß√£o especial em como o c√©rebro est√° configurado.  Por enquanto, as configura√ß√µes padr√£o ser√£o suficientes. </li><li>  Clique em Reproduzir no editor do Unity e veja como o agente se move com sucesso pelo jogo. </li><li>  O sucesso do agente no jogo depende do tempo de seu treinamento.  Se voc√™ permitir que ele complete o treinamento, o agente ser√° semelhante a um agente do Unity totalmente treinado. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt454612/">https://habr.com/ru/post/pt454612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt454600/index.html">Como fizemos um acordo seguro no Freelansim: escolha, reduza recursos, compare comiss√µes</a></li>
<li><a href="../pt454604/index.html">Gerando um aplicativo React com um back-end do GraphQL em minutos</a></li>
<li><a href="../pt454606/index.html">Recursos do atributo inputmode para SO e navegadores m√≥veis</a></li>
<li><a href="../pt454608/index.html">Contrato de n√≠vel de servi√ßo: escrevemos SLA para ... outros, ou a conclus√£o de um SLA com um operador de telecomunica√ß√µes</a></li>
<li><a href="../pt454610/index.html">Marketing de conte√∫do, SEO, testes e pesquisas: 9 ferramentas para promover uma startup no exterior</a></li>
<li><a href="../pt454614/index.html">XXE: entidade externa XML</a></li>
<li><a href="../pt454616/index.html">O uso da IA ‚Äã‚Äãpara aumentar a efici√™ncia dos trabalhadores mentais</a></li>
<li><a href="../pt454618/index.html">Po√ßo da produtividade: como o Slack prejudica nosso fluxo de trabalho</a></li>
<li><a href="../pt454620/index.html">#NoDeployFriday: ajuda ou prejudica?</a></li>
<li><a href="../pt454622/index.html">Kreisel EVEX 910e: modelo hist√≥rico - nova vida</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>