<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåÇ üç≥ ü¶å M√©todos cuasi-newtonianos, o cuando hay demasiadas segundas derivadas para Athos üå°Ô∏è üóúÔ∏è üßöüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Al primer conocimiento de los m√©todos cuasi-newtonianos, uno puede sorprenderse dos veces. En primer lugar, despu√©s de un r√°pido vistazo a las f√≥rmula...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√©todos cuasi-newtonianos, o cuando hay demasiadas segundas derivadas para Athos</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470638/">  Al primer conocimiento de los m√©todos cuasi-newtonianos, uno puede sorprenderse dos veces.  En primer lugar, despu√©s de un r√°pido vistazo a las f√≥rmulas, surgen dudas de que esto pueda funcionar en absoluto.  Sin embargo, funcionan.  Adem√°s, parece dudoso que funcionen bien.  Y es a√∫n m√°s sorprendente ver cu√°n m√°s r√°pidos son que las diversas variaciones del descenso de gradiente, no en tareas especialmente construidas, sino en tareas reales tomadas de la pr√°ctica.  Y si despu√©s de esto todav√≠a hay dudas mezcladas con inter√©s, entonces debes entender por qu√© esto funciona en absoluto. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ya se han considerado el</a> origen y las ideas b√°sicas que impulsan los m√©todos de gradiente, incluido el m√©todo de Newton.  Es decir, confiamos en la informaci√≥n sobre el comportamiento de la funci√≥n en la vecindad de la posici√≥n actual, lo que nos da un an√°lisis matem√°tico simple.  Como m√≠nimo, se asumi√≥ que la informaci√≥n sobre los primeros derivados estaba disponible para nosotros.  ¬øQu√© pasa si esto es todo lo que est√° disponible para nosotros?  ¬øEl gradiente descendente es nuestra oraci√≥n?  Por supuesto, s√≠, a menos que de repente recuerde que estamos tratando con un <i>proceso</i> en el que la funci√≥n objetivo se procesa correctamente.  Y si es as√≠, ¬øpor qu√© no usamos la informaci√≥n acumulada sobre el comportamiento de la funci√≥n para hacer que nuestra caminata en su superficie sea un poco menos ciega? <br><br>  La idea de utilizar informaci√≥n sobre el camino cubierto es el n√∫cleo de la mayor√≠a de las formas de acelerar los m√©todos de descenso.  Este art√≠culo analiza una de las formas m√°s efectivas, aunque no la m√°s barata, de dar cuenta de este tipo de informaci√≥n, lo que lleva a la idea de m√©todos cuasi-newtonianos. <br><br>  Para comprender de d√≥nde crecen las patas de los m√©todos cuasi-newtonianos y de d√≥nde proviene el nombre, nuevamente tenemos que volver al m√©todo de minimizaci√≥n basado en la soluci√≥n directa de la ecuaci√≥n de punto estacionario <img src="https://habrastorage.org/getpro/habr/post_images/c43/946/5f9/c439465f905d9f366a2f4b3296306290.gif" title="&quot;\ bigtriangledown f = 0&quot;">  .  As√≠ como la consideraci√≥n del m√©todo de Newton aplicado a la soluci√≥n de esta ecuaci√≥n nos llev√≥ al m√©todo de optimizaci√≥n del mismo nombre (que, a diferencia de su progenitor, tiene una regi√≥n de convergencia global), podemos esperar que la consideraci√≥n de otros m√©todos para resolver sistemas de ecuaciones no lineales sea fruct√≠fera. planificar ideas para construir otros m√©todos de optimizaci√≥n. <br><br><h2>  Metodos </h2><br>  Perm√≠teme recordarte que el m√©todo de Newton para resolver el sistema de ecuaciones <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  , se basa en el reemplazo en el vecindario de alg√∫n punto cercano a la soluci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  las funciones <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  su aproximaci√≥n lineal <img src="https://habrastorage.org/getpro/habr/post_images/d15/479/f23/d15479f235f0d60ce8837c9043a0d2cc.gif" title="&quot;L (p) = F (x) + J (x) p&quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  Es un operador lineal que, cuando <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  es un vector y <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  tiene derivadas parciales con respecto a cada variable, coincide con la matriz de Jacobi <img src="https://habrastorage.org/getpro/habr/post_images/4d2/826/ff6/4d2826ff6ba22f9f67cab70bfbe17a16.gif" title="&quot;J_ {ij} = \ dfrac {\ partial F_ {i}} {\ partial x_ {j}}&quot;">  .  A continuaci√≥n, se resuelve la ecuaci√≥n. <img src="https://habrastorage.org/getpro/habr/post_images/c9d/8be/3f2/c9d8be3f2d70054db890ea34e3409544.gif" title="&quot;L (p) = 0&quot;">  y punto <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&amp; quot; x '= x + p &amp; quot;">  tomado como una nueva aproximaci√≥n a la soluci√≥n deseada.  Es simple y funciona. <br><br>  Pero, ¬øy si por alguna raz√≥n no podemos calcular la matriz de Jacobi?  Lo primero que viene a la mente en este caso es que si no podemos calcular las derivadas parciales anal√≠ticamente, entonces podemos obtener una aproximaci√≥n num√©rica para ellas.  La opci√≥n m√°s simple (aunque de ninguna manera la √∫nica) para tal aproximaci√≥n puede ser la f√≥rmula de diferencias finitas correctas: <img src="https://habrastorage.org/getpro/habr/post_images/149/708/f5b/149708f5b8bab4374023295557622e82.gif" title="&quot;\ dfrac {\ partial F_ {i}} {\ partial x_ {j}} \ aprox \ dfrac {F_ {i} (x + h_ {j} e_ {j}) - F_ {i} (x)} { h_ {j}} &quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/459/e61/aa0/459e61aa08f7fe807167a596e7ebd8a9.gif" title="&quot;e_ {j}&quot;">  Es el j-√©simo vector base.  La matriz compuesta de tales aproximaciones se denotar√° por <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  .  Un an√°lisis de cu√°nto reemplazo <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  en <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  en el m√©todo de Newton, su convergencia afecta, se dedica un n√∫mero bastante grande de trabajos, pero en este caso estamos interesados ‚Äã‚Äãen otro aspecto.  Es decir, tal aproximaci√≥n requiere el c√°lculo de la funci√≥n en N puntos adicionales y, adem√°s, la funci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/6c4/afd/100/6c4afd1002ddcfa43d07afbc9f103a9d.gif" title="&quot;\ bar {L} (p) = F (x) + \ bar {J} p&quot;">  en estos puntos <i>interpola la</i> funci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  es decir <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c56/5c9/4b4/c565c94b4a37b9cd5f42fc1be92b2e15.gif" title="&quot;\ bar {L (} h_ {j} e_ {j}) = F (x) + h_ {j} \ dfrac {F (x + h_ {j} e_ {j}) - F (x)} {h_ {j}} = F (x) + F (x + h_ {j} e_ {j}) - F (x) = F (x + h_ {j} e_ {j}) &quot;."><br><br>  No todas las aproximaciones de la matriz de Jacobi tienen esta propiedad, pero cada matriz de una funci√≥n af√≠n que tiene esta propiedad es una aproximaci√≥n de la matriz de Jacobi.  De hecho, si <img src="https://habrastorage.org/getpro/habr/post_images/88f/5c8/dd7/88f5c8dd7e9876a2d0e0980882f261da.gif" title="&quot;F (x + p_ {j}) = F (x) + J (x) p_ {j} + o \ left (\ left \ Vert p_ {j} \ right \ Vert ^ {2} \ right)&quot;">  y <img src="https://habrastorage.org/getpro/habr/post_images/5ad/e1e/ad2/5ade1ead2804a3bfaa8ffdf9122a179a.gif" title="&quot;\ bar {J} p_ {j} = F (x + p_ {j}) - F (x)&quot;">  luego en <img src="https://habrastorage.org/getpro/habr/post_images/803/ca4/351/803ca4351b87edf1a13a2a2947772fa7.gif" title="&quot;\ left \ Vert p_ {j} \ right \ Vert \ rightarrow0 \ quad \ bar {J} (x) p_ {j} \ rightarrow J (x) p_ {j}&quot;">  .  Esta propiedad, es decir, la propiedad de interpolaci√≥n, nos brinda una forma constructiva de generalizar el m√©todo de Newton. <br><br>  Dejar <img src="https://habrastorage.org/getpro/habr/post_images/194/ad1/d42/194ad1d42aa4320679b9498748ceb78d.gif" title="&quot;\ bar {L} (p) = a + Ap&quot;">  - funci√≥n que cumple el requisito <img src="https://habrastorage.org/getpro/habr/post_images/06e/34e/3a7/06e34e3a7a0d0058ef351da74258a637.gif" title="&quot;\ bar {L} (p_ {i}) = F (x + p_ {i})&quot;">  para alg√∫n sistema de vectores linealmente independientes <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  .  Entonces dicha funci√≥n se llama funci√≥n <i>secante</i> <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  , y la ecuaci√≥n que lo define es <i>la ecuaci√≥n secante</i> .  Si el sistema de vectores <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  est√° completo (es decir, hay exactamente N de ellos y todav√≠a son linealmente independientes) y, adem√°s, el sistema de vectores <img src="https://habrastorage.org/getpro/habr/post_images/94f/cf5/579/94fcf55798902795ffb670e35359d2af.gif" title="&quot;\ left \ {F (x + p_ {i}), i = 1 \ puntos N \ right \}&quot;">  linealmente independiente entonces <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  definido de forma √∫nica. <br><br>  Cualquier m√©todo basado en el cambio local de ecuaci√≥n. <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  ecuaci√≥n de la forma <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  satisface <i>la ecuaci√≥n secante</i> , llamada <i>m√©todo secante</i> . <br><br>  Una pregunta justa surge sobre c√≥mo construir la secante para una funci√≥n de la manera m√°s racional. <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  La siguiente l√≠nea de razonamiento parece obvia: dejar que se construya un modelo af√≠n en el punto x que interpola la funci√≥n dada en los puntos <img src="https://habrastorage.org/getpro/habr/post_images/462/bcf/f32/462bcff32469c0ec5f8ccfc80534c05c.gif" title="&quot;x-x_ {1}, x-x_ {2}, \ puntos, x-x_ {N}&quot;">  .  Soluci√≥n de ecuaciones <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  nos da un nuevo punto <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&amp; quot; x '= x + p &amp; quot;">  .  Luego para construir un modelo af√≠n en un punto <img src="https://habrastorage.org/getpro/habr/post_images/787/cf7/c3a/787cf7c3a3d374114b3a07305b7fa446.gif" title="&amp; quot; x '&amp; quot;">  es m√°s razonable elegir puntos de interpolaci√≥n para que el valor <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  ya conocido, es decir, sacarlos del set <img src="https://habrastorage.org/getpro/habr/post_images/333/297/225/33329722533f0b608b0994d2a5ba83fa.gif" title="&amp; quot; \ left \ {x'-x, x'-x_ {1}, x'-x_ {2}, \ dots, x'-x_ {N} \ right \} &amp; quot;">  .  Existen diferentes opciones para los puntos a elegir entre los muchos utilizados anteriormente.  Por ejemplo, puede tomar como puntos de interpolaci√≥n aquellos en los que <img src="https://habrastorage.org/getpro/habr/post_images/bb3/e3a/cd5/bb3e3acd5043b859fe89006d4cabe5a0.gif" title="&quot;\ left \ Vert F \ right \ Vert&quot;">  importa menos o solo el primero <img src="https://habrastorage.org/getpro/habr/post_images/055/8e9/3d9/0558e93d918ff32e873b6a71703e9969.gif" title="&quot;N&quot;">  puntos.  En cualquier caso, parece obvio que <img src="https://habrastorage.org/getpro/habr/post_images/95f/756/92b/95f75692ba0aeefcef24ae42714dbc1b.gif" title="&amp; quot; p = x'-x &amp; quot;">  debe incluirse en muchos puntos de interpolaci√≥n para el nuevo modelo af√≠n.  As√≠ que m√°s all√° <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  Los pasos del proceso iterativo en nuestro conjunto pueden ser hasta <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  desplazamientos construidos en puntos previamente pasados.  Si el proceso se construye de tal manera que el nuevo modelo af√≠n no use m√°s <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  de los valores anteriores, entonces dicho proceso se llama m√©todo secante de punto p. <br><br>  A primera vista, podr√≠a parecer que el m√©todo secante de punto N es el mejor candidato para el papel de reemplazar el m√©todo de Newton, ya que hace el m√°ximo uso de la informaci√≥n que obtenemos en el proceso de resoluci√≥n, mientras minimizamos el n√∫mero de c√°lculos adicionales: utilizamos el valor de la funci√≥n N puntos pasados.  Lamentablemente, esto no es as√≠.  El caso es que el sistema vectorial <img src="https://habrastorage.org/getpro/habr/post_images/ed0/117/8ca/ed01178ca46506fa4588780d16d705a1.gif" title="&quot;F (x_ {0}), F (x_ {1}), \ puntos F (x_ {N})&quot;">  obstinadamente se niega a ser linealmente independiente con un N. suficientemente grande. Adem√°s, incluso si esta condici√≥n se cumple y el modelo af√≠n correspondiente todav√≠a existe, existe la posibilidad de que las direcciones <img src="https://habrastorage.org/getpro/habr/post_images/602/ff2/50c/602ff250c473d5b28e08a1453d4175b3.gif" title="&quot;p_ {j} = x_ {j} -x_ {0}&quot;">  Tambi√©n demuestra ser linealmente independiente, resulta a√∫n menos.  Y esto implica el hecho de que el modelo af√≠n, aunque existe, es degenerado y pr√°cticamente inadecuado. <br><br>  En general, el m√°s estable es el m√©todo secante de 2 puntos.  Es decir, un m√©todo en el que en cada iteraci√≥n tenemos que calcular valores N-1 adicionales de la funci√≥n.  Esto claramente no es adecuado para nuestros prop√≥sitos pr√°cticos. <br><br>  Entonces la pregunta es: ¬øqu√© fue todo esto? <br><br><h2>  M√©todos cuasi-newtonianos para resolver ecuaciones. </h2><br><br>  La salida es simple, aunque no obvia.  Si no tenemos la capacidad t√©cnica, basada en los valores ya calculados, para determinar de manera √∫nica un modelo af√≠n que satisfaga la ecuaci√≥n secante, entonces no es necesario.  Tomamos la ecuaci√≥n de secantes como base, pero exigiremos que se satisfaga solo para alg√∫n sistema incompleto de vectores <img src="https://habrastorage.org/getpro/habr/post_images/e5e/f2b/432/e5ef2b43292735aa2a68afffb80bf520.gif" title="&quot;\ left \ {p_ {1}, p_ {2}, \ dots, p_ {m} \ right \}, m &amp; lt; N&quot;">  .  En otras palabras, requeriremos que la condici√≥n de interpolaci√≥n se satisfaga solo para un n√∫mero suficientemente peque√±o de valores conocidos.  Por supuesto, en este caso ya no podemos garantizar que la matriz utilizada en dicho modelo tender√° a la matriz de Jacobi, pero no la necesitaremos.  Adem√°s de esto, el modelo af√≠n debe interpolar la funci√≥n en el punto actual, es decir. <img src="https://habrastorage.org/getpro/habr/post_images/3b9/9d1/7fa/3b99d17fa378aeaf36097faef3830bd5.gif" title="&quot;\ bar {L} (0) = F (x)&quot;">  , obtenemos la siguiente formulaci√≥n del m√©todo secante: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/78c/f30/421/78cf304219a4bc90d4f900062bf2d027.gif" title="&quot;\\ \ bar {L} (p_ {i}) = F (x) + Ap_ {i} = F (x + p_ {i}), \ quad i = 1 \ puntos m \\ \ bar {L} (p) = 0 \ quad \ Rightarrow p = A ^ {- 1} F (x) &quot;"><br><br>  Bruiden fue el primero en considerar m√©todos de este tipo para m = 1, llam√°ndolos cuasi-newtonianos.  Est√° claro que la condici√≥n secante en este caso nos permite identificar de manera √∫nica la matriz <img src="https://habrastorage.org/getpro/habr/post_images/c9d/999/d9a/c9d999d9a4e8bd3d6f8e50519d1dfaa8.gif" title="&quot;A&quot;">  solo si se le imponen condiciones adicionales, y cada una de esas condiciones adicionales da lugar a un m√©todo separado.  Bruyden mismo razon√≥ de la siguiente manera: <br><br>  <i>como el movimiento en la direcci√≥n</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>desde el punto</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;"></i>  <i>hasta el punto</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/1d0/56f/301/1d056f3016bc715aacc23418d8629173.gif" title="&quot;x_ {1}&quot;"></i>  <i>no nos proporciona ninguna informaci√≥n adicional sobre c√≥mo cambia la funci√≥n de otra manera que</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>direcciones, luego el efecto de la nueva funci√≥n af√≠n en el vector</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>deber√≠a diferir del efecto de la funci√≥n anterior en el mismo vector cuanto menos, m√°s diferente</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>de</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>.</i>  <i>Como √∫ltimo recurso, cuando</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>ortogonal</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>, el comportamiento de la nueva funci√≥n no debe ser diferente del comportamiento de la anterior.</i> <i><br></i> <br>  La idea de Breiden es brillante en su simplicidad.  De hecho, si no tenemos nueva informaci√≥n sobre el comportamiento de la funci√≥n, lo mejor que podemos hacer es tratar de no da√±ar la anterior.  Entonces la condici√≥n adicional <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b3/d43/d20/0b3d43d207b144b926274d0c81abccbf.gif" title="&quot;\ bar {L} _ {1} q = \ bar {L} _ {0} q&quot;">  para todos <img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q">  tal que <img src="https://habrastorage.org/getpro/habr/post_images/c16/9f6/315/c169f6315171249a34b50b26a2975c6e.gif" title="&quot;q ^ {T} p = 0&quot;"><br><br>  le permite determinar de manera √∫nica la matriz de la nueva transformaci√≥n: se obtiene al agregar una correcci√≥n de rango 1 a la matriz anterior. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/522/f36/1f9/522f361f94a7b5a2e9da68094983b21d.gif" title="&quot;\\ A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T}} {p ^ {T} p} \\ y = F (x_ {0}) -F (x_ {1}) &quot;"><br><br>  Sin embargo, a pesar de la simplicidad y consistencia de las conclusiones hechas por Bruiden, no proporcionan el punto de apoyo que podr√≠a servir como base para construir otros m√©todos similares.  Afortunadamente, hay una expresi√≥n m√°s formal de su idea.  A saber, la matriz construida de esta manera <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  Resulta ser la soluci√≥n al siguiente problema: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/028/96f/3fa/02896f3facb898d70f26abad02fe90a9.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ F (x_ {1}) - Ap = F (x_ {0})&quot;"><br><br>  La restricci√≥n de la tarea no es m√°s que la ecuaci√≥n secante, y la condici√≥n de minimizaci√≥n refleja nuestro deseo de guardar tanta informaci√≥n como sea posible en la matriz. <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  .  La medida de la discrepancia entre las matrices en este caso es la norma de Frobenius, en la cual el problema planteado tiene una soluci√≥n inequ√≠voca.  Esta formulaci√≥n bien puede servir como punto de partida para construir otros m√©todos.  Es decir, podemos cambiar tanto la <i>medida</i> por la cual evaluamos los cambios introducidos como las <i>condiciones</i> impuestas en la matriz.  En general, ya se puede trabajar con tal formulaci√≥n del m√©todo. <br><br><h2>  M√©todos de optimizaci√≥n cuasi-Newton </h2><br><br>  Habiendo entendido la idea principal, finalmente podemos volver a los problemas de optimizaci√≥n y notar que aplicar la f√≥rmula de Bruyden para recalcular el modelo af√≠n no se ajusta muy bien a nuestra tarea.  De hecho, la primera derivada de la funci√≥n de gradiente <img src="https://habrastorage.org/getpro/habr/post_images/6b8/82e/be7/6b882ebe727121dcb5fc21b091044b5a.gif" title="&quot;\ bigtriangledown f&quot;">  no hay nada m√°s que la matriz de Hesse, que por construcci√≥n es sim√©trica.  Al mismo tiempo, la actualizaci√≥n de acuerdo con la regla de Bruyden conduce a una matriz asim√©trica. <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  incluso si <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  fue sim√©trico  Esto no significa que el m√©todo de Bruden no se pueda aplicar para resolver la ecuaci√≥n del punto estacionario, pero con base en dicha regla de actualizaci√≥n, es poco probable que podamos construir buenos m√©todos de optimizaci√≥n.  En general, es bastante obvio que el m√©todo cuasi-newtoniano deber√≠a funcionar mejor y con mayor precisi√≥n el sistema de condiciones del problema describe los detalles de una matriz de Jacobi espec√≠fica. <br><br>  Para corregir este inconveniente, agregamos una restricci√≥n adicional al problema de minimizaci√≥n de Bruden, que requiere expl√≠citamente que la nueva matriz sea sim√©trica junto con la anterior: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/03e/167/aa2/03e167aa25e0f4aa6b8df8546552e79a.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0} ) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  La soluci√≥n a este problema es <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df8/356/74b/df835674b94bab190bca3c18efed98ce.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T} + p (y-A_ {0} p) ^ {T}} {p ^ {T} p} - \ dfrac {(y-A_ {0} p) ^ {T} p} {\ left (p ^ {T} p \ right) ^ {2}} pp ^ {T} &quot;"><br><br>  Aqui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/89c/f84/f29/89cf84f292ed72d1b20755677688a054.gif" title="y = \ bigtriangledowndown f (x_ {1}) - \ bigtriangledowndown f (x_ {0})"></a>  , y la f√≥rmula de rec√°lculo matricial lleva el nombre de sus creadores: Powell, Shanno y Bruyden (PSB).  La matriz resultante es sim√©trica, pero claramente no positiva definida, aunque solo de repente <img src="https://habrastorage.org/getpro/habr/post_images/6c7/040/47d/6c704047d3148fd7a8b563aaf79dd7f4.gif" title="&quot;y&quot;">  no ser√° colineal <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  .  Y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vimos</a> que la certeza positiva es altamente deseable en los m√©todos de optimizaci√≥n. <br><br>  Nuevamente, corregiremos la condici√≥n del problema, usando esta vez la norma de Frobenius escalada como una medida de la divergencia de la matriz. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f37/eb1/0f4/f37eb10f4eb10d0c54acc9adab962f10.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} -A_ {0} \ right) T ^ {- 1} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  El origen de tal enunciado de la pregunta es un gran tema separado, pero es interesante que si la matriz T es tal que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/673/872/131/673872131fa6cb0f44e6839be0e448e7.gif" title="T ^ {T} T = G, Gp = y"></a>  (es decir, G tambi√©n es una matriz de transformaci√≥n af√≠n que satisface la ecuaci√≥n secante para la direcci√≥n p), entonces la soluci√≥n a este problema resulta ser independiente de la elecci√≥n de T y conduce a la f√≥rmula de actualizaci√≥n <br><br><img src="https://habrastorage.org/getpro/habr/post_images/135/ea6/c14/135ea6c14ea8f63f961e83576f1be5d5.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) y ^ {T} + y (y-A_ {0} p) ^ {T}} {y ^ {T} p} - \ dfrac {\ left (y-A_ {0} p \ right) ^ {T} p} {\ left (y ^ {T} p \ right) ^ {2}} aa ^ {T} &quot;"><br><br>  conocida como la f√≥rmula de Davidon-Fletcher-Powell.  Este m√©todo de actualizaci√≥n se ha probado en la pr√°ctica, ya que tiene la siguiente propiedad: <br><br>  <i>si</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/e3e/c44/1c1/e3ec441c17e1b43df108a7d8e15d3dd6.gif" title="&quot;y ^ {T} p &amp; gt; 0&quot;"></i>  <i>y</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;"></i>  <i>positivo definitivo entonces</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;"></i>  <i>Tambi√©n positivamente identificado.</i> <br><br>  Observo despu√©s que si la primera condici√≥n no se cumple, entonces no existe una funci√≥n af√≠n con una matriz definida positiva que satisfaga la ecuaci√≥n secante. <br><br>  Si en el problema que conduce al m√©todo DFP, tomamos, como una medida de la discrepancia de modelos afines, la distancia no entre las matrices mismas, sino entre las matrices inversas a ellas, obtenemos un problema de la forma <br><br><img src="https://habrastorage.org/getpro/habr/post_images/337/0b0/af2/3370b0af216ab9695789eeb586cf3604.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} ^ {- 1} -A_ {0} ^ {- 1} \ right) T ^ {- 1} \ right \ Vert _ { F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Su soluci√≥n es una f√≥rmula bien conocida, descubierta casi simult√°neamente por Breiden, Fletcher, Goldfarb y Shanno (BFGS). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3fa/840/c7b/3fa840c7b6ec3de81eb02bb0e9240722.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {aa ^ {T}} {y ^ {T} p} - \ dfrac {A_ {0} pp ^ {T} A_ {0}} {p ^ { T} A_ {0} p} &quot;"><br><br>  Hasta la fecha, se cree que el rec√°lculo de acuerdo con esta f√≥rmula es el m√°s eficiente desde el punto de vista computacional y, al mismo tiempo, es menos propenso a la degeneraci√≥n de la matriz con un gran n√∫mero de iteraciones.  En las mismas condiciones que DFP, esta f√≥rmula conserva la propiedad de la definici√≥n positiva. <br><br>  Todos los m√©todos descritos para actualizar la matriz requieren una correcci√≥n de rango 2. Esto hace que invertir la matriz sea f√°cil y sencillo <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  usando la f√≥rmula de Sherman-Morrison y el valor <img src="https://habrastorage.org/getpro/habr/post_images/5f6/3ac/2d9/5f63ac2d91f47a730fee01b5db38f3bd.gif" title="&quot;A_ {0} ^ {- 1}&quot;">  . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed0/9f8/002/ed09f80027e56f58a3502cc943758509.gif" title="&quot;B_ {1} = B_ {0} + uv ^ {T} \ Rightarrow B_ {1} ^ {- 1} = B_ {0} ^ {- 1} + \ dfrac {B_ {0} ^ {- 1} uv ^ {T} B_ {0} ^ {- 1}} {1 + v ^ {T} B_ {0} ^ {- 1} u} &quot;"><br><br>  siempre que el denominador de la f√≥rmula sea distinto de cero.  No dar√© f√≥rmulas espec√≠ficas para actualizar las matrices inversas de los m√©todos enumerados, ya que son f√°ciles de encontrar o derivar de forma independiente.  Lo √∫nico que debe tenerse en cuenta en este caso es que las variantes de los m√©todos con la actualizaci√≥n de la matriz inversa suelen ser mucho menos estables (es decir, sufren m√°s errores de redondeo) que las que sugieren actualizar la matriz original.  Es m√°s eficiente actualizar no la matriz en s√≠, sino su descomposici√≥n de Cholesky (a menos, por supuesto, que se produzca dicha descomposici√≥n), ya que dicha opci√≥n de implementaci√≥n es m√°s estable num√©ricamente y, adem√°s, minimiza el costo de resolver una ecuaci√≥n que determina la direcci√≥n del movimiento. <br><br>  Queda por considerar la cuesti√≥n de c√≥mo deber√≠a verse la primera matriz en el proceso cuasi-newtoniano.  Aqu√≠ todo es obvio: cuanto m√°s cerca est√© de la matriz de Hesse o de su versi√≥n corregida, si el Hesse de repente no resulta ser definitivo positivo, mejor ser√° desde el punto de vista de la convergencia.  Sin embargo, en principio, cualquier matriz definida positiva puede ser adecuada para nosotros.  La versi√≥n m√°s simple de dicha matriz es √∫nica, y luego la primera iteraci√≥n coincide con la iteraci√≥n del descenso del gradiente.  Fletcher y Powell demostraron (naturalmente, para el m√©todo DFP) que si la funci√≥n cuadr√°tica se minimiza, independientemente de qu√© matriz (positiva definida) se use como la iteraci√≥n inicial de DFP, conducir√°n a una soluci√≥n en exactamente N iteraciones, donde N es dimensi√≥n del problema, y ‚Äã‚Äãla matriz cuasi-newtoniana coincide con la matriz de Hesse en el punto m√≠nimo.  En el caso general no lineal de dicha felicidad, por supuesto, no esperaremos, pero esto al menos da razones para no preocuparnos demasiado por la mala elecci√≥n de la matriz inicial. <br><br><h2>  Conclusi√≥n </h2><br><br>  El enfoque descrito para la construcci√≥n de m√©todos cuasi-newtonianos no es el √∫nico posible.  Como m√≠nimo, los descubridores de los m√©todos cuasi-newtonianos descritos y muchos investigadores posteriores llegaron a las mismas f√≥rmulas sobre la base de consideraciones completamente diferentes.  Sin embargo, es interesante que tan pronto como apareci√≥ un cierto m√©todo cuasi-newtoniano, independientemente del m√©todo para obtenerlo, despu√©s de un tiempo bastante corto qued√≥ claro que era una soluci√≥n a un problema de optimizaci√≥n muy f√°cil de interpretar.  En mi opini√≥n, es notable que sea posible traer alg√∫n denominador com√∫n para m√©todos tan diversos, ya que esto proporciona la base para construir otros m√©todos que tengan mejor en cuenta los detalles de una tarea en particular.  En particular, existen m√©todos cuasi-newtonianos dise√±ados para actualizar matrices dispersas, m√©todos en los que la menor cantidad posible de elementos est√°n sujetos a cambios, y muchos otros ser√≠an una fantas√≠a. <br><br>  Tambi√©n se debe tener en cuenta que los m√©todos de m√©tricas variables, a pesar de su nombre, no siempre conducen a la construcci√≥n de matrices, que en realidad son m√©tricas, aunque lo hacen cada vez que es posible.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo general, este no es un gran problema, pero aquellos que desean protegerse de una posible verg√ºenza pueden recurrir a los mismos trucos que se hicieron para superar un problema similar con el m√©todo de Newton, por ejemplo, cambiando la direcci√≥n o aplicando </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">el esquema de Levenberg-Marquardt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Es cierto, en este caso, las preguntas sobre c√≥mo elegir la forma de una regi√≥n de confianza volver√°n a ser relevantes, pero aqu√≠ tienes que elegir el menor de los males. Otra soluci√≥n al problema es utilizar m√©todos de b√∫squeda lineal para garantizar que se cumplan las condiciones necesarias para mantener una certeza positiva. La regla de Wolfe garantiza el cumplimiento de esta condici√≥n, mientras que las reglas de Armijo y Goldstein no lo hacen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Te√≥ricamente, es casi imposible determinar cu√°l de la gran cantidad de m√©todos cuasi-newtonianos posibles ser√° el m√°s efectivo con respecto a una determinada clase de problemas. Por lo general, al formular un m√©todo, se limitan a mostrar su efectividad para minimizar una funci√≥n cuadr√°tica (por cierto, un m√©todo se considera efectivo si conduce a una soluci√≥n exacta en N iteraciones, es decir, no m√°s lentamente que los m√©todos directos para resolver SLAEs). En casos m√°s raros, uno puede encontrar estudios del orden de convergencia del m√©todo (que generalmente es superlineal, es decir, significativamente mejor que el que nos brinda el descenso de gradiente), estabilidad y otras caracter√≠sticas de inter√©s. Pero en general, el √∫nico criterio razonable para juzgar la efectividad de un m√©todo particular para una clase particular de tareas es la pr√°ctica.As√≠ que palas en mano, y √©xito en la aplicaci√≥n.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/470638/">https://habr.com/ru/post/470638/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../470620/index.html">Infraestructura como c√≥digo: c√≥mo superar problemas con XP</a></li>
<li><a href="../470622/index.html">Descripci√≥n general de los m√©todos de selecci√≥n de funciones</a></li>
<li><a href="../470628/index.html">Simulador de naves espaciales construcci√≥n naval</a></li>
<li><a href="../470632/index.html">Arend: lenguaje de tipo dependiente basado en HoTT (parte 2)</a></li>
<li><a href="../470634/index.html">Identifique la comunidad cruzada en Instagram para identificar los intereses de los usuarios.</a></li>
<li><a href="../470640/index.html">Dimensionar Elasticsearch</a></li>
<li><a href="../470642/index.html">Conoce a Yandex.Station Mini. Gran historia de un peque√±o dispositivo.</a></li>
<li><a href="../470644/index.html">Por qu√© los blogs corporativos a veces son amargos: algunas observaciones y consejos</a></li>
<li><a href="../470646/index.html">Matem√°ticas para la ciencia de datos. Nuevo curso de OTUS</a></li>
<li><a href="../470648/index.html">IBM LTO-8: forma f√°cil de almacenar datos en fr√≠o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>