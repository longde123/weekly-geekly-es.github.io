<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😂 ⬇️ ↪️ NeurIPS: Como conquistar a melhor conferência de ML 👩‍🏭 📓 💨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS - uma conferência que atualmente é considerada o evento mais importante do mundo do aprendizado de máquina. Hoje vou falar sobre minha experiê...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NeurIPS: Como conquistar a melhor conferência de ML</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/430712/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NeurIPS</a> - uma conferência que atualmente é considerada o evento mais importante do mundo do aprendizado de máquina.  Hoje vou falar sobre minha experiência em participar de concursos do NeurIPS: como competir com os melhores acadêmicos do mundo, ganhar um prêmio e publicar um artigo. </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  Qual é a essência da conferência? </h1><br><p>  O NeurIPS suporta a introdução de métodos de aprendizado de máquina em várias disciplinas científicas.  Aproximadamente 10 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">faixas são</a> lançadas anualmente para resolver problemas prementes do mundo acadêmico.  De acordo com os resultados da competição, os vencedores falam na conferência com relatórios, novos desenvolvimentos e algoritmos.  Acima de tudo, sou apaixonada por aprendizado reforçado (Reinforcement Learning ou RL), é por isso que tenho participado de concursos de RL dedicados ao NeurIPS pelo segundo ano. </p><br><h1 id="pochemu-neurips">  Porquê NeurIPS </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  O NeurIPS é focado principalmente na ciência, não no dinheiro.  Ao participar de concursos, você está fazendo algo realmente importante, lidando com questões prementes. <br><p>  Em segundo lugar, esta conferência é um evento global, cientistas de diferentes países se reúnem em um só lugar, com cada um dos quais você pode conversar. </p><br><p>  Além disso, toda a conferência é repleta das mais recentes realizações científicas e resultados de última geração, é extremamente importante para as pessoas do campo da ciência de dados conhecê-las e monitorá-las. </p><br><h1 id="kak-nachat">  Como começar? </h1><br><p>  Começar a participar de tais competições é bastante simples.  Se você entende tanto de DL que pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinar o ResNet</a> - basta: inscreva-se e pronto.  Sempre existe uma tabela de classificação pública na qual você pode avaliar com sobriedade seu nível em comparação com outros participantes.  E se algo não estiver claro - sempre existem canais em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">folga</a> / discórdia / violência / etc para discutir todas as questões emergentes.  Se o tópico for realmente “seu”, nada o impedirá de receber o valor estimado - em todas as competições em que participei, todas as abordagens e soluções foram estudadas e implementadas no decorrer da competição. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  Estudo de caso do NeurIPS: aprendendo a correr </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Edição </h3><br><p>  A marcha de uma pessoa é o resultado da interação de músculos, ossos, órgãos de visão e ouvido interno.  Em caso de perturbação do sistema nervoso central, podem ocorrer certos distúrbios motores, incluindo distúrbios da marcha - abasia. <br>  Pesquisadores do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Laboratório de Biomecânica Neuromuscular de Stanford</a> decidiram conectar o aprendizado de máquina à questão do tratamento para poder experimentar e testar suas teorias em um modelo virtual do esqueleto, e não em pessoas vivas. </p><br><h3 id="postanovka-zadachi">  Declaração do problema </h3><br><p>  Os participantes receberam um esqueleto humano virtual (no simulador <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenSim</a> ), que possuía uma prótese no lugar de uma perna.  A tarefa era ensinar o esqueleto a se mover em uma determinada direção a uma determinada velocidade.  Durante a simulação, a direção e a velocidade podem mudar. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg"><br><br>  Para obter um modelo de controle do esqueleto virtual, foi proposto o uso de aprendizado por reforço.  O simulador nos deu algum estado do esqueleto S (um vetor de ~ 400 números).  Era necessário prever que ação A precisa ser executada (as forças de ativação dos músculos das pernas são um vetor de 19 números).  No decorrer da simulação, o esqueleto recebeu um prêmio R - como uma espécie de constante menos uma penalidade por desviar-se de uma determinada velocidade e direção. <br><div class="spoiler">  <b class="spoiler_title">Sobre treinamento de reforço</b> <div class="spoiler_text"><p>  Aprendizado por Reforço (RL) é uma área que lida com a teoria da decisão e a busca de políticas comportamentais ideais. </p><br><p>  Lembre-se de como eles ensinam <del>  gato </del>  truques novos do cachorrinho.  Repita alguma ação, dê um gostoso por executar um truque e não dê por não cumprimento.  O cão deve entender tudo isso e encontrar uma estratégia comportamental ("política" ou "política" em termos de RL), que maximize o número de doces recebidos. </p><br><p>  Formalmente, temos um agente (cão) treinado na história das interações com o meio ambiente (pessoa).  Ao mesmo tempo, o ambiente, avaliando as ações do agente, oferece a ele uma recompensa (deliciosa) - quanto melhor o comportamento do agente, maior a recompensa.  Consequentemente, a tarefa do agente é encontrar uma política que maximize bem a recompensa por todo o tempo de interação com o ambiente. </p><br><p>  Desenvolvendo esse tópico ainda mais, soluções baseadas em regras - software 1.0, quando todas as regras foram definidas pelo desenvolvedor, aprendizado supervisionado - o próprio software 2.0, quando o sistema aprende a si próprio usando os exemplos disponíveis e encontra dependências de dados, o aprendizado por reforço é um passo um pouco mais longe quando o próprio sistema aprende a pesquisar, experimentar e encontrar as dependências necessárias em suas decisões.  Quanto mais avançamos, melhor tentamos repetir como uma pessoa aprende. </p></div></div><br><h3 id="osobennosti-zadachi">  Recursos da tarefa </h3><br><p>  A tarefa se parece com um representante típico do aprendizado reforçado para tarefas com espaço de ação contínuo (RL para espaço de ação contínua).  Difere da RL comum, pois, em vez de escolher uma ação específica (pressionando o botão do joystick), essa ação é necessária para prever com precisão (e existem infinitas possibilidades). </p><br><p>  A abordagem básica da solução ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deep Deterministic Policy Gradient</a> ) foi inventada em 2015, que por um longo tempo pelos padrões da DL, a região continua a se desenvolver ativamente em aplicativos para robótica e aplicativos RL do mundo real.  Há algo a ser aprimorado: abordagens robustas (para não quebrar um robô real), eficiência da amostra (para não coletar dados de robôs reais por meses) e outros problemas de RL (trade-off de exploração x exploração, etc.).  Nesta competição, eles não nos deram um robô real - apenas uma simulação, mas o simulador em si era 2.000 vezes mais lento que os de código aberto (nos quais todos verificam seus algoritmos de RL) e, portanto, trouxe o problema da eficiência da amostra para um novo nível. </p><br><h3 id="etapy-sorevnovaniya">  Etapas da competição </h3><br><p>  A competição ocorreu em três etapas, durante as quais a tarefa e as condições mudaram um pouco. </p><br><ul><li>  Etapa 1: o esqueleto aprendeu a andar em linha reta a uma velocidade de 3 metros por segundo.  A tarefa foi considerada concluída se o agente passou por 300 etapas. </li><li>  Etapa 2: velocidade e direção alteradas com uma frequência regular.  O comprimento da distância aumentou para 1000 etapas. </li><li>  Etapa 3: a solução final teve que ser empacotada em uma imagem do docker e enviada para verificação.  No total, 10 parcelas podem ser feitas. </li></ul><br><p>  A principal métrica de qualidade foi considerada a recompensa total pela simulação, que mostrou o quão bem o esqueleto aderiu a uma determinada direção e velocidade ao longo da distância. </p><br><p>  Durante a 1ª e a 2ª etapa, o progresso de cada participante foi exibido na tabela de classificação.  A solução final precisava ser enviada como uma imagem do docker.  Previa restrições quanto ao horário e recursos de trabalho. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: ranking público e RL</b> <div class="spoiler_text"><p>  Devido à disponibilidade da tabela de classificação, ninguém mostra o seu melhor modelo para dar "um pouco mais do que o habitual" na rodada final e surpreender os rivais. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  Por que as imagens do docker são tão importantes </h6><br><p>  No ano passado, ocorreu um pequeno incidente ao avaliar as decisões no primeiro turno.  Naquela época, a verificação passou pela interação http com a plataforma, e uma face das condições para o teste foi encontrada.  Pode-se descobrir em quais situações particulares o agente foi avaliado e treiná-lo novamente nessas condições.  O que, é claro, não resolveu o problema real.  Por isso, eles decidiram transferir o sistema de envios para imagens de encaixe e iniciá-lo nos servidores remotos dos organizadores.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dbrain</a> usa o mesmo sistema para calcular o resultado de competições precisamente pelas mesmas razões. </p><br><h1 id="klyuchevye-momenty">  Pontos-chave </h1><br><h3 id="komanda">  A equipe </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  A primeira coisa que é importante para o sucesso de toda a empresa é a equipe.  Não importa quão bom você seja (e quão poderoso seja suas patas) - a participação na equipe aumenta muito as chances de sucesso.  A razão é simples - uma variedade de opiniões e abordagens, revisando hipóteses, a capacidade de paralelizar o trabalho e realizar mais experimentos.  Tudo isso é extremamente importante na solução de novos problemas que você precisa enfrentar. <br><p>  Idealmente, seus conhecimentos e habilidades devem estar no mesmo nível e se complementam.  Por exemplo, este ano, plantei nossa equipe no PyTorch e tive algumas idéias iniciais sobre a implementação de um sistema de treinamento de agentes distribuídos. </p><br><p>  Como encontrar uma equipe?  Em primeiro lugar, você pode se juntar às fileiras dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ods</a> e procurar pessoas com a mesma opinião.  Em segundo lugar, para os bolsistas da RL há uma sala de bate-papo separada em um telegrama - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">clube da RL</a> .  Em terceiro lugar, você pode fazer um curso maravilhoso com o ShAD - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Practical RL</a> , após o qual você certamente conhecerá alguns conhecidos. </p><br><p>  No entanto, vale lembrar a política de "submissão - ou não era".  Se você quiser se unir, primeiro tome sua decisão, envie, apareça na tabela de classificação e mostre seu nível.  Como mostra a prática, essas equipes são muito mais equilibradas. </p><br><h3 id="motivaciya">  Motivação </h3><br><p>  Como já escrevi, se o tópico for “seu”, nada o impedirá.  Isso significa que a região não gosta apenas de você, mas a inspira - você a queima, quer se tornar a melhor. <br>  Conheci a RL há quatro anos - durante a passagem do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Berkeley 188x - Introdução à IA</a> - e ainda não consigo parar de me perguntar sobre o progresso nessa área. </p><br><h3 id="sistematichnost">  Sistemática </h3><br><p>  Terceiro, mas igualmente importante - você deve ser capaz de fazer o que prometeu, investir na competição todos os dias e apenas ... resolvê-la.  Todo dia  Nenhum talento inato pode ser comparado com a capacidade de fazer algo, mesmo que um pouco, mas todos os dias.  É para isso que a motivação será necessária.  Para ter sucesso, recomendo a leitura do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepWork</a> e da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AMA ternaus</a> . </p><br><h3 id="time-management">  Gerenciamento de tempo </h3><br><p>  Outra habilidade extremamente importante é a capacidade de distribuir a força e usar corretamente o tempo livre.  Combinar trabalho em tempo integral e participação em competições é uma tarefa não trivial.  O mais importante nessas condições é não queimar e suportar toda a carga.  Para fazer isso, você precisa gerenciar adequadamente seu tempo, avaliar sobriamente sua força e não se esqueça de relaxar na hora certa. </p><br><h3 id="overwork">  Excesso de trabalho </h3><br><p>  Na fase final da competição, geralmente surge uma situação em que literalmente em uma semana você precisa fazer não muito, mas MUITO.  Para o melhor resultado, você precisa se forçar a sentar e dar o último impulso ao cobiçado prêmio. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: prazo após prazo</b> <div class="spoiler_text"><p>  Por que, em geral, pode ser necessário reciclar para o benefício da competição?  A resposta é bastante simples - transferência de prazo.  Nessas competições, os organizadores geralmente não conseguem prever tudo, por isso a maneira mais fácil é dar mais tempo aos participantes.  Este ano, a competição foi prorrogada três vezes: primeiro por um mês, depois por uma semana e no último momento (24 horas antes do prazo) - por mais 2 dias.  E se durante as duas primeiras transferências você apenas precisou organizar o tempo extra corretamente, nos últimos dois dias você apenas teve que arar. </p></div></div><br><h3 id="theory">  Teoria </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Entre outras coisas, não se esqueça da teoria - estar ciente do que está acontecendo no campo e ser capaz de observar o relevante.  Por exemplo, para resolver no ano passado, nossa equipe decolou dos seguintes artigos: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O controle contínuo com o aprendizado por reforço profundo</a> é um artigo básico sobre o aprendizado por reforço profundo para tarefas com espaço de ação contínuo. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parâmetro Noise Space for Exploration</a> - um estudo sobre a adição de ruído aos pesos dos agentes para um melhor estudo do ambiente.  Por experiência - uma das melhores técnicas de exploração em RL. </li></ul><br><p>  Este ano, mais alguns foram adicionados a eles: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma perspectiva distributiva do aprendizado por reforço</a> - Um novo olhar sobre as previsões de uma possível recompensa.  Em vez de simplesmente prever a média, é calculada a distribuição de recompensas futuras. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O aprendizado de reforço distributivo com regressão quantílica</a> é uma continuação do trabalho anterior, mas com a "quantização" da distribuição. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Replay da Experiência Priorizada Distribuída</a> - trabalhe a partir da direção do aprendizado profundo por reforço em escala.  Sobre como organizar adequadamente a arquitetura do experimento para maximizar o uso dos recursos disponíveis e aumentar a velocidade dos agentes de treinamento. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gradientes de política determinística distributiva distribuída</a> - uma combinação dos três artigos anteriores para tarefas com um espaço de ação contínuo. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tratamento de erros de aproximação de funções em métodos críticos para atores</a> - excelente trabalho para aumentar a robustez dos agentes RL.  Eu recomendo a leitura. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O aprendizado de reforço hierárquico com eficiência de dados</a> é um desenvolvimento de um artigo anterior no campo do aprendizado de reforço hierárquico (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Leitura adicional</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Critico para o ator suave: aprendizado de reforço profundo por entropia máxima fora da política com um ator estocástico</a> - os autores propuseram um método para o treinamento de políticas estocásticas com o aprendizado de reforço fora da política.  Graças a este artigo, tornou-se possível formar políticos não deterministas, mesmo em tarefas com um espaço de ação contínuo. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">As políticas de espaço latente para o aprendizado de reforço hierárquico</a> são uma continuação de um artigo anterior do HRL com políticas estocásticas de vários níveis. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Diversidade é tudo o que você precisa: habilidades de aprendizado sem uma função de recompensa</a> - este artigo contém uma abordagem para aprender muitas políticas estocásticas aleatórias de baixo nível sem nenhuma recompensa do ambiente.  Posteriormente, quando definimos a função de recompensa, o mais correlacionado ao prêmio pode ser usado para ensinar política de alto nível. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado e controle de reforço como inferência probabilística: tutorial e revisão</a> - uma visão geral de todos os tipos de métodos de aprendizado de reforço de entropia máxima de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sergey Levine</a> . </li></ul><br><p>  Também aconselho à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI uma seleção de artigos</a> sobre aprendizado por reforço e sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">versão para mendeley</a> .  E se você estiver interessado no tópico de treinamento por reforço, junte-se aos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentos do</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">clube</a> e da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RL</a> . </p></div></div><br><h3 id="practice">  Prática </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Conhecer a teoria por si só não é suficiente - é importante poder colocar todas essas abordagens em prática e estabelecer o sistema de validação correto para avaliar decisões.  Por exemplo, este ano descobrimos que nosso agente lida mal com alguns casos regionais apenas 2 dias antes do final da competição.  Por causa disso, não tivemos tempo para consertar completamente nosso modelo e não conseguimos literalmente alguns pontos para o cobiçado segundo lugar.  Se descobríssemos isso mesmo em uma semana - o resultado poderia ser melhor. <br><div class="spoiler">  <b class="spoiler_title">História legal: episódio III</b> <div class="spoiler_text"><p>  O prêmio médio para 10 episódios de teste serviu como avaliação final da solução. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  O gráfico mostra os resultados do teste do nosso agente: 9 de 10 episódios, nosso esqueleto correu bem (média - 9955,66), mas um episódio ... O episódio 3 não foi dado a ele (recompensa 9870).  Foi esse erro que levou à queda da velocidade final para 9947 (-8 pontos). </p></div></div><br><h3 id="udacha">  Boa sorte </h3><br><p>  E finalmente - não se esqueça da sorte banal.  Não pense que este é um ponto controverso.  Pelo contrário, um pouco de sorte contribui muito para o trabalho constante: mesmo que a probabilidade de sorte seja de apenas 10%, uma pessoa que tentou participar da competição 100 vezes terá muito mais sucesso do que alguém que tentou apenas uma vez e abandonou a ideia. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  Ida e volta: decisão do ano passado - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">terceiro lugar</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  No ano passado, nossa equipe - Mikhail Pavlov e eu - participamos das competições NeurIPS pela primeira vez e a principal motivação era simplesmente participar da primeira competição NeurIPS no aprendizado por reforço.  Acabei de terminar o curso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RL prática</a> no SHAD e queria testar as habilidades adquiridas.  Como resultado, conquistamos um terceiro lugar honroso, perdendo apenas para o nnaisene (Schmidhuber) e a equipe universitária da China.  Naquela época, nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">solução</a> era "bastante simples" e baseava-se no DDPG distribuído com parâmetros de ruído ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">publicação</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">apresentação em ml</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trainings</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  A decisão deste ano é o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">terceiro lugar</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  Houve algumas mudanças este ano.  Em primeiro lugar, não havia desejo de apenas participar desta competição, eu queria vencê-la.  Em segundo lugar, a composição da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">equipe</a> também mudou: Alexey Grinchuk, Anton Pechenko e eu.  Pegue e ganhe - não funcionou, mas novamente conquistamos o 3º lugar. <br>  Nossa solução será apresentada oficialmente no NeurIPS e agora nos limitaremos a um pequeno número de detalhes.  Com base na decisão do ano passado e no sucesso do aprendizado de reforço fora da política deste ano (artigos acima), adicionamos vários desenvolvimentos próprios, sobre os quais falaremos no NeurIPS, e obtivemos o Critic Distributed Quantile Ensemble, com o qual ocupamos o terceiro lugar. </p><br><p>  Todas as nossas práticas recomendadas - um sistema de aprendizado distribuído, algoritmos etc. serão publicadas e disponíveis no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Catalyst.RL</a> após o NeurIPS. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: meninos grandes - armas grandes</b> <div class="spoiler_text"><p>  Nossa equipe confiantemente foi para o 1º lugar durante toda a competição.  No entanto, os grandões tinham outros planos - dois grandes jogadores entraram na competição duas semanas antes do final da competição: FireWork (Baidu) e nnaisense (Schmidhuber).  E se não pudéssemos fazer nada com o Google chinês, então com a equipe Schmidhuber por um bom tempo conseguimos lutar honestamente pelo segundo lugar, perdendo apenas com uma margem mínima.  Parece-me muito bom para os amantes. </p></div></div><br><h1 id="zachem-eto-vse">  Por que isso é tudo? </h1><br><ul><li>  Comunicação.  Os principais pesquisadores vêm à conferência com quem você pode conversar ao vivo, o que não fornecerá nenhuma correspondência por e-mail. </li><li>  Publicação  Se a solução receber o prêmio, a equipe será convidada para a conferência (ou talvez mais de uma) para apresentar sua decisão e publicar o artigo. </li><li>  Oferta de emprego e doutorado.  A publicação e o prêmio em uma conferência desse tipo aumentam significativamente suas chances de conseguir uma posição em empresas líderes como OpenAI, DeepMind, Google, Facebook, Microsoft. </li><li>  Valor do mundo real.  O NeurIPS é realizado para solucionar problemas prementes do mundo acadêmico e real.  Você pode ter certeza de que os resultados não irão para a mesa, mas serão realmente procurados e ajudarão a melhorar o mundo. </li><li>  Drive  Resolver tais concursos ... apenas interessante.  Em uma competição, você pode ter muitas idéias novas, testar diferentes abordagens - apenas para ser o melhor.  E vamos ser honestos, quando mais você pode dirigir esqueletos, jogar e tudo isso com um olhar sério e pelo bem da ciência? </li></ul><br><div class="spoiler">  <b class="spoiler_title">História legal: visto e RL</b> <div class="spoiler_text"><p>  Eu não recomendo tentar explicar ao americano que você está indo para a conferência, enquanto treina esqueletos virtuais para executar simulações.  Basta ir à conferência com uma palestra. </p></div></div><br><h1 id="itogi">  Sumário </h1><br><p>  Participar do NeurIPS é uma experiência difícil de superestimar.  Não tenha medo de títulos de destaque: basta se recompor e começar a decidir. </p><br><p>  E vá para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Catalyst.RL</a> , então o que. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt430712/">https://habr.com/ru/post/pt430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt430702/index.html">Treinamento muito estranho</a></li>
<li><a href="../pt430704/index.html">Como as tecnologias de inteligência artificial ajudam a Aviasales a crescer: sete exemplos</a></li>
<li><a href="../pt430706/index.html">Nova teoria da evolução</a></li>
<li><a href="../pt430708/index.html">Tic Tac Toe “Sem Fronteiras”</a></li>
<li><a href="../pt430710/index.html">O que fazer se a Black Friday for amanhã e seus servidores não estiverem prontos</a></li>
<li><a href="../pt430714/index.html">VMware compra Heptio - o que isso significa para o Kubernetes</a></li>
<li><a href="../pt430718/index.html">Para quais objetos vale a pena usar a vigilância por vídeo na nuvem?</a></li>
<li><a href="../pt430720/index.html">Intel RealSense D435i: pequena atualização e breve digressão histórica</a></li>
<li><a href="../pt430722/index.html">Desempenho do PHP: planejamento, criação de perfil, otimização</a></li>
<li><a href="../pt430724/index.html">DEFCON 21. A conferência DNS pode ser perigosa para sua saúde. Parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>