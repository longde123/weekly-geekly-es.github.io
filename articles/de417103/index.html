<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍋 👖 👇🏽 Spark SQL. Ein bisschen über das Abfrageoptimierungsprogramm 👩🏻‍🎤 📓 🚌</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo an alle. Als Einführung möchte ich Ihnen erzählen, wie ich zu einem solchen Leben gekommen bin. 



 Insbesondere vor dem Treffen mit Big Data u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Spark SQL. Ein bisschen über das Abfrageoptimierungsprogramm</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neoflex/blog/417103/"><p>  Hallo an alle.  Als Einführung möchte ich Ihnen erzählen, wie ich zu einem solchen Leben gekommen bin. <br></p><br><p>  Insbesondere vor dem Treffen mit Big Data und Spark musste ich häufig SQL-Abfragen optimieren, zuerst für MSSQL, dann für Oracle, und jetzt bin ich auf SparkSQL gestoßen. <br></p><br><p>  Und wenn es bereits viele gute Bücher für das DBMS gibt, die die Methodik und die „Stifte“ beschreiben, die Sie drehen können, um den optimalen Abfrageplan zu erhalten, habe ich solche Bücher für Spark nicht gesehen.  Ich bin auf mehr Artikel und Vorgehensweisen gestoßen, die sich eher auf die Arbeit mit der RDD / Dataset-API als auf reines SQL beziehen.  Für mich ist eines der Nachschlagewerke zur SQL-Optimierung das Buch Oracle von J. Lewis.  Grundlagen der Kostenoptimierung. "  Ich suchte nach etwas Ähnlichem in der Tiefe des Studiums.  Warum war das Forschungsgebiet speziell SparkSQL und nicht die zugrunde liegende API?  Dann wurde das Interesse durch die Funktionen des Projekts verursacht, an dem ich arbeite. <br></p><br><img src="https://habrastorage.org/webt/po/1f/un/po1fun6vgbktou6lykepwmrncci.jpeg"><br><a name="habracut"></a><br><p>  Für einen unserer Kunden entwickelt unser Unternehmen ein Data Warehouse, von dem sich eine detaillierte Schicht und ein Teil der Vitrinen im Hadoop-Cluster und die endgültigen Vitrinen in Oracle befinden.  Dieses Projekt umfasst eine umfangreiche Datenkonvertierungsschicht, die auf Spark implementiert ist.  Um die Entwicklung und Konnektivität von ETL-Entwicklern zu beschleunigen, die nicht mit den Feinheiten der Big Data-Technologien vertraut sind, aber mit SQL- und ETL-Tools vertraut sind, wurde ein Tool entwickelt, das andere ETL-Tools, z. B. Informatica, ideologisch erinnert und es Ihnen ermöglicht, ETL-Prozesse mit nachfolgender Generation visuell zu entwerfen Code für Spark.  Aufgrund der Komplexität der Algorithmen und der großen Anzahl von Transformationen verwenden Entwickler hauptsächlich SparkSQL-Abfragen. <br></p><br><p> Und hier beginnt die Geschichte, da ich eine große Anzahl von Fragen des Formulars „Warum funktioniert die Anfrage nicht / arbeitet langsam / funktioniert sie anders als Oracle?“ Beantworten musste.  Dieser erwies sich für mich als der interessanteste Teil: „Warum funktioniert er langsam?“.  Im Gegensatz zu dem DBMS, mit dem ich zuvor gearbeitet habe, können Sie außerdem in den Quellcode gelangen und die Antwort auf Ihre Fragen erhalten. <br></p><cut text="    "></cut><br><h2>  Einschränkungen und Annahmen </h2><br><p>  Mit Spark 2.3.0 werden Beispiele ausgeführt und der Quellcode analysiert. <br>  Es wird davon ausgegangen, dass der Leser mit der Spark-Architektur und den allgemeinen Prinzipien des Abfrageoptimierers für eines der DBMS vertraut ist.  Zumindest sollte der Ausdruck "Abfrageplan" sicherlich nicht überraschend sein. <br></p><br><p>  Außerdem versucht dieser Artikel, keine Übersetzung des Spark-Optimierungscodes ins Russische zu werden. Für Dinge, die aus Sicht des Optimierers sehr interessant sind, aber im Quellcode gelesen werden können, werden sie hier einfach kurz mit Links zu den entsprechenden Klassen erwähnt. <br></p><br><h2>  Mach weiter mit dem Lernen </h2><br><p>  Beginnen wir mit einer kleinen Abfrage, um die grundlegenden Phasen zu untersuchen, die vom Parsen bis zur Ausführung durchlaufen werden. <br></p><br><pre><code class="scala hljs">scala&gt; spark.read.orc(<span class="hljs-string"><span class="hljs-string">"/user/test/balance"</span></span>).createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"bal"</span></span>) scala&gt; spark.read.orc(<span class="hljs-string"><span class="hljs-string">"/user/test/customer"</span></span>).createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"cust"</span></span>) scala&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> df = spark.sql(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">" | select bal.account_rk, cust.full_name | from bal | join cust | on bal.party_rk = cust.party_rk | and bal.actual_date = cust.actual_date | where bal.actual_date = cast('2017-12-31' as date) | "</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>) df: org.apache.spark.sql.<span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = [account_rk: decimal(<span class="hljs-number"><span class="hljs-number">38</span></span>,<span class="hljs-number"><span class="hljs-number">18</span></span>), full_name: string] scala&gt; df.explain(<span class="hljs-literal"><span class="hljs-literal">true</span></span>)</code> </pre> <br><p>  Das Hauptmodul, das für das Parsen von SQL und die Optimierung des Abfrageausführungsplans verantwortlich ist, ist Spark Catalyst. <br></p><br><p>  Mit der erweiterten Ausgabe in der Beschreibung des Anforderungsplans (df.explain (true)) können Sie alle Phasen verfolgen, die die Anforderung durchläuft: <br></p><br><ul><li>  Analysierter logischer Plan - Nach dem Parsen von SQL abrufen.  In dieser Phase wird nur die syntaktische Richtigkeit der Anforderung überprüft. </li></ul><br><pre> <code class="hljs rust">== Parsed Logical Plan == <span class="hljs-symbol"><span class="hljs-symbol">'Project</span></span> [<span class="hljs-symbol"><span class="hljs-symbol">'bal</span></span>.account_rk, <span class="hljs-symbol"><span class="hljs-symbol">'cust</span></span>.full_name] +- <span class="hljs-symbol"><span class="hljs-symbol">'Filter</span></span> (<span class="hljs-symbol"><span class="hljs-symbol">'bal</span></span>.actual_date = cast(<span class="hljs-number"><span class="hljs-number">2017</span></span>-<span class="hljs-number"><span class="hljs-number">12</span></span>-<span class="hljs-number"><span class="hljs-number">31</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> date)) +- <span class="hljs-symbol"><span class="hljs-symbol">'Join</span></span> Inner, ((<span class="hljs-symbol"><span class="hljs-symbol">'bal</span></span>.party_rk = <span class="hljs-symbol"><span class="hljs-symbol">'cust</span></span>.party_rk) &amp;&amp; (<span class="hljs-symbol"><span class="hljs-symbol">'bal</span></span>.actual_date = <span class="hljs-symbol"><span class="hljs-symbol">'cust</span></span>.actual_date)) :- <span class="hljs-symbol"><span class="hljs-symbol">'UnresolvedRelation</span></span> `bal` +- <span class="hljs-symbol"><span class="hljs-symbol">'UnresolvedRelation</span></span> `cust`</code> </pre><br><ul><li>  Analysierter logischer Plan - In dieser Phase werden Informationen zur Struktur der verwendeten Entitäten hinzugefügt, die Übereinstimmung der Struktur und der angeforderten Attribute überprüft. </li></ul><br><pre> <code class="hljs delphi">== Analyzed Logical Plan == account_rk: decimal(<span class="hljs-number"><span class="hljs-number">38</span></span>,<span class="hljs-number"><span class="hljs-number">18</span></span>), full_name: <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> Project [account_rk<span class="hljs-string"><span class="hljs-string">#1</span></span>, full_name<span class="hljs-string"><span class="hljs-string">#59</span></span>] +- Filter (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = cast(<span class="hljs-number"><span class="hljs-number">2017</span></span>-<span class="hljs-number"><span class="hljs-number">12</span></span>-<span class="hljs-number"><span class="hljs-number">31</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> date)) +- Join Inner, ((party_rk<span class="hljs-string"><span class="hljs-string">#18</span></span> = party_rk<span class="hljs-string"><span class="hljs-string">#57</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span>)) :- SubqueryAlias bal : +- Relation[ACTUAL_END_DATE<span class="hljs-string"><span class="hljs-string">#0</span></span>,ACCOUNT_RK<span class="hljs-string"><span class="hljs-string">#1</span></span>,... <span class="hljs-number"><span class="hljs-number">4</span></span> more fields] orc +- SubqueryAlias cust +- Relation[ACTUAL_END_DATE<span class="hljs-string"><span class="hljs-string">#56</span></span>,PARTY_RK<span class="hljs-string"><span class="hljs-string">#57</span></span>... <span class="hljs-number"><span class="hljs-number">9</span></span> more fields] orc</code> </pre><br><ul><li>  Der optimierte logische Plan ist für uns am interessantesten.  In dieser Phase wird der resultierende Abfragebaum basierend auf den verfügbaren Optimierungsregeln konvertiert. </li></ul><br><pre> <code class="hljs delphi">== Optimized Logical Plan == Project [account_rk<span class="hljs-string"><span class="hljs-string">#1</span></span>, full_name<span class="hljs-string"><span class="hljs-string">#59</span></span>] +- Join Inner, ((party_rk<span class="hljs-string"><span class="hljs-string">#18</span></span> = party_rk<span class="hljs-string"><span class="hljs-string">#57</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span>)) :- Project [ACCOUNT_RK<span class="hljs-string"><span class="hljs-string">#1</span></span>, PARTY_RK<span class="hljs-string"><span class="hljs-string">#18</span></span>, ACTUAL_DATE<span class="hljs-string"><span class="hljs-string">#27</span></span>] : +- Filter ((isnotnull(actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>)) &amp;&amp; isnotnull(party_rk<span class="hljs-string"><span class="hljs-string">#18</span></span>)) : +- Relation[ACTUAL_END_DATE<span class="hljs-string"><span class="hljs-string">#0</span></span>,ACCOUNT_RK<span class="hljs-string"><span class="hljs-string">#1</span></span>,... <span class="hljs-number"><span class="hljs-number">4</span></span> more fields] orc +- Project [PARTY_RK<span class="hljs-string"><span class="hljs-string">#57</span></span>, FULL_NAME<span class="hljs-string"><span class="hljs-string">#59</span></span>, ACTUAL_DATE<span class="hljs-string"><span class="hljs-string">#88</span></span>] +- Filter ((isnotnull(actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span>) &amp;&amp; isnotnull(party_rk<span class="hljs-string"><span class="hljs-string">#57</span></span>)) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>)) +- Relation[ACTUAL_END_DATE<span class="hljs-string"><span class="hljs-string">#56</span></span>,PARTY_RK<span class="hljs-string"><span class="hljs-string">#57</span></span>,... <span class="hljs-number"><span class="hljs-number">9</span></span> more fields] orc</code> </pre><br><ul><li>  Physischer Plan - Funktionen für den Zugriff auf Quelldaten werden zunehmend berücksichtigt, einschließlich Optimierungen zum Filtern von Partitionen und Daten, um den resultierenden Datensatz zu minimieren.  Die Join-Ausführungsstrategie ist ausgewählt (mehr zu den unten verfügbaren Optionen). </li></ul><br><pre> <code class="hljs pgsql">== Physical Plan == *(<span class="hljs-number"><span class="hljs-number">2</span></span>) Project [account_rk#<span class="hljs-number"><span class="hljs-number">1</span></span>, full_name#<span class="hljs-number"><span class="hljs-number">59</span></span>] +- *(<span class="hljs-number"><span class="hljs-number">2</span></span>) BroadcastHashJoin [party_rk#<span class="hljs-number"><span class="hljs-number">18</span></span>, actual_date#<span class="hljs-number"><span class="hljs-number">27</span></span>], [party_rk#<span class="hljs-number"><span class="hljs-number">57</span></span>, actual_date#<span class="hljs-number"><span class="hljs-number">88</span></span>], <span class="hljs-keyword"><span class="hljs-keyword">Inner</span></span>, BuildRight :- *(<span class="hljs-number"><span class="hljs-number">2</span></span>) Project [ACCOUNT_RK#<span class="hljs-number"><span class="hljs-number">1</span></span>, PARTY_RK#<span class="hljs-number"><span class="hljs-number">18</span></span>, ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">27</span></span>] : +- *(<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">Filter</span></span> isnotnull(party_rk#<span class="hljs-number"><span class="hljs-number">18</span></span>) : +- *(<span class="hljs-number"><span class="hljs-number">2</span></span>) FileScan orc [ACCOUNT_RK#<span class="hljs-number"><span class="hljs-number">1</span></span>,PARTY_RK#<span class="hljs-number"><span class="hljs-number">18</span></span>,ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">27</span></span>] Batched: <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">Format</span></span>: ORC, <span class="hljs-keyword"><span class="hljs-keyword">Location</span></span>: InMemoryFileIndex[hdfs://<span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>:<span class="hljs-number"><span class="hljs-number">8020</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/test/balance], PartitionCount: <span class="hljs-number"><span class="hljs-number">1</span></span>, PartitionFilters: [isnotnull(ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">27</span></span>), (ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">27</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>)], PushedFilters: [IsNotNull(PARTY_RK)], ReadSchema: struct&lt;ACCOUNT_RK:<span class="hljs-type"><span class="hljs-type">decimal</span></span>(<span class="hljs-number"><span class="hljs-number">38</span></span>,<span class="hljs-number"><span class="hljs-number">18</span></span>),PARTY_RK:<span class="hljs-type"><span class="hljs-type">decimal</span></span>(<span class="hljs-number"><span class="hljs-number">38</span></span>,<span class="hljs-number"><span class="hljs-number">18</span></span>)&gt; +- BroadcastExchange HashedRelationBroadcastMode(List(<span class="hljs-keyword"><span class="hljs-keyword">input</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-type"><span class="hljs-type">decimal</span></span>(<span class="hljs-number"><span class="hljs-number">38</span></span>,<span class="hljs-number"><span class="hljs-number">18</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>], <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>[<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-type"><span class="hljs-type">date</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>])) +- *(<span class="hljs-number"><span class="hljs-number">1</span></span>) Project [PARTY_RK#<span class="hljs-number"><span class="hljs-number">57</span></span>, FULL_NAME#<span class="hljs-number"><span class="hljs-number">59</span></span>, ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">88</span></span>] +- *(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">Filter</span></span> isnotnull(party_rk#<span class="hljs-number"><span class="hljs-number">57</span></span>) +- *(<span class="hljs-number"><span class="hljs-number">1</span></span>) FileScan orc [PARTY_RK#<span class="hljs-number"><span class="hljs-number">57</span></span>,FULL_NAME#<span class="hljs-number"><span class="hljs-number">59</span></span>,ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">88</span></span>] Batched: <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">Format</span></span>: ORC, <span class="hljs-keyword"><span class="hljs-keyword">Location</span></span>: InMemoryFileIndex[hdfs://<span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>:<span class="hljs-number"><span class="hljs-number">8020</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/test/customer], PartitionCount: <span class="hljs-number"><span class="hljs-number">1</span></span>, PartitionFilters: [isnotnull(ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">88</span></span>), (ACTUAL_DATE#<span class="hljs-number"><span class="hljs-number">88</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>)], PushedFilters: [IsNotNull(PARTY_RK)], ReadSchema: struct&lt;PARTY_RK:<span class="hljs-type"><span class="hljs-type">decimal</span></span>(<span class="hljs-number"><span class="hljs-number">38</span></span>,<span class="hljs-number"><span class="hljs-number">18</span></span>),FULL_NAME:string&gt;</code> </pre><br><p>  Die folgenden Phasen der Optimierung und Ausführung (z. B. WholeStageCodegen) gehen über den Rahmen dieses Artikels hinaus, werden jedoch (sowie die oben beschriebenen Phasen) in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mastering Spark Sql ausführlich beschrieben</a> . <br></p><br><p>  Das Lesen des Abfrageausführungsplans erfolgt normalerweise „von innen“ und „von unten nach oben“, dh die am meisten verschachtelten Teile werden zuerst ausgeführt und gelangen schrittweise zur endgültigen Projektion ganz oben. <br></p><br><h2>  Arten von Abfrageoptimierern </h2><br><p>  Es können zwei Arten von Abfrageoptimierern unterschieden werden: </p><br><ul><li>  Regelbasierte Optimierer (RBOs). </li><li>  Optimierer basierend auf einer Schätzung der Kosten für die Ausführung von Abfragen (Kostenbasierter Optimierer, CBO). </li></ul><br><p>  Die ersten konzentrieren sich auf die Verwendung eines Satzes fester Regeln, zum Beispiel die Anwendung von Filterbedingungen, aus denen in früheren Stadien, wenn möglich, die Berechnung von Konstanten usw. <br></p><br><p>  Um die Qualität des resultierenden Plans zu bewerten, verwendet der CBO-Optimierer eine Kostenfunktion, die normalerweise von der Menge der verarbeiteten Daten, der Anzahl der Zeilen, die unter die Filter fallen, und den Kosten für die Ausführung bestimmter Vorgänge abhängt. <br></p><br><p>  Um mehr über die CBO-Designspezifikation für Apache Spark zu erfahren, folgen Sie bitte den Links: der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spezifikation</a> und der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hauptaufgabe von JIRA für die Implementierung</a> . <br></p><br><p>  Der Ausgangspunkt für die Erkundung aller vorhandenen Optimierungen ist der Code Optimizer.scala. <br></p><br><p>  Hier ist ein kurzer Auszug aus einer langen Liste verfügbarer Optimierungen: <br></p><br><pre> <code class="hljs perl">def batches: Se<span class="hljs-string"><span class="hljs-string">q[Batch]</span></span> = { val operatorOptimizationRuleSet = Se<span class="hljs-string"><span class="hljs-string">q( // Operator push down PushProjectionThroughUnion, ReorderJoin, EliminateOuterJoin, PushPredicateThroughJoin, PushDownPredicate, LimitPushDown, ColumnPruning, InferFiltersFromConstraints, // Operator combine CollapseRepartition, CollapseProject, CollapseWindow, CombineFilters, CombineLimits, CombineUnions, // Constant folding and strength reduction NullPropagation, ConstantPropagation, ........</span></span></code> </pre><br><p>  Es ist zu beachten, dass die Liste dieser Optimierungen sowohl regelbasierte Optimierungen als auch Optimierungen basierend auf Abfragekostenschätzungen enthält, die nachstehend erläutert werden. <br></p><br><p>  Ein Merkmal von CBO ist, dass es für einen korrekten Betrieb Informationen über die Statistiken der in der Abfrage verwendeten Daten kennen und speichern muss - Anzahl der Datensätze, Datensatzgröße, Histogramme der Datenverteilung in den Tabellenspalten. <br></p><br><p>  Zum Sammeln von Statistiken wird eine Reihe von SQL-Befehlen ANALYZE TABLE ... COMPUTE STATISTICS verwendet. Außerdem wird eine Reihe von Tabellen zum Speichern von Informationen benötigt. Die API wird über ExternalCatalog bereitgestellt, genauer gesagt über HiveExternalCatalog. <br></p><br><p>  Da CBO derzeit standardmäßig deaktiviert ist, liegt der Schwerpunkt auf der Untersuchung der verfügbaren Optimierung und Nuancen von RBO. <br></p><br><h2>  Typen und Auswahl der Join-Strategie </h2><br><p>  In der Phase der Erstellung des physischen Plans zur Ausführung der Anforderung wird die Verbindungsstrategie ausgewählt.  Die folgenden Optionen sind derzeit in Spark verfügbar (Sie können Code aus dem Code in SparkStrategies.scala lernen). <br></p><br><h3>  Broadcast-Hash-Join </h3><br><p>  Die beste Option ist, wenn eine der Join-Parteien klein genug ist (das Suffizienzkriterium wird durch den Parameter spark.sql.autoBroadcastJoinThreshold in SQLConf festgelegt).  In diesem Fall wird diese Seite vollständig auf alle Executoren kopiert, bei denen ein Hash-Join mit der Haupttabelle vorhanden ist.  Zusätzlich zur Größe sollte beachtet werden, dass im Fall einer äußeren Verknüpfung nur die Außenseite kopiert werden kann. Wenn möglich, müssen Sie als führende Tabelle im Fall einer äußeren Verknüpfung die Tabelle mit der größten Datenmenge verwenden. <br></p><br><pre> <code class="hljs pgsql">  ,    ,     <span class="hljs-keyword"><span class="hljs-keyword">SQL</span></span>      Oracle,   <span class="hljs-comment"><span class="hljs-comment">/*+ broadcast(t1, t2) */</span></span></code> </pre><br><h3>  Sort Merge Join </h3><br><p>  <em>Wenn spark.sql.join.preferSortMergeJoin</em> standardmäßig aktiviert ist, wird diese Methode standardmäßig angewendet, wenn die Schlüssel für den Join sortiert werden können. <br>  Von den Merkmalen kann angemerkt werden, dass im Gegensatz zum vorherigen Verfahren die Codegenerierungsoptimierung zum Ausführen der Operation nur für die innere Verknüpfung verfügbar ist. <br></p><br><h3>  Shuffle Hash Join </h3><br><p>  Wenn die Schlüssel nicht sortiert werden können oder die Standardauswahloption für Sortierzusammenführungsverknüpfungen deaktiviert ist, versucht Catalyst, einen Shuffle-Hash-Join anzuwenden.  Zusätzlich zur Überprüfung der Einstellungen wird auch überprüft, ob Spark über genügend Speicher verfügt, um eine lokale Hash-Map für eine Partition zu erstellen (die Gesamtzahl der Partitionen wird durch Festlegen von <em>spark.sql.shuffle.partitions festgelegt</em> ). </p><br><h3>  BroadcastNestedLoopJoin und CartesianProduct </h3><br><p>  In dem Fall, in dem keine Möglichkeit eines direkten Vergleichs nach Schlüsseln besteht (z. B. eine Bedingung wie) oder keine Schlüssel zum Verknüpfen von Tabellen vorhanden sind, wird abhängig von der Größe der Tabellen entweder dieser Typ oder CartesianProduct ausgewählt. <br></p><br><h3>  Die Reihenfolge der Angabe von Tabellen in join'ah </h3><br><p>  In jedem Fall erfordert der Join das Mischen von Tabellen nach Schlüssel.  Daher ist derzeit die Reihenfolge der Angabe von Tabellen wichtig, insbesondere bei der Ausführung mehrerer Verknüpfungen in einer Reihe (wenn Sie eine Bohrung sind, ist CBO nicht aktiviert und die Einstellung JOIN_REORDER_ENABLED ist nicht aktiviert). <br></p><br><p>  Wenn möglich, sollte die Reihenfolge der Verknüpfungstabellen die Anzahl der Mischvorgänge für große Tabellen minimieren, für die Verknüpfungen mit demselben Schlüssel nacheinander ausgeführt werden sollten.  Vergessen Sie auch nicht, die Daten für die Verknüpfung zu minimieren, um die Broadcast-Hash-Verknüpfung zu aktivieren. <br></p><br><h2>  Transitive Anwendung von Filterbedingungen </h2><br><p>  Betrachten Sie die folgende Abfrage: <br></p><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> bal.account_rk, cust.full_name <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> balance bal <span class="hljs-keyword"><span class="hljs-keyword">join</span></span> customer cust <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> bal.party_rk = cust.party_rk <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> bal.actual_date = cust.actual_date <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> bal.actual_date = <span class="hljs-keyword"><span class="hljs-keyword">cast</span></span>(<span class="hljs-string"><span class="hljs-string">'2017-12-31'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">date</span></span>)</code> </pre><br><p>  Hier verbinden wir zwei Tabellen, die gemäß dem Feld actual_date auf dieselbe Weise partitioniert sind, und wenden einen expliziten Filter nur auf die Partition gemäß der Balance-Tabelle an. <br></p><br><p>  Wie aus dem optimierten Abfrageplan hervorgeht, wird der Filter nach Datum auch auf den Kunden angewendet, und zum Zeitpunkt des Lesens von Daten von der Festplatte wird festgestellt, dass genau eine Partition benötigt wird. <br></p><br><pre> <code class="hljs delphi">== Optimized Logical Plan == Project [account_rk<span class="hljs-string"><span class="hljs-string">#1</span></span>, full_name<span class="hljs-string"><span class="hljs-string">#59</span></span>] +- Join Inner, ((party_rk<span class="hljs-string"><span class="hljs-string">#18</span></span> = party_rk<span class="hljs-string"><span class="hljs-string">#57</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span>)) :- Project [ACCOUNT_RK<span class="hljs-string"><span class="hljs-string">#1</span></span>, PARTY_RK<span class="hljs-string"><span class="hljs-string">#18</span></span>, ACTUAL_DATE<span class="hljs-string"><span class="hljs-string">#27</span></span>] : +- Filter ((isnotnull(actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>)) &amp;&amp; isnotnull(party_rk<span class="hljs-string"><span class="hljs-string">#18</span></span>)) : +- Relation[,... <span class="hljs-number"><span class="hljs-number">4</span></span> more fields] orc +- Project [PARTY_RK<span class="hljs-string"><span class="hljs-string">#57</span></span>, FULL_NAME<span class="hljs-string"><span class="hljs-string">#59</span></span>, ACTUAL_DATE<span class="hljs-string"><span class="hljs-string">#88</span></span>] +- Filter (((actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>) &amp;&amp; isnotnull(actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span>)) &amp;&amp; isnotnull(party_rk<span class="hljs-string"><span class="hljs-string">#57</span></span>)) +- Relation[,... <span class="hljs-number"><span class="hljs-number">9</span></span> more fields] orc</code> </pre><br><p>  Sie müssen jedoch nur den inneren Join durch den linken äußeren in der Abfrage ersetzen, da das Push-Prädikat für die Kundentabelle sofort abfällt und ein vollständiger Scan erfolgt, was ein unerwünschter Effekt ist. <br></p><br><pre> <code class="hljs delphi">== Optimized Logical Plan == Project [account_rk<span class="hljs-string"><span class="hljs-string">#1</span></span>, full_name<span class="hljs-string"><span class="hljs-string">#59</span></span>] +- Join LeftOuter, ((party_rk<span class="hljs-string"><span class="hljs-string">#18</span></span> = party_rk<span class="hljs-string"><span class="hljs-string">#57</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = actual_date<span class="hljs-string"><span class="hljs-string">#88</span></span>)) :- Project [ACCOUNT_RK<span class="hljs-string"><span class="hljs-string">#1</span></span>, PARTY_RK<span class="hljs-string"><span class="hljs-string">#18</span></span>, ACTUAL_DATE<span class="hljs-string"><span class="hljs-string">#27</span></span>] : +- Filter (isnotnull(actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span>) &amp;&amp; (actual_date<span class="hljs-string"><span class="hljs-string">#27</span></span> = <span class="hljs-number"><span class="hljs-number">17531</span></span>)) : +- Relation[,... <span class="hljs-number"><span class="hljs-number">4</span></span> more fields] orc +- Project [PARTY_RK<span class="hljs-string"><span class="hljs-string">#57</span></span>, FULL_NAME<span class="hljs-string"><span class="hljs-string">#59</span></span>, ACTUAL_DATE<span class="hljs-string"><span class="hljs-string">#88</span></span>] +- Relation[,... <span class="hljs-number"><span class="hljs-number">9</span></span> more fields] orc</code> </pre><br><h2>  Typkonvertierung </h2><br><p>  Betrachten Sie ein einfaches Beispiel für die Auswahl aus einer Tabelle mit Filterung nach Clienttyp. Im Schema ist der Typ des Felds party_type string. <br></p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> party_rk, full_name <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> cust <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> actual_date = cast(<span class="hljs-string"><span class="hljs-string">'2017-12-31'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-type"><span class="hljs-type">date</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> party_type = <span class="hljs-number"><span class="hljs-number">101</span></span> <span class="hljs-comment"><span class="hljs-comment">--   -- and party_type = '101' --    </span></span></code> </pre><br><p>  Und vergleichen Sie die beiden resultierenden Pläne, den ersten - wenn wir uns auf den falschen Typ beziehen (es wird eine implizite Umwandlung in int geben), den zweiten - wenn der Typ dem Schema entspricht. <br></p><br><pre> <code class="hljs powershell">PushedFilters: [<span class="hljs-type"><span class="hljs-type">IsNotNull</span></span>(<span class="hljs-type"><span class="hljs-type">PARTY_TYPE</span></span>)] //            . PushedFilters: [<span class="hljs-type"><span class="hljs-type">IsNotNull</span></span>(<span class="hljs-type"><span class="hljs-type">PARTY_TYPE</span></span>), <span class="hljs-type"><span class="hljs-type">EqualTo</span></span>(<span class="hljs-type"><span class="hljs-type">PARTY_TYPE</span></span>,<span class="hljs-number"><span class="hljs-number">101</span></span>)] //             .</code> </pre><br><p>  Ein ähnliches Problem wird beim Vergleichen von Datumsangaben mit einer Zeichenfolge beobachtet. Es gibt einen Filter zum Vergleichen von Zeichenfolgen.  Ein Beispiel: <br></p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">where</span></span> OPER_DATE = <span class="hljs-string"><span class="hljs-string">'2017-12-31'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Filter</span></span> (isnotnull(oper_date#<span class="hljs-number"><span class="hljs-number">0</span></span>) &amp;&amp; (cast(oper_date#<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> string) = <span class="hljs-number"><span class="hljs-number">2017</span></span><span class="hljs-number"><span class="hljs-number">-12</span></span><span class="hljs-number"><span class="hljs-number">-31</span></span>) PushedFilters: [IsNotNull(OPER_DATE)] <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> OPER_DATE = cast(<span class="hljs-string"><span class="hljs-string">'2017-12-31'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-type"><span class="hljs-type">date</span></span>) PushedFilters: [IsNotNull(OPER_DATE), EqualTo(OPER_DATE,<span class="hljs-number"><span class="hljs-number">2017</span></span><span class="hljs-number"><span class="hljs-number">-12</span></span><span class="hljs-number"><span class="hljs-number">-31</span></span>)]</code> </pre><br><p>  Für den Fall, dass eine implizite Typkonvertierung möglich ist, z. B. int -&gt; decimal, führt der Optimierer dies selbst aus. <br></p><br><h2>  Weitere Forschung </h2><br><p>  Viele interessante Informationen zu den „Reglern“, mit denen Catalyst fein eingestellt werden kann, sowie zu den Möglichkeiten (Gegenwart und Zukunft) des Optimierers erhalten Sie bei SQLConf.scala. <br></p><br><p>  Wie Sie standardmäßig sehen können, ist das Kostenoptimierungsprogramm derzeit noch deaktiviert. <br></p><br><pre> <code class="hljs scala"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">CBO_ENABLED</span></span> = buildConf(<span class="hljs-string"><span class="hljs-string">"spark.sql.cbo.enabled"</span></span>) .doc(<span class="hljs-string"><span class="hljs-string">"Enables CBO for estimation of plan statistics when set true."</span></span>) .booleanConf .createWithDefault(<span class="hljs-literal"><span class="hljs-literal">false</span></span>)</code> </pre><br><p>  Sowie seine abhängigen Optimierungen im Zusammenhang mit der Neuordnung von join'ov. <br></p><br><pre> <code class="hljs scala"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">JOIN_REORDER_ENABLED</span></span> = buildConf(<span class="hljs-string"><span class="hljs-string">"spark.sql.cbo.joinReorder.enabled"</span></span>) .doc(<span class="hljs-string"><span class="hljs-string">"Enables join reorder in CBO."</span></span>) .booleanConf .createWithDefault(<span class="hljs-literal"><span class="hljs-literal">false</span></span>)</code> </pre><br><p>  oder </p><br><pre> <code class="hljs scala"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">STARSCHEMA_DETECTION</span></span> = buildConf(<span class="hljs-string"><span class="hljs-string">"spark.sql.cbo.starSchemaDetection"</span></span>) .doc(<span class="hljs-string"><span class="hljs-string">"When true, it enables join reordering based on star schema detection. "</span></span>) .booleanConf .createWithDefault(<span class="hljs-literal"><span class="hljs-literal">false</span></span>)</code> </pre><br><h2>  Kurze Zusammenfassung </h2><br><p>  Nur ein kleiner Teil der vorhandenen Optimierungen wurde berührt. Experimente zur Kostenoptimierung, die viel mehr Raum für die Abfragekonvertierung bieten können, stehen an.  Eine weitere interessante Frage ist der Vergleich einer Reihe von Optimierungen beim Lesen von Dateien aus Parkett und Ork. Nach dem Jira des Projekts geht es um Parität, aber ist es wirklich so? <br></p><br><p>  Außerdem: </p><br><ul><li>  Die Analyse und Optimierung von Anfragen ist interessant und aufregend, insbesondere angesichts der Verfügbarkeit von Quellcodes. </li><li>  Die Einbeziehung von CBO bietet Raum für weitere Optimierungen und Forschungsarbeiten. </li><li>  Es ist notwendig, die Anwendbarkeit der Grundregeln zu überwachen, die es Ihnen ermöglichen, so viele "zusätzliche" Daten wie möglich zum frühestmöglichen Zeitpunkt herauszufiltern. </li><li>  Join ist ein notwendiges Übel, aber wenn möglich, lohnt es sich, sie zu minimieren und zu verfolgen, welche Implementierung unter der Haube verwendet wird. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417103/">https://habr.com/ru/post/de417103/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417091/index.html">Erstellen Sie einen Cartoon-Water-Shader für das Web. Teil 3</a></li>
<li><a href="../de417093/index.html">Touch-Schalter mit Modbus: Warum werden sie benötigt und wie werden sie in einer intelligenten Wohnung angewendet?</a></li>
<li><a href="../de417097/index.html">JavaScript-Metaprogrammierung</a></li>
<li><a href="../de417099/index.html">Wie habe ich die Standard-C ++ 11-Bibliothek geschrieben oder warum ist Boost so beängstigend? Kapitel 2</a></li>
<li><a href="../de417101/index.html">Definition von Ready - Was wir vergessen haben zu erzählen</a></li>
<li><a href="../de417105/index.html">Drucken auf einem 3D-Drucker. Die geheimen Erfahrungen von 3Dtool</a></li>
<li><a href="../de417107/index.html">Entwickler des Spiels während True: Erfahren Sie () mehr über Gamedev-Programmierung, VR-Probleme und ML-Simulationen</a></li>
<li><a href="../de417109/index.html">Richard Hamming: Kapitel 10. Codierungstheorie - I.</a></li>
<li><a href="../de417111/index.html">Online-Konferenzen: Streaming vs Webinar</a></li>
<li><a href="../de417113/index.html">Italienischer 3D-Drucker in Russland: Raise3D N1 Dual - Modellierung und Prototyping</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>