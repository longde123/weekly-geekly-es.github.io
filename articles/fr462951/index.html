<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😆 🥠 👨🏻‍🌾 Là où une personne voit des formes, l'IA voit des textures 🦗 🤸🏽 🚦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Étonnamment, les chercheurs possédant des algorithmes de vision par ordinateur en apprentissage profond ne parviennent souvent pas à classer les image...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Là où une personne voit des formes, l'IA voit des textures</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462951/"> Étonnamment, les chercheurs possédant des algorithmes de vision par ordinateur en apprentissage profond ne parviennent souvent pas à classer les images car ils se concentrent principalement sur les textures plutôt que sur les formes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/953/444/41f/95344441f850333698aa94694c254d32.jpg"><br><br>  Si vous regardez une photo d'un chat, il est fort probable que vous puissiez reconnaître cet animal, qu'il soit rouge ou rayé - ou même si la photo est en noir et blanc, tachée, battue ou ternie.  Vous pourrez probablement remarquer un chat quand il se recroqueville derrière un oreiller ou saute sur une table, représentant seulement une forme floue.  Vous avez naturellement appris à reconnaître les chats dans presque toutes les situations.  Mais les systèmes de vision industrielle basés sur des réseaux de neurones profonds, bien qu'ils puissent parfois fournir aux gens des tâches de reconnaissance de chats dans des conditions fixes, peuvent être confondus avec des images qui sont au moins légèrement différentes de ce qu'elles savent, ou qui contiennent du bruit ou grain solide. <br><a name="habracut"></a><br>  Et maintenant, les chercheurs allemands ont découvert une raison inattendue à cela: si les gens prêtent attention aux formes des objets représentés, la vision par ordinateur avec un apprentissage profond s'accroche aux textures des objets. <br><br>  Cette <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">découverte</a> , présentée en mai lors d'une conférence internationale des représentations de l'apprentissage, souligne le contraste frappant entre la «pensée» des personnes et des machines, et illustre à quel point nous pouvons nous tromper dans la compréhension du fonctionnement de l'IA.  Et cela peut aussi nous dire pourquoi notre vision est devenue ainsi à la suite de l'évolution. <br><br><h2>  Chats d'ivoire et avions de montre </h2><br>  Les algorithmes d'apprentissage en profondeur fonctionnent en conduisant des milliers d'images à travers un réseau de neurones qui ont ou non un chat.  Le système recherche des motifs dans ces données, qu'il utilise ensuite pour mettre la meilleure marque sur l'image qu'il n'a pas rencontrée auparavant.  L'architecture du réseau est un peu comme la structure du système visuel humain, car elle a des couches connectées qui lui permettent d'extraire de plus en plus de caractéristiques abstraites de l'image.  Cependant, le processus de construction d'un système d'associations conduisant à la bonne réponse est une boîte noire que les gens ne peuvent essayer d'interpréter qu'après coup.  "Nous avons essayé de comprendre ce qui mène au succès de ces algorithmes de vision par ordinateur en apprentissage profond, et pourquoi ils sont si vulnérables", a déclaré <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Thomas Ditterich</a> , un spécialiste informatique à l'Université de l'Oregon qui n'est pas affilié à cette étude. <br><br>  Certains chercheurs préfèrent étudier ce qui se passe lorsqu'ils tentent de tromper le réseau en modifiant légèrement l'image.  Ils ont constaté que même de petits changements peuvent entraîner un marquage incorrect de l'image par le système - et des changements importants <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">peuvent ne pas provoquer</a> de changement d'étiquette.  Pendant ce temps, d'autres experts suivent les changements dans le système pour analyser comment les neurones individuels répondent à l'image et composent un « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">atlas d'activation</a> » basé sur les attributs que le système a appris. <br><br>  Mais un groupe de scientifiques des laboratoires du neurobiologiste computationnel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Matias Betge</a> et du psychophysiologiste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Felix Wichmann</a> de l'Université de Tübingen en Allemagne ont choisi une approche qualitative.  L'année dernière, l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">équipe a signalé</a> que lors de la formation d'images modifiées par un bruit d'un certain type, le réseau a commencé à mieux reconnaître les images que les personnes qui tentaient de distinguer les mêmes images bruyantes.  Cependant, les mêmes images, modifiées légèrement différemment, ont complètement confondu le réseau, bien que pour les gens, la nouvelle distorsion soit presque la même que l'ancienne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c18/418/8c4/c184188c4f088a155c652e51562c42f6.jpg" width="60%"><br>  <i>Robert Geyros, étudiant de troisième cycle en neurobiologie computationnelle de l'Université de Tübingen</i> <br><br>  Pour expliquer ce résultat, les chercheurs se sont demandé quelle qualité d'image change le plus même avec l'ajout d'un peu de bruit.  Le choix évident est la texture.  "La forme d'un objet reste plus ou moins indemne si vous ajoutez beaucoup de bruit pendant longtemps", a déclaré <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Robert Geyros</a> , étudiant diplômé dans les laboratoires de Betge et Wichmann, l'auteur principal de l'étude.  Mais "la structure de l'image locale est déformée très rapidement lorsqu'une petite quantité de bruit est ajoutée".  Ils ont donc trouvé un moyen délicat de tester comment les systèmes visuels des machines et des personnes traitent les images. <br><br>  Geyros, Betge et leurs collègues ont créé des images avec deux caractéristiques contradictoires, prenant la forme d'un objet et la texture d'un autre: par exemple, une silhouette de chat peinte en texture de peau d'éléphant gris, ou un ours fait de boîtes en aluminium, ou une silhouette plane remplie de chevauchements les uns des autres avec des images de cadrans.  Les gens ont étiqueté des centaines de ces images en fonction de leurs formes - chat, ours, avion - presque à chaque fois, comme prévu.  Cependant, quatre algorithmes de classification différents se penchaient dans la direction opposée, donnant des étiquettes reflétant les textures des objets: éléphant, boîtes de conserve, montres. <br><br>  "Cela change notre compréhension de la façon dont les réseaux de neurones profonds avec distribution directe - sans paramètres supplémentaires, après le processus d'apprentissage habituel - reconnaissent les images", a déclaré <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nikolaus Kriegscorte</a> , neuroscientifique en informatique à l'Université Columbia qui n'était pas impliqué dans l'étude. <br><br>  À première vue, la préférence pour les textures AI sur les formes peut sembler étrange, mais cela a du sens.  "La texture est un peu une forme à haute résolution", a déclaré Kriegscorte.  Et il est plus facile pour le système de s'accrocher à une telle échelle: le nombre de pixels avec des informations de texture dépasse considérablement le nombre de pixels qui composent la limite de l'objet, et les toutes premières étapes du réseau sont liées à la reconnaissance des caractéristiques locales, telles que les lignes et les visages.  «C'est exactement ce qu'est la texture», a déclaré <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">John Tsotsos</a> , spécialiste en vision par ordinateur à l'Université York à Toronto, qui n'est pas associé à cette étude.  "Par exemple, un regroupement de segments alignés de la même manière." <br><br>  Geyros et ses collègues ont montré que ces panneaux locaux suffisent au réseau pour effectuer le classement.  C'est la preuve de Betge et d'un autre des auteurs de l'étude, le postdoctorant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wiland Brendel</a> , mis au point dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ouvrage</a> , qui a également été présenté à la conférence de mai.  Dans ce travail, ils ont construit un système d'apprentissage en profondeur qui fonctionnait de la même manière que les algorithmes de classification fonctionnaient avant la diffusion de l'apprentissage en profondeur - basé sur le principe du «sac d'attributs».  L'algorithme divise l'image en petits fragments, comme les modèles actuels (tels que Geyros utilisé dans son expérience), mais ensuite, au lieu d'intégrer progressivement ces informations pour extraire des signes d'un niveau d'abstraction plus élevé, l'algorithme fait immédiatement une hypothèse sur le contenu de chaque pièce ( «Dans cette pièce, il y a des preuves d'un vélo, dans cela - des preuves d'un oiseau»).  Il a simplement plié toutes les décisions pour déterminer l'objet («si plus de pièces contiennent des signes d'un vélo, alors c'est un vélo»), sans prêter attention aux relations spatiales des pièces.  Et pourtant, il était capable de reconnaître des objets avec une précision inattendue. <br><br>  "Ce travail remet en question l'hypothèse selon laquelle le deep learning fait quelque chose de complètement différent", a déclaré Brendel.  «Évidemment, un grand bond a été fait.  Je dis juste que ce n'était pas aussi grand que certains l'espéraient. " <br><br>  Selon Amir Rosenfeld, un postdoc de l'Université York et de l'Université de Toronto, qui n'a pas participé à l'étude, «il y a une grande différence entre ce que les réseaux de neurones devraient, à notre avis, et ce qu'ils font», y compris la façon dont ils gèrent bien reproduire le comportement humain. <br><br>  Le bretzel a parlé dans la même veine.  Il est facile de supposer que les réseaux de neurones résoudront les problèmes de la même manière que les gens, a-t-il déclaré.  "Cependant, nous oublions constamment l'existence d'autres méthodes." <br><br><h2>  Un virage vers une vision plus humaine </h2><br>  Les méthodes modernes d'apprentissage en profondeur peuvent intégrer des caractéristiques locales, telles que les textures, dans des modèles plus globaux, tels que les formes.  "Ce qui est montré de manière inattendue et très convaincante dans ces travaux - bien que l'architecture vous permette de classer les images standard, cela ne se produit pas automatiquement si vous formez simplement le réseau à ce sujet", a déclaré Kriegescorte. <br><br>  Geyros voulait voir ce qui se passerait si l'équipe obligeait les modèles à ignorer les textures.  L'équipe a pris les images traditionnellement utilisées pour l'apprentissage des algorithmes de classification et les a peintes dans différents styles, les privant d'informations utiles sur la texture.  Lorsqu'ils ont recyclé chaque modèle dans les nouvelles images, les systèmes ont commencé à s'appuyer sur des modèles globaux plus grands et ont montré une plus grande tendance à la reconnaissance des modèles, qui ressemblait davantage à des personnes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c60/18d/44b/c6018d44bb8459f3b0496d19975c6c5d.jpg" width="60%"><br>  <i>Wieland Brendel, neuroscientifique computationnel à l'Université de Tübingen en Allemagne</i> <br><br>  Et après cela, les algorithmes ont commencé à mieux classer les images bruyantes, même lorsqu'elles n'étaient pas formées pour faire face à de telles distorsions.  «Le réseau de reconnaissance de formes est devenu totalement plus fiable et gratuit», a déclaré Geyros.  "Cela suggère que le bon biais pour effectuer certaines tâches, dans notre cas, la propension à utiliser des formulaires, contribue à généraliser les connaissances à de nouvelles conditions." <br><br>  Cela suggère également que chez l'homme, une telle tendance aurait pu se former naturellement, car l'utilisation de formes est un moyen plus fiable de reconnaître ce que nous voyons dans des conditions nouvelles ou bruyantes.  Les gens vivent dans un monde en trois dimensions, où les objets sont visibles sous de nombreux angles dans de nombreuses conditions différentes, et où nos autres sentiments, comme le toucher, peuvent éventuellement compléter la reconnaissance des objets.  Par conséquent, pour notre vision, il est logique de donner à la forme une texture prioritaire.  En outre, certains psychologues ont montré un lien entre le langage, l'apprentissage et une tendance à utiliser des formes: lorsque les enfants apprenaient à prêter plus d'attention aux formes lors de l'étude de certaines catégories de mots, ils pouvaient plus tard développer un vocabulaire beaucoup plus étendu des noms que d'autres. <br><br>  Ce travail nous rappelle que «les données ont un effet plus fort sur les préjugés et les biais des modèles que nous ne le pensions», a déclaré Wichman.  Ce n'est pas la première fois que les chercheurs rencontrent ce problème: il a déjà été démontré que les programmes de reconnaissance faciale, la recherche de CV automatique et d'autres réseaux de neurones accordent trop d'importance aux signes inattendus en raison de préjugés profondément enracinés dans les données sur lesquelles ils sont formés.  L'élimination des préjugés indésirables du processus de prise de décision s'est avérée être une tâche difficile, mais Wichman a déclaré que les nouveaux travaux démontrent que cela est possible en principe et encourageant. <br><br>  Cependant, même les modèles de Geyros qui se concentrent sur les formes peuvent être trompés en ajoutant trop de bruit aux images ou en modifiant certains pixels, ce qui signifie qu'ils ont encore beaucoup de chemin à parcourir pour atteindre une qualité comparable à la vision humaine.  Dans la même veine, un nouveau travail de Rosenfeld, Tsotsos et Marcus Solbach, un étudiant diplômé du laboratoire Tsotsos, démontre que les algorithmes d'apprentissage automatique ne sont pas en mesure de capturer la similitude d'images différentes de la même manière que les gens.  Néanmoins, ces travaux «aident à indiquer exactement sous quels aspects ces modèles ne reproduisent pas encore des aspects importants du cerveau humain», a déclaré Kriegscorte.  Et Wichman a déclaré que "dans certains cas, il peut être plus important d'examiner l'ensemble de données." <br><br>  Sanya Fiedler, spécialiste des TI à l'Université de Toronto qui n'a pas participé à l'étude, est d'accord.  «C'est notre travail de développer des données intelligentes», a-t-elle déclaré.  Elle et ses collègues étudient comment les tâches auxiliaires peuvent aider les réseaux de neurones à améliorer la qualité de leurs tâches principales.  Inspirés par les découvertes de Geyros, ils ont récemment formé l'algorithme de classification d'images non seulement pour reconnaître les objets eux-mêmes, mais aussi pour déterminer quels pixels appartiennent à leurs contours.  Et le réseau a automatiquement amélioré la reconnaissance des objets.  "Si l'on ne vous confie qu'une seule tâche, le résultat est une attention sélective et la cécité par rapport à beaucoup d'autres choses", a déclaré Fiedler.  «Si je vous confie plusieurs tâches, vous en apprendrez sur différentes choses et cela risque de ne pas se produire.»  C'est la même chose avec ces algorithmes. "  Résoudre divers problèmes les aide à «développer une tendance à diverses informations», ce qui est similaire à ce qui s'est passé dans l'expérience de Geyros avec des formes et des textures. <br><br>  Toutes ces études sont «une étape très intéressante vers l'approfondissement de notre compréhension de ce qui se passe avec l'apprentissage en profondeur, et peut-être cela nous aidera à surmonter les limites auxquelles nous sommes confrontés», a déclaré Dietrich.  "C'est pourquoi j'aime cette série de travaux." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr462951/">https://habr.com/ru/post/fr462951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr462939/index.html">Top 10 des rapports C ++ Russie et liste de lecture de conférence en accès libre</a></li>
<li><a href="../fr462943/index.html">Chasser les Wumpus ou expérimenter l'écriture d'un jeu Android classique</a></li>
<li><a href="../fr462945/index.html">Générez des mots de passe à usage unique pour 2FA dans JS à l'aide de l'API Web Crypto</a></li>
<li><a href="../fr462947/index.html">L'histoire de la façon dont PVS-Studio a trouvé une erreur dans la bibliothèque utilisée dans ... PVS-Studio</a></li>
<li><a href="../fr462949/index.html">L'histoire de la façon dont PVS-Studio a trouvé une erreur dans la bibliothèque utilisée dans ... PVS-Studio</a></li>
<li><a href="../fr462955/index.html">Transformation numérique de la formation et de la certification du personnel de terrain</a></li>
<li><a href="../fr462957/index.html">Avantages et inconvénients: le seuil de prix pour .org est toujours annulé</a></li>
<li><a href="../fr462959/index.html">Traitement du langage naturel des chèques en ligne: un cours de leçons magiques pour un chat ordinaire et d'autres problèmes</a></li>
<li><a href="../fr462961/index.html">Data Science Digest (août 2019)</a></li>
<li><a href="../fr462963/index.html">Utilisation de l'API contextuelle dans React pour créer un thème d'application global</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>