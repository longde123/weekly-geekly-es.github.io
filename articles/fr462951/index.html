<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ˜† ğŸ¥  ğŸ‘¨ğŸ»â€ğŸŒ¾ LÃ  oÃ¹ une personne voit des formes, l'IA voit des textures ğŸ¦— ğŸ¤¸ğŸ½ ğŸš¦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ã‰tonnamment, les chercheurs possÃ©dant des algorithmes de vision par ordinateur en apprentissage profond ne parviennent souvent pas Ã  classer les image...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>LÃ  oÃ¹ une personne voit des formes, l'IA voit des textures</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462951/"> Ã‰tonnamment, les chercheurs possÃ©dant des algorithmes de vision par ordinateur en apprentissage profond ne parviennent souvent pas Ã  classer les images car ils se concentrent principalement sur les textures plutÃ´t que sur les formes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/953/444/41f/95344441f850333698aa94694c254d32.jpg"><br><br>  Si vous regardez une photo d'un chat, il est fort probable que vous puissiez reconnaÃ®tre cet animal, qu'il soit rouge ou rayÃ© - ou mÃªme si la photo est en noir et blanc, tachÃ©e, battue ou ternie.  Vous pourrez probablement remarquer un chat quand il se recroqueville derriÃ¨re un oreiller ou saute sur une table, reprÃ©sentant seulement une forme floue.  Vous avez naturellement appris Ã  reconnaÃ®tre les chats dans presque toutes les situations.  Mais les systÃ¨mes de vision industrielle basÃ©s sur des rÃ©seaux de neurones profonds, bien qu'ils puissent parfois fournir aux gens des tÃ¢ches de reconnaissance de chats dans des conditions fixes, peuvent Ãªtre confondus avec des images qui sont au moins lÃ©gÃ¨rement diffÃ©rentes de ce qu'elles savent, ou qui contiennent du bruit ou grain solide. <br><a name="habracut"></a><br>  Et maintenant, les chercheurs allemands ont dÃ©couvert une raison inattendue Ã  cela: si les gens prÃªtent attention aux formes des objets reprÃ©sentÃ©s, la vision par ordinateur avec un apprentissage profond s'accroche aux textures des objets. <br><br>  Cette <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dÃ©couverte</a> , prÃ©sentÃ©e en mai lors d'une confÃ©rence internationale des reprÃ©sentations de l'apprentissage, souligne le contraste frappant entre la Â«pensÃ©eÂ» des personnes et des machines, et illustre Ã  quel point nous pouvons nous tromper dans la comprÃ©hension du fonctionnement de l'IA.  Et cela peut aussi nous dire pourquoi notre vision est devenue ainsi Ã  la suite de l'Ã©volution. <br><br><h2>  Chats d'ivoire et avions de montre </h2><br>  Les algorithmes d'apprentissage en profondeur fonctionnent en conduisant des milliers d'images Ã  travers un rÃ©seau de neurones qui ont ou non un chat.  Le systÃ¨me recherche des motifs dans ces donnÃ©es, qu'il utilise ensuite pour mettre la meilleure marque sur l'image qu'il n'a pas rencontrÃ©e auparavant.  L'architecture du rÃ©seau est un peu comme la structure du systÃ¨me visuel humain, car elle a des couches connectÃ©es qui lui permettent d'extraire de plus en plus de caractÃ©ristiques abstraites de l'image.  Cependant, le processus de construction d'un systÃ¨me d'associations conduisant Ã  la bonne rÃ©ponse est une boÃ®te noire que les gens ne peuvent essayer d'interprÃ©ter qu'aprÃ¨s coup.  "Nous avons essayÃ© de comprendre ce qui mÃ¨ne au succÃ¨s de ces algorithmes de vision par ordinateur en apprentissage profond, et pourquoi ils sont si vulnÃ©rables", a dÃ©clarÃ© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Thomas Ditterich</a> , un spÃ©cialiste informatique Ã  l'UniversitÃ© de l'Oregon qui n'est pas affiliÃ© Ã  cette Ã©tude. <br><br>  Certains chercheurs prÃ©fÃ¨rent Ã©tudier ce qui se passe lorsqu'ils tentent de tromper le rÃ©seau en modifiant lÃ©gÃ¨rement l'image.  Ils ont constatÃ© que mÃªme de petits changements peuvent entraÃ®ner un marquage incorrect de l'image par le systÃ¨me - et des changements importants <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">peuvent ne pas provoquer</a> de changement d'Ã©tiquette.  Pendant ce temps, d'autres experts suivent les changements dans le systÃ¨me pour analyser comment les neurones individuels rÃ©pondent Ã  l'image et composent un Â« <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">atlas d'activation</a> Â» basÃ© sur les attributs que le systÃ¨me a appris. <br><br>  Mais un groupe de scientifiques des laboratoires du neurobiologiste computationnel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Matias Betge</a> et du psychophysiologiste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Felix Wichmann</a> de l'UniversitÃ© de TÃ¼bingen en Allemagne ont choisi une approche qualitative.  L'annÃ©e derniÃ¨re, l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ã©quipe a signalÃ©</a> que lors de la formation d'images modifiÃ©es par un bruit d'un certain type, le rÃ©seau a commencÃ© Ã  mieux reconnaÃ®tre les images que les personnes qui tentaient de distinguer les mÃªmes images bruyantes.  Cependant, les mÃªmes images, modifiÃ©es lÃ©gÃ¨rement diffÃ©remment, ont complÃ¨tement confondu le rÃ©seau, bien que pour les gens, la nouvelle distorsion soit presque la mÃªme que l'ancienne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c18/418/8c4/c184188c4f088a155c652e51562c42f6.jpg" width="60%"><br>  <i>Robert Geyros, Ã©tudiant de troisiÃ¨me cycle en neurobiologie computationnelle de l'UniversitÃ© de TÃ¼bingen</i> <br><br>  Pour expliquer ce rÃ©sultat, les chercheurs se sont demandÃ© quelle qualitÃ© d'image change le plus mÃªme avec l'ajout d'un peu de bruit.  Le choix Ã©vident est la texture.  "La forme d'un objet reste plus ou moins indemne si vous ajoutez beaucoup de bruit pendant longtemps", a dÃ©clarÃ© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Robert Geyros</a> , Ã©tudiant diplÃ´mÃ© dans les laboratoires de Betge et Wichmann, l'auteur principal de l'Ã©tude.  Mais "la structure de l'image locale est dÃ©formÃ©e trÃ¨s rapidement lorsqu'une petite quantitÃ© de bruit est ajoutÃ©e".  Ils ont donc trouvÃ© un moyen dÃ©licat de tester comment les systÃ¨mes visuels des machines et des personnes traitent les images. <br><br>  Geyros, Betge et leurs collÃ¨gues ont crÃ©Ã© des images avec deux caractÃ©ristiques contradictoires, prenant la forme d'un objet et la texture d'un autre: par exemple, une silhouette de chat peinte en texture de peau d'Ã©lÃ©phant gris, ou un ours fait de boÃ®tes en aluminium, ou une silhouette plane remplie de chevauchements les uns des autres avec des images de cadrans.  Les gens ont Ã©tiquetÃ© des centaines de ces images en fonction de leurs formes - chat, ours, avion - presque Ã  chaque fois, comme prÃ©vu.  Cependant, quatre algorithmes de classification diffÃ©rents se penchaient dans la direction opposÃ©e, donnant des Ã©tiquettes reflÃ©tant les textures des objets: Ã©lÃ©phant, boÃ®tes de conserve, montres. <br><br>  "Cela change notre comprÃ©hension de la faÃ§on dont les rÃ©seaux de neurones profonds avec distribution directe - sans paramÃ¨tres supplÃ©mentaires, aprÃ¨s le processus d'apprentissage habituel - reconnaissent les images", a dÃ©clarÃ© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nikolaus Kriegscorte</a> , neuroscientifique en informatique Ã  l'UniversitÃ© Columbia qui n'Ã©tait pas impliquÃ© dans l'Ã©tude. <br><br>  Ã€ premiÃ¨re vue, la prÃ©fÃ©rence pour les textures AI sur les formes peut sembler Ã©trange, mais cela a du sens.  "La texture est un peu une forme Ã  haute rÃ©solution", a dÃ©clarÃ© Kriegscorte.  Et il est plus facile pour le systÃ¨me de s'accrocher Ã  une telle Ã©chelle: le nombre de pixels avec des informations de texture dÃ©passe considÃ©rablement le nombre de pixels qui composent la limite de l'objet, et les toutes premiÃ¨res Ã©tapes du rÃ©seau sont liÃ©es Ã  la reconnaissance des caractÃ©ristiques locales, telles que les lignes et les visages.  Â«C'est exactement ce qu'est la textureÂ», a dÃ©clarÃ© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">John Tsotsos</a> , spÃ©cialiste en vision par ordinateur Ã  l'UniversitÃ© York Ã  Toronto, qui n'est pas associÃ© Ã  cette Ã©tude.  "Par exemple, un regroupement de segments alignÃ©s de la mÃªme maniÃ¨re." <br><br>  Geyros et ses collÃ¨gues ont montrÃ© que ces panneaux locaux suffisent au rÃ©seau pour effectuer le classement.  C'est la preuve de Betge et d'un autre des auteurs de l'Ã©tude, le postdoctorant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wiland Brendel</a> , mis au point dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ouvrage</a> , qui a Ã©galement Ã©tÃ© prÃ©sentÃ© Ã  la confÃ©rence de mai.  Dans ce travail, ils ont construit un systÃ¨me d'apprentissage en profondeur qui fonctionnait de la mÃªme maniÃ¨re que les algorithmes de classification fonctionnaient avant la diffusion de l'apprentissage en profondeur - basÃ© sur le principe du Â«sac d'attributsÂ».  L'algorithme divise l'image en petits fragments, comme les modÃ¨les actuels (tels que Geyros utilisÃ© dans son expÃ©rience), mais ensuite, au lieu d'intÃ©grer progressivement ces informations pour extraire des signes d'un niveau d'abstraction plus Ã©levÃ©, l'algorithme fait immÃ©diatement une hypothÃ¨se sur le contenu de chaque piÃ¨ce ( Â«Dans cette piÃ¨ce, il y a des preuves d'un vÃ©lo, dans cela - des preuves d'un oiseauÂ»).  Il a simplement pliÃ© toutes les dÃ©cisions pour dÃ©terminer l'objet (Â«si plus de piÃ¨ces contiennent des signes d'un vÃ©lo, alors c'est un vÃ©loÂ»), sans prÃªter attention aux relations spatiales des piÃ¨ces.  Et pourtant, il Ã©tait capable de reconnaÃ®tre des objets avec une prÃ©cision inattendue. <br><br>  "Ce travail remet en question l'hypothÃ¨se selon laquelle le deep learning fait quelque chose de complÃ¨tement diffÃ©rent", a dÃ©clarÃ© Brendel.  Â«Ã‰videmment, un grand bond a Ã©tÃ© fait.  Je dis juste que ce n'Ã©tait pas aussi grand que certains l'espÃ©raient. " <br><br>  Selon Amir Rosenfeld, un postdoc de l'UniversitÃ© York et de l'UniversitÃ© de Toronto, qui n'a pas participÃ© Ã  l'Ã©tude, Â«il y a une grande diffÃ©rence entre ce que les rÃ©seaux de neurones devraient, Ã  notre avis, et ce qu'ils fontÂ», y compris la faÃ§on dont ils gÃ¨rent bien reproduire le comportement humain. <br><br>  Le bretzel a parlÃ© dans la mÃªme veine.  Il est facile de supposer que les rÃ©seaux de neurones rÃ©soudront les problÃ¨mes de la mÃªme maniÃ¨re que les gens, a-t-il dÃ©clarÃ©.  "Cependant, nous oublions constamment l'existence d'autres mÃ©thodes." <br><br><h2>  Un virage vers une vision plus humaine </h2><br>  Les mÃ©thodes modernes d'apprentissage en profondeur peuvent intÃ©grer des caractÃ©ristiques locales, telles que les textures, dans des modÃ¨les plus globaux, tels que les formes.  "Ce qui est montrÃ© de maniÃ¨re inattendue et trÃ¨s convaincante dans ces travaux - bien que l'architecture vous permette de classer les images standard, cela ne se produit pas automatiquement si vous formez simplement le rÃ©seau Ã  ce sujet", a dÃ©clarÃ© Kriegescorte. <br><br>  Geyros voulait voir ce qui se passerait si l'Ã©quipe obligeait les modÃ¨les Ã  ignorer les textures.  L'Ã©quipe a pris les images traditionnellement utilisÃ©es pour l'apprentissage des algorithmes de classification et les a peintes dans diffÃ©rents styles, les privant d'informations utiles sur la texture.  Lorsqu'ils ont recyclÃ© chaque modÃ¨le dans les nouvelles images, les systÃ¨mes ont commencÃ© Ã  s'appuyer sur des modÃ¨les globaux plus grands et ont montrÃ© une plus grande tendance Ã  la reconnaissance des modÃ¨les, qui ressemblait davantage Ã  des personnes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c60/18d/44b/c6018d44bb8459f3b0496d19975c6c5d.jpg" width="60%"><br>  <i>Wieland Brendel, neuroscientifique computationnel Ã  l'UniversitÃ© de TÃ¼bingen en Allemagne</i> <br><br>  Et aprÃ¨s cela, les algorithmes ont commencÃ© Ã  mieux classer les images bruyantes, mÃªme lorsqu'elles n'Ã©taient pas formÃ©es pour faire face Ã  de telles distorsions.  Â«Le rÃ©seau de reconnaissance de formes est devenu totalement plus fiable et gratuitÂ», a dÃ©clarÃ© Geyros.  "Cela suggÃ¨re que le bon biais pour effectuer certaines tÃ¢ches, dans notre cas, la propension Ã  utiliser des formulaires, contribue Ã  gÃ©nÃ©raliser les connaissances Ã  de nouvelles conditions." <br><br>  Cela suggÃ¨re Ã©galement que chez l'homme, une telle tendance aurait pu se former naturellement, car l'utilisation de formes est un moyen plus fiable de reconnaÃ®tre ce que nous voyons dans des conditions nouvelles ou bruyantes.  Les gens vivent dans un monde en trois dimensions, oÃ¹ les objets sont visibles sous de nombreux angles dans de nombreuses conditions diffÃ©rentes, et oÃ¹ nos autres sentiments, comme le toucher, peuvent Ã©ventuellement complÃ©ter la reconnaissance des objets.  Par consÃ©quent, pour notre vision, il est logique de donner Ã  la forme une texture prioritaire.  En outre, certains psychologues ont montrÃ© un lien entre le langage, l'apprentissage et une tendance Ã  utiliser des formes: lorsque les enfants apprenaient Ã  prÃªter plus d'attention aux formes lors de l'Ã©tude de certaines catÃ©gories de mots, ils pouvaient plus tard dÃ©velopper un vocabulaire beaucoup plus Ã©tendu des noms que d'autres. <br><br>  Ce travail nous rappelle que Â«les donnÃ©es ont un effet plus fort sur les prÃ©jugÃ©s et les biais des modÃ¨les que nous ne le pensionsÂ», a dÃ©clarÃ© Wichman.  Ce n'est pas la premiÃ¨re fois que les chercheurs rencontrent ce problÃ¨me: il a dÃ©jÃ  Ã©tÃ© dÃ©montrÃ© que les programmes de reconnaissance faciale, la recherche de CV automatique et d'autres rÃ©seaux de neurones accordent trop d'importance aux signes inattendus en raison de prÃ©jugÃ©s profondÃ©ment enracinÃ©s dans les donnÃ©es sur lesquelles ils sont formÃ©s.  L'Ã©limination des prÃ©jugÃ©s indÃ©sirables du processus de prise de dÃ©cision s'est avÃ©rÃ©e Ãªtre une tÃ¢che difficile, mais Wichman a dÃ©clarÃ© que les nouveaux travaux dÃ©montrent que cela est possible en principe et encourageant. <br><br>  Cependant, mÃªme les modÃ¨les de Geyros qui se concentrent sur les formes peuvent Ãªtre trompÃ©s en ajoutant trop de bruit aux images ou en modifiant certains pixels, ce qui signifie qu'ils ont encore beaucoup de chemin Ã  parcourir pour atteindre une qualitÃ© comparable Ã  la vision humaine.  Dans la mÃªme veine, un nouveau travail de Rosenfeld, Tsotsos et Marcus Solbach, un Ã©tudiant diplÃ´mÃ© du laboratoire Tsotsos, dÃ©montre que les algorithmes d'apprentissage automatique ne sont pas en mesure de capturer la similitude d'images diffÃ©rentes de la mÃªme maniÃ¨re que les gens.  NÃ©anmoins, ces travaux Â«aident Ã  indiquer exactement sous quels aspects ces modÃ¨les ne reproduisent pas encore des aspects importants du cerveau humainÂ», a dÃ©clarÃ© Kriegscorte.  Et Wichman a dÃ©clarÃ© que "dans certains cas, il peut Ãªtre plus important d'examiner l'ensemble de donnÃ©es." <br><br>  Sanya Fiedler, spÃ©cialiste des TI Ã  l'UniversitÃ© de Toronto qui n'a pas participÃ© Ã  l'Ã©tude, est d'accord.  Â«C'est notre travail de dÃ©velopper des donnÃ©es intelligentesÂ», a-t-elle dÃ©clarÃ©.  Elle et ses collÃ¨gues Ã©tudient comment les tÃ¢ches auxiliaires peuvent aider les rÃ©seaux de neurones Ã  amÃ©liorer la qualitÃ© de leurs tÃ¢ches principales.  InspirÃ©s par les dÃ©couvertes de Geyros, ils ont rÃ©cemment formÃ© l'algorithme de classification d'images non seulement pour reconnaÃ®tre les objets eux-mÃªmes, mais aussi pour dÃ©terminer quels pixels appartiennent Ã  leurs contours.  Et le rÃ©seau a automatiquement amÃ©liorÃ© la reconnaissance des objets.  "Si l'on ne vous confie qu'une seule tÃ¢che, le rÃ©sultat est une attention sÃ©lective et la cÃ©citÃ© par rapport Ã  beaucoup d'autres choses", a dÃ©clarÃ© Fiedler.  Â«Si je vous confie plusieurs tÃ¢ches, vous en apprendrez sur diffÃ©rentes choses et cela risque de ne pas se produire.Â»  C'est la mÃªme chose avec ces algorithmes. "  RÃ©soudre divers problÃ¨mes les aide Ã  Â«dÃ©velopper une tendance Ã  diverses informationsÂ», ce qui est similaire Ã  ce qui s'est passÃ© dans l'expÃ©rience de Geyros avec des formes et des textures. <br><br>  Toutes ces Ã©tudes sont Â«une Ã©tape trÃ¨s intÃ©ressante vers l'approfondissement de notre comprÃ©hension de ce qui se passe avec l'apprentissage en profondeur, et peut-Ãªtre cela nous aidera Ã  surmonter les limites auxquelles nous sommes confrontÃ©sÂ», a dÃ©clarÃ© Dietrich.  "C'est pourquoi j'aime cette sÃ©rie de travaux." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr462951/">https://habr.com/ru/post/fr462951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr462939/index.html">Top 10 des rapports C ++ Russie et liste de lecture de confÃ©rence en accÃ¨s libre</a></li>
<li><a href="../fr462943/index.html">Chasser les Wumpus ou expÃ©rimenter l'Ã©criture d'un jeu Android classique</a></li>
<li><a href="../fr462945/index.html">GÃ©nÃ©rez des mots de passe Ã  usage unique pour 2FA dans JS Ã  l'aide de l'API Web Crypto</a></li>
<li><a href="../fr462947/index.html">L'histoire de la faÃ§on dont PVS-Studio a trouvÃ© une erreur dans la bibliothÃ¨que utilisÃ©e dans ... PVS-Studio</a></li>
<li><a href="../fr462949/index.html">L'histoire de la faÃ§on dont PVS-Studio a trouvÃ© une erreur dans la bibliothÃ¨que utilisÃ©e dans ... PVS-Studio</a></li>
<li><a href="../fr462955/index.html">Transformation numÃ©rique de la formation et de la certification du personnel de terrain</a></li>
<li><a href="../fr462957/index.html">Avantages et inconvÃ©nients: le seuil de prix pour .org est toujours annulÃ©</a></li>
<li><a href="../fr462959/index.html">Traitement du langage naturel des chÃ¨ques en ligne: un cours de leÃ§ons magiques pour un chat ordinaire et d'autres problÃ¨mes</a></li>
<li><a href="../fr462961/index.html">Data Science Digest (aoÃ»t 2019)</a></li>
<li><a href="../fr462963/index.html">Utilisation de l'API contextuelle dans React pour crÃ©er un thÃ¨me d'application global</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>