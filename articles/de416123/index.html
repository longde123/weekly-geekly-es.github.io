<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😵 ☹️ 🏬 Erkennung von Waren in Regalen mithilfe neuronaler Netze mithilfe der Keras- und Tensorflow-Objekterkennungs-API-Technologien 🛐 👨‍👦‍👦 👩🏼‍🤝‍👨🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In dem Artikel werden wir über die Verwendung von Faltungs-Neuronalen Netzen sprechen, um eine praktische Geschäftsaufgabe zu lösen, bei der ein Realo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erkennung von Waren in Regalen mithilfe neuronaler Netze mithilfe der Keras- und Tensorflow-Objekterkennungs-API-Technologien</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/416123/">  In dem Artikel werden wir über die Verwendung von Faltungs-Neuronalen Netzen sprechen, um eine praktische Geschäftsaufgabe zu lösen, bei der ein Realogramm aus Fotografien von Regalen mit Waren wiederhergestellt wird.  Mit der Tensorflow-Objekterkennungs-API trainieren wir das Such- / Lokalisierungsmodell.  Wir werden die Qualität der Suche nach kleinen Produkten in hochauflösenden Fotos mithilfe eines schwebenden Fensters und eines nicht maximalen Unterdrückungsalgorithmus verbessern.  Bei Keras implementieren wir einen Klassifizierer von Waren nach Marke.  Parallel dazu werden wir Ansätze und Ergebnisse mit Entscheidungen von vor 4 Jahren vergleichen.  Alle im Artikel verwendeten Daten stehen zum Download zur Verfügung. Der voll funktionsfähige Code befindet sich auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> und ist als Lernprogramm konzipiert. <br><br><img src="https://habrastorage.org/webt/aw/27/ar/aw27argaeseqsgos5cbpwdwt89s.jpeg"><br><a name="habracut"></a><br><h3>  Einführung </h3><br>  Was ist ein Planogramm?  Das Layoutdiagramm der Warenanzeige auf den konkreten Handelsgeräten des Geschäfts. <br><br>  Was ist ein Realogramm?  Das Layout der Waren auf einem bestimmten Handelsgerät, das hier und jetzt im Geschäft vorhanden ist. <br><br>  Planogramm - wie es sollte, Realogramm - was wir haben. <br><br><img src="https://habrastorage.org/webt/ym/oo/ew/ymooewlirqpaosatarrenno8caw.jpeg"><br><br>  Bis jetzt ist in vielen Geschäften die Verwaltung der restlichen Waren in Regalen, Regalen, Theken und Regalen ausschließlich Handarbeit.  Tausende Mitarbeiter prüfen die Verfügbarkeit von Produkten manuell, berechnen den Saldo, prüfen den Standort anhand der Anforderungen.  Es ist teuer und Fehler sind sehr wahrscheinlich.  Eine falsche Anzeige oder ein Mangel an Waren führt zu geringeren Umsätzen. <br><br>  Außerdem schließen viele Hersteller Vereinbarungen mit Einzelhändlern, um ihre Waren auszustellen.  Und da es viele Hersteller gibt, beginnt zwischen ihnen der Kampf um den besten Platz im Regal.  Jeder möchte, dass sein Produkt in der Mitte gegenüber den Augen des Käufers liegt und die größtmögliche Fläche einnimmt.  Es besteht Bedarf an einer kontinuierlichen Prüfung. <br><br>  Tausende von Merchandisern ziehen von Geschäft zu Geschäft, um sicherzustellen, dass die Produkte ihres Unternehmens im Regal stehen und vertragsgemäß präsentiert werden.  Manchmal sind sie faul: Es ist viel angenehmer, einen Bericht zu erstellen, ohne das Haus zu verlassen, als zu einer Verkaufsstelle zu gehen.  Es ist eine ständige Prüfung der Abschlussprüfer erforderlich. <br><br>  Die Aufgabe der Automatisierung und Vereinfachung dieses Prozesses ist natürlich schon lange gelöst.  Einer der schwierigsten Teile war die Bildverarbeitung: das Finden und Erkennen von Produkten.  Und erst vor relativ kurzer Zeit wurde diese Aufgabe so stark vereinfacht, dass für einen bestimmten Fall in vereinfachter Form die vollständige Lösung in einem Artikel beschrieben werden kann.  Das werden wir tun. <br><br>  Der Artikel enthält ein Minimum an Code (nur für Fälle, in denen der Code klarer als der Text ist).  Die Komplettlösung ist als illustriertes Tutorial in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jupyter-Notizbüchern verfügbar</a> .  Der Artikel enthält keine Beschreibung der Architektur neuronaler Netze, der Prinzipien von Neuronen und mathematischer Formeln.  In diesem Artikel verwenden wir sie als Engineering-Tool, ohne zu sehr auf die Details des Geräts einzugehen. <br><br><h3>  Daten und Ansatz </h3><br>  Wie bei jedem datengesteuerten Ansatz erfordern neuronale Netzwerklösungen Daten.  Sie können sie auch manuell zusammenstellen: um mehrere hundert Zähler zu erfassen und sie beispielsweise mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LabelImg zu markieren</a> .  Sie können Markups beispielsweise bei Yandex.Tolok bestellen. <br><br><img src="https://habrastorage.org/webt/ji/he/lv/jihelvxh9vkmixjzsxink1nknya.jpeg"><br><br>  Wir können die Details eines realen Projekts nicht offenlegen, daher werden wir die Technologie anhand offener Daten erläutern.  Es war zu faul, einkaufen zu gehen und Fotos zu machen (und wir würden uns dort nicht verstehen), und der Wunsch, die im Internet gefundenen Fotos selbst zu markieren, endete nach dem hundertsten klassifizierten Objekt.  Zum Glück bin ich ganz zufällig auf das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grocery Dataset-</a> Archiv <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gestoßen</a> . <br><br>  Im Jahr 2014 haben Mitarbeiter von Idea Teknoloji, Istanbul, Türkei, 354 Fotos aus 40 Geschäften hochgeladen, die mit 4 Kameras erstellt wurden.  Auf jeder dieser Fotografien wurden insgesamt mehrere tausend Objekte mit Rechtecken hervorgehoben, von denen einige in 10 Kategorien eingeteilt wurden. <br><br>  Dies sind Bilder von Zigarettenschachteln.  Wir fördern oder fördern das Rauchen nicht.  Es gab einfach nichts Neutraleres.  Wir versprechen, dass wir überall im Artikel, wo es die Situation erlaubt, Fotos von Katzen verwenden werden. <br><br><img src="https://habrastorage.org/webt/04/8x/ek/048xekrylrnspiwd7vefbux_kgu.jpeg"><br><br>  Zusätzlich zu den mit Tags versehenen Fotos der Regale verfassten sie einen Artikel zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkennung von Einzelhandelsprodukten in Lebensmittelregalen</a> mit einer Lösung für das Problem der Lokalisierung und Klassifizierung.  Dies stellte eine Art Bezugspunkt dar: Unsere Lösung mit neuen Ansätzen sollte sich als einfacher und genauer herausstellen, da sie sonst nicht interessant ist.  Ihr Ansatz besteht aus einer Kombination von Algorithmen: <br><br><img src="https://habrastorage.org/webt/j5/p4/t_/j5p4t_ul9ungv_luvlzxa5aa1gi.jpeg"><br><br>  In jüngster Zeit haben Faltungs-Neuronale Netze (CNNs) das Gebiet der Bildverarbeitung revolutioniert und den Ansatz zur Lösung solcher Probleme grundlegend geändert.  In den letzten Jahren sind diese Technologien für eine Vielzahl von Entwicklern verfügbar geworden, und APIs auf hoher Ebene wie Keras haben ihre Eintrittsschwelle erheblich gesenkt.  Jetzt kann fast jeder Entwickler nach nur wenigen Tagen Datierung die volle Leistung von Faltungs-Neuronalen Netzen nutzen.  Der Artikel beschreibt die Verwendung dieser Technologien anhand eines Beispiels und zeigt, wie eine ganze Kaskade von Algorithmen ohne Genauigkeitsverlust leicht durch nur zwei neuronale Netze ersetzt werden kann. <br><br>  Wir werden das Problem in Schritten lösen: <br><br><ul><li>  Datenaufbereitung.  Wir pumpen die Archive aus und verwandeln sie in eine bequeme Ansicht für die Arbeit. </li><li>  Markenklassifizierung.  Wir lösen das Klassifizierungsproblem mithilfe eines neuronalen Netzwerks. </li><li>  Suchen Sie nach Produkten auf dem Foto.  Wir trainieren das neuronale Netz, um nach Waren zu suchen. </li><li>  Suche Implementierung.  Wir werden die Erkennungsqualität mithilfe eines schwebenden Fensters und eines Algorithmus zur Unterdrückung von Nichtmaxima verbessern. </li><li>  Fazit  Erklären Sie kurz, warum das wirkliche Leben viel komplizierter ist als dieses Beispiel. </li></ul><br><h3>  Technologie </h3><br>  Die wichtigsten Technologien, die wir verwenden werden: Tensorflow, Keras, Tensorflow-Objekterkennungs-API, OpenCV.  Obwohl sowohl Windows als auch Mac OS für die Arbeit mit Tensorflow geeignet sind, empfehlen wir weiterhin die Verwendung von Ubuntu.  Selbst wenn Sie noch nie mit diesem Betriebssystem gearbeitet haben, sparen Sie durch die Verwendung eine Menge Zeit.  Die Installation von Tensorflow für die Arbeit mit der GPU ist ein Thema, das einen separaten Artikel verdient.  Glücklicherweise existieren solche Artikel bereits.  Beispiel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren von TensorFlow unter Ubuntu 16.04 mit einer Nvidia-GPU</a> .  Einige Anweisungen davon sind möglicherweise veraltet. <br><br>  <b>Schritt 1. Daten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorbereiten</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github-Link</a> )</b> <br><br>  Dieser Schritt dauert in der Regel viel länger als die Simulation selbst.  Glücklicherweise verwenden wir vorgefertigte Daten, die wir in das von uns benötigte Formular konvertieren. <br><br>  Sie können auf folgende Weise herunterladen und entpacken: <br><br><pre><code class="bash hljs">wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz tar -xvzf GroceryDataset_part1.tar.gz tar -xvzf GroceryDataset_part2.tar.gz</code> </pre> <br>  Wir erhalten folgende Ordnerstruktur: <br><br><img src="https://habrastorage.org/webt/n1/yi/8n/n1yi8n3faxzmxia70bxsee-b69u.jpeg"><br><br>  Wir verwenden Informationen aus den Verzeichnissen ShelfImages und ProductImagesFromShelves. <br>  ShelfImages enthält Bilder der Regale selbst.  Im Namen ist die Kennung des Racks mit der Kennung des Bildes codiert.  Es können mehrere Bilder von einem Rack vorhanden sein.  Zum Beispiel ein Foto in seiner Gesamtheit und 5 Fotos in Teilen mit Schnittpunkten. <br><br>  Datei C1_P01_N1_S2_2.JPG (Rack C1_P01, Schnappschuss N1_S2_2): <br><br><img src="https://habrastorage.org/webt/nv/jd/or/nvjdorlqwj1qc7asuzktk7dqccs.jpeg"><br><br>  Wir gehen alle Dateien durch und sammeln Informationen im Pandas-Datenrahmen photos_df: <br><br><img src="https://habrastorage.org/webt/us/zq/zq/uszqzqw3haortnmdvscxp0sq1cq.png"><br>  ProductImagesFromShelves enthält ausgeschnittene Fotos von Waren aus Regalen in 11 Unterverzeichnissen: 0 - nicht klassifiziert, 1 - Marlboro, 2 - Kent usw.  Um sie nicht zu bewerben, verwenden wir nur Kategorienummern ohne Angabe von Namen.  Dateien in den Namen enthalten Informationen über das Rack, die Position und Größe der Packung. <br><br>  Datei C1_P01_N1_S3_1.JPG_1276_1828_276_448.png aus Verzeichnis 1 (Kategorie 1, Rack C1_P01, Bild N1_S3_1, Koordinaten der oberen linken Ecke (1276, 1828), Breite 276, Höhe 448): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/-x/pc/f0-xpc4nqkymbj-ycs1my_sldhi.jpeg"></div><br>  Wir brauchen die Fotos der einzelnen Packungen selbst nicht (wir werden sie aus den Bildern der Regale ausschneiden) und sammeln Informationen über ihre Kategorie und Position im Pandas-Datenrahmen products_df: <br><br><img src="https://habrastorage.org/webt/0x/nh/rv/0xnhrvv5nibludwh7svn54hj_ba.png"><br>  Im gleichen Schritt teilen wir alle unsere Informationen in zwei Abschnitte auf: Schulung für die Schulung und Validierung für die Überwachung der Schulung.  In realen Projekten lohnt sich das natürlich nicht.  Und vertraue auch nicht denen, die dies tun.  Sie müssen mindestens einen weiteren Test für den endgültigen Test zuweisen.  Aber auch bei diesem nicht sehr ehrlichen Ansatz ist es wichtig, dass wir uns nicht viel täuschen. <br><br>  Wie bereits erwähnt, kann es mehrere Fotos von einem Rack geben.  Dementsprechend kann dieselbe Packung in mehrere Bilder fallen.  Wir empfehlen Ihnen daher, nicht nach Bildern, sondern vor allem nicht nach Packungen, sondern nach Gestellen aufzuschlüsseln.  Dies ist notwendig, damit nicht dasselbe Objekt aus verschiedenen Blickwinkeln sowohl im Zug als auch in der Validierung landet. <br><br>  Wir machen eine 70/30-Aufteilung (30% der Racks werden validiert, der Rest für das Training): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># get distinct shelves shelves = list(set(photos_df['shelf_id'].values)) # use train_test_split from sklearn shelves_train, shelves_validation, _, _ = train_test_split(   shelves, shelves, test_size=0.3, random_state=6) # mark all records in data frames with is_train flag def is_train(shelf_id): return shelf_id in shelves_train photos_df['is_train'] = photos_df.shelf_id.apply(is_train) products_df['is_train'] = products_df.shelf_id.apply(is_train)</span></span></code> </pre> <br>  Wir werden sicherstellen, dass bei der Trennung genügend Vertreter jeder Klasse für Schulung und Validierung zur Verfügung stehen: <br><img src="https://habrastorage.org/webt/9n/w_/xj/9nw_xjw1qiqc21sbi5q3s0pv3_q.jpeg"><br>  Die blaue Farbe zeigt die Anzahl der Produkte in der Kategorie für die Validierung und die orange für die Schulung.  Mit der Kategorie 3 zur Validierung ist die Situation nicht sehr gut, aber im Prinzip gibt es nur wenige Vertreter. <br><br>  In der Phase der Datenaufbereitung ist es wichtig, keinen Fehler zu machen, da alle weiteren Arbeiten auf den Ergebnissen basieren.  Wir haben immer noch einen Fehler gemacht und viele glückliche Stunden damit verbracht zu verstehen, warum die Qualität der Modelle sehr mittelmäßig ist.  Fühlte mich bereits als Verlierer der „Old School“ -Technologien, bis Sie versehentlich bemerkten, dass einige der Originalfotos um 90 Grad gedreht wurden und einige verkehrt herum gemacht wurden. <br><br>  Gleichzeitig wird das Markup so erstellt, als ob die Fotos korrekt ausgerichtet wären.  Nach einer schnellen Lösung ging es viel lustiger. <br><br>  Wir speichern unsere Daten in pkl-Dateien, um sie in den folgenden Schritten zu verwenden.  Insgesamt haben wir: <br><br><ul><li>  Ein Verzeichnis von Fotos von Gestellen und ihren Teilen mit Bündeln, </li><li>  Ein Datenrahmen mit einer Beschreibung jedes Racks mit einem Hinweis, ob es für das Training vorgesehen ist. </li><li>  Ein Datenrahmen mit Informationen zu allen Produkten in den Regalen, in denen Position, Größe, Kategorie und Kennzeichnung angegeben sind, ob sie für Schulungen vorgesehen sind. </li></ul><br>  Zur Überprüfung zeigen wir ein Rack gemäß unseren Daten an: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function to display shelf photo with rectangled products def draw_shelf_photo(file):   file_products_df = products_df[products_df.file == file]   coordinates = file_products_df[['xmin', 'ymin', 'xmax', 'ymax']].values   im = cv2.imread(f'{shelf_images}{file}')   im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)      for xmin, ymin, xmax, ymax in coordinates:       cv2.rectangle(im, (xmin, ymin), (xmax, ymax), (0, 255, 0), 5)   plt.imshow(im) # draw one photo to check our data fig = plt.gcf() fig.set_size_inches(18.5, 10.5) draw_shelf_photo('C3_P07_N1_S6_1.JPG')</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/1m/7a/xr/1m7axrc3gcdvgt1sg0sc9-dbs0y.png"><br><br>  <b>Schritt 2. Klassifizierung nach Marke ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link auf Github</a> )</b> <br><br>  Die Klassifizierung von Bildern ist die Hauptaufgabe im Bereich der Bildverarbeitung.  Das Problem ist die „semantische Lücke“: Fotografie ist nur eine große Zahlenmatrix [0, 255].  Zum Beispiel 800x600x3 (3 RGB-Kanäle). <br><br><img src="https://habrastorage.org/webt/0w/mi/rx/0wmirxot0tx0_dl_b-m_gpdunse.jpeg"><br><br>  Warum diese Aufgabe schwierig ist: <br><br><img src="https://habrastorage.org/webt/gx/il/fh/gxilfhn6woijdjngbgrbgfzgjmo.png"><br><br>  Wie bereits erwähnt, haben die Autoren der von uns verwendeten Daten 10 Marken identifiziert.  Dies ist eine äußerst vereinfachte Aufgabe, da viel mehr Zigarettenmarken in den Regalen stehen.  Aber alles, was nicht in diese 10 Kategorien fiel, wurde an 0 gesendet - nicht klassifiziert: <br><br><img src="https://habrastorage.org/webt/wv/-j/hm/wv-jhmn18kt8ta1zhsai4fzdxbq.png">  "" <br><br>  Ihr Artikel bietet einen solchen Klassifizierungsalgorithmus mit einer Gesamtgenauigkeit von 92%: <br><img src="https://habrastorage.org/webt/vv/d3/ub/vvd3ubvr_tvjxasrwvs7jap6h7o.jpeg"><br>  Was werden wir tun: <br><br><ul><li>  Wir werden die Daten für das Training vorbereiten, </li><li>  Wir trainieren ein neuronales Faltungsnetzwerk mit der ResNet v1-Architektur. </li><li>  Überprüfen Sie die Fotos zur Validierung. </li></ul><br>  Es klingt "voluminös", aber wir haben gerade Keras 'Beispiel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trainiert ein ResNet auf dem CIFAR10-Dataset</a> " verwendet, das die Funktion zum Erstellen von ResNet v1 übernimmt. <br><br>  Um den Trainingsprozess zu starten, müssen Sie zwei Arrays vorbereiten: x - Fotos von Packungen mit einer Dimension (Anzahl der Packungen, Höhe, Breite, 3) und y - ihre Kategorien mit einer Dimension (Anzahl der Packungen, 10).  Das Array y enthält die sogenannten 1-Hot-Vektoren.  Wenn die Kategorie eines Trainingspakets die Nummer 2 hat (von 0 bis 9), entspricht dies dem Vektor [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. <br><br>  Eine wichtige Frage ist, was mit der Breite und Höhe zu tun ist, da alle Fotos mit unterschiedlichen Auflösungen aus unterschiedlichen Entfernungen aufgenommen wurden.  Wir müssen eine feste Größe wählen, zu der wir alle unsere Bilder der Packungen bringen können.  Diese feste Größe ist ein Metaparameter, der bestimmt, wie unser neuronales Netzwerk trainiert und funktioniert. <br><br>  Einerseits möchte ich diese Größe so groß wie möglich machen, damit kein Detail des Bildes unbemerkt bleibt.  Auf der anderen Seite kann dies bei unserer geringen Menge an Trainingsdaten zu einer schnellen Umschulung führen: Das Modell funktioniert perfekt mit Trainingsdaten, aber schlecht mit Validierungsdaten.  Wir haben die Größe 120x80 gewählt, vielleicht würden wir bei einer anderen Größe ein besseres Ergebnis erzielen.  Zoomfunktion: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># resize pack to fixed size SHAPE_WIDTH x SHAPE_HEIGHT def resize_pack(pack):   fx_ratio = SHAPE_WIDTH / pack.shape[1]   fy_ratio = SHAPE_HEIGHT / pack.shape[0]      pack = cv2.resize(pack, (0, 0), fx=fx_ratio, fy=fy_ratio)   return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]</span></span></code> </pre> <br>  Skalieren Sie eine Packung und zeigen Sie sie zur Überprüfung an.  Der Name der Marke ist für eine Person schwer zu lesen. Lassen Sie uns sehen, wie das neuronale Netzwerk mit der Aufgabe der Klassifizierung fertig wird: <br><br><img src="https://habrastorage.org/webt/z_/8p/0f/z_8p0f5kuxx27mneryilqv81ols.png"><br><br>  Nachdem wir uns gemäß dem im vorherigen Schritt erhaltenen Flag vorbereitet haben, teilen wir die x- und y-Arrays in x_train / x_validation und y_train / y_validation auf. Wir erhalten: <br><br><pre> <code class="bash hljs">x_train shape: (1969, 120, 80, 3) y_train shape: (1969, 10) 1969 train samples 775 validation samples</code> </pre><br>  Wenn die Daten vorbereitet sind, kopieren wir die Funktion des neuronalen Netzwerkkonstruktors der ResNet v1-Architektur aus dem Keras-Beispiel: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">resnet_v1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape, depth, num_classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   …</code> </pre> <br>  Wir konstruieren ein Modell: <br><br><pre> <code class="python hljs">model = resnet_v1(input_shape=x_train.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:], depth=depth, num_classes=num_classes) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>,             optimizer=Adam(lr=lr_schedule(<span class="hljs-number"><span class="hljs-number">0</span></span>)), metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  Wir haben einen ziemlich begrenzten Datensatz.  Um zu verhindern, dass das Modell während des Trainings jedes Mal dasselbe Foto sieht, verwenden wir Augmentation: Verschieben Sie das Bild nach dem Zufallsprinzip und drehen Sie es ein wenig.  Keras bietet hierfür folgende Optionen: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator(   featurewise_center=False,  # set input mean to 0 over the dataset   samplewise_center=False,  # set each sample mean to 0   featurewise_std_normalization=False,  # divide inputs by std of the dataset   samplewise_std_normalization=False,  # divide each input by its std   zca_whitening=False,  # apply ZCA whitening   rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)   width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)   height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)   horizontal_flip=False,  # randomly flip images   vertical_flip=False)  # randomly flip images datagen.fit(x_train)</span></span></code> </pre> <br>  Wir beginnen den Trainingsprozess. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's run training process, 20 epochs is enough batch_size = 50 epochs = 15 model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),                   validation_data=(x_validation, y_validation),                   epochs=epochs, verbose=1, workers=4,                   callbacks=[LearningRateScheduler(lr_schedule)])</span></span></code> </pre> <br>  Nach dem Training und der Bewertung erhalten wir eine Genauigkeit im Bereich von 92%.  Möglicherweise erhalten Sie eine andere Genauigkeit: Es gibt nur sehr wenige Daten, daher hängt die Genauigkeit stark vom Erfolg der Partition ab.  Auf dieser Partition haben wir keine wesentlich höhere Genauigkeit als im Artikel angegeben, aber wir haben selbst praktisch nichts getan und wenig Code geschrieben.  Darüber hinaus können wir leicht eine neue Kategorie hinzufügen, und die Genauigkeit sollte (theoretisch) erheblich zunehmen, wenn wir mehr Daten vorbereiten. <br><br>  Vergleichen Sie bei Interesse die Verwirrungsmatrizen: <br><img src="https://habrastorage.org/webt/ee/ns/tm/eenstmjcconudxyjjkmtpzcdmow.jpeg"><br>  Fast alle Kategorien, die unser neuronales Netzwerk definiert, mit Ausnahme der Kategorien 4 und 7. Es ist auch nützlich, die hellsten Vertreter jeder Verwirrungsmatrixzelle zu betrachten: <br><img src="https://habrastorage.org/webt/qv/tm/lw/qvtmlwxgqvbdhut73zgsbbtfqqo.jpeg"><br>  Sie können auch verstehen, warum das Parlament mit Camel verwechselt wurde, aber warum Winston mit Lucky Strike verwechselt wurde, ist völlig unverständlich, aber sie haben nichts gemeinsam.  Dies ist das Hauptproblem neuronaler Netze - die vollständige Undurchsichtigkeit dessen, was im Inneren geschieht.  Sie können natürlich einige Ebenen visualisieren, aber für uns sieht diese Visualisierung folgendermaßen aus: <br><br><img src="https://habrastorage.org/webt/w1/tn/et/w1tnetor61yz-uwvmjlserwvh3m.jpeg"><br><br>  Eine offensichtliche Möglichkeit, die Erkennungsqualität unter unseren Bedingungen zu verbessern, besteht darin, weitere Fotos hinzuzufügen. <br><br>  Der Klassifikator ist also fertig.  Gehe zum Detektor. <br><br>  <b>Schritt 3. Suchen Sie nach Produkten auf dem Foto ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link auf Github</a> )</b> <br><br>  Die folgenden wichtigen Aufgaben im Bereich Computer Vision sind semantische Segmentierung, Lokalisierung, Objektsuche und Instanzsegmentierung. <br><br><img src="https://habrastorage.org/webt/z0/y9/bf/z0y9bfe7f6qp143c3wubj8rvnd8.jpeg"><br><br>  Unsere Aufgabe erfordert die Objekterkennung.  Der Artikel von 2014 bietet einen Ansatz, der auf der Viola-Jones- und der HOG-Methode mit visueller Genauigkeit basiert: <br><br><img src="https://habrastorage.org/webt/11/jt/8j/11jt8jcggyutkrxyelswr8psm8s.jpeg"><br><br>  Dank der Verwendung zusätzlicher statistischer Einschränkungen ist ihre Genauigkeit sehr gut: <br><br><img src="https://habrastorage.org/webt/od/nb/yl/odnbylwn0yo-k92q_nu6mazz8fq.jpeg"><br><br>  Jetzt wird die Aufgabe der Objekterkennung mit Hilfe neuronaler Netze erfolgreich gelöst.  Wir werden das Tensorflow Object Detection API-System verwenden und ein neuronales Netzwerk mit der Mobilenet V1 SSD-Architektur trainieren.  Das Trainieren eines solchen Modells von Grund auf erfordert viele Daten und kann Tage dauern. Daher verwenden wir ein Modell, das auf COCO-Daten nach dem Prinzip des Transferlernens trainiert wurde. <br><br>  Das Schlüsselkonzept dieses Ansatzes ist dies.  Warum muss ein Kind nicht Millionen von Objekten zeigen, damit es lernt, einen Ball von einem Würfel zu finden und zu unterscheiden?  Weil das Kind 500 Millionen Jahre Entwicklung des visuellen Kortex hat.  Die Evolution hat das Sehen zum größten sensorischen System gemacht.  Fast 50% (aber das ist nicht genau) der Neuronen des menschlichen Gehirns sind für die Bildverarbeitung verantwortlich.  Eltern können nur den Ball und den Würfel zeigen und das Kind dann mehrmals korrigieren, damit es das eine perfekt findet und voneinander unterscheidet. <br><br>  Aus philosophischer Sicht (mit mehr als allgemeinen technischen Unterschieden) funktioniert das Transferlernen in neuronalen Netzen auf ähnliche Weise.  Faltungs-Neuronale Netze bestehen aus Ebenen, von denen jede immer komplexere Formen definiert: Sie identifizieren Schlüsselpunkte, kombinieren sie zu Linien, die sich wiederum zu Figuren verbinden.  Und erst auf der letzten Ebene bestimmt aus der Gesamtheit der gefundenen Zeichen das Objekt. <br><br>  Objekte der realen Welt haben viel gemeinsam.  Beim Transferlernen verwenden wir die bereits trainierten Definitionsebenen der Grundfunktionen und trainieren nur die Ebenen, die für die Identifizierung von Objekten verantwortlich sind.  Dazu reichen uns ein paar hundert Fotos und ein paar Betriebsstunden einer normalen GPU.  Das Netzwerk wurde ursprünglich auf dem COCO-Datensatz (Microsoft Common Objects in Context) trainiert, der 91 Kategorien und 2.500.000 Bilder umfasst!  Viele, wenn auch nicht 500 Millionen Jahre Evolution. <br><br>  Mit Blick auf die Zukunft visualisiert diese GIF-Animation (etwas langsam, nicht sofort scrollen) vom Tensorboard den Lernprozess.  Wie Sie sehen können, liefert das Modell fast sofort ein vollständig hochwertiges Ergebnis, und dann kommt das Schleifen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d2d/356/aa4/d2d356aa49bedf31aa918930d8bf1545.gif" alt="Bild"><br><br>  Der „Trainer“ des Tensorflow-Objekterkennungs-API-Systems kann unabhängig voneinander eine Erweiterung durchführen, zufällige Teile von Bildern für das Training ausschneiden und „negative“ Beispiele auswählen (Fotoabschnitte, die keine Objekte enthalten).  Theoretisch ist keine Fotovorverarbeitung erforderlich.  Auf einem Heimcomputer mit Festplatte und wenig RAM weigerte er sich jedoch, mit hochauflösenden Bildern zu arbeiten: Zuerst hing er lange, raschelte mit einer Festplatte und flog dann heraus. <br><br>  Infolgedessen haben wir die Fotos auf eine Größe von 1000 x 1000 Pixel komprimiert, während das Seitenverhältnis beibehalten wurde.  Da jedoch beim Komprimieren eines großen Fotos viele Zeichen verloren gehen, wurden zunächst mehrere Quadrate einer zufälligen Größe aus jedem Foto des Racks ausgeschnitten und in 1000 x 1000 gepresst.  Infolgedessen fielen Packs in hoher Auflösung (aber nicht genug) und in kleinen (aber vielen) in die Trainingsdaten.  Wir wiederholen: Dieser Schritt ist erzwungen und höchstwahrscheinlich völlig unnötig und möglicherweise schädlich. <br><br>  Die vorbereiteten und komprimierten Fotos werden in separaten Verzeichnissen (eval und train) gespeichert und ihre Beschreibung (mit den darin enthaltenen Bündeln) wird in Form von zwei Pandas-Datenrahmen (train_df und eval_df) gebildet: <br><br><img src="https://habrastorage.org/webt/dl/lx/pn/dllxpn03xu5us7wr6h7y3rgzayg.png"><br>  Das Tensorflow Object Detection API-System erfordert, dass Eingaben als tfrecord-Dateien dargestellt werden.  Sie können sie mit dem Dienstprogramm erstellen, aber wir machen daraus einen Code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">class_text_to_int</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(row_label)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> row_label == <span class="hljs-string"><span class="hljs-string">'pack'</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">split</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, group)</span></span></span><span class="hljs-function">:</span></span>   data = namedtuple(<span class="hljs-string"><span class="hljs-string">'data'</span></span>, [<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, <span class="hljs-string"><span class="hljs-string">'object'</span></span>])   gb = df.groupby(group)   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [data(filename, gb.get_group(x))           <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename, x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(gb.groups.keys(), gb.groups)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_tf_example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(group, path)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.gfile.GFile(os.path.join(path, <span class="hljs-string"><span class="hljs-string">'{}'</span></span>.format(group.filename)), <span class="hljs-string"><span class="hljs-string">'rb'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> fid:       encoded_jpg = fid.read()   encoded_jpg_io = io.BytesIO(encoded_jpg)   image = Image.open(encoded_jpg_io)   width, height = image.size   filename = group.filename.encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>)   image_format = <span class="hljs-string"><span class="hljs-string">b'jpg'</span></span>   xmins = []   xmaxs = []   ymins = []   ymaxs = []   classes_text = []   classes = []   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> index, row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> group.object.iterrows():       xmins.append(row[<span class="hljs-string"><span class="hljs-string">'xmin'</span></span>] / width)       xmaxs.append(row[<span class="hljs-string"><span class="hljs-string">'xmax'</span></span>] / width)       ymins.append(row[<span class="hljs-string"><span class="hljs-string">'ymin'</span></span>] / height)       ymaxs.append(row[<span class="hljs-string"><span class="hljs-string">'ymax'</span></span>] / height)       classes_text.append(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>].encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>))       classes.append(class_text_to_int(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>]))   tf_example = tf.train.Example(features=tf.train.Features(feature={       <span class="hljs-string"><span class="hljs-string">'image/height'</span></span>: dataset_util.int64_feature(height),       <span class="hljs-string"><span class="hljs-string">'image/width'</span></span>: dataset_util.int64_feature(width),       <span class="hljs-string"><span class="hljs-string">'image/filename'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/source_id'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/encoded'</span></span>: dataset_util.bytes_feature(encoded_jpg),       <span class="hljs-string"><span class="hljs-string">'image/format'</span></span>: dataset_util.bytes_feature(image_format),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmin'</span></span>: dataset_util.float_list_feature(xmins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmax'</span></span>: dataset_util.float_list_feature(xmaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymin'</span></span>: dataset_util.float_list_feature(ymins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymax'</span></span>: dataset_util.float_list_feature(ymaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/class/text'</span></span>: dataset_util.bytes_list_feature(classes_text),       <span class="hljs-string"><span class="hljs-string">'image/object/class/label'</span></span>: dataset_util.int64_list_feature(classes),   }))   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf_example <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert_to_tf_records</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_path, examples, dst_file)</span></span></span><span class="hljs-function">:</span></span>   writer = tf.python_io.TFRecordWriter(dst_file)   grouped = split(examples, <span class="hljs-string"><span class="hljs-string">'filename'</span></span>)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> grouped:       tf_example = create_tf_example(group, images_path)       writer.write(tf_example.SerializeToString())   writer.close() convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">train/'</span></span>, train_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">train.record'</span></span>) convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">eval/'</span></span>, eval_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">eval.record'</span></span>)</code> </pre><br>  Es bleibt uns überlassen, ein spezielles Verzeichnis vorzubereiten und die Prozesse zu starten: <br><br><img src="https://habrastorage.org/webt/ld/yd/rb/ldydrbk3ivkixsx9kep69b5eyoy.jpeg"><br><br>  Die Struktur mag unterschiedlich sein, aber wir finden es sehr praktisch. <br><br>  Das Datenverzeichnis enthält die Dateien, die wir mit tfrecords (train.record und eval.record) erstellt haben, sowie pack.pbtxt mit den Objekttypen, für die wir das neuronale Netzwerk trainieren werden.  Wir müssen nur einen Objekttyp definieren, daher ist die Datei sehr kurz: <br><br><img src="https://habrastorage.org/webt/pc/jw/x0/pcjwx0l3fc-wuzhiyt7lfo9xtcq.png"><br><br>  Das Modellverzeichnis (es kann viele Modelle zur Lösung eines Problems geben) im untergeordneten Verzeichnis ssd_mobilenet_v1 enthält die Einstellungen für das Training in der Datei .config sowie zwei leere Verzeichnisse: train und eval.  Im Zug speichert der „Trainer“ die Modellkontrollpunkte, der „Bewerter“ nimmt sie auf, führt sie für die zu bewertenden Daten aus und legt sie im Bewertungsverzeichnis ab.  Tensorboard verfolgt diese beiden Verzeichnisse und zeigt Prozessinformationen an. <br><br>  Detaillierte Beschreibung der Struktur von Konfigurationsdateien usw.  finden Sie <a href="">hier</a> und <a href="">hier</a> .  Installationsanweisungen für die Tensorflow Object Detection API finden Sie <a href="">hier</a> . <br><br>  Wir gehen in das Verzeichnis models / research / object_detection und entleeren das vorab trainierte Modell: <br><br><pre> <code class="bash hljs">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz tar -xvzf ssd_mobilenet_v1_coco_2017_11_17.tar.gz</code> </pre><br>  Dort kopieren wir das von uns erstellte Verzeichnis pack_detector. <br><br>  Starten Sie zunächst den Trainingsprozess: <br><br><pre> <code class="bash hljs">python3 train.py --logtostderr \   --train_dir=pack_detector/models/ssd_mobilenet_v1/train/ \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config</code> </pre><br>  Wir starten den Bewertungsprozess.  Wir haben keine zweite Grafikkarte, daher starten wir sie auf dem Prozessor (mit der Anweisung CUDA_VISIBLE_DEVICES = "").  Aus diesem Grund wird er sehr spät in Bezug auf den Trainingsprozess sein, aber das ist nicht so schlimm: <br><br><pre> <code class="bash hljs">CUDA_VISIBLE_DEVICES=<span class="hljs-string"><span class="hljs-string">""</span></span> python3 eval.py \   --logtostderr \   --checkpoint_dir=pack_detector/models/ssd_mobilenet_v1/train \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --eval_dir=pack_detector/models/ssd_mobilenet_v1/<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span></code> </pre> <br>  Wir starten den Tensorboard-Prozess: <br><br><pre> <code class="bash hljs">tensorboard --logdir=pack_detector/models/ssd_mobilenet_v1</code> </pre> <br>  Danach können wir schöne Grafiken sowie die tatsächliche Arbeit des Modells an den geschätzten Daten sehen (GIF am Anfang): <br><br><img src="https://habrastorage.org/webt/qc/wt/rl/qcwtrlmdugb4zgyhy6gnnaoia9w.jpeg"><br><br>  Der Trainingsprozess kann jederzeit gestoppt und fortgesetzt werden.  Wenn wir glauben, dass das Modell gut genug ist, speichern wir den Prüfpunkt in Form eines Inferenzgraphen: <br><br><pre> <code class="bash hljs">python3 export_inference_graph.py \   --input_type image_tensor \   --pipeline_config_path pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --trained_checkpoint_prefix pack_detector/models/ssd_mobilenet_v1/train/model.ckpt-13756 \   --output_directory pack_detector/models/ssd_mobilenet_v1/pack_detector_2018_06_03</code> </pre> <br>  In diesem Schritt haben wir also ein Inferenzdiagramm erhalten, mit dem wir nach Bündelobjekten suchen können.  Wir gehen zu seiner Verwendung über. <br><br>  <b>Schritt 4. Implementierung der Suche ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github-Link</a> )</b> <br><br>  Der Lade- und Initialisierungscode des Inferenzdiagramms befindet sich unter dem obigen Link.  Wichtige Suchfunktionen: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's write function that executes detection def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):   # Run inference   expanded_dims = np.expand_dims(image, 0)   output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})   # all outputs are float32 numpy arrays, so convert types as appropriate   output_dict['num_detections'] = int(output_dict['num_detections'][0])   output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)   output_dict['detection_boxes'] = output_dict['detection_boxes'][0]   output_dict['detection_scores'] = output_dict['detection_scores'][0]   return output_dict # it is useful to be able to run inference not only on the whole image, # but also on its parts # cutoff - minimum detection score needed to take box def run_inference_for_image_part(image_tensor, sess, tensor_dict,                                image, cutoff, ax0, ay0, ax1, ay1):   boxes = []   im = image[ay0:ay1, ax0:ax1]   h, w, c = im.shape   output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)   for i in range(100):       if output_dict['detection_scores'][i] &lt; cutoff:           break       y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \                               output_dict['detection_scores'][i]       x0, y0, x1, y1, score = int(x0*w), int(y0*h), \                               int(x1*w), int(y1*h), \                               int(score * 100)       boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))   return boxes</span></span></code> </pre> <br>  Die Funktion findet begrenzte Kästchen für Packs nicht auf dem gesamten Foto, sondern in seinem Teil.  Die Funktion filtert auch die gefundenen Rechtecke mit einer niedrigen Erkennungsbewertung heraus, die im Cutoff-Parameter angegeben ist. <br><br>  Es stellt sich als Dilemma heraus.  Einerseits verlieren wir mit einem hohen Cutoff viele Objekte, andererseits finden wir mit einem niedrigen Cutoff viele Objekte, die keine Bündel sind.  Gleichzeitig finden wir immer noch nicht alles und nicht ideal: <br><img src="https://habrastorage.org/webt/-u/ao/7y/-uao7ylwycrzrn3xqd1kfh2q0bs.jpeg"><br>  Beachten Sie jedoch, dass die Erkennung mit Cutoff = 0,9 nahezu perfekt ist, wenn wir die Funktion für ein kleines Stück des Fotos ausführen. <br><br><img src="https://habrastorage.org/webt/gg/qn/ia/ggqniasnrkggb6bjnarwum_6i48.jpeg"><br><br>  Dies liegt an der Tatsache, dass das MobileNet V1 SSD-Modell 300 x 300 Fotos als Eingabe akzeptiert.  Natürlich gehen bei einer solchen Komprimierung viele Zeichen verloren. <br><br>  Diese Zeichen bleiben jedoch bestehen, wenn wir ein kleines Quadrat mit mehreren Packungen ausschneiden.  Dies legt die Idee nahe, ein schwebendes Fenster zu verwenden: Wir laufen durch ein kleines Rechteck auf einem Foto und erinnern uns an alles, was wir gefunden haben. <br><br><img src="https://habrastorage.org/webt/zn/7s/dz/zn7sdzl4wcb9dwugzkxilg2d2hk.jpeg"><br><br>  :          ,     .         .   :          (detection score),  ,    ,        overlapTresh (       ): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function for non-maximum suppression def non_max_suppression(boxes, overlapThresh):   if len(boxes) == 0:       return np.array([]).astype("int")   if boxes.dtype.kind == "i":       boxes = boxes.astype("float")   pick = []   x1 = boxes[:,0]   y1 = boxes[:,1]   x2 = boxes[:,2]   y2 = boxes[:,3]   sc = boxes[:,4]   area = (x2 - x1 + 1) * (y2 - y1 + 1)   idxs = np.argsort(sc)   while len(idxs) &gt; 0:       last = len(idxs) - 1       i = idxs[last]       pick.append(i)       xx1 = np.maximum(x1[i], x1[idxs[:last]])       yy1 = np.maximum(y1[i], y1[idxs[:last]])       xx2 = np.minimum(x2[i], x2[idxs[:last]])       yy2 = np.minimum(y2[i], y2[idxs[:last]])       w = np.maximum(0, xx2 - xx1 + 1)       h = np.maximum(0, yy2 - yy1 + 1)       #todo fix overlap-contains...       overlap = (w * h) / area[idxs[:last]]              idxs = np.delete(idxs, np.concatenate(([last],           np.where(overlap &gt; overlapThresh)[0])))     return boxes[pick].astype("int")</span></span></code> </pre> <br>     : <br><br><img src="https://habrastorage.org/webt/go/hd/yz/gohdyzvwuyu8ja4w893big9c1yy.jpeg"><br><br>          : <br><br><img src="https://habrastorage.org/webt/jm/oi/da/jmoida7irvm4u3drey8nu6g00kw.jpeg"><br><br>   ,           ,    . <br><br><h3>  Fazit </h3><br>       «»:         ,       . ,    ,         ..    . <br><br>       ,    ,    : <br><br><ol><li>  150  ,     ,   , </li><li>        3-7  , </li><li>   100    , </li><li>        , </li><li>        (), </li><li>    (,  ), </li><li>    ,        «», </li><li>  ,   ,     (SSD  ), </li><li>      ,  , </li><li>  . </li></ol><br>         ,      ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416123/">https://habr.com/ru/post/de416123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416111/index.html">GraphQL-Datenkonvertierung für CustomTreeData-Komponente von DevExtreme-Reactive</a></li>
<li><a href="../de416113/index.html">Steven Wolfram: Erinnerungen an Steve Jobs</a></li>
<li><a href="../de416115/index.html">10 kleine Designfehler, die wir immer noch machen</a></li>
<li><a href="../de416119/index.html">Freitagspost am Mittwoch: Top der „wichtigsten“ NPM-Pakete</a></li>
<li><a href="../de416121/index.html">Fujitsu Artificial Intelligence berechnet die Geometrie magnetischer Materialien</a></li>
<li><a href="../de416125/index.html">Installation, Einrichtung des Systems und Steuerung für Kameras</a></li>
<li><a href="../de416127/index.html">CUDA und Remote-GPU</a></li>
<li><a href="../de416129/index.html">Wie KI lernt, Katzenbilder zu erzeugen</a></li>
<li><a href="../de416131/index.html">Wie man mit PD in der Russischen Föderation umgeht und nicht gegen das Gesetz verstößt</a></li>
<li><a href="../de416133/index.html">Rechenzentrum im Ausland: Equinix LD8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>