<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòµ ‚òπÔ∏è üè¨ Erkennung von Waren in Regalen mithilfe neuronaler Netze mithilfe der Keras- und Tensorflow-Objekterkennungs-API-Technologien üõê üë®‚Äçüë¶‚Äçüë¶ üë©üèº‚Äçü§ù‚Äçüë®üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In dem Artikel werden wir √ºber die Verwendung von Faltungs-Neuronalen Netzen sprechen, um eine praktische Gesch√§ftsaufgabe zu l√∂sen, bei der ein Realo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erkennung von Waren in Regalen mithilfe neuronaler Netze mithilfe der Keras- und Tensorflow-Objekterkennungs-API-Technologien</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/416123/">  In dem Artikel werden wir √ºber die Verwendung von Faltungs-Neuronalen Netzen sprechen, um eine praktische Gesch√§ftsaufgabe zu l√∂sen, bei der ein Realogramm aus Fotografien von Regalen mit Waren wiederhergestellt wird.  Mit der Tensorflow-Objekterkennungs-API trainieren wir das Such- / Lokalisierungsmodell.  Wir werden die Qualit√§t der Suche nach kleinen Produkten in hochaufl√∂senden Fotos mithilfe eines schwebenden Fensters und eines nicht maximalen Unterdr√ºckungsalgorithmus verbessern.  Bei Keras implementieren wir einen Klassifizierer von Waren nach Marke.  Parallel dazu werden wir Ans√§tze und Ergebnisse mit Entscheidungen von vor 4 Jahren vergleichen.  Alle im Artikel verwendeten Daten stehen zum Download zur Verf√ºgung. Der voll funktionsf√§hige Code befindet sich auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> und ist als Lernprogramm konzipiert. <br><br><img src="https://habrastorage.org/webt/aw/27/ar/aw27argaeseqsgos5cbpwdwt89s.jpeg"><br><a name="habracut"></a><br><h3>  Einf√ºhrung </h3><br>  Was ist ein Planogramm?  Das Layoutdiagramm der Warenanzeige auf den konkreten Handelsger√§ten des Gesch√§fts. <br><br>  Was ist ein Realogramm?  Das Layout der Waren auf einem bestimmten Handelsger√§t, das hier und jetzt im Gesch√§ft vorhanden ist. <br><br>  Planogramm - wie es sollte, Realogramm - was wir haben. <br><br><img src="https://habrastorage.org/webt/ym/oo/ew/ymooewlirqpaosatarrenno8caw.jpeg"><br><br>  Bis jetzt ist in vielen Gesch√§ften die Verwaltung der restlichen Waren in Regalen, Regalen, Theken und Regalen ausschlie√ülich Handarbeit.  Tausende Mitarbeiter pr√ºfen die Verf√ºgbarkeit von Produkten manuell, berechnen den Saldo, pr√ºfen den Standort anhand der Anforderungen.  Es ist teuer und Fehler sind sehr wahrscheinlich.  Eine falsche Anzeige oder ein Mangel an Waren f√ºhrt zu geringeren Ums√§tzen. <br><br>  Au√üerdem schlie√üen viele Hersteller Vereinbarungen mit Einzelh√§ndlern, um ihre Waren auszustellen.  Und da es viele Hersteller gibt, beginnt zwischen ihnen der Kampf um den besten Platz im Regal.  Jeder m√∂chte, dass sein Produkt in der Mitte gegen√ºber den Augen des K√§ufers liegt und die gr√∂√ütm√∂gliche Fl√§che einnimmt.  Es besteht Bedarf an einer kontinuierlichen Pr√ºfung. <br><br>  Tausende von Merchandisern ziehen von Gesch√§ft zu Gesch√§ft, um sicherzustellen, dass die Produkte ihres Unternehmens im Regal stehen und vertragsgem√§√ü pr√§sentiert werden.  Manchmal sind sie faul: Es ist viel angenehmer, einen Bericht zu erstellen, ohne das Haus zu verlassen, als zu einer Verkaufsstelle zu gehen.  Es ist eine st√§ndige Pr√ºfung der Abschlusspr√ºfer erforderlich. <br><br>  Die Aufgabe der Automatisierung und Vereinfachung dieses Prozesses ist nat√ºrlich schon lange gel√∂st.  Einer der schwierigsten Teile war die Bildverarbeitung: das Finden und Erkennen von Produkten.  Und erst vor relativ kurzer Zeit wurde diese Aufgabe so stark vereinfacht, dass f√ºr einen bestimmten Fall in vereinfachter Form die vollst√§ndige L√∂sung in einem Artikel beschrieben werden kann.  Das werden wir tun. <br><br>  Der Artikel enth√§lt ein Minimum an Code (nur f√ºr F√§lle, in denen der Code klarer als der Text ist).  Die Komplettl√∂sung ist als illustriertes Tutorial in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jupyter-Notizb√ºchern verf√ºgbar</a> .  Der Artikel enth√§lt keine Beschreibung der Architektur neuronaler Netze, der Prinzipien von Neuronen und mathematischer Formeln.  In diesem Artikel verwenden wir sie als Engineering-Tool, ohne zu sehr auf die Details des Ger√§ts einzugehen. <br><br><h3>  Daten und Ansatz </h3><br>  Wie bei jedem datengesteuerten Ansatz erfordern neuronale Netzwerkl√∂sungen Daten.  Sie k√∂nnen sie auch manuell zusammenstellen: um mehrere hundert Z√§hler zu erfassen und sie beispielsweise mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LabelImg zu markieren</a> .  Sie k√∂nnen Markups beispielsweise bei Yandex.Tolok bestellen. <br><br><img src="https://habrastorage.org/webt/ji/he/lv/jihelvxh9vkmixjzsxink1nknya.jpeg"><br><br>  Wir k√∂nnen die Details eines realen Projekts nicht offenlegen, daher werden wir die Technologie anhand offener Daten erl√§utern.  Es war zu faul, einkaufen zu gehen und Fotos zu machen (und wir w√ºrden uns dort nicht verstehen), und der Wunsch, die im Internet gefundenen Fotos selbst zu markieren, endete nach dem hundertsten klassifizierten Objekt.  Zum Gl√ºck bin ich ganz zuf√§llig auf das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grocery Dataset-</a> Archiv <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gesto√üen</a> . <br><br>  Im Jahr 2014 haben Mitarbeiter von Idea Teknoloji, Istanbul, T√ºrkei, 354 Fotos aus 40 Gesch√§ften hochgeladen, die mit 4 Kameras erstellt wurden.  Auf jeder dieser Fotografien wurden insgesamt mehrere tausend Objekte mit Rechtecken hervorgehoben, von denen einige in 10 Kategorien eingeteilt wurden. <br><br>  Dies sind Bilder von Zigarettenschachteln.  Wir f√∂rdern oder f√∂rdern das Rauchen nicht.  Es gab einfach nichts Neutraleres.  Wir versprechen, dass wir √ºberall im Artikel, wo es die Situation erlaubt, Fotos von Katzen verwenden werden. <br><br><img src="https://habrastorage.org/webt/04/8x/ek/048xekrylrnspiwd7vefbux_kgu.jpeg"><br><br>  Zus√§tzlich zu den mit Tags versehenen Fotos der Regale verfassten sie einen Artikel zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkennung von Einzelhandelsprodukten in Lebensmittelregalen</a> mit einer L√∂sung f√ºr das Problem der Lokalisierung und Klassifizierung.  Dies stellte eine Art Bezugspunkt dar: Unsere L√∂sung mit neuen Ans√§tzen sollte sich als einfacher und genauer herausstellen, da sie sonst nicht interessant ist.  Ihr Ansatz besteht aus einer Kombination von Algorithmen: <br><br><img src="https://habrastorage.org/webt/j5/p4/t_/j5p4t_ul9ungv_luvlzxa5aa1gi.jpeg"><br><br>  In j√ºngster Zeit haben Faltungs-Neuronale Netze (CNNs) das Gebiet der Bildverarbeitung revolutioniert und den Ansatz zur L√∂sung solcher Probleme grundlegend ge√§ndert.  In den letzten Jahren sind diese Technologien f√ºr eine Vielzahl von Entwicklern verf√ºgbar geworden, und APIs auf hoher Ebene wie Keras haben ihre Eintrittsschwelle erheblich gesenkt.  Jetzt kann fast jeder Entwickler nach nur wenigen Tagen Datierung die volle Leistung von Faltungs-Neuronalen Netzen nutzen.  Der Artikel beschreibt die Verwendung dieser Technologien anhand eines Beispiels und zeigt, wie eine ganze Kaskade von Algorithmen ohne Genauigkeitsverlust leicht durch nur zwei neuronale Netze ersetzt werden kann. <br><br>  Wir werden das Problem in Schritten l√∂sen: <br><br><ul><li>  Datenaufbereitung.  Wir pumpen die Archive aus und verwandeln sie in eine bequeme Ansicht f√ºr die Arbeit. </li><li>  Markenklassifizierung.  Wir l√∂sen das Klassifizierungsproblem mithilfe eines neuronalen Netzwerks. </li><li>  Suchen Sie nach Produkten auf dem Foto.  Wir trainieren das neuronale Netz, um nach Waren zu suchen. </li><li>  Suche Implementierung.  Wir werden die Erkennungsqualit√§t mithilfe eines schwebenden Fensters und eines Algorithmus zur Unterdr√ºckung von Nichtmaxima verbessern. </li><li>  Fazit  Erkl√§ren Sie kurz, warum das wirkliche Leben viel komplizierter ist als dieses Beispiel. </li></ul><br><h3>  Technologie </h3><br>  Die wichtigsten Technologien, die wir verwenden werden: Tensorflow, Keras, Tensorflow-Objekterkennungs-API, OpenCV.  Obwohl sowohl Windows als auch Mac OS f√ºr die Arbeit mit Tensorflow geeignet sind, empfehlen wir weiterhin die Verwendung von Ubuntu.  Selbst wenn Sie noch nie mit diesem Betriebssystem gearbeitet haben, sparen Sie durch die Verwendung eine Menge Zeit.  Die Installation von Tensorflow f√ºr die Arbeit mit der GPU ist ein Thema, das einen separaten Artikel verdient.  Gl√ºcklicherweise existieren solche Artikel bereits.  Beispiel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren von TensorFlow unter Ubuntu 16.04 mit einer Nvidia-GPU</a> .  Einige Anweisungen davon sind m√∂glicherweise veraltet. <br><br>  <b>Schritt 1. Daten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorbereiten</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github-Link</a> )</b> <br><br>  Dieser Schritt dauert in der Regel viel l√§nger als die Simulation selbst.  Gl√ºcklicherweise verwenden wir vorgefertigte Daten, die wir in das von uns ben√∂tigte Formular konvertieren. <br><br>  Sie k√∂nnen auf folgende Weise herunterladen und entpacken: <br><br><pre><code class="bash hljs">wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz tar -xvzf GroceryDataset_part1.tar.gz tar -xvzf GroceryDataset_part2.tar.gz</code> </pre> <br>  Wir erhalten folgende Ordnerstruktur: <br><br><img src="https://habrastorage.org/webt/n1/yi/8n/n1yi8n3faxzmxia70bxsee-b69u.jpeg"><br><br>  Wir verwenden Informationen aus den Verzeichnissen ShelfImages und ProductImagesFromShelves. <br>  ShelfImages enth√§lt Bilder der Regale selbst.  Im Namen ist die Kennung des Racks mit der Kennung des Bildes codiert.  Es k√∂nnen mehrere Bilder von einem Rack vorhanden sein.  Zum Beispiel ein Foto in seiner Gesamtheit und 5 Fotos in Teilen mit Schnittpunkten. <br><br>  Datei C1_P01_N1_S2_2.JPG (Rack C1_P01, Schnappschuss N1_S2_2): <br><br><img src="https://habrastorage.org/webt/nv/jd/or/nvjdorlqwj1qc7asuzktk7dqccs.jpeg"><br><br>  Wir gehen alle Dateien durch und sammeln Informationen im Pandas-Datenrahmen photos_df: <br><br><img src="https://habrastorage.org/webt/us/zq/zq/uszqzqw3haortnmdvscxp0sq1cq.png"><br>  ProductImagesFromShelves enth√§lt ausgeschnittene Fotos von Waren aus Regalen in 11 Unterverzeichnissen: 0 - nicht klassifiziert, 1 - Marlboro, 2 - Kent usw.  Um sie nicht zu bewerben, verwenden wir nur Kategorienummern ohne Angabe von Namen.  Dateien in den Namen enthalten Informationen √ºber das Rack, die Position und Gr√∂√üe der Packung. <br><br>  Datei C1_P01_N1_S3_1.JPG_1276_1828_276_448.png aus Verzeichnis 1 (Kategorie 1, Rack C1_P01, Bild N1_S3_1, Koordinaten der oberen linken Ecke (1276, 1828), Breite 276, H√∂he 448): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/-x/pc/f0-xpc4nqkymbj-ycs1my_sldhi.jpeg"></div><br>  Wir brauchen die Fotos der einzelnen Packungen selbst nicht (wir werden sie aus den Bildern der Regale ausschneiden) und sammeln Informationen √ºber ihre Kategorie und Position im Pandas-Datenrahmen products_df: <br><br><img src="https://habrastorage.org/webt/0x/nh/rv/0xnhrvv5nibludwh7svn54hj_ba.png"><br>  Im gleichen Schritt teilen wir alle unsere Informationen in zwei Abschnitte auf: Schulung f√ºr die Schulung und Validierung f√ºr die √úberwachung der Schulung.  In realen Projekten lohnt sich das nat√ºrlich nicht.  Und vertraue auch nicht denen, die dies tun.  Sie m√ºssen mindestens einen weiteren Test f√ºr den endg√ºltigen Test zuweisen.  Aber auch bei diesem nicht sehr ehrlichen Ansatz ist es wichtig, dass wir uns nicht viel t√§uschen. <br><br>  Wie bereits erw√§hnt, kann es mehrere Fotos von einem Rack geben.  Dementsprechend kann dieselbe Packung in mehrere Bilder fallen.  Wir empfehlen Ihnen daher, nicht nach Bildern, sondern vor allem nicht nach Packungen, sondern nach Gestellen aufzuschl√ºsseln.  Dies ist notwendig, damit nicht dasselbe Objekt aus verschiedenen Blickwinkeln sowohl im Zug als auch in der Validierung landet. <br><br>  Wir machen eine 70/30-Aufteilung (30% der Racks werden validiert, der Rest f√ºr das Training): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># get distinct shelves shelves = list(set(photos_df['shelf_id'].values)) # use train_test_split from sklearn shelves_train, shelves_validation, _, _ = train_test_split(   shelves, shelves, test_size=0.3, random_state=6) # mark all records in data frames with is_train flag def is_train(shelf_id): return shelf_id in shelves_train photos_df['is_train'] = photos_df.shelf_id.apply(is_train) products_df['is_train'] = products_df.shelf_id.apply(is_train)</span></span></code> </pre> <br>  Wir werden sicherstellen, dass bei der Trennung gen√ºgend Vertreter jeder Klasse f√ºr Schulung und Validierung zur Verf√ºgung stehen: <br><img src="https://habrastorage.org/webt/9n/w_/xj/9nw_xjw1qiqc21sbi5q3s0pv3_q.jpeg"><br>  Die blaue Farbe zeigt die Anzahl der Produkte in der Kategorie f√ºr die Validierung und die orange f√ºr die Schulung.  Mit der Kategorie 3 zur Validierung ist die Situation nicht sehr gut, aber im Prinzip gibt es nur wenige Vertreter. <br><br>  In der Phase der Datenaufbereitung ist es wichtig, keinen Fehler zu machen, da alle weiteren Arbeiten auf den Ergebnissen basieren.  Wir haben immer noch einen Fehler gemacht und viele gl√ºckliche Stunden damit verbracht zu verstehen, warum die Qualit√§t der Modelle sehr mittelm√§√üig ist.  F√ºhlte mich bereits als Verlierer der ‚ÄûOld School‚Äú -Technologien, bis Sie versehentlich bemerkten, dass einige der Originalfotos um 90 Grad gedreht wurden und einige verkehrt herum gemacht wurden. <br><br>  Gleichzeitig wird das Markup so erstellt, als ob die Fotos korrekt ausgerichtet w√§ren.  Nach einer schnellen L√∂sung ging es viel lustiger. <br><br>  Wir speichern unsere Daten in pkl-Dateien, um sie in den folgenden Schritten zu verwenden.  Insgesamt haben wir: <br><br><ul><li>  Ein Verzeichnis von Fotos von Gestellen und ihren Teilen mit B√ºndeln, </li><li>  Ein Datenrahmen mit einer Beschreibung jedes Racks mit einem Hinweis, ob es f√ºr das Training vorgesehen ist. </li><li>  Ein Datenrahmen mit Informationen zu allen Produkten in den Regalen, in denen Position, Gr√∂√üe, Kategorie und Kennzeichnung angegeben sind, ob sie f√ºr Schulungen vorgesehen sind. </li></ul><br>  Zur √úberpr√ºfung zeigen wir ein Rack gem√§√ü unseren Daten an: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function to display shelf photo with rectangled products def draw_shelf_photo(file):   file_products_df = products_df[products_df.file == file]   coordinates = file_products_df[['xmin', 'ymin', 'xmax', 'ymax']].values   im = cv2.imread(f'{shelf_images}{file}')   im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)      for xmin, ymin, xmax, ymax in coordinates:       cv2.rectangle(im, (xmin, ymin), (xmax, ymax), (0, 255, 0), 5)   plt.imshow(im) # draw one photo to check our data fig = plt.gcf() fig.set_size_inches(18.5, 10.5) draw_shelf_photo('C3_P07_N1_S6_1.JPG')</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/1m/7a/xr/1m7axrc3gcdvgt1sg0sc9-dbs0y.png"><br><br>  <b>Schritt 2. Klassifizierung nach Marke ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link auf Github</a> )</b> <br><br>  Die Klassifizierung von Bildern ist die Hauptaufgabe im Bereich der Bildverarbeitung.  Das Problem ist die ‚Äûsemantische L√ºcke‚Äú: Fotografie ist nur eine gro√üe Zahlenmatrix [0, 255].  Zum Beispiel 800x600x3 (3 RGB-Kan√§le). <br><br><img src="https://habrastorage.org/webt/0w/mi/rx/0wmirxot0tx0_dl_b-m_gpdunse.jpeg"><br><br>  Warum diese Aufgabe schwierig ist: <br><br><img src="https://habrastorage.org/webt/gx/il/fh/gxilfhn6woijdjngbgrbgfzgjmo.png"><br><br>  Wie bereits erw√§hnt, haben die Autoren der von uns verwendeten Daten 10 Marken identifiziert.  Dies ist eine √§u√üerst vereinfachte Aufgabe, da viel mehr Zigarettenmarken in den Regalen stehen.  Aber alles, was nicht in diese 10 Kategorien fiel, wurde an 0 gesendet - nicht klassifiziert: <br><br><img src="https://habrastorage.org/webt/wv/-j/hm/wv-jhmn18kt8ta1zhsai4fzdxbq.png">  "" <br><br>  Ihr Artikel bietet einen solchen Klassifizierungsalgorithmus mit einer Gesamtgenauigkeit von 92%: <br><img src="https://habrastorage.org/webt/vv/d3/ub/vvd3ubvr_tvjxasrwvs7jap6h7o.jpeg"><br>  Was werden wir tun: <br><br><ul><li>  Wir werden die Daten f√ºr das Training vorbereiten, </li><li>  Wir trainieren ein neuronales Faltungsnetzwerk mit der ResNet v1-Architektur. </li><li>  √úberpr√ºfen Sie die Fotos zur Validierung. </li></ul><br>  Es klingt "volumin√∂s", aber wir haben gerade Keras 'Beispiel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trainiert ein ResNet auf dem CIFAR10-Dataset</a> " verwendet, das die Funktion zum Erstellen von ResNet v1 √ºbernimmt. <br><br>  Um den Trainingsprozess zu starten, m√ºssen Sie zwei Arrays vorbereiten: x - Fotos von Packungen mit einer Dimension (Anzahl der Packungen, H√∂he, Breite, 3) und y - ihre Kategorien mit einer Dimension (Anzahl der Packungen, 10).  Das Array y enth√§lt die sogenannten 1-Hot-Vektoren.  Wenn die Kategorie eines Trainingspakets die Nummer 2 hat (von 0 bis 9), entspricht dies dem Vektor [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. <br><br>  Eine wichtige Frage ist, was mit der Breite und H√∂he zu tun ist, da alle Fotos mit unterschiedlichen Aufl√∂sungen aus unterschiedlichen Entfernungen aufgenommen wurden.  Wir m√ºssen eine feste Gr√∂√üe w√§hlen, zu der wir alle unsere Bilder der Packungen bringen k√∂nnen.  Diese feste Gr√∂√üe ist ein Metaparameter, der bestimmt, wie unser neuronales Netzwerk trainiert und funktioniert. <br><br>  Einerseits m√∂chte ich diese Gr√∂√üe so gro√ü wie m√∂glich machen, damit kein Detail des Bildes unbemerkt bleibt.  Auf der anderen Seite kann dies bei unserer geringen Menge an Trainingsdaten zu einer schnellen Umschulung f√ºhren: Das Modell funktioniert perfekt mit Trainingsdaten, aber schlecht mit Validierungsdaten.  Wir haben die Gr√∂√üe 120x80 gew√§hlt, vielleicht w√ºrden wir bei einer anderen Gr√∂√üe ein besseres Ergebnis erzielen.  Zoomfunktion: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># resize pack to fixed size SHAPE_WIDTH x SHAPE_HEIGHT def resize_pack(pack):   fx_ratio = SHAPE_WIDTH / pack.shape[1]   fy_ratio = SHAPE_HEIGHT / pack.shape[0]      pack = cv2.resize(pack, (0, 0), fx=fx_ratio, fy=fy_ratio)   return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]</span></span></code> </pre> <br>  Skalieren Sie eine Packung und zeigen Sie sie zur √úberpr√ºfung an.  Der Name der Marke ist f√ºr eine Person schwer zu lesen. Lassen Sie uns sehen, wie das neuronale Netzwerk mit der Aufgabe der Klassifizierung fertig wird: <br><br><img src="https://habrastorage.org/webt/z_/8p/0f/z_8p0f5kuxx27mneryilqv81ols.png"><br><br>  Nachdem wir uns gem√§√ü dem im vorherigen Schritt erhaltenen Flag vorbereitet haben, teilen wir die x- und y-Arrays in x_train / x_validation und y_train / y_validation auf. Wir erhalten: <br><br><pre> <code class="bash hljs">x_train shape: (1969, 120, 80, 3) y_train shape: (1969, 10) 1969 train samples 775 validation samples</code> </pre><br>  Wenn die Daten vorbereitet sind, kopieren wir die Funktion des neuronalen Netzwerkkonstruktors der ResNet v1-Architektur aus dem Keras-Beispiel: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">resnet_v1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape, depth, num_classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   ‚Ä¶</code> </pre> <br>  Wir konstruieren ein Modell: <br><br><pre> <code class="python hljs">model = resnet_v1(input_shape=x_train.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:], depth=depth, num_classes=num_classes) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>,             optimizer=Adam(lr=lr_schedule(<span class="hljs-number"><span class="hljs-number">0</span></span>)), metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  Wir haben einen ziemlich begrenzten Datensatz.  Um zu verhindern, dass das Modell w√§hrend des Trainings jedes Mal dasselbe Foto sieht, verwenden wir Augmentation: Verschieben Sie das Bild nach dem Zufallsprinzip und drehen Sie es ein wenig.  Keras bietet hierf√ºr folgende Optionen: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator(   featurewise_center=False,  # set input mean to 0 over the dataset   samplewise_center=False,  # set each sample mean to 0   featurewise_std_normalization=False,  # divide inputs by std of the dataset   samplewise_std_normalization=False,  # divide each input by its std   zca_whitening=False,  # apply ZCA whitening   rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)   width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)   height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)   horizontal_flip=False,  # randomly flip images   vertical_flip=False)  # randomly flip images datagen.fit(x_train)</span></span></code> </pre> <br>  Wir beginnen den Trainingsprozess. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's run training process, 20 epochs is enough batch_size = 50 epochs = 15 model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),                   validation_data=(x_validation, y_validation),                   epochs=epochs, verbose=1, workers=4,                   callbacks=[LearningRateScheduler(lr_schedule)])</span></span></code> </pre> <br>  Nach dem Training und der Bewertung erhalten wir eine Genauigkeit im Bereich von 92%.  M√∂glicherweise erhalten Sie eine andere Genauigkeit: Es gibt nur sehr wenige Daten, daher h√§ngt die Genauigkeit stark vom Erfolg der Partition ab.  Auf dieser Partition haben wir keine wesentlich h√∂here Genauigkeit als im Artikel angegeben, aber wir haben selbst praktisch nichts getan und wenig Code geschrieben.  Dar√ºber hinaus k√∂nnen wir leicht eine neue Kategorie hinzuf√ºgen, und die Genauigkeit sollte (theoretisch) erheblich zunehmen, wenn wir mehr Daten vorbereiten. <br><br>  Vergleichen Sie bei Interesse die Verwirrungsmatrizen: <br><img src="https://habrastorage.org/webt/ee/ns/tm/eenstmjcconudxyjjkmtpzcdmow.jpeg"><br>  Fast alle Kategorien, die unser neuronales Netzwerk definiert, mit Ausnahme der Kategorien 4 und 7. Es ist auch n√ºtzlich, die hellsten Vertreter jeder Verwirrungsmatrixzelle zu betrachten: <br><img src="https://habrastorage.org/webt/qv/tm/lw/qvtmlwxgqvbdhut73zgsbbtfqqo.jpeg"><br>  Sie k√∂nnen auch verstehen, warum das Parlament mit Camel verwechselt wurde, aber warum Winston mit Lucky Strike verwechselt wurde, ist v√∂llig unverst√§ndlich, aber sie haben nichts gemeinsam.  Dies ist das Hauptproblem neuronaler Netze - die vollst√§ndige Undurchsichtigkeit dessen, was im Inneren geschieht.  Sie k√∂nnen nat√ºrlich einige Ebenen visualisieren, aber f√ºr uns sieht diese Visualisierung folgenderma√üen aus: <br><br><img src="https://habrastorage.org/webt/w1/tn/et/w1tnetor61yz-uwvmjlserwvh3m.jpeg"><br><br>  Eine offensichtliche M√∂glichkeit, die Erkennungsqualit√§t unter unseren Bedingungen zu verbessern, besteht darin, weitere Fotos hinzuzuf√ºgen. <br><br>  Der Klassifikator ist also fertig.  Gehe zum Detektor. <br><br>  <b>Schritt 3. Suchen Sie nach Produkten auf dem Foto ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link auf Github</a> )</b> <br><br>  Die folgenden wichtigen Aufgaben im Bereich Computer Vision sind semantische Segmentierung, Lokalisierung, Objektsuche und Instanzsegmentierung. <br><br><img src="https://habrastorage.org/webt/z0/y9/bf/z0y9bfe7f6qp143c3wubj8rvnd8.jpeg"><br><br>  Unsere Aufgabe erfordert die Objekterkennung.  Der Artikel von 2014 bietet einen Ansatz, der auf der Viola-Jones- und der HOG-Methode mit visueller Genauigkeit basiert: <br><br><img src="https://habrastorage.org/webt/11/jt/8j/11jt8jcggyutkrxyelswr8psm8s.jpeg"><br><br>  Dank der Verwendung zus√§tzlicher statistischer Einschr√§nkungen ist ihre Genauigkeit sehr gut: <br><br><img src="https://habrastorage.org/webt/od/nb/yl/odnbylwn0yo-k92q_nu6mazz8fq.jpeg"><br><br>  Jetzt wird die Aufgabe der Objekterkennung mit Hilfe neuronaler Netze erfolgreich gel√∂st.  Wir werden das Tensorflow Object Detection API-System verwenden und ein neuronales Netzwerk mit der Mobilenet V1 SSD-Architektur trainieren.  Das Trainieren eines solchen Modells von Grund auf erfordert viele Daten und kann Tage dauern. Daher verwenden wir ein Modell, das auf COCO-Daten nach dem Prinzip des Transferlernens trainiert wurde. <br><br>  Das Schl√ºsselkonzept dieses Ansatzes ist dies.  Warum muss ein Kind nicht Millionen von Objekten zeigen, damit es lernt, einen Ball von einem W√ºrfel zu finden und zu unterscheiden?  Weil das Kind 500 Millionen Jahre Entwicklung des visuellen Kortex hat.  Die Evolution hat das Sehen zum gr√∂√üten sensorischen System gemacht.  Fast 50% (aber das ist nicht genau) der Neuronen des menschlichen Gehirns sind f√ºr die Bildverarbeitung verantwortlich.  Eltern k√∂nnen nur den Ball und den W√ºrfel zeigen und das Kind dann mehrmals korrigieren, damit es das eine perfekt findet und voneinander unterscheidet. <br><br>  Aus philosophischer Sicht (mit mehr als allgemeinen technischen Unterschieden) funktioniert das Transferlernen in neuronalen Netzen auf √§hnliche Weise.  Faltungs-Neuronale Netze bestehen aus Ebenen, von denen jede immer komplexere Formen definiert: Sie identifizieren Schl√ºsselpunkte, kombinieren sie zu Linien, die sich wiederum zu Figuren verbinden.  Und erst auf der letzten Ebene bestimmt aus der Gesamtheit der gefundenen Zeichen das Objekt. <br><br>  Objekte der realen Welt haben viel gemeinsam.  Beim Transferlernen verwenden wir die bereits trainierten Definitionsebenen der Grundfunktionen und trainieren nur die Ebenen, die f√ºr die Identifizierung von Objekten verantwortlich sind.  Dazu reichen uns ein paar hundert Fotos und ein paar Betriebsstunden einer normalen GPU.  Das Netzwerk wurde urspr√ºnglich auf dem COCO-Datensatz (Microsoft Common Objects in Context) trainiert, der 91 Kategorien und 2.500.000 Bilder umfasst!  Viele, wenn auch nicht 500 Millionen Jahre Evolution. <br><br>  Mit Blick auf die Zukunft visualisiert diese GIF-Animation (etwas langsam, nicht sofort scrollen) vom Tensorboard den Lernprozess.  Wie Sie sehen k√∂nnen, liefert das Modell fast sofort ein vollst√§ndig hochwertiges Ergebnis, und dann kommt das Schleifen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d2d/356/aa4/d2d356aa49bedf31aa918930d8bf1545.gif" alt="Bild"><br><br>  Der ‚ÄûTrainer‚Äú des Tensorflow-Objekterkennungs-API-Systems kann unabh√§ngig voneinander eine Erweiterung durchf√ºhren, zuf√§llige Teile von Bildern f√ºr das Training ausschneiden und ‚Äûnegative‚Äú Beispiele ausw√§hlen (Fotoabschnitte, die keine Objekte enthalten).  Theoretisch ist keine Fotovorverarbeitung erforderlich.  Auf einem Heimcomputer mit Festplatte und wenig RAM weigerte er sich jedoch, mit hochaufl√∂senden Bildern zu arbeiten: Zuerst hing er lange, raschelte mit einer Festplatte und flog dann heraus. <br><br>  Infolgedessen haben wir die Fotos auf eine Gr√∂√üe von 1000 x 1000 Pixel komprimiert, w√§hrend das Seitenverh√§ltnis beibehalten wurde.  Da jedoch beim Komprimieren eines gro√üen Fotos viele Zeichen verloren gehen, wurden zun√§chst mehrere Quadrate einer zuf√§lligen Gr√∂√üe aus jedem Foto des Racks ausgeschnitten und in 1000 x 1000 gepresst.  Infolgedessen fielen Packs in hoher Aufl√∂sung (aber nicht genug) und in kleinen (aber vielen) in die Trainingsdaten.  Wir wiederholen: Dieser Schritt ist erzwungen und h√∂chstwahrscheinlich v√∂llig unn√∂tig und m√∂glicherweise sch√§dlich. <br><br>  Die vorbereiteten und komprimierten Fotos werden in separaten Verzeichnissen (eval und train) gespeichert und ihre Beschreibung (mit den darin enthaltenen B√ºndeln) wird in Form von zwei Pandas-Datenrahmen (train_df und eval_df) gebildet: <br><br><img src="https://habrastorage.org/webt/dl/lx/pn/dllxpn03xu5us7wr6h7y3rgzayg.png"><br>  Das Tensorflow Object Detection API-System erfordert, dass Eingaben als tfrecord-Dateien dargestellt werden.  Sie k√∂nnen sie mit dem Dienstprogramm erstellen, aber wir machen daraus einen Code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">class_text_to_int</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(row_label)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> row_label == <span class="hljs-string"><span class="hljs-string">'pack'</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">split</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, group)</span></span></span><span class="hljs-function">:</span></span>   data = namedtuple(<span class="hljs-string"><span class="hljs-string">'data'</span></span>, [<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, <span class="hljs-string"><span class="hljs-string">'object'</span></span>])   gb = df.groupby(group)   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [data(filename, gb.get_group(x))           <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename, x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(gb.groups.keys(), gb.groups)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_tf_example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(group, path)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.gfile.GFile(os.path.join(path, <span class="hljs-string"><span class="hljs-string">'{}'</span></span>.format(group.filename)), <span class="hljs-string"><span class="hljs-string">'rb'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> fid:       encoded_jpg = fid.read()   encoded_jpg_io = io.BytesIO(encoded_jpg)   image = Image.open(encoded_jpg_io)   width, height = image.size   filename = group.filename.encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>)   image_format = <span class="hljs-string"><span class="hljs-string">b'jpg'</span></span>   xmins = []   xmaxs = []   ymins = []   ymaxs = []   classes_text = []   classes = []   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> index, row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> group.object.iterrows():       xmins.append(row[<span class="hljs-string"><span class="hljs-string">'xmin'</span></span>] / width)       xmaxs.append(row[<span class="hljs-string"><span class="hljs-string">'xmax'</span></span>] / width)       ymins.append(row[<span class="hljs-string"><span class="hljs-string">'ymin'</span></span>] / height)       ymaxs.append(row[<span class="hljs-string"><span class="hljs-string">'ymax'</span></span>] / height)       classes_text.append(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>].encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>))       classes.append(class_text_to_int(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>]))   tf_example = tf.train.Example(features=tf.train.Features(feature={       <span class="hljs-string"><span class="hljs-string">'image/height'</span></span>: dataset_util.int64_feature(height),       <span class="hljs-string"><span class="hljs-string">'image/width'</span></span>: dataset_util.int64_feature(width),       <span class="hljs-string"><span class="hljs-string">'image/filename'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/source_id'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/encoded'</span></span>: dataset_util.bytes_feature(encoded_jpg),       <span class="hljs-string"><span class="hljs-string">'image/format'</span></span>: dataset_util.bytes_feature(image_format),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmin'</span></span>: dataset_util.float_list_feature(xmins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmax'</span></span>: dataset_util.float_list_feature(xmaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymin'</span></span>: dataset_util.float_list_feature(ymins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymax'</span></span>: dataset_util.float_list_feature(ymaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/class/text'</span></span>: dataset_util.bytes_list_feature(classes_text),       <span class="hljs-string"><span class="hljs-string">'image/object/class/label'</span></span>: dataset_util.int64_list_feature(classes),   }))   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf_example <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert_to_tf_records</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_path, examples, dst_file)</span></span></span><span class="hljs-function">:</span></span>   writer = tf.python_io.TFRecordWriter(dst_file)   grouped = split(examples, <span class="hljs-string"><span class="hljs-string">'filename'</span></span>)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> grouped:       tf_example = create_tf_example(group, images_path)       writer.write(tf_example.SerializeToString())   writer.close() convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">train/'</span></span>, train_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">train.record'</span></span>) convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">eval/'</span></span>, eval_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">eval.record'</span></span>)</code> </pre><br>  Es bleibt uns √ºberlassen, ein spezielles Verzeichnis vorzubereiten und die Prozesse zu starten: <br><br><img src="https://habrastorage.org/webt/ld/yd/rb/ldydrbk3ivkixsx9kep69b5eyoy.jpeg"><br><br>  Die Struktur mag unterschiedlich sein, aber wir finden es sehr praktisch. <br><br>  Das Datenverzeichnis enth√§lt die Dateien, die wir mit tfrecords (train.record und eval.record) erstellt haben, sowie pack.pbtxt mit den Objekttypen, f√ºr die wir das neuronale Netzwerk trainieren werden.  Wir m√ºssen nur einen Objekttyp definieren, daher ist die Datei sehr kurz: <br><br><img src="https://habrastorage.org/webt/pc/jw/x0/pcjwx0l3fc-wuzhiyt7lfo9xtcq.png"><br><br>  Das Modellverzeichnis (es kann viele Modelle zur L√∂sung eines Problems geben) im untergeordneten Verzeichnis ssd_mobilenet_v1 enth√§lt die Einstellungen f√ºr das Training in der Datei .config sowie zwei leere Verzeichnisse: train und eval.  Im Zug speichert der ‚ÄûTrainer‚Äú die Modellkontrollpunkte, der ‚ÄûBewerter‚Äú nimmt sie auf, f√ºhrt sie f√ºr die zu bewertenden Daten aus und legt sie im Bewertungsverzeichnis ab.  Tensorboard verfolgt diese beiden Verzeichnisse und zeigt Prozessinformationen an. <br><br>  Detaillierte Beschreibung der Struktur von Konfigurationsdateien usw.  finden Sie <a href="">hier</a> und <a href="">hier</a> .  Installationsanweisungen f√ºr die Tensorflow Object Detection API finden Sie <a href="">hier</a> . <br><br>  Wir gehen in das Verzeichnis models / research / object_detection und entleeren das vorab trainierte Modell: <br><br><pre> <code class="bash hljs">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz tar -xvzf ssd_mobilenet_v1_coco_2017_11_17.tar.gz</code> </pre><br>  Dort kopieren wir das von uns erstellte Verzeichnis pack_detector. <br><br>  Starten Sie zun√§chst den Trainingsprozess: <br><br><pre> <code class="bash hljs">python3 train.py --logtostderr \   --train_dir=pack_detector/models/ssd_mobilenet_v1/train/ \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config</code> </pre><br>  Wir starten den Bewertungsprozess.  Wir haben keine zweite Grafikkarte, daher starten wir sie auf dem Prozessor (mit der Anweisung CUDA_VISIBLE_DEVICES = "").  Aus diesem Grund wird er sehr sp√§t in Bezug auf den Trainingsprozess sein, aber das ist nicht so schlimm: <br><br><pre> <code class="bash hljs">CUDA_VISIBLE_DEVICES=<span class="hljs-string"><span class="hljs-string">""</span></span> python3 eval.py \   --logtostderr \   --checkpoint_dir=pack_detector/models/ssd_mobilenet_v1/train \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --eval_dir=pack_detector/models/ssd_mobilenet_v1/<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span></code> </pre> <br>  Wir starten den Tensorboard-Prozess: <br><br><pre> <code class="bash hljs">tensorboard --logdir=pack_detector/models/ssd_mobilenet_v1</code> </pre> <br>  Danach k√∂nnen wir sch√∂ne Grafiken sowie die tats√§chliche Arbeit des Modells an den gesch√§tzten Daten sehen (GIF am Anfang): <br><br><img src="https://habrastorage.org/webt/qc/wt/rl/qcwtrlmdugb4zgyhy6gnnaoia9w.jpeg"><br><br>  Der Trainingsprozess kann jederzeit gestoppt und fortgesetzt werden.  Wenn wir glauben, dass das Modell gut genug ist, speichern wir den Pr√ºfpunkt in Form eines Inferenzgraphen: <br><br><pre> <code class="bash hljs">python3 export_inference_graph.py \   --input_type image_tensor \   --pipeline_config_path pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --trained_checkpoint_prefix pack_detector/models/ssd_mobilenet_v1/train/model.ckpt-13756 \   --output_directory pack_detector/models/ssd_mobilenet_v1/pack_detector_2018_06_03</code> </pre> <br>  In diesem Schritt haben wir also ein Inferenzdiagramm erhalten, mit dem wir nach B√ºndelobjekten suchen k√∂nnen.  Wir gehen zu seiner Verwendung √ºber. <br><br>  <b>Schritt 4. Implementierung der Suche ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github-Link</a> )</b> <br><br>  Der Lade- und Initialisierungscode des Inferenzdiagramms befindet sich unter dem obigen Link.  Wichtige Suchfunktionen: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's write function that executes detection def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):   # Run inference   expanded_dims = np.expand_dims(image, 0)   output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})   # all outputs are float32 numpy arrays, so convert types as appropriate   output_dict['num_detections'] = int(output_dict['num_detections'][0])   output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)   output_dict['detection_boxes'] = output_dict['detection_boxes'][0]   output_dict['detection_scores'] = output_dict['detection_scores'][0]   return output_dict # it is useful to be able to run inference not only on the whole image, # but also on its parts # cutoff - minimum detection score needed to take box def run_inference_for_image_part(image_tensor, sess, tensor_dict,                                image, cutoff, ax0, ay0, ax1, ay1):   boxes = []   im = image[ay0:ay1, ax0:ax1]   h, w, c = im.shape   output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)   for i in range(100):       if output_dict['detection_scores'][i] &lt; cutoff:           break       y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \                               output_dict['detection_scores'][i]       x0, y0, x1, y1, score = int(x0*w), int(y0*h), \                               int(x1*w), int(y1*h), \                               int(score * 100)       boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))   return boxes</span></span></code> </pre> <br>  Die Funktion findet begrenzte K√§stchen f√ºr Packs nicht auf dem gesamten Foto, sondern in seinem Teil.  Die Funktion filtert auch die gefundenen Rechtecke mit einer niedrigen Erkennungsbewertung heraus, die im Cutoff-Parameter angegeben ist. <br><br>  Es stellt sich als Dilemma heraus.  Einerseits verlieren wir mit einem hohen Cutoff viele Objekte, andererseits finden wir mit einem niedrigen Cutoff viele Objekte, die keine B√ºndel sind.  Gleichzeitig finden wir immer noch nicht alles und nicht ideal: <br><img src="https://habrastorage.org/webt/-u/ao/7y/-uao7ylwycrzrn3xqd1kfh2q0bs.jpeg"><br>  Beachten Sie jedoch, dass die Erkennung mit Cutoff = 0,9 nahezu perfekt ist, wenn wir die Funktion f√ºr ein kleines St√ºck des Fotos ausf√ºhren. <br><br><img src="https://habrastorage.org/webt/gg/qn/ia/ggqniasnrkggb6bjnarwum_6i48.jpeg"><br><br>  Dies liegt an der Tatsache, dass das MobileNet V1 SSD-Modell 300 x 300 Fotos als Eingabe akzeptiert.  Nat√ºrlich gehen bei einer solchen Komprimierung viele Zeichen verloren. <br><br>  Diese Zeichen bleiben jedoch bestehen, wenn wir ein kleines Quadrat mit mehreren Packungen ausschneiden.  Dies legt die Idee nahe, ein schwebendes Fenster zu verwenden: Wir laufen durch ein kleines Rechteck auf einem Foto und erinnern uns an alles, was wir gefunden haben. <br><br><img src="https://habrastorage.org/webt/zn/7s/dz/zn7sdzl4wcb9dwugzkxilg2d2hk.jpeg"><br><br>  :          ,     .         .   :          (detection score),  ,    ,        overlapTresh (       ): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function for non-maximum suppression def non_max_suppression(boxes, overlapThresh):   if len(boxes) == 0:       return np.array([]).astype("int")   if boxes.dtype.kind == "i":       boxes = boxes.astype("float")   pick = []   x1 = boxes[:,0]   y1 = boxes[:,1]   x2 = boxes[:,2]   y2 = boxes[:,3]   sc = boxes[:,4]   area = (x2 - x1 + 1) * (y2 - y1 + 1)   idxs = np.argsort(sc)   while len(idxs) &gt; 0:       last = len(idxs) - 1       i = idxs[last]       pick.append(i)       xx1 = np.maximum(x1[i], x1[idxs[:last]])       yy1 = np.maximum(y1[i], y1[idxs[:last]])       xx2 = np.minimum(x2[i], x2[idxs[:last]])       yy2 = np.minimum(y2[i], y2[idxs[:last]])       w = np.maximum(0, xx2 - xx1 + 1)       h = np.maximum(0, yy2 - yy1 + 1)       #todo fix overlap-contains...       overlap = (w * h) / area[idxs[:last]]              idxs = np.delete(idxs, np.concatenate(([last],           np.where(overlap &gt; overlapThresh)[0])))     return boxes[pick].astype("int")</span></span></code> </pre> <br>     : <br><br><img src="https://habrastorage.org/webt/go/hd/yz/gohdyzvwuyu8ja4w893big9c1yy.jpeg"><br><br>          : <br><br><img src="https://habrastorage.org/webt/jm/oi/da/jmoida7irvm4u3drey8nu6g00kw.jpeg"><br><br>   ,           ,    . <br><br><h3>  Fazit </h3><br>       ¬´¬ª:         ,       . ,    ,         ..    . <br><br>       ,    ,    : <br><br><ol><li>  150  ,     ,   , </li><li>        3-7  , </li><li>   100    , </li><li>        , </li><li>        (), </li><li>    (,  ), </li><li>    ,        ¬´¬ª, </li><li>  ,   ,     (SSD  ), </li><li>      ,  , </li><li>  . </li></ol><br>         ,      ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416123/">https://habr.com/ru/post/de416123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416111/index.html">GraphQL-Datenkonvertierung f√ºr CustomTreeData-Komponente von DevExtreme-Reactive</a></li>
<li><a href="../de416113/index.html">Steven Wolfram: Erinnerungen an Steve Jobs</a></li>
<li><a href="../de416115/index.html">10 kleine Designfehler, die wir immer noch machen</a></li>
<li><a href="../de416119/index.html">Freitagspost am Mittwoch: Top der ‚Äûwichtigsten‚Äú NPM-Pakete</a></li>
<li><a href="../de416121/index.html">Fujitsu Artificial Intelligence berechnet die Geometrie magnetischer Materialien</a></li>
<li><a href="../de416125/index.html">Installation, Einrichtung des Systems und Steuerung f√ºr Kameras</a></li>
<li><a href="../de416127/index.html">CUDA und Remote-GPU</a></li>
<li><a href="../de416129/index.html">Wie KI lernt, Katzenbilder zu erzeugen</a></li>
<li><a href="../de416131/index.html">Wie man mit PD in der Russischen F√∂deration umgeht und nicht gegen das Gesetz verst√∂√üt</a></li>
<li><a href="../de416133/index.html">Rechenzentrum im Ausland: Equinix LD8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>