<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📽️ 👩‍❤️‍💋‍👩 🧓🏼 Introdução à tarefa de reconhecer emoções 🥩 🙇🏿 🙇🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O reconhecimento de emoções é um tópico importante no campo da inteligência artificial. As áreas mais interessantes de aplicação de tais tecnologias i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introdução à tarefa de reconhecer emoções</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/speechpro/blog/418151/"><p>  O reconhecimento de emoções é um tópico importante no campo da inteligência artificial.  As áreas mais interessantes de aplicação de tais tecnologias incluem: reconhecimento de motorista, pesquisa de mercado, sistemas de análise de vídeo para cidades inteligentes, interação homem-máquina, monitoramento de estudantes de cursos on-line, dispositivos vestíveis, etc. </p><br><p>  Este ano, a MDG dedicou sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escola de aprendizado de máquina de verão</a> a esse tópico.  Neste artigo, tentarei fazer uma breve excursão ao problema de reconhecer o estado emocional de uma pessoa e falar sobre abordagens para sua solução. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/626/db4/5bb/626db45bb7b0aad7fdbc2970c0b4183f.jpg" alt="imagem"></div><a name="habracut"></a><br><h3 id="chto-takoe-emocii">  O que são emoções? </h3><br><p>  A emoção é um tipo especial de processo mental que expressa a experiência de uma pessoa em seu relacionamento com o mundo e com ela mesma.  De acordo com uma das teorias, cujo autor é o fisiologista russo P.K.  Além disso, a capacidade de experimentar emoções foi desenvolvida no processo de evolução como um meio de adaptação mais bem-sucedida dos seres vivos às condições de existência.  A emoção foi útil para a sobrevivência e permitiu que os seres vivos respondessem rápida e economicamente às influências externas. </p><br><p>  As emoções desempenham um papel enorme na vida humana e na comunicação interpessoal.  Eles podem ser expressos de várias maneiras: expressões faciais, postura, reações motoras, voz e reações autonômicas (frequência cardíaca, pressão arterial, frequência respiratória).  No entanto, o rosto da pessoa tem mais expressividade. </p><br><p>  Cada pessoa expressa emoções de várias maneiras diferentes.  O famoso psicólogo americano <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Paul Ekman</a> , estudando o comportamento não verbal de tribos isoladas na Papua Nova Guiné nos anos 70 do século passado, descobriu que várias emoções, a saber: raiva, medo, tristeza, repulsa, desprezo, surpresa e alegria são universais e podem para ser entendido pelo homem, independentemente de sua cultura. </p><br><p>  As pessoas são capazes de expressar uma ampla gama de emoções.  Acredita-se que eles possam ser descritos como uma combinação de emoções básicas (por exemplo, nostalgia é algo entre tristeza e alegria).  Mas essa abordagem categórica nem sempre é conveniente, porque  não permite quantificar o poder da emoção.  Portanto, juntamente com modelos discretos de emoções, foram desenvolvidos vários contínuos.  O modelo de J. Russell tem uma base bidimensional na qual cada emoção é caracterizada por um sinal (valência) e intensidade (excitação).  Devido à sua simplicidade, o modelo Russell tornou-se cada vez mais popular no contexto da tarefa de classificar automaticamente expressões faciais. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/f59/cd1/a24/f59cd1a24e77d64759641a076d86bef1.png" alt="imagem"></div><br><p>  Então, descobrimos que, se você não está tentando esconder a excitação emocional, seu estado atual pode ser estimado por expressões faciais.  Além disso, usando conquistas modernas no campo do aprendizado profundo, é até possível construir um detector de mentiras baseado na série “Lie to me”, cuja base científica foi fornecida diretamente por Paul Ekman.  No entanto, esta tarefa está longe de ser simples.  Como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mostraram</a> os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">estudos da</a> neurocientista Lisa Feldman Barrett, ao reconhecer emoções, uma pessoa usa ativamente informações contextuais: voz, ações, situação.  Dê uma olhada nas fotos abaixo, realmente é.  Usando apenas a área do rosto, uma previsão correta é impossível.  Nesse sentido, para resolver esse problema, é necessário usar modalidades e informações adicionais sobre alterações nos sinais ao longo do tempo. </p><br><div style="text-align:center;"><img height="200" src="https://habrastorage.org/webt/el/a6/t7/ela6t7ig73rti-peenhb7_f0sgs.jpeg" alt="imagem"></div><br><p>  Aqui consideraremos abordagens para a análise de apenas duas modalidades: áudio e vídeo, uma vez que esses sinais podem ser obtidos sem contato.  Para abordar a tarefa, você primeiro precisa obter os dados.  Aqui está uma lista dos maiores bancos de dados de emoções disponíveis ao público que conheço  Imagens e vídeos nesses bancos de dados foram marcados manualmente, alguns usando o Amazon Mechanical Turk. </p><br><table><thead><tr><th>  Título </th><th>  Dados </th><th>  Marcação </th><th>  Ano de fabricação </th></tr></thead><tbody><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio OMG-Emotion</a> </td><td>  áudio / vídeo </td><td>  7 categorias, valência / excitação </td><td>  2018 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio Emotiw</a> </td><td>  áudio / vídeo </td><td>  6 categorias </td><td>  2018 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Affectnet</a> </td><td>  imagens </td><td>  7 categorias, valência / excitação </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AFEW-VA</a> </td><td>  o video </td><td>  valência / excitação </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio EmotioNet</a> </td><td>  imagens </td><td>  16 categorias </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Emoreact</a> </td><td>  áudio / vídeo </td><td>  17 categorias </td><td>  2016 </td></tr></tbody></table><br><h3 id="klassicheskiy-podhod-k-zadache-klassifikacii-emociy">  A abordagem clássica para a classificação das emoções </h3><br><p>  A maneira mais fácil de determinar a emoção de uma imagem de rosto é baseada na classificação de pontos-chave (pontos faciais), cujas coordenadas podem ser obtidas usando vários algoritmos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PDM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CML</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AAM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DPM</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CNN</a> .  Normalmente marque de 5 a 68 pontos, amarrando-os na posição das sobrancelhas, olhos, lábios, nariz, mandíbula, o que permite capturar parcialmente expressões faciais.  As coordenadas normalizadas dos pontos podem ser enviadas diretamente ao classificador (por exemplo, SVM ou Random Forest) e obter uma solução básica.  Naturalmente, a posição das pessoas deve estar alinhada. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/c60/24a/dbe/c6024adbeaecca98404dcaae3361785e.jpg" alt="imagem"></div><br><p>  O uso simples de coordenadas sem um componente visual leva a uma perda significativa de informações úteis, portanto, vários descritores são computados nesses pontos para melhorar o sistema: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LBP</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LATCH</a> etc. Depois que os descritores são concatenados e a dimensão é reduzida usando PCA, o vetor de recurso resultante pode ser usado para classificação emoções. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/lh/oa/u7lhoatsm4vbqzb_zlksw9ekygy.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><p>  No entanto, essa abordagem já é considerada obsoleta, pois é sabido que redes de convolução profunda são a melhor opção para a análise de dados visuais. </p><br><h3 id="klassifikaciya-emociy-s-primeneniem-deep-learning">  Classificação de emoções usando aprendizagem profunda </h3><br><p>  Para criar um classificador de rede neural, basta ter alguma rede com uma arquitetura básica, previamente treinada no ImageNet, e treinar novamente as últimas camadas.  Assim, você pode obter uma boa solução básica para classificar vários dados, mas, considerando as especificidades da tarefa, as redes neurais usadas para tarefas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reconhecimento de rosto em</a> larga escala serão mais adequadas. </p><br><p>  Portanto, é bastante simples criar um classificador de emoções para imagens individuais, mas, como descobrimos, os instantâneos não refletem com precisão as verdadeiras emoções que uma pessoa experimenta em uma determinada situação.  Portanto, para aumentar a precisão do sistema, é necessário analisar a sequência de quadros.  Existem duas maneiras de fazer isso.  A primeira maneira é alimentar recursos de alto nível recebidos de uma CNN que classifique cada quadro individual em uma rede recorrente (por exemplo, LSTM) para capturar o componente de tempo. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/webt/ik/wi/zh/ikwizhwy65xydfoyu15ql3szjbi.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><p>  A segunda maneira é alimentar diretamente uma sequência de quadros tirados do vídeo em algumas etapas para a entrada 3D-CNN.  CNNs semelhantes usam convoluções com três graus de liberdade que transformam a entrada quadridimensional em mapas de características tridimensionais. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/iv/a2/kd/iva2kdloqzpghk8gfcymk6g5sc4.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><p>  De fato, no caso geral, essas duas abordagens podem ser combinadas com a construção de um monstro. </p><br><div style="text-align:center;"><img height="400" src="https://habrastorage.org/webt/oo/ir/yo/ooiryorm9sh8n0cht5oud1yqbd4.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><h3 id="klassifikaciya-emociy-po-rechi">  Classificação de fala das emoções </h3><br><p>  Com base nos dados visuais, o sinal de emoção pode ser previsto com alta precisão, mas é preferível usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sinais de fala</a> ao determinar a intensidade.  A análise do áudio é um pouco mais difícil devido à alta variabilidade da duração da fala e das vozes dos falantes.  Geralmente, eles não usam a onda sonora original, mas vários conjuntos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atributos</a> , por exemplo: F0, MFCC, LPC, i-vetores, etc. No problema de reconhecer emoções pela fala, a biblioteca aberta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenSMILE</a> tem uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">boa reputação</a> e contém um rico conjunto de algoritmos para análise de fala e música. sinais.  Após a extração, os atributos podem ser enviados ao SVM ou LSTM para classificação. </p><br><p>  Recentemente, no entanto, redes neurais convolucionais também começaram a penetrar no campo da análise sonora, substituindo as abordagens estabelecidas.  Para aplicá-los, o som é representado na forma de espectrogramas em escala linear ou em escala mel, após o que são operados com os espectrogramas obtidos, como nas imagens bidimensionais comuns.  Nesse caso, o problema de um tamanho arbitrário de espectrogramas ao longo do eixo do tempo é elegantemente resolvido usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pool estatístico</a> ou incorporando uma rede recorrente na arquitetura. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kq/4d/ts/kq4dtsbybmnn3nsq6cwq_wek19u.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><h3 id="audiovizualnoe-raspoznavanie-emociy">  Reconhecimento audiovisual das emoções </h3><br><p>  Assim, examinamos várias abordagens para a análise das modalidades de áudio e vídeo, o estágio final permaneceu - a combinação de classificadores para produzir a solução final.  A maneira mais simples é combinar diretamente suas classificações.  Nesse caso, basta levar o máximo ou a média.  Uma opção mais difícil é combinar no nível de incorporação para cada modalidade.  O SVM é frequentemente usado para isso, mas isso nem sempre é correto, pois os incorporamentos podem ter uma taxa diferente.  Nesse sentido, algoritmos mais avançados foram desenvolvidos, por exemplo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Multiple Kernel Learning</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ModDrop</a> . </p><br><p>  E, é claro, vale mencionar a classe das chamadas soluções de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ponta a</a> ponta que podem ser treinadas diretamente em dados brutos de vários sensores sem qualquer processamento preliminar. </p><br><p>  Em geral, a tarefa de reconhecimento automático de emoções ainda está longe de ser resolvida.  A julgar pelos resultados do concurso Emotion Recognition in the Wild do ano passado, as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">melhores soluções</a> alcançam precisão de cerca de 60%.  Espero que as informações apresentadas neste artigo sejam suficientes para tentar construir nosso próprio sistema de reconhecimento de emoções. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt418151/">https://habr.com/ru/post/pt418151/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt418141/index.html">RE: Ghat / AFR Iniciante Skipper Race</a></li>
<li><a href="../pt418143/index.html">PVS-Studio como uma solução SAST</a></li>
<li><a href="../pt418145/index.html">O primeiro processo contra Roskomnadzor de uma empresa que sofreu quando o Telegram foi bloqueado</a></li>
<li><a href="../pt418147/index.html">Silêncio de execuções Ruby: Rails transacionais / PostgreSQL Thriller</a></li>
<li><a href="../pt418149/index.html">Phishing com tag de título</a></li>
<li><a href="../pt418155/index.html">Diodo LED Diodo Zener</a></li>
<li><a href="../pt418157/index.html">O livro "Objetos elegantes. Edição Java »</a></li>
<li><a href="../pt418159/index.html">Onde ir para o designer: prêmios de prestígio da Rússia, Europa Oriental e países da CEI</a></li>
<li><a href="../pt418161/index.html">Em Stanford, as baterias de streaming em temperatura ambiente desenvolvidas</a></li>
<li><a href="../pt418163/index.html">Testes de produção: Netflix Chaos Automation Platform</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>