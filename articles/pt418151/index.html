<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìΩÔ∏è üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë© üßìüèº Introdu√ß√£o √† tarefa de reconhecer emo√ß√µes ü•© üôáüèø üôáüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O reconhecimento de emo√ß√µes √© um t√≥pico importante no campo da intelig√™ncia artificial. As √°reas mais interessantes de aplica√ß√£o de tais tecnologias i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introdu√ß√£o √† tarefa de reconhecer emo√ß√µes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/speechpro/blog/418151/"><p>  O reconhecimento de emo√ß√µes √© um t√≥pico importante no campo da intelig√™ncia artificial.  As √°reas mais interessantes de aplica√ß√£o de tais tecnologias incluem: reconhecimento de motorista, pesquisa de mercado, sistemas de an√°lise de v√≠deo para cidades inteligentes, intera√ß√£o homem-m√°quina, monitoramento de estudantes de cursos on-line, dispositivos vest√≠veis, etc. </p><br><p>  Este ano, a MDG dedicou sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escola de aprendizado de m√°quina de ver√£o</a> a esse t√≥pico.  Neste artigo, tentarei fazer uma breve excurs√£o ao problema de reconhecer o estado emocional de uma pessoa e falar sobre abordagens para sua solu√ß√£o. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/626/db4/5bb/626db45bb7b0aad7fdbc2970c0b4183f.jpg" alt="imagem"></div><a name="habracut"></a><br><h3 id="chto-takoe-emocii">  O que s√£o emo√ß√µes? </h3><br><p>  A emo√ß√£o √© um tipo especial de processo mental que expressa a experi√™ncia de uma pessoa em seu relacionamento com o mundo e com ela mesma.  De acordo com uma das teorias, cujo autor √© o fisiologista russo P.K.  Al√©m disso, a capacidade de experimentar emo√ß√µes foi desenvolvida no processo de evolu√ß√£o como um meio de adapta√ß√£o mais bem-sucedida dos seres vivos √†s condi√ß√µes de exist√™ncia.  A emo√ß√£o foi √∫til para a sobreviv√™ncia e permitiu que os seres vivos respondessem r√°pida e economicamente √†s influ√™ncias externas. </p><br><p>  As emo√ß√µes desempenham um papel enorme na vida humana e na comunica√ß√£o interpessoal.  Eles podem ser expressos de v√°rias maneiras: express√µes faciais, postura, rea√ß√µes motoras, voz e rea√ß√µes auton√¥micas (frequ√™ncia card√≠aca, press√£o arterial, frequ√™ncia respirat√≥ria).  No entanto, o rosto da pessoa tem mais expressividade. </p><br><p>  Cada pessoa expressa emo√ß√µes de v√°rias maneiras diferentes.  O famoso psic√≥logo americano <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Paul Ekman</a> , estudando o comportamento n√£o verbal de tribos isoladas na Papua Nova Guin√© nos anos 70 do s√©culo passado, descobriu que v√°rias emo√ß√µes, a saber: raiva, medo, tristeza, repulsa, desprezo, surpresa e alegria s√£o universais e podem para ser entendido pelo homem, independentemente de sua cultura. </p><br><p>  As pessoas s√£o capazes de expressar uma ampla gama de emo√ß√µes.  Acredita-se que eles possam ser descritos como uma combina√ß√£o de emo√ß√µes b√°sicas (por exemplo, nostalgia √© algo entre tristeza e alegria).  Mas essa abordagem categ√≥rica nem sempre √© conveniente, porque  n√£o permite quantificar o poder da emo√ß√£o.  Portanto, juntamente com modelos discretos de emo√ß√µes, foram desenvolvidos v√°rios cont√≠nuos.  O modelo de J. Russell tem uma base bidimensional na qual cada emo√ß√£o √© caracterizada por um sinal (val√™ncia) e intensidade (excita√ß√£o).  Devido √† sua simplicidade, o modelo Russell tornou-se cada vez mais popular no contexto da tarefa de classificar automaticamente express√µes faciais. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/f59/cd1/a24/f59cd1a24e77d64759641a076d86bef1.png" alt="imagem"></div><br><p>  Ent√£o, descobrimos que, se voc√™ n√£o est√° tentando esconder a excita√ß√£o emocional, seu estado atual pode ser estimado por express√µes faciais.  Al√©m disso, usando conquistas modernas no campo do aprendizado profundo, √© at√© poss√≠vel construir um detector de mentiras baseado na s√©rie ‚ÄúLie to me‚Äù, cuja base cient√≠fica foi fornecida diretamente por Paul Ekman.  No entanto, esta tarefa est√° longe de ser simples.  Como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mostraram</a> os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">estudos da</a> neurocientista Lisa Feldman Barrett, ao reconhecer emo√ß√µes, uma pessoa usa ativamente informa√ß√µes contextuais: voz, a√ß√µes, situa√ß√£o.  D√™ uma olhada nas fotos abaixo, realmente √©.  Usando apenas a √°rea do rosto, uma previs√£o correta √© imposs√≠vel.  Nesse sentido, para resolver esse problema, √© necess√°rio usar modalidades e informa√ß√µes adicionais sobre altera√ß√µes nos sinais ao longo do tempo. </p><br><div style="text-align:center;"><img height="200" src="https://habrastorage.org/webt/el/a6/t7/ela6t7ig73rti-peenhb7_f0sgs.jpeg" alt="imagem"></div><br><p>  Aqui consideraremos abordagens para a an√°lise de apenas duas modalidades: √°udio e v√≠deo, uma vez que esses sinais podem ser obtidos sem contato.  Para abordar a tarefa, voc√™ primeiro precisa obter os dados.  Aqui est√° uma lista dos maiores bancos de dados de emo√ß√µes dispon√≠veis ao p√∫blico que conhe√ßo  Imagens e v√≠deos nesses bancos de dados foram marcados manualmente, alguns usando o Amazon Mechanical Turk. </p><br><table><thead><tr><th>  T√≠tulo </th><th>  Dados </th><th>  Marca√ß√£o </th><th>  Ano de fabrica√ß√£o </th></tr></thead><tbody><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio OMG-Emotion</a> </td><td>  √°udio / v√≠deo </td><td>  7 categorias, val√™ncia / excita√ß√£o </td><td>  2018 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio Emotiw</a> </td><td>  √°udio / v√≠deo </td><td>  6 categorias </td><td>  2018 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Affectnet</a> </td><td>  imagens </td><td>  7 categorias, val√™ncia / excita√ß√£o </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AFEW-VA</a> </td><td>  o video </td><td>  val√™ncia / excita√ß√£o </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio EmotioNet</a> </td><td>  imagens </td><td>  16 categorias </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Emoreact</a> </td><td>  √°udio / v√≠deo </td><td>  17 categorias </td><td>  2016 </td></tr></tbody></table><br><h3 id="klassicheskiy-podhod-k-zadache-klassifikacii-emociy">  A abordagem cl√°ssica para a classifica√ß√£o das emo√ß√µes </h3><br><p>  A maneira mais f√°cil de determinar a emo√ß√£o de uma imagem de rosto √© baseada na classifica√ß√£o de pontos-chave (pontos faciais), cujas coordenadas podem ser obtidas usando v√°rios algoritmos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PDM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CML</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AAM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DPM</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CNN</a> .  Normalmente marque de 5 a 68 pontos, amarrando-os na posi√ß√£o das sobrancelhas, olhos, l√°bios, nariz, mand√≠bula, o que permite capturar parcialmente express√µes faciais.  As coordenadas normalizadas dos pontos podem ser enviadas diretamente ao classificador (por exemplo, SVM ou Random Forest) e obter uma solu√ß√£o b√°sica.  Naturalmente, a posi√ß√£o das pessoas deve estar alinhada. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/c60/24a/dbe/c6024adbeaecca98404dcaae3361785e.jpg" alt="imagem"></div><br><p>  O uso simples de coordenadas sem um componente visual leva a uma perda significativa de informa√ß√µes √∫teis, portanto, v√°rios descritores s√£o computados nesses pontos para melhorar o sistema: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LBP</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LATCH</a> etc. Depois que os descritores s√£o concatenados e a dimens√£o √© reduzida usando PCA, o vetor de recurso resultante pode ser usado para classifica√ß√£o emo√ß√µes. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/lh/oa/u7lhoatsm4vbqzb_zlksw9ekygy.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><p>  No entanto, essa abordagem j√° √© considerada obsoleta, pois √© sabido que redes de convolu√ß√£o profunda s√£o a melhor op√ß√£o para a an√°lise de dados visuais. </p><br><h3 id="klassifikaciya-emociy-s-primeneniem-deep-learning">  Classifica√ß√£o de emo√ß√µes usando aprendizagem profunda </h3><br><p>  Para criar um classificador de rede neural, basta ter alguma rede com uma arquitetura b√°sica, previamente treinada no ImageNet, e treinar novamente as √∫ltimas camadas.  Assim, voc√™ pode obter uma boa solu√ß√£o b√°sica para classificar v√°rios dados, mas, considerando as especificidades da tarefa, as redes neurais usadas para tarefas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reconhecimento de rosto em</a> larga escala ser√£o mais adequadas. </p><br><p>  Portanto, √© bastante simples criar um classificador de emo√ß√µes para imagens individuais, mas, como descobrimos, os instant√¢neos n√£o refletem com precis√£o as verdadeiras emo√ß√µes que uma pessoa experimenta em uma determinada situa√ß√£o.  Portanto, para aumentar a precis√£o do sistema, √© necess√°rio analisar a sequ√™ncia de quadros.  Existem duas maneiras de fazer isso.  A primeira maneira √© alimentar recursos de alto n√≠vel recebidos de uma CNN que classifique cada quadro individual em uma rede recorrente (por exemplo, LSTM) para capturar o componente de tempo. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/webt/ik/wi/zh/ikwizhwy65xydfoyu15ql3szjbi.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><p>  A segunda maneira √© alimentar diretamente uma sequ√™ncia de quadros tirados do v√≠deo em algumas etapas para a entrada 3D-CNN.  CNNs semelhantes usam convolu√ß√µes com tr√™s graus de liberdade que transformam a entrada quadridimensional em mapas de caracter√≠sticas tridimensionais. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/iv/a2/kd/iva2kdloqzpghk8gfcymk6g5sc4.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><p>  De fato, no caso geral, essas duas abordagens podem ser combinadas com a constru√ß√£o de um monstro. </p><br><div style="text-align:center;"><img height="400" src="https://habrastorage.org/webt/oo/ir/yo/ooiryorm9sh8n0cht5oud1yqbd4.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><h3 id="klassifikaciya-emociy-po-rechi">  Classifica√ß√£o de fala das emo√ß√µes </h3><br><p>  Com base nos dados visuais, o sinal de emo√ß√£o pode ser previsto com alta precis√£o, mas √© prefer√≠vel usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sinais de fala</a> ao determinar a intensidade.  A an√°lise do √°udio √© um pouco mais dif√≠cil devido √† alta variabilidade da dura√ß√£o da fala e das vozes dos falantes.  Geralmente, eles n√£o usam a onda sonora original, mas v√°rios conjuntos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atributos</a> , por exemplo: F0, MFCC, LPC, i-vetores, etc. No problema de reconhecer emo√ß√µes pela fala, a biblioteca aberta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenSMILE</a> tem uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">boa reputa√ß√£o</a> e cont√©m um rico conjunto de algoritmos para an√°lise de fala e m√∫sica. sinais.  Ap√≥s a extra√ß√£o, os atributos podem ser enviados ao SVM ou LSTM para classifica√ß√£o. </p><br><p>  Recentemente, no entanto, redes neurais convolucionais tamb√©m come√ßaram a penetrar no campo da an√°lise sonora, substituindo as abordagens estabelecidas.  Para aplic√°-los, o som √© representado na forma de espectrogramas em escala linear ou em escala mel, ap√≥s o que s√£o operados com os espectrogramas obtidos, como nas imagens bidimensionais comuns.  Nesse caso, o problema de um tamanho arbitr√°rio de espectrogramas ao longo do eixo do tempo √© elegantemente resolvido usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pool estat√≠stico</a> ou incorporando uma rede recorrente na arquitetura. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kq/4d/ts/kq4dtsbybmnn3nsq6cwq_wek19u.jpeg" alt="imagem"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para o artigo</a> </p><br><h3 id="audiovizualnoe-raspoznavanie-emociy">  Reconhecimento audiovisual das emo√ß√µes </h3><br><p>  Assim, examinamos v√°rias abordagens para a an√°lise das modalidades de √°udio e v√≠deo, o est√°gio final permaneceu - a combina√ß√£o de classificadores para produzir a solu√ß√£o final.  A maneira mais simples √© combinar diretamente suas classifica√ß√µes.  Nesse caso, basta levar o m√°ximo ou a m√©dia.  Uma op√ß√£o mais dif√≠cil √© combinar no n√≠vel de incorpora√ß√£o para cada modalidade.  O SVM √© frequentemente usado para isso, mas isso nem sempre √© correto, pois os incorporamentos podem ter uma taxa diferente.  Nesse sentido, algoritmos mais avan√ßados foram desenvolvidos, por exemplo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Multiple Kernel Learning</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ModDrop</a> . </p><br><p>  E, √© claro, vale mencionar a classe das chamadas solu√ß√µes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ponta a</a> ponta que podem ser treinadas diretamente em dados brutos de v√°rios sensores sem qualquer processamento preliminar. </p><br><p>  Em geral, a tarefa de reconhecimento autom√°tico de emo√ß√µes ainda est√° longe de ser resolvida.  A julgar pelos resultados do concurso Emotion Recognition in the Wild do ano passado, as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">melhores solu√ß√µes</a> alcan√ßam precis√£o de cerca de 60%.  Espero que as informa√ß√µes apresentadas neste artigo sejam suficientes para tentar construir nosso pr√≥prio sistema de reconhecimento de emo√ß√µes. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt418151/">https://habr.com/ru/post/pt418151/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt418141/index.html">RE: Ghat / AFR Iniciante Skipper Race</a></li>
<li><a href="../pt418143/index.html">PVS-Studio como uma solu√ß√£o SAST</a></li>
<li><a href="../pt418145/index.html">O primeiro processo contra Roskomnadzor de uma empresa que sofreu quando o Telegram foi bloqueado</a></li>
<li><a href="../pt418147/index.html">Sil√™ncio de execu√ß√µes Ruby: Rails transacionais / PostgreSQL Thriller</a></li>
<li><a href="../pt418149/index.html">Phishing com tag de t√≠tulo</a></li>
<li><a href="../pt418155/index.html">Diodo LED Diodo Zener</a></li>
<li><a href="../pt418157/index.html">O livro "Objetos elegantes. Edi√ß√£o Java ¬ª</a></li>
<li><a href="../pt418159/index.html">Onde ir para o designer: pr√™mios de prest√≠gio da R√∫ssia, Europa Oriental e pa√≠ses da CEI</a></li>
<li><a href="../pt418161/index.html">Em Stanford, as baterias de streaming em temperatura ambiente desenvolvidas</a></li>
<li><a href="../pt418163/index.html">Testes de produ√ß√£o: Netflix Chaos Automation Platform</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>