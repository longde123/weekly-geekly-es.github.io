<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üß† ü•ã üêè Datenbankskalierung in hoch belasteten Systemen ‚úåüèø ‚õ≥Ô∏è ‚¨õÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei der letzten internen Kundgebung von Pyrus sprachen wir √ºber modernen verteilten Speicher, und Maxim Nalsky, CEO und Gr√ºnder von Pyrus, teilte sein...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Datenbankskalierung in hoch belasteten Systemen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440306/"> Bei der letzten internen Kundgebung von Pyrus sprachen wir √ºber modernen verteilten Speicher, und Maxim Nalsky, CEO und Gr√ºnder von Pyrus, teilte seinen ersten Eindruck von FoundationDB.  In diesem Artikel sprechen wir √ºber die technischen Nuancen, mit denen Sie bei der Auswahl einer Technologie zur Skalierung der Speicherung strukturierter Daten konfrontiert sind. <br><br>  Wenn der Dienst f√ºr Benutzer f√ºr einige Zeit nicht verf√ºgbar ist, ist er √§u√üerst unangenehm, aber immer noch nicht t√∂dlich.  Der Verlust von Kundendaten ist jedoch absolut inakzeptabel.  Daher bewerten wir jede Technologie zum Speichern von Daten sorgf√§ltig anhand von zwei bis drei Dutzend Parametern. <a name="habracut"></a>  Einige von ihnen bestimmen die aktuelle Auslastung des Dienstes. <br><br><img src="https://habrastorage.org/webt/1c/p2/gm/1cp2gmh6pjentlkkocm1k2msgay.png">  <font color="#777777">Aktuelle Last.</font>  <font color="#777777">Wir w√§hlen die Technologie unter Ber√ºcksichtigung des Wachstums dieser Indikatoren aus.</font> <br><br><h2>  Client-Server-Architektur </h2><br>  Das klassische Client-Server-Modell ist das einfachste Beispiel f√ºr ein verteiltes System.  Ein Server ist ein Synchronisationspunkt, mit dem mehrere Clients koordiniert etwas gemeinsam tun k√∂nnen. <br><br><img src="https://habrastorage.org/webt/jq/hy/ee/jqhyeeqxfebgrvhzvkwsrz58jhy.png">  <font color="#777777">Ein sehr vereinfachtes Schema der Client-Server-Interaktion.</font> <br><br>  Was ist in der Client-Server-Architektur unzuverl√§ssig?  Offensichtlich kann der Server abst√ºrzen.  Und wenn der Server abst√ºrzt, k√∂nnen nicht alle Clients arbeiten.  Um dies zu vermeiden, haben sich die Leute eine Master-Slave-Verbindung ausgedacht (die jetzt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">politisch korrekt ist und als Leader-Follower bezeichnet wird</a> ).  Unter dem Strich gibt es zwei Server, alle Clients kommunizieren mit dem Hauptserver, und auf dem zweiten Server werden alle Daten einfach repliziert. <br><br><img src="https://habrastorage.org/webt/tk/bo/vu/tkbovuwxni03qjfpkx7m5iwmfie.png">  <font color="#777777">Client-Server-Architektur mit Datenreplikation f√ºr Follower.</font> <br><br>  Es ist klar, dass dies ein zuverl√§ssigeres System ist: Wenn der Hauptserver abst√ºrzt, befindet sich eine Kopie aller Daten auf dem Follower und kann schnell ausgel√∂st werden. <br><br>  Es ist wichtig zu verstehen, wie die Replikation funktioniert.  Wenn es synchron ist, muss die Transaktion gleichzeitig auf dem Leader und auf dem Follower gespeichert werden. Dies kann langsam sein.  Wenn die Replikation asynchron ist, k√∂nnen Sie nach einem Failover einige Daten verlieren. <br><br>  Und was passiert, wenn der Anf√ºhrer nachts f√§llt, wenn alle schlafen?  Es gibt Daten √ºber den Follower, aber niemand hat ihm gesagt, dass er jetzt ein Anf√ºhrer ist und Kunden keine Verbindung zu ihm herstellen.  OK, lassen Sie uns den Anh√§nger mit der Logik ausstatten, dass er beginnt, sich als Hauptsache zu betrachten, wenn die Verbindung zum Anf√ºhrer verloren geht.  Dann k√∂nnen wir leicht ein gespaltenes Gehirn bekommen - ein Konflikt, wenn die Verbindung zwischen dem Anf√ºhrer und dem Anh√§nger unterbrochen wird und beide denken, dass sie die wichtigsten sind.  Dies geschieht wirklich auf vielen Systemen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wie z. B. RabbitMQ</a> , der heute beliebtesten Warteschlangentechnologie. <br><br>  Um diese Probleme zu l√∂sen, organisieren Sie das automatische Failover. F√ºgen Sie einen dritten Server hinzu (Zeuge, Zeuge).  Es stellt sicher, dass wir nur einen F√ºhrer haben.  Und wenn der Leader abf√§llt, schaltet sich der Follower automatisch mit einer minimalen Ausfallzeit ein, die auf einige Sekunden reduziert werden kann.  Nat√ºrlich m√ºssen Kunden in diesem Schema die Adressen des Leiters und des Nachfolgers im Voraus kennen und die Logik der automatischen Wiederverbindung zwischen ihnen implementieren. <br><br><img src="https://habrastorage.org/webt/n1/ve/aq/n1veaqz67xiw-ozn0cnsudwx7cw.png">  <font color="#777777">Der Zeuge garantiert, dass es nur einen F√ºhrer gibt.</font>  <font color="#777777">Wenn der Anf√ºhrer abf√§llt, schaltet sich der Follower automatisch ein.</font> <br><br>  Ein solches System funktioniert jetzt bei uns.  Es gibt eine Hauptdatenbank, eine Ersatzdatenbank, einen Zeugen und ja - manchmal kommen wir morgens und sehen, dass der Wechsel nachts stattgefunden hat. <br><br>  Dieses Schema hat aber auch Nachteile.  Stellen Sie sich vor, Sie installieren Service Packs oder aktualisieren das Betriebssystem auf einem Leader-Server.  Vorher haben Sie die Last des Mitnehmers manuell umgeschaltet und dann ... f√§llt sie!  Katastrophe, Ihr Dienst ist nicht verf√ºgbar.  Was tun, um sich davor zu sch√ºtzen?  F√ºgen Sie einen dritten Sicherungsserver hinzu - einen weiteren Follower.  Drei ist eine Art magische Zahl.  Wenn das System zuverl√§ssig funktionieren soll, reichen zwei Server nicht aus, Sie ben√∂tigen drei.  Einer f√ºr die Wartung, der zweite f√§llt, der dritte bleibt. <br><br><img src="https://habrastorage.org/webt/57/et/ao/57etao8c03-skbgh4_evsna3vwu.png">  <font color="#777777">Der dritte Server bietet einen zuverl√§ssigen Betrieb, wenn die ersten beiden nicht verf√ºgbar sind.</font> <br><br>  Zusammenfassend sollte die Redundanz gleich zwei sein.  Eine Redundanz von eins reicht nicht aus.  Aus diesem Grund wurde in Festplatten-Arrays das RAID6-Schema anstelle von RAID5 verwendet, um den Ausfall von zwei Festplatten zu √ºberstehen. <br><br><h2>  Transaktionen </h2><br>  Vier grundlegende Anforderungen f√ºr Transaktionen sind bekannt: Atomizit√§t, Konsistenz, Isolation und Haltbarkeit (Atomizit√§t, Konsistenz, Isolation, Haltbarkeit - ACID). <br><br>  Wenn wir √ºber verteilte Datenbanken sprechen, meinen wir, dass die Daten skaliert werden m√ºssen.  Das Lesen l√§sst sich sehr gut skalieren - Tausende von Transaktionen k√∂nnen problemlos Daten parallel lesen.  Wenn jedoch andere Transaktionen gleichzeitig mit dem Lesen Daten schreiben, sind verschiedene unerw√ºnschte Effekte m√∂glich.  Es ist sehr einfach, eine Situation zu erhalten, in der eine Transaktion unterschiedliche Werte derselben Datens√§tze liest.  Hier sind einige Beispiele. <br><br>  <b>Dirty liest.</b>  In der ersten Transaktion senden wir dieselbe Anfrage zweimal: Nehmen Sie alle Benutzer mit der ID = 1. Wenn die zweite Transaktion diese Zeile √§ndert und dann ein Rollback durchf√ºhrt, werden in der Datenbank einerseits keine √Ñnderungen angezeigt, andererseits Bei der ersten Transaktion werden unterschiedliche Alterswerte f√ºr Joe gelesen. <br><br><img src="https://habrastorage.org/webt/qj/zl/d6/qjzld6frtj0ogu9lpfoss5wva2k.png"><br><br>  <b>Nicht wiederholbare Lesevorg√§nge.</b>  Ein anderer Fall ist, wenn die Schreibtransaktion erfolgreich abgeschlossen wurde und die Lesetransaktion w√§hrend der Ausf√ºhrung derselben Anforderung unterschiedliche Daten empfangen hat. <br><br><img src="https://habrastorage.org/webt/85/wd/ug/85wdugy_ypmk0cfxpxl1hy5pf-s.png"><br><br>  Im ersten Fall las der Client Daten, die im Allgemeinen in der Datenbank nicht vorhanden waren.  Im zweiten Fall liest der Client beide Male die Daten aus der Datenbank, sie sind jedoch unterschiedlich, obwohl das Lesen innerhalb derselben Transaktion erfolgt. <br><br>  <b>Phantom-Lesevorg√§nge sind,</b> wenn wir einen Bereich innerhalb derselben Transaktion erneut lesen und einen anderen Satz von Zeilen erhalten.  Irgendwo in der Mitte hat eine andere Transaktion Datens√§tze eingegeben und eingef√ºgt oder gel√∂scht. <br><br><img src="https://habrastorage.org/webt/k4/65/yq/k465yqehsgbjd0ertvg7poziaie.png"><br><br>  Um diese unerw√ºnschten Effekte zu vermeiden, implementieren moderne DBMS Sperrmechanismen (eine Transaktion beschr√§nkt den Zugriff auf die Daten, mit denen sie gerade arbeitet, auf andere Transaktionen) oder die Multiversion-Versionskontrolle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MVCC</a> (eine Transaktion √§ndert niemals zuvor aufgezeichnete Daten und erstellt immer eine neue Version). <br><br>  Der ANSI / ISO-SQL-Standard definiert 4 Isolationsstufen f√ºr Transaktionen, die sich auf den Grad der gegenseitigen Blockierung auswirken.  Je h√∂her der Isolationsgrad, desto weniger unerw√ºnschte Wirkungen.  Der Preis daf√ºr besteht darin, die Anwendung zu verlangsamen (da Transaktionen h√§ufiger darauf warten, die ben√∂tigten Daten freizuschalten) und die Wahrscheinlichkeit von Deadlocks zu erh√∂hen. <br><br><img src="https://habrastorage.org/webt/0-/iy/4h/0-iy4hwem9b4-mkupvpetxgc9so.png"><br><br>  F√ºr einen Anwendungsprogrammierer ist die serialisierbare Ebene am angenehmsten - es gibt keine unerw√ºnschten Auswirkungen und die gesamte Komplexit√§t der Sicherstellung der Datenintegrit√§t wird auf das DBMS verlagert. <br><br>  Lassen Sie uns √ºber die naive Implementierung der serialisierbaren Ebene nachdenken - mit jeder Transaktion blockieren wir einfach alle anderen.  Jede Schreibtransaktion kann theoretisch in 50 ¬µs ausgef√ºhrt werden (die Zeit einer Schreiboperation auf modernen SSD-Festplatten).  Und wir wollen Daten auf drei Maschinen speichern, erinnerst du dich?  Befinden sie sich im selben Rechenzentrum, dauert die Aufzeichnung 1-3 ms.  Wenn sie sich aus Gr√ºnden der Zuverl√§ssigkeit in verschiedenen St√§dten befinden, kann die Aufzeichnung problemlos 10 bis 12 ms dauern (die Reisezeit eines Netzwerkpakets von Moskau nach St. Petersburg und umgekehrt).  Das hei√üt, mit einer naiven Implementierung der serialisierbaren Ebene durch sequentielle Aufzeichnung k√∂nnen wir nicht mehr als 100 Transaktionen pro Sekunde ausf√ºhren.  Mit einer separaten SSD k√∂nnen Sie ungef√§hr 20.000 Schreibvorg√§nge pro Sekunde ausf√ºhren! <br><br>  Fazit: Schreibtransaktionen m√ºssen parallel ausgef√ºhrt werden. Um sie zu skalieren, ben√∂tigen Sie einen guten Konfliktl√∂sungsmechanismus. <br><br><h2>  Scherben </h2><br>  Was tun, wenn die Daten nicht mehr auf einem Server gespeichert werden?  Es gibt zwei Standard-Zoommechanismen: <br><br><ul><li>  <b>Aufrecht,</b> wenn wir diesem Server nur Speicher und Festplatten hinzuf√ºgen.  Dies hat seine Grenzen - in Bezug auf die Anzahl der Kerne pro Prozessor, die Anzahl der Prozessoren und die Speichermenge. <br></li><li>  <b>Horizontal,</b> wenn wir viele Maschinen verwenden und Daten zwischen ihnen verteilen.  S√§tze solcher Maschinen werden Cluster genannt.  Um Daten in einen Cluster zu stellen, m√ºssen sie gespalten werden. Das hei√üt, f√ºr jeden Datensatz wird festgelegt, auf welchem ‚Äã‚ÄãServer sie sich befinden. <br></li></ul><br>  Ein Sharding-Schl√ºssel ist ein Parameter, mit dem Daten zwischen Servern verteilt werden, z. B. eine Client- oder Organisationskennung. <br><br>  Stellen Sie sich vor, Sie m√ºssen Daten √ºber alle Bewohner der Erde in einem Cluster aufzeichnen.  Als Shard-Schl√ºssel k√∂nnen Sie beispielsweise das Geburtsjahr der Person verwenden.  Dann reichen 116 Server aus (und jedes Jahr muss ein neuer Server hinzugef√ºgt werden).  Oder Sie k√∂nnen das Land, in dem die Person lebt, als Schl√ºssel nehmen, dann ben√∂tigen Sie ungef√§hr 250 Server.  Die erste Option ist jedoch vorzuziehen, da sich das Geburtsdatum der Person nicht √§ndert und Sie niemals Daten √ºber sie zwischen den Servern √ºbertragen m√ºssen. <br><br><img src="https://habrastorage.org/webt/zr/8h/sz/zr8hszqcct-xf5q07gcow3cf7lu.png"><br><br>  In Pyrus k√∂nnen Sie eine Organisation als Sharding-Schl√ºssel verwenden.  Ihre Gr√∂√üe ist jedoch sehr unterschiedlich: Es gibt sowohl eine riesige Sovcombank (mehr als 15.000 Benutzer) als auch Tausende kleiner Unternehmen.  Wenn Sie einer Organisation einen bestimmten Server zuweisen, wissen Sie nicht im Voraus, wie dieser wachsen wird.  Wenn die Organisation gro√ü ist und den Dienst aktiv nutzt, werden die Daten fr√ºher oder sp√§ter nicht mehr auf einem Server gespeichert, und Sie m√ºssen erneut ein Hardharding durchf√ºhren.  Und das ist nicht einfach, wenn die Daten Terabyte sind.  Stellen Sie sich vor: Bei einem geladenen System werden Transaktionen jede Sekunde ausgef√ºhrt, und unter diesen Bedingungen m√ºssen Sie Daten von einem Ort an einen anderen verschieben.  Sie k√∂nnen das System nicht stoppen, ein solches Volumen kann mehrere Stunden lang gepumpt werden, und Gesch√§ftskunden werden eine so lange Ausfallzeit nicht √ºberleben. <br><br>  Als Sharding-Schl√ºssel ist es besser, Daten auszuw√§hlen, die sich selten √§ndern.  Eine angewandte Aufgabe macht dies jedoch bei weitem nicht immer einfach. <br><br><h2>  Konsens im Cluster </h2><br>  Wenn sich viele Computer im Cluster befinden und einige den Kontakt zu anderen verlieren, wie kann dann entschieden werden, wer die neueste Version der Daten speichert?  Das Zuweisen eines Zeugenservers reicht nicht aus, da dadurch auch der Kontakt zum gesamten Cluster verloren gehen kann.  Dar√ºber hinaus k√∂nnen in einer Situation mit geteiltem Gehirn mehrere Maschinen unterschiedliche Versionen derselben Daten aufzeichnen - und Sie m√ºssen irgendwie herausfinden, welche am relevantesten ist.  Um dieses Problem zu l√∂sen, wurden Konsensalgorithmen entwickelt.  Sie erm√∂glichen es mehreren identischen Maschinen, durch Abstimmung zu einem einzigen Ergebnis zu gelangen.  1989 wurde der erste derartige Algorithmus, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Paxos</a> , ver√∂ffentlicht, und 2014 entwickelten die Jungs von Stanford ein einfacher zu implementierendes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Flo√ü</a> .  Streng genommen reicht es aus, wenn ein Cluster von (2N + 1) Servern einen Konsens erzielt, dass gleichzeitig nicht mehr als N Fehler auftreten.  Um 2 Fehler zu √ºberleben, muss der Cluster √ºber mindestens 5 Server verf√ºgen. <br><br><h2>  Relationale DBMS-Skalierung </h2><br>  Die meisten Datenbanken, mit denen Entwickler arbeiten, unterst√ºtzen relationale Algebra.  Die Daten werden in Tabellen gespeichert, und manchmal m√ºssen Sie die Daten aus verschiedenen Tabellen mithilfe der Operation JOIN verkn√ºpfen.  Betrachten Sie ein Beispiel f√ºr eine Datenbank und eine einfache Abfrage. <br><br><img src="https://habrastorage.org/webt/hv/fo/d-/hvfod-guiaz4-52ji4epjhizpaq.png"><br><br>  Angenommen, A.id ist ein Prim√§rschl√ºssel mit einem Clustered-Index.  Anschlie√üend erstellt der Optimierer einen Plan, der h√∂chstwahrscheinlich zuerst die erforderlichen Datens√§tze aus Tabelle A ausw√§hlt und dann die entsprechenden Links zu den Datens√§tzen in Tabelle B aus einem geeigneten Index (A, B) entnimmt. Die Ausf√ºhrungszeit dieser Abfrage w√§chst logarithmisch aus der Anzahl der Datens√§tze in den Tabellen. <br><br>  Stellen Sie sich nun vor, dass die Daten auf vier Server im Cluster verteilt sind und Sie dieselbe Abfrage ausf√ºhren m√ºssen: <br><br><img src="https://habrastorage.org/webt/kr/yw/45/kryw45daubuflf-r14zqswson_o.png"><br><br>  Wenn das DBMS nicht alle Datens√§tze des gesamten Clusters anzeigen m√∂chte, wird es wahrscheinlich versuchen, Datens√§tze mit einer A.id von 128, 129 oder 130 zu finden und die entsprechenden Datens√§tze aus Tabelle B zu finden. Wenn A.id jedoch kein Shard-Schl√ºssel ist, wird das DBMS im Voraus ausgef√ºhrt Ich kann nicht wissen, auf welchem ‚Äã‚ÄãServer sich die Daten von Tabelle A befinden. Ich muss mich trotzdem an alle Server wenden, um herauszufinden, ob f√ºr unseren Zustand geeignete A.id-Datens√§tze vorhanden sind.  Dann kann jeder Server einen JOIN in sich selbst erstellen, aber das reicht nicht aus.  Sie sehen, wir brauchen den Datensatz auf Knoten 2 im Beispiel, aber es gibt keinen Datensatz mit A.id = 128?  Wenn die Knoten 1 und 2 unabh√§ngig voneinander JOIN ausf√ºhren, ist das Abfrageergebnis unvollst√§ndig - wir erhalten keinen Teil der Daten. <br><br>  Um diese Anforderung zu erf√ºllen, muss sich jeder Server an alle anderen wenden.  Die Laufzeit w√§chst quadratisch mit der Anzahl der Server.  (Sie haben Gl√ºck, wenn Sie alle Tabellen mit demselben Schl√ºssel sharden k√∂nnen, m√ºssen Sie nicht alle Server umgehen. In der Praxis ist dies jedoch unrealistisch. Es wird immer Abfragen geben, bei denen das Abrufen nicht auf dem Shard-Schl√ºssel basiert.) <br><br>  Daher sind JOIN-Operationen grunds√§tzlich schlecht skalierbar, und dies ist ein grundlegendes Problem des relationalen Ansatzes. <br><br><h2>  NoSQL-Ansatz </h2><br>  Schwierigkeiten bei der Skalierung klassischer DBMS haben dazu gef√ºhrt, dass Benutzer NoSQL-Datenbanken ohne JOIN-Operationen entwickelt haben.  Keine Joins - kein Problem.  Es gibt jedoch keine ACID-Eigenschaften, die jedoch in Marketingmaterialien nicht erw√§hnt wurden.  Schnell <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gefundene Handwerker,</a> die die St√§rke verschiedener verteilter Systeme testen und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Ergebnisse √∂ffentlich ver√∂ffentlichen</a> .  Es stellte sich heraus, dass es Szenarien gibt, in denen der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Redis-Cluster 45% der gespeicherten Daten verliert, der</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RabbitMQ-Cluster - 35% der Nachrichten</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MongoDB - 9% der Datens√§tze</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cassandra - bis zu 5%</a> .  Und wir sprechen <b>√ºber den Verlust, nachdem der Cluster den Client √ºber das erfolgreiche Speichern informiert hat.</b>  Normalerweise erwarten Sie von der gew√§hlten Technologie ein h√∂heres Ma√ü an Zuverl√§ssigkeit. <br><br>  Google hat die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spanner-</a> Datenbank entwickelt, die weltweit t√§tig ist.  Spanner garantiert ACID-Eigenschaften, Serialisierbarkeit und mehr.  Sie verf√ºgen √ºber Atomuhren in Rechenzentren, die eine genaue Zeit liefern. Auf diese Weise k√∂nnen Sie eine globale Reihenfolge von Transaktionen erstellen, ohne Netzwerkpakete zwischen Kontinenten weiterleiten zu m√ºssen.  Die Idee von Spanner ist, dass es f√ºr Programmierer besser ist, mit Leistungsproblemen umzugehen, die bei einer gro√üen Anzahl von Transaktionen auftreten, als mit dem Mangel an Transaktionen.  Spanner ist jedoch eine geschlossene Technologie. Sie passt nicht zu Ihnen, wenn Sie aus irgendeinem Grund nicht von einem Anbieter abh√§ngig sein m√∂chten. <br><br>  Die Eingeborenen von Google entwickelten ein Open-Source-Analogon von Spanner und nannten es CockroachDB ("Kakerlake" auf Englisch "Kakerlake", was die √úberlebensf√§higkeit der Datenbank symbolisieren sollte).  On Habr√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schrieb bereits</a> √ºber die Nichtverf√ºgbarkeit des Produkts f√ºr die Produktion, da der Cluster Daten verlor.  Wir haben uns f√ºr die neuere Version 2.0 entschieden und sind zu einem √§hnlichen Ergebnis gekommen.  Wir haben die Daten nicht verloren, aber einige der einfachsten Abfragen wurden unangemessen lange ausgef√ºhrt. <br><br><hr><br>  Infolgedessen gibt es heute relationale Datenbanken, die nur vertikal gut skaliert werden k√∂nnen, was teuer ist.  Und es gibt NoSQL-L√∂sungen ohne Transaktionen und ohne ACID-Garantien (wenn Sie ACID m√∂chten, schreiben Sie Kr√ºcken). <br><br>  Wie erstelle ich gesch√§ftskritische Anwendungen, bei denen Daten nicht auf einen Server passen?  Neue L√∂sungen erscheinen auf dem Markt, und √ºber eine davon - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FoundationDB</a> - werden wir Ihnen im n√§chsten Artikel mehr erz√§hlen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de440306/">https://habr.com/ru/post/de440306/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de440296/index.html">6 Anwendungen f√ºr das industrielle IoT</a></li>
<li><a href="../de440298/index.html">Valentinstag-Anwendung auf Libgdx</a></li>
<li><a href="../de440300/index.html">10 Konsolenbefehle zur Diskussion von JavaScript-Code wie bei einem PRO</a></li>
<li><a href="../de440302/index.html">CRM - Erfolgskosten, Fehlerkosten, Betriebskosten</a></li>
<li><a href="../de440304/index.html">Interrupts von externen Ger√§ten in einem x86-System. Teil 3. Konfigurieren des Interrupt-Routings im Chipsatz anhand des Coreboot-Beispiels</a></li>
<li><a href="../de440308/index.html">Teilen und erobern oder langsam schreiben - schnell lesen</a></li>
<li><a href="../de440310/index.html">Wie man einer Maschine beibringt, Rechnungen zu verstehen und Daten daraus zu extrahieren</a></li>
<li><a href="../de440312/index.html">Hackquest 2018. Ergebnisse & Zuschreibungen. Tag 4-7</a></li>
<li><a href="../de440314/index.html">JDK 12 Release Candidate: Shenandoah, G1, JMH, Arm64. Bugs in Swing schlagen zur√ºck</a></li>
<li><a href="../de440316/index.html">Gleichm√§√üige Verteilung der Punkte in einem Dreieck</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>