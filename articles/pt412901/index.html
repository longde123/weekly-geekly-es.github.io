<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👈🏻 👩🏿‍🤝‍👨🏾 🉑 Monitoramento e Kubernetes (revisão e reportagem em vídeo) 🕎 💇🏼 👾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em 28 de maio, na conferência RootConf 2018, realizada como parte do festival RIT ++ 2018, na seção "Logging and Monitoring", foi entregue um relatóri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Monitoramento e Kubernetes (revisão e reportagem em vídeo)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/412901/">  Em 28 de maio, na conferência <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RootConf</a> 2018, realizada como parte do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">festival</a> RIT ++ 2018, na seção "Logging and Monitoring", foi entregue um relatório "Monitoring and Kubernetes".  Ele conta sobre a experiência de monitorar a instalação com o Prometheus, que foi obtida por Flant como resultado da operação de dezenas de projetos Kubernetes em produção. <br><br><img src="https://habrastorage.org/webt/pm/o9/dm/pmo9dmnz9jf7b-yhej9shmir92q.jpeg"><br><br>  Por tradição, temos o prazer de apresentar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><b>vídeo com um relatório</b></a> (cerca de uma hora, <b>muito mais</b> informativo <b>que o</b> artigo) e o aperto principal em forma de texto.  Vamos lá! <a name="habracut"></a><br><br><h2>  O que é monitoramento? </h2><br>  Existem muitos sistemas de monitoramento: <br><br><img src="https://habrastorage.org/webt/pa/07/i0/pa07i0ojuohdqwxbk6lvf86lgls.png"><br><br>  Parece que pegar e instalar um deles - isso é tudo, a questão está encerrada.  Mas a prática mostra que não é assim.  E aqui está o porquê: <br><br><ol><li>  <b>Velocímetro mostra velocidade</b> .  Se medirmos a velocidade uma vez por minuto pelo velocímetro, a velocidade média, calculada com base nesses dados, não coincidirá com os dados do odômetro.  E se, no caso de um carro, isso é óbvio, então, quando se trata de muitos e muitos indicadores para o servidor, muitas vezes esquecemos. <br><img src="https://habrastorage.org/webt/s9/13/fv/s913fvrbguhqbp3iuuobwrmazt8.png"><br>  <i>O que medimos e como realmente viajamos</i> </li><li>  <b>Mais medições</b> .  Quanto mais indicadores <i>diferentes</i> obtivermos, mais preciso será o diagnóstico dos problemas ... mas apenas com a condição de que sejam indicadores realmente úteis, e não apenas tudo o que você conseguiu coletar. </li><li>  <b>Alertas</b> .  Não há nada complicado no envio de alertas.  No entanto, dois problemas típicos: a) alarmes falsos ocorrem com tanta frequência que paramos de responder a qualquer alerta; b) os alertas chegam em um momento em que é tarde demais (tudo já explodiu).  E alcançar no monitoramento que esses problemas não surgiram é arte genuína! </li></ol><br>  O monitoramento é um conjunto de três camadas, cada uma das quais é crítica: <br><br><ol><li>  Antes de tudo, este é um sistema que permite a <b>prevenção de acidentes</b> , a <b>notificação de acidentes</b> (se eles não puderam ser evitados) e <b>o diagnóstico rápido de</b> problemas. </li><li>  O que é necessário para isso?  <b>Dados precisos</b> , <b>gráficos úteis</b> (olhe para eles e entenda onde está o problema), <b>alertas relevantes</b> (cheguem na hora certa e contenham informações claras). </li><li>  E, para que tudo isso funcione, <b>é</b> necessário um <b>sistema de monitoramento</b> . </li></ol><br>  A configuração adequada de um sistema de monitoramento que realmente funciona não é uma tarefa fácil, exigindo uma abordagem criteriosa da implementação, mesmo sem o Kubernetes.  Mas o que acontece com sua aparência? <br><br><h2>  Especificações de monitoramento do Kubernetes </h2><br><h3>  No. 1.  Maior e mais rápido </h3><br>  O Kubernetes está mudando bastante porque a infraestrutura está ficando maior e mais rápida.  Se anteriormente, com servidores comuns de ferro, seu número era muito limitado e o processo de adição era muito longo (demorava dias ou semanas); então, nas máquinas virtuais, o número de entidades aumentava significativamente e o tempo de sua introdução na batalha era reduzido para segundos. <br><br>  Com o Kubernetes, o número de entidades cresceu em uma ordem de magnitude, sua adição é totalmente automatizada (gerenciamento de configuração é necessário, porque sem uma descrição, um novo pod simplesmente não pode ser criado), toda a infraestrutura se tornou muito dinâmica (por exemplo, os pods são excluídos e liberados toda vez são criados novamente). <br><br><img src="https://habrastorage.org/webt/01/fv/cf/01fvcfbc_i2roepw7nh0q6womsk.png"><br><br>  O que isso muda? <br><br><ol><li>  Em princípio, paramos de observar vagens ou contêineres individuais - agora estamos interessados <b>apenas em grupos de objetos</b> . </li><li>  <b>A descoberta de serviços se torna estritamente obrigatória</b> , porque as "velocidades" já são tais que, em princípio, não podemos iniciar / excluir manualmente novas entidades, como antes, quando novos servidores foram comprados. </li><li>  <b>A quantidade de dados está crescendo significativamente</b> .  Se métricas anteriores foram coletadas de servidores ou máquinas virtuais, agora de pods, cujo número é muito maior. </li><li>  A mudança mais interessante que chamei de " <b>fluxo de metadados</b> " e vou falar mais sobre isso. </li></ol><br>  Vou começar com esta comparação: <br><br><ul><li>  Quando você envia seu filho para o jardim de infância, ele recebe uma caixa pessoal, que é atribuída a ele para o próximo ano (ou mais) e na qual seu nome é indicado. </li><li>  Quando você chega à piscina, seu armário não é assinado e é emitido para você por uma "sessão". </li></ul><br>  Portanto, <b>os sistemas clássicos de monitoramento pensam que são um jardim de infância</b> , não uma piscina: eles assumem que o objeto de monitoramento os procurou para sempre ou por um longo tempo, e lhes dão armários de acordo.  Mas as realidades em Kubernetes são diferentes: um pod chegou à piscina (isto é, foi criado), nadou (até uma nova implantação) e saiu (foi destruído) - tudo isso acontece de forma rápida e regular.  Assim, o sistema de monitoramento deve entender que os objetos que monitora têm uma vida curta e devem ser capazes de esquecê-lo completamente no momento certo. <br><br><h3>  No. 2.  A realidade paralela existe </h3><br>  Outro ponto importante - com o advento do Kubernetes, temos simultaneamente duas "realidades": <br><br><ol><li>  Kubernetes mundo em que existem namespaces, implantações, pods, contêineres.  Este é um mundo complexo, mas é lógico, estruturado. </li><li>  O mundo "físico", composto por muitos (literalmente - montões) de contêineres em cada nó. </li></ol><br><img src="https://habrastorage.org/webt/1p/wc/xj/1pwcxjjt1xbgwqufldfm1upua10.png"><br>  <i>Um e o mesmo contêiner na “realidade virtual” do Kubernetes (acima) e no mundo físico dos nós (abaixo)</i> <br><br>  E no processo de monitoramento, precisamos <b>comparar</b> constantemente <b>o mundo físico dos contêineres com a realidade do Kubernetes</b> .  Por exemplo, quando olhamos para algum espaço para nome, queremos saber onde estão localizados todos os seus contêineres (ou os contêineres de uma de suas lareiras).  Sem isso, os alertas não serão visuais e convenientes de usar - porque é importante entendermos quais objetos eles estão relatando. <br><br><img src="https://habrastorage.org/webt/2p/n7/3t/2pn73tojozunuxdnjiba8yqw7-y.png"><br>  <i>Diferentes tipos de alertas - o último é mais visual e conveniente no trabalho do que o resto</i> <br><br>  <b>As conclusões</b> aqui são: <br><br><ol><li>  O sistema de monitoramento deve usar as primitivas internas do Kubernetes. </li><li>  Há mais de uma realidade: geralmente os problemas não acontecem com a lareira, mas com um nó específico, e precisamos entender constantemente em que tipo de "realidade" eles estão. </li><li>  Em um cluster, como regra, existem vários ambientes (além da produção), o que significa que isso deve ser levado em consideração (por exemplo, para não receber alertas noturnos sobre problemas no desenvolvedor). </li></ol><br>  Portanto, temos três condições necessárias para que tudo dê certo: <br><br><ol><li>  Entendemos bem o que é monitoramento. </li><li>  Conhecemos seus recursos, que aparecem no Kubernetes. </li><li>  Adotamos o Prometeu. </li></ol><br>  E assim, para realmente funcionar, resta apenas fazer <i>muito</i> esforço!  A propósito, por que exatamente Prometeu? <br><br><h2>  Prometeu </h2><br>  Há duas maneiras de responder à pergunta sobre a escolha do Prometheus: <br><br><ol><li>  Veja quem e o que geralmente é usado para monitorar o Kubernetes. </li><li>  Considere suas vantagens técnicas. </li></ol><br>  No primeiro, usei os dados da pesquisa do The New Stack (do e-book <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O estado do ecossistema Kubernetes</a> ), segundo o qual o Prometheus é pelo menos mais popular do que outras soluções (tanto de código aberto quanto de SaaS), e, se você observar, possui uma vantagem estatística em cinco vezes . <br><br>  Agora vamos ver como o Prometheus funciona, em paralelo com a forma como seus recursos se combinam com o Kubernetes e resolvem desafios relacionados. <br><br><h2>  Como o Prometheus está estruturado? </h2><br>  O Prometheus é escrito em Go e distribuído como um único arquivo binário, no qual tudo está embutido.  O algoritmo básico para sua operação é o seguinte: <br><br><img src="https://habrastorage.org/webt/nh/xt/hp/nhxthp-dm3wveymrgbejbanainw.png"><br><br><ul><li>  <b>O coletor</b> lê a <b>tabela de destinos</b> , ou seja,  uma lista de objetos a serem monitorados e a frequência de suas pesquisas (por padrão - 60 segundos). </li><li>  Depois disso, o coletor envia uma solicitação HTTP para cada pod de que você <b>precisa</b> e recebe uma resposta com um conjunto de métricas - pode haver cem, mil, dez mil ... Cada métrica possui um nome, valor e <b>rótulos</b> . </li><li>  A resposta recebida é <b>armazenada</b> no banco de dados <b>TSDB</b> , onde o <b>registro de</b> data e hora do recebimento e os rótulos do objeto do qual foi retirado são adicionados aos dados métricos recebidos. <br><br><div class="spoiler">  <b class="spoiler_title">Brevemente sobre TSDB</b> <div class="spoiler_text">  <i>TSDB - banco de dados de séries temporais (DB para séries temporais) on Go, que permite armazenar dados por um número especificado de dias e o faz com muita eficiência (em tamanho, memória e entrada / saída).</i>  <i>Os dados são armazenados apenas localmente, sem cluster e replicação, o que é um plus (funciona de maneira simples e garantida) e um sinal de menos (não há dimensionamento horizontal do armazenamento), mas, no caso do Prometheus, o sharding é bem feito, federação - mais sobre isso posteriormente.</i> <br></div></div></li><li>  Apresentado no esquema, o <b>Service Discovery</b> é um mecanismo de descoberta de serviço integrado ao Prometheus que permite receber dados (por meio da API Kubernetes) para criar uma tabela de objetivos pronta para uso. </li></ul><br>  Como é esta tabela?  Para cada entrada, ele armazena o URL usado para obter métricas, a frequência de chamadas e rótulos. <br><br><img src="https://habrastorage.org/webt/xu/9m/c_/xu9mc_ikwk-sdzkrs_fjt6lsfj0.png"><br><br>  Os rótulos são usados ​​para justapor os "mundos" de Kubernetes com o físico.  Por exemplo, para encontrar um pod com o Redis, precisamos ter o namespace dos valores, o serviço (usado em vez da implantação devido aos recursos técnicos de um caso específico) e o pod real.  Assim, esses três rótulos são armazenados nas entradas da tabela de metas para as métricas do Redis. <br><br>  Essas entradas na tabela são formadas com base na <code>scrape_configs</code> do Prometheus na qual os objetos de monitoramento são descritos: na seção <code>scrape_configs</code> , <code>scrape_configs</code> definidas, indicando por quais rótulos procurar objetos para monitorar, como filtrá-los e quais rótulos devem ser registrados. <br><br><h2>  Quais dados o Kubernetes coleta? </h2><br><ul><li>  Primeiro, o <b>assistente</b> no Kubernetes é bastante complicado - e é essencial monitorar o estado de seu trabalho (kube-apiserver, kube-controller-manager, kube-scheduler, kube-scheduler, kube-etcd3 ...); além disso, está vinculado ao nó do cluster. </li><li>  Em segundo lugar, é importante saber o que está acontecendo no <b>Kubernetes.Para</b> fazer isso, obtemos dados de: <br><ul><li>  <i>kubelet</i> - esse componente Kubernetes está sendo executado em cada nó do cluster (e se conecta ao assistente do K8s);  o cAdvisor é incorporado a ele (todas as métricas por contêineres) e também armazena informações sobre volumes persistentes conectados; </li><li>  <i>métricas do estado do kube</i> - na verdade, este é o Exportador do Prometheus para a API do Kubernetes (permite obter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">informações sobre objetos</a> armazenados no Kubernetes: pods, serviços, implantações etc.); por exemplo, não saberemos sem ele status de contêiner ou lareira); </li><li>  <i>exportador de nó</i> - fornece informações sobre o próprio nó, métricas básicas no sistema Linux (CPU, diskstats, meminfo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">etc.</a> ). </li></ul></li><li>  A seguir, estão os <b>componentes do Kubernetes</b> , como kube-dns, kube-prometheus-operator e kube-prometheus, ingress-nginx-controller, etc. </li><li>  A próxima categoria de objetos a monitorar é na verdade o <b>software</b> lançado no Kubernetes.  Esses são serviços típicos de servidor como nginx, php-fpm, Redis, MongoDB, RabbitMQ ... Fazemos isso sozinhos para que, quando adicionamos determinados rótulos ao serviço, ele automaticamente comece a coletar os dados necessários, o que cria o painel atual no Grafana. </li><li>  Por fim, a categoria para todo o resto é <b>personalizada</b> .  As ferramentas do Prometheus permitem automatizar a coleta de métricas arbitrárias (por exemplo, o número de pedidos) adicionando simplesmente um rótulo de <code>prometheus-custom-target</code> à descrição do serviço. </li></ul><br><img src="https://habrastorage.org/webt/0s/e6/c3/0se6c3ygspys909x3ju6m9aq7uu.gif"><br><h2>  Gráficos </h2><br>  Os dados recebidos <i>(descritos acima) são</i> usados ​​para enviar alertas e criar gráficos.  <b>Desenhamos</b> gráficos usando <b>Grafana</b> .  E um "detalhe" importante aqui é o <b>PromQL</b> , a linguagem de consulta do Prometheus que se integra perfeitamente ao Grafana. <br><br><img src="https://habrastorage.org/webt/_a/sl/7w/_asl7wbfscz1eyxstacdij_liio.png"><br><br>  É bastante simples e conveniente para a maioria das tarefas <i>(mas, por exemplo, juntar junções já é inconveniente, mas você ainda precisa)</i> .  O PromQL permite que você resolva todas as tarefas necessárias: selecione rapidamente as métricas necessárias, compare valores, realize operações aritméticas nelas, agrupe, trabalhe com intervalos de tempo e muito mais.  Por exemplo: <br><br><img src="https://habrastorage.org/webt/3h/1d/0v/3h1d0vskm__dosoxpx1rzd5jis8.png"><br><br>  Além disso, o Prometheus possui um <b>Avaliador</b> , que, usando o mesmo PromQL, pode acessar o TSDB com a frequência especificada.  Por que isso?  Exemplo: comece a enviar alertas nos casos em que, de acordo com as métricas disponíveis, tivermos um erro 500 no servidor da web nos últimos 5 minutos.  Além dos rótulos que estavam na solicitação, o Avaliador adiciona outros dados aos alertas (como configuramos), após o que são enviados no formato JSON para outro componente do Prometheus - <b>Alertmanager</b> . <br><br>  O Prometheus periodicamente (uma vez a cada 30 segundos) envia alertas ao Alertmanager, que os deduplica (tendo recebido o primeiro alerta, ele será enviado e os próximos não serão enviados novamente). <br><br><img src="https://habrastorage.org/webt/ju/0e/lg/ju0elg1u57wtxfjopy6wz38vtfg.png"><br><br>  <i><b>Nota</b> : Nós não usamos o Alertmanager em casa, mas enviamos dados do Prometheus diretamente para o nosso sistema, com o qual nossos atendentes trabalham, mas isso não importa no esquema geral.</i> <br><br><h2>  Prometeu em Kubernetes: o panorama geral </h2><br>  Agora vamos ver como esse pacote completo do Prometheus funciona no Kubernetes: <br><br><img src="https://habrastorage.org/webt/w-/gu/ue/w-guue_2b8q12romg-qv7uw2hpi.png"><br><br><ul><li>  O Kubernetes possui seu próprio namespace para Prometheus <i>(temos o <code>kube-prometheus</code> na ilustração)</i> . </li><li>  Esse espaço para nome hospeda o pod com a instalação do Prometheus, que a cada 30 segundos coleta métricas de todos os destinos recebidos por meio da descoberta de serviços no cluster. </li><li>  Ele também abriga um pod com o Alertmanager, que recebe dados do Prometheus e envia alertas <i>(para correio, Slack, PagerDuty, WeChat, integração de terceiros e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">assim por diante</a> )</i> . </li><li>  O Prometheus está enfrentando um balanceador de carga - um serviço regular em Kubernetes - e o Grafana acessa o Prometheus por meio dele.  Para <b>garantir a tolerância a falhas, o Prometheus</b> usa vários pods com instalações do Prometheus, cada um dos quais coleta todos os dados e os armazena em seu TSDB.  Através do balanceador, Grafana atinge um deles. </li><li>  O número de pods com o Prometheus é controlado pela configuração <i>StatefulSet</i> - geralmente fazemos no máximo dois pods, mas você pode aumentar esse número.  Da mesma forma, o Alertmanager é implantado através do StatefulSet, cuja tolerância a falhas é necessária pelo menos 3 pods (já que é necessário um quorum para tomar decisões sobre o envio de alertas). <br></li></ul><br>  O que está faltando aqui? .. <br><br><h2>  Federação para Prometeu </h2><br>  Quando os dados são coletados a cada 30 (ou 60) segundos, o local para armazená-los rapidamente termina e, pior ainda, requer muitos recursos de computação (ao receber e processar um número tão grande de pontos do TSDB).  Mas queremos armazenar e ter a capacidade de baixar informações por <b>grandes <i>e</i> e intervalos de tempo</b> .  Como conseguir isso? <br><br>  É suficiente adicionar <b>mais uma instalação do Prometheus</b> (a <i>longo prazo</i> ) ao esquema geral, no qual o Service Discovery está desabilitado, e na tabela de objetivos há o único registro estático que leva ao Prometheus <i>principal</i> ( <i>principal</i> ).  <b>Isso é possível graças à <a href="">federação</a></b> : o Prometheus permite retornar os valores mais recentes de todas as métricas em uma única consulta.  Portanto, a primeira instalação do Prometheus ainda funciona (acessa a cada 60 ou, por exemplo, 30 segundos) a todos os destinos no cluster Kubernetes e a segunda - uma vez a cada 5 minutos, recebe dados do primeiro e o armazena para poder assistir dados por um longo período ( mas sem detalhes profundos). <br><br><img src="https://habrastorage.org/webt/zc/uj/k3/zcujk3s5oxler1lhwaaswadmiyy.png"><br>  <i>A segunda instalação do Prometheus não precisa do Service Discovery e a tabela de objetivos consistirá em uma linha</i> <br><br><img src="https://habrastorage.org/webt/od/zr/ow/odzrowlvdyq3qmhfldvy085naou.png"><br>  <i>O quadro completo com as instalações do Prometheus de dois tipos: principal (superior) e de longo prazo</i> <br><br>  O toque final é <b>conectar o Grafana</b> às instalações do Prometheus e criar painéis de maneira especial para que você possa alternar entre as fontes de dados ( <i>principal</i> ou a <i>longo prazo</i> ).  Para fazer isso, usando o mecanismo de modelo, substitua a variável <code>$prometheus</code> vez da fonte de dados em todos os painéis. <br><br><img src="https://habrastorage.org/webt/ju/v3/9u/juv39un0v2qdtwegrt07qpg-1yi.png"><br><br><h2>  O que mais é importante nos gráficos? </h2><br>  Dois pontos-chave a serem considerados na organização de agendas são o suporte às primitivas do Kubernetes e a capacidade de <b>detalhar</b> rapidamente da imagem geral (ou uma "exibição" inferior)) para um serviço específico e vice-versa. <br><br>  O suporte a primitivos (namespaces, pods etc.) já foi mencionado - essa é uma condição necessária em princípio para um trabalho confortável nas realidades do Kubernetes.  E aqui está um exemplo sobre drill down: <br><br><ul><li>  Observamos os gráficos do consumo de recursos em três projetos (ou seja, três espaços para nome) - vemos que a parte principal da CPU (ou memória, ou rede, ...) fica no projeto A. </li><li>  Observamos os mesmos gráficos, mas já para os serviços do Projeto A: qual deles consome mais CPU? </li><li>  Passamos aos gráficos do serviço desejado: qual pod é "culpado"? </li><li>  Passamos aos gráficos do grupo desejado: qual recipiente é o "culpado"?  Este é o objetivo desejado! </li></ul><br><img src="https://habrastorage.org/webt/fq/vb/ly/fqvblyn0wdnoqqzqgwi0ublftjy.png"><br><h2>  Sumário </h2><br><ul><li>  Declare com precisão o que é o monitoramento.  <i>(Deixe o “bolo de três camadas” servir como um lembrete disso ... assim como o fato de que assar com competência não é fácil, mesmo sem o Kubernetes!)</i> </li><li>  Lembre-se de que o Kubernetes adiciona especificações obrigatórias: agrupamento de destinos, descoberta de serviços, grandes quantidades de dados, fluxo de metadados.  Além disso: <br><ul><li>  sim, alguns deles são magicamente resolvidos em Prometeu; </li><li>  no entanto, resta outra parte que precisa ser monitorada de maneira independente e cuidadosa. </li></ul></li></ul><br>  E lembre-se de que o <b>conteúdo é mais importante que um sistema</b> , ou seja,  gráficos e alertas corretos são primários, e não o Prometheus (ou qualquer outro software similar) como tal. <br><br><img src="https://habrastorage.org/webt/ml/61/ou/ml61oub7wmavnyfdmjepkm7bd-y.png"><br><br><h2>  Vídeos e slides </h2><br>  Vídeo da apresentação (cerca de uma hora): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zj6SlzzBRaA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Apresentação do relatório: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Outros relatórios em nosso blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bancos de dados e Kubernetes</a> ";  <i>(Dmitry Stolyarov; 8 de novembro de 2018 no HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores práticas de CI / CD com Kubernetes e GitLab</a> ”;  <i>(Dmitry Stolyarov; 7 de novembro de 2017 no HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nossa experiência com o Kubernetes em pequenos projetos</a> ”;  <i>(Dmitry Stolyarov; 6 de junho de 2017 na RootConf)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Coletamos imagens do Docker para CI / CD de forma rápida e conveniente com o dapp</a> ” <i>(Dmitry Stolyarov; 8 de novembro de 2016 no HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Práticas de entrega contínua com Docker</a> ” <i>(Dmitry Stolyarov; 31 de maio de 2016 na RootConf)</i> . </li></ul><br>  Você também pode estar interessado nas seguintes publicações: <br><br><ul><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O dispositivo e mecanismo de operação do Operador Prometheus em Kubernetes</a> ”; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Monitoramento com Prometheus em Kubernetes em 15 minutos</a> ”; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Infraestrutura com o Kubernetes como um serviço acessível</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt412901/">https://habr.com/ru/post/pt412901/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt412891/index.html">E / S do Citrix XenServer 7.0 não otimizada Agente de Gerenciamento não instalado</a></li>
<li><a href="../pt412893/index.html">Para alcançar um programador sênior em quatro anos: o método "Escola 21"</a></li>
<li><a href="../pt412895/index.html">Vesta Matveeva: a luta contra o cibercrime é uma escolha moral</a></li>
<li><a href="../pt412897/index.html">Monitorando produtos Atlassian com Prometheus</a></li>
<li><a href="../pt412899/index.html">Leitura de fim de semana: 30 materiais sobre som, a história das marcas de áudio e a indústria cinematográfica</a></li>
<li><a href="../pt412903/index.html">Como pintamos Habr</a></li>
<li><a href="../pt412905/index.html">Sobre a LL Parsing: uma abordagem para analisar através do conceito de corte de cordas</a></li>
<li><a href="../pt412911/index.html">Desenvolvedores falam sobre recursos cortados de jogos</a></li>
<li><a href="../pt412913/index.html">"Baikal-T1" foi colocado à venda por 3990 rublos</a></li>
<li><a href="../pt412915/index.html">Determinação da densidade do gás a partir dos resultados da medição de pressão e temperatura com sensores Arduino</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>