<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏿‍⚕️ 🧑 👨🏻‍🚒 [Illustré] Guide de mise en réseau dans Kubernetes. 3e partie 🤴🏻 🎟️ 👉🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : Cet article poursuit la série de documents sur le dispositif de base des réseaux dans Kubernetes, qui est décrit sous une forme acce...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>[Illustré] Guide de mise en réseau dans Kubernetes. 3e partie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/433382/">  <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: Cet article poursuit la série de documents sur le dispositif de base des réseaux dans Kubernetes, qui est décrit sous une forme accessible et avec des illustrations illustratives (cependant, il n'y avait pratiquement aucune illustration dans cette partie du béton pour le moment).</i>  <i>En traduisant les deux parties précédentes de cette série, nous les avons combinées en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une seule publication</a> , qui parlait du modèle de réseau K8s (interaction au sein des nœuds et entre les nœuds) et des réseaux de superposition.</i>  <i>Sa lecture préliminaire est souhaitable (recommandée par l'auteur lui-même).</i>  <i>La suite est consacrée aux services Kubernetes et au traitement du trafic sortant et entrant.</i> <i><br></i>  <i><b>NB</b> : Pour la commodité de l'auteur, le texte de l'auteur est complété par des liens (principalement vers la documentation officielle des K8).</i> <br><br><img src="https://habrastorage.org/webt/x4/ks/w-/x4ksw-pth57rmrhkhvmh-1sua4s.png"><a name="habracut"></a><br><br><h2>  Dynamique des clusters </h2><br>  En raison de la nature dynamique et en constante évolution de Kubernetes et des systèmes distribués en général, les pods (et, par conséquent, leurs adresses IP) sont également en constante évolution.  Les raisons de cela varient des mises à jour entrantes pour atteindre l'état et les événements souhaités conduisant à la mise à l'échelle, aux plantages imprévus du pod ou du nœud.  Par conséquent, les adresses IP du pod ne peuvent pas être utilisées directement pour la communication. <br><br>  Le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">service</a> dans Kubernetes entre en jeu - une IP virtuelle avec un groupe d'adresses IP de pod qui sont utilisées comme points de terminaison et identifiées par des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sélecteurs d'étiquettes</a> .  Un tel service fonctionne comme un équilibreur de charge virtuel, dont l'adresse IP reste constante, et en même temps, les adresses IP du pod qu'il présente peuvent changer en permanence. <br><br><img src="https://habrastorage.org/webt/ub/jt/pi/ubjtpikee0m_4jbl1vxvqayaptu.png"><br>  <i>Sélecteur d'étiquette dans l'objet Service dans Kubernetes</i> <br><br>  Derrière l'implémentation complète de cette IP virtuelle se trouvent des règles iptables (les dernières versions de Kubernetes ont également <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la possibilité d'</a> utiliser IPVS, mais c'est un sujet pour une autre discussion), qui sont contrôlées par un composant Kubernetes appelé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kube-proxy</a> .  Cependant, un tel nom est trompeur dans les réalités d'aujourd'hui.  Kube-proxy était vraiment utilisé comme proxy dans les jours précédant la sortie de Kubernetes v1.0, mais cela a conduit à une grande consommation de ressources et de freins en raison des opérations de copie constantes entre l'espace du noyau et l'espace utilisateur.  Maintenant, c'est juste un contrôleur - comme beaucoup d'autres contrôleurs dans Kubernetes.  Il surveille le serveur API pour les modifications des points de terminaison et met à jour les règles iptables en conséquence. <br><br>  Selon ces règles iptables, si le paquet est destiné à l'adresse IP du service, une traduction d'adresse réseau de destination (DNAT) est effectuée pour lui: cela signifie que son adresse IP passera de l'IP du service à l'un des points de terminaison, c'est-à-dire  l'une des adresses IP du pod, que iptables sélectionne au hasard.  Cela garantit que la charge est uniformément répartie entre les pods. <br><br><img src="https://habrastorage.org/webt/1a/o4/_v/1ao4_vk1xd_grzyxccfiommthms.png"><br>  <i>DNAT dans iptables</i> <br><br>  Dans le cas d'un tel DNAT, les informations nécessaires sont stockées dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conntrack</a> - la table de comptabilité des connexions sous Linux (il stocke les traductions à cinq paires faites par iptables: <code>protocol</code> , <code>srcIP</code> , <code>srcPort</code> , <code>dstIP</code> , <code>dstPort</code> ).  Tout est organisé de telle manière que lorsqu'une réponse est renvoyée, une opération DNAT inverse (non-DNAT) peut se produire, c'est-à-dire  Remplacement de la source IP du Pod IP au Service IP.  Grâce à ce client, il n'est absolument pas nécessaire de savoir comment travailler avec les packages en arrière-plan. <br><br><img src="https://habrastorage.org/webt/tm/dp/n4/tmdpn4mhfrqoizyhighdxs8jit4.png"><br>  <i>Entrées à cinq paires (5 tuples) dans la table conntrack</i> <br><br>  Ainsi, en utilisant les services Kubernetes, nous pouvons travailler avec les mêmes ports sans aucun conflit (car la réaffectation des ports aux points de terminaison est possible).  Cela rend la découverte de services très facile.  Il suffit d'utiliser le DNS interne et de coder en dur l'hôte des services.  Vous pouvez même utiliser les variables préconfigurées de Kubernetes avec l'hôte et le port de service. <br><br>  <b>Astuce</b> : En choisissant le deuxième chemin, vous économisez beaucoup d'appels DNS inutiles! <br><br><h2>  Trafic sortant </h2><br>  Les services Kubernetes décrits ci-dessus fonctionnent au sein d'un cluster.  Dans la pratique, les applications ont généralement besoin d'accéder à certains API / sites externes. <br><br>  En général, les hôtes peuvent avoir des adresses IP privées et publiques.  Pour accéder à Internet, un NAT individuel est fourni pour ces adresses IP privées et publiques - ceci est particulièrement vrai pour les environnements cloud. <br><br>  Pour une interaction normale de l'hôte avec l'adresse IP externe, l'IP source passe de l'IP hôte privé à l'IP publique pour les paquets sortants et pour les paquets entrants - dans la direction opposée.  Cependant, dans les cas où la connexion à l'IP externe est initiée par le pod, l'adresse IP source est l'IP du pod, que le mécanisme NAT du fournisseur de cloud ne connaît pas.  Par conséquent, il supprimera simplement les paquets avec des adresses IP source différentes des adresses IP hôtes. <br><br>  Et ici, vous l'aurez deviné, nous aurons encore plus besoin d'iptables!  Cette fois, les règles, qui sont également ajoutées par kube-proxy, sont exécutées par SNAT (Source Network Address Translation), alias <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">IP MASQUERADE</a> (masquerading).  Au lieu de l'adresse IP source, le noyau est invité à utiliser l'interface IP à partir de laquelle le paquet arrive.  Une entrée apparaît dans conntrack pour poursuivre l'opération inverse (non-SNAT) sur la réponse. <br><br><h2>  Trafic entrant </h2><br>  Jusqu'à présent, tout allait bien.  Les pods peuvent communiquer entre eux et avec Internet.  Cependant, il nous manque toujours l'essentiel - servir le trafic des utilisateurs.  Il existe actuellement deux façons de le mettre en œuvre: <br><br><h3>  1. NodePort / Cloud Load Balancer (niveau L4: IP et port) </h3><br>  La définition de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>NodePort</code></a> comme type de service affectera le service <code>NodePort</code> dans une plage de 30 000 à <code>NodePort</code> 000.  Ce <code>nodePort</code> ouvert sur chaque nœud, même si aucun pod n'est en cours d'exécution sur le nœud.  Le trafic entrant sur ce <code>NodePort</code> envoyé à l'un des pods (qui peuvent même apparaître sur un autre nœud!), Toujours en utilisant iptables. <br><br>  Le type de service <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>LoadBalancer</code></a> dans les environnements cloud crée un équilibreur de charge cloud (par exemple, ELB) en face de tous les nœuds, fonctionnant plus loin avec le même <code>NodePort</code> . <br><br><h3>  2. Entrée (niveau L7: HTTP / TCP) </h3><br>  De nombreuses autres implémentations effectuent également le mappage hôte / chemin HTTP avec les backends correspondants - par exemple, nginx, traefik, HAProxy, etc.  Avec eux, le LoadBalancer et le NodePort redeviennent le point d'entrée pour le trafic, mais il y a l'avantage ici que nous n'avons besoin que d'une seule entrée pour servir le trafic entrant de tous les services au lieu de nombreux NodePort / LoadBalancers. <br><br><h2>  Stratégies réseau </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les stratégies réseau</a> peuvent être considérées comme des groupes de sécurité / ACL pour les pods.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>NetworkPolicy</code></a> règles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>NetworkPolicy</code></a> autorisent / refusent le trafic entre les pods.  Leur implémentation exacte dépend de la couche réseau / CNI, mais la plupart d'entre eux utilisent simplement iptables. <br><br><h2>  ... </h2><br>  C’est tout.  Dans les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tranches précédentes,</a> nous avons appris les bases de la mise en réseau dans Kubernetes et le fonctionnement des superpositions.  Nous savons maintenant comment l'abstraction du service aide dans un cluster dynamique et rend la découverte des services très simple.  Nous avons également examiné comment le trafic sortant / entrant circule et quelles stratégies réseau peuvent être utiles pour sécuriser un cluster. <br><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un guide illustré du réseautage chez Kubernetes.»</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Parties 1 et 2</a> "; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dans les coulisses du réseau à Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Expériences avec kube-proxy et inaccessibilité de l'hôte dans Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comparaison des performances réseau pour Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Container Networking Interface (CNI) - l'interface réseau et la norme pour les conteneurs Linux</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Conduit est un maillage de service léger pour Kubernetes</a> .» </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr433382/">https://habr.com/ru/post/fr433382/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr433372/index.html">Vélo de voiture</a></li>
<li><a href="../fr433374/index.html">Toute la vérité sur RTOS. Article # 26. Canaux: services auxiliaires et structures de données</a></li>
<li><a href="../fr433376/index.html">Cours MIT "Sécurité des systèmes informatiques". Conférence 21: Suivi des données, partie 1</a></li>
<li><a href="../fr433378/index.html">Cours MIT "Sécurité des systèmes informatiques". Conférence 21: Suivi des données, partie 2</a></li>
<li><a href="../fr433380/index.html">Cours MIT "Sécurité des systèmes informatiques". Conférence 21: Suivi des données, partie 3</a></li>
<li><a href="../fr433384/index.html">Nos données personnelles sont toujours vendues effrontément</a></li>
<li><a href="../fr433386/index.html">Comment utiliser une vision par ordinateur pour évaluer l'état de la voiture. Découvrez Yandex.Taxi</a></li>
<li><a href="../fr433388/index.html">Fan de formule ou pourquoi nous jouons à des jeux</a></li>
<li><a href="../fr433390/index.html">Yandex a rejoint la protection de Linux et de l'industrie informatique contre la pêche à la traîne des brevets</a></li>
<li><a href="../fr433392/index.html">Courte critique du téléphone IP Snom D725</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>