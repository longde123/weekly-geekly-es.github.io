<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤¸ğŸ½ ğŸ‘©ğŸ»â€ğŸ’¼ ğŸ§“ Google spricht Ã¼ber ein exponentielles KI-Wachstum, das die Art des Computing verÃ¤ndert ğŸ“´ ğŸ‘¦ğŸ¿ ğŸ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Clif Young, ein Google-Programmierer, erklÃ¤rt, wie die explosive Entwicklung von Deep-Learning-Algorithmen mit dem Scheitern des Moore'schen Gesetzes ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google spricht Ã¼ber ein exponentielles KI-Wachstum, das die Art des Computing verÃ¤ndert</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/429794/"><h3>  Clif Young, ein Google-Programmierer, erklÃ¤rt, wie die explosive Entwicklung von Deep-Learning-Algorithmen mit dem Scheitern des Moore'schen Gesetzes zusammenfÃ¤llt, das seit Jahrzehnten an der Faustregel fÃ¼r den Fortschritt von Computerchips arbeitet, und die Entwicklung grundlegend neuer Rechenschemata erzwingt </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/f16/257/7ce/f162577cea1b28010ec340bfff65f307.jpg"><br><br>  Die explosive Entwicklung von KI- und maschinellen Lernalgorithmen verÃ¤ndert die Art des Rechnens - wie es in einem der grÃ¶ÃŸten Unternehmen heiÃŸt, die KI praktizieren - bei Google.  Der Google-Programmierer Cliff Young sprach bei der ErÃ¶ffnung der Herbst-Mikroprozessorkonferenz der Linley Group, einem beliebten Computerchip-Symposium des ehrwÃ¼rdigen Halbleiterunternehmens. <br><br>  Young sagte, dass der Einsatz von KI in dem Moment in die "exponentielle Phase" eintrat, als das Mooresche Gesetz, die Faustregel fÃ¼r den Fortschritt von Computerchips seit Jahrzehnten, vollstÃ¤ndig gehemmt wurde. <br><a name="habracut"></a><br>  "Die Zeiten sind ziemlich nervÃ¶s", sagte er nachdenklich.  â€Digitale CMOS verlangsamen sich, wir sehen Probleme mit dem 10-nm-Prozess bei Intel, wir sehen sie beim 7-nm-Prozess von GlobalFoundries und gleichzeitig mit der Entwicklung von Deep Learning entsteht eine wirtschaftliche Nachfrage.  CMOS, eine komplementÃ¤re Metalloxid-Halbleiter-Struktur, ist das am hÃ¤ufigsten verwendete Material zur Herstellung von Computerchips. <br><br>  WÃ¤hrend klassische Chips die Effizienz und ProduktivitÃ¤t kaum steigern kÃ¶nnen, steigen die Anfragen von KI-Forschern, sagte Young.  Er erstellte einige Statistiken: Die Anzahl der wissenschaftlichen Arbeiten zum maschinellen Lernen, die auf der von der Cornell University unterhaltenen arXiv-Preprint-Site gespeichert sind, verdoppelt sich alle 18 Monate.  Und die Anzahl der internen Projekte, die sich bei Google auf KI konzentrieren, verdoppelt sich ebenfalls alle 18 Monate.  Der Bedarf an Gleitkommaoperationen, die zur Verarbeitung der beim maschinellen Lernen verwendeten neuronalen Netze erforderlich sind, wÃ¤chst noch schneller - er verdoppelt sich alle dreieinhalb Monate. <br><br>  All dieses Wachstum bei Computerabfragen wird zu "Moores Supergesetz" kombiniert, sagte Young, und er nannte das PhÃ¤nomen "ein wenig beÃ¤ngstigend" und "ein wenig gefÃ¤hrlich" und "etwas, worÃ¼ber man sich Sorgen machen muss". <br><br>  "Woher kam all dieses exponentielle Wachstum", fragte er im Bereich der KI.  â€Der springende Punkt ist insbesondere, dass tiefes Lernen einfach funktioniert.  In meiner Karriere habe ich maschinelles Lernen lange ignoriert â€œ, sagte er.  "Es war nicht offensichtlich, dass diese Dinge abheben konnten." <br><br>  Aber dann entstanden schnell DurchbrÃ¼che wie die Mustererkennung, und es wurde klar, dass tiefes Lernen â€unglaublich effektiv istâ€œ, sagte er.  â€In den letzten fÃ¼nf Jahren waren wir grÃ¶ÃŸtenteils das Unternehmen, das KI an erster Stelle stellt, und wir haben die meisten auf KI basierenden Unternehmen neu gestaltetâ€œ, von der Suche Ã¼ber die Werbung bis hin zu vielem mehr. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a72/3ff/1e9/a723ff1e9203c6ee395dc5cd8d8bd296.jpg"><br><br>  Das Google Brain-Projektteam, ein fÃ¼hrendes KI-Forschungsprojekt, benÃ¶tigt â€riesige Maschinenâ€œ, sagte Young.  Beispielsweise werden neuronale Netze manchmal anhand der Anzahl der in ihnen verwendeten â€Gewichteâ€œ gemessen, dh anhand der Variablen, die auf das neuronale Netz angewendet werden, und beeinflussen, wie es die Daten verarbeitet. <br><br>  Und wenn gewÃ¶hnliche neuronale Netze Hunderttausende oder sogar Millionen von Gewichten enthalten kÃ¶nnen, die berechnet werden mÃ¼ssen, benÃ¶tigen Google-Forscher â€Tera-Weight-Maschinenâ€œ, dh Computer, die Billionen von Gewichten berechnen kÃ¶nnen.  Denn "jedes Mal, wenn wir die GrÃ¶ÃŸe des neuronalen Netzwerks verdoppeln, verbessern wir seine Genauigkeit."  Die Regel der KI-Entwicklung ist, immer grÃ¶ÃŸer zu werden. <br><br>  Auf Anfrage von Google entwickeln sie eine eigene Chipreihe fÃ¼r das MO, die Tensor Processing Unit.  TPU und dergleichen werden benÃ¶tigt, da herkÃ¶mmliche CPUs und GPU-Grafikchips die Last nicht bewÃ¤ltigen kÃ¶nnen. <br><br>  "Wir haben uns sehr lange zurÃ¼ckgehalten und gesagt, dass Intel und Nvidia sehr gut darin sind, Hochleistungssysteme zu entwickeln", sagte Young.  "Aber wir haben diese Grenze vor fÃ¼nf Jahren Ã¼berschritten." <br><br>  TPU sorgte nach dem ersten Ã¶ffentlichen Auftritt im Jahr 2017 fÃ¼r Aufsehen, da behauptet wurde, dass es in Bezug auf die Geschwindigkeit die normalen Chips Ã¼bertrifft.  Google arbeitet bereits an der TPU der dritten Generation, verwendet sie in seinen Projekten und bietet Computerfunktionen bei Bedarf Ã¼ber den Google Cloud-Dienst an. <br><br>  Das Unternehmen stellt weiterhin immer grÃ¶ÃŸere TPUs her.  In seiner "Legacy" -Konfiguration sind 1024 TPUs gemeinsam mit einem neuen Supercomputertyp verbunden, und Google plant laut Young, dieses System weiter auszubauen. <br><br>  "Wir bauen riesige Multicomputer mit einer KapazitÃ¤t von mehreren zehn Petabyte", sagte er.  "Wir bewegen uns unermÃ¼dlich gleichzeitig in mehrere Richtungen, und der Betrieb im Terabyte-Bereich wÃ¤chst weiter."  Solche Projekte werfen alle Probleme auf, die mit der Entwicklung von Supercomputern verbunden sind. <br><br>  Zum Beispiel haben Google-Ingenieure die Tricks des legendÃ¤ren Cray-Supercomputers Ã¼bernommen.  Sie kombinierten das gigantische â€Matrixmultiplikationsmodulâ€œ, den Teil des Chips, der die Hauptlast fÃ¼r die Berechnung neuronaler Netze trÃ¤gt, mit dem â€Vektor-Allzweckmodulâ€œ und dem â€skalaren Allzweckmodulâ€œ, wie dies in Cray durchgefÃ¼hrt wurde.  "Die Kombination von Skalar- und Vektormodulen ermÃ¶glichte es Cray, alle in Bezug auf die Leistung zu Ã¼berholen", sagte er. <br><br>  Google hat seine eigenen innovativen arithmetischen Designs fÃ¼r die Programmierung von Chips entwickelt.  Eine bestimmte Art der Darstellung von reellen Zahlen, die als bfloat16 bezeichnet wird, bietet eine erhÃ¶hte Effizienz bei der Verarbeitung von Zahlen in neuronalen Netzen.  In der Umgangssprache wird es als â€Brain Floatâ€œ bezeichnet. <br><br>  TPU verwendet die schnellsten Speicherchips, Speicher mit hoher Bandbreite oder HBM (Speicher mit hoher Bandbreite).  Er sagte, dass die Nachfrage nach groÃŸen Speichermengen beim Training neuronaler Netze schnell wÃ¤chst. <br><br>  â€Das GedÃ¤chtnis wird wÃ¤hrend des Trainings intensiver genutzt.  Die Leute sprechen von Hunderten Millionen Gewichten, aber es gibt Probleme bei der Verarbeitung der Aktivierung von "Variablen eines neuronalen Netzwerks". <br><br>  Google passt auch die Art und Weise an, wie neuronale Netze programmiert werden, um das Beste aus Eisen herauszuholen.  "Wir arbeiten an Modelldaten und ParallelitÃ¤t" in Projekten wie "Mesh TensorFlow" - Anpassungen der TensorFlow-Softwareplattform "Kombination von Daten und ParallelitÃ¤t auf der Skala des Pods". <br><br>  Young gab einige technische Details nicht bekannt.  Er bemerkte, dass das Unternehmen nicht Ã¼ber interne Verbindungen sprach, darÃ¼ber, wie sich Daten entlang des Chips bewegen - er bemerkte einfach, dass "unsere Konnektoren gigantisch sind".  Er weigerte sich, auf dieses Thema einzugehen, was das Publikum zum Lachen brachte. <br><br>  Young wies auf noch interessantere Bereiche des Rechnens hin, die bald zu uns kommen kÃ¶nnten.  Zum Beispiel schlug er vor, dass Berechnungen mit analogen Chips, Schaltkreisen, die Eingangsdaten in Form von kontinuierlichen Werten anstelle von Nullen und Einsen verarbeiten, eine wichtige Rolle spielen kÃ¶nnen.  "Vielleicht wenden wir uns dem analogen Bereich zu. In der Physik gibt es viele interessante Dinge, die mit analogen Computern und NVM-Speicher zusammenhÃ¤ngen." <br><br>  Er Ã¤uÃŸerte auch die Hoffnung auf den Erfolg der auf der Konferenz vorgestellten Chip-Start-ups: â€Hier gibt es sehr coole Start-ups, und wir brauchen sie, um zu funktionieren, da die MÃ¶glichkeiten des digitalen CMOS nicht unbegrenzt sind.  Ich mÃ¶chte, dass all diese Investitionen ausgelÃ¶st werden. â€œ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de429794/">https://habr.com/ru/post/de429794/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de429782/index.html">Die Geschichte, wie wir Tests 12 Mal beschleunigt haben</a></li>
<li><a href="../de429786/index.html">Fast Sin and Cos auf eingebettetem ASM fÃ¼r Delphi</a></li>
<li><a href="../de429788/index.html">Ein weiterer Grund, warum Docker-Container verlangsamt werden</a></li>
<li><a href="../de429790/index.html">Julia und die Bewegung eines geladenen Teilchens in einem elektromagnetischen Feld</a></li>
<li><a href="../de429792/index.html">Physikbasierte kÃ¼nstliche Intelligenz kann auf die Gesetze imaginÃ¤rer Universen schlieÃŸen</a></li>
<li><a href="../de429796/index.html">Wie DeviceLock DLP vertrauliche Datenlecks auf GitHub verhindert</a></li>
<li><a href="../de429798/index.html">Verkauf von Plug-in-Elektrofahrzeugen in den USA (mit Grafiken): Oktober 2018</a></li>
<li><a href="../de429800/index.html">Symfony Bundle zum Exportieren von Statistiken im Prometheus-Format</a></li>
<li><a href="../de429804/index.html">Freundlicher Schutz einer WEB-Ressource vor Brute-Force-Angriffen</a></li>
<li><a href="../de429808/index.html">Roscosmos kann aufgrund des FSB den grÃ¶ÃŸten Auftrag verlieren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>