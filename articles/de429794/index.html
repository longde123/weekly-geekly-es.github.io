<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤸🏽 👩🏻‍💼 🧓 Google spricht über ein exponentielles KI-Wachstum, das die Art des Computing verändert 📴 👦🏿 🐝</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Clif Young, ein Google-Programmierer, erklärt, wie die explosive Entwicklung von Deep-Learning-Algorithmen mit dem Scheitern des Moore'schen Gesetzes ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google spricht über ein exponentielles KI-Wachstum, das die Art des Computing verändert</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/429794/"><h3>  Clif Young, ein Google-Programmierer, erklärt, wie die explosive Entwicklung von Deep-Learning-Algorithmen mit dem Scheitern des Moore'schen Gesetzes zusammenfällt, das seit Jahrzehnten an der Faustregel für den Fortschritt von Computerchips arbeitet, und die Entwicklung grundlegend neuer Rechenschemata erzwingt </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/f16/257/7ce/f162577cea1b28010ec340bfff65f307.jpg"><br><br>  Die explosive Entwicklung von KI- und maschinellen Lernalgorithmen verändert die Art des Rechnens - wie es in einem der größten Unternehmen heißt, die KI praktizieren - bei Google.  Der Google-Programmierer Cliff Young sprach bei der Eröffnung der Herbst-Mikroprozessorkonferenz der Linley Group, einem beliebten Computerchip-Symposium des ehrwürdigen Halbleiterunternehmens. <br><br>  Young sagte, dass der Einsatz von KI in dem Moment in die "exponentielle Phase" eintrat, als das Mooresche Gesetz, die Faustregel für den Fortschritt von Computerchips seit Jahrzehnten, vollständig gehemmt wurde. <br><a name="habracut"></a><br>  "Die Zeiten sind ziemlich nervös", sagte er nachdenklich.  „Digitale CMOS verlangsamen sich, wir sehen Probleme mit dem 10-nm-Prozess bei Intel, wir sehen sie beim 7-nm-Prozess von GlobalFoundries und gleichzeitig mit der Entwicklung von Deep Learning entsteht eine wirtschaftliche Nachfrage.  CMOS, eine komplementäre Metalloxid-Halbleiter-Struktur, ist das am häufigsten verwendete Material zur Herstellung von Computerchips. <br><br>  Während klassische Chips die Effizienz und Produktivität kaum steigern können, steigen die Anfragen von KI-Forschern, sagte Young.  Er erstellte einige Statistiken: Die Anzahl der wissenschaftlichen Arbeiten zum maschinellen Lernen, die auf der von der Cornell University unterhaltenen arXiv-Preprint-Site gespeichert sind, verdoppelt sich alle 18 Monate.  Und die Anzahl der internen Projekte, die sich bei Google auf KI konzentrieren, verdoppelt sich ebenfalls alle 18 Monate.  Der Bedarf an Gleitkommaoperationen, die zur Verarbeitung der beim maschinellen Lernen verwendeten neuronalen Netze erforderlich sind, wächst noch schneller - er verdoppelt sich alle dreieinhalb Monate. <br><br>  All dieses Wachstum bei Computerabfragen wird zu "Moores Supergesetz" kombiniert, sagte Young, und er nannte das Phänomen "ein wenig beängstigend" und "ein wenig gefährlich" und "etwas, worüber man sich Sorgen machen muss". <br><br>  "Woher kam all dieses exponentielle Wachstum", fragte er im Bereich der KI.  „Der springende Punkt ist insbesondere, dass tiefes Lernen einfach funktioniert.  In meiner Karriere habe ich maschinelles Lernen lange ignoriert “, sagte er.  "Es war nicht offensichtlich, dass diese Dinge abheben konnten." <br><br>  Aber dann entstanden schnell Durchbrüche wie die Mustererkennung, und es wurde klar, dass tiefes Lernen „unglaublich effektiv ist“, sagte er.  „In den letzten fünf Jahren waren wir größtenteils das Unternehmen, das KI an erster Stelle stellt, und wir haben die meisten auf KI basierenden Unternehmen neu gestaltet“, von der Suche über die Werbung bis hin zu vielem mehr. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a72/3ff/1e9/a723ff1e9203c6ee395dc5cd8d8bd296.jpg"><br><br>  Das Google Brain-Projektteam, ein führendes KI-Forschungsprojekt, benötigt „riesige Maschinen“, sagte Young.  Beispielsweise werden neuronale Netze manchmal anhand der Anzahl der in ihnen verwendeten „Gewichte“ gemessen, dh anhand der Variablen, die auf das neuronale Netz angewendet werden, und beeinflussen, wie es die Daten verarbeitet. <br><br>  Und wenn gewöhnliche neuronale Netze Hunderttausende oder sogar Millionen von Gewichten enthalten können, die berechnet werden müssen, benötigen Google-Forscher „Tera-Weight-Maschinen“, dh Computer, die Billionen von Gewichten berechnen können.  Denn "jedes Mal, wenn wir die Größe des neuronalen Netzwerks verdoppeln, verbessern wir seine Genauigkeit."  Die Regel der KI-Entwicklung ist, immer größer zu werden. <br><br>  Auf Anfrage von Google entwickeln sie eine eigene Chipreihe für das MO, die Tensor Processing Unit.  TPU und dergleichen werden benötigt, da herkömmliche CPUs und GPU-Grafikchips die Last nicht bewältigen können. <br><br>  "Wir haben uns sehr lange zurückgehalten und gesagt, dass Intel und Nvidia sehr gut darin sind, Hochleistungssysteme zu entwickeln", sagte Young.  "Aber wir haben diese Grenze vor fünf Jahren überschritten." <br><br>  TPU sorgte nach dem ersten öffentlichen Auftritt im Jahr 2017 für Aufsehen, da behauptet wurde, dass es in Bezug auf die Geschwindigkeit die normalen Chips übertrifft.  Google arbeitet bereits an der TPU der dritten Generation, verwendet sie in seinen Projekten und bietet Computerfunktionen bei Bedarf über den Google Cloud-Dienst an. <br><br>  Das Unternehmen stellt weiterhin immer größere TPUs her.  In seiner "Legacy" -Konfiguration sind 1024 TPUs gemeinsam mit einem neuen Supercomputertyp verbunden, und Google plant laut Young, dieses System weiter auszubauen. <br><br>  "Wir bauen riesige Multicomputer mit einer Kapazität von mehreren zehn Petabyte", sagte er.  "Wir bewegen uns unermüdlich gleichzeitig in mehrere Richtungen, und der Betrieb im Terabyte-Bereich wächst weiter."  Solche Projekte werfen alle Probleme auf, die mit der Entwicklung von Supercomputern verbunden sind. <br><br>  Zum Beispiel haben Google-Ingenieure die Tricks des legendären Cray-Supercomputers übernommen.  Sie kombinierten das gigantische „Matrixmultiplikationsmodul“, den Teil des Chips, der die Hauptlast für die Berechnung neuronaler Netze trägt, mit dem „Vektor-Allzweckmodul“ und dem „skalaren Allzweckmodul“, wie dies in Cray durchgeführt wurde.  "Die Kombination von Skalar- und Vektormodulen ermöglichte es Cray, alle in Bezug auf die Leistung zu überholen", sagte er. <br><br>  Google hat seine eigenen innovativen arithmetischen Designs für die Programmierung von Chips entwickelt.  Eine bestimmte Art der Darstellung von reellen Zahlen, die als bfloat16 bezeichnet wird, bietet eine erhöhte Effizienz bei der Verarbeitung von Zahlen in neuronalen Netzen.  In der Umgangssprache wird es als „Brain Float“ bezeichnet. <br><br>  TPU verwendet die schnellsten Speicherchips, Speicher mit hoher Bandbreite oder HBM (Speicher mit hoher Bandbreite).  Er sagte, dass die Nachfrage nach großen Speichermengen beim Training neuronaler Netze schnell wächst. <br><br>  „Das Gedächtnis wird während des Trainings intensiver genutzt.  Die Leute sprechen von Hunderten Millionen Gewichten, aber es gibt Probleme bei der Verarbeitung der Aktivierung von "Variablen eines neuronalen Netzwerks". <br><br>  Google passt auch die Art und Weise an, wie neuronale Netze programmiert werden, um das Beste aus Eisen herauszuholen.  "Wir arbeiten an Modelldaten und Parallelität" in Projekten wie "Mesh TensorFlow" - Anpassungen der TensorFlow-Softwareplattform "Kombination von Daten und Parallelität auf der Skala des Pods". <br><br>  Young gab einige technische Details nicht bekannt.  Er bemerkte, dass das Unternehmen nicht über interne Verbindungen sprach, darüber, wie sich Daten entlang des Chips bewegen - er bemerkte einfach, dass "unsere Konnektoren gigantisch sind".  Er weigerte sich, auf dieses Thema einzugehen, was das Publikum zum Lachen brachte. <br><br>  Young wies auf noch interessantere Bereiche des Rechnens hin, die bald zu uns kommen könnten.  Zum Beispiel schlug er vor, dass Berechnungen mit analogen Chips, Schaltkreisen, die Eingangsdaten in Form von kontinuierlichen Werten anstelle von Nullen und Einsen verarbeiten, eine wichtige Rolle spielen können.  "Vielleicht wenden wir uns dem analogen Bereich zu. In der Physik gibt es viele interessante Dinge, die mit analogen Computern und NVM-Speicher zusammenhängen." <br><br>  Er äußerte auch die Hoffnung auf den Erfolg der auf der Konferenz vorgestellten Chip-Start-ups: „Hier gibt es sehr coole Start-ups, und wir brauchen sie, um zu funktionieren, da die Möglichkeiten des digitalen CMOS nicht unbegrenzt sind.  Ich möchte, dass all diese Investitionen ausgelöst werden. “ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de429794/">https://habr.com/ru/post/de429794/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de429782/index.html">Die Geschichte, wie wir Tests 12 Mal beschleunigt haben</a></li>
<li><a href="../de429786/index.html">Fast Sin and Cos auf eingebettetem ASM für Delphi</a></li>
<li><a href="../de429788/index.html">Ein weiterer Grund, warum Docker-Container verlangsamt werden</a></li>
<li><a href="../de429790/index.html">Julia und die Bewegung eines geladenen Teilchens in einem elektromagnetischen Feld</a></li>
<li><a href="../de429792/index.html">Physikbasierte künstliche Intelligenz kann auf die Gesetze imaginärer Universen schließen</a></li>
<li><a href="../de429796/index.html">Wie DeviceLock DLP vertrauliche Datenlecks auf GitHub verhindert</a></li>
<li><a href="../de429798/index.html">Verkauf von Plug-in-Elektrofahrzeugen in den USA (mit Grafiken): Oktober 2018</a></li>
<li><a href="../de429800/index.html">Symfony Bundle zum Exportieren von Statistiken im Prometheus-Format</a></li>
<li><a href="../de429804/index.html">Freundlicher Schutz einer WEB-Ressource vor Brute-Force-Angriffen</a></li>
<li><a href="../de429808/index.html">Roscosmos kann aufgrund des FSB den größten Auftrag verlieren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>