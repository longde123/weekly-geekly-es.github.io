<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíê ‚ÄºÔ∏è ‚ô£Ô∏è Wir analysieren die Tonalit√§t von Texten mit Fast.ai üçû üìÜ üë¥üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In dem Artikel wird die Klassifizierung der Tonalit√§t von Textnachrichten in russischer Sprache (und im Wesentlichen jede Klassifizierung von Texten m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wir analysieren die Tonalit√§t von Texten mit Fast.ai</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/472988/">  In dem Artikel wird die Klassifizierung der Tonalit√§t von Textnachrichten in russischer Sprache (und im Wesentlichen jede Klassifizierung von Texten mit derselben Technologie) er√∂rtert.  Wir werden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen</a> Artikel als Grundlage nehmen, in dem die Klassifizierung der Tonalit√§t in der CNN-Architektur unter Verwendung des Word2vec-Modells ber√ºcksichtigt wurde.  In unserem Beispiel l√∂sen wir das gleiche Problem der Trennung von Tweets in positive und negative <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daten</a> im selben Datensatz mithilfe des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ULMFit-</a> Modells.  Das Ergebnis des Artikels (durchschnittlicher F1-Score = 0,78142) wird als Basis akzeptiert. <a name="habracut"></a><br><br><h4>  Einleitung </h4><br>  Das ULMFIT-Modell wurde 2018 von den Entwicklern von fast.ai (Jeremy Howard, Sebastian Ruder) eingef√ºhrt.  Der Kern des Ansatzes besteht darin, Transferlernen in NLP-Aufgaben zu verwenden, wenn Sie vorab trainierte Modelle verwenden, die Zeit f√ºr das Training Ihrer Modelle zu verk√ºrzen und die Anforderungen an die Gr√∂√üe der gekennzeichneten Testprobe zu reduzieren. <br><br>  Das Trainingsschema in unserem Fall sieht folgenderma√üen aus: <br><br><img src="https://habrastorage.org/webt/qx/pm/yh/qxpmyh8a6woo8qw72dvgzbqjiec.png"><br><br>  Die Bedeutung des Sprachmodells besteht darin, das n√§chste Wort nacheinander vorhersagen zu k√∂nnen.  Es ist problematisch, lange verbundene Texte auf diese Weise zu erhalten, aber dennoch k√∂nnen Sprachmodelle die Eigenschaften der Sprache erfassen, den Kontext der Verwendung von W√∂rtern verstehen, daher ist das Sprachmodell (und nicht beispielsweise die Vektoranzeige von W√∂rtern) die Grundlage der Technologie.  F√ºr die Modellierung der Sprache verwendet ULMFit die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWD-LSTM-</a> Architektur, bei der Dropout nach M√∂glichkeit aktiv verwendet wird und sinnvoll ist.  Die Art des Sprachmodelltrainings wird manchmal als halb√ºberwachtes Lernen bezeichnet, da das Etikett hier das n√§chste Wort ist und Sie nichts mit Ihren H√§nden markieren m√ºssen. <br><br>  Als vorab trainiertes Sprachmodell werden wir fast das einzige √∂ffentlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verf√ºgbare verwenden</a> . <br>  Lassen Sie uns den Lernalgorithmus von Anfang an durchgehen. <br><br>  Wir laden Bibliotheken (wir √ºberpr√ºfen die Version von Fast.ai bei Inkompatibilit√§ten): <br><br><pre><code class="python hljs">%load_ext autoreload %autoreload <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statistics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> fastai print(<span class="hljs-string"><span class="hljs-string">'fast.ai version is:'</span></span>, fastai.__version__) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> fastai <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> fastai.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split path = <span class="hljs-string"><span class="hljs-string">''</span></span></code> </pre> <br><pre> <code class="dos hljs"><span class="hljs-function"><span class="hljs-function">Out: </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fast.ai</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">version</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">is</span></span></span><span class="hljs-function">: 1.0.58</span></span></code> </pre> <br><h4>  Wir bereiten Daten f√ºr das Training vor </h4><br>  In Analogie werden wir Schulungen zu den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurztexten RuTweetCorp von Yulia Rubtsova durchf√ºhren</a> , die auf der Grundlage russischsprachiger Nachrichten von Twitter erstellt wurden.  Der K√∂rper enth√§lt 114.991 positive Tweets und 111.923 negative Tweets im CSV-Format.  Dar√ºber hinaus gibt es eine Datenbank mit nicht zugewiesenen Tweets mit einem Volumen von 17 639 674 Datens√§tzen im SQL-Format.  Die Aufgabe unseres Klassifikators besteht darin, festzustellen, ob der Tweet positiv oder negativ ist. <br><br>  Da <s>es lange gedauert hat, das Sprachmodell f√ºr 17 Millionen Tweets</s> neu zu trainieren, <s>und die</s> Aufgabe darin bestand, die Faulheit beim Transferlernen zu zeigen, werden wir das Sprachmodell f√ºr einen Text aus dem Trainingsdatensatz neu trainieren, wobei die Basis nicht zugeordneter Tweets vollst√§ndig ignoriert wird.  Wenn Sie diese Basis zum ‚ÄûSch√§rfen‚Äú des Sprachmodells verwenden, k√∂nnen Sie wahrscheinlich das Gesamtergebnis verbessern. <br><br>  Wir bilden Datens√§tze f√ºr Training und Test mit vorl√§ufiger Textverarbeitung.  Wir nehmen den Code aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel</a> : <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount'] data_positive = pd.read_csv('data/positive.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) data_negative = pd.read_csv('data/negative.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) #    sample_size = min(data_positive.shape[0], data_negative.shape[0]) raw_data = np.concatenate((data_positive['text'].values[:sample_size], data_negative['text'].values[:sample_size]), axis=0) labels = [1] * sample_size + [0] * sample_size</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = text.lower().replace(<span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>) text = re.sub(<span class="hljs-string"><span class="hljs-string">'((www\.[^\s]+)|(https?://[^\s]+))'</span></span>, <span class="hljs-string"><span class="hljs-string">'URL'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'@[^\s]+'</span></span>, <span class="hljs-string"><span class="hljs-string">'USER'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() data = [preprocess_text(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> raw_data]</code> </pre> <br><pre> <code class="python hljs">df_train=pd.DataFrame(columns=[<span class="hljs-string"><span class="hljs-string">'Text'</span></span>, <span class="hljs-string"><span class="hljs-string">'Label'</span></span>]) df_test=pd.DataFrame(columns=[<span class="hljs-string"><span class="hljs-string">'Text'</span></span>, <span class="hljs-string"><span class="hljs-string">'Label'</span></span>]) df_train[<span class="hljs-string"><span class="hljs-string">'Text'</span></span>], df_test[<span class="hljs-string"><span class="hljs-string">'Text'</span></span>], df_train[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>], df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>] = train_test_split(data, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><pre> <code class="python hljs">df_val=pd.DataFrame(columns=[<span class="hljs-string"><span class="hljs-string">'Text'</span></span>, <span class="hljs-string"><span class="hljs-string">'Label'</span></span>]) df_train, df_val = train_test_split(df_train, test_size=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Wir schauen uns an, was passiert ist: <br><br><pre> <code class="python hljs">df_train.groupby(<span class="hljs-string"><span class="hljs-string">'Label'</span></span>).count()</code> </pre> <br><img src="https://habrastorage.org/webt/52/mj/2h/52mj2hhy5_l76e3ac5-wqedflw8.png"><br><br><pre> <code class="python hljs">df_val.groupby(<span class="hljs-string"><span class="hljs-string">'Label'</span></span>).count()</code> </pre> <br><img src="https://habrastorage.org/webt/wh/ub/cb/whubcbjiw1lqkzdw9-r_xiet1gk.png"><br><br><pre> <code class="python hljs">df_test.groupby(<span class="hljs-string"><span class="hljs-string">'Label'</span></span>).count()</code> </pre> <br><img src="https://habrastorage.org/webt/kn/4i/ps/kn4ipsrgguvkzr8lrwc5pwcyti0.png"><br><br><h4>  Ein Sprachmodell lernen </h4><br>  Daten laden: <br><br><pre> <code class="python hljs">tokenizer=Tokenizer(lang=<span class="hljs-string"><span class="hljs-string">'xx'</span></span>) data_lm = TextLMDataBunch.from_df(path, tokenizer=tokenizer, bs=<span class="hljs-number"><span class="hljs-number">16</span></span>, train_df=df_train, valid_df=df_val, text_cols=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br>  Wir schauen uns den Inhalt an: <br><br><pre> <code class="python hljs">data_lm.show_batch()</code> </pre> <br><img src="https://habrastorage.org/webt/xc/da/fo/xcdafozibrlzriutj9jt3d4hdp0.png"><br><br>  Wir bieten Links zu den gespeicherten Gewichten des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorgefertigten</a> Modells und ein W√∂rterbuch: <br><br><pre> <code class="python hljs">weights_pretrained = <span class="hljs-string"><span class="hljs-string">'ULMFit/lm_5_ep_lr2-3_5_stlr'</span></span> itos_pretrained = <span class="hljs-string"><span class="hljs-string">'ULMFit/itos'</span></span> pretained_data = (weights_pretrained, itos_pretrained)</code> </pre> <br>  Wir schaffen Lernende, aber vorher - eine Kr√ºcke f√ºr fast.ai.  Das vorab trainierte Modell wurde auf einer √§lteren Version der Bibliothek trainiert, daher m√ºssen Sie die Anzahl der Knoten in der verborgenen Schicht des neuronalen Netzwerks anpassen. <br><br><pre> <code class="python hljs">config = awd_lstm_lm_config.copy() config[<span class="hljs-string"><span class="hljs-string">'n_hid'</span></span>] = <span class="hljs-number"><span class="hljs-number">1150</span></span> learn_lm = language_model_learner(data_lm, AWD_LSTM, config=config, pretrained_fnames=pretained_data, drop_mult=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) learn_lm.freeze()</code> </pre> <br>  Wir suchen die optimale Lernrate: <br><br><pre> <code class="python hljs">learn_lm.lr_find() learn_lm.recorder.plot()</code> </pre> <br><img src="https://habrastorage.org/webt/ra/zd/tu/razdtu4ybrn60knvib8mq9fgyzc.png"><br>  Wir trainieren das Modell der 3. √Ñra (im Modell ist nur die letzte Gruppe von Schichten nicht gefroren). <br><br><pre> <code class="python hljs">learn_lm.fit_one_cycle(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1e-2</span></span>, moms=(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, <span class="hljs-number"><span class="hljs-number">0.7</span></span>))</code> </pre> <br><img src="https://habrastorage.org/webt/an/fb/rg/anfbrgrfrwbuwboj_zbizaqznps.png"><br>  Auftauen des Modells, Unterrichten von 5 weiteren Epochen mit einer niedrigeren Lernrate: <br><br><pre> <code class="python hljs">learn_lm.unfreeze() learn_lm.fit_one_cycle(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1e-3</span></span>, moms=(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, <span class="hljs-number"><span class="hljs-number">0.7</span></span>))</code> </pre> <br><img src="https://habrastorage.org/webt/a2/qy/cl/a2qyclrnexjry_gxqecltoyctle.png"><br><br><pre> <code class="python hljs">learn_lm.save(<span class="hljs-string"><span class="hljs-string">'lm_ft'</span></span>)</code> </pre><br>  Wir versuchen, Text auf einem trainierten Modell zu generieren. <br><br><pre> <code class="python hljs">learn_lm.predict(<span class="hljs-string"><span class="hljs-string">"  "</span></span>, n_words=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre> <br><pre> <code class="dos hljs"><span class="hljs-function"><span class="hljs-function">Out: '       '</span></span></code> </pre> <br><pre> <code class="python hljs">learn_lm.predict(<span class="hljs-string"><span class="hljs-string">",  "</span></span>, n_words=<span class="hljs-number"><span class="hljs-number">4</span></span>)</code> </pre> <br><pre> <code class="dos hljs"><span class="hljs-function"><span class="hljs-function">Out: ',      '</span></span></code> </pre> <br>  Wir sehen - etwas, was das Modell tut.  Unsere Hauptaufgabe ist jedoch die Klassifizierung, und f√ºr ihre L√∂sung werden wir einen Encoder aus dem Modell nehmen. <br><br><pre> <code class="python hljs">learn_lm.save_encoder(<span class="hljs-string"><span class="hljs-string">'ft_enc'</span></span>)</code> </pre><br><h4>  Wir trainieren den Klassifikator </h4><br>  Laden Sie Daten f√ºr das Training herunter <br><br><pre> <code class="python hljs">data_clas = TextClasDataBunch.from_df(path, vocab=data_lm.train_ds.vocab, bs=<span class="hljs-number"><span class="hljs-number">32</span></span>, train_df=df_train, valid_df=df_val, text_cols=<span class="hljs-number"><span class="hljs-number">0</span></span>, label_cols=<span class="hljs-number"><span class="hljs-number">1</span></span>, tokenizer=tokenizer)</code> </pre> <br>  Schauen wir uns die Daten an, wir sehen, dass die Labels erfolgreich gez√§hlt wurden (0 bedeutet negativ und 1 bedeutet einen positiven Kommentar): <br><br><pre> <code class="python hljs">data_clas.show_batch()</code> </pre> <br><img src="https://habrastorage.org/webt/gi/vd/4v/givd4vcr8dw_kuqypq4cmlvs8yi.png"><br><br>  Erstellen Sie einen Lernenden mit einer √§hnlichen Kr√ºcke: <br><br><pre> <code class="python hljs">config = awd_lstm_clas_config.copy() config[<span class="hljs-string"><span class="hljs-string">'n_hid'</span></span>] = <span class="hljs-number"><span class="hljs-number">1150</span></span> learn = text_classifier_learner(data_clas, AWD_LSTM, config=config, drop_mult=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)</code> </pre> <br>  Wir laden den in der vorherigen Phase trainierten Encoder und frieren das Modell mit Ausnahme der letzten Gewichtsgruppe ein: <br><pre> <code class="python hljs">learn.load_encoder(<span class="hljs-string"><span class="hljs-string">'ft_enc'</span></span>) learn.freeze()</code> </pre> <br>  Wir suchen die optimale Lernrate: <br><br><pre> <code class="python hljs">learn.lr_find() learn.recorder.plot(skip_start=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/dq/d0/by/dqd0bylpp_8mgn78zbaxrn_ssfe.png"><br>  Wir trainieren das Modell mit dem allm√§hlichen Auftauen von Schichten. <br><br><pre> <code class="python hljs">learn.fit_one_cycle(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2e-2</span></span>, moms=(<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.7</span></span>))</code> </pre> <br><img src="https://habrastorage.org/webt/uv/zo/qg/uvzoqgzdislgxbirp3cnwu5fg4u.png"><br><br><pre> <code class="python hljs">learn.freeze_to(<span class="hljs-number"><span class="hljs-number">-2</span></span>) learn.fit_one_cycle(<span class="hljs-number"><span class="hljs-number">3</span></span>, slice(<span class="hljs-number"><span class="hljs-number">1e-2</span></span>/(<span class="hljs-number"><span class="hljs-number">2.6</span></span>**<span class="hljs-number"><span class="hljs-number">4</span></span>),<span class="hljs-number"><span class="hljs-number">1e-2</span></span>), moms=(<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.7</span></span>))</code> </pre> <br><img src="https://habrastorage.org/webt/bx/pz/xq/bxpzxqpr7d7qqs1ays0dmgl3r-w.png"><br><br><pre> <code class="python hljs">learn.freeze_to(<span class="hljs-number"><span class="hljs-number">-3</span></span>) learn.fit_one_cycle(<span class="hljs-number"><span class="hljs-number">2</span></span>, slice(<span class="hljs-number"><span class="hljs-number">5e-3</span></span>/(<span class="hljs-number"><span class="hljs-number">2.6</span></span>**<span class="hljs-number"><span class="hljs-number">4</span></span>),<span class="hljs-number"><span class="hljs-number">5e-3</span></span>), moms=(<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.7</span></span>))</code> </pre> <br><img src="https://habrastorage.org/webt/3b/7t/gj/3b7tgjzickijejuevvgsoy3xe6o.png"><br><br><pre> <code class="python hljs">learn.unfreeze() learn.fit_one_cycle(<span class="hljs-number"><span class="hljs-number">2</span></span>, slice(<span class="hljs-number"><span class="hljs-number">1e-3</span></span>/(<span class="hljs-number"><span class="hljs-number">2.6</span></span>**<span class="hljs-number"><span class="hljs-number">4</span></span>),<span class="hljs-number"><span class="hljs-number">1e-3</span></span>), moms=(<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.7</span></span>))</code> </pre> <br><img src="https://habrastorage.org/webt/t9/6o/3k/t96o3k72yvcmzq5es58pb-tiqo4.png"><br><br><pre> <code class="python hljs">learn.save(<span class="hljs-string"><span class="hljs-string">'tweet-0801'</span></span>)</code> </pre> <br>  Wir sehen, dass sie an der Validierungsprobe eine Genauigkeit von 80,1% erreichten. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">Wir werden das</a> Modell anhand des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">ZlodeiBaal-</a> Kommentars zu meinem vorherigen Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">testen</a> : <br><br><pre> <code class="python hljs">learn.predict(<span class="hljs-string"><span class="hljs-string">'        ‚Äî ?'</span></span>)</code> </pre> <br><pre> <code class="dos hljs"><span class="hljs-function"><span class="hljs-function">Out: (</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Category</span></span></span><span class="hljs-function"> 0, </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tensor</span></span></span><span class="hljs-function">(0), </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tensor</span></span></span><span class="hljs-function">([0.6283, 0.3717]))</span></span></code> </pre> <br>  Wir sehen, dass das Modell diesen Kommentar negativ zugeschrieben hat :-) <br><br><h4>  √úberpr√ºfen des Modells an einem Testmuster </h4><br>  Die Hauptaufgabe in dieser Phase besteht darin, das Modell auf seine Generalisierungsf√§higkeit zu testen.  Dazu validieren wir das Modell f√ºr den im DataFrame df_test gespeicherten Datensatz, der bis dahin weder f√ºr das Sprachmodell noch f√ºr den Klassifikator verf√ºgbar war. <br><br><pre> <code class="python hljs">data_test_clas = TextClasDataBunch.from_df(path, vocab=data_lm.train_ds.vocab, bs=<span class="hljs-number"><span class="hljs-number">32</span></span>, train_df=df_train, valid_df=df_test, text_cols=<span class="hljs-number"><span class="hljs-number">0</span></span>, label_cols=<span class="hljs-number"><span class="hljs-number">1</span></span>, tokenizer=tokenizer)</code> </pre> <br><pre> <code class="python hljs">config = awd_lstm_clas_config.copy() config[<span class="hljs-string"><span class="hljs-string">'n_hid'</span></span>] = <span class="hljs-number"><span class="hljs-number">1150</span></span> learn_test = text_classifier_learner(data_test_clas, AWD_LSTM, config=config, drop_mult=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)</code> </pre> <br><pre> <code class="python hljs">learn_test.load_encoder(<span class="hljs-string"><span class="hljs-string">'ft_enc'</span></span>) learn_test.load(<span class="hljs-string"><span class="hljs-string">'tweet-0801'</span></span>)</code> </pre> <br><pre> <code class="python hljs">learn_test.validate()</code> </pre> <br><pre> <code class="dos hljs"><span class="hljs-function"><span class="hljs-function">Out: [0.4391682, </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tensor</span></span></span><span class="hljs-function">(0.7973)]</span></span></code> </pre> <br>  Wir sehen, dass die Genauigkeit der Testprobe 79,7% betrug. <br><br>  Schauen Sie sich Confusion Matrix an: <br><br><pre> <code class="python hljs">interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix()</code> </pre> <br><img src="https://habrastorage.org/webt/em/8g/3t/em8g3tjcki4tueqv-mrc9jmf3t8.png"><br><br>  Wir berechnen die Parameter f√ºr Genauigkeit, R√ºckruf und f1-Punktzahl. <br><br><pre> <code class="python hljs">neg_precision = interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] / (interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] + interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]) neg_recall = interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] / (interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] + interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>]) pos_precision = interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>] / (interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>] + interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>]) pos_recall = interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>] / (interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>] + interp.confusion_matrix()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]) neg_f1score = <span class="hljs-number"><span class="hljs-number">2</span></span> * (neg_precision * neg_recall) / (neg_precision + neg_recall) pos_f1score = <span class="hljs-number"><span class="hljs-number">2</span></span> * (pos_precision * pos_recall) / (pos_precision + pos_recall)</code> </pre> <br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'    F1-score'</span></span>) print(<span class="hljs-string"><span class="hljs-string">' Negative {0:1.5f} {1:1.5f} {2:1.5f}'</span></span>.format(neg_precision, neg_recall, neg_f1score)) print(<span class="hljs-string"><span class="hljs-string">' Positive {0:1.5f} {1:1.5f} {2:1.5f}'</span></span>.format(pos_precision, pos_recall, pos_f1score)) print(<span class="hljs-string"><span class="hljs-string">' Average {0:1.5f} {1:1.5f} {2:1.5f}'</span></span>.format(statistics.mean([neg_precision, pos_precision]), statistics.mean([neg_recall, pos_recall]), statistics.mean([neg_f1score, pos_f1score])))</code> </pre> <br><pre> <code class="dos hljs"><span class="hljs-function"><span class="hljs-function">Out:     </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">F1</span></span></span><span class="hljs-function">-</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">score</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Negative</span></span></span><span class="hljs-function"> 0.79989 0.80451 0.80219 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Positive</span></span></span><span class="hljs-function"> 0.80142 0.79675 0.79908 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Average</span></span></span><span class="hljs-function"> 0.80066 0.80063 0.80064</span></span></code> </pre> <br>  Das in der Testprobe gezeigte Ergebnis ist ein durchschnittlicher F1-Score = 0,80064. <br><br>  Gespeicherte Modellgewichte k√∂nnen hier genommen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472988/">https://habr.com/ru/post/de472988/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472972/index.html">Wir arbeiten mit Cookies als Javascript-Klasse</a></li>
<li><a href="../de472978/index.html">Arduino-Autorenkurs f√ºr seinen eigenen Sohn</a></li>
<li><a href="../de472980/index.html">Shorts Belokamentseva</a></li>
<li><a href="../de472982/index.html">"H√∂ren Sie, um eine Aufschl√ºsselung zu finden": Audioaufnahmen von ausgefallenen Industriemaschinen ver√∂ffentlicht</a></li>
<li><a href="../de472984/index.html">ROS Lkw-Wagen. Teil 7. Lokalisierung des Roboters: Gmapping, AMCL, Referenzpunkte auf der Raumkarte</a></li>
<li><a href="../de472994/index.html">Was machen wir falsch mit dem Fr√ºhling?</a></li>
<li><a href="../de472996/index.html">Pentagon entwickelt Drohnensteuerungstechnologie mit den Gedanken der Soldaten</a></li>
<li><a href="../de473000/index.html">Ein kurzer Mathef√ºhrer f√ºr Ausl√§nder</a></li>
<li><a href="../de473002/index.html">Erkl√§rung des Fermi-Paradoxons im Rahmen der Weltraumsoziologie Liu Qixin</a></li>
<li><a href="../de473006/index.html">DevOps - alles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>