<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí∂ üë©üèº‚Äçüé® ‚ûñ Aceleraci√≥n de hardware de redes neuronales profundas: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP y otras letras üßëüèª üìî üë©‚Äçüç≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El 14 de mayo, cuando Trump se estaba preparando para lanzar todos los perros en Huawei, me sent√© pac√≠ficamente en Shenzhen en el Huawei STW 2019, una...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aceleraci√≥n de hardware de redes neuronales profundas: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP y otras letras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455353/"><img src="https://habrastorage.org/getpro/habr/post_images/1d8/7a7/de6/1d87a7de6c72f1f712049ce550978d7c.png"><br><br>  El 14 de mayo, cuando Trump se estaba preparando para lanzar todos los perros en Huawei, me sent√© pac√≠ficamente en Shenzhen en el Huawei STW 2019, una gran conferencia para 1000 participantes, que incluy√≥ informes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Philip Wong</a> , vicepresidente de investigaci√≥n de TSMC sobre las perspectivas de la computaci√≥n no von Neumann arquitecturas, y Heng Liao, becario de Huawei, cient√≠fico jefe del laboratorio de Huawei 2012, sobre el desarrollo de una nueva arquitectura de procesadores de tensor y neuroprocesadores.  TSMC, si lo sabe, fabrica aceleradores neuronales para Apple y Huawei con tecnolog√≠a de 7 nm (que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pocas personas poseen</a> ), y Huawei est√° listo para competir con Google y NVIDIA en t√©rminos de neuroprocesadores. <br><br>  Google en China est√° prohibido, no me molest√© en poner una VPN en la tableta, as√≠ que us√© <s>patriotamente</s> Yandex para ver cu√°l es la situaci√≥n con otros fabricantes de hierro similar y qu√© sucede generalmente.  En general, observ√© la situaci√≥n, pero solo despu√©s de estos informes me di cuenta de lo grande que se estaba preparando la revoluci√≥n en las entra√±as de las compa√±√≠as y el silencio de las salas cient√≠ficas. <br><br>  Solo el a√±o pasado, se invirtieron m√°s de $ 3 mil millones en el tema.  Google ha declarado durante mucho tiempo que las redes neuronales son un √°rea estrat√©gica, est√° construyendo activamente su soporte de hardware y software.  NVIDIA, sintiendo que el trono es asombroso, est√° haciendo esfuerzos fant√°sticos en bibliotecas de aceleraci√≥n de redes neuronales y nuevo hardware.  Intel en 2016 gast√≥ 0.8 mil millones para comprar dos compa√±√≠as involucradas en la aceleraci√≥n de hardware de redes neuronales.  Y esto a pesar del hecho de que las compras principales a√∫n no han comenzado, y el n√∫mero de jugadores ha superado los cincuenta y est√° creciendo r√°pidamente. <br><br><div style="text-align:center;"><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/74c/308/a37/74c308a372700574cc0e29f13347ede5.png"></div><br>  TPU, VPU, IPU, DPU, NPU, RPU, NNP: ¬øqu√© significa todo esto y qui√©n ganar√°?  Tratemos de resolverlo.  ¬øA qui√©n le importa? ¬°Bienvenido al gato! <br><a name="habracut"></a><br><hr>  <b><font color="#ff0000">Descargo de responsabilidad: el</font></b> autor tuvo que reescribir completamente los algoritmos de procesamiento de video para una implementaci√≥n efectiva en ASIC, y los clientes hicieron prototipos en FPGA, por lo que hay una idea de la profundidad de la diferencia en las arquitecturas.  Sin embargo, el autor no ha trabajado directamente con el hierro recientemente.  Pero √©l anticipa que tendr√° que profundizar. <br><br><h2>  Antecedentes de problemas </h2><br>  El n√∫mero de c√°lculos requeridos est√° creciendo r√°pidamente, a la gente le encantar√≠a tomar m√°s capas, m√°s opciones de arquitectura, jugar m√°s activamente con hiperpar√°metros, pero ... depende del rendimiento.  Al mismo tiempo, por ejemplo, con el crecimiento de la productividad de los viejos procesadores buenos, grandes problemas.  Todo lo bueno llega a su fin: la ley de Moore, como saben, se est√° agotando y la tasa de crecimiento del rendimiento del procesador cae: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/704/8e8/bab/7048e8bab326e1f4f62058e5f3a925f4.png"></div><br>  <i>C√°lculos del rendimiento real de operaciones enteras en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SPECint en</a> comparaci√≥n con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VAX11-780</a> , en lo sucesivo a menudo una escala logar√≠tmica</i> <br><br>  Si desde mediados de los 80 hasta mediados de los 2000, en los a√±os bendecidos del apogeo de las computadoras, el crecimiento fue a una tasa promedio de 52% por a√±o, en los √∫ltimos a√±os ha disminuido a 3% por a√±o.  Y esto es un problema (una traducci√≥n de un art√≠culo reciente del patriarca John Hennessey sobre los problemas y las perspectivas de la arquitectura moderna <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estaba en Habr√©</a> ). <br><br>  Hay muchas razones, por ejemplo, la frecuencia de los procesadores ha dejado de crecer: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/a49/5d6/be5/a495d6be5c00ce436bb4c2f1f7f356fc.png"></div><br>  Se hizo m√°s dif√≠cil reducir el tama√±o de los transistores.  La √∫ltima desgracia que reduce dr√°sticamente el rendimiento (incluido el rendimiento de las CPU ya lanzadas) es (redoble de tambores) ... correcto, seguridad.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Meltdown</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spectre</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otras</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vulnerabilidades</a> causan un da√±o enorme a la tasa de crecimiento de la potencia de procesamiento de la CPU ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo de deshabilitaci√≥n de hyperthreading</a> (!)).  El tema se ha vuelto popular y se encuentran nuevas vulnerabilidades de este tipo <i>casi mensualmente</i> .  Y esto es una especie de pesadilla, porque duele en t√©rminos de rendimiento. <br><br>  Al mismo tiempo, el desarrollo de muchos algoritmos est√° firmemente vinculado al crecimiento familiar en la potencia del procesador.  Por ejemplo, muchos investigadores de hoy en d√≠a no est√°n preocupados por la velocidad de los algoritmos; se les ocurrir√° algo.  Y ser√≠a bueno cuando se aprende: las redes se vuelven grandes y "dif√≠ciles" de usar.  Esto es especialmente evidente en el video, para el cual la mayor√≠a de los enfoques, en principio, no son aplicables a alta velocidad.  Y a menudo tienen sentido solo en tiempo real.  Esto tambi√©n es un problema. <br><br>  Asimismo, se est√°n desarrollando nuevos est√°ndares de compresi√≥n que implican un aumento en la potencia del decodificador.  ¬øY si la potencia del procesador no crece?  La generaci√≥n anterior recuerda c√≥mo en la d√©cada de 2000 hubo problemas para reproducir videos de alta definici√≥n en la entonces nueva <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">H.264</a> en computadoras m√°s antiguas.  S√≠, la calidad era mejor con un tama√±o m√°s peque√±o, pero en escenas r√°pidas la imagen se colgaba o el sonido se rasgaba.  Tengo que comunicarme con los desarrolladores del nuevo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VVC / H.266</a> (se planea un lanzamiento para el pr√≥ximo a√±o).  No los envidiar√°s. <br><br>  Entonces, ¬øqu√© nos prepara el pr√≥ximo siglo a la luz de la disminuci√≥n en la tasa de crecimiento del rendimiento del procesador aplicado a las redes neuronales? <br><br><h2>  CPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/b44/44b/f1eb4444b3e5e320447c6f65fa4ef14c.png"><br><br>  Una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CPU</a> normal es una gran trituradora que se ha perfeccionado durante d√©cadas.  Por desgracia, para otras tareas. <br><br>  Cuando trabajamos con redes neuronales, especialmente las profundas, nuestra red puede ocupar cientos de megabytes.  Por ejemplo, los requisitos de memoria de las redes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">detecci√≥n</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">objetos son los</a> siguientes: <br><div class="scrollable-table"><table><tbody><tr><td>  modelo <br></td><td>  tama√±o de entrada <br></td><td>  memoria param <br></td><td>  funci√≥n de memoria <br></td></tr><tr><td>  <a href="">rfcn-res50-pascal</a> <br></td><td>  600 x 850 <br></td><td>  122 MB <br></td><td>  1 GB <br></td></tr><tr><td>  <a href="">rfcn-res101-pascal</a> <br></td><td>  600 x 850 <br></td><td>  194 MB <br></td><td>  2 GB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-300</a> <br></td><td>  300 x 300 <br></td><td>  100 MB <br></td><td>  116 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-512</a> <br></td><td>  512 x 512 <br></td><td>  104 MB <br></td><td>  337 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-mobilenet-ft</a> <br></td><td>  300 x 300 <br></td><td>  22 MB <br></td><td>  37 MB <br></td></tr><tr><td>  <a href="">M√°s r√°pido-rcnn-vggvd-pascal</a> <br></td><td>  600 x 850 <br></td><td>  523 MB <br></td><td>  600 MB <br></td></tr></tbody></table></div><br><br>  En nuestra experiencia, los coeficientes de una red neuronal profunda para procesar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bordes transl√∫cidos</a> pueden ocupar 150-200 MB.  Los colegas de la red neuronal determinan la edad y el sexo del tama√±o de los coeficientes del orden de 50 MB.  Y durante la optimizaci√≥n para la versi√≥n m√≥vil de precisi√≥n reducida: aproximadamente 25 MB (float32‚áífloat16). <br><br>  Al mismo tiempo, el gr√°fico de retraso al acceder a la memoria, dependiendo del tama√±o de los datos, se distribuye <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aproximadamente de esta manera</a> (la escala horizontal es logar√≠tmica): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1f4/2aa/134/1f42aa13427972cd1762383b33cfe974.png"></div><br><br>  Es decir  Con un aumento en el volumen de datos de m√°s de 16 MB, el retraso aumenta en 50 veces o m√°s, lo que afecta fatalmente el rendimiento.  De hecho, la mayor√≠a de las veces, la CPU, cuando trabaja con redes neuronales profundas, espera datos <s>est√∫pidamente</s> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los datos de Intel</a> sobre la aceleraci√≥n de varias redes son interesantes, donde, de hecho, la aceleraci√≥n ocurre solo cuando la red se vuelve peque√±a (por ejemplo, como resultado de la cuantificaci√≥n de los pesos), para comenzar a ingresar al menos parcialmente el cach√© junto con los datos procesados.  Tenga en cuenta que el cach√© de una CPU moderna consume hasta la mitad de la energ√≠a del procesador.  En el caso de redes neuronales pesadas, es ineficaz y funciona un calentador excesivamente caro. <br><br><div class="spoiler">  <b class="spoiler_title">Para seguidores de redes neuronales en la CPU</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Seg√∫n</a> nuestras pruebas internas, incluso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Intel OpenVINO</a> pierde la implementaci√≥n del marco de multiplicaci√≥n de matriz + NNPACK en muchas arquitecturas de red (especialmente en arquitecturas simples donde el ancho de banda es importante para el procesamiento de datos en tiempo real en modo de subproceso √∫nico).  Tal escenario es relevante para varios clasificadores de objetos en la imagen (donde la red neuronal necesita ejecutarse una gran cantidad de veces, 50-100 en t√©rminos de la cantidad de objetos en la imagen) y la sobrecarga de iniciar OpenVINO se vuelve excesivamente alta. <br></div></div><br>  <b>Pros:</b> <br><br><ul><li>  "Todo el mundo lo tiene", y generalmente est√° inactivo, es decir  precio de <i>entrada</i> relativamente bajo para facturaci√≥n e implementaci√≥n. <br></li><li>  Hay redes separadas que no son CV que se adaptan bien a la CPU, los colegas llaman, por ejemplo, Wide &amp; Deep y GNMT. <br></li></ul><br>  <b>Menos:</b> <br><ul><li>  La CPU es ineficiente cuando se trabaja con redes neuronales profundas (cuando el n√∫mero de capas de red y el tama√±o de los datos de entrada son grandes), todo funciona muy lentamente. <br></li></ul><br><h2>  GPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/aee/06a/d5b/aee06ad5beab0896005e75769b3ba910.png"><br><br>  El tema es bien conocido, por lo que resumimos brevemente lo principal.  En el caso de las redes neuronales, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GPU</a> tiene una ventaja de rendimiento significativa en tareas masivamente paralelas: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/717/03d/d9b/71703dd9bce82918d1e0c7138a09adf0.png"></div><br>  Preste atenci√≥n a c√≥mo se recoce el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Xeon Phi 7290 de</a> 72 n√∫cleos, mientras que el "azul" tambi√©n es el servidor Xeon, es decir.  Intel no se rinde tan f√°cilmente, lo cual se discutir√° a continuaci√≥n.  Pero lo m√°s importante, la memoria de las tarjetas de video fue dise√±ada originalmente para un rendimiento aproximadamente 5 veces mayor.  En las redes neuronales, la computaci√≥n con datos es extremadamente simple.  Algunas acciones elementales, y necesitamos nuevos datos.  Como resultado, la velocidad de acceso a los datos es cr√≠tica para la operaci√≥n eficiente de una red neuronal.  Una memoria de alta velocidad "integrada" en la GPU y un sistema de administraci√≥n de cach√© m√°s flexible que en la CPU pueden resolver este problema: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/30a/d1b/07e/30ad1b07e7b22d97dc3372e4351c2e3c.png"></div><br><br>  Tim Detmers ha estado apoyando la interesante revisi√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Qu√© GPU (s) obtener para el aprendizaje profundo: mi experiencia y consejos para usar GPU en el aprendizaje profundo"</a> desde hace varios a√±os.  Est√° claro que Tesla y Titans gobiernan para el entrenamiento, aunque la diferencia en las arquitecturas puede causar arrebatos interesantes, por ejemplo, en el caso de redes neuronales recurrentes (y el l√≠der en general es TPU, tenga en cuenta para el futuro): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6e6/bb0/43a/6e6bb043aab85679d1ddf01028e19728.png"></div><br>  Sin embargo, hay un gr√°fico de rendimiento extremadamente √∫til para el d√≥lar, en el caballo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RTX</a> (probablemente debido a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sus</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">n√∫cleos Tensor</a> ), si tiene suficiente memoria para ello, por supuesto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f13/338/032/f13338032ca812397cbbe0228bda9ed5.png"></div><br>  Por supuesto, el costo de la inform√°tica es importante.  El segundo lugar de la primera calificaci√≥n y el √∫ltimo de la segunda: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tesla V100</a> se vende por 700 mil rublos, como 10 computadoras "normales" (+ interruptor Infiniband costoso, si desea entrenar en varios nodos).  Verdadero V100 y funciona para diez.  Las personas est√°n dispuestas a pagar de m√°s por una aceleraci√≥n tangible del aprendizaje. <br><br>  Total, resumir! <br><br>  <b>Pros:</b> <br><ul><li>  Cardinal - 10-100 veces - aceleraci√≥n en comparaci√≥n con la CPU. <br></li><li>  Extremadamente efectivo para el entrenamiento (y algo menos efectivo para el uso). <br></li></ul><br>  <b>Menos:</b> <br><ul><li>  El costo de las tarjetas de video de gama alta (que tienen suficiente memoria para entrenar redes grandes) excede el costo del resto de la computadora ... <br></li></ul><br><h2>  FPGA </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/0d5/554/90b/0d555490b015730051341cc857d14606.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FPGA</a> ya es m√°s interesante.  Esta es una red de varios millones de bloques programables, que tambi√©n podemos interconectar mediante programaci√≥n.  La red y los bloques se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ven m√°s o menos</a> as√≠ (el cuello de botella es el cuello de botella, preste atenci√≥n, nuevamente frente a la memoria del chip, pero es m√°s f√°cil, que se describir√° a continuaci√≥n): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d8/8f9/789/0d88f9789f60720da892b7acae7ab8ec.png"><br>  Naturalmente, tiene sentido usar FPGA ya en la etapa de usar una red neuronal (en la mayor√≠a de los casos, no hay suficiente memoria para el entrenamiento).  Adem√°s, el tema de la ejecuci√≥n en FPGA ahora ha comenzado a desarrollarse activamente.  Por ejemplo, aqu√≠ est√° el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">marco fpgaConvNet</a> , que puede acelerar significativamente el uso de CNN en FPGA y reducir el consumo de energ√≠a. <br><br>  La ventaja clave de FPGA es que podemos almacenar la red directamente en las celdas, es decir.  desaparece m√°gicamente un punto delgado en forma de cientos de megabytes de los mismos datos que se transfieren 25 veces por segundo (para video) en la misma direcci√≥n.  Esto permite una velocidad de reloj m√°s baja y la ausencia de cach√©s en lugar de un rendimiento m√°s bajo para obtener un aumento notable.  S√≠, y reduce dr√°sticamente el consumo de energ√≠a del <s>calentamiento global</s> por unidad de c√°lculo. <br><br>  Intel se uni√≥ activamente al proceso, lanzando el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenVINO Toolkit</a> en c√≥digo abierto el a√±o pasado, que incluye el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deep Learning Deployment Toolkit</a> (parte de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenCV</a> ).  Adem√°s, el rendimiento de los FPGA en diferentes cuadr√≠culas parece bastante interesante, y la ventaja de los FPGA en comparaci√≥n con las GPU (aunque las GPU integradas por Intel) es bastante significativa: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/8ba/a66/37e/8baa6637e904732e4e883a196b8d803d.png"></div><br>  Lo que calienta especialmente el alma del autor: se comparan los FPS, es decir  fotogramas por segundo es la m√©trica m√°s pr√°ctica para video.  Dado que Intel compr√≥ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Altera</a> , el segundo jugador m√°s grande en el mercado de FPGA, en 2015, la tabla ofrece buenos motivos para pensar. <br><br>  Y, obviamente, la barrera de entrada a tales arquitecturas es mayor, por lo que debe pasar alg√∫n tiempo antes de que aparezcan herramientas convenientes que tengan en cuenta la arquitectura FPGA fundamentalmente diferente.  Pero subestimar el potencial de la tecnolog√≠a no vale la pena.  Borda dolorosamente muchos lugares delgados. <br><br>  Finalmente, enfatizamos que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">programar FPGAs</a> es un arte separado.  Como tal, el programa no se ejecuta all√≠, y todos los c√°lculos se realizan en t√©rminos de flujos de datos, retrasos de flujo (que afecta el rendimiento) y puertas usadas (que siempre faltan).  Por lo tanto, para comenzar a programar de manera efectiva, debe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cambiar</a> completamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">su propio firmware</a> (en la red neuronal que est√° entre sus o√≠dos).  Con buena eficiencia, esto no se obtiene en absoluto.  Sin embargo, los nuevos marcos pronto ocultar√°n la diferencia externa de los investigadores. <br><br>  <b>Pros:</b> <br><br><ul><li>  Ejecuci√≥n de red potencialmente m√°s r√°pida. <br></li><li>  Consumo de energ√≠a significativamente menor en comparaci√≥n con la CPU y la GPU (esto es especialmente importante para las soluciones m√≥viles). <br></li></ul><br>  <b>Contras:</b> <br><br><ul><li>  Principalmente ayudan a acelerar la ejecuci√≥n; la capacitaci√≥n sobre ellos, a diferencia de la GPU, es notablemente menos conveniente. <br></li><li>  Programaci√≥n m√°s compleja en comparaci√≥n con las opciones anteriores. <br></li><li>  Notablemente menos especialistas. <br></li></ul><br><h2>  ASIC </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/dee/5e4/059/dee5e40597454e07651718c325f2ac3f.png"><br><br>  El siguiente es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ASIC</a> , que es la abreviatura de Circuito integrado espec√≠fico de la aplicaci√≥n, es decir  circuito integrado para nuestra tarea.  Por ejemplo, realizar una red neuronal colocada en hierro.  Sin embargo, la mayor√≠a de los nodos inform√°ticos pueden funcionar en paralelo.  De hecho, solo las dependencias de datos y la inform√°tica desigual en diferentes niveles de la red pueden evitar que usemos constantemente todas las ALU que funcionan. <br><br>  Quiz√°s la miner√≠a de criptomonedas ha hecho el anuncio ASIC m√°s grande entre el p√∫blico en general en los √∫ltimos a√±os.  Al principio, la miner√≠a en la CPU fue bastante rentable, luego tuve que comprar una GPU, luego FPGA y luego ASIC especializados, ya que la gente (l√©ase - el mercado) madur√≥ para pedidos en los que su producci√≥n se volvi√≥ rentable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de1/fda/062/de1fda062c797262a51047f57eac2f11.png"><br>  En nuestra √°rea, tambi√©n han aparecido servicios (¬°naturalmente!) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Que</a> ayudan a poner una red neuronal en hierro con las caracter√≠sticas necesarias para el consumo de energ√≠a, FPS y precio.  M√°gicamente, de acuerdo! <br><br>  PERO!  Estamos perdiendo la personalizaci√≥n de la red.  Y, por supuesto, la gente tambi√©n lo piensa.  Por ejemplo, aqu√≠ hay un art√≠culo con el dicho: " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øPuede una arquitectura reconfigurable vencer al ASIC como un acelerador CNN?</a> " ("¬øPuede una arquitectura configurable vencer al ASIC como un acelerador CNN?").  Hay suficiente trabajo sobre este tema, porque la pregunta no est√° inactiva.  La principal desventaja de ASIC es que despu√©s de que hemos conducido la red al hardware, nos resulta dif√≠cil cambiarla.  Son m√°s beneficiosos para los casos en que ya necesitamos una red que funcione bien con millones de chips con bajo consumo de energ√≠a y alto rendimiento.  Y esta situaci√≥n se est√° desarrollando gradualmente en el mercado de autom√≥viles de piloto autom√°tico, por ejemplo.  O en c√°maras de vigilancia.  O en las c√°maras de las aspiradoras robotizadas.  O en las c√°maras de un refrigerador casero.  O en una c√°mara de cafetera.  <s>O en la c√°mara de hierro.</s>  Bueno, entiendes la idea, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">resumen</a> . <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51a/b5e/949/51ab5e9497fee71cf9f23606d3de4fb6.png"></div><br><br>  Es importante que en la producci√≥n en masa el chip sea barato, funcione r√°pidamente y consuma un m√≠nimo de energ√≠a. <br><br>  <b>Pros:</b> <br><br><ul><li>  El costo de chip m√°s bajo en comparaci√≥n con todas las soluciones anteriores. <br></li><li>  El menor consumo de energ√≠a por unidad de operaci√≥n. <br></li><li>  Muy alta velocidad (incluido, si lo desea, un registro). <br></li></ul><br>  <b>Contras:</b> <br><br><ul><li>  Capacidad muy limitada para actualizar la red y la l√≥gica. <br></li><li>  El mayor costo de desarrollo en comparaci√≥n con todas las soluciones anteriores. <br></li><li>  Usar ASIC es rentable principalmente para grandes corridas. <br></li></ul><br><h2>  TPU </h2><br>  Recuerde que cuando se trabaja con redes, hay dos tareas: capacitaci√≥n y ejecuci√≥n (inferencia).  Si los FPGA / ASIC se centran principalmente en acelerar la ejecuci√≥n (incluida alguna red fija), entonces la TPU (Unidad de procesamiento de tensor o procesadores de tensor) es una aceleraci√≥n de aprendizaje basada en hardware o una aceleraci√≥n relativamente universal de una red arbitraria.  El nombre es hermoso, de acuerdo, aunque de hecho, los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tensores de</a> rango 2 con una unidad de multiplicaci√≥n mixta (MXU) conectada a la memoria de alto ancho de banda (HBM) todav√≠a se est√°n utilizando.  A continuaci√≥n se muestra el diagrama de arquitectura de la segunda y tercera versi√≥n de Google TPU: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f5e/29c/696/f5e29c696e265435fadbc2e56f67e12e.png"></div><br><h2>  TPU Google </h2><br>  En general, Google hizo un anuncio para el nombre de TPU, revelando desarrollos internos en 2017: <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4af/ea1/4ca/4afea14cad5dc0a7d941ae6b7963c171.png"></div><br>  Comenzaron a trabajar preliminarmente en procesadores especializados para redes neuronales con sus palabras en 2006, en 2013 crearon un proyecto con buena financiaci√≥n, y en 2015 comenzaron a trabajar con los primeros chips que ayudaron mucho con las redes neuronales para el servicio en la nube de Google Translate y m√°s.  Y esto fue, enfatizamos, la aceleraci√≥n de <i>la</i> red.  Una ventaja importante para los centros de datos es la eficiencia energ√©tica de TPU en dos √≥rdenes de magnitud m√°s alta en comparaci√≥n con las CPU (gr√°fico para TPU v1): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/877/a91/f61/877a91f61ce45f2d04c7c3fe9df55348.png"></div><br>  Adem√°s, como regla, en comparaci√≥n con la GPU, el <i>rendimiento de la</i> red es 10-30 veces mejor para mejor: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/10a/302/83b/10a30283b9c0fe8f0c7c1993f0bc68aa.png"></div><br>  La diferencia es incluso 10 veces significativa.  Est√° claro que la diferencia con la GPU en 20-30 veces determina el desarrollo de esta direcci√≥n. <br><br>  Y, afortunadamente, Google no est√° solo. <br><br><h2>  TPU Huawei </h2><br>  Hoy, el sufrido Huawei tambi√©n comenz√≥ a desarrollar TPU hace varios a√±os bajo el nombre de Huawei Ascend, y en dos versiones a la vez: para centros de datos (como Google) y para dispositivos m√≥viles (que Google tambi√©n comenz√≥ a hacer recientemente).  Si crees en los materiales de Huawei, superaron el nuevo Google TPU v3 de FP16 2.5 veces y NVIDIA V100 2 veces: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb8/692/66a/bb869266a2cfa5e28c2f086026309b01.png"><br><br>  Como de costumbre, una buena pregunta: c√≥mo se comportar√° este chip en tareas reales.  En el gr√°fico, como puede ver, el m√°ximo rendimiento.  Adem√°s, Google TPU v3 es bueno en muchos sentidos porque puede funcionar eficazmente en grupos de 1024 procesadores.  Huawei tambi√©n anunci√≥ cl√∫steres de servidores para el Ascend 910, pero no hay detalles.  En general, los ingenieros de Huawei han demostrado ser extremadamente competentes en los √∫ltimos 10 a√±os, y existe la posibilidad de que se use un rendimiento m√°ximo 2.8 veces mayor en comparaci√≥n con Google TPU v3, junto con la √∫ltima tecnolog√≠a de proceso de 7 nm. <br><br>  La memoria y el bus de datos son cr√≠ticos para el rendimiento, y la diapositiva muestra que se ha prestado considerable atenci√≥n a estos componentes (incluida la velocidad de comunicaci√≥n con la memoria mucho m√°s r√°pida que la de la GPU): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/98e/7bd/ab1/98e7bdab1c9e1a81e891bb72db3976c2.png"><br><br>  El chip tambi√©n usa un enfoque ligeramente diferente: no escalas MXU 128x128 bidimensionales, sino c√°lculos en un cubo tridimensional de un tama√±o m√°s peque√±o 16x16xN, donde N = {16,8,4,2,1}.  Por lo tanto, la pregunta clave es qu√© tan bien se ubicar√° en la aceleraci√≥n real de redes espec√≠ficas (por ejemplo, los c√°lculos en un cubo son convenientes para las im√°genes).  Adem√°s, un estudio cuidadoso de la diapositiva muestra que, a diferencia de Google, el chip incorpora de inmediato el trabajo con video FullHD comprimido.  Para el autor, ¬°esto suena <b>muy</b> alentador! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como se mencion√≥ anteriormente, en la misma l√≠nea, los procesadores se desarrollan para dispositivos m√≥viles para los que la eficiencia energ√©tica es cr√≠tica y en los que la red se ejecutar√° principalmente (es decir, por separado, procesadores para el aprendizaje en la nube y por separado, para la ejecuci√≥n): </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ea0/a3c/7cb/ea0a3c7cb72def7195afa2011ba02907.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y con este par√°metro, todo Se ve bien en comparaci√≥n con NVIDIA al menos (tenga en cuenta que no trajeron una comparaci√≥n con Google, sin embargo, Google no da a la mano TPU en la nube) Y sus chips m√≥viles competir√°n con los procesadores de Apple, Google y otras compa√±√≠as, pero es demasiado pronto para hacer un balance aqu√≠. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se ve claramente que los nuevos chips Nano, Tiny y Lite deber√≠an ser a√∫n mejores. Queda claro </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">por qu√© Trump estaba asustado</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Por qu√© muchos fabricantes est√°n examinando cuidadosamente los √©xitos de Huawei (que super√≥ a todas las empresas de hierro de EE. UU. en ingresos, incluido Intel en 2018). </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Redes an√°logas profundas </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como saben, la tecnolog√≠a a menudo se desarrolla en espiral, cuando los enfoques antiguos y olvidados se vuelven relevantes en una nueva ronda. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Algo similar podr√≠a suceder con las redes neuronales. Es posible que haya escuchado que una vez que las operaciones de multiplicaci√≥n y suma se llevaron a cabo mediante tubos de electrones y transistores (por ejemplo, la conversi√≥n de espacios de color, una multiplicaci√≥n t√≠pica de matrices, se realiz√≥ en todos los televisores en color hasta mediados de los 90). Surgi√≥ una buena pregunta: si nuestra red neuronal es relativamente resistente a los c√°lculos inexactos en el interior, ¬øqu√© pasa si convertimos estos c√°lculos a forma anal√≥gica? Inmediatamente obtenemos una notable aceleraci√≥n de los c√°lculos y una reducci√≥n potencialmente dram√°tica en el consumo de energ√≠a para una operaci√≥n:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/6b2/e08/aff/6b2e08afff1639668e02a548ed663fba.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Con este enfoque, DNN (Red Neural Profunda) se calcula r√°pidamente y con eficiencia energ√©tica. Pero hay un problema, estos son DAC / ADC (DAC / ADC), convertidores de digital a anal√≥gico y viceversa, que reducen tanto la eficiencia energ√©tica como la precisi√≥n del proceso. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, en 2017, IBM Research </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">propuso CMOS anal√≥gico</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para RPU ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unidades de procesamiento resistivas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), que le permiten almacenar datos procesados ‚Äã‚Äãtambi√©n en forma anal√≥gica y aumentar significativamente la eficiencia general del enfoque:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/0d3/720/3a8/0d37203a85a89791e8701098583e01ea.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adem√°s, adem√°s de la memoria anal√≥gica, puede ser de gran ayuda reducir la precisi√≥n de una red neuronal: esta es la clave para miniaturizar las RPU, lo que significa aumentar el n√∫mero de c√©lulas computacionales en un chip. </font><font style="vertical-align: inherit;">Y aqu√≠, IBM tambi√©n es un l√≠der, y en particular, recientemente este a√±o han mejorado con bastante √©xito la red a una precisi√≥n de 2 bits y van a llevar la precisi√≥n a un bit (y dos bits durante el entrenamiento), lo que potencialmente permitir√° 100 veces (!) Incrementar la productividad en comparaci√≥n con GPU modernas:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1c4/1eb/c09/1c41ebc09f5e53d206534eefbb105eac.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Es demasiado pronto para hablar de neurochips anal√≥gicos en detalle, porque si bien todo esto se est√° probando a nivel de los primeros prototipos: </font></font><br><br><div style="text-align:center;"><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/498/f6c/bdb/498f6cbdb53f20806cf41a78b7110e8b.png"></div><br>  Sin embargo, la direcci√≥n potencial de la computaci√≥n anal√≥gica parece <b>extremadamente</b> interesante. <br><br>  Lo √∫nico que confunde es que es IBM, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">que ya ha presentado docenas de patentes sobre el tema</a> .  Seg√∫n la experiencia, debido a las peculiaridades de la cultura corporativa, cooperan de manera relativamente d√©bil con otras compa√±√≠as y, al poseer algo de tecnolog√≠a, es m√°s probable que desaceleren su desarrollo entre otros que compartirlo de manera efectiva.  Por ejemplo, IBM alguna vez se neg√≥ a otorgar licencias de compresi√≥n aritm√©tica para JPEG al comit√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ISO</a> , a pesar de que el borrador del est√°ndar era una opci√≥n con compresi√≥n aritm√©tica.  Como resultado, JPEG cobr√≥ vida con la compresi√≥n Huffman y una picadura del 10-15% peor de lo que podr√≠a.  La misma situaci√≥n fue con los est√°ndares de compresi√≥n de video.  Y la industria cambi√≥ masivamente a la compresi√≥n aritm√©tica en c√≥decs solo cuando 5 patentes de IBM expiraron 12 a√±os despu√©s ... Esperemos que IBM est√© m√°s inclinado a cooperar esta vez y, en consecuencia, <b>deseamos el m√°ximo √©xito en el campo para todos los que no est√°n asociados con IBM</b> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hay muchas de esas personas y empresas</a> . <br><br>  Si funciona, <b>ser√° una revoluci√≥n en el uso de redes neuronales y una revoluci√≥n en muchas √°reas de la inform√°tica.</b> <br><br><h2>  Otras letras miscel√°neas </h2><br>  En general, el tema de la aceleraci√≥n de las redes neuronales se ha puesto de moda, todas las grandes empresas y docenas de nuevas empresas est√°n involucradas en √©l, y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">al menos 5 de ellas han atra√≠do m√°s de $ 100 millones en</a> inversiones a principios de 2018.  En total, en 2017, se invirtieron $ 1.5 MIL MILLONES en nuevas empresas relacionadas con el desarrollo de chips.  A pesar de que los inversores no se dieron cuenta de los fabricantes de chips durante 15 a√±os (porque no hab√≠a nada que atrapar en el contexto de los gigantes).  En general, ahora hay una posibilidad real de una peque√±a revoluci√≥n de hierro.  Adem√°s, es extremadamente dif√≠cil predecir qu√© arquitectura ganar√°, la necesidad de revoluci√≥n ha madurado y las posibilidades de aumentar la productividad son grandes.  La cl√°sica situaci√≥n revolucionaria ha madurado: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Moore</a> ya no puede y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dean</a> a√∫n no est√° listo. <br><br>  Bueno, dado que la ley de mercado m√°s importante, sea diferente, hay muchas cartas nuevas, por ejemplo: <br><br><ul><li>  <b>Unidad de procesamiento neuronal ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">NPU</a> )</b> : un neuroprocesador, a veces maravillosamente, un chip neurom√≥rfico, en t√©rminos generales, el nombre general de un acelerador de redes neuronales, que se llaman chips <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Samsung</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Huawei</a> y m√°s en la lista ... <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/d28/900/72e/d2890072e95d7e63f70543b09ba759bb.png"></div>  <i>De aqu√≠ en adelante en esta secci√≥n, se presentar√°n principalmente diapositivas de presentaciones corporativas como ejemplos de <b>nombres</b> de tecnolog√≠a.</i> <br><br>  Est√° claro que una comparaci√≥n directa es problem√°tica, pero aqu√≠ hay algunos datos interesantes que comparan chips con neuroprocesadores de Apple y Huawei, producidos por TSMC mencionados al principio.  Se puede ver que la competencia es dura, la nueva generaci√≥n muestra un aumento de la productividad de 2 a 8 veces y la complejidad de los procesos tecnol√≥gicos: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ee/ga/iu/eegaiu5u_rw5trv0-exwjngc7sw.png"></div><br></li><li>  <b>Procesador de red neuronal (NNP)</b> : procesador de red neuronal. <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/917/1bb/ad2/9171bbad26941f97399ce80a56373e51.png"></div><br>  Este es el nombre de su familia de chips, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Intel</a> (originalmente era la compa√±√≠a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Nervana Systems</a> , que Intel compr√≥ en 2016 por m√°s de $ 400 millones).  Sin embargo, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culos</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">libros, el</a> nombre NNP tambi√©n es bastante com√∫n. <br></li><li>  <b>Unidad de procesamiento de inteligencia (IPU)</b> - un procesador inteligente - el nombre de los chips promovidos por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Graphcore</a> (por cierto, que ya recibi√≥ una inversi√≥n de $ 310 millones). <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e93/18f/8e7/e9318f8e7711ea5b3a669946484c72bf.png"></div><br>  Produce tarjetas especiales para computadoras, pero orientadas al entrenamiento de redes neuronales, con un rendimiento de entrenamiento RNN 180‚Äì240 veces mayor que el del NVIDIA P100. <br></li><li>  <b>Unidad de procesamiento de flujo de datos (DPU)</b> - procesador de procesamiento de datos - el nombre es promovido por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">WAVE Computing</a> , que ya recibi√≥ una inversi√≥n de $ 203 millones.  Produce aproximadamente los mismos aceleradores que Graphcore: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f53/e5e/25a/f53e5e25abb2fc2fee636d1949242be8.png"></div><br>  Como recibieron 100 millones menos, declaran que el entrenamiento es solo 25 veces m√°s r√°pido que en la GPU (aunque prometen que ser√° 1000 veces pronto).  A ver ... <br></li><li>  <b>Unidad de procesamiento de visi√≥n ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VPU</a> )</b> - Procesador de visi√≥n por computadora: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/80e/756/110/80e7561100a794bb005635899b06ec40.png"></div><br>  El t√©rmino se usa en productos de varias compa√±√≠as, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Myriad X VPU</a> de Movidius (tambi√©n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">comprado por Intel</a> en 2016). <br></li><li>  Uno de los competidores de IBM (que, recordamos, usa el t√©rmino <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RPU</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mythic</a> , est√° moviendo <b>Analog DNN</b> , que tambi√©n almacena la red en el chip y una ejecuci√≥n relativamente r√°pida.  Hasta ahora solo tienen promesas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aunque serias</a> : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/01e/ae6/253/01eae6253a1d7ef3e2dec9401857a33a.png"></div><br></li></ul><br>  Y esto enumera solo las √°reas m√°s grandes en el desarrollo en las que se han invertido cientos de millones (esto es importante en el desarrollo del hierro). <br><br>  En general, como vemos, todas las flores florecen r√°pidamente.  Gradualmente, las empresas asimilar√°n miles de millones de d√≥lares en inversiones (por lo general, toma de 1,5 a 3 a√±os fabricar chips), el polvo se asentar√°, el l√≠der se aclarar√°, los ganadores, como de costumbre, escribir√°n una historia, y el nombre de la tecnolog√≠a m√°s exitosa en el mercado ser√° generalmente aceptado.  Esto ya ha sucedido m√°s de una vez ("IBM PC", "Smartphone", "Xerox", etc.). <br><br><h2>  Algunas palabras sobre la comparaci√≥n correcta </h2><br>  Como ya se se√±al√≥ anteriormente, comparar correctamente el rendimiento de las redes neuronales no es f√°cil.  Esto es exactamente por qu√© Google publica un gr√°fico en el que TPU v1 crea el NVIDIA V100.  NVIDIA, al ver tal desgracia, publica un cronograma donde Google TPU v1 pierde el V100.  (¬°Entonces!) Google publica el siguiente cuadro, donde el V100 pierde en Google TPU v2 y v3.  Y finalmente, Huawei es el horario en el que todos pierden en el Huawei Ascend, pero el V100 es mejor que el TPU v3.  Circo, en resumen.  Lo que es caracter√≠stico: ¬° <i>cada</i> gr√°fico <i>tiene su propia</i> verdad! <br><br>  Las causas profundas de la situaci√≥n son claras: <br><br><ul><li>  Puede medir la velocidad de aprendizaje o la velocidad de ejecuci√≥n (lo que sea m√°s conveniente). <br></li><li>  Es posible medir diferentes redes neuronales, porque la velocidad de ejecuci√≥n / entrenamiento de diferentes redes neuronales en arquitecturas espec√≠ficas puede diferir significativamente debido a la arquitectura de la red y la cantidad de datos requeridos. <br></li><li>  Y puede medir el rendimiento m√°ximo del acelerador (quiz√°s el m√°s abstracto de todos los anteriores). <br></li></ul><br>  Como un intento de poner las cosas en orden en este zool√≥gico, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">apareci√≥ la</a> prueba <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MLPerf</a> , que ahora tiene la versi√≥n 0.5 disponible, es decir  est√° en el proceso de desarrollar una metodolog√≠a de comparaci√≥n, que se planea llevar a la primera versi√≥n en el <a href="">tercer trimestre de este a√±o</a> : <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/d14/e35/575/d14e3557543c05b83b99f12dc9e572ce.png"></div><br>  Dado que los autores son uno de los principales contribuyentes a TensorFlow, existe la posibilidad de descubrir cu√°l es la mejor manera de entrenar y posiblemente usarlo (porque la versi√≥n m√≥vil de TF tambi√©n se incluir√° en esta prueba con el tiempo). <br><br>  Recientemente, la organizaci√≥n internacional <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">IEEE</a> , que publica la tercera parte de la literatura t√©cnica mundial sobre electr√≥nica de radio, computadoras e ingenier√≠a el√©ctrica, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">prohibi√≥ a Huawei</a> la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cara de un</a> ni√±o, y pronto, sin embargo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cancel√≥ la</a> prohibici√≥n.  Huawei todav√≠a no est√° en el ranking <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">actual de</a> MLPerf, mientras que Huawei TPU es un serio competidor de Google TPU y tarjetas NVIDIA (es decir, adem√°s de las pol√≠ticas, hay razones econ√≥micas para ignorar a Huawei, francamente).  ¬°Con un inter√©s no disimulado seguiremos el desarrollo de eventos! <br><br><h2>  Todo al cielo!  M√°s cerca de las nubes! </h2><br>  Y, dado que se trataba de capacitaci√≥n, vale la pena decir algunas palabras sobre sus detalles: <br><br><ul><li>  Con la salida generalizada de la investigaci√≥n en redes neuronales profundas (con docenas y cientos de capas que realmente desgarran a todos), fue necesario moler cientos de megabytes de coeficientes, lo que inmediatamente hizo ineficaces todos los cach√©s de procesador de generaciones anteriores.  Al mismo tiempo, el cl√°sico ImageNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">discute</a> una correlaci√≥n estricta entre el tama√±o de la red y su precisi√≥n (cuanto m√°s alta mejor, la derecha, mayor es la red, el eje horizontal es logar√≠tmico): <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5fa/788/84f/5fa78884f561b6f93dfa126be53376f4.png"></div><br></li><li>  El proceso de c√°lculo dentro de la red neuronal sigue un esquema fijo, es decir,  donde todas las "ramificaciones" y "transiciones" (en t√©rminos del siglo pasado) se llevar√°n a cabo en la gran mayor√≠a de los casos se conoce con precisi√≥n de antemano, lo que deja la ejecuci√≥n especulativa de instrucciones sin trabajo, lo que anteriormente aumenta significativamente la productividad: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ss/-6/9n/ss-69n-vr5c3rszuvmhkaomtct0.png"></div><br>  Esto hace que los mecanismos de predicci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">superescalares</a> acumulados para la ramificaci√≥n y los c√°lculos previos de las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">d√©cadas</a> anteriores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de</a> mejora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">del</a> procesador sean ineficaces (desafortunadamente, esta parte del chip tambi√©n contribuye al calentamiento global m√°s bien como DNN en el cach√© de DNN). <br></li><li>  Adem√°s, el entrenamiento de la red neuronal est√° relativamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">escalado horizontalmente</a> .  Es decir  no podemos tomar 1000 computadoras poderosas y obtener aceleraci√≥n de aprendizaje 1000 veces.  E incluso a 100 no podemos (al menos hasta que se resuelva el problema te√≥rico del deterioro de la calidad de la formaci√≥n en un gran tama√±o del lote).  En general, es bastante dif√≠cil para nosotros distribuir algo en varias computadoras, porque tan pronto como disminuye la velocidad de acceso a la memoria unificada en la que se encuentra la red, la velocidad de su aprendizaje disminuye catastr√≥ficamente.  Por lo tanto, si un investigador tiene acceso a 1000 computadoras potentes <s>de forma gratuita</s> , seguramente las tomar√° todas pronto, pero lo m√°s probable (si no hay infiniband + RDMA), habr√° muchas redes neuronales con diferentes hiperpar√°metros.  Es decir  el tiempo total de entrenamiento ser√° solo varias veces menor que con 1 computadora.  All√≠ es posible jugar con los tama√±os del lote y la educaci√≥n superior y otras nuevas tecnolog√≠as de moda, pero la conclusi√≥n principal es s√≠, con un aumento en el n√∫mero de computadoras, la eficiencia del trabajo y la probabilidad de lograr un resultado aumentar√°n, pero no linealmente.  Y hoy, el tiempo de un investigador de Data Science es costoso y, a menudo, si puede gastar muchos autom√≥viles (aunque no sea razonable), pero obtenga aceleraci√≥n; esto se hace (vea el ejemplo con 1, 2 y 4 V100 caros en las nubes justo debajo). <br></li></ul><br>  Exactamente estos puntos explican por qu√© tanta gente se ha apresurado hacia el desarrollo de hierro especializado para redes neuronales profundas.  ¬øY por qu√© obtuvieron sus miles de millones?  Realmente hay luz visible al final del t√∫nel y no solo Graphcore (que, recordemos, 240 veces el entrenamiento RNN se aceler√≥). <br><br>  Por ejemplo, los caballeros de IBM Research <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">son optimistas de</a> que el desarrollo de chips especiales que aumentar√°n la eficiencia inform√°tica en un orden de magnitud en 5 a√±os (y en 10 a√±os en 2 √≥rdenes de magnitud, alcanzando un aumento de 1000 veces en comparaci√≥n con 2016, en este gr√°fico, es cierto , en eficiencia por vatio, pero la potencia del n√∫cleo tambi√©n aumentar√°): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51c/81f/226/51c81f2267b99ba00231532e606cf7fd.png"></div><br>  Todo esto significa la aparici√≥n de piezas de hierro, cuya capacitaci√≥n ser√° relativamente r√°pida, pero que ser√° costosa, lo que naturalmente lleva a la idea de compartir el tiempo de uso de esta costosa pieza de hierro entre los investigadores.  Y esta idea hoy no menos nos lleva naturalmente a la computaci√≥n en la nube.  Y la transici√≥n del aprendizaje a las nubes lleva mucho tiempo en marcha. <br><br>  Tenga en cuenta que ahora la capacitaci√≥n de los mismos modelos puede diferir en el tiempo en un orden de magnitud de diferentes servicios en la nube.  Amazon lidera en el plomo, y Colab gratis de Google viene √∫ltimo.  Tenga en cuenta c√≥mo el resultado de la cantidad de V100 cambia entre los l√≠deres: un aumento en la cantidad de tarjetas en 4 veces (!) Aumenta la productividad en menos de un tercio (!!!) de azul a p√∫rpura, y Google tiene a√∫n menos: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/78d/892/d83/78d892d833c0858572548aee7fca2697.png"></div><br>  Parece que en los pr√≥ximos a√±os la diferencia crecer√° a dos √≥rdenes de magnitud.  Se√±or!  Cocinar dinero!  Devolveremos amigablemente inversiones multimillonarias a los inversores m√°s exitosos ... <br><br><h2>  En resumen </h2><br>  Intentemos resumir los puntos clave en la tableta: <br><div class="scrollable-table"><table><tbody><tr><td>  Tipo <br></td><td>  Lo que acelera <br></td><td>  Comentario <br></td></tr><tr><td>  CPU <br></td><td>  B√°sicamente haciendo <br></td><td>  Usualmente es el peor en velocidad y eficiencia energ√©tica, pero bastante adecuado para realizar redes neuronales de peque√±o tama√±o. <br></td></tr><tr><td>  GPU <br></td><td>  Ejecuci√≥n + <br>  entrenamiento <br></td><td>  La soluci√≥n m√°s universal, pero bastante cara, tanto en t√©rminos de costo de c√°lculos como de eficiencia energ√©tica. <br></td></tr><tr><td>  FPGA <br></td><td>  Cumplimiento <br></td><td>  Una soluci√≥n relativamente universal para la ejecuci√≥n de redes, en algunos casos puede acelerar dr√°sticamente la implementaci√≥n <br></td></tr><tr><td>  ASIC <br></td><td>  Cumplimiento <br></td><td>  La versi√≥n de la red m√°s barata, m√°s r√°pida y con mayor eficiencia energ√©tica, pero se necesitan tiradas grandes <br></td></tr><tr><td>  TPU <br></td><td>  Ejecuci√≥n + <br>  entrenamiento <br></td><td>  Las primeras versiones se usaron para acelerar la ejecuci√≥n, ahora se usan para acelerar r√°pidamente la ejecuci√≥n y el entrenamiento. <br></td></tr><tr><td>  IPU, DPU ... NNP <br></td><td>  Principalmente entrenando <br></td><td>  Muchas cartas de marketing que ser√°n olvidadas con seguridad en los pr√≥ximos a√±os.  La principal ventaja de este zool√≥gico es la verificaci√≥n de diferentes direcciones de aceleraci√≥n DNN <br></td></tr><tr><td>  DNN / RPU anal√≥gico <br></td><td>  Ejecuci√≥n + <br>  entrenamiento <br></td><td>  Los aceleradores potencialmente anal√≥gicos pueden revolucionar la velocidad y la eficiencia energ√©tica al realizar y entrenar redes neuronales <br></td></tr></tbody></table></div><br><h2>  Algunas palabras sobre la aceleraci√≥n de software </h2><br>  Para ser justos, mencionamos que hoy el gran tema es la aceleraci√≥n de software de la ejecuci√≥n y capacitaci√≥n de redes neuronales profundas.  La ejecuci√≥n puede acelerarse significativamente principalmente debido a la llamada cuantizaci√≥n de la red.  Quiz√°s esto se deba, en primer lugar, a que el rango de pesos utilizado no es tan grande y, a menudo, es posible aumentar los pesos de un valor de punto flotante de 4 bytes a un entero de 1 byte (y, recordando los √©xitos de IBM, a√∫n m√°s fuerte).  En segundo lugar, la red capacitada en su conjunto es bastante resistente al ruido computacional y la precisi√≥n de la transici√≥n a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">int8</a> cae ligeramente.  Al mismo tiempo, a pesar del hecho de que el n√∫mero de operaciones puede incluso aumentar (debido a la escala al calcular), el hecho de que la red se reduzca en tama√±o 4 veces y pueda considerarse operaciones vectoriales r√°pidas aumenta significativamente la velocidad general de ejecuci√≥n.  Esto es especialmente importante para las aplicaciones m√≥viles, pero tambi√©n funciona en las nubes (un ejemplo de ejecuci√≥n acelerada en las nubes de Amazon): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e52/bbd/2cc/e52bbd2cc36e24eff9a788c2d8371c83.png"></div><br>  Hay otras formas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">acelerar</a> algor√≠tmicamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la ejecuci√≥n de la red</a> e incluso m√°s formas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">acelerar el aprendizaje</a> .  Sin embargo, estos son grandes temas separados sobre los cuales no esta vez. <br><br><h2>  En lugar de una conclusi√≥n </h2><br>  En sus conferencias, el inversionista y autor <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tony Ceba</a> da un magn√≠fico ejemplo: en 2000, la supercomputadora No. 1 con una capacidad de 1 teraflops ocup√≥ 150 metros cuadrados, cost√≥ $ 46 millones y consumi√≥ 850 kW: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/2v/6y/xe/2v6yxeo59aewcqngaybonbftsae.png"></div><br>  15 a√±os despu√©s, la GPU NVIDIA con un rendimiento de 2.3 teraflops (2 veces m√°s) cab√≠a en una mano, costaba $ 59 (una mejora de aproximadamente un mill√≥n de veces) y consum√≠a 15 vatios (una mejora de 56 mil veces): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/gx/wz/qq/gxwzqqj1si3e3ho33nnypq_au3w.png"></div><br>  En marzo de este a√±o, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Google present√≥ TPU Pods</a> , que en realidad son supercomputadoras refrigeradas por l√≠quido basadas en TPU v3, cuya caracter√≠stica clave es que pueden trabajar juntas en sistemas de 1024 TPU.  Se ven bastante impresionantes: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/37c/849/2ec/37c8492ec7e380c5a26f8a420fc591d9.png"></div><br>  No se dan los datos exactos, pero se dice que el sistema es comparable a las supercomputadoras Top-5 del mundo.  TPU Pod puede aumentar dram√°ticamente la velocidad de aprendizaje de las redes neuronales.  Para aumentar la velocidad de interacci√≥n, los TPU est√°n conectados por l√≠neas de alta velocidad en una estructura toroidal: <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/0f0/e26/532/0f0e2653272b633b8af1b6103540e95c.gif"><br>  Parece que despu√©s de 15 a√±os, este neuroprocesador dos veces m√°s potente tambi√©n podr√° caber en su mano, como el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">procesador Skynet</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">debe</a> admitir que es algo similar): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/cf2/5da/db6/cf25dadb60a350332cfef6bd147ec91b.png"></div>  <i>Disparo de la versi√≥n de director de la pel√≠cula <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Terminator 2"</a></i> <br><br>  Dada la tasa actual de mejora de los aceleradores de hardware de las redes neuronales profundas y el ejemplo anterior, esto es completamente real.  Existe la posibilidad en pocos a√±os de adquirir un chip con un rendimiento como el Pod TPU de hoy. <br><br>  Por cierto, es curioso que en la pel√≠cula los fabricantes de chips (aparentemente, imaginando hacia d√≥nde podr√≠a conducir la red de autoformaci√≥n) desactivaron el reciclaje por defecto.  Caracter√≠sticamente, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">T-800 en</a> s√≠ no pod√≠a habilitar el modo de entrenamiento y funcionaba en modo de inferencia (ver la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versi√≥n</a> m√°s larga del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">director</a> ).  Adem√°s, su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">procesador de red neuronal</a> era avanzado y, al activar el reentrenamiento, pod√≠a usar los datos acumulados previamente para actualizar el modelo.  Nada mal para 1991. <br><br>  Este texto se inici√≥ en el caliente 13 millones de Shenzhen.  Me sent√© en uno de los 27,000 taxis el√©ctricos de la ciudad y mir√© las 4 pantallas de cristal l√≠quido del autom√≥vil con gran inter√©s.  Uno peque√±o, entre los dispositivos frente al conductor, dos, en el centro en el tablero y el √∫ltimo, transl√∫cido, en el espejo retrovisor, combinado con un DVR, una c√°mara de video vigilancia y un Android a bordo (a juzgar por la l√≠nea superior con el nivel de carga y la comunicaci√≥n con la red).  Mostraba los datos del conductor (de qui√©n quejarse, si es as√≠), un pron√≥stico del tiempo fresco y parec√≠a haber una conexi√≥n con la flota de taxis.  El conductor no sab√≠a ingl√©s y no logr√≥ preguntarle sobre sus impresiones sobre la m√°quina el√©ctrica.  Por lo tanto, presion√≥ perezosamente el pedal, moviendo ligeramente el autom√≥vil en un atasco.  Y mir√© la ventana con una mirada futurista con inter√©s: los chinos en sus chaquetas conduc√≠an desde el trabajo en scooters el√©ctricos y monowels ... y me preguntaba c√≥mo se ver√≠a todo en 15 a√±os ... <br><br>  En realidad, hoy en d√≠a, el espejo retrovisor, que utiliza los datos de la c√°mara del DVR y la <i>aceleraci√≥n de hardware de las redes neuronales</i> , es capaz de controlar el autom√≥vil en el tr√°fico y establecer la ruta.  Por la tarde, al menos).  Despu√©s de 15 a√±os, el sistema claramente no solo podr√° conducir un autom√≥vil, sino que tambi√©n estar√° encantado de proporcionarme las caracter√≠sticas de los veh√≠culos el√©ctricos chinos nuevos.  En ruso, naturalmente (como opci√≥n: ingl√©s, chino ... alban√©s, finalmente).  El conductor aqu√≠ es superfluo, mal entrenado, un enlace. <br><br>  Se√±or!  <b>EXTREMADAMENTE INTERESANTE ¬°</b> 15 a√±os nos esperan! <br><br>  Est√©n atentos! <br><br>  Vuelvo!  ))) <br><br><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/3e8/caf/2cd/3e8caf2cde12f7b9bce6cd64de106357.png"><br><br>  <b>UPD:</b> Los comentarios m√°s interesantes: <br>  Sobre cuantizaci√≥n y aceleraci√≥n de c√°lculos en FPGA <br><div class="spoiler">  <b class="spoiler_title">Comentarios @Mirn</b> <div class="spoiler_text"><br>  En FPGA, no solo se dispone de aritm√©tica de precisi√≥n arbitraria, sino tambi√©n de una gran capacidad para guardar y procesar datos de bits arbitrarios.  Por ejemplo, hay demasiados coeficientes en el molesto MobileNetV2 W y B y puede cuantificarlos sin mucha p√©rdida de precisi√≥n a solo 16 bits, o tendr√° que volver a entrenar.  Pero si observa el interior y recopila estad√≠sticas sobre canales y capas, puede ver que los 16 bits se usan solo a la entrada de los primeros coeficientes de 1000 W, el resto puntual tiene 8-11 bits, de los cuales solo 2-3 bits y signos m√°s significativos son realmente importantes, y estad√≠sticas sobre el uso de canales, de modo que hay muchos canales donde generalmente ceros, o valores peque√±os, o canales donde casi todos los valores son 8-11 bits, es decir  Es posible clavar al expositor en clavos en tiempo de compilaci√≥n y no almacenarlo, es decir  de hecho, es posible almacenar en la memoria ROM valores no de 16 bits sino de 4 bits, e incluso puede almacenar toda la red neuronal en FPGAs baratos sin mucha p√©rdida de precisi√≥n (menos del 1%), y tambi√©n procesarla a velocidades de hasta decenas de miles de FPS con latencia de modo que obtengamos una respuesta de red neuronal de inmediato ¬øC√≥mo termina la recepci√≥n de la trama? <br><br>  Sobre la cuantizaci√≥n: mi idea es que si en varias etapas de la computaci√≥n W los coeficientes para el canal No. 0 cambian solo de +50 a -50, entonces tiene sentido comprimir el bitness a 7, y si de -123 a +124 por ejemplo, entonces a 8 (incluido el signo )    FPGA      ,      7, 8         ROM        .                 ,      . <br><br>           (,  , ),  RTL            ,          ,      .        GCC  AVX256    bitperfect (    FPGA  )           FPS      (       W  B,         ). <br><br>            W  fc   , ..      -100  +100   +10000      255      9        ( ). <br><br>                  !  porque dephwise    . <br><br>        u-law       (  !                ). <br><br>  ,          ,   6,      ,       . <br><br>                          (         ).  ‚Äî   ,    FixedPoint  dot product  ‚Äî     Fractional part,         ‚Äî       ,    ,      fc             . </div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Acerca de la compilaci√≥n √≥ptima autom√°tica de modelos en GPU, FPGA, ASIC y otro hardware </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comentario de @BigPack</font></font></b> <div class="spoiler_text"><br> -        TVM   ( tvm.ai/about),      (   Keras)    .   ,      ‚Äî  ¬´¬ª-  (bare metal,    ISA, FPGA  .)       edge computing.       TVM   HLS  TVM    FPGA.  HLS     FPGA ¬´¬ª    ,  ( )      FPGA    ,    GPU/TPU . <br><br> PS  FPGA    transparent hardware (  ‚Äî open-source hardware),        ,          (    ¬´¬ª )    .     -.  , FPGA        ‚Äî         </div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sobre el anuncio de innovaciones en la arquitectura FPGA, el uso de Microsoft FPGA y la optimizaci√≥n autom√°tica de redes neuronales </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Grandes comentarios @ Brak0del</font></font></b> <div class="spoiler_text"><br>    FPGA,   2019       ,      .       ‚Äî   .   /   dsp-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Xilinx</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Achronix</a> ,       DDR. <br><br>   , ,    , FPGA    ASIC-.   FPGA     :        ,       ASIC     ,  FPGA     -    .  Es decir     -    .            , ASIC-, ,     .  ,        FPGA  ,   ASIC. <br><br> ,    CPU, FPGA           ,      ,        . <br> ,    GPU      ,   FPGA    ,     :  , - ,    GPU      ,       ,     , -   (     ,   ,  ,   ,  FPGA   ,   GPU     , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> ).  , FPGA       ,   ,   ,    ASIC-. <br><br>       Microsoft ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> Catapult v.2</a> ),       FPGA-.  ,        FPGA.      ()    . <br><br>       FPGA         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ristretto</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deephi</a> ,      ,  Deephi       FPGA.   ,     ,         ,  . <br>   FPGA              . </div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Acerca de la econom√≠a de desarrollo de FPGA versus ASIC </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comentario de @Mirn</font></font></b> <div class="spoiler_text"><br>  ,  FPGA   : <br>     ,            ASIC. <br><br>  : <br> <b>FPGA</b> <br>     (  ),     (   , ,   IP      30-50   5     ). <br>   ,     10       (    ),    5*(N+1) <br>   , ,     ‚Äî    10     ,          ,   120*N <br>        (    ,     ‚Äî   ) <br>   : (120+50+5)*N,  5   880  <br>          <br><br> <b>ASIC</b> <br>              (    2 ) <br>             <br>           (3-4 ) <br>  ASIC      ¬´ ¬ª       ‚Äî    :     ,     <br>  ,      (             ), ,     ‚Äî         ,       . <br>  :          ‚Äî     ,    ,        . <br><br>                    (     MiT ‚Äî   ,          ,           ,    ) <br><br>     ,      ,    10              3-5 ,      (  ‚Äî   ,     ,   ‚Äî      ,   ‚Äî       )          ,    :      . <br>     ! !    .  NEC  SONY (c      ,        10-15        ,    ) <br><br> : FPGA             ASIC. </div></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agradecimientos</font></font></b> <div class="spoiler_text">    : <br><br><ul><li>      . ..           , <br></li><li>  ,     ,    , <br></li><li>   ,      ,       , <br></li><li> , ,    ,  ,  ,  ,  ,  ,  ,  ,  ,            ,     ! <br></li></ul><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455353/">https://habr.com/ru/post/455353/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455341/index.html">Lo que se sabe sobre la certificaci√≥n ITIL 4</a></li>
<li><a href="../455343/index.html">Entrenamiento Cisco 200-125 CCNA v3.0. D√≠a 9. El mundo f√≠sico de los interruptores. Parte 2</a></li>
<li><a href="../455345/index.html">Precauci√≥n Doctor</a></li>
<li><a href="../455347/index.html">Interfaces funcionales ... en VBA</a></li>
<li><a href="../455351/index.html">VMware EMPOWER 2019: los principales anuncios y conclusiones de la conferencia</a></li>
<li><a href="../455355/index.html">Redes de TV por cable para los m√°s peque√±os. Parte 8: red troncal √≥ptica</a></li>
<li><a href="../455359/index.html">Swift funcional es f√°cil</a></li>
<li><a href="../455361/index.html">Creamos una extensi√≥n de navegador que verifica los resultados del examen.</a></li>
<li><a href="../455367/index.html">C√≥digo m√≠nimo VueJs + MVC funcionalidad m√°xima</a></li>
<li><a href="../455369/index.html">Certificaci√≥n de administradores de bases de datos y mucho m√°s en el aniversario DevConfX (21-22 de junio en Mosc√∫)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>