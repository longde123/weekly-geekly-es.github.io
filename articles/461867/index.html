<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö∂üèΩ üî• üë®üèæ‚Äçüè≠ C√≥mo reconocer im√°genes y textos en su tel√©fono usando ML Kit üëâüèæ ‚ôéÔ∏è üé∂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hace dos a√±os, Sundar Pichai, jefe de Google, dijo que la compa√±√≠a de mobile-first se convierte primero en AI y se enfoca en el aprendizaje autom√°tico...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo reconocer im√°genes y textos en su tel√©fono usando ML Kit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/461867/"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  Hace dos a√±os, Sundar Pichai, jefe de Google, dijo que la compa√±√≠a de mobile-first se convierte primero en AI y se enfoca en el aprendizaje autom√°tico.  Un a√±o despu√©s, se lanz√≥ el Kit de aprendizaje autom√°tico, un conjunto de herramientas con las que puedes usar ML de manera efectiva en iOS y Android. </p><br><p>  Se habla mucho sobre el Kit ML en los Estados Unidos, pero casi no hay informaci√≥n en ruso.  Y dado que lo usamos para algunas tareas en Yandex.Money, decid√≠ compartir mi experiencia y mostrar con ejemplos c√≥mo usarlo para hacer cosas interesantes. </p><br><p>  Mi nombre es Yura. El a√±o pasado estuve trabajando en el equipo Yandex.Money en una billetera m√≥vil.  Hablaremos sobre el aprendizaje autom√°tico en dispositivos m√≥viles. </p><a name="habracut"></a><br><hr><br><p>  Nota  Equipo editorial: esta publicaci√≥n es un recuento del informe de Yuri Chechetkin "Desde el m√≥vil primero hasta la IA primero" del Yandex.Money metap <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  ¬øQu√© es el Kit ML? </h2><br><p>  Este es el SDK m√≥vil de Google que facilita el uso del aprendizaje autom√°tico en dispositivos Android e iOS.  No es necesario ser un experto en ML o en inteligencia artificial, porque en unas pocas l√≠neas de c√≥digo puede implementar cosas muy complejas.  Adem√°s, no es necesario saber c√≥mo funcionan las redes neuronales o la optimizaci√≥n del modelo. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  ¬øQu√© puede hacer el Kit ML? </h2><br><p>  Las caracter√≠sticas b√°sicas son bastante amplias.  Por ejemplo, puede reconocer texto, caras, buscar y rastrear objetos, crear etiquetas para im√°genes y sus propios modelos de clasificaci√≥n, escanear c√≥digos de barras y etiquetas QR. </p><br><p>  Ya utilizamos el reconocimiento de c√≥digo QR en la aplicaci√≥n Yandex.Money. </p><br><p>  Tambi√©n hay un kit ML </p><br><ol><li>  Reconocimiento de hito; </li><li>  Definici√≥n del idioma en el que se escribe el texto; </li><li>  Traducci√≥n de textos en el dispositivo; </li><li>  Respuesta r√°pida a una carta o mensaje. </li></ol><br><p>  Adem√°s de una gran cantidad de m√©todos listos para usar, hay soporte para modelos personalizados, lo que pr√°cticamente ofrece infinitas posibilidades, por ejemplo, puede colorear fotograf√≠as en blanco y negro y hacerlas colorear. </p><br><p>  Es importante que no necesite utilizar ning√∫n servicio, API o back-end para esto.  Todo se puede hacer directamente en el dispositivo, por lo que no cargamos el tr√°fico de usuarios, no obtenemos un mont√≥n de errores relacionados con la red, no tenemos que procesar un mont√≥n de casos, por ejemplo, falta de Internet, p√©rdida de conexi√≥n, etc.  Adem√°s, en el dispositivo funciona mucho m√°s r√°pido que a trav√©s de una red. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="1"></p><br><h2 id="raspoznavanie-teksta">  Reconocimiento de texto </h2><br><p>  <strong>Tarea: dada una fotograf√≠a, debe hacer que el texto circule en un rect√°ngulo.</strong> </p><br><p>  Comenzamos con la dependencia en Gradle.  Es suficiente para conectar una dependencia, y estamos listos para trabajar. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  Vale la pena especificar metadatos que indiquen que el modelo se descargar√° al dispositivo mientras se descarga la aplicaci√≥n desde Play Market.  Si no hace esto y accede a la API sin un modelo, obtendremos un error y el modelo deber√° descargarse en segundo plano.  Si necesita usar varios modelos, es recomendable especificarlos separados por comas.  En nuestro ejemplo, usamos el modelo OCR, y el nombre del resto se puede encontrar en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  Despu√©s de la configuraci√≥n del proyecto, debe establecer los valores de entrada.  ML Kit funciona con el tipo FirebaseVisionImage, tenemos cinco m√©todos, cuya firma escrib√≠ a continuaci√≥n.  Convierten los tipos habituales de Android y Java en los tipos de ML Kit, con los que es conveniente trabajar. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Preste atenci√≥n a los dos √∫ltimos: funcionan con una matriz de bytes y con un b√∫fer de bytes, y necesitamos especificar metadatos para que ML Kit comprenda c√≥mo manejarlo todo.  Los metadatos, de hecho, describen el formato, en este caso, el ancho y la altura, el formato predeterminado, IMAGE_FORMAT_NV21 y la rotaci√≥n. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  Cuando se recopilan los datos de entrada, cree un detector que reconozca el texto. </p><br><p>  Hay dos tipos de detectores, en el dispositivo y en la nube, se crean literalmente en una l√≠nea.  Vale la pena se√±alar que el detector en el dispositivo solo funciona con ingl√©s.  El detector de nubes admite m√°s de 20 idiomas; deben especificarse en el m√©todo setLanguageHints especial. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  El n√∫mero de idiomas admitidos es m√°s de 20, todos est√°n en el sitio web oficial.  En nuestro ejemplo, solo ingl√©s y ruso. </p><br><p>  Despu√©s de tener una entrada y un detector, simplemente llame al m√©todo processImage en este detector.  Obtenemos el resultado en forma de una tarea, en la que colgamos dos devoluciones de llamada: por √©xito y por error.  La expresi√≥n est√°ndar llega a un error, y el tipo FirebaseVisionText llega al √©xito de onSuccessListener. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  ¬øC√≥mo trabajar con el tipo FirebaseVisionText? </h2><br><p>  Consiste en bloques de texto (TextBlock), los que a su vez consisten en l√≠neas (L√≠nea) y l√≠neas de elementos (Elemento).  Est√°n anidados el uno en el otro. </p><br><p>  Adem√°s, cada una de estas clases tiene cinco m√©todos que devuelven datos diferentes sobre el objeto.  Un rect√°ngulo es el √°rea donde se encuentra el texto, la confianza es la precisi√≥n del texto reconocido, los puntos de esquina son los puntos de esquina en el sentido de las agujas del reloj, comenzando desde la esquina superior izquierda, los idiomas reconocidos y el texto mismo. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  ¬øPara qu√© es esto? </h2><br><p>  Podemos reconocer tanto el texto completo de la imagen como sus p√°rrafos individuales, piezas, l√≠neas o solo palabras.  Y como ejemplo, podemos iterar, en cada etapa tomar un texto, tomar los bordes de este texto y dibujar.  Muy comodo </p><br><p>  Tenemos la intenci√≥n de utilizar esta herramienta en nuestra aplicaci√≥n para reconocer tarjetas bancarias, cuyas etiquetas se encuentran no est√°ndar.  No todas las bibliotecas de reconocimiento de tarjetas funcionan bien, y para tarjetas personalizadas, el Kit ML ser√≠a muy √∫til.  Como hay poco texto, es muy f√°cil procesarlo de esta manera. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Reconocimiento de objetos en la foto. </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  Usando la siguiente herramienta como ejemplo, me gustar√≠a mostrar que el principio de funcionamiento es aproximadamente el mismo.  En este caso, reconocimiento de lo que se representa en el objeto.  Tambi√©n creamos dos detectores, uno en el dispositivo y el otro en la nube, podemos especificar la precisi√≥n m√≠nima como par√°metros.  El valor predeterminado es 0.5, indicado 0.7, y listo para funcionar.  Tambi√©n obtenemos el resultado en forma de FirebaseImageLabel, esta es una lista de etiquetas, cada una de las cuales contiene una identificaci√≥n, descripci√≥n y precisi√≥n. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold ocultando la felicidad </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  Puede intentar comprender qu√© tan bien Harold oculta el dolor y si es feliz al mismo tiempo.  Utilizamos una herramienta de reconocimiento facial, que, adem√°s de reconocer los rasgos faciales, puede decir qu√© tan feliz es una persona.  Al final result√≥ que, Harold est√° 93% feliz.  O esconde muy bien el dolor. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="4 4"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  De f√°cil a f√°cil, pero un poco m√°s complicado.  Modelos a medida. </h2><br><p>  <strong>Tarea: clasificaci√≥n de lo que se representa en la foto.</strong> </p><br><p>  Tom√© una foto de la computadora port√°til y reconoc√≠ el m√≥dem, la computadora de escritorio y el teclado.  Suena como la verdad.  Hay mil clasificadores, y √©l toma tres de ellos que mejor describen esta foto. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7 7"></p><br><p>  Al trabajar con modelos personalizados, tambi√©n podemos trabajar con ellos tanto en el dispositivo como a trav√©s de la nube. </p><br><p>  Si trabajamos a trav√©s de la nube, debe ir a Firebase Console, a la pesta√±a ML Kit y tocar personalizado, donde podemos cargar nuestro modelo en TensorFlow Lite, porque ML Kit funciona con modelos con esta resoluci√≥n.  Si lo usamos en un dispositivo, simplemente podemos poner el modelo en cualquier parte del proyecto como un activo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6 6"></p><br><p>  Se√±alamos la dependencia del int√©rprete, que puede trabajar con modelos personalizados, y no nos olvidemos del permiso para trabajar con Internet. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  Para aquellos modelos que est√°n en el dispositivo, debe indicar en Gradle que el modelo no debe comprimirse, ya que puede distorsionarse. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  Cuando hemos configurado todo en nuestro entorno, debemos establecer condiciones especiales, que incluyen, por ejemplo, el uso de Wi-Fi, tambi√©n requieren carga y requieren que el dispositivo inactivo est√© disponible con Android N; estas condiciones indican que el tel√©fono se est√° cargando o est√° en modo de espera. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  Cuando creamos un modelo remoto, establecemos las condiciones de inicializaci√≥n y actualizaci√≥n, as√≠ como el indicador de si nuestro modelo debe actualizarse.  El nombre del modelo debe coincidir con el que especificamos en la consola de Firebase.  Cuando creamos el modelo remoto, debemos registrarlo en Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  Hacemos los mismos pasos para el modelo local, especificamos su nombre, la ruta al modelo y lo registramos en Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  Despu√©s de eso, debe crear esas opciones donde especificamos los nombres de nuestros modelos, instalamos el modelo remoto, instalamos el modelo local y creamos un int√©rprete con estas opciones.  Podemos especificar un modelo remoto o solo uno local, y el int√©rprete comprender√° con qui√©n trabajar. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  ML Kit no sabe nada sobre el formato de los datos de entrada y salida de los modelos personalizados, por lo que debe especificarlos. </p><br><p>  Los datos de entrada son una matriz multidimensional, donde 1 es el n√∫mero de im√°genes, 224x224 es la resoluci√≥n y 3 es una imagen RGB de tres canales.  Bueno, el tipo de datos es bytes. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  Los valores de salida son 1000 clasificadores.  Establecemos el formato de los valores de entrada y salida en bytes con las matrices multidimensionales especificadas.  Adem√°s de bytes, float, long, int tambi√©n est√°n disponibles. </p><br><p>  Ahora establecemos los valores de entrada.  Tomamos Bitmap, lo comprimimos a 224 por 224, lo convertimos a ByteBuffer y creamos valores de entrada usando FirebaseModelInput usando un generador especial. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  Y ahora, cuando hay un int√©rprete, el formato de los valores de entrada y salida y los propios valores de entrada, podemos ejecutar la solicitud utilizando el m√©todo de ejecuci√≥n.  Transferimos todo lo anterior como par√°metros, y como resultado obtenemos FirebaseModelOutput, que contiene un gen√©rico del tipo que especificamos.  En este caso, se trataba de una matriz de bytes, despu√©s de lo cual podemos comenzar a procesar.  Este es exactamente el millar de clasificadores que solicitamos, y mostramos, por ejemplo, los 3 m√°s adecuados. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  Implementaci√≥n de un d√≠a </h2><br><p>  Todo es bastante f√°cil de implementar, y el reconocimiento de objetos con herramientas integradas se puede realizar en solo un d√≠a.  La herramienta est√° disponible en iOS y Android, adem√°s, puede usar el mismo modelo TensorFlow para ambas plataformas. </p><br><p>  Adem√°s, hay toneladas de m√©todos disponibles listos para usar que pueden cubrir muchos casos.  La mayor√≠a de las API est√°n disponibles en el dispositivo, es decir, el reconocimiento funcionar√° incluso sin Internet. </p><br><p>  Y lo m√°s importante: soporte para modelos personalizados que se pueden usar como desee para cualquier tarea. </p><br><h2 id="poleznye-ssylki">  Enlaces utiles </h2><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n del kit ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Proyecto de demostraci√≥n del kit Github ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje autom√°tico para dispositivos m√≥viles con Firebase (Google I / O'19)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Machine Learning SDK para desarrolladores m√≥viles (Google I / O'18)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Creaci√≥n de un esc√°ner de tarjeta de cr√©dito con Firebase ML Kit (Medium.com)</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/461867/">https://habr.com/ru/post/461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461851/index.html">Blockchain y Electricidad</a></li>
<li><a href="../461855/index.html">Salarios en TI en el primer semestre de 2019: seg√∫n la calculadora de salarios My Circle</a></li>
<li><a href="../461859/index.html">No sabes nada de tecnolog√≠a de alimentos.</a></li>
<li><a href="../461861/index.html">Office 365 Cloud Security: Check Point CloudGuard SaaS Testing</a></li>
<li><a href="../461865/index.html">Video curso ‚ÄúIntroducci√≥n a la inversi√≥n desde cero utilizando IDA PRO. Capitulo 1</a></li>
<li><a href="../461871/index.html">101 consejos para convertirte en un buen programador (y humano)</a></li>
<li><a href="../461873/index.html">ViewPager 2: nueva funcionalidad en el contenedor antiguo</a></li>
<li><a href="../461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../461877/index.html">Java vs Kotlin para Android: opiniones de desarrolladores</a></li>
<li><a href="../461879/index.html">El libro "Linux en acci√≥n"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>