<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ö´Ô∏è üè® üë®üèø Multitraitement et rapprochement des donn√©es de diverses sources üíº üïµüèæ üõåüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 

 √âtant donn√© la vari√©t√© des syst√®mes distribu√©s, la disponibilit√© des informations v√©rifi√©es dans le stockage cible est un crit√®re im...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Multitraitement et rapprochement des donn√©es de diverses sources</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480076/"> Bonjour, Habr! <br><br>  √âtant donn√© la vari√©t√© des syst√®mes distribu√©s, la disponibilit√© des informations v√©rifi√©es dans le stockage cible est un crit√®re important pour la coh√©rence des donn√©es. <br><br>  Il existe de nombreuses approches et m√©thodes √† cet effet, et nous nous concentrerons sur la r√©conciliation, dont les aspects th√©oriques ont √©t√© abord√©s <a href="https://habr.com/ru/post/428443/">ici dans cet article.</a>  Je propose d'envisager la mise en ≈ìuvre pratique de ce syst√®me, √©volutif et adapt√© √† une grande quantit√© de donn√©es. <br><br>  Comment impl√©menter ce cas sur le bon vieux Python - lisez-le sous la coupe!  C'est parti! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/zx/hg/iczxhgu9zvlumwggetuoblxm1ra.jpeg"></div><br>  <a href="https://www.megapixl.com/alexdobysh-stock-images-videos-portfolio" rel="nofollow">(Source de l'image)</a> <br><a name="habracut"></a><br><h2>  Pr√©sentation </h2><br>  Imaginons qu'une institution financi√®re dispose de plusieurs syst√®mes distribu√©s et nous sommes confront√©s √† la t√¢che de v√©rifier les transactions dans ces syst√®mes et de t√©l√©charger les donn√©es rapproch√©es vers le stockage cible. <br><br>  En tant que source de donn√©es, prenez un grand fichier texte et une table dans une base de donn√©es PostgreSQL.  Supposons que les donn√©es de ces sources aient les m√™mes transactions, mais qu'elles peuvent avoir des diff√©rences, et qu'elles doivent donc √™tre v√©rifi√©es et √©crites dans les donn√©es v√©rifi√©es dans le stockage final pour analyse. <br><br>  De plus, il est n√©cessaire de pr√©voir le lancement parall√®le de plusieurs rapprochements sur la m√™me base de donn√©es et d'adapter le syst√®me √† un volume important en utilisant le multiprocessing. <br><br>  Le module de <a href="https://docs.python.org/dev/library/multiprocessing.html" rel="nofollow">multitraitement</a> est id√©al pour parall√©liser les op√©rations en Python et, dans un sens, contourne certains d√©fauts GIL.  Nous utiliserons les capacit√©s de cette biblioth√®que ci-dessous. <br><br><h2>  Architecture du syst√®me en cours de d√©veloppement </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/je/dm/hu/jedmhumxsx9d-mxu-bbfbzqbulq.png"></div><br>  Composants utilis√©s: <br><br><ul><li>  <b>G√©n√©rateur de donn√©es al√©atoires</b> - un script Python qui g√©n√®re un fichier CSV et remplit sur sa base une table dans une base de donn√©es; </li><li>  <b>Sources de donn√©es</b> - fichier et table CSV dans la base de donn√©es PostgreSQL; </li><li>  <b>Adaptateurs</b> - dans ce cas, nous utilisons deux adaptateurs qui extrairont les donn√©es de leurs sources (CSV ou base de donn√©es) et entreront des informations dans la base de donn√©es interm√©diaire; </li><li>  <b>Bases de donn√©es</b> - au nombre de trois: des donn√©es brutes, une base de donn√©es interm√©diaire qui stocke les informations captur√©es par les adaptateurs et une base de donn√©es "propre" contenant des transactions rapproch√©es des deux sources. </li></ul><br><h2>  Formation initiale </h2><br>  En tant qu'outil de stockage de donn√©es, nous utiliserons la <a href="https://hub.docker.com/_/postgres" rel="nofollow">base de donn√©es PostgreSQL dans le conteneur Docker</a> et interagirons avec notre base de donn√©es via <a href="https://hub.docker.com/r/dpage/pgadmin4/" rel="nofollow">pgAdmin ex√©cut√© dans le conteneur</a> : <br><br><pre><code class="bash hljs">docker run --name pg -d -e <span class="hljs-string"><span class="hljs-string">"POSTGRES_USER=my_user"</span></span> -e <span class="hljs-string"><span class="hljs-string">"POSTGRES_PASSWORD=my_password"</span></span> postgres</code> </pre> <br>  Ex√©cution de pgAdmin: <br><br><pre> <code class="bash hljs">docker run -p 80:80 -e <span class="hljs-string"><span class="hljs-string">"PGADMIN_DEFAULT_EMAIL=user@domain.com"</span></span> -e <span class="hljs-string"><span class="hljs-string">"PGADMIN_DEFAULT_PASSWORD=12345"</span></span> -d dpage/pgadmin4</code> </pre> <br>  Une fois que tout a commenc√©, n'oubliez pas de sp√©cifier dans le fichier de configuration (conf / db.ini) la cha√Æne de connexion √† la base de donn√©es (pour un exemple de formation, vous pouvez!): <br><br><pre> <code class="bash hljs">[POSTGRESQL] db_url=postgresql://my_user:my_password@172.17.0.2:5432/my_user</code> </pre><br>  En principe, l'utilisation d'un conteneur est facultative et vous pouvez utiliser votre serveur de base de donn√©es. <br><br><h2>  G√©n√©ration d'entr√©e </h2><br>  Le script Python <b>generate_test_data</b> est responsable de la g√©n√©ration des donn√©es de test, qui prend le nombre d'entr√©es √† g√©n√©rer.  La s√©quence des op√©rations peut √™tre facilement trac√©e par la fonction principale de la classe <b>GenerateTestData</b> : <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta"> @m.timing def run(self, num_rows): """ Run the process """ m.info('START!') self.create_db_schema() self.create_folder('data') self.create_csv_file(num_rows) self.bulk_copy_to_db() self.random_delete_rows() self.random_update_rows() m.info('END!')</span></span></code> </pre> <br>  Ainsi, la fonction effectue les √©tapes suivantes: <br><br><ul><li>  Cr√©ation de sch√©mas dans la base de donn√©es (nous cr√©ons tous les sch√©mas et tables principaux); </li><li>  Cr√©ation d'un dossier pour stocker un fichier de test; </li><li>  G√©n√©rer un fichier de test avec un nombre donn√© de lignes; </li><li>  Ins√©rer des donn√©es en bloc dans la table cible transaction_db_raw.transaction_log; </li><li>  Suppression accidentelle de plusieurs lignes dans ce tableau; </li><li>  Mise √† jour al√©atoire de plusieurs lignes de ce tableau. </li></ul><br>  La suppression et la modification sont n√©cessaires pour que les objets compar√©s pr√©sentent au moins une certaine diff√©rence.  Il est important de pouvoir rechercher ces √©carts! <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing @m.wrapper(m.entering, m.exiting) def random_delete_rows(self): """ Random deleting some rows from the table """ sql_command = sql.SQL(""" delete from {0}.{1} where ctid = any(array( select ctid from {0}.{1} tablesample bernoulli (1) ))""").format(sql.Identifier(self.schema_raw), sql.Identifier(self.raw_table_name)) try: rows = self.database.execute(sql_command) m.info('Has been deleted [%s rows] from table %s' % (rows, self.raw_table_name)) except psycopg2.Error as err: m.error('Oops! Delete random rows has been FAILED. Reason: %s' % err.pgerror) @m.timing @m.wrapper(m.entering, m.exiting) def random_update_rows(self): """ Random update some rows from the table """ sql_command = sql.SQL(""" update {0}.{1} set transaction_amount = round(random()::numeric, 2) where ctid = any(array( select ctid from {0}.{1} tablesample bernoulli (1) ))""").format(sql.Identifier(self.schema_raw), sql.Identifier(self.raw_table_name)) try: rows = self.database.execute(sql_command) m.info('Has been updated [%s rows] from table %s' % (rows, self.raw_table_name)) except psycopg2.Error as err: m.error('Oops! Delete random rows has been FAILED. Reason: %s' % err.pgerror)</span></span></code> </pre> <br>  La g√©n√©ration d'un ensemble de donn√©es de test et l'enregistrement ult√©rieur dans un fichier texte au format CSV est le suivant: <br><br><ul><li>  Un UID de transaction al√©atoire est cr√©√©; </li><li>  Un num√©ro de compte UID al√©atoire est cr√©√© (par d√©faut, nous prenons dix comptes uniques, mais cette valeur peut √™tre modifi√©e √† l'aide du fichier de configuration en modifiant le param√®tre "random_accounts"); </li><li>  Date de transaction - une date al√©atoire √† partir de la date sp√©cifi√©e dans le fichier de configuration (initial_date); </li><li>  Type de transaction (transaction / commission); </li><li>  Montant de la transaction; </li><li>  Le travail principal dans la g√©n√©ration de donn√©es est effectu√© par la m√©thode <i>generate_test_data_by_chunk</i> de la classe <b>TestDataCreator</b> : </li></ul><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing def generate_test_data_by_chunk(self, chunk_start, chunk_end): """ Generating and saving to the file """ num_rows_mp = chunk_end - chunk_start new_rows = [] for _ in range(num_rows_mp): transaction_uid = uuid.uuid4() account_uid = choice(self.list_acc) transaction_date = (self.get_random_date(self.date_in, 0) .__next__() .strftime('%Y-%m-%d %H:%M:%S')) type_deal = choice(self.list_type_deal) transaction_amount = randint(-1000, 1000) new_rows.append([transaction_uid, account_uid, transaction_date, type_deal, transaction_amount]) self.write_in_file(new_rows, chunk_start, chunk_end)</span></span></code> </pre> <br><blockquote>  Une caract√©ristique de cette fonction est le lancement de plusieurs processus asynchrones parall√©lis√©s, chacun g√©n√©rant sa propre portion de 50 000 enregistrements.  Cette "puce" vous permettra de cr√©er un fichier sur plusieurs millions de lignes assez rapidement </blockquote><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run_csv_writing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Writing the test data into csv file """</span></span> pool = mp.Pool(mp.cpu_count()) jobs = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> chunk_start, chunk_end <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.divide_into_chunks(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.num_rows): jobs.append(pool.apply_async(self.generate_test_data_by_chunk, (chunk_start, chunk_end))) <span class="hljs-comment"><span class="hljs-comment"># wait for all jobs to finish for job in jobs: job.get() # clean up pool.close() pool.join()</span></span></code> </pre> <br>  Une fois le fichier texte termin√©, la commande bulk_insert est trait√©e et toutes les donn√©es de ce fichier tombent dans la table <b>transaction_db_raw.transaction_log.</b> <br><br>  De plus, les deux sources contiendront exactement les m√™mes donn√©es et la r√©conciliation ne trouvera rien d'int√©ressant, nous supprimons et modifions donc plusieurs lignes al√©atoires dans la base de donn√©es. <br><br>  Ex√©cutez le script et g√©n√©rez un fichier CSV de test avec des transactions sur 10 000 lignes: <br><br><pre> <code class="bash hljs">./generate_test_data.py 10000</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4c/cp/hm/4ccphmc5dcjcgxuy54p_9limlz4.png"></div><br>  La capture d'√©cran montre qu'un fichier de 10 000 lignes a √©t√© re√ßu, 10 000 ont √©t√© charg√©s dans la base de donn√©es, mais 112 lignes ont √©t√© supprim√©es de la base de donn√©es et 108 autres ont √©t√© modifi√©es. R√©sultat: le fichier et la table de la base de donn√©es diff√®rent de 220 entr√©es. <br><br>  "Eh bien, o√π est le multitraitement?", Demandez-vous. <br>  Et son travail peut √™tre vu lorsque vous g√©n√©rez un fichier plus volumineux, non pas par 10 000 enregistrements, mais, par exemple, par 1 Mo.  Allons-nous essayer? <br><br><pre> <code class="bash hljs">./generate_test_data.py 1000000</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rw/_a/ne/rw_aneqnairixqk-wglpqxgjvkc.png"></div><br>  Apr√®s avoir charg√© les donn√©es, supprim√© et modifi√© les enregistrements al√©atoires, nous voyons les diff√©rences du fichier texte du tableau: 19 939 lignes (dont 10 022 ont √©t√© supprim√©es au hasard et 9 917 ont √©t√© modifi√©es). <br><br><blockquote>  L'image montre que la g√©n√©ration des enregistrements √©tait asynchrone, incoh√©rente.  Cela signifie que le processus suivant peut commencer sans prendre en compte l'ordre de d√©marrage d√®s que le pr√©c√©dent est termin√©.  Il n'y a aucune garantie que le r√©sultat sera dans le m√™me ordre que l'entr√©e. </blockquote><br><div class="spoiler">  <b class="spoiler_title">Est-ce vraiment plus rapide?</b> <div class="spoiler_text">  Un million de lignes ne se trouvant pas sur la machine virtuelle la plus rapide a √©t√© ¬´invent√©e¬ª en 15,5 secondes - et c'est une option valable.  Apr√®s avoir d√©marr√© la m√™me g√©n√©ration s√©quentiellement, sans utiliser le multi-traitement, j'ai obtenu le r√©sultat: la g√©n√©ration de fichiers √©tait plus de trois fois plus lente (plus de 52 secondes au lieu de 15,5): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sb/kb/ck/sbkbckylzluoyyslyflwak5udrq.png"></div><br></div></div><br><h2>  Adaptateur pour CSV </h2><br>  Cet adaptateur hache la ligne, ne laissant que la premi√®re colonne, l'ID de transaction, inchang√© et enregistre les donn√©es re√ßues dans le fichier <i>data / transaction_hashed.csv</i> .  La derni√®re √©tape de son travail consiste √† charger ce fichier √† l'aide de la commande COPY dans la table temporaire du sch√©ma <b>reconciliation_db.</b> <br><br>  La lecture optimale des fichiers est effectu√©e par plusieurs processus parall√®les.  Nous lisons ligne par ligne, en morceaux de 5 m√©gaoctets chacun.  Le chiffre "5 m√©gaoctets" a √©t√© obtenu par la m√©thode empirique.  C'est avec cette taille d'un morceau de texte que nous avons pu obtenir le plus petit temps pour lire de gros fichiers sur notre machine virtuelle.  Vous pouvez exp√©rimenter sur votre environnement avec ce param√®tre et voir comment la dur√©e de fonctionnement va changer: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing def process_wrapper(self, chunk_start, chunk_size): """ Read a particular chunk """ with open(self.file_name_raw, newline='\n') as file: file.seek(chunk_start) lines = file.read(chunk_size).splitlines() for line in lines: self.process(line) def chunkify(self, size=1024*1024*5): """ Return a new chunk """ with open(self.file_name_raw, 'rb') as file: chunk_end = file.tell() while True: chunk_start = chunk_end file.seek(size, 1) file.readline() chunk_end = file.tell() if chunk_end &gt; self.file_end: chunk_end = self.file_end yield chunk_start, chunk_end - chunk_start break else: yield chunk_start, chunk_end - chunk_start @m.timing def run_reading(self): """ The main method for the reading """ # init objects pool = mp.Pool(mp.cpu_count()) jobs = [] m.info('Run csv reading...') # create jobs for chunk_start, chunk_size in self.chunkify(): jobs.append(pool.apply_async(self.process_wrapper, (chunk_start, chunk_size))) # wait for all jobs to finish for job in jobs: job.get() # clean up pool.close() pool.join() m.info('CSV file reading has been completed')</span></span></code> </pre> <br>  Exemple de lecture d'un fichier cr√©√© pr√©c√©demment sur des enregistrements 1M: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9p/z1/zr/9pz1zrkzeelnep_r8oppk0sxhok.png"></div><br>  La capture d'√©cran montre la cr√©ation d'une table temporaire avec un nom unique pour le cycle de rapprochement en cours.  Vient ensuite la lecture asynchrone du fichier en plusieurs parties et le hachage de chaque ligne.  L'insertion de donn√©es de l'adaptateur dans la table cible termine le travail avec cet adaptateur. <br><blockquote>  L'utilisation d'une table temporaire avec un nom unique pour chaque processus de r√©conciliation vous permet en outre de parall√©liser le processus de r√©conciliation dans une base de donn√©es. </blockquote><br><h2>  Adaptateur pour PostgreSQL </h2><br>  L'adaptateur pour le traitement des donn√©es stock√©es dans la table fonctionne approximativement la m√™me logique que l'adaptateur pour le fichier: <br><br><ul><li>  lire certaines parties du tableau (s'il est volumineux, plus de 100 000 entr√©es) et prendre un hachage pour toutes les colonnes √† l'exception de l'identifiant de transaction; </li><li>  puis il y a une insertion des donn√©es trait√©es dans la table <b>reconciliation_db.</b>  <b>stockage _ $ (int (time.time ())</b> . </li></ul><br>  Une caract√©ristique int√©ressante de cet adaptateur est qu'il utilise un pool de connexions √† la base de donn√©es, qui recherchera par index les donn√©es n√©cessaires dans la table et les traitera. <br><br>  En fonction de la taille du tableau, le nombre de processus n√©cessaires au traitement est calcul√© et au sein de chaque processus, il y a une division en 10 t√¢ches. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Read the data from the postgres and shared those records with each processor to perform their operation using threads """</span></span> threads_array = self.get_threads(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.max_id_num_row, self.pid_max) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> pid <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(threads_array) + <span class="hljs-number"><span class="hljs-number">1</span></span>): m.info(<span class="hljs-string"><span class="hljs-string">'Process %s'</span></span> % pid) <span class="hljs-comment"><span class="hljs-comment"># Getting connection from the connection pool select_conn = self._select_conn_pool.getconn() select_conn.autocommit = 1 # Creating 10 process to perform the operation process = Process(target=self.process_data, args=(self.data_queque, pid, threads_array[pid-1][0], threads_array[pid-1][1], select_conn)) process.daemon = True process.start() process.join() select_conn.close()</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pw/kt/kk/pwktkkisxg3sud4dyss_gtyi4gy.png"></div><br><h2>  Rechercher les √©carts </h2><br>  Nous proc√©dons √† la v√©rification des donn√©es re√ßues de deux adaptateurs. <br><br>  La r√©conciliation (ou la r√©ception d'un rapport d'anomalie) se produit du c√¥t√© serveur de la base de donn√©es, en utilisant toute la puissance du langage SQL. <br><br>  La requ√™te SQL est assez simple - c'est juste une jointure de table avec les donn√©es des adaptateurs √† elle-m√™me par ID de transaction: <br><br><pre> <code class="python hljs">sql_command = sql.SQL(<span class="hljs-string"><span class="hljs-string">""" select s1.adapter_name, count(s1.transaction_uid) as tran_count from {0}.{1} s1 full join {0}.{1} s2 on s2.transaction_uid = s1.transaction_uid and s2.adapter_name != s1.adapter_name and s2.hash = s1.hash where s2.transaction_uid is null group by s1.adapter_name;"""</span></span>).format(sql.Identifier(self.schema_target), sql.Identifier(self.storage_table))</code> </pre><br>  La sortie est un rapport: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5c/ou/gy/5cougys1gkflsplq2hvkoleooto.png"></div><br>  V√©rifiez si tout est correct dans l'image ci-dessus.  Nous nous souvenons que 9917 ont √©t√© supprim√©s de la table dans la base de donn√©es et 10 022 lignes ont √©t√© modifi√©es.  Total 19939 lignes, ce qui est √©vident dans le rapport. <br><br><h2>  Tableau r√©capitulatif </h2><br>  Il ne reste plus qu'√† ins√©rer dans la table de stockage des transactions ¬´propres¬ª qui correspondent √† tous √©gards (par hachage) √† diff√©rents adaptateurs.  Ce processus est effectu√© par la requ√™te SQL suivante: <br><br><pre> <code class="python hljs">sql_command = sql.SQL(<span class="hljs-string"><span class="hljs-string">""" with reconcil_data as ( select s1.transaction_uid from {0}.{1} s1 join {0}.{1} s2 on s2.transaction_uid = s1.transaction_uid and s2.adapter_name != s1.adapter_name where s2.hash = s1.hash and s1.adapter_name = 'postresql_adapter' ) insert into {2}.transaction_log select t.transaction_uid, t.account_uid, t.transaction_date, t.type_deal, t.transaction_amount from {3}.transaction_log t join reconcil_data r on t.transaction_uid = r.transaction_uid where not exists ( select 1 from {2}.transaction_log tl where tl.transaction_uid = t.transaction_uid ) """</span></span>).format(sql.Identifier(self.schema_target), sql.Identifier(self.storage_table), sql.Identifier(self.schema_db_clean), sql.Identifier(self.schema_raw))</code> </pre><br>  La table temporaire que nous avons utilis√©e comme stockage interm√©diaire des donn√©es des adaptateurs peut √™tre supprim√©e. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uq/sr/te/uqsrte2g0thu2woasaxqojdbc88.png"></div><br><h2>  Conclusion </h2><br>  Au cours des travaux effectu√©s, un syst√®me de r√©conciliation des donn√©es provenant de diff√©rentes sources a √©t√© d√©velopp√©: un fichier texte et un tableau dans la base de donn√©es.  Utilis√© un minimum d'outils suppl√©mentaires. <br><br>  Un lecteur averti peut peut-√™tre remarquer que l'utilisation de frameworks tels qu'Apache Spark, associ√©e √† la conversion de donn√©es brutes au format parquet, peut acc√©l√©rer consid√©rablement ce processus, en particulier pour les gros volumes.  Mais l'objectif principal de ce travail est d'√©crire un syst√®me en Python nu et d'√©tudier le traitement de donn√©es multiprocessing.  Avec ce que nous avons, √† mon avis, trait√©. <br><br>  Le code source de l'ensemble du projet se trouve <a href="https://github.com/igorgorbenko/transact_reconciliation" rel="nofollow">dans mon r√©f√©rentiel sur GitHub</a> , je vous sugg√®re de vous familiariser avec lui. <br><br>  Je serai heureux de r√©pondre √† toutes les questions et de prendre connaissance de vos commentaires. <br><br>  Je vous souhaite du succ√®s! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr480076/">https://habr.com/ru/post/fr480076/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr480062/index.html">10 syst√®mes de contr√¥le. O√π est-il plus pratique de communiquer sur les t√¢ches et de partager des fichiers?</a></li>
<li><a href="../fr480064/index.html">Mots d'apprentissage regroup√©s par th√®me</a></li>
<li><a href="../fr480068/index.html">[Mise √† jour] Nos gens sont battus et nous nous tairons?</a></li>
<li><a href="../fr480070/index.html">Avantages React: une b√©n√©diction pour les entreprises?</a></li>
<li><a href="../fr480072/index.html">Kubernetes: pourquoi est-il si important de configurer la gestion des ressources syst√®me?</a></li>
<li><a href="../fr480078/index.html">Nouvelles biblioth√®ques frontales sur les p√©riph√©riques React</a></li>
<li><a href="../fr480080/index.html">De quoi avez-vous besoin pour prendre des notes?</a></li>
<li><a href="../fr480082/index.html">Utilisation du partitionnement dans MySQL pour Zabbix avec un grand nombre d'objets de surveillance</a></li>
<li><a href="../fr480086/index.html">Comment se conformer aux exigences de 152-FZ, prot√©ger les donn√©es personnelles de nos clients et ne pas marcher sur notre r√¢teau</a></li>
<li><a href="../fr480088/index.html">DevOps - OK, mais que faire? Comment r√©duire le travail manuel et obtenir le r√©sultat souhait√©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>