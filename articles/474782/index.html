<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚öôÔ∏è üö£ ‚õàÔ∏è Descripci√≥n general de la tecnolog√≠a de s√≠ntesis de voz üåí üëåüèΩ üçé</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos! Mi nombre es Vlad y trabajo como cient√≠fico de datos en el equipo de tecnolog√≠as del habla de Tinkoff que se utiliza en nuestro asistent...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Descripci√≥n general de la tecnolog√≠a de s√≠ntesis de voz</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/tinkoff/blog/474782/"><p>  Hola a todos!  Mi nombre es Vlad y trabajo como cient√≠fico de datos en el equipo de tecnolog√≠as del habla de Tinkoff que se utiliza en nuestro asistente de voz Oleg. </p><br><p>  En este art√≠culo, me gustar√≠a dar una breve descripci√≥n de las tecnolog√≠as de s√≠ntesis de voz utilizadas en la industria y compartir la experiencia de nuestro equipo en la construcci√≥n de nuestro propio motor de s√≠ntesis. </p><br><p><img src="https://habrastorage.org/webt/fc/3j/vs/fc3jvsr59z_90ojbvjushekmmm4.png" alt="imagen"></p><a name="habracut"></a><br><h3 id="sintez-rechi">  S√≠ntesis de voz </h3><br><p>  La s√≠ntesis de voz es la creaci√≥n de sonido basado en texto.  Este problema hoy se resuelve mediante dos enfoques: </p><br><ul><li>  Selecci√≥n de unidad [1], o un enfoque concatenativo.  Se basa en pegar fragmentos de audio grabado.  Desde finales de los 90, se ha considerado durante mucho tiempo el est√°ndar de facto para desarrollar motores de s√≠ntesis de voz.  Por ejemplo, una voz sonada por el m√©todo de selecci√≥n de unidad se puede encontrar en Siri [2]. </li><li>  S√≠ntesis de voz param√©trica [3], cuya esencia es construir un modelo probabil√≠stico que prediga las propiedades ac√∫sticas de una se√±al de audio para un texto dado. </li></ul><br><p>  El discurso de los modelos de selecci√≥n de unidades es de alta calidad, baja variabilidad y requiere una gran cantidad de datos para el entrenamiento.  Al mismo tiempo, para entrenar modelos param√©tricos, se necesita una cantidad mucho menor de datos, generan entonaciones m√°s diversas, pero hasta hace poco sufr√≠an una calidad de sonido general bastante pobre en comparaci√≥n con el enfoque de selecci√≥n de unidades. </p><br><p>  Sin embargo, con el desarrollo de tecnolog√≠as de aprendizaje profundo, los modelos de s√≠ntesis param√©trica han logrado un crecimiento significativo en todas las m√©tricas de calidad y pueden crear un habla que es pr√°cticamente indistinguible del habla humana. </p><br><h3 id="metriki-kachestva">  M√©tricas de calidad </h3><br><p> Antes de hablar sobre qu√© modelos de s√≠ntesis de voz son mejores, debe determinar las m√©tricas de calidad mediante las cuales se comparar√°n los algoritmos. </p><br><p>  Dado que el mismo texto puede leerse de infinitas maneras, a priori no existe la forma correcta de pronunciar una frase espec√≠fica.  Por lo tanto, a menudo las m√©tricas para la calidad de la s√≠ntesis del habla son subjetivas y dependen de la percepci√≥n del oyente. </p><br><p>  La m√©trica est√°ndar es el MOS (puntaje de opini√≥n promedio), una evaluaci√≥n promedio de la naturalidad del habla, dada por evaluadores para audio sintetizado en una escala de 1 a 5. Uno significa sonido completamente inveros√≠mil, y cinco significa habla que es indistinguible del humano.  Los registros de personas reales generalmente obtienen alrededor de 4.5, y un valor superior a 4 se considera bastante alto. </p><br><h3 id="kak-rabotaet-sintez-rechi">  C√≥mo funciona la s√≠ntesis del habla </h3><br><p>  El primer paso para construir cualquier sistema de s√≠ntesis de voz es recolectar datos para el entrenamiento.  Por lo general, estas son grabaciones de audio de alta calidad en las que el locutor lee frases especialmente seleccionadas.  El tama√±o aproximado del conjunto de datos requerido para los modelos de selecci√≥n de unidades de entrenamiento es de 10 a 20 horas de discurso puro [2], mientras que para los m√©todos param√©tricos de la red neuronal, el l√≠mite superior es de aproximadamente 25 horas [4, 5]. </p><br><p>  Discutimos ambas tecnolog√≠as de s√≠ntesis. </p><br><h3 id="unit-selection">  Selecci√≥n de unidad </h3><br><p><img src="https://habrastorage.org/webt/9-/r7/dm/9-r7dmw2tieg5ypyjbt-lddxddc.png" alt="imagen"></p><br><p>  Normalmente, el discurso grabado del hablante no puede cubrir todos los casos posibles en los que se utilizar√° la s√≠ntesis.  Por lo tanto, la esencia del m√©todo es dividir toda la base de audio en peque√±os fragmentos, llamados unidades, que luego se unen usando un procesamiento posterior m√≠nimo.  Por lo general, las unidades son unidades m√≠nimas de lenguaje ac√∫stico, como los medios tel√©fonos o los difones [2]. <br>  Todo el proceso de generaci√≥n consta de dos etapas: el frontend de PNL, que es responsable de extraer la representaci√≥n ling√º√≠stica del texto, y el backend, que calcula la funci√≥n de penalizaci√≥n de la unidad para las caracter√≠sticas ling√º√≠sticas dadas.  La interfaz de PNL incluye: </p><br><ol><li>  La tarea de normalizar el texto es la traducci√≥n de todos los caracteres que no son letras (n√∫meros, signos de porcentaje, monedas, etc.) en su representaci√≥n verbal.  Por ejemplo, "5%" debe convertirse a "cinco por ciento". </li><li>  Extraer caracter√≠sticas ling√º√≠sticas de un texto normalizado: representaci√≥n de fonemas, estr√©s, partes del discurso, etc. </li></ol><br><p>  Por lo general, la interfaz de PNL se implementa utilizando reglas prescritas manualmente para un idioma espec√≠fico, pero recientemente ha habido un sesgo creciente hacia el uso de modelos de aprendizaje autom√°tico [7]. </p><br><p>  La penalizaci√≥n estimada por el subsistema de fondo es la suma del costo objetivo, o la correspondencia de la representaci√≥n ac√∫stica de la unidad para un fonema particular, y el costo de concatenaci√≥n, es decir, la conveniencia de conectar dos unidades vecinas.  Para evaluar las funciones finas, se pueden usar las reglas o el modelo ac√∫stico ya entrenado de s√≠ntesis param√©trica [2].  La selecci√≥n de la secuencia de unidades m√°s √≥ptima desde el punto de vista de las penalizaciones definidas anteriormente se realiza utilizando el algoritmo de Viterbi [1]. </p><br><p>  Valores aproximados de los modelos de selecci√≥n de unidades MOS para el idioma ingl√©s: 3.7-4.1 [2, 4, 5]. </p><br><p>  Ventajas del enfoque de selecci√≥n de unidades: </p><br><ul><li>  El sonido natural. </li><li>  Generaci√≥n de alta velocidad. </li><li>  Tama√±o peque√±o de modelos: esto le permite utilizar la s√≠ntesis directamente en su dispositivo m√≥vil. </li></ul><br><p>  Desventajas </p><br><ul><li>  El discurso sintetizado es mon√≥tono, no contiene emociones. </li><li>  Artefactos de encolado caracter√≠sticos. </li><li>  Requiere una base de entrenamiento de datos de audio lo suficientemente grande como para cubrir todo tipo de contextos. </li><li>  En principio, no puede generar sonido que no se encuentra en el conjunto de entrenamiento. </li></ul><br><h3 id="parametricheskiy-sintez-rechi">  S√≠ntesis param√©trica del habla </h3><br><p>  El enfoque param√©trico se basa en la idea de construir un modelo probabil√≠stico que estima la distribuci√≥n de las caracter√≠sticas ac√∫sticas de un texto dado. <br>  El proceso de generaci√≥n de voz en s√≠ntesis param√©trica se puede dividir en cuatro etapas: </p><br><ol><li>  La interfaz de PNL es la misma etapa de preprocesamiento de datos que en el enfoque de selecci√≥n de unidades, cuyo resultado es una gran cantidad de caracter√≠sticas ling√º√≠sticas sensibles al contexto. </li><li>  Modelo de duraci√≥n que predice la duraci√≥n del fonema. </li><li>  Un modelo ac√∫stico que restaura la distribuci√≥n de las caracter√≠sticas ac√∫sticas sobre las ling√º√≠sticas.  Las caracter√≠sticas ac√∫sticas incluyen valores de frecuencia fundamentales, representaci√≥n espectral de la se√±al, etc. </li><li>  Un vocoder que traduce caracter√≠sticas ac√∫sticas en una onda de sonido. </li></ol><br><p>  Para la duraci√≥n del entrenamiento y los modelos ac√∫sticos, se pueden utilizar modelos ocultos de Markov [3], redes neuronales profundas o sus variedades recurrentes [6].  Un vocoder tradicional es un algoritmo basado en el modelo de filtro fuente [3], que supone que el habla es el resultado de aplicar un filtro de ruido lineal a la se√±al original. <br>  La calidad general del habla de los m√©todos param√©tricos cl√°sicos es bastante baja debido a la gran cantidad de suposiciones independientes sobre la estructura del proceso de generaci√≥n de sonido. </p><br><p>  Sin embargo, con el advenimiento de las tecnolog√≠as de aprendizaje profundo, ha sido posible entrenar modelos de extremo a extremo que predicen directamente los signos ac√∫sticos por letra.  Por ejemplo, las redes neuronales Tacotron [4] y Tacotron 2 [5] ingresan una secuencia de letras y devuelven el espectrograma de tiza utilizando el algoritmo seq2seq [8].  Por lo tanto, los pasos 1-3 del enfoque cl√°sico se reemplazan por una √∫nica red neuronal.  El siguiente diagrama muestra la arquitectura de la red Tacotron 2, que logra una calidad de sonido bastante alta. </p><br><p><img src="https://habrastorage.org/webt/tv/8l/pc/tv8lpchvxw75yr3msdbhjaqscwc.jpeg" alt="imagen"></p><br><p>  Otro factor de crecimiento significativo en la calidad del habla sintetizada fue el uso de vocoders de redes neuronales en lugar de algoritmos de procesamiento de se√±ales digitales. </p><br><p>  El primer vocoder de este tipo fue la red neuronal WaveNet [9], que secuencialmente, paso a paso, predijo la amplitud de la onda de sonido. </p><br><p>  Debido al uso de una gran cantidad de capas convolucionales con espacios para capturar m√°s contexto y omitir la conexi√≥n en la arquitectura de red, fue posible lograr una mejora de aproximadamente el 10% en MOS en comparaci√≥n con los modelos de selecci√≥n de unidades.  El siguiente diagrama muestra la arquitectura de la red WaveNet. </p><br><p><img src="https://habrastorage.org/webt/lg/ei/df/lgeidfeylr_yu-u-ucmdsxi7xki.png" alt="imagen"></p><br><p>  La principal desventaja de WaveNet es la baja velocidad asociada con un circuito de muestreo de se√±al en serie.  Este problema se puede resolver utilizando la optimizaci√≥n de ingenier√≠a para una arquitectura de hierro particular o reemplazando el esquema de muestreo por uno m√°s r√°pido. <br>  Ambos enfoques se han implementado con √©xito en la industria.  El primero est√° en Tinkoff.ru, y como parte del segundo enfoque, Google introdujo la red Parallel WaveNet [10] en 2017, cuyos logros se utilizan en el Asistente de Google. </p><br><p>  Valores aproximados de MOS para m√©todos de redes neuronales: 4.4‚Äì4.5 [5, 11], es decir, el habla sintetizada pr√°cticamente no es diferente del habla humana. </p><br><p>  Ventajas de la s√≠ntesis param√©trica: </p><br><ul><li>  Sonido natural y suave cuando se utiliza el enfoque de extremo a extremo. </li><li>  Mayor variedad en entonaci√≥n. </li><li>  Use menos datos que los modelos de selecci√≥n de unidades. </li></ul><br><p>  Desventajas </p><br><ul><li>  Baja velocidad en comparaci√≥n con la selecci√≥n de unidades. </li><li>  Gran complejidad computacional. </li></ul><br><h3 id="kak-rabotaet-sintez-rechi-v-tinkoff">  C√≥mo funciona la s√≠ntesis de voz de Tinkoff </h3><br><p>  Como se desprende de la revisi√≥n, los m√©todos de s√≠ntesis de voz param√©trica basados ‚Äã‚Äãen redes neuronales son actualmente de calidad significativamente superior al enfoque de selecci√≥n de unidades y son mucho m√°s simples de desarrollar.  Por lo tanto, para construir nuestro propio motor de s√≠ntesis, los usamos. <br>  Para los modelos de entrenamiento, se utilizaron aproximadamente 25 horas de discurso puro de un orador profesional.  La lectura de textos se seleccion√≥ especialmente para cubrir m√°s completamente la fon√©tica del habla coloquial.  Adem√°s, para agregar m√°s variedad a la s√≠ntesis en la entonaci√≥n, le pedimos al locutor que lea los textos con una expresi√≥n seg√∫n el contexto. </p><br><p>  La arquitectura de nuestra soluci√≥n conceptualmente se ve as√≠: </p><br><ul><li>  Interfaz de PNL, que incluye la normalizaci√≥n del texto de la red neuronal y un modelo para colocar pausas y tensiones. </li><li>  Tacotron 2 aceptando letras como entrada. </li><li>  WaveNet autorregresivo, trabajando en tiempo real en la CPU. </li></ul><br><p>  Gracias a esta arquitectura, nuestro motor genera voz expresiva de alta calidad en tiempo real, no requiere la construcci√≥n de un diccionario de fonemas y permite controlar el estr√©s en palabras individuales.  Se pueden escuchar ejemplos de audio sintetizado haciendo clic en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> . </p><br><h3 id="ssylki">  Referencias </h3><br><p>  [1] AJ Hunt, AW Black.  Selecci√≥n de unidades en un sistema de s√≠ntesis de voz concatenativo utilizando una gran base de datos de voz, ICASSP, 1996. <br>  [2] T. Capes, P. Coles, A. Conkie, L. Golipour, A. Hadjitarkhani, Q. Hu, N. Huddleston, M. Hunt, J. Li, M. Neeracher, K. Prahallad, T. Raitio , R. Rasipuram, G. Townsend, B. Williamson, D. Winarsky, Z. Wu, H. Zhang.  Sistema de texto a voz de selecci√≥n de unidad guiada por aprendizaje profundo en dispositivo Siri, Interspeech, 2017. <br>  [3] H. Zen, K. Tokuda, AW Black.  S√≠ntesis estad√≠stica param√©trica del habla, Speech Communication, vol.  51, no.  11, pp.  1039-1064, 2009. <br>  [4] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous .  Tacotron: Hacia la s√≠ntesis de voz de extremo a extremo. <br>  [5] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu.  S√≠ntesis natural de TTS por condicionamiento WaveNet en predicciones de espectrograma de Mel. <br>  [6] Heiga Zen, Andrew Senior, Mike Schuster.  S√≠ntesis estad√≠stica param√©trica del habla utilizando redes neuronales profundas. <br>  [7] Hao Zhang, Richard Sproat, Axel H. Ng, Felix Stahlberg, Xiaochang Peng, Kyle Gorman, Brian Roark.  Modelos neuronales de normalizaci√≥n de texto para aplicaciones de voz. <br>  [8] Ilya Sutskever, Oriol Vinyals, Quoc V. Le.  Secuencia a secuencia de aprendizaje con redes neuronales. <br>  [9] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu.  WaveNet: un modelo generativo para audio sin formato. <br>  [10] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman , Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, Demis Hassabis.  Parallel WaveNet: S√≠ntesis de voz r√°pida de alta fidelidad. <br>  [11] Wei Ping Kainan Peng Jitong Chen.  ClariNet: Generaci√≥n de ondas paralelas en texto a voz de extremo a extremo. <br>  [12] Dario Rethage, Jordi Pons, Xavier Serra.  Una red de ondas para el discurso de Denoising. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/474782/">https://habr.com/ru/post/474782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../474762/index.html">Seminario: Soluciones de TI h√≠bridas para empresas. 14 de noviembre, Mosc√∫</a></li>
<li><a href="../474768/index.html">Transmisi√≥n abierta del Main Hall HighLoad ++ 2019</a></li>
<li><a href="../474770/index.html">C√≥mo llevamos a cabo las pruebas de regresi√≥n de n√≥mina en SAP HCM</a></li>
<li><a href="../474772/index.html">Una startup que us√≥ IA para desarrollar una cura en 21 d√≠as</a></li>
<li><a href="../474776/index.html">Teor√≠a general y arqueolog√≠a de virtualizaci√≥n x86</a></li>
<li><a href="../474784/index.html">Arcade Stick Story</a></li>
<li><a href="../474788/index.html">Organizaci√≥n de rutas en Laravel</a></li>
<li><a href="../474790/index.html">Negociador Cuentos</a></li>
<li><a href="../474792/index.html">6-8 de diciembre - Rosbank Tech.Madness Hackathon</a></li>
<li><a href="../474796/index.html">¬øQu√© es el Internet de las cosas y c√≥mo ayudar√° a las empresas a ganar m√°s?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>