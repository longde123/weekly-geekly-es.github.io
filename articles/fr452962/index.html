<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👟 👨🏿‍🎨 🏴 La principale cause d'accidents dans les centres de données est la pose entre l'ordinateur et le fauteuil 🔑 👩🏾‍🤝‍👩🏻 🤸🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le sujet des accidents majeurs dans les datacenters modernes soulève des questions auxquelles le premier article n'a pas répondu - nous avons décidé d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La principale cause d'accidents dans les centres de données est la pose entre l'ordinateur et le fauteuil</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/delta/blog/452962/"><p>  <em>Le sujet des accidents majeurs dans les datacenters modernes soulève des questions auxquelles le premier article n'a pas répondu - nous avons décidé de le développer.</em> </p><br><img src="https://habrastorage.org/webt/dl/vb/qt/dlvbqt9aeha5coudsza9otjxu7m.jpeg"><a name="habracut"></a><br><br><p>  Selon les statistiques de l'Uptime Institute, la plupart des incidents dans les centres de données sont liés à des défaillances du système d'alimentation - ils représentent 39% des incidents.  Ils sont suivis par le facteur humain - c'est encore 24% des accidents.  La troisième raison (15%) la plus importante était la défaillance du système de climatisation, et la quatrième place (12%) était les catastrophes naturelles.  La part totale des autres problèmes n'est que de 10%.  Sans remettre en question les données d'une organisation réputée, nous mettons en évidence quelque chose de commun dans différents accidents et essayons de comprendre s'il était possible de les éviter.  Spoiler: possible dans la plupart des cas. </p><br><h2>  Contactez Science </h2><br><p>  En termes simples, il n'y a que deux problèmes avec l'alimentation: soit il n'y a pas de contact là où il devrait être, soit c'est là qu'il ne devrait pas y avoir de contact.  Vous pouvez parler longtemps de la fiabilité des systèmes modernes d'alimentation sans coupure, mais ils n'économisent pas toujours.  Prenons par exemple le cas sensationnel d'un centre de données utilisé par British Airways appartenant à la société mère International Airlines Group.  Il y a deux de ces installations près de l'aéroport d'Heathrow - Boadicea House et Comet House.  Dans le premier d'entre eux, le 27 mai 2017, une panne de courant accidentelle s'est produite, entraînant une surcharge et une panne du système UPS.  En conséquence, une partie de l'équipement informatique a été physiquement endommagée et il a fallu trois jours pour résoudre le dernier accident. </p><br><p> Les compagnies aériennes ont dû annuler ou reprogrammer plus d'un millier de vols, environ 75 000 passagers n'ont pas pu voler à temps - 128 millions de dollars ont été dépensés en compensation, sans compter les coûts des centres de données nécessaires pour restaurer la fonctionnalité.  L'histoire des raisons de la panne d'électricité est incompréhensible.  Si vous croyez aux résultats de l'enquête interne, exprimée par le directeur général du groupe International Airlines, Willie Walsh, cela est dû à une erreur des ingénieurs.  Néanmoins, le système d'alimentation sans coupure a dû résister à un tel arrêt - pour cela, il a été monté.  Le centre de données était géré par des spécialistes de la société d'externalisation CBRE Managed Services, de sorte que British Airways a tenté de récupérer le montant des dommages par un tribunal de Londres. </p><br><img src="https://habrastorage.org/webt/x1/8l/nd/x18lndo_idxsb_t91ppiiw6fxrs.jpeg"><br><p>  Les pannes de courant se produisent selon des scénarios similaires: d'abord, la panne est due à la faute du fournisseur d'électricité, parfois en raison de mauvaises conditions météorologiques ou de problèmes internes (y compris des erreurs de personnel), puis le système d'alimentation sans coupure ne peut pas faire face à la charge ou une courte interruption de la sinusoïde entraîne l'échec de nombreux services, rétablissement de la santé qui laisse la percée du temps et de l'argent.  Est-il possible d'éviter de tels accidents?  Bien sûr.  Si vous concevez le système correctement, cependant, même les créateurs de grands centres de données ne sont pas à l'abri des erreurs. </p><br><h2>  Facteur humain </h2><br><p>  Lorsque la cause directe d'un incident est les mauvaises actions du personnel du centre de données, les problèmes affectent le plus souvent (mais pas toujours) la partie logicielle de l'infrastructure informatique.  De tels accidents se produisent même dans les grandes entreprises.  En février 2017, en raison d'un membre mal tapé de l'équipe de maintenance technique de l'une des équipes du centre de données, certains serveurs Amazon Web Services ont été déconnectés.  Une erreur s'est produite lors du débogage du processus de facturation pour les clients cloud Amazon Simple Storage Service (S3).  L'employé a tenté de supprimer un certain nombre de serveurs virtuels utilisés par le système de facturation, mais a touché un cluster plus important. </p><br><img src="https://habrastorage.org/webt/pt/lw/1o/ptlw1ovbtuenmceearv7in_9pna.jpeg"><br><p>  En raison de l'erreur de l'ingénieur, les serveurs sur lesquels les modules logiciels de stockage cloud d'Amazon étaient exécutés ont été supprimés.  Tout d'abord, le sous-système d'indexation a été endommagé, contenant des informations sur les métadonnées et l'emplacement de tous les objets S3 dans la région américaine US-EAST-1.  L'incident a également affecté le sous-système utilisé pour stocker les données et gérer l'espace de stockage disponible.  Après la suppression des machines virtuelles, ces deux sous-systèmes ont nécessité un redémarrage complet, puis les ingénieurs d'Amazon ont été surpris par le fait que pendant longtemps le stockage dans le cloud public ne pouvait pas répondre aux demandes des clients. </p><br><p>  L'effet a été généralisé, car de nombreuses ressources importantes utilisent Amazon S3.  Des dysfonctionnements ont affecté Trello, Coursera, IFTTT et, ce qui est le plus désagréable, les services de grands partenaires amazoniens de la liste S&amp;P 500. Les dommages dans de tels cas ne sont pas faciles à compter, mais sa commande était de l'ordre de centaines de millions de dollars américains.  Comme vous pouvez le voir, pour désactiver le service de la plus grande plateforme cloud, une seule mauvaise équipe suffit.  Ce n'est pas un cas isolé, le 16 mai 2019, lors des travaux de maintenance, le service Yandex.Cloud a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">supprimé les</a> machines virtuelles des utilisateurs de la zone ru-central1-c qui étaient au moins une fois dans l'état SUSPENDU.  Ici, les données clients ont déjà été affectées, dont certaines ont été irrémédiablement perdues.  Bien sûr, les gens sont imparfaits, mais les systèmes de sécurité de l'information modernes sont depuis longtemps capables de contrôler les actions des utilisateurs privilégiés avant d'exécuter les commandes qu'ils entrent.  Si vous implémentez de telles solutions dans Yandex ou Amazon, de tels incidents peuvent être évités. </p><br><img src="https://habrastorage.org/webt/su/xt/jv/suxtjvznx-faduemuumrw1nnhhk.jpeg"><br><h2>  Refroidissement gelé </h2><br><p>  En janvier 2017, un accident majeur s'est produit dans le centre de données de Dmitrov à Megafon.  Ensuite, la température dans la région de Moscou est tombée à -35 ° C, ce qui a entraîné la défaillance du système de refroidissement de l'installation.  Le service de presse de l'opérateur n'a pas particulièrement évoqué les causes de l'incident - les entreprises russes sont extrêmement réticentes à parler des accidents survenus dans leurs locaux, en termes de publicité, nous sommes loin derrière l'Occident.  Dans les réseaux sociaux, il y avait une version sur le gel du liquide de refroidissement dans les tuyaux posés le long de la rue et la fuite d'éthylène glycol.  Si vous la croyez, le service d'exploitation n'a pas pu, en raison des longues vacances, recevoir rapidement 30 tonnes de liquide de refroidissement et en est sorti par des moyens improvisés, organisant un freecooling impromptu en violation des règles de fonctionnement du système.  Le froid sévère a aggravé le problème - en janvier, l'hiver est soudainement arrivé en Russie, bien que personne ne l'attendait.  En conséquence, le personnel a dû mettre hors tension une partie des racks de serveurs, raison pour laquelle certains services d'opérateur n'étaient pas disponibles pendant deux jours. </p><br><img src="https://habrastorage.org/webt/yj/qx/9r/yjqx9r6jawenp3wckp61odbcp8g.jpeg"><br><p>  Vous pouvez probablement parler ici de l'anomalie météorologique, mais de telles gelées ne sont pas inhabituelles pour la région de la capitale.  La température hivernale dans la région de Moscou peut chuter à des niveaux inférieurs, de sorte que les centres de données sont construits dans l'espoir d'un fonctionnement stable à −42 ° C.  Le plus souvent, les systèmes de refroidissement par temps froid échouent en raison d'une concentration insuffisamment élevée de glycols et d'un excès d'eau dans la solution de refroidissement.  Il y a des problèmes d'installation de tuyaux ou d'erreurs de calcul dans la conception et les tests du système, liés principalement au désir d'économiser.  En conséquence, un accident grave se produit à l'improviste, ce qui pourrait bien être évité. </p><br><h2>  Catastrophes naturelles </h2><br><p>  Le plus souvent, des orages et / ou des ouragans perturbent le travail de l'infrastructure d'ingénierie du centre de données, ce qui entraîne une interruption des services et / ou des dommages physiques aux équipements.  Les incidents causés par le mauvais temps se produisent assez souvent.  En 2012, l'ouragan Sandy a balayé la côte ouest des États-Unis avec de fortes pluies.  Situé dans un immeuble de grande hauteur dans le Lower Manhattan, le centre de données Peer 1 a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perdu son alimentation électrique externe</a> après que l'eau salée ait inondé les sous-sols.  Les générateurs d'urgence de l'installation étaient situés au 18ème étage et leur approvisionnement en carburant était limité - les règles introduites à New York après les attentats du 11 septembre interdisent de stocker de grandes quantités de carburant dans les étages supérieurs. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/68154979" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><p>  La pompe à carburant est également tombée en panne, car le personnel a traîné manuellement pendant plusieurs jours le diesel des générateurs.  L'héroïsme de l'équipe a sauvé le centre de données d'un grave accident, mais était-ce si nécessaire?  Nous vivons sur une planète avec une atmosphère d'azote et d'oxygène et beaucoup d'eau.  Les orages et les ouragans sont monnaie courante (en particulier dans les zones côtières).  Les concepteurs devraient probablement prendre en compte les risques qui leur sont associés et construire un système d'alimentation électrique ininterrompu approprié.  Ou du moins, choisissez un endroit plus approprié pour le centre de données que le gratte-ciel de l'île. </p><br><h2>  Tout le reste </h2><br><p>  L'Uptime Institute distingue divers incidents dans cette catégorie, parmi lesquels il est difficile de choisir un incident typique.  Vol de câbles en cuivre s'écrasant sur le centre de données, les tours de transmission de puissance et les voitures des postes de transformation, incendies, excavatrices endommageant l'optique, rongeurs (rats, lapins et même wombats, qui appartiennent généralement à des marsupiaux), ainsi que des amateurs pour pratiquer le tir sur fils - le menu est vaste .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Des</a> coupures de courant peuvent même être <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">causées par une</a> plantation illégale de marijuana voleuse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d'</a> énergie.  Dans la plupart des cas, les auteurs de l'incident sont des personnes spécifiques, c'est-à-dire que nous avons à nouveau affaire au facteur humain lorsque le problème a un nom et un prénom.  Même si à première vue l'accident est associé à un dysfonctionnement technique ou à des catastrophes naturelles, il peut être évité si l'installation est correctement conçue et correctement exploitée.  Les seules exceptions sont les cas de dommages critiques à l'infrastructure du centre de données ou de destruction de bâtiments et de structures en raison de catastrophes naturelles.  Ce sont vraiment des circonstances de force majeure, et tous les autres problèmes sont causés par la pose entre l'ordinateur et le fauteuil - c'est peut-être la partie la moins fiable de tout système complexe. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr452962/">https://habr.com/ru/post/fr452962/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr452952/index.html">Journée portes ouvertes JetBrains à Saint-Pétersbourg</a></li>
<li><a href="../fr452954/index.html">Temps de construction, vitesse du réseau et routage: comment nous avons amélioré notre réseau maillé et un peu sur les réseaux de neurones</a></li>
<li><a href="../fr452956/index.html">Référence de consommation de CPU pour Istio et Linkerd</a></li>
<li><a href="../fr452958/index.html">JMAP - un protocole ouvert remplace IMAP lors de l'échange de courriels</a></li>
<li><a href="../fr452960/index.html">Vous répondrez de tout! Contrats axés sur le consommateur à travers les yeux du développeur</a></li>
<li><a href="../fr452964/index.html">Une explication abordable de l'hypothèse de Riemann</a></li>
<li><a href="../fr452966/index.html">Le mythe de la pleine conscience: une vision «neurocentrique» de la méditation</a></li>
<li><a href="../fr452968/index.html">Index dans PostgreSQL - 10 (Bloom)</a></li>
<li><a href="../fr452974/index.html">Programmation asynchrone (cours complet)</a></li>
<li><a href="../fr452978/index.html">ok.tech: Explication des données</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>