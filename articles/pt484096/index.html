<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§≠ üë±üèø üëêüèø A batalha dos dois Yakozun, ou Cassandra vs HBase. Experi√™ncia da equipe Sberbank üè≥Ô∏è üßùüèø üßó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Isso nem √© uma piada, parece que essa imagem em particular reflete com mais precis√£o a ess√™ncia desses bancos de dados e, no final, ficar√° claro o por...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>A batalha dos dois Yakozun, ou Cassandra vs HBase. Experi√™ncia da equipe Sberbank</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/484096/">  Isso nem √© uma piada, parece que essa imagem em particular reflete com mais precis√£o a ess√™ncia desses bancos de dados e, no final, ficar√° claro o porqu√™: <br><br><img src="https://habrastorage.org/webt/i2/lk/zo/i2lkzo9tq7zpeprcbtgm3-mufk4.png"><br><br>  De acordo com o DB-Engines Ranking, as duas bases de coluna NoSQL mais populares s√£o Cassandra (daqui em diante CS) e HBase (HB). <br><br><img src="https://habrastorage.org/webt/su/rd/39/surd39n7bmrbnxgpn0512tnxamm.png"><br><br>  Por vontade do destino, nossa equipe de gerenciamento de carregamento de dados no Sberbank trabalha h√° muito tempo com a HB.  Durante esse per√≠odo, estudamos bem seus pontos fortes e fracos e aprendemos a cozinh√°-lo.  No entanto, a presen√ßa de uma alternativa na forma de CS o tempo todo me fez atormentar-me com d√∫vidas: fizemos a escolha certa?  Al√©m disso, os resultados da <a href="https://www.datastax.com/products/compare/nosql-performance-benchmarks">compara√ß√£o</a> realizada pela DataStax disseram que o CS derrota facilmente o HB com quase uma pontua√ß√£o esmagadora.  Por outro lado, o DataStax √© uma pessoa interessada e voc√™ n√£o deve dar uma palavra aqui.  Al√©m disso, uma quantidade bastante pequena de informa√ß√µes sobre as condi√ß√µes de teste foi embara√ßosa, por isso decidimos descobrir por n√≥s mesmos quem √© o rei do BigData NoSql, e os resultados foram muito interessantes. <br><a name="habracut"></a><br>  No entanto, antes de passar para os resultados dos testes realizados, √© necess√°rio descrever os aspectos essenciais das configura√ß√µes do ambiente.  O fato √© que o CS pode ser usado no modo de toler√¢ncia √† perda de dados.  I.e.  √© quando apenas um servidor (n√≥) √© respons√°vel pelos dados de uma determinada chave e, se cair por algum motivo, o valor dessa chave ser√° perdido.  Para muitas tarefas, isso n√£o √© cr√≠tico, mas para o setor banc√°rio, essa √© a exce√ß√£o e n√£o a regra.  No nosso caso, √© importante ter v√°rias c√≥pias de dados para armazenamento confi√°vel. <br><br>  Portanto, apenas o modo CS de replica√ß√£o tripla foi considerado, ou seja,  a cria√ß√£o do caso foi realizada com os seguintes par√¢metros: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> KEYSPACE ks <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> <span class="hljs-keyword"><span class="hljs-keyword">REPLICATION</span></span> = {<span class="hljs-string"><span class="hljs-string">'class'</span></span> : <span class="hljs-string"><span class="hljs-string">'NetworkTopologyStrategy'</span></span>, <span class="hljs-string"><span class="hljs-string">'datacenter1'</span></span> : <span class="hljs-number"><span class="hljs-number">3</span></span>};</code> </pre> <br>  Al√©m disso, existem duas maneiras de garantir o n√≠vel de consist√™ncia necess√°rio.  Regra geral: <br>  NW + NR&gt; RF <br><br>  Isso significa que o n√∫mero de confirma√ß√µes dos n√≥s ao escrever (NW) mais o n√∫mero de confirma√ß√µes dos n√≥s ao ler (NR) deve ser maior que o fator de replica√ß√£o.  No nosso caso, RF = 3 e, portanto, as seguintes op√ß√µes s√£o adequadas: <br>  2 + 2&gt; 3 <br>  3 + 1&gt; 3 <br><br>  Como √© de fundamental import√¢ncia manter os dados o mais confi√°vel poss√≠vel, foi escolhido um esquema 3 + 1.  Al√©m disso, a HB trabalha em uma base semelhante, ou seja,  essa compara√ß√£o seria mais honesta. <br><br>  Deve-se notar que o DataStax fez o oposto em sua pesquisa; eles definiram RF = 1 para CS e HB (para o √∫ltimo, alterando as configura√ß√µes do HDFS).  Esse √© um aspecto realmente importante, porque o impacto no desempenho do CS neste caso √© enorme.  Por exemplo, a figura abaixo mostra o aumento no tempo necess√°rio para carregar dados no CS: <br><br><img src="https://habrastorage.org/webt/fw/az/r9/fwazr9muypegpgjpyaq4rnagg8u.png"><br><br>  Aqui vemos o seguinte: quanto mais threads concorrentes gravam dados, mais tempo leva.  Isso √© natural, mas √© importante que a degrada√ß√£o do desempenho para RF = 3 seja significativamente maior.  Em outras palavras, se escrevermos em 4 tabelas em cada um dos 5 fluxos (total de 20), ent√£o RF = 3 perde cerca de 2 vezes (150 segundos RF = 3 versus 75 para RF = 1).  Mas se aumentarmos a carga carregando dados em 8 tabelas em cada um dos 5 fluxos (total 40), perder RF = 3 j√° √© 2,7 vezes (375 segundos versus 138). <br><br>  Talvez em parte esse seja o segredo do sucesso do teste de estresse DataStax para CS, porque para a HB em nosso estande, a altera√ß√£o do fator de replica√ß√£o de 2 para 3 n√£o teve efeito.  I.e.  discos n√£o s√£o o gargalo da HB para nossa configura√ß√£o.  No entanto, existem muitas outras armadilhas, porque deve-se notar que nossa vers√£o do HB foi ligeiramente corrigida e obscurecida, os ambientes s√£o completamente diferentes etc.  Tamb√©m vale a pena notar que talvez eu simplesmente n√£o saiba como preparar o CS adequadamente e que haja maneiras mais eficazes de trabalhar com ele, e espero nos coment√°rios que descobriremos.  Mas as primeiras coisas primeiro. <br><br>  Todos os testes foram realizados em um cluster de ferro composto por 4 servidores, cada um em uma configura√ß√£o: <br><br>  <i>CPU: Xeon E5-2680 v4 a 2.40GHz 64 threads.</i> <i><br></i>  <i>Discos: 12 pe√ßas de disco r√≠gido SATA</i> <i><br></i>  <i>vers√£o java: 1.8.0_111</i> <i><br></i> <br><br>  Vers√£o CS: 3.11.5 <br><br><div class="spoiler">  <b class="spoiler_title">Par√¢metros cassandra.yml</b> <div class="spoiler_text">  num_tokens: 256 <br>  hinted_handoff_enabled: true <br>  hinted_handoff_throttle_in_kb: 1024 <br>  max_hints_delivery_threads: 2 <br>  hints_directory: / data10 / cassandra / hints <br>  hints_flush_period_in_ms: 10000 <br>  max_hints_file_size_in_mb: 128 <br>  batchlog_replay_throttle_in_kb: 1024 <br>  autenticador: AllowAllAuthenticator <br>  authorizer: AllowAllAuthorizer <br>  role_manager: CassandraRoleManager <br>  Pap√©is_validez_em_ms: 2000 <br>  Permissions_validity_in_ms: 2000 <br>  credentials_validity_in_ms: 2000 <br>  particionador: org.apache.cassandra.dht.Murmur3Partitioner <br>  data_file_directories: <br>  - / data1 / cassandra / data # cada diret√≥rio dataN √© uma unidade separada <br>  - / data2 / cassandra / data <br>  - / data3 / cassandra / data <br>  - / data4 / cassandra / data <br>  - / data5 / cassandra / data <br>  - / data6 / cassandra / data <br>  - / data7 / cassandra / data <br>  - / data8 / cassandra / data <br>  diret√≥rio do commitlog: / data9 / cassandra / commitlog <br>  cdc_enabled: false <br>  disk_failure_policy: stop <br>  commit_failure_policy: stop <br>  prepare_statements_cache_size_mb: <br>  thrift_prepared_statements_cache_size_mb: <br>  key_cache_size_in_mb: <br>  key_cache_save_period: 14400 <br>  row_cache_size_in_mb: 0 <br>  row_cache_save_period: 0 <br>  counter_cache_size_in_mb: <br>  counter_cache_save_period: 7200 <br>  diret√≥rio_de_caches salvos: / data10 / cassandra /caches_cart√µes <br>  commitlog_sync: periodic <br>  commitlog_sync_period_in_ms: 10000 <br>  commitlog_segment_size_in_mb: 32 <br>  seed_provider: <br>  - class_name: org.apache.cassandra.locator.SimpleSeedProvider <br>  par√¢metros: <br>  - sementes: "*, *" <br>  concurrent_reads: 256 # tentou 64 - nenhuma diferen√ßa notada <br>  concurrent_writes: 256 # tentou 64 - nenhuma diferen√ßa notada <br>  concurrent_counter_writes: 256 # tentou 64 - nenhuma diferen√ßa notada <br>  concurrent_materialized_view_writes: 32 <br>  memtable_heap_space_in_mb: 2048 # tentou 16 GB - foi mais lento <br>  memtable_allocation_type: heap_buffers <br>  index_summary_capacity_in_mb: <br>  index_summary_resize_interval_in_minutes: 60 <br>  trickle_fsync: false <br>  trickle_fsync_interval_in_kb: 10240 <br>  storage_port: 7000 <br>  ssl_storage_port: 7001 <br>  listen_address: * <br>  broadcast_address: * <br>  listen_on_broadcast_address: true <br>  internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator <br>  start_native_transport: true <br>  native_transport_port: 9042 <br>  start_rpc: true <br>  rpc_address: * <br>  rpc_port: 9160 <br>  rpc_keepalive: true <br>  rpc_server_type: sync <br>  thrift_framed_transport_size_in_mb: 15 <br>  incremental_backups: false <br>  snapshot_before_compaction: false <br>  auto_snapshot: true <br>  column_index_size_in_kb: 64 <br>  column_index_cache_size_in_kb: 2 <br>  concurrent_compactors: 4 <br>  compaction_throughput_mb_per_sec: 1600 <br>  sstable_preemptive_open_interval_in_mb: 50 <br>  read_request_timeout_in_ms: 100000 <br>  range_request_timeout_in_ms: 200000 <br>  write_request_timeout_in_ms: 40000 <br>  counter_write_request_timeout_in_ms: 100000 <br>  cas_contention_timeout_in_ms: 20000 <br>  truncate_request_timeout_in_ms: 60000 <br>  request_timeout_in_ms: 200000 <br>  slow_query_log_timeout_in_ms: 500 <br>  cross_node_timeout: false <br>  endpoint_snitch: GossipingPropertyFileSnitch <br>  dynamic_snitch_update_interval_in_ms: 100 <br>  dynamic_snitch_reset_interval_in_ms: 600000 <br>  dynamic_snitch_badness_threshold: 0.1 <br>  request_scheduler: org.apache.cassandra.scheduler.NoScheduler <br>  server_encryption_options: <br>  internode_encryption: nenhum <br>  client_encryption_options: <br>  enabled: false <br>  internode_compression: dc <br>  inter_dc_tcp_nodelay: false <br>  tracetype_query_ttl: 86400 <br>  tracetype_repair_ttl: 604800 <br>  enable_user_defined_functions: false <br>  enable_scripted_user_defined_functions: false <br>  windows_timer_interval: 1 <br>  transparent_data_encryption_options: <br>  enabled: false <br>  tombstone_warn_threshold: 1000 <br>  tombstone_failure_threshold: 100000 <br>  batch_size_warn_threshold_in_kb: 200 <br>  batch_size_fail_threshold_in_kb: 250 <br>  unlogged_batch_across_partitions_warn_threshold: 10 <br>  compaction_large_partition_warning_threshold_mb: 100 <br>  gc_warn_threshold_in_ms: 1000 <br>  back_pressure_enabled: false <br>  enable_materialized_views: true <br>  enable_sasi_indexes: true <br></div></div><br>  Configura√ß√µes do GC: <br><br><div class="spoiler">  <b class="spoiler_title">### Configura√ß√µes do CMS</b> <div class="spoiler_text">  -XX: + UseParNewGC <br>  -XX: + UseConcMarkSweepGC <br>  -XX: + CMSParallelRemarkEnabled <br>  -XX: SurvivorRatio = 8 <br>  -XX: MaxTenuringThreshold = 1 <br>  -XX: CMSInitiatingOccupancyFraction = 75 <br>  -XX: + UseCMSInitiatingOccupancyOnly <br>  -XX: CMSWaitDuration = 10000 <br>  -XX: + CMSParallelInitialMarkEnabled <br>  -XX: + CMSEdenChunksRecordAlways <br>  -XX: + CMSClassUnloadingEnabled <br><br></div></div><br>  A mem√≥ria jvm.options recebeu 16 Gb (ainda tentou 32 Gb, nenhuma diferen√ßa foi observada). <br><br>  A cria√ß√£o de tabelas foi realizada pelo comando: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> ks.t1 (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span> <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span> PRIMARY <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>, title <span class="hljs-built_in"><span class="hljs-built_in">text</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> compression = {<span class="hljs-string"><span class="hljs-string">'sstable_compression'</span></span>: <span class="hljs-string"><span class="hljs-string">'LZ4Compressor'</span></span>, <span class="hljs-string"><span class="hljs-string">'chunk_length_kb'</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span>};</code> </pre> <br>  Vers√£o do HB: 1.2.0-cdh5.14.2 (na classe org.apache.hadoop.hbase.regionserver.HRegion, exclu√≠mos MetricsRegion, que levou ao GC com mais de 1000 regi√µes no RegionServer) <br><br><div class="spoiler">  <b class="spoiler_title">Op√ß√µes HBase n√£o padr√£o</b> <div class="spoiler_text">  zookeeper.session.timeout: 120000 <br>  hbase.rpc.timeout: 2 minuto (s) <br>  hbase.client.scanner.timeout.period: 2 minuto (s) <br>  hbase.master.handler.count: 10 <br>  hbase.regionserver.lease.period, hbase.client.scanner.timeout.period: 2 minuto (s) <br>  hbase.regionserver.handler.count: 160 <br>  hbase.regionserver.metahandler.count: 30 <br>  hbase.regionserver.logroll.period: 4 hora (s) <br>  hbase.regionserver.maxlogs: 200 <br>  hbase.hregion.memstore.flush.size: 1 GiB <br>  hbase.hregion.memstore.block.multiplier: 6 <br>  hbase.hstore.compactionThreshold: 5 <br>  hbase.hstore.blockingStoreFiles: 200 <br>  hbase.hregion.majorcompaction: 1 dia (s) <br>  Snippet de configura√ß√£o avan√ßada do servi√ßo HBase (v√°lvula de seguran√ßa) para hbase-site.xml: <br>  hbase.regionserver.wal.codecorg.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec <br>  hbase.master.namespace.init.timeout3600000 <br>  hbase.regionserver.optionalcacheflushinterval18000000 <br>  hbase.regionserver.thread.compaction.large12 <br>  hbase.regionserver.wal.enablecompressiontrue <br>  hbase.hstore.compaction.max.size1073741824 <br>  hbase.server.compactchecker.interval.multiplier200 <br>  Op√ß√µes de configura√ß√£o do Java para o HBase RegionServer: <br>  -XX: + UseParNewGC -XX: + UseConcMarkSweepGC -XX: CMSInitiatingOccupancyFraction = 70 -XX: + CMSParallelRemarkEnabled -XX: ReservedCodeCacheSize = 256m <br>  hbase.snapshot.master.timeoutMillis: 2 minuto (s) <br>  hbase.snapshot.region.timeout: 2 minuto (s) <br>  hbase.snapshot.master.timeout.millis: 2 minuto (s) <br>  Tamanho m√°ximo do log do servidor REST HBase: 100 MiB <br>  Backups m√°ximos de arquivos de log do servidor HBase REST: 5 <br>  Tamanho m√°ximo do log do servidor HBase Thrift: 100 MiB <br>  Backups m√°ximos de arquivos de log do HBase Thrift Server: 5 <br>  Tamanho m√°ximo do registro mestre: 100 MiB <br>  Backups mestres do arquivo de log m√°ximo: 5 <br>  RegionServer Tamanho m√°ximo do log: 100 MiB <br>  Backups m√°ximos de arquivos de log do RegionServer: 5 <br>  Janela de detec√ß√£o do HBase Active Master: 4 minuto (s) <br>  dfs.client.hedged.read.threadpool.size: 40 <br>  dfs.client.hedged.read.threshold.millis: 10 milissegundos <br>  hbase.rest.threads.min: 8 <br>  hbase.rest.threads.max: 150 <br>  Descritores m√°ximos de arquivos de processo: 180.000 <br>  hbase.thrift.minWorkerThreads: 200 <br>  hbase.master.executor.openregion.threads: 30 <br>  hbase.master.executor.closeregion.threads: 30 <br>  hbase.master.executor.serverops.threads: 60 <br>  hbase.regionserver.thread.compaction.small: 6 <br>  hbase.ipc.server.read.threadpool.size: 20 <br>  T√≥picos de mudan√ßas de regi√£o: 6 <br>  Tamanho de heap Java do cliente em bytes: 1 GiB <br>  Grupo Padr√£o do Servidor REST HBase: 3 GiB <br>  Grupo padr√£o do servidor HBase Thrift: 3 GiB <br>  Tamanho de heap Java do mestre HBase em bytes: 16 GiB <br>  Tamanho de heap Java da regi√£o HBaseServer em bytes: 32 GiB <br><br>  + ZooKeeper <br>  maxClientCnxns: 601 <br>  maxSessionTimeout: 120000 </div></div><br>  Criando tabelas: <br>  <i>hbase org.apache.hadoop.hbase.util.RegionSplitter ns: t1 UniformSplit -c 64 -f cf</i> <i><br></i>  <i>alterar 'ns: t1', {NOME =&gt; 'cf', DATA_BLOCK_ENCODING =&gt; 'FAST_DIFF', COMPRESS√ÉO =&gt; 'GZ'}</i> <br><br>  H√° um ponto importante - a descri√ß√£o do DataStax n√£o diz quantas regi√µes foram usadas para criar as tabelas HB, embora isso seja cr√≠tico para grandes volumes.  Portanto, para os testes, foi escolhido o n√∫mero = 64, o que permite armazenar at√© 640 GB, ou seja,  tabela de tamanho m√©dio. <br><br>  No momento do teste, o HBase tinha 22 mil tabelas e 67 mil regi√µes (isso seria fatal para a vers√£o 1.2.0, se n√£o fosse o patch mencionado acima). <br><br>  Agora para o c√≥digo.  Como n√£o ficou claro quais configura√ß√µes s√£o mais vantajosas para um banco de dados espec√≠fico, os testes foram realizados em v√°rias combina√ß√µes.  I.e.  em alguns testes, a carga foi simultaneamente para 4 tabelas (todos os 4 n√≥s foram usados ‚Äã‚Äãpara conex√£o).  Em outros testes, eles trabalharam com 8 tabelas diferentes.  Em alguns casos, o tamanho do lote era 100, em outros 200 (par√¢metro do lote - veja o c√≥digo abaixo).  O tamanho dos dados para o valor √© 10 bytes ou 100 bytes (dataSize).  No total, 5 milh√µes de registros foram gravados e subtra√≠dos a cada vez em cada tabela.  Ao mesmo tempo, 5 fluxos foram gravados / lidos em cada tabela (o n√∫mero do fluxo √© thNum), cada um dos quais usou seu pr√≥prio intervalo de chaves (contagem = 1 milh√£o): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"BEGIN BATCH "</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); sb.append(<span class="hljs-string"><span class="hljs-string">"INSERT INTO "</span></span>) .append(tableName) .append(<span class="hljs-string"><span class="hljs-string">"(id, title) "</span></span>) .append(<span class="hljs-string"><span class="hljs-string">"VALUES ("</span></span>) .append(key) .append(<span class="hljs-string"><span class="hljs-string">", '"</span></span>) .append(value) .append(<span class="hljs-string"><span class="hljs-string">"');"</span></span>); key++; } sb.append(<span class="hljs-string"><span class="hljs-string">"APPLY BATCH;"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); session.execute(query); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"SELECT * FROM "</span></span>).append(tableName).append(<span class="hljs-string"><span class="hljs-string">" WHERE id IN ("</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { sb = sb.append(key); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i+<span class="hljs-number"><span class="hljs-number">1</span></span> &lt; batch) sb.append(<span class="hljs-string"><span class="hljs-string">","</span></span>); key++; } sb = sb.append(<span class="hljs-string"><span class="hljs-string">");"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); ResultSet rs = session.execute(query); } }</code> </pre><br>  Consequentemente, uma funcionalidade semelhante foi fornecida para a HB: <br><br><pre> <code class="java hljs">Configuration conf = getConf(); HTable table = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HTable(conf, keyspace + <span class="hljs-string"><span class="hljs-string">":"</span></span> + tableName); table.setAutoFlush(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); List&lt;Get&gt; lGet = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); List&lt;Put&gt; lPut = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] cf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"cf"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] qf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lPut.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Put p = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Put(makeHbaseRowKey(key)); String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); p.addColumn(cf, qf, value.getBytes()); lPut.add(p); key++; } table.put(lPut); table.flushCommits(); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lGet.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Get g = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Get(makeHbaseRowKey(key)); lGet.add(g); key++; } Result[] rs = table.get(lGet); } }</code> </pre><br>  Como o cliente deve cuidar da distribui√ß√£o uniforme dos dados no HB, a fun√ß√£o de salga principal era assim: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] makeHbaseRowKey(<span class="hljs-keyword"><span class="hljs-keyword">long</span></span> key) { <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] nonSaltedRowKey = Bytes.toBytes(key); CRC32 crc32 = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CRC32(); crc32.update(nonSaltedRowKey); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> crc32Value = crc32.getValue(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] salt = Arrays.copyOfRange(Bytes.toBytes(crc32Value), <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ArrayUtils.addAll(salt, nonSaltedRowKey); }</code> </pre><br>  Agora, os mais interessantes s√£o os resultados: <br><br><img src="https://habrastorage.org/webt/id/yd/pc/idydpc9plsmulsycf0i-wqy3c3c.png"><br><br>  O mesmo que um gr√°fico: <br><br><img src="https://habrastorage.org/webt/72/ag/o1/72ago1u2gdnlufjanqwavk5p1jk.png"><br><br>  A vantagem do HB √© t√£o surpreendente que h√° suspeita de algum tipo de gargalo nas configura√ß√µes de CS.  No entanto, pesquisar e torcer os par√¢metros mais √≥bvios (como concurrent_writes ou memtable_heap_space_in_mb) n√£o deu acelera√ß√£o.  Ao mesmo tempo, os logs est√£o limpos, n√£o juram nada. <br><br>  Os dados s√£o distribu√≠dos igualmente entre os n√≥s, as estat√≠sticas de todos os n√≥s s√£o aproximadamente as mesmas. <br><br><div class="spoiler">  <b class="spoiler_title">Aqui est√£o as estat√≠sticas na tabela com um dos n√≥s</b> <div class="spoiler_text">  Espa√ßo na chave: ks <br>  Contagem de Leitura: 9383707 <br>  Lat√™ncia de leitura: 0.04287025042448576 ms <br>  N√∫mero de grava√ß√µes: 15462012 <br>  Lat√™ncia de grava√ß√£o: 0.1350068438699957 ms <br>  Flushes pendentes: 0 <br>  Tabela: t1 <br>  Contagem de tabelas: 16 <br>  Espa√ßo usado (ao vivo): 148.59 MiB <br>  Espa√ßo usado (total): 148.59 MiB <br>  Espa√ßo usado pelos instant√¢neos (total): 0 bytes <br>  Mem√≥ria de heap desativada usada (total): 5,17 MiB <br>  Taxa de compacta√ß√£o SSTable: 0.5720989576459437 <br>  N√∫mero de parti√ß√µes (estimativa): 3970323 <br>  Contagem de c√©lulas memor√°veis: 0 <br>  Tamanho memor√°vel dos dados: 0 bytes <br>  Memtable fora da mem√≥ria heap usada: 0 bytes <br>  Contagem de interruptores memor√°vel: 5 <br>  Contagem de leitura local: 2346045 <br>  Lat√™ncia de leitura local: NaN ms <br>  Contagem de grava√ß√£o local: 3865503 <br>  Lat√™ncia de grava√ß√£o local: NaN ms <br>  Libera√ß√µes pendentes: 0 <br>  Porcentagem reparada: 0.0 <br>  Filtro de falsos positivos positivos: 25 <br>  Rela√ß√£o falsa do filtro Bloom: 0,00000 <br>  Espa√ßo de filtro Bloom usado: 4.57 MiB <br>  Filtro Bloom fora da mem√≥ria heap usada: 4.57 MiB <br>  Resumo do √≠ndice da mem√≥ria heap usada: 590,02 KiB <br>  Metadados de compacta√ß√£o da mem√≥ria heap usada: 19,45 KiB <br>  Bytes m√≠nimos da parti√ß√£o compactada: 36 <br>  Bytes m√°ximos da parti√ß√£o compactada: 42 <br>  Bytes m√©dios da parti√ß√£o compactada: 42 <br>  M√©dia de c√©lulas vivas por fatia (√∫ltimos cinco minutos): NaN <br>  M√°ximo de c√©lulas vivas por fatia (√∫ltimos cinco minutos): 0 <br>  L√°pides m√©dias por fatia (√∫ltimos cinco minutos): NaN <br>  L√°pides m√°ximas por fatia (√∫ltimos cinco minutos): 0 <br>  Muta√ß√µes eliminadas: 0 bytes <br></div></div><br>  Uma tentativa de reduzir o tamanho do lote (at√© o envio de um por um) n√£o teve efeito, apenas piorou.  √â poss√≠vel que esse seja realmente o desempenho m√°ximo para o CS, pois os resultados obtidos no CS s√£o semelhantes aos obtidos no DataStax - cerca de centenas de milhares de opera√ß√µes por segundo.  Al√©m disso, se voc√™ observar a utiliza√ß√£o dos recursos, ver√° que o CS usa muito mais CPU e discos: <br><br><img src="https://habrastorage.org/webt/us/fo/i4/usfoi4-mgkktzlosilmz2ogm7uu.png"><br>  <i>A figura mostra a utiliza√ß√£o durante a execu√ß√£o de todos os testes seguidos nos dois bancos de dados.</i> <br><br>  Em rela√ß√£o aos poderosos benef√≠cios de leitura da HB.  Pode-se observar que, para ambos os bancos de dados, a utiliza√ß√£o do disco durante a leitura √© extremamente baixa (os testes de leitura s√£o a parte final do ciclo de testes de cada banco de dados, por exemplo, para CS das 15:20 √†s 15:40).  No caso da HB, o motivo √© claro - a maioria dos dados fica na mem√≥ria, no memstore e alguns foram armazenados em cache no blockcache.  Quanto ao CS, n√£o est√° muito claro como ele funciona, no entanto, a utiliza√ß√£o do disco tamb√©m n√£o √© vis√≠vel, mas, no caso, foi feita uma tentativa de ativar o cache row_cache_size_in_mb = 2048 e definir o cache = {'keys': 'ALL', 'lines_per_partition': ' 2.000.000 '}, mas isso piorou ainda mais. <br><br>  Tamb√©m vale mais uma vez dizer um ponto significativo sobre o n√∫mero de regi√µes no HB.  No nosso caso, foi indicado o valor 64. Se voc√™ reduzi-lo e torn√°-lo igual ao exemplo 4, ao ler a velocidade cai 2 vezes.  O motivo √© que o memstore ir√° entupir mais rapidamente e os arquivos ser√£o liberados com mais frequ√™ncia e, ao ler, ser√° necess√°rio processar mais arquivos, o que √© uma opera√ß√£o bastante complicada para o HB.  Em condi√ß√µes reais, isso pode ser tratado pensando sobre a estrat√©gia de pr√©-implanta√ß√£o e compacta√ß√£o, em particular, usamos um utilit√°rio criado por voc√™ que coleta lixo e comprime HFiles constantemente em segundo plano.  √â poss√≠vel que para os testes do DataStax, geralmente 1 regi√£o tenha sido alocada por tabela (o que n√£o est√° correto) e isso esclareceria um pouco por que a HB perdeu tanto em seus testes de leitura. <br><br>  As conclus√µes preliminares disso s√£o as seguintes.  Supondo que nenhum erro grave tenha sido cometido durante o teste, Cassandra √© como um colosso com p√©s de barro.  Mais precisamente, enquanto ela se equilibra sobre uma perna, como na foto no in√≠cio do artigo, ela mostra resultados relativamente bons, mas quando luta nas mesmas condi√ß√µes, perde totalmente.  Ao mesmo tempo, levando em considera√ß√£o a baixa utiliza√ß√£o da CPU em nosso hardware, aprendemos a plantar dois HBs RegionServer por host e, assim, dobramos a produtividade.  I.e.  levando em considera√ß√£o a utiliza√ß√£o de recursos, a situa√ß√£o do CS √© ainda mais deplor√°vel. <br><br>  Obviamente, esses testes s√£o bastante sint√©ticos e a quantidade de dados usada aqui √© relativamente modesta.  √â poss√≠vel que, ao mudar para terabytes, a situa√ß√£o seja diferente, mas se for poss√≠vel carregar terabytes no HB, ent√£o no CS isso se mostrou problem√°tico.  Muitas vezes, ele emitia uma OperationTimedOutException mesmo com esses volumes, embora os par√¢metros de expectativa de resposta j√° tivessem aumentado v√°rias vezes em compara√ß√£o com os padr√£o. <br><br>  Espero que, atrav√©s de esfor√ßos conjuntos, encontremos os gargalos do CS e, se conseguirmos aceler√°-lo, definitivamente adicionarei informa√ß√µes sobre os resultados finais no final do post. <br><br>  <b>UPD: As</b> seguintes diretrizes foram aplicadas na configura√ß√£o do CS: <br><br>  <i>disk_optimization_strategy: spinning</i> <i><br></i>  <i>MAX_HEAP_SIZE = "32G"</i> <i><br></i>  <i>HEAP_NEWSIZE = "3200M"</i> <i><br></i>  <i>-Xms32G</i> <i><br></i>  <i>-Xmx32G</i> <i><br></i>  <i>-XX: + UseG1GC</i> <i><br></i>  <i>-XX: G1RSetUpdatingPauseTimePercent = 5</i> <i><br></i>  <i>-XX: MaxGCPauseMillis = 500</i> <i><br></i>  <i>-XX: InitiatingHeapOccupancyPercent = 70</i> <i><br></i>  <i>-XX: ParallelGCThreads = 32</i> <i><br></i>  <i>-XX: ConcGCThreads = 8</i> <br><br>  Quanto √†s configura√ß√µes do sistema operacional, este √© um procedimento bastante longo e complicado (obten√ß√£o de raiz, reinicializa√ß√£o de servidores etc.), portanto, essas recomenda√ß√µes n√£o foram aplicadas.  Por outro lado, os dois bancos de dados est√£o em condi√ß√µes iguais, ent√£o tudo √© justo. <br><br>  Na parte do c√≥digo, √© criado um conector para todos os threads que est√£o gravando na tabela: <br><pre> <code class="java hljs">connector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CassandraConnector(); connector.connect(node, <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>, CL); session = connector.getSession(); session.getCluster().getConfiguration().getSocketOptions().setConnectTimeoutMillis(<span class="hljs-number"><span class="hljs-number">120000</span></span>); KeyspaceRepository sr = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> KeyspaceRepository(session); sr.useKeyspace(keyspace); prepared = session.prepare(<span class="hljs-string"><span class="hljs-string">"insert into "</span></span> + tableName + <span class="hljs-string"><span class="hljs-string">" (id, title) values (?, ?)"</span></span>);</code> </pre> <br><br>  Os dados foram enviados por meio de liga√ß√£o: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); session.execute(prepared.bind(key, value)); }</code> </pre> <br><br>  Isso n√£o teve um impacto significativo no desempenho da grava√ß√£o.  Para maior confiabilidade, lancei a carga com a ferramenta YCSB, absolutamente o mesmo resultado.  Abaixo est√£o as estat√≠sticas de um segmento (de 4): <br><br>  <i>2020-01-18 14: 41: 53: 180 315 seg: 10.000.000 opera√ß√µes;</i>  <i>21589.1 ops / s atuais;</i>  <i>[LIMPEZA: Contagem = 100, M√°x = 2236415, M√≠n = 1, M√©dia = 22356,39, 90 = 4, 99 = 24, 99,9 = 2236415, 99,99 = 2236415] [INSERIR: Contagem = 119551, M√°ximo = 174463, M√≠n = 273, M√©dia = 2582,71, 90 = 3491, 99 = 16767, 99,9 = 99711, 99,99 = 171263]</i> <i><br></i>  <i>[GERAL], Tempo de execu√ß√£o (ms), 315539</i> <i><br></i>  <i>[GERAL], Taxa de transfer√™ncia (ops / s), 31691.803548848162</i> <i><br></i>  <i>[TOTAL_GCS_PS_Scavenge], contagem, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_Scavenge], tempo (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_Scavenge], Tempo (%), 0,7710615803434757</i> <i><br></i>  <i>[TOTAL_GCS_PS_MarkSweep], contagem, 0</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_MarkSweep], tempo (ms), 0</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_MarkSweep], tempo (%), 0,0</i> <i><br></i>  <i>[TOTAL_GCs], contagem, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME], tempo (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME_%], tempo (%), 0,7710615803434757</i> <i><br></i>  <i>[INSERIR], Opera√ß√µes, 10.000.000</i> <i><br></i>  <i>[INSERT], AverageLatency (us), 3114.2427012</i> <i><br></i>  <i>[INSERIR], MinLatency (eua), 269</i> <i><br></i>  <i>[INSERIR], MaxLatency (us), 609279</i> <i><br></i>  <i>[INSERT], 95thPercentileLatency (us), 5007</i> <i><br></i>  <i>[INSERT], 99thPercentileLatency (us), 33439</i> <i><br></i>  <i>[INSERIR], Retorno = OK, 10000000</i> <i><br></i> <br><br>  Aqui voc√™ pode ver que a velocidade de um fluxo √© de cerca de 32 mil registros por segundo, 4 fluxos funcionados, resultando em 128 mil. Parece que n√£o h√° mais nada a se espremer nas configura√ß√µes atuais do subsistema de disco. <br><br>  Sobre a leitura mais interessante.  Gra√ßas ao conselho dos camaradas, ele conseguiu acelerar radicalmente.  A leitura foi realizada n√£o em 5 fluxos, mas em 100. Um aumento para 200 n√£o produziu efeito.  Tamb√©m inclu√≠do no construtor: <br>  .withLoadBalancingPolicy (novo TokenAwarePolicy (DCAwareRoundRobinPolicy.builder (). build ())) <br><br>  Como resultado, se anteriormente o teste mostrava 159 644 opera√ß√µes (5 fluxos, 4 tabelas, 100 lotes), agora: <br>  100 threads, 4 tabelas, lote = 1 (individualmente): 301 969 ops <br>  100 threads, 4 tabelas, lote = 10: 447608 ops <br>  100 threads, 4 tabelas, lote = 100: 625 655 ops <br><br>  Como os resultados s√£o melhores com lotes, executei testes * semelhantes com o HB: <br><img src="https://habrastorage.org/webt/ct/bk/-y/ctbk-yrecbwegasrbpauq6f1vv8.png"><br>  <i>* Como, ao trabalhar em 400 threads, a fun√ß√£o RandomStringUtils, usada anteriormente, carregava a CPU em 100%, foi substitu√≠da por um gerador mais r√°pido.</i> <br><br>  Assim, um aumento no n√∫mero de threads ao carregar dados fornece um pequeno aumento no desempenho da HB. <br><br>  Quanto √† leitura, aqui est√£o os resultados de v√°rias op√ß√µes.  A pedido de <a href="https://habr.com/ru/users/0x62ash/" class="user_link">0x62ash</a> , o comando flush foi executado antes da leitura e v√°rias outras op√ß√µes tamb√©m s√£o fornecidas para compara√ß√£o: <br>  Memstore - leitura da mem√≥ria, ou seja,  antes de liberar para o disco. <br>  HFile + zip - leitura de arquivos compactados pelo algoritmo GZ. <br>  HFile + upzip - leia arquivos sem compacta√ß√£o. <br><br>  Um recurso interessante √© digno de nota - arquivos pequenos (consulte o campo "Dados", onde s√£o escritos 10 bytes) s√£o processados ‚Äã‚Äãmais lentamente, especialmente se forem compactados.  Obviamente, isso s√≥ √© poss√≠vel at√© um determinado tamanho; obviamente, um arquivo de 5 GB n√£o ser√° processado mais r√°pido que 10 MB, mas indica claramente que em todos esses testes ainda n√£o h√° campo para pesquisa de v√°rias configura√ß√µes. <br><br>  Por interesse, corrigi o c√≥digo YCSB para trabalhar com lotes HB de 100 pe√ßas para medir a lat√™ncia e muito mais.  Abaixo est√° o resultado do trabalho de 4 c√≥pias que escreveram em suas tabelas, cada uma com 100 threads.  O resultado foi o seguinte: <br><div class="spoiler">  <b class="spoiler_title">Uma opera√ß√£o = 100 registros</b> <div class="spoiler_text">  [GERAL], Tempo de execu√ß√£o (ms), 1165415 <br>  [GERAL], Taxa de transfer√™ncia (ops / s), 858.06343662987 <br>  [TOTAL_GCS_PS_Scavenge], conde, 798 <br>  [TOTAL_GC_TIME_PS_Scavenge], tempo (ms), 7346 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], tempo (%), 0,6303334005483026 <br>  [TOTAL_GCS_PS_MarkSweep], contagem, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], tempo (ms), 74 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], tempo (%), 0,006349669431061038 <br>  [TOTAL_GCs], contagem, 799 <br>  [TOTAL_GC_TIME], tempo (ms), 7420 <br>  [TOTAL_GC_TIME_%], tempo (%), 0,6366830699793635 <br>  [INSERIR], Opera√ß√µes, 1.000.000 <br>  [INSERIR], AverageLatency (us), 115893.891644 <br>  [INSERIR], MinLatency (us), 14528 <br>  [INSERIR], MaxLatency (us), 1470463 <br>  [INSERT], 95thPercentileLatency (us), 248319 <br>  [INSERT], 99thPercentileLatency (us), 445951 <br>  [INSERIR], Retorno = OK, 1.000.000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Fechando o zookeeper sessionid = 0x36f98ad0a4ad8cc <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cc fechado <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread desligado <br>  [GERAL], Tempo de execu√ß√£o (ms), 1165806 <br>  [GERAL], Taxa de transfer√™ncia (ops / s), 857.7756504941646 <br>  [TOTAL_GCS_PS_Scavenge], contagem, 776 <br>  [TOTAL_GC_TIME_PS_Scavenge], tempo (ms), 7517 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], tempo (%), 0,6447899564764635 <br>  [TOTAL_GCS_PS_MarkSweep], contagem, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], tempo (ms), 63 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], tempo (%), 0,005403986598113236 <br>  [TOTAL_GCs], conde, 777 <br>  [TOTAL_GC_TIME], tempo (ms), 7580 <br>  [TOTAL_GC_TIME_%], tempo (%), 0,6501939430745767 <br>  [INSERIR], Opera√ß√µes, 1.000.000 <br>  [INSERIR], AverageLatency (us), 116042.207936 <br>  [INSERIR], MinLatency (us), 14056 <br>  [INSERIR], MaxLatency (us), 1462271 <br>  [INSERT], 95thPercentileLatency (us), 250239 <br>  [INSERT], 99thPercentileLatency (us), 446719 <br>  [INSERIR], Retorno = OK, 1.000.000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Fechando o zookeeper sessionid = 0x26f98ad07b6d67e <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Session: 0x26f98ad07b6d67e fechado <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread desligado <br>  [GERAL], Tempo de execu√ß√£o (ms), 1165999 <br>  [GERAL], Taxa de transfer√™ncia (ops / s), 857.63366863951 <br>  [TOTAL_GCS_PS_Scavenge], conde, 818 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7557 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6481137633908777 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 79 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.006775305982252128 <br> [TOTAL_GCs], Count, 819 <br> [TOTAL_GC_TIME], Time(ms), 7636 <br> [TOTAL_GC_TIME_%], Time(%), 0.6548890693731299 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116172.212864 <br> [INSERT], MinLatency(us), 7952 <br> [INSERT], MaxLatency(us), 1458175 <br> [INSERT], 95thPercentileLatency(us), 250879 <br> [INSERT], 99thPercentileLatency(us), 446463 <br> [INSERT], Return=OK, 1000000 <br><br> 20/01/19 13:19:17 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x36f98ad0a4ad8cd <br> 20/01/19 13:19:17 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cd closed <br> 20/01/19 13:19:17 INFO zookeeper.ClientCnxn: EventThread shut down <br> [OVERALL], RunTime(ms), 1166860 <br> [OVERALL], Throughput(ops/sec), 857.000839860823 <br> [TOTAL_GCS_PS_Scavenge], Count, 707 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7239 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6203829079752499 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 67 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.0057419056270675145 <br> [TOTAL_GCs], Count, 708 <br> [TOTAL_GC_TIME], Time(ms), 7306 <br> [TOTAL_GC_TIME_%], Time(%), 0.6261248136023173 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116230.849308 <br> [INSERT], MinLatency(us), 7352 <br> [INSERT], MaxLatency(us), 1443839 <br> [INSERT], 95thPercentileLatency(us), 250623 <br> [INSERT], 99thPercentileLatency(us), 447487 <br> [INSERT], Return=OK, 1000000 </div></div><br><br> ,    CS AverageLatency(us)   3114,   HB AverageLatency(us) = 1162 (,  1  = 100     ). <br><br>      ‚Äî        HBase.   ,  SSD       .   ,       ,    ,     4 ,  400    ,      .   :  ‚Äî  .  Voc√™ tem que tentar.   ScyllaDB     ,    ‚Ä¶ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt484096/">https://habr.com/ru/post/pt484096/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt484084/index.html">1C-Bitrix e uma tentativa de introduzi-lo</a></li>
<li><a href="../pt484088/index.html">Desfile de senhas (an√°lise de ~ 5 bilh√µes de senhas devido a vazamentos)</a></li>
<li><a href="../pt484090/index.html">Nova infraestrutura de TI para o russo Post Data Center</a></li>
<li><a href="../pt484092/index.html">Pr√≠ncipes e nobres um tanto vestidos</a></li>
<li><a href="../pt484094/index.html">Crie um jogo de tiro em zumbi na terceira pessoa com DOTS</a></li>
<li><a href="../pt484100/index.html">Trabalhando com a interface no SDK do Google Maps para Android</a></li>
<li><a href="../pt484102/index.html">PHP vs Python vs Ruby on Rails: Compara√ß√£o Detalhada</a></li>
<li><a href="../pt484106/index.html">MVCC no PostgreSQL-6. V√°cuo</a></li>
<li><a href="../pt484108/index.html">Encapsulador Etherblade.net e Substitui√ß√£o de Importa√ß√£o para Componentes de Rede (Parte Dois)</a></li>
<li><a href="../pt484112/index.html">√â poss√≠vel invadir um avi√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>