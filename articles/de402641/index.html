<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚ÄçüöÄ ü§≤üèΩ üêç Wie tief neuronale Netze aussehen und warum sie so viel Speicher ben√∂tigen üëãüèΩ üë∞üèæ üë®üèø‚Äçüéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Heute ist das Diagramm eine der akzeptabelsten M√∂glichkeiten, um die im maschinellen Lernsystem erstellten Modelle zu beschreiben. Diese Berechnungsgr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie tief neuronale Netze aussehen und warum sie so viel Speicher ben√∂tigen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402641/"><img src="https://habrastorage.org/getpro/geektimes/post_images/802/6f4/eab/8026f4eab4ee028d6894159b00421c56.jpg" alt="Bild"><br><br>  Heute ist das Diagramm eine der akzeptabelsten M√∂glichkeiten, um die im maschinellen Lernsystem erstellten Modelle zu beschreiben.  Diese Berechnungsgraphen bestehen aus Neuronenscheitelpunkten, die durch Synapsenkanten verbunden sind und die Verbindungen zwischen Scheitelpunkten beschreiben. <br><br>  Im Gegensatz zu einem skalaren Zentral- oder Vektorgrafikprozessor k√∂nnen Sie mit IPU - einem neuen Prozessortyp f√ºr maschinelles Lernen - solche Diagramme erstellen.  Ein Computer f√ºr die Grafikverwaltung ist eine ideale Maschine f√ºr rechnergest√ºtzte Grafikmodelle, die im Rahmen des maschinellen Lernens erstellt wurden. <br><br>  Eine der einfachsten M√∂glichkeiten, die Funktionsweise von Machine Intelligence zu beschreiben, besteht darin, sie zu visualisieren.  Das Entwicklungsteam von Graphcore hat eine Sammlung dieser Bilder erstellt, die auf der IPU angezeigt werden.  Grundlage war die Pappelsoftware, die die Arbeit der k√ºnstlichen Intelligenz visualisiert.  Forscher dieses Unternehmens fanden auch heraus, warum tiefe Netzwerke so viel Speicher ben√∂tigen und welche L√∂sungen es gibt. <a name="habracut"></a><br><br>  Poplar enth√§lt einen grafischen Compiler, der von Grund auf neu erstellt wurde, um die im Rahmen des maschinellen Lernens verwendeten Standardoperationen in hochoptimierten Anwendungscode f√ºr IPUs zu √ºbersetzen.  Sie k√∂nnen diese Diagramme nach demselben Prinzip zusammenstellen, wie POPNNs zusammengestellt werden.  Die Bibliothek enth√§lt eine Reihe verschiedener Arten von Scheitelpunkten f√ºr verallgemeinerte Grundelemente. <br><br>  Grafiken sind das Paradigma, auf dem die gesamte Software basiert.  In Pappel k√∂nnen Sie mithilfe von Diagrammen den Berechnungsprozess definieren, bei dem Scheitelpunkte Operationen ausf√ºhren und Kanten die Beziehung zwischen ihnen beschreiben.  Wenn Sie beispielsweise zwei Zahlen addieren m√∂chten, k√∂nnen Sie einen Scheitelpunkt mit zwei Eingaben (die Zahlen, die Sie hinzuf√ºgen m√∂chten), einigen Berechnungen (die Funktion zum Hinzuf√ºgen von zwei Zahlen) und der Ausgabe (Ergebnis) definieren. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/708/107/2ad/7081072ad3e33d401cabae1f3914bccb.jpg" alt="Bild"><br><br>  Normalerweise sind Scheitelpunktoperationen viel komplizierter als im oben beschriebenen Beispiel.  Oft werden sie durch kleine Programme definiert, die als Codelets (Codenamen) bezeichnet werden.  Die grafische Abstraktion ist attraktiv, da sie keine Annahmen √ºber die Struktur von Berechnungen macht und die Berechnung in Komponenten aufteilt, mit denen der IPU-Prozessor arbeiten kann. <br><br>  Pappel verwendet diese einfache Abstraktion, um sehr gro√üe Diagramme zu erstellen, die als Bilder dargestellt werden.  Durch die programmatische Generierung des Diagramms k√∂nnen wir es an die spezifischen Berechnungen anpassen, die f√ºr eine m√∂glichst effiziente Nutzung der IPU-Ressourcen erforderlich sind. <br><br>  Der Compiler √ºbersetzt Standardoperationen, die in maschinellen Lernsystemen verwendet werden, in hochoptimierten Anwendungscode f√ºr IPUs.  Ein Diagramm-Compiler erstellt ein Zwischenbild eines Berechnungsdiagramms, das auf einem oder mehreren IPU-Ger√§ten bereitgestellt wird.  Der Compiler kann diesen Berechnungsgraphen anzeigen, sodass eine auf der Ebene der neuronalen Netzwerkstruktur geschriebene Anwendung ein Bild des Berechnungsgraphen anzeigt, der auf der IPU ausgef√ºhrt wird. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/45a/eb2/4b8/45aeb24b80e244c6c16c6fc584d99678.jpg" alt="Bild"><br>  <i>AlexNet-Lernzyklus f√ºr den gesamten Zyklus vorw√§rts und r√ºckw√§rts</i> <br><br>  Der Grafik-Compiler Poplar verwandelte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> Beschreibung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AlexNet</a> in einen Rechengraphen mit 18,7 Millionen Eckpunkten und 115,8 Millionen Kanten.  Deutlich sichtbares Clustering ist das Ergebnis einer starken Verbindung zwischen den Prozessen in jeder Schicht des Netzwerks mit einer einfacheren Verbindung zwischen den Ebenen. <br><br>  Ein weiteres Beispiel ist ein einfaches Netzwerk mit vollst√§ndiger Konnektivit√§t, das bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST</a> geschult wurde - ein einfacher Datensatz f√ºr Computer Vision, eine Art ‚ÄûHallo Welt‚Äú im maschinellen Lernen.  Ein einfaches Netzwerk zum Durchsuchen dieses Datensatzes hilft dabei, die Diagramme zu verstehen, die von Pappelanwendungen gesteuert werden.  Durch die Integration von Diagrammbibliotheken in Umgebungen wie TensorFlow bietet das Unternehmen eine der einfachsten M√∂glichkeiten, IPUs in Anwendungen f√ºr maschinelles Lernen zu verwenden. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/f07/3bd/4e3/f073bd4e334225380ae51b16fb2e94b6.jpg" alt="Bild"><br><br>  Nachdem das Diagramm mit dem Compiler erstellt wurde, muss es ausgef√ºhrt werden.  Dies ist mit der Graph Engine m√∂glich.  Am Beispiel von ResNet-50 wird dessen Funktionsweise demonstriert. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/5d7/cb1/ce5/5d7cb1ce55a80f2b8e4ebefe38681710.jpg" alt="Bild"><br>  <i>Z√§hlen Sie ResNet-50</i> <br><br>  Mit der ResNet-50-Architektur k√∂nnen Sie aus sich wiederholenden Partitionen tiefe Netzwerke erstellen.  Der Prozessor muss diese Partitionen nur einmal ermitteln und erneut aufrufen.  Beispielsweise wird ein Cluster auf Conv4-Ebene sechsmal ausgef√ºhrt, jedoch nur einmal auf das Diagramm angewendet.  Das Bild zeigt auch die Vielfalt der Formen von Faltungsschichten, da jede von ihnen einen Graphen aufweist, der gem√§√ü der nat√ºrlichen Berechnungsform erstellt wurde. <br><br>  Die Engine erstellt und steuert die Ausf√ºhrung eines maschinellen Lernmodells mithilfe eines vom Compiler erstellten Diagramms.  Nach der Bereitstellung √ºberwacht Graph Engine IPUs oder Ger√§te, die von Anwendungen verwendet werden, und reagiert darauf. <br><br>  Image ResNet-50 zeigt das gesamte Modell.  Auf dieser Ebene ist es schwierig, zwischen einzelnen Scheitelpunkten zu unterscheiden. Es lohnt sich daher, vergr√∂√üerte Bilder zu betrachten.  Das Folgende sind einige Beispiele f√ºr Abschnitte innerhalb von Schichten eines neuronalen Netzwerks. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/dbf/166/408/dbf166408a0b1626a4bd720e1be1dbe9.jpg" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/d69/9c1/fb6/d699c1fb6f2e43b9f4d03b405c543186.jpg" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/802/6f4/eab/8026f4eab4ee028d6894159b00421c56.jpg" alt="Bild"><br><br><h3>  Warum brauchen tiefe Netzwerke so viel Speicher? </h3><br>  Gro√üe Mengen an belegtem Speicher sind eines der gr√∂√üten Probleme tiefer neuronaler Netze.  Forscher versuchen, mit der begrenzten Bandbreite von DRAM-Ger√§ten umzugehen, die von modernen Systemen verwendet werden sollten, um eine gro√üe Anzahl von Gewichten und Aktivierungen in einem tiefen neuronalen Netzwerk zu speichern. <br><br>  Die Architekturen wurden unter Verwendung von Prozessorchips entwickelt, die f√ºr die sequentielle Verarbeitung und Optimierung des DRAM f√ºr Speicher mit hoher Dichte ausgelegt sind.  Die Schnittstelle zwischen den beiden Ger√§ten ist ein Engpass, der Bandbreitenbeschr√§nkungen einf√ºhrt und den Energieverbrauch erheblich erh√∂ht. <br><br>  Obwohl wir immer noch kein vollst√§ndiges Bild des menschlichen Gehirns und seiner Funktionsweise haben, ist im Allgemeinen klar, dass es keinen gro√üen separaten Speicher f√ºr das Ged√§chtnis gibt.  Es wird angenommen, dass die Funktion des Langzeit- und Kurzzeitged√§chtnisses im menschlichen Gehirn in die Struktur von Neuronen + Synapsen eingebettet ist.  Selbst einfache Organismen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">W√ºrmer</a> mit einer neuronalen Struktur des Gehirns, die aus etwas mehr als 300 Neuronen bestehen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben einen</a> gewissen Grad an Ged√§chtnisfunktion. <br><br>  Der Aufbau von Speicher in herk√∂mmlichen Prozessoren ist eine M√∂glichkeit, Speicherengp√§sse zu umgehen, indem eine gro√üe Bandbreite mit viel weniger Stromverbrauch erschlossen wird.  Trotzdem ist Speicher auf einem Chip eine teure Sache, die nicht f√ºr wirklich gro√üe Speichermengen ausgelegt ist, die mit den zentralen und grafischen Prozessoren verbunden sind, die derzeit f√ºr die Vorbereitung und Bereitstellung tiefer neuronaler Netze verwendet werden. <br><br>  Daher ist es hilfreich zu untersuchen, wie Speicher heute in Zentraleinheiten und Deep-Learning-Systemen auf Grafikbeschleunigern verwendet wird, und sich zu fragen: Warum ben√∂tigen sie so gro√üe Speicherger√§te, wenn das menschliche Gehirn ohne sie gut funktioniert? <br><br>  Neuronale Netze ben√∂tigen Speicher, um Eingabedaten, Gewichtungsparameter und Aktivierungsfunktionen zu speichern, da die Eingabe √ºber das Netzwerk verteilt wird.  Im Training muss die Aktivierung am Eingang beibehalten werden, bis die Fehler der Gradienten am Ausgang berechnet werden k√∂nnen. <br><br>  Beispielsweise verf√ºgt ein 50-lagiges ResNet-Netzwerk √ºber etwa 26 Millionen Gewichtungsparameter und berechnet 16 Millionen Vorw√§rtsaktivierungen.  Wenn Sie eine 32-Bit-Gleitkommazahl verwenden, um jedes Gewicht und jede Aktivierung zu speichern, sind ca. 168 MB Speicherplatz erforderlich.  Mit einem niedrigeren Genauigkeitswert zum Speichern dieser Skalen und Aktivierungen k√∂nnten wir diesen Speicherbedarf halbieren oder sogar vervierfachen. <br><br>  Ein ernstes Speicherproblem ergibt sich aus der Tatsache, dass GPUs auf Daten beruhen, die als dichte Vektoren dargestellt werden.  Daher k√∂nnen sie einen einzelnen Befehlsstrom (SIMD) verwenden, um eine Berechnung mit hoher Dichte zu erreichen.  Der Zentralprozessor verwendet √§hnliche Vektorbl√∂cke f√ºr Hochleistungsrechnen. <br><br>  In GPUs ist die Synapse 1024 Bit breit, daher verwenden sie 32-Bit-Gleitkommadaten, sodass sie diese h√§ufig in parallele Mini-Batches von 32 Abtastwerten aufteilen, um 1024-Bit-Datenvektoren zu erstellen.  Dieser Ansatz zur Organisation der Vektorparallelit√§t erh√∂ht die Anzahl der Aktivierungen um das 32-fache und den Bedarf an lokalem Speicher mit einer Kapazit√§t von mehr als 2 GB. <br><br>  GPUs und andere Maschinen, die f√ºr die Matrixalgebra entwickelt wurden, unterliegen ebenfalls einer Speicherbelastung durch die Gewichte oder die Aktivierung des neuronalen Netzwerks.  GPUs k√∂nnen kleine Faltungen, die in tiefen neuronalen Netzen verwendet werden, nicht effizient ausf√ºhren.  Daher wird eine Transformation namens "Downgrade" verwendet, um diese Faltungen in Matrix-Matrix-Multiplikationen (GEMMs) umzuwandeln, die Grafikbeschleuniger effektiv verarbeiten k√∂nnen. <br><br>  Zus√§tzlicher Speicher ist auch erforderlich, um Eingabedaten, Zeitwerte und Programmanweisungen zu speichern.  Die Messung der Speichernutzung beim Training von ResNet-50 auf einer Hochleistungs-GPU hat gezeigt, dass mehr als 7,5 GB lokaler DRAM erforderlich sind. <br><br>  Vielleicht entscheidet jemand, dass eine geringere Genauigkeit den Speicherbedarf verringern kann, aber das ist nicht der Fall.  Wenn Sie Datenwerte f√ºr Gewichte und Aktivierungen auf die halbe Genauigkeit umschalten, f√ºllen Sie nur die H√§lfte der Vektorbreite der SIMD aus und verbrauchen die H√§lfte der verf√ºgbaren Rechenressourcen.  Um dies zu kompensieren, m√ºssen Sie beim Umschalten von voller Genauigkeit auf halbe Genauigkeit auf der GPU die Gr√∂√üe des Mini-Batches verdoppeln, um eine ausreichende Datenparallelit√§t zu erreichen, damit alle verf√ºgbaren Berechnungen verwendet werden k√∂nnen.  Der √úbergang zu Skalen und Aktivierungen mit geringerer Genauigkeit auf der GPU erfordert daher immer noch mehr als 7,5 GB dynamischen Speicher mit freiem Zugriff. <br><br>  Bei so vielen zu speichernden Daten ist es einfach unm√∂glich, all dies in die GPU zu integrieren.  Auf jeder Schicht des Faltungs-Neuronalen Netzwerks muss der Status des externen DRAM gespeichert, die n√§chste Netzwerkschicht geladen und dann die Daten in das System geladen werden.  Infolgedessen leidet die externe Speicherschnittstelle, die bereits durch die Speicherbandbreite begrenzt ist, unter der zus√§tzlichen Belastung, das Guthaben st√§ndig neu zu laden sowie Aktivierungsfunktionen zu speichern und abzurufen.  Dies verlangsamt die Trainingszeit erheblich und erh√∂ht den Energieverbrauch erheblich. <br><br>  Es gibt verschiedene L√∂sungen f√ºr dieses Problem.  Erstens k√∂nnen Vorg√§nge wie Aktivierungsfunktionen ‚Äûvor Ort‚Äú ausgef√ºhrt werden, sodass Sie die Eingabe direkt in die Ausgabe √ºberschreiben k√∂nnen.  Somit kann vorhandener Speicher wiederverwendet werden.  Zweitens kann die M√∂glichkeit zur Wiederverwendung von Speicher erhalten werden, indem die Abh√§ngigkeit von Daten zwischen Operationen im Netzwerk und die Verteilung desselben Speichers f√ºr Operationen analysiert wird, die ihn derzeit nicht verwenden. <br><br>  Der zweite Ansatz ist besonders effektiv, wenn das gesamte neuronale Netzwerk in der Kompilierungsphase analysiert werden kann, um einen fest zugewiesenen Speicher zu erstellen, da die Kosten f√ºr die Speicherverwaltung auf nahezu Null reduziert werden.  Es stellte sich heraus, dass eine Kombination dieser Methoden die Speichernutzung des neuronalen Netzwerks um das Zwei- bis Dreifache reduziert. <br>  Ein dritter wichtiger Ansatz wurde k√ºrzlich vom Baidu Deep Speech-Team entdeckt.  Sie verwendeten verschiedene Methoden zum Speichern von Speicher, um den Speicherverbrauch durch Aktivierungsfunktionen um das 16-fache zu reduzieren, wodurch sie Netzwerke mit 100 Schichten trainieren konnten.  Zuvor konnten sie mit der gleichen Speichermenge Netzwerke mit neun Schichten trainieren. <br><br>  Die Kombination von Speicher- und Verarbeitungsressourcen in einem Ger√§t bietet ein erhebliches Potenzial zur Steigerung der Produktivit√§t und Effizienz von Faltungs-Neuronalen Netzen sowie anderer Formen des maschinellen Lernens.  Sie k√∂nnen einen Kompromiss zwischen Speicher und Computerressourcen eingehen, um die Funktionen und die Leistung des Systems in Einklang zu bringen. <br><br>  Neuronale Netze und Wissensmodelle in anderen Methoden des maschinellen Lernens k√∂nnen als mathematische Graphen betrachtet werden.  In diesen Diagrammen ist eine gro√üe Menge an Parallelit√§t konzentriert.  Ein Parallelprozessor, der f√ºr die Verwendung von Parallelit√§t in Diagrammen ausgelegt ist, ist nicht auf Mini-Batch angewiesen und kann den erforderlichen lokalen Speicher erheblich reduzieren. <br><br>  Moderne Forschungsergebnisse haben gezeigt, dass all diese Methoden die Leistung neuronaler Netze erheblich verbessern k√∂nnen.  Moderne Grafik- und Zentraleinheiten haben einen sehr begrenzten internen Speicher, insgesamt nur wenige Megabyte.  Neue Prozessorarchitekturen, die speziell f√ºr maschinelles Lernen entwickelt wurden, bieten ein Gleichgewicht zwischen Speicher und On-Chip-Computing und steigern die Leistung und Effizienz im Vergleich zu modernen Zentraleinheiten und Grafikbeschleunigern erheblich. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de402641/">https://habr.com/ru/post/de402641/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de402629/index.html">Meine kleinen Relais: Brainfuck Computer ist Magie</a></li>
<li><a href="../de402631/index.html">Welchen Herzfrequenzmesser Sie in der neuen Saison w√§hlen sollten: Kompromissl√∂sungen innerhalb von drei bis viertausend Rubel</a></li>
<li><a href="../de402633/index.html">Die Geschichte von Battlefield 1 in Full HD √ºber die integrierte Grafik im Prozessor und die Montage der Konsole f√ºr "unverg√§nglich"</a></li>
<li><a href="../de402637/index.html">Der 17-j√§hrige Student hat den NASA-Fehler korrigiert</a></li>
<li><a href="../de402639/index.html">Peter Watts √ºber SOMA</a></li>
<li><a href="../de402643/index.html">"D√ºnne Welt." Kapitel 10</a></li>
<li><a href="../de402645/index.html">Mit dem ServoStudio 12-Programm und der Arduino-Karte k√∂nnen Sie Ihren eigenen Roboter erstellen, ohne eine einzige Codezeile schreiben zu m√ºssen</a></li>
<li><a href="../de402649/index.html">Das genaueste der Welt: Valencell-Herzfrequenzmesser f√ºr Jabra, Suunto, Atlas, Sony und andere</a></li>
<li><a href="../de402651/index.html">Polyethylenimplantat mit ultrahohem Molekulargewicht ersetzte Knochengewebe oder Eisenpolymer</a></li>
<li><a href="../de402653/index.html">Robomobile haben Probleme mit Radfahrern</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>