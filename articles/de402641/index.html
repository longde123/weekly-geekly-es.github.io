<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍🚀 🤲🏽 🐍 Wie tief neuronale Netze aussehen und warum sie so viel Speicher benötigen 👋🏽 👰🏾 👨🏿‍🎓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Heute ist das Diagramm eine der akzeptabelsten Möglichkeiten, um die im maschinellen Lernsystem erstellten Modelle zu beschreiben. Diese Berechnungsgr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie tief neuronale Netze aussehen und warum sie so viel Speicher benötigen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402641/"><img src="https://habrastorage.org/getpro/geektimes/post_images/802/6f4/eab/8026f4eab4ee028d6894159b00421c56.jpg" alt="Bild"><br><br>  Heute ist das Diagramm eine der akzeptabelsten Möglichkeiten, um die im maschinellen Lernsystem erstellten Modelle zu beschreiben.  Diese Berechnungsgraphen bestehen aus Neuronenscheitelpunkten, die durch Synapsenkanten verbunden sind und die Verbindungen zwischen Scheitelpunkten beschreiben. <br><br>  Im Gegensatz zu einem skalaren Zentral- oder Vektorgrafikprozessor können Sie mit IPU - einem neuen Prozessortyp für maschinelles Lernen - solche Diagramme erstellen.  Ein Computer für die Grafikverwaltung ist eine ideale Maschine für rechnergestützte Grafikmodelle, die im Rahmen des maschinellen Lernens erstellt wurden. <br><br>  Eine der einfachsten Möglichkeiten, die Funktionsweise von Machine Intelligence zu beschreiben, besteht darin, sie zu visualisieren.  Das Entwicklungsteam von Graphcore hat eine Sammlung dieser Bilder erstellt, die auf der IPU angezeigt werden.  Grundlage war die Pappelsoftware, die die Arbeit der künstlichen Intelligenz visualisiert.  Forscher dieses Unternehmens fanden auch heraus, warum tiefe Netzwerke so viel Speicher benötigen und welche Lösungen es gibt. <a name="habracut"></a><br><br>  Poplar enthält einen grafischen Compiler, der von Grund auf neu erstellt wurde, um die im Rahmen des maschinellen Lernens verwendeten Standardoperationen in hochoptimierten Anwendungscode für IPUs zu übersetzen.  Sie können diese Diagramme nach demselben Prinzip zusammenstellen, wie POPNNs zusammengestellt werden.  Die Bibliothek enthält eine Reihe verschiedener Arten von Scheitelpunkten für verallgemeinerte Grundelemente. <br><br>  Grafiken sind das Paradigma, auf dem die gesamte Software basiert.  In Pappel können Sie mithilfe von Diagrammen den Berechnungsprozess definieren, bei dem Scheitelpunkte Operationen ausführen und Kanten die Beziehung zwischen ihnen beschreiben.  Wenn Sie beispielsweise zwei Zahlen addieren möchten, können Sie einen Scheitelpunkt mit zwei Eingaben (die Zahlen, die Sie hinzufügen möchten), einigen Berechnungen (die Funktion zum Hinzufügen von zwei Zahlen) und der Ausgabe (Ergebnis) definieren. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/708/107/2ad/7081072ad3e33d401cabae1f3914bccb.jpg" alt="Bild"><br><br>  Normalerweise sind Scheitelpunktoperationen viel komplizierter als im oben beschriebenen Beispiel.  Oft werden sie durch kleine Programme definiert, die als Codelets (Codenamen) bezeichnet werden.  Die grafische Abstraktion ist attraktiv, da sie keine Annahmen über die Struktur von Berechnungen macht und die Berechnung in Komponenten aufteilt, mit denen der IPU-Prozessor arbeiten kann. <br><br>  Pappel verwendet diese einfache Abstraktion, um sehr große Diagramme zu erstellen, die als Bilder dargestellt werden.  Durch die programmatische Generierung des Diagramms können wir es an die spezifischen Berechnungen anpassen, die für eine möglichst effiziente Nutzung der IPU-Ressourcen erforderlich sind. <br><br>  Der Compiler übersetzt Standardoperationen, die in maschinellen Lernsystemen verwendet werden, in hochoptimierten Anwendungscode für IPUs.  Ein Diagramm-Compiler erstellt ein Zwischenbild eines Berechnungsdiagramms, das auf einem oder mehreren IPU-Geräten bereitgestellt wird.  Der Compiler kann diesen Berechnungsgraphen anzeigen, sodass eine auf der Ebene der neuronalen Netzwerkstruktur geschriebene Anwendung ein Bild des Berechnungsgraphen anzeigt, der auf der IPU ausgeführt wird. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/45a/eb2/4b8/45aeb24b80e244c6c16c6fc584d99678.jpg" alt="Bild"><br>  <i>AlexNet-Lernzyklus für den gesamten Zyklus vorwärts und rückwärts</i> <br><br>  Der Grafik-Compiler Poplar verwandelte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> Beschreibung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AlexNet</a> in einen Rechengraphen mit 18,7 Millionen Eckpunkten und 115,8 Millionen Kanten.  Deutlich sichtbares Clustering ist das Ergebnis einer starken Verbindung zwischen den Prozessen in jeder Schicht des Netzwerks mit einer einfacheren Verbindung zwischen den Ebenen. <br><br>  Ein weiteres Beispiel ist ein einfaches Netzwerk mit vollständiger Konnektivität, das bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST</a> geschult wurde - ein einfacher Datensatz für Computer Vision, eine Art „Hallo Welt“ im maschinellen Lernen.  Ein einfaches Netzwerk zum Durchsuchen dieses Datensatzes hilft dabei, die Diagramme zu verstehen, die von Pappelanwendungen gesteuert werden.  Durch die Integration von Diagrammbibliotheken in Umgebungen wie TensorFlow bietet das Unternehmen eine der einfachsten Möglichkeiten, IPUs in Anwendungen für maschinelles Lernen zu verwenden. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/f07/3bd/4e3/f073bd4e334225380ae51b16fb2e94b6.jpg" alt="Bild"><br><br>  Nachdem das Diagramm mit dem Compiler erstellt wurde, muss es ausgeführt werden.  Dies ist mit der Graph Engine möglich.  Am Beispiel von ResNet-50 wird dessen Funktionsweise demonstriert. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/5d7/cb1/ce5/5d7cb1ce55a80f2b8e4ebefe38681710.jpg" alt="Bild"><br>  <i>Zählen Sie ResNet-50</i> <br><br>  Mit der ResNet-50-Architektur können Sie aus sich wiederholenden Partitionen tiefe Netzwerke erstellen.  Der Prozessor muss diese Partitionen nur einmal ermitteln und erneut aufrufen.  Beispielsweise wird ein Cluster auf Conv4-Ebene sechsmal ausgeführt, jedoch nur einmal auf das Diagramm angewendet.  Das Bild zeigt auch die Vielfalt der Formen von Faltungsschichten, da jede von ihnen einen Graphen aufweist, der gemäß der natürlichen Berechnungsform erstellt wurde. <br><br>  Die Engine erstellt und steuert die Ausführung eines maschinellen Lernmodells mithilfe eines vom Compiler erstellten Diagramms.  Nach der Bereitstellung überwacht Graph Engine IPUs oder Geräte, die von Anwendungen verwendet werden, und reagiert darauf. <br><br>  Image ResNet-50 zeigt das gesamte Modell.  Auf dieser Ebene ist es schwierig, zwischen einzelnen Scheitelpunkten zu unterscheiden. Es lohnt sich daher, vergrößerte Bilder zu betrachten.  Das Folgende sind einige Beispiele für Abschnitte innerhalb von Schichten eines neuronalen Netzwerks. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/dbf/166/408/dbf166408a0b1626a4bd720e1be1dbe9.jpg" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/d69/9c1/fb6/d699c1fb6f2e43b9f4d03b405c543186.jpg" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/802/6f4/eab/8026f4eab4ee028d6894159b00421c56.jpg" alt="Bild"><br><br><h3>  Warum brauchen tiefe Netzwerke so viel Speicher? </h3><br>  Große Mengen an belegtem Speicher sind eines der größten Probleme tiefer neuronaler Netze.  Forscher versuchen, mit der begrenzten Bandbreite von DRAM-Geräten umzugehen, die von modernen Systemen verwendet werden sollten, um eine große Anzahl von Gewichten und Aktivierungen in einem tiefen neuronalen Netzwerk zu speichern. <br><br>  Die Architekturen wurden unter Verwendung von Prozessorchips entwickelt, die für die sequentielle Verarbeitung und Optimierung des DRAM für Speicher mit hoher Dichte ausgelegt sind.  Die Schnittstelle zwischen den beiden Geräten ist ein Engpass, der Bandbreitenbeschränkungen einführt und den Energieverbrauch erheblich erhöht. <br><br>  Obwohl wir immer noch kein vollständiges Bild des menschlichen Gehirns und seiner Funktionsweise haben, ist im Allgemeinen klar, dass es keinen großen separaten Speicher für das Gedächtnis gibt.  Es wird angenommen, dass die Funktion des Langzeit- und Kurzzeitgedächtnisses im menschlichen Gehirn in die Struktur von Neuronen + Synapsen eingebettet ist.  Selbst einfache Organismen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Würmer</a> mit einer neuronalen Struktur des Gehirns, die aus etwas mehr als 300 Neuronen bestehen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben einen</a> gewissen Grad an Gedächtnisfunktion. <br><br>  Der Aufbau von Speicher in herkömmlichen Prozessoren ist eine Möglichkeit, Speicherengpässe zu umgehen, indem eine große Bandbreite mit viel weniger Stromverbrauch erschlossen wird.  Trotzdem ist Speicher auf einem Chip eine teure Sache, die nicht für wirklich große Speichermengen ausgelegt ist, die mit den zentralen und grafischen Prozessoren verbunden sind, die derzeit für die Vorbereitung und Bereitstellung tiefer neuronaler Netze verwendet werden. <br><br>  Daher ist es hilfreich zu untersuchen, wie Speicher heute in Zentraleinheiten und Deep-Learning-Systemen auf Grafikbeschleunigern verwendet wird, und sich zu fragen: Warum benötigen sie so große Speichergeräte, wenn das menschliche Gehirn ohne sie gut funktioniert? <br><br>  Neuronale Netze benötigen Speicher, um Eingabedaten, Gewichtungsparameter und Aktivierungsfunktionen zu speichern, da die Eingabe über das Netzwerk verteilt wird.  Im Training muss die Aktivierung am Eingang beibehalten werden, bis die Fehler der Gradienten am Ausgang berechnet werden können. <br><br>  Beispielsweise verfügt ein 50-lagiges ResNet-Netzwerk über etwa 26 Millionen Gewichtungsparameter und berechnet 16 Millionen Vorwärtsaktivierungen.  Wenn Sie eine 32-Bit-Gleitkommazahl verwenden, um jedes Gewicht und jede Aktivierung zu speichern, sind ca. 168 MB Speicherplatz erforderlich.  Mit einem niedrigeren Genauigkeitswert zum Speichern dieser Skalen und Aktivierungen könnten wir diesen Speicherbedarf halbieren oder sogar vervierfachen. <br><br>  Ein ernstes Speicherproblem ergibt sich aus der Tatsache, dass GPUs auf Daten beruhen, die als dichte Vektoren dargestellt werden.  Daher können sie einen einzelnen Befehlsstrom (SIMD) verwenden, um eine Berechnung mit hoher Dichte zu erreichen.  Der Zentralprozessor verwendet ähnliche Vektorblöcke für Hochleistungsrechnen. <br><br>  In GPUs ist die Synapse 1024 Bit breit, daher verwenden sie 32-Bit-Gleitkommadaten, sodass sie diese häufig in parallele Mini-Batches von 32 Abtastwerten aufteilen, um 1024-Bit-Datenvektoren zu erstellen.  Dieser Ansatz zur Organisation der Vektorparallelität erhöht die Anzahl der Aktivierungen um das 32-fache und den Bedarf an lokalem Speicher mit einer Kapazität von mehr als 2 GB. <br><br>  GPUs und andere Maschinen, die für die Matrixalgebra entwickelt wurden, unterliegen ebenfalls einer Speicherbelastung durch die Gewichte oder die Aktivierung des neuronalen Netzwerks.  GPUs können kleine Faltungen, die in tiefen neuronalen Netzen verwendet werden, nicht effizient ausführen.  Daher wird eine Transformation namens "Downgrade" verwendet, um diese Faltungen in Matrix-Matrix-Multiplikationen (GEMMs) umzuwandeln, die Grafikbeschleuniger effektiv verarbeiten können. <br><br>  Zusätzlicher Speicher ist auch erforderlich, um Eingabedaten, Zeitwerte und Programmanweisungen zu speichern.  Die Messung der Speichernutzung beim Training von ResNet-50 auf einer Hochleistungs-GPU hat gezeigt, dass mehr als 7,5 GB lokaler DRAM erforderlich sind. <br><br>  Vielleicht entscheidet jemand, dass eine geringere Genauigkeit den Speicherbedarf verringern kann, aber das ist nicht der Fall.  Wenn Sie Datenwerte für Gewichte und Aktivierungen auf die halbe Genauigkeit umschalten, füllen Sie nur die Hälfte der Vektorbreite der SIMD aus und verbrauchen die Hälfte der verfügbaren Rechenressourcen.  Um dies zu kompensieren, müssen Sie beim Umschalten von voller Genauigkeit auf halbe Genauigkeit auf der GPU die Größe des Mini-Batches verdoppeln, um eine ausreichende Datenparallelität zu erreichen, damit alle verfügbaren Berechnungen verwendet werden können.  Der Übergang zu Skalen und Aktivierungen mit geringerer Genauigkeit auf der GPU erfordert daher immer noch mehr als 7,5 GB dynamischen Speicher mit freiem Zugriff. <br><br>  Bei so vielen zu speichernden Daten ist es einfach unmöglich, all dies in die GPU zu integrieren.  Auf jeder Schicht des Faltungs-Neuronalen Netzwerks muss der Status des externen DRAM gespeichert, die nächste Netzwerkschicht geladen und dann die Daten in das System geladen werden.  Infolgedessen leidet die externe Speicherschnittstelle, die bereits durch die Speicherbandbreite begrenzt ist, unter der zusätzlichen Belastung, das Guthaben ständig neu zu laden sowie Aktivierungsfunktionen zu speichern und abzurufen.  Dies verlangsamt die Trainingszeit erheblich und erhöht den Energieverbrauch erheblich. <br><br>  Es gibt verschiedene Lösungen für dieses Problem.  Erstens können Vorgänge wie Aktivierungsfunktionen „vor Ort“ ausgeführt werden, sodass Sie die Eingabe direkt in die Ausgabe überschreiben können.  Somit kann vorhandener Speicher wiederverwendet werden.  Zweitens kann die Möglichkeit zur Wiederverwendung von Speicher erhalten werden, indem die Abhängigkeit von Daten zwischen Operationen im Netzwerk und die Verteilung desselben Speichers für Operationen analysiert wird, die ihn derzeit nicht verwenden. <br><br>  Der zweite Ansatz ist besonders effektiv, wenn das gesamte neuronale Netzwerk in der Kompilierungsphase analysiert werden kann, um einen fest zugewiesenen Speicher zu erstellen, da die Kosten für die Speicherverwaltung auf nahezu Null reduziert werden.  Es stellte sich heraus, dass eine Kombination dieser Methoden die Speichernutzung des neuronalen Netzwerks um das Zwei- bis Dreifache reduziert. <br>  Ein dritter wichtiger Ansatz wurde kürzlich vom Baidu Deep Speech-Team entdeckt.  Sie verwendeten verschiedene Methoden zum Speichern von Speicher, um den Speicherverbrauch durch Aktivierungsfunktionen um das 16-fache zu reduzieren, wodurch sie Netzwerke mit 100 Schichten trainieren konnten.  Zuvor konnten sie mit der gleichen Speichermenge Netzwerke mit neun Schichten trainieren. <br><br>  Die Kombination von Speicher- und Verarbeitungsressourcen in einem Gerät bietet ein erhebliches Potenzial zur Steigerung der Produktivität und Effizienz von Faltungs-Neuronalen Netzen sowie anderer Formen des maschinellen Lernens.  Sie können einen Kompromiss zwischen Speicher und Computerressourcen eingehen, um die Funktionen und die Leistung des Systems in Einklang zu bringen. <br><br>  Neuronale Netze und Wissensmodelle in anderen Methoden des maschinellen Lernens können als mathematische Graphen betrachtet werden.  In diesen Diagrammen ist eine große Menge an Parallelität konzentriert.  Ein Parallelprozessor, der für die Verwendung von Parallelität in Diagrammen ausgelegt ist, ist nicht auf Mini-Batch angewiesen und kann den erforderlichen lokalen Speicher erheblich reduzieren. <br><br>  Moderne Forschungsergebnisse haben gezeigt, dass all diese Methoden die Leistung neuronaler Netze erheblich verbessern können.  Moderne Grafik- und Zentraleinheiten haben einen sehr begrenzten internen Speicher, insgesamt nur wenige Megabyte.  Neue Prozessorarchitekturen, die speziell für maschinelles Lernen entwickelt wurden, bieten ein Gleichgewicht zwischen Speicher und On-Chip-Computing und steigern die Leistung und Effizienz im Vergleich zu modernen Zentraleinheiten und Grafikbeschleunigern erheblich. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de402641/">https://habr.com/ru/post/de402641/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de402629/index.html">Meine kleinen Relais: Brainfuck Computer ist Magie</a></li>
<li><a href="../de402631/index.html">Welchen Herzfrequenzmesser Sie in der neuen Saison wählen sollten: Kompromisslösungen innerhalb von drei bis viertausend Rubel</a></li>
<li><a href="../de402633/index.html">Die Geschichte von Battlefield 1 in Full HD über die integrierte Grafik im Prozessor und die Montage der Konsole für "unvergänglich"</a></li>
<li><a href="../de402637/index.html">Der 17-jährige Student hat den NASA-Fehler korrigiert</a></li>
<li><a href="../de402639/index.html">Peter Watts über SOMA</a></li>
<li><a href="../de402643/index.html">"Dünne Welt." Kapitel 10</a></li>
<li><a href="../de402645/index.html">Mit dem ServoStudio 12-Programm und der Arduino-Karte können Sie Ihren eigenen Roboter erstellen, ohne eine einzige Codezeile schreiben zu müssen</a></li>
<li><a href="../de402649/index.html">Das genaueste der Welt: Valencell-Herzfrequenzmesser für Jabra, Suunto, Atlas, Sony und andere</a></li>
<li><a href="../de402651/index.html">Polyethylenimplantat mit ultrahohem Molekulargewicht ersetzte Knochengewebe oder Eisenpolymer</a></li>
<li><a href="../de402653/index.html">Robomobile haben Probleme mit Radfahrern</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>