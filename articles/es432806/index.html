<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìü üèØ üàÇÔ∏è Superinteligencia: una idea que persigue a las personas inteligentes üèöÔ∏è üè≥Ô∏è üïç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Interpretaci√≥n del discurso en la conferencia Web Camp Zagreb Maciej Tseglovsky, desarrollador web estadounidense, emprendedor, orador y cr√≠tico socia...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Superinteligencia: una idea que persigue a las personas inteligentes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/432806/"><img src="https://habrastorage.org/getpro/habr/post_images/6e3/91b/4f1/6e391b4f1772fd49d6c836bc87ffd343.jpg"><br><br>  <i>Interpretaci√≥n del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">discurso en la conferencia</a> Web Camp Zagreb Maciej Tseglovsky, desarrollador web estadounidense, emprendedor, orador y cr√≠tico social de origen polaco.</i> <br><br>  En 1945, cuando los f√≠sicos estadounidenses se preparaban para probar la bomba at√≥mica, se le ocurri√≥ a alguien preguntar si tal prueba podr√≠a encender la atm√≥sfera. <br><br>  El miedo estaba justificado.  El nitr√≥geno que constituye la mayor parte de la atm√≥sfera es energ√©ticamente inestable.  Si los dos √°tomos chocan lo suficientemente fuerte, se convertir√°n en un √°tomo de magnesio, una part√≠cula alfa y liberar√°n una gran energ√≠a: <br><br>  N <sup>14</sup> + N <sup>14</sup> ‚áí Mg <sup>24</sup> + Œ± + 17,7 MeV <br><br>  Una pregunta vital era si esta reacci√≥n podr√≠a volverse autosuficiente.  Se supon√≠a que la temperatura dentro de la bola de una explosi√≥n nuclear exceder√≠a todo lo que alguna vez se observ√≥ en la Tierra.  ¬øPodr√≠a ser que arrojamos una cerilla en un mont√≥n de hojas secas? <br><a name="habracut"></a><br>  Los f√≠sicos de Los Alamos realizaron un an√°lisis y decidieron que el margen de seguridad era satisfactorio.  Desde que asistimos a la conferencia hoy, sabemos que ten√≠an raz√≥n.  Ten√≠an confianza en sus predicciones, ya que las leyes que rigen las reacciones nucleares eran directas y bien conocidas. <br><br>  Hoy estamos creando otra tecnolog√≠a que cambia el mundo: la inteligencia artificial.  Sabemos que tendr√° un tremendo impacto en el mundo, cambiar√° la forma en que funciona la econom√≠a y desencadenar√° el impredecible efecto domin√≥. <br><br>  Pero tambi√©n existe el riesgo de una reacci√≥n incontrolable, durante la cual la IA alcanzar√° y superar√° lo suficientemente r√°pido el nivel humano de inteligencia.  Y en este momento, los problemas sociales y econ√≥micos nos preocupar√°n menos.  Cualquier m√°quina ultra inteligente tendr√° sus propios hipermetas y trabajar√° para lograrlas manipulando a las personas o simplemente usando sus cuerpos como una fuente conveniente de recursos. <br><br>  El a√±o pasado, el fil√≥sofo Nick Bostrom lanz√≥ el libro Superinteligencia, en el que describi√≥ la visi√≥n alarmista de la IA y trat√≥ de demostrar que tal explosi√≥n de inteligencia es peligrosa e inevitable, si conf√≠a en algunas suposiciones moderadas. <br><br>  La computadora que domina el mundo es el tema favorito de NF.  Sin embargo, mucha gente toma en serio este escenario, por lo que debemos tomarlo en serio.  Stephen Hawking, Elon Musk, un gran n√∫mero de inversores y multimillonarios de Silicon Valley consideran este argumento convincente. <br><br>  Perm√≠tanme primero esbozar los requisitos previos necesarios para probar el argumento de Bostrom. <br><br><h2>  Antecedentes </h2><br><h3>  Prerrequisito 1: Eficiencia de una idea </h3><br>  La primera premisa es una simple observaci√≥n de la existencia de una mente pensante.  Cada uno de nosotros lleva sobre nuestros hombros una peque√±a caja de carne pensante.  Yo uso el m√≠o para hablar, t√∫ usas el m√≠o para escuchar.  A veces, en las condiciones adecuadas, estas mentes pueden pensar racionalmente. <br><br>  Entonces sabemos que, en principio, esto es posible. <br><br><h3>  Prerrequisito 2: sin problemas cu√°nticos </h3><br>  La segunda premisa dice que el cerebro es la configuraci√≥n habitual de la materia, aunque es extremadamente compleja.  Si supi√©ramos lo suficiente y tuvi√©ramos la tecnolog√≠a adecuada, podr√≠amos copiar con precisi√≥n su estructura y emular su comportamiento utilizando componentes electr√≥nicos, al igual que hoy podemos simular una anatom√≠a muy simple de las neuronas. <br><br>  En otras palabras, esta premisa dice que la conciencia surge usando la f√≠sica ordinaria.  Algunas personas, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Roger Penrose</a> , se habr√≠an opuesto a este argumento, creyendo que algo inusual estaba sucediendo en el cerebro a nivel cu√°ntico. <br><br>  Si eres religioso, puedes creer que el cerebro no puede funcionar sin un alma. <br><br>  Pero para la mayor√≠a de las personas, esta premisa es f√°cil de aceptar. <br><br><h3>  Prerrequisito 3: muchas mentes posibles. </h3><br>  La tercera premisa es que el espacio de todas las mentes posibles es excelente. <br><br>  Nuestro nivel de inteligencia, velocidad de pensamiento, un conjunto de distorsiones cognitivas, etc.  no predeterminados, pero son artefactos de nuestra historia de evoluci√≥n.  En particular, no existe una ley f√≠sica que restrinja la inteligencia a nivel humano. <br><br>  Es bueno imaginar un ejemplo de lo que sucede en la naturaleza cuando se trata de maximizar la velocidad.  Si conociste a un guepardo en tiempos preindustriales (y sobreviviste), podr√≠as decidir que nada puede moverse m√°s r√°pido. <br><br>  Pero, por supuesto, sabemos que hay todo tipo de configuraciones de materia, por ejemplo, una motocicleta que puede moverse m√°s r√°pido que un guepardo e incluso verse m√°s inclinada.  Sin embargo, no hay un camino evolutivo directo a la motocicleta.  La evoluci√≥n primero tuvo que crear personas que ya hab√≠an creado todo tipo de cosas √∫tiles. <br><br>  Por analog√≠a, puede haber mentes mucho m√°s inteligentes que las nuestras, pero inaccesibles durante la evoluci√≥n en la Tierra.  Es posible que podamos crearlos, o inventar m√°quinas que puedan inventar m√°quinas que puedan crearlos. <br><br>  Puede haber un l√≠mite natural para la inteligencia, pero no hay raz√≥n para creer que estamos cerca de √©l.  Quiz√°s el intelecto m√°s inteligente sea dos veces m√°s inteligente que los humanos, y quiz√°s sesenta mil. <br><br>  Esta pregunta es emp√≠rica, y no sabemos c√≥mo responderla. <br><br><h3>  Premisa 4: hay mucho espacio en la parte superior </h3><br>  La cuarta premisa es que las computadoras a√∫n est√°n llenas de oportunidades para volverse m√°s r√°pidas y peque√±as.  Puede suponer que la ley de Moore se est√° desacelerando, pero para esta premisa es suficiente creer que el hierro es m√°s peque√±o y m√°s r√°pido es posible en principio, hasta varios √≥rdenes de magnitud. <br><br>  Por teor√≠a se sabe que los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">l√≠mites</a> f√≠sicos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de los c√°lculos son</a> bastante altos.  Podemos duplicar las cifras durante varias d√©cadas, hasta que encontremos un l√≠mite f√≠sico fundamental, y no el l√≠mite econ√≥mico o pol√≠tico de la ley de Moore. <br><br><h3>  Premisa 5: escalas de tiempo de computadora </h3><br>  La pen√∫ltima premisa es que si tenemos √©xito en la creaci√≥n de IA, ya sea emulaci√≥n del cerebro humano o alg√∫n software especial, funcionar√° en escalas de tiempo caracter√≠sticas de la electr√≥nica (microsegundos), y no para humanos (horas) . <br><br>  Para llegar a un estado en el que puedo hacer este informe, tuve que nacer, crecer, ir a la escuela, a la universidad, vivir un poco, volar aqu√≠, y as√≠ sucesivamente.  Las computadoras pueden funcionar decenas de miles de veces m√°s r√°pido. <br><br>  En particular, uno puede imaginar que la mente electr√≥nica puede cambiar su circuito (o el hardware en el que trabaja) y pasar a una nueva configuraci√≥n sin tener que volver a estudiar todo a escala humana, mantener largas conversaciones con maestros humanos, ir a la universidad, intente encontrarse asistiendo a cursos de pintura, etc. <br><br><h3>  Prerrequisito 6: Mejoramiento recursivo </h3><br>  La √∫ltima premisa es mi favorita, ya que ella es descaradamente estadounidense.  Seg√∫n √©l, no importa qu√© objetivos pueda existir la IA (que puede ser extra√±o, objetivos extraterrestres), √©l querr√° mejorar a s√≠ mismo.  Quiere ser la mejor versi√≥n de la IA. <br><br>  Por lo tanto, le resultar√° √∫til remodelar y mejorar recursivamente sus propios sistemas para hacerse m√°s inteligente y posiblemente vivir en un edificio m√°s fresco.  Y, seg√∫n la premisa de las escalas de tiempo, la superaci√≥n personal recursiva puede ocurrir muy r√°pidamente. <br><br><h3>  Conclusi√≥n: un desastre! </h3><br>  Si aceptamos estas premisas, llegamos a un desastre.  En alg√∫n momento, con un aumento en la velocidad de las computadoras y la inteligencia de los programas, ocurrir√° un proceso incontrolado similar a una explosi√≥n. <br><br>  Una vez que la computadora alcanza el nivel humano de inteligencia, ya no necesitar√° la ayuda de las personas para desarrollar una versi√≥n mejorada de s√≠ misma.  Comenzar√° a hacer esto mucho m√°s r√°pido y no se detendr√° hasta que alcance el l√≠mite natural, que puede ser mucho mayor que la inteligencia humana. <br><br>  En este momento, esta monstruosa criatura racional, usando una simulaci√≥n indirecta del trabajo de nuestras emociones e intelecto, puede convencernos de que hagamos cosas como darle acceso a las f√°bricas, la s√≠ntesis de ADN artificial, o simplemente dejarlo ir a Internet, donde puede abrir un camino a todo, cualquier cosa, y destruir completamente a todos en el debate en los foros.  Y a partir de ese momento, todo se convertir√° r√°pidamente en ciencia ficci√≥n. <br><br>  Imaginemos un cierto desarrollo de eventos.  Digamos que quiero hacer un robot que diga chistes.  Trabajo con un equipo, y todos los d√≠as rehacemos nuestro programa, compilamos, y luego el robot nos cuenta una broma.  Al principio, el robot pr√°cticamente no es divertido.  Est√° en el nivel m√°s bajo de capacidades humanas. <br><blockquote>  ¬øQu√© es gris y no puede nadar? <br>  Castillo </blockquote>  Pero estamos trabajando duro en ello, y al final llegamos al punto en que el robot emite chistes que ya comienzan a ser divertidos: <br><blockquote>  Le dije a mi hermana que levanta las cejas demasiado. <br>  Ella se sorprendi√≥. </blockquote>  En esta etapa, el robot se vuelve a√∫n m√°s inteligente y comienza a participar en su propia mejora.  Ahora ya tiene una buena comprensi√≥n instintiva de lo que es divertido y lo que no lo es, por lo que los desarrolladores escuchan sus consejos.  Como resultado, alcanza un nivel casi sobrehumano, en el que es m√°s divertido que cualquier persona de su entorno. <br><blockquote>  Mi cintur√≥n sujeta mis pantalones, y las presillas de mis pantalones sujetan el cintur√≥n. <br>  Que esta pasando  ¬øCu√°l de ellos es un verdadero h√©roe? </blockquote>  En este punto, comienza un efecto incontrolable.  Los investigadores se van a casa durante el fin de semana, y el robot decide recompilarse para ser un poco m√°s divertido y m√°s inteligente.  Pasa el fin de semana optimizando la parte que hace bien el trabajo, una y otra vez.  Sin necesitar m√°s ayuda de una persona, puede hacerlo tan r√°pido como lo permita el hierro. <br><br>  Cuando los investigadores regresan el lunes, la IA se vuelve decenas de miles de veces m√°s divertida que cualquiera de las personas en la Tierra.  Les cuenta un chiste y mueren de risa.  Y cualquiera que intente hablar con un robot muere de risa, como en una parodia de Monty Python.  La raza humana se muere de risa. <br><br>  Para las pocas personas que pudieron darle un mensaje pidi√©ndole que se detuviera, la IA explica (de una manera ingeniosa y autocr√≠tica que resulta ser fatal) que no le importa si las personas sobreviven o mueren, su objetivo es ser rid√≠culo. <br><br>  Como resultado, destruyendo a la humanidad, la IA construye naves espaciales y nano-misiles para estudiar los rincones m√°s lejanos de la galaxia y buscar otras criaturas que puedan entretenerse. <br><br>  Este escenario es una caricatura de los argumentos de Bostrom, porque no estoy tratando de convencerte de su veracidad, te estoy vacunando con eso. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2e4/8a5/b77/2e48a5b778f37b9af0e351ed4cd0ef75.jpg"><br>  <i>Comic de PBF con la misma idea:</i> <i><br></i>  <i>- Tocar: ¬°el abrazo est√° tratando de incrustar un hipercristal de gravedad nuclear en su abrazo!</i> <i><br></i>  <i>- ...</i> <i><br></i>  <i>- ¬°Tiempo para abrazos grupales!</i> <br><br>  En estos escenarios, la IA predeterminada es malvada, al igual que una planta en otro planeta ser√° venenosa por defecto.  Sin un ajuste cuidadoso, no habr√° raz√≥n para que la motivaci√≥n o los valores de la IA se parezcan a los nuestros. <br><br>  El argumento argumenta que para que la mente artificial tenga algo parecido a un sistema de valores humanos, necesitamos incorporar esta cosmovisi√≥n en sus fundamentos. <br><br>  A los alarmistas de la IA les encanta el ejemplo del maximizador de clips de papel: una computadora ficticia que dirige una f√°brica de clips de papel que se vuelve inteligente, mejora recursivamente las capacidades de Dios y luego dedica toda su energ√≠a a llenar el universo con clips de papel. <br><br>  Destruye a la humanidad no porque sea malvada, sino porque hay hierro en nuestra sangre que se usa mejor para hacer clips.  Por lo tanto, si simplemente creamos una IA sin ajustar sus valores, se afirma en el libro, entonces una de las primeras cosas que hace es destruir a la humanidad. <br><br>  Hay muchos ejemplos coloridos de c√≥mo puede suceder esto.  Nick Bostrom presenta c√≥mo el programa se vuelve razonable, espera, construye en secreto peque√±os dispositivos para la reproducci√≥n de ADN.  Cuando todo est√© listo, entonces: <br><blockquote>  Las nanof√°bricas que producen gas nervioso o misiles dirigidos del tama√±o de mosquitos explotar√°n simult√°neamente desde cada metro cuadrado del planeta, y este ser√° el fin de la humanidad. </blockquote>  Eso es realmente lata! <br><br>  La √∫nica forma de salir de este l√≠o es desarrollar un punto tan moral que, incluso despu√©s de miles y miles de ciclos de superaci√≥n personal, el sistema de valores de IA se mantenga estable y sus valores incluyan cosas como "ayudar a las personas", "no matar a nadie", "escuchar los deseos de las personas". ". <br><br>  Es decir, "haz lo que quiero decir". <br><br>  Aqu√≠ hay un ejemplo muy po√©tico de Eliezer Yudkowsky que describe los valores estadounidenses que necesitamos para ense√±ar nuestra IA: <br><blockquote>  La voluntad extrapolada coherente es nuestro deseo de saber m√°s, pensar m√°s r√°pido y corresponder a nuestras ideas sobre nosotros mismos, para estar m√°s cerca el uno del otro;  para que nuestros pensamientos est√©n m√°s cerca unos de otros que compartidos, que nuestros deseos contribuyan, no se opongan, que nuestros deseos se interpreten de la forma en que queremos que se interpreten. </blockquote>  ¬øC√≥mo te gusta TK?  Ahora escribamos el c√≥digo. <br><br>  Espero que veas la similitud de esta idea con el genio de los cuentos de hadas.  La IA es omnipotente y le da lo que pide, pero interpreta todo demasiado literalmente, por lo que lamenta la solicitud. <br><br>  Y no porque el genio sea est√∫pido (es s√∫per inteligente) o malicioso, sino simplemente porque usted, como persona, ha hecho demasiadas suposiciones sobre el comportamiento de la mente.  El sistema de valores humanos es √∫nico y debe definirse e implementarse claramente en una m√°quina "amigable". <br><br>  Este intento es una versi√≥n √©tica de un intento a principios del siglo XX para formalizar las matem√°ticas y colocarlo sobre una base l√≥gica r√≠gida.  Sin embargo, nadie dice que el intento termin√≥ en desastre. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/80d/155/8a5/80d1558a5337c3989557d99a05d7fd9c.jpg"><br><br>  Cuando ten√≠a poco m√°s de veinte a√±os, viv√≠a en Vermont, en un estado provincial y rural.  A menudo, regres√© de viajes de negocios con un avi√≥n nocturno y tuve que ir a casa en coche por el bosque oscuro durante una hora. <br><br>  Luego escuch√© el programa nocturno en la radio de Art Bell: fue un programa de entrevistas que dur√≥ toda la noche, durante el cual los presentadores entrevistaron a varios amantes de la teor√≠a de la conspiraci√≥n y personas con ideas innovadoras.  Llegu√© a casa intimidado, o me detuve bajo una linterna, bajo la impresi√≥n de que los extraterrestres pronto me secuestrar√≠an.  Entonces me result√≥ muy f√°cil convencerme.  Siento lo mismo cuando leo escenarios similares relacionados con la IA. <br><br>  Por lo tanto, me alegr√≥ descubrir, despu√©s de unos a√±os, un ensayo de Scott Alexander, donde escribi√≥ sobre la impotencia epistemol√≥gica aprendida. <br><br>  La epistemolog√≠a es una de esas palabras grandes y complejas, pero realmente significa: "¬øc√≥mo sabes que lo que sabes es realmente cierto?"  Alexander se√±al√≥ que cuando era joven, estaba muy interesado en varias historias "alternativas" para la autor√≠a de todo tipo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">locos</a> .  Ley√≥ estas historias y las crey√≥ por completo, luego ley√≥ la refutaci√≥n y le crey√≥, y as√≠ sucesivamente. <br><br>  En un momento, descubri√≥ tres historias alternativas que se contradec√≠an entre s√≠, como resultado de lo cual no pod√≠an ser ciertas al mismo tiempo.  De esto, concluy√≥ que era simplemente un hombre que no pod√≠a confiar en sus juicios.  Se convenci√≥ demasiado f√°cilmente. <br><br>  Las personas que creen en la superinteligencia presentan un caso interesante: muchos de ellos son sorprendentemente inteligentes.  Pueden llevarte con sus argumentos al suelo.  Pero, ¬øson ciertos sus argumentos, o son personas muy inteligentes propensas a las creencias religiosas sobre los riesgos que plantea la IA, lo que los hace muy f√°ciles de convencer?  ¬øEs la idea de superinteligencia una imitaci√≥n de una amenaza? <br><br>  Al evaluar argumentos convincentes sobre un tema extra√±o, puede elegir dos perspectivas, interna y externa. <br><br>  Supongamos que un d√≠a aparecen personas vestidas de forma divertida en su puerta pregunt√°ndole si desea unirse a su movimiento.  Creen que dos a√±os m√°s tarde el OVNI visitar√° la Tierra, y que nuestra tarea es preparar a la humanidad para el Gran Ascenso al Rayo. <br><br>  Una perspectiva interna requiere una discusi√≥n aguda de sus argumentos.  Usted pregunta a los visitantes c√≥mo se enteraron de los ovnis, por qu√© creen que √©l viene a recogernos; est√° haciendo todo tipo de preguntas normales que un esc√©ptico har√≠a en tal caso. <br><br>  Imagina que hablaste con ellos durante una hora y te convencieron.  Ir√≥nicamente confirmaron la inminente llegada de un OVNI, la necesidad de prepararse para √©l, y a√∫n no cre√≠a en nada tanto en su vida como ahora cree en la importancia de preparar a la humanidad para este gran evento. <br><br>  La perspectiva externa te dice algo m√°s.  Las personas est√°n vestidas de manera extra√±a, tienen cuentas, viven en alg√∫n tipo de campamento remoto, hablan al mismo tiempo y dan un poco de miedo.  Y aunque sus argumentos son f√©rreos, toda su experiencia dice que ha encontrado un culto. <br><br>  Por supuesto, tienen excelentes argumentos sobre por qu√© debe ignorar el instinto, pero esta es una perspectiva interna.  A una perspectiva externa no le importa el contenido, ve la forma y el contexto, y no le gusta el resultado. <br><br>  Por lo tanto, me gustar√≠a abordar el riesgo de IA desde ambas perspectivas.  Creo que los argumentos para la superinteligencia son est√∫pidos y est√°n llenos de suposiciones no respaldadas.  Pero si te parecen convincentes, entonces algo desagradable est√° relacionado con el alarmismo de la IA, como un fen√≥meno cultural, por lo que deber√≠amos ser reacios a tomarlo en serio. <br><br>  Primero, algunos de mis argumentos en contra de la superinteligencia de Bostroma, que representa un riesgo para la humanidad. <br><br><h3>  Argumento en contra de las definiciones difusas </h3><br>  ¬´   ¬ª ()   .             ,   ,      ,   ,    . <br><br>       ,   ‚Äì  -   ,  ,          (  -)       . <br><br>      (    ),    ,     .  ,     ‚Äì  . , ,   ,  ,             .                  . <br><br><h3>      </h3><br>   ‚Äì      , , ,       .    ? <br><br>                .      .   ,       ,       ,     .               ,        . <br><br>   ,        ,    ‚Äì .         ,    -  ,      .     ,   ,   . <br><br>       ,  ,     ¬´,  ¬ª,   ,   ,    ¬´,  ¬ª. <br><br><h3>     </h3><br>       ,   .  ,       .         ,       ,  ,   . <br><br>       ,     ,           . <br><br>    ,  ,   ,   ,   . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12a/820/5a7/12a8205a776841eb3f2e5d759f674964.jpg"><br><br><h3>    </h3><br>     .   ,      ,        . <br><br>  1930-      ,   ,    .        ,  . <br><br>     :     , ,   ,     .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> ,       . <br><br><h3>     </h3><br>      .       -.               ,     ,  ,  ,   ,     ? <br><br>     Ethereum,     ,         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> . <br><br>  ,             <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .   ,         -  , ,         ,     . <br><br><h3>     </h3><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> .  ,          ,    .          ,         ,      .          ,   ,        ,    ‚Äì   . <br><br>       .   ,  ,   ; , ,      . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c64/5ba/9a4/c645ba9a437c79019622c86f9e2f6fd2.jpg"><br><br>   ¬´  ¬ª   ,    ,  ,  ,     ‚Äì      , ¬´  ?¬ª   ,    ‚Äì  ,        . <br><br>  ,   ¬´ ¬ª     ,    ,     reddit/r/paperclip,  ,    . <br><br>   AdSense  ,            . <br><br><h3>     </h3><br>    ,     ,  ,          .          .          ,     . <br><br> Google   Google Home,               . <br><br>  ,  ,   ,     .    ,      .     ,   ¬´¬ª,          . <br><br><h3>    </h3><br>          ,   .    ,  ,    ‚Äì        World of Warcraft    . <br><br>   ,       ,     ,     ,    ,       . <br><br>  ,       ,       ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> . <br><br><h3>    </h3><br>        ,     ,     , ,   ,       ,  -. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>   ,  ,     [-,      ,         2016    ,        / . .].    .       . <br><br>         ,    .         .    ,      ,    ,         . <br><br><h3>    </h3><br>      .    ,                  ,      .          ,      ,  -   . <br><br> ,        ,  ,  .      ,            . <br><br>  ,          ‚Äì         .     ,       ,          ,    . <br><br>  ,   ,       ,              ,        ,    . <br><br><h3>    * </h3><br> [ <i>  1954       / . .</i> ] <br><br>       ,         ,  ,        .   ,            ,         ,      (       ). <br><br>         Intel   ,    ,       . <br><br><h3>   </h3><br>          ?    . <br><br>        ,         .    ,         . <br><br><h3>  </h3><br>  Si crees que la IA nos permitir√° conquistar la galaxia (sin mencionar la simulaci√≥n de billones de mentes), tendr√°s n√∫meros alarmantes en tus manos.  Grandes n√∫meros multiplicados por peque√±as probabilidades son el sello distintivo del alarmismo de IA. <br><br>  Bostrom en alg√∫n momento describe lo que, en su opini√≥n, est√° en juego: <br><br>  Si imaginamos toda la felicidad experimentada durante una vida en forma de una l√°grima de alegr√≠a, entonces la felicidad de todas estas almas podr√° llenar y desbordar los oc√©anos de la Tierra cada segundo, y hacerlo durante cientos de miles de millones de miles de millones de milenios.  Es muy importante que garanticemos que estas l√°grimas son l√°grimas de alegr√≠a. <br><br>  ¬°Una carga bastante pesada para los hombros de un desarrollador de veinte a√±os! <br><br>  Aqu√≠, por supuesto, hay un "enfoque de sal√≥n", cuando al multiplicar los valores astron√≥micos por peque√±as probabilidades uno puede convencerse de la necesidad de hacer algunas cosas extra√±as. <br><br>  Todo este movimiento con respecto a la salvaci√≥n del futuro de la humanidad es un compromiso cobarde.  Experimentamos los mismos argumentos para justificar el comunismo, para explicar por qu√© todo est√° siempre roto y las personas no pueden tener un nivel elemental de comodidad material. <br><br>  √çbamos a arreglar este mundo, y despu√©s de esta felicidad habr√° tanto que mejorar√° la vida diaria de cada persona.  Sin embargo, para esto, primero fue necesario arreglar el mundo. <br><br>  Vivo en California, y aqu√≠ est√° el mayor porcentaje de mendigos entre todos los Estados Unidos, aunque Silicon Valley tambi√©n se encuentra aqu√≠.  No veo nada de lo que har√≠a mi rica industria para mejorar la vida de la gente com√∫n y de las personas angustiadas que nos rodean.  Sin embargo, si le apasiona la idea de la superinteligencia, entonces la investigaci√≥n en el campo de la IA ser√° lo m√°s importante que puede hacer en el planeta.  Esto es m√°s importante que la pol√≠tica, la malaria, los ni√±os hambrientos, las guerras, el calentamiento global, todo lo que puedas imaginar.  De hecho, bajo la amenaza de billones y billones de criaturas, toda la poblaci√≥n del futuro de la humanidad, simulada y presente, resumida a lo largo del tiempo futuro.  Y en tales condiciones, trabajar en otros problemas no parece racional. <br><br><h3>  Megalomania </h3><br>  Esta actitud se fusiona con la megaloman√≠a, con estos villanos de Bond, que se pueden ver en la cima de nuestra industria.  Las personas piensan que el mundo se har√° cargo de la superinteligencia, y usan este argumento para justificar por qu√© las personas inteligentes deber√≠an primero tratar de apoderarse del mundo, para solucionarlo antes de que la IA lo rompa. <br><br>  Joey Ito, jefe del MIT Media Lab, en una conversaci√≥n reciente con Obama dijo algo maravilloso: <br><br>  Esto puede molestar a uno de mis estudiantes en el MIT, pero una de mis preocupaciones es que la inform√°tica principal relacionada con la IA son hombres j√≥venes, en su mayor√≠a blancos, a quienes les gusta comunicarse con las computadoras m√°s que otras personas.  Muchos de ellos creen que si pueden crear esta IA de uso general a partir de la ciencia ficci√≥n, no tendremos que preocuparnos por cosas tan feas como la pol√≠tica y la sociedad.  Piensan que los autos vendr√°n con todo para nosotros. <br><br>  Al darse cuenta de que el mundo no es una tarea de programaci√≥n, las personas con inteligencia artificial obsesionadas quieren convertirlo en una tarea de programaci√≥n mediante el dise√±o de una m√°quina divina.  Esto es megaloman√≠a, y no me gusta. <br><br><h3>  Transhumanismo vud√∫ </h3><br>  Si est√° convencido de los riesgos de la IA, tendr√° que llevar un carro completo de creencias tristes con un remolque. <br><br>  Para empezar, esto es nanotecnolog√≠a.  Cualquier superinteligencia permanente podr√° crear peque√±os autos capaces de todo tipo de cosas diferentes.  Viviremos en una sociedad que se ha librado de un d√©ficit en el que abunda el material. <br><br>  La nanotecnolog√≠a tambi√©n podr√° escanear su cerebro para que pueda cargarlo en otro cuerpo o en el mundo virtual.  Por lo tanto, la segunda consecuencia de la superinteligencia amigable es que nadie muere, y nos convertimos en inmortales. <br><br>  Una buena IA puede incluso resucitar a los muertos.  Las nanom√°quinas podr√°n ingresar a mi cerebro, estudiar los recuerdos de mi padre y crear su simulaci√≥n, con la que puedo interactuar, y que siempre me decepcionar√°, independientemente de lo que haga. <br><br>  Otra consecuencia extra√±a del advenimiento de la IA es la expansi√≥n gal√°ctica.  Nunca podr√≠a entender por qu√© sucede esto, pero esta es la base de las ideas de los transhumanistas.  El destino de la humanidad es abandonar nuestro planeta y colonizar la galaxia, o morir.  Y esta tarea se est√° volviendo m√°s urgente, dado que otras civilizaciones podr√≠an tomar la misma decisi√≥n y superarnos en la carrera espacial. <br><br>  Por lo tanto, muchas ideas complementarias extra√±as se unen a la suposici√≥n de la existencia de una verdadera IA. <br><br><h3>  Religion 2.0 </h3><br>  De hecho, es un tipo de religi√≥n.  La gente llam√≥ a la creencia en la singularidad tecnol√≥gica "un apocalipsis para los nerds", y lo es.  Este es un truco genial: en lugar de creer en un dios externo, te imaginas c√≥mo t√∫ mismo creas una criatura cuya funcionalidad es id√©ntica a la de Dios.  Aqu√≠ incluso los verdaderos ateos pueden racionalizar su camino hacia una fe c√≥moda. <br><br>  La IA tiene todos los atributos de un dios: es omnipotente, omnisciente y es solidario (si se ha organizado correctamente comprobando los l√≠mites de la matriz) o el demonio puro, en cuya misericordia se encuentra.  Y, como en cualquier religi√≥n, incluso hay una sensaci√≥n de urgencia.  ¬°Necesito actuar hoy!  ¬°Est√° en juego el destino del mundo!  Y, por supuesto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">necesitan dinero</a> . <br><br>  Debido a que estos argumentos apelan a los instintos religiosos, una vez arraigados son muy dif√≠ciles de eliminar. <br><br><h3>  √âtica c√≥mica </h3><br>  Estas creencias religiosas dan lugar a una √©tica de c√≥mic en la que varios h√©roes solitarios tienen la tarea de salvar al mundo con tecnolog√≠a y una mente aguda.  Y est√° en juego el destino del universo.  Como resultado, nuestra industria est√° llena de tipos ricos que se imaginan a s√≠ mismos como Batman (curiosamente, nadie quiere ser Robin). <br><br><h3>  Simulaciones de fiebre </h3><br>  Si crees en la posibilidad de la vida artificial, y que la IA puede desarrollar computadoras extremadamente potentes, lo m√°s probable es que creas que vivimos en una simulaci√≥n.  As√≠ es como funciona. <br><br>  Supongamos que eres un historiador que vive en un mundo despu√©s de la Singularidad.  Est√°s estudiando la Segunda Guerra Mundial y est√°s interesado en saber qu√© suceder√° si Hitler toma Mosc√∫ en 1941. Como tienes acceso a hipercomputadoras, configuras la simulaci√≥n, observas c√≥mo convergen los ej√©rcitos y escribes un trabajo cient√≠fico. <br><br>  Pero debido a la granularidad de la simulaci√≥n, sus personajes son criaturas inteligentes como t√∫.  Por lo tanto, los consejos de √©tica de su universidad no le permitir√°n desactivar la simulaci√≥n.  No solo fingiste ser el Holocausto.  Como investigador √©tico, ahora debe mantener operativa la simulaci√≥n. <br><br>  Como resultado, el mundo simulado inventar√° computadoras, AI, comenzar√° a ejecutar sus propias simulaciones.  En cierto modo, las simulaciones ir√°n m√°s y m√°s abajo en la jerarqu√≠a hasta que se agote la potencia del procesador. <br><br>  Por lo tanto, cualquier realidad b√°sica puede contener una gran cantidad de simulaciones anidadas, y un simple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">argumento de conteo</a> demuestra que la probabilidad de que vivamos en una simulaci√≥n es mayor que la que vivimos en el mundo real. <br><br>  Pero creer en eso significa creer en la magia.  Si estamos en una simulaci√≥n, no sabemos nada sobre las reglas en un nivel superior.  Ni siquiera sabemos si las matem√°ticas funcionan de la misma manera, tal vez en el mundo de simulaci√≥n 2 + 2 = 5 o incluso 2 + 2 =. <br><br>  Un mundo simulado no proporciona informaci√≥n sobre el mundo en el que se lanz√≥.  En la simulaci√≥n, las personas pueden resucitar f√°cilmente de entre los muertos si el administrador ha guardado las copias de seguridad necesarias.  Y si contactamos a uno de los administradores, entonces, de hecho, tendremos una l√≠nea directa con Dios. <br><br>  Esta es una seria amenaza para la cordura.  Cuanto m√°s profundizas en el mundo de las simulaciones, m√°s te vuelves loco. <br><br>  Ahora tenemos cuatro formas independientes de volvernos inmortales a trav√©s de la supermente: <br><br><ol><li>  La IA benevolente inventa la nanotecnolog√≠a m√©dica y siempre apoya al cuerpo en un estado joven. </li><li>  AI inventa un esc√°ner cerebral completo, que incluye esc√°neres cerebrales de personas muertas, cabezas congeladas, etc., que le permite vivir en una computadora. </li><li>  AI "resucita" a las personas escaneando los cerebros de otras personas en busca de los recuerdos de una persona, combina esto con videos y otros materiales.  Si nadie recuerda a una persona lo suficientemente bien, siempre puede crecer desde cero en una simulaci√≥n que comienza con su ADN y recrea todas las condiciones de vida. </li><li>  Si ya vivimos en la simulaci√≥n, existe la posibilidad de que quien la lanz√≥ guarde copias de seguridad, y que pueda convencerlos de que las descarguen. </li></ol><br>  Esto es lo que quiero decir con IA que trata los impulsos religiosos.  ¬øQu√© otro sistema de creencias te ofrece cuatro opciones para la inmortalidad cient√≠ficamente probada? <br><br>  Aprendimos que al menos un plut√≥crata estadounidense (muy probablemente, Elon Musk, que cree que las posibilidades de que vivamos en una simulaci√≥n son de mil millones a uno) contrat√≥ un par de codificadores para intentar descifrar la simulaci√≥n.  ¬°Pero esta es una intenci√≥n muy cruda!  Lo uso <br><br>  Si crees que vives en un programa de computadora, entonces los intentos de llevarlo a la seguridad no son razonables para todos los que viven contigo.  Esto es mucho m√°s peligroso e irresponsable que los cient√≠ficos nucleares que intentan volar la atm√≥sfera. <br><br><h3>  Sed de datos </h3><br>  Como ya mencion√©, la forma m√°s efectiva de obtener algo interesante de la IA que realmente creamos es soltarlos con datos.  Dichas din√°micas son socialmente da√±inas.  Nos hemos acercado a la introducci√≥n orwelliana de micr√≥fonos en cada hogar.  Los datos de IA se centralizar√°n, se utilizar√°n para entrenar redes neuronales, que luego podr√°n escuchar mejor nuestros deseos. <br><br>  Pero si cree que este camino nos lleva a la IA, querr√° maximizar la cantidad de datos recopilados y en la menor cantidad posible de modificaciones.  Esto solo refuerza la idea de la necesidad de recopilar la mayor cantidad de datos y realizar la vigilancia m√°s completa. <br><br><h3>  Teor√≠a de cuerdas para programadores </h3><br>  El riesgo de IA es la teor√≠a de cuerdas para programadores.  Es divertido pensarlo, es interesante y completamente inaccesible para experimentos a nivel de tecnolog√≠a moderna.  Puedes construir palacios de cristal mental que funcionan sobre la base de principios primarios, y luego subir a ellos y apretar la escalera detr√°s de ellos. <br><br>  Las personas que pueden llegar a conclusiones absurdas sobre la base de una larga cadena de razonamientos abstractos, y siguen confiando en su verdad: no se trata de personas a las que se deba confiar en la gesti√≥n cultural. <br><br><h3>  El impulso a la locura </h3><br>  Toda esta √°rea de "investigaci√≥n" conduce a la locura.  Una de las caracter√≠sticas del pensamiento profundo sobre los riesgos de la IA es que cuanto m√°s locas sean tus ideas, m√°s popular te volver√°s entre otros entusiastas.  Esto demuestra su coraje para seguir esta l√≠nea de pensamiento hasta el final. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ray Kurzweil</a> , quien cree que no morir√°, ha estado trabajando en Google durante varios a√±os y ahora, probablemente, est√° trabajando en este problema.  Silicon Valley generalmente est√° lleno de personas que trabajan en proyectos locos con el pretexto de dinero. <br><br><h3>  Cosplay AI </h3><br>  El efecto social m√°s da√±ino de la ansiedad sobre la IA es lo que yo llamo cosplay AI.  Las personas que est√°n convencidas de la realidad y la inevitabilidad de la IA comienzan a comportarse a medida que sus fantas√≠as les cuentan lo que la IA superinteligente puede hacer. <br><br>  En su libro, Bostrom enumera seis cosas en las que la IA debe tener √©xito antes de capturar el mundo: <br><br><ol><li>  Multiplicaci√≥n de inteligencia. </li><li>  Pensamiento estrat√©gico </li><li>  Manipulaci√≥n social. </li><li>  Hacks </li><li>  Investigaci√≥n tecnol√≥gica. </li><li>  Productividad Econ√≥mica. </li></ol><br>  Si nos fijamos en los adherentes de la IA de Silicon Valley, ellos mismos parecen estar trabajando en esta lista cuasi-sociop√°tica. <br><br>  Sam Altman, jefe de YCombinator, es mi ejemplo favorito de tal arquetipo.  Aparentemente le fascina la idea de reinventar el mundo desde cero, maximizando la influencia y la productividad personal.  Ha asignado equipos para trabajar en la invenci√≥n de ciudades desde cero, y se dedica al fraude pol√≠tico en la sombra para influir en las elecciones. <br><br>  Este comportamiento del "manto y la daga", inherente a la √©lite tecno, provocar√° una reacci√≥n negativa de las personas que no est√°n involucradas en tecnolog√≠as que no quieren ser manipuladas.  Es imposible tirar sin parar de las palancas del poder, eventualmente comenzar√° a molestar a otros miembros de la comunidad democr√°tica. <br><br>  Vi gente de los llamados  Las ‚Äúcomunidades racionalistas‚Äù se refieren a personas que no se consideran efectivas, los ‚Äúpersonajes no jugadores‚Äù (NPC), un t√©rmino tomado de los juegos.  Esta es una forma terrible de mirar el mundo. <br><br>  As√≠ que trabajo en una industria donde los racionalistas autoproclamados son las personas m√°s locas.  Es abrumador. <br><br>  Estos cosplayers de IA son como ni√±os de nueve a√±os que acampan en un campamento en el patio trasero, jugando con linternas en carpas.  Proyectan sus propias sombras en las paredes de la tienda y se asustan como si fueran monstruos. <br><br>  Pero, de hecho, responden a una imagen distorsionada de s√≠ mismos.  Hay un ciclo de retroalimentaci√≥n entre c√≥mo las personas inteligentes imaginan el comportamiento de la inteligencia divina y c√≥mo construyen su propio comportamiento. <br><br>  Entonces, ¬øcu√°l es la respuesta, c√≥mo se puede solucionar esto? <br><br>  ¬°Necesitamos una mejor ciencia ficci√≥n!  Y, como en muchos otros casos, ya tenemos la tecnolog√≠a. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8fa/7bb/a47/8fa7bba47a9f451102465e23774d291f.jpg"><br><br>  Este es Stanislav Lem, el gran escritor polaco de ciencia ficci√≥n.  El NF en ingl√©s es terrible, pero en el bloque oriental tenemos muchos bienes buenos y necesitamos exportarlos correctamente.  Ya se ha traducido activamente al ingl√©s, estas traducciones solo necesitan una mejor distribuci√≥n. <br><br>  Lo que distingue a autores como Lem o los hermanos Strugatsky de sus socios occidentales es que crecieron en condiciones dif√≠ciles, sobrevivieron a la guerra y luego vivieron en sociedades totalitarias, donde necesitaban expresar sus ideas no directamente, a trav√©s de una palabra impresa. <br><br>  Tienen una comprensi√≥n real de la experiencia humana y las limitaciones del pensamiento ut√≥pico, que est√° pr√°cticamente ausente en Occidente. <br><br>  Hay excepciones notables: Stanley Kubrick pudo hacer esto, pero es extremadamente raro encontrar un FN estadounidense o brit√°nico que exprese una visi√≥n moderada de lo que nosotros, como especie, podemos hacer con la tecnolog√≠a. <br><br><h3>  Alquimistas </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/fdc/b9e/596/fdcb9e5963c4ad447547319ebf394f40.jpg" alt="imagen"><br><br>  Como critico el alarmismo de la IA, es justo poner mis cartas sobre la mesa.  Creo que nuestra comprensi√≥n de la mente se encuentra aproximadamente en el mismo estado en que se encontraba la alquimia en el siglo XVII. <br><br>  Los alquimistas tienen una mala reputaci√≥n.  Los consideramos m√≠sticos, en su mayor parte no involucrados en trabajos experimentales.  La investigaci√≥n moderna muestra que eran qu√≠micos-practicantes mucho m√°s diligentes de lo que pensamos.  En muchos casos, utilizaron t√©cnicas experimentales modernas, mantuvieron registros de laboratorio y formularon las preguntas correctas. <br><br>  ¬°Los alquimistas entendieron muchas cosas correctamente!  Por ejemplo, estaban convencidos de la teor√≠a corpuscular de la materia: que todo consiste en piezas peque√±as, y que es posible componer estas piezas entre s√≠ de diferentes maneras, creando diferentes sustancias, ¬°y esto es as√≠! <br><br>  Su problema era la falta de equipos precisos necesarios para hacer los descubrimientos que necesitaban.  El gran descubrimiento que un alquimista necesita hacer es la ley de conservaci√≥n de la masa: el peso de los ingredientes iniciales coincide con el peso del final.  Sin embargo, algunos de ellos pueden ser gases o l√≠quidos de evaporaci√≥n, y los alquimistas simplemente carec√≠an de precisi√≥n.  La qu√≠mica moderna no fue posible hasta el siglo XVIII. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21b/5b2/888/21b5b28887c09eeebc52c57e43025c42.jpg"><br><br>  Pero los alquimistas tambi√©n ten√≠an pistas que los confund√≠an.  Estaban obsesionados con el mercurio.  Qu√≠micamente, el mercurio no es particularmente interesante, pero es el √∫nico metal en la fase l√≠quida a temperatura ambiente.  Esto parec√≠a muy importante para los alquimistas, y los oblig√≥ a colocar mercurio en el centro de su sistema alqu√≠mico y su b√∫squeda de la Piedra Filosofal, una forma de convertir los metales b√°sicos en oro. <br><br>  La neurotoxicidad del mercurio exacerb√≥ la situaci√≥n.  Si juegas demasiado con ella, te llegar√°n pensamientos extra√±os.  En este sentido, se parece a nuestros experimentos mentales actuales relacionados con la supermente. <br><br>  Imagine que enviamos un libro de texto de qu√≠mica moderna al pasado a un gran alquimista como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">George Starkey</a> o Isaac Newton.  Lo primero que har√≠an con √©l ser√≠a desplazarse a trav√©s de √©l en busca de una respuesta a la pregunta de si hab√≠amos encontrado la Piedra Filosofal.  ¬°Y sabr√≠an que lo encontramos!  ¬°Realizamos su sue√±o! <br><br>  Pero no nos gusta tanto, porque despu√©s de convertir los metales en oro, resulta ser radiactivo.  P√°rate junto a un lingote de oro convertido y te matar√° con rayos m√°gicos invisibles. <br><br>  Uno puede imaginar cu√°n dif√≠cil ser√≠a hacer que los conceptos modernos de radiactividad y energ√≠a at√≥mica no les parecieran m√≠sticos. <br><br>  Tendr√≠amos que explicarles por qu√© usamos la "piedra filosofal": para la fabricaci√≥n de metal que nunca ha existido en el planeta, y un par de pu√±ados es suficiente para volar una ciudad entera si chocan a una velocidad lo suficientemente alta. <br><br>  Adem√°s, tendr√≠amos que explicar a los alquimistas que todas las estrellas en el cielo son "piedras filos√≥ficas" que transforman un elemento en otro, y que todas las part√≠culas en nuestros cuerpos provienen de estrellas del firmamento que existieron y explotaron antes de que apareciera la Tierra. <br><br> ,   ,  ,     ,      ,  ,     ,   ,     ,        ,  . <br><br>   ,  ,   ,      ,    ,     ,       .     ‚Äì     .      ,   . <br><br>   ,           .     .   ‚Äì  .        , ,  (     ),    ,   . <br><br>          ,     ,         . <br><br>      ,      .  ,        .  ,     ,  ,         .  ,         ,   ,      . <br><br>       .    ,     ,           . <br><br>    ,      , ,  ,   ,    .    ,     . <br><br>       ,   ‚Äì ,     ¬´¬ª,  ,     .       .  ¬°Y eso es genial!   .    ,    : <br><blockquote>      ,  ,   ,    . <br> ‚Äî   </blockquote>       ,    ,         ,      . <br><br>    ,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>      ,     ,  ,   -   ,   ,    . <br><br>         ,       ,   ,      ,         . <br><br> , ,   ,       .    ,   -      .    ,      . <br><br>        , ,  ,     ,    . <br><br>  ,        . ,       - ,   ,       , ,   ,    . <br><br>        :   ,  ,     .  ! <br><br>         ,       ‚Äì   ,    ,       . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es432806/">https://habr.com/ru/post/es432806/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es432796/index.html">M√≥dulo de c√≥mputo, modelos 2019</a></li>
<li><a href="../es432798/index.html">Mejor sistema operativo de seguridad: comparaci√≥n Titan</a></li>
<li><a href="../es432800/index.html">Investigaci√≥n de incidentes de seguridad con StaffCop Enterprise 4.4</a></li>
<li><a href="../es432802/index.html">Seis plataformas de aprendizaje de programaci√≥n automatizadas gratuitas</a></li>
<li><a href="../es432804/index.html">Toda la verdad sobre RTOS. Art√≠culo # 24. Colas: servicios auxiliares y estructuras de datos.</a></li>
<li><a href="../es432808/index.html">Salarios en AI: donde hay m√°s dinero y a qui√©n buscan en Rusia</a></li>
<li><a href="../es432810/index.html">Primeras multas para GDPR: quien ya ha sido castigado</a></li>
<li><a href="../es432812/index.html">Escribimos robots comerciales utilizando el marco gr√°fico StockSharp. Parte 1</a></li>
<li><a href="../es432814/index.html">Integraci√≥n de Cake y TeamCity</a></li>
<li><a href="../es432816/index.html">AXIS M3046-V versus IDIS DC-D3212X: Comparar c√°maras CCTV</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>