<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐃 🐾 🕒 AI gera sons realistas nas filmagens 👩🏻‍✈️ 💇🏼 ⛹🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Funcionários do Instituto de Ciência da Computação e Inteligência Artificial do Massachusetts (CSAIL) e do Google Research projetaram uma rede neural ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI gera sons realistas nas filmagens</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/395243/"><img src="https://habrastorage.org/files/ffc/d91/c15/ffcd91c151014c829f06083223c2a1ec.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Funcionários </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">do</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Instituto de </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">Ciência da Computação e Inteligência Artificial do</font></a><font style="vertical-align: inherit;"> Massachusetts (CSAIL) e do Google Research projetaram uma rede neural que aprendeu a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tocar seqüências de vídeo arbitrárias</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , gerando sons realistas e prevendo as propriedades dos objetos. </font><font style="vertical-align: inherit;">O programa analisa o vídeo, reconhece objetos, seus movimentos e tipo de contato - choque, deslizamento, atrito e assim por diante. </font><font style="vertical-align: inherit;">Com base nessas informações, gera um som que uma pessoa em 40% dos casos considera mais realista do que o real.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os cientistas sugerem que esse desenvolvimento será amplamente utilizado no cinema e na televisão para gerar efeitos sonoros a partir de uma sequência de vídeo sem som. </font><font style="vertical-align: inherit;">Além disso, pode ser útil para treinar robôs para entender melhor as propriedades do mundo.</font></font><br>
<a name="habracut"></a><br>
<iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://www.youtube.com/embed/0FW99AQmMc8%3Ffeature%3Doembed&amp;usg=ALkJrhgRq7Orzlv7osdIq9tXhXp8lyi0Rg" frameborder="0" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os sons ambientais dizem muito sobre as propriedades dos objetos circundantes; portanto, no processo de auto-aprendizado, os futuros robôs podem agir como crianças - tocar objetos, experimentá-los por toque, enfiar um pedaço de pau neles, tentar se mover, levantar. Nesse caso, o robô recebe feedback, reconhecendo as propriedades do objeto - seu peso, elasticidade e assim por diante. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O som emitido por um objeto em contato também carrega informações importantes sobre as propriedades do objeto. “Quando você desliza o dedo sobre um copo de vinho, o som emitido corresponde à quantidade de líquido derramado no copo”, explica o estudante graduado </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Owens</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , principal autor de um artigo científico publicado, que ainda não está pronto para uma revista científica, mas apenas </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicamente disponível em arXiv.org. A apresentação do trabalho científico ocorrerá na conferência anual sobre visão de máquina e reconhecimento de padrões (CVPR) em Las Vegas neste mês. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os cientistas selecionaram 977 vídeos nos quais as pessoas realizam ações com objetos ao redor, consistindo de vários materiais: arranhe, bata-os com um pau, etc. No total, os vídeos continham 46.577 ações. Os alunos do CSAIL marcaram manualmente todas as ações, indicando o tipo de material, o local de contato, o tipo de ação (choque / arranhão / outro) e o tipo de reação do material ou objeto (deformação, forma estática, movimento rígido, etc.). Vídeos com som foram usados ​​para treinar a rede neural e tags colocadas manualmente foram usadas </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apenas para analisar o</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> resultado do treinamento da rede neural, mas </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">não para treiná-la.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br>
<img src="https://habrastorage.org/files/bc6/750/52a/bc675052a44746f08b569350f7b6481c.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede neural analisou as características do som que correspondem a cada tipo de interação com os objetos - volume, tom e outras características. Durante o treinamento, o sistema estudou o vídeo quadro a quadro, analisou o som nesse quadro e encontrou uma correspondência com o som mais semelhante no banco de dados já acumulado. O mais importante era ensinar a rede neural a esticar o som em quadros. </font></font><br>
<br>
<img src="https://habrastorage.org/files/5bd/97d/f05/5bd97df05e7b4df88695bbc282d8e93a.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A cada novo vídeo, a precisão da previsão de sons aumentava. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O som gerado pela rede neural para diferentes cenas, em comparação com o real</font></font></b><br>
<img src="https://habrastorage.org/files/657/99e/efb/65799eefbb5149d2ad06460f27cdd984.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
, resultando em uma rede neural aprendida a prever com precisão os sons mais diversos com todas as nuances: de bater pedras a farfalhar hera.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"As abordagens atuais dos pesquisadores no campo da inteligência artificial concentram-se em apenas um dos cinco sentidos: especialistas em visão de máquina estudam imagens visuais, especialistas em reconhecimento de fala estudam som e assim por diante", </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diz</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Abhinav Gupta, professor assistente de robótica na Universidade Carnegie - Mellon. "Este estudo é um passo na direção certa que imita o processo de aprendizado da mesma maneira que as pessoas, integrando som e visão". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para testar a eficácia da IA, os cientistas realizaram um estudo on-line no Amazon Mechanical Turk, cujos participantes foram convidados a comparar duas opções para o som de um vídeo em particular e determinar qual som é real e o que não é.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado do experimento, a IA </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">conseguiu enganar as pessoas em 40% dos casos</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . No entanto, de acordo com </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">alguns comentaristas dos fóruns</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , não é tão difícil enganar uma pessoa, porque uma parte significativa do conhecimento sobre a imagem sonora do mundo é obtida pelas pessoas modernas em filmes e jogos de computador. A gama de sons para filmes e jogos é composta por especialistas, usando coleções de amostras padrão. Ou seja, ouvimos constantemente sobre a mesma coisa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em um experimento online, em dois de cinco casos, as pessoas pensaram que o som gerado pelo programa era mais realista do que o som real do vídeo. Este é um resultado melhor do que outros métodos para sintetizar sons realistas.</font></font><br>
<br>
<img src="https://habrastorage.org/files/3ba/1d7/6af/3ba1d76afd8e467f8871e2a5a676442a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na maioria das vezes, a IA enganou os participantes do experimento com sons de materiais como folhas e sujeira, porque esses sons são mais complexos e não tão "limpos" quanto os feitos, por exemplo, por madeira ou metal.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voltando ao treinamento da rede neural, como subproduto do estudo, verificou-se que o algoritmo pode distinguir entre materiais macios e duros com uma precisão de 67%, simplesmente prevendo o som deles. </font><font style="vertical-align: inherit;">Em outras palavras, o robô pode olhar para o caminho do asfalto e a grama à sua frente - e concluir que o asfalto é duro e a grama é macia. </font><font style="vertical-align: inherit;">O robô saberá disso pelo som previsto, sem pisar no asfalto e na grama. </font><font style="vertical-align: inherit;">Depois, ele pode ir onde quiser - e testar seus sentimentos, verificando com o banco de dados e, se necessário, fazendo correções na biblioteca de amostras de som. </font><font style="vertical-align: inherit;">Dessa forma, no futuro, os robôs estudarão e dominarão o mundo ao seu redor.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No entanto, os pesquisadores ainda têm muito trabalho a fazer para melhorar a tecnologia. </font><font style="vertical-align: inherit;">A rede neural agora é frequentemente confundida com o movimento rápido de objetos, sem cair no momento exato do contato. </font><font style="vertical-align: inherit;">Além disso, a IA só pode gerar som com base no contato direto gravado em vídeo, e há tantos sons ao nosso redor que não se baseiam no contato visual: o barulho das árvores, o zumbido de um ventilador no computador. </font><font style="vertical-align: inherit;">"O que seria realmente legal é simular um som que não está tão intimamente relacionado às imagens", </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diz</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andrew Owens.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt395243/">https://habr.com/ru/post/pt395243/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt395233/index.html">Para classificar. Simular. Repetir</a></li>
<li><a href="../pt395235/index.html">Base russa de longo prazo na lua acomodará 12 pessoas</a></li>
<li><a href="../pt395237/index.html">Kazan space</a></li>
<li><a href="../pt395239/index.html">Apresentação na Apple Worldwide Developers Conference (WWDC) 2016 [transmissão de texto]</a></li>
<li><a href="../pt395241/index.html">Fazendo uma placa de circuito impresso usando um laser de diodo em vez de um ferro. Faça você mesmo do início ao fim</a></li>
<li><a href="../pt395245/index.html">Os desenvolvedores de jogos de No Man's Sky defendem o nome da Sky</a></li>
<li><a href="../pt395247/index.html">Programa de entrevistas: Como os podcasts criam, desenvolvem e ouvem</a></li>
<li><a href="../pt395249/index.html">Equipamento de Mobilização de Verão</a></li>
<li><a href="../pt395251/index.html">Aftershokz bluez 2 fone de ouvido com condução óssea e deficiência auditiva</a></li>
<li><a href="../pt395253/index.html">Файловая система Apple File System (APFS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>