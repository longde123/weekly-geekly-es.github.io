<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßùüèΩ üé≤ üë¶üèΩ Apache Spark, avalia√ß√£o lenta e consultas SQL de v√°rias p√°ginas ü¶é üê≥ üë®‚Äçüéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O famoso: spark trabalha com quadros de dados, que s√£o algoritmos de transforma√ß√£o. O algoritmo √© lan√ßado no √∫ltimo momento para "dar mais espa√ßo" √† o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Spark, avalia√ß√£o lenta e consultas SQL de v√°rias p√°ginas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/alfastrah/blog/481924/"><p>  O famoso: spark trabalha com quadros de dados, que s√£o algoritmos de transforma√ß√£o.  O algoritmo √© lan√ßado no √∫ltimo momento para "dar mais espa√ßo" √† otimiza√ß√£o e devido √† otimiza√ß√£o para execut√°-lo da maneira mais eficiente poss√≠vel. </p><br><p>  De acordo com o corte, analisaremos como decompor uma consulta SQL de v√°rias p√°ginas em √°tomos (sem perda de efici√™ncia) e como reduzir significativamente o tempo de execu√ß√£o do pipeline ETL devido a isso. </p><a name="habracut"></a><br><h1 id="lazy-evaluation">  Avalia√ß√£o pregui√ßosa </h1><br><p>  Um recurso funcional interessante do spark √© a avalia√ß√£o lenta: as transforma√ß√µes s√£o executadas apenas quando as a√ß√µes s√£o conclu√≠das.  Como funciona (aproximadamente): os algoritmos para a constru√ß√£o dos quadros de dados que precedem a a√ß√£o s√£o "presos", o otimizador cria o algoritmo final mais eficiente do seu ponto de vista, que inicia e fornece o resultado (o que foi solicitado pela a√ß√£o). </p><br><p>  O que √© interessante aqui no contexto de nossa apresenta√ß√£o: qualquer consulta complexa pode ser decomposta em "√°tomos" sem perda de efici√™ncia.  Vamos analisar um pouco mais. </p><br><h1 id="mnogostranichnyy-sql">  SQL de v√°rias p√°ginas </h1><br><p>  H√° muitas raz√µes pelas quais escrevemos consultas SQL de "v√°rias p√°ginas", uma das principais, provavelmente, a relut√¢ncia em criar objetos intermedi√°rios (relut√¢ncia apoiada por requisitos de efici√™ncia).  A seguir, √© apresentado um exemplo de uma consulta relativamente complexa (√© claro, √© at√© muito simples, mas, para fins de apresenta√ß√£o adicional, teremos o suficiente). </p><br><pre><code class="python hljs">qSel = <span class="hljs-string"><span class="hljs-string">""" select con.contract_id as con_contract_id, con.begin_date as con_begin_date, con.product_id as con_product_id, cst.contract_status_type_id as cst_status_type_id, sbj.subject_id as sbj_subject_id, sbj.subject_name as sbj_subject_name, pp.birth_date as pp_birth_date from kasko.contract con join kasko.contract_status cst on cst.contract_status_id = con.contract_status_id join kasko.subject sbj on sbj.subject_id = con.owner_subject_id left join kasko.physical_person pp on pp.subject_id = con.owner_subject_id """</span></span> dfSel = sp.sql(qSel)</code> </pre> <br><p>  O que vemos: </p><br><ul><li>  os dados s√£o selecionados em v√°rias tabelas </li><li>  diferentes tipos de jun√ß√£o s√£o usados </li><li>  colunas selecion√°veis ‚Äã‚Äãs√£o distribu√≠das por parte selecionada, jun√ß√£o de parte (e onde parte, mas aqui n√£o est√° aqui - eu a removi por simplicidade) </li></ul><br><p>  Essa consulta pode ser decomposta em simples (por exemplo, primeiro combine as tabelas contract e contract_status, salve o resultado em uma tabela tempor√°ria, depois combine-a com o assunto, salve tamb√©m o resultado em uma tabela tempor√°ria etc.).  Certamente, quando criamos consultas realmente complexas, fazemos isso, s√≥ ent√£o - ap√≥s a depura√ß√£o - coletamos tudo isso em um bloco de v√°rias p√°ginas. </p><br><p>  O que h√° de ruim aqui?  Nada, de fato, todo mundo trabalha assim e est√° acostumado a isso. </p><br><p>  Mas h√° desvantagens - ou melhor, o que melhorar - continue lendo. </p><br><h1 id="tot-zhe-zapros-v-spark">  A mesma consulta no spark </h1><br><p>  Ao usar o spark para transforma√ß√£o, √© claro, voc√™ pode simplesmente aceitar e executar essa solicita√ß√£o (e ser√° bom, de fato, n√≥s tamb√©m a executaremos), mas voc√™ pode seguir o outro caminho, vamos tentar. </p><br><p>  Vamos decompor essa consulta "complexa" em "√°tomos" - quadros de dados elementares.  Obteremos o n√∫mero de tabelas envolvidas na consulta (neste caso, 4). </p><br><p>  Aqui est√£o eles - "√°tomos": </p><br><pre> <code class="python hljs">dfCon = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select contract_id as con_contract_id, begin_date as con_begin_date, product_id as con_product_id, owner_subject_id as con_owner_subject_id, contract_status_id as con_contract_status_id from kasko.contract"""</span></span>) dfCStat = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select contract_status_id as cst_status_id, contract_status_type_id as cst_status_type_id from kasko.contract_status"""</span></span>) dfSubj = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select subject_id as sbj_subject_id, subject_type_id as sbj_subject_type_id, subject_name as sbj_subject_name from kasko.subject"""</span></span>) dfPPers = sp.sql(<span class="hljs-string"><span class="hljs-string">"""select subject_id as pp_subject_id, birth_date as pp_birth_date from kasko.physical_person"""</span></span>)</code> </pre> <br><p>  O Spark permite que voc√™ se junte a eles usando express√µes separadas dos "√°tomos" reais; vamos fazer o seguinte: </p><br><pre> <code class="python hljs">con_stat = f.col(<span class="hljs-string"><span class="hljs-string">"cst_status_id"</span></span>)==f.col(<span class="hljs-string"><span class="hljs-string">"con_contract_status_id"</span></span>) con_subj_own = f.col(<span class="hljs-string"><span class="hljs-string">"con_owner_subject_id"</span></span>)==f.col(<span class="hljs-string"><span class="hljs-string">"sbj_subject_id"</span></span>) con_ppers_own = f.col(<span class="hljs-string"><span class="hljs-string">"con_owner_subject_id"</span></span>)==f.col(<span class="hljs-string"><span class="hljs-string">"pp_subject_id"</span></span>)</code> </pre> <br><p>  Em seguida, nossa "consulta complexa" ficar√° assim: </p><br><pre> <code class="python hljs">dfAtom = dfCon.join(dfCStat,con_stat, <span class="hljs-string"><span class="hljs-string">"inner"</span></span>)\ .join(dfSubj,con_subj_own,<span class="hljs-string"><span class="hljs-string">"inner"</span></span>) \ .join(dfPPers,con_ppers_own, <span class="hljs-string"><span class="hljs-string">"left"</span></span>) \ .drop(<span class="hljs-string"><span class="hljs-string">"con_contract_status_id"</span></span>,<span class="hljs-string"><span class="hljs-string">"sbj_subject_type_id"</span></span>, <span class="hljs-string"><span class="hljs-string">"pp_subject_id"</span></span>,<span class="hljs-string"><span class="hljs-string">"con_owner_subject_id"</span></span>,<span class="hljs-string"><span class="hljs-string">"cst_status_id"</span></span>)</code> </pre> <br><p>  O que √© bom aqui?  √Ä primeira vista, n√£o √© nada, muito pelo contr√°rio: usando SQL "complexo", voc√™ pode entender o que est√° acontecendo; por nossa consulta "at√¥mica", √© mais dif√≠cil de entender, √© preciso olhar para "√°tomos" e express√µes. </p><br><p>  Primeiro, certifique-se de que essas consultas sejam equivalentes - no livro jupiter, por <a href="https://github.com/korolmi/dataeng/tree/master/data_doc">refer√™ncia</a> , dei planos para atender as duas consultas (os curiosos podem encontrar 10 diferen√ßas, mas a ess√™ncia - equival√™ncia - √© √≥bvia).  Isso, √© claro, n√£o √© um milagre, deve ser assim (veja acima para avalia√ß√£o e otimiza√ß√£o pregui√ßosas). </p><br><p>  O que temos no final - a solicita√ß√£o de "v√°rias p√°ginas" e a solicita√ß√£o "at√¥mica" funcionam com a mesma efici√™ncia (isso √© importante, sem que essas considera√ß√µes adicionais percam parcialmente seu significado). </p><br><p>  Bem, agora vamos encontrar o lado bom da maneira "at√¥mica" de criar consultas. </p><br><p>  O que √© um "√°tomo" (quadro de dados elementar) √© o nosso conhecimento de um subconjunto da √°rea de assunto (parte da tabela relacional).  Ao isolar esses "√°tomos", automaticamente (e, o que √© mais importante, algor√≠tmica e reprodut√≠vel), selecionamos uma parte significativa da coisa sem limites para n√≥s chamada "modelo de dados f√≠sicos". </p><br><p>  Qual √© a express√£o que usamos quando ingressamos?  Isso tamb√©m √© conhecimento sobre a √°rea de assunto - √© assim que (como indicado na express√£o) as entidades da √°rea de assunto (tabelas no banco de dados) est√£o interconectadas. </p><br><p>  Repito - isso √© importante - esse "conhecimento" (√°tomos e express√µes) √© materializado no c√≥digo execut√°vel (n√£o no diagrama ou na descri√ß√£o verbal), este √© o c√≥digo que √© executado toda vez que o pipeline ETL √© executado (o exemplo, a prop√≥sito, √© tirado da vida real). </p><br><p>  O c√≥digo execut√°vel - como sabemos pelo codificador limpo - √© um dos dois artefatos objetivamente existentes que afirmam ser o "t√≠tulo" da documenta√ß√£o.  Ou seja, o uso de "√°tomos" nos permite dar um passo adiante em um processo t√£o importante como a documenta√ß√£o de dados. </p><br><p>  O que mais pode ser encontrado em "atomicidade"? </p><br><h1 id="optimizaciya-konveyerov">  Otimiza√ß√£o do transportador </h1><br><p>  Na vida real, um engenheiro de dados - a prop√≥sito, eu n√£o me apresentei - um pipeline de ETL consiste em dezenas de transforma√ß√µes semelhantes √†s anteriores.  As tabelas s√£o muitas vezes repetidas nelas (calculei de alguma forma no Excel - algumas tabelas s√£o usadas em 40% das consultas). </p><br><p>  O que acontece em termos de efici√™ncia?  Confus√£o - a mesma tabela √© lida v√°rias vezes a partir da fonte ... </p><br><p>  Como melhor√°-lo?  O Spark possui um mecanismo para armazenar em cache os quadros de dados - podemos especificar explicitamente quais quadros de dados e quanto queremos manter no cache. </p><br><p>  O que temos que fazer para isso √© selecionar tabelas duplicadas e criar consultas de forma a minimizar o tamanho total do cache (porque todas as tabelas, por defini√ß√£o, n√£o se encaixam nela, ent√£o h√° big data). </p><br><p>  Isso pode ser feito usando consultas SSQ de v√°rias p√°ginas?  Sim, mas ... um pouco complicado (n√£o temos realmente quadros de dados l√°, apenas tabelas, eles tamb√©m podem ser armazenados em cache - a comunidade spark est√° trabalhando nisso). </p><br><p>  Isso pode ser feito usando consultas at√¥micas?  Sim  E n√£o √© dif√≠cil, precisamos apenas generalizar os "√°tomos" - adicione as colunas usadas em todas as consultas do nosso pipeline a eles.  Se voc√™ pensar bem, isso √© "correto" do ponto de vista da documenta√ß√£o: se uma coluna √© usada em alguma consulta (mesmo na parte where), ela faz parte dos dados da √°rea de interesse que s√£o interessantes para n√≥s. </p><br><p>  E ent√£o tudo √© simples - armazenamos em cache √°tomos repetidos (quadros de dados), constru√≠mos uma cadeia de transforma√ß√µes para que a interse√ß√£o de quadros de dados em cache seja m√≠nima (a prop√≥sito, isso n√£o √© trivial, mas sim program√°vel). </p><br><p>  E obtemos o transportador mais eficiente completamente "gratuito".  Al√©m disso, um artefato √∫til e importante √© a "prepara√ß√£o" para documenta√ß√£o de dados na √°rea de assunto. </p><br><h1 id="robotizaciya-i-avtomatizaciya">  Rob√≥tica e Automa√ß√£o </h1><br><p>  Os √°tomos s√£o mais suscet√≠veis ao processamento autom√°tico do que o ‚Äúgrande e poderoso SQL‚Äù - sua estrutura √© simples e clara, o spark faz uma an√°lise para n√≥s (pelos quais agradecemos especialmente), ele tamb√©m cria planos de consulta, analisando o que voc√™ pode reordenar automaticamente a sequ√™ncia do processamento da consulta. </p><br><p>  Ent√£o aqui voc√™ pode tocar alguma coisa. </p><br><h1 id="v-zaklyuchenie">  Em conclus√£o </h1><br><p>  Talvez eu esteja otimista demais - parece-me que esse caminho (atomiza√ß√£o de consulta) est√° mais funcionando do que tentar descrever uma fonte de dados ap√≥s o fato.  Al√©m disso - a prop√≥sito, qual √© o uso de "aditivos" - obtemos um aumento de efici√™ncia.  Por que considero a abordagem at√¥mica "funcional"?  Faz parte do processo regular, o que significa que os artefatos descritos t√™m uma chance real de serem relevantes a longo prazo. </p><br><p>  Provavelmente perdi alguma coisa - ajuda a encontrar (nos coment√°rios)? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt481924/">https://habr.com/ru/post/pt481924/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt481910/index.html">Como escrever VerticalSwipeBehavior flex√≠vel</a></li>
<li><a href="../pt481912/index.html">Durante uma semana eu era estagi√°rio na SRE-engenheiro. Assista atrav√©s dos olhos de um engenheiro de software</a></li>
<li><a href="../pt481914/index.html">Spring Boot vs Spring MVC vs Spring - Como eles se comparam?</a></li>
<li><a href="../pt481916/index.html">Qual foi o ano de 2019 lembrado em desenvolvimento?</a></li>
<li><a href="../pt481922/index.html">Ano Novo IMaskjs 6 - Reagir Native, Pipes, ESM</a></li>
<li><a href="../pt481926/index.html">Conhe√ßa a nova solu√ß√£o Veeam Backup for AWS</a></li>
<li><a href="../pt481930/index.html">Cultura de desenvolvimento: como o desempenho e a efici√™ncia s√£o avaliados</a></li>
<li><a href="../pt481932/index.html">Implanta√ß√£o e bancos de dados sem tempo de inatividade</a></li>
<li><a href="../pt481934/index.html">An√°lise: por que as a√ß√µes da Tesla est√£o crescendo em pre√ßo</a></li>
<li><a href="../pt481936/index.html">Pr√≥s e contras dos testes A / B: experi√™ncia de grandes empresas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>