<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤰🏽 💃 Ⓜ️ Classificação profunda para comparar duas imagens 🤰🏾 👩‍👦‍👦 🚣🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá Habr! Apresento a você a tradução do artigo “Similaridade de imagem usando classificação profunda”, de Akarsh Zingade. 

 Algoritmo de classificaç...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Classificação profunda para comparar duas imagens</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/457928/">  Olá Habr!  Apresento a você a tradução do artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">“Similaridade de imagem usando classificação profunda”,</a> de Akarsh Zingade. <br><br><h2>  Algoritmo de classificação profunda </h2><br>  O conceito de " <i>similaridade de duas imagens</i> " não foi introduzido, portanto, vamos introduzir esse conceito pelo menos na estrutura do artigo. <br><br>  <b>A semelhança de duas imagens</b> é o resultado da comparação de duas imagens de acordo com determinados critérios.  Sua medida quantitativa determina o grau de similaridade entre os diagramas de intensidade de duas imagens.  Usando uma medida de similaridade, alguns recursos que descrevem as imagens são comparados.  Como medida de semelhança, a distância de Hamming, a distância euclidiana, a distância de Manhattan, etc. <br><a name="habracut"></a><br>  <b>Classificação Profunda</b> - estuda a similaridade de imagem refinada, caracterizando a taxa de similaridade de imagem finamente dividida usando um conjunto de trigêmeos. <br><br><h3>  O que é um trigêmeo? </h3><br>  O trigêmeo contém a imagem solicitada, a imagem positiva e a negativa.  Onde uma imagem positiva é mais uma imagem de solicitação do que uma negativa. <br><br>  <b>Um exemplo de um conjunto de trigêmeos:</b> <br><br><img src="https://cdn-images-1.medium.com/max/1000/0*vw4M7uZ5exyLZfLv." alt="imagem"><br><br>  A primeira, segunda e terceira linha correspondem à imagem da solicitação.  A segunda linha (imagens positivas) é mais semelhante à solicitação de imagens do que a terceira (imagens negativas). <br><br><h3>  Arquitetura de rede de classificação profunda </h3><br>  A rede consiste em 3 partes: amostragem tripla, ConvNet e uma camada de classificação. <br>  A rede aceita trigêmeos de imagens como entrada.  Um trigêmeo de imagem contém uma imagem de solicitação <math> </math> $ inline $ p_i $ inline $   imagem positiva <math> </math> $ inline $ p_i ^ + $ inline $   e imagem negativa <math> </math> $ inline $ p_i ^ - $ inline $   transmitidos independentemente para três redes neurais profundas idênticas. <br><br>  A camada superior de classificação - avalia a função de perda de trigêmeos.  Este erro é corrigido nas camadas inferiores para minimizar a função de perda. <br><img src="https://cdn-images-1.medium.com/max/1000/0*ICR1FjUFC8xHXoh1." alt="imagem"><br><br>  Vamos agora dar uma olhada na camada do meio: <br><br><img src="https://cdn-images-1.medium.com/max/1000/0*tOd5HC3R-kFqobpM." alt="imagem"><br><br>  ConvNet pode ser qualquer rede neural profunda (este artigo discutirá uma das implementações da rede neural convolucional VGG16).  ConvNet contém camadas convolucionais, uma camada máxima de pool, camadas de normalização local e camadas totalmente conectadas. <br>  As outras duas partes recebem imagens com uma taxa de amostragem reduzida e realizam o estágio de convolução e o pool máximo.  Em seguida, o estágio de normalização das três partes ocorre e no final elas são combinadas com uma camada linear com a normalização subsequente. <br><br><h3>  Formação de trigêmeos </h3><br>  Existem várias maneiras de criar um arquivo tripleto, por exemplo, usar uma avaliação especializada.  Mas este artigo usará o seguinte algoritmo: <br><br><ol><li>  Cada imagem na classe forma uma imagem de solicitação. </li><li>  Cada imagem, exceto a imagem solicitada, formará uma imagem positiva.  Mas você pode limitar o número de imagens positivas para cada solicitação de imagem </li><li>  Uma imagem negativa é selecionada aleatoriamente em qualquer classe que não seja uma classe de imagem solicitada </li></ol><br><h3>  Função de perda de trigêmeos </h3><br>  O objetivo é treinar uma função que atribua uma pequena distância para as imagens mais semelhantes e uma grande para diferentes.  Isso pode ser expresso como: <br><img src="https://cdn-images-1.medium.com/max/1000/0*kC_DghyAeP7ulsGL." alt="imagem"><br>  Onde <b>l</b> é o coeficiente de perda para trigêmeos, <b>g</b> é o coeficiente de diferença entre a distância entre dois pares de imagens: ( <math> </math> $ inline $ p_i $ inline $   , <math> </math> $ inline $ p_i ^ + $ inline $   ) e ( <math> </math> $ inline $ p_i $ inline $   , <math> </math> $ inline $ p_i ^ - $ inline $   ), <b>f</b> - função de incorporação que exibe a imagem em um vetor, <math> </math> $ inline $ p_i $ inline $   A imagem da solicitação é <math> </math> $ inline $ p_i ^ + $ inline $   É uma imagem positiva, <math> </math> $ inline $ p_i ^ - $ inline $   É uma imagem negativa e <b>D</b> é a distância euclidiana entre dois pontos euclidianos. <br><br><h3>  Implementação de algoritmo de classificação profunda </h3><br>  Implementação com Keras. <br><br>  Três redes paralelas são usadas para consulta, imagem positiva e negativa. <br><br>  Existem três partes principais na implementação, são elas: <br><br><ol><li>  Implementação de três redes neurais paralelas multiescala </li><li>  Implementação da função de perda </li><li>  Geração de trigêmeos </li></ol><br>  O aprendizado de três redes profundas paralelas consumirá muitos recursos de memória.  Em vez de três redes profundas paralelas que recebem uma imagem solicitada, uma imagem positiva e uma negativa, essas imagens serão alimentadas sequencialmente a uma rede neural profunda na entrada de uma rede neural.  O tensor transferido para a camada de perda conterá um anexo de imagem em cada linha.  Cada linha corresponde a cada imagem de entrada em um pacote.  Como a imagem solicitada, a imagem positiva e a imagem negativa são transmitidas sequencialmente, a primeira linha corresponde à imagem solicitada, a segunda à imagem positiva e a terceira à imagem negativa, e depois repetida até o final do pacote.  Assim, a camada de classificação recebe uma incorporação de todas as imagens.  Depois disso, a função de perda é calculada. <br><br>  Para implementar a camada de classificação, precisamos escrever nossa própria função de perda, que calculará a distância euclidiana entre a imagem solicitada e a imagem positiva, bem como a distância euclidiana entre a imagem solicitada e a imagem negativa. <br><br><div class="spoiler">  <b class="spoiler_title">Implementação da função de cálculo de perdas</b> <div class="spoiler_text"><pre><code class="python hljs">_EPSILON = K.epsilon() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_loss_tensor</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> y_pred = K.clip(y_pred, _EPSILON, <span class="hljs-number"><span class="hljs-number">1.0</span></span>-_EPSILON) loss = tf.convert_to_tensor(<span class="hljs-number"><span class="hljs-number">0</span></span>,dtype=tf.float32) <span class="hljs-comment"><span class="hljs-comment"># initialise the loss variable to zero g = tf.constant(1.0, shape=[1], dtype=tf.float32) # set the value for constant 'g' for i in range(0,batch_size,3): try: q_embedding = y_pred[i+0] # procure the embedding for query image p_embedding = y_pred[i+1] # procure the embedding for positive image n_embedding = y_pred[i+2] # procure the embedding for negative image D_q_p = K.sqrt(K.sum((q_embedding - p_embedding)**2)) # calculate the euclidean distance between query image and positive image D_q_n = K.sqrt(K.sum((q_embedding - n_embedding)**2)) # calculate the euclidean distance between query image and negative image loss = (loss + g + D_q_p - D_q_n ) # accumulate the loss for each triplet except: continue loss = loss/(batch_size/3) # Average out the loss zero = tf.constant(0.0, shape=[1], dtype=tf.float32) return tf.maximum(loss,zero)</span></span></code> </pre> <br></div></div><br>  O tamanho do pacote deve sempre ser um múltiplo de 3. Como um trigêmeo contém 3 imagens, e as imagens são transmitidas seqüencialmente (enviamos cada imagem para uma rede neural profunda sequencialmente) <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O restante do código está aqui</a> <br><br><div class="spoiler">  <b class="spoiler_title">Referências</b> <div class="spoiler_text">  [1] Reconhecimento de objetos a partir de recursos invariantes em escala local - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">www.cs.ubc.ca/~lowe/papers/iccv99.pdf</a> <br><br>  [2] Histogramas de gradientes orientados para detecção <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">humana- cursos.engr.illinois.edu/ece420/fa2017/hog_for_human_detection.pdf</a> <br><br>  [3] Aprendendo a similaridade de imagem refinada com a classificação profunda- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">static.googleusercontent.com/media/research.google.com/en//pubs/archive/42945.pdf</a> <br><br>  [4] Classificação ImageNet com redes neurais profundas convolucionais- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a> <br><br>  [5] Abandono: uma maneira simples de impedir que redes neurais se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">adaptem demais - www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a> <br><br>  [6] ImageNet: Um banco de dados hierárquico de imagens em grande escala - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">www.image-net.org/papers/imagenet_cvpr09.pdf</a> <br><br>  [7] Querying-grail.cs.washington.edu/projects/query/mrquery.pdf. <br><br>  [8] Recuperação de imagem em larga escala com vetores compactados de Fisher - citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.401.9140&amp;rep=rep1&amp;type=pdf <br><br>  [9] Além dos sacos de recursos: correspondência na pirâmide espacial para o reconhecimento de categorias de cenas naturais - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ieeexplore.ieee.org/document/1641019</a> <br><br>  [10] Amostragem consistente aprimorada, esboços ponderados de Minhash e L1- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">static.googleusercontent.com/media/research.google.com/en//pubs/archive/36928.pdf</a> <br><br>  [11] Aprendizado on-line em larga escala da semelhança de imagens através do ranking- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">jmlr.csail.mit.edu/papers/volume11/chechik10a/chechik10a.pdf</a> <br><br>  [12] Aprendendo a similaridade de imagem refinada com a classificação profunda - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">users.eecs.northwestern.edu/~jwa368/pdfs/deep_ranking.pdf</a> <br><br>  [13] Similaridade de imagens usando o Ranking profundo - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">medium.com/@akarshzingade/image-similarity-using-deep-ranking-c1nst3855978</a> <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt457928/">https://habr.com/ru/post/pt457928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt457906/index.html">Os médicos acreditam que, em um futuro próximo, os dispositivos de vacinação aparecerão em residências e farmácias</a></li>
<li><a href="../pt457910/index.html">WebFPGA - Desenvolvimento Verilog no navegador</a></li>
<li><a href="../pt457916/index.html">A solução das tarefas WorldSkills do módulo Rede na competência de "CCA". Parte 2 - Configuração básica</a></li>
<li><a href="../pt457920/index.html">Jet world: acesso livre e aberto aos relatórios da conferência Joker 2018 + revisão dos dez primeiros</a></li>
<li><a href="../pt457926/index.html">Comparação de Certificação Ágil, Parte 1 - ICAgile, Scrum.org, ScrumAlliance e PMI</a></li>
<li><a href="../pt457930/index.html">Digitação dinâmica estaticamente segura no Python</a></li>
<li><a href="../pt457932/index.html">Análise do concurso IDS Bypass no Positive Hack Days 9</a></li>
<li><a href="../pt457936/index.html">Convidamos você para a primeira conferência Zabbix na Rússia</a></li>
<li><a href="../pt457940/index.html">Como examinar a contraparte</a></li>
<li><a href="../pt457942/index.html">O que aprendi sobre otimização em Python</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>