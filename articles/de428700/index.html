<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèüÔ∏è üë®‚Äçüè´ üëâ Wie maschinelles Lernen bei YouDo in die Produktion einflie√üt. Vortrag in Yandex üòÆ üòã üö¥üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In gro√üen Diensten bedeutet das L√∂sen eines Problems mithilfe von maschinellem Lernen, nur einen Teil der Arbeit zu erledigen. Das Einbetten von ML-Mo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie maschinelles Lernen bei YouDo in die Produktion einflie√üt. Vortrag in Yandex</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/428700/">  In gro√üen Diensten bedeutet das L√∂sen eines Problems mithilfe von maschinellem Lernen, nur einen Teil der Arbeit zu erledigen.  Das Einbetten von ML-Modellen ist nicht so einfach, und das Erstellen von CI / CD-Prozessen um sie herum ist noch schwieriger.  Auf der Yandex-Konferenz <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûData &amp; Science: Das Anwendungsprogramm‚Äú</a> sprach Adam Eldarov <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">,</a> Leiter Data Science bei YouDo, dar√ºber, wie man den Lebenszyklus von Modellen verwaltet, Umschulungs- und Umschulungsprozesse einrichtet, skalierbare Mikrodienste entwickelt und vieles mehr. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/k1Rp0A2NVdk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - Beginnen wir mit der Einf√ºhrung.  Es gibt einen Datenwissenschaftler, der Code in das Jupyter-Notizbuch schreibt, Feature-Engineering, Kreuzvalidierung und Modellmodelle trainiert.  Die Geschwindigkeit w√§chst. <a name="habracut"></a><br><br><img src="https://habrastorage.org/webt/fr/sg/9x/frsg9xiv8sslwj5gritig9jx4xc.jpeg"><br><br>  Aber irgendwann versteht er: Um dem Unternehmen einen gesch√§ftlichen Wert zu verleihen, muss er die L√∂sung irgendwo in der Produktion an eine mythische Produktion anh√§ngen, die uns viele Probleme bereitet.  Der Laptop, den wir in den meisten F√§llen in der Produktion gesehen haben, kann nicht gesendet werden.  Und es stellt sich die Frage, wie dieser Code im Laptop an einen bestimmten Dienst gesendet werden kann.  In den meisten F√§llen m√ºssen Sie einen Dienst mit einer API schreiben.  Oder sie kommunizieren √ºber PubSub, √ºber Warteschlangen. <br><br><img src="https://habrastorage.org/webt/lj/al/h0/ljalh0b3jiosapstldusxdueenk.jpeg"><br><br>  Wenn wir Empfehlungen aussprechen, m√ºssen wir h√§ufig Modelle trainieren und neu trainieren.  Dieser Prozess muss √ºberwacht werden.  In diesem Fall muss immer sowohl der Code selbst als auch die Modelle mit Tests √ºberpr√ºft werden, damit unser Modell nicht in einem Moment verr√ºckt wird und nicht immer anf√§ngt, Null vorherzusagen.  Es muss auch bei echten Benutzern durch AB-Tests √ºberpr√ºft werden - was wir besser oder zumindest nicht schlechter gemacht haben. <br><br>  Wie n√§hern wir uns dem Code?  Wir haben GitLab.  Unser gesamter Code ist in viele kleine Bibliotheken aufgeteilt, die ein bestimmtes Dom√§nenproblem l√∂sen.  Gleichzeitig handelt es sich um ein separates GitLab-Projekt, eine Git-Versionskontrolle und das GitFlow-Verzweigungsmodell.  Wir verwenden Dinge wie Pre-Commit-Hooks, damit Sie keinen Code festschreiben k√∂nnen, der unsere statistischen Testpr√ºfungen nicht erf√ºllt.  Und die Tests selbst, Unit-Tests.  Wir verwenden f√ºr sie den eigenschaftsbasierten Testansatz. <br><br><img src="https://habrastorage.org/webt/l8/bm/3y/l8bm3y6qju0gpmusipaz_1dvatk.jpeg"><br><br>  Wenn Sie Tests schreiben, meinen Sie normalerweise, dass Sie eine Testfunktion und die Argumente haben, die Sie mit Ihren H√§nden erstellen, einige Beispiele und welche Werte Ihre Testfunktion zur√ºckgibt.  Dies ist unpraktisch.  Der Code ist aufgeblasen, viele sind im Prinzip zu faul, um ihn zu schreiben.  Infolgedessen haben wir eine Menge Code, der durch Tests aufgedeckt wurde.  Eigenschaftsbasiertes Testen impliziert, dass alle Ihre Argumente eine bestimmte Verteilung haben.  Lassen Sie uns phasenweise vorgehen und alle unsere Argumente aus diesen Verteilungen h√§ufig testen, die zu testende Funktion mit diesen Argumenten aufrufen und das Ergebnis dieser Funktion auf bestimmte Eigenschaften √ºberpr√ºfen.  Infolgedessen haben wir viel weniger Code und gleichzeitig gibt es viel mehr Tests. <br><br><img src="https://habrastorage.org/webt/6i/x6/pc/6ix6pclinddxyed-lumlmodfbdk.jpeg"><br><br>  Was ist GitFlow?  Dies ist ein Verzweigungsmodell, das impliziert, dass Sie zwei Hauptzweige haben - Entwickeln und Mastern, in denen sich der produktionsbereite Code befindet, und die gesamte Entwicklung in dem Entwicklungszweig ausgef√ºhrt wird, in dem alle neuen Funktionen von Feature-Brunchs stammen.  Das hei√üt, jedes Feature ist ein neuer Feature-Brunch, w√§hrend der Feature-Brunch kurzlebig und endg√ºltig sein sollte - auch durch Feature-Toggle abgedeckt.  Wir machen dann eine Ver√∂ffentlichung von dev, werfen die √Ñnderungen auf master und setzen das Versions-Tag unserer Bibliothek oder unseres Dienstes darauf. <br><br><img src="https://habrastorage.org/webt/ne/a7/5h/nea75hpkozbra0cmpe1q-iheh8q.jpeg"><br><br>  Wir entwickeln, s√§gen einige Funktionen, √ºbertragen sie an GitLab und erstellen eine Zusammenf√ºhrungsanforderung vom Feature-Brunch an Jungfrauen.  Trigger funktionieren, Tests ausf√ºhren, wenn alles in Ordnung ist, k√∂nnen wir es einfrieren.  Aber nicht wir halten es, sondern jemand aus dem Team.  Es √ºberarbeitet den Code und erh√∂ht dadurch den Busfaktor.  Dieser Codeabschnitt ist bereits zwei Personen bekannt.  Wenn jemand von einem Bus angefahren wird, wei√ü jemand bereits, was er tut. <br><br><img src="https://habrastorage.org/webt/vg/fn/69/vgfn69mplj3hogej0adwyqe4cxe.jpeg"><br><br>  Die kontinuierliche Integration von Bibliotheken sieht normalerweise wie ein Test auf √Ñnderungen aus.  Und wenn wir es ver√∂ffentlichen, wird es auch auf dem privaten PyPI-Server unseres Pakets ver√∂ffentlicht. <br><br><img src="https://habrastorage.org/webt/b5/cy/fr/b5cyfrfqzlecbvb53clsbhklvp4.jpeg"><br><br>  Weiter k√∂nnen wir es in Pipelines sammeln.  Daf√ºr nutzen wir die Luigi-Bibliothek.  Es funktioniert mit einer Entit√§t wie einer Aufgabe, die eine Ausgabe hat, in der das w√§hrend der Ausf√ºhrung der Aufgabe erzeugte Artefakt gespeichert wird.  Es gibt einen Task-Parameter, der die von ihm ausgef√ºhrte Gesch√§ftslogik parametrisiert, die Task und ihre Ausgabe identifiziert.  Gleichzeitig haben Aufgaben immer Anforderungen, die andere Aufgaben stellen.  Wenn wir eine Aufgabe ausf√ºhren, werden alle Abh√§ngigkeiten durch √úberpr√ºfen der Ausgaben √ºberpr√ºft.  Wenn die Ausgabe vorhanden ist, startet unsere Abh√§ngigkeit nicht.  Wenn das Artefakt in einem Speicher fehlt, wird es gestartet.  Dies bildet eine Pipeline, einen gerichteten zyklischen Graphen. <br><br><img src="https://habrastorage.org/webt/4k/xg/_j/4kxg_j0yykqghkej-ikbbo7y_oq.jpeg"><br><br>  Alle Parameter identifizieren die Gesch√§ftslogik.  Dabei identifizieren sie das Artefakt.  Es ist immer ein Datum mit einer gewissen Granularit√§t, Empfindlichkeit oder einer Woche, einem Tag, einer Stunde oder drei Stunden.  Wenn wir ein Modell trainieren, hat Luigi taska immer Hyperparameter dieser Aufgabe, sie lecken in das von uns produzierte Artefakt, Hyperparameter spiegeln sich im Namen des Artefakts wider.  Daher versionieren wir im Wesentlichen alle Zwischendatens√§tze und endg√ºltigen Artefakte, und sie werden niemals √ºberschrieben, sondern nur f√ºr den Speicher verwendet. Der Speicher ist HDFS und S3 privat, wobei die endg√ºltigen Artefakte einiger Gurken, Modelle oder etwas anderem angezeigt werden .  Der gesamte Pipeline-Code befindet sich im Serviceprojekt in dem Repository, auf das er sich bezieht. <br><br><img src="https://habrastorage.org/webt/o6/a1/h_/o6a1h_rv7c9-vtggkki_vmbe7iq.jpeg"><br><br>  Es muss irgendwie behoben werden.  Der HashiCorp-Stack kommt zur Rettung, wir verwenden Terraform, um die Infrastruktur in Form von Code zu deklarieren, Vault, um Geheimnisse zu verwalten, es gibt alle Passw√∂rter, Erscheinungen in der Datenbank.  Consul ist ein Erkennungsdienst, der √ºber den Schl√ºsselwertspeicher verteilt wird und den Sie zum Konfigurieren verwenden k√∂nnen.  Au√üerdem f√ºhrt Consul Integrit√§tspr√ºfungen Ihrer Knoten und Ihrer Dienste durch und √ºberpr√ºft deren Verf√ºgbarkeit. <br><br>  Und - Nomade.  Es ist ein Orchestrierungssystem, das Ihre Dienste und eine Art Batch-Job vergie√üt. <br><br><img src="https://habrastorage.org/webt/cy/zb/rd/cyzbrd9uibdyssgdczrckszrpwk.jpeg"><br><br>  Wie nutzen wir das?  Es gibt eine Luigi-Pipeline, die wir in den Docker-Container packen, den Schl√§ger oder den periodischen Batch-Job in Nomad ablegen.  Stapeljob - dies ist etwas erledigt, vorbei, und wenn alles erfolgreich ist - alles ist in Ordnung, k√∂nnen wir es manuell erneut starten.  Aber wenn etwas schief gelaufen ist, versucht Nomad es erneut, bis der Versuch ersch√∂pft ist oder es nicht erfolgreich endet. <br><br>  Periodischer Stapeljob - das ist genau das gleiche, funktioniert nur nach einem Zeitplan. <br><br>  Es gibt ein Problem.  Wenn wir einen Container f√ºr ein Orchestrierungssystem bereitstellen, m√ºssen wir angeben, wie viel Speicher dieser Container, diese CPU oder dieser Speicher ben√∂tigt.  Wenn wir eine Pipeline haben, die drei Stunden lang l√§uft, verbrauchen zwei Stunden 10 GB RAM, 1 Stunde - 70 GB.  Wenn wir das Limit √ºberschreiten, das wir ihm gegeben haben, kommt der Docker-Daemon und t√∂tet Docker und (nrzb.) [02:26:13] Wir m√∂chten nicht st√§ndig aus dem Speicher herausholen, daher m√ºssen wir alle 70 GB angeben, die maximale Speicherlast.  Aber hier ist das Problem: Alle 70 GB f√ºr drei Stunden werden zugewiesen und sind f√ºr keinen anderen Job zug√§nglich. <br><br>  Deshalb sind wir den anderen Weg gegangen.  Unsere gesamte Luigi-Pipeline startet keine Gesch√§ftslogik, sondern startet lediglich eine Reihe von W√ºrfeln in Nomad, dem sogenannten parametrisierten Job.  Tats√§chlich ist dies ein Analogon der Serverfunktionen (NRZB.) [02:26:39], AVS Lambda, wer wei√ü.  Wenn wir eine Bibliothek erstellen, stellen wir unseren gesamten Code √ºber CI in Form von parametrisierten Jobs bereit, dh einem Container mit einigen Parametern.  Angenommen, Lite JBM Classifier enth√§lt einen Parameter zum Pfad zu den Eingabedaten f√ºr das Training, Hyperparameter der Modelle und den Pfad zu den Ausgabeartefakten.  All dies ist in Nomad registriert, und dann k√∂nnen wir aus der Luigi-Pipeline alle diese Nomad-Jobs √ºber die API abrufen, w√§hrend Luigi sicherstellt, dass nicht dieselbe Aufgabe viele Male ausgef√ºhrt wird. <br><br>  Angenommen, wir haben dieselbe Textverarbeitung.  Es gibt 10 bedingte Modelle, und wir m√∂chten die Textverarbeitung nicht jedes Mal neu starten.  Es wird nur einmal gestartet und gleichzeitig wird jedes Mal, wenn es wiederverwendet wird, ein fertiges Ergebnis angezeigt.  Gleichzeitig funktioniert dies alles auf verteilte Weise. Wir k√∂nnen eine riesige Rastersuche in einem gro√üen Cluster durchf√ºhren und haben nur Zeit, Gusseisen zu verwenden. <br><br><img src="https://habrastorage.org/webt/ig/bg/fv/igbgfv9ciptp0thzljej2u1pa-a.jpeg"><br><br>  Wir haben ein Artefakt, wir m√ºssen dies irgendwie in Form eines Dienstes arrangieren.  Dienste stellen entweder eine HTTP-API bereit oder kommunizieren √ºber Warteschlangen.  In diesem Beispiel ist dies die HTTP-API, das einfachste Beispiel.  Gleichzeitig validiert die Kommunikation mit dem Dienst oder unser Dienst mit anderen Diensten √ºber die HTTP-JSON-API das JSON-Schema.  Der Dienst selbst beschreibt immer ein JSON-Objekt in der Dokumentation f√ºr seine API und das Schema dieses Objekts.  Es werden jedoch nicht immer alle Felder des JSON-Objekts ben√∂tigt. Daher werden verbrauchergesteuerte Vertr√§ge validiert. Dieses Schema wird validiert. Die Kommunikation erfolgt √ºber den Leistungsschalter, um zu verhindern, dass unser verteiltes System aufgrund von Kaskadenfehlern ausf√§llt. <br><br>  Gleichzeitig muss der Dienst eine HTTP-Integrit√§tspr√ºfung festlegen, damit Consul die Verf√ºgbarkeit dieses Dienstes √ºberpr√ºfen kann.  Gleichzeitig kann Nomad es so gestalten, dass es einen Dienst f√ºr drei Hallo-Checks hintereinander gibt. Es kann den Dienst neu starten, um ihm zu helfen.  Der Dienst schreibt alle seine Protokolle im JSON-Format.  Wir verwenden den JSON-Protokollierungstreiber und den Elastics-Stack. An jedem Punkt nimmt FileBit einfach alle JSON-Protokolle und wirft sie in den Protokollcache. Von dort gelangen sie zu Elastic. Wir k√∂nnen KBan analysieren.  Gleichzeitig verwenden wir keine Protokolle zum Sammeln von Metriken und zum Erstellen von Dashboards. Dies ist ineffizient. Wir verwenden hierf√ºr das Prometheus-Entoring-System. Wir haben einen Prozess zum Erstellen von Vorlagen f√ºr jeden Dashboard-Service und k√∂nnen technische Metriken analysieren, die vom Service erstellt werden. <br><br>  Wenn etwas schief gelaufen ist, werden Warnungen angezeigt, die jedoch in den meisten F√§llen nicht ausreichen.  Sentry hilft uns, dies ist eine Sache f√ºr die Vorfallanalyse.  Tats√§chlich erfassen wir alle Fehlerstufenprotokolle vom Sentry-Handler und √ºbertragen sie in Sentry.  Und dann gibt es einen detaillierten Traceback, es gibt alle Informationen dar√ºber, in welcher Umgebung sich der Dienst befand, in welcher Version, welche Funktionen von welchen Argumenten aufgerufen wurden und welche Variablen in diesem Bereich mit welchen Werten versehen waren.  Alle Konfigurationen, all dies ist sichtbar und es hilft sehr, schnell zu verstehen, was passiert ist, und den Fehler zu beheben. <br><br><img src="https://habrastorage.org/webt/yx/oj/rl/yxojrltjutx_s1_tll0fa9xajrc.jpeg"><br><br>  Infolgedessen sieht der Service ungef√§hr so ‚Äã‚Äãaus.  Separates GitLab-Projekt, Pipeline-Code, Testcode, Service-Code selbst, eine Reihe verschiedener Konfigurationen, Nomad, CI-Konfigurationen, API-Dokumentation, Commit-Hooks und mehr. <br><br><img src="https://habrastorage.org/webt/_b/8q/f_/_b8qf_bf1ninu3_2gafa6qmm9by.jpeg"><br><br>  CI: Wenn wir ein Release erstellen, gehen wir folgenderma√üen vor: Erstellen Sie einen Container, f√ºhren Sie Tests durch, werfen Sie einen Cluster auf eine B√ºhne, f√ºhren Sie dort einen Testvertrag f√ºr unseren Service aus, f√ºhren Sie Stresstests durch, um sicherzustellen, dass unsere Vorhersage nicht zu langsam ist, und halten Sie die Last, die wir denken .  Wenn alles in Ordnung ist, werden wir diesen Service f√ºr die Produktion bereitstellen.  Und es gibt zwei M√∂glichkeiten: Wir k√∂nnen die Pipeline bereitstellen, wenn der periodische Stapeljob irgendwo im Hintergrund funktioniert und Artefakte erzeugt, oder wenn wir mit den Stiften eine Pipeline ausl√∂sen, ein Modell trainieren, danach verstehen wir, dass alles in Ordnung ist und stellen Sie den Dienst bereit. <br><br><img src="https://habrastorage.org/webt/ee/uo/uv/eeuouvuw2tpqohzhcwtz1x-qjje.jpeg"><br><br>  Was passiert sonst noch in diesem Fall?  Ich sagte, dass es bei der Entwicklung von Feature-Brunchs ein solches Paradigma gibt, wie Feature-Toggles.  Auf eine gute Weise m√ºssen Sie Features mit einigen Umschaltern abdecken, um ein Feature im Kampf zu reduzieren, wenn etwas schief gelaufen ist.  Wir k√∂nnen dann alle Funktionen in Release-Z√ºgen sammeln und selbst wenn die Funktionen noch nicht fertig sind, k√∂nnen wir sie bereitstellen.  Nur Feature-Toggle wird deaktiviert.  Da wir alle Data Scientists sind, m√∂chten wir auch AV-Tests durchf√ºhren.  Angenommen, wir haben LightGBM durch CatBoost ersetzt.  Wir m√∂chten dies √ºberpr√ºfen, aber gleichzeitig wird der AV-Test unter Bezugnahme auf eine Benutzer-ID verwaltet.  Das Umschalten von Funktionen ist an die Benutzer-ID gebunden und besteht somit den AV-Test.  Wir m√ºssen diese Metriken hier √ºberpr√ºfen. <br><br>  Alle Dienste werden f√ºr Nomad bereitgestellt.  Wir haben zwei Nomad-Produktionscluster - einen f√ºr Batch-Jobs und einen f√ºr Services. <br><br><img src="https://habrastorage.org/webt/xc/ku/mm/xckummfxsskmztnet3fnzklweny.jpeg"><br><br>  Sie schieben alle ihre Gesch√§ftsveranstaltungen nach Kafka.  Von dort k√∂nnen wir sie abholen.  Im Wesentlichen handelt es sich um eine Lammarchitektur.  Wir k√∂nnen HDFS mit einigen Diensten abonnieren, Echtzeitanalysen durchf√ºhren und gleichzeitig ClickHouse einbinden und Dashboards erstellen, um alle Gesch√§ftsereignisse f√ºr unsere Dienste zu analysieren.  Wir k√∂nnen AV-Tests analysieren, was auch immer. <br><br><img src="https://habrastorage.org/webt/e9/_z/g1/e9_zg1a4j-ycqi6c0ghuzmphd0c.jpeg"><br><br>  Und wenn wir den Code nicht ge√§ndert haben, verwenden Sie keine Feature-Toggles.  Wir haben gerade angefangen, mit einigen Stiften an einer Pipeline zu arbeiten. Er hat uns ein neues Modell beigebracht.  Wir haben einen neuen Weg dorthin.  Wir √§ndern einfach den Nomad-Pfad zum Modell in der Konfiguration, geben den neuen Dienst frei, und hier hilft uns das Canary Deployment-Paradigma. Es ist sofort in Nomad verf√ºgbar. <br><br>  Wir haben die aktuelle Version des Dienstes in drei F√§llen.  Wir sagen, wir wollen drei Kanarienv√∂gel - drei weitere Repliken neuer Versionen werden bereitgestellt, ohne die alten zu reduzieren.  Infolgedessen beginnt sich der Verkehr in zwei Teile aufzuteilen.  Ein Teil des Datenverkehrs entf√§llt auf neue Versionen von Diensten.  Alle Dienste senden alle ihre Gesch√§ftsereignisse an Kafka.  Als Ergebnis k√∂nnen wir Metriken in Echtzeit analysieren. <br><br>  Wenn alles in Ordnung ist, k√∂nnen wir sagen, dass alles in Ordnung ist.  Bei der Bereitstellung wird Nomad alle alten Versionen vorsichtig ausschalten und neue skalieren. <br><br>  Dieses Modell ist insofern schlecht, als wir das Versionsrouting durch eine Entit√§t, User Item, binden m√ºssen.  Ein solches Schema funktioniert nicht, da der Verkehr durch Round-Robin ausgeglichen wird.  Deshalb sind wir den folgenden Weg gegangen und haben den Service in zwei Teile zers√§gt. <br><br><img src="https://habrastorage.org/webt/mr/s4/hf/mrs4hf-bhaqlntomrhwjah_0qms.jpeg"><br><br>  Dies ist die Gateway-Schicht und die Worker-Schicht.  Der Client kommuniziert √ºber HTTP mit der Gateway-Schicht. Die gesamte Logik der Versionsauswahl und des Datenverkehrsausgleichs befindet sich im Gateway.  Gleichzeitig befinden sich alle E / A-gebundenen Aufgaben, die zum Abschlie√üen des Pr√§dikats erforderlich sind, auch im Gateway.  Angenommen, wir erhalten eine Benutzer-ID im Pr√§dikat in der Anforderung, die wir mit einigen Informationen anreichern m√ºssen.  Wir m√ºssen andere Microservices abrufen und alle Informationen, Funktionen oder Grundlagen abrufen.  Infolgedessen geschieht dies alles im Gateway.  Er kommuniziert mit Arbeitern, die nur im Modell sind, und tut eines - eine Vorhersage.  Ein- und Ausgabe. <br><br>  Da wir unseren Service jedoch in zwei Teile aufteilten, trat aufgrund eines Remote-Netzwerkanrufs Overhead auf.  Wie kann man es nivellieren?  Das JRPC-Framework von Google, das RPC von Google, das auf HTTP2 ausgef√ºhrt wird, hilft dabei.  Sie k√∂nnen Multiplexing und Komprimierung verwenden.  JPRC verwendet Protobuff.  Dies ist ein stark typisiertes Bin√§rprotokoll mit schneller Serialisierung und Deserialisierung. <br><br>  Dadurch haben wir auch die M√∂glichkeit, Gateway und Worker unabh√§ngig voneinander zu skalieren.  Angenommen, wir k√∂nnen eine bestimmte Anzahl offener HTTP-Verbindungen nicht beibehalten.  Okay, das Gateway skalieren.  Unsere Vorhersage ist zu langsam, wir haben keine Zeit, die Last zu halten - ok, wir skalieren die Arbeiter.  Dieser Ansatz passt sehr gut zu mehrarmigen Banditen.  Da in Gateway die gesamte Logik des Verkehrsausgleichs implementiert ist, kann es zu externen Microservices gehen und dort alle Statistiken f√ºr jede Version entnehmen sowie Entscheidungen zum Ausgleich des Verkehrs treffen.  Angenommen, Sie verwenden Thompson Sampling. <br><br><img src="https://habrastorage.org/webt/wh/nf/3e/whnf3envxyd5biwjhjftw4yjlsu.jpeg"><br><br>  Alles in Ordnung, die Modelle waren irgendwie trainiert, wir haben sie in der Nomad-Konfiguration registriert.  Aber was ist, wenn es ein Modell von Empfehlungen gibt, das bereits Zeit hat, w√§hrend des Trainings √ºberholt zu werden, und wir sie st√§ndig neu schulen m√ºssen?  Alles wird auf die gleiche Weise erledigt: Durch regelm√§√üige Batch-Jobs wird ein Artefakt erzeugt - beispielsweise alle drei Stunden.  Gleichzeitig legt die Pipeline am Ende ihrer Arbeit den Weg f√ºr das neue Modell in Consul fest.  Dies ist der Schl√ºsselwertspeicher, der f√ºr die Konfiguration verwendet wird.  Nomad kann Konfigurationen konfigurieren.  Es soll eine Umgebungsvariable geben, die auf den Werten des Schl√ºsselwertspeichers Consul basiert.  Er √ºberwacht √Ñnderungen und entscheidet, sobald ein neuer Pfad angezeigt wird, dass zwei Pfade eingeschlagen werden k√∂nnen.  Er l√§dt das Artefakt selbst √ºber einen neuen Link herunter, legt den Service-Container mithilfe von Volume in Docker ab und l√§dt ihn neu - und tut dies alles, damit keine Ausfallzeiten auftreten, dh langsam und individuell.  Oder er rendert eine neue Konfiguration und meldet ihm den Dienst.  Oder der Dienst selbst erkennt es - und kann sein Modell in sich selbst live aktualisieren.  Das ist alles, danke. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428700/">https://habr.com/ru/post/de428700/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428688/index.html">Einrichten der Arbeitsumgebung in Docker f√ºr die yii-Framework-Anwendung</a></li>
<li><a href="../de428690/index.html">Wie Sie Ihrer Freundin das Programmieren beibringen, wenn Sie kein Lehrer sind, aber sie an Sie glaubt</a></li>
<li><a href="../de428694/index.html">Die Geschichte eines einzelnen Spiels oder einer 4x-Strategie, die vor 20 Jahren begann und noch lebt</a></li>
<li><a href="../de428696/index.html">Kommentare zum Telegrammkanal</a></li>
<li><a href="../de428698/index.html">Der schwer fassbare Weltraumpirat: Verstecke dich im K√ºhlschrank vor den Bullen, besiege den Droidenkrieg und spucke Sauron ins Auge</a></li>
<li><a href="../de428702/index.html">Echos der Magie auf der Hut vor den exakten Wissenschaften</a></li>
<li><a href="../de428704/index.html">Prolog Workouts</a></li>
<li><a href="../de428706/index.html">Krypta-Fehler</a></li>
<li><a href="../de428708/index.html">Leichtes Schwimmen mit Kubernetes (Comic)</a></li>
<li><a href="../de428710/index.html">Verwendung und Wiederherstellung von Bleibatterien nach meiner Erfahrung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>