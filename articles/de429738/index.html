<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚶🏽 🈲 📣 Optimale Shard-Anordnung im Elasticsearch-Petabyte-Cluster: Lineare Programmierung 🍼 🤴🏾 ☀️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Herzstück der Suchmaschinen von Meltwater und Fairhair.ai ist Elasticsearch, ein Cluster von Clustern mit Milliarden von Medien- und Social-Media-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimale Shard-Anordnung im Elasticsearch-Petabyte-Cluster: Lineare Programmierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/429738/"><img src="https://habrastorage.org/getpro/habr/post_images/fb5/ee1/f72/fb5ee1f72a2519aae061ef6be05aa09a.png" align="left">  Das Herzstück der Suchmaschinen von Meltwater und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fairhair.ai</a> ist Elasticsearch, ein Cluster von Clustern mit Milliarden von Medien- und Social-Media-Artikeln. <br><br>  Index-Shards in Clustern unterscheiden sich stark in Zugriffsstruktur, Arbeitslast und Größe, was einige sehr interessante Probleme aufwirft. <br><br>  In diesem Artikel beschreiben wir, wie wir mithilfe der linearen Programmierung (lineare Optimierung) die Such- und Indizierungsarbeitslast so gleichmäßig wie möglich auf alle Knoten in den Clustern verteilt haben.  Diese Lösung verringert die Wahrscheinlichkeit, dass ein Knoten zu einem Engpass im System wird.  Infolgedessen haben wir die Suchgeschwindigkeit erhöht und Infrastruktur gespart. <br><a name="habracut"></a><br><h1>  Hintergrund </h1><br>  Die Suchmaschinen von Fairhair.ai enthalten etwa 40 Milliarden Beiträge aus sozialen Medien und Leitartikeln, die täglich Millionen von Anfragen bearbeiten.  Die Plattform bietet Kunden Suchergebnisse, Grafiken, Analysen und Datenexporte für erweiterte Analysen. <br><br>  Diese massiven Datensätze befinden sich in mehreren Elasticsearch-Clustern mit 750 Knoten und Tausenden von Indizes in über 50.000 Shards. <br><br>  Weitere Informationen zu unserem Cluster finden Sie in früheren Artikeln zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Architektur</a> und zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Load Balancer für maschinelles Lernen</a> . <br><br><h1>  Ungleichmäßige Arbeitslastverteilung </h1><br>  Sowohl unsere Daten als auch Benutzeranfragen sind normalerweise datumsgebunden.  Die meisten Anfragen fallen in einen bestimmten Zeitraum, z. B. letzte Woche, letzten Monat, letztes Quartal oder einen beliebigen Bereich.  Um die Indizierung und Abfragen zu vereinfachen, verwenden wir die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zeitindizierung</a> ähnlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem ELK-Stapel</a> . <br><br>  Diese Indexarchitektur bietet mehrere Vorteile.  Sie können beispielsweise eine effiziente Massenindizierung durchführen und ganze Indizes löschen, wenn Daten veraltet sind.  Dies bedeutet auch, dass die Arbeitslast für einen bestimmten Index im Laufe der Zeit stark variiert. <br><br>  Im Vergleich zu den alten werden exponentiell mehr Abfragen an die neuesten Indizes gesendet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cd9/4a6/25a/cd94a625ae06e77932216d721bb5eea7.png"><br>  <i><font color="gray">Abb.</font></i>  <i><font color="gray">1. Zugriffsschema für Zeitindizes.</font></i>  <i><font color="gray">Die vertikale Achse repräsentiert die Anzahl der abgeschlossenen Abfragen, die horizontale Achse repräsentiert das Alter des Index.</font></i>  <i><font color="gray">Wöchentliche, monatliche und jährliche Hochebenen sind deutlich sichtbar, gefolgt von einem langen Schwanz geringerer Arbeitsbelastung bei älteren Indizes</font></i> <br><br>  Die Muster in Abb.  Ich war ziemlich vorhersehbar, da unsere Kunden mehr an frischen Informationen interessiert sind und regelmäßig den aktuellen Monat mit der Vergangenheit und / oder dieses Jahr mit dem vergangenen Jahr vergleichen.  Das Problem ist, dass Elasticsearch dieses Muster nicht kennt und nicht automatisch für die beobachtete Arbeitslast optimiert! <br><br>  Der integrierte Elasticsearch-Shard-Zuweisungsalgorithmus berücksichtigt nur zwei Faktoren: <br><br><ol><li>  <i>Die Anzahl der Shards</i> auf jedem Knoten.  Der Algorithmus versucht, die Anzahl der Shards pro Knoten im gesamten Cluster gleichmäßig auszugleichen. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschriftet den</a> freien Speicherplatz.  Elasticsearch berücksichtigt den verfügbaren Speicherplatz auf einem Knoten, bevor entschieden wird, ob diesem Knoten neue Shards zugewiesen oder Segmente von diesem Knoten auf andere verschoben werden sollen.  Bei 80% der verwendeten Festplatte ist es verboten, neue Shards auf einem Knoten zu platzieren. 90% des Systems beginnen, Shards aktiv von diesem Knoten zu übertragen. </li></ol><br>  Die Grundannahme des Algorithmus ist, dass jedes Segment im Cluster ungefähr die gleiche Arbeitslast erhält und dass jeder die gleiche Größe hat.  In unserem Fall ist dies sehr weit von der Wahrheit entfernt. <br><br>  Standard-Lastausgleich führt schnell zu Hot Spots im Cluster.  Sie erscheinen und verschwinden zufällig, wenn sich die Arbeitslast im Laufe der Zeit ändert. <br><br>  Ein Hot Spot ist im Wesentlichen ein Host, der nahe der Grenze einer oder mehrerer Systemressourcen arbeitet, z. B. einer CPU, einer Festplatten-E / A oder einer Netzwerkbandbreite.  In diesem Fall stellt der Knoten die Anforderungen zunächst für eine Weile in die Warteschlange, wodurch sich die Antwortzeit auf die Anforderung erhöht.  Wenn die Überlastung jedoch lange anhält, werden die Anforderungen letztendlich abgelehnt und Benutzer erhalten Fehler. <br><br>  Eine weitere häufige Folge der Überlastung ist der instabile Druck des JVM-Mülls aufgrund von Abfragen und Indexierungsvorgängen, der zum Phänomen der „gruseligen Hölle“ des JVM-Müllsammlers führt.  In einer solchen Situation kann die JVM den Speicher entweder nicht schnell genug abrufen und stürzt nicht mehr ab oder sie bleibt in einem endlosen Speicherbereinigungszyklus stecken, friert ein und reagiert nicht mehr auf Clusteranforderungen und Pings. <br><br>  Das Problem verschlimmerte sich, als wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unsere Architektur unter AWS überarbeiteten</a> .  Zuvor wurden wir durch die Tatsache „gerettet“, dass wir bis zu vier Elasticsearch-Knoten auf unseren eigenen leistungsstarken Servern (24 Kerne) in unserem Rechenzentrum ausgeführt haben.  Dies maskierte den Einfluss der asymmetrischen Verteilung der Scherben: Die Last wurde durch eine relativ große Anzahl von Kernen auf der Maschine weitgehend geglättet. <br><br>  Nach dem Refactoring haben wir jeweils nur einen Knoten auf weniger leistungsstarken Maschinen (8 Kerne) platziert - und die ersten Tests haben sofort große Probleme mit den „Hot Spots“ ergeben. <br><br>  Elasticsearch weist Shards in zufälliger Reihenfolge zu, und bei mehr als 500 Knoten in einem Cluster hat die Wahrscheinlichkeit zu vieler „heißer“ Shards auf einem einzelnen Knoten stark zugenommen - und solche Knoten sind schnell übergelaufen. <br><br>  Für Benutzer würde dies eine ernsthafte Verschlechterung der Arbeit bedeuten, da überlastete Knoten langsam reagieren und Anforderungen oder Abstürze manchmal vollständig ablehnen.  Wenn Sie ein solches System in die Produktion bringen, werden Benutzer anscheinend häufig zufällige Verlangsamungen der Benutzeroberfläche und zufällige Zeitüberschreitungen feststellen. <br><br>  Gleichzeitig bleibt eine große Anzahl von Knoten mit Shards ohne viel Last übrig, die tatsächlich inaktiv sind.  Dies führt zu einer ineffizienten Nutzung unserer Clusterressourcen. <br><br>  Beide Probleme könnten vermieden werden, wenn Elasticsearch Shards intelligenter verteilt, da der durchschnittliche Verbrauch von Systemressourcen an allen Knoten bei einem gesunden Niveau von 40% liegt. <br><br><h3>  Cluster Continuous Change </h3><br>  Bei der Arbeit mit mehr als 500 Knoten haben wir noch eines beobachtet: eine ständige Änderung des Knotenzustands.  Scherben bewegen sich in Knoten unter dem Einfluss der folgenden Faktoren ständig hin und her: <br><br><ul><li>  Neue Indizes werden erstellt und alte verworfen. </li><li>  Datenträgerbezeichnungen werden aufgrund von Indizierung und anderen Shard-Änderungen ausgelöst. </li><li>  Elasticsearch entscheidet zufällig, dass der Knoten im Vergleich zum Durchschnittswert des Clusters zu wenige oder zu viele Shards enthält. </li><li>  Hardware-Abstürze und Abstürze auf Betriebssystemebene führen dazu, dass neue AWS-Instanzen gestartet und dem Cluster hinzugefügt werden.  Bei 500 Knoten geschieht dies durchschnittlich mehrmals pro Woche. </li><li>  Aufgrund des normalen Datenwachstums werden fast jede Woche neue Websites hinzugefügt. </li></ul><br>  Unter Berücksichtigung all dessen kamen wir zu dem Schluss, dass eine komplexe und kontinuierliche Lösung aller Probleme einen kontinuierlichen und dynamischen Algorithmus zur Neuoptimierung erfordert. <br><br><h3>  Lösung: Shardonnay </h3><br>  Nach einer langen Untersuchung der verfügbaren Optionen kamen wir zu dem Schluss, dass wir: <br><br><ol><li>  Erstellen Sie Ihre eigene Lösung.  Wir haben keine guten Artikel, Codes oder andere vorhandene Ideen gefunden, die in unserem Maßstab und für unsere Aufgaben gut funktionieren würden. </li><li>  Starten Sie den Neuausgleichsprozess außerhalb von Elasticsearch und verwenden Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Clustered Redirect-APIs,</a> anstatt zu versuchen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, ein Plugin</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erstellen</a> .  Wir wollten eine schnelle Rückkopplungsschleife, und die Bereitstellung eines Plugins in einem Cluster dieser Größenordnung kann mehrere Wochen dauern. </li><li>  Verwenden Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lineare Programmierung</a> , um zu jedem Zeitpunkt optimale Shard-Bewegungen zu berechnen. </li><li>  Führen Sie die Optimierung kontinuierlich durch, damit der Clusterstatus allmählich optimal wird. </li><li>  Bewegen Sie nicht zu viele Scherben gleichzeitig. </li></ol><br>  Wir haben eine interessante Sache bemerkt: Wenn Sie zu viele Scherben gleichzeitig bewegen, ist es sehr einfach, einen <i>kaskadierenden Sturm von Scherbenbewegungen</i> auszulösen.  Nach dem Einsetzen eines solchen Sturms kann es stundenlang andauern, wenn sich die Scherben unkontrolliert hin und her bewegen und an verschiedenen Stellen Markierungen über den kritischen Speicherplatz auftreten.  Dies führt wiederum zu neuen Splitterbewegungen und so weiter. <br><br>  Um zu verstehen, was passiert, ist es wichtig zu wissen, dass beim Verschieben eines aktiv indizierten Segments tatsächlich viel mehr Speicherplatz auf der Festplatte belegt wird, von der es verschoben wird.  Dies liegt daran, wie Elasticsearch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transaktionsprotokolle</a> speichert.  Wir haben Fälle gesehen, in denen sich der Index beim Verschieben eines Knotens verdoppelt hat.  Dies bedeutet, dass der Knoten, der die Shard-Bewegung aufgrund der hohen Speicherplatznutzung initiiert hat, <i>für eine</i> Weile <i>noch mehr Speicherplatz benötigt,</i> bis genügend Shards auf andere Knoten verschoben werden. <br><br>  Um dieses Problem zu lösen, haben wir den <i>Shardonnay-</i> Service zu Ehren der berühmten Chardonnay-Rebsorte entwickelt. <br><br><h3>  Lineare Optimierung </h3><br>  Die lineare Optimierung (oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lineare Programmierung</a> , LP) ist eine Methode, um das beste Ergebnis wie maximalen Gewinn oder niedrigste Kosten in einem mathematischen Modell zu erzielen, dessen Anforderungen durch lineare Beziehungen dargestellt werden. <br><br>  Die Optimierungsmethode basiert auf einem System linearer Variablen, einigen Einschränkungen, die erfüllt sein müssen, und einer Zielfunktion, die bestimmt, wie eine erfolgreiche Lösung aussieht.  Das Ziel der linearen Optimierung besteht darin, die Werte von Variablen zu finden, die die Zielfunktion unter Einschränkungen minimieren. <br><br><h3>  Scherbenverteilung als lineares Optimierungsproblem </h3><br>  Shardonnay sollte kontinuierlich arbeiten und bei jeder Iteration den folgenden Algorithmus ausführen: <br><br><ol><li>  Mithilfe der API ruft Elasticsearch Informationen zu vorhandenen Shards, Indizes und Knoten im Cluster sowie deren aktuellen Speicherort ab. </li><li>  Modelliert den Status eines Clusters als Satz binärer LP-Variablen.  Jede Kombination (Knoten, Index, Shard, Replikat) erhält eine eigene Variable.  Im LP-Modell gibt es eine Reihe sorgfältig entworfener Heuristiken, Einschränkungen und eine objektive Funktion, mehr dazu weiter unten. </li><li>  Sendet das LP-Modell an einen linearen Löser, der unter Berücksichtigung der Einschränkungen und der Zielfunktion eine optimale Lösung bietet.  Die Lösung besteht darin, Shards Knoten neu zuzuweisen. </li><li>  Interpretiert die Lösung der LP und wandelt sie in eine Folge von Splitterbewegungen um. </li><li>  Weist Elasticsearch an, Shards durch die Clusterumleitungs-API zu verschieben. </li><li>  Wartet darauf, dass der Cluster die Shards verschiebt. </li><li>  Kehrt zu Schritt 1 zurück. </li></ol><br>  Die Hauptsache ist, die richtigen Einschränkungen und die richtige Zielfunktion zu entwickeln.  Der Rest wird von Solver LP und Elasticsearch erledigt. <br><br>  Es überrascht nicht, dass die Aufgabe für einen Cluster dieser Größe und Komplexität sehr schwierig war! <br><br><h3>  Einschränkungen </h3><br>  Wir stützen das Modell auf einige Einschränkungen, die auf den von Elasticsearch selbst vorgegebenen Regeln basieren.  Halten Sie sich beispielsweise immer an Festplattenetiketten oder verbieten Sie das Platzieren eines Replikats auf demselben Knoten wie ein anderes Replikat desselben Shards. <br><br>  Andere werden aufgrund der jahrelangen Erfahrung mit großen Clustern hinzugefügt.  Hier sind einige Beispiele für unsere eigenen Einschränkungen: <br><br><ul><li>  Verschieben Sie die heutigen Indizes nicht, da sie am heißesten sind und das Lesen und Schreiben nahezu konstant belasten. </li><li>  Bevorzugen Sie kleinere Shards, da Elasticsearch sie schneller verarbeitet. </li><li>  Es ist ratsam, zukünftige Shards einige Tage vor ihrer Aktivierung zu erstellen und zu platzieren, mit der Indizierung zu beginnen und einer hohen Belastung zu unterliegen. </li></ul><br><br><h3>  Kostenfunktion </h3><br>  Unsere Kostenfunktion wiegt verschiedene Faktoren zusammen.  Zum Beispiel wollen wir: <br><br><ul><li>  Minimieren Sie die Varianz von Indizierungs- und Suchanfragen, um die Anzahl der "Hot Spots" zu verringern. </li><li>  Halten Sie die minimale Varianz der Festplattennutzung für einen stabilen Systembetrieb ein. </li><li>  Minimieren Sie die Anzahl der Splitterbewegungen, damit "Stürme" mit einer Kettenreaktion nicht wie oben beschrieben beginnen. </li></ul><br><h3>  Reduktion von LP-Variablen </h3><br>  In unserer Größenordnung wird die Größe dieser LP-Modelle zum Problem.  Wir haben schnell erkannt, dass Probleme mit mehr als 60 Millionen Variablen nicht in angemessener Zeit gelöst werden können.  Daher haben wir viele Optimierungs- und Modellierungstricks angewendet, um die Anzahl der Variablen drastisch zu reduzieren.  Dazu gehören voreingenommene Stichproben, Heuristiken, die Divide-and-Conquer-Methode, iterative Relaxation und Optimierung. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/4db/240/fb2/4db240fb21653d48fd6a5ad69728082e.png"></a> <br>  <i><font color="gray">Abb.</font></i>  <i><font color="gray">2. Die Heatmap zeigt die unausgeglichene Last des Elasticsearch-Clusters.</font></i>  <i><font color="gray">Dies äußert sich in einer großen Streuung des Ressourcenverbrauchs auf der linken Seite des Diagramms.</font></i>  <i><font color="gray">Durch kontinuierliche Optimierung stabilisiert sich die Situation allmählich</font></i> <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/19c/fa6/f18/19cfa6f1813be7f3a2ae9d90c58570b2.png"></a> <br>  <i><font color="gray">Abb.</font></i>  <i><font color="gray">3. Die Heatmap zeigt die CPU-Auslastung auf allen Knoten des Clusters vor und nach dem Einrichten der Hotness-Funktion in Shardonnay.</font></i>  <i><font color="gray">Eine signifikante Änderung der CPU-Auslastung ist bei konstanter Arbeitslast zu beobachten.</font></i> <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/dfc/288/81f/dfc28881fa593f543c14c8aa14069525.png"></a> <br>  <i><font color="gray">Abb.</font></i>  <i><font color="gray">4. Die Heatmap zeigt den Lesedurchsatz der Festplatten im gleichen Zeitraum wie in Abb. 4.</font></i>  <i><font color="gray">3. Lesevorgänge sind auch gleichmäßiger über den Cluster verteilt.</font></i> <br><br><h1>  Ergebnisse </h1><br>  Dadurch findet unser LP-Solver in wenigen Minuten gute Lösungen, selbst für unseren riesigen Cluster.  Somit verbessert das System iterativ den Zustand des Clusters in Richtung der Optimalität. <br><br>  Und das Beste daran ist, dass die Streuung der Arbeitslast und der Festplattennutzung wie erwartet konvergiert - und dieser nahezu optimale Zustand bleibt nach vielen absichtlichen und unerwarteten Änderungen des Clusterzustands seitdem erhalten! <br><br>  Wir unterstützen jetzt eine gesunde Arbeitslastverteilung in unseren Elasticsearch-Clustern.  Alles dank linearer Optimierung und unserem Service, den wir gerne <i>Chardonnay</i> nennen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de429738/">https://habr.com/ru/post/de429738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de429724/index.html">Nützliche Bewertung. 28 Bücher, die mein Denken beeinflusst, inspiriert oder verbessert haben</a></li>
<li><a href="../de429728/index.html">Moderne MVI-Architektur basierend auf Kotlin</a></li>
<li><a href="../de429732/index.html">Du wirst das oder eine Geschichte darüber hassen, wie gut Code aussehen sollte</a></li>
<li><a href="../de429734/index.html">Der Traum vom Fliegen mit elektrischer Vorspannung</a></li>
<li><a href="../de429736/index.html">Hogweed von Sosnowski. In MO wurden Bußgelder für den Vertrieb eingeführt</a></li>
<li><a href="../de429744/index.html">Lerne OpenGL. Lektion 6.4 - IBL. Spiegelexposition</a></li>
<li><a href="../de429750/index.html">Entwicklerkochbuch: DDD-Rezepte (Teil 3, Anwendungsarchitektur)</a></li>
<li><a href="../de429754/index.html">Schwerwiegende Hardware-Integrationsfehler</a></li>
<li><a href="../de429756/index.html">So konfigurieren Sie die Installation der Umgebungsvariablen von Nuxt.j zur Laufzeit oder wie Sie alles tun, was nicht jedem gefällt, und es nicht bereuen</a></li>
<li><a href="../de429758/index.html">Warum SRE-Dokumentation wichtig ist. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>