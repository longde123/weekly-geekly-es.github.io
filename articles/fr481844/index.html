<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßíüèæ üíê üë∏üèª 7 ans de battage m√©diatique sur les r√©seaux de neurones dans les graphiques et les perspectives inspirantes du Deep Learning 2020 ü¶ç üï¥üèø üòê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La nouvelle ann√©e se rapproche, les ann√©es 2010 se termineront bient√¥t, donnant au monde la renaissance sensationnelle des r√©seaux de neurones. J'ai √©...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>7 ans de battage m√©diatique sur les r√©seaux de neurones dans les graphiques et les perspectives inspirantes du Deep Learning 2020</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481844/"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/0a2/971/f4c0a297160b156ef22379a9555bd5fd.png"><br><br>  La nouvelle ann√©e se rapproche, les ann√©es 2010 se termineront bient√¥t, donnant au monde la renaissance sensationnelle des r√©seaux de neurones.  J'ai √©t√© troubl√© <s>et priv√© de sommeil par une</s> simple pens√©e: ¬´Comment peut-on estimer r√©trospectivement la vitesse de d√©veloppement des r√©seaux de neurones?¬ª Pour ¬´Celui qui conna√Æt le pass√© conna√Æt l'avenir¬ª.  √Ä quelle vitesse les diff√©rents algorithmes ont-ils d√©coll√©?  Comment √©valuer la vitesse des progr√®s dans ce domaine et estimer la vitesse des progr√®s de la prochaine d√©cennie? <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/cad/e4a/11b/cade4a11b5be7a57182eddbf3765ba4c.png"><br><br>  Il est clair que vous pouvez calculer approximativement le nombre d'articles dans diff√©rents domaines.  La m√©thode n'est pas id√©ale, vous devez prendre en compte les sous-domaines, mais en g√©n√©ral, vous pouvez essayer.  Je donne une id√©e, sur <a href="https://scholar.google.com/scholar%3Fhl%3Den%26as_sdt%3D0%252C5%26q%3Dbatch%2Bnormalization" rel="nofollow">Google Scholar (BatchNorm)</a> c'est bien r√©el!  Vous pouvez envisager de nouveaux ensembles de donn√©es, vous pouvez de nouveaux cours.  Votre humble serviteur, apr√®s avoir <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">tri√©</a> plusieurs options, a <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">opt√©</a> pour <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">Google Trends (BatchNorm)</a> . <br><br>  Mes coll√®gues et moi avons pris les demandes des principales technologies ML / DL, par exemple, <a href="https://en.wikipedia.org/wiki/Batch_normalization" rel="nofollow">Batch Normalization</a> , comme dans l'image ci-dessus, ajout√© la date de publication de l'article avec un point, et obtenu un calendrier assez long pour d√©coller la popularit√© du sujet.  Mais pas pour tous ceux-l√†, le <s>chemin est jonch√© de roses, le</s> d√©collage est tellement √©vident et beau, comme la batnorm.  Certains termes, tels que r√©gularisation ou saut de connexion, n'ont pas pu √™tre cr√©√©s du tout en raison du bruit des donn√©es.  Mais en g√©n√©ral, nous avons r√©ussi √† collecter les tendances. <br><br>  Peu importe ce qui s'est pass√© - bienvenue dans la coupe! <br><a name="habracut"></a><br><h1>  Au lieu d'introduire ou de reconna√Ætre l'image </h1><br>  Alors!  Les donn√©es initiales √©taient assez bruyantes, parfois il y avait de fortes pointes. <img src="https://habrastorage.org/webt/wn/ej/-p/wnej-pvivjixvw84l9sibvriuno.png"><br>  <i>Source: <a href="https://twitter.com/karpathy/status/849338608297406465" rel="nofollow">Andrei Karpaty twitter - les √©tudiants se tiennent dans les all√©es d'un vaste public pour √©couter une conf√©rence sur les r√©seaux de neurones convolutifs</a></i> <br><br>  Conventionnellement, il suffisait √† <a href="https://en.wikipedia.org/wiki/Andrej_Karpathy" rel="nofollow">Andrey Karpaty</a> de donner une conf√©rence sur le l√©gendaire <a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n: R√©seaux de neurones convolutifs pour la reconnaissance visuelle</a> pour 750 personnes avec la vulgarisation du concept de la fa√ßon dont un pic pointu va.  Par cons√©quent, les donn√©es ont √©t√© liss√©es avec un simple <a href="http://nghiaho.com/%3Fp%3D1159" rel="nofollow">filtre-bo√Æte</a> (toutes les sorties liss√©es sont marqu√©es comme liss√©es sur l'axe).  Comme nous voulions comparer le taux de croissance de la popularit√© - apr√®s le lissage, toutes les donn√©es ont √©t√© normalis√©es.  Cela s'est av√©r√© assez dr√¥le.  Voici un graphique des principales architectures en comp√©tition sur ImageNet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fa/4d6/f7b/0fa4d6f7bad7b232ec50e2f0bde6559b.png"><br>  <i>Source: Ci-apr√®s - les calculs de l'auteur selon Google Trends</i> <br><br>  Le graphique montre tr√®s clairement qu'apr√®s la publication sensationnelle <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">AlexNet</a> , qui a brass√© la bouillie du battage m√©diatique actuel des r√©seaux de neurones √† la fin de 2012, pendant pr√®s de deux ans, il bouillonnait, <s>contrairement aux affirmations du tas,</s> seul un cercle relativement restreint de sp√©cialistes s'est <s>joint</s> .  Le sujet n'est all√© au grand public qu'√† l'hiver 2014-2015.  Faites attention √† la p√©riodicit√© du calendrier √† partir de 2017: de nouveaux sommets chaque printemps.  <s>En psychiatrie, cela s'appelle une exacerbation printani√®re ...</s> C'est un signe certain que maintenant le terme est principalement utilis√© par les √©tudiants, et en moyenne, l'int√©r√™t pour AlexNet diminue par rapport au pic de popularit√©. <br><br>  De plus, au second semestre 2014, <a href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="nofollow">VGG</a> est apparu.  Soit dit en passant, <a href="" rel="nofollow">VGG a</a> co-√©crit avec le superviseur des <a href="" rel="nofollow">√©tudes</a> mon ancienne √©tudiante <a href="https://scholar.google.com/citations%3Fuser%3DL7lMQkQAAAAJ%26hl%3Den" rel="nofollow">Karen Simonyan</a> , qui travaille maintenant dans Google DeepMind ( <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> , <a href="https://en.wikipedia.org/wiki/AlphaZero" rel="nofollow">AlphaZero</a> , etc.).  Pendant ses √©tudes √† l'Universit√© d'√âtat de Moscou en 3e ann√©e, Karen a mis en ≈ìuvre un bon <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">algorithme d'estimation de mouvement</a> , qui sert de r√©f√©rence aux √©tudiants de 2 ans depuis 12 ans.  De plus, les t√¢ches y sont quelque peu insaisissables.  Comparez: <br><br><img width="50%" src="https://habrastorage.org/webt/mo/w0/9w/mow09w8lyvaxyw3b1ztxm2wxtxe.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/01a/c45/c94/01ac45c944275e9045557c9d453ff938.png"><br>  <i>Source: fonction de perte pour les t√¢ches d'estimation de mouvement (mat√©riel d'auteur) et <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">VGG-56</a></i> <br><br>  Sur la gauche, vous devez trouver le point le plus profond d'une surface non triviale en fonction des donn√©es d'entr√©e pour le nombre minimum de mesures (de nombreux minima locaux sont possibles), et sur la droite, vous devez trouver un point inf√©rieur avec des calculs minimaux (et √©galement un tas de minima locaux, et la surface d√©pend √©galement des donn√©es) .  √Ä gauche, nous obtenons le vecteur de mouvement pr√©vu et √† droite, le r√©seau form√©.  Et la diff√©rence est qu'√† gauche, il n'y a qu'une mesure implicite de l'espace colorim√©trique et √† droite, une paire de mesures de centaines de millions.  Eh bien, la complexit√© de calcul √† droite est d'environ 12 ordres de grandeur (!) Sup√©rieure.  Un peu comme √ßa ... Mais la deuxi√®me ann√©e, m√™me avec une t√¢che simple, oscille comme ... [coup√© par la censure].  Et le niveau de programmation des √©coliers d'hier pour des raisons inconnues au cours des 15 derni√®res ann√©es a nettement baiss√©.  Ils doivent dire: "Vous le ferez bien, ils vous emm√®neront √† DeepMind!"  On pourrait dire ¬´inventer le VGG¬ª, mais ¬´ils vont se tourner vers DeepMind¬ª pour une raison quelconque, cela motive mieux.  Ceci, √©videmment, est un analogue moderne avanc√© du classique "Vous mangerez de la semoule, vous deviendrez astronaute!".  Cependant, dans notre cas, si l'on compte le nombre d'enfants dans le pays et la taille du corps des cosmonautes, les chances sont des millions de fois plus √©lev√©es, car deux d'entre nous travaillent d√©j√† chez DeepMind depuis notre laboratoire. <br><br>  Vient ensuite <a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="nofollow">ResNet</a> , brisant la barre du nombre de couches et commen√ßant √† d√©coller apr√®s six mois.  Et enfin, DenseNet, qui est venu au d√©but du battage m√©diatique <a href="https://towardsdatascience.com/densenet-2810936aeebb" rel="nofollow">,</a> a d√©coll√© presque imm√©diatement, encore plus cool que ResNet. <br><br>  Si nous parlons de popularit√©, je voudrais ajouter quelques mots sur les caract√©ristiques du r√©seau et les performances, dont d√©pend √©galement la popularit√©.  Si vous regardez comment la classe <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">ImageNet</a> est pr√©dite en fonction du nombre d'op√©rations sur le r√©seau, la disposition sera la suivante (en haut et √† gauche - mieux): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c08/f56/f11/c08f56f11908ffb9f78a0d5d66a71342.png"><br>  <i>Source: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Analyse comparative des architectures repr√©sentatives des r√©seaux de neurones profonds</a></i> <br><br>  Tapez AlexNet n'est plus un g√¢teau, et ils gouvernent les r√©seaux bas√©s sur ResNet.  Cependant, si vous regardez l'√©valuation pratique du <abbr title="nombre d'images trait√©es par seconde">FPS</abbr> plus pr√®s de mon c≈ìur, vous pouvez clairement voir que VGG est plus proche de l'optimum ici, et en g√©n√©ral, l'alignement change sensiblement.  Y compris AlexNet de fa√ßon inattendue sur l'enveloppe Pareto-optimale (l'√©chelle horizontale est logarithmique, mieux au-dessus et √† droite): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/3b9/412/6423b941235280be5ec1182b91cf6d6e.png"><br>  <i>Source: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Analyse comparative des architectures repr√©sentatives des r√©seaux de neurones profonds</a></i> <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  Dans les ann√©es √† venir, l'alignement des architectures √† forte probabilit√© changera de mani√®re tr√®s significative en raison de la <a href="https://habr.com/post/455353/">progression des acc√©l√©rateurs de r√©seaux neuronaux</a> , lorsque certaines architectures vont dans des paniers et d'autres d√©collent soudainement, simplement parce qu'il vaut mieux se reposer sur un nouveau mat√©riel.  Par exemple, <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">dans l'article mentionn√©</a> , une comparaison est effectu√©e sur la carte NVIDIA Titan X Pascal et la carte NVIDIA Jetson TX1, et la disposition change sensiblement.  Dans le m√™me temps, les progr√®s du TPU, du NPU et d'autres ne font que commencer. <br></li><li>  En tant que praticien, je ne peux m'emp√™cher de remarquer que la comparaison sur ImageNet est effectu√©e par d√©faut sur ImageNet-1k, et non sur ImageNet-22k, simplement parce que la plupart forment leurs r√©seaux sur ImageNet-1k, o√π il y a 22 fois moins de classes (ce √† la fois plus facile et plus rapide).  Le passage √† ImageNet-22k, qui est plus pertinent pour de nombreuses applications pratiques, changera √©galement l'alignement (pour ceux qui sont affin√©s de 1k - beaucoup). <br></li></ul><br><h1>  Plus profond√©ment dans la technologie et l'architecture </h1><br>  Mais revenons √† la technologie.  Le terme <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="nofollow">abandon en</a> tant que mot de recherche est assez bruyant, mais une croissance de 5 fois est clairement associ√©e aux r√©seaux de neurones.  Et le d√©clin de son int√©r√™t est tr√®s probable avec un <a href="https://patents.google.com/patent/US9406017B2/en" rel="nofollow">brevet Google</a> et l'av√®nement de nouvelles m√©thodes.  Veuillez noter qu'environ un an et demi s'est √©coul√© entre la publication de l' <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">article original</a> et le regain d'int√©r√™t pour la m√©thode: <br><img src="https://habrastorage.org/getpro/habr/post_images/162/fca/4e4/162fca4e4b15574f66f3df40f4230090.png"><br><br>  Cependant, si nous parlons de la p√©riode pr√©c√©dant la mont√©e en popularit√©, alors en DL l'une des premi√®res places est clairement prise par <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">les r√©seaux r√©currents</a> et le <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C" rel="nofollow">LSTM</a> : <br><img src="https://habrastorage.org/getpro/habr/post_images/9bf/423/789/9bf423789d111f7b95352dd9a7b062c1.png"><br><br>  20 ans avant le pic actuel de popularit√©, et maintenant, avec leur utilisation, la traduction automatique, l'analyse du g√©nome ont √©t√© radicalement am√©lior√©es, et dans un avenir proche (si vous prenez de ma r√©gion), YouTube, le trafic Netflix va baisser deux fois avec la m√™me qualit√© visuelle.  Si vous apprenez correctement les le√ßons de l'histoire, il est √©vident qu'une partie des id√©es de la s√©rie d'articles actuelle ne ¬´d√©collera¬ª qu'apr√®s 20 ans.  Menez une vie saine, prenez soin de vous et vous le verrez personnellement! <br><br>  Maintenant plus proche du battage m√©diatique promis.  <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="nofollow">Les GAN</a> ont d√©coll√© comme ceci: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/80c/272/c0d/80c272c0d6207b79b2736559480aea3d.png"></a> <br><br>  On peut voir clairement que pendant presque un an, il y a eu un silence total et qu'en 2016, apr√®s 2 ans, une forte augmentation a commenc√© (les r√©sultats ont √©t√© sensiblement am√©lior√©s).  Ce d√©collage un an plus tard a donn√© le sensationnel DeepFake, qui, cependant, a √©galement d√©coll√© de 1,5 an.  Autrement dit, m√™me les technologies tr√®s prometteuses n√©cessitent beaucoup de temps pour passer d'une id√©e √† des applications que tout le monde peut utiliser. <br><br>  Si vous regardez quelles images le GAN a g√©n√©r√©es dans l' <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">article original</a> et ce qui peut √™tre construit avec <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="nofollow">StyleGAN</a> , il devient assez √©vident pourquoi il y avait un tel silence.  En 2014, seuls les sp√©cialistes ont pu √©valuer √† quel point c'√©tait cool - faire, en substance, un autre r√©seau en tant que fonction de perte et les former ensemble.  Et en 2019, chaque √©colier pourrait appr√©cier √† quel point c'est cool (sans comprendre compl√®tement comment cela se fait): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ee/6ca/51a/5ee6ca51ac69315327c2dc31f8f80c15.png"><br><br>  Il <a href="https://www.eff.org/ai/metrics" rel="nofollow">existe de nombreux</a> probl√®mes diff√©rents r√©solus avec succ√®s par les r√©seaux de neurones aujourd'hui, vous pouvez prendre les meilleurs r√©seaux et cr√©er des graphiques de popularit√© pour chaque direction, g√©rer le bruit et les pics de requ√™tes de recherche, etc.  Afin de ne pas r√©pandre mes pens√©es sur l'arbre, nous terminerons cette s√©lection par le th√®me des algorithmes de segmentation, o√π les id√©es de <a href="https://medium.com/%40sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="nofollow">convolution atreuse / dilat√©e</a> et d' <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" rel="nofollow">ASSP au</a> cours de la derni√®re ann√©e et demie se sont tout √† fait enflamm√©es <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">dans l'algorithme de r√©f√©rence</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f52/6c8/6d5/f526c86d5b81454b2a62f77193be0335.png"><br>  Il convient √©galement de noter que si <a href="https://arxiv.org/pdf/1412.7062.pdf" rel="nofollow">DeepLabv1</a> plus d'un an a ¬´attendu¬ª la mont√©e en popularit√©, <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">DeepLabv2</a> a d√©coll√© en un an et <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">DeepLabv3</a> presque imm√©diatement.  C'est-√†-dire  en g√©n√©ral, nous pouvons parler d'acc√©l√©rer la croissance de l'int√©r√™t au fil du temps (enfin, ou d'acc√©l√©rer la croissance de l'int√©r√™t pour les technologies d'auteurs r√©put√©s). <br><br>  Tout cela ensemble a conduit √† la cr√©ation du probl√®me mondial suivant - une augmentation explosive du nombre de publications sur le sujet: <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/99b/bd7/66e/99bbd766ebd801adb403efa6ee5efb4e.png"><br>  <i>Source: <a href="http://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" rel="nofollow">Trop de documents d'apprentissage automatique?</a></i> <br><br>  Cette ann√©e, nous recevons environ 150 √† 200 articles par jour, √©tant donn√© que tous ne sont pas publi√©s sur arXiv-e.  Aujourd'hui, lire des articles, m√™me dans leur propre sous-domaine, est totalement impossible.  En cons√©quence, de nombreuses id√©es int√©ressantes seront certainement enfouies sous les d√©combres de nouvelles publications, ce qui affectera le moment de leur ¬´d√©collage¬ª.  Cependant, l'augmentation <i>explosive</i> du nombre de sp√©cialistes comp√©tents employ√©s dans la r√©gion donne <s>peu d'</s> espoir de faire face au probl√®me. <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  En plus d'ImageNet et de l'histoire en coulisses des succ√®s du jeu DeepMind, les GAN ont donn√© naissance √† une nouvelle vague de vulgarisation des r√©seaux de neurones.  Avec eux, il √©tait vraiment possible de <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">¬´filmer¬ª des acteurs</a> sans <a href="https://www.youtube.com/watch%3Fv%3DWm3squcz7Aw" rel="nofollow">utiliser d'appareil photo</a> .  Et s'il y en aura plus!  Sous ce bruit informationnel, des technologies de traitement et de reconnaissance moins sonores, mais tout √† fait fonctionnelles seront financ√©es. <br></li><li>  Comme il y a trop de publications, nous attendons avec impatience l'√©mergence de nouvelles m√©thodes de r√©seau neuronal pour une analyse rapide des articles, car elles seules nous sauveront (une blague avec une fraction de blague!). <br></li></ul><br><h1>  Robots de travail, homme heureux </h1><br>  Depuis 2 ans maintenant, AutoML gagne en popularit√© <s>sur les pages des journaux</s> .  Tout a commenc√© traditionnellement avec ImageNet, dans lequel, dans la pr√©cision Top-1, il a commenc√© √† prendre fermement la premi√®re place: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/9a1/498/e6b9a14988b3d708bdbc73aebbf567d2.png"><br>  L'essence d'AutoML est tr√®s simple, un r√™ve centenaire de scientifiques des donn√©es s'y est r√©alis√© - pour un r√©seau neuronal de s√©lectionner des hyper-param√®tres.  L'id√©e a √©t√© accueillie avec √©clat: <br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4f8/c12/56a/4f8c1256ab40bc119fa0c971a8f8bbc1.png"></div><br>  Ci-dessous sur le graphique, nous voyons une situation assez rare lorsque, apr√®s la publication des premiers articles sur <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">NASNet</a> et <a href="https://arxiv.org/pdf/1802.01548.pdf" rel="nofollow">AmoebaNet</a> , ils commencent √† gagner en popularit√© par rapport aux normes des id√©es pr√©c√©dentes presque instantan√©ment (un √©norme int√©r√™t pour le sujet est affect√©): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0aa/81b/662/0aa81b6628967bc6e77468365cce8554.png"><br>  L'image idyllique est quelque peu g√¢ch√©e par deux points.  Tout d'abord, toute conversation sur AutoML commence par la phrase: "Si vous avez un dofigalion GPU ...".  Et c'est √ßa le probl√®me.  Google, bien s√ªr, affirme qu'avec leur <a href="https://cloud.google.com/automl/" rel="nofollow">Cloud AutoML,</a> cela est facilement r√©solu, l' <s>essentiel est que vous ayez suffisamment d'argent</s> , mais tout le monde n'est pas d'accord avec cette approche.  Deuxi√®mement, cela fonctionne jusqu'√† pr√©sent <a href="https://towardsdatascience.com/automl-is-overhyped-1b5511ded65f" rel="nofollow">imparfaitement</a> .  En revanche, rappelant les GAN, cinq ans ne se sont pas encore √©coul√©s, et l'id√©e elle-m√™me s'annonce tr√®s prometteuse. <br><br>  Dans tous les cas, le d√©collage principal d'AutoML commencera avec la prochaine g√©n√©ration d'acc√©l√©rateurs mat√©riels pour les r√©seaux de neurones et, en fait, avec des algorithmes am√©lior√©s. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ec/e67/3c5/7ece673c519b57348f556aa7f7b9dfa6.png"><br>  <i>Source: Image de Dmitry Konovalchuk, documents de l'auteur</i> <br><br>  <b>Total: En fait, les data scientists n'auront pas de vacances √©ternelles, bien s√ªr, car pendant tr√®s longtemps il restera un gros mal de t√™te avec les donn√©es.</b>  <b>Mais avant la nouvelle ann√©e et le d√©but des ann√©es 2020, pourquoi ne pas r√™ver?</b> <br><br><h1>  Quelques mots sur les outils </h1><br>  L'efficacit√© de la recherche d√©pend beaucoup des outils.  Si pour programmer AlexNet, vous aviez besoin d'une programmation non triviale, aujourd'hui un tel r√©seau peut √™tre rassembl√© en plusieurs lignes dans de nouveaux frameworks. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/002/004/b7c0020046f4f5a5b021c6e4a943c9a5.png"><br>  On voit clairement comment la popularit√© √©volue par vagues.  Aujourd'hui, le plus populaire (y compris <a href="https://paperswithcode.com/trends" rel="nofollow">selon PapersWithCode</a> ) est <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> .  Et une fois que le populaire <a href="http://caffe.berkeleyvision.org/" rel="nofollow">Caffe</a> sort magnifiquement tr√®s bien.  (Remarque: le sujet et le logiciel signifient que le filtrage des sujets de Google a √©t√© utilis√© lors du tra√ßage.) <br><br>  Eh bien, puisque nous avons abord√© les outils de d√©veloppement, il convient de mentionner les biblioth√®ques pour acc√©l√©rer l'ex√©cution du r√©seau: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/de3/fb6/8af/de3fb68af88d9afabe23929e1b060309.png"></a> <br><br>  La plus ancienne du sujet est (respect NVIDIA) <a href="https://developer.nvidia.com/cudnn" rel="nofollow">cuDNN</a> , et, heureusement pour les d√©veloppeurs, au cours des derni√®res ann√©es, le nombre de biblioth√®ques a augment√© plusieurs fois, et le d√©but de leur popularit√© est devenu nettement plus rapide.  Et il semble que tout cela ne soit qu'un d√©but. <br><br>  <b>Total: M√™me au cours des 3 derni√®res ann√©es, les outils ont consid√©rablement chang√© pour le mieux.</b>  <b>Et il y a 3 ans, selon les normes d'aujourd'hui, ils ne l'√©taient pas du tout.</b>  <b>La progression est tr√®s bonne!</b> <br><br><h1>  Perspectives promises du r√©seau neuronal </h1><br>  Mais le plaisir commence plus tard.  Cet √©t√©, dans un <a href="https://habr.com/post/455353/">grand article s√©par√©,</a> j'ai d√©crit en d√©tail pourquoi le CPU et m√™me le GPU ne sont pas assez efficaces pour fonctionner avec les r√©seaux de neurones, pourquoi des milliards de dollars affluent dans le d√©veloppement de nouvelles puces et quelles sont les perspectives.  Je ne vais pas me r√©p√©ter.  Vous trouverez ci-dessous une g√©n√©ralisation et l'ajout du texte pr√©c√©dent. <br><br>  Pour commencer, vous devez comprendre les diff√©rences entre les calculs de r√©seau de neurones et les calculs dans l'architecture von Neumann famili√®re (dans laquelle ils peuvent, bien s√ªr, √™tre calcul√©s, mais de mani√®re moins efficace): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1c/b6a/2f7/a1cb6a2f730e5d5f66a0e29b3fa7d1ac.png"><br>  <i>Source: Image de Dmitry Konovalchuk, documents de l'auteur</i> <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Architecture de Von Neumann</b> <br></td><td>  <b>R√©seaux de neurones</b> <br></td></tr><tr><td>  La plupart des calculs sont des op√©rations s√©quentielles. <br></td><td>  Calcul massivement parall√®le (vous avez besoin d'une architecture avec un grand nombre de modules informatiques et une acc√©l√©ration du calcul tensoriel) <br></td></tr><tr><td>  Le cours des calculs change <br>  selon les conditions ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C" rel="nofollow">superscalarit√©</a> n√©cessaire) <br></td><td>  La structure de calcul est presque toujours fixe et connue √† l'avance (la superscalarit√© est inefficace) <br></td></tr><tr><td>  Il y a une localit√© selon les donn√©es (le cache fonctionne bien) <br></td><td>  Aucune localit√© de donn√©es (le cache r√©chauffe l'air) <br></td></tr><tr><td>  Calculs pr√©cis <br></td><td>  Les calculs peuvent ne pas √™tre pr√©cis. <br></td></tr><tr><td>  Les donn√©es changent diff√©remment pour diff√©rents algorithmes <br></td><td>  Des dizaines de m√©gaoctets de coefficients de r√©seau sont inchang√©s lorsque les donn√©es sont ex√©cut√©es √† plusieurs reprises via un r√©seau de neurones <br></td></tr></tbody></table></div><br>  La fois pr√©c√©dente, la discussion principale a tourn√© autour du FPGA / ASIC, et les calculs inexacts sont pass√©s presque inaper√ßus, nous allons donc nous attarder sur eux plus en d√©tail.  Les √©normes perspectives de r√©duction des puces des g√©n√©rations futures r√©sident pr√©cis√©ment dans la capacit√© de lire de mani√®re inexacte (et de stocker les donn√©es de coefficient localement).  Le grossissement, en fait, est √©galement utilis√© en arithm√©tique exacte, lorsque les poids du r√©seau sont convertis en nombres entiers et quantifi√©s, mais √† un nouveau niveau.  √Ä titre d'exemple, consid√©rons un additionneur √† un seul bit (l'exemple est assez abstrait): <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/959/bbf/f16/959bbff160d7f0b33e4d46ce7a5be453.png"><br>  <i>Source: <a href="https://www.researchgate.net/publication/270898651_A_High_Speed_and_Low_Power_8_Bit_x_8_Bit_Multiplier_Design_using_Novel_Two_Transistor_2T_XOR_Gates" rel="nofollow">conception de multiplicateur 8 bits x 8 bits haute vitesse et faible consommation utilisant de nouvelles portes XOR √† deux transistors (2T)</a></i> <br><br>  Il a besoin de 6 transistors (il existe diff√©rentes approches, le nombre de transistors requis peut √™tre de plus en plus, mais en g√©n√©ral, quelque chose comme √ßa).  Pour 8 bits, environ <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">48 transistors sont</a> n√©cessaires.  Dans ce cas, l'additionneur analogique ne n√©cessite que 2 (deux!) Transistors, c'est-√†-dire  24 fois moins: <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f34/85f/2a5/f3485f2a5ac6fa241d2e59cd80ed1064.png"><br>  <i>Source: <a href="http://www.iitk.ac.in/eclub/ee381/AnalogMultipliers.pdf" rel="nofollow">Multiplicateurs analogiques (analyse et conception de circuits int√©gr√©s analogiques)</a></i> <br><br>  Si la pr√©cision est plus √©lev√©e (par exemple, √©quivalente √† 10 ou 16 bits num√©riques), la diff√©rence sera encore plus grande.  Encore plus int√©ressante est la situation avec la multiplication!  Si un multiplexeur num√©rique 8 bits n√©cessite environ <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">400 transistors</a> , alors un analogique 6, c'est-√†-dire  67 fois (!) De moins.  Bien s√ªr, les transistors ¬´analogiques¬ª et ¬´num√©riques¬ª sont sensiblement diff√©rents du point de vue des circuits, mais l'id√©e est claire - si nous parvenons √† augmenter la pr√©cision des calculs analogiques, nous atteignons facilement la situation lorsque nous avons besoin de deux ordres de grandeur de transistors en moins.  Et l‚Äôimportant n‚Äôest pas tant de r√©duire la taille (ce qui est important dans le cadre du ¬´ralentissement de la loi de Moore¬ª), mais de r√©duire la consommation d‚Äô√©lectricit√©, ce qui est essentiel pour les plates-formes mobiles.  Et pour les centres de donn√©es, ce ne sera pas superflu. <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/077/0b2/42b/0770b242bb2693acbe79a2165a4e8c68.png"><br>  <i>Source: <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/" rel="nofollow">IBM pense que les puces analogiques acc√©l√®rent l'apprentissage automatique</a></i> <br><br>  La cl√© du succ√®s ici sera une r√©duction de la pr√©cision, et l√† encore IBM est au premier plan: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a87/570/9f1/a875709f1c76de4e228a567f293a6618.png"><br>  <i>Source: <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/" rel="nofollow">IBM Research Blog: pr√©cision 8 bits pour la formation de syst√®mes d'apprentissage en profondeur</a></i> <br><br>  Ils sont d√©j√† engag√©s dans des ASIC sp√©cialis√©s pour les r√©seaux de neurones, qui pr√©sentent une sup√©riorit√© de plus de 10 fois sur le GPU, et pr√©voient d'atteindre une sup√©riorit√© de 100 fois dans les ann√©es √† venir.  Cela semble extr√™mement encourageant, nous l'attendons vraiment avec impatience, car, je le r√©p√®te, ce sera une perc√©e pour les appareils mobiles. <br><br>  En attendant, la situation n'est pas si magique, bien qu'il y ait de s√©rieux succ√®s.  Voici un test int√©ressant des acc√©l√©rateurs mat√©riels mobiles actuels des r√©seaux de neurones (l'image est cliquable, et cela r√©chauffe encore l'√¢me de l'auteur, √©galement en images par seconde): <br><br> <a href="" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/175/fe0/10d/175fe010d6d9742ddabe6d20d50edb67.png"></a> <br>  <i>Source: <a href="https://www.groundai.com/project/ai-benchmark-all-about-deep-learning-on-smartphones-in-2019/1" rel="nofollow">Evolution des performances des acc√©l√©rateurs d'intelligence artificielle mobiles: d√©bit d'image pour le mod√®le float Inception-V3 (mod√®le FP16 utilisant TensorFlow Lite et NNAPI)</a></i> <br><br>  Le vert indique les puces mobiles, le bleu indique le CPU, l'orange indique le GPU.  On voit clairement que les puces mobiles actuelles, et tout d'abord, la puce haut de gamme de Huawei, d√©passent d√©j√† les processeurs des dizaines de fois plus grandes (et la consommation d'√©nergie).  Et c'est fort!  Avec le GPU, jusqu'√† pr√©sent, tout n'est pas si magique, mais il y aura autre chose.  Vous pouvez regarder les r√©sultats plus en d√©tail sur un site Web s√©par√© <a href="http://ai-benchmark.com/" rel="nofollow">http://ai-benchmark.com/</a> , faites attention √† la section des tests l√†-bas, ils ont choisi un bon ensemble d'algorithmes pour la comparaison. <br><br>  <b>Total: Les progr√®s des acc√©l√©rateurs analogiques sont aujourd'hui assez difficiles √† √©valuer.</b>  <b>Il y a une course.</b>  <b>Mais les produits ne sont pas encore sortis, il y a donc <a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2Baccelerator%2Bfiletype%253Apdf" rel="nofollow">relativement peu de</a> publications.</b>  <b>Vous pouvez surveiller les brevets apparaissant avec un retard (par exemple, un flux dense <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU" rel="nofollow">d'IBM</a> ) ou <a href="https://patents.google.com/%3Fq%3Danalog%26q%3Ddnn%26q%3Daccelerator%26oq%3Danalog%2Bdnn%2Baccelerator" rel="nofollow">rechercher des brevets rares d'</a> autres fabricants.</b>  <b>Il semble que ce sera une r√©volution tr√®s s√©rieuse, principalement dans les smartphones et les TPU de serveurs.</b> <br><br><h1>  Au lieu d'une conclusion </h1><br>  ML / DL est aujourd'hui appel√© une nouvelle technologie de programmation, lorsque nous n'√©crivons pas de programme, mais ins√©rons un bloc et le formons.  C'est-√†-dire  Comme au d√©but, il y avait un assembleur, puis C, puis C ++, et maintenant, apr√®s 30 longues ann√©es d'attente, la prochaine √©tape est ML / DL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfd/8ae/5f7/cfd8ae5f7236a0c9781199044966d2c5.png"></div><br>  Cela a du sens.  R√©cemment, dans les entreprises avanc√©es, les lieux de d√©cision dans les programmes sont remplac√©s par des r√©seaux de neurones.  C'est-√†-dire<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">si hier il y avait des d√©cisions ¬´sur les FI¬ª ou sur des heuristiques qui √©taient bonnes pour le c≈ìur du programmeur ou m√™me les √©quations de Lagrange (wow!) et d'autres r√©alisations plus complexes de d√©cennies de d√©veloppement de la th√©orie du contr√¥le ont √©t√© utilis√©es, aujourd'hui, ils ont mis un r√©seau neuronal simple avec 3-5 couches avec plusieurs entr√©es et des dizaines de cotes. Elle apprend instantan√©ment, travaille beaucoup plus efficacement et le d√©veloppement de code devient plus rapide. Si auparavant, il fallait s'asseoir, chaman </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, allumer le cerveau</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , maintenant je l'ai coll√©, aliment√© des donn√©es, et cela a fonctionn√©, et vous √™tes occup√© avec des choses de niveau sup√©rieur. Juste une sorte de vacances! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturellement, le d√©bogage est d√©sormais diff√©rent. Si avant, quand quelque chose ne fonctionnait pas, il y avait une demande: "Envoyez un exemple sur lequel √ßa ne marche pas!" Et puis un </font><s><font style="vertical-align: inherit;">barbu</font></s><font style="vertical-align: inherit;"> s√©rieux et exp√©riment√©</font></font><s><font style="vertical-align: inherit;"></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">le programmeur connaissait le code et l'heuristique, a statu√© quelques coefficients, et s'il a devin√© une g√©n√©ralisation de l'exemple pour tous ces cas et l'a corrig√© au bon endroit, alors d'autres exemples similaires ont commenc√© (oh, bonheur!) √† fonctionner. Avec un bloc de r√©seau neuronal, une telle focalisation ne fonctionnera pas et la demande sera: "Donnez un exemple et des donn√©es balis√©es, pliz!" Et puis il y aura une autre formation avec le contr√¥le d'obtenir un nombre suffisant d'exemples sur tous les n≈ìuds potentiellement impliqu√©s dans la mauvaise d√©cision. Et encore plus loin dans la production, un gros bouton rouge "Retrain" appara√Ætra simplement avec la m√™me inscription grande et rouge en dessous "Appuyez pas plus d'une fois par mois!" (Afin de limiter le r√©glage du fichier). Et l'√©conomie mondiale deviendra encore plus efficace. All√©luia!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, en tant qu'outil math√©matique, le ML / DL en g√©n√©ral, et les r√©seaux de neurones en particulier, est clairement quelque chose de plus que la prochaine technologie de programmation. </font><font style="vertical-align: inherit;">Les m√™mes r√©seaux de neurones se retrouvent d√©sormais simplement √† chaque √©tape:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Le smartphone prend des photos du texte et le reconna√Æt - ce sont des r√©seaux de neurones, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Un smartphone traduit bien √† la vol√©e d'une langue √† l'autre et parle une traduction - r√©seaux de neurones et encore une fois r√©seaux de neurones, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Le navigateur et le haut-parleur intelligent reconnaissent assez bien la parole - encore une fois les r√©seaux de neurones, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Le t√©l√©viseur affiche une image de contraste lumineux de 8K √† partir de la vid√©o d'entr√©e 2K - √©galement un r√©seau neuronal, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Les robots en production sont devenus plus pr√©cis, ils ont commenc√© √† mieux voir et reconna√Ætre les situations anormales - encore une fois les r√©seaux de neurones, </font></font><br></li><li>   10       ,       <s> </s>        ,    - 90- ‚Äî   , <br></li><li>             ‚Äî   , <br></li><li>    ‚Äî               -   <a href="https://www.cgevent.ru/archives/28741" rel="nofollow">     </a> ‚Äî    , <br></li><li>   ‚Äî    !  ) <br></li></ul><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f28/b47/999/f28b47999ec31c004bc0b917fed05633.png"><br><br>  Seulement 4 ans se sont √©coul√©s depuis que les gens ont appris √† former des r√©seaux neuronaux tr√®s profonds √† bien des √©gards gr√¢ce √† BatchNorm (2015) et √† sauter les connexions (2015), et 3 ans se sont √©coul√©s depuis leur ¬´d√©collage¬ª, et nous lisons vraiment les r√©sultats de leur travail n'a pas vu.  Et maintenant, ils atteindront les produits.  Quelque chose nous dit que dans les ann√©es √† venir, beaucoup de choses int√©ressantes nous attendent.  Surtout quand les acc√©l√©rateurs "d√©collent" ... <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/a88/6b9/3dc/a886b93dc708002de50fac65c7d84a7a.png"><br><br>  Il √©tait une fois, si quelqu'un se souvient, Prom√©th√©e a vol√© le feu d'Olympe et l'a remis aux gens.  Angry Zeus avec d'autres dieux a cr√©√© la premi√®re beaut√© d'une femme-homme nomm√©e Pandora, qui √©tait dot√©e de nombreuses qualit√©s f√©minines merveilleuses <s>(j'ai soudainement r√©alis√© que le r√©cit politiquement correct de certains des mythes de la Gr√®ce antique est extr√™mement difficile)</s> .  Pandora a √©t√© envoy√©e aux gens, mais Prom√©th√©e, qui soup√ßonnait que quelque chose n'allait pas, a r√©sist√© √† son sort, et son fr√®re √âpim√©th√©e ne l'a pas fait.  En cadeau pour le mariage, Zeus a envoy√© un beau cercueil avec Mercure et Mercure, une √¢me gentille, a rempli la commande - il a donn√© le cercueil √† Epim√©th√©e, mais l'a averti de ne pas l'ouvrir de toute fa√ßon.  La curieuse Pandore a vol√© le cercueil de son mari, l'a ouvert, mais il n'y avait que des p√©ch√©s, des maladies, des guerres et d'autres probl√®mes de l'humanit√©.  Elle a essay√© de fermer le cercueil, mais il √©tait trop tard: <br><br><img src="https://habrastorage.org/webt/fi/hk/wh/fihkwhcd3uam5uyejc6thklshcm.png"><br>  <i>Source: <a href="https://regnum.ru/pictures/2414568/6.html" rel="nofollow">Artiste Frederick Stuart Church, Bo√Æte ouverte de Pandore</a></i> <br><br>  Depuis lors, la phrase ¬´ouvrir la bo√Æte de Pandore¬ª a disparu, c'est-√†-dire effectuer <s>par curiosit√© une</s> action irr√©versible, dont les cons√©quences peuvent ne pas √™tre aussi belles que les d√©corations du cercueil √† l'ext√©rieur. <br><br>  Vous savez, plus je plonge profond√©ment dans les r√©seaux de neurones, plus distinct est le sentiment qu'il s'agit d'une autre bo√Æte de Pandore.  Cependant, l'humanit√© a la plus riche exp√©rience dans l'ouverture de telles bo√Ætes!  De la r√©cente r√©cente - c'est l'√©nergie nucl√©aire et Internet.  Donc, je pense que nous pouvons faire face ensemble.  Pas √©tonnant qu'un groupe d'hommes <s>barbus</s> durs parmi les ouvreurs.  Eh bien, un cercueil est beau, d'accord!  Et ce n'est pas vrai qu'il n'y a que des probl√®mes, un tas de bonnes choses ont d√©j√† √©t√© obtenues.  Par cons√©quent, ils se sont r√©unis et ... nous ouvrons plus loin! <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  <b>L'article n'inclut pas de nombreux sujets int√©ressants, par exemple, les algorithmes ML classiques, l'apprentissage par transfert, l'apprentissage par renforcement, la popularit√© des ensembles de donn√©es, etc.</b>  <b>(Messieurs, vous pouvez continuer le sujet!)</b> <b><br></b> </li><li>  <b>√Ä la question sur le cercueil: je pense personnellement que <a href="https://habr.com/ru/post/411323/">les programmeurs de Google</a> qui ont permis √† Google <a href="https://tproger.ru/news/google-drops-pentagon/" rel="nofollow">d'abandonner le contrat de 10 milliards de dollars avec le Pentagone</a> sont grands et beaux.</b>  <b>Ils respectent et respectent.</b>  <b>Cependant, notez que quelqu'un a remport√© cet appel d'offres majeur.</b> <b><br></b> </li></ul><br>  Lisez aussi: <br><br><ul><li>  <a href="https://habr.com/post/455353/">Acc√©l√©ration mat√©rielle des r√©seaux de neurones profonds: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP et autres lettres</a> - le texte de l'auteur sur l'√©tat actuel et les perspectives de l'acc√©l√©ration mat√©rielle des r√©seaux de neurones par rapport aux approches actuelles. <br></li><li>  <a href="https://habr.com/post/480348/">Deep Fake Science, la crise de la reproductibilit√© et d'o√π viennent les r√©f√©rentiels vides</a> - sur les probl√®mes scientifiques g√©n√©r√©s par le ML / DL. <br></li><li>  <a href="https://habr.com/post/451664/">Comparaison des codecs magiques de rue.</a>  <a href="https://habr.com/post/451664/">Nous r√©v√©lons des secrets</a> - un exemple de faux bas√© sur des r√©seaux de neurones. <br></li></ul><br><h3>  Autant de <i>nouvelles d√©couvertes int√©ressantes</i> dans les ann√©es 2020 en g√©n√©ral et dans la nouvelle ann√©e en particulier! </h3><br><h2>  Remerciements </h2><br>  Je remercie chaleureusement: <br><br><ul><li>  Laboratoire d'infographie et multim√©dia VMK Universit√© d'√âtat de Moscou  M.V.  Lomonosov pour sa contribution au d√©veloppement de l'apprentissage profond en Russie et pas seulement <br></li><li>  personnellement Konstantin Kozhemyakov et Dmitry Konovalchuk, qui ont fait beaucoup pour rendre cet article meilleur et plus visuel, <br></li><li>  et enfin, un grand merci √† Kirill Malyshev, Yegor Sklyarov, Nikolai Oplachko, Andrey Moskalenko, Ivan Molodetsky, Evgeny Lyapustin, Roman Kazantsev, Alexander Yakovenko et Dmitry Klepikov pour de nombreux commentaires et corrections utiles qui ont rendu ce texte bien meilleur! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr481844/">https://habr.com/ru/post/fr481844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr481828/index.html">L'histoire des logiciels √©ducatifs: les syst√®mes de gestion de l'apprentissage et l'essor de l'√©ducation en ligne</a></li>
<li><a href="../fr481836/index.html">Pizza as a service: comment Amazon a migr√© vers Redshift</a></li>
<li><a href="../fr481838/index.html">WireGuard, configuration de plusieurs clients pour NAT, et o√π va STUN?</a></li>
<li><a href="../fr481840/index.html">Prot√©gez votre API GraphQL contre les vuln√©rabilit√©s</a></li>
<li><a href="../fr481842/index.html">Passer au stockage pur: notre nouveau stockage</a></li>
<li><a href="../fr481846/index.html">Utilisation de GitHub CI pour les projets Elixir</a></li>
<li><a href="../fr481848/index.html">Formation du personnel exp√©riment√©</a></li>
<li><a href="../fr481850/index.html">L'Inquisition espagnole et le robot pour l'humiliation: quelles sont les conf√©rences "pr√©datrices" pour le fric</a></li>
<li><a href="../fr481852/index.html">Revue de l'imprimante 3D Anet N4 // Comment colorer de mani√®re r√©aliste un personnage de Dark Souls</a></li>
<li><a href="../fr481854/index.html">Tester des id√©es gr√¢ce au prototypage de tableaux de bord</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>