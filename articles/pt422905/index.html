<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîï üé≤ üßíüèæ Mas voc√™ diz que Ceph ... ele √© t√£o bom? üë®üèº‚Äçüíº ‚öìÔ∏è üî∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eu amo Ceph. Trabalho com ele h√° 4 anos (0,80.x -  12.2.6  12.2.5). √Äs vezes sou t√£o apaixonada por ele que passo noites e noites na companhia dele, e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mas voc√™ diz que Ceph ... ele √© t√£o bom?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/422905/"><p><img src="https://habrastorage.org/webt/fm/pp/3b/fmpp3bma4xf_j2pxwystnjgezc0.png"></p><br><p>  Eu amo Ceph.  Trabalho com ele h√° 4 anos (0,80.x - <del>  12.2.6 </del> 12.2.5).  √Äs vezes sou t√£o apaixonada por ele que passo noites e noites na companhia dele, e n√£o com minha namorada.  Encontrei v√°rios problemas neste produto e continuo vivendo com alguns at√© hoje.  √Äs vezes me alegrava com decis√µes f√°ceis, e √†s vezes sonhava em me encontrar com desenvolvedores para expressar minha indigna√ß√£o.  Mas o Ceph ainda √© usado em nosso projeto e √© poss√≠vel que seja usado em novas tarefas, pelo menos por mim.  Nesta hist√≥ria, compartilharei nossa experi√™ncia em operar o Ceph, de alguma maneira me expressarei sobre o que n√£o gosto nessa solu√ß√£o e talvez ajude quem est√° apenas olhando para ela.  Os eventos que come√ßaram cerca de um ano atr√°s, quando eu trouxe o Dell EMC ScaleIO, agora conhecido como Dell EMC VxFlex OS, levaram-me a escrever este artigo. </p><br><p>  Isso n√£o √© de forma alguma um an√∫ncio para a Dell EMC ou seu produto!  Pessoalmente, n√£o sou muito bom com grandes corpora√ß√µes e caixas-pretas como o VxFlex OS.  Mas como voc√™ sabe, tudo no mundo √© relativo e, usando o exemplo do VxFlex OS, √© muito conveniente mostrar o que o Ceph √© do ponto de vista da opera√ß√£o, e tentarei faz√™-lo. <a name="habracut"></a></p><br><h2 id="parametry-rech-idet-o-4-znachnyh-chislah">  Par√¢metros  S√£o cerca de 4 d√≠gitos! </h2><br><p>  Servi√ßos Ceph, como MON, OSD, etc.  possui v√°rios par√¢metros para configurar todos os tipos de subsistemas.  Os par√¢metros s√£o definidos no arquivo de configura√ß√£o, os daemons os leem no momento do lan√ßamento.  Alguns valores podem ser convenientemente alterados em tempo real usando o mecanismo de "inje√ß√£o", descrito abaixo.  Tudo √© quase super, se voc√™ omitir o momento em que existem centenas de par√¢metros: <br><br>  Martelo: </p><br><pre><code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 863</code> </pre> <br><p>  Luminoso: </p><br><pre> <code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 1401</code> </pre> <br><p>  Acontece ~ 500 novos par√¢metros em dois anos.  Em geral, a parametriza√ß√£o √© legal, n√£o √© legal que haja dificuldades para entender 80% desta lista.  A documenta√ß√£o descrita pelas minhas estimativas ~ 20% e em alguns lugares √© amb√≠gua.  Um entendimento do significado da maioria dos par√¢metros deve ser encontrado no github do projeto ou nas listas de discuss√£o, mas isso nem sempre ajuda. </p><br><p>  Aqui est√° um exemplo de v√°rios par√¢metros nos quais eu estava interessado recentemente, encontrei-os no blog de um Ceph-gadfly: </p><br><pre> <code class="html hljs xml">throttler_perf_counter = false // enable/disable throttler perf counter osd_enable_op_tracker = false // enable/disable OSD op tracking</code> </pre> <br><p>  Codifique coment√°rios no esp√≠rito das melhores pr√°ticas.  Como se eu entendesse as palavras e at√© aproximadamente o que elas tratam, mas o que isso me dar√° n√£o √©. </p><br><p>  Ou aqui: <strong>osd_op_threads</strong> no Luminous se foi e apenas as fontes ajudaram a encontrar um novo nome: <strong>osd_peering_wq threads</strong> </p><br><p>  Eu tamb√©m gosto que existem op√ß√µes especialmente hol√≠sticas.  Aqui, cara, mostra que aumentar o <strong>rgw_num _rados_handles</strong> √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bom</a> : </p><br><p>  e o outro cara acha que&gt; 1 √© imposs√≠vel e at√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perigoso</a> . </p><br><p>  E minha coisa favorita √© que os iniciantes d√£o exemplos de uma configura√ß√£o em suas postagens no blog, onde todos os par√¢metros s√£o impensadamente copiados (parece-me) copiados de outro blog do mesmo tipo e, portanto, v√°rios par√¢metros que ningu√©m conhece, exceto o autor do c√≥digo, desviam-se de config para config. </p><br><p>  Eu tamb√©m apenas queimo muito com o que eles fizeram no Luminous.  H√° um recurso super bacana - alterar par√¢metros em tempo real, sem reiniciar os processos.  Voc√™ pode, por exemplo, alterar o par√¢metro de um OSD espec√≠fico: </p><br><pre> <code class="html hljs xml">&gt; ceph tell osd.12 injectargs '--filestore_fd_cache_size=512'</code> </pre> <br><p>  ou coloque '*' em vez de 12 e o valor ser√° alterado em todos os OSDs.  √â muito legal mesmo.  Mas, como em Ceph, isso √© feito com o p√© esquerdo.  Design Bai nem todos os valores dos par√¢metros podem ser alterados em tempo real.  Mais precisamente, eles podem ser definidos e aparecer√£o alterados na sa√≠da, mas, na verdade, apenas alguns s√£o relidos e reaplicados.  Por exemplo, voc√™ n√£o pode alterar o tamanho do pool de threads sem reiniciar o processo.  Para que o executor da equipe entenda que √© in√∫til alterar o par√¢metro dessa maneira - eles decidiram imprimir uma mensagem.  Ol√°. </p><br><p>  Por exemplo: </p><br><pre> <code class="html hljs xml">&gt; ceph tell mon.* injectargs '--mon_allow_pool_delete=true' mon.c: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.a: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.b: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</code> </pre> <br><p>  Amb√≠guo.  De fato, a remo√ß√£o de piscinas se torna poss√≠vel ap√≥s a inje√ß√£o.  Ou seja, esse aviso n√£o √© relevante para esse par√¢metro.  Ok, mas ainda existem centenas de par√¢metros, incluindo par√¢metros muito √∫teis, que tamb√©m t√™m um aviso e n√£o h√° como verificar sua aplicabilidade real.  No momento, eu n√£o consigo entender pelo c√≥digo quais par√¢metros s√£o aplicados ap√≥s a inje√ß√£o e quais n√£o s√£o.  Para garantir a confiabilidade, voc√™ precisa reiniciar os servi√ßos e isso, enfurece.  Enfurece porque sei que existe um mecanismo de inje√ß√£o. </p><br><p>  E o VxFlex OS?  Processos semelhantes como MON (no VxFlex √© MDM), OSD (SDS no VxFlex) tamb√©m possuem arquivos de configura√ß√£o, nos quais existem dezenas de par√¢metros para todos.  √â verdade que seus nomes tamb√©m n√£o dizem nada, mas a boa not√≠cia √© que nunca os recorremos a queimar tanto quanto em Ceph. </p><br><h2 id="tehnicheskiy-dolg">  D√≠vida t√©cnica </h2><br><p>  Quando voc√™ come√ßa a conhecer o Ceph com a vers√£o mais relevante para hoje, tudo parece bem e voc√™ deseja escrever um artigo positivo.  Mas quando voc√™ mora com ele no prod da vers√£o 0.80, tudo n√£o parece t√£o otimista. </p><br><p>  Antes do Jewel, os processos do Ceph eram executados como raiz.  A Jewel decidiu que eles deveriam trabalhar com o usu√°rio 'ceph' e isso exigia uma mudan√ßa de propriedade para todos os diret√≥rios usados ‚Äã‚Äãpelos servi√ßos Ceph.  Parece que isso?  Imagine um OSD que atenda a um disco magn√©tico SATA de 2 TB com capacidade total.  Portanto, a exibi√ß√£o desse disco, em paralelo (para subdiret√≥rios diferentes) com uma utiliza√ß√£o completa do disco, leva de 3 a 4 horas.  Imagine, por exemplo, voc√™ ter 3 centenas desses discos.  Mesmo se voc√™ atualizar os n√≥s (exibir imediatamente de 8 a 12 discos), voc√™ receber√° uma atualiza√ß√£o bastante longa, na qual o cluster ter√° OSD de vers√µes diferentes e uma r√©plica de dados ser√° menor no momento em que o servidor for atualizado.  Em geral, pensamos que era absurdo, reconstru√≠mos os pacotes Ceph e deixamos o OSD rodando como root.  Decidimos que, ao inserir ou substituir o OSD, os transferiremos para um novo usu√°rio.  Agora estamos alterando 2-3 unidades por m√™s e adicionando 1-2, acho que podemos lidar com isso at√© 2022). </p><br><p>  Ajust√°veis ‚Äã‚ÄãCRUSH </p><br><p>  <strong>CRUSH</strong> √© o cora√ß√£o de Ceph, tudo gira em torno dele.  Esse √© o algoritmo pelo qual, de maneira pseudo-aleat√≥ria, o local dos dados √© selecionado e, gra√ßas ao qual os clientes que trabalham com o cluster RADOS descobrem em qual OSD os dados (objetos) de que precisam est√£o armazenados.  O principal recurso do CRUSH √© que n√£o h√° necessidade de servidores de metadados, como Luster ou IBM GPFS (agora Spectrum Scale).  O CRUSH permite que clientes e OSD interajam diretamente entre si.  Embora, √© claro, seja dif√≠cil comparar o armazenamento de objetos RADOS e os sistemas de arquivos primitivos, que dei como exemplo, mas acho que a ideia √© clara. </p><br><p>  Os ajust√°veis ‚Äã‚Äãdo CRUSH, por sua vez, s√£o um conjunto de par√¢metros / sinalizadores que afetam a opera√ß√£o do CRUSH, tornando-o mais eficiente, pelo menos em teoria. </p><br><p>  Portanto, ao atualizar do Hammer para o Jewel (teste naturalmente), um aviso apareceu, dizendo que o perfil ajust√°vel pode ter par√¢metros que n√£o s√£o ideais para a vers√£o atual (Jewel) e √© recomend√°vel mudar o perfil para o ideal.  Em geral, tudo est√° claro.  O dock diz que isso √© muito importante e √© o caminho certo, mas tamb√©m √© dito que ap√≥s a troca de dados haver√° uma rebeli√£o de 10% dos dados.  10% - n√£o parece assustador, mas decidimos test√°-lo.  Para um cluster, √© cerca de 10 vezes menor do que em um produto, com o mesmo n√∫mero de PGs por OSD, preenchidos com dados de teste, obtemos uma rebeli√£o de 60%!  Imagine, por exemplo, com 100 TB de dados, 60 TB come√ßam a se mover entre OSDs e isso ocorre com a carga constante do cliente exigindo lat√™ncia!  Se ainda n√£o disse, fornecemos o s3 e n√£o temos muito menos carga no rgw mesmo √† noite, dos quais existem 8 e 4 em sites est√°ticos.  Em geral, decidimos que esse n√£o era o nosso caminho, principalmente porque a reconstru√ß√£o da nova vers√£o, com a qual n√£o hav√≠amos trabalhado no prod, era pelo menos otimista demais.  Al√©m disso, t√≠nhamos grandes √≠ndices de bucket que est√£o sendo reconstru√≠dos muito pouco e esse tamb√©m foi o motivo do atraso na mudan√ßa de perfil.  Sobre os √≠ndices ser√£o separadamente um pouco mais baixos.  No final, simplesmente removemos o aviso e decidimos voltar a ele mais tarde. </p><br><p>  E ao alternar o perfil nos testes, os cephfs-clients que est√£o nos kernels do CentOS 7.2 ca√≠ram porque n√£o puderam trabalhar com o algoritmo de hash mais novo do novo perfil que veio.  N√£o usamos cephfs no prod, mas se costum√°vamos usar, esse seria outro motivo para n√£o mudar de perfil. </p><br><p>  A prop√≥sito, o banco dos r√©us diz que se o que acontece durante a rebeli√£o n√£o combina com voc√™, voc√™ pode reverter o perfil.  De fato, ap√≥s uma instala√ß√£o limpa da vers√£o Hammer e a atualiza√ß√£o para o Jewel, o perfil fica assim: </p><br><pre> <code class="html hljs xml">&gt; ceph osd crush show-tunables { ... "straw_calc_version": 1, "allowed_bucket_algs": 22, "profile": "unknown", "optimal_tunables": 0, ... }</code> </pre> <br><p>  √â importante que seja "desconhecido" e se voc√™ tentar interromper a reconstru√ß√£o alternando para "legado" (como declarado no banco dos r√©us) ou mesmo para "martelar", a rebeli√£o n√£o ir√° parar, apenas continuar√° de acordo com outros ajust√°veis, e n√£o " ideal ".  Em geral, tudo precisa ser minuciosamente verificado e verificado, o ceph n√£o √© confi√°vel. </p><br><p>  CRUSH trade-of </p><br><p>  Como voc√™ sabe, tudo neste mundo √© equilibrado e as desvantagens s√£o aplicadas a todas as vantagens.  A desvantagem do CRUSH √© que os PGs s√£o distribu√≠dos de maneira desigual entre diferentes OSDs, mesmo com o mesmo peso do √∫ltimo.  Al√©m disso, nada impede que diferentes PGs cres√ßam em velocidades diferentes, enquanto a fun√ß√£o hash cair√°.  Especificamente, temos uma faixa de utiliza√ß√£o de OSD de 48 a 84%, apesar de terem o mesmo tamanho e, consequentemente, peso.  At√© tentamos igualar os servidores em peso, mas √© assim, apenas o nosso perfeccionismo, nada mais.  E com o fato de que as E / Ss s√£o distribu√≠das de maneira desigual entre os discos, o pior √© que, quando voc√™ alcan√ßa o status completo (95%) de pelo menos um OSD no cluster, toda a grava√ß√£o √© interrompida e o cluster passa a ser apenas leitura.  Todo o aglomerado!  E n√£o importa que o cluster ainda esteja cheio de espa√ßo.  Tudo, a final, sai!  Esse √© um recurso arquitet√¥nico do CRUSH.  Imagine que voc√™ est√° de f√©rias, algum OSD quebrou a marca de 85% (o primeiro aviso por padr√£o) e voc√™ tem 10% em estoque para impedir que a grava√ß√£o pare.  E 10% com grava√ß√£o ativa n√£o √© muito / muito longo.  Idealmente, com esse projeto, o Ceph precisa de uma pessoa de plant√£o que possa seguir as instru√ß√µes preparadas nesses casos. </p><br><p>  Ent√£o, decidimos que isso significa desequilibrar os dados no cluster, porque  v√°rios OSDs estavam pr√≥ximos da marca quase completa (85%). </p><br><p>  Existem v√°rias maneiras: </p><br><ul><li>  Adicionar unidades </li></ul><br><p>  A maneira mais f√°cil √© um pouco de desperd√≠cio e n√£o muito eficaz, porque  os dados em si podem n√£o sair do OSD lotado ou o movimento ser√° insignificante. </p><br><ul><li>  Alterar o peso permanente do OSD (PESO) </li></ul><br><p>  Isso leva a uma altera√ß√£o no peso de toda a hierarquia de bucket mais alta (terminologia CRUSH), servidor OSD, data center etc.  e, como resultado, √† movimenta√ß√£o de dados, inclusive daqueles que n√£o s√£o necess√°rios. <br>  Tentamos, reduzimos o peso de um OSD, depois que os dados de reconstru√ß√£o de outro foram preenchidos, reduzimos e depois o terceiro e percebemos que tocar√≠amos isso por um longo tempo. </p><br><ul><li>  Alterar o peso OSD n√£o permanente (REWEIGHT) </li></ul><br><p>  Isto √© o que √© feito chamando 'ceph osd reweight-by-use'.  Isso leva a uma altera√ß√£o no chamado peso de ajuste do OSD, e o peso do balde superior n√£o muda.  Como resultado, os dados s√£o balanceados entre diferentes OSDs de um servidor, por assim dizer, sem ir al√©m dos limites do intervalo CRUSH.  Realmente gostamos dessa abordagem, analisamos o funcionamento a seco que mudan√ßas seriam e as que ocorreram no produto.  Tudo correu bem at√© que o processo de rebeldia tenha uma participa√ß√£o no meio.  Mais uma vez pesquisando no Google, lendo boletins, experimentando diferentes op√ß√µes e, no final, a parada foi causada pela falta de alguns ajustes no perfil mencionado acima.  Novamente fomos apanhados em d√≠vidas t√©cnicas.  Como resultado, seguimos o caminho de adicionar discos e a reconstru√ß√£o mais ineficaz.  Felizmente, ainda precis√°vamos fazer isso porque  Foi planejado mudar o perfil CRUSH com uma margem suficiente em capacidade. </p><br><p>  Sim, conhecemos o balanceador (Luminoso e superior), que faz parte do mgr, projetado para resolver o problema de distribui√ß√£o desigual de dados movendo o PG entre OSDs, por exemplo, √† noite.  Mas ainda n√£o ouvi cr√≠ticas positivas sobre seu trabalho, mesmo no atual Mimic. </p><br><p>  Voc√™ provavelmente dir√° que a d√≠vida t√©cnica √© puramente um problema nosso e eu provavelmente concordaria.  Mas por quatro anos com Ceph no produto, tivemos apenas um tempo de inatividade s3 registrado, que durou uma hora inteira.  E ent√£o, o problema n√£o estava no RADOS, mas no RGW, que, tendo digitado seus 100 threads padr√£o, ficou travado e a maioria dos usu√°rios n√£o atendeu √†s solicita√ß√µes.  Ainda estava no Hammer.  Na minha opini√£o, este √© um bom indicador e √© alcan√ßado devido ao fato de n√£o fazermos movimentos bruscos e sermos bastante c√©ticos em rela√ß√£o a tudo no Ceph. </p><br><h2 id="dikiy-gc">  Wild gc </h2><br><p>  Como voc√™ sabe, excluir dados diretamente do disco √© uma tarefa bastante exigente e, em sistemas avan√ßados, a exclus√£o √© atrasada ou n√£o √© realizada.  O Ceph tamb√©m √© um sistema avan√ßado e, no caso do RGW, ao excluir um objeto s3, os objetos RADOS correspondentes n√£o s√£o exclu√≠dos imediatamente do disco.  O RGW marca os objetos s3 como exclu√≠dos e um gc-stream separado exclui os objetos diretamente dos pools RADOS e, portanto, √© adiado dos discos.  Ap√≥s a atualiza√ß√£o para Luminous, o comportamento do gc mudou notavelmente, ele come√ßou a trabalhar de forma mais agressiva, embora os par√¢metros do gc continuassem os mesmos.  Pela palavra visivelmente, quero dizer que come√ßamos a ver o gc trabalhando no monitoramento externo do servi√ßo para aumentar a lat√™ncia.  Isso foi acompanhado por um IO alto no pool rgw.gc.  Mas o problema que enfrentamos √© muito mais √©pico do que apenas IO.  Quando o gc est√° em execu√ß√£o, muitos logs do formul√°rio s√£o gerados: </p><br><pre> <code class="html hljs xml">0 <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">cls</span></span></span><span class="hljs-tag">&gt;</span></span> /builddir/build/BUILD/ceph-12.2.5/src/cls/rgw/cls_rgw.cc:3284: gc_iterate_entries end_key=1_01530264199.726582828</code> </pre> <br><p>  Onde 0 no in√≠cio √© o n√≠vel de registro no qual esta mensagem √© impressa.  Por assim dizer, n√£o h√° lugar para baixar o log abaixo de zero.  Como resultado, ~ 1 GB de logs foram gerados em n√≥s por um OSD em algumas horas, e tudo ficaria bem se os n√≥s ceph n√£o estivessem sem disco ... Carregamos o sistema operacional via PXE diretamente na mem√≥ria e n√£o usamos disco local ou NFS, NBD para a parti√ß√£o do sistema (/)  Acontece servidores sem estado.  Ap√≥s uma reinicializa√ß√£o, todo o estado √© rolado pela automa√ß√£o.  Como funciona, descreverei de alguma forma em um artigo separado, agora √© importante que 6 GB de mem√≥ria sejam alocados para "/", dos quais ~ 4 geralmente s√£o gratuitos.  Enviamos todos os logs para o Graylog e usamos uma pol√≠tica de rota√ß√£o de logs bastante agressiva e geralmente n√£o temos problemas com o estouro de disco / RAM.  Mas n√£o est√°vamos prontos para isso, com 12 OSDs, o servidor "/" encheu-se muito rapidamente, os atendentes no prazo n√£o responderam ao gatilho no Zabbix e o OSD come√ßou a parar devido √† incapacidade de gravar um log.  Como resultado, reduzimos a intensidade de gc, o ticket n√£o foi iniciado porque  Ele j√° estava l√° e adicionamos um script ao cron, no qual for√ßamos os logs OSD a truncar quando uma certa quantia √© excedida sem aguardar a rota√ß√£o do log.  A prop√≥sito, o n√≠vel de registro foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aumentado</a> . </p><br><h2 id="placement-groups-i-hvalyonaya-masshtabiruemost">  Grupos de canais e escalabilidade elogiada </h2><br><p>  Na minha opini√£o, PG √© a abstra√ß√£o mais dif√≠cil de entender.  O PG √© necess√°rio para tornar o CRUSH mais eficaz.  O principal objetivo do PG √© agrupar objetos para reduzir o consumo de recursos, aumentar a produtividade e a escalabilidade.  Endere√ßar objetos diretamente, individualmente, sem combin√°-los no PG seria muito caro. </p><br><p>  O principal problema do PG √© determinar seu n√∫mero para um novo pool.  No blog Ceph: </p><br><blockquote>  "Escolher o n√∫mero certo de PGs para o seu cluster √© um pouco de arte negra - e um pesadelo na usabilidade". </blockquote><p>  Isso √© sempre muito espec√≠fico para uma instala√ß√£o espec√≠fica e requer muita reflex√£o e c√°lculo. </p><br><p>  Principais recomenda√ß√µes: </p><br><ul><li>  Muitos PGs no OSD s√£o ruins; haver√° um gasto excessivo de recursos para manuten√ß√£o e freios durante o reequil√≠brio / recupera√ß√£o. </li><li>  Poucos PGs no OSD s√£o ruins, o desempenho sofrer√° e os OSDs ser√£o preenchidos de maneira desigual. </li><li>  O n√∫mero PG deve ser um m√∫ltiplo de grau 2. Isso ajudar√° a obter o "poder do CRUSH". </li></ul><br><p>  E aqui queima comigo.  PGs n√£o s√£o limitados em volume ou no n√∫mero de objetos.  Quantos recursos (em n√∫meros reais) s√£o necess√°rios para atender um PG?  Depende do seu tamanho?  Depende do n√∫mero de r√©plicas deste PG?  Devo tomar banho a vapor se tiver mem√≥ria suficiente, CPUs r√°pidas e uma boa rede? <br>  E voc√™ tamb√©m precisa pensar sobre o crescimento futuro do cluster.  O n√∫mero do PG n√£o pode ser reduzido - apenas aumentado.  Ao mesmo tempo, n√£o √© recomend√°vel fazer isso, pois isso implicar√°, em ess√™ncia, a divis√£o de parte do PG na reconstru√ß√£o nova e selvagem. </p><br><blockquote>  "Aumentar a contagem de PG de um pool √© um dos eventos mais impactantes em um Ceph Cluster e deve ser evitado para clusters de produ√ß√£o, se poss√≠vel." </blockquote><p>  Portanto, voc√™ precisa pensar no futuro imediatamente, se poss√≠vel. </p><br><p>  Um exemplo real. </p><br><p>  Um cluster de 3 servidores com OSD de 14x2 TB cada, um total de 42 OSDs.  R√©plica 3, local √∫til ~ 28 TB.  Para ser usado no S3, voc√™ precisa calcular o n√∫mero de PGs para o conjunto de dados e o conjunto de √≠ndices.  O RGW usa mais pools, mas os dois s√£o prim√°rios. </p><br><p>  Entramos na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">calculadora PG</a> (existe uma calculadora), consideramos com os 100 PG recomendados no OSD, obtemos apenas 1312 PG.  Mas nem tudo √© t√£o simples: temos uma introdu√ß√£o - o cluster definitivamente crescer√° tr√™s vezes em um ano, mas o ferro ser√° comprado um pouco mais tarde.  Aumentamos "PGs alvo por OSD" tr√™s vezes, para 300 e obtemos 4480 PG. </p><br><p><img src="https://habrastorage.org/webt/ce/o3/bo/ceo3boailgnmcx_2_w6un6_covi.png"></p><br><p>  Defina o n√∫mero de PGs para os pools correspondentes - recebemos um aviso: muitos PG por OSD ... chegaram.  Recebeu ~ 300 PG no OSD com um limite de 200 (Luminoso).  A prop√≥sito, costumava ser 300.  E o mais interessante √© que nem todos os PGs desnecess√°rios podem espiar, ou seja, isso n√£o √© apenas um aviso.  Como resultado, acreditamos que estamos fazendo tudo certo, aumentando limites, desativando o aviso e seguindo em frente. </p><br><p>  Outro exemplo real √© mais interessante. </p><br><p>  S3, volume utiliz√°vel de 152 TB, OSD 252 a 1,81 TB, ~ 105 PG no OSD.  O cluster cresceu gradualmente, tudo correu bem at√© que, com as novas leis em nosso pa√≠s, era necess√°rio aumentar para 1 PB, ou seja, ~ ~ 850 TB e, ao mesmo tempo, voc√™ precisava manter o desempenho, que agora √© muito bom para o S3.  Suponha que pegamos discos de 6 TB (5,7 reais) e, levando em considera√ß√£o a r√©plica 3, obtemos + 447 OSD.  Levando em conta os atuais, obtemos 699 OSDs com 37 PGs cada e, se levarmos em conta pesos diferentes, os OSDs antigos t√™m apenas uma d√∫zia de PGs.  Ent√£o voc√™ me diz o qu√£o toler√°vel isso vai funcionar?  O desempenho de um cluster com um n√∫mero diferente de PGs √© bastante dif√≠cil de medir sinteticamente, mas os testes que eu conduzi mostram que, para um desempenho ideal, √© necess√°rio de 50 PG a OSD de 2 TB.  E quanto a mais crescimento?  Sem aumentar o n√∫mero de PG, voc√™ pode ir para o mapeamento de PG para OSD 1: 1.  Talvez eu n√£o entenda alguma coisa? </p><br><p>  Sim, voc√™ pode criar um novo pool para o RGW com o n√∫mero desejado de PGs e mapear uma regi√£o S3 separada para ele.  Ou at√© mesmo criar um novo cluster nas proximidades.  Mas voc√™ deve admitir que essas s√£o todas muletas.  E acontece que parece um Ceph bem escal√°vel devido ao seu conceito, o PG √© escal√°vel com reservas.  Voc√™ ter√° que conviver com vorings desativados na prepara√ß√£o para o crescimento ou, em algum momento, reconstruir todos os dados no cluster ou pontuar no desempenho e viver com o que acontece.  Ou passar por tudo isso. </p><br><p>  Fico feliz que os desenvolvedores do Ceph <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entendam</a> que o PG √© uma abstra√ß√£o complexa e sup√©rflua para o usu√°rio e √© melhor ele n√£o saber disso. </p><br><blockquote>  "No Luminous, tomamos as principais medidas para finalmente eliminar uma das maneiras mais comuns de levar seu cluster a um fosso. Esperamos, com o futuro, ocultar os PGs por completo para que eles n√£o sejam algo que a maioria dos usu√°rios precise saber ou pense em ". </blockquote><p>  No vxFlex, n√£o h√° conceito de PG ou qualquer an√°logo.  Voc√™ acabou de adicionar discos ao pool e √© isso.  E assim por diante at√© 16 PB.  Imagine, nada precisa ser calculado, n√£o h√° montes de status desses PGs; os discos s√£o descartados uniformemente durante o crescimento.  Porque  os discos s√£o fornecidos ao vxFlex como um todo (n√£o existe um sistema de arquivos em cima deles), n√£o h√° como avaliar a plenitude e n√£o existe esse problema.  Eu nem sei como transmitir a voc√™ como √© agrad√°vel. </p><br><h2 id="nuzhno-zhdat-sp1">  "Precisa esperar pelo SP1" </h2><br><p>  Outra hist√≥ria de "sucesso".  Como voc√™ sabe, o RADOS √© o armazenamento de valor-chave mais primitivo.  O S3, implementado sobre o RADOS, tamb√©m √© primitivo, mas ainda um pouco mais funcional. ,  S3      .  ,   , RGW       .   ‚Äî  RADOS-,      OSD.         .   ,               . OSD            down.   ,      ,   .  ,   scrub'          .      ,    -  503,      . </p><br><p> <strong>Bucket Index resharding</strong> ‚Äî  ,       (RADOS-) , ,    OSD,         . </p><br><p> ,  ,        Jewel        !  Hammer,      ..    -.       ? </p><br><p>  Hammer       20+  ,       ,     OSD     Graylog ,    .     , ..   IO   .    Luminous, ..         .    Luminous,    , .   ,      . IO    index-,   ,         .    ,  IO     ,      . ,     ‚Ä¶   ; <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> : </p><br><p>  ,      .       , ..           ,   . </p><br><p> ,  Hammer-&gt;Jewel   -   . OSD     -  .        ,    OSD       . </p><br><p>    ‚Äî   ,     ,       .   Hammer    s3,   .      ,  .       ,    ,      etag,   body,     .           .      ,     . Suspend    .   ""           .            ,        . </p><br><h2 id="holivary-na-temu-chisla-replik">      </h2><br><p>       ,    2 ‚Äî  ,         Cloudmouse. ,    Ceph, , . </p><br><p>  vxFlex OS   2    . ,             .       ,    .           ,         .        ,    ,    ,     Dell EMC. </p><br><h2 id="proizvoditelnost">  </h2><br><p>    .       ,       ?  . ,      .   ,     Ceph, vxFlex          .       -  .        ,               . </p><br><p>   9   ceph-devel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> : ,     CPU  (  Xeon'  !)   IOPS  All-NVMe   Ceph 12.2.7  bluestore. </p><br><p> ,       ,  "" Ceph    .    (  Hammer)         Ceph    ,        s3     .  ,  ScaleIO  Ceph RBD   .   Ceph,     ‚Äî      CPU.       RDMA  InfiniBand, jemalloc   . ,    10-20 ,       iops,      io, Ceph      . vxFlex          .    ‚Äî  Ceph  system time,  scaleio ‚Äî io wait. ,    bluestore,      ,    ,         -, ,     Ceph.    ScaleIO  .  ,      , Ceph           Dell EMC. </p><br><p> ,       ,         PG.        (),     IO. -   PG       IO,     ,      . ,               nearfull.       ,    . </p><br><p>  vxFlex     -   ,      .       (   ceph-volume),         ,     . </p><br><h2 id="scrub"> Scrub </h2><br><p> , .  , ,      Ceph. </p><br><p>             ,      . " "    ‚Äî   -    ,    . ,      2 TB     &gt;50%,       Ceph,     .            .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ,       . </p><br><p>  vxFlex OS         ,    ,     .        ‚Äî bandwidth  .            .         ,        . </p><br><p> ,  , vxFlex     scrub-error. Ceph      2   . </p><br><h2 id="monitoring">  </h2><br><p> Luminous ‚Äî     .        .    MGR-     Zabbix              (3 ).       .   ,   ,  -         IO  ,     gc, .   ‚Äî   RGW . </p><br><p><img src="https://habrastorage.org/webt/ys/ya/xq/ysyaxqffjukjtgkv3ecnu0_q2ro.png"></p><br><p>       .     . <br>      S3,    "" : </p><br><p><img src="https://habrastorage.org/webt/ax/ge/u5/axgeu5iyyjami3qszjo97akj6oc.png"></p><br><p>   Ceph  , ,   ,       ,    . </p><br><p>  ,   eph   Graylog   GELF    .  , ,  OSD down, out, failed  .          , ,   OSD    down  ,     . </p><br><p><img src="https://habrastorage.org/webt/sd/hz/av/sdhzavl-jyjrmajz5zlsjamujzo.png"></p><br><p> - ,    OSD       heartbeat      failed (.  ).    <code>vm.zone_reclaim_mode=1</code>     NUMA. </p><br><p>     Ceph.  c vxFlex   .       : </p><br><p><img src="https://habrastorage.org/webt/hu/nz/e2/hunze2e6ucygc4anzyep2wku050.png"></p><br><p>     : </p><br><p><img src="https://habrastorage.org/webt/jv/fa/ey/jvfaeyd7ql3kalcrcabrckekuqg.png"></p><br><p>  IO    : </p><br><p><img src="https://habrastorage.org/webt/sy/vp/lq/syvplqbfmjtek_wii033vszoq9s.png"></p><br><p>              IO,      Ceph. </p><br><p>    : </p><br><p><img src="https://habrastorage.org/webt/h2/8j/ff/h28jff_jzpstuucf5wfx8vrrmay.png"></p><br><p>   Ceph,    Luminous     .   2.0,    Mimic  ,      . </p><br><h2 id="vxflex-tozhe-ne-idealen"> vxFlex    </h2><br><p>     <strong>Degraded state</strong> ,          . </p><br><p>  vxFlex ‚Äî        RH   .   7.5  , .  Ceph   RBD  cephfs ‚Äî          . </p><br><p> vxFlex       Ceph. vxFlex ‚Äî    ,   , , . </p><br><p>     16 PB,     .  eph     2 PB ‚Ä¶ </p><br><h2 id="zaklyuchenie">  Conclus√£o </h2><br><p>  ,  Ceph       ,     ,      ,      Ceph ‚Äî .      . </p><br><p>       ,  Ceph       " ".       ,  "  ,   ,     R&amp;D,  - ".    .       " ",  Ceph      ,    ,     . </p><br><p>      Ceph  2k18  ,    .      24/7       ( S3, ,  EBS),             ,   Ceph    .  ,    .        ‚Äî       .         /     maintenance        backfilling  ,  c Ceph   , ,       . </p><br><p>      Ceph    ?  , "     ".      Ceph.    .      ,         ,   ,  ,    ‚Ä¶ <br>    ! <br>  HEALTH_OK! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt422905/">https://habr.com/ru/post/pt422905/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt422895/index.html">Google quer matar URLs</a></li>
<li><a href="../pt422897/index.html">Ruim, mas meu: como escrever CSS realmente horr√≠vel</a></li>
<li><a href="../pt422899/index.html">Sob supervis√£o vigilante: como monitorar tarifas hoster e manter o cat√°logo VPS atualizado</a></li>
<li><a href="../pt422901/index.html">Um monitor de freq√º√™ncia card√≠aca para Putin, ou o que √© um Ritmer</a></li>
<li><a href="../pt422903/index.html">Como e por que escrevemos um servi√ßo escal√°vel altamente carregado para 1C: Enterprise: Java, PostgreSQL, Hazelcast</a></li>
<li><a href="../pt422907/index.html">Refer√™ncia r√°pida do aspirador de p√≥ 2018</a></li>
<li><a href="../pt422909/index.html">Os 10 v√≠deos retro talk mais populares do 404 Festival</a></li>
<li><a href="../pt422915/index.html">Estou procurando um idoso sem escrit√≥rio e cookies: como organizamos uma pesquisa por funcion√°rios 100% remotos</a></li>
<li><a href="../pt422917/index.html">N√£o tenho boca, mas tenho que gritar. Reflex√µes sobre IA e √©tica</a></li>
<li><a href="../pt422919/index.html">SIPs de bicicleta e conversas em telefonia na nuvem</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>