<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÖüèº üè® üóúÔ∏è End2 End Approach em tarefas de reconhecimento autom√°tico de fala üßîüèº üöº üôåüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O que √© o reconhecimento de fala End2End e por que √© necess√°rio? Qual √© a diferen√ßa da abordagem cl√°ssica? E por que, para treinar um bom modelo basea...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>End2 End Approach em tarefas de reconhecimento autom√°tico de fala</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/468663/">  O que √© o reconhecimento de fala End2End e por que √© necess√°rio?  Qual √© a diferen√ßa da abordagem cl√°ssica?  E por que, para treinar um bom modelo baseado no End2End, precisamos de uma enorme quantidade de dados - em nossa publica√ß√£o hoje. <br><br><h4>  A abordagem cl√°ssica ao reconhecimento de fala </h4><br>  Antes de falar sobre a abordagem End2End, voc√™ deve primeiro falar sobre a abordagem cl√°ssica do reconhecimento de fala.  Como ele √©? <br><br><img src="https://habrastorage.org/webt/xk/xm/sc/xkxmscrxxx0n2hvkoxdxyflqaoq.png"><br><a name="habracut"></a><br><h4>  Extra√ß√£o de recursos </h4><br>  De fato, essa n√£o √© uma sequ√™ncia completamente linear de blocos de a√ß√£o.  Vamos nos aprofundar em cada bloco com mais detalhes.  Temos algum tipo de discurso de entrada, ele cai no primeiro bloco - Extra√ß√£o de Recursos.  Este √© um bloco que puxa sinais da fala.  Deve-se ter em mente que a pr√≥pria fala √© uma coisa bastante complicada.  Voc√™ precisa trabalhar com ele de alguma forma, para que existam m√©todos padr√£o para isolar recursos da teoria do processamento de sinais.  Por exemplo, coeficientes Mel-cepstral (MFCC) e assim por diante. <br><br><h4>  Modelo ac√∫stico </h4><br>  O pr√≥ximo componente √© o modelo ac√∫stico.  Pode ser baseado em redes neurais profundas ou com base em misturas de distribui√ß√µes gaussianas e modelos ocultos de Markov.  Seu principal objetivo √© obter de uma se√ß√£o do sinal ac√∫stico as distribui√ß√µes de probabilidade de v√°rios fonemas nesta se√ß√£o. <br><br>  Em seguida, vem o decodificador, que procura o caminho mais prov√°vel no gr√°fico com base no resultado da √∫ltima etapa.  O resgate √© o toque final no reconhecimento, cuja principal tarefa √© pesar novamente as hip√≥teses e produzir o resultado final. <br><br><img src="https://habrastorage.org/webt/pp/hh/2i/pphh2ietnprdh5aeqtbvh06kzpu.png"><br><br>  Vamos falar mais detalhadamente do modelo ac√∫stico.  Como ela √©?  Temos algumas grava√ß√µes de voz que entram em um determinado sistema baseado no GMM (mono Gausovy mix) ou HMM.  Ou seja, temos representa√ß√µes na forma de fonemas, usamos monofones, ou seja, fonemas independentes do contexto.  Al√©m disso, fazemos misturas de distribui√ß√µes gaussianas baseadas em fonemas sens√≠veis ao contexto.  Ele usa clustering com base em √°rvores de decis√£o. <br><br>  Ent√£o tentamos construir alinhamento.  Um m√©todo completamente n√£o trivial nos permite obter um modelo ac√∫stico.  N√£o parece muito simples, na verdade √© ainda mais complicado, existem muitas nuances, recursos.  Mas, como resultado, um modelo treinado em centenas de horas √© muito capaz de simular ac√∫stica. <br><br><img src="https://habrastorage.org/webt/vt/-x/gl/vt-xgltq9lf3fujagn2jkqdriqy.png"><br><br><h4>  Decodificador </h4><br>  O que √© um decodificador?  Este √© o m√≥dulo que seleciona o caminho de transi√ß√£o mais prov√°vel de acordo com o gr√°fico HCLG, que consiste em 4 partes: <br><br>  M√≥dulo H baseado em HMM <br>  M√≥dulo de Depend√™ncia de Contexto C <br>  M√≥dulo de pron√∫ncia L <br>  M√≥dulo de modelo de linguagem G <br><br>  Constru√≠mos um gr√°fico sobre esses quatro componentes, com base nos quais decodificaremos nossos recursos ac√∫sticos em certas constru√ß√µes verbais. <br><br>  Mais ou menos, √© claro que a abordagem cl√°ssica √© bastante complicada e dif√≠cil, √© dif√≠cil de treinar, pois consiste em um grande n√∫mero de partes separadas, para cada uma das quais voc√™ precisa preparar seus pr√≥prios dados para treinamento. <br><br><h4>  II Abordagem End2End </h4><br>  Ent√£o, o que √© o reconhecimento de voz End2End e por que √© necess√°rio?  Este √© um determinado sistema, projetado para refletir diretamente a sequ√™ncia de sinais ac√∫sticos na sequ√™ncia de grafemas (letras) ou palavras.  Voc√™ tamb√©m pode dizer que este √© um sistema que otimiza crit√©rios que afetam diretamente a m√©trica final da avalia√ß√£o da qualidade.  Por exemplo, nossa tarefa √© especificamente a palavra taxa de erro.  Como eu disse, h√° apenas uma motiva√ß√£o - apresentar esses componentes complexos de v√°rios est√°gios como um componente simples que exibir√° diretamente, produzir√° palavras ou grafemas a partir do discurso de entrada. <br><br><h4>  Problema de simula√ß√£o </h4><br>  Aqui temos um problema imediatamente: a fala sonora √© uma sequ√™ncia e, na sa√≠da, tamb√©m precisamos dar uma sequ√™ncia.  E at√© 2006, n√£o havia maneira adequada de modelar isso.  Qual √© o problema da modelagem?  Havia a necessidade de cada registro criar uma marca√ß√£o complexa, o que implica em que segundo pronunciamos um som ou letra em particular.  Esse √© um layout complexo e muito complexo e, portanto, um grande n√∫mero de estudos sobre esse t√≥pico n√£o foi realizado.  Em 2006, foi publicado um interessante artigo de Alex Graves, ‚ÄúClassifica√ß√£o temporal conexionista‚Äù (CTC), no qual esse problema √©, em princ√≠pio, resolvido.  Mas o artigo foi publicado e n√£o havia poder computacional suficiente na √©poca.  E algoritmos de reconhecimento de fala de trabalho reais apareceram muito mais tarde. <br><br>  No total, temos: o algoritmo CTC foi proposto por Alex Graves h√° treze anos, como uma ferramenta que permite treinar / treinar modelos ac√∫sticos sem a necessidade dessa marca√ß√£o complexa - alinhamento dos quadros de sequ√™ncia de entrada e sa√≠da.  Com base nesse algoritmo, inicialmente apareceu um trabalho que n√£o estava completo no final do final; os fonemas foram emitidos como resultado.  Vale ressaltar que os fonemas sens√≠veis ao contexto baseados no STS alcan√ßam um dos melhores resultados no reconhecimento da liberdade de express√£o.  Mas tamb√©m vale a pena notar que esse algoritmo, aplicado diretamente √†s palavras, permanece em algum lugar no momento. <br><br><img src="https://habrastorage.org/webt/ns/oe/e2/nsoee2tomucbndnfsaj__7va-o4.png"><br><br><h4>  O que √© STS </h4><br>  Agora falaremos um pouco mais detalhadamente sobre o que √© o STS e por que √© necess√°rio, que fun√ß√£o ele executa.  O STS √© necess√°rio para treinar o modelo ac√∫stico sem a necessidade de alinhamento quadro a quadro entre som e transcri√ß√£o.  O alinhamento quadro a quadro ocorre quando dizemos que um quadro espec√≠fico de um som corresponde a um quadro da transcri√ß√£o.  Temos um codificador convencional que aceita sinais ac√∫sticos como entrada - ele fornece algum tipo de oculta√ß√£o do estado, com base no qual obtemos probabilidades condicionais usando o softmax.  O codificador geralmente consiste em v√°rias camadas de LSTMs ou outras varia√ß√µes de RNNs.  Vale ressaltar que o STS opera al√©m dos caracteres comuns com um caractere especial chamado caractere vazio ou s√≠mbolo em branco.  Para resolver o problema que surge devido ao fato de que nem todo quadro ac√∫stico possui um quadro na transcri√ß√£o e vice-versa (ou seja, temos letras ou sons que soam muito mais longos e h√° sons curtos, repetidos), e h√° este s√≠mbolo em branco. <br><br>  O STS propriamente dito visa maximizar a probabilidade final de sequ√™ncias de caracteres e generalizar o poss√≠vel alinhamento.  Como queremos usar esse algoritmo em redes neurais, entende-se que devemos entender como funcionam seus modos de opera√ß√£o para frente e para tr√°s.  N√£o vamos nos debru√ßar sobre a justificativa matem√°tica e os recursos da opera√ß√£o desse algoritmo, caso contr√°rio, isso levar√° muito tempo. <br><br>  O que temos: o primeiro ASR baseado no algoritmo STS aparece em 2014.  Mais uma vez, Alex Graves apresentou uma publica√ß√£o baseada no STS caractere por caractere que exibe diretamente a fala de entrada em uma sequ√™ncia de palavras.  Um dos coment√°rios que eles fizeram neste artigo √© que o uso de um modelo de som externo √© importante para obter um bom resultado. <br><br><h4>  5 maneiras de melhorar o algoritmo </h4><br>  Existem muitas varia√ß√µes e melhorias diferentes no algoritmo acima.  Aqui est√£o, por exemplo, os cinco mais populares recentemente. <br><br>  ‚Ä¢ O modelo de linguagem est√° inclu√≠do na decodifica√ß√£o durante a primeira passagem <br>  o [Hannun et al., 2014] [Maas et al., 2015]: decodifica√ß√£o direta de primeira passagem com um LM em vez de resgatar como em [Graves &amp; Jaitly, 2014] <br>  o [Miao et al., 2015]: estrutura EESEN para decodifica√ß√£o com WFSTs, kit de ferramentas de c√≥digo aberto <br>  ‚Ä¢ Treinamento em larga escala na GPU;  Aumento de Dados  v√°rias l√≠nguas <br>  o [Hannun et al., 2014;  DeepSpeech] [Amodei et al., 2015;  DeepSpeech2]: treinamento em GPU em larga escala;  Aumento de Dados;  Mandarim e Ingl√™s <br>  ‚Ä¢ Uso de unidades longas: palavras em vez de caracteres <br>  o [Soltau et al., 2017]: metas de CTC no n√≠vel da palavra, treinadas em 125.000 horas de fala.  Desempenho pr√≥ximo ou melhor que um sistema convencional, mesmo sem o uso de um LM! <br>  o [Audhkhasi et al., 2017]: Modelos de ac√∫stica direta para palavra no quadro de distribui√ß√£o <br><br>  Vale a pena prestar aten√ß√£o √† implementa√ß√£o do DeepSpeach como um bom exemplo de solu√ß√£o CTC end2end e a uma varia√ß√£o que usa um n√≠vel verbal.  Mas h√° uma ressalva: para treinar esse modelo, voc√™ precisa de 125 mil horas de dados rotulados, o que na verdade √© bastante real. <br><br><h4>  O que √© importante observar sobre o STS </h4><br><ul><li>  Quest√µes ou omiss√µes.  Para efici√™ncia, √© importante fazer suposi√ß√µes sobre independ√™ncia.  Ou seja, o STS pressup√µe que a sa√≠da da rede em diferentes quadros seja condicionalmente independente, o que √© realmente incorreto.  Mas essa suposi√ß√£o √© feita para simplificar, sem ela, tudo se torna muito mais complicado. </li><li>  Para obter um bom desempenho do modelo STS, √© necess√°rio o uso de um modelo de linguagem externo, pois a decodifica√ß√£o direta gananciosa n√£o funciona muito bem. </li></ul><br><h4>  Aten√ß√£o </h4><br>  Que alternativa temos para este STS?  Provavelmente n√£o √© segredo para ningu√©m que existe algo como Aten√ß√£o ou "Aten√ß√£o", que revolucionou em certa medida e foi diretamente das tarefas de tradu√ß√£o autom√°tica.  E agora a maioria de todas as decis√µes de modelagem de sequ√™ncia de sequ√™ncia s√£o baseadas nesse mecanismo.  Como ele √©?  Vamos tentar descobrir.  Pela primeira vez sobre Aten√ß√£o nas tarefas de reconhecimento de fala, as publica√ß√µes apareceram em 2015.  Algu√©m Chen e Cherowski publicaram duas publica√ß√µes semelhantes e diferentes ao mesmo tempo. <br><br>  Vamos nos debru√ßar sobre o primeiro - chamado Ouvir, assistir e soletrar.  Em nossa simula√ß√£o cl√°ssica, na sequ√™ncia em que temos um codificador e um decodificador, outro elemento √© adicionado, chamado aten√ß√£o.  O echnoder executar√° as fun√ß√µes que o modelo ac√∫stico costumava executar.  Sua tarefa √© transformar o discurso de entrada em recursos ac√∫sticos de alto n√≠vel.  Nosso decodificador executar√° as tarefas que executamos anteriormente no modelo de idioma e modelo de pron√∫ncia (l√©xico); ele prever√° automaticamente cada token de sa√≠da, como uma fun√ß√£o dos anteriores.  E a pr√≥pria aten√ß√£o dir√° diretamente qual quadro de entrada √© mais relevante / importante para prever essa sa√≠da. <br><br><img src="https://habrastorage.org/webt/gi/hd/1c/gihd1cppd9nsldy12rqffbuheoe.png"><br><br>  O que s√£o esses blocos?  O codificador ecol√≥gico no artigo √© descrito como um ouvinte, √© um RNN bidirecional cl√°ssico baseado em LSTMs ou qualquer outra coisa.  Em geral, nada de novo - o sistema simplesmente simula a sequ√™ncia de entrada em recursos complexos. <br><br>  Aten√ß√£o, por outro lado, cria um certo vetor de contexto C a partir desses vetores, o que ajudar√° a decodificar o decodificador corretamente diretamente, o pr√≥prio decodificador, que √©, por exemplo, tamb√©m alguns LSTMs que ser√£o decodificados na sequ√™ncia de entrada dessa camada de aten√ß√£o, que j√° destacou os sinais de estado mais importantes, alguma sequ√™ncia de sa√≠da de caracteres. <br><br>  Tamb√©m existem diferentes representa√ß√µes dessa pr√≥pria aten√ß√£o - que √© a diferen√ßa entre essas duas publica√ß√µes publicadas por Chen e Charowski.  Eles usam aten√ß√£o diferente.  Chen usa Aten√ß√£o com produtos pontuais e Charowski usa Aten√ß√£o com Aditivos. <br><br><img src="https://habrastorage.org/webt/r4/oa/tu/r4oatu1zxse9xn13cmmbx01viem.png"><br><br><h4>  Para onde ir a seguir? </h4><br>  Trata-se de mais ou menos todas as principais realiza√ß√µes recebidas at√© o momento em quest√µes de reconhecimento de fala n√£o on-line.  Que melhorias s√£o poss√≠veis aqui?  Para onde ir a seguir?  O mais √≥bvio √© o uso de um modelo em peda√ßos de palavras, em vez de usar grafemas diretamente.  Pode ser alguns morfemas separados ou algo mais. <br><br>  Qual √© a motiva√ß√£o para usar fatias de palavras?  Normalmente, os modelos de linguagem do n√≠vel verbal t√™m muito menos perplexidade em compara√ß√£o com o n√≠vel do grafema.  A modelagem de palavras permite criar um decodificador mais forte do modelo de linguagem.  E a modelagem de elementos mais longos pode melhorar a efici√™ncia da mem√≥ria em um decodificador baseado em LSTMs.  Tamb√©m permite que voc√™ se lembre potencialmente da ocorr√™ncia de palavras de frequ√™ncia.  Elementos mais longos permitem decodificar em menos etapas, o que acelera diretamente a infer√™ncia deste modelo. <br><br>  Al√©m disso, o modelo em peda√ßos de palavras nos permite resolver o problema das palavras OOV (fora do vocabul√°rio) que surgem no modelo de linguagem, uma vez que podemos modelar qualquer palavra usando peda√ßos de palavras.  E vale a pena notar que esses modelos s√£o treinados para maximizar a probabilidade de um modelo de linguagem em um conjunto de dados de treinamento.  Esses modelos s√£o dependentes da posi√ß√£o e podemos usar o algoritmo guloso para decodificar. <br><br>  Que outras melhorias al√©m do modelo de palavras podem ser?  Existe um mecanismo chamado aten√ß√£o m√∫ltipla.  Foi descrito pela primeira vez em 2017 para tradu√ß√£o autom√°tica.  A aten√ß√£o de v√°rias cabe√ßas implica um mecanismo que possui v√°rias chamadas cabe√ßas que permitem gerar uma distribui√ß√£o diferente dessa mesma aten√ß√£o, o que melhora os resultados diretamente. <br><br><h4>  Modelos online </h4><br>  Passamos para a parte mais interessante - estes s√£o modelos online.  √â importante observar que o LAS n√£o est√° sendo transmitido.  Ou seja, este modelo n√£o pode funcionar no modo de decodifica√ß√£o online.  Vamos considerar os dois modelos online mais populares at√© o momento.  Transdutor RNN e Transdutor Neural. <br><br>  O transdutor RNN foi proposto por Graves em 2012-2017.  A id√©ia principal √© complicar um pouco nosso modelo STS com a ajuda de um modelo recursivo. <br><br><img src="https://habrastorage.org/webt/5c/y6/lc/5cy6lcuv2l7q1nymrr9j4idp5pi.png"><br><br>  Vale a pena notar que os dois componentes s√£o treinados juntos com os dados ac√∫sticos dispon√≠veis.  Como o STS, essa abordagem n√£o requer alinhamento de quadros no conjunto de dados de treinamento.  Como podemos ver na figura: √† esquerda est√° o nosso STS cl√°ssico e √† direita est√° o transdutor RNN.  E temos dois novos elementos - a <b>rede prevista</b> e a <b>rede de jun√ß√£o</b> . <br><br>  O codificador STS √© exatamente o mesmo - este √© o n√≠vel de entrada RNN, que determina a distribui√ß√£o em todos os alinhamentos com todas as seq√º√™ncias de sa√≠da que n√£o excedem o comprimento da sequ√™ncia de entrada - isso foi descrito por Graves em 2006.  No entanto, a tarefa dessas convers√µes de texto em fala tamb√©m √© exclu√≠da, onde a sequ√™ncia de entrada maior que a sequ√™ncia de entrada do STS n√£o modela o relacionamento entre as sa√≠das.  O transdutor expande esse mesmo STS, determinando a distribui√ß√£o das seq√º√™ncias de sa√≠da de todos os comprimentos e modelando em conjunto a depend√™ncia da entrada-sa√≠da e sa√≠da-sa√≠da. <br><br>  Acontece que nosso modelo √© capaz de lidar com as depend√™ncias da sa√≠da da entrada e da sa√≠da da √∫ltima etapa. <br><br>  Ent√£o, o que √© uma <b>rede prevista</b> ou uma rede preditiva?  Ela tenta modelar cada elemento levando em considera√ß√£o os anteriores, portanto, √© semelhante ao RNN padr√£o com a previs√£o da pr√≥xima etapa.  Somente com a capacidade adicional de fazer hip√≥teses nulas. <br><br>  Como podemos ver na figura, temos uma rede prevista, que recebe o valor anterior da sa√≠da, e h√° um codificador, que recebe o valor atual da entrada.  E na sa√≠da n√≥s novamente, esse tem o valor atual <img src="https://habrastorage.org/webt/x4/db/gw/x4dbgwm67dwzli8xp3ysvhqm-tu.png">  . <br><br>  <b>Transdutor Neural</b> .  Esta √© uma complica√ß√£o da abordagem cl√°ssica seq-2seq.  A sequ√™ncia ac√∫stica de entrada √© processada pelo codificador para criar vetores de estado ocultos a cada etapa do tempo.  Tudo parece ser como sempre.  Mas h√° um elemento transdutor adicional que recebe um bloco de entrada a cada etapa e gera at√© tokens de sa√≠da M usando o modelo baseado em seq-2seq acima dessa entrada.  O transdutor mant√©m seu estado em blocos usando conex√µes peri√≥dicas com as etapas de tempo anteriores. <br><br><img src="https://habrastorage.org/webt/hh/pn/wx/hhpnwx3l2sco6phy37tmuawvcni.png"><br><br>  <i>A figura mostra o transdutor, produzindo tokens para o bloco para a sequ√™ncia usada no bloco do Ym correspondente.</i> <br><br>  Ent√£o, examinamos o estado atual do reconhecimento de fala com base na abordagem End2End.  Vale dizer que, infelizmente, essas abordagens hoje exigem uma grande quantidade de dados.  E os resultados reais alcan√ßados pela abordagem cl√°ssica, exigindo de 200 a 500 horas de grava√ß√µes sonoras marcadas para o treinamento de um bom modelo baseado no End2End, exigir√£o v√°rios, ou talvez dezenas de vezes mais dados.  Agora, este √© o maior problema com essas abordagens.  Mas talvez em breve tudo mude. <br><br>  <i>Desenvolvedor l√≠der do centro de AI MTS Nikita Semenov.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt468663/">https://habr.com/ru/post/pt468663/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt468641/index.html">Tutorial para criar solu√ß√£o Blockchain no Hyperledger Composer</a></li>
<li><a href="../pt468645/index.html">Duas p√°ginas foram suficientes para provar a hip√≥tese de 30 anos no campo da ci√™ncia da computa√ß√£o.</a></li>
<li><a href="../pt468647/index.html">M√∫sica arriscada em uma antiga impressora de linha de mainframe IBM</a></li>
<li><a href="../pt468653/index.html">Qual √© a resolu√ß√£o do olho humano (ou quantos megapixels vemos em um determinado momento)</a></li>
<li><a href="../pt468657/index.html">Dan√ßas com apoio: tipos e formas de apoio. Sistemas de suporte trabalhando em batalha</a></li>
<li><a href="../pt468665/index.html">Mas √© hora de comprar um irrigador?</a></li>
<li><a href="../pt468677/index.html">O an√∫ncio do smartphone Xiaomi Mi Mix Alpha</a></li>
<li><a href="../pt468679/index.html">Os ABCs de seguran√ßa no Kubernetes: autentica√ß√£o, autoriza√ß√£o, auditoria</a></li>
<li><a href="../pt468683/index.html">Teoria e pr√°tica da padroniza√ß√£o de servi√ßos Docker</a></li>
<li><a href="../pt468687/index.html">Analisamos as novas iniciativas do Banco Central para regular o mercado de a√ß√µes: 3 grupos de investidores, restri√ß√µes para iniciantes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>