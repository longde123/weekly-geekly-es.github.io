<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏿‍🤝‍👩🏾 👨🏾‍✈️ 👩🏿‍🚒 Cache adalah raja kinerja: apakah prosesor memerlukan tingkat caching keempat 🌠 🗝️ 🤰🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kesenjangan antara kecepatan prosesor dalam arti umum dan kecepatan DRAM utama, juga dalam arti umum, telah menjadi masalah dalam 30 tahun terakhir - ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cache adalah raja kinerja: apakah prosesor memerlukan tingkat caching keempat</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485374/"><img src="https://habrastorage.org/getpro/habr/post_images/2ac/73c/219/2ac73c219095e9b2a9f772e767ba9cb7.jpg"><br><br>  Kesenjangan antara kecepatan prosesor dalam arti umum dan kecepatan DRAM utama, juga dalam arti umum, telah menjadi masalah dalam 30 tahun terakhir - selama periode ini, kesenjangan mulai benar-benar tumbuh.  Dan layak dikatakan secara jujur ​​bahwa para insinyur yang mengembangkan baik peralatan maupun program yang menciptakan hierarki cache dan perangkat lunak yang dapat memanfaatkannya melakukannya dengan sangat cemerlang.  Ini adalah salah satu arsitektur paling sulit yang pernah disusun oleh manusia. <br><br>  Namun, sekarang kita berada di ambang hirarki memori yang terus berkembang, ketika memori non-volatile seperti Optane 3D XPoint (varian memori dengan perubahan fasa) dalam format DIMM dan SSD mulai muncul, serta protokol baru (CXL, OpenCAPI, CCIX, NVLink dan Gen-Z), muncul pertanyaan: apakah sudah waktunya menambahkan cache tingkat keempat ke server?  Karena pekerjaan sejumlah perangkat tergantung pada kompleks CPU - beberapa di antaranya lebih dekat, sementara yang lain lebih jauh - masuk akal untuk memikirkan apakah kita memerlukan tingkat cache lain yang menutupi penundaan jenis memori lain ini dan meningkatkan throughput seluruh sistem. <br><a name="habracut"></a><br>  Untuk memperkenalkan peluang, kami mencari-cari di memori kami sendiri, dan pada saat yang sama kami berbicara dengan pengembang arsitektur chip dari IBM, Intel, AMD dan Marvell untuk memahami apa yang mereka pikirkan tentang menggunakan cache L4 di server.  Cache L4, tentu saja, bukan kata baru dalam kecepatan, tetapi tidak begitu umum dalam arsitektur sistem. <br><br>  Namun, sebelum kita harus membahas sejarah masalah ini. <br><br>  Menambahkan cache tingkat pertama ke prosesor yang hanya memiliki satu inti pada saat itu adalah kompromi pada 1980-an yang menambahkan latensi ke subsistem memori sambil mengurangi rata-rata latensi permintaan data dan instruksi dari prosesor.  L1 cache awalnya terletak di SRAM eksternal yang terletak di motherboard dan terhubung ke kompleks CPU-memory.  Cache L1 seperti itu sangat dekat dengan prosesor, baik dalam hal frekuensi clock dan dalam hal ruang fisik di papan tulis, dan memungkinkan untuk meningkatkan beban CPU.  Kemudian cache ini dibagi sehingga data yang sering digunakan dapat disimpan dalam satu blok, dan instruksi populer di blok kedua, dan ini sedikit meningkatkan kinerja.  Pada beberapa titik dalam peningkatan kecepatan clock prosesor dan kesenjangan yang sesuai dalam kecepatan CPU dan DRAM, lebih banyak lemak, tetapi juga cache L2 yang lebih lambat ditambahkan (tetapi lebih murah dalam hal bandwidth), sekali lagi pada awalnya mereka berada di luar kasing CPU, tetapi kemudian diintegrasikan ke dalamnya.  Dan ketika semakin banyak inti mulai ditambahkan ke CPU, serta semakin banyak pengontrol DRAM untuk memuatnya, blok cache L3 yang lebih besar ditambahkan ke hierarki. <br><br>  Sebagian besar, sistem seperti itu bekerja dengan sangat baik.  Di beberapa sirkuit CPU, kami bahkan melihat aturan praktis tertentu yang mencerminkan level hierarki cache, yang akan memungkinkan kami untuk memperkirakan kemungkinan yang terkait dengan level keempat. <br><br>  Chris Gianos, seorang insinyur chip dan arsitek di Intel, yang telah memimpin pengembangan banyak generasi prosesor Xeon, menjelaskan hal ini: "Dengan setiap level cache, kita biasanya membutuhkan mereka untuk tumbuh cukup kuat dari level sebelumnya, sehingga semuanya masuk akal, karena untuk mencapai peningkatan kinerja sistem yang nyata, Anda harus mencapai frekuensi panggilan yang berhasil yang agak menarik.  Jika Anda “jatuh ke dalam” data cache hanya dalam beberapa persen kasus, akan sulit untuk diperhatikan.  Segala sesuatu yang lain memperlambat kecepatan Anda, dan peningkatan ini tidak akan terlihat.  Oleh karena itu, cache yang relatif besar diperlukan, dan ketika sampai pada level yang lebih tinggi, cache yang sangat besar diperlukan.  Saat ini, L2 diukur dalam megabita, L3 diukur dalam puluhan atau ratusan megabita.  Jadi jelas bahwa jika Anda mulai berpikir tentang cache L4, maka kita akan berbicara tentang ratusan megabyte, jika bukan gigabyte.  Dan ukuran seperti itu pasti akan menyebabkan biaya tinggi.  Diperlukan adanya kondisi tertentu, agar opsi ini menjadi menarik, dan tentu saja tidak akan murah. ” <br><br>  Insinyur AMD yang kami wawancarai ingin tetap anonim karena mereka tidak ingin memberi kesan bahwa perusahaan akan menambahkan cache L4 ke jalur prosesor Epyc - dan, tepatnya, AMD tidak menjanjikan hal seperti itu.  Namun, perusahaan tersebut mengakui bahwa ini adalah langkah nyata berikutnya yang perlu dipertimbangkan, dan, seperti halnya Intel, ia percaya bahwa semua insinyur sedang berpikir untuk mengimplementasikan cache L4.  Pada dasarnya, AMD mengatakan bahwa pertukaran yang terkait dengan tingkat cache dan latensi telah dipelajari secara luas baik di industri dan akademisi, dan bahwa dengan setiap tingkat baru yang lebih besar dan lebih lambat dari yang sebelumnya, ada pertukaran dalam meningkatkan jalur keseluruhan ke DRAM.  Ini juga ditunjukkan oleh Intel Gianos, berbicara tentang perlunya menemukan keseimbangan antara permintaan cache yang berhasil dan volumenya. <br><br>  IBM, tentu saja, menambahkan cache L4 ke beberapa chipset X86 pada 2000-an, dan pada 2010 menambahkan L4 ke chipset NUMA ( <a href="https://ru.wikipedia.org/wiki/Non-Uniform_Memory_Access" rel="nofollow">akses memori tidak merata</a> ) pada mainframe System z11.  Prosesor z11 memiliki empat inti, cache L1 64 KB untuk instruksi dan cache L1 128 KB untuk data, ditambah cache L2 1,5 MB untuk setiap core dan 24 MB L3 cache bersama untuk semua core.  Chipset NUMA untuk z10 memiliki dua bank 96 L4 cache, yaitu total 192 MB.  Dengan merilis z12, IBM mengurangi ukuran cache L1 menjadi 98 KB per core, tetapi meningkatkan cache L2 menjadi 2 MB per core, membaginya menjadi dua bagian, untuk instruksi dan untuk data, seperti dalam kasus L1.  Dia juga menggandakan ukuran cache L3 menjadi 48 MB untuk enam core, dan ukuran cache L4 meningkat menjadi 384 MB untuk sepasang chip dalam chipset.  Saat generasi prosesor System z berubah, ukuran cache bertambah, dan untuk prosesor z15 yang diumumkan pada bulan September, sepasang cache L1 akan memiliki berat masing-masing 128 KB, sepasang cache L2 akan memiliki berat masing-masing 4 MB, dan cache L3 yang dibagi untuk 256 core akan memiliki kapasitas 256 MB.  Cache L4 di setiap ruang mainframe adalah 960 MB, dan total volume untuk seluruh sistem, yang terdiri dari lima rongga, adalah 4,68 GB. <br><br>  Seperti yang kami <a href="https://www.nextplatform.com/2018/08/28/ibm-power-chips-blur-the-lines-to-memory-and-accelerators/" rel="nofollow">tunjukkan sebelumnya</a> , prosesor Power8 dan Power9 memiliki buffered memory, dan IBM menambahkan 16 MB L4 cache ke setiap buffer Centaur, yang merupakan 128 MB L4 cache per socket untuk 32 slot memori.  Mesin Power9 termurah tidak memiliki buffer memori, dan karenanya tidak ada cache L4.  Arsitek yang mengembangkan sirkuit Power10 sibuk mengembangkan sirkuit untuk Power11, dan karena itu tidak dapat menjawab pertanyaan kami, tetapi William Stark, yang mengelola pengembangan Power10, menemukan sedikit waktu untuk kami, dan memperhatikan hal berikut: <br><br>  "Secara umum, kami sampai pada kesimpulan bahwa cache tingkat besar dari level terakhir berguna untuk meningkatkan kecepatan sistem industri," Stark menjelaskan kepada kami melalui email.  "Latensi tinggi yang terkait dengan memori non-volatile, khususnya dengan fase-state memory, menghasilkan permintaan cache - mungkin untuk cache tipe L4 - dalam hirarki memori penyimpanan." <br><br>  Itulah tepatnya yang kami pikirkan.  Dan omong-omong, kami tidak mengklaim bahwa cache L4 akan selalu berada di dekat dengan memori buffered DIMM DDR5 masa depan.  Mungkin lebih baik untuk menempatkannya di antara cache prosesor PCI-Express dan L3, dan bahkan lebih baik, di buffer memori dan antara cache PCI-Express dan prosesor L3.  Mungkin itu harus ditempatkan di atas I / O controller dan memori dalam arsitektur server masa depan, yang sedikit mirip <a href="https://www.nextplatform.com/2018/12/13/intel-bets-heavily-on-chip-stacking-for-the-future-of-compute/" rel="nofollow">dengan teknologi Intel Foveros</a> . <br><br>  Dimungkinkan untuk melihat ini dari sudut pandang yang berbeda - misalnya, IBM memiliki kesempatan untuk mengubah ukuran kristal, dan para insinyur memutuskan untuk menambahkan cache L4 ke bus Sistem z NUMA atau ke chip buffering memori Power8 dan Power9, bukan untuk kepentingannya sendiri, tetapi hanya karena mereka masih memiliki kesempatan untuk menambahkan transistor setelah semua fungsi yang diperlukan diimplementasikan.  Kadang-kadang tampak bagi kita bahwa jumlah inti dalam prosesor Intel X86 tergantung pada ukuran cache L3 yang mereka mampu.  Kadang-kadang tampaknya Intel menetapkan ukuran maksimum cache L3 untuk satu kristal, dan setelah itu kristal Xeon dengan tiga ukuran berbeda dibuat sesuai dengan spesifikasi ini - pada generasi terbaru mereka memiliki 10, 18 atau 28 core pada proses manufaktur 14 nm. <br><br>  Semua ini, tentu saja, murni masalah akademis, tetapi memberi kita motivasi potensial bagi IBM dan produsen chipset lainnya untuk menambahkan cache L4.  Tidak hanya ini dapat membantu dalam beberapa kasus, itu hanya hal yang sangat jelas.  Kami berpikir bahwa pada rakasa I / O seperti mainframe System z, cache L4 ada di tempatnya tanpa ada pertanyaan dan menguntungkan semua pelanggan, meningkatkan throughput mesin ini dan memungkinkan mereka untuk bekerja pada beban prosesor 98-99%, karena berapa banyak inti , dan skala NUMA di mainframe telah berkembang banyak belakangan ini. <br><br>  Tidak ada alasan untuk membuat L4 cache secara eksklusif pada DRAM internal (seperti yang dilakukan IBM dengan chip-nya) atau berdasarkan SRAM yang jauh lebih mahal - inilah yang Rabin Sugumar, arsitek chip dari Cray Research, Sun Microsystems, Oracle, Broadcom mengingatkan kita. , Kavium dan Marvell: <br><br>  "L3 cache kami sudah cukup besar," kata Sugumar.  - Jadi L4 dalam hal Anda tertarik perlu dilakukan menggunakan teknologi yang berbeda.  Mungkin eDRAM atau bahkan HBM atau DRAM.  Dalam konteks ini, implementasi cache L4 berbasis HBM terlihat seperti opsi yang menarik, dan cache ini tidak memecahkan banyak masalah latensi seperti bandwidth.  Karena kapasitas HBM terbatas dan bandwidth besar, kami bisa mendapatkan peningkatan kecepatan tertentu - dan dalam beberapa kasus khusus kami benar-benar melihat peningkatan bandwidth yang signifikan. "  Sugumar menambahkan bahwa untuk jumlah aplikasi yang cukup besar, jumlah cache yang diamati cukup banyak.  Namun, Anda perlu menghitung apakah menambahkan level cache berikutnya akan sepadan. <br><br>  Kasus penggunaan lain yang mungkin untuk sesuatu seperti L4 cache, kata Sugumar, adalah menggunakan DRAM lokal sebagai cache.  “Kami tidak melakukan penelitian seperti itu di laboratorium, tetapi anggaplah kami memiliki antarmuka bandwidth tinggi pada chip, terhubung ke memori bersama di suatu tempat di ujung lain dari loop, pada jarak 500 ns hingga 1 μs.  Maka salah satu kasus penggunaan adalah untuk membuat cache yang memindahkan data ini dari memori bersama ke DRAM lokal.  Anda dapat membayangkan pekerjaan mesin negara mengelola memori ini, sehingga sebagian besar waktu panggilan akan pergi ke DRAM lokal, dan Anda dapat meminimalkan jumlah panggilan ke DRAM yang didistribusikan secara umum. " <br><br>  Bagi kami, opsi ini sangat menarik bagi NUMA.  Omong-omong, Sugumar sedang mengerjakan memori terdistribusi untuk sistem paralel berkecepatan tinggi di Sun Microsystems bahkan sebelum memori non-volatile muncul.  Dan salah satu masalah dengan varian yang berbeda dari hirarki memori ini adalah bahwa jika salah satu dari mereka hilang karena kegagalan jaringan atau bus, maka seluruh mesin akan crash.  "Pada sistem memori terdistribusi, kegagalan jaringan harus ditangani dengan lebih elegan, dan ini menyebabkan banyak tantangan desain." <br><br>  Poin lain adalah bahwa kita ingin cache tingkat tinggi, bahkan L4, direalisasikan secara maksimal dengan bantuan besi dan dengan minimum dengan bantuan perangkat lunak.  Kernel sistem operasi dan perangkat lunak lain selalu memerlukan waktu untuk mengejar perangkat kerasnya, apakah itu menambah kernel baru, atau cache L3 atau L4, atau memori non-volatile yang dapat dialamatkan. <br><br>  "Pada titik tertentu, level cache tambahan akan menjadi tak terhindarkan," kata Gianos.  - Kami mendapat level cache pertama, dan pada titik tertentu yang kedua muncul.  Dan akhirnya kami menambahkan yang ketiga.  Dan suatu hari nanti kita akan memiliki yang keempat.  Satu-satunya pertanyaan adalah kapan dan mengapa.  Dan menurut saya pengamatan Anda tentang kemampuan cache ini cukup menarik.  Tetapi Intel belum memutuskan kapan atau mengapa hal-hal seperti itu akan dipublikasikan.  Perusahaan lain juga mempelajari masalah ini;  bodoh jika tidak memeriksanya.  Cepat atau lambat ini akan terjadi, tetapi segera akan, atau tidak terlalu - belum jelas. " </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id485374/">https://habr.com/ru/post/id485374/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id485358/index.html">Apakah pengembangan seluler mudah dan membosankan? Laporan Yandex</a></li>
<li><a href="../id485362/index.html">Inovasi JavaScript ES2020 dengan contoh sederhana</a></li>
<li><a href="../id485364/index.html">Cara berhenti membuang waktu pengembang pada utang teknis</a></li>
<li><a href="../id485370/index.html">Bagaimana pengembang dapat membantu manajer membuat kesepakatan</a></li>
<li><a href="../id485372/index.html">Tentang yang tidak berubah: sejarah tempat ke-9 Piala AI Rusia 2019</a></li>
<li><a href="../id485376/index.html">Bagaimana membuat frontend tiga kali lebih cepat dan kapan menerapkan perintah alih-alih repositori? Video</a></li>
<li><a href="../id485378/index.html">Studi Kasus: Cara Mendapatkan Fitur di Google Play dan Beradaptasi ASO ke Berbagai Negara</a></li>
<li><a href="../id485380/index.html">Craft dan Sukses TI</a></li>
<li><a href="../id485384/index.html">NeurIPS 2019: Tren ML yang akan bersama kita untuk dekade berikutnya</a></li>
<li><a href="../id485386/index.html">Microbrows ada di mana-mana. Tapi apa yang kita ketahui tentang mereka?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>