<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕺🏻 🗺️ 👨🏽‍🤝‍👨🏼 Aperçu des solutions AI et ML en 2018 et prévisions pour 2019: Partie 1 - PNL, vision par ordinateur 🖍️ ⤴️ 💻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour à tous! Je vous présente une traduction d'un article d' Analytics Vidhya avec un aperçu des événements AI / ML dans les tendances 2018 et 2019...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aperçu des solutions AI et ML en 2018 et prévisions pour 2019: Partie 1 - PNL, vision par ordinateur</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439688/"><blockquote> Bonjour à tous!  Je vous présente une traduction d'un article d' <i>Analytics Vidhya</i> avec un aperçu des événements AI / ML dans les tendances 2018 et 2019.  Le matériau est assez grand, il est donc divisé en 2 parties.  J'espère que l'article intéressera non seulement les spécialistes spécialisés, mais aussi ceux qui s'intéressent au sujet de l'IA.  Bonne lecture! <br><br><div class="spoiler">  <b class="spoiler_title">Navigation dans l'article</b> <div class="spoiler_text">  <b>Partie 1</b> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Traitement du langage naturel (PNL)</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tendances PNL pour 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Vision par ordinateur</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tendances de la vision industrielle pour 2019</a> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2e partie</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Outils et bibliothèques</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tendances AutoML pour 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage par renforcement</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tendances d'apprentissage par renforcement pour 2019</a> <br>  - L' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">IA pour les bons garçons - évolution vers une IA «éthique»</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tendances éthiques de l'IA pour 2019</a> <br></div></div></blockquote><br><h2>  Présentation </h2><br>  Les dernières années pour les passionnés de l'IA et les professionnels du machine learning se sont écoulées dans la poursuite d'un rêve.  Ces technologies ont cessé d'être des créneaux, sont devenues courantes et affectent déjà la vie de millions de personnes en ce moment.  Des ministères de l'IA ont été créés dans différents pays [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plus de détails ici</a> - env.  par.] et les budgets sont alloués pour suivre cette course. <br><br>  Il en va de même pour les professionnels de la science des données.  Il y a quelques années, vous pouviez vous sentir à l'aise avec quelques outils et astuces, mais ce temps est révolu.  Le nombre d'événements récents en science des données et la quantité de connaissances nécessaires pour suivre le rythme dans ce domaine sont incroyables. <br><br>  J'ai décidé de prendre du recul et de regarder les développements dans certains domaines clés dans le domaine de l'intelligence artificielle du point de vue des experts en science des données.  Quelles évasions se sont produites?  Que s'est-il passé en 2018 et à quoi s'attendre en 2019?  Lisez cet article pour des réponses! <a name="habracut"></a><br><br>  PS Comme dans toute prévision, voici mes conclusions personnelles basées sur les tentatives de combiner des fragments individuels dans l'image entière.  Si votre point de vue est différent du mien, je serai heureux de connaître votre opinion sur ce qui pourrait changer dans la science des données en 2019. <br><br>  Les domaines que nous aborderons dans cet article sont: <br><br>  - Processus de langage naturel (PNL) <br>  - Vision par ordinateur <br>  - Outils et bibliothèques <br>  - Apprentissage par renforcement <br>  - Problèmes éthiques en IA <br><br><a name="NLP"></a><h2>  Traitement du langage naturel (PNL) </h2><br>  Forcer les machines à analyser des mots et des phrases a toujours semblé être un rêve chimérique.  Il y a beaucoup de nuances et de fonctionnalités dans les langues qui sont parfois difficiles à comprendre même pour les gens, mais 2018 a été un véritable tournant pour la PNL. <br><br>  Nous avons observé une formidable percée après l'autre: ULMFiT, ELMO, OpenAl Transformer, Google BERT, et ce n'est pas une liste complète.  L'application réussie de l'apprentissage par transfert (l'art d'appliquer des modèles pré-formés aux données) a ouvert la porte à la PNL dans une variété de tâches. <br><blockquote>  Transfert d'apprentissage - vous permet d'adapter un modèle / système pré-formé à votre tâche spécifique en utilisant une quantité relativement faible de données. </blockquote>  Examinons plus en détail certains de ces développements clés. <br><br><h3>  ULMFiT </h3><br>  Développé par Sebastian Ruder et Jeremy Howard (fast.ai), ULMFiT a été le premier framework à recevoir un apprentissage par transfert cette année.  Pour les non-initiés, l'acronyme ULMFiT signifie «Universal Language Model Fine-Tuning».  Jeremy et Sebastian ont ajouté à juste titre le mot «universel» à ULMFiT - ce cadre peut être appliqué à presque toutes les tâches de PNL! <br><br>  La meilleure chose à propos d'ULMFiT est que vous n'avez pas besoin de former des modèles à partir de zéro!  Les chercheurs ont déjà fait le plus difficile pour vous - prenez et postulez dans vos projets.  ULMFiT a surpassé les autres méthodes dans six tâches de classification de texte. <br><br>  Vous pouvez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lire le</a> tutoriel de Pratek Joshi [Pateek Joshi - env.  trans.] sur la façon de commencer à utiliser ULMFiT pour toute tâche de classification de texte. <br><br><h3>  ELMo </h3><br>  Devinez ce que l'abréviation ELMo signifie?  Acronyme de Embeddings from Language Models [pièces jointes des modèles linguistiques - env.  trans.].  Et ELMo a attiré l'attention de la communauté ML juste après la sortie. <br><br>  ELMo utilise des modèles de langage pour recevoir des pièces jointes pour chaque mot, et prend également en compte le contexte dans lequel le mot s'inscrit dans une phrase ou un paragraphe.  Le contexte est un aspect critique de la PNL, dans lequel la plupart des développeurs ont précédemment échoué.  ELMo utilise des LSTM bidirectionnels pour créer des pièces jointes. <br><blockquote>  La mémoire à court terme à long terme (LSTM) est un type d'architecture de réseaux de neurones récurrents proposé en 1997 par Sepp Hochreiter et Jürgen Schmidhuber.  Comme la plupart des réseaux de neurones récurrents, un réseau LSTM est universel dans le sens où, avec un nombre suffisant d'éléments de réseau, il peut effectuer tout calcul dont un ordinateur ordinaire est capable, ce qui nécessite une matrice de poids appropriée qui peut être considérée comme un programme.  Contrairement aux réseaux de neurones récurrents traditionnels, le réseau LSTM est bien adapté à la formation sur les problèmes de classification, de traitement et de prévision des séries chronologiques dans les cas où des événements importants sont séparés par des décalages temporels à durée et limites indéfinies. <br><br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">source.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wikipédia</a> </blockquote>  Comme ULMFiT, ELMo améliore considérablement la productivité dans la résolution d'un grand nombre de tâches PNL, telles que l'analyse de l'humeur du texte ou la réponse aux questions. <br><br><h3>  BERT de Google </h3><br>  Beaucoup d'experts notent que la sortie de BERT a marqué le début d'une nouvelle ère en PNL.  Après ULMFiT et ELMo, BERT a pris les devants, démontrant des performances élevées.  Comme l'indique l'annonce originale: «L'ORET est conceptuellement simple et empiriquement puissant.» <br><br>  Le BERT a montré des résultats exceptionnels dans 11 tâches PNL!  Voir les résultats dans les tests SQuAD: <br><br><img src="https://habrastorage.org/webt/rf/6n/cz/rf6nczjjvbcz1cg4nxfeo-lm7ou.png"><br><br>  Vous voulez l'essayer?  Vous pouvez utiliser la réimplémentation sur PyTorch ou le code TensorFlow de Google et essayer de répéter le résultat sur votre machine. <br><br><h3>  Facebook PyText </h3><br>  Comment Facebook pourrait-il rester à l'écart de cette course?  La société propose son propre framework NLP open source appelé PyText.  Selon une étude publiée par Facebook, PyText a augmenté la précision des modèles conversationnels de 10% et réduit le temps de formation. <br><br>  PyText est en fait derrière plusieurs des propres produits de Facebook, tels que Messenger.  Travailler avec lui ajoutera donc un bon point à votre portefeuille et des connaissances inestimables que vous gagnerez sans aucun doute. <br><br>  Vous pouvez l'essayer vous-même, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">téléchargez le code depuis GitHub</a> . <br><br><h3>  Google duplex </h3><br>  Il est difficile de croire que vous n'avez pas entendu parler de Google Duplex.  Voici une démo qui a longtemps fait la une des journaux: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO0-5MuJvew" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Comme il s'agit d'un produit Google, il y a peu de chances que tôt ou tard le code soit publié pour tout le monde.  Bien sûr, cette démonstration soulève de nombreuses questions: de l'éthique aux problèmes de confidentialité, mais nous en parlerons plus tard.  Pour l'instant, profitez simplement du chemin parcouru avec ML ces dernières années. <br><br><a name="NLPtrends"></a><h2>  Tendances PNL 2019 </h2><br>  Qui mieux que Sebastian Ruder lui-même peut donner une idée de la direction que prendra la PNL en 2019?  Voici ses conclusions: <br><blockquote><ol><li>  L'utilisation de modèles d'investissement linguistique pré-formés se généralisera;  les modèles avancés sans support seront très rares. </li><li>  Des vues pré-formées apparaîtront qui peuvent coder des informations spécialisées qui complètent les pièces jointes du modèle de langage.  Nous pourrons regrouper différents types de présentations pré-formées en fonction des exigences de la tâche. </li><li>  D'autres travaux apparaîtront dans le domaine des applications multilingues et des modèles multilingues.  En particulier, en s'appuyant sur l'intégration des mots dans les langues, nous verrons l'émergence de représentations interlangues pré-entraînées profondes. </li></ol></blockquote><a name="cv"></a><h2>  Vision par ordinateur </h2><br><img src="https://habrastorage.org/webt/pu/aj/_c/puaj_c89feaiultos4yynrcj7x4.jpeg"><br><br>  Aujourd'hui, la vision par ordinateur est le domaine le plus populaire dans le domaine de l'apprentissage en profondeur.  Il semble que les premiers fruits de la technologie aient déjà été obtenus et nous sommes au stade de développement actif.  Qu'il s'agisse de cette image ou de cette vidéo, nous voyons l'émergence de nombreux frameworks et bibliothèques qui résolvent facilement les problèmes de vision par ordinateur. <br><br>  Voici ma liste des meilleures solutions qui pourraient être vues cette année. <br><br><h3>  BigGANs Out </h3><br>  Ian Goodfellow a conçu les GAN en 2014, et le concept a engendré une grande variété d'applications.  Année après année, nous avons observé comment le concept original a été finalisé pour une utilisation sur des cas réels.  Mais une chose est restée inchangée jusqu'à cette année - les images générées par ordinateur étaient trop faciles à distinguer.  Une certaine incohérence est toujours apparue dans le cadre, ce qui a rendu la différence très évidente. <br><br>  Au cours des derniers mois, des changements sont apparus dans cette direction et, avec la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">création de BigGAN</a> , de tels problèmes peuvent être résolus une fois pour toutes.  Regardez les images générées par cette méthode: <br><br><img src="https://habrastorage.org/webt/mo/w7/ow/mow7owldedw4r1jtwex6wbfwwje.png"><br><br>  Sans microscope, il est difficile de dire ce qui ne va pas avec ces images.  Bien sûr, chacun décidera pour lui-même, mais il ne fait aucun doute que le GAN change la façon dont nous percevons les images numériques (et la vidéo). <br><br>  Pour référence: ces modèles ont d'abord été formés sur l'ensemble de données ImageNet, puis sur le JFT-300M pour démontrer que ces modèles sont bien transférés d'un ensemble de données à un autre.  Voici un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien vers une page</a> de la liste de diffusion GAN expliquant comment visualiser et comprendre le GAN. <br><br><h3>  Model Fast.ai formé sur ImageNet en 18 minutes </h3><br>  C'est une implémentation vraiment cool.  Il est largement admis que, pour effectuer des tâches d'apprentissage en profondeur, vous aurez besoin de téraoctets de données et de grandes ressources informatiques.  Il en va de même pour l'entraînement du modèle à partir de zéro sur les données ImageNet.  La plupart d'entre nous pensaient la même chose avant que quelques personnes sur fast.ai ne soient pas en mesure de prouver le contraire à tout le monde. <br><br>  Leur modèle a donné une précision de 93% avec un impressionnant 18 minutes.  Le matériel qu'ils ont utilisé, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">décrit</a> en détail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur leur blog</a> , se composait de 16 instances cloud AWS publiques, chacune avec 8 GPU NVIDIA V100.  Ils ont construit un algorithme utilisant les bibliothèques fast.ai et PyTorch. <br><br>  Le coût total de montage n'était que de 40 $!  Jeremy a décrit leurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">approches et méthodes</a> plus en détail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  C'est une victoire commune! <br><br><h3>  vid2vid de NVIDIA </h3><br>  Au cours des 5 dernières années, le traitement d'image a fait de grands progrès, mais qu'en est-il de la vidéo?  Les méthodes de conversion d'une trame statique en une trame dynamique se sont avérées un peu plus compliquées que prévu.  Pouvez-vous prendre une séquence d'images d'une vidéo et prédire ce qui se passera dans l'image suivante?  De telles études ont été faites auparavant, mais les publications étaient au mieux vagues. <br><br><img src="https://habrastorage.org/webt/hz/ox/hj/hzoxhjbehlnlzl8ivc-bgiz0vh0.png"><br><br>  NVIDIA a décidé de rendre sa décision accessible au public plus tôt cette année [2018 - env.  per.], qui a été évalué positivement par la société.  Le but de vid2vid est de dériver une fonction d'affichage d'une vidéo d'entrée donnée afin de créer une vidéo de sortie qui transmet le contenu de la vidéo d'entrée avec une précision incroyable. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S1OwOd-war8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Vous pouvez essayer leur implémentation sur PyTorch, apportez-la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">à GitHub ici</a> . <br><br><a name="cvtrends"></a><h2>  Tendances de vision industrielle pour 2019 </h2><br>  Comme je l'ai mentionné plus tôt, en 2019, nous sommes plus susceptibles de voir le développement des tendances de 2018, plutôt que de nouvelles percées: voitures autonomes, algorithmes de reconnaissance faciale, réalité virtuelle et plus encore.  Pouvez-vous être en désaccord avec moi si vous avez un point de vue différent ou des ajouts, partagez-le avec nous, à quoi pouvons-nous nous attendre en 2019? <br><br>  La question des drones, en attendant l'approbation des politiciens et du gouvernement, pourrait enfin obtenir le feu vert aux États-Unis (l'Inde est loin derrière dans ce dossier).  Personnellement, j'aimerais que davantage de recherches soient effectuées dans des scénarios réels.  Des conférences telles que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CVPR</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICML donnent une</a> bonne couverture des dernières réalisations dans ce domaine, mais la proximité des projets avec la réalité n'est pas très claire. <br><br>  La «réponse visuelle aux questions» et les «systèmes de dialogue visuel» pourraient enfin débuter avec un début tant attendu.  Ces systèmes n'ont pas la capacité de généraliser, mais il est prévu que nous verrons bientôt une approche multimodale intégrée. <br><br><img src="https://habrastorage.org/webt/s5/bn/uy/s5bnuydmsc8hf37vm26icbmwrgc.jpeg"><br><br>  L'autoformation est apparue cette année.  Je parie que l'année prochaine, il trouvera une application dans un plus grand nombre d'études.  C'est une direction vraiment cool: les signes sont déterminés directement à partir des données d'entrée, au lieu de perdre du temps à marquer manuellement les images.  Croisons les doigts! <br><br><h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lire la suite: Partie 2 - Outils et bibliothèques, AutoML, apprentissage par renforcement, éthique en IA</a> </h4></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr439688/">https://habr.com/ru/post/fr439688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr439676/index.html">Réagissez à l'intégration native et C ++ pour iOS et Android</a></li>
<li><a href="../fr439678/index.html">Soumettez au défi F # appliqué</a></li>
<li><a href="../fr439680/index.html">Environ 50% des Russes sont prêts à vendre leurs données personnelles</a></li>
<li><a href="../fr439682/index.html">Formation Cisco 200-125 CCNA v3.0. Spécialiste réseau certifié Cisco (CCNA). Jour 4. Dispositifs de passerelle</a></li>
<li><a href="../fr439684/index.html">Postulez pour le défi F # appliqué</a></li>
<li><a href="../fr439690/index.html">Comparaison des performances des machines virtuelles pour 6 plateformes cloud: Selectel, MCS, I. Cloud, Google Cloud, AWS et Azure</a></li>
<li><a href="../fr439692/index.html">AT&T poursuivi pour avoir changé l'icône du réseau de 4G en 5G E</a></li>
<li><a href="../fr439694/index.html">Tissus intelligents sensibles aux changements de température corporelle</a></li>
<li><a href="../fr439696/index.html">Sur la crête d'une vague, ou "je veux intégrer" - mais ça vaut le coup?</a></li>
<li><a href="../fr439698/index.html">Introduction à la programmation: un simple jeu de tir 3D à partir de zéro au cours du week-end, partie 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>