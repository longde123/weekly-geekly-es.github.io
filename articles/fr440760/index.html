<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÖüèæ üîí üöù Le nouvel √¢ge d'or de l'architecture informatique üíü üêÖ üòï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les auteurs sont John Hennessey et David Patterson, laur√©ats du prix Turing 2017 "pour une approche innovante, syst√©matique et mesurable de la concept...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le nouvel √¢ge d'or de l'architecture informatique</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440760/">  <i><font color="gray">Les auteurs sont John Hennessey et David Patterson, laur√©ats du prix Turing 2017 "pour une approche innovante, syst√©matique et mesurable de la conception et du test d'architectures informatiques qui a eu un impact durable sur l'ensemble de l'industrie des microprocesseurs".</font></i>  <i><font color="gray">Article publi√© dans Communications of the ACM, f√©vrier 2019, Volume 62, No 2, pp. 48-60, doi: 10.1145 / 3282307</font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f32/766/d00/f32766d00d21a30b98a879e2a636a82f.jpg" align="left">  <i>¬´Ceux qui ne se souviennent pas du pass√© sont condamn√©s √† le r√©p√©ter¬ª</i> - George Santayana, 1905 <br><br>  Nous avons commenc√© notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conf√©rence Turing</a> le 4 juin 2018 avec une revue de l'architecture informatique √† partir des ann√©es 60.  En plus de lui, nous mettons en √©vidence les probl√®mes actuels et essayons d'identifier les opportunit√©s futures qui promettent un nouvel √¢ge d'or dans le domaine de l'architecture informatique au cours de la prochaine d√©cennie.  La m√™me chose que dans les ann√©es 1980, lorsque nous avons men√© nos recherches sur l'am√©lioration du co√ªt, de l'efficacit√© √©nerg√©tique, de la s√©curit√© et des performances des processeurs, pour lesquelles nous avons re√ßu ce prix honorable. <br><br><h4>  Id√©es cl√©s </h4><br><ul><li>  Le progr√®s logiciel peut stimuler l'innovation architecturale <br></li><li>  L'augmentation du niveau des interfaces logicielles et mat√©rielles cr√©e des opportunit√©s d'innovation architecturale <br></li><li>  Le march√© d√©termine en fin de compte le gagnant dans le diff√©rend d'architecture </li></ul><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/3LVeEjsn8Ts" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Le logiciel "communique" avec l'√©quipement via un dictionnaire appel√© "architecture du jeu d'instructions" (ISA).  Au d√©but des ann√©es 1960, IBM poss√©dait quatre s√©ries d'ordinateurs incompatibles, chacune avec ses propres ISA, pile logicielle, syst√®me d'E / S et niche de march√© - orient√©es respectivement vers les petites entreprises, les grandes entreprises, les applications scientifiques et les syst√®mes en temps r√©el.  Les ing√©nieurs d'IBM, dont le laur√©at du prix Turing Frederick Brooks Jr., ont d√©cid√© de cr√©er un seul ISA qui unit efficacement les quatre. <br><br>  Ils avaient besoin d'une solution technique sur la fa√ßon de fournir un ISA tout aussi rapide pour les ordinateurs √©quip√©s de bus 8 bits et 64 bits.  Dans un sens, les bus sont les ¬´muscles¬ª des ordinateurs: ils font le travail, mais sont relativement faciles √† ¬´compresser¬ª et √† ¬´√©tendre¬ª.  Autrefois, le plus grand d√©fi pour les concepteurs est le "cerveau" de l'√©quipement de contr√¥le du processeur.  Inspir√© par la programmation, Maurice Wilkes, pionnier de l'informatique et laur√©at du prix Turing, a propos√© des options pour simplifier ce syst√®me.  Le contr√¥le a √©t√© pr√©sent√© comme un tableau bidimensionnel, qu'il a appel√© le "magasin de contr√¥le" (magasin de contr√¥le).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chaque colonne de la matrice correspondait √† une ligne de contr√¥le, chaque ligne √©tait une micro-instruction et l'enregistrement des micro-instructions √©tait appel√© microprogrammation</a> .  La m√©moire de contr√¥le contient un interpr√©teur ISA √©crit par des micro-instructions, donc l'ex√©cution d'une instruction normale prend plusieurs micro-instructions.  La m√©moire de contr√¥le est impl√©ment√©e, en fait, en m√©moire, et c'est beaucoup moins cher que les √©l√©ments logiques. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2f4/43f/818/2f443f8180eb7edbc2dbd66873b71c51.jpg"><br>  <i><font color="gray">Caract√©ristiques des quatre mod√®les de la famille IBM System / 360;</font></i>  <i><font color="gray">IPS signifie op√©rations par seconde</font></i> <br><br>  Le tableau montre quatre mod√®les de la nouvelle ISA dans System / 360 d'IBM, introduite le 7 avril 1964.  Les bus diff√®rent de 8 fois, la capacit√© de m√©moire est de 16, la vitesse d'horloge est presque de 4, les performances sont de 50 et le co√ªt est de presque 6. Les ordinateurs les plus chers ont la m√©moire de contr√¥le la plus √©tendue, car les bus de donn√©es plus complexes utilisent plus de lignes de contr√¥le .  Les ordinateurs les moins chers ont moins de m√©moire de contr√¥le en raison d'un mat√©riel plus simple, mais ils avaient besoin de plus de micro-instructions, car ils avaient besoin de plus de cycles d'horloge pour ex√©cuter l'instruction System / 360. <br><br>  Gr√¢ce √† la microprogrammation, IBM a pari√© que la nouvelle ISA va r√©volutionner l'industrie informatique - et a gagn√© le pari.  IBM a domin√© ses march√©s et les descendants d'anciens ordinateurs centraux IBM de 55 ans g√©n√®rent toujours 10 milliards de dollars de revenus par an. <br><br>  Comme cela a √©t√© not√© √† plusieurs reprises, bien que le march√© soit un arbitre imparfait en tant que technologie, mais √©tant donn√© les liens √©troits entre l'architecture et les ordinateurs commerciaux, il d√©termine en fin de compte le succ√®s des innovations architecturales, qui n√©cessitent souvent d'importants investissements en ing√©nierie. <br><br><h3>  Circuits int√©gr√©s, CISC, 432, 8086, IBM PC </h3><br>  Lorsque les ordinateurs sont pass√©s aux circuits int√©gr√©s, la loi de Moore signifiait que la m√©moire de contr√¥le pouvait devenir beaucoup plus grande.  √Ä son tour, cela a permis une ISA beaucoup plus complexe.  Par exemple, la m√©moire de contr√¥le VAX-11/780 de Digital Equipment Corp.  en 1977, il √©tait de 5120 mots en 96 bits, alors que son pr√©d√©cesseur n'utilisait que 256 mots en 56 bits. <br><br>  Certains fabricants ont activ√© le micrologiciel pour certains clients qui peuvent avoir ajout√© des fonctionnalit√©s personnalis√©es.  C'est ce qu'on appelle le magasin de contr√¥le accessible en √©criture (WCS).  L'ordinateur WCS le plus c√©l√®bre √©tait <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Alto</a> , que les laur√©ats du prix Turing Chuck Tucker et Butler Lampson et ses coll√®gues ont cr√©√© pour le Xerox Palo Alto Research Center en 1973.  C'√©tait vraiment le premier ordinateur personnel: voici le premier √©cran avec imagerie √©l√©ment par √©l√©ment et le premier r√©seau Ethernet local.  Les contr√¥leurs de l'affichage innovant et de la carte r√©seau √©taient des microprogrammes qui sont stock√©s dans le WCS avec une capacit√© de 4096 mots en 32 bits. <br><br>  Dans les ann√©es 70, les processeurs restaient toujours 8 bits (par exemple, Intel 8080) et √©taient programm√©s principalement en assembleur.  Les concurrents ont ajout√© de nouvelles instructions pour se surpasser, montrant leurs r√©alisations avec des exemples d'assembleurs. <br><br>  Gordon Moore pensait que la prochaine ISA d'Intel durerait pour toujours pour l'entreprise, il a donc embauch√© beaucoup de m√©decins intelligents en informatique et les a envoy√©s dans une nouvelle installation √† Portland pour inventer la prochaine grande ISA.  Le processeur 8800, comme Intel l'appelait √† l'origine, √©tait un projet d'architecture informatique absolument ambitieux pour toutes les √©poques, et c'√©tait certainement le projet le plus agressif des ann√©es 80.  Il comprenait un adressage bas√© sur les capacit√©s 32 bits, une architecture orient√©e objet, des instructions de longueur variable et son propre syst√®me d'exploitation dans le nouveau langage de programmation Ada. <br><br>  Malheureusement, ce projet ambitieux a n√©cessit√© plusieurs ann√©es de d√©veloppement, ce qui a oblig√© Intel √† lancer un projet de sauvegarde d'urgence √† Santa Clara afin de sortir rapidement un processeur 16 bits en 1979.  Intel a donn√© √† la nouvelle √©quipe 52 semaines pour d√©velopper la nouvelle ISA "8086", concevoir et construire la puce.  Compte tenu d'un calendrier serr√©, la conception d'ISA n'a pris que 10 semaines-personne pendant trois semaines civiles r√©guli√®res, principalement en raison de l'extension des registres 8 bits et d'un ensemble d'instructions 8080 √† 16 bits.  L'√©quipe a achev√© 8086 dans les d√©lais, mais ce processeur crash-made a √©t√© annonc√© sans grande fanfare. <br><br>  Intel a eu beaucoup de chance qu'IBM d√©veloppe un ordinateur personnel pour concurrencer l'Apple II et avait besoin d'un microprocesseur 16 bits.  IBM envisageait le Motorola 68000 avec un ISA similaire √† l'IBM 360, mais il √©tait derri√®re le calendrier agressif d'IBM.  Au lieu de cela, IBM est pass√© √† la version 8 bits du bus 8086. Quand IBM a annonc√© le PC le 12 ao√ªt 1981, il esp√©rait vendre 250 000 ordinateurs d'ici 1986.  Au lieu de cela, la soci√©t√© a vendu 100 millions de dollars dans le monde, pr√©sentant un avenir tr√®s prometteur pour l'ISA d'urgence d'Intel. <br><br>  Le projet Intel 8800 d'origine a √©t√© renomm√© iAPX-432.  Enfin, il a √©t√© annonc√© en 1981, mais il n√©cessitait plusieurs puces et avait de s√©rieux probl√®mes de performances.  Il a √©t√© achev√© en 1986, un an apr√®s qu'Intel ait √©tendu l'ISA 8086 16 bits √† 80386, faisant passer les registres de 16 bits √† 32 bits.  Ainsi, la pr√©diction de Moore concernant l'ISA s'est av√©r√©e correcte, mais le march√© a choisi le 8086 fabriqu√© en deux, plut√¥t que l'iAPX-432 oint.  Comme les architectes des processeurs Motorola 68000 et iAPX-432 l'ont compris, le march√© est rarement capable de faire preuve de patience. <br><br><h3>  Du jeu d'instructions complexe au jeu abr√©g√© </h3><br>  Au d√©but des ann√©es 1980, plusieurs √©tudes sur les ordinateurs avec un ensemble d'instructions complexes (CISC) ont √©t√© men√©es: ils ont de grands microprogrammes dans une grande m√©moire de contr√¥le.  Lorsque Unix a d√©montr√© que m√™me le syst√®me d'exploitation peut √™tre √©crit dans un langage de haut niveau, la question principale √©tait: "Quelles instructions les compilateurs g√©n√©reront-ils?"  au lieu de l'ancien "Quel assembleur les programmeurs utiliseront-ils?"  Une augmentation significative du niveau de l'interface mat√©riel-logiciel a cr√©√© une opportunit√© pour l'innovation en architecture. <br><br>  Le laur√©at du prix Turing John Kokk et ses coll√®gues ont d√©velopp√© des ISA et des compilateurs de mini-ordinateurs plus simples.  √Ä titre d'exp√©rience, ils ont r√©orient√© leurs compilateurs de recherche vers l'utilisation de l'IBM 360 ISA pour n'utiliser que des op√©rations simples entre les registres et le chargement avec de la m√©moire, en √©vitant des instructions plus complexes.  Ils ont remarqu√© que les programmes s'ex√©cutent trois fois plus vite s'ils utilisent un simple sous-ensemble.  Emer et Clark ont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">constat√©</a> que 20% des instructions VAX occupent 60% du microcode et ne prennent que 0,2% du temps d'ex√©cution.  Un auteur de cet article (Patterson) a pass√© des vacances cr√©atives au DEC, contribuant √† r√©duire les erreurs dans le microcode VAX.  Si les fabricants de microprocesseurs devaient suivre les conceptions ISA avec un ensemble de commandes CISC complexes dans de grands ordinateurs, ils s'attendaient √† un grand nombre d'erreurs de microcode et voulaient trouver un moyen de les corriger.  Il a √©crit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un tel article</a> , mais <i>le</i> magazine <i>Computer l'</i> a rejet√©.  Les examinateurs ont sugg√©r√© que la terrible id√©e de construire des microprocesseurs avec ISA est si complexe qu'ils doivent √™tre r√©par√©s sur le terrain.  Cet √©chec a jet√© un doute sur la valeur du CISC pour les microprocesseurs.  Ironiquement, les microprocesseurs CISC modernes incluent des m√©canismes de r√©cup√©ration de microcode, mais le refus de publier l'article a inspir√© l'auteur √† d√©velopper une ISA moins complexe pour les microprocesseurs - des ordinateurs avec un jeu d'instructions r√©duit (RISC). <br><br>  Ces commentaires et la transition vers des langues de haut niveau ont permis la transition du CISC au RISC.  Tout d'abord, les instructions RISC sont simplifi√©es, il n'y a donc pas besoin d'un interpr√®te.  Les instructions RISC sont g√©n√©ralement simples comme des micro-instructions et peuvent √™tre ex√©cut√©es directement par le mat√©riel.  Deuxi√®mement, la m√©moire rapide qui √©tait auparavant utilis√©e pour l'interpr√©teur de microcode CISC a √©t√© repens√©e dans le cache d'instructions RISC (le cache est une petite m√©moire rapide qui met en m√©moire tampon les instructions r√©cemment ex√©cut√©es, car ces instructions sont susceptibles d'√™tre r√©utilis√©es dans un avenir proche).  Troisi√®mement, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les allocateurs de registres bas√©s sur le sch√©ma de coloration du graphique de Gregory Chaitin ont</a> grandement facilit√© l'utilisation efficace des registres pour les compilateurs, qui b√©n√©ficiaient de ces ISA avec les op√©rations registre-registre.  Enfin, la loi de Moore a conduit au fait que dans les ann√©es 1980, il y avait suffisamment de transistors sur une puce pour accueillir un bus 32 bits complet sur une seule puce, ainsi que des caches pour les instructions et les donn√©es. <br><br>  Par exemple, sur la fig.  La figure 1 montre les microprocesseurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RISC-I</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MIPS</a> d√©velopp√©s √† l'Universit√© de Californie √† Berkeley et √† l'Universit√© de Stanford en 1982 et 1983, qui ont d√©montr√© les avantages du RISC.  En cons√©quence, en 1984, ces processeurs ont √©t√© pr√©sent√©s √† la principale conf√©rence sur la conception des circuits, la Conf√©rence internationale des circuits √† semi-conducteurs de l'IEEE ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2</a> ).  Ce fut un moment merveilleux lorsque plusieurs √©tudiants dipl√¥m√©s de Berkeley et Stanford ont cr√©√© des microprocesseurs qui d√©passaient les capacit√©s de l'industrie de cette √©poque. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/410/ca5/11d/410ca511d626e92ae1ea3179fa59bddb.jpg"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">1. Processeurs RISC-I de l'Universit√© de Californie √† Berkeley et MIPS de l'Universit√© de Stanford</font></i> <br><br>  Ces puces acad√©miques ont inspir√© de nombreuses entreprises √† cr√©er des microprocesseurs RISC, qui ont √©t√© les plus rapides au cours des 15 prochaines ann√©es.  L'explication est li√©e √† la formule de performances de processeur suivante: <br><br>  <i>Temps / Programme = (Instructions / Programme) √ó (mesures / instruction) √ó (temps / mesure)</i> <br><br>  Les ing√©nieurs de DEC ont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">montr√©</a> plus tard que pour un programme, les CISC plus complexes n√©cessitent 75% du nombre d'instructions RISC (le premier terme de la formule), mais dans une technologie similaire (troisi√®me terme), chaque instruction CISC prend 5-6 cycles de plus (deuxi√®me terme), ce qui rend les microprocesseurs RISC environ 4 fois plus rapides. <br><br>  Il n'y avait pas de telles formules dans la litt√©rature informatique des ann√©es 80, ce qui nous a fait √©crire le livre <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Computer Architecture: A Quantitective Approach</a></i> en 1989.  Le sous-titre explique le th√®me du livre: utiliser des mesures et des rep√®res pour quantifier les compromis, plut√¥t que de s'appuyer sur l'intuition et l'exp√©rience du concepteur, comme par le pass√©.  Notre approche quantitative a √©galement √©t√© inspir√©e par ce que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le livre du laur√©at Turing Donald Knuth a</a> fait pour les algorithmes. <br><br><h3>  VLIW, EPIC, Itanium </h3><br>  La prochaine ISA innovante devait d√©passer le succ√®s du RISC et du CISC.  La tr√®s longue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">architecture d'</a> instructions machine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VLIW</a> et son cousin EPIC (Computing with explicit machine instruction parallelism) d'Intel et Hewlett-Packard utilisaient des instructions longues, chacune compos√©e de plusieurs op√©rations ind√©pendantes li√©es entre elles.  Les partisans de VLIW et d'EPIC √† l'√©poque croyaient que si une instruction pouvait indiquer, disons, six op√©rations ind√©pendantes - deux transferts de donn√©es, deux op√©rations enti√®res et deux op√©rations en virgule flottante - et la technologie du compilateur pourrait affecter efficacement les op√©rations √† six emplacements d'instruction, alors l'√©quipement peut √™tre simplifi√©.  Semblable √† l'approche de RISC, VLIW et EPIC ont transf√©r√© le travail du mat√©riel au compilateur. <br><br>  Ensemble, Intel et Hewlett-Packard ont d√©velopp√© un processeur EPIC 64 bits pour remplacer l'architecture x86 32 bits.  De grandes attentes √©taient plac√©es sur le premier processeur EPIC appel√© Itanium, mais la r√©alit√© ne correspondait pas aux premi√®res d√©clarations des d√©veloppeurs.  Bien que l'approche EPIC ait bien fonctionn√© pour les programmes √† virgule flottante hautement structur√©s, elle n'a pas pu atteindre de hautes performances pour les programmes entiers avec des branchements et des √©checs de cache moins pr√©visibles.  Comme Donald Knuth l'a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">not√©</a> plus tard: "Itanium √©tait cens√© √™tre ... g√©nial - jusqu'√† ce qu'il s'av√®re que les compilateurs souhait√©s √©taient fondamentalement impossibles √† √©crire."  Les critiques ont not√© des retards dans la lib√©ration d'Itanium et l'ont surnomm√© Itanik en l'honneur du malheureux navire √† passagers Titanic.  Le march√© n'a de nouveau pas fait preuve de patience et a adopt√© la version 64 bits de x86, et non Itanium, comme successeur. <br><br>  La bonne nouvelle est que VLIW est toujours adapt√© √† des applications plus sp√©cialis√©es qui ex√©cutent de petits programmes avec des branches plus simples sans manque de cache, y compris le traitement du signal num√©rique. <br><br><h1>  RISC vs CISC √† l'√®re PC et post-PC </h1><br>  AMD et Intel avaient besoin de 500 √©quipes de conception et d'une technologie de semi-conducteurs sup√©rieure pour combler l'√©cart de performances entre x86 et RISC.  Encore une fois, par souci de performances gr√¢ce au pipeline, un d√©codeur d'instructions √† la vol√©e traduit les instructions x86 complexes en micro-instructions internes de type RISC.  AMD et Intel construisent ensuite un pipeline pour leur impl√©mentation.  Toutes les id√©es que les concepteurs de RISC ont utilis√©es pour am√©liorer les performances - caches d'instructions et de donn√©es s√©par√©s, caches de deuxi√®me niveau sur la puce, pipeline profond et r√©ception et ex√©cution simultan√©es de plusieurs instructions - ont ensuite √©t√© incluses dans x86.  Au plus fort de l'√®re des ordinateurs personnels en 2011, AMD et Intel ont livr√© environ 350 millions de microprocesseurs x86 par an.  Les volumes √©lev√©s et les faibles marges de l'industrie signifient √©galement des prix inf√©rieurs √† ceux des ordinateurs RISC. <br><br>  Avec des centaines de millions d'ordinateurs vendus chaque ann√©e, les logiciels sont devenus un √©norme march√©.  Alors que les fournisseurs de logiciels Unix devaient publier diff√©rentes versions de logiciels pour diff√©rentes architectures RISC - Alpha, HP-PA, MIPS, Power et SPARC - les ordinateurs personnels avaient un ISA, les d√©veloppeurs ont donc publi√© un logiciel ¬´r√©tr√©ci¬ª qui n'√©tait compatible binaire qu'avec l'architecture x86.  En raison de sa base logicielle beaucoup plus √©tendue, de performances similaires et de prix plus bas, en 2000, l'architecture x86 dominait les march√©s des ordinateurs de bureau et des petits serveurs. <br><br>  Apple a aid√© √† inaugurer l'√®re post-PC avec l'iPhone en 2007.  Au lieu d'acheter des microprocesseurs, les fabricants de smartphones ont cr√©√© leurs propres syst√®mes sur puce (SoC) en utilisant les d√©veloppements d'autres personnes, y compris les processeurs RISC d'ARM.  Ici, les concepteurs sont importants non seulement pour les performances, mais √©galement pour la consommation d'√©nergie et la surface de la puce, ce qui d√©savantage l'architecture CISC.  En outre, l'Internet des objets a consid√©rablement augment√© √† la fois le nombre de processeurs et les compromis n√©cessaires en termes de taille de puce, de puissance, de co√ªt et de performances.  Cette tendance a accru l'importance du temps et du co√ªt de conception, aggravant encore la position des processeurs CISC.  Dans l'√®re post-PC d'aujourd'hui, les livraisons annuelles de x86 ont chut√© de pr√®s de 10% depuis le pic de 2011, tandis que les puces RISC sont mont√©es en fl√®che √† 20 milliards.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aujourd'hui, 99% des processeurs 32 et 64 bits dans le monde sont RISC. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En conclusion de cette revue historique, on peut dire que le march√© a r√©gl√© le diff√©rend entre RISC et CISC. </font><font style="vertical-align: inherit;">Bien que le CISC ait remport√© les derni√®res √©tapes de l'√®re PC, RISC gagne maintenant que l'√®re post-PC est arriv√©e. </font><font style="vertical-align: inherit;">Il n'y a pas de nouvelles normes ISA au CISC depuis des d√©cennies. </font><font style="vertical-align: inherit;">√Ä notre grande surprise, le consensus g√©n√©ral sur les meilleurs principes ISA pour les processeurs √† usage g√©n√©ral est toujours en faveur du RISC, 35 ans apr√®s son invention.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> D√©fis modernes pour l'architecture de processeur </font></font></h1><br> <i>¬´     , ,   ,  ,     ¬ª</i> ‚Äî   <br><br>            (ISA),        ISA,    ISA    .   70-       - (MOS),  n- (nMOS),    (CMOS).     MOS ‚Äî      ‚Äî   ,            ISA.    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> 1965 </a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pr√©voyait un doublement annuel de la densit√© des transistors; </font><font style="vertical-align: inherit;">en 1975, il l'a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r√©vis√©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pr√©voyant un doublement tous les deux ans. </font><font style="vertical-align: inherit;">En fin de compte, cette pr√©vision a commenc√© √† s'appeler la loi de Moore. </font><font style="vertical-align: inherit;">√âtant donn√© que la densit√© des transistors cro√Æt de fa√ßon quadratique et que la vitesse cro√Æt de fa√ßon lin√©aire, l'utilisation de plus de transistors peut augmenter la productivit√©.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La fin de la loi de Moore et de la loi d'√©chelle de Dennard </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien que la loi de Moore soit en vigueur depuis de nombreuses d√©cennies (voir figure 2), vers 2000, elle a commenc√© √† ralentir et, en 2018, l'√©cart entre les pr√©visions de Moore et les capacit√©s actuelles s'est agrandi jusqu'√† 15 fois. En 2003, Moore a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sugg√©r√© que c'√©tait in√©vitable</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Il est actuellement pr√©vu que l'√©cart continuera de s'√©largir √† mesure que la technologie CMOS approche des limites fondamentales. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/073/9b2/026/0739b20261c12234f330eff27f9c3062.jpg"><br> <i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig. 2. Le nombre de transistors sur une puce Intel par rapport √† la</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> loi de Moore. </font><i><font color="gray"><font style="vertical-align: inherit;">La</font></font></i><font style="vertical-align: inherit;"> loi de Moore √©tait accompagn√©e d'une projection r√©alis√©e par Robert Dennard intitul√©e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Dennard Scaling"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qu'au fur et √† mesure que la densit√© des transistors augmente, la consommation d'√©nergie du transistor diminue, de sorte que la consommation par mm¬≤ de silicium sera presque constante. √Ä mesure que la puissance de calcul d'un millim√®tre de silicium augmentait √† chaque nouvelle g√©n√©ration de technologie, les ordinateurs devenaient plus √©conomes en √©nergie. La mise √† l'√©chelle de Dennard a commenc√© √† ralentir consid√©rablement en 2007 et, en 2012, elle √©tait pratiquement tomb√©e √† n√©ant (voir figure 3). </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e40/d10/c1d/e40d10c1d7a21c64618f4bd246411e11.jpg"><br> <i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig. 3. Le nombre de transistors par puce et la consommation d'√©nergie par mm¬≤</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De 1986 √† 2002, la concurrence d'acc√®s au niveau d'instruction (ILP) a √©t√© la principale m√©thode architecturale pour augmenter la productivit√©. Parall√®lement √† l'augmentation de la vitesse des transistors, cela a donn√© une augmentation annuelle de la productivit√© d'environ 50%. La fin de la mise √† l'√©chelle de Dennard signifiait que les architectes devaient trouver de meilleures fa√ßons d'utiliser la concurrence.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour comprendre pourquoi une augmentation de l'efficacit√© ILP a r√©duit l'efficacit√©, consid√©rez le c≈ìur des processeurs ARM, Intel et AMD modernes. Supposons qu'il dispose d'un pipeline √† 15 √©tapes et de quatre instructions par horloge. Ainsi, √† tout moment sur le convoyeur il y a jusqu'√† 60 instructions, dont une quinzaine de succursales, puisqu'elles repr√©sentent environ 25% des instructions ex√©cut√©es. Pour remplir le pipeline, des branches sont pr√©dites et le code est plac√© de mani√®re sp√©culative dans le pipeline pour ex√©cution. La pr√©vision sp√©culative est √† la fois la source des performances et de l'inefficacit√© de l'ILP. Lorsque la pr√©diction de branche est id√©ale, la sp√©culation am√©liore les performances et n'augmente que l√©g√®rement la consommation d'√©nergie - et peut m√™me √©conomiser de l'√©nergie - mais lorsque les branches ne sont pas pr√©dites correctement, le processeur doit jeter les mauvais calculs.et tout le travail et l'√©nergie gaspill√©s. L'√©tat interne du processeur devra √©galement √™tre r√©tabli dans l'√©tat qui existait avant la branche mal comprise, au d√©triment du temps et de l'√©nergie suppl√©mentaires.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour comprendre la complexit√© d'une telle conception, imaginez la difficult√© de pr√©dire correctement les r√©sultats de 15 branches. Si le concepteur du processeur fixe une limite de 10% de perte, le processeur doit pr√©dire correctement chaque branche avec une pr√©cision de 99,3%. Il n'y a pas beaucoup de programmes de branche √† usage g√©n√©ral qui peuvent √™tre pr√©dits avec autant de pr√©cision.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour √©valuer en quoi consiste ce travail gaspill√©, consid√©rez les donn√©es de la Fig. 4, montrant la proportion d'instructions qui sont ex√©cut√©es efficacement mais qui sont gaspill√©es car le processeur a incorrectement pr√©dit la ramification. Dans les tests SPEC sur Intel Core i7, en moyenne 19% des instructions sont perdues. Cependant, la quantit√© d'√©nergie d√©pens√©e est plus grande, car le processeur doit utiliser de l'√©nergie suppl√©mentaire pour restaurer l'√©tat lorsqu'il est incorrectement pr√©dit. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/9a2/ce3/f54/9a2ce3f54d26359ce98e036eac216a5d.jpg"><br> <i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig.4. Gaspillage d'instructions en pourcentage de toutes les instructions ex√©cut√©es sur Intel Core i7 pour divers tests SPEC entiers.</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> De telles mesures ont conduit beaucoup √† conclure qu'une approche diff√©rente devrait √™tre recherch√©e pour obtenir de meilleures performances. L'√®re du multic≈ìur est donc n√©e.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans ce concept, la responsabilit√© d'identifier la concurrence et de d√©cider comment l'utiliser est transf√©r√©e au programmeur et au syst√®me de langage. Le multic≈ìur ne r√©sout pas le probl√®me de l'informatique √©co√©nerg√©tique, aggrav√© par la fin de la mise √† l'√©chelle de Dennard. Chaque c≈ìur actif consomme de l'√©nergie, qu'il soit impliqu√© dans des calculs efficaces. L'obstacle principal est une vieille observation appel√©e loi d'Amdahl. Il indique que les avantages du calcul parall√®le sont limit√©s par la fraction du calcul s√©quentiel. Pour √©valuer l'importance de cette observation, consid√©rons la figure 5. Elle montre √† quelle vitesse l'application fonctionne avec 64 c≈ìurs par rapport √† un c≈ìur, en supposant une proportion diff√©rente de calculs s√©quentiels lorsqu'un seul processeur est actif. Par exemplesi 1% du temps le calcul est effectu√© s√©quentiellement, l'avantage de la configuration √† 64 processeurs n'est que de 35%. Malheureusement, la consommation d'√©nergie est proportionnelle √† 64 processeurs, donc environ 45% de l'√©nergie est gaspill√©e.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a8e/d85/9c6/a8ed859c61efb49a292ca4f5ac3cb5f4.jpg"><br> <i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig. 5. L'effet de la loi d'Amdahl sur l'augmentation de la vitesse, en tenant compte de la proportion de mesures en mode s√©quentiel.</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bien entendu, les programmes r√©els ont une structure plus complexe. Il existe des fragments qui vous permettent d'utiliser √† tout moment un nombre diff√©rent de processeurs. Cependant, la n√©cessit√© d'interagir p√©riodiquement et de les synchroniser signifie que la plupart des applications ont certaines parties qui ne peuvent utiliser efficacement qu'une partie des processeurs. Bien que la loi d'Amdahl ait plus de 50 ans, elle reste un obstacle difficile.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Avec la fin de la mise √† l'√©chelle de Dennard, une augmentation du nombre de c≈ìurs sur la puce signifiait que la puissance augmentait √©galement presque au m√™me rythme. Malheureusement, la tension fournie au processeur doit alors √™tre supprim√©e sous forme de chaleur. Ainsi, les processeurs multic≈ìurs sont limit√©s par la puissance de sortie thermique (TDP) ou la quantit√© moyenne d'√©nergie que le ch√¢ssis et le syst√®me de refroidissement peuvent supprimer. Bien que certains centres de donn√©es haut de gamme utilisent des technologies de refroidissement plus avanc√©es, aucun utilisateur ne voudra mettre un petit √©changeur de chaleur sur la table ou transporter un radiateur sur le dos pour refroidir le t√©l√©phone mobile. La limite TDP a conduit √† l'√®re du silicium noir, lorsque les processeurs ralentissent la vitesse d'horloge et d√©sactivent les c≈ìurs inactifs pour √©viter la surchauffe. Une autre fa√ßon de consid√©rer cette approche est deque certains microcircuits peuvent redistribuer leur pr√©cieux pouvoir des noyaux inactifs aux noyaux actifs.</font></font><br><br>    ,        , ,           (. . 6). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0a/f8f/cd0/f0af8fcd06385641a5cc1266fda37a0f.jpg"><br> <i><font color="gray">. 6.       (SPECintCPU)</font></i> <br><br>       ‚Äî      80-  90-  ‚Äî    ,        .       ,        ‚Äî . <br><br><h1>   </h1><br>  Dans les ann√©es 70, les d√©veloppeurs de processeurs ont diligemment assur√© la s√©curit√© informatique √† l'aide de divers concepts, allant des anneaux de protection aux fonctions sp√©ciales.  Ils comprenaient bien que la plupart des bogues se trouveraient dans le logiciel, mais pensaient que le support architectural pouvait aider.  Ces fonctionnalit√©s n'√©taient g√©n√©ralement pas utilis√©es par les syst√®mes d'exploitation qui fonctionnaient dans des environnements suppos√©s s√ªrs (comme les ordinateurs personnels).  Par cons√©quent, les fonctions associ√©es √† une surcharge importante ont √©t√© supprim√©es.  Dans la communaut√© des logiciels, beaucoup pensaient que des tests formels et des m√©thodes comme l'utilisation d'un micro-noyau fourniraient des m√©canismes efficaces pour cr√©er des logiciels hautement s√©curis√©s.  Malheureusement, l'ampleur de nos syst√®mes logiciels communs et la recherche de performances signifiaient que de telles m√©thodes ne pouvaient pas suivre les performances.  En cons√©quence, les grands syst√®mes logiciels pr√©sentent encore de nombreuses failles de s√©curit√©, et l'effet est amplifi√© en raison de la quantit√© √©norme et croissante d'informations personnelles sur Internet et de l'utilisation du cloud computing, o√π les utilisateurs partagent le m√™me √©quipement physique avec un attaquant potentiel. <br><br>  Bien que les concepteurs de processeurs et d'autres n'aient peut-√™tre pas tout de suite compris l'importance croissante de la s√©curit√©, ils ont commenc√© √† inclure la prise en charge mat√©rielle des machines virtuelles et le chiffrement.  Malheureusement, la pr√©diction de branche a introduit une faille de s√©curit√© inconnue mais significative dans de nombreux processeurs.  En particulier, les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">vuln√©rabilit√©s Meltdown et Spectre exploitent les fonctionnalit√©s de microarchitecture, permettant la fuite d'informations prot√©g√©es</a> .  Ils utilisent tous deux les soi-disant attaques sur des canaux tiers lorsque des informations fuient en fonction de la diff√©rence de temps pass√© sur la t√¢che.  En 2018, les chercheurs ont montr√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">comment utiliser l'une des options Spectre pour extraire des informations sur le r√©seau sans t√©l√©charger de code vers le processeur cible</a> .  Bien que cette attaque, appel√©e NetSpectre, transf√®re lentement les informations, le fait qu'elle vous permette d'attaquer n'importe quelle machine sur le m√™me r√©seau local (ou dans le m√™me cluster dans le cloud) cr√©e de nombreux nouveaux vecteurs d'attaque.  Par la suite, deux autres vuln√©rabilit√©s dans l'architecture des machines virtuelles ont √©t√© signal√©es ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2</a> ).  L'un d'eux, appel√© Foreshadow, vous permet de p√©n√©trer les m√©canismes de s√©curit√© Intel SGX con√ßus pour prot√©ger les donn√©es les plus pr√©cieuses (telles que les cl√©s de chiffrement).  De nouvelles vuln√©rabilit√©s sont trouv√©es chaque mois. <br><br>  Les attaques sur les canaux tiers ne sont pas nouvelles, mais dans la plupart des cas, les bogues logiciels √©taient la faute plus t√¥t.  Dans Meltdown, Spectre et d'autres attaques, il s'agit d'une faille dans l'impl√©mentation mat√©rielle.  Il y a une difficult√© fondamentale dans la fa√ßon dont les architectes de processeurs d√©terminent quelle est la bonne impl√©mentation d'ISA car la d√©finition standard ne dit rien sur les effets de performance de l'ex√©cution d'une s√©quence d'instructions, seulement l'√©tat architectural visible d'ex√©cution de l'ISA.  Les architectes devraient repenser leur d√©finition de la bonne mise en ≈ìuvre d'ISA pour √©viter de telles failles de s√©curit√©.  Dans le m√™me temps, ils doivent repenser l'attention qu'ils accordent √† la s√©curit√© informatique et la fa√ßon dont les architectes peuvent travailler avec les d√©veloppeurs de logiciels pour mettre en ≈ìuvre des syst√®mes plus s√©curis√©s.  Les architectes (et tout le monde) ne devraient pas prendre la s√©curit√© autrement que comme un besoin principal. <br><br><h1>  Opportunit√©s futures en architecture informatique </h1><br>  <i>¬´Nous avons d'incroyables opportunit√©s d√©guis√©es en probl√®mes insolubles.¬ª</i> - John Gardner, 1965 <br><br>  L'inefficacit√© inh√©rente des processeurs √† usage g√©n√©ral, qu'il s'agisse de la technologie ILP ou des processeurs multic≈ìurs, combin√©e √† l'ach√®vement de la mise √† l'√©chelle de Dennard et √† la loi de Moore, il est peu probable que les architectes et les d√©veloppeurs de processeurs soient en mesure de maintenir un rythme significatif dans l'am√©lioration des performances des processeurs √† usage g√©n√©ral.  √âtant donn√© l'importance d'am√©liorer la productivit√© des logiciels, nous devons nous poser la question suivante: quelles sont les autres approches prometteuses? <br><br>  Il y a deux possibilit√©s √©videntes, ainsi qu'une troisi√®me cr√©√©e en combinant les deux.  Premi√®rement, les m√©thodes de d√©veloppement de logiciels existantes utilisent largement des langages de haut niveau avec une frappe dynamique.  Malheureusement, ces langues sont g√©n√©ralement interpr√©t√©es et ex√©cut√©es de mani√®re extr√™mement inefficace.  Pour illustrer cette inefficacit√©, Leiserson et ses coll√®gues ont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">donn√© un petit exemple: la multiplication matricielle</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/31e/c75/ebe/31ec75ebe5c3134c14020655b89e8ac1.jpg"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">7. Acc√©l√©ration potentielle de la multiplication des matrices Python apr√®s quatre optimisations</font></i> <br><br>  Comme le montre la fig.  7, la simple r√©√©criture du code de Python vers C am√©liore les performances de 47 fois.  L'utilisation de boucles parall√®les sur de nombreux c≈ìurs donne un facteur suppl√©mentaire d'environ 7. L'optimisation de la structure de la m√©moire pour l'utilisation des caches donne un facteur de 20, et le dernier facteur de 9 provient de l'utilisation d'extensions mat√©rielles pour effectuer des op√©rations SIMD parall√®les, qui sont capables d'ex√©cuter 16 instructions 32 bits.  Apr√®s cela, la version finale hautement optimis√©e s'ex√©cute sur le processeur multic≈ìur d'Intel 62 806 fois plus rapidement que la version Python d'origine.  Ceci, bien s√ªr, est un petit exemple.  On peut supposer que les programmeurs utiliseront une biblioth√®que optimis√©e.  Bien que l'√©cart de performances soit exag√©r√©, il existe probablement de nombreux programmes qui peuvent √™tre optimis√©s 100 √† 1000 fois. <br><br>  Un domaine de recherche int√©ressant est la question de savoir s'il est possible de combler certains √©carts de performances avec la nouvelle technologie de compilateur, √©ventuellement avec des am√©liorations architecturales.  Bien qu'il soit difficile de traduire et de compiler efficacement des langages de script de haut niveau tels que Python, le gain potentiel est √©norme.  M√™me une petite optimisation peut conduire au fait que les programmes Python s'ex√©cuteront des dizaines √† des centaines de fois plus rapidement.  Cet exemple simple montre l'ampleur de l'√©cart entre les langages modernes ax√©s sur les performances des programmeurs et les approches traditionnelles qui mettent l'accent sur les performances. <br><br><h3>  Architectures sp√©cialis√©es </h3><br>  Une approche plus orient√©e mat√©riel est la conception d'architectures adapt√©es √† un domaine sp√©cifique, o√π elles d√©montrent une efficacit√© significative.  Il s'agit d'architectures sp√©cifiques au domaine (architectures sp√©cifiques au domaine, DSA).  Ce sont g√©n√©ralement des processeurs programmables et complets, mais prenant en compte une classe sp√©cifique de t√¢ches.  En ce sens, ils diff√®rent des circuits int√©gr√©s sp√©cifiques √† l'application (ASIC), qui sont souvent utilis√©s pour la m√™me fonction que le code qui change rarement.  Les DSA sont souvent appel√©s acc√©l√©rateurs, car ils acc√©l√®rent certaines applications par rapport √† l'ex√©cution de l'application enti√®re sur un CPU √† usage g√©n√©ral.  De plus, les DSA peuvent offrir de meilleures performances car ils sont plus pr√©cis√©ment adapt√©s aux besoins de l'application.  Les exemples de DSA incluent les processeurs graphiques (GPU), les processeurs de r√©seau neuronal utilis√©s pour l'apprentissage en profondeur et les processeurs pour les r√©seaux d√©finis par logiciel (SDN).  Les DSA atteignent des performances et une efficacit√© √©nerg√©tique sup√©rieures pour quatre raisons principales. <br><br>  Premi√®rement, les DSA utilisent une forme de concurrence plus efficace pour un domaine sp√©cifique.  Par exemple, SIMD (flux d'instructions unique, flux de donn√©es multiples) est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plus efficace que MIMD</a> (flux d'instructions multiples, flux de donn√©es multiples).  Bien que SIMD soit moins flexible, il est bien adapt√© √† de nombreux DSA.  Les processeurs sp√©cialis√©s peuvent √©galement utiliser les approches ILP de VLIW au lieu de m√©canismes peu sp√©culatifs.  Comme mentionn√© pr√©c√©demment, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les processeurs VLIW sont mal adapt√©s au code √† usage g√©n√©ral</a> , mais pour les zones √©troites, ils sont beaucoup plus efficaces car les m√©canismes de contr√¥le sont plus simples.  En particulier, les processeurs polyvalents les plus haut de gamme sont excessivement multi-pipelined, ce qui n√©cessite une logique de contr√¥le complexe pour d√©marrer et terminer les instructions.  En revanche, VLIW effectue l'analyse et la planification n√©cessaires au moment de la compilation, ce qui peut bien fonctionner pour un programme clairement parall√®le. <br><br>  Deuxi√®mement, les services DSA utilisent mieux la hi√©rarchie de la m√©moire.  L'acc√®s √† la m√©moire est devenu beaucoup plus cher que les calculs arithm√©tiques, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">comme l'a not√© Horowitz</a> .  Par exemple, l'acc√®s √† un bloc dans un cache de 32 Ko n√©cessite environ 200 fois plus d'√©nergie que l'ajout d'entiers 32 bits.  Une telle diff√©rence √©norme rend l'optimisation de l'acc√®s √† la m√©moire critique pour atteindre une efficacit√© √©nerg√©tique √©lev√©e.  Les processeurs √† usage g√©n√©ral ex√©cutent du code dans lequel les acc√®s √† la m√©moire pr√©sentent g√©n√©ralement une localit√© spatiale et temporelle, mais sont par ailleurs peu pr√©visibles au moment de la compilation.  Par cons√©quent, pour augmenter le d√©bit, les processeurs utilisent des caches √† plusieurs niveaux et masquent le retard dans les DRAM relativement lentes en dehors de la puce.  Ces caches √† plusieurs niveaux consomment souvent environ la moiti√© de l'√©nergie du processeur, mais ils emp√™chent presque tous les appels √† la DRAM, ce qui prend environ 10 fois plus d'√©nergie que l'acc√®s au cache de dernier niveau. <br><br>  Les caches ont deux d√©fauts notables. <br><br>  <i>Lorsque les ensembles de donn√©es sont tr√®s volumineux</i> .  Les caches ne fonctionnent tout simplement pas bien lorsque les ensembles de donn√©es sont tr√®s volumineux, ont une faible localisation temporelle ou spatiale. <br><br>  <i>Quand les caches fonctionnent bien</i> .  Lorsque les caches fonctionnent bien, la localit√© est tr√®s √©lev√©e, c'est-√†-dire que, par d√©finition, la majeure partie du cache est inactive la plupart du temps. <br><br>  Dans les applications o√π les mod√®les d'acc√®s √† la m√©moire sont bien d√©finis et compr√©hensibles au moment de la compilation, ce qui est vrai pour les langages sp√©cifiques √† un domaine (DSL), les programmeurs et les compilateurs peuvent optimiser l'utilisation de la m√©moire mieux que les caches allou√©s dynamiquement.  Ainsi, les DSA utilisent g√©n√©ralement une hi√©rarchie de m√©moire mobile qui est explicitement contr√¥l√©e par le logiciel, semblable au fonctionnement des processeurs vectoriels.  Dans les applications correspondantes, le contr√¥le de m√©moire utilisateur ¬´manuel¬ª vous permet de d√©penser beaucoup moins d'√©nergie que le cache standard. <br><br>  Troisi√®mement, DSA peut r√©duire la pr√©cision des calculs si une pr√©cision √©lev√©e n'est pas n√©cessaire.  Les processeurs √† usage g√©n√©ral prennent g√©n√©ralement en charge les calculs d'entiers 32 bits et 64 bits, ainsi que les donn√©es √† virgule flottante (FP).  Pour de nombreuses applications d'apprentissage automatique et de graphisme, il s'agit d'une pr√©cision redondante.  Par exemple, dans les r√©seaux de neurones profonds, le calcul utilise souvent des nombres de 4, 8 ou 16 bits, am√©liorant √† la fois le d√©bit de donn√©es et la puissance de traitement.  De m√™me, les calculs en virgule flottante sont utiles pour la formation des r√©seaux de neurones, mais 32 bits, et souvent 16 bits, suffisent. <br><br>  Enfin, les DSA b√©n√©ficient de programmes √©crits dans des langages sp√©cifiques au domaine qui permettent plus de simultan√©it√©, am√©liorent la structure, la pr√©sentation de l'acc√®s √† la m√©moire et simplifient la superposition d'application efficace sur un processeur d√©di√©. <br><br><h1>  Langues orient√©es sujet </h1><br>  Les DSA n√©cessitent que les op√©rations de niveau sup√©rieur soient adapt√©es √† l'architecture du processeur, mais il est tr√®s difficile de le faire dans un langage g√©n√©ral tel que Python, Java, C ou Fortran.  Les langages sp√©cifiques au domaine (DSL) aident √† cela et vous permettent de programmer efficacement les DSA.  Par exemple, les DSL peuvent rendre explicites les op√©rations vectorielles explicites, √† matrice dense et √† matrice clairsem√©e, permettant au compilateur DSL de mapper efficacement les op√©rations au processeur.  Parmi les langages sp√©cifiques au domaine, on trouve Matlab, un langage pour travailler avec des matrices, TensorFlow pour programmer des r√©seaux de neurones, P4 pour programmer des r√©seaux d√©finis par logiciel et Halide pour traiter des images avec des transformations de haut niveau. <br><br>  Le probl√®me du DSL est de savoir comment maintenir une ind√©pendance architecturale suffisante pour que le logiciel puisse √™tre port√© sur diverses architectures, tout en atteignant une grande efficacit√© lors de la comparaison de logiciels avec un DSA de base.  Par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un syst√®me XLA traduit le</a> code <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow</a> en syst√®mes h√©t√©rog√®nes avec des GPU Nvidia ou des processeurs tenseurs (TPU).  √âquilibrer la portabilit√© entre les DSA tout en maintenant l'efficacit√© est une t√¢che de recherche int√©ressante pour les d√©veloppeurs de langage, les compilateurs et les DSA eux-m√™mes. <br><br><h3>  Exemple de DSA: TPU v1 </h3><br>  √Ä titre d'exemple de DSA, consid√©rons Google TPU v1, qui est con√ßu pour acc√©l√©rer le fonctionnement d'un r√©seau de neurones ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2</a> ).  Ce TPU a √©t√© produit depuis 2015, et de nombreuses applications ont √©t√© ex√©cut√©es dessus: des requ√™tes de recherche √† la traduction de texte et √† la reconnaissance d'images dans AlphaGo et AlphaZero, des programmes DeepMind pour jouer au go et aux √©checs.  L'objectif √©tait d'augmenter de 10 fois la productivit√© et l'efficacit√© √©nerg√©tique des r√©seaux de neurones profonds. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/057/3d6/a1a/0573d6a1a8ed756450ae9088f8606897.jpg"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">8. Organisation fonctionnelle Google Tensor Processing Unit (TPU v1)</font></i> <br><br>  Comme le montre la figure 8, l'organisation d'un TPU est radicalement diff√©rente d'un processeur √† usage g√©n√©ral.  L'unit√© de calcul principale est l'unit√© matricielle, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">structure des r√©seaux systoliques</a> , qui chaque cycle produit 256 √ó 256 multiplier-accumuler.  La combinaison d'une pr√©cision de 8 bits, d'une structure systolique tr√®s efficace, d'un contr√¥le SIMD et de l'allocation d'une partie importante de la puce pour cette fonction aide √† effectuer environ 100 fois plus d'op√©rations de multiplication d'accumulation par cycle qu'un c≈ìur de processeur √† usage g√©n√©ral.  Au lieu des caches, TPU utilise 24 Mo de m√©moire locale, ce qui repr√©sente environ le double des caches CPU √† usage g√©n√©ral de 2015 avec le m√™me TDP.  Enfin, la m√©moire d'activation des neurones et la m√©moire d'√©quilibre du r√©seau neuronal (y compris la structure FIFO qui stocke les poids) sont connect√©es via des canaux √† grande vitesse contr√¥l√©s par l'utilisateur.  Les performances TPU moyennes pond√©r√©es pour six probl√®mes typiques de sortie logique des r√©seaux de neurones dans les centres de donn√©es Google sont 29 fois sup√©rieures √† celles des processeurs √† usage g√©n√©ral.  √âtant donn√© que les TPU n√©cessitent moins de la moiti√© de la puissance, son efficacit√© √©nerg√©tique pour cette charge de travail est plus de 80 fois sup√©rieure √† celle des processeurs universels. <br><br><h1>  R√©sum√© </h1><br>  Nous avons examin√© deux approches diff√©rentes pour am√©liorer les performances du programme en augmentant l'efficacit√© de l'utilisation des technologies mat√©rielles.  Premi√®rement, en augmentant la productivit√© des langues modernes de haut niveau qui sont g√©n√©ralement interpr√©t√©es.  Deuxi√®mement, en cr√©ant des architectures pour des domaines sp√©cifiques, qui am√©liorent consid√©rablement les performances et l'efficacit√© par rapport aux processeurs √† usage g√©n√©ral.  Les langages sp√©cifiques au domaine sont un autre exemple de la fa√ßon d'am√©liorer l'interface mat√©riel-logiciel qui permet des innovations architecturales telles que DSA.  Pour obtenir un succ√®s significatif en utilisant de telles approches, une √©quipe de projet int√©gr√©e verticalement sera n√©cessaire qui est vers√©e dans les applications, les langages th√©matiques et les technologies de compilation connexes, l'architecture informatique, ainsi que la technologie de mise en ≈ìuvre de base.  Le besoin d'int√©gration verticale et de prise de d√©cisions de conception √† diff√©rents niveaux d'abstraction √©tait typique de la plupart des premiers travaux dans le domaine de la technologie informatique avant que l'industrie ne soit structur√©e horizontalement.  Dans cette nouvelle √®re, l'int√©gration verticale est devenue plus importante.  Des avantages seront accord√©s aux √©quipes capables de trouver et d'accepter des compromis et des optimisations complexes. <br><br>  Cette opportunit√© a d√©j√† conduit √† une vague d'innovation architecturale, attirant de nombreuses philosophies architecturales concurrentes: <br><br>  <i>GPU</i>  Les GPU Nvidia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">utilisent</a> plusieurs c≈ìurs, chacun avec des fichiers de registre volumineux, plusieurs flux mat√©riels et des caches. <br><br>  <i>TPU</i>  Les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">TPU de</a> Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reposent</a> sur de grandes matrices systoliques bidimensionnelles et une m√©moire programmable sur puce. <br><br>  <i>FPGA</i>  Microsoft Corporation dans ses centres de donn√©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">impl√©mente</a> des matrices de portes programmables par l'utilisateur (FPGA), qui sont utilis√©es dans les applications de r√©seau neuronal. <br><br>  <i>CPU</i>  Intel propose des processeurs avec de nombreux c≈ìurs, un grand cache √† plusieurs niveaux et des instructions SIMD unidimensionnelles, d'une certaine mani√®re comme le FPGA de Microsoft, et le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nouveau neuroprocesseur est plus proche du TPU que du CPU</a> . <br><br>  En plus de ces acteurs majeurs, des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dizaines de startups mettent en ≈ìuvre leurs propres id√©es</a> .  Pour r√©pondre √† la demande croissante, les concepteurs combinent des centaines et des milliers de puces pour cr√©er des superordinateurs de r√©seau neuronal. <br><br>  Cette avalanche d'architectures de r√©seaux de neurones indique qu'un moment int√©ressant est venu dans l'histoire de l'architecture informatique.  En 2019, il est difficile de pr√©dire lequel de ces nombreux domaines gagnera (si quelqu'un gagne du tout), mais le march√© d√©terminera certainement le r√©sultat, tout comme il a r√©gl√© le d√©bat architectural du pass√©. <br><br><h1>  Architecture ouverte </h1><br>  Suivant l'exemple d'un logiciel open source r√©ussi, l'open ISA repr√©sente une opportunit√© alternative en architecture informatique.  Ils sont n√©cessaires pour cr√©er une sorte de ¬´Linux pour les processeurs¬ª, afin que la communaut√© puisse cr√©er des noyaux open source en plus des entreprises individuelles qui poss√®dent des noyaux propri√©taires.  Si de nombreuses organisations con√ßoivent des processeurs utilisant la m√™me ISA, une concurrence accrue peut conduire √† une innovation encore plus rapide.  L'objectif est de fournir une architecture pour les processeurs co√ªtant de quelques centimes √† 100 $. <br><br>  Le premier exemple est RISC-V (RISC Five), la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cinqui√®me architecture RISC d√©velopp√©e √† l'Universit√© de Californie √† Berkeley</a> .  Elle est soutenue par une communaut√© dirig√©e par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la Fondation RISC-V</a> .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'ouverture de l'architecture permet de faire √©voluer l'ISA aux yeux du public, avec la participation d'experts jusqu'√† la prise de d√©cision finale. Un avantage suppl√©mentaire d'un fonds ouvert est qu'il est peu probable que l'ISA se d√©veloppe principalement pour des raisons de marketing, car c'est parfois la seule explication de l'expansion de leurs propres ensembles d'instructions.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RISC-V est un jeu d'instructions modulaire. Une petite base d'instructions lance une pile logicielle open source compl√®te, suivie d'extensions standard suppl√©mentaires que les concepteurs peuvent activer ou d√©sactiver en fonction de leurs besoins. Cette base de donn√©es contient des versions 32 et 64 bits des adresses. RISC-V ne peut se d√©velopper que gr√¢ce √† des extensions facultatives; la pile logicielle fonctionnera toujours bien, m√™me si les architectes n'acceptent pas les nouvelles extensions. Les architectures propri√©taires n√©cessitent g√©n√©ralement une compatibilit√© ascendante au niveau binaire: cela signifie que si la soci√©t√© de traitement ajoute une nouvelle fonctionnalit√©, tous les futurs processeurs devraient √©galement l'inclure. RISC-V ne le fait pas, ici toutes les am√©liorations sont facultatives et peuvent √™tre supprim√©es si l'application n'en a pas besoin.Voici les extensions standard pour le moment, avec les premi√®res lettres du nom complet:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> M. Multiplication / division d'un entier. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A. Op√©rations de m√©moire atomique. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">F / d. </font><font style="vertical-align: inherit;">Op√©rations en virgule flottante simple / double pr√©cision.</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C. Instructions compress√©es. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La troisi√®me caract√©ristique de RISC-V est la simplicit√© de l'ISA. </font><font style="vertical-align: inherit;">Bien que cet indicateur ne soit pas quantifiable, voici deux comparaisons avec l'architecture ARMv8, qui a √©t√© d√©velopp√©e en parall√®le par ARM:</font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Moins d'instructions</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">RISC-V contient beaucoup moins d'instructions. </font><font style="vertical-align: inherit;">Il y en a 50 dans la base de donn√©es, et ils sont </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©tonnamment similaires en nombre et en caract√®re au RISC-I d'origine</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Le reste des extensions standard (M, A, F et D) ajoute 53 instructions, plus C en ajoute 34 de plus, donc le nombre total est de 137. Pour comparaison, ARMv8 a plus de 500 instructions.</font></font><br></li><li> <b>  </b> .  RISC-V    : ,    ARMv8    14. </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La simplicit√© simplifie √† la fois la conception de la conception des processeurs et la v√©rification de leur exactitude. Parce que RISC-V se concentre sur tout, des centres de donn√©es aux appareils IoT, la validation de la conception peut repr√©senter une part importante des co√ªts de d√©veloppement. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quatri√®mement, RISC-V est une conception vierge apr√®s 25 ans, o√π les architectes apprennent des erreurs de leurs pr√©d√©cesseurs. Contrairement √† l'architecture RISC de premi√®re g√©n√©ration, elle √©vite la microarchitecture ou les fonctions qui d√©pendent de la technologie (telles que les branches diff√©r√©es et les t√©l√©chargements diff√©r√©s) ou des innovations (telles que les fen√™tres de registre), qui ont √©t√© supplant√©es par les avanc√©es du compilateur. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enfin, RISC-V prend en charge DSA, r√©servant un espace opcode √©tendu pour les acc√©l√©rateurs personnalis√©s. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En plus de RISC-V, Nvidia a √©galement annonc√© (en 2017)</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Une architecture libre et ouverte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , elle l'appelle Nvidia Deep Learning Accelerator (NVDLA). Il s'agit d'un DSA √©volutif et personnalisable pour l'inf√©rence dans l'apprentissage automatique. Les param√®tres de configuration incluent le type de donn√©es (int8, int16 ou fp16) et la taille de la matrice de multiplication bidimensionnelle. L'√©chelle du substrat de silicium varie de 0,5 mm¬≤ √† 3 mm¬≤, et la consommation d'√©nergie est de 20 mW √† 300 mW. ISA, la pile logicielle et l'impl√©mentation sont ouvertes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Des architectures ouvertes et simples vont bien avec la s√©curit√©. Premi√®rement, les experts en s√©curit√© ne croient pas √† la s√©curit√© par l'obscurit√©, les impl√©mentations open source sont donc attrayantes et les impl√©mentations open source n√©cessitent une architecture ouverte. Tout aussi importante est l'augmentation du nombre de personnes et d'organisations qui peuvent innover dans le domaine des architectures s√©curis√©es. Les architectures propri√©taires limitent la participation des employ√©s, mais les architectures ouvertes permettent aux meilleurs cerveaux du monde universitaire et de l'industrie d'aider √† la s√©curit√©. Enfin, la simplicit√© de RISC-V simplifie la v√©rification de ses impl√©mentations. De plus, les architectures ouvertes, les impl√©mentations et les piles de logiciels, ainsi que la flexibilit√© des FPGA, permettent aux architectes de d√©ployer et d'√©valuer de nouvelles solutions en ligne avec des cycles de publication hebdomadaires plut√¥t qu'annuels. Bien que les FPGA soient 10 fois plus lents que les puces personnalis√©es,mais leurs performances sont suffisantes pour travailler en ligne et pr√©senter des innovations de s√©curit√© devant de vrais attaquants pour v√©rification. Nous nous attendons √† ce que les architectures ouvertes soient des exemples de conception collaborative de mat√©riel et de logiciels par des architectes et des experts en s√©curit√©.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> D√©veloppement mat√©riel flexible </font></font></h1><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Flexible Software Development Manifesto (2001)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Beck et al.R√©volutionn√© le d√©veloppement de logiciels en √©liminant les probl√®mes d'un syst√®me de cascade traditionnel bas√© sur la planification et la documentation. </font><font style="vertical-align: inherit;">De petites √©quipes de programmeurs cr√©ent rapidement des prototypes fonctionnels mais incomplets et re√ßoivent les commentaires des clients avant de commencer la prochaine it√©ration. </font><font style="vertical-align: inherit;">La version Scrum d'Agile rassemble des √©quipes de cinq √† dix programmeurs qui sprint pendant deux √† quatre semaines par it√©ration.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s avoir repris l'id√©e du d√©veloppement logiciel, il est possible d'organiser un d√©veloppement mat√©riel flexible. La bonne nouvelle est que les outils modernes de conception assist√©e par ordinateur (ECAD) ont augment√© le niveau d'abstraction, permettant un d√©veloppement flexible. Ce niveau d'abstraction plus √©lev√© augmente √©galement le niveau de r√©utilisation du travail entre diff√©rentes conceptions. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les sprints de quatre semaines semblent peu plausibles pour les processeurs, √©tant donn√© les mois entre la cr√©ation du design et la production de puces. Dans la fig. La figure 9 montre comment une m√©thode flexible peut fonctionner </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en modifiant un prototype √† un niveau appropri√©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/278/4ae/e4c/2784aee4c39cbdfaeab3bbbfd500a056.jpg"><br> <i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig. 9. M√©thodologie flexible de d√©veloppement de l'√©quipement</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le niveau le plus int√©rieur est un simulateur de logiciel, l'endroit le plus simple et le plus rapide pour effectuer des changements. Le niveau suivant est celui des puces FPGA, qui peuvent fonctionner des centaines de fois plus rapidement qu'un simulateur logiciel d√©taill√©. Les FPGA peuvent fonctionner avec des syst√®mes d'exploitation et des r√©f√©rences compl√®tes, telles que la Standard Performance Evaluation Corporation (SPEC), qui permet une √©valuation beaucoup plus pr√©cise des prototypes. Amazon Web Services propose des FPGA dans le cloud, afin que les architectes puissent utiliser des FPGA sans avoir √† acheter d‚Äô√©quipement ni √† installer un laboratoire. Le niveau suivant utilise des outils ECAD pour g√©n√©rer un circuit √† puce, pour documenter la taille et la consommation d'√©nergie. M√™me apr√®s le travail des outils, il est n√©cessaire de suivre certaines √©tapes manuelles pour affiner les r√©sultats avant d'envoyer le nouveau processeur en production.Les d√©veloppeurs de processeurs appellent ce niveau sup√©rieur.</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bande</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Ces quatre premiers niveaux prennent en charge les sprints de quatre semaines. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√Ä des fins de recherche, nous pourrions nous arr√™ter au niveau quatre, car les estimations de la superficie, de l'√©nergie et des performances sont tr√®s pr√©cises. Mais c‚Äôest comme si un coureur a couru un marathon et s‚Äôest arr√™t√© 5 m√®tres avant l‚Äôarriv√©e, car son temps d‚Äôarriv√©e est d√©j√† clair. Malgr√© la pr√©paration difficile du marathon, il manquera le frisson et le plaisir de franchir la ligne d'arriv√©e. L'un des avantages des ing√©nieurs en mat√©riel par rapport aux ing√©nieurs en logiciel est qu'ils cr√©ent des choses physiques. Obtenir des puces de l'usine: mesurer, ex√©cuter de vrais programmes, les montrer √† des amis et √† la famille est une grande joie pour le concepteur.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De nombreux chercheurs pensent qu'ils devraient arr√™ter parce que la fabrication de puces est trop abordable. </font><font style="vertical-align: inherit;">Mais si le design est petit, il est √©tonnamment bon march√©. </font><font style="vertical-align: inherit;">Les ing√©nieurs peuvent commander 100 micropuces de 1 mm¬≤ pour seulement 14 000 $. √Ä 28 nm, une puce de 1 mm¬≤ contient des millions de transistors: cela suffit pour le processeur RISC-V et l'acc√©l√©rateur NVLDA. </font><font style="vertical-align: inherit;">Le niveau le plus externe est cher si le concepteur a l'intention de cr√©er une grande puce, mais de nombreuses nouvelles id√©es peuvent √™tre d√©montr√©es sur de petites puces.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Conclusion </font></font></h1><br> <i>¬´   ‚Äî   ¬ª</i> ‚Äî  , 1650 <br><br>      ,    ,       ,     /             . iAPX-432  Itanium ,        ,     S/360, 8086  ARM    ,    . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'ach√®vement de la loi de Moore et la mise √† l'√©chelle de Dennard, ainsi que le ralentissement des performances des microprocesseurs standard, ne sont pas des probl√®mes qui devraient √™tre r√©solus, mais un fait qui, comme vous le savez, offre des opportunit√©s int√©ressantes. Les langages et architectures de haut niveau ax√©s sur les mati√®res, lib√©r√©s des cha√Ænes de jeux d'instructions propri√©taires, ainsi que la demande du public pour une s√©curit√© accrue, ouvriront un nouvel √¢ge d'or pour l'architecture informatique. Dans les √©cosyst√®mes open source, les puces con√ßues artificiellement d√©montreront de mani√®re convaincante les r√©alisations et acc√©l√©reront ainsi la mise en ≈ìuvre commerciale. La philosophie du processeur √† usage g√©n√©ral dans ces puces est probablement RISC, qui a r√©sist√© √† l'√©preuve du temps. Attendez-vous √† la m√™me innovation rapide que vous avez fait au cours du dernier √¢ge d'or,mais cette fois en termes de co√ªt, d'√©nergie et de s√©curit√©, pas seulement de performances.</font></font><br><br>         ,            . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr440760/">https://habr.com/ru/post/fr440760/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr440748/index.html">Le supercalculateur le plus rapide du monde bat le record de l'IA</a></li>
<li><a href="../fr440752/index.html">S√©lection de la priorit√© de la demande de l'utilisateur</a></li>
<li><a href="../fr440754/index.html">Utilitaire anglais multiplateforme pour visualiser les certificats qualifi√©s russes x509</a></li>
<li><a href="../fr440756/index.html">CI / CD sans serveur sur AWS</a></li>
<li><a href="../fr440758/index.html">Allez Meetup √† Acronis! (Moscou, Fiztehpark)</a></li>
<li><a href="../fr440762/index.html">Avis des employeurs: la nature et le non-sens des avis anonymes</a></li>
<li><a href="../fr440766/index.html">Des geeks aux geeks: des cadeaux pour le 23 f√©vrier</a></li>
<li><a href="../fr440772/index.html">Conception bas√©e sur le domaine: une recette pour un pragmatique</a></li>
<li><a href="../fr440774/index.html">De graves erreurs math√©matiques de la NHTSA permettent √† Tesla de revendiquer la s√©curit√© du pilote automatique</a></li>
<li><a href="../fr440776/index.html">Courriel, vue int√©rieure</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>