<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐯 🏰 ☝️ Tupperware: Kubernetes, o assassino do Facebook? 👩🏼‍🤝‍👩🏻 ✔️ 👨‍🏫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Gerenciamento de cluster eficiente e confiável em qualquer escala com o Tupperware 





 Hoje, na conferência Systems @Scale, apresentamos o Tupperwa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tupperware: Kubernetes, o assassino do Facebook?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/455579/"><p>  Gerenciamento de cluster eficiente e confiável em qualquer escala com o Tupperware </p><br><p><img src="https://habrastorage.org/webt/gm/mv/jn/gmmvjn5ev7lk2kyhaiejighzrf0.jpeg"></p><br><p>  Hoje, na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conferência Systems @Scale,</a> apresentamos o Tupperware, nosso sistema de gerenciamento de cluster que orquestra contêineres em milhões de servidores, onde quase todos os nossos serviços funcionam.  Lançamos o Tupperware pela primeira vez em 2011 e, desde então, nossa infraestrutura cresceu de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1 data center</a> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">15 data centers distribuídos geograficamente</a> .  Todo esse tempo a Tupperware não parou e se desenvolveu conosco.  Informaremos em que situações a Tupperware fornece gerenciamento de cluster de primeira classe, incluindo suporte conveniente para serviços com estado, um painel de controle único para todos os data centers e a capacidade de distribuir energia entre serviços em tempo real.  E compartilharemos as lições que aprendemos à medida que nossa infraestrutura se desenvolveu. </p><br><p> Tupperware executa várias tarefas.  Os desenvolvedores de aplicativos o utilizam para fornecer e gerenciar aplicativos.  Ele compacta as dependências de código e aplicativo em uma imagem e as entrega aos servidores na forma de contêineres.  Os contêineres fornecem isolamento entre aplicativos no mesmo servidor, para que os desenvolvedores estejam ocupados com a lógica do aplicativo e não pensem em como encontrar servidores ou controlar atualizações.  A Tupperware também monitora o desempenho do servidor e, se encontrar uma falha, transfere contêineres do servidor com problema. </p><a name="habracut"></a><br><p>  Os engenheiros de planejamento de capacidade usam o Tupperware para distribuir as capacidades do servidor em equipes de acordo com o orçamento e as restrições.  Eles também o usam para melhorar a utilização do servidor.  Os operadores de data center recorrem à Tupperware para distribuir adequadamente os contêineres entre os data centers e parar ou mover contêineres durante a manutenção.  Devido a isso, a manutenção de servidores, redes e equipamentos requer um envolvimento humano mínimo. </p><br><h3 id="arhitektura-tupperware">  Arquitetura Tupperware </h3><br><p> <a href=""><img src="https://habrastorage.org/webt/e7/q1/oz/e7q1ozhv85xlsvgczzofpgwoikg.jpeg"></a> </p><br><p>  <em>Arquitetura O Tupperware PRN é uma das regiões de nossos data centers.</em>  <em>A região consiste em vários edifícios de data center (PRN1 e PRN2) localizados nas proximidades.</em>  <em>Planejamos criar um painel de controle que gerencie todos os servidores em uma região.</em> </p><br><p>  Os desenvolvedores de aplicativos fornecem serviços na forma de trabalhos da Tupperware.  Uma tarefa consiste em vários contêineres, e todos eles geralmente executam o mesmo código de aplicativo. </p><br><p>  A Tupperware é responsável pelo provisionamento de contêineres e pelo gerenciamento do ciclo de vida.  Consiste em vários componentes: </p><br><ul><li>  O Tupperware Frontend fornece uma API para a interface do usuário, a CLI e outras ferramentas de automação por meio das quais você pode interagir com o Tupperware.  Eles ocultam toda a estrutura interna dos proprietários de tarefas da Tupperware. </li><li>  O Tupperware Scheduler é o painel de controle responsável pelo gerenciamento do ciclo de vida do contêiner e do trabalho.  Ele é implantado nos níveis regional e global, onde um agendador regional gerencia servidores em uma região e um agendador global gerencia servidores de diferentes regiões.  O planejador é dividido em shards, e cada shard controla um conjunto de tarefas. </li><li>  O proxy do planejador no Tupperware oculta o sharding interno e fornece um painel de controle unificado conveniente para os usuários do Tupperware. </li><li>  O distribuidor Tupperware atribui contêineres aos servidores.  O planejador é responsável por interromper, iniciar, atualizar e falhar contêineres.  Atualmente, um único distribuidor pode gerenciar uma região inteira sem se dividir em shards.  (Observe a diferença na terminologia. Por exemplo, o planejador no Tupperware corresponde ao painel de controle no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes</a> , e o distribuidor do Tupperware é chamado de planejador no Kubernetes.) </li><li>  O intermediário de recursos armazena a fonte da verdade para os eventos do servidor e de serviço.  Executamos um intermediário de recursos para cada data center e ele armazena todas as informações do servidor nesse data center.  Um intermediário de recursos e um sistema de gerenciamento de capacidade, ou sistema de alocação de recursos, decidem dinamicamente qual fornecimento de planejador controla qual servidor.  O serviço de verificação de integridade monitora servidores e armazena dados sobre sua saúde no intermediário de recursos.  Se o servidor tiver problemas ou precisar de manutenção, o intermediário de recursos solicitará ao distribuidor e ao planejador que parem os contêineres ou os transfiram para outros servidores. </li><li>  O Tupperware Agent é um daemon em execução em cada servidor que prepara e remove contêineres.  Os aplicativos funcionam dentro do contêiner, o que lhes confere mais isolamento e reprodutibilidade.  Na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conferência Systems @Scale do ano passado,</a> já descrevemos como os contêineres Tupperware individuais são criados usando imagens, btrfs, cgroupv2 e systemd. </li></ul><br><h3 id="otlichitelnye-osobennosti-tupperware">  Recursos distintos do Tupperware </h3><br><p>  O Tupperware é muito semelhante a outros sistemas de gerenciamento de cluster, como Kubernetes e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mesos</a> , mas existem algumas diferenças: </p><br><ul><li>  Suporte nativo para serviços com estado. </li><li>  Um único painel de controle para servidores em diferentes data centers para automatizar a entrega de contêineres com base em intenção, desclassificação de clusters e manutenção. </li><li>  Separação clara do painel de controle para zoom. </li><li>  Cálculos flexíveis permitem distribuir energia entre serviços em tempo real. </li></ul><br><p>  Projetamos esses recursos interessantes para oferecer suporte a uma variedade de aplicativos sem estado e sem estado em um enorme parque de servidores compartilhados globalmente. </p><br><h3 id="vstroennaya-podderzhka-stateful-sevisov">  Suporte nativo para serviços com estado. </h3><br><p>  A Tupperware gerencia muitos serviços stateful críticos que armazenam dados persistentes de produtos para Facebook, Instagram, Messenger e WhatsApp.  Podem ser grandes pares de valores-chave (por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ZippyDB</a> ) e armazenamento de dados de monitoramento (por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ODS Gorilla</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Scuba</a> ).  Manter serviços com estado não é fácil, porque o sistema deve garantir que as entregas de contêineres possam suportar falhas em grande escala, incluindo uma queda de energia ou uma queda de energia.  Embora os métodos convencionais, como a distribuição de contêineres pelos domínios de falha, sejam adequados para serviços sem estado, os serviços com estado precisam de suporte adicional. </p><br><p>  Por exemplo, se, como resultado de uma falha no servidor, uma réplica do banco de dados se tornar indisponível, é necessário permitir a manutenção automática que atualizará os kernels em 50 servidores a partir de um pool de 10 milésimos?  Depende da situação.  Se em um desses 50 servidores houver outra réplica do mesmo banco de dados, é melhor aguardar e não perder duas réplicas de uma só vez.  Para tomar decisões dinamicamente sobre a manutenção e a integridade do sistema, você precisa de informações sobre a replicação de dados internos e a lógica de localização de cada serviço com estado. </p><br><p>  A interface TaskControl permite que serviços com estado influenciem decisões que afetam a disponibilidade de dados.  Usando essa interface, o planejador notifica aplicativos externos sobre operações de contêiner (reinicialização, atualização, migração, manutenção).  O serviço Stateful implementa um controlador que informa ao Tupperware quando cada operação pode ser executada com segurança e essas operações podem ser trocadas ou atrasadas temporariamente.  No exemplo acima, o controlador do banco de dados pode instruir a Tupperware a atualizar 49 dos 50 servidores, mas não tocar em um servidor específico (X) até o momento.  Como resultado, se o período de atualização do kernel passar e o banco de dados ainda não conseguir restaurar a réplica do problema, o Tupperware ainda atualizará o servidor X. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/xu/xi/5q/xuxi5qpox1v3gpc6khbipxgna0i.jpeg"></a> </p><br><p>  Muitos serviços com estado no Tupperware não usam o TaskControl diretamente, mas através do ShardManager, uma plataforma comum para a criação de serviços com estado no Facebook.  Com a Tupperware, os desenvolvedores podem indicar sua intenção de como os contêineres devem ser distribuídos pelos datacenters.  Com o ShardManager, os desenvolvedores indicam sua intenção de como os shards de dados devem ser distribuídos pelos contêineres.  O ShardManager está ciente da hospedagem e replicação de dados de seus aplicativos e interage com o Tupperware por meio da interface TaskControl para planejar operações de contêiner sem o envolvimento direto do aplicativo.  Essa integração simplifica bastante o gerenciamento de serviços com estado, mas o TaskControl é capaz de mais.  Por exemplo, nossa extensa camada da web é sem estado e usa o TaskControl para ajustar dinamicamente a velocidade das atualizações em contêineres.  Como resultado, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">camada</a> da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Web pode concluir rapidamente várias versões de software</a> por dia, sem comprometer a disponibilidade. </p><br><h3 id="upravlenie-serverami-v-datacentrah">  Gerenciamento de servidor em data centers </h3><br><p>  Quando a Tupperware apareceu pela primeira vez em 2011, um agendador separado controlava cada cluster de servidores.  Em seguida, o cluster do Facebook era um grupo de racks de servidores conectados a um comutador de rede, e o datacenter continha vários clusters.  O planejador pode gerenciar servidores em apenas um cluster, ou seja, a tarefa não pode se estender a vários clusters.  Nossa infraestrutura estava crescendo, estávamos cada vez mais cancelando clusters.  Como o Tupperware não pôde transferir a tarefa do cluster descomissionado para outros clusters sem alterações, foi necessário muito esforço e coordenação cuidadosa entre os desenvolvedores de aplicativos e os operadores de data center.  Esse processo levou a um desperdício de recursos quando os servidores ficaram ociosos por meses devido ao procedimento de descomissionamento. </p><br><p>  Criamos um intermediário de recursos para resolver o problema de desclassificação de clusters e coordenar outros tipos de tarefas de manutenção.  O intermediário de recursos monitora todas as informações físicas associadas ao servidor e decide dinamicamente qual planejador gerencia cada servidor.  A ligação dinâmica de servidores a agendadores permite que o agendador gerencie servidores em diferentes datacenters.  Como o trabalho da Tupperware não se limita mais a um cluster, os usuários da Tupperware podem especificar como os contêineres devem ser distribuídos pelos domínios de falha.  Por exemplo, um desenvolvedor pode declarar sua intenção (por exemplo: "executar minha tarefa em 2 domínios de falha na região PRN") sem especificar zonas de disponibilidade específicas.  A própria Tupperware encontrará os servidores certos para incorporar essa intenção, mesmo no caso de descomissionamento de um cluster ou serviço. </p><br><h3 id="masshtabirovanie-dlya-podderzhki-vsey-globalnoy-sistemy">  Escalonamento para suportar todo o sistema global </h3><br><p>  Historicamente, nossa infraestrutura foi dividida em centenas de pools de servidores dedicados para equipes individuais.  Devido à fragmentação e à falta de padrões, tínhamos altos custos de transação e servidores inativos eram mais difíceis de usar novamente.  Na conferência <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Systems @Scale</a> do ano passado, introduzimos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">infraestrutura como serviço (IaaS)</a> , que deve integrar nossa infraestrutura a uma grande frota de servidores unificada.  Mas uma única frota de servidores tem suas próprias dificuldades.  Ele deve atender a certos requisitos: </p><br><ul><li>  <strong>Escalabilidade.</strong>  Nossa infraestrutura cresceu com a adição de data centers em cada região.  Os servidores tornaram-se menores e mais eficientes em termos de energia; portanto, em cada região há muito mais.  Como resultado, um único planejador para uma região não pode lidar com o número de contêineres que podem ser executados em centenas de milhares de servidores em cada região. </li><li>  <strong>Confiabilidade</strong>  Mesmo que a escala do planejador possa ser aumentada, devido ao grande escopo do planejador, o risco de erros será maior e toda a região de contêineres poderá se tornar incontrolável. </li><li>  <strong>Tolerância a falhas.</strong>  No caso de uma grande falha na infraestrutura (por exemplo, devido a uma falha na rede ou falta de energia, os servidores em que o agendador está executando falharão), apenas uma parte dos servidores da região terá consequências negativas. </li><li>  <strong>Facilidade de uso.</strong>  Pode parecer que você precise executar vários agendadores independentes em uma região.  Mas em termos de conveniência, um único ponto de entrada em um pool comum na região simplifica o gerenciamento de capacidade e trabalho. </li></ul><br><p>  Dividimos o planejador em shards para resolver problemas de suporte a um grande pool compartilhado.  Cada fragmento do planejador gerencia seu conjunto de tarefas na região, e isso reduz o risco associado ao planejador.  À medida que o pool total cresce, podemos adicionar mais shards do agendador.  Para usuários da Tupperware, os shards e os agendadores de proxy se parecem com um painel de controle.  Eles não precisam trabalhar com muitos fragmentos que orquestram tarefas.  Os shards do agendador são fundamentalmente diferentes dos agendadores de cluster que usamos anteriormente, quando o painel de controle foi dividido sem separação estática do pool de servidores comum, de acordo com a topologia de rede. </p><br><h3 id="povyshenie-effektivnosti-ispolzovaniya-s-pomoschyu-elastichnyh-vychisleniy">  Melhorando a utilização com computação elástica </h3><br><p>  Quanto maior a nossa infraestrutura, mais importante é usar nossos servidores com eficiência para otimizar os custos da infraestrutura e reduzir a carga.  Existem duas maneiras de melhorar a utilização do servidor: </p><br><ul><li>  Computação flexível - reduza a escala de serviços online durante horas tranquilas e use os servidores liberados para cargas offline, por exemplo, para aprendizado de máquina e tarefas MapReduce. </li><li>  Carga excessiva - hospede serviços online e cargas de trabalho em lote nos mesmos servidores para que as cargas em lote sejam executadas com baixa prioridade. </li></ul><br><p>  O gargalo em nossos data centers é <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o consumo de energia</a> .  Portanto, preferimos servidores pequenos e com baixo consumo de energia, que juntos fornecem mais poder de processamento.  Infelizmente, em pequenos servidores com uma pequena quantidade de recursos e memória do processador, o carregamento excessivo é menos eficiente.  Obviamente, podemos colocar vários contêineres de pequenos serviços em um pequeno servidor com baixo consumo de energia que consome poucos recursos e memória do processador, mas os serviços grandes terão baixo desempenho nessa situação.  Portanto, aconselhamos os desenvolvedores de nossos grandes serviços a otimizá-los para que usem o servidor inteiro. </p><br><p>  Basicamente, aprimoramos a utilização com computação elástica.  A intensidade do uso de muitos de nossos grandes serviços, por exemplo, feeds de notícias, recursos de mensagens e nível da web de front-end, depende da hora do dia.  Intencionalmente reduzimos a escala de serviços online durante horas tranquilas e usamos os servidores liberados para cargas offline, por exemplo, para tarefas de aprendizado de máquina e MapReduce. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/6w/zu/dp/6wzudppzm9tobgoryvtrssaxlra.jpeg"></a> </p><br><p>  Por experiência, sabemos que é melhor fornecer servidores inteiros como unidades de energia elástica, porque os grandes serviços são os principais doadores e os principais consumidores de energia elástica e são otimizados para o uso de servidores inteiros.  Quando o servidor é liberado do serviço online nas horas tranquilas, o intermediário de recursos entrega o servidor ao planejador para uso temporário, para que ele execute carregamentos offline.  Se ocorrer um pico de carga em um serviço online, o intermediário de recursos recupera rapidamente o servidor emprestado e, juntamente com o planejador, o retorna ao serviço online. </p><br><h3 id="usvoennye-uroki-i-plany-na-buduschee">  Lições aprendidas e planos futuros </h3><br><p>  Nos últimos 8 anos, desenvolvemos a Tupperware para acompanhar o rápido desenvolvimento do Facebook.  Falamos sobre o que aprendemos e esperamos que ajude outras pessoas a gerenciar infraestruturas em rápido crescimento: </p><br><ul><li>  Configure comunicações flexíveis entre o painel de controle e os servidores que gerencia.  Essa flexibilidade permite ao painel de controle gerenciar servidores em diferentes datacenters, ajuda a automatizar o descomissionamento e manutenção de clusters e fornece distribuição dinâmica de energia usando computação flexível. </li><li>  Com um único painel de controle na região, fica mais conveniente trabalhar com tarefas e mais fácil gerenciar uma grande frota comum de servidores.  Observe que o painel de controle suporta um único ponto de entrada, mesmo que sua estrutura interna seja dividida por razões de escala ou tolerância a falhas. </li><li>  Usando o modelo de plug-in, o painel de controle pode notificar aplicativos externos sobre as próximas operações de contêiner.  Além disso, os serviços com estado podem usar a interface do plug-in para configurar o gerenciamento de contêiner.  Usando esse modelo de plug-in, o painel de controle fornece simplicidade e serve efetivamente a muitos serviços com estado diferentes. </li><li>  Acreditamos que a computação elástica, na qual contratamos servidores inteiros para trabalhos em lotes, aprendizado de máquina e outros serviços não urgentes dos serviços de doadores, é a melhor maneira de aumentar a eficiência do uso de servidores pequenos e com baixo consumo de energia. </li></ul><br><p>  Estamos apenas começando a implementar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">único parque de servidores comum global</a> .  Agora, cerca de 20% dos nossos servidores estão no pool comum.  Para atingir 100%, você precisa resolver muitos problemas, incluindo o suporte a um pool comum para sistemas de armazenamento, automatizando a manutenção, gerenciando os requisitos de diferentes clientes, melhorando a utilização do servidor e melhorando o suporte para cargas de trabalho de aprendizado de máquina.  Mal podemos esperar para realizar essas tarefas e compartilhar nossos sucessos. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt455579/">https://habr.com/ru/post/pt455579/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt455565/index.html">A mente pode fingir o universo?</a></li>
<li><a href="../pt455569/index.html">Convidamos você para a Conferência Tarantool em 17 de junho</a></li>
<li><a href="../pt455571/index.html">Cursores do banco de dados em Doutrina</a></li>
<li><a href="../pt455575/index.html">Correspondência neural: como adaptar o conteúdo às realidades do Google</a></li>
<li><a href="../pt455577/index.html">Lições do SDL 2: Lição 3 - Eventos</a></li>
<li><a href="../pt455580/index.html">Animações de aplicativos móveis indispensáveis</a></li>
<li><a href="../pt455582/index.html">Navegação na loja: através da realidade aumentada até a prateleira desejada</a></li>
<li><a href="../pt455584/index.html">Entrevistas personalizadas com as forças internas da empresa: através de erros para descobertas</a></li>
<li><a href="../pt455586/index.html">Série de palestras sobre robótica do professor Gregor Schöner, diretor do Instituto de Neuroinformática (INI) Bochum, Alemanha</a></li>
<li><a href="../pt455588/index.html">Como educar sua comunidade para não dançar com um pandeiro</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>