<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👌🏿 ㊙️ 👨🏻‍🎓 Teknik reduksi jaringan konvolusi Jedi - pemangkasan 🧢 🏢 👩🏿‍🤝‍👩🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sebelum Anda lagi, tugas mendeteksi objek. Prioritas - kecepatan dengan akurasi yang dapat diterima. Anda mengambil arsitektur YOLOv3 dan melatihnya. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teknik reduksi jaringan konvolusi Jedi - pemangkasan</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482050/"><p><img src="https://habrastorage.org/webt/tf/oa/br/tfoabr16w_dawnzb9hnjndyv_bg.png" alt="gambar"></p><br><p>  Sebelum Anda lagi, tugas mendeteksi objek.  Prioritas - kecepatan dengan akurasi yang dapat diterima.  Anda mengambil arsitektur YOLOv3 dan melatihnya.  Akurasi (mAp75) lebih besar dari 0,95.  Namun kecepatan lari masih rendah.  Neraka </p><br><p>  Hari ini kita akan melewati kuantisasi.  Dan di bawah potongan, pertimbangkan <strong>Model Pruning</strong> - memangkas bagian jaringan yang berlebihan untuk mempercepat Inferensi tanpa kehilangan keakuratan.  Secara visual - di mana, berapa banyak dan cara memotong.  Mari kita cari tahu cara melakukannya secara manual dan di mana Anda dapat mengotomatisasi.  Pada akhirnya adalah repositori di keras. </p><a name="habracut"></a><br><h3 id="vvedenie">  Pendahuluan </h3><br><p>  Di tempat kerja terakhir, Perm Macroscop, saya punya satu kebiasaan - selalu memantau waktu eksekusi algoritma.  Dan waktu menjalankan jaringan harus selalu diperiksa melalui filter kecukupan.  Biasanya state-of-the-art di prod tidak lulus filter ini, yang membuat saya ke Pruning. </p><br><p>  Pemangkasan adalah tema lama yang dibicarakan di <a href="https://www.youtube.com/watch%3Fv%3DeZdOkDtYMoo" rel="nofollow">kuliah Stanford</a> 2017.  Gagasan utamanya adalah mengurangi ukuran jaringan terlatih tanpa kehilangan keakuratan dengan menghapus berbagai node.  Kedengarannya keren, tapi saya jarang mendengar tentang penggunaannya.  Mungkin, tidak ada implementasi yang cukup, tidak ada artikel berbahasa Rusia, atau hanya semua orang menganggap pemangkasan pengetahuan dan diam. <br>  Tapi pergi bongkar </p><br><h3 id="vzglyad-v-biologiyu">  Pandangan tentang biologi </h3><br><p>  Saya suka ketika di Deep Learning ide-ide berasal dari biologi.  Mereka, seperti evolusi, dapat dipercaya (tahukah Anda bahwa ReLU sangat mirip dengan <a href="http://www.gatsby.ucl.ac.uk/~lmate/biblio/dayanabbott.pdf" rel="nofollow">fungsi mengaktifkan neuron di otak</a> ?) </p><br><p>  Proses Pemangkasan Model juga dekat dengan biologi.  Respons jaringan di sini dapat dibandingkan dengan plastisitas otak.  Beberapa contoh menarik ada dalam buku <a href="https://www.litres.ru/norman-doydzh/plastichnost-mozga/%3Futm_medium%3Dcpc%26utm_source%3Dgoogle%26utm_campaign%3DDSA%257C149839530%26utm_term%3D%26utm_content%3Dk50id%257Caud-499675211712%253Adsa-179513627318%257Ccid%257C149839530%257Caid%257C248455294996%257Cgid%257C6837176850%257Cpos%257C1t1%257Csrc%257Cg_%257Cdvc%257Cc%257Creg%257C1011993%257Crin%257C%257C%26k50id%3D6837176850%257Caud-499675211712%253Adsa-179513627318%26gclid%3DCj0KCQiA0ZHwBRCRARIsAK0Tr-oKPqkmL7_Oxg62JZO8Jlk9zO-9nYKIRFxHi_lgoCvsQQadvUGxUzkaApgpEALw_wcB" rel="nofollow">Norman Dodge</a> : </p><br><ol><li>  Otak seorang wanita yang hanya memiliki separuh dari kelahiran memprogram ulang dirinya untuk melakukan fungsi-fungsi setengah yang hilang </li><li>  Pria itu menembak dirinya sendiri bagian otak yang bertanggung jawab untuk penglihatan.  Seiring waktu, bagian otak lain mengambil alih fungsi-fungsi ini.  (jangan coba lagi) </li></ol><br><p>  Jadi dari model Anda, Anda dapat memotong beberapa bundel yang lemah.  Dalam kasus ekstrim, bundel yang tersisa akan membantu menggantikan yang dipotong. </p><br><h3 id="lyubish-transfer-learning-ili-uchish-s-nulya">  Apakah Anda suka Transfer Learning atau belajar dari awal? </h3><br><p>  <strong>Opsi nomor satu.</strong>  Anda menggunakan Transfer Learning di Yolov3.  Retina, Mask-RCNN atau U-Net.  Tetapi lebih sering daripada tidak, kita tidak perlu mengenali 80 kelas objek, seperti pada COCO.  Dalam latihan saya, semuanya terbatas pada 1-2 kelas.  Dapat diasumsikan bahwa arsitektur untuk 80 kelas berlebihan di sini.  Ini memunculkan pemikiran bahwa arsitektur perlu dikurangi.  Selain itu, saya ingin melakukan ini tanpa kehilangan bobot pra-pelatihan yang ada. </p><br><p>  <strong>Opsi nomor dua.</strong>  Mungkin Anda memiliki banyak data dan sumber daya komputasi, atau Anda hanya perlu arsitektur super-kustom.  Itu tidak masalah.  Tetapi Anda mempelajari jaringan dari awal.  Urutan yang biasa adalah untuk melihat struktur data, pilih arsitektur yang MENGURANGI kekuatan dan mendorong dropout dari pelatihan ulang.  Saya melihat putus 0,6, Carl. </p><br><p>  Dalam kedua kasus, jaringan dapat dikurangi.  Dipromosikan.  Sekarang mari kita cari tahu seperti apa <del>  sunat </del>  pemangkasan </p><br><h3 id="obschiy-algoritm">  Algoritma umum </h3><br><p>  Kami memutuskan bahwa kami dapat menghapus konvolusi.  Ini terlihat sangat sederhana: </p><br><p><img src="https://habrastorage.org/webt/ey/yt/-g/eyyt-g-b6pfzjrbnim_ssosyqqk.png"></p><br><p>  Menghapus konvolusi adalah tekanan untuk jaringan, yang biasanya menyebabkan peningkatan kesalahan.  Di satu sisi, pertumbuhan kesalahan ini merupakan indikator seberapa benar kita menghapus konvolusi (misalnya, pertumbuhan besar menunjukkan bahwa kita melakukan sesuatu yang salah).  Tetapi pertumbuhan kecil cukup dapat diterima dan sering dihilangkan dengan pelatihan selanjutnya yang mudah dengan LR kecil.  Kami menambahkan langkah pelatihan ulang: </p><br><p><img src="https://habrastorage.org/webt/kb/ui/5d/kbui5dm1k8sgflm5xzu0wggbbhs.png"></p><br><p>  Sekarang kita perlu memahami kapan kita ingin menghentikan Pembelajaran kita &lt;-&gt; Siklus Pemangkasan.  Mungkin ada opsi eksotis ketika kita perlu mengurangi jaringan ke ukuran dan kecepatan tertentu (misalnya, untuk perangkat seluler).  Namun, opsi yang paling umum adalah melanjutkan siklus sampai kesalahan menjadi lebih tinggi dari yang diizinkan.  Tambahkan kondisi: </p><br><p><img src="https://habrastorage.org/webt/1i/pi/52/1ipi52uqhkciw2ne-1zt2rbdmje.png"></p><br><p>  Jadi, algoritma menjadi jelas.  Masih harus dibongkar cara menentukan konvolusi yang dihapus. </p><br><h3 id="poisk-udalyaemyh-svertok">  Cari konvolusi untuk dihapus </h3><br><p>  Kami perlu menghapus beberapa konvolusi.  Merobek ke depan dan "menembak mati" adalah ide yang buruk, meskipun itu akan berhasil.  Tetapi jika Anda memiliki kepala, Anda dapat berpikir dan mencoba memilih konvolusi “lemah” untuk dihapus.  Ada beberapa opsi: </p><br><ol><li>  <a href="https://openreview.net/pdf%3Fid%3DrJqFGTslg" rel="nofollow">Ukuran L1 terkecil atau low_magnitude_pruning</a> .  Gagasan bahwa konvolusi dengan bobot kecil memberikan kontribusi kecil pada keputusan akhir </li><li>  Pengukuran L1 terkecil dengan memperhitungkan simpangan rata-rata dan standar.  Kami melengkapi penilaian sifat distribusi. </li><li>  <a href="https://arxiv.org/abs/1512.08571" rel="nofollow">Masking convolutions dan menghilangkan yang paling sedikit mempengaruhi akurasi yang dihasilkan</a> .  Definisi yang lebih akurat tentang konvolusi tidak signifikan, tetapi sangat memakan waktu dan sumber daya intensif. </li><li>  Lainnya </li></ol><br><p>  Setiap opsi memiliki hak untuk hidup dan fitur implementasinya sendiri.  Di sini kami mempertimbangkan varian dengan ukuran L1 terkecil </p><br><h3 id="ruchnoy-process-dlya-yolov3">  Proses manual untuk YOLOv3 </h3><br><p>  Arsitektur asli berisi blok residu.  Tetapi tidak peduli seberapa kerennya mereka untuk jaringan yang dalam, mereka akan agak menghalangi kita.  Kesulitannya adalah Anda tidak dapat menghapus rekonsiliasi dengan indeks berbeda di lapisan ini: </p><br><p><img src="https://habrastorage.org/webt/mh/p-/-k/mhp--ksk3ifgurz5jx6exgcrm5c.png"></p><br><p>  Oleh karena itu, kami memilih lapisan dari mana kami dapat dengan bebas menghapus rekonsiliasi: </p><br><p><img src="https://habrastorage.org/webt/qy/ek/zo/qyekzofcur-q0auqurg3egxnato.png"></p><br><p>  Sekarang mari kita membangun siklus kerja: </p><br><ol><li>  Bongkar aktivasi </li><li>  Kami bertanya-tanya berapa banyak yang harus dipotong </li><li>  Hentikan </li><li>  Pelajari 10 era dengan LR = 1e-4 </li><li>  Pengujian </li></ol><br><p>  Membongkar konvolusi berguna untuk mengevaluasi bagian mana yang dapat kita hapus pada langkah tertentu.  Bongkar contoh: </p><br><p><img src="https://habrastorage.org/webt/rp/jo/pk/rpjopk6dzfrl6psoucr8tgj0log.png"></p><br><p>  Kami melihat bahwa hampir di mana-mana 5% konvolusi memiliki norma L1 yang sangat rendah dan kami dapat menghapusnya.  Pada setiap langkah, pembongkaran seperti itu diulangi dan dilakukan penilaian lapisan mana dan berapa banyak yang bisa dipotong. </p><br><p>  Seluruh proses diselesaikan dalam 4 langkah (di sini dan di mana-mana nomor untuk RTX 2060 Super): </p><br><div class="scrollable-table"><table><thead><tr><th>  Langkah </th><th>  mAp75 </th><th>  Jumlah parameter, juta </th><th>  Ukuran jaringan, mb </th><th>  Dari aslinya,% </th><th>  Jalankan waktu, ms </th><th>  Kondisi sunat </th></tr></thead><tbody><tr><td>  0 </td><td>  0,9656 </td><td>  60 </td><td>  241 </td><td>  100 </td><td>  180 </td><td>  - </td></tr><tr><td>  1 </td><td>  0,9622 </td><td>  55 </td><td>  218 </td><td>  91 </td><td>  175 </td><td>  5% dari semuanya </td></tr><tr><td>  2 </td><td>  0,9625 </td><td>  50 </td><td>  197 </td><td>  83 </td><td>  168 </td><td>  5% dari semuanya </td></tr><tr><td>  3 </td><td>  0,9633 </td><td>  39 </td><td>  155 </td><td>  64 </td><td>  155 </td><td>  15% untuk lapisan dengan 400+ konvolusi </td></tr><tr><td><del>  4 </del></td><td><del>  0,9555 </del></td><td><del>  31 </del></td><td><del>  124 </del></td><td><del>  51 </del></td><td><del>  146 </del></td><td><del>  10% untuk lapisan dengan konvolusi 100+ </del></td></tr></tbody></table></div><br><p>  Untuk langkah 2, satu efek positif ditambahkan - patch-size 4 masuk ke memori, yang sangat mempercepat proses pelatihan ulang. <br>  Pada langkah 4, proses dihentikan, karena  bahkan pendidikan lanjutan yang berkepanjangan tidak meningkatkan nilai mAp75 ke nilai lama. <br>  Hasilnya, kami berhasil mempercepat inferensi sebesar <strong>15%</strong> , mengurangi ukuran sebesar <strong>35%</strong> dan tidak kehilangan keakuratannya. </p><br><h3 id="avtomatizaciya-dlya-bolee-prostyh-arhitektur">  Otomatisasi untuk arsitektur yang lebih sederhana </h3><br><p>  Untuk arsitektur jaringan yang lebih sederhana (tanpa blok tambahan bersyarat, konaternat, dan residual), sangat mungkin untuk fokus pada pemrosesan semua lapisan konvolusional dan mengotomatiskan proses pemotongan konvolusi. </p><br><p>  Saya telah menerapkan opsi <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">ini di sini</a> . <br>  Sederhana: Anda hanya memiliki fungsi kehilangan, pengoptimal, dan generator kumpulan: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pruning <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequence train_batch_generator = BatchGenerator... score_batch_generator = BatchGenerator... opt = Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>) pruner = pruning.Pruner(<span class="hljs-string"><span class="hljs-string">"config.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"categorical_crossentropy"</span></span>, opt) pruner.prune(train_batch, valid_batch)</code> </pre> <br><p>  Jika perlu, Anda dapat mengubah parameter konfigurasi: </p><br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"input_model_path"</span></span>: <span class="hljs-string"><span class="hljs-string">"model.h5"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"output_model_path"</span></span>: <span class="hljs-string"><span class="hljs-string">"model_pruned.h5"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"finetuning_epochs"</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>, # the number of epochs for train between pruning steps <span class="hljs-attr"><span class="hljs-attr">"stop_loss"</span></span>: <span class="hljs-number"><span class="hljs-number">0.1</span></span>, # loss for stopping process <span class="hljs-attr"><span class="hljs-attr">"pruning_percent_step"</span></span>: <span class="hljs-number"><span class="hljs-number">0.05</span></span>, # part of convs for delete on every pruning step <span class="hljs-attr"><span class="hljs-attr">"pruning_standart_deviation_part"</span></span>: <span class="hljs-number"><span class="hljs-number">0.2</span></span> # shift for limit pruning part }</code> </pre> <br><p>  Selain itu, pembatasan berdasarkan standar deviasi diterapkan.  Tujuannya adalah membatasi sebagian yang dihapus, tidak termasuk konvolusi dengan ukuran L1 yang sudah "cukup": </p><br><p><img src="https://habrastorage.org/webt/bh/_9/nq/bh_9nqasnp91xifixn7ilhco0mw.png"></p><br><p>  Dengan demikian, kami hanya dapat menghapus konvolusi lemah dari distribusi yang mirip dengan kanan dan tidak memengaruhi penghapusan dari distribusi seperti kiri: </p><br><p><img src="https://habrastorage.org/webt/pr/r5/zp/prr5zpjrdvh1wow6slejnn6axya.png"></p><br><p>  Ketika distribusi mendekati normal, koefisien pruning_standart_deviation_part dapat dipilih dari: </p><br><p><img src="https://habrastorage.org/webt/dl/yl/7d/dlyl7dub216jsr67dcbnhez5fl8.png"><br>  Saya merekomendasikan asumsi 2 sigma.  Atau Anda tidak dapat fokus pada fitur ini, meninggalkan nilai &lt;1.0. </p><br><p>  Outputnya adalah grafik ukuran jaringan, kehilangan dan waktu menjalankan jaringan untuk seluruh tes, dinormalisasi ke 1.0.  Misalnya, di sini ukuran jaringan berkurang hampir 2 kali tanpa kehilangan kualitas (jaringan konvolusi kecil untuk bobot 100rb): </p><br><p><img src="https://habrastorage.org/webt/ig/hu/x_/ighux_gyoaptm71iu2hk_txga_g.png"></p><br><p>  Kecepatan lari tunduk pada fluktuasi normal dan tidak banyak berubah.  Ada penjelasan untuk ini: </p><br><ol><li>  Jumlah konvolusi berubah dari nyaman (32, 64, 128) menjadi tidak nyaman untuk kartu video - 27, 51, dll.  Di sini saya bisa salah, tetapi kemungkinan besar itu mempengaruhi. </li><li>  Arsitekturnya tidak luas, tetapi konsisten.  Mengurangi lebar, kita tidak menyentuh kedalaman.  Jadi, kami mengurangi beban, tetapi tidak mengubah kecepatan. </li></ol><br><p>  Oleh karena itu, peningkatan dinyatakan dalam penurunan beban CUDA selama menjalankan oleh 20-30%, tetapi tidak dalam penurunan dalam waktu berjalan </p><br><h3 id="itogi">  Ringkasan </h3><br><p>  Renungkan.  Kami menganggap 2 opsi pemangkasan - untuk YOLOv3 (ketika Anda harus bekerja dengan tangan Anda) dan untuk jaringan dengan arsitektur lebih mudah.  Dapat dilihat bahwa dalam kedua kasus ini dimungkinkan untuk mencapai pengurangan ukuran jaringan dan akselerasi tanpa kehilangan keakuratan.  Hasil: </p><br><ul><li>  Perampingan </li><li>  Jalankan akselerasi </li><li>  Pengurangan beban CUDA </li><li>  Akibatnya, ramah lingkungan (Kami mengoptimalkan penggunaan sumber daya komputasi di masa depan. Di suatu tempat <a href="https://meduza.io/feature/2019/12/12/kto-takaya-greta-tunberg-i-pochemu-ona-stala-chelovekom-goda-zhurnal-time" rel="nofollow">Greta Tunberg</a> bersukacita sendirian) </li></ul><br><h3 id="appendix">  Lampiran </h3><br><ul><li>  Setelah langkah pemangkasan, Anda juga dapat memutar kuantisasi (misalnya, dengan TensorRT) </li><li>  Tensorflow menyediakan fitur untuk <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="nofollow">low_magnitude_pruning</a> .  Itu bekerja. </li><li>  Saya ingin mengembangkan <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">repositori</a> dan saya akan dengan senang hati membantu </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id482050/">https://habr.com/ru/post/id482050/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id482034/index.html">Yandex: ada segalanya ... tentang pengguna</a></li>
<li><a href="../id482038/index.html">Kami merangkum hasil 2019 di Haber Career</a></li>
<li><a href="../id482040/index.html">Fitur program pembuatan profil dalam C ++</a></li>
<li><a href="../id482042/index.html">Bekerja dengan pustaka Newtonsoft.Json dengan contoh nyata. Bagian 2</a></li>
<li><a href="../id482044/index.html">10 praktik terbaik untuk mengamankan gambar Docker. Bagian 2</a></li>
<li><a href="../id482052/index.html">Dataset Tahun Baru 2019: kamus nada terbuka bahasa Rusia</a></li>
<li><a href="../id482054/index.html">3. Tumpukan elastis: analisis log keamanan. Dasbor</a></li>
<li><a href="../id482058/index.html">Predator atau mangsa? Siapa yang akan melindungi otoritas sertifikasi</a></li>
<li><a href="../id482060/index.html">Model Mandat Akses Kontrol (MAC): Tinjauan Umum dan Aplikasi Aplikasi</a></li>
<li><a href="../id482064/index.html">Kemudahan mengembangkan situs multibahasa di CMS Umbraco 8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>