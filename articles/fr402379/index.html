<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêò ‚õπüèª üë®üèø‚Äçüè´ Ce que les chercheurs en IA pensent des risques possibles associ√©s üôãüèª üÜî ‚èπÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Je me suis int√©ress√© aux risques associ√©s √† l'IA en 2007. A cette √©poque, la r√©action de la plupart des gens √† ce sujet √©tait quelque chose comme ceci...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ce que les chercheurs en IA pensent des risques possibles associ√©s</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/"> Je me suis int√©ress√© aux risques associ√©s √† l'IA en 2007.  A cette √©poque, la r√©action de la plupart des gens √† ce sujet √©tait quelque chose comme ceci: "C'est tr√®s dr√¥le, revenez quand quelqu'un d'autre que des cr√©tins d'Internet y croira." <br><br>  Au cours des ann√©es suivantes, plusieurs personnalit√©s extr√™mement intelligentes et influentes, dont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stephen Hawking</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Elon Musk</a> , ont publiquement partag√© leurs pr√©occupations concernant les risques de l'IA, suivies par des centaines d'autres intellectuels, des philosophes d'Oxford aux cosmologistes du MIT et des investisseurs de la Silicon Valley. .  Et nous revoil√†. <br><br>  Ensuite, la r√©action a chang√©: "Eh bien, quelques scientifiques et hommes d'affaires peuvent y croire, mais il est peu probable qu'ils soient de vrais experts dans ce domaine qui connaissent vraiment la situation." <br><br>  De l√† sont venus des d√©clarations comme l'article de vulgarisation scientifique " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bill Gates a peur de l'IA, mais les chercheurs en IA devraient savoir</a> ": <br><blockquote>  Apr√®s avoir parl√© avec des chercheurs en IA - de vrais chercheurs, qui ne font gu√®re fonctionner de tels syst√®mes, sans parler du bon fonctionnement, il devient clair qu'ils n'ont pas peur que la superintelligence leur arrive soudainement, ni maintenant ni √† l'avenir. .  Malgr√© toutes les histoires effrayantes racont√©es par Mask, les chercheurs ne sont pas press√©s de construire des salles de protection et d'autodestruction avec un compte √† rebours. </blockquote><a name="habracut"></a><br>  Ou, comme ils l'ont √©crit sur Fusion.net dans l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Objection √† propos des robots tueurs d'une personne qui d√©veloppe r√©ellement l'IA</a> ": <br><blockquote>  Andrew Angie d√©veloppe professionnellement des syst√®mes d'IA.  Il a enseign√© un cours d'IA √† Stanford, d√©velopp√© l'IA sur Google, puis est pass√© au moteur de recherche chinois Baidu pour continuer son travail √† l'avant-garde de l'application de l'IA √† des probl√®mes du monde r√©el.  Donc, quand il apprend comment Elon Musk ou Stephen Hawking - des gens qui ne connaissent pas directement la technologie moderne - parlent de l'IA, qui pourrait potentiellement d√©truire l'humanit√©, on peut presque l'entendre se couvrir le visage avec ses mains. </blockquote><br>  Ramez Naam de Marginal Revolution r√©p√®te √† peu pr√®s la m√™me chose dans l'article ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Que pensent les chercheurs des risques de l'IA?</a> ¬ª: <br><blockquote>  Elon Musk, Stephen Hawking et Bill Gates ont r√©cemment exprim√© leurs inqui√©tudes quant au fait que le d√©veloppement de l'IA pourrait mettre en ≈ìuvre le sc√©nario du ¬´tueur d'IA¬ª, et potentiellement conduire √† l'extinction de l'humanit√©.  Ce ne sont pas des chercheurs en IA, et pour autant que je sache, ils n'ont pas travaill√© directement avec l'IA.  Que pensent les vrais chercheurs en IA des risques de l'IA? </blockquote><br>  Il cite les mots de chercheurs en IA sp√©cialement s√©lectionn√©s, comme les auteurs d'autres histoires - puis s'arr√™te, sans mentionner d'opinions diff√©rentes de cela. <br><br>  Mais ils existent.  Les chercheurs en IA, y compris les chefs de file dans le domaine, ont activement exprim√© leurs pr√©occupations concernant les risques de l'IA et au-del√† du renseignement, et depuis le tout d√©but.  Je commencerai par √©num√©rer ces personnes, malgr√© la liste de Naam, puis je passerai √† la raison pour laquelle je ne consid√®re pas cela comme une ¬´discussion¬ª au sens classique attendu de la liste des √©toiles. <br><br>  Les crit√®res de ma liste sont les suivants: je ne mentionne que les chercheurs les plus prestigieux, ou les professeurs de science dans les bons instituts avec de nombreuses citations d'articles scientifiques, ou les scientifiques tr√®s respect√©s de l'industrie qui travaillent pour de grandes entreprises et ont de bons ant√©c√©dents.  Ils sont engag√©s dans l'IA et l'apprentissage automatique.  Ils ont plusieurs d√©clarations fortes √† l'appui d'un certain point de vue concernant l'apparition d'une singularit√© ou d'un risque s√©rieux de l'IA dans un proche avenir.  Certains d'entre eux ont √©crit des ouvrages ou des livres sur ce sujet.  D'autres ont simplement exprim√© leurs pens√©es, estimant qu'il s'agit d'un sujet important qui m√©rite d'√™tre √©tudi√©. <br><br>  Si quelqu'un n'est pas d'accord avec l'inclusion d'une personne dans cette liste ou pense que j'ai oubli√© quelque chose d'important, faites-le moi savoir. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stuart Russell</a> est professeur d'informatique √† Berkeley, laur√©at du prix IJCAI Computers And Thought, chercheur √† la Computer Mechanization Association, chercheur √† l'American Academy of Advanced Scientific Research, directeur du Center for Intelligent Systems, laur√©at du prix Blaise Pascal, etc.  etc.  Co-auteur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AI: A Modern Approach</a> , un manuel classique utilis√© dans 1 200 universit√©s √† travers le monde.  Sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">son site Internet,</a> il √©crit: <br><blockquote>  Dans le domaine de l'IA, 50 ans de recherche sont en cours sous la banni√®re de l'hypoth√®se que le plus intelligent est le mieux.  Le souci du bien de l'humanit√© doit √™tre combin√© √† cela.  L'argument est simple: <br><br>  1. L'IA est susceptible d'√™tre cr√©√©e avec succ√®s. <br>  2. Un succ√®s illimit√© entra√Æne de grands risques et de grands avantages. <br>  3. Que pouvons-nous faire pour augmenter les chances d'obtenir des avantages et d'√©viter les risques? <br><br>  Certaines organisations travaillent d√©j√† sur ces questions, notamment l'Institute for the Future of Humanity d'Oxford, le Center for Existential Risk Studies de Cambridge (CSER), l'Institute for the Study of Machine Intelligence de Berkeley et l'Institute for Future Life de Harvard / MIT (FLI).  Je fais partie de conseils consultatifs avec le CSER et FLI. <br><br>  Tout comme les chercheurs en fusion nucl√©aire ont consid√©r√© le probl√®me de la limitation des r√©actions nucl√©aires comme l'un des probl√®mes les plus importants dans leur domaine, le d√©veloppement du domaine de l'IA posera in√©vitablement des questions de contr√¥le et de s√©curit√©.  Les chercheurs commencent d√©j√† √† poser des questions, de purement techniques (les principaux probl√®mes de rationalit√© et d'utilit√©, etc.) √† des probl√®mes largement philosophiques. </blockquote><br>  Sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">edge.org,</a> il d√©crit un point de vue similaire: <br><blockquote>  Comme l'expliquent Steve Omohandro, Nick Bostrom et d'autres, une divergence de valeurs avec les syst√®mes de prise de d√©cision, dont les possibilit√©s ne cessent de cro√Ætre, peut entra√Æner des probl√®mes - peut-√™tre m√™me des probl√®mes d'√©chelle d'extinction, si les machines sont plus capables que les humains.  Certains croient qu‚Äôil n‚Äôy a aucun risque pr√©visible pour l‚Äôhumanit√© dans les si√®cles √† venir, oubliant peut-√™tre que la diff√©rence de temps entre la d√©claration confiante de Rutherford selon laquelle l‚Äô√©nergie atomique ne peut jamais √™tre extraite et moins de 24 heures se sont √©coul√©es par l‚Äôinvention de la r√©action en cha√Æne nucl√©aire d√©clench√©e par les neutrons . </blockquote><br>  Il a √©galement essay√© de devenir un repr√©sentant de ces id√©es dans la communaut√© universitaire, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">indiquant</a> : <br><blockquote>  Je trouve que les principaux acteurs de cette industrie, qui n'ont jamais exprim√© de craintes auparavant, se disent que ce probl√®me doit √™tre pris tr√®s au s√©rieux, et le plus t√¥t nous le prendrons au s√©rieux, mieux ce sera. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">David McAllister</a> est professeur et agr√©g√© sup√©rieur au Toyota Institute of Technology, affili√© √† l'Universit√© de Chicago, qui a auparavant travaill√© dans les facult√©s du MIT et du Cornell Institute.  Il est membre de l'American AI Association, a publi√© plus d'une centaine d'ouvrages, men√© des recherches dans les domaines de l'apprentissage automatique, de la th√©orie de la programmation, de la prise de d√©cision automatique, de la planification de l'IA, de la linguistique computationnelle, et a eu un impact majeur sur les algorithmes du c√©l√®bre ordinateur d'√©checs Deep Blue.  Selon un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> de la Pittsburgh Tribune Review: <br><blockquote>  Le professeur de Chicago, David McAllister, consid√®re in√©vitable l'√©mergence de la capacit√© des machines intelligentes enti√®rement automatiques √† concevoir et √† cr√©er des versions plus intelligentes d'elles-m√™mes, c'est-√†-dire le d√©but d'un √©v√©nement connu sous le nom de singularit√© [technologique].  La singularit√© permettra aux machines de devenir infiniment intelligentes, conduisant √† un ¬´sc√©nario incroyablement dangereux¬ª, dit-il. </blockquote><br>  Dans son blog, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pens√©es sur les voitures</a> , il √©crit: <br><blockquote>  La plupart des informaticiens refusent de parler de v√©ritables succ√®s en IA.  Je pense qu'il serait plus raisonnable de dire que personne ne peut pr√©dire quand une IA comparable √† l'esprit humain sera re√ßue.  John MacArthy m'a dit une fois que lorsqu'on lui demandait combien de temps l'IA au niveau humain serait cr√©√©e, il r√©pond qu'elle a cinq √† cinq cents ans.  MacArthy √©tait intelligent.  Compte tenu des incertitudes dans ce domaine, il est raisonnable de consid√©rer le probl√®me de l'IA amicale ... <br><br>  Au d√©but, l'IA g√©n√©ralis√©e sera s√ªre.  Cependant, les premiers stades de l'OII seront un excellent site de test pour l'IA en tant que serviteur ou d'autres options d'IA amicales.  Ben Goertzel annonce √©galement une approche exp√©rimentale dans un bon article sur son blog.  Si l'√®re des OII s√ªrs et pas trop intelligents nous attend, alors nous aurons le temps de penser √† des temps plus dangereux. </blockquote><br>  Il √©tait membre du groupe d'experts AAAI Panel On Long-Term AI Futures d√©di√© aux perspectives √† long terme de l'IA, a pr√©sid√© le comit√© de contr√¥le √† long terme et est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©crit comme suit</a> : <br><blockquote>  Makalister m'a parl√© de l'approche de la "singularit√©", un √©v√©nement o√π les ordinateurs deviennent plus intelligents que les gens.  Il n'a pas nomm√© la date exacte de son occurrence, mais a d√©clar√© que cela pourrait se produire dans les deux prochaines d√©cennies, et √† la fin, cela arriverait certainement.  Voici ses vues sur la singularit√©.  Deux √©v√©nements importants se produiront: l'intelligence op√©rationnelle, dans laquelle nous pouvons facilement parler aux ordinateurs, et une r√©action en cha√Æne de l'IA, dans laquelle l'ordinateur peut s'am√©liorer sans aucune aide, puis la r√©p√©ter √† nouveau.  Le premier √©v√©nement que nous remarquerons dans les syst√®mes d'assistance automatique qui nous aidera vraiment.  Plus tard, il deviendra vraiment int√©ressant de communiquer avec les ordinateurs.  Et pour que les ordinateurs puissent faire tout ce que les gens peuvent faire, vous devez attendre que le deuxi√®me √©v√©nement se produise. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Hans Moravek</a> est un ancien professeur √† l'Institut de robotique de l'Universit√© Carnegie Mellon, du nom de lui dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">paradis de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Moravec</a> , fondateur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SeeGrid Corporation</a> , qui se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sp√©cialise</a> dans les syst√®mes de vision industrielle pour les applications industrielles.  Son travail, ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Synthesis of Sensors in the Certainty Grids of Mobile Robots</a> ¬ª, a √©t√© cit√© plus de mille fois, et il a √©t√© invit√© √† √©crire un article pour la British Encyclopedia of Robotics, √† une √©poque o√π les articles des encyclop√©dies √©taient √©crits par des experts mondiaux dans ce domaine, et non par des centaines de commentateurs Internet anonymes. <br><br>  Il est √©galement l'auteur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Robot: From a Simple Machine to a Transcendental Mind</a> , qu'Amazon d√©crit comme suit: <br><blockquote>  Dans ce livre passionnant, Hans Moravek pr√©dit que d'ici 2040, les machines approcheront le niveau intellectuel des gens, et d'ici 2050, elles nous d√©passeront.  Mais alors que Moravec pr√©dit la fin d'une √®re de domination humaine, sa vision de cet √©v√©nement n'est pas si sombre.  Il n'est pas isol√© d'un avenir dans lequel les machines gouvernent le monde, mais l'accepte et d√©crit un point de vue √©tonnant selon lequel les robots intelligents deviendront nos descendants √©volutionnaires.  Moravec pense qu'√† la fin de ce processus, "le vaste cyberespace s'unira au supramental inhumain et traitera les affaires aussi loin des gens que les affaires humaines des bact√©ries". </blockquote><br>  Shane Leg est co-fondateur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMind Technologies</a> , une startup de l'IA achet√©e en 2014 pour 500 millions de dollars par Google.  Il a re√ßu son doctorat de l'Institut de l'IA nomm√© d'apr√®s  Dale Moul en Suisse, et a √©galement travaill√© dans la division de neurobiologie computationnelle.  Gatsby √† Londres.  Au terme de sa th√®se, ¬´superintelligence machine¬ª, il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©crit</a> : <br><blockquote>  Si quelque chose appara√Æt qui peut s'approcher de la puissance absolue, ce sera une machine super intelligente.  Par d√©finition, elle pourra atteindre un grand nombre d'objectifs dans une grande vari√©t√© d'environnements.  Si nous nous pr√©parons soigneusement √† une telle opportunit√© √† l'avance, nous pouvons non seulement √©viter la catastrophe, mais aussi commencer une √®re de prosp√©rit√©, contrairement √† tout ce qui existait auparavant. </blockquote><br>  Dans une interview ult√©rieure, il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dit</a> : <br><blockquote>  L'IA est maintenant l√† o√π Internet √©tait en 1988.  Les besoins d'apprentissage automatique sont requis dans des applications sp√©ciales (moteurs de recherche tels que Google, hedge funds et bioinformatique), et leur nombre augmente chaque ann√©e.  Je pense que vers le milieu de la prochaine d√©cennie, ce processus deviendra massif et perceptible.  Le boom de l'IA devrait avoir lieu vers 2020, suivi d'une d√©cennie de progr√®s rapides, peut-√™tre apr√®s les corrections du march√©.  L'IA au niveau humain sera cr√©√©e vers la mi-2020, bien que beaucoup de gens n'accepteront pas le d√©but de cet √©v√©nement.  Apr√®s cela, les risques associ√©s √† l'IA avanc√©e seront mis en pratique.  Je ne parlerai pas de la ¬´singularit√©¬ª, mais ils s'attendent √† ce qu'√† un moment donn√© apr√®s la cr√©ation de l'OII, des choses folles commencent √† se produire.  C'est entre 2025 et 2040. </blockquote><br>  Lui et ses co-fondateurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Demis Khasabis</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mustafa Suleiman ont</a> sign√© une p√©tition pour l'Institut pour la vie future concernant les risques li√©s √† l'IA, et l'une de leurs conditions pour rejoindre Google √©tait que l'entreprise accepte d'organiser un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conseil d'√©thique de l'IA</a> pour √©tudier ces questions. <br><br>  Steve Omohundro est un ancien professeur d'informatique √† l'Universit√© de l'Illinois, le fondateur du groupe de vision par ordinateur et de formation au Center for the Study of Complex Systems, et l'inventeur de divers d√©veloppements importants dans l'apprentissage automatique et la vision par ordinateur.  Il a travaill√© sur des robots lisant les l√®vres, le langage de programmation parall√®le StarLisp, des algorithmes d'apprentissage g√©om√©trique.  Il dirige actuellement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Self-Aware Systems</a> , "une √©quipe de scientifiques qui veille √† ce que les technologies intelligentes profitent √† l'humanit√©".  Son travail, ¬´Les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bases de la motivation de l'IA</a> ¬ª, a aid√© √† cr√©er le domaine de l'√©thique des machines, car il a not√© que les syst√®mes super-intelligents seront dirig√©s vers des objectifs potentiellement dangereux.  Il √©crit: <br><blockquote>  Nous avons montr√© que tous les syst√®mes d'IA avanc√©s sont susceptibles d'avoir un ensemble de motivations fondamentales.  Il est imp√©ratif de comprendre ces motivations afin de cr√©er des technologies qui garantissent un avenir positif √† l'humanit√©.  Yudkovsky a appel√© √† une ¬´IA amicale¬ª.  Pour ce faire, nous devons d√©velopper une approche scientifique du ¬´d√©veloppement utilitaire¬ª, qui nous permettra de d√©velopper des fonctions socialement utiles qui conduiront aux s√©quences que nous d√©sirons.  Les progr√®s rapides du progr√®s technologique sugg√®rent que ces probl√®mes pourraient bient√¥t devenir critiques. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Vous</a> pouvez trouver ses articles sur le sujet "AI rationnelle pour le bien commun" sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Murray Shanahan</a> est titulaire d'un doctorat en informatique de Cambridge et est maintenant professeur de robotique cognitive √† l'Imperial College de Londres.  Il a publi√© des travaux dans des domaines tels que la robotique, la logique, les syst√®mes dynamiques, la neurobiologie computationnelle et la philosophie de l'esprit.  Il travaille actuellement sur le livre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Technological Singularity</a> , qui sera publi√© en ao√ªt.  L'annotation promotionnelle d'Amazon est la suivante: <br><blockquote>  Shanahan d√©crit les progr√®s technologiques de l'IA, √† la fois r√©alis√©s sous l'influence des connaissances de la biologie et d√©velopp√©s √† partir de z√©ro.  Il explique que lorsque l'IA du niveau humain sera cr√©√©e - une t√¢che th√©oriquement possible mais difficile - la transition vers une IA superintelligente sera tr√®s rapide.  Le Shanahan r√©fl√©chit √† ce que l'existence de machines superintelligentes peut mener √† des domaines tels que la personnalit√©, la responsabilit√©, les droits et l'individualit√©.  Certains repr√©sentants de l'intelligence artificielle superintelligente peuvent √™tre cr√©√©s au profit de l'homme, certains peuvent devenir incontr√¥lables.  (C'est-√†-dire, Siri ou HAL?) La singularit√© repr√©sente pour l'humanit√© √† la fois une menace existentielle et une opportunit√© existentielle de surmonter ses limites.  Shanahan indique clairement que si nous voulons obtenir un meilleur r√©sultat, nous devons imaginer les deux possibilit√©s. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Marcus Hutter</a> est professeur de recherche en informatique √† la National Australia University.  Avant cela, il a travaill√© √† l'Institut de l'IA nomm√© d'apr√®s  Dale Mole en Suisse et √† l'Institut national de l'informatique et des communications en Australie, et a √©galement travaill√© sur l'apprentissage stimul√©, les d√©couvertes bay√©siennes, la th√©orie de la complexit√© computationnelle, la th√©orie de Salomon sur les pr√©dictions inductives, la vision par ordinateur et les profils g√©n√©tiques.  Il a √©galement beaucoup √©crit sur la singularit√©.  Dans l'article ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'intelligence peut-elle exploser?</a> ¬ª, Il √©crit: <br><blockquote>  Ce si√®cle peut √™tre t√©moin d'une explosion technologique dont l'ampleur m√©rite le nom de singularit√©.  Le sc√©nario par d√©faut est une communaut√© d'individus intelligents en interaction dans le monde virtuel, simul√©e sur des ordinateurs avec des ressources informatiques en augmentation hyperbolique.  Cela s'accompagne in√©vitablement d'une explosion de vitesse, mesur√©e par le temps physique, mais pas n√©cessairement d'une explosion d'intelligence.  Si le monde virtuel est peupl√© d'individus libres et en interaction, la pression √©volutive conduira √† l'√©mergence d'individus de plus en plus intelligents qui rivaliseront pour les ressources informatiques.  Le point final de cette acc√©l√©ration √©volutive de l'intelligence peut √™tre la communaut√© des individus les plus intelligents.  Certains aspects de cette communaut√© singuli√®re peuvent th√©oriquement √™tre √©tudi√©s √† l'aide d'outils scientifiques modernes.  Bien avant l'apparition de cette singularit√©, en pla√ßant m√™me cette communaut√© virtuelle dans notre imagination, on peut imaginer l'√©mergence de diff√©rences, comme par exemple une forte baisse de la valeur d'un individu qui peut entra√Æner des cons√©quences radicales. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">J√ºrgen Schmidhuber</a> est professeur d'IA √† l'Universit√© de Lugano et ancien professeur de robotique cognitive √† l'Universit√© de technologie de Munich.  Il d√©veloppe certains des r√©seaux de neurones les plus avanc√©s au monde, travaille sur la robotique √©volutionnaire et la th√©orie de la complexit√© informatique, et est chercheur √† l'Acad√©mie europ√©enne des sciences et des arts.  Dans son livre, ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Hypoth√®ses des singularit√©s</a> ¬ª, il soutient qu '¬´avec la poursuite des tendances existantes, nous ferons face √† une explosion intellectuelle au cours des prochaines d√©cennies¬ª.  Interrog√© directement √† l'AMA de Reddit sur les risques associ√©s √† l'IA, il a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©pondu</a> : <br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trouver un cr√©neau pour la survie</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . "</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est professeur et membre du comit√© iCORE de l'Universit√© de l'Alberta. </font><font style="vertical-align: inherit;">Il est chercheur √† l'Association pour le d√©veloppement de l'IA, co-auteur du </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manuel le plus populaire sur l'apprentissage stimul√©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pionnier de la m√©thode des diff√©rences de temps, l'un des plus importants dans ce domaine. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans son </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rapport</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lors d'une conf√©rence sur l'IA organis√©e par l'Institute for the Future of Life, Saton a fait valoir qu '"il y a une r√©elle chance que m√™me avec nos vies" une IA soit cr√©√©e qui soit intellectuellement comparable aux humains, et a ajout√© que cette IA "ne nous ob√©ira pas", " pour concurrencer et coop√©rer avec nous ¬ª, et que¬´ si nous cr√©ons des esclaves super intelligents, nous obtenons des adversaires super intelligents. ¬ª En conclusion, il a d√©clar√© que "nous devons r√©fl√©chir √† des m√©canismes (sociaux, juridiques, politiques, culturels) pour garantir le r√©sultat souhait√©", mais que "les gens ordinaires deviendront in√©vitablement moins importants". Il a √©galement mentionn√© des probl√®mes similaires lors de la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pr√©sentation du</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gadsby Institute. Aussi dans le livre de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">il existe de telles lignes: "Richard Saton, l'un des plus grands sp√©cialistes de l'IA, pr√©dit une explosion de l'intelligence quelque part au milieu du si√®cle." </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est professeur de vision industrielle √† l'Imperial College de Londres, leader dans les groupes de vision robotique et le Dyson Robotics Laboratory, et inventeur du syst√®me de localisation et de balisage informatis√© MonoSLAM. Sur son site Internet, il </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©crit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         ¬´ ¬ª (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    ¬´ ¬ª    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> n'ont pas besoin d'√™tre pr√©sent√©s. Turing a invent√© les fondements math√©matiques de la science informatique et a donn√© son nom √† une machine de Turing, √† l'exhaustivit√© de Turing et au test de Turing. Goode a travaill√© avec Turing √† Bletchley Park, a aid√© √† cr√©er l'un des premiers ordinateurs et a invent√© de nombreux algorithmes bien connus, par exemple, l'algorithme de transform√©e de Fourier discr√®te rapide, connu sous le nom d'algorithme FFT. Dans leur travail, les voitures num√©riques peuvent-elles penser? Turing √©crit:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supposons que de telles machines puissent √™tre cr√©√©es et consid√©rons les cons√©quences de leur cr√©ation. </font><font style="vertical-align: inherit;">Un tel acte rencontrera sans aucun doute l'hostilit√©, √† moins que nous n'ayons progress√© dans la tol√©rance religieuse depuis l'√©poque de Galil√©e. </font><font style="vertical-align: inherit;">L'opposition sera constitu√©e d'intellectuels craignant de perdre leur emploi. </font><font style="vertical-align: inherit;">Mais il est probable que les intellectuels se tromperont. </font><font style="vertical-align: inherit;">Il sera possible de faire beaucoup de choses pour tenter de maintenir leur intellect au niveau des normes fix√©es par les machines, car apr√®s le d√©but de la m√©thode de la machine, cela ne prend pas beaucoup de temps jusqu'au moment o√π les machines d√©passent nos capacit√©s insignifiantes. </font><font style="vertical-align: inherit;">√Ä un certain stade, nous devons nous attendre √† ce que les machines prennent le contr√¥le.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tout en travaillant √† Atlas Computer Lab dans les ann√©es 60, Goode a d√©velopp√© cette id√©e dans ¬´ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Raisonnement pour la premi√®re machine ultra-intelligente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ¬ª:</font></font><br><blockquote>   ,  ,       .     ‚Äì     ,       .  ,   ,   ¬´ ¬ª,      . ,    ‚Äì   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cela me d√©range que cette liste puisse donner l'impression d'une certaine querelle entre ¬´croyants¬ª et ¬´sceptiques¬ª dans ce domaine, au cours de laquelle ils se fracassent mutuellement. Mais je ne le pensais pas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lorsque je lis des articles sur les sceptiques, je tombe toujours sur deux arguments. Premi√®rement, nous sommes encore tr√®s loin de l'IA du niveau humain, sans parler de la superintelligence, et il n'y a aucun moyen √©vident d'atteindre de tels sommets. Deuxi√®mement, si vous demandez des interdictions de recherche sur l'IA, vous √™tes un idiot. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je suis enti√®rement d'accord avec les deux points. Comme les dirigeants du mouvement du risque de l'IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enqu√™te aupr√®s des chercheurs en IA ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) ont montr√© qu'en moyenne, ils donnent 50% pour le fait que l'IA de niveau humain appara√Ætra d'ici 2040 ode, et 90% - qu'elle appara√Ætra d'ici 2075. En moyenne, 75% d'entre eux pensent que la superintelligence (¬´l'intelligence artificielle, d√©passant s√©rieusement les capacit√©s de chaque personne dans la plupart des professions ¬ª) appara√Ætra dans les 30 ans suivant l'av√®nement de l'IA au niveau humain. Et bien que la technique de ce sondage soul√®ve certains doutes, si vous acceptez ses r√©sultats, il s'av√®re que la plupart des chercheurs en IA conviennent que quelque chose qui m√©rite d'√™tre inqui√©t√© appara√Ætra dans une ou deux g√©n√©rations. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais le directeur de l'Institut de l'intelligence artificielle, Luke Muelhauser, et le directeur de l'Institut pour l'avenir de l'humanit√©, Nick Bostrom, ont d√©clar√© que leurs pr√©dictions pour le d√©veloppement de l'IA sont bien plus tardives que les pr√©visions des scientifiques participant √† l'enqu√™te. Si vous √©tudiez</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">les donn√©es sur les pr√©dictions de l'IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de Stuart Armstrong, on peut voir que, en g√©n√©ral, les estimations sur le moment de l'apparition de l'IA faites par les partisans de l'IA ne diff√®rent pas des estimations faites par les sceptiques de l'IA. De plus, la pr√©diction √† long terme dans ce tableau appartient √† Armstrong lui-m√™me. Cependant, Armstrong travaille actuellement √† l'Institut pour l'avenir de l'humanit√©, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">attirant l'attention sur les risques de l'IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et la n√©cessit√© de rechercher les objectifs de la superintelligence. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La diff√©rence entre les partisans et les sceptiques n'est pas dans leurs √©valuations du moment o√π nous devrions nous attendre √† l'apparition de l'IA au niveau humain, mais dans le moment o√π nous devons commencer √† nous pr√©parer √† cela.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ce qui nous am√®ne au deuxi√®me point. La position des sceptiques, semble-t-il, est que m√™me si nous devrions probablement envoyer quelques personnes intelligentes pour travailler sur une √©valuation pr√©liminaire du probl√®me, il n'est absolument pas n√©cessaire de paniquer ou d'interdire l'√©tude de l'IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les fans de l'IA insistent sur le fait que m√™me si nous n'avons pas besoin de paniquer ou d'interdire la recherche sur l'IA, cela vaut probablement la peine d'envoyer quelques personnes intelligentes pour travailler sur une √©valuation pr√©liminaire du probl√®me. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun est sans doute le sceptique le plus ardent des risques li√©s √† l'IA. Il a √©t√© abondamment cit√© dans un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article sur la science populaire</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dans un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article sur la r√©volution marginale</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , et il a √©galement parl√© avec </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sur les "questions in√©vitables de la singularit√©", qu'il d√©crit lui-m√™me comme "√©tant si loin que la science-fiction peut √™tre √©crite √† leur sujet". </font><font style="vertical-align: inherit;">Mais lorsqu'on lui a demand√© de clarifier sa position, il a d√©clar√©:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk est tr√®s inquiet des menaces existentielles contre l'humanit√© (c'est pourquoi il construit des roquettes pour envoyer des gens coloniser d'autres plan√®tes). </font><font style="vertical-align: inherit;">Et bien que le risque d'une r√©bellion de l'IA soit tr√®s faible et tr√®s √©loign√© de l'avenir, nous devons y r√©fl√©chir, d√©velopper des mesures et des r√®gles de pr√©caution. </font><font style="vertical-align: inherit;">Tout comme le comit√© de bio√©thique est apparu dans les ann√©es 1970 et 1980, avant l'utilisation g√©n√©ralis√©e de la g√©n√©tique, nous avons besoin de comit√©s d'√©thique de l'IA. </font><font style="vertical-align: inherit;">Mais, comme l'a √©crit Yoshua Benjio, nous avons encore beaucoup de temps.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz est un autre expert souvent d√©sign√© comme le principal porte-parole du scepticisme et des limites. </font><font style="vertical-align: inherit;">Son point de vue a √©t√© d√©crit dans des articles tels que ¬´Le </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">directeur de Microsoft Research pense que l'IA hors de contr√¥le ne nous tuera pas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ¬ª et ¬´ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz de Microsoft pense que l'IA ne devrait pas avoir peur</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ¬ª. </font><font style="vertical-align: inherit;">Mais voici ce qu'il a dit dans une interview plus longue avec NPR:</font></font><br><blockquote> Keist: Horvitz doute que les secr√©taires virtuels ne se transformeront jamais en quelque chose qui conqu√©rera le monde.  Il dit qu'il faut s'attendre √† ce qu'un kite √©volue en Boeing 747. Est-ce √† dire qu'il se moque d'une singularit√©? <br><br>  Horvitz: Non.  Je pense qu'il y avait un m√©lange de concepts, et moi aussi j'ai des sentiments mitig√©s. <br><br>  Keist: En particulier, en raison d'id√©es telles que la singularit√©, Horvits et d'autres experts en IA tentent de plus en plus de traiter les probl√®mes √©thiques qui peuvent survenir avec une IA √©troitement cibl√©e dans les ann√©es √† venir.  Ils posent √©galement des questions plus futuristes.  Par exemple, comment puis-je cr√©er un bouton d'arr√™t d'urgence pour un ordinateur qui peut se changer lui-m√™me? <br><br>  Horvits: Je crois vraiment que les enjeux sont suffisamment √©lev√©s pour consacrer du temps et de l'√©nergie √† rechercher activement des solutions, m√™me si la probabilit√© de tels √©v√©nements est faible. </blockquote><br>  Cela co√Øncide g√©n√©ralement avec la position de bon nombre des agitateurs √† risque d'IA les plus ardents.  Avec de tels amis, les ennemis ne sont pas n√©cessaires. <br><br>  L'article de Slate, ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">N'ayez pas peur de l'IA</a> ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">,</a> met √©galement les choses sous un jour surprenant: <br><blockquote>  Comme le dit Musk lui-m√™me, la solution au risque d'IA r√©side dans la collaboration sobre et rationnelle des scientifiques et des l√©gislateurs.  Cependant, il est assez difficile de comprendre comment parler de ¬´d√©mons¬ª peut aider √† atteindre ce noble objectif.  Elle peut m√™me la g√™ner. <br><br>  Premi√®rement, il y a d'√©normes trous dans l'id√©e du script Skynet.  Bien que les chercheurs dans le domaine de l'informatique croient que le raisonnement de Mask n'est ¬´pas compl√®tement fou¬ª, ils sont encore trop loin d'un monde dans lequel le battage m√©diatique autour de l'IA d√©guise une r√©alit√© l√©g√®rement moins IA que nos informaticiens sont confront√©s. <br><br>  Ian Lekun, chef du laboratoire d'IA de Facebook, a r√©sum√© cette id√©e dans un article sur Google+ en 2013: le battage m√©diatique nuit √† l'IA.  Au cours des cinq derni√®res d√©cennies, le battage m√©diatique a tu√© AI quatre fois.  Elle doit √™tre arr√™t√©e. "Lekun et d'autres ont √† juste titre peur du battage m√©diatique. Le fait de ne pas r√©pondre aux attentes √©lev√©es impos√©es par la science-fiction entra√Æne de s√©rieuses coupes budg√©taires pour la recherche en IA. </blockquote><br>  Les scientifiques travaillant sur l'IA sont des gens intelligents.  Ils ne sont pas int√©ress√©s √† tomber dans des pi√®ges politiques classiques, dans lesquels ils seraient divis√©s en camps et s‚Äôaccuseraient de panique ou d‚Äôostrichisme.  Apparemment, ils essaient de trouver un √©quilibre entre la n√©cessit√© de commencer les travaux pr√©liminaires li√©s √† la menace qui se profile quelque part au loin, et le risque de provoquer une sensation si forte qui les frappera. <br><br>  Je ne veux pas dire qu'il n'y a pas de divergence d'opinion sur la rapidit√© avec laquelle vous devez commencer √† r√©soudre ce probl√®me.  Fondamentalement, tout se r√©sume √† savoir s'il est possible de dire que ¬´nous r√©soudrons le probl√®me lorsque nous le rencontrerons¬ª ou d'attendre un d√©collage aussi inattendu, en raison duquel tout deviendra incontr√¥lable et pour lequel, par cons√©quent, nous devons nous pr√©parer √† l'avance.  Je vois moins de preuves que je ne le souhaiterais que la majorit√© des chercheurs en IA avec leurs propres opinions comprennent la deuxi√®me possibilit√©.  Que puis-je dire, m√™me si dans un article sur Marginal Revolution un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">expert est cit√©</a> disant que la superintelligence ne repr√©sente pas une grande menace, car "les ordinateurs intelligents ne peuvent pas se fixer des objectifs", bien que quiconque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lit Bostrom</a> sache que tout le probl√®me est. <br><br>  Il y a encore une montagne de travail √† faire.  Mais juste pour ne pas s√©lectionner sp√©cifiquement des articles dans lesquels "les vrais experts en IA ne se soucient pas de la superintelligence". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr402379/">https://habr.com/ru/post/fr402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr402367/index.html">Comment arr√™ter de payer pour l'itin√©rance, ou avec un seul num√©ro partout dans le monde</a></li>
<li><a href="../fr402369/index.html">Comment mesurer la vitesse d'une imprimante 3D - son extr√©mit√© chaude. Et pas seulement la vitesse</a></li>
<li><a href="../fr402373/index.html">Ce qui donne la "G√©n√©tique du Microbiote"</a></li>
<li><a href="../fr402375/index.html">Commutateur ca √† 4 canaux de 8 kilowatts avec mesure de la consommation. Partie 1</a></li>
<li><a href="../fr402377/index.html">Que pensent vos smartphones de la charge USB de la voiture</a></li>
<li><a href="../fr402381/index.html">Comment recruter des astronautes</a></li>
<li><a href="../fr402383/index.html">Saw, Shura: comment nous avons con√ßu l'application mobile de suivi des chiens Mishiko</a></li>
<li><a href="../fr402385/index.html">Pourquoi vous devriez vous attendre √† un boom dans le domaine de la cr√©ation de robots pour les locaux commerciaux</a></li>
<li><a href="../fr402387/index.html">Stylo 3D pour imprimantes 3D</a></li>
<li><a href="../fr402389/index.html">La MPAA et la RIAA pr√©voient de r√©cup√©rer les donn√©es des disques durs d√©faillants du partage de fichiers Megaupload</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>