<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐘 ⛹🏻 👨🏿‍🏫 Ce que les chercheurs en IA pensent des risques possibles associés 🙋🏻 🆔 ⏹️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Je me suis intéressé aux risques associés à l'IA en 2007. A cette époque, la réaction de la plupart des gens à ce sujet était quelque chose comme ceci...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ce que les chercheurs en IA pensent des risques possibles associés</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/"> Je me suis intéressé aux risques associés à l'IA en 2007.  A cette époque, la réaction de la plupart des gens à ce sujet était quelque chose comme ceci: "C'est très drôle, revenez quand quelqu'un d'autre que des crétins d'Internet y croira." <br><br>  Au cours des années suivantes, plusieurs personnalités extrêmement intelligentes et influentes, dont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stephen Hawking</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Elon Musk</a> , ont publiquement partagé leurs préoccupations concernant les risques de l'IA, suivies par des centaines d'autres intellectuels, des philosophes d'Oxford aux cosmologistes du MIT et des investisseurs de la Silicon Valley. .  Et nous revoilà. <br><br>  Ensuite, la réaction a changé: "Eh bien, quelques scientifiques et hommes d'affaires peuvent y croire, mais il est peu probable qu'ils soient de vrais experts dans ce domaine qui connaissent vraiment la situation." <br><br>  De là sont venus des déclarations comme l'article de vulgarisation scientifique " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bill Gates a peur de l'IA, mais les chercheurs en IA devraient savoir</a> ": <br><blockquote>  Après avoir parlé avec des chercheurs en IA - de vrais chercheurs, qui ne font guère fonctionner de tels systèmes, sans parler du bon fonctionnement, il devient clair qu'ils n'ont pas peur que la superintelligence leur arrive soudainement, ni maintenant ni à l'avenir. .  Malgré toutes les histoires effrayantes racontées par Mask, les chercheurs ne sont pas pressés de construire des salles de protection et d'autodestruction avec un compte à rebours. </blockquote><a name="habracut"></a><br>  Ou, comme ils l'ont écrit sur Fusion.net dans l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Objection à propos des robots tueurs d'une personne qui développe réellement l'IA</a> ": <br><blockquote>  Andrew Angie développe professionnellement des systèmes d'IA.  Il a enseigné un cours d'IA à Stanford, développé l'IA sur Google, puis est passé au moteur de recherche chinois Baidu pour continuer son travail à l'avant-garde de l'application de l'IA à des problèmes du monde réel.  Donc, quand il apprend comment Elon Musk ou Stephen Hawking - des gens qui ne connaissent pas directement la technologie moderne - parlent de l'IA, qui pourrait potentiellement détruire l'humanité, on peut presque l'entendre se couvrir le visage avec ses mains. </blockquote><br>  Ramez Naam de Marginal Revolution répète à peu près la même chose dans l'article « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Que pensent les chercheurs des risques de l'IA?</a> »: <br><blockquote>  Elon Musk, Stephen Hawking et Bill Gates ont récemment exprimé leurs inquiétudes quant au fait que le développement de l'IA pourrait mettre en œuvre le scénario du «tueur d'IA», et potentiellement conduire à l'extinction de l'humanité.  Ce ne sont pas des chercheurs en IA, et pour autant que je sache, ils n'ont pas travaillé directement avec l'IA.  Que pensent les vrais chercheurs en IA des risques de l'IA? </blockquote><br>  Il cite les mots de chercheurs en IA spécialement sélectionnés, comme les auteurs d'autres histoires - puis s'arrête, sans mentionner d'opinions différentes de cela. <br><br>  Mais ils existent.  Les chercheurs en IA, y compris les chefs de file dans le domaine, ont activement exprimé leurs préoccupations concernant les risques de l'IA et au-delà du renseignement, et depuis le tout début.  Je commencerai par énumérer ces personnes, malgré la liste de Naam, puis je passerai à la raison pour laquelle je ne considère pas cela comme une «discussion» au sens classique attendu de la liste des étoiles. <br><br>  Les critères de ma liste sont les suivants: je ne mentionne que les chercheurs les plus prestigieux, ou les professeurs de science dans les bons instituts avec de nombreuses citations d'articles scientifiques, ou les scientifiques très respectés de l'industrie qui travaillent pour de grandes entreprises et ont de bons antécédents.  Ils sont engagés dans l'IA et l'apprentissage automatique.  Ils ont plusieurs déclarations fortes à l'appui d'un certain point de vue concernant l'apparition d'une singularité ou d'un risque sérieux de l'IA dans un proche avenir.  Certains d'entre eux ont écrit des ouvrages ou des livres sur ce sujet.  D'autres ont simplement exprimé leurs pensées, estimant qu'il s'agit d'un sujet important qui mérite d'être étudié. <br><br>  Si quelqu'un n'est pas d'accord avec l'inclusion d'une personne dans cette liste ou pense que j'ai oublié quelque chose d'important, faites-le moi savoir. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stuart Russell</a> est professeur d'informatique à Berkeley, lauréat du prix IJCAI Computers And Thought, chercheur à la Computer Mechanization Association, chercheur à l'American Academy of Advanced Scientific Research, directeur du Center for Intelligent Systems, lauréat du prix Blaise Pascal, etc.  etc.  Co-auteur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AI: A Modern Approach</a> , un manuel classique utilisé dans 1 200 universités à travers le monde.  Sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">son site Internet,</a> il écrit: <br><blockquote>  Dans le domaine de l'IA, 50 ans de recherche sont en cours sous la bannière de l'hypothèse que le plus intelligent est le mieux.  Le souci du bien de l'humanité doit être combiné à cela.  L'argument est simple: <br><br>  1. L'IA est susceptible d'être créée avec succès. <br>  2. Un succès illimité entraîne de grands risques et de grands avantages. <br>  3. Que pouvons-nous faire pour augmenter les chances d'obtenir des avantages et d'éviter les risques? <br><br>  Certaines organisations travaillent déjà sur ces questions, notamment l'Institute for the Future of Humanity d'Oxford, le Center for Existential Risk Studies de Cambridge (CSER), l'Institute for the Study of Machine Intelligence de Berkeley et l'Institute for Future Life de Harvard / MIT (FLI).  Je fais partie de conseils consultatifs avec le CSER et FLI. <br><br>  Tout comme les chercheurs en fusion nucléaire ont considéré le problème de la limitation des réactions nucléaires comme l'un des problèmes les plus importants dans leur domaine, le développement du domaine de l'IA posera inévitablement des questions de contrôle et de sécurité.  Les chercheurs commencent déjà à poser des questions, de purement techniques (les principaux problèmes de rationalité et d'utilité, etc.) à des problèmes largement philosophiques. </blockquote><br>  Sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">edge.org,</a> il décrit un point de vue similaire: <br><blockquote>  Comme l'expliquent Steve Omohandro, Nick Bostrom et d'autres, une divergence de valeurs avec les systèmes de prise de décision, dont les possibilités ne cessent de croître, peut entraîner des problèmes - peut-être même des problèmes d'échelle d'extinction, si les machines sont plus capables que les humains.  Certains croient qu’il n’y a aucun risque prévisible pour l’humanité dans les siècles à venir, oubliant peut-être que la différence de temps entre la déclaration confiante de Rutherford selon laquelle l’énergie atomique ne peut jamais être extraite et moins de 24 heures se sont écoulées par l’invention de la réaction en chaîne nucléaire déclenchée par les neutrons . </blockquote><br>  Il a également essayé de devenir un représentant de ces idées dans la communauté universitaire, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">indiquant</a> : <br><blockquote>  Je trouve que les principaux acteurs de cette industrie, qui n'ont jamais exprimé de craintes auparavant, se disent que ce problème doit être pris très au sérieux, et le plus tôt nous le prendrons au sérieux, mieux ce sera. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">David McAllister</a> est professeur et agrégé supérieur au Toyota Institute of Technology, affilié à l'Université de Chicago, qui a auparavant travaillé dans les facultés du MIT et du Cornell Institute.  Il est membre de l'American AI Association, a publié plus d'une centaine d'ouvrages, mené des recherches dans les domaines de l'apprentissage automatique, de la théorie de la programmation, de la prise de décision automatique, de la planification de l'IA, de la linguistique computationnelle, et a eu un impact majeur sur les algorithmes du célèbre ordinateur d'échecs Deep Blue.  Selon un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> de la Pittsburgh Tribune Review: <br><blockquote>  Le professeur de Chicago, David McAllister, considère inévitable l'émergence de la capacité des machines intelligentes entièrement automatiques à concevoir et à créer des versions plus intelligentes d'elles-mêmes, c'est-à-dire le début d'un événement connu sous le nom de singularité [technologique].  La singularité permettra aux machines de devenir infiniment intelligentes, conduisant à un «scénario incroyablement dangereux», dit-il. </blockquote><br>  Dans son blog, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pensées sur les voitures</a> , il écrit: <br><blockquote>  La plupart des informaticiens refusent de parler de véritables succès en IA.  Je pense qu'il serait plus raisonnable de dire que personne ne peut prédire quand une IA comparable à l'esprit humain sera reçue.  John MacArthy m'a dit une fois que lorsqu'on lui demandait combien de temps l'IA au niveau humain serait créée, il répond qu'elle a cinq à cinq cents ans.  MacArthy était intelligent.  Compte tenu des incertitudes dans ce domaine, il est raisonnable de considérer le problème de l'IA amicale ... <br><br>  Au début, l'IA généralisée sera sûre.  Cependant, les premiers stades de l'OII seront un excellent site de test pour l'IA en tant que serviteur ou d'autres options d'IA amicales.  Ben Goertzel annonce également une approche expérimentale dans un bon article sur son blog.  Si l'ère des OII sûrs et pas trop intelligents nous attend, alors nous aurons le temps de penser à des temps plus dangereux. </blockquote><br>  Il était membre du groupe d'experts AAAI Panel On Long-Term AI Futures dédié aux perspectives à long terme de l'IA, a présidé le comité de contrôle à long terme et est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">décrit comme suit</a> : <br><blockquote>  Makalister m'a parlé de l'approche de la "singularité", un événement où les ordinateurs deviennent plus intelligents que les gens.  Il n'a pas nommé la date exacte de son occurrence, mais a déclaré que cela pourrait se produire dans les deux prochaines décennies, et à la fin, cela arriverait certainement.  Voici ses vues sur la singularité.  Deux événements importants se produiront: l'intelligence opérationnelle, dans laquelle nous pouvons facilement parler aux ordinateurs, et une réaction en chaîne de l'IA, dans laquelle l'ordinateur peut s'améliorer sans aucune aide, puis la répéter à nouveau.  Le premier événement que nous remarquerons dans les systèmes d'assistance automatique qui nous aidera vraiment.  Plus tard, il deviendra vraiment intéressant de communiquer avec les ordinateurs.  Et pour que les ordinateurs puissent faire tout ce que les gens peuvent faire, vous devez attendre que le deuxième événement se produise. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Hans Moravek</a> est un ancien professeur à l'Institut de robotique de l'Université Carnegie Mellon, du nom de lui dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">paradis de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Moravec</a> , fondateur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SeeGrid Corporation</a> , qui se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">spécialise</a> dans les systèmes de vision industrielle pour les applications industrielles.  Son travail, « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Synthesis of Sensors in the Certainty Grids of Mobile Robots</a> », a été cité plus de mille fois, et il a été invité à écrire un article pour la British Encyclopedia of Robotics, à une époque où les articles des encyclopédies étaient écrits par des experts mondiaux dans ce domaine, et non par des centaines de commentateurs Internet anonymes. <br><br>  Il est également l'auteur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Robot: From a Simple Machine to a Transcendental Mind</a> , qu'Amazon décrit comme suit: <br><blockquote>  Dans ce livre passionnant, Hans Moravek prédit que d'ici 2040, les machines approcheront le niveau intellectuel des gens, et d'ici 2050, elles nous dépasseront.  Mais alors que Moravec prédit la fin d'une ère de domination humaine, sa vision de cet événement n'est pas si sombre.  Il n'est pas isolé d'un avenir dans lequel les machines gouvernent le monde, mais l'accepte et décrit un point de vue étonnant selon lequel les robots intelligents deviendront nos descendants évolutionnaires.  Moravec pense qu'à la fin de ce processus, "le vaste cyberespace s'unira au supramental inhumain et traitera les affaires aussi loin des gens que les affaires humaines des bactéries". </blockquote><br>  Shane Leg est co-fondateur de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMind Technologies</a> , une startup de l'IA achetée en 2014 pour 500 millions de dollars par Google.  Il a reçu son doctorat de l'Institut de l'IA nommé d'après  Dale Moul en Suisse, et a également travaillé dans la division de neurobiologie computationnelle.  Gatsby à Londres.  Au terme de sa thèse, «superintelligence machine», il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">écrit</a> : <br><blockquote>  Si quelque chose apparaît qui peut s'approcher de la puissance absolue, ce sera une machine super intelligente.  Par définition, elle pourra atteindre un grand nombre d'objectifs dans une grande variété d'environnements.  Si nous nous préparons soigneusement à une telle opportunité à l'avance, nous pouvons non seulement éviter la catastrophe, mais aussi commencer une ère de prospérité, contrairement à tout ce qui existait auparavant. </blockquote><br>  Dans une interview ultérieure, il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dit</a> : <br><blockquote>  L'IA est maintenant là où Internet était en 1988.  Les besoins d'apprentissage automatique sont requis dans des applications spéciales (moteurs de recherche tels que Google, hedge funds et bioinformatique), et leur nombre augmente chaque année.  Je pense que vers le milieu de la prochaine décennie, ce processus deviendra massif et perceptible.  Le boom de l'IA devrait avoir lieu vers 2020, suivi d'une décennie de progrès rapides, peut-être après les corrections du marché.  L'IA au niveau humain sera créée vers la mi-2020, bien que beaucoup de gens n'accepteront pas le début de cet événement.  Après cela, les risques associés à l'IA avancée seront mis en pratique.  Je ne parlerai pas de la «singularité», mais ils s'attendent à ce qu'à un moment donné après la création de l'OII, des choses folles commencent à se produire.  C'est entre 2025 et 2040. </blockquote><br>  Lui et ses co-fondateurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Demis Khasabis</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mustafa Suleiman ont</a> signé une pétition pour l'Institut pour la vie future concernant les risques liés à l'IA, et l'une de leurs conditions pour rejoindre Google était que l'entreprise accepte d'organiser un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conseil d'éthique de l'IA</a> pour étudier ces questions. <br><br>  Steve Omohundro est un ancien professeur d'informatique à l'Université de l'Illinois, le fondateur du groupe de vision par ordinateur et de formation au Center for the Study of Complex Systems, et l'inventeur de divers développements importants dans l'apprentissage automatique et la vision par ordinateur.  Il a travaillé sur des robots lisant les lèvres, le langage de programmation parallèle StarLisp, des algorithmes d'apprentissage géométrique.  Il dirige actuellement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Self-Aware Systems</a> , "une équipe de scientifiques qui veille à ce que les technologies intelligentes profitent à l'humanité".  Son travail, «Les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bases de la motivation de l'IA</a> », a aidé à créer le domaine de l'éthique des machines, car il a noté que les systèmes super-intelligents seront dirigés vers des objectifs potentiellement dangereux.  Il écrit: <br><blockquote>  Nous avons montré que tous les systèmes d'IA avancés sont susceptibles d'avoir un ensemble de motivations fondamentales.  Il est impératif de comprendre ces motivations afin de créer des technologies qui garantissent un avenir positif à l'humanité.  Yudkovsky a appelé à une «IA amicale».  Pour ce faire, nous devons développer une approche scientifique du «développement utilitaire», qui nous permettra de développer des fonctions socialement utiles qui conduiront aux séquences que nous désirons.  Les progrès rapides du progrès technologique suggèrent que ces problèmes pourraient bientôt devenir critiques. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Vous</a> pouvez trouver ses articles sur le sujet "AI rationnelle pour le bien commun" sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Murray Shanahan</a> est titulaire d'un doctorat en informatique de Cambridge et est maintenant professeur de robotique cognitive à l'Imperial College de Londres.  Il a publié des travaux dans des domaines tels que la robotique, la logique, les systèmes dynamiques, la neurobiologie computationnelle et la philosophie de l'esprit.  Il travaille actuellement sur le livre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Technological Singularity</a> , qui sera publié en août.  L'annotation promotionnelle d'Amazon est la suivante: <br><blockquote>  Shanahan décrit les progrès technologiques de l'IA, à la fois réalisés sous l'influence des connaissances de la biologie et développés à partir de zéro.  Il explique que lorsque l'IA du niveau humain sera créée - une tâche théoriquement possible mais difficile - la transition vers une IA superintelligente sera très rapide.  Le Shanahan réfléchit à ce que l'existence de machines superintelligentes peut mener à des domaines tels que la personnalité, la responsabilité, les droits et l'individualité.  Certains représentants de l'intelligence artificielle superintelligente peuvent être créés au profit de l'homme, certains peuvent devenir incontrôlables.  (C'est-à-dire, Siri ou HAL?) La singularité représente pour l'humanité à la fois une menace existentielle et une opportunité existentielle de surmonter ses limites.  Shanahan indique clairement que si nous voulons obtenir un meilleur résultat, nous devons imaginer les deux possibilités. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Marcus Hutter</a> est professeur de recherche en informatique à la National Australia University.  Avant cela, il a travaillé à l'Institut de l'IA nommé d'après  Dale Mole en Suisse et à l'Institut national de l'informatique et des communications en Australie, et a également travaillé sur l'apprentissage stimulé, les découvertes bayésiennes, la théorie de la complexité computationnelle, la théorie de Salomon sur les prédictions inductives, la vision par ordinateur et les profils génétiques.  Il a également beaucoup écrit sur la singularité.  Dans l'article « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'intelligence peut-elle exploser?</a> », Il écrit: <br><blockquote>  Ce siècle peut être témoin d'une explosion technologique dont l'ampleur mérite le nom de singularité.  Le scénario par défaut est une communauté d'individus intelligents en interaction dans le monde virtuel, simulée sur des ordinateurs avec des ressources informatiques en augmentation hyperbolique.  Cela s'accompagne inévitablement d'une explosion de vitesse, mesurée par le temps physique, mais pas nécessairement d'une explosion d'intelligence.  Si le monde virtuel est peuplé d'individus libres et en interaction, la pression évolutive conduira à l'émergence d'individus de plus en plus intelligents qui rivaliseront pour les ressources informatiques.  Le point final de cette accélération évolutive de l'intelligence peut être la communauté des individus les plus intelligents.  Certains aspects de cette communauté singulière peuvent théoriquement être étudiés à l'aide d'outils scientifiques modernes.  Bien avant l'apparition de cette singularité, en plaçant même cette communauté virtuelle dans notre imagination, on peut imaginer l'émergence de différences, comme par exemple une forte baisse de la valeur d'un individu qui peut entraîner des conséquences radicales. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jürgen Schmidhuber</a> est professeur d'IA à l'Université de Lugano et ancien professeur de robotique cognitive à l'Université de technologie de Munich.  Il développe certains des réseaux de neurones les plus avancés au monde, travaille sur la robotique évolutionnaire et la théorie de la complexité informatique, et est chercheur à l'Académie européenne des sciences et des arts.  Dans son livre, « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Hypothèses des singularités</a> », il soutient qu '«avec la poursuite des tendances existantes, nous ferons face à une explosion intellectuelle au cours des prochaines décennies».  Interrogé directement à l'AMA de Reddit sur les risques associés à l'IA, il a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">répondu</a> : <br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trouver un créneau pour la survie</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . "</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est professeur et membre du comité iCORE de l'Université de l'Alberta. </font><font style="vertical-align: inherit;">Il est chercheur à l'Association pour le développement de l'IA, co-auteur du </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manuel le plus populaire sur l'apprentissage stimulé</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pionnier de la méthode des différences de temps, l'un des plus importants dans ce domaine. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans son </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rapport</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lors d'une conférence sur l'IA organisée par l'Institute for the Future of Life, Saton a fait valoir qu '"il y a une réelle chance que même avec nos vies" une IA soit créée qui soit intellectuellement comparable aux humains, et a ajouté que cette IA "ne nous obéira pas", " pour concurrencer et coopérer avec nous », et que« si nous créons des esclaves super intelligents, nous obtenons des adversaires super intelligents. » En conclusion, il a déclaré que "nous devons réfléchir à des mécanismes (sociaux, juridiques, politiques, culturels) pour garantir le résultat souhaité", mais que "les gens ordinaires deviendront inévitablement moins importants". Il a également mentionné des problèmes similaires lors de la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">présentation du</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gadsby Institute. Aussi dans le livre de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">il existe de telles lignes: "Richard Saton, l'un des plus grands spécialistes de l'IA, prédit une explosion de l'intelligence quelque part au milieu du siècle." </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est professeur de vision industrielle à l'Imperial College de Londres, leader dans les groupes de vision robotique et le Dyson Robotics Laboratory, et inventeur du système de localisation et de balisage informatisé MonoSLAM. Sur son site Internet, il </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">écrit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         « » (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    « »    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> n'ont pas besoin d'être présentés. Turing a inventé les fondements mathématiques de la science informatique et a donné son nom à une machine de Turing, à l'exhaustivité de Turing et au test de Turing. Goode a travaillé avec Turing à Bletchley Park, a aidé à créer l'un des premiers ordinateurs et a inventé de nombreux algorithmes bien connus, par exemple, l'algorithme de transformée de Fourier discrète rapide, connu sous le nom d'algorithme FFT. Dans leur travail, les voitures numériques peuvent-elles penser? Turing écrit:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supposons que de telles machines puissent être créées et considérons les conséquences de leur création. </font><font style="vertical-align: inherit;">Un tel acte rencontrera sans aucun doute l'hostilité, à moins que nous n'ayons progressé dans la tolérance religieuse depuis l'époque de Galilée. </font><font style="vertical-align: inherit;">L'opposition sera constituée d'intellectuels craignant de perdre leur emploi. </font><font style="vertical-align: inherit;">Mais il est probable que les intellectuels se tromperont. </font><font style="vertical-align: inherit;">Il sera possible de faire beaucoup de choses pour tenter de maintenir leur intellect au niveau des normes fixées par les machines, car après le début de la méthode de la machine, cela ne prend pas beaucoup de temps jusqu'au moment où les machines dépassent nos capacités insignifiantes. </font><font style="vertical-align: inherit;">À un certain stade, nous devons nous attendre à ce que les machines prennent le contrôle.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tout en travaillant à Atlas Computer Lab dans les années 60, Goode a développé cette idée dans « </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Raisonnement pour la première machine ultra-intelligente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> »:</font></font><br><blockquote>   ,  ,       .     –     ,       .  ,   ,   « »,      . ,    –   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cela me dérange que cette liste puisse donner l'impression d'une certaine querelle entre «croyants» et «sceptiques» dans ce domaine, au cours de laquelle ils se fracassent mutuellement. Mais je ne le pensais pas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lorsque je lis des articles sur les sceptiques, je tombe toujours sur deux arguments. Premièrement, nous sommes encore très loin de l'IA du niveau humain, sans parler de la superintelligence, et il n'y a aucun moyen évident d'atteindre de tels sommets. Deuxièmement, si vous demandez des interdictions de recherche sur l'IA, vous êtes un idiot. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je suis entièrement d'accord avec les deux points. Comme les dirigeants du mouvement du risque de l'IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enquête auprès des chercheurs en IA ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) ont montré qu'en moyenne, ils donnent 50% pour le fait que l'IA de niveau humain apparaîtra d'ici 2040 ode, et 90% - qu'elle apparaîtra d'ici 2075. En moyenne, 75% d'entre eux pensent que la superintelligence («l'intelligence artificielle, dépassant sérieusement les capacités de chaque personne dans la plupart des professions ») apparaîtra dans les 30 ans suivant l'avènement de l'IA au niveau humain. Et bien que la technique de ce sondage soulève certains doutes, si vous acceptez ses résultats, il s'avère que la plupart des chercheurs en IA conviennent que quelque chose qui mérite d'être inquiété apparaîtra dans une ou deux générations. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais le directeur de l'Institut de l'intelligence artificielle, Luke Muelhauser, et le directeur de l'Institut pour l'avenir de l'humanité, Nick Bostrom, ont déclaré que leurs prédictions pour le développement de l'IA sont bien plus tardives que les prévisions des scientifiques participant à l'enquête. Si vous étudiez</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">les données sur les prédictions de l'IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de Stuart Armstrong, on peut voir que, en général, les estimations sur le moment de l'apparition de l'IA faites par les partisans de l'IA ne diffèrent pas des estimations faites par les sceptiques de l'IA. De plus, la prédiction à long terme dans ce tableau appartient à Armstrong lui-même. Cependant, Armstrong travaille actuellement à l'Institut pour l'avenir de l'humanité, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">attirant l'attention sur les risques de l'IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et la nécessité de rechercher les objectifs de la superintelligence. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La différence entre les partisans et les sceptiques n'est pas dans leurs évaluations du moment où nous devrions nous attendre à l'apparition de l'IA au niveau humain, mais dans le moment où nous devons commencer à nous préparer à cela.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ce qui nous amène au deuxième point. La position des sceptiques, semble-t-il, est que même si nous devrions probablement envoyer quelques personnes intelligentes pour travailler sur une évaluation préliminaire du problème, il n'est absolument pas nécessaire de paniquer ou d'interdire l'étude de l'IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les fans de l'IA insistent sur le fait que même si nous n'avons pas besoin de paniquer ou d'interdire la recherche sur l'IA, cela vaut probablement la peine d'envoyer quelques personnes intelligentes pour travailler sur une évaluation préliminaire du problème. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun est sans doute le sceptique le plus ardent des risques liés à l'IA. Il a été abondamment cité dans un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article sur la science populaire</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dans un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article sur la révolution marginale</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , et il a également parlé avec </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sur les "questions inévitables de la singularité", qu'il décrit lui-même comme "étant si loin que la science-fiction peut être écrite à leur sujet". </font><font style="vertical-align: inherit;">Mais lorsqu'on lui a demandé de clarifier sa position, il a déclaré:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk est très inquiet des menaces existentielles contre l'humanité (c'est pourquoi il construit des roquettes pour envoyer des gens coloniser d'autres planètes). </font><font style="vertical-align: inherit;">Et bien que le risque d'une rébellion de l'IA soit très faible et très éloigné de l'avenir, nous devons y réfléchir, développer des mesures et des règles de précaution. </font><font style="vertical-align: inherit;">Tout comme le comité de bioéthique est apparu dans les années 1970 et 1980, avant l'utilisation généralisée de la génétique, nous avons besoin de comités d'éthique de l'IA. </font><font style="vertical-align: inherit;">Mais, comme l'a écrit Yoshua Benjio, nous avons encore beaucoup de temps.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz est un autre expert souvent désigné comme le principal porte-parole du scepticisme et des limites. </font><font style="vertical-align: inherit;">Son point de vue a été décrit dans des articles tels que «Le </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">directeur de Microsoft Research pense que l'IA hors de contrôle ne nous tuera pas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> » et « </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz de Microsoft pense que l'IA ne devrait pas avoir peur</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ». </font><font style="vertical-align: inherit;">Mais voici ce qu'il a dit dans une interview plus longue avec NPR:</font></font><br><blockquote> Keist: Horvitz doute que les secrétaires virtuels ne se transformeront jamais en quelque chose qui conquérera le monde.  Il dit qu'il faut s'attendre à ce qu'un kite évolue en Boeing 747. Est-ce à dire qu'il se moque d'une singularité? <br><br>  Horvitz: Non.  Je pense qu'il y avait un mélange de concepts, et moi aussi j'ai des sentiments mitigés. <br><br>  Keist: En particulier, en raison d'idées telles que la singularité, Horvits et d'autres experts en IA tentent de plus en plus de traiter les problèmes éthiques qui peuvent survenir avec une IA étroitement ciblée dans les années à venir.  Ils posent également des questions plus futuristes.  Par exemple, comment puis-je créer un bouton d'arrêt d'urgence pour un ordinateur qui peut se changer lui-même? <br><br>  Horvits: Je crois vraiment que les enjeux sont suffisamment élevés pour consacrer du temps et de l'énergie à rechercher activement des solutions, même si la probabilité de tels événements est faible. </blockquote><br>  Cela coïncide généralement avec la position de bon nombre des agitateurs à risque d'IA les plus ardents.  Avec de tels amis, les ennemis ne sont pas nécessaires. <br><br>  L'article de Slate, « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">N'ayez pas peur de l'IA</a> » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">,</a> met également les choses sous un jour surprenant: <br><blockquote>  Comme le dit Musk lui-même, la solution au risque d'IA réside dans la collaboration sobre et rationnelle des scientifiques et des législateurs.  Cependant, il est assez difficile de comprendre comment parler de «démons» peut aider à atteindre ce noble objectif.  Elle peut même la gêner. <br><br>  Premièrement, il y a d'énormes trous dans l'idée du script Skynet.  Bien que les chercheurs dans le domaine de l'informatique croient que le raisonnement de Mask n'est «pas complètement fou», ils sont encore trop loin d'un monde dans lequel le battage médiatique autour de l'IA déguise une réalité légèrement moins IA que nos informaticiens sont confrontés. <br><br>  Ian Lekun, chef du laboratoire d'IA de Facebook, a résumé cette idée dans un article sur Google+ en 2013: le battage médiatique nuit à l'IA.  Au cours des cinq dernières décennies, le battage médiatique a tué AI quatre fois.  Elle doit être arrêtée. "Lekun et d'autres ont à juste titre peur du battage médiatique. Le fait de ne pas répondre aux attentes élevées imposées par la science-fiction entraîne de sérieuses coupes budgétaires pour la recherche en IA. </blockquote><br>  Les scientifiques travaillant sur l'IA sont des gens intelligents.  Ils ne sont pas intéressés à tomber dans des pièges politiques classiques, dans lesquels ils seraient divisés en camps et s’accuseraient de panique ou d’ostrichisme.  Apparemment, ils essaient de trouver un équilibre entre la nécessité de commencer les travaux préliminaires liés à la menace qui se profile quelque part au loin, et le risque de provoquer une sensation si forte qui les frappera. <br><br>  Je ne veux pas dire qu'il n'y a pas de divergence d'opinion sur la rapidité avec laquelle vous devez commencer à résoudre ce problème.  Fondamentalement, tout se résume à savoir s'il est possible de dire que «nous résoudrons le problème lorsque nous le rencontrerons» ou d'attendre un décollage aussi inattendu, en raison duquel tout deviendra incontrôlable et pour lequel, par conséquent, nous devons nous préparer à l'avance.  Je vois moins de preuves que je ne le souhaiterais que la majorité des chercheurs en IA avec leurs propres opinions comprennent la deuxième possibilité.  Que puis-je dire, même si dans un article sur Marginal Revolution un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">expert est cité</a> disant que la superintelligence ne représente pas une grande menace, car "les ordinateurs intelligents ne peuvent pas se fixer des objectifs", bien que quiconque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lit Bostrom</a> sache que tout le problème est. <br><br>  Il y a encore une montagne de travail à faire.  Mais juste pour ne pas sélectionner spécifiquement des articles dans lesquels "les vrais experts en IA ne se soucient pas de la superintelligence". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr402379/">https://habr.com/ru/post/fr402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr402367/index.html">Comment arrêter de payer pour l'itinérance, ou avec un seul numéro partout dans le monde</a></li>
<li><a href="../fr402369/index.html">Comment mesurer la vitesse d'une imprimante 3D - son extrémité chaude. Et pas seulement la vitesse</a></li>
<li><a href="../fr402373/index.html">Ce qui donne la "Génétique du Microbiote"</a></li>
<li><a href="../fr402375/index.html">Commutateur ca à 4 canaux de 8 kilowatts avec mesure de la consommation. Partie 1</a></li>
<li><a href="../fr402377/index.html">Que pensent vos smartphones de la charge USB de la voiture</a></li>
<li><a href="../fr402381/index.html">Comment recruter des astronautes</a></li>
<li><a href="../fr402383/index.html">Saw, Shura: comment nous avons conçu l'application mobile de suivi des chiens Mishiko</a></li>
<li><a href="../fr402385/index.html">Pourquoi vous devriez vous attendre à un boom dans le domaine de la création de robots pour les locaux commerciaux</a></li>
<li><a href="../fr402387/index.html">Stylo 3D pour imprimantes 3D</a></li>
<li><a href="../fr402389/index.html">La MPAA et la RIAA prévoient de récupérer les données des disques durs défaillants du partage de fichiers Megaupload</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>