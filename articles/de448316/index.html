<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∑üèª üë®üèª‚Äç‚öïÔ∏è ü§∑üèø Bestimmung der Hunderasse: Ein vollst√§ndiger Entwicklungszyklus, von einem neuronalen Netzwerk in Python bis zu einer Anwendung bei Google Play üë®üèΩ‚Äçüíº üë®üèº‚Äçüé® üöÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fortschritte auf dem Gebiet der neuronalen Netze im Allgemeinen und der Mustererkennung im Besonderen haben dazu gef√ºhrt, dass es den Anschein hat, al...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bestimmung der Hunderasse: Ein vollst√§ndiger Entwicklungszyklus, von einem neuronalen Netzwerk in Python bis zu einer Anwendung bei Google Play</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/448316/">  Fortschritte auf dem Gebiet der neuronalen Netze im Allgemeinen und der Mustererkennung im Besonderen haben dazu gef√ºhrt, dass es den Anschein hat, als sei die Erstellung einer neuronalen Netzanwendung f√ºr die Arbeit mit Bildern eine Routineaufgabe.  In gewissem Sinne ist es so - wenn Sie auf eine Idee zur Mustererkennung gekommen sind, zweifeln Sie nicht daran, dass jemand bereits so etwas geschrieben hat.  Sie m√ºssen lediglich den entsprechenden Code in Google finden und vom Autor ‚Äûkompilieren‚Äú. <br><br>  Es gibt jedoch immer noch zahlreiche Details, die die Aufgabe nicht so sehr unl√∂sbar machen, sondern ... langweilig, w√ºrde ich sagen.  Es dauert zu lange, besonders wenn Sie ein Anf√§nger sind, der Schritt f√ºr Schritt F√ºhrung ben√∂tigt, ein Projekt, das direkt vor Ihren Augen durchgef√ºhrt und von Anfang bis Ende abgeschlossen wird.  Ohne das in solchen F√§llen √ºbliche Ausreden ‚Äûdiesen offensichtlichen Teil √ºberspringen‚Äú. <br><br>  In diesem Artikel werden wir uns mit der Aufgabe befassen, einen Hunderassen-Identifikator zu erstellen: Wir werden ein neuronales Netzwerk erstellen und trainieren, es dann auf Java f√ºr Android portieren und auf Google Play ver√∂ffentlichen. <br><br>  Wenn Sie sich das fertige Ergebnis ansehen m√∂chten, finden Sie es hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeuroDog App</a> bei Google Play. <br><br>  Website mit meiner Robotik (in Bearbeitung): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">robotics.snowcron.com</a> . <br>  Website mit dem Programm selbst, einschlie√ülich eines Handbuchs: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeuroDog User Guide</a> . <br><br>  Und hier ist ein Screenshot des Programms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/186/b91/457/186b914572170b01446ed1d722bce200.png" alt="Bild"><br><br><a name="habracut"></a><br><br><h3>  Erkl√§rung des Problems </h3><br><br>  Wir werden Keras verwenden: eine Google-Bibliothek f√ºr die Arbeit mit neuronalen Netzen.  Dies ist eine Bibliothek auf hoher Ebene, was bedeutet, dass sie im Vergleich zu den mir bekannten Alternativen einfacher zu verwenden ist.  Wenn √ºberhaupt - es gibt viele Lehrb√ºcher √ºber Keras im Netzwerk von hoher Qualit√§t. <br><br>  Wir werden CNN - Convolutional Neural Networks verwenden.  CNN (und darauf basierende erweiterte Konfigurationen) sind der De-facto-Standard bei der Bilderkennung.  Gleichzeitig ist das Trainieren eines solchen Netzwerks nicht immer einfach: Sie m√ºssen die richtige Netzwerkstruktur und Trainingsparameter ausw√§hlen (all diese Lernraten, Impulse, L1 und L2 usw.).  Die Aufgabe erfordert erhebliche Rechenressourcen. Um sie zu l√∂sen, schlagen ALLE Parameter fehl. <br><br>  Dies ist einer von mehreren Gr√ºnden, warum sie in den meisten F√§llen das sogenannte ‚ÄûTransferwissen‚Äú anstelle des sogenannten ‚ÄûVanille‚Äú -Ansatzes verwenden.  Transfer Knowlege verwendet ein neuronales Netzwerk, das von jemandem vor uns (z. B. Google) trainiert wurde, und normalerweise f√ºr eine √§hnliche, aber immer noch andere Aufgabe.  Wir nehmen die ersten Ebenen daraus, ersetzen die letzten Ebenen durch unseren eigenen Klassifikator - und es funktioniert und es funktioniert gro√üartig. <br><br>  Ein solches Ergebnis mag zun√§chst √ºberraschend sein: Wie kommt es, dass wir ein Google-Netzwerk genutzt haben, um Katzen von St√ºhlen zu unterscheiden, und das Hunderassen f√ºr uns erkennt?  Um zu verstehen, wie dies geschieht, m√ºssen Sie die Grundprinzipien der Arbeit von Deep Neural Networks verstehen, einschlie√ülich derer, die f√ºr die Mustererkennung verwendet werden. <br><br>  Wir haben dem Netzwerk ein Bild (also ein Array von Zahlen) als Eingabe ‚Äûzugef√ºhrt‚Äú.  Die erste Ebene analysiert das Bild auf einfache Muster wie ‚Äûhorizontale Linie‚Äú, ‚ÄûBogen‚Äú usw.  Die n√§chste Schicht empf√§ngt diese Muster als Eingabe und erzeugt Muster zweiter Ordnung wie ‚ÄûFell‚Äú, ‚ÄûAugenwinkel‚Äú ... Letztendlich erhalten wir ein Puzzle, aus dem wir den Hund rekonstruieren k√∂nnen: Wolle, zwei Augen und eine menschliche Hand in Z√§hnen. <br><br>  All dies wurde mit Hilfe von vorgefertigten Schichten durchgef√ºhrt, die von uns (zum Beispiel von Google) erhalten wurden.  Als n√§chstes f√ºgen wir unsere Ebenen hinzu und bringen ihnen bei, Rasseninformationen aus diesen Mustern zu extrahieren.  Klingt logisch. <br><br>  Zusammenfassend werden wir in diesem Artikel sowohl "Vanilla" CNN als auch verschiedene "Transfer Learning" -Varianten verschiedener Netzwerktypen erstellen.  Was "Vanille" betrifft: Ich werde es erstellen, aber ich habe nicht vor, es durch Auswahl von Parametern zu konfigurieren, da es viel einfacher ist, "vorab trainierte" Netzwerke zu trainieren und zu konfigurieren. <br><br>  Da wir unserem neuronalen Netzwerk beibringen wollen, Hunderassen zu erkennen, m√ºssen wir ihm Proben verschiedener Rassen ‚Äûzeigen‚Äú.  Gl√ºcklicherweise gibt es hier eine Reihe von Fotos, die f√ºr eine √§hnliche Aufgabe erstellt wurden (das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Original ist hier</a> ). <br><br>  Dann plane ich, das beste der empfangenen Netzwerke f√ºr Android zu portieren.  Das Portieren von Kerasov-Netzwerken auf Android ist relativ einfach, gut formalisiert und wir werden alle erforderlichen Schritte ausf√ºhren, sodass es nicht schwierig sein wird, diesen Teil zu reproduzieren. <br><br>  Dann werden wir all dies auf Google Play ver√∂ffentlichen.  Nat√ºrlich wird Google Widerstand leisten, daher werden zus√§tzliche Tricks verwendet.  Beispielsweise ist die Gr√∂√üe unserer Anwendung (aufgrund eines sperrigen neuronalen Netzwerks) gr√∂√üer als die zul√§ssige Gr√∂√üe der von Google Play akzeptierten Android-APK: Wir m√ºssen Bundles verwenden.  Dar√ºber hinaus zeigt Google unsere Anwendung nicht in den Suchergebnissen an. Dies kann behoben werden, indem Such-Tags in der Anwendung registriert werden, oder warten Sie einfach ... ein oder zwei Wochen. <br><br>  Als Ergebnis erhalten wir eine voll funktionsf√§hige "kommerzielle" (in Anf√ºhrungszeichen, wie es kostenlos ausgelegt ist) Anwendung f√ºr Android und unter Verwendung neuronaler Netze. <br><br><h3>  Entwicklungsumgebung </h3><br><br>  Sie k√∂nnen f√ºr Keras auf verschiedene Arten programmieren, abh√§ngig vom verwendeten Betriebssystem (Ubuntu empfohlen), dem Vorhandensein oder Fehlen einer Grafikkarte usw.  Es gibt nichts Schlechtes an der Entwicklung auf dem lokalen Computer (und entsprechend seiner Konfiguration), au√üer dass dies nicht der einfachste Weg ist. <br><br>  Die Installation und Konfiguration einer gro√üen Anzahl von Tools und Bibliotheken nimmt zun√§chst Zeit in Anspruch. Wenn neue Versionen ver√∂ffentlicht werden, m√ºssen Sie erneut Zeit aufwenden.  Zweitens erfordern neuronale Netze eine gro√üe Rechenleistung f√ºr das Training.  Sie k√∂nnen diesen Vorgang beschleunigen (um das 10-fache oder mehr), wenn Sie eine GPU verwenden. Zum Zeitpunkt des Schreibens dieses Artikels kosten die f√ºr diese Arbeit am besten geeigneten GPUs 2.000 bis 7.000 US-Dollar.  Und ja, sie m√ºssen auch konfiguriert werden. <br><br>  Also werden wir den anderen Weg gehen.  Tatsache ist, dass Google armen Igeln wie uns erlaubt, GPUs aus ihrem Cluster zu verwenden - kostenlos, f√ºr Berechnungen in Bezug auf neuronale Netze, bietet es auch eine vollst√§ndig konfigurierte Umgebung, alles zusammen wird dies als Google Colab bezeichnet.  Der Dienst bietet Ihnen Zugriff auf Jupiter Notebook mit Python, Keras und einer Vielzahl anderer bereits konfigurierter Bibliotheken.  Sie m√ºssen lediglich ein Google-Konto einrichten (ein Google Mail-Konto einrichten, damit Sie auf alles andere zugreifen k√∂nnen). <br><br>  Im Moment kann Colab hier eingestellt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> , aber wenn Sie Google kennen, kann sich dies jederzeit √§ndern.  Google einfach Google Colab. <br><br>  Das offensichtliche Problem bei der Verwendung von Colab ist, dass es sich um einen WEB-Dienst handelt.  Wie greifen wir auf unsere Daten zu?  Speichern Sie das neuronale Netzwerk nach dem Training, laden Sie beispielsweise f√ºr unsere Aufgabe spezifische Daten herunter und so weiter? <br><br>  Es gibt verschiedene (zum Zeitpunkt des Schreibens dieses Artikels - drei) verschiedene M√∂glichkeiten, wir verwenden die, die ich f√ºr am bequemsten halte - wir verwenden Google Drive. <br><br>  Google Drive ist ein Cloud-basierter Datenspeicher, der √§hnlich wie eine normale Festplatte funktioniert und auf Google Colab abgebildet werden kann (siehe Code unten).  Danach k√∂nnen Sie damit arbeiten, als w√ºrden Sie mit Dateien auf einer lokalen Festplatte arbeiten.  Das hei√üt, um auf die Fotos von Hunden zuzugreifen, um unser neuronales Netzwerk zu trainieren, m√ºssen wir sie auf Google Drive hochladen, das ist alles. <br><br><h2>  Erstellen und Trainieren eines neuronalen Netzwerks </h2><br><br>  Unten gebe ich den Code in Python Block f√ºr Block (aus dem Jupiter-Notizbuch).  Sie k√∂nnen diesen Code in Ihr Jupiter-Notizbuch kopieren und auch Block f√ºr Block ausf√ºhren, da Bl√∂cke unabh√§ngig voneinander ausgef√ºhrt werden k√∂nnen (nat√ºrlich k√∂nnen im sp√§ten Block definierte Variablen im sp√§ten Block erforderlich sein, dies ist jedoch eine offensichtliche Abh√§ngigkeit). <br><br><h3>  Initialisierung </h3><br><br>  Lassen Sie uns zun√§chst Google Drive einbinden.  Nur zwei Zeilen.  Dieser Code sollte nur einmal in einer Colab-Sitzung ausgef√ºhrt werden (z. B. alle 6 Stunden).  Wenn Sie es ein zweites Mal aufrufen, w√§hrend die Sitzung noch "aktiv" ist, wird es √ºbersprungen, da das Laufwerk bereits bereitgestellt ist. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> drive drive.mount(<span class="hljs-string"><span class="hljs-string">'/content/drive/'</span></span>)</code> </pre> <br><br>  Beim ersten Start werden Sie gebeten, Ihre Absichten zu best√§tigen, es gibt nichts Kompliziertes.  So sieht es aus: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Go to this URL <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a browser: ... &gt;&gt;&gt; Enter your authorization code: &gt;&gt;&gt; ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ &gt;&gt;&gt; Mounted at /content/drive/</code> </pre><br><br>  Ein vollst√§ndig standardm√§√üiger <i>Include-</i> Abschnitt;  Es ist m√∂glich, dass einige der enthaltenen Dateien nicht ben√∂tigt werden.  Da ich verschiedene neuronale Netze testen werde, m√ºssen Sie einige der enthaltenen Module f√ºr bestimmte Arten von neuronalen Netzen kommentieren / auskommentieren: Um beispielsweise InceptionV3 NN zu verwenden, die Aufnahme von InceptionV3 auskommentieren und beispielsweise ResNet50 auskommentieren.  Oder auch nicht: Alles, was sich daraus √§ndert, ist die Gr√∂√üe des verwendeten Speichers, und das ist nicht sehr stark. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> regularizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Conv2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, GlobalAveragePooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback, EarlyStopping <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReduceLROnPlateau <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.vgg16 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> inception_v3_preprocessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.mobilenetv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MobileNetV2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.nasnet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NASNetMobile</code> </pre><br><br>  In Google Drive erstellen wir einen Ordner f√ºr unsere Dateien.  Die zweite Zeile zeigt den Inhalt an: <br><br><pre> <code class="python hljs">working_path = <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data/"</span></span> !ls <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data"</span></span> &gt;&gt;&gt; all_images labels.csv models test train valid</code> </pre><br><br>  Wie Sie sehen k√∂nnen, werden die Fotos der Hunde (kopiert aus dem Stanford-Datensatz (siehe oben) in Google Drive) zuerst im Ordner <i>all_images gespeichert</i> .  Sp√§ter werden wir sie in die <i>Zug-, G√ºltigkeits-</i> und <i>Testverzeichnisse</i> kopieren.  Wir speichern trainierte Modelle im Modellordner.  Die Datei labels.csv ist Teil des Datensatzes mit Fotos und enth√§lt eine Korrespondenztabelle mit den Namen von Bildern und Hunderassen. <br><br>  Es gibt viele Tests, die Sie ausf√ºhren k√∂nnen, um zu verstehen, was genau wir f√ºr die vor√ºbergehende Verwendung von Google erhalten haben.  Zum Beispiel: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Is GPU Working? import tensorflow as tf tf.test.gpu_device_name() &gt;&gt;&gt; '/device:GPU:0'</span></span></code> </pre><br><br>  Wie Sie sehen, ist die GPU wirklich verbunden. Wenn nicht, m√ºssen Sie diese Option in den Jupiter Notebook-Einstellungen suchen und aktivieren. <br><br>  Als n√§chstes m√ºssen wir einige Konstanten deklarieren, wie z. B. die Gr√∂√üe von Bildern usw.  Wir werden Bilder mit einer Gr√∂√üe von 256 x 256 Pixel verwenden. Dies ist ein Bild, das gro√ü genug ist, um keine Details zu verlieren, und klein genug, damit alles in den Speicher passt.  Beachten Sie jedoch, dass einige Arten von neuronalen Netzen, die wir verwenden werden, Bilder mit 224 x 224 Pixel erwarten.  In solchen F√§llen kommentieren wir 256 und kommentieren 224 aus. <br><br>  Der gleiche Ansatz (Kommentar eins - Kommentar) wird auf die Namen der Modelle angewendet, die wir speichern, einfach weil wir keine Dateien √ºberschreiben m√∂chten, die m√∂glicherweise noch n√ºtzlich sind. <br><pre> <code class="python hljs">warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">"ignore"</span></span>) os.environ[<span class="hljs-string"><span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span></span>] = <span class="hljs-string"><span class="hljs-string">'2'</span></span> np.random.seed(<span class="hljs-number"><span class="hljs-number">7</span></span>) start = dt.datetime.now() BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">16</span></span> EPOCHS = <span class="hljs-number"><span class="hljs-number">15</span></span> TESTING_SPLIT=<span class="hljs-number"><span class="hljs-number">0.3</span></span> <span class="hljs-comment"><span class="hljs-comment"># 70/30 % NUM_CLASSES = 120 IMAGE_SIZE = 256 #strModelFileName = "models/ResNet50.h5" # strModelFileName = "models/InceptionV3.h5" strModelFileName = "models/InceptionV3_Sgd.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/MobileNetV2.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/NASNetMobileSgd.h5"</span></span></code> </pre><br><br><h3>  Laden von Daten </h3><br><br>  <i>Laden Sie zun√§chst die</i> Datei <i>labels.csv hoch</i> und <i>teilen Sie</i> sie in die Teile Training und Validierung auf.  Beachten Sie, dass es noch keinen Testteil gibt, da ich schummeln werde, um mehr Trainingsdaten zu erhalten. <br><br><pre> <code class="python hljs">labels = pd.read_csv(working_path + <span class="hljs-string"><span class="hljs-string">'labels.csv'</span></span>) print(labels.head()) train_ids, valid_ids = train_test_split(labels, test_size = TESTING_SPLIT) print(len(train_ids), <span class="hljs-string"><span class="hljs-string">'train ids'</span></span>, len(valid_ids), <span class="hljs-string"><span class="hljs-string">'validation ids'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'Total'</span></span>, len(labels), <span class="hljs-string"><span class="hljs-string">'testing images'</span></span>) &gt;&gt;&gt; id breed &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">000</span></span>bec180eb18c7604dcecc8fe0dba07 boston_bull &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">001513</span></span>dfcb2ffafc82cccf4d8bbaba97 dingo &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">001</span></span>cdf01b096e06d78e9e5112d419397 pekinese &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">00214</span></span>f311d5d2247d5dfe4fe24b2303d bluetick &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">0021</span></span>f9ceb3235effd7fcde7f7538ed62 golden_retriever &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">7155</span></span> train ids <span class="hljs-number"><span class="hljs-number">3067</span></span> validation ids &gt;&gt;&gt; Total <span class="hljs-number"><span class="hljs-number">10222</span></span> testing images</code> </pre><br><br>  Kopieren Sie anschlie√üend die Bilddateien entsprechend den Dateinamen in die Ordner f√ºr Schulung, Validierung und Test.  Die folgende Funktion kopiert die Dateien, deren Namen wir in den angegebenen Ordner √ºbertragen. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copyFileSet</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(strDirFrom, strDirTo, arrFileNames)</span></span></span><span class="hljs-function">:</span></span> arrBreeds = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) arrFileNames = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'id'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo): os.makedirs(strDirTo) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(range(len(arrFileNames))): strFileNameFrom = strDirFrom + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> strFileNameTo = strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>): os.makedirs(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># As a new breed dir is created, copy 1st file # to "test" under name of that breed if not os.path.exists(working_path + "test/"): os.makedirs(working_path + "test/") strFileNameTo = working_path + "test/" + arrBreeds[i] + ".jpg" shutil.copy(strFileNameFrom, strFileNameTo) shutil.copy(strFileNameFrom, strFileNameTo)</span></span></code> </pre><br><br>  Wie Sie sehen k√∂nnen, kopieren wir nur eine Datei f√ºr jede Hunderasse als <i>Test</i> .  Au√üerdem erstellen wir beim Kopieren Unterordner, einen f√ºr jede Rasse.  Dementsprechend werden Fotos nach Rasse in Unterordner kopiert. <br><br>  Dies geschieht, weil Keras mit einem Verzeichnis √§hnlicher Struktur arbeiten kann und Bilddateien nach Bedarf und nicht alle gleichzeitig l√§dt, wodurch Speicherplatz gespart wird.  Es ist eine schlechte Idee, alle 15.000 Bilder gleichzeitig hochzuladen. <br><br>  Wir m√ºssen diese Funktion nur einmal aufrufen, da sie Bilder kopiert - und nicht mehr ben√∂tigt wird.  Dementsprechend m√ºssen wir es f√ºr die zuk√ºnftige Verwendung kommentieren: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Move the data in subfolders so we can # use the Keras ImageDataGenerator. # This way we can also later use Keras # Data augmentation features. # --- Uncomment once, to copy files --- #copyFileSet(working_path + "all_images/", # working_path + "train/", train_ids) #copyFileSet(working_path + "all_images/", # working_path + "valid/", valid_ids)</span></span></code> </pre><br><br>  Holen Sie sich eine Liste der Hunderassen: <br><br><pre> <code class="python hljs">breeds = np.unique(labels[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) map_characters = {} <span class="hljs-comment"><span class="hljs-comment">#{0:'none'} for i in range(len(breeds)): map_characters[i] = breeds[i] print("&lt;item&gt;" + breeds[i] + "&lt;/item&gt;") &gt;&gt;&gt; &lt;item&gt;affenpinscher&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;afghan_hound&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;african_hunting_dog&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;airedale&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;american_staffordshire_terrier&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;appenzeller&lt;/item&gt;</span></span></code> </pre><br><br><h3>  Bildverarbeitung </h3><br><br>  Wir werden die Keras-Bibliotheksfunktion namens ImageDataGenerators verwenden.  ImageDataGenerator kann das Bild verarbeiten, skalieren, drehen usw.  Es kann auch eine <i>Verarbeitungsfunktion</i> akzeptieren, die Bilder zus√§tzlich verarbeiten kann. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) <span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255... img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. #img = cv2.blur(img,(5,5)) return img_1[0]</span></span></code> </pre><br><br>  Beachten Sie den folgenden Code: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255...</span></span></code> </pre><br><br>  Wir k√∂nnen in ImageDataGenerator selbst normalisieren (Unterdaten im Bereich von 0-1 anstelle der urspr√ºnglichen 0-255).  Warum brauchen wir dann einen Pr√§prozessor?  Betrachten Sie als Beispiel den unscharfen Aufruf (auskommentiert, ich verwende ihn nicht): Dies ist dieselbe benutzerdefinierte Bildmanipulation, die beliebig sein kann.  Alles von Kontrast zu HDR. <br><br>  Wir werden zwei verschiedene ImageDataGenerators verwenden, einen f√ºr das Training und einen f√ºr die Validierung.  Der Unterschied besteht darin, dass wir f√ºr das Training Drehungen und Skalierungen ben√∂tigen, um die ‚ÄûVielfalt‚Äú der Daten zu erh√∂hen. F√ºr die Validierung ben√∂tigen wir sie jedoch nicht, zumindest nicht f√ºr diese Aufgabe. <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( preprocessing_function=preprocess, <span class="hljs-comment"><span class="hljs-comment">#rescale=1./255, # done in preprocess() # randomly rotate images (degrees, 0 to 30) rotation_range=30, # randomly shift images horizontally # (fraction of total width) width_shift_range=0.3, height_shift_range=0.3, # randomly flip images horizontal_flip=True, ,vertical_flip=False, zoom_range=0.3) val_datagen = ImageDataGenerator( preprocessing_function=preprocess) train_gen = train_datagen.flow_from_directory( working_path + "train/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical") val_gen = val_datagen.flow_from_directory( working_path + "valid/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical")</span></span></code> </pre><br><br><h3>  Erstellen eines neuronalen Netzwerks </h3><br><br>  Wie bereits erw√§hnt, werden wir verschiedene Arten von neuronalen Netzen erstellen.  Jedes Mal, wenn wir eine andere Funktion zum Erstellen aufrufen, schlie√üen Sie andere Dateien ein und bestimmen manchmal eine andere Bildgr√∂√üe.  Um zwischen verschiedenen Arten von neuronalen Netzen zu wechseln, m√ºssen wir den entsprechenden Code kommentieren / auskommentieren. <br><br>  Erstellen Sie zun√§chst ein "Vanille" CNN.  Es funktioniert nicht gut, weil ich beschlossen habe, keine Zeit mit dem Debuggen zu verschwenden, aber es bietet zumindest eine Grundlage, die entwickelt werden kann, wenn ein Wunsch besteht (normalerweise ist dies eine schlechte Idee, da vorab trainierte Netzwerke das beste Ergebnis liefern). <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelVanilla</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() <span class="hljs-comment"><span class="hljs-comment"># Note the (7, 7) here. This is one of technics # used to reduce memory use by the NN: we scan # the image in a larger steps. # Also note regularizers.l2: this technic is # used to prevent overfitting. The "0.001" here # is an empirical value and can be optimized. model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.001))) # Note the use of a standard CNN building blocks: # Conv2D - BatchNormalization - Activation # MaxPooling2D - Dropout # The last two are used to avoid overfitting, also, # MaxPooling2D reduces memory use. model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(16, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) # This is the end on "convolutional" part of CNN. # Now we need to transform multidementional # data into one-dim. array for a fully-connected # classifier: model.add(Flatten()) # And two layers of classifier itself (plus an # Activation layer in between): model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) model.add(Activation("relu")) model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) # We need to compile the resulting network. # Note that there are few parameters we can # try here: the best performing one is uncommented, # the rest is commented out for your reference. #model.compile(optimizer='rmsprop', # loss='categorical_crossentropy', # metrics=['accuracy']) #model.compile( # optimizer=keras.optimizers.RMSprop(lr=0.0005), # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #model.compile(optimizer='adadelta', # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.Adadelta(lr=1.0, # rho=0.95, epsilon=0.01, decay=0.01) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.RMSprop(lr=0.0005, # rho=0.9, epsilon=None, decay=0.0001) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) # model.summary() return(model)</span></span></code> </pre><br><br>  Wenn wir mithilfe von <i>Transferlernen</i> Netzwerke erstellen, √§ndert sich das Verfahren: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelMobileNetV2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, create the NN and load pre-trained # weights for it ('imagenet') # Note that we are not loading last layers of # the network (include_top=False), as we are # going to add layers of our own: base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Then attach our layers at the end. These are # to build "classifier" that makes sense of # the patterns previous layers provide: x = base_model.output x = Dense(512)(x) x = Activation('relu')(x) x = Dropout(0.5)(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Create a model model = Model(inputs=base_model.input, outputs=predictions) # We need to make sure that pre-trained # layers are not changed when we train # our classifier: # Either this: #model.layers[0].trainable = False # or that: for layer in base_model.layers: layer.trainable = False # As always, there are different possible # settings, I tried few and chose the best: # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Das Erstellen anderer Netzwerktypen erfolgt nach demselben Muster: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelResNet50</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> base_model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>, input_shape=(IMAGE_SIZE, IMAGE_SIZE, <span class="hljs-number"><span class="hljs-number">3</span></span>)) x = base_model.output x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>)(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) predictions = Dense(NUM_CLASSES, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=base_model.input, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#model.layers[0].trainable = False # model.compile(loss='categorical_crossentropy', # optimizer='adam', metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Achtung: Gewinner!  Dieser NN zeigte das beste Ergebnis: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelInceptionV3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Noch eine: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelNASNetMobile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = NASNetMobile(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Verschiedene Arten von neuronalen Netzen k√∂nnen f√ºr verschiedene Aufgaben verwendet werden.  Zus√§tzlich zu den Anforderungen an die Vorhersagegenauigkeit kann die Gr√∂√üe eine Rolle spielen (das mobile NN ist f√ºnfmal kleiner als Inception) und die Geschwindigkeit (wenn wir eine Echtzeitverarbeitung eines Videostreams ben√∂tigen, muss die Genauigkeit geopfert werden). <br><br><h3>  Neuronales Netzwerktraining </h3><br><br>  Zun√§chst <i>experimentieren</i> wir, damit wir die gespeicherten neuronalen Netze entfernen k√∂nnen, die wir jedoch nicht mehr verwenden.  Die folgende Funktion entfernt NN, falls vorhanden: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make sure that previous "best network" is deleted. def deleteSavedNet(best_weights_filepath): if(os.path.isfile(best_weights_filepath)): os.remove(best_weights_filepath) print("deleteSavedNet():File removed") else: print("deleteSavedNet():No file to remove")</span></span></code> </pre><br><br>  Die Art und Weise, wie wir neuronale Netze erstellen und l√∂schen, ist recht einfach und unkompliziert.  L√∂schen Sie zuerst.  Wenn Sie <i>delete</i> (nur) aufrufen, sollten Sie ber√ºcksichtigen, dass das Jupiter-Notizbuch √ºber eine Funktion zum Ausf√ºhren der Auswahl verf√ºgt. W√§hlen Sie nur das aus, was Sie verwenden m√∂chten, und f√ºhren Sie es aus. <br><br>  Dann erstellen wir ein neuronales Netzwerk, wenn seine Datei nicht vorhanden war, oder rufen das <i>Laden auf,</i> wenn es vorhanden ist: Nat√ºrlich k√∂nnen wir nicht "delete" aufrufen und dann erwarten, dass NN vorhanden ist. Um also ein gespeichertes neuronales Netzwerk zu verwenden, rufen Sie nicht <i>delete auf</i> . <br><br>  Mit anderen Worten, wir k√∂nnen je nach Situation und dem, mit dem wir gerade experimentieren, ein neues NN erstellen oder das vorhandene verwenden.  Ein einfaches Szenario: Wir haben ein neuronales Netzwerk trainiert und sind dann in den Urlaub gefahren.  Sie sind zur√ºckgekehrt, und Google hat die Sitzung festgenagelt. Daher m√ºssen wir die zuvor gespeicherte laden: Kommentieren Sie "L√∂schen" aus und kommentieren Sie "Laden" aus. <br><br><pre> <code class="python hljs">deleteSavedNet(working_path + strModelFileName) <span class="hljs-comment"><span class="hljs-comment">#if not os.path.exists(working_path + "models"): # os.makedirs(working_path + "models") # #if not os.path.exists(working_path + # strModelFileName): # model = createModelResNet50() model = createModelInceptionV3() # model = createModelMobileNetV2() # model = createModelNASNetMobile() #else: # model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  <b>Checkpoints</b> sind ein sehr wichtiges Element unseres Programms.  Wir k√∂nnen eine Reihe von Funktionen erstellen, die am Ende jeder Trainings√§ra aufgerufen werden sollen, und diese an den Checkpoint √ºbergeben.  Sie k√∂nnen beispielsweise ein neuronales Netzwerk speichern, <i>wenn</i> Ergebnisse angezeigt werden, die besser sind als die bereits gespeicherten. <br><br><pre> <code class="python hljs">checkpoint = ModelCheckpoint(working_path + strModelFileName, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, save_weights_only=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) callbacks_list = [ checkpoint ]</code> </pre><br><br>  Schlie√ülich unterrichten wir das neuronale Netzwerk auf dem Trainingsset: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Calculate sizes of training and validation sets STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size STEP_SIZE_VALID=val_gen.n//val_gen.batch_size # Set to False if we are experimenting with # some other part of code, use history that # was calculated before (and is still in # memory bDoTraining = True if bDoTraining == True: # model.fit_generator does the actual training # Note the use of generators and callbacks # that were defined earlier history = model.fit_generator(generator=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=val_gen, validation_steps=STEP_SIZE_VALID, epochs=EPOCHS, callbacks=callbacks_list) # --- After fitting, load the best model # This is important as otherwise we'll # have the LAST model loaded, not necessarily # the best one. model.load_weights(working_path + strModelFileName) # --- Presentation part # summarize history for accuracy plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['acc', 'val_acc'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['loss', 'val_loss'], loc='upper left') plt.show() # As grid optimization of NN would take too long, # I did just few tests with different parameters. # Below I keep results, commented out, in the same # code. As you can see, Inception shows the best # results: # Inception: # adam: val_acc 0.79393 # sgd: val_acc 0.80892 # Mobile: # adam: val_acc 0.65290 # sgd: Epoch 00015: val_acc improved from 0.67584 to 0.68469 # sgd-30 epochs: 0.68 # NASNetMobile, adam: val_acc did not improve from 0.78335 # NASNetMobile, sgd: 0.8</span></span></code> </pre><br><br>  Die Diagramme f√ºr Genauigkeit und Verlust f√ºr die besten Konfigurationen lauten wie folgt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/97d/9cc/f0e97d9ccdc8f8ed9e44ddba02cf1f8d.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/612/e09/8b0/612e098b088979768d1cc66c2f6972bc.png"><br><br>  Wie Sie sehen k√∂nnen, lernt das neuronale Netzwerk und ist nicht schlecht. <br><br><h3>  Testen neuronaler Netze </h3><br><br>  Nach Abschluss des Trainings m√ºssen wir das Ergebnis testen.  Daf√ºr pr√§sentiert NN Bilder, die sie noch nie gesehen hat - die, die wir in den Testordner kopiert haben - eines f√ºr jede Hunderasse. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># --- Test j = 0 # Final cycle performs testing on the entire # testing set. for file_name in os.listdir( working_path + "test/"): img = image.load_img(working_path + "test/" + file_name); img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict_on_batch(img_1) # get 5 best predictions y_pred_ids = y_pred[0].argsort()[-5:][::-1] print(file_name) for i in range(len(y_pred_ids)): print("\n\t" + map_characters[y_pred_ids[i]] + " (" + str(y_pred[0][y_pred_ids[i]]) + ")") print("--------------------\n") j = j + 1</span></span></code> </pre><br><br><h3>  Exportieren Sie ein neuronales Netzwerk in eine Java-Anwendung </h3><br><br>  Zun√§chst m√ºssen wir das Laden des neuronalen Netzwerks von der Festplatte organisieren.  Der Grund ist klar: Der Export findet in einem anderen Codeblock statt, daher werden wir den Export h√∂chstwahrscheinlich separat starten - wenn das neuronale Netzwerk in seinen optimalen Zustand gebracht wird.  Das hei√üt, unmittelbar vor dem Export werden wir im selben Programmlauf das Netzwerk nicht trainieren.  Wenn Sie den hier gezeigten Code verwenden, gibt es keinen Unterschied, das optimale Netzwerk wurde f√ºr Sie ausgew√§hlt.  Aber wenn Sie etwas Eigenes lernen, ist es Zeitverschwendung, alles vor dem Speichern neu zu trainieren, wenn Sie zuvor alles gespeichert haben. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Test: load and run model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  Aus dem gleichen Grund - um nicht √ºber den Code zu springen - f√ºge ich hier die f√ºr den Export erforderlichen Dateien ein.  Niemand st√∂rt Sie, sie an den Anfang des Programms zu verschieben, wenn Ihr Sinn f√ºr Sch√∂nheit dies erfordert: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre><br><br>  Ein kleiner Test nach dem Laden eines neuronalen Netzwerks, um sicherzustellen, dass alles geladen ist - funktioniert: <br><br><pre> <code class="python hljs">img = image.load_img(working_path + <span class="hljs-string"><span class="hljs-string">"test/affenpinscher.jpg"</span></span>) <span class="hljs-comment"><span class="hljs-comment">#basset.jpg") img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict(img_1) Y_pred_classes = np.argmax(y_pred,axis = 1) # print(y_pred) fig, ax = plt.subplots() ax.imshow(img) ax.axis('off') ax.set_title(map_characters[Y_pred_classes[0]]) plt.show()</span></span></code> </pre><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/032/846/05c03284674e4337a2e5a3ba617634dd.png" alt="Bild"><br><br>  Als n√§chstes m√ºssen wir die Namen der Eingabe- und Ausgabeschichten des Netzwerks abrufen (entweder diese oder die Erstellungsfunktion, wir m√ºssen die Schichten explizit "benennen", was wir nicht getan haben). <br><br><pre> <code class="python hljs">model.summary() &gt;&gt;&gt; Layer (type) &gt;&gt;&gt; ====================== &gt;&gt;&gt; input_7 (InputLayer) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; conv2d_283 (Conv2D) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; ... &gt;&gt;&gt; dense_14 (Dense) &gt;&gt;&gt; ====================== &gt;&gt;&gt; Total params: <span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">913</span></span>,<span class="hljs-number"><span class="hljs-number">432</span></span> &gt;&gt;&gt; Trainable params: <span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">110</span></span>,<span class="hljs-number"><span class="hljs-number">648</span></span> &gt;&gt;&gt; Non-trainable params: <span class="hljs-number"><span class="hljs-number">21</span></span>,<span class="hljs-number"><span class="hljs-number">802</span></span>,<span class="hljs-number"><span class="hljs-number">784</span></span></code> </pre><br><br>  Wir werden die Namen der Eingabe- und Ausgabeebenen sp√§ter verwenden, wenn wir das neuronale Netzwerk in eine Java-Anwendung importieren. <br><br>  Ein weiterer Code, der das Netzwerk durchstreift, um diese Daten abzurufen: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">print_graph_nodes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> g = tf.GraphDef() g.ParseFromString(open(filename, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>).read()) print() print(filename) print(<span class="hljs-string"><span class="hljs-string">"=======================INPUT==================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'input'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"=======================OUTPUT=================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'output'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"===================KERAS_LEARNING=============="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'keras_learning_phase'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"==============================================="</span></span>) print() <span class="hljs-comment"><span class="hljs-comment">#def get_script_path(): # return os.path.dirname(os.path.realpath(sys.argv[0]))</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber ich mag ihn nicht und ich empfehle ihn nicht. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der folgende Code exportiert das Keras Neural Network in das </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pb-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Format, das wir von Android erfassen werden.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">keras_to_tensorflow</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keras_model, output_dir, model_name,out_prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"output_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, log_tensorboard=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> os.path.exists(output_dir) == <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: os.mkdir(output_dir) out_nodes = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(keras_model.outputs)): out_nodes.append(out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) tf.identity(keras_model.output[i], out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) sess = K.get_session() <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> graph_util <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework graph_io init_graph = sess.graph.as_graph_def() main_graph = graph_util.convert_variables_to_constants( sess, init_graph, out_nodes) graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> log_tensorboard: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> import_pb_to_tensorboard import_pb_to_tensorboard.import_to_tensorboard( os.path.join(output_dir, model_name), output_dir)</code> </pre><br><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aufruf dieser Funktionen zum Exportieren eines neuronalen Netzwerks: </font></font><br><br></p><pre> <code class="python hljs">model = load_model(working_path + strModelFileName) keras_to_tensorflow(model, output_dir=working_path + strModelFileName, model_name=working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>) print_graph_nodes(working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>)</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die letzte Zeile gibt die Struktur des resultierenden neuronalen Netzwerks aus. </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Erstellen einer Android-Anwendung mithilfe eines neuronalen Netzwerks </font></font></h2><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Export neuronaler Netze in Android ist gut formalisiert und sollte keine Schwierigkeiten verursachen. </font><font style="vertical-align: inherit;">Es gibt wie immer verschiedene M√∂glichkeiten, die wir (zum Zeitpunkt des Schreibens) am beliebtesten verwenden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zun√§chst verwenden wir Android Studio, um ein neues Projekt zu erstellen. </font><font style="vertical-align: inherit;">Wir werden "Ecken abschneiden", weil unsere Aufgabe kein Android-Tutorial ist. </font><font style="vertical-align: inherit;">Die Anwendung enth√§lt also nur eine Aktivit√§t. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b3/76e/997/6b376e997b34f45359c46923f6613d60.png" alt="Bild"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sie sehen k√∂nnen, haben wir den Ordner "Assets" hinzugef√ºgt und unser neuronales Netzwerk in dieses kopiert (das zuvor exportierte).</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gradle-Datei </font></font></h3><br><br>       .  ,     <i>tensorflow-android</i> .    ,    Tensorflow (, , Keras)  Java: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a16/091/fab/a16091fab2166f834827812611142d26.png" alt="Bild"><br><br>     : <i>versionCode</i>  <i>versionName</i> .    ,        Google Play.     gdadle (, 1 -&gt; 2 -&gt; 3...)     ,     ¬´   ¬ª. <br><br><h3>  </h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zuallererst wird unsere Anwendung "schwer" sein - 100 Mb Neural Network wird leicht in den Speicher moderner Mobiltelefone passen, aber es ist definitiv eine schlechte Idee, f√ºr jedes von Facebook "geteilte" Foto eine separate Instanz zu √∂ffnen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daher verbieten wir, mehr als eine Instanz unserer Anwendung zu erstellen:</font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">activity</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">".MainActivity"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:launchMode</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"singleTask"</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Durch Hinzuf√ºgen von </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">android: launchMode = "singleTask"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu MainActivity weisen wir Android an, eine vorhandene Kopie der Anwendung zu √∂ffnen (zu aktivieren), anstatt eine andere Instanz zu erstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann m√ºssen wir unsere Anwendung in die Liste aufnehmen, die das System anzeigt, wenn jemand das Bild ‚Äûteilt‚Äú:</font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Send action required to display activity in share list --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">action</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.action.SEND"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Make activity default to launch --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">category</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.category.DEFAULT"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Mime type ie what can be shared with this activity only image and text --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">data</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:mimeType</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"image/*"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Schlie√ülich m√ºssen wir Funktionen und Berechtigungen anfordern, die unsere Anwendung verwenden wird: </font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-feature</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.hardware.camera"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:required</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"true"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">= </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.WRITE_EXTERNAL_STORAGE"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.READ_PHONE_STATE"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tools:node</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"remove"</span></span></span><span class="hljs-tag"> /&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wenn Sie mit der Programmierung f√ºr Android vertraut sind, sollte dieser Teil keine Fragen aufwerfen. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Layout-Anwendung. </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir werden zwei Layouts erstellen, eines f√ºr Hochformat und eines f√ºr Querformat. </font><font style="vertical-align: inherit;">So sieht das </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Portrait-Layout</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aus </font><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was wir hinzuf√ºgen werden: ein gro√ües Feld (Ansicht) zum Anzeigen des Bildes, eine nervige Liste von Anzeigen (angezeigt, wenn die Schaltfl√§che mit einem Knochen gedr√ºckt wird), die Hilfeschaltfl√§che, Schaltfl√§chen zum Herunterladen eines Bilds aus der Datei / Galerie und zum Aufnehmen von der Kamera und schlie√ülich (zun√§chst) versteckt) Schaltfl√§che "Verarbeiten" f√ºr die Bildverarbeitung. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f71/882/81f/f7188281ff581965c20c7e818cb0fd77.png" alt="Bild"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Aktivit√§t selbst enth√§lt die gesamte Logik zum Ein- und Ausblenden sowie zum Aktivieren / Deaktivieren von Schaltfl√§chen, abh√§ngig vom Status der Anwendung.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hauptaktivit√§t </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Diese Aktivit√§t erbt (erweitert) die Standard-Android-Aktivit√§t: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MainActivity</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Activity</span></span></span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Betrachten Sie den Code, der f√ºr den Betrieb des neuronalen Netzwerks verantwortlich ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zun√§chst akzeptiert das neuronale Netzwerk Bitmap. </font><font style="vertical-align: inherit;">Dies ist zun√§chst eine gro√üe Bitmap (beliebiger Gr√∂√üe) von der Kamera oder aus einer Datei (m_bitmap), dann transformieren wir sie und f√ºhren zu den Standardpixeln mit 256 x 256 Pixel (m_bitmapForNn). </font><font style="vertical-align: inherit;">Wir speichern auch die Bitmap-Gr√∂√üe (256) in einer Konstanten:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmap = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmapForNn = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> m_nImageSize = <span class="hljs-number"><span class="hljs-number">256</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir m√ºssen dem neuronalen Netzwerk die Namen der Eingabe- und Ausgabeschichten mitteilen. </font><font style="vertical-align: inherit;">Wir haben sie fr√ºher erhalten (siehe Auflistung), aber denken Sie daran, dass sie in Ihrem Fall unterschiedlich sein k√∂nnen:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String INPUT_NAME = <span class="hljs-string"><span class="hljs-string">"input_7_1"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String OUTPUT_NAME = <span class="hljs-string"><span class="hljs-string">"output_1"</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann deklarieren wir eine Variable, die das TensofFlow-Objekt enth√§lt. </font><font style="vertical-align: inherit;">Au√üerdem speichern wir den Pfad zur neuronalen Netzwerkdatei (die in Assets liegt):</font></font><br><br><p></p><pre><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">private TensorFlowInferenceInterface tf;</font></font><font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
private String MODEL_PATH = </font></font><font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
	"file: ///android_asset/dogs.pb";</font></font><font></font>
</pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir speichern die Hunderassen in der Liste, damit sie sp√§ter dem Benutzer angezeigt werden und nicht die Array-Indizes: </font></font><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String[] m_arrBreedsArray;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zun√§chst haben wir Bitmap heruntergeladen. </font><font style="vertical-align: inherit;">Das neuronale Netzwerk erwartet jedoch ein Array von RGB-Werten, und seine Ausgabe ist ein Array von Wahrscheinlichkeiten, dass diese Rasse das ist, was im Bild gezeigt wird. </font><font style="vertical-align: inherit;">Dementsprechend m√ºssen wir zwei weitere Arrays hinzuf√ºgen (beachten Sie, dass 120 die Anzahl der Hunderassen ist, die in unseren Trainingsdaten enthalten sind):</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrPrediction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">120</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrInput = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tensorflow-Inferenzbibliothek herunterladen: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> { System.loadLibrary(<span class="hljs-string"><span class="hljs-string">"tensorflow_inference"</span></span>); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Da neuronale Netzwerkoperationen Zeit ben√∂tigen, m√ºssen sie in einem separaten Thread ausgef√ºhrt werden. Andernfalls besteht die M√∂glichkeit, dass wir eine Systemmeldung erhalten, dass die Anwendung nicht antwortet, ganz zu schweigen von einem unzufriedenen Benutzer. </font></font><br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PredictionTask</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AsyncTask</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onPreExecute</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onPreExecute(); } <span class="hljs-comment"><span class="hljs-comment">// --- @Override protected Void doInBackground(Void... params) { try { # We get RGB values packed in integers # from the Bitmap, then break those # integers into individual triplets m_arrInput = new float[ m_nImageSize * m_nImageSize * 3]; int[] intValues = new int[ m_nImageSize * m_nImageSize]; m_bitmapForNn.getPixels(intValues, 0, m_nImageSize, 0, 0, m_nImageSize, m_nImageSize); for (int i = 0; i &lt; intValues.length; i++) { int val = intValues[i]; m_arrInput[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 2] = (val &amp; 0xFF) / 255f; } // --- tf = new TensorFlowInferenceInterface( getAssets(), MODEL_PATH); //Pass input into the tensorflow tf.feed(INPUT_NAME, m_arrInput, 1, m_nImageSize, m_nImageSize, 3); //compute predictions tf.run(new String[]{OUTPUT_NAME}, false); //copy output into PREDICTIONS array tf.fetch(OUTPUT_NAME, m_arrPrediction); } catch (Exception e) { e.getMessage(); } return null; } // --- @Override protected void onPostExecute(Void result) { super.onPostExecute(result); // --- enableControls(true); // --- tf = null; m_arrInput = null; # strResult contains 5 lines of text # with most probable dog breeds and # their probabilities m_strResult = ""; # What we do below is sorting the array # by probabilities (using map) # and getting in reverse order) the # first five entries TreeMap&lt;Float, Integer&gt; map = new TreeMap&lt;Float, Integer&gt;( Collections.reverseOrder()); for(int i = 0; i &lt; m_arrPrediction.length; i++) map.put(m_arrPrediction[i], i); int i = 0; for (TreeMap.Entry&lt;Float, Integer&gt; pair : map.entrySet()) { float key = pair.getKey(); int idx = pair.getValue(); String strBreed = m_arrBreedsArray[idx]; m_strResult += strBreed + ": " + String.format("%.6f", key) + "\n"; i++; if (i &gt; 5) break; } m_txtViewBreed.setVisibility(View.VISIBLE); m_txtViewBreed.setText(m_strResult); } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In onCreate () der MainActivity m√ºssen wir den onClickListener f√ºr die Schaltfl√§che "Process" hinzuf√ºgen: </font></font><br><br><pre> <code class="java hljs">m_btn_process.setOnClickListener(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> View.OnClickListener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onClick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(View v)</span></span></span><span class="hljs-function"> </span></span>{ processImage(); } });</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hier ruft processImage () nur den oben beschriebenen Thread auf: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">processImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { enableControls(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); <span class="hljs-comment"><span class="hljs-comment">// --- PredictionTask prediction_task = new PredictionTask(); prediction_task.execute(); } catch (Exception e) { e.printStackTrace(); } }</span></span></code> </pre><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Zus√§tzliche Hinweise </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir planen nicht, die Details der UI-Programmierung f√ºr Android zu diskutieren, da dies sicherlich nicht f√ºr die Portierung neuronaler Netze gilt. </font><font style="vertical-align: inherit;">Eines ist jedoch noch erw√§hnenswert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir die Erstellung zus√§tzlicher Instanzen unserer Anwendung verhindert haben, haben wir auch die normale Reihenfolge der Erstellung und L√∂schung von Aktivit√§ten (Kontrollfluss) verletzt: Wenn Sie ein Bild von Facebook ‚Äûfreigeben‚Äú und dann ein anderes freigeben, wird die Anwendung nicht neu gestartet. </font><font style="vertical-align: inherit;">Dies bedeutet, dass die ‚Äûtraditionelle‚Äú Methode zum Abfangen √ºbertragener Daten in onCreate nicht ausreicht, da onCreate nicht aufgerufen wird. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So l√∂sen Sie dieses Problem: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Rufen Sie in onCreate in MainActivity die Funktion onSharedIntent auf:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onCreate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( Bundle savedInstanceState)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onCreate(savedInstanceState); .... onSharedIntent(); ....</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir f√ºgen auch einen Handler f√ºr onNewIntent hinzu: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onNewIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Intent intent)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onNewIntent(intent); setIntent(intent); onSharedIntent(); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hier ist die onSharedIntent-Funktion selbst: </font></font><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onSharedIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ Intent receivedIntent = getIntent(); String receivedAction = receivedIntent.getAction(); String receivedType = receivedIntent.getType(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (receivedAction.equals(Intent.ACTION_SEND)) { <span class="hljs-comment"><span class="hljs-comment">// If mime type is equal to image if (receivedType.startsWith("image/")) { m_txtViewBreed.setText(""); m_strResult = ""; Uri receivedUri = receivedIntent.getParcelableExtra( Intent.EXTRA_STREAM); if (receivedUri != null) { try { Bitmap bitmap = MediaStore.Images.Media.getBitmap( this.getContentResolver(), receivedUri); if(bitmap != null) { m_bitmap = bitmap; m_picView.setImageBitmap(m_bitmap); storeBitmap(); enableControls(true); } } catch (Exception e) { e.printStackTrace(); } } } } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Jetzt verarbeiten wir die √ºbertragenen Daten in onCreate (wenn sich die Anwendung nicht im Speicher befand) oder in onNewIntent (wenn sie fr√ºher gestartet wurde). </font></font><br><br><br><br><br>  Viel Gl√ºck<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Ihnen der Artikel gefallen hat, "m√∂gen" Sie ihn bitte auf alle m√∂glichen Arten. Es gibt auch "soziale" Schaltfl√§chen auf der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Website</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de448316/">https://habr.com/ru/post/de448316/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de448300/index.html">Konferenz mailto: CLOUD - √ºber Wolken und Umgebung</a></li>
<li><a href="../de448302/index.html">Durch die Sicherheitsanf√§lligkeit in AdBlock- und uBlock-Filtern kann beliebiger Code auf der Benutzerseite ausgef√ºhrt werden</a></li>
<li><a href="../de448304/index.html">Das Buch "Vue.js in Aktion"</a></li>
<li><a href="../de448308/index.html">Data Science Digest (April 2019)</a></li>
<li><a href="../de448310/index.html">Schreiben eines Telegrammbots in Python mit der Telebot-Bibliothek Teil 1</a></li>
<li><a href="../de448320/index.html">Warum Silizium und warum CMOS?</a></li>
<li><a href="../de448322/index.html">C ++ Russia 2019: kostenlose √úbertragung der ersten Halle und ein wenig dar√ºber, was auf der Konferenz sein wird</a></li>
<li><a href="../de448324/index.html">Erstellen Sie prozedurale Planetenkugeln</a></li>
<li><a href="../de448326/index.html">Durchschauen. Wie kann man F√§cher studieren, ohne sie zu brechen?</a></li>
<li><a href="../de448328/index.html">In Moskau wird ein Drucker gezeigt, der Organe und Gewebe druckt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>