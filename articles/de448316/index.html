<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤷🏻 👨🏻‍⚕️ 🤷🏿 Bestimmung der Hunderasse: Ein vollständiger Entwicklungszyklus, von einem neuronalen Netzwerk in Python bis zu einer Anwendung bei Google Play 👨🏽‍💼 👨🏼‍🎨 🚃</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fortschritte auf dem Gebiet der neuronalen Netze im Allgemeinen und der Mustererkennung im Besonderen haben dazu geführt, dass es den Anschein hat, al...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bestimmung der Hunderasse: Ein vollständiger Entwicklungszyklus, von einem neuronalen Netzwerk in Python bis zu einer Anwendung bei Google Play</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/448316/">  Fortschritte auf dem Gebiet der neuronalen Netze im Allgemeinen und der Mustererkennung im Besonderen haben dazu geführt, dass es den Anschein hat, als sei die Erstellung einer neuronalen Netzanwendung für die Arbeit mit Bildern eine Routineaufgabe.  In gewissem Sinne ist es so - wenn Sie auf eine Idee zur Mustererkennung gekommen sind, zweifeln Sie nicht daran, dass jemand bereits so etwas geschrieben hat.  Sie müssen lediglich den entsprechenden Code in Google finden und vom Autor „kompilieren“. <br><br>  Es gibt jedoch immer noch zahlreiche Details, die die Aufgabe nicht so sehr unlösbar machen, sondern ... langweilig, würde ich sagen.  Es dauert zu lange, besonders wenn Sie ein Anfänger sind, der Schritt für Schritt Führung benötigt, ein Projekt, das direkt vor Ihren Augen durchgeführt und von Anfang bis Ende abgeschlossen wird.  Ohne das in solchen Fällen übliche Ausreden „diesen offensichtlichen Teil überspringen“. <br><br>  In diesem Artikel werden wir uns mit der Aufgabe befassen, einen Hunderassen-Identifikator zu erstellen: Wir werden ein neuronales Netzwerk erstellen und trainieren, es dann auf Java für Android portieren und auf Google Play veröffentlichen. <br><br>  Wenn Sie sich das fertige Ergebnis ansehen möchten, finden Sie es hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeuroDog App</a> bei Google Play. <br><br>  Website mit meiner Robotik (in Bearbeitung): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">robotics.snowcron.com</a> . <br>  Website mit dem Programm selbst, einschließlich eines Handbuchs: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeuroDog User Guide</a> . <br><br>  Und hier ist ein Screenshot des Programms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/186/b91/457/186b914572170b01446ed1d722bce200.png" alt="Bild"><br><br><a name="habracut"></a><br><br><h3>  Erklärung des Problems </h3><br><br>  Wir werden Keras verwenden: eine Google-Bibliothek für die Arbeit mit neuronalen Netzen.  Dies ist eine Bibliothek auf hoher Ebene, was bedeutet, dass sie im Vergleich zu den mir bekannten Alternativen einfacher zu verwenden ist.  Wenn überhaupt - es gibt viele Lehrbücher über Keras im Netzwerk von hoher Qualität. <br><br>  Wir werden CNN - Convolutional Neural Networks verwenden.  CNN (und darauf basierende erweiterte Konfigurationen) sind der De-facto-Standard bei der Bilderkennung.  Gleichzeitig ist das Trainieren eines solchen Netzwerks nicht immer einfach: Sie müssen die richtige Netzwerkstruktur und Trainingsparameter auswählen (all diese Lernraten, Impulse, L1 und L2 usw.).  Die Aufgabe erfordert erhebliche Rechenressourcen. Um sie zu lösen, schlagen ALLE Parameter fehl. <br><br>  Dies ist einer von mehreren Gründen, warum sie in den meisten Fällen das sogenannte „Transferwissen“ anstelle des sogenannten „Vanille“ -Ansatzes verwenden.  Transfer Knowlege verwendet ein neuronales Netzwerk, das von jemandem vor uns (z. B. Google) trainiert wurde, und normalerweise für eine ähnliche, aber immer noch andere Aufgabe.  Wir nehmen die ersten Ebenen daraus, ersetzen die letzten Ebenen durch unseren eigenen Klassifikator - und es funktioniert und es funktioniert großartig. <br><br>  Ein solches Ergebnis mag zunächst überraschend sein: Wie kommt es, dass wir ein Google-Netzwerk genutzt haben, um Katzen von Stühlen zu unterscheiden, und das Hunderassen für uns erkennt?  Um zu verstehen, wie dies geschieht, müssen Sie die Grundprinzipien der Arbeit von Deep Neural Networks verstehen, einschließlich derer, die für die Mustererkennung verwendet werden. <br><br>  Wir haben dem Netzwerk ein Bild (also ein Array von Zahlen) als Eingabe „zugeführt“.  Die erste Ebene analysiert das Bild auf einfache Muster wie „horizontale Linie“, „Bogen“ usw.  Die nächste Schicht empfängt diese Muster als Eingabe und erzeugt Muster zweiter Ordnung wie „Fell“, „Augenwinkel“ ... Letztendlich erhalten wir ein Puzzle, aus dem wir den Hund rekonstruieren können: Wolle, zwei Augen und eine menschliche Hand in Zähnen. <br><br>  All dies wurde mit Hilfe von vorgefertigten Schichten durchgeführt, die von uns (zum Beispiel von Google) erhalten wurden.  Als nächstes fügen wir unsere Ebenen hinzu und bringen ihnen bei, Rasseninformationen aus diesen Mustern zu extrahieren.  Klingt logisch. <br><br>  Zusammenfassend werden wir in diesem Artikel sowohl "Vanilla" CNN als auch verschiedene "Transfer Learning" -Varianten verschiedener Netzwerktypen erstellen.  Was "Vanille" betrifft: Ich werde es erstellen, aber ich habe nicht vor, es durch Auswahl von Parametern zu konfigurieren, da es viel einfacher ist, "vorab trainierte" Netzwerke zu trainieren und zu konfigurieren. <br><br>  Da wir unserem neuronalen Netzwerk beibringen wollen, Hunderassen zu erkennen, müssen wir ihm Proben verschiedener Rassen „zeigen“.  Glücklicherweise gibt es hier eine Reihe von Fotos, die für eine ähnliche Aufgabe erstellt wurden (das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Original ist hier</a> ). <br><br>  Dann plane ich, das beste der empfangenen Netzwerke für Android zu portieren.  Das Portieren von Kerasov-Netzwerken auf Android ist relativ einfach, gut formalisiert und wir werden alle erforderlichen Schritte ausführen, sodass es nicht schwierig sein wird, diesen Teil zu reproduzieren. <br><br>  Dann werden wir all dies auf Google Play veröffentlichen.  Natürlich wird Google Widerstand leisten, daher werden zusätzliche Tricks verwendet.  Beispielsweise ist die Größe unserer Anwendung (aufgrund eines sperrigen neuronalen Netzwerks) größer als die zulässige Größe der von Google Play akzeptierten Android-APK: Wir müssen Bundles verwenden.  Darüber hinaus zeigt Google unsere Anwendung nicht in den Suchergebnissen an. Dies kann behoben werden, indem Such-Tags in der Anwendung registriert werden, oder warten Sie einfach ... ein oder zwei Wochen. <br><br>  Als Ergebnis erhalten wir eine voll funktionsfähige "kommerzielle" (in Anführungszeichen, wie es kostenlos ausgelegt ist) Anwendung für Android und unter Verwendung neuronaler Netze. <br><br><h3>  Entwicklungsumgebung </h3><br><br>  Sie können für Keras auf verschiedene Arten programmieren, abhängig vom verwendeten Betriebssystem (Ubuntu empfohlen), dem Vorhandensein oder Fehlen einer Grafikkarte usw.  Es gibt nichts Schlechtes an der Entwicklung auf dem lokalen Computer (und entsprechend seiner Konfiguration), außer dass dies nicht der einfachste Weg ist. <br><br>  Die Installation und Konfiguration einer großen Anzahl von Tools und Bibliotheken nimmt zunächst Zeit in Anspruch. Wenn neue Versionen veröffentlicht werden, müssen Sie erneut Zeit aufwenden.  Zweitens erfordern neuronale Netze eine große Rechenleistung für das Training.  Sie können diesen Vorgang beschleunigen (um das 10-fache oder mehr), wenn Sie eine GPU verwenden. Zum Zeitpunkt des Schreibens dieses Artikels kosten die für diese Arbeit am besten geeigneten GPUs 2.000 bis 7.000 US-Dollar.  Und ja, sie müssen auch konfiguriert werden. <br><br>  Also werden wir den anderen Weg gehen.  Tatsache ist, dass Google armen Igeln wie uns erlaubt, GPUs aus ihrem Cluster zu verwenden - kostenlos, für Berechnungen in Bezug auf neuronale Netze, bietet es auch eine vollständig konfigurierte Umgebung, alles zusammen wird dies als Google Colab bezeichnet.  Der Dienst bietet Ihnen Zugriff auf Jupiter Notebook mit Python, Keras und einer Vielzahl anderer bereits konfigurierter Bibliotheken.  Sie müssen lediglich ein Google-Konto einrichten (ein Google Mail-Konto einrichten, damit Sie auf alles andere zugreifen können). <br><br>  Im Moment kann Colab hier eingestellt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> , aber wenn Sie Google kennen, kann sich dies jederzeit ändern.  Google einfach Google Colab. <br><br>  Das offensichtliche Problem bei der Verwendung von Colab ist, dass es sich um einen WEB-Dienst handelt.  Wie greifen wir auf unsere Daten zu?  Speichern Sie das neuronale Netzwerk nach dem Training, laden Sie beispielsweise für unsere Aufgabe spezifische Daten herunter und so weiter? <br><br>  Es gibt verschiedene (zum Zeitpunkt des Schreibens dieses Artikels - drei) verschiedene Möglichkeiten, wir verwenden die, die ich für am bequemsten halte - wir verwenden Google Drive. <br><br>  Google Drive ist ein Cloud-basierter Datenspeicher, der ähnlich wie eine normale Festplatte funktioniert und auf Google Colab abgebildet werden kann (siehe Code unten).  Danach können Sie damit arbeiten, als würden Sie mit Dateien auf einer lokalen Festplatte arbeiten.  Das heißt, um auf die Fotos von Hunden zuzugreifen, um unser neuronales Netzwerk zu trainieren, müssen wir sie auf Google Drive hochladen, das ist alles. <br><br><h2>  Erstellen und Trainieren eines neuronalen Netzwerks </h2><br><br>  Unten gebe ich den Code in Python Block für Block (aus dem Jupiter-Notizbuch).  Sie können diesen Code in Ihr Jupiter-Notizbuch kopieren und auch Block für Block ausführen, da Blöcke unabhängig voneinander ausgeführt werden können (natürlich können im späten Block definierte Variablen im späten Block erforderlich sein, dies ist jedoch eine offensichtliche Abhängigkeit). <br><br><h3>  Initialisierung </h3><br><br>  Lassen Sie uns zunächst Google Drive einbinden.  Nur zwei Zeilen.  Dieser Code sollte nur einmal in einer Colab-Sitzung ausgeführt werden (z. B. alle 6 Stunden).  Wenn Sie es ein zweites Mal aufrufen, während die Sitzung noch "aktiv" ist, wird es übersprungen, da das Laufwerk bereits bereitgestellt ist. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> drive drive.mount(<span class="hljs-string"><span class="hljs-string">'/content/drive/'</span></span>)</code> </pre> <br><br>  Beim ersten Start werden Sie gebeten, Ihre Absichten zu bestätigen, es gibt nichts Kompliziertes.  So sieht es aus: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Go to this URL <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a browser: ... &gt;&gt;&gt; Enter your authorization code: &gt;&gt;&gt; ·········· &gt;&gt;&gt; Mounted at /content/drive/</code> </pre><br><br>  Ein vollständig standardmäßiger <i>Include-</i> Abschnitt;  Es ist möglich, dass einige der enthaltenen Dateien nicht benötigt werden.  Da ich verschiedene neuronale Netze testen werde, müssen Sie einige der enthaltenen Module für bestimmte Arten von neuronalen Netzen kommentieren / auskommentieren: Um beispielsweise InceptionV3 NN zu verwenden, die Aufnahme von InceptionV3 auskommentieren und beispielsweise ResNet50 auskommentieren.  Oder auch nicht: Alles, was sich daraus ändert, ist die Größe des verwendeten Speichers, und das ist nicht sehr stark. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> regularizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Conv2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, GlobalAveragePooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback, EarlyStopping <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReduceLROnPlateau <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.vgg16 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> inception_v3_preprocessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.mobilenetv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MobileNetV2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.nasnet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NASNetMobile</code> </pre><br><br>  In Google Drive erstellen wir einen Ordner für unsere Dateien.  Die zweite Zeile zeigt den Inhalt an: <br><br><pre> <code class="python hljs">working_path = <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data/"</span></span> !ls <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data"</span></span> &gt;&gt;&gt; all_images labels.csv models test train valid</code> </pre><br><br>  Wie Sie sehen können, werden die Fotos der Hunde (kopiert aus dem Stanford-Datensatz (siehe oben) in Google Drive) zuerst im Ordner <i>all_images gespeichert</i> .  Später werden wir sie in die <i>Zug-, Gültigkeits-</i> und <i>Testverzeichnisse</i> kopieren.  Wir speichern trainierte Modelle im Modellordner.  Die Datei labels.csv ist Teil des Datensatzes mit Fotos und enthält eine Korrespondenztabelle mit den Namen von Bildern und Hunderassen. <br><br>  Es gibt viele Tests, die Sie ausführen können, um zu verstehen, was genau wir für die vorübergehende Verwendung von Google erhalten haben.  Zum Beispiel: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Is GPU Working? import tensorflow as tf tf.test.gpu_device_name() &gt;&gt;&gt; '/device:GPU:0'</span></span></code> </pre><br><br>  Wie Sie sehen, ist die GPU wirklich verbunden. Wenn nicht, müssen Sie diese Option in den Jupiter Notebook-Einstellungen suchen und aktivieren. <br><br>  Als nächstes müssen wir einige Konstanten deklarieren, wie z. B. die Größe von Bildern usw.  Wir werden Bilder mit einer Größe von 256 x 256 Pixel verwenden. Dies ist ein Bild, das groß genug ist, um keine Details zu verlieren, und klein genug, damit alles in den Speicher passt.  Beachten Sie jedoch, dass einige Arten von neuronalen Netzen, die wir verwenden werden, Bilder mit 224 x 224 Pixel erwarten.  In solchen Fällen kommentieren wir 256 und kommentieren 224 aus. <br><br>  Der gleiche Ansatz (Kommentar eins - Kommentar) wird auf die Namen der Modelle angewendet, die wir speichern, einfach weil wir keine Dateien überschreiben möchten, die möglicherweise noch nützlich sind. <br><pre> <code class="python hljs">warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">"ignore"</span></span>) os.environ[<span class="hljs-string"><span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span></span>] = <span class="hljs-string"><span class="hljs-string">'2'</span></span> np.random.seed(<span class="hljs-number"><span class="hljs-number">7</span></span>) start = dt.datetime.now() BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">16</span></span> EPOCHS = <span class="hljs-number"><span class="hljs-number">15</span></span> TESTING_SPLIT=<span class="hljs-number"><span class="hljs-number">0.3</span></span> <span class="hljs-comment"><span class="hljs-comment"># 70/30 % NUM_CLASSES = 120 IMAGE_SIZE = 256 #strModelFileName = "models/ResNet50.h5" # strModelFileName = "models/InceptionV3.h5" strModelFileName = "models/InceptionV3_Sgd.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/MobileNetV2.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/NASNetMobileSgd.h5"</span></span></code> </pre><br><br><h3>  Laden von Daten </h3><br><br>  <i>Laden Sie zunächst die</i> Datei <i>labels.csv hoch</i> und <i>teilen Sie</i> sie in die Teile Training und Validierung auf.  Beachten Sie, dass es noch keinen Testteil gibt, da ich schummeln werde, um mehr Trainingsdaten zu erhalten. <br><br><pre> <code class="python hljs">labels = pd.read_csv(working_path + <span class="hljs-string"><span class="hljs-string">'labels.csv'</span></span>) print(labels.head()) train_ids, valid_ids = train_test_split(labels, test_size = TESTING_SPLIT) print(len(train_ids), <span class="hljs-string"><span class="hljs-string">'train ids'</span></span>, len(valid_ids), <span class="hljs-string"><span class="hljs-string">'validation ids'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'Total'</span></span>, len(labels), <span class="hljs-string"><span class="hljs-string">'testing images'</span></span>) &gt;&gt;&gt; id breed &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">000</span></span>bec180eb18c7604dcecc8fe0dba07 boston_bull &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">001513</span></span>dfcb2ffafc82cccf4d8bbaba97 dingo &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">001</span></span>cdf01b096e06d78e9e5112d419397 pekinese &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">00214</span></span>f311d5d2247d5dfe4fe24b2303d bluetick &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">0021</span></span>f9ceb3235effd7fcde7f7538ed62 golden_retriever &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">7155</span></span> train ids <span class="hljs-number"><span class="hljs-number">3067</span></span> validation ids &gt;&gt;&gt; Total <span class="hljs-number"><span class="hljs-number">10222</span></span> testing images</code> </pre><br><br>  Kopieren Sie anschließend die Bilddateien entsprechend den Dateinamen in die Ordner für Schulung, Validierung und Test.  Die folgende Funktion kopiert die Dateien, deren Namen wir in den angegebenen Ordner übertragen. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copyFileSet</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(strDirFrom, strDirTo, arrFileNames)</span></span></span><span class="hljs-function">:</span></span> arrBreeds = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) arrFileNames = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'id'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo): os.makedirs(strDirTo) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(range(len(arrFileNames))): strFileNameFrom = strDirFrom + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> strFileNameTo = strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>): os.makedirs(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># As a new breed dir is created, copy 1st file # to "test" under name of that breed if not os.path.exists(working_path + "test/"): os.makedirs(working_path + "test/") strFileNameTo = working_path + "test/" + arrBreeds[i] + ".jpg" shutil.copy(strFileNameFrom, strFileNameTo) shutil.copy(strFileNameFrom, strFileNameTo)</span></span></code> </pre><br><br>  Wie Sie sehen können, kopieren wir nur eine Datei für jede Hunderasse als <i>Test</i> .  Außerdem erstellen wir beim Kopieren Unterordner, einen für jede Rasse.  Dementsprechend werden Fotos nach Rasse in Unterordner kopiert. <br><br>  Dies geschieht, weil Keras mit einem Verzeichnis ähnlicher Struktur arbeiten kann und Bilddateien nach Bedarf und nicht alle gleichzeitig lädt, wodurch Speicherplatz gespart wird.  Es ist eine schlechte Idee, alle 15.000 Bilder gleichzeitig hochzuladen. <br><br>  Wir müssen diese Funktion nur einmal aufrufen, da sie Bilder kopiert - und nicht mehr benötigt wird.  Dementsprechend müssen wir es für die zukünftige Verwendung kommentieren: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Move the data in subfolders so we can # use the Keras ImageDataGenerator. # This way we can also later use Keras # Data augmentation features. # --- Uncomment once, to copy files --- #copyFileSet(working_path + "all_images/", # working_path + "train/", train_ids) #copyFileSet(working_path + "all_images/", # working_path + "valid/", valid_ids)</span></span></code> </pre><br><br>  Holen Sie sich eine Liste der Hunderassen: <br><br><pre> <code class="python hljs">breeds = np.unique(labels[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) map_characters = {} <span class="hljs-comment"><span class="hljs-comment">#{0:'none'} for i in range(len(breeds)): map_characters[i] = breeds[i] print("&lt;item&gt;" + breeds[i] + "&lt;/item&gt;") &gt;&gt;&gt; &lt;item&gt;affenpinscher&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;afghan_hound&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;african_hunting_dog&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;airedale&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;american_staffordshire_terrier&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;appenzeller&lt;/item&gt;</span></span></code> </pre><br><br><h3>  Bildverarbeitung </h3><br><br>  Wir werden die Keras-Bibliotheksfunktion namens ImageDataGenerators verwenden.  ImageDataGenerator kann das Bild verarbeiten, skalieren, drehen usw.  Es kann auch eine <i>Verarbeitungsfunktion</i> akzeptieren, die Bilder zusätzlich verarbeiten kann. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) <span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255... img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. #img = cv2.blur(img,(5,5)) return img_1[0]</span></span></code> </pre><br><br>  Beachten Sie den folgenden Code: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255...</span></span></code> </pre><br><br>  Wir können in ImageDataGenerator selbst normalisieren (Unterdaten im Bereich von 0-1 anstelle der ursprünglichen 0-255).  Warum brauchen wir dann einen Präprozessor?  Betrachten Sie als Beispiel den unscharfen Aufruf (auskommentiert, ich verwende ihn nicht): Dies ist dieselbe benutzerdefinierte Bildmanipulation, die beliebig sein kann.  Alles von Kontrast zu HDR. <br><br>  Wir werden zwei verschiedene ImageDataGenerators verwenden, einen für das Training und einen für die Validierung.  Der Unterschied besteht darin, dass wir für das Training Drehungen und Skalierungen benötigen, um die „Vielfalt“ der Daten zu erhöhen. Für die Validierung benötigen wir sie jedoch nicht, zumindest nicht für diese Aufgabe. <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( preprocessing_function=preprocess, <span class="hljs-comment"><span class="hljs-comment">#rescale=1./255, # done in preprocess() # randomly rotate images (degrees, 0 to 30) rotation_range=30, # randomly shift images horizontally # (fraction of total width) width_shift_range=0.3, height_shift_range=0.3, # randomly flip images horizontal_flip=True, ,vertical_flip=False, zoom_range=0.3) val_datagen = ImageDataGenerator( preprocessing_function=preprocess) train_gen = train_datagen.flow_from_directory( working_path + "train/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical") val_gen = val_datagen.flow_from_directory( working_path + "valid/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical")</span></span></code> </pre><br><br><h3>  Erstellen eines neuronalen Netzwerks </h3><br><br>  Wie bereits erwähnt, werden wir verschiedene Arten von neuronalen Netzen erstellen.  Jedes Mal, wenn wir eine andere Funktion zum Erstellen aufrufen, schließen Sie andere Dateien ein und bestimmen manchmal eine andere Bildgröße.  Um zwischen verschiedenen Arten von neuronalen Netzen zu wechseln, müssen wir den entsprechenden Code kommentieren / auskommentieren. <br><br>  Erstellen Sie zunächst ein "Vanille" CNN.  Es funktioniert nicht gut, weil ich beschlossen habe, keine Zeit mit dem Debuggen zu verschwenden, aber es bietet zumindest eine Grundlage, die entwickelt werden kann, wenn ein Wunsch besteht (normalerweise ist dies eine schlechte Idee, da vorab trainierte Netzwerke das beste Ergebnis liefern). <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelVanilla</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() <span class="hljs-comment"><span class="hljs-comment"># Note the (7, 7) here. This is one of technics # used to reduce memory use by the NN: we scan # the image in a larger steps. # Also note regularizers.l2: this technic is # used to prevent overfitting. The "0.001" here # is an empirical value and can be optimized. model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.001))) # Note the use of a standard CNN building blocks: # Conv2D - BatchNormalization - Activation # MaxPooling2D - Dropout # The last two are used to avoid overfitting, also, # MaxPooling2D reduces memory use. model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(16, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) # This is the end on "convolutional" part of CNN. # Now we need to transform multidementional # data into one-dim. array for a fully-connected # classifier: model.add(Flatten()) # And two layers of classifier itself (plus an # Activation layer in between): model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) model.add(Activation("relu")) model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) # We need to compile the resulting network. # Note that there are few parameters we can # try here: the best performing one is uncommented, # the rest is commented out for your reference. #model.compile(optimizer='rmsprop', # loss='categorical_crossentropy', # metrics=['accuracy']) #model.compile( # optimizer=keras.optimizers.RMSprop(lr=0.0005), # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #model.compile(optimizer='adadelta', # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.Adadelta(lr=1.0, # rho=0.95, epsilon=0.01, decay=0.01) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.RMSprop(lr=0.0005, # rho=0.9, epsilon=None, decay=0.0001) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) # model.summary() return(model)</span></span></code> </pre><br><br>  Wenn wir mithilfe von <i>Transferlernen</i> Netzwerke erstellen, ändert sich das Verfahren: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelMobileNetV2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, create the NN and load pre-trained # weights for it ('imagenet') # Note that we are not loading last layers of # the network (include_top=False), as we are # going to add layers of our own: base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Then attach our layers at the end. These are # to build "classifier" that makes sense of # the patterns previous layers provide: x = base_model.output x = Dense(512)(x) x = Activation('relu')(x) x = Dropout(0.5)(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Create a model model = Model(inputs=base_model.input, outputs=predictions) # We need to make sure that pre-trained # layers are not changed when we train # our classifier: # Either this: #model.layers[0].trainable = False # or that: for layer in base_model.layers: layer.trainable = False # As always, there are different possible # settings, I tried few and chose the best: # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Das Erstellen anderer Netzwerktypen erfolgt nach demselben Muster: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelResNet50</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> base_model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>, input_shape=(IMAGE_SIZE, IMAGE_SIZE, <span class="hljs-number"><span class="hljs-number">3</span></span>)) x = base_model.output x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>)(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) predictions = Dense(NUM_CLASSES, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=base_model.input, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#model.layers[0].trainable = False # model.compile(loss='categorical_crossentropy', # optimizer='adam', metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Achtung: Gewinner!  Dieser NN zeigte das beste Ergebnis: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelInceptionV3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Noch eine: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelNASNetMobile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = NASNetMobile(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Verschiedene Arten von neuronalen Netzen können für verschiedene Aufgaben verwendet werden.  Zusätzlich zu den Anforderungen an die Vorhersagegenauigkeit kann die Größe eine Rolle spielen (das mobile NN ist fünfmal kleiner als Inception) und die Geschwindigkeit (wenn wir eine Echtzeitverarbeitung eines Videostreams benötigen, muss die Genauigkeit geopfert werden). <br><br><h3>  Neuronales Netzwerktraining </h3><br><br>  Zunächst <i>experimentieren</i> wir, damit wir die gespeicherten neuronalen Netze entfernen können, die wir jedoch nicht mehr verwenden.  Die folgende Funktion entfernt NN, falls vorhanden: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make sure that previous "best network" is deleted. def deleteSavedNet(best_weights_filepath): if(os.path.isfile(best_weights_filepath)): os.remove(best_weights_filepath) print("deleteSavedNet():File removed") else: print("deleteSavedNet():No file to remove")</span></span></code> </pre><br><br>  Die Art und Weise, wie wir neuronale Netze erstellen und löschen, ist recht einfach und unkompliziert.  Löschen Sie zuerst.  Wenn Sie <i>delete</i> (nur) aufrufen, sollten Sie berücksichtigen, dass das Jupiter-Notizbuch über eine Funktion zum Ausführen der Auswahl verfügt. Wählen Sie nur das aus, was Sie verwenden möchten, und führen Sie es aus. <br><br>  Dann erstellen wir ein neuronales Netzwerk, wenn seine Datei nicht vorhanden war, oder rufen das <i>Laden auf,</i> wenn es vorhanden ist: Natürlich können wir nicht "delete" aufrufen und dann erwarten, dass NN vorhanden ist. Um also ein gespeichertes neuronales Netzwerk zu verwenden, rufen Sie nicht <i>delete auf</i> . <br><br>  Mit anderen Worten, wir können je nach Situation und dem, mit dem wir gerade experimentieren, ein neues NN erstellen oder das vorhandene verwenden.  Ein einfaches Szenario: Wir haben ein neuronales Netzwerk trainiert und sind dann in den Urlaub gefahren.  Sie sind zurückgekehrt, und Google hat die Sitzung festgenagelt. Daher müssen wir die zuvor gespeicherte laden: Kommentieren Sie "Löschen" aus und kommentieren Sie "Laden" aus. <br><br><pre> <code class="python hljs">deleteSavedNet(working_path + strModelFileName) <span class="hljs-comment"><span class="hljs-comment">#if not os.path.exists(working_path + "models"): # os.makedirs(working_path + "models") # #if not os.path.exists(working_path + # strModelFileName): # model = createModelResNet50() model = createModelInceptionV3() # model = createModelMobileNetV2() # model = createModelNASNetMobile() #else: # model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  <b>Checkpoints</b> sind ein sehr wichtiges Element unseres Programms.  Wir können eine Reihe von Funktionen erstellen, die am Ende jeder Trainingsära aufgerufen werden sollen, und diese an den Checkpoint übergeben.  Sie können beispielsweise ein neuronales Netzwerk speichern, <i>wenn</i> Ergebnisse angezeigt werden, die besser sind als die bereits gespeicherten. <br><br><pre> <code class="python hljs">checkpoint = ModelCheckpoint(working_path + strModelFileName, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, save_weights_only=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) callbacks_list = [ checkpoint ]</code> </pre><br><br>  Schließlich unterrichten wir das neuronale Netzwerk auf dem Trainingsset: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Calculate sizes of training and validation sets STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size STEP_SIZE_VALID=val_gen.n//val_gen.batch_size # Set to False if we are experimenting with # some other part of code, use history that # was calculated before (and is still in # memory bDoTraining = True if bDoTraining == True: # model.fit_generator does the actual training # Note the use of generators and callbacks # that were defined earlier history = model.fit_generator(generator=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=val_gen, validation_steps=STEP_SIZE_VALID, epochs=EPOCHS, callbacks=callbacks_list) # --- After fitting, load the best model # This is important as otherwise we'll # have the LAST model loaded, not necessarily # the best one. model.load_weights(working_path + strModelFileName) # --- Presentation part # summarize history for accuracy plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['acc', 'val_acc'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['loss', 'val_loss'], loc='upper left') plt.show() # As grid optimization of NN would take too long, # I did just few tests with different parameters. # Below I keep results, commented out, in the same # code. As you can see, Inception shows the best # results: # Inception: # adam: val_acc 0.79393 # sgd: val_acc 0.80892 # Mobile: # adam: val_acc 0.65290 # sgd: Epoch 00015: val_acc improved from 0.67584 to 0.68469 # sgd-30 epochs: 0.68 # NASNetMobile, adam: val_acc did not improve from 0.78335 # NASNetMobile, sgd: 0.8</span></span></code> </pre><br><br>  Die Diagramme für Genauigkeit und Verlust für die besten Konfigurationen lauten wie folgt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/97d/9cc/f0e97d9ccdc8f8ed9e44ddba02cf1f8d.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/612/e09/8b0/612e098b088979768d1cc66c2f6972bc.png"><br><br>  Wie Sie sehen können, lernt das neuronale Netzwerk und ist nicht schlecht. <br><br><h3>  Testen neuronaler Netze </h3><br><br>  Nach Abschluss des Trainings müssen wir das Ergebnis testen.  Dafür präsentiert NN Bilder, die sie noch nie gesehen hat - die, die wir in den Testordner kopiert haben - eines für jede Hunderasse. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># --- Test j = 0 # Final cycle performs testing on the entire # testing set. for file_name in os.listdir( working_path + "test/"): img = image.load_img(working_path + "test/" + file_name); img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict_on_batch(img_1) # get 5 best predictions y_pred_ids = y_pred[0].argsort()[-5:][::-1] print(file_name) for i in range(len(y_pred_ids)): print("\n\t" + map_characters[y_pred_ids[i]] + " (" + str(y_pred[0][y_pred_ids[i]]) + ")") print("--------------------\n") j = j + 1</span></span></code> </pre><br><br><h3>  Exportieren Sie ein neuronales Netzwerk in eine Java-Anwendung </h3><br><br>  Zunächst müssen wir das Laden des neuronalen Netzwerks von der Festplatte organisieren.  Der Grund ist klar: Der Export findet in einem anderen Codeblock statt, daher werden wir den Export höchstwahrscheinlich separat starten - wenn das neuronale Netzwerk in seinen optimalen Zustand gebracht wird.  Das heißt, unmittelbar vor dem Export werden wir im selben Programmlauf das Netzwerk nicht trainieren.  Wenn Sie den hier gezeigten Code verwenden, gibt es keinen Unterschied, das optimale Netzwerk wurde für Sie ausgewählt.  Aber wenn Sie etwas Eigenes lernen, ist es Zeitverschwendung, alles vor dem Speichern neu zu trainieren, wenn Sie zuvor alles gespeichert haben. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Test: load and run model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  Aus dem gleichen Grund - um nicht über den Code zu springen - füge ich hier die für den Export erforderlichen Dateien ein.  Niemand stört Sie, sie an den Anfang des Programms zu verschieben, wenn Ihr Sinn für Schönheit dies erfordert: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre><br><br>  Ein kleiner Test nach dem Laden eines neuronalen Netzwerks, um sicherzustellen, dass alles geladen ist - funktioniert: <br><br><pre> <code class="python hljs">img = image.load_img(working_path + <span class="hljs-string"><span class="hljs-string">"test/affenpinscher.jpg"</span></span>) <span class="hljs-comment"><span class="hljs-comment">#basset.jpg") img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict(img_1) Y_pred_classes = np.argmax(y_pred,axis = 1) # print(y_pred) fig, ax = plt.subplots() ax.imshow(img) ax.axis('off') ax.set_title(map_characters[Y_pred_classes[0]]) plt.show()</span></span></code> </pre><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/032/846/05c03284674e4337a2e5a3ba617634dd.png" alt="Bild"><br><br>  Als nächstes müssen wir die Namen der Eingabe- und Ausgabeschichten des Netzwerks abrufen (entweder diese oder die Erstellungsfunktion, wir müssen die Schichten explizit "benennen", was wir nicht getan haben). <br><br><pre> <code class="python hljs">model.summary() &gt;&gt;&gt; Layer (type) &gt;&gt;&gt; ====================== &gt;&gt;&gt; input_7 (InputLayer) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; conv2d_283 (Conv2D) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; ... &gt;&gt;&gt; dense_14 (Dense) &gt;&gt;&gt; ====================== &gt;&gt;&gt; Total params: <span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">913</span></span>,<span class="hljs-number"><span class="hljs-number">432</span></span> &gt;&gt;&gt; Trainable params: <span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">110</span></span>,<span class="hljs-number"><span class="hljs-number">648</span></span> &gt;&gt;&gt; Non-trainable params: <span class="hljs-number"><span class="hljs-number">21</span></span>,<span class="hljs-number"><span class="hljs-number">802</span></span>,<span class="hljs-number"><span class="hljs-number">784</span></span></code> </pre><br><br>  Wir werden die Namen der Eingabe- und Ausgabeebenen später verwenden, wenn wir das neuronale Netzwerk in eine Java-Anwendung importieren. <br><br>  Ein weiterer Code, der das Netzwerk durchstreift, um diese Daten abzurufen: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">print_graph_nodes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> g = tf.GraphDef() g.ParseFromString(open(filename, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>).read()) print() print(filename) print(<span class="hljs-string"><span class="hljs-string">"=======================INPUT==================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'input'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"=======================OUTPUT=================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'output'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"===================KERAS_LEARNING=============="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'keras_learning_phase'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"==============================================="</span></span>) print() <span class="hljs-comment"><span class="hljs-comment">#def get_script_path(): # return os.path.dirname(os.path.realpath(sys.argv[0]))</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber ich mag ihn nicht und ich empfehle ihn nicht. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der folgende Code exportiert das Keras Neural Network in das </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pb-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Format, das wir von Android erfassen werden.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">keras_to_tensorflow</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keras_model, output_dir, model_name,out_prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"output_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, log_tensorboard=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> os.path.exists(output_dir) == <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: os.mkdir(output_dir) out_nodes = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(keras_model.outputs)): out_nodes.append(out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) tf.identity(keras_model.output[i], out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) sess = K.get_session() <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> graph_util <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework graph_io init_graph = sess.graph.as_graph_def() main_graph = graph_util.convert_variables_to_constants( sess, init_graph, out_nodes) graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> log_tensorboard: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> import_pb_to_tensorboard import_pb_to_tensorboard.import_to_tensorboard( os.path.join(output_dir, model_name), output_dir)</code> </pre><br><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aufruf dieser Funktionen zum Exportieren eines neuronalen Netzwerks: </font></font><br><br></p><pre> <code class="python hljs">model = load_model(working_path + strModelFileName) keras_to_tensorflow(model, output_dir=working_path + strModelFileName, model_name=working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>) print_graph_nodes(working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>)</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die letzte Zeile gibt die Struktur des resultierenden neuronalen Netzwerks aus. </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Erstellen einer Android-Anwendung mithilfe eines neuronalen Netzwerks </font></font></h2><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Export neuronaler Netze in Android ist gut formalisiert und sollte keine Schwierigkeiten verursachen. </font><font style="vertical-align: inherit;">Es gibt wie immer verschiedene Möglichkeiten, die wir (zum Zeitpunkt des Schreibens) am beliebtesten verwenden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zunächst verwenden wir Android Studio, um ein neues Projekt zu erstellen. </font><font style="vertical-align: inherit;">Wir werden "Ecken abschneiden", weil unsere Aufgabe kein Android-Tutorial ist. </font><font style="vertical-align: inherit;">Die Anwendung enthält also nur eine Aktivität. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b3/76e/997/6b376e997b34f45359c46923f6613d60.png" alt="Bild"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sie sehen können, haben wir den Ordner "Assets" hinzugefügt und unser neuronales Netzwerk in dieses kopiert (das zuvor exportierte).</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gradle-Datei </font></font></h3><br><br>       .  ,     <i>tensorflow-android</i> .    ,    Tensorflow (, , Keras)  Java: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a16/091/fab/a16091fab2166f834827812611142d26.png" alt="Bild"><br><br>     : <i>versionCode</i>  <i>versionName</i> .    ,        Google Play.     gdadle (, 1 -&gt; 2 -&gt; 3...)     ,     «   ». <br><br><h3>  </h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zuallererst wird unsere Anwendung "schwer" sein - 100 Mb Neural Network wird leicht in den Speicher moderner Mobiltelefone passen, aber es ist definitiv eine schlechte Idee, für jedes von Facebook "geteilte" Foto eine separate Instanz zu öffnen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daher verbieten wir, mehr als eine Instanz unserer Anwendung zu erstellen:</font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">activity</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">".MainActivity"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:launchMode</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"singleTask"</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Durch Hinzufügen von </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">android: launchMode = "singleTask"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu MainActivity weisen wir Android an, eine vorhandene Kopie der Anwendung zu öffnen (zu aktivieren), anstatt eine andere Instanz zu erstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann müssen wir unsere Anwendung in die Liste aufnehmen, die das System anzeigt, wenn jemand das Bild „teilt“:</font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Send action required to display activity in share list --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">action</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.action.SEND"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Make activity default to launch --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">category</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.category.DEFAULT"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Mime type ie what can be shared with this activity only image and text --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">data</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:mimeType</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"image/*"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Schließlich müssen wir Funktionen und Berechtigungen anfordern, die unsere Anwendung verwenden wird: </font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-feature</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.hardware.camera"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:required</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"true"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">= </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.WRITE_EXTERNAL_STORAGE"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.READ_PHONE_STATE"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tools:node</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"remove"</span></span></span><span class="hljs-tag"> /&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wenn Sie mit der Programmierung für Android vertraut sind, sollte dieser Teil keine Fragen aufwerfen. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Layout-Anwendung. </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir werden zwei Layouts erstellen, eines für Hochformat und eines für Querformat. </font><font style="vertical-align: inherit;">So sieht das </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Portrait-Layout</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aus </font><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was wir hinzufügen werden: ein großes Feld (Ansicht) zum Anzeigen des Bildes, eine nervige Liste von Anzeigen (angezeigt, wenn die Schaltfläche mit einem Knochen gedrückt wird), die Hilfeschaltfläche, Schaltflächen zum Herunterladen eines Bilds aus der Datei / Galerie und zum Aufnehmen von der Kamera und schließlich (zunächst) versteckt) Schaltfläche "Verarbeiten" für die Bildverarbeitung. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f71/882/81f/f7188281ff581965c20c7e818cb0fd77.png" alt="Bild"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Aktivität selbst enthält die gesamte Logik zum Ein- und Ausblenden sowie zum Aktivieren / Deaktivieren von Schaltflächen, abhängig vom Status der Anwendung.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hauptaktivität </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Diese Aktivität erbt (erweitert) die Standard-Android-Aktivität: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MainActivity</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Activity</span></span></span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Betrachten Sie den Code, der für den Betrieb des neuronalen Netzwerks verantwortlich ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zunächst akzeptiert das neuronale Netzwerk Bitmap. </font><font style="vertical-align: inherit;">Dies ist zunächst eine große Bitmap (beliebiger Größe) von der Kamera oder aus einer Datei (m_bitmap), dann transformieren wir sie und führen zu den Standardpixeln mit 256 x 256 Pixel (m_bitmapForNn). </font><font style="vertical-align: inherit;">Wir speichern auch die Bitmap-Größe (256) in einer Konstanten:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmap = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmapForNn = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> m_nImageSize = <span class="hljs-number"><span class="hljs-number">256</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir müssen dem neuronalen Netzwerk die Namen der Eingabe- und Ausgabeschichten mitteilen. </font><font style="vertical-align: inherit;">Wir haben sie früher erhalten (siehe Auflistung), aber denken Sie daran, dass sie in Ihrem Fall unterschiedlich sein können:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String INPUT_NAME = <span class="hljs-string"><span class="hljs-string">"input_7_1"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String OUTPUT_NAME = <span class="hljs-string"><span class="hljs-string">"output_1"</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann deklarieren wir eine Variable, die das TensofFlow-Objekt enthält. </font><font style="vertical-align: inherit;">Außerdem speichern wir den Pfad zur neuronalen Netzwerkdatei (die in Assets liegt):</font></font><br><br><p></p><pre><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">private TensorFlowInferenceInterface tf;</font></font><font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
private String MODEL_PATH = </font></font><font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
	"file: ///android_asset/dogs.pb";</font></font><font></font>
</pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir speichern die Hunderassen in der Liste, damit sie später dem Benutzer angezeigt werden und nicht die Array-Indizes: </font></font><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String[] m_arrBreedsArray;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zunächst haben wir Bitmap heruntergeladen. </font><font style="vertical-align: inherit;">Das neuronale Netzwerk erwartet jedoch ein Array von RGB-Werten, und seine Ausgabe ist ein Array von Wahrscheinlichkeiten, dass diese Rasse das ist, was im Bild gezeigt wird. </font><font style="vertical-align: inherit;">Dementsprechend müssen wir zwei weitere Arrays hinzufügen (beachten Sie, dass 120 die Anzahl der Hunderassen ist, die in unseren Trainingsdaten enthalten sind):</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrPrediction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">120</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrInput = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tensorflow-Inferenzbibliothek herunterladen: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> { System.loadLibrary(<span class="hljs-string"><span class="hljs-string">"tensorflow_inference"</span></span>); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Da neuronale Netzwerkoperationen Zeit benötigen, müssen sie in einem separaten Thread ausgeführt werden. Andernfalls besteht die Möglichkeit, dass wir eine Systemmeldung erhalten, dass die Anwendung nicht antwortet, ganz zu schweigen von einem unzufriedenen Benutzer. </font></font><br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PredictionTask</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AsyncTask</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onPreExecute</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onPreExecute(); } <span class="hljs-comment"><span class="hljs-comment">// --- @Override protected Void doInBackground(Void... params) { try { # We get RGB values packed in integers # from the Bitmap, then break those # integers into individual triplets m_arrInput = new float[ m_nImageSize * m_nImageSize * 3]; int[] intValues = new int[ m_nImageSize * m_nImageSize]; m_bitmapForNn.getPixels(intValues, 0, m_nImageSize, 0, 0, m_nImageSize, m_nImageSize); for (int i = 0; i &lt; intValues.length; i++) { int val = intValues[i]; m_arrInput[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 2] = (val &amp; 0xFF) / 255f; } // --- tf = new TensorFlowInferenceInterface( getAssets(), MODEL_PATH); //Pass input into the tensorflow tf.feed(INPUT_NAME, m_arrInput, 1, m_nImageSize, m_nImageSize, 3); //compute predictions tf.run(new String[]{OUTPUT_NAME}, false); //copy output into PREDICTIONS array tf.fetch(OUTPUT_NAME, m_arrPrediction); } catch (Exception e) { e.getMessage(); } return null; } // --- @Override protected void onPostExecute(Void result) { super.onPostExecute(result); // --- enableControls(true); // --- tf = null; m_arrInput = null; # strResult contains 5 lines of text # with most probable dog breeds and # their probabilities m_strResult = ""; # What we do below is sorting the array # by probabilities (using map) # and getting in reverse order) the # first five entries TreeMap&lt;Float, Integer&gt; map = new TreeMap&lt;Float, Integer&gt;( Collections.reverseOrder()); for(int i = 0; i &lt; m_arrPrediction.length; i++) map.put(m_arrPrediction[i], i); int i = 0; for (TreeMap.Entry&lt;Float, Integer&gt; pair : map.entrySet()) { float key = pair.getKey(); int idx = pair.getValue(); String strBreed = m_arrBreedsArray[idx]; m_strResult += strBreed + ": " + String.format("%.6f", key) + "\n"; i++; if (i &gt; 5) break; } m_txtViewBreed.setVisibility(View.VISIBLE); m_txtViewBreed.setText(m_strResult); } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In onCreate () der MainActivity müssen wir den onClickListener für die Schaltfläche "Process" hinzufügen: </font></font><br><br><pre> <code class="java hljs">m_btn_process.setOnClickListener(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> View.OnClickListener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onClick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(View v)</span></span></span><span class="hljs-function"> </span></span>{ processImage(); } });</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hier ruft processImage () nur den oben beschriebenen Thread auf: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">processImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { enableControls(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); <span class="hljs-comment"><span class="hljs-comment">// --- PredictionTask prediction_task = new PredictionTask(); prediction_task.execute(); } catch (Exception e) { e.printStackTrace(); } }</span></span></code> </pre><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Zusätzliche Hinweise </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir planen nicht, die Details der UI-Programmierung für Android zu diskutieren, da dies sicherlich nicht für die Portierung neuronaler Netze gilt. </font><font style="vertical-align: inherit;">Eines ist jedoch noch erwähnenswert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir die Erstellung zusätzlicher Instanzen unserer Anwendung verhindert haben, haben wir auch die normale Reihenfolge der Erstellung und Löschung von Aktivitäten (Kontrollfluss) verletzt: Wenn Sie ein Bild von Facebook „freigeben“ und dann ein anderes freigeben, wird die Anwendung nicht neu gestartet. </font><font style="vertical-align: inherit;">Dies bedeutet, dass die „traditionelle“ Methode zum Abfangen übertragener Daten in onCreate nicht ausreicht, da onCreate nicht aufgerufen wird. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So lösen Sie dieses Problem: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Rufen Sie in onCreate in MainActivity die Funktion onSharedIntent auf:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onCreate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( Bundle savedInstanceState)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onCreate(savedInstanceState); .... onSharedIntent(); ....</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir fügen auch einen Handler für onNewIntent hinzu: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onNewIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Intent intent)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onNewIntent(intent); setIntent(intent); onSharedIntent(); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hier ist die onSharedIntent-Funktion selbst: </font></font><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onSharedIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ Intent receivedIntent = getIntent(); String receivedAction = receivedIntent.getAction(); String receivedType = receivedIntent.getType(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (receivedAction.equals(Intent.ACTION_SEND)) { <span class="hljs-comment"><span class="hljs-comment">// If mime type is equal to image if (receivedType.startsWith("image/")) { m_txtViewBreed.setText(""); m_strResult = ""; Uri receivedUri = receivedIntent.getParcelableExtra( Intent.EXTRA_STREAM); if (receivedUri != null) { try { Bitmap bitmap = MediaStore.Images.Media.getBitmap( this.getContentResolver(), receivedUri); if(bitmap != null) { m_bitmap = bitmap; m_picView.setImageBitmap(m_bitmap); storeBitmap(); enableControls(true); } } catch (Exception e) { e.printStackTrace(); } } } } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Jetzt verarbeiten wir die übertragenen Daten in onCreate (wenn sich die Anwendung nicht im Speicher befand) oder in onNewIntent (wenn sie früher gestartet wurde). </font></font><br><br><br><br><br>  Viel Glück<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Ihnen der Artikel gefallen hat, "mögen" Sie ihn bitte auf alle möglichen Arten. Es gibt auch "soziale" Schaltflächen auf der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Website</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de448316/">https://habr.com/ru/post/de448316/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de448300/index.html">Konferenz mailto: CLOUD - über Wolken und Umgebung</a></li>
<li><a href="../de448302/index.html">Durch die Sicherheitsanfälligkeit in AdBlock- und uBlock-Filtern kann beliebiger Code auf der Benutzerseite ausgeführt werden</a></li>
<li><a href="../de448304/index.html">Das Buch "Vue.js in Aktion"</a></li>
<li><a href="../de448308/index.html">Data Science Digest (April 2019)</a></li>
<li><a href="../de448310/index.html">Schreiben eines Telegrammbots in Python mit der Telebot-Bibliothek Teil 1</a></li>
<li><a href="../de448320/index.html">Warum Silizium und warum CMOS?</a></li>
<li><a href="../de448322/index.html">C ++ Russia 2019: kostenlose Übertragung der ersten Halle und ein wenig darüber, was auf der Konferenz sein wird</a></li>
<li><a href="../de448324/index.html">Erstellen Sie prozedurale Planetenkugeln</a></li>
<li><a href="../de448326/index.html">Durchschauen. Wie kann man Fächer studieren, ohne sie zu brechen?</a></li>
<li><a href="../de448328/index.html">In Moskau wird ein Drucker gezeigt, der Organe und Gewebe druckt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>