<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëô ü¶è üßïüèº Verst√§rkungstraining: Parsen von Videospielen ‚òòÔ∏è ‚úäüèº ü§ôüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Auf der KI-Konferenz wird Vladimir Ivanov vivanov879 , Sr. √ºber den Einsatz von verst√§rktem Lernen sprechen Deep Learning Ingenieur bei Nvidia . Der E...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verst√§rkungstraining: Parsen von Videospielen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/smileexpo/blog/428329/"><img align="left" src="https://habrastorage.org/webt/ls/va/9_/lsva9_rsrvkdmceogvse3sexdog.png"><br>  Auf der KI-Konferenz wird <b>Vladimir Ivanov <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">vivanov879</a> , Sr.</b> √ºber den Einsatz von verst√§rktem Lernen sprechen  <b>Deep Learning Ingenieur bei Nvidia</b> .  Der Experte besch√§ftigt sich mit maschinellem Lernen in der Testabteilung: ‚ÄûIch analysiere die Daten, die wir beim Testen von Videospielen und Hardware sammeln.  Daf√ºr benutze ich maschinelles Lernen und Computer Vision.  Der Hauptteil der Arbeit besteht in der Bildanalyse, Datenbereinigung vor dem Training, Datenmarkierung und Visualisierung der erhaltenen L√∂sungen. ‚Äú <br><br>  In dem heutigen Artikel erkl√§rt Vladimir anhand von Videospielbeispielen, warum verst√§rktes Lernen in autonomen Autos eingesetzt wird, und spricht dar√ºber, wie ein Agent in einer sich √§ndernden Umgebung geschult wird. <br><br>  In den letzten Jahren hat die Menschheit eine riesige Datenmenge gesammelt.  Einige Datens√§tze werden gemeinsam genutzt und manuell angelegt.  Zum Beispiel das CIFAR-Dataset, in dem jedes Bild signiert ist und zu welcher Klasse es geh√∂rt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/ko/lp/16kolpeo1k3j1j9ppdh9vfjb6vc.jpeg"></div><br><br>  Es gibt Datens√§tze, in denen Sie nicht nur dem gesamten Bild, sondern jedem Pixel im Bild eine Klasse zuweisen m√ºssen.  Wie zum Beispiel in CityScapes. <br><br><img src="https://habrastorage.org/webt/eq/hz/b0/eqhzb0gt9v2demntxriobd5ewji.png"><br><br>  Was diese Aufgaben verbindet, ist, dass sich ein lernendes neuronales Netzwerk nur die Muster in den Daten merken muss.  Daher lernt das neuronale Netzwerk mit ausreichend gro√üen Datenmengen und im Fall von CIFAR mit 80 Millionen Bildern, zu verallgemeinern.  Infolgedessen kommt sie gut mit der Klassifizierung von Bildern zurecht, die sie noch nie zuvor gesehen hatte. <br><br>  Im Rahmen der Unterrichtstechnik mit dem Lehrer, der zum Markieren von Bildern arbeitet, ist es jedoch unm√∂glich, Probleme zu l√∂sen, bei denen wir die Note nicht vorhersagen, sondern Entscheidungen treffen wollen.  Wie zum Beispiel beim autonomen Fahren, bei dem es darum geht, den Endpunkt der Route sicher und zuverl√§ssig zu erreichen. <a name="habracut"></a><br><br>  Bei den Klassifizierungsproblemen haben wir die Unterrichtstechnik mit dem Lehrer verwendet - wenn jedem Bild eine bestimmte Klasse zugewiesen wurde.  Was aber, wenn wir kein solches Markup haben, aber es einen Agenten und eine Umgebung gibt, in der er bestimmte Aktionen ausf√ºhren kann?  Lassen Sie es zum Beispiel ein Videospiel sein, und wir k√∂nnen auf die Steuerpfeile klicken. <br><br><img src="https://habrastorage.org/webt/bn/-c/ad/bn-cadnljvu7relew1mfes6p0xw.jpeg"><br><br>  Diese Art von Problem sollte mit einem Verst√§rkungstraining gel√∂st werden.  In der allgemeinen Erkl√§rung des Problems m√∂chten wir lernen, wie die richtige Abfolge von Aktionen ausgef√ºhrt wird.  Es ist von grundlegender Bedeutung, dass der Agent in der Lage ist, immer wieder Aktionen auszuf√ºhren und so die Umgebung zu erkunden, in der er sich befindet.  Und anstatt der richtigen Antwort, was in einer bestimmten Situation zu tun ist, erh√§lt er eine Belohnung f√ºr eine korrekt erledigte Aufgabe.  Im Fall eines autonomen Taxis erh√§lt der Fahrer beispielsweise f√ºr jede Fahrt einen Bonus. <br><br>  Kehren wir zu einem einfachen Beispiel zur√ºck - einem Videospiel.  Nehmen Sie etwas Einfaches, wie das Atari-Tischtennisspiel. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hg/ev/0b/hgev0bl9kpvpzlllfzyyroentow.jpeg"></div><br><br>  Wir werden das Tablet auf der linken Seite steuern.  Wir werden gegen den Computer spielen, der nach den Regeln auf der rechten Seite programmiert ist.  Da wir mit einem Bild arbeiten und neuronale Netze am erfolgreichsten Informationen aus Bildern extrahieren, wenden wir ein Bild auf die Eingabe eines dreischichtigen neuronalen Netzes mit einer Kernelgr√∂√üe von 3 x 3 an.  Am Ausgang muss sie eine von zwei Aktionen ausw√§hlen: Bewegen Sie das Brett nach oben oder unten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/o_/5z/mwo_5zeixn2llke6ethglty43ao.png"></div><br><br>  Wir trainieren das neuronale Netzwerk, um Aktionen auszuf√ºhren, die zum Sieg f√ºhren.  Die Trainingstechnik ist wie folgt.  Wir lassen das neuronale Netz ein paar Runden Tischtennis spielen.  Dann beginnen wir, die gespielten Spiele zu sortieren.  In den Spielen, in denen sie gewonnen hat, markieren wir die Bilder mit der Aufschrift ‚ÄûUp‚Äú, wo sie den Schl√§ger hob, und ‚ÄûDown‚Äú, wo sie sie senkte.  In verlorenen Spielen machen wir das Gegenteil.  Wir markieren die Bilder, auf denen sie die Tafel mit dem Etikett ‚ÄûUp‚Äú abgesenkt und dort, wo sie sie angehoben hat, ‚ÄûDown‚Äú.  So reduzieren wir das Problem auf den Ansatz, den wir bereits kennen - das Training mit einem Lehrer.  Wir haben eine Reihe von Bildern mit Tags. <br><br><img src="https://habrastorage.org/webt/al/16/48/al1648vo1sfp72qj_8n2fpif3va.png"><br><br>  Mit dieser Trainingstechnik lernt unser Agent in ein paar Stunden, einen nach den Regeln programmierten Computerspieler zu schlagen. <br><br>  Was tun mit autonomem Fahren?  Tatsache ist, dass Tischtennis ein sehr einfaches Spiel ist.  Und es kann Tausende von Bildern pro Sekunde erzeugen.  In unserem Netzwerk gibt es jetzt nur noch 3 Schichten.  Daher ist der Lernprozess blitzschnell.  Das Spiel generiert eine gro√üe Datenmenge und wir verarbeiten sie sofort.  Beim autonomen Fahren ist das Sammeln von Daten viel l√§nger und teurer.  Autos sind teuer und mit einem Auto erhalten wir nur 60 Bilder pro Sekunde.  Au√üerdem steigt der Fehlerpreis.  In einem Videospiel k√∂nnten wir es uns leisten, zu Beginn des Trainings Spiel f√ºr Spiel zu spielen.  Aber wir k√∂nnen es uns nicht leisten, das Auto zu verderben. <br><br>  In diesem Fall helfen wir dem neuronalen Netzwerk zu Beginn des Trainings.  Wir befestigen die Kamera am Auto, setzen einen erfahrenen Fahrer ein und nehmen Fotos von der Kamera auf.  F√ºr jedes Bild abonnieren wir den Lenkwinkel des Autos.  Wir werden das neuronale Netzwerk trainieren, um das Verhalten eines erfahrenen Fahrers zu kopieren.  So haben wir die Aufgabe wieder auf den bereits bekannten Unterricht mit einem Lehrer reduziert. <br><br><img src="https://habrastorage.org/webt/pl/d0/oc/pld0oc75oafeojutvborl_upb8a.png"><br><br>  Mit einem ausreichend gro√üen und vielf√§ltigen Datensatz, der verschiedene Landschaften, Jahreszeiten und Wetterbedingungen umfasst, lernt das neuronale Netzwerk, wie das Auto genau gesteuert werden kann. <br><br>  Es gab jedoch ein Problem mit den Daten.  Sie sind sehr lang und teuer zu sammeln.  Verwenden wir einen Simulator, in dem die gesamte Physik der Fahrzeugbewegung implementiert wird - zum Beispiel DeepDrive.  Wir k√∂nnen es lernen, ohne Angst zu haben, ein Auto zu verlieren. <br><br><img src="https://habrastorage.org/webt/fe/c0/4v/fec04vmzbamtd60xxv3ypctf2ua.jpeg"><br><br>  In diesem Simulator haben wir Zugriff auf alle Indikatoren des Autos und der Welt.  Au√üerdem sind alle Personen, Autos, ihre Geschwindigkeiten und Entfernungen zu ihnen markiert. <br><br><img src="https://habrastorage.org/webt/hq/37/1k/hq371k3xvab9kfcworznwlkelai.jpeg"><br><br>  Aus Sicht des Ingenieurs k√∂nnen Sie in einem solchen Simulator sicher neue Trainingstechniken ausprobieren.  Was soll ein Forscher tun?  Zum Beispiel das Studieren verschiedener Optionen f√ºr den Gradientenabstieg bei Lernproblemen mit Verst√§rkung.  Um eine einfache Hypothese zu testen, m√∂chte ich keine Spatzen aus einer Kanone schie√üen und einen Agenten in einer komplexen virtuellen Welt ausf√ºhren und dann tagelang auf Simulationsergebnisse warten.  In diesem Fall nutzen wir unsere Rechenleistung effizienter.  Lassen Sie die Agenten einfacher sein.  Nehmen Sie zum Beispiel ein vierbeiniges Spinnenmodell.  Im Mujoco-Simulator sieht es so aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yp/ad/dg/ypaddgwogfekvvjemaf3vseohdi.png"></div><br><br>  Wir haben ihm die Aufgabe gestellt, mit der h√∂chstm√∂glichen Geschwindigkeit in eine bestimmte Richtung zu laufen - zum Beispiel nach rechts.  Die Anzahl der beobachteten Parameter f√ºr eine Spinne ist ein 39-dimensionaler Vektor, der die Position und Geschwindigkeit aller ihrer Gliedma√üen aufzeichnet.  Im Gegensatz zum neuronalen Netzwerk f√ºr Tischtennis, bei dem nur ein Neuron am Ausgang vorhanden war, befinden sich acht am Ausgang (da die Spinne in diesem Modell 8 Gelenke hat). <br><br>  In solch einfachen Modellen k√∂nnen verschiedene Hypothesen √ºber die Unterrichtstechnik getestet werden.  Vergleichen wir zum Beispiel die Lerngeschwindigkeit in Abh√§ngigkeit von der Art des neuronalen Netzwerks.  Sei es ein einschichtiges neuronales Netzwerk, ein dreischichtiges neuronales Netzwerk, ein Faltungsnetzwerk und ein wiederkehrendes Netzwerk: <br><br><img src="https://habrastorage.org/webt/sq/zl/xu/sqzlxuj-k_nts13x7x5veh-56ay.png"><br><br>  Die Schlussfolgerung kann wie folgt gezogen werden: Da das Spinnenmodell und die Aufgabe recht einfach sind, sind die Trainingsergebnisse f√ºr verschiedene Modelle ungef√§hr gleich.  Ein dreischichtiges Netzwerk ist zu komplex und lernt daher schlechter. <br><br>  Trotz der Tatsache, dass der Simulator mit einem einfachen Spinnenmodell arbeitet, kann das Training je nach der Aufgabe, die der Spinne gestellt wird, Tage dauern.  In diesem Fall animieren wir mehrere hundert Spinnen gleichzeitig auf einer Oberfl√§che anstatt auf einer und lernen aus den Daten, die wir von allen erhalten.  Wir werden das Training also um das Hundertfache beschleunigen.  Hier ist ein Beispiel f√ºr die Flex-Engine. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/p4/6l/4j/p46l4jyulhhspwa9e5z_4oe40oq.png"></div><br><br>  Das einzige, was sich in Bezug auf die Optimierung neuronaler Netze ge√§ndert hat, ist die Datenerfassung.  Wenn wir nur eine Spinne liefen, erhielten wir nacheinander Daten.  Ein Lauf nach dem anderen. <br><br><img src="https://habrastorage.org/webt/8t/cd/hr/8tcdhrcxqbx4mk3w99lacw2tfbu.png"><br><br>  Jetzt kann es vorkommen, dass einige Spinnen gerade das Rennen starten, w√§hrend andere schon lange laufen. <br><br><img src="https://habrastorage.org/webt/6u/i6/jv/6ui6jv9y3zmbdy3cwfweuv1drt4.png"><br><br>  Wir werden dies bei der Optimierung des neuronalen Netzwerks ber√ºcksichtigen.  Ansonsten bleibt alles beim Alten.  Infolgedessen wird das Training hunderte Male beschleunigt, je nach Anzahl der Spinnen, die gleichzeitig auf dem Bildschirm angezeigt werden. <br><br>  Da wir einen effektiven Simulator haben, versuchen wir, komplexere Probleme zu l√∂sen.  Zum Beispiel √ºber unwegsames Gel√§nde laufen. <br><br><img src="https://habrastorage.org/webt/zo/sq/dn/zosqdnbygiz_2p-cvxl4hvtuvqw.png"><br><br>  Da die Umgebung in diesem Fall aggressiver geworden ist, sollten wir die Aufgaben w√§hrend des Trainings √§ndern und komplizieren.  Es ist schwer zu lernen, aber im Kampf einfach.  Zum Beispiel alle paar Minuten, um das Gel√§nde zu wechseln.  Lassen Sie uns au√üerdem externe Agenten an den Agenten weiterleiten.  Lassen Sie uns zum Beispiel B√§lle auf ihn werfen und den Wind ein- und ausschalten.  Dann lernt der Agent, auch auf Oberfl√§chen zu laufen, die er noch nie getroffen hat.  Zum Beispiel Treppen steigen. <br><br><img src="https://habrastorage.org/webt/zc/-7/bq/zc-7bqzt5kcltfgrwfc9piw3mli.png"><br><br>  Da wir so effektiv gelernt haben, in Simulationen zu laufen, sollten wir die Techniken des Verst√§rkungstrainings in Wettbewerbsdisziplinen √ºberpr√ºfen.  Zum Beispiel bei Schie√üspielen.  Die VizDoom-Plattform bietet eine Welt, in der Sie schie√üen, Waffen sammeln und die Gesundheit wieder auff√ºllen k√∂nnen.  In diesem Spiel werden wir auch ein neuronales Netzwerk verwenden.  Erst jetzt hat sie f√ºnf Ausg√§nge: vier f√ºr Bewegung und einen f√ºr Schie√üen. <br><br>  Damit das Training effektiv ist, nehmen wir es schrittweise.  Von einfach bis komplex.  Am Eingang erh√§lt das neuronale Netzwerk ein Bild, und bevor es anf√§ngt, etwas Bewusstes zu tun, muss es lernen, zu verstehen, woraus die Welt besteht.  Wenn sie in einfachen Szenarien studiert, lernt sie zu verstehen, welche Objekte auf der Welt leben und wie sie mit ihnen interagieren.  Beginnen wir mit dem Bindestrich: <br><br><img src="https://habrastorage.org/webt/99/cy/xm/99cyxm9xxkmd3wg9zryiguyqnoe.png"><br><br>  Nachdem der Agent dieses Szenario gemeistert hat, wird er verstehen, dass es Feinde gibt, und sie sollten erschossen werden, da Sie Punkte f√ºr sie erhalten.  Dann werden wir ihn in einem Szenario trainieren, in dem die Gesundheit st√§ndig abnimmt und Sie sie wieder auff√ºllen m√ºssen. <br><br><img src="https://habrastorage.org/webt/06/gs/jr/06gsjr-tuvruizcpmy3da0gsyq4.jpeg"><br><br>  Hier erf√§hrt er, dass er gesund ist und wieder aufgef√ºllt werden muss, da der Agent im Todesfall eine negative Belohnung erh√§lt.  Au√üerdem wird er lernen, dass Sie es sammeln k√∂nnen, wenn Sie sich dem Thema n√§hern.  Im ersten Szenario konnte sich der Agent nicht bewegen. <br><br>  Und im letzten, dritten Szenario lassen wir ihn mit den Bots schie√üen, die nach den Spielregeln programmiert sind, damit er seine F√§higkeiten verbessern kann. <br><br><img src="https://habrastorage.org/webt/dn/pz/iq/dnpziqqditttkptyp7zflcdh4ag.png"><br><br>  W√§hrend des Trainings in diesem Szenario ist die richtige Auswahl der Belohnungen, die der Agent erh√§lt, sehr wichtig.  Wenn Sie beispielsweise nur f√ºr besiegte Rivalen eine Belohnung geben, ist das Signal sehr selten: Wenn nur wenige Spieler anwesend sind, erhalten wir alle paar Minuten Punkte.  Verwenden wir daher die Kombination der vorherigen Belohnungen.  Der Agent erh√§lt eine Belohnung f√ºr jede n√ºtzliche Aktion, unabh√§ngig davon, ob er die Gesundheit verbessert, Patronen ausw√§hlt oder einen Gegner trifft. <br><br><blockquote>  Infolgedessen ist ein Agent, der mit ausgew√§hlten Belohnungen trainiert wurde, st√§rker als seine rechenintensiveren Gegner.  2016 gewann ein solches System den VizDoom-Wettbewerb mit einem Vorsprung von mehr als der H√§lfte der vom zweiten Platz erzielten Punkte.  Das Zweitplatzierte Team verwendete auch ein neuronales Netzwerk, nur mit einer gro√üen Anzahl von Ebenen und zus√§tzlichen Informationen von der Spiel-Engine w√§hrend des Trainings.  Zum Beispiel Informationen dar√ºber, ob sich Feinde im Sichtfeld des Agenten befinden. </blockquote><br>  Wir haben L√∂sungsans√§tze untersucht, bei denen es wichtig ist, Entscheidungen zu treffen.  Viele Aufgaben mit diesem Ansatz bleiben jedoch ungel√∂st.  Zum Beispiel das Questspiel Montezuma Revenge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s9/vp/_9/s9vp_9botnvsnfqcaxutbv-2pn0.jpeg"></div><br><br>  Hier m√ºssen Sie nach Schl√ºsseln suchen, um die T√ºren zu benachbarten R√§umen zu √∂ffnen.  Wir bekommen selten Schl√ºssel und √∂ffnen noch seltener R√§ume.  Es ist auch wichtig, sich nicht von Fremdk√∂rpern ablenken zu lassen.  Wenn Sie das System wie in den vorherigen Aufgaben trainieren und Belohnungen f√ºr geschlagene Feinde vergeben, wird der rollende Sch√§del einfach immer wieder ausgeschaltet und die Karte nicht untersucht.  Wenn Sie interessiert sind, kann ich in einem separaten Artikel √ºber die L√∂sung solcher Probleme sprechen. <br><br>  <b>Sie k√∂nnen die Rede von Vladimir Ivanov auf der AI-Konferenz am 22. November h√∂ren</b> .  Ein detailliertes Programm und Tickets finden Sie auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Website der</a> Veranstaltung. <br><br>  Lesen Sie hier das Interview mit Vladimir. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428329/">https://habr.com/ru/post/de428329/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428313/index.html">Telegramm unter MacOS speichert [vermutlich] auch lokal Korrespondenz in einer zug√§nglichen Form</a></li>
<li><a href="../de428315/index.html">5 √Ñngste vor Entwicklern, die wir √ºberwunden haben</a></li>
<li><a href="../de428317/index.html">Haken reagieren - gewinnen oder verlieren?</a></li>
<li><a href="../de428321/index.html">Predictive Data Analytics - Modellierung und Validierung</a></li>
<li><a href="../de428327/index.html">Worauf Sie achten sollten: Europ√§ische eIDAS-Verordnung zur elektronischen Identifizierung</a></li>
<li><a href="../de428333/index.html">2018 RAIF Hackathon AI Hackathon Ergebnisse</a></li>
<li><a href="../de428335/index.html">Siri Shortcut Update</a></li>
<li><a href="../de428337/index.html">Unterhaltsames JavaScript: Ohne geschweifte Klammern</a></li>
<li><a href="../de428339/index.html">Automatisieren Sie es nicht: Schlechte Gesch√§ftstipps</a></li>
<li><a href="../de428341/index.html">Qsan RAID EE-Technologie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>