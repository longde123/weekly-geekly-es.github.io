<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛌🏻 👩‍👩‍👦 📞 Styling von Musik mit neuronalen Netzen 🤶🏿 🙅🏿 🐥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den letzten zehn Jahren haben sich Deep Neural Networks (DNNs) zu einem hervorragenden Werkzeug für eine Reihe von KI-Aufgaben wie Bildklassifizier...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Styling von Musik mit neuronalen Netzen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/409697/"><p><img src="https://habrastorage.org/webt/qu/r7/29/qur729mzadpog93xrhjwkj9c51i.jpeg"></p><br><p>  In den letzten zehn Jahren haben sich Deep Neural Networks (DNNs) zu einem hervorragenden Werkzeug für eine Reihe von KI-Aufgaben wie Bildklassifizierung, Spracherkennung und sogar Teilnahme an Spielen entwickelt.  Als die Entwickler versuchten zu zeigen, was den Erfolg von DNN im Bereich der Bildklassifizierung verursacht hat, und Visualisierungstools (z. B. Deep Dream, Filter) erstellten, um zu verstehen, „was“ das DNN-Modell genau „studiert“, entstand eine neue interessante Anwendung : „Stil“ aus einem Bild extrahieren und auf einen anderen, anderen Inhalt anwenden.  Dies wurde als "Bildstilübertragung" bezeichnet. </p><a name="habracut"></a><br><p><img src="https://habrastorage.org/webt/ma/w_/uy/maw_uyr88ft3g7oiwpx8yp6vfwi.jpeg"><br>  <em>Links: Bild mit nützlichem Inhalt, in der Mitte: Bild mit Stil, rechts: Inhalt + Stil (Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Research Blog</a> )</em> </p><br><p>  Dies weckte nicht nur das Interesse vieler anderer Forscher (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> ), sondern führte auch zur Entstehung mehrerer erfolgreicher mobiler Anwendungen.  In den letzten Jahren haben sich diese visuellen Übertragungsmethoden erheblich verbessert. </p><br><p><img src="https://habrastorage.org/webt/ij/oz/7-/ijoz7-xp5fibweiwdt3skobj_cy.jpeg"><br>  <em>Wrapping im Adobe-Stil (Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Engadget</a> )</em> </p><br><p><img src="https://habrastorage.org/webt/rx/sx/gc/rxsxgcb9dxcf7gk5iaq6e2dkjbo.jpeg"><br>  <em>Beispiel von der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prisma-</a> Website</em> </p><br><p>  Eine kurze Einführung in solche Algorithmen: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/WHmp26bh0tI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Trotz der Fortschritte bei der Arbeit mit Bildern war die Anwendung dieser Techniken in anderen Bereichen, beispielsweise zur Verarbeitung von Musik, sehr begrenzt (siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">4</a> ), und die Ergebnisse sind überhaupt nicht so beeindruckend wie bei Bildern.  Dies deutet darauf hin, dass es viel schwieriger ist, Stil in der Musik zu übertragen.  In diesem Artikel werden wir das Problem genauer untersuchen und einige mögliche Ansätze diskutieren. </p><br><h2 id="pochemu-tak-trudno-perenosit-stil-v-muzyke">  Warum ist es so schwierig, Stil in der Musik zu übertragen? </h2><br><p>  Beantworten wir zunächst die Frage: <strong>Was ist "Stilübertragung" in der Musik</strong> ?  Die Antwort ist nicht so offensichtlich.  In Bildern sind die Konzepte „Inhalt“ und „Stil“ intuitiv.  "Bildinhalt" beschreibt die dargestellten Objekte, z. B. Hunde, Häuser, Gesichter usw., und "Bildstil" bezieht sich auf Farben, Beleuchtung, Pinselstriche und Textur. </p><br><p>  Musik ist jedoch <strong>semantisch abstrakt und mehrdimensionaler</strong> Natur.  "Musikinhalt" kann in verschiedenen Kontexten unterschiedliche Bedeutungen haben.  Oft ist der Inhalt der Musik mit einer Melodie verbunden und der Stil mit einem Arrangement oder einer Harmonisierung.  Der Inhalt kann jedoch der Text sein, und verschiedene Melodien, die zum Singen verwendet werden, können als verschiedene Stile interpretiert werden.  In der klassischen Musik kann der Inhalt als Partitur betrachtet werden (einschließlich Harmonisierung), während der Stil die Interpretation der Noten durch den Interpreten ist, der seinen eigenen Ausdruck einbringt (Variieren und Hinzufügen einiger Klänge von sich selbst).  Schauen Sie sich einige dieser Videos an, um die Essenz der Stilübertragung in der Musik besser zu verstehen: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S75gYhODS0M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><iframe width="560" height="315" src="https://www.youtube.com/embed/buXqNqBFd6E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Im zweiten Video werden verschiedene <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Techniken des</a> maschinellen Lernens verwendet. </p><br><p>  Daher ist die Übertragung von Stil in der Musik per Definition schwer zu formalisieren.  Es gibt andere Schlüsselfaktoren, die die Aufgabe erschweren: </p><br><ol><li>  <strong>Maschinen BAD verstehen Musik</strong> ( <strong>vorerst</strong> ): Der Erfolg bei der Übertragung von Stil in Bildern beruht auf dem Erfolg von DNN bei Aufgaben im Zusammenhang mit dem Verständnis von Bildern, wie z. B. der Objekterkennung.  Da DNNs Eigenschaften lernen können, die von Objekt zu Objekt unterschiedlich sind, können Backpropagation-Techniken verwendet werden, um das Zielbild so zu ändern, dass es den Eigenschaften des Inhalts entspricht.  Obwohl wir erhebliche <a href="">Fortschritte</a> bei der Erstellung von DNN-basierten Modellen erzielt haben, die in der Lage sind, musikalische Aufgaben zu verstehen (z. B. das Transkribieren von Melodien, das Definieren eines Genres usw.), sind wir noch weit von den in der Bildverarbeitung erreichten Höhen entfernt.  Dies ist ein ernstes Hindernis für die Übertragung von Stil in der Musik.  Bestehende Modelle können einfach nicht die „hervorragenden“ Eigenschaften erlernen, mit denen Musik klassifiziert werden kann. Dies bedeutet, dass die direkte Anwendung von Stilübertragungsalgorithmen, die beim Arbeiten mit Bildern verwendet werden, nicht zum gleichen Ergebnis führt. </li><li>  Musik ist <strong>flüchtig</strong> : Es sind Daten, die dynamische Serien darstellen, dh ein musikalisches Fragment ändert sich mit der Zeit.  Dies erschwert das Lernen.  Obwohl wiederkehrende neuronale Netze und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LSTM</a> (Long Short-Term Memory) es Ihnen ermöglichen, mehr aus transienten Daten zu lernen, müssen wir dennoch zuverlässige Modelle erstellen, die lernen, wie die langfristige Struktur von Musik reproduziert werden kann (Hinweis: Dies ist ein aktuelles Forschungsgebiet, und Wissenschaftler des Google-Teams Magenta hat hier einige Erfolge erzielt. </li><li>  Musik ist <strong>diskret</strong> (zumindest auf symbolischer Ebene): Symbolisch oder auf Papier aufgenommene Musik ist diskreter Natur.  In dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einheitlichen Temperament</a> , dem heute beliebtesten Stimmsystem für Musikinstrumente, nehmen Klangtöne auf einer kontinuierlichen Frequenzskala diskrete Positionen ein.  Gleichzeitig liegt die Dauer der Töne auch im diskreten Raum (normalerweise Vierteltöne, Volltöne usw.).  Daher ist es sehr schwierig, die Pixel-Back-Propagation-Methoden (die zum Arbeiten mit Bildern verwendet werden) im Bereich der symbolischen Musik anzupassen. </li></ol><br><p><img src="https://habrastorage.org/webt/tb/cj/a0/tbcja0feucuwkgawqqlxqvfqtna.png"><br>  <em>Die diskrete Natur von Noten in einem einheitlichen Temperament.</em> </p><br><p>  Daher sind die Techniken zur Übertragung des Stils in Bildern nicht direkt auf Musik anwendbar.  Dazu müssen sie mit Schwerpunkt auf musikalischen Konzepten und Ideen verarbeitet werden. </p><br><h2 id="dlya-chego-nuzhen-perenos-stilya-v-muzyke">  Wofür ist die Übertragung von Stil in der Musik? </h2><br><p>  Warum müssen Sie dieses Problem lösen?  Wie bei Bildern sind die möglichen Anwendungen der Stilübertragung in der Musik sehr interessant.  Zum Beispiel die <strong>Entwicklung eines Tools zur Unterstützung von Komponisten</strong> .  Zum Beispiel ist ein automatisches Instrument, das eine Melodie mithilfe von Arrangements aus verschiedenen Genres transformieren kann, für Komponisten äußerst nützlich, die schnell verschiedene Ideen ausprobieren müssen.  DJs werden sich auch für solche Instrumente interessieren. </p><br><p>  Ein indirektes Ergebnis dieser Forschung wird eine signifikante Verbesserung der Musikinformatik sein.  Wie oben erläutert, müssen die von uns erstellten Modelle für die Übertragung des Stils auf die Arbeit in der Musik lernen, verschiedene Aspekte besser zu <strong>"verstehen"</strong> . </p><br><h2 id="uproschenie-zadachi-perenosa-stilya-v-muzyke">  Vereinfachen Sie die Übertragung von Stilen in der Musik </h2><br><p>  Beginnen wir mit einer sehr einfachen Aufgabe, monophone Melodien in verschiedenen Genres zu analysieren.  Monophone Melodien sind Notenfolgen, die jeweils durch Ton und Dauer bestimmt werden.  Der Tonhöhenverlauf hängt größtenteils von der Tonleiter der Melodie ab, und der Verlauf der Dauer hängt vom Rhythmus ab.  Zunächst werden wir <strong>"</strong> Tonhöheninhalt" und <strong>"rhythmischen Stil"</strong> klar als zwei Einheiten <strong>trennen</strong> , mit denen Sie die Aufgabe der Stilübertragung umformulieren können.  Wenn wir mit monophonen Melodien arbeiten, vermeiden wir jetzt auch die Aufgaben, die mit Arrangement und Text verbunden sind. </p><br><p>  In Ermangelung vorgefertigter Modelle, die erfolgreich zwischen Tonverläufen und Rhythmen monophoner Melodien unterscheiden können, greifen wir zunächst auf einen sehr einfachen Ansatz zur Stilübertragung zurück.  Anstatt zu versuchen, den in der Zielmelodie erlernten Toninhalt durch den im Zielrhythmus erlernten rhythmischen Stil zu ändern, werden wir versuchen, Muster von Tönen und Dauern aus verschiedenen Genres einzeln zu unterrichten und sie dann zu kombinieren.  Ungefähres Schema des Ansatzes: </p><br><p><img src="https://habrastorage.org/webt/ek/i8/3o/eki83obepvf1nd44pgzupr-czmu.png"><br>  <em>Schema der Methode des Intergenre-Style-Transfers.</em> </p><br><h2 id="obuchaem-otdelno-tonovym-i-ritmovym-progressiyam">  Wir unterrichten getrennt Ton- und Rhythmusverläufe </h2><br><p>  <strong><em>Datenpräsentation</em></strong> </p><br><p>  Wir werden monophone Melodien als eine Folge von Noten präsentieren, von denen jede einen Tonindex und eine Folge hat.  Damit unser Präsentationsschlüssel unabhängig ist, verwenden wir die Präsentation anhand von Intervallen: Der Ton der nächsten Note wird als Abweichung (± Halbton) vom Ton der vorherigen Note dargestellt.  Erstellen wir zwei Wörterbücher für Töne und Dauern, in denen jedem diskreten Zustand (für Ton: +1, -1, +2, -2 usw.; für Dauern: eine Viertelnote, eine vollständige Note, ein Viertel mit einem Punkt usw.) ein Index zugewiesen wird Wörterbuch. </p><br><p><img src="https://habrastorage.org/webt/v2/h6/z6/v2h6z6pbrq6ezu9br3jv_actjia.png"><br>  <em>Präsentation von Daten.</em> </p><br><p>  <strong><em>Modellarchitektur</em></strong> </p><br><p>  Wir werden dieselbe Architektur verwenden, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Colombo und Kollegen verwendet haben</a> - sie haben gleichzeitig zwei neuronale LSTM-Netze demselben Musikgenre beigebracht: a) das Tonnetzwerk, das gelernt hat, den nächsten Ton basierend auf der vorherigen Note und der vorherigen Dauer vorherzusagen, b) das Dauer-Netzwerk, das gelernt hat, die nächste Dauer basierend auf der nächsten Note vorherzusagen und vorherige Dauer.  Außerdem werden wir vor LSTM-Netzwerken Einbettungsebenen hinzufügen, um Eingangstonindizes und -dauern in gespeicherten Einbettungsräumen zu vergleichen.  Die Architektur des neuronalen Netzwerks ist im Bild dargestellt: </p><br><p><img src="https://habrastorage.org/webt/6q/fg/nk/6qfgnkiake_rls5yymzbvxdvzg0.png"></p><br><p>  <strong><em>Trainingsverfahren</em></strong> </p><br><p>  Für jedes Genre werden gleichzeitig Netzwerke trainiert, die für Töne und Dauer verantwortlich sind.  Wir werden zwei Datensätze verwenden: a) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Norbeck Folk Dataset</a> , der ungefähr 2.000 irische und schwedische Volkslieder abdeckt, b) einen Jazzdatensatz (nicht öffentlich verfügbar), der ungefähr 500 Jazzmelodien abdeckt. </p><br><p>  <strong><em>Zusammenführung trainierter Modelle</em></strong> </p><br><p>  Während des Testens wird die Melodie zuerst unter Verwendung des Tonnetzwerks und des im ersten Genre trainierten Dauer-Netzwerks (z. B. Folk) erzeugt.  Dann wird die Tonfolge aus der erzeugten Melodie am Eingang für ein Netzwerk von Sequenzen verwendet, die in einem anderen Genre (z. B. Jazz) trainiert wurden, und das Ergebnis ist eine neue Folge von Dauern.  Daher hat eine Melodie, die unter Verwendung einer Kombination von zwei neuronalen Netzen erzeugt wird, eine Folge von Tönen, die dem ersten Genre (Folk) entsprechen, und eine Folge von Dauern, die dem zweiten Genre (Jazz) entsprechen. </p><br><h2 id="predvaritelnye-rezultaty">  Vorläufige Ergebnisse </h2><br><p>  Kurze Auszüge aus einigen der resultierenden Melodien: <br>  <a href="">Volkstöne und Volksdauern</a> </p><br><p><img src="https://habrastorage.org/webt/c9/ic/zv/c9iczvv4qkjjv6lve-sj5wnb3dk.png"><br>  <em>Auszug aus der Notenschrift.</em> </p><br><p>  <a href="">Volkstöne und Jazzdauern</a> </p><br><p><img src="https://habrastorage.org/webt/-p/oz/57/-poz57oqe5kpunzo3jqpikcjjju.png"><br>  <em>Auszug aus der Notenschrift.</em> </p><br><p>  <a href="">Jazz-Töne und Jazz-Sequenzen</a> </p><br><p><img src="https://habrastorage.org/webt/8s/ia/4x/8sia4xupwzfbnz2tyzof4xw14li.png"><br>  <em>Auszug aus der Notenschrift</em> . </p><br><p>  <a href="">Jazz-Töne und Folk-Sequenzen</a> </p><br><p><img src="https://habrastorage.org/webt/hf/mh/8p/hfmh8p9kyorpupmmvydj_psxb9u.png"><br>  <em>Auszug aus der Notenschrift.</em> </p><br><h2 id="zaklyuchenie">  Fazit </h2><br><p>  Obwohl der aktuelle Algorithmus zunächst nicht schlecht ist, weist er eine Reihe kritischer Nachteile auf: </p><br><ol><li>  <strong>Es ist unmöglich, den Stil basierend auf einer bestimmten Zielmelodie zu übertragen</strong> .  Modelle lernen Muster von Tönen und Dauern in einem Genre, was bedeutet, dass alle Transformationen vom Genre bestimmt werden.  Es wäre ideal, ein Musikstück im Stil eines bestimmten Zielsongs oder -stücks zu modifizieren. </li><li>  <strong>Es ist nicht möglich, den Grad der</strong> Stiländerung <strong>zu steuern</strong> .  Es wäre sehr interessant, einen „Griff“ zu bekommen, der diesen Aspekt regelt. </li><li>  Beim Zusammenführen von Genres ist <strong>es unmöglich, die musikalische Struktur</strong> in einer transformierten Melodie beizubehalten.  Eine langfristige Struktur ist wichtig für die musikalische Bewertung im Allgemeinen, und damit die erzeugten Melodien musikalisch ästhetisch sind, muss die Struktur erhalten bleiben. </li></ol><br><p>  In den folgenden Artikeln werden Möglichkeiten zur Umgehung dieser Mängel untersucht. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de409697/">https://habr.com/ru/post/de409697/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de409687/index.html">Freitag Format: Frauen in der elektronischen Musik - Wendy Carlos und Susan Chani</a></li>
<li><a href="../de409689/index.html">Die Glücklichen und Verlierer in der Welt des Bitcoin: 7 Geschichten stammen aus dem Jahr 2017</a></li>
<li><a href="../de409691/index.html">Medien: Die USA werden die Finanzierung der ISS im Jahr 2025 einstellen</a></li>
<li><a href="../de409693/index.html">Temperatur- und Druckfiktion, 2/3</a></li>
<li><a href="../de409695/index.html">486. aus der Republik China</a></li>
<li><a href="../de409699/index.html">Die am meisten erwarteten wissenschaftlichen Experimente des nächsten Jahrzehnts</a></li>
<li><a href="../de409701/index.html">Die Behörden werden Bergleute in der Russischen Föderation anhand von Stromrechnungen identifizieren</a></li>
<li><a href="../de409703/index.html">KI und unbemannte Fahrzeuge - willkommen zum Reden</a></li>
<li><a href="../de409705/index.html">Kingston Nucleum USB-Hub - verwandelt einen portlosen Laptop in eine vollwertige Arbeitsmaschine</a></li>
<li><a href="../de409707/index.html">Taschentastatur oder sinnloses, aber niedliches Design</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>