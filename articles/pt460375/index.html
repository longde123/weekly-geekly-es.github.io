<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëñ ‚ô†Ô∏è üö∫ Livro "Aprendizado de m√°quina para neg√≥cios e marketing" üë©üèº‚Äçüé§ üßíüèº ü§ûüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A ci√™ncia de dados est√° se tornando parte integrante de qualquer atividade de marketing, e este livro √© um retrato vivo da transforma√ß√£o digital em ma...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livro "Aprendizado de m√°quina para neg√≥cios e marketing"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/460375/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/gq/td/mc/gqtdmc8joactk6gu7xrzcdr0f4i.jpeg" align="left" alt="imagem"></a>  A ci√™ncia de dados est√° se tornando parte integrante de qualquer atividade de marketing, e este livro √© um retrato vivo da transforma√ß√£o digital em marketing.  A an√°lise de dados e algoritmos inteligentes automatizam tarefas de marketing demoradas.  O processo de tomada de decis√£o est√° se tornando n√£o apenas mais perfeito, mas tamb√©m mais r√°pido, o que √© de grande import√¢ncia em um ambiente competitivo em constante acelera√ß√£o. <br><br>  ‚ÄúEste livro √© um retrato vivo da transforma√ß√£o digital em marketing.  Ele mostra como a ci√™ncia de dados est√° se tornando parte integrante de qualquer atividade de marketing.  Ele descreve em detalhes como abordagens baseadas em an√°lise de dados e algoritmos inteligentes contribuem para a automa√ß√£o profunda de tarefas de marketing tradicionalmente trabalhosas.  O processo de tomada de decis√£o est√° se tornando n√£o apenas mais avan√ßado, mas tamb√©m mais r√°pido, o que √© importante em nosso ambiente competitivo em constante acelera√ß√£o.  Este livro deve ser lido por especialistas em processamento de dados e especialistas em marketing, e √© melhor se eles lerem juntos. ‚Äù  Andrey Sebrant, diretor de marketing estrat√©gico da Yandex. <br><a name="habracut"></a><br><h3>  Trecho.  5.8.3  Modelos de fator oculto </h3><br>  Nos algoritmos de filtragem conjunta discutidos at√© agora, a maioria dos c√°lculos √© baseada nos elementos individuais da matriz de classifica√ß√£o.  Os m√©todos baseados em proximidade avaliam as classifica√ß√µes ausentes diretamente dos valores conhecidos na matriz de classifica√ß√µes.  Os m√©todos baseados em modelo adicionam uma camada de abstra√ß√£o na parte superior da matriz de classifica√ß√£o, criando um modelo preditivo que captura certos padr√µes de relacionamento entre usu√°rios e elementos, mas o treinamento do modelo ainda √© altamente dependente das propriedades da matriz de classifica√ß√£o.  Como resultado, essas t√©cnicas de filtragem colaborativa geralmente enfrentam os seguintes problemas: <br><br>  A matriz de classifica√ß√£o pode conter milh√µes de usu√°rios, milh√µes de elementos e bilh√µes de classifica√ß√µes conhecidas, o que cria s√©rios problemas de complexidade e escalabilidade computacional. <br><br>  A matriz de classifica√ß√£o √© geralmente muito escassa (na pr√°tica, cerca de 99% das classifica√ß√µes podem estar ausentes).  Isso afeta a estabilidade computacional dos algoritmos de recomenda√ß√£o e leva a estimativas n√£o confi√°veis ‚Äã‚Äãquando o usu√°rio ou elemento n√£o possui vizinhos realmente semelhantes.  Esse problema geralmente √© exacerbado pelo fato de que os algoritmos mais b√°sicos s√£o orientados ao usu√°rio ou ao elemento, o que limita sua capacidade de registrar todos os tipos de semelhan√ßas e relacionamentos dispon√≠veis na matriz de classifica√ß√£o. <br><br>  Os dados na matriz de classifica√ß√£o s√£o geralmente fortemente correlacionados devido a semelhan√ßas entre usu√°rios e elementos.  Isso significa que os sinais dispon√≠veis na matriz de classifica√ß√£o s√£o n√£o apenas esparsos, mas tamb√©m redundantes, o que contribui para a exacerba√ß√£o do problema de escalabilidade. <br><br>  As considera√ß√µes acima indicam que a matriz de classifica√ß√£o original pode n√£o ser a melhor representa√ß√£o de sinais, e outras representa√ß√µes alternativas mais adequadas para a filtragem de juntas devem ser consideradas.  Para explorar essa id√©ia, voltemos ao ponto de partida e pensemos um pouco sobre a natureza dos servi√ßos de recomenda√ß√£o.  De fato, o servi√ßo de recomenda√ß√£o pode ser considerado como um algoritmo que prev√™ classifica√ß√µes com base em alguma medida de similaridade entre o usu√°rio e o elemento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/bc/6n/atbc6no-aj2vssp1mrgyctgu_oy.png" alt="imagem"></div><br>  Uma maneira de determinar essa medida de similaridade √© usar a abordagem de fatores ocultos e mapear usu√°rios e elementos para pontos em algum espa√ßo k-dimensional, para que cada usu√°rio e cada elemento seja representado por um vetor k-dimensional: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9e/ck/no/9eckno4ntrf2yl0hw1q2tr2irbk.png" alt="imagem"></div><br>  Os vetores devem ser constru√≠dos de modo que as dimens√µes correspondentes peq sejam compar√°veis ‚Äã‚Äãentre si.  Em outras palavras, cada dimens√£o pode ser considerada como um sinal ou conceito, ou seja, puj √© uma medida da proximidade do usu√°rio u e do conceito j, e qij, respectivamente, √© uma medida do elemento ie do conceito j.  Na pr√°tica, essas dimens√µes s√£o frequentemente interpretadas como g√™neros, estilos e outros atributos que se aplicam simultaneamente a usu√°rios e elementos.  A semelhan√ßa entre o usu√°rio e o elemento e, consequentemente, a classifica√ß√£o pode ser definida como o produto dos vetores correspondentes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k3/1w/9w/k31w9wqmwas5pfvhr8_mopo7ibg.png" alt="imagem"></div><br>  Como cada classifica√ß√£o pode ser decomposta em um produto de dois vetores que pertencem a um espa√ßo conceitual que n√£o √© observado diretamente na matriz de classifica√ß√£o original, p e q s√£o chamados de fatores ocultos.  O sucesso dessa abordagem abstrata, √© claro, depende inteiramente de como os fatores ocultos s√£o determinados e constru√≠dos.  Para responder a essa pergunta, observamos que a express√£o 5.92 pode ser reescrita na forma de matriz da seguinte maneira: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2i/bp/j9/2ibpj9pmptpun2amylvxf41okjw.png" alt="imagem"></div><br>  onde P √© a matriz n √ó k montada a partir dos vetores p, e Q √© a matriz m √ó k montada a partir dos vetores q, como mostrado na Fig.  5.13  O principal objetivo de um sistema de filtragem conjunta √© geralmente minimizar os erros de previs√£o da classifica√ß√£o, o que permite determinar diretamente o problema de otimiza√ß√£o em rela√ß√£o √† matriz de fatores ocultos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9h/nm/mx/9hnmmxregnvn9snp9empxqf91qk.png" alt="imagem"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z-/vl/nu/z-vlnuz-v9git5dmm0nj79etl5g.png" alt="imagem"></div><br>  Supondo que o n√∫mero de dimens√µes ocultas k seja fixo e k ‚â§ n e k ‚â§ m, o problema de otimiza√ß√£o 5,94 reduz-se ao problema de aproxima√ß√£o de baixa classifica√ß√£o, que consideramos no Cap√≠tulo 2. Para demonstrar a abordagem da solu√ß√£o, vamos assumir por um momento que a matriz de classifica√ß√£o est√° completa.  Nesse caso, o problema de otimiza√ß√£o possui uma solu√ß√£o anal√≠tica em termos da Decomposi√ß√£o de Valor Singular (SVD) da matriz de classifica√ß√£o.  Em particular, usando o algoritmo SVD padr√£o, a matriz pode ser decomposta no produto de tr√™s matrizes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7z/ic/x0/7zicx0iwp156hctp6t7axd5bzu4.png" alt="imagem"></div><br>  onde U √© a matriz n √ó n ortonormalizada por colunas, Œ£ √© a matriz diagonal n √ó m e V √© a matriz m √ó m ortonormalizada por colunas.  Uma solu√ß√£o ideal para o problema 5.94 pode ser obtida em termos desses fatores, truncados para as k dimens√µes mais significativas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/i-/os/mwi-oskhgcs-ehf_bhepytyo_os.png" alt="imagem"></div><br>  Consequentemente, fatores ocultos que s√£o √≥timos em termos de precis√£o de previs√£o podem ser obtidos por decomposi√ß√£o singular, como mostrado abaixo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uc/ih/yl/ucihyltxuqye29vkocknsbzyguw.png" alt="imagem"></div><br>  Esse modelo de fator oculto baseado em SVD ajuda a resolver os problemas de co-filtragem descritos no in√≠cio desta se√ß√£o.  Primeiro, ela substitui a matriz de classifica√ß√£o n √ó m grande por matrizes de fator n √ó k e m √ó k, que geralmente s√£o muito menores, porque, na pr√°tica, o n√∫mero ideal de dimens√µes ocultas k √© geralmente pequeno.  Por exemplo, h√° um caso em que a matriz de classifica√ß√£o com 500.000 usu√°rios e 17.000 elementos foi capaz de ser razoavelmente bem aproximada usando 40 medi√ß√µes [Funk, 2016].  Al√©m disso, o SVD elimina a correla√ß√£o na matriz de classifica√ß√£o: as matrizes de fatores latentes definidas por 5,97 s√£o ortonormais em colunas, ou seja, as dimens√µes ocultas n√£o s√£o correlacionadas.  Se, o que geralmente √© verdade na pr√°tica, o SVD tamb√©m resolve o problema de escassez, porque o sinal presente na matriz de classifica√ß√£o inicial √© efetivamente concentrado (lembre-se de que selecionamos k dimens√µes com a maior energia de sinal) e as matrizes de fator latente n√£o s√£o escassas.  A Figura 5.14 ilustra essa propriedade.  O algoritmo de proximidade baseado no usu√°rio (5.14, a) recolhe vetores de classifica√ß√£o esparsos para um determinado elemento e um determinado usu√°rio para obter uma pontua√ß√£o de classifica√ß√£o.  O modelo de fator oculto (5.14, b), pelo contr√°rio, estima a classifica√ß√£o por convolu√ß√£o de dois vetores de dimens√£o reduzida e com maior densidade de energia. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v3/te/fm/v3tefmto-k2yf54og0xn4rpbxlq.png" alt="imagem"></div><br>  A abordagem descrita acima parece uma solu√ß√£o coerente para o problema dos fatores ocultos, mas, na verdade, apresenta uma s√©ria desvantagem devido √† suposi√ß√£o de que a matriz de classifica√ß√£o est√° completa.  Se a matriz de classifica√ß√£o for escassa, o que √© quase sempre o caso, o algoritmo SVD padr√£o n√£o pode ser aplicado diretamente, pois n√£o √© capaz de processar elementos ausentes (indefinidos).  A solu√ß√£o mais simples nesse caso √© preencher as classifica√ß√µes ausentes com algum valor padr√£o, mas isso pode levar a um s√©rio vi√©s na previs√£o.  Al√©m disso, √© computacionalmente ineficiente porque a complexidade computacional de tal solu√ß√£o √© igual √† complexidade SVD para a matriz n √ó m completa, enquanto √© desej√°vel ter um m√©todo com complexidade proporcional ao n√∫mero de classifica√ß√µes conhecidas.  Esses problemas podem ser resolvidos usando os m√©todos alternativos de decomposi√ß√£o descritos nas se√ß√µes a seguir. <br><br><h3>  5.8.3.1  Decomposi√ß√£o ilimitada </h3><br>  O algoritmo SVD padr√£o √© uma solu√ß√£o anal√≠tica para o problema de aproxima√ß√£o de baixo escal√£o.  No entanto, esse problema pode ser considerado como um problema de otimiza√ß√£o, e tamb√©m podem ser aplicados m√©todos de otimiza√ß√£o universal.  Uma das abordagens mais simples √© usar o m√©todo de descida de gradiente para refinar iterativamente os valores de fatores ocultos.  O ponto de partida √© a defini√ß√£o da fun√ß√£o de custo J como o erro de previs√£o residual: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g8/d_/gk/g8d_gkpksz8-d3wyg0xs9ca60a8.png" alt="imagem"></div><br>  Observe que desta vez n√£o impomos restri√ß√µes, como a ortogonalidade, √† matriz de fatores ocultos.  Calculando o gradiente da fun√ß√£o de custo em rela√ß√£o a fatores ocultos, obtemos o seguinte resultado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jn/_l/cu/jn_lcurj48vk0kuluh8mojadgcy.png" alt="imagem"></div><br>  onde E √© a matriz de erro residual: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6z/wt/at/6zwtat7l4h9qoo0fq7ejzatlobi.png" alt="imagem"></div><br>  O algoritmo de descida do gradiente minimiza a fun√ß√£o de custo movendo-se em cada etapa na dire√ß√£o negativa do gradiente.  Portanto, voc√™ pode encontrar fatores ocultos que minimizam o erro quadrado da previs√£o de classifica√ß√£o alterando iterativamente as matrizes P e Q para convergir, de acordo com as seguintes express√µes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/uw/gu/f9uwguspa118fw__nuchwxu1ycy.png" alt="imagem"></div><br>  onde Œ± √© a velocidade de aprendizado.  A desvantagem do m√©todo de descida de gradiente √© a necessidade de calcular toda a matriz de erros residuais e alterar simultaneamente todos os valores dos fatores ocultos em cada itera√ß√£o.  Uma abordagem alternativa, que pode ser mais adequada para matrizes grandes, √© a descida estoc√°stica do gradiente [Funk, 2016].  O algoritmo de descida do gradiente estoc√°stico usa o fato de que o erro total de previs√£o J √© a soma dos erros de elementos individuais da matriz de classifica√ß√£o; portanto, o gradiente geral J pode ser aproximado por um gradiente em um ponto de dados e os fatores ocultos podem ser alterados em termos de elementos.  A implementa√ß√£o completa dessa id√©ia √© mostrada no algoritmo 5.1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/le/c5/ct/lec5ctumggjq0twtb30wnamvj0g.png" alt="imagem"></div><br>  O primeiro est√°gio do algoritmo √© a inicializa√ß√£o da matriz de fatores ocultos.  A escolha desses valores iniciais n√£o √© muito importante, mas, neste caso, √© escolhida uma distribui√ß√£o uniforme da energia das classifica√ß√µes conhecidas entre os fatores ocultos gerados aleatoriamente.  Em seguida, o algoritmo otimiza sequencialmente as dimens√µes do conceito.  Para cada medi√ß√£o, ele percorre repetidamente todas as classifica√ß√µes no conjunto de treinamento, prediz cada classifica√ß√£o usando os valores atuais dos fatores ocultos, estima o erro e corrige os valores dos fatores de acordo com as express√µes 5.101.  A otimiza√ß√£o da medi√ß√£o √© conclu√≠da quando a condi√ß√£o de converg√™ncia √© atendida, ap√≥s o que o algoritmo prossegue para a pr√≥xima medi√ß√£o. <br><br>  O algoritmo 5.1 ajuda a superar as limita√ß√µes do m√©todo SVD padr√£o.  Ele otimiza fatores ocultos percorrendo pontos de dados individuais e, assim, evitando problemas com classifica√ß√µes ausentes e opera√ß√µes alg√©bricas com matrizes gigantes.  A abordagem iterativa tamb√©m torna a descida do gradiente estoc√°stico mais conveniente para aplica√ß√µes pr√°ticas do que a descida do gradiente, que modifica matrizes inteiras usando as express√µes 5.101. <br><br><h3>  EXEMPLO 5.6 </h3><br>  De fato, uma abordagem baseada em fatores ocultos √© um grupo inteiro de m√©todos de ensino de representa√ß√µes que podem identificar padr√µes impl√≠citos na matriz de classifica√ß√£o e represent√°-los explicitamente na forma de conceitos.  √Äs vezes, os conceitos t√™m uma interpreta√ß√£o completamente significativa, especialmente de alta energia, embora isso n√£o signifique que todos os conceitos sempre tenham um significado significativo.  Por exemplo, a aplica√ß√£o do algoritmo de decomposi√ß√£o da matriz a um banco de dados de classifica√ß√µes de filmes pode criar fatores que correspondem aproximadamente a dimens√µes psicogr√°ficas, como melodrama, com√©dia, horror etc. Vamos ilustrar esse fen√¥meno com um pequeno exemplo num√©rico que usa a matriz de classifica√ß√£o da Tabela.  5.3: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/6a/vu/xw6avuizjd2x8aku8k-0r89ieem.png" alt="imagem"></div><br>  Primeiro, subtraia a m√©dia global Œº = 2,82 de todos os elementos para centralizar a matriz e, em seguida, execute o algoritmo 5.1 com k = 3 medi√ß√µes ocultas e a taxa de aprendizado Œ± = 0,01 para obter as duas seguintes matrizes de fatores: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k-/rr/vd/k-rrvde1sezar3qo0e2vqfb7jh8.png" alt="imagem"></div><br>  Cada linha nessas matrizes corresponde a um usu√°rio ou um filme, e todos os 12 vetores de linha s√£o mostrados na Fig.  5.15  Observe que os elementos na primeira coluna (o primeiro vetor de conceitos) t√™m os maiores valores e os valores nas colunas subsequentes diminuem gradualmente.  Isso √© explicado pelo fato de que o primeiro vetor conceitual captura tanta energia de sinal quanto poss√≠vel de capturar com uma medi√ß√£o, o segundo vetor conceitual captura apenas parte da energia residual, etc. Al√©m disso, observe que o primeiro conceito pode ser semanticamente interpretado como o eixo do drama. - filme de a√ß√£o, onde a dire√ß√£o positiva corresponde ao g√™nero do filme de a√ß√£o, e o negativo - ao g√™nero do drama.  As classifica√ß√µes neste exemplo s√£o altamente correlacionadas, portanto, pode-se ver claramente que os tr√™s primeiros usu√°rios e os tr√™s primeiros filmes t√™m grandes valores negativos no primeiro vetor conceitual (filmes de drama e usu√°rios que gostam de tais filmes), enquanto os tr√™s √∫ltimos usu√°rios e os tr√™s √∫ltimos os filmes t√™m √≥timos significados positivos na mesma coluna (filmes de a√ß√£o e usu√°rios que preferem esse g√™nero).  A segunda dimens√£o nesse caso em particular corresponde principalmente ao vi√©s do usu√°rio ou elemento, que pode ser interpretado como um atributo psicogr√°fico (criticidade dos julgamentos do usu√°rio? Popularidade do cinema?).  Outros conceitos podem ser considerados como ru√≠do. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z0/s6/t9/z0s6t9vbnwdtgonx0p6rswo_ufe.png" alt="imagem"></div><br>  A matriz de fatores resultante n√£o √© completamente ortogonal nas colunas, mas tende a ser ortogonal, porque isso decorre da otimiza√ß√£o da solu√ß√£o SVD.  Isso pode ser visto observando os produtos PTP e QTQ, que s√£o pr√≥ximos das matrizes diagonais: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/zr/sf/pbzrsfbqm0hjvmoqgh1xlurslnm.png" alt="imagem"></div><br>  As matrizes 5.103 s√£o essencialmente um modelo preditivo que pode ser usado para avaliar classifica√ß√µes conhecidas e ausentes.  √â poss√≠vel obter estimativas multiplicando dois fatores e adicionando de volta a m√©dia global: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qg/du/d2/qgdud2i8fqm_ut6doi-8jnpllq8.png" alt="imagem"></div><br>  Os resultados reproduzem com precis√£o o conhecido e preveem as classifica√ß√µes ausentes de acordo com as expectativas intuitivas.  A precis√£o das estimativas pode ser aumentada ou diminu√≠da alterando o n√∫mero de medi√ß√µes, e o n√∫mero ideal de medi√ß√µes pode ser determinado na pr√°tica atrav√©s da verifica√ß√£o cruzada e da escolha de um compromisso razo√°vel entre complexidade e precis√£o computacional. <br><br>  ¬ªMais informa√ß√µes sobre o livro podem ser encontradas no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site do editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Conte√∫do</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trecho</a> <br><br>  Cupom de desconto de 25% para vendedores ambulantes - <b>Machine Learning</b> <br><br>  Ap√≥s o pagamento da vers√£o impressa do livro, um livro eletr√¥nico √© enviado por e-mail. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt460375/">https://habr.com/ru/post/pt460375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt460361/index.html">√ìtimas perguntas frequentes sobre seguran√ßa cibern√©tica de sistemas de informa√ß√µes m√©dicas</a></li>
<li><a href="../pt460363/index.html">7 fatores ausentes na abordagem 12 Factor App</a></li>
<li><a href="../pt460365/index.html">Rastreio distribu√≠do: fizemos tudo errado</a></li>
<li><a href="../pt460367/index.html">Engenharia do Caos: a arte da destrui√ß√£o intencional. Parte 1</a></li>
<li><a href="../pt460373/index.html">P√°ginas sob o cap√¥ Turbo: arquitetura da tecnologia de download r√°pido de p√°ginas da Web</a></li>
<li><a href="../pt460377/index.html">Usando o Liquibase para gerenciar a estrutura do banco de dados em um aplicativo Spring Boot. Parte 1</a></li>
<li><a href="../pt460381/index.html">O que √© assertividade e por que √© necess√°rio</a></li>
<li><a href="../pt460383/index.html">As transi√ß√µes de tela em Legend of Zelda usam os recursos n√£o documentados do NES</a></li>
<li><a href="../pt460387/index.html">Guia do Iniciante do SELinux</a></li>
<li><a href="../pt460393/index.html">Hist√≥rico: o que esperar do Fedora Silverblue</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>