<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ’Š ğŸ‡ ğŸ•µğŸ½ äººå·¥æ™ºèƒ½å¦‚ä½•å­¦ä¹ ç”ŸæˆçŒ«å›¾åƒ ğŸ§•ğŸ½ ğŸ§ ğŸ‘©ğŸ»â€ğŸ”¬</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="AIç¿»è¯‘å¦‚ä½•å­¦ä¹ ç”ŸæˆçŒ«çš„å›¾ç‰‡ ã€‚ 

 ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ ï¼ˆGANï¼‰äº2014å¹´å‘å¸ƒï¼Œæ˜¯ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„ä¸€é¡¹çªç ´ã€‚ é¦–å¸­ç ”ç©¶å‘˜Yann Lekunç§°å¯¹æŠ—ç½‘ç»œä¸ºâ€œè¿‡å»20å¹´æ¥æœºå™¨å­¦ä¹ çš„æœ€ä½³æ„æƒ³â€ã€‚ ä»Šå¤©ï¼Œå€ŸåŠ©è¿™ç§æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¯ç”Ÿæˆé€¼çœŸçš„çŒ«å›¾åƒçš„AIã€‚ å¥½é…·ï¼ 


 åŸ¹è®­æœŸé—´çš„DCGAN 

 æ‰€æœ‰å·¥...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>äººå·¥æ™ºèƒ½å¦‚ä½•å­¦ä¹ ç”ŸæˆçŒ«å›¾åƒ</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/416129/"><img src="https://habrastorage.org/webt/_h/iv/p2/_hivp2o-k9yn82z0cmffh1y__yu.jpeg"><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">AI</a>ç¿»è¯‘<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">å¦‚ä½•å­¦ä¹ ç”ŸæˆçŒ«çš„å›¾ç‰‡</a></i> ã€‚ <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</a> ï¼ˆGANï¼‰äº2014å¹´å‘å¸ƒï¼Œæ˜¯ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„ä¸€é¡¹çªç ´ã€‚ é¦–å¸­ç ”ç©¶å‘˜Yann Lekunç§°å¯¹æŠ—ç½‘ç»œä¸ºâ€œè¿‡å»20å¹´æ¥æœºå™¨å­¦ä¹ çš„æœ€ä½³æ„æƒ³â€ã€‚ ä»Šå¤©ï¼Œå€ŸåŠ©è¿™ç§æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¯ç”Ÿæˆé€¼çœŸçš„çŒ«å›¾åƒçš„AIã€‚ å¥½é…·ï¼ <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a9/2c3/4bd/3a92c34bdd409aead0b270fa21a59d86.gif"><br>  <i>åŸ¹è®­æœŸé—´çš„DCGAN</i> <br><a name="habracut"></a><br> æ‰€æœ‰å·¥ä½œä»£ç éƒ½åœ¨<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Githubå­˜å‚¨åº“ä¸­</a> ã€‚ å¦‚æœæ‚¨å…·æœ‰Pythonç¼–ç¨‹ï¼Œæ·±åº¦å­¦ä¹ ï¼Œä½¿ç”¨Tensorflowå’Œå·ç§¯ç¥ç»ç½‘ç»œçš„ç»éªŒï¼Œè¿™å°†å¯¹æ‚¨å¾ˆæœ‰ç”¨ã€‚ <br><br> å¦‚æœæ‚¨æ˜¯æ·±åº¦å­¦ä¹ çš„æ–°æ‰‹ï¼Œæˆ‘å»ºè®®æ‚¨ç†Ÿæ‚‰<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£</a>çš„å‡ºè‰²æ–‡ç« ç³»åˆ—<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ï¼</a> <br><br><h3> ä»€ä¹ˆæ˜¯DCGANï¼Ÿ </h3><br> æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå…¶ç”Ÿæˆçš„æ•°æ®ç±»ä¼¼äºè®­ç»ƒé›†ä¸­çš„æ•°æ®ã€‚ <br><br> è¯¥æ¨¡å‹ç”¨å·ç§¯å±‚ä»£æ›¿äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å®Œå…¨è¿æ¥å±‚ã€‚ ä¸ºäº†äº†è§£DCGANçš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“å®¶è‰ºæœ¯è¯„è®ºå®¶å’Œä¼ªé€ è€…ä¹‹é—´å¯¹æŠ—çš„éšå–»ã€‚ <br><br> ä¼ªé€ è€…ï¼ˆâ€œå‘ç”µæœºâ€ï¼‰æ­£åœ¨å°è¯•åˆ›å»ºå‡æ¢µé«˜å›¾ç‰‡å¹¶å°†å…¶ä½œä¸ºçœŸå®å›¾ç‰‡ä¼ é€’ã€‚ <br><br><img src="https://habrastorage.org/getpro/habr/post_images/814/e92/796/814e927961f7a7ac8541ae2104d0b9c0.png"><br><br> ä¸€ä½è‰ºæœ¯è¯„è®ºå®¶ï¼ˆâ€œæ­§è§†è€…â€ï¼‰è¯•å›¾åˆ©ç”¨å¯¹æ¢µé«˜çœŸå®ç”»å¸ƒçš„äº†è§£ï¼Œå¯¹ä¼ªé€ è€…å®šç½ªã€‚ <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae1/81d/424/ae181d42436f5ac37219ac75b9371c35.png"><br><br> éšç€æ—¶é—´çš„æµé€ï¼Œè‰ºæœ¯è¯„è®ºå®¶è¶Šæ¥è¶Šå¤šåœ°å®šä¹‰å‡è´§ï¼Œè€Œä¼ªé€ è€…åˆ™ä½¿å®ƒä»¬æ›´åŠ å®Œç¾ã€‚ <br><br><img src="https://habrastorage.org/webt/vq/lk/gt/vqlkgtau1xlmqscetwib8uuixbk.png"><br>  <i>å¦‚æ‚¨æ‰€è§ï¼ŒDCGANç”±ç›¸äº’ç«äº‰çš„ä¸¤ä¸ªå•ç‹¬çš„æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œç»„æˆã€‚</i> <br><br><ul><li> ç”Ÿæˆå™¨æ­£åœ¨å°è¯•åˆ›å»ºå¯ä¿¡çš„æ•°æ®ã€‚ ä»–ä¸çŸ¥é“çœŸå®çš„æ•°æ®æ˜¯ä»€ä¹ˆï¼Œä½†æ˜¯ä»–ä»æ•Œäººçš„ç¥ç»ç½‘ç»œçš„å“åº”ä¸­å­¦ä¹ ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šæ”¹å˜ä»–çš„å·¥ä½œç»“æœã€‚ </li><li>é‰´åˆ«å™¨è¯•å›¾ç¡®å®šå‡æ•°æ®ï¼ˆä¸çœŸå®æ•°æ®æ¯”è¾ƒï¼‰ï¼Œä»è€Œå°½å¯èƒ½é¿å…ç›¸å¯¹äºçœŸå®æ•°æ®çš„è¯¯æŠ¥ã€‚ è¯¥æ¨¡å‹çš„ç»“æœæ˜¯ç”Ÿæˆå™¨çš„åé¦ˆã€‚ </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/b49/a43/c45/b49a43c45d9a54e577e6729ccbdeba33.png"><br>  <i>DCGANæ¨¡å¼ã€‚</i> <br><br><ul><li> ç”Ÿæˆå™¨è·å–éšæœºå™ªå£°çŸ¢é‡å¹¶ç”Ÿæˆå›¾åƒã€‚ </li><li> å°†å›¾åƒæä¾›ç»™é‰´åˆ«å™¨ï¼Œä»–å°†å…¶ä¸è®­ç»ƒæ ·æœ¬è¿›è¡Œæ¯”è¾ƒã€‚ </li><li> é‰´åˆ«ç¬¦è¿”å›ä¸€ä¸ªæ•°å­—-0ï¼ˆä¼ªï¼‰æˆ–1ï¼ˆçœŸå®å›¾åƒï¼‰ã€‚ </li></ul><br><h3> è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªDCGANï¼ </h3><br> ç°åœ¨æˆ‘ä»¬å‡†å¤‡åˆ›å»ºè‡ªå·±çš„AIã€‚ <br><br> åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºæ¨¡å‹çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚ å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ•´ä¸ªä»£ç ï¼Œè¯·è½¬åˆ°<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">æ­¤å¤„</a> ã€‚ <br><br><h4> è¾“å…¥æ•°æ® </h4><br> ä¸ºè¾“å…¥åˆ›å»ºå­˜æ ¹ï¼šå¯¹äºé‰´åˆ«<code>inputs_z</code>ä¸º<code>inputs_z</code> ï¼Œå¯¹äºç”Ÿæˆå™¨ä¸º<code>inputs_z</code> ã€‚ è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†æœ‰ä¸¤ç§å­¦ä¹ ç‡ï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ã€‚ <br><br>  DCGANå¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿï¼Œå› æ­¤å¾®è°ƒå®ƒä»¬éå¸¸é‡è¦ã€‚ <br> <code>def model_inputs(real_dim, z_dim):</code> <br> <br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" Create the model inputs :param real_dim: tuple containing width, height and channels :param z_dim: The dimension of Z :return: Tuple of (tensor of real input images, tensor of z data, learning rate G, learning rate D) """</span></span> <span class="hljs-comment"><span class="hljs-comment"># inputs_real for Discriminator inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real') # inputs_z for Generator inputs_z = tf.placeholder(tf.float32, (None, z_dim), name="input_z") # Two different learning rate : one for the generator, one for the discriminator learning_rate_G = tf.placeholder(tf.float32, name="learning_rate_G") learning_rate_D = tf.placeholder(tf.float32, name="learning_rate_D") return inputs_real, inputs_z, learning_rate_G, learning_rate_D</span></span></code> </pre> <br><h4> é‰´åˆ«å™¨å’Œå‘ç”Ÿå™¨ </h4><br> æˆ‘ä»¬ä½¿ç”¨<code>tf.variable_scope</code>æœ‰ä¸¤ä¸ªåŸå› ã€‚ <br><br> é¦–å…ˆï¼Œç¡®ä¿æ‰€æœ‰å˜é‡åéƒ½ä»¥generator / discriminatorå¼€å¤´ã€‚ ç¨åè¿™å°†å¸®åŠ©æˆ‘ä»¬è®­ç»ƒä¸¤ä¸ªç¥ç»ç½‘ç»œã€‚ <br> å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„è¾“å…¥æ•°æ®é‡ç”¨è¿™äº›ç½‘ç»œï¼š <br><br><ul><li> æˆ‘ä»¬å°†è®­ç»ƒç”Ÿæˆå™¨ï¼Œç„¶åå¯¹å®ƒç”Ÿæˆçš„å›¾åƒè¿›è¡Œé‡‡æ ·ã€‚ </li><li> åœ¨é‰´åˆ«å™¨ä¸­ï¼Œæˆ‘ä»¬å°†å…±äº«ä¼ªé€ å’ŒçœŸå®è¾“å…¥å›¾åƒçš„å˜é‡ã€‚ </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/b62/c51/df9/b62c51df96feab5b9b1e482d9b0b0133.png"><br><br> è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªé‰´åˆ«å™¨ã€‚ è¯·è®°ä½ï¼Œä½œä¸ºè¾“å…¥ï¼Œå®ƒä¼šæ‹æ‘„çœŸå®æˆ–ä¼ªé€ çš„å›¾åƒå¹¶ä½œä¸ºå“åº”è¿”å›0æˆ–1ã€‚ <br><br> ä¸€äº›æ³¨æ„äº‹é¡¹ï¼š <br><br><ul><li> æˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ªå·ç§¯å±‚ä¸­å°†æ»¤æ³¢å™¨å¤§å°åŠ å€ã€‚ </li><li> ä¸å»ºè®®ä½¿ç”¨ä¸‹é‡‡æ ·ã€‚ ç›¸åï¼Œä»…å‰¥ç¦»çš„å·ç§¯å±‚é€‚ç”¨ã€‚ </li><li> åœ¨æ¯ä¸€å±‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰¹å¤„ç†å½’ä¸€åŒ–ï¼ˆè¾“å…¥å±‚é™¤å¤–ï¼‰ï¼Œå› ä¸ºè¿™ä¼šå‡å°‘åæ–¹å·®æ¼‚ç§»ã€‚ åœ¨è¿™ç¯‡<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ç²¾å½©çš„æ–‡ç« ä¸­</a>é˜…è¯»æ›´å¤š<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">å†…å®¹</a> ã€‚ </li><li> æˆ‘ä»¬å°†ä½¿ç”¨Leaky ReLUä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œè¿™å°†æœ‰åŠ©äºé¿å…â€œæ¶ˆå¤±â€æ¢¯åº¦çš„å½±å“ã€‚ </li></ul><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">discriminator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, is_reuse=False, alpha = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Build the discriminator network. Arguments --------- x : Input tensor for the discriminator n_units: Number of units in hidden layer reuse : Reuse the variables with tf.variable_scope alpha : leak parameter for leaky ReLU Returns ------- out, logits: '''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">"discriminator"</span></span>, reuse = is_reuse): <span class="hljs-comment"><span class="hljs-comment"># Input layer 128*128*3 --&gt; 64x64x64 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv1 = tf.layers.conv2d(inputs = x, filters = 64, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv1') batch_norm1 = tf.layers.batch_normalization(conv1, training = True, epsilon = 1e-5, name = 'batch_norm1') conv1_out = tf.nn.leaky_relu(batch_norm1, alpha=alpha, name="conv1_out") # 64x64x64--&gt; 32x32x128 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv2 = tf.layers.conv2d(inputs = conv1_out, filters = 128, kernel_size = [5, 5], strides = [2, 2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv2') batch_norm2 = tf.layers.batch_normalization(conv2, training = True, epsilon = 1e-5, name = 'batch_norm2') conv2_out = tf.nn.leaky_relu(batch_norm2, alpha=alpha, name="conv2_out") # 32x32x128 --&gt; 16x16x256 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv3 = tf.layers.conv2d(inputs = conv2_out, filters = 256, kernel_size = [5, 5], strides = [2, 2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv3') batch_norm3 = tf.layers.batch_normalization(conv3, training = True, epsilon = 1e-5, name = 'batch_norm3') conv3_out = tf.nn.leaky_relu(batch_norm3, alpha=alpha, name="conv3_out") # 16x16x256 --&gt; 16x16x512 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv4 = tf.layers.conv2d(inputs = conv3_out, filters = 512, kernel_size = [5, 5], strides = [1, 1], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv4') batch_norm4 = tf.layers.batch_normalization(conv4, training = True, epsilon = 1e-5, name = 'batch_norm4') conv4_out = tf.nn.leaky_relu(batch_norm4, alpha=alpha, name="conv4_out") # 16x16x512 --&gt; 8x8x1024 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv5 = tf.layers.conv2d(inputs = conv4_out, filters = 1024, kernel_size = [5, 5], strides = [2, 2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv5') batch_norm5 = tf.layers.batch_normalization(conv5, training = True, epsilon = 1e-5, name = 'batch_norm5') conv5_out = tf.nn.leaky_relu(batch_norm5, alpha=alpha, name="conv5_out") # Flatten it flatten = tf.reshape(conv5_out, (-1, 8*8*1024)) # Logits logits = tf.layers.dense(inputs = flatten, units = 1, activation = None) out = tf.sigmoid(logits) return out, logits</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/c2a/438/e2e/c2a438e2e1bb8160f30c6e62297153f4.png"><br><br> æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç”Ÿæˆå™¨ã€‚ è¯·è®°ä½ï¼Œå®ƒä»¥å™ªå£°çŸ¢é‡ï¼ˆzï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸”ç”±äºè½¬ç½®äº†å·ç§¯å±‚ï¼Œå› æ­¤åˆ›å»ºäº†ä¼ªå›¾åƒã€‚ <br><br> åœ¨æ¯ä¸€å±‚ä¸Šï¼Œæˆ‘ä»¬å°†æ»¤é•œçš„å°ºå¯¸å‡åŠï¼Œå¹¶ä¸”å°†å›¾åƒçš„å°ºå¯¸åŠ å€ã€‚ <br><br> å½“ä½¿ç”¨<code>tanh</code>ä½œä¸ºè¾“å‡ºæ¿€æ´»åŠŸèƒ½æ—¶ï¼Œå‘ç”Ÿå™¨å·¥ä½œå¾—æœ€å¥½ã€‚ <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, output_channel_dim, is_train=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Build the generator network. Arguments --------- z : Input tensor for the generator output_channel_dim : Shape of the generator output n_units : Number of units in hidden layer reuse : Reuse the variables with tf.variable_scope alpha : leak parameter for leaky ReLU Returns ------- out: '''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">"generator"</span></span>, reuse= <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> is_train): <span class="hljs-comment"><span class="hljs-comment"># First FC layer --&gt; 8x8x1024 fc1 = tf.layers.dense(z, 8*8*1024) # Reshape it fc1 = tf.reshape(fc1, (-1, 8, 8, 1024)) # Leaky ReLU fc1 = tf.nn.leaky_relu(fc1, alpha=alpha) # Transposed conv 1 --&gt; BatchNorm --&gt; LeakyReLU # 8x8x1024 --&gt; 16x16x512 trans_conv1 = tf.layers.conv2d_transpose(inputs = fc1, filters = 512, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv1") batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1, training=is_train, epsilon=1e-5, name="batch_trans_conv1") trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1, alpha=alpha, name="trans_conv1_out") # Transposed conv 2 --&gt; BatchNorm --&gt; LeakyReLU # 16x16x512 --&gt; 32x32x256 trans_conv2 = tf.layers.conv2d_transpose(inputs = trans_conv1_out, filters = 256, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv2") batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2, training=is_train, epsilon=1e-5, name="batch_trans_conv2") trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2, alpha=alpha, name="trans_conv2_out") # Transposed conv 3 --&gt; BatchNorm --&gt; LeakyReLU # 32x32x256 --&gt; 64x64x128 trans_conv3 = tf.layers.conv2d_transpose(inputs = trans_conv2_out, filters = 128, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv3") batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3, training=is_train, epsilon=1e-5, name="batch_trans_conv3") trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3, alpha=alpha, name="trans_conv3_out") # Transposed conv 4 --&gt; BatchNorm --&gt; LeakyReLU # 64x64x128 --&gt; 128x128x64 trans_conv4 = tf.layers.conv2d_transpose(inputs = trans_conv3_out, filters = 64, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv4") batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4, training=is_train, epsilon=1e-5, name="batch_trans_conv4") trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4, alpha=alpha, name="trans_conv4_out") # Transposed conv 5 --&gt; tanh # 128x128x64 --&gt; 128x128x3 logits = tf.layers.conv2d_transpose(inputs = trans_conv4_out, filters = 3, kernel_size = [5,5], strides = [1,1], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="logits") out = tf.tanh(logits, name="out") return out</span></span></code> </pre> <br><h4> é‰´åˆ«å™¨å’Œå‘ç”Ÿå™¨çš„æŸå¤± </h4><br> ç”±äºæˆ‘ä»¬åŒæ—¶è®­ç»ƒäº†ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è®¡ç®—ä¸¤ä¸ªç¥ç»ç½‘ç»œçš„æŸè€—ã€‚ è¾¨åˆ«å™¨åœ¨â€œè®¤ä¸ºâ€å›¾åƒä¸ºçœŸå®å›¾åƒæ—¶åº”ç»™å‡º1ï¼Œè€Œåœ¨å›¾åƒä¸ºå‡å›¾åƒæ—¶åº”ç»™å‡º0ã€‚ æŒ‰ç…§è¿™ä¸€ç‚¹ï¼Œæ‚¨éœ€è¦é…ç½®æŸå¤±ã€‚ è¾¨åˆ«å™¨æŸè€—è®¡ç®—ä¸ºçœŸå®å’Œä¼ªé€ å›¾åƒçš„æŸè€—ä¹‹å’Œï¼š <br><br> <code>d_loss = d_loss_real + d_loss_fake</code> <br> <br> å…¶ä¸­<code>d_loss_real</code>æ˜¯é‰´åˆ«å™¨è®¤ä¸ºå›¾åƒä¸ºå‡çš„æŸå¤±ï¼Œä½†å®é™…ä¸Šå®ƒæ˜¯çœŸå®çš„ã€‚ è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š <br><br><ul><li> æˆ‘ä»¬ä½¿ç”¨<code>d_logits_real</code> ï¼Œæ‰€æœ‰æ ‡ç­¾éƒ½ç­‰äº1ï¼ˆå› ä¸ºæ‰€æœ‰æ•°æ®éƒ½æ˜¯çœŸå®çš„ï¼‰ã€‚ </li><li>  <code>labels = tf.ones_like(tensor) * (1 - smooth)</code> ã€‚ è®©æˆ‘ä»¬ä½¿ç”¨æ ‡ç­¾å¹³æ»‘å¤„ç†ï¼šå°†æ ‡ç­¾å€¼ä»1.0é™ä½åˆ°0.9ï¼Œä»¥å¸®åŠ©åŒºåˆ†å™¨æ›´å¥½åœ°æ³›åŒ–ã€‚ </li></ul><br> å½“é‰´åˆ«å™¨è®¤ä¸ºå›¾åƒæ˜¯çœŸå®çš„æ—¶ï¼Œ <code>d_loss_fake</code>æ˜¯ä¸€ç§æŸå¤±ï¼Œä½†å®é™…ä¸Šå®ƒæ˜¯ä¼ªé€ çš„ã€‚ <br><br><ul><li> æˆ‘ä»¬ä½¿ç”¨<code>d_logits_fake</code> ï¼Œæ‰€æœ‰æ ‡ç­¾å‡ä¸º0ã€‚ </li></ul><br> è¦ä¸¢å¤±ç”Ÿæˆå™¨ï¼Œ <code>d_logits_fake</code>ä½¿ç”¨é‰´åˆ«å™¨ä¸­çš„<code>d_logits_fake</code> ã€‚ è¿™æ¬¡ï¼Œæ‰€æœ‰æ ‡ç­¾å‡ä¸º1ï¼Œå› ä¸ºç”Ÿæˆå™¨è¦æ¬ºéª—é‰´åˆ«å™¨ã€‚ <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_real, input_z, output_channel_dim, alpha)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Get the loss for the discriminator and generator :param input_real: Images from the real dataset :param input_z: Z input :param out_channel_dim: The number of channels in the output image :return: A tuple of (discriminator loss, generator loss) """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Generator network here g_model = generator(input_z, output_channel_dim) # g_model is the generator output # Discriminator network here d_model_real, d_logits_real = discriminator(input_real, alpha=alpha) d_model_fake, d_logits_fake = discriminator(g_model,is_reuse=True, alpha=alpha) # Calculate losses d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real))) d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake))) d_loss = d_loss_real + d_loss_fake g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake))) return d_loss, g_loss</span></span></code> </pre> <br><h4> ä¼˜åŒ–å™¨ </h4><br> è®¡ç®—æŸå¤±åï¼Œå¿…é¡»åˆ†åˆ«æ›´æ–°ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ã€‚ ä¸ºæ­¤ï¼Œè¯·ä½¿ç”¨<code>tf.trainable_variables()</code>åˆ›å»ºä¸€ä¸ªå›¾è¡¨ä¸­å®šä¹‰çš„æ‰€æœ‰å˜é‡çš„åˆ—è¡¨ã€‚ <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_optimizers</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(d_loss, g_loss, lr_D, lr_G, beta1)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Get optimization operations :param d_loss: Discriminator loss Tensor :param g_loss: Generator loss Tensor :param learning_rate: Learning Rate Placeholder :param beta1: The exponential decay rate for the 1st moment in the optimizer :return: A tuple of (discriminator training operation, generator training operation) """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get the trainable_variables, split into G and D parts t_vars = tf.trainable_variables() g_vars = [var for var in t_vars if var.name.startswith("generator")] d_vars = [var for var in t_vars if var.name.startswith("discriminator")] update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Generator update gen_updates = [op for op in update_ops if op.name.startswith('generator')] # Optimizers with tf.control_dependencies(gen_updates): d_train_opt = tf.train.AdamOptimizer(learning_rate=lr_D, beta1=beta1).minimize(d_loss, var_list=d_vars) g_train_opt = tf.train.AdamOptimizer(learning_rate=lr_G, beta1=beta1).minimize(g_loss, var_list=g_vars) return d_train_opt, g_train_opt</span></span></code> </pre> <br><h4> åŸ¹è®­è¯¾ç¨‹ </h4><br> ç°åœ¨æˆ‘ä»¬æ‰§è¡Œè®­ç»ƒåŠŸèƒ½ã€‚ è¿™ä¸ªæƒ³æ³•å¾ˆç®€å•ï¼š <br><br><ul><li> æˆ‘ä»¬æ¯äº”ä¸ªå‘¨æœŸï¼ˆæ—¶ä»£ï¼‰ä¿å­˜ä¸€æ¬¡æ¨¡å‹ã€‚ </li><li> æˆ‘ä»¬æ¯10ä¸ªç»è¿‡è®­ç»ƒçš„æ‰¹æ¬¡å°†å›¾ç‰‡ä¿å­˜åœ¨åŒ…å«å›¾åƒçš„æ–‡ä»¶å¤¹ä¸­ã€‚ </li><li> æ¯15ä¸ªå‘¨æœŸæˆ‘ä»¬æ˜¾ç¤º<code>g_loss</code> ï¼Œ <code>d_loss</code>å’Œç”Ÿæˆçš„å›¾åƒã€‚ è¿™æ˜¯å› ä¸ºJupyterç¬”è®°æœ¬ç”µè„‘åœ¨æ˜¾ç¤ºå¤ªå¤šå›¾ç‰‡æ—¶å¯èƒ½ä¼šå´©æºƒã€‚ </li><li> æˆ–è€…æˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ è½½ä¿å­˜çš„æ¨¡å‹ç›´æ¥ç”ŸæˆçœŸå®å›¾åƒï¼ˆè¿™å°†èŠ‚çœ20ä¸ªå°æ—¶çš„è®­ç»ƒï¼‰ã€‚ </li></ul><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(epoch_count, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, get_batches, data_shape, data_image_mode, alpha)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Train the GAN :param epoch_count: Number of epochs :param batch_size: Batch Size :param z_dim: Z dimension :param learning_rate: Learning Rate :param beta1: The exponential decay rate for the 1st moment in the optimizer :param get_batches: Function to get batches :param data_shape: Shape of the data :param data_image_mode: The image mode to use for images ("RGB" or "L") """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Create our input placeholders input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], z_dim) # Losses d_loss, g_loss = model_loss(input_images, input_z, data_shape[3], alpha) # Optimizers d_opt, g_opt = model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1) i = 0 version = "firstTrain" with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # Saver saver = tf.train.Saver() num_epoch = 0 if from_checkpoint == True: saver.restore(sess, "./models/model.ckpt") show_generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path, True, False) else: for epoch_i in range(epoch_count): num_epoch += 1 if num_epoch % 5 == 0: # Save model every 5 epochs #if not os.path.exists("models/" + version): # os.makedirs("models/" + version) save_path = saver.save(sess, "./models/model.ckpt") print("Model saved") for batch_images in get_batches(batch_size): # Random noise batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim)) i += 1 # Run optimizers _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: learning_rate_D}) _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: learning_rate_G}) if i % 10 == 0: train_loss_d = d_loss.eval({input_z: batch_z, input_images: batch_images}) train_loss_g = g_loss.eval({input_z: batch_z}) # Save it image_name = str(i) + ".jpg" image_path = "./images/" + image_name show_generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path, True, False) # Print every 5 epochs (for stability overwize the jupyter notebook will bug) if i % 1500 == 0: image_name = str(i) + ".jpg" image_path = "./images/" + image_name print("Epoch {}/{}...".format(epoch_i+1, epochs), "Discriminator Loss: {:.4f}...".format(train_loss_d), "Generator Loss: {:.4f}".format(train_loss_g)) show_generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path, False, True) return losses, samples</span></span></code> </pre> <br><h4> æ€ä¹ˆè·‘ </h4><br> å¦‚æœæ‚¨å‡†å¤‡ç­‰å¾…10å¹´ï¼Œé‚£ä¹ˆæ‰€æœ‰è¿™äº›æ“ä½œéƒ½å¯ä»¥åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šè¿è¡Œï¼Œâ€‹â€‹å› æ­¤æœ€å¥½ä½¿ç”¨åŸºäºäº‘çš„GPUæœåŠ¡ï¼Œä¾‹å¦‚AWSæˆ–FloydHubã€‚ æˆ‘ä¸ªäººåœ¨Microsoft AzureåŠå…¶<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">æ·±åº¦å­¦ä¹ è™šæ‹Ÿæœº</a>ä¸Šå¯¹DCGANè¿›è¡Œäº†20å°æ—¶çš„åŸ¹è®­ã€‚ æˆ‘ä¸Azureæ²¡æœ‰ä¸šåŠ¡å…³ç³»ï¼Œå°±åƒä»–ä»¬çš„å®¢æˆ·æœåŠ¡ä¸€æ ·ã€‚ <br><br> å¦‚æœæ‚¨åœ¨è™šæ‹Ÿæœºä¸Šè¿è¡Œæœ‰ä»»ä½•å›°éš¾ï¼Œè¯·å‚é˜…è¿™ç¯‡ç²¾å½©çš„<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">æ–‡ç« </a> ã€‚ <br><br> å¦‚æœæ‚¨æ”¹è¿›æ¨¡å‹ï¼Œè¯·éšæ—¶æå‡ºè¯·æ±‚ã€‚ <br><br><img src="https://habrastorage.org/getpro/habr/post_images/02d/bf7/78d/02dbf778db1b4d64066fed444f385724.gif"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN416129/">https://habr.com/ru/post/zh-CN416129/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN416119/index.html">å‘¨ä¸‰çš„å‘¨äº”å¸–å­ï¼šæœ€â€œå¿…è¦â€çš„NPMè½¯ä»¶åŒ…çš„é¡¶éƒ¨</a></li>
<li><a href="../zh-CN416121/index.html">å¯Œå£«é€šäººå·¥æ™ºèƒ½è®¡ç®—ç£æ€§ææ–™çš„å‡ ä½•å½¢çŠ¶</a></li>
<li><a href="../zh-CN416123/index.html">ä½¿ç”¨Keraså’ŒTensorflow Object Detection APIæŠ€æœ¯çš„ç¥ç»ç½‘ç»œè¯†åˆ«è´§æ¶ä¸Šçš„å•†å“</a></li>
<li><a href="../zh-CN416125/index.html">æ‘„åƒæœºçš„å®‰è£…ï¼Œç³»ç»Ÿè®¾ç½®å’Œæ§åˆ¶</a></li>
<li><a href="../zh-CN416127/index.html">CUDAå’Œè¿œç¨‹GPU</a></li>
<li><a href="../zh-CN416131/index.html">å¦‚ä½•åœ¨ä¿„ç½—æ–¯è”é‚¦å¤„ç†è¿çº¦æ¦‚ç‡è€Œä¸è¿åæ³•å¾‹</a></li>
<li><a href="../zh-CN416133/index.html">å›½å¤–æ•°æ®ä¸­å¿ƒï¼šEquinix LD8</a></li>
<li><a href="../zh-CN416135/index.html">å°äº1 kbçš„GUIåº”ç”¨ç¨‹åº</a></li>
<li><a href="../zh-CN416137/index.html">Zabbixä½œä¸ºå®‰å…¨æ‰«æç¨‹åº</a></li>
<li><a href="../zh-CN416139/index.html">å¼ºå¤§çš„èº«ä»½éªŒè¯æ˜¯GDPRæˆ˜ç•¥çš„ä¸€éƒ¨åˆ†</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>