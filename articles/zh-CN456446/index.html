<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸšµğŸ¿ ğŸ’” ğŸ‘¨ğŸ½â€ğŸ¨ Ceph-ä»â€œè†ç›–â€åˆ°â€œç”Ÿäº§â€ ğŸ‘¨ğŸ»â€ğŸ« ğŸš  ğŸ¦‹</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="CEPHçš„é€‰æ‹©ã€‚ ç¬¬ä¸€éƒ¨åˆ† 


 æˆ‘ä»¬æœ‰äº”ä¸ªæœºæ¶ï¼Œåä¸ªå…‰äº¤æ¢æœºï¼Œé…ç½®çš„BGPï¼Œå‡ åä¸ªSSDå’Œä¸€å †å„ç§é¢œè‰²å’Œå¤§å°çš„SASç£ç›˜ï¼Œä»¥åŠproxmoxå’Œå°†æ‰€æœ‰é™æ€æ•°æ®æ”¾å…¥æˆ‘ä»¬è‡ªå·±çš„S3å­˜å‚¨ä¸­çš„æ„¿æœ›ã€‚ å¹¶ä¸æ˜¯æ‰€æœ‰è¿™äº›å¯¹äºè™šæ‹ŸåŒ–éƒ½æ˜¯å¿…è¦çš„ï¼Œä½†æ˜¯ä¸€æ—¦æ‚¨å¼€å§‹ä½¿ç”¨å¼€æºï¼Œç„¶åä¾¿ä¼šæŠ•å…¥åˆ°æœ€åã€‚ å”¯ä¸€å›°æ‰°æˆ‘çš„æ˜¯BGPã€‚ ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph-ä»â€œè†ç›–â€åˆ°â€œç”Ÿäº§â€</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456446/"><h1 id="vybor-ceph-chast-1">  CEPHçš„é€‰æ‹©ã€‚ ç¬¬ä¸€éƒ¨åˆ† </h1><br><p>  <em>æˆ‘ä»¬æœ‰äº”ä¸ªæœºæ¶ï¼Œåä¸ªå…‰äº¤æ¢æœºï¼Œé…ç½®çš„BGPï¼Œå‡ åä¸ªSSDå’Œä¸€å †å„ç§é¢œè‰²å’Œå¤§å°çš„SASç£ç›˜ï¼Œä»¥åŠproxmoxå’Œå°†æ‰€æœ‰é™æ€æ•°æ®æ”¾å…¥æˆ‘ä»¬è‡ªå·±çš„S3å­˜å‚¨ä¸­çš„æ„¿æœ›ã€‚</em>  <em>å¹¶ä¸æ˜¯æ‰€æœ‰è¿™äº›å¯¹äºè™šæ‹ŸåŒ–éƒ½æ˜¯å¿…è¦çš„ï¼Œä½†æ˜¯ä¸€æ—¦æ‚¨å¼€å§‹ä½¿ç”¨å¼€æºï¼Œç„¶åä¾¿ä¼šæŠ•å…¥åˆ°æœ€åã€‚</em>  <em>å”¯ä¸€å›°æ‰°æˆ‘çš„æ˜¯BGPã€‚</em>  <em>ä¸–ç•Œä¸Šæ²¡æœ‰æ¯”å†…éƒ¨BGPè·¯ç”±æ›´æ— åŠ©ï¼Œä¸è´Ÿè´£ä»»å’Œä¸é“å¾·çš„äº†ã€‚</em>  <em>æˆ‘çŸ¥é“å¾ˆå¿«æˆ‘ä»¬å°±ä¼šæŠ•å…¥å…¶ä¸­ã€‚</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/09e/a36/178/09ea3617814a9598a6aa9784abc14a76.jpg"></p><br><p> è¿™é¡¹ä»»åŠ¡å¾ˆå¹³å¸¸-æœ‰CEPHï¼Œä½†æ•ˆæœä¸æ˜¯å¾ˆå¥½ã€‚ æœ‰å¿…è¦åšâ€œå¥½â€ã€‚ <br> æˆ‘å¾—åˆ°çš„ç¾¤é›†æ˜¯å¼‚ç±»çš„ï¼Œç»è¿‡æ•´ç†åå‡ ä¹æ²¡æœ‰è¿›è¡Œè°ƒæ•´ã€‚ å®ƒç”±ä¸¤ç»„ä¸åŒçš„èŠ‚ç‚¹ç»„æˆï¼Œå…¶ä¸­ä¸€ä¸ªå…¬å…±ç½‘æ ¼åŒæ—¶å……å½“é›†ç¾¤å’Œå…¬å…±ç½‘ç»œã€‚ èŠ‚ç‚¹è£…æœ‰å››ç§ç±»å‹çš„ç£ç›˜-ä¸¤ç§ç±»å‹çš„SSDï¼Œç»„è£…æˆä¸¤ä¸ªå•ç‹¬çš„æ”¾ç½®è§„åˆ™ï¼Œä¸¤ç§ç±»å‹çš„ä¸åŒå¤§å°çš„HDDï¼Œç»„è£…æˆç¬¬ä¸‰ç»„ã€‚ é€šè¿‡ä¸åŒçš„OSDæƒé‡å¯ä»¥è§£å†³å…·æœ‰ä¸åŒå¤§å°çš„é—®é¢˜ã€‚ </p><br><p> è®¾ç½®æœ¬èº«åˆ†ä¸ºä¸¤éƒ¨åˆ†- <strong>è°ƒæ•´æ“ä½œç³»ç»Ÿ</strong>å’Œ<strong>è°ƒæ•´CEPHæœ¬èº«</strong>åŠå…¶è®¾ç½®ã€‚ </p><a name="habracut"></a><br><h2 id="prokachka-os"> æ‰¾å¹³æ“ä½œç³»ç»Ÿ </h2><br><h3 id="network"> è”æ’­ç½‘ </h3><br><p> å½•åˆ¶å’Œå¹³è¡¡æ—¶ï¼Œé«˜å»¶è¿Ÿéƒ½ä¼šå—åˆ°å½±å“ã€‚ è®°å½•æ—¶-ç”±äºå®¢æˆ·ç«¯ç›´åˆ°å…¶ä»–æ”¾ç½®ç»„ä¸­çš„æ•°æ®å‰¯æœ¬ç¡®è®¤æˆåŠŸåæ‰æ”¶åˆ°æœ‰å…³æˆåŠŸè®°å½•çš„å“åº”ã€‚ ç”±äºåœ¨CRUSHæ˜ å°„ä¸­åˆ†å‘å‰¯æœ¬çš„è§„åˆ™ï¼Œæˆ‘ä»¬æ¯ä¸ªä¸»æœºåªæœ‰ä¸€ä¸ªå‰¯æœ¬ï¼Œå› æ­¤å§‹ç»ˆä½¿ç”¨ç½‘ç»œã€‚ </p><br><p> å› æ­¤ï¼Œç¬¬ä¸€ä»¶äº‹æ˜¯æˆ‘å†³å®šç•¥å¾®é…ç½®å½“å‰ç½‘ç»œï¼ŒåŒæ—¶è¯•å›¾è¯´æœæˆ‘è¿ç§»åˆ°å•ç‹¬çš„ç½‘ç»œã€‚ </p><br><p> é¦–å…ˆï¼Œè¯·æ‰­æ›²ç½‘å¡çš„è®¾ç½®ã€‚ é¦–å…ˆè®¾ç½®é˜Ÿåˆ—ï¼š </p><br><p> ä»€ä¹ˆæ˜¯ï¼š </p><br><div class="spoiler">  <b class="spoiler_title">ethtool -l ens1f1</b> <div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1 root@ceph01:~# ethtool -g ens1f1 Ring parameters for ens1f1: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1</code> </pre> </div></div><br><p> å¯ä»¥çœ‹å‡ºï¼Œå½“å‰å‚æ•°è¿œéæœ€å¤§å€¼ã€‚ å¢åŠ ï¼š </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ethtool -G ens1f0 rx 4096 root@ceph01:~#ethtool -G ens1f0 tx 4096 root@ceph01:~#ethtool -L ens1f0 combined 63</code> </pre> <br><p> ä¸€ç¯‡å‡ºè‰²çš„æ–‡ç« æŒ‡å¯¼ </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://blog.packagecloud.io/chi/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a> </p><br><p> å°†<strong>txqueuelen</strong>æ´¾é£<strong>é˜Ÿåˆ—</strong>çš„é•¿åº¦ä»1000å¢åŠ åˆ°10,000 </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set ens1f0 txqueuelen 10000</code> </pre> <br><p> å¥½å§ï¼Œéµå¾ªcephæœ¬èº«çš„æ–‡æ¡£ </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://ceph.com/geen-categorie/ceph-loves-jumbo-frames/</a> </p><br><p>  <strong>MTU</strong>å¢åŠ åˆ°9000ã€‚ </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set dev ens1f0 mtu 9000</code> </pre> <br><p> æ·»åŠ åˆ°/ etc / network / interfacesï¼Œä»¥ä¾¿åœ¨å¯åŠ¨æ—¶åŠ è½½ä»¥ä¸Šæ‰€æœ‰å†…å®¹ </p><br><div class="spoiler">  <b class="spoiler_title">cat / etc / network / interfaces</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01:~# cat /etc/network/interfaces auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet manual post-up /sbin/ethtool -G ens1f0 rx 4096 post-up /sbin/ethtool -G ens1f0 tx 4096 post-up /sbin/ethtool -L ens1f0 combined 63 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 auto ens1f1 iface ens1f1 inet manual post-up /sbin/ethtool -G ens1f1 rx 4096 post-up /sbin/ethtool -G ens1f1 tx 4096 post-up /sbin/ethtool -L ens1f1 combined 63 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000</code> </pre> </div></div><br><p> ä¹‹åï¼Œæ ¹æ®åŒä¸€ç¯‡æ–‡ç« ï¼Œä»–å¼€å§‹ä»”ç»†æ•´ç†4.15å†…æ ¸å¥æŸ„ã€‚ è€ƒè™‘åˆ°åœ¨128G RAMèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬è·å¾—äº†<strong>sysctl</strong>çš„æŸä¸ªé…ç½®æ–‡ä»¶ </p><br><div class="spoiler">  <b class="spoiler_title">çŒ«/etc/sysctl.d/50-ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">net.core.rmem_max = 56623104 #        54M net.core.wmem_max = 56623104 #        54M net.core.rmem_default = 56623104 #        . 54M net.core.wmem_default = 56623104 #         54M #    net.ipv4.tcp_rmem = 4096 87380 56623104 # (,  , )    tcp_rmem #  3  ,      TCP. # :   TCP       #   .     #       (moderate memory pressure). #       8  (8192). #  :  ,    #   TCP  .     #  /proc/sys/net/core/rmem_default,   . #       ( ) #  87830 .     65535  #     tcp_adv_win_scale  tcp_app_win = 0, #  ,       tcp_app_win. # :   ,     #     TCP.     , #    /proc/sys/net/core/rmem_max.  Â«Â» #     SO_RCVBUF     . net.ipv4.tcp_wmem = 4096 65536 56623104 net.core.somaxconn = 5000 #    ,  . net.ipv4.tcp_timestamps=1 #     (timestamps),    RFC 1323. net.ipv4.tcp_sack=1 #     TCP net.core.netdev_max_backlog=5000 ( 1000) #       ,  #    ,     . net.ipv4.tcp_max_tw_buckets=262144 #   ,    TIME-WAIT . #     â€“ Â«Â»     #    . net.ipv4.tcp_tw_reuse=1 #   TIME-WAIT   , #     . net.core.optmem_max=4194304 #   - ALLOCATABLE #    (4096 ) net.ipv4.tcp_low_latency=1 #  TCP/IP      #     . net.ipv4.tcp_adv_win_scale=1 #          , #    TCP-    . #   tcp_adv_win_scale ,     #   : # Bytes- bytes\2  -tcp_adv_win_scale #  bytes â€“     .   tcp_adv_win_scale # ,       : # Bytes- bytes\2  tcp_adv_win_scale #    .  - â€“ 2, # ..     Â¼  ,   # tcp_rmem. net.ipv4.tcp_slow_start_after_idle=0 #    ,     # ,       . #   SSR  ,    #  . net.ipv4.tcp_no_metrics_save=1 #    TCP      . net.ipv4.tcp_syncookies=0 #   syncookie net.ipv4.tcp_ecn=0 #Explicit Congestion Notification (   )  # TCP-.      Â«Â» #       .     # -        #    . net.ipv4.conf.all.send_redirects=0 #   ICMP Redirect â€¦  .    #   ,        . #    . net.ipv4.ip_forward=0 #  .   ,     , #    . net.ipv4.icmp_echo_ignore_broadcasts=1 #   ICMP ECHO ,    net.ipv4.tcp_fin_timeout=10 #      FIN-WAIT-2   #   .  60 net.core.netdev_budget=600 # ( 300) #        , #          #  .    NIC ,    . # ,     SoftIRQs # ( )  CPU.    netdev_budget. #    300.    SoftIRQ  # 300   NIC     CPU net.ipv4.tcp_fastopen=3 # TFO TCP Fast Open #        TFO,      #    TCP .     ,  #  )</code> </pre> </div></div><br><p> ä½¿ç”¨<strong>å…‰æ³½ç½‘ç»œæ—¶ï¼Œå®ƒ</strong>åœ¨å•ç‹¬çš„10Gbpsç½‘ç»œæ¥å£ä¸Šåˆ†é…ç»™äº†å•ç‹¬çš„å¹³é¢ç½‘ç»œã€‚ ä¸¤å°å•ç‹¬çš„10Gbpsäº¤æ¢æœºä¸­<strong>éšé™„çš„Mellanox</strong> 10/25 GbpsåŒç«¯å£ç½‘å¡å·²åœ¨æ¯å°è®¡ç®—æœºä¸Šäº¤ä»˜ã€‚ èšåˆæ˜¯ä½¿ç”¨OSPFè¿›è¡Œçš„ï¼Œå› ä¸ºä¸lacpç»‘å®šç”±äºæŸç§åŸå› æ˜¾ç¤ºçš„æ€»å¸¦å®½æœ€å¤§ä¸º16 Gbpsï¼Œè€Œospfåˆ™æˆåŠŸåœ°åœ¨æ¯å°è®¡ç®—æœºä¸Šå®Œå…¨åˆ©ç”¨äº†æ•°åä¸ªå¸¦å®½ã€‚ æœªæ¥çš„è®¡åˆ’æ˜¯åœ¨è¿™äº›é»‘è‰²ç´ ä¸Šä½¿ç”¨ROCEæ¥å‡å°‘å»¶è¿Ÿã€‚ å¦‚ä½•é…ç½®ç½‘ç»œçš„è¿™ä¸€éƒ¨åˆ†ï¼š </p><br><ol><li> ç”±äºæœºå™¨æœ¬èº«åœ¨BGPä¸Šå…·æœ‰å¤–éƒ¨IPï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è½¯ä»¶- <em>ï¼ˆæˆ–è€…ï¼Œåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œå®ƒæ˜¯<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">frr = 6.0-1</a> ï¼‰</em> ã€‚ </li><li> æœºå™¨æ€»å…±æœ‰ä¸¤ä¸ªç½‘ç»œæ¥å£å’Œä¸¤ä¸ªæ¥å£-æ€»å…±4ä¸ªç«¯å£ã€‚ ä¸€å¼ ç½‘å¡æŸ¥çœ‹å¸¦æœ‰ä¸¤ä¸ªç«¯å£çš„å·¥å‚ï¼Œå¹¶åœ¨ä¸Šé¢é…ç½®äº†BGPï¼Œç¬¬äºŒå¼ -çœ‹ä¸¤ä¸ªç«¯å£æŸ¥çœ‹äº†ä¸¤ä¸ªä¸åŒçš„äº¤æ¢æœºï¼Œå¹¶åœ¨ä¸Šé¢è®¾ç½®äº†OSPF </li></ol><br><p>  OSPFè®¾ç½®è¯¦ç»†ä¿¡æ¯ï¼šä¸»è¦ä»»åŠ¡æ˜¯èšåˆä¸¤ä¸ªé“¾è·¯å¹¶å…·æœ‰å®¹é”™èƒ½åŠ›ã€‚ <br> åœ¨ä¸¤ä¸ªç®€å•çš„å¹³é¢ç½‘ç»œä¸­é…ç½®äº†ä¸¤ä¸ªç½‘ç»œæ¥å£-10.10.10.0/24å’Œ10.10.20.0/24 </p><br><pre> <code class="plaintext hljs">1: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.10.2/24 brd 10.10.10.255 scope global ens1f0 2: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.20.2/24 brd 10.10.20.255 scope global ens1f1</code> </pre> <br><p> å“ªè¾†è½¦å½¼æ­¤è§é¢ã€‚ </p><br><h3 id="disk"> ç£ç›˜ </h3><br><p> ä¸‹ä¸€æ­¥æ˜¯ä¼˜åŒ–ç£ç›˜æ€§èƒ½ã€‚ å¯¹äºSSDï¼Œæˆ‘å°†è°ƒåº¦ç¨‹åºæ›´æ”¹ä¸º<strong>noop</strong> ï¼ˆå¯¹äºHDD <strong>æˆªæ­¢æ—¥æœŸï¼‰</strong> ã€‚ å¦‚æœå¤§è‡´-NOOPéµå¾ªâ€œè°å…ˆç«™èµ·æ¥-å’Œæ‹–é‹â€çš„åŸåˆ™ï¼Œåœ¨è‹±è¯­ä¸­å¬èµ·æ¥åƒæ˜¯â€œ FIFOï¼ˆå…ˆè¿›å…ˆå‡ºï¼‰â€ã€‚ è¯·æ±‚åœ¨å¯ç”¨æ—¶æ’é˜Ÿã€‚  DEADLINEæ›´å…·åªè¯»æ€§ï¼Œè€Œä¸”åœ¨æ“ä½œæ—¶ï¼Œé˜Ÿåˆ—ä¸­çš„è¿›ç¨‹å‡ ä¹å¯ä»¥ç‹¬å è®¿é—®ç£ç›˜ã€‚ è¿™å¯¹æˆ‘ä»¬çš„ç³»ç»Ÿéå¸¸æœ‰ç”¨-æ¯•ç«Ÿï¼Œæ¯ä¸ªç£ç›˜ä¸Šåªæœ‰ä¸€ä¸ªè¿›ç¨‹å¯ä»¥å·¥ä½œ-OSDå®ˆæŠ¤ç¨‹åºã€‚ <br>  ï¼ˆé‚£äº›å¸Œæœ›æ²‰æµ¸åœ¨I / Oè°ƒåº¦ç¨‹åºä¸­çš„äººå¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æœ‰å…³å®ƒçš„ä¿¡æ¯ï¼š <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> </p><br><p> æœ€å¥½ä»¥ä¿„è¯­é˜…è¯»ï¼š <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">//www.opennet.ru/base/sys/linux_shedulers.txt.html</a> ï¼‰ </p><br><p> åœ¨å»ºè®®çš„è°ƒæ•´Linuxä¸­ï¼Œå»ºè®®ä¹Ÿå¢åŠ nr_request </p><br><blockquote>  <em>nr_requests</em> <em><br></em>  <em>nr_requestsçš„å€¼ç¡®å®šåœ¨I / Oè°ƒåº¦ç¨‹åºå‘å—è®¾å¤‡å‘é€/æ¥æ”¶æ•°æ®ä¹‹å‰ï¼Œè¦ç¼“å†²çš„I / Oè¯·æ±‚çš„æ•°é‡ï¼ˆå¦‚æœä½¿ç”¨çš„RAIDå¡/å—è®¾å¤‡å¯ä»¥å¤„ç†æ¯”Iå¤§çš„é˜Ÿåˆ—ï¼‰ / Oè°ƒåº¦ç¨‹åºè®¾ç½®ä¸ºï¼Œå½“æœåŠ¡å™¨ä¸Šå‘ç”Ÿå¤§é‡I / Oæ—¶ï¼Œæé«˜nr_requestsçš„å€¼å¯èƒ½æœ‰åŠ©äºæ•´ä½“æ”¹å–„å¹¶å‡å°‘æœåŠ¡å™¨è´Ÿè½½ã€‚</em>  <em>å¦‚æœå°†Deadlineæˆ–CFQç”¨ä½œè°ƒåº¦ç¨‹åºï¼Œå»ºè®®æ‚¨å°†nr_requestå€¼è®¾ç½®ä¸ºé˜Ÿåˆ—æ·±åº¦å€¼çš„2å€ã€‚</em> </blockquote><p> ä½†æ˜¯ï¼ å…¬æ°‘æœ¬èº«å°±æ˜¯CEPHå¼€å‘äººå‘˜ï¼Œä»–ä»¬ä½¿æˆ‘ä»¬ç›¸ä¿¡ä»–ä»¬çš„ä¼˜å…ˆçº§ç³»ç»Ÿæ•ˆæœæ›´å¥½ </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b7e/fd3/bd0/b7efd3bd03fedc88307e200905c8c6a9.gif"></p><br><div class="spoiler">  <b class="spoiler_title">WBThrottleå’Œ/æˆ–nr_requests</b> <div class="spoiler_text"><blockquote>  WBThrottleå’Œ/æˆ–nr_requests <br> æ–‡ä»¶å­˜å‚¨ä½¿ç”¨ç¼“å†²çš„I / Oè¿›è¡Œå†™å…¥ï¼› å¦‚æœæ–‡ä»¶å­˜å‚¨æ—¥å¿—ä½äºè¾ƒå¿«çš„ä»‹è´¨ä¸Šï¼Œè¿™å°†å¸¦æ¥è®¸å¤šå¥½å¤„ã€‚ ä¸€æ—¦å°†æ•°æ®å†™å…¥æ—¥å¿—ï¼Œä¾¿ä¼šé€šçŸ¥å®¢æˆ·ç«¯è¯·æ±‚ï¼Œç„¶ååœ¨ä»¥åä½¿ç”¨æ ‡å‡†LinuxåŠŸèƒ½å°†å…¶åˆ·æ–°åˆ°æ•°æ®ç£ç›˜æœ¬èº«ã€‚ è¿™æ ·ï¼ŒOSDä¸»è½´ç£ç›˜å°±å¯ä»¥åœ¨å†™å…¥å°æ•°æ®åŒ…æ—¶æä¾›ç±»ä¼¼äºSSDçš„å†™å…¥å»¶è¿Ÿã€‚ è¿™ç§å»¶è¿Ÿçš„å»¶è¿Ÿå†™å…¥æ“ä½œè¿˜ä½¿å†…æ ¸æœ¬èº«å¯ä»¥é‡å»ºå¯¹ç£ç›˜çš„I / Oè¯·æ±‚ï¼Œå¸Œæœ›å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ï¼Œæˆ–è€…å…è®¸ç°æœ‰çš„ç£ç›˜å¤´åœ¨å…¶æ¿é¡¶ä¸Šé€‰æ‹©ä¸€äº›æœ€ä½³è·¯å¾„ã€‚ æœ€ç»ˆç»“æœæ˜¯ï¼Œä¸ç›´æ¥æˆ–åŒæ­¥I / Oç›¸æ¯”ï¼Œæ‚¨å¯ä»¥ä»æ¯ä¸ªç£ç›˜ä¸­æŒ¤å‡ºæ›´å¤šçš„I / Oã€‚ </blockquote><p> ä½†æ˜¯ï¼Œå¦‚æœç»™å®šCephç¾¤é›†ä¸­çš„ä¼ å…¥è®°å½•é‡è¶…è¿‡åŸºç¡€ç£ç›˜çš„æ‰€æœ‰åŠŸèƒ½ï¼Œåˆ™ä¼šå‡ºç°æŸäº›é—®é¢˜ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœªå†³I / Oå¾…å†™å…¥ç£ç›˜çš„æ€»æ•°å¯èƒ½ä¼šä¸å—æ§åˆ¶åœ°å¢é•¿ï¼Œå¹¶å¯¼è‡´å¡«æ»¡æ•´ä¸ªç£ç›˜å’ŒCephé˜Ÿåˆ—çš„I / Oæ“ä½œé˜Ÿåˆ—ã€‚ è¯»è¯·æ±‚çš„å·¥ä½œç‰¹åˆ«ç³Ÿç³•ï¼Œå› ä¸ºå®ƒä»¬å¡åœ¨å†™è¯·æ±‚ä¹‹é—´ï¼Œè¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿçš„æ—¶é—´æ‰èƒ½åˆ·æ–°åˆ°ä¸»ç£ç›˜ã€‚ </p><br><p> ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒCephåœ¨æ–‡ä»¶å­˜å‚¨ä¸­å†…ç½®äº†ç§°ä¸ºWBThrottleçš„å†™å›é™åˆ¶æœºåˆ¶ã€‚ å®ƒæ—¨åœ¨é™åˆ¶å¯æ’é˜Ÿçš„æœªå†³å†™I / Oæ“ä½œçš„æ€»æ•°ï¼Œå¹¶æ¯”ç”±äºå†…æ ¸æœ¬èº«åŒ…å«è€Œè‡ªç„¶å‘ç”Ÿçš„å¯åŠ¨ææ—©å¯åŠ¨é‡ç½®è¿‡ç¨‹ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œæµ‹è¯•è¡¨æ˜é»˜è®¤å€¼å¯èƒ½ä»æ— æ³•å°†ç°æœ‰è¡Œä¸ºé™ä½åˆ°å¯ä»¥å‡å°‘è¿™ç§å¯¹è¯»å–æ“ä½œå»¶è¿Ÿçš„å½±å“çš„æ°´å¹³ã€‚ è°ƒæ•´å¯ä»¥æ”¹å˜è¿™ç§è¡Œä¸ºï¼Œå‡å°‘è®°å½•é˜Ÿåˆ—çš„æ€»é•¿åº¦ï¼Œå¹¶ä½¿è¿™ç§å½±å“ä¸å¤§ã€‚ ä½†æ˜¯ï¼Œè¿™æ˜¯ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆï¼šé€šè¿‡å‡å°‘å…è®¸æ’é˜Ÿçš„æœ€å¤§æ¡ç›®æ€»æ•°ï¼Œå¯ä»¥é™ä½å†…æ ¸æœ¬èº«åœ¨å¯¹ä¼ å…¥è¯·æ±‚è¿›è¡Œæ’åºæ—¶æœ€å¤§åŒ–å…¶æ•ˆç‡çš„èƒ½åŠ›ã€‚ å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæ‚¨éœ€è¦é’ˆå¯¹ç‰¹å®šâ€‹â€‹çš„åº”ç”¨ç¨‹åºï¼Œå·¥ä½œè´Ÿè½½å¹¶è¿›è¡Œè°ƒæ•´ä»¥é€‚åº”å®ƒä»¬ã€‚ </p><br><p> è¦æ§åˆ¶æ­¤ç±»æŒ‚èµ·çš„å†™é˜Ÿåˆ—çš„æ·±åº¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨WBThrottleè®¾ç½®æ¥å‡å°‘å¤±è´¥çš„I / Oæ“ä½œçš„æœ€å¤§æ€»æ•°ï¼Œæˆ–è€…åœ¨å†…æ ¸çš„æ•´ä¸ªå—çº§åˆ«ä¸Šå‡å°‘å¤±è´¥çš„æ“ä½œçš„æœ€å¤§å€¼ã€‚ ä¸¤è€…éƒ½å¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶ç›¸åŒçš„è¡Œä¸ºï¼Œè€Œæ‚¨çš„åå¥½å°†æˆä¸ºå®ç°æ­¤è®¾ç½®çš„åŸºç¡€ã€‚ <br> è¿˜åº”æ³¨æ„ï¼Œå¯¹äºç£ç›˜çº§åˆ«çš„è¾ƒçŸ­æŸ¥è¯¢ï¼ŒCephæ“ä½œä¼˜å…ˆçº§ç³»ç»Ÿæ›´ä¸ºæœ‰æ•ˆã€‚ å°†æ€»é˜Ÿåˆ—å‡å°‘åˆ°ç»™å®šç£ç›˜æ—¶ï¼Œä¸»é˜Ÿåˆ—ä½ç½®ç§»è‡³Cephï¼Œåœ¨æ­¤å®ƒå¯ä»¥æ›´å¥½åœ°æ§åˆ¶I / Oæ“ä½œçš„ä¼˜å…ˆçº§ã€‚ è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹ï¼š </p><br><pre> <code class="plaintext hljs">echo 8 &gt; /sys/block/sda/queue/nr_requests</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> </p></div></div><br><h3 id="common"> å…±åŒç‚¹ </h3><br><p> è¿˜æœ‰ä¸€äº›å…¶ä»–çš„å†…æ ¸è®¾ç½® <del> ä½ çš„è½¦æŸ”è½¯å¦‚ä¸ </del> å‘æŒ¥æ›´å¤šæ€§èƒ½ </p><br><div class="spoiler">  <b class="spoiler_title">çŒ«/etc/sysctl.d/60-ceph2.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> kernel.pid_max = 4194303 #     25,       kernel.threads-max=2097152 # , , . vm.max_map_count=524288 #      . #        #         # malloc,    mmap, mprotect  madvise,     #  . fs.aio-max-nr=50000000 #   input-output #  Linux     - (AIO), #       - # ,    -  . #     , #      -. #  aio-max-nr     #  . vm.min_free_kbytes=1048576 #       . #  1Gb,       , #    OOM Killer   OSD.     #    ,      vm.swappiness=10 #       10% . #   128G ,  10%  12 .     . #    60%   ,   , #       vm.vfs_cache_pressure=1000 #    100.     #     . vm.zone_reclaim_mode=0 #         #  ,     . #     ,     . #       # ,    , zone_reclaim_mode #  ,   , # ,   ,   . vm.dirty_ratio=20 #   ,     ""  #    : #   128  . #   20  SSD,     CEPH  #     3G . #   40  HDD,      1G # 20%  128  25.6 . ,     , #    2.4G .         #    -    DevOps   . vm.dirty_background_ratio=3 #   ,    dirty pages  , #    pdflush/flush/kdmflush     fs.file-max=524288 #      ,,   ,    .</code> </pre> </div></div><br><h2 id="pogruzhenie-v--ceph"> åœ¨CEPHæ½œæ°´ </h2><br><p> æˆ‘æƒ³æ›´è¯¦ç»†åœ°äº†è§£çš„è®¾ç½®ï¼š </p><br><div class="spoiler">  <b class="spoiler_title">çŒ«/etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">osd: journal_aio: true #  ,  journal_block_align: true #  i/o journal_dio: true #   journal_max_write_bytes: 1073714824 #     #      journal_max_write_entries: 10000 #      journal_queue_max_bytes: 10485760000 journal_queue_max_ops: 50000 rocksdb_separate_wal_dir: true #    wal #       # NVMe bluestore_block_db_create: true #       bluestore_block_db_size: '5368709120 #5G' bluestore_block_wal_create: true bluestore_block_wal_size: '1073741824 #1G' bluestore_cache_size_hdd: '3221225472 # 3G' #     #     bluestore_cache_size_ssd: '9663676416 # 9G' keyring: /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap: '1073741824 #1G' osd_disk_thread_ioprio_class: idle osd_disk_thread_ioprio_priority: 7 osd_disk_threads: 2 #        osd_failsafe_full_ratio: 0.95 osd_heartbeat_grace: 5 osd_heartbeat_interval: 3 osd_map_dedup: true osd_max_backfills: 2 #       . osd_max_write_size: 256 osd_mon_heartbeat_interval: 5 osd_op_threads: 16 osd_op_num_threads_per_shard: 1 osd_op_num_threads_per_shard_hdd: 2 osd_op_num_threads_per_shard_ssd: 2 osd_pool_default_min_size: 1 #  .    osd_pool_default_size: 2 #  ,    #     #   osd_recovery_delay_start: 10.000000 osd_recovery_max_active: 2 osd_recovery_max_chunk: 1048576 osd_recovery_max_single_start: 3 osd_recovery_op_priority: 1 osd_recovery_priority: 1 #       osd_recovery_sleep: 2 osd_scrub_chunk_max: 4</code> </pre> </div></div><br><p> åœ¨cephç‰ˆæœ¬12.2.2ä¸­ç¼ºå°‘åœ¨ç‰ˆæœ¬12.2.12çš„è´¨é‡æ£€æŸ¥ä¸­æµ‹è¯•è¿‡çš„æŸäº›å‚æ•°ï¼Œä¾‹å¦‚<strong>osd_recovery_threadsã€‚</strong> å› æ­¤ï¼Œè®¡åˆ’åŒ…æ‹¬å°†äº§å“æ›´æ–°ä¸º12.2.12ã€‚ å®è·µè¯æ˜åœ¨ä¸€ä¸ªç‰ˆæœ¬ä¸º12.2.2å’Œ12.2.12çš„ç¾¤é›†ä¸­å…·æœ‰å…¼å®¹æ€§ï¼Œè¯¥ç¾¤é›†å…è®¸æ»šåŠ¨æ›´æ–°ã€‚ </p><br><h3 id="testovyy-klaster"> æµ‹è¯•é›†ç¾¤ </h3><br><p> è‡ªç„¶ï¼Œä¸ºäº†è¿›è¡Œæµ‹è¯•ï¼Œå¿…é¡»å…·æœ‰ä¸æˆ˜æ–—ä¸­ç›¸åŒçš„ç‰ˆæœ¬ï¼Œä½†æ˜¯å½“æˆ‘å¼€å§‹ä½¿ç”¨å­˜å‚¨åº“ä¸­çš„é›†ç¾¤æ—¶ï¼Œåªæœ‰ä¸€ä¸ªè¾ƒæ–°çš„ç‰ˆæœ¬ã€‚ çœ‹åˆ°æ¬¡è¦ç‰ˆæœ¬ä¸æ˜¯å¾ˆå¤§ï¼ˆé…ç½®ä¸­ä¸º<strong>1393</strong>è¡Œï¼Œè€Œæ–°ç‰ˆæœ¬ä¸­ä¸º<strong>1436</strong>è¡Œï¼‰ï¼Œæˆ‘ä»¬å†³å®šå¼€å§‹æµ‹è¯•ä¸€ä¸ªæ–°ç‰ˆæœ¬ï¼ˆä»å¤„äºæ›´æ–°çŠ¶æ€ï¼Œä¸ºä»€ä¹ˆè¦ç»§ç»­ä½¿ç”¨æ—§çš„åºŸçº¸ï¼‰ï¼‰ </p><br><p> æˆ‘ä»¬å°è¯•ä¿ç•™æ—§ç‰ˆæœ¬çš„å”¯ä¸€æ–¹æ³•æ˜¯<strong>ceph-deploy</strong>è½¯ä»¶åŒ…<strong>ï¼Œ</strong>å› ä¸ºéƒ¨åˆ†å®ç”¨ç¨‹åºï¼ˆå’Œéƒ¨åˆ†å‘˜å·¥ï¼‰çš„è¯­æ³•å·²å¾—åˆ°åŠ å¼ºã€‚ æ–°ç‰ˆæœ¬æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œä½†æ˜¯å®ƒå¹¶æ²¡æœ‰å½±å“ç¾¤é›†æœ¬èº«çš„è¿è¡Œï¼Œè€Œ<strong>1.5.39</strong>ç‰ˆ<strong>åˆ™</strong>ä¿ç•™äº†<strong>å®ƒã€‚</strong> </p><br><p> ç”±äºceph-diskå‘½ä»¤æ¸…æ¥šåœ°è¡¨æ˜å®ƒå·²è¢«å¼ƒç”¨ï¼Œäº²çˆ±çš„ceph-volumeå‘½ä»¤å·²ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬å¼€å§‹ä½¿ç”¨æ­¤å‘½ä»¤åˆ›å»ºOSDï¼Œè€Œä¸ä¼šæµªè´¹æ—¶é—´ã€‚ </p><br><p> è®¡åˆ’æ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªSSDç£ç›˜çš„é•œåƒï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢æ”¾ç½®OSDæ—¥å¿—ï¼Œè¿™äº›æ—¥å¿—åˆä½äºä¸»è½´SASä¸Šã€‚ å› æ­¤ï¼Œå½“å¸¦æœ‰æ—¥å¿—çš„ç£ç›˜æ‰è½æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¿æŠ¤è‡ªå·±å…å—æ•°æ®é—®é¢˜çš„å½±å“ã€‚ </p><br><p> åˆ›å»ºé’¢æ–‡æ¡£é›† </p><br><div class="spoiler">  <b class="spoiler_title">çŒ«/etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01-qa:~# cat /etc/ceph/ceph.conf #     [client] rbd_cache = true rbd_cache_max_dirty = 50331648 rbd_cache_max_dirty_age = 2 rbd_cache_size = 67108864 rbd_cache_target_dirty = 33554432 rbd_cache_writethrough_until_flush = true rbd_concurrent_management_ops = 10 rbd_default_format = 2 [global] auth_client_required = cephx auth_cluster_required = cephx auth_service_required = cephx cluster network = 10.10.10.0/24 debug_asok = 0/0 debug_auth = 0/0 debug_buffer = 0/0 debug_client = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_filer = 0/0 debug_filestore = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_journal = 0/0 debug_journaler = 0/0 debug_lockdep = 0/0 debug_mon = 0/0 debug_monc = 0/0 debug_ms = 0/0 debug_objclass = 0/0 debug_objectcatcher = 0/0 debug_objecter = 0/0 debug_optracker = 0/0 debug_osd = 0/0 debug_paxos = 0/0 debug_perfcounter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_rgw = 0/0 debug_throttle = 0/0 debug_timer = 0/0 debug_tp = 0/0 fsid = d0000000d-4000-4b00-b00b-0123qwe123qwf9 mon_host = ceph01-q, ceph02-q, ceph03-q mon_initial_members = ceph01-q, ceph02-q, ceph03-q public network = 8.8.8.8/28 #  ,  )) rgw_dns_name = s3-qa.mycompany.ru #     rgw_host = s3-qa.mycompany.ru #    [mon] mon allow pool delete = true mon_max_pg_per_osd = 300 #     #     #  , ,    , #     OSD.     PG #     -    mon_osd_backfillfull_ratio = 0.9 mon_osd_down_out_interval = 5 mon_osd_full_ratio = 0.95 #   SSD     #   -      #   5%   (   1.2Tb) #   ,     # bluestore_block_db_size     #   mon_osd_nearfull_ratio = 0.9 mon_pg_warn_max_per_osd = 520 [osd] bluestore_block_db_create = true bluestore_block_db_size = 5368709120 #5G bluestore_block_wal_create = true bluestore_block_wal_size = 1073741824 #1G bluestore_cache_size_hdd = 3221225472 # 3G bluestore_cache_size_ssd = 9663676416 # 9G journal_aio = true journal_block_align = true journal_dio = true journal_max_write_bytes = 1073714824 journal_max_write_entries = 10000 journal_queue_max_bytes = 10485760000 journal_queue_max_ops = 50000 keyring = /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap = 1073741824 #1G osd_disk_thread_ioprio_class = idle osd_disk_thread_ioprio_priority = 7 osd_disk_threads = 2 osd_failsafe_full_ratio = 0.95 osd_heartbeat_grace = 5 osd_heartbeat_interval = 3 osd_map_dedup = true osd_max_backfills = 4 osd_max_write_size = 256 osd_mon_heartbeat_interval = 5 osd_op_num_threads_per_shard = 1 osd_op_num_threads_per_shard_hdd = 2 osd_op_num_threads_per_shard_ssd = 2 osd_op_threads = 16 osd_pool_default_min_size = 1 osd_pool_default_size = 2 osd_recovery_delay_start = 10.0 osd_recovery_max_active = 1 osd_recovery_max_chunk = 1048576 osd_recovery_max_single_start = 3 osd_recovery_op_priority = 1 osd_recovery_priority = 1 osd_recovery_sleep = 2 osd_scrub_chunk_max = 4 osd_scrub_chunk_min = 2 osd_scrub_sleep = 0.1 rocksdb_separate_wal_dir = true</code> </pre> </div></div><br><pre> <code class="plaintext hljs">#   root@ceph01-qa:~#ceph-deploy mon create ceph01-q #        root@ceph01-qa:~#ceph-deploy gatherkeys ceph01-q #   .       - ,       # mon_initial_members = ceph01-q, ceph02-q, ceph03-q #         root@ceph01-qa:~#ceph-deploy mon create-initial #        root@ceph01-qa:~#cat ceph.bootstrap-osd.keyring &gt; /var/lib/ceph/bootstrap-osd/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-mgr.keyring &gt; /var/lib/ceph/bootstrap-mgr/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-rgw.keyring &gt; /var/lib/ceph/bootstrap-rgw/ceph.keyring #      root@ceph01-qa:~#ceph-deploy admin ceph01-q #  ,   root@ceph01-qa:~#ceph-deploy mgr create ceph01-q</code> </pre> <br><p> ,        ceph-deploy    12.2.12 â€”      OSD  db    - </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 blkid could not detect a PARTUUID for device: /dev/md1</code> </pre> <br><p> , blkid   PARTUUID,    : </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#parted /dev/md0 mklabel GPT #   , #  GPT     #        = bluestore_block_db_size: '5368709120 #5G' #    20  OSD,     #    root@ceph01-qa:~#for i in {1..20}; do echo -e "n\n\n\n+5G\nw" | fdisk /dev/md0; done</code> </pre> <br><p>   ,     OSD     (, ,    ) </p><br><p>   OSD  bluestore     WAL,    db </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 stderr: 2019-04-12 10:39:27.211242 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid stderr: 2019-04-12 10:39:27.213185 7eff461b6e00 -1 bdev(0x55824c273680 /var/lib/ceph/osd/ceph-0//block.wal) open open got: (22) Invalid argument stderr: 2019-04-12 10:39:27.213201 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _open_db add block device(/var/lib/ceph/osd/ceph-0//block.wal) returned: (22) Invalid argument stderr: 2019-04-12 10:39:27.999039 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) mkfs failed, (22) Invalid argument stderr: 2019-04-12 10:39:27.999057 7eff461b6e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (22) Invalid argument stderr: 2019-04-12 10:39:27.999141 7eff461b6e00 -1 ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0/: (22) Invalid argumen</code> </pre> <br><p>     -  (   ,  )      WAL      OSD â€”     (    WAL,  , ,   ). </p><br><p> ,         WAL  NVMe,     . </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sdf --block.wal /dev/md0p2 --block.db /dev/md1p2</code> </pre> <br><p>  ,   OSD.      ,        â€”    SSD  ,     SAS. </p><br><p>       20 ,     ,  â€” . <br> , ,   : </p><br><div class="spoiler"> <b class="spoiler_title">ceph osd tree</b> <div class="spoiler_text"><p> root@eph01-q:~# ceph osd tree <br> ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF <br> -1 14.54799 root default <br> -3 9.09200 host ceph01-q <br> 0 ssd 1.00000 osd.0 up 1.00000 1.00000 <br> 1 ssd 1.00000 osd.1 up 1.00000 1.00000 <br> 2 ssd 1.00000 osd.2 up 1.00000 1.00000 <br> 3 ssd 1.00000 osd.3 up 1.00000 1.00000 <br> 4 hdd 1.00000 osd.4 up 1.00000 1.00000 <br> 5 hdd 0.27299 osd.5 up 1.00000 1.00000 <br> 6 hdd 0.27299 osd.6 up 1.00000 1.00000 <br> 7 hdd 0.27299 osd.7 up 1.00000 1.00000 <br> 8 hdd 0.27299 osd.8 up 1.00000 1.00000 <br> 9 hdd 0.27299 osd.9 up 1.00000 1.00000 <br> 10 hdd 0.27299 osd.10 up 1.00000 1.00000 <br> 11 hdd 0.27299 osd.11 up 1.00000 1.00000 <br> 12 hdd 0.27299 osd.12 up 1.00000 1.00000 <br> 13 hdd 0.27299 osd.13 up 1.00000 1.00000 <br> 14 hdd 0.27299 osd.14 up 1.00000 1.00000 <br> 15 hdd 0.27299 osd.15 up 1.00000 1.00000 <br> 16 hdd 0.27299 osd.16 up 1.00000 1.00000 <br> 17 hdd 0.27299 osd.17 up 1.00000 1.00000 <br> 18 hdd 0.27299 osd.18 up 1.00000 1.00000 <br> 19 hdd 0.27299 osd.19 up 1.00000 1.00000 <br> -5 5.45599 host ceph02-q <br> 20 ssd 0.27299 osd.20 up 1.00000 1.00000 <br> 21 ssd 0.27299 osd.21 up 1.00000 1.00000 <br> 22 ssd 0.27299 osd.22 up 1.00000 1.00000 <br> 23 ssd 0.27299 osd.23 up 1.00000 1.00000 <br> 24 hdd 0.27299 osd.24 up 1.00000 1.00000 <br> 25 hdd 0.27299 osd.25 up 1.00000 1.00000 <br> 26 hdd 0.27299 osd.26 up 1.00000 1.00000 <br> 27 hdd 0.27299 osd.27 up 1.00000 1.00000 <br> 28 hdd 0.27299 osd.28 up 1.00000 1.00000 <br> 29 hdd 0.27299 osd.29 up 1.00000 1.00000 <br> 30 hdd 0.27299 osd.30 up 1.00000 1.00000 <br> 31 hdd 0.27299 osd.31 up 1.00000 1.00000 <br> 32 hdd 0.27299 osd.32 up 1.00000 1.00000 <br> 33 hdd 0.27299 osd.33 up 1.00000 1.00000 <br> 34 hdd 0.27299 osd.34 up 1.00000 1.00000 <br> 35 hdd 0.27299 osd.35 up 1.00000 1.00000 <br> 36 hdd 0.27299 osd.36 up 1.00000 1.00000 <br> 37 hdd 0.27299 osd.37 up 1.00000 1.00000 <br> 38 hdd 0.27299 osd.38 up 1.00000 1.00000 <br> 39 hdd 0.27299 osd.39 up 1.00000 1.00000 <br> -7 6.08690 host ceph03-q <br> 40 ssd 0.27299 osd.40 up 1.00000 1.00000 <br> 41 ssd 0.27299 osd.41 up 1.00000 1.00000 <br> 42 ssd 0.27299 osd.42 up 1.00000 1.00000 <br> 43 ssd 0.27299 osd.43 up 1.00000 1.00000 <br> 44 hdd 0.27299 osd.44 up 1.00000 1.00000 <br> 45 hdd 0.27299 osd.45 up 1.00000 1.00000 <br> 46 hdd 0.27299 osd.46 up 1.00000 1.00000 <br> 47 hdd 0.27299 osd.47 up 1.00000 1.00000 <br> 48 hdd 0.27299 osd.48 up 1.00000 1.00000 <br> 49 hdd 0.27299 osd.49 up 1.00000 1.00000 <br> 50 hdd 0.27299 osd.50 up 1.00000 1.00000 <br> 51 hdd 0.27299 osd.51 up 1.00000 1.00000 <br> 52 hdd 0.27299 osd.52 up 1.00000 1.00000 <br> 53 hdd 0.27299 osd.53 up 1.00000 1.00000 <br> 54 hdd 0.27299 osd.54 up 1.00000 1.00000 <br> 55 hdd 0.27299 osd.55 up 1.00000 1.00000 <br> 56 hdd 0.27299 osd.56 up 1.00000 1.00000 <br> 57 hdd 0.27299 osd.57 up 1.00000 1.00000 <br> 58 hdd 0.27299 osd.58 up 1.00000 1.00000 <br> 59 hdd 0.89999 osd.59 up 1.00000 1.00000 </p></div></div><br><p>          : </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket rack01 root #  root root@ceph01-q:~#ceph osd crush add-bucket ceph01-q host #   root@ceph01-q:~#ceph osd crush move ceph01-q root=rack01 #     root@ceph01-q:~#osd crush add 28 1.0 host=ceph02-q #     #       root@ceph01-q:~# ceph osd crush remove osd.4 root@ceph01-q:~# ceph osd crush remove rack01</code> </pre> <br><p> ,      <strong></strong> ,            â€”  <strong>ceph osd crush move ceph01-host root=rack01</strong> ,      .    CTRL+C     . </p><br><p>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://tracker.ceph.com/issues/23386</a> </p><br><p>    crushmap     <strong>rule replicated_ruleset</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd getcrushmap -o crushmap.row #     root@ceph01-prod:~#crushtool -d crushmap.row -o crushmap.txt #   root@ceph01-prod:~#vim crushmap.txt #,  rule replicated_ruleset root@ceph01-prod:~#crushtool -c crushmap.txt -o new_crushmap.row #  root@ceph01-prod:~#ceph osd setcrushmap -i new_crushmap.row #  </code> </pre> <br><p> <strong>:</strong>      placement group  OSD.    ,   . </p><br><p>  ,        â€”  ,     OSD ,        ,    root default. <br>  ,   ,      root  ssd     ,          default root.   OSD     . <br> <em>     ,     .     </em> </p><br><h3 id="kak-my-delali-razlichnye-gruppy-po-tipam-diskov">        . </h3><br><p>     root- â€”  ssd   hdd </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket ssd-root root root@ceph01-q:~#ceph osd crush add-bucket hdd-root root</code> </pre> <br><p>        â€”          </p><br><pre> <code class="plaintext hljs"># : root@ceph01-q:~#ceph osd crush add-bucket ssd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack02 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack03 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack #  root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph03-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host</code> </pre> <br><p>          </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#   0  3  SSD,   ceph01-q,     root@ceph01-q:~# ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 0 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 1 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 2 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 3 1 host=ssd-ceph01-q root-ceph01-q:~#    </code> </pre> <br><p>     ssd-root  hdd-root   root-default ,     </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush remove default</code> </pre> <br><p>     ,        â€”      root          â€”        ,     (    root,    ) </p><br><p>        : <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules</a> </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush rule create-simple rule-ssd ssd-root host firstn root-ceph01-q:~#ceph osd crush rule create-simple rule-hdd hdd-root host firstn root-ceph01-q:~#    ,     root-ceph01-q:~#   -        , root-ceph01-q:~#       root-ceph01-q:~#  ,   ,    root-ceph01-q:~#        : root-ceph01-q:~# ##ceph osd crush rule create-simple rule-ssd ssd-root rack firstn</code> </pre> <br><p>    ,            â€” PROXMOX: </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~# #ceph osd pool create {NAME} {pg_num} {pgp_num} root-ceph01-q:~# ceph osd pool create ssd_pool 1024 1024 root-ceph01-q:~# ceph osd pool create hdd_pool 1024 1024</code> </pre> <br><p>         </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~#ceph osd crush rule ls #    root-ceph01-q:~#ceph osd crush rule dump rule-ssd | grep rule_id # ID  root-ceph01-q:~#ceph osd pool set ssd_pool crush_rule 2</code> </pre><br><p>               â€”     ,    (    )   ,    . </p><br><p>      300    ,        â€”        10 Tb    10 PG â€”      (pg)   â€”           ). </p><br><p>        PG â€”         â€”     . </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a> ,    CEPH. </p><br><p>  : </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://tracker.ceph.com/issues/23386</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://ceph.com/pgcalc/</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN456446/">https://habr.com/ru/post/zh-CN456446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN456428/index.html">å¦‚ä½•å®æ–½BIæ–¹æ³•è¿›è¡Œæ•°æ®åˆ†æï¼šå®ç”¨å»ºè®®</a></li>
<li><a href="../zh-CN456430/index.html">æŠ€æœ¯å€ºåŠ¡çš„æ°¸æ’é—®é¢˜</a></li>
<li><a href="../zh-CN456436/index.html">æ˜ŸæœŸä¸€çš„ç®€çŸ­JSä»»åŠ¡</a></li>
<li><a href="../zh-CN456440/index.html">éš¾ä»¥æ‰æ‘¸çš„é©¬å°”ç“¦é‡Œå†é™©è®°ï¼Œç¬¬ä¸€éƒ¨åˆ†</a></li>
<li><a href="../zh-CN456442/index.html">åœ¨Yandexå’ŒJetBrainsçš„æ”¯æŒä¸‹è¿›å…¥åœ£å½¼å¾—å ¡å›½ç«‹å¤§å­¦çš„æœ¬ç§‘è¯¾ç¨‹</a></li>
<li><a href="../zh-CN456448/index.html">é€‰æ‹©JSæ¡†æ¶çš„è§„åˆ™</a></li>
<li><a href="../zh-CN456450/index.html">DO-RA.Aviaç”¨äºç›‘æ§èˆªç©ºå®‡å®™è¾å°„</a></li>
<li><a href="../zh-CN456452/index.html">Rangesä¹‹å‰å’Œä¹‹åçš„C ++ä»£ç ç¤ºä¾‹</a></li>
<li><a href="../zh-CN456462/index.html">å°†è§’åº¦ç»„ä»¶åº“ç»„è£…ä¸ºWebç»„ä»¶</a></li>
<li><a href="../zh-CN456466/index.html">å¦‚ä»Šçš„PHPæ³›å‹ï¼ˆå·®ä¸å¤šï¼‰</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>