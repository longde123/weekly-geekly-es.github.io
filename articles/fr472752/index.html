<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äç‚úàÔ∏è üå´Ô∏è ü•§ CSE: Kubernetes pour n'importe qui dans vCloud üò¥ ‚ô£Ô∏è üåø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour √† tous! 

 Il se trouve que notre petite √©quipe de d√©veloppement, pour ne pas dire que r√©cemment, et certainement pas tout d'un coup, a grandi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CSE: Kubernetes pour n'importe qui dans vCloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/linxdatacenter/blog/472752/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/0v/u9/pk0vu9vp36lten5zgvswrtyzpl8.png"></div><br>  <b>Bonjour √† tous!</b> <br><br>  Il se trouve que notre petite √©quipe de d√©veloppement, pour ne pas dire que r√©cemment, et certainement pas tout d'un coup, a grandi pour transf√©rer certains (et √† l'avenir, tous) des produits √† Kubernetes. <br><br>  Il y avait plusieurs raisons √† cela, mais notre histoire ne concerne pas l'holivar. <br><br>  Du point de vue des infrastructures, nous avions peu de choix.  vCloud Director et vCloud Director.  Nous avons choisi une version plus r√©cente et avons d√©cid√© de commencer. <br><br>  Une fois de plus, en parcourant ¬´The Hard Way¬ª, j'ai rapidement conclu qu'un outil pour automatiser au moins les processus de base, tels que le d√©ploiement et le dimensionnement, √©tait n√©cessaire hier.  Une immersion profonde dans Google a mis au jour un produit tel que VMware Container Service Extension (CSE) - un produit open source qui vous permet d'automatiser la cr√©ation et le dimensionnement des clusters k8s pour ceux de vCloud. <a name="habracut"></a><br><blockquote>  Avis de non-responsabilit√©: le CST a ses limites, mais pour nos besoins, il a parfaitement fonctionn√©.  De plus, la solution doit √™tre prise en charge par le fournisseur de cloud, mais comme la partie serveur est √©galement open-source, demandez-la au gestionnaire le plus proche :) </blockquote><br><h2>  Installation du client CSE </h2><br><ol><li>  Pour commencer, vous aurez besoin d'un compte administrateur dans l'organisation vCloud et d'un r√©seau rout√© cr√©√© √† l'avance pour le cluster.  <i><b>Important:</b> pendant le processus de d√©ploiement, vous avez besoin d'un acc√®s Internet √† partir de ce r√©seau, n'oubliez pas de configurer le pare-feu / NAT.</i> <br><br>  L'adressage n'a pas d'importance.  Dans cet exemple, prenez 10.0.240.0/24: <br><br><img src="https://habrastorage.org/webt/lz/3i/0e/lz3i0euak9ln8kfp1fmbwuobcdi.png"><br><blockquote>  √âtant donn√© qu'apr√®s la cr√©ation du cluster, il sera n√©cessaire de g√©rer d'une mani√®re ou d'une autre, la pr√©sence d'un VPN avec routage vers le r√©seau cr√©√© est recommand√©e.  Nous utilisons un VPN SSL standard configur√© sur la passerelle Edge de notre organisation. </blockquote></li><li>  Ensuite, vous devez installer le client CSE l√† o√π les clusters k8s seront g√©r√©s.  Dans mon cas, il s'agit d'un ordinateur portable fonctionnel et de quelques conteneurs <s>bien cach√©s</s> qui dirigent l'automatisation. <br><br>  Le client n√©cessite Python install√© 3.7.3 et sup√©rieur et un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">module vcd-cli</a> install√©, nous allons donc installer les deux. <br><br><pre><code class="bash hljs">pip3 install vcd-cli pip3 install container-service-extension</code> </pre> </li><li>  Apr√®s l'installation, v√©rifiez la version de CSE et obtenez les √©l√©ments suivants: <br><br><pre> <code class="plaintext hljs"># vcd cse version Error: No such command "cse".</code> </pre><br>  De fa√ßon inattendue, mais r√©parable. </li><li>  Il s'est av√©r√© que le CSE devait √™tre viss√© en tant que module √† vcd-cli. <br>  Pour ce faire, vous devez d'abord vous connecter vcd-cli √† notre organisation: <br><br><pre> <code class="plaintext hljs"># vcd login MyCloud.provider.com org-dev admin Password: admin logged in, org: 'org-dev', vdc: 'org-dev_vDC01'</code> </pre></li><li>  Apr√®s cela, vcd-cli cr√©era le fichier de configuration <i>~ / .vcd-cli / profiles.yaml</i> <br>  √Ä sa fin, vous devez ajouter ce qui suit: <br><br><pre> <code class="plaintext hljs">extensions: - container_service_extension.client.cse</code> </pre></li><li>  Ensuite, nous v√©rifions √† nouveau: <br><br><pre> <code class="plaintext hljs"># vcd cse version CSE, Container Service Extension for VMware vCloud Director, version 2.5.0</code> </pre></li></ol><br>  La phase d'installation du client est termin√©e.  Essayons de d√©ployer le premier cluster. <br><br><h2>  D√©ploiement de cluster </h2><br>  CSE a plusieurs ensembles de param√®tres d'utilisation, qui peuvent tous √™tre consult√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici.</a> <br><br><ol><li>  Cr√©ez d'abord les cl√©s pour un acc√®s sans mot de passe au futur cluster.  Ce point est important, car par d√©faut, la saisie du mot de passe sur les n≈ìuds sera d√©sactiv√©e.  Et, si vous ne sp√©cifiez pas les cl√©s, vous pouvez obtenir beaucoup de travail via la console de la machine virtuelle, ce qui ne veut pas dire pratique. <br><br><pre> <code class="plaintext hljs"># ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub.</code> </pre></li><li>  Nous essayons de commencer √† cr√©er un cluster: <br><br><pre> <code class="bash hljs">vcd cse cluster create MyCluster --network k8s_cluster_net --ssh-key ~/.ssh/id_rsa.pub --nodes 3 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-nfs</code> </pre> </li><li>  Si nous obtenons une <i>erreur: la session a expir√© ou l'utilisateur n'est pas connect√©.</i>  <i>Veuillez vous reconnecter.</i>  - reconnectez vcd-cli √† vCloud, comme d√©crit ci-dessus, et r√©essayez. <br><br>  Cette fois, tout va bien et la t√¢che de cr√©ation d'un cluster a commenc√©. <br><br><pre> <code class="bash hljs">cluster operation: Creating cluster vApp <span class="hljs-string"><span class="hljs-string">'MyCluster'</span></span> (38959587-54f4-4a49-8f2e-61c3a3e879e0) from template <span class="hljs-string"><span class="hljs-string">'photon-v2_k8-1.12_weave-2.3.0'</span></span> (revision 1)</code> </pre></li><li>  Il faudra environ 20 minutes pour terminer la t√¢che.  En attendant, nous analyserons les principaux param√®tres de lancement. <br><blockquote><ul><li>  --network - le r√©seau que nous avons cr√©√© plus t√¥t. </li><li>  --ssh-key - cl√©s cr√©√©es par nous, qui seront √©crites sur les n≈ìuds du cluster. </li><li>  --nodes n - Le nombre de n≈ìuds Worker du cluster.  Il y aura toujours un assistant, il s'agit d'une limitation CSE. </li><li>  --enable-nfs - cr√©e un n≈ìud suppl√©mentaire pour les boules NFS sous des volumes persistants.  Un peu une option de p√©dale, nous reviendrons un peu plus tard pour ajuster ce qu'il fait. </li></ul><br></blockquote></li><li>  Pendant ce temps, dans vCloud, vous pouvez observer visuellement la cr√©ation d'un cluster. <br><br><img src="https://habrastorage.org/webt/x0/ni/qk/x0niqksfidmbunupuflf61u7qww.png"></li><li>  Une fois la t√¢che de cr√©ation d'un cluster termin√©e, il est pr√™t √† d√©marrer. </li><li>  V√©rifiez le d√©ploiement avec la <i>commande MyCluster vcd cse cluster info</i> . <br><br><img src="https://habrastorage.org/webt/fx/4m/tj/fx4mtj6uzscxl3pns7shd8lni6a.png"></li><li>  Ensuite, nous devons obtenir la configuration du cluster pour utiliser <i>kubectl</i> . <br><br><pre> <code class="plaintext hljs"># vcd cse cluster config MyCluster &gt; ./.kube/config</code> </pre></li><li>  Et vous pouvez v√©rifier l'√©tat du cluster en l'utilisant: <br><br><img src="https://habrastorage.org/webt/vu/1k/58/vu1k58dcahsedsulmu_vixrstge.png"></li></ol><br><h2>  Pas si simple </h2><br>  √Ä ce stade, le cluster peut √™tre consid√©r√© comme conditionnellement op√©rationnel, sinon pour l'histoire avec des volumes persistants.  Puisque nous sommes dans vCloud, l'utilisation de vSphere Provider √©chouera.  L'option <i>--enable-nfs</i> est con√ßue pour att√©nuer ce probl√®me, mais elle n'a pas fonctionn√© jusqu'√† la fin.  Un r√©glage manuel est requis. <br><br><ol><li>  Pour commencer, notre n≈ìud doit cr√©er un disque ind√©pendant distinct dans vCloud.  Cela garantit que nos donn√©es ne disparaissent pas avec le cluster s'il est supprim√©.  Connectez √©galement le lecteur √† NFS. <br><br><pre> <code class="plaintext hljs"># vcd disk create nfs-shares-1 100g --description 'Kubernetes NFS shares' # vcd vapp attach mycluster nfsd-9604 nfs-shares-1</code> </pre></li><li>  Apr√®s cela, nous passons par ssh (avez-vous vraiment cr√©√© les cl√©s?) √Ä notre n≈ìud NFS et enfin connectez le disque: <br><br><pre> <code class="plaintext hljs">root@nfsd-9604:~# parted /dev/sdb (parted) mklabel gpt Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes (parted) unit GB (parted) mkpart primary 0 100 (parted) print Model: VMware Virtual disk (scsi) Disk /dev/sdb: 100GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 0.00GB 100GB 100GB primary (parted) quit root@nfsd-9604:~# mkfs -t ext4 /dev/sdb1 Creating filesystem with 24413696 4k blocks and 6111232 inodes Filesystem UUID: 8622c0f5-4044-4ebf-95a5-0372256b34f0 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done</code> </pre></li><li>  Cr√©ez un r√©pertoire pour les donn√©es et montez-y une nouvelle section: <br><br><pre> <code class="plaintext hljs">mkdir /export echo '/dev/sdb1 /export ext4 defaults 0 0' &gt;&gt; /etc/fstab mount -a</code> </pre></li><li>  Cr√©ons cinq sections de test et partageons-les pour le cluster: <br><br><pre> <code class="bash hljs">&gt;<span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span> &gt;mkdir vol1 vol2 vol3 vol4 vol5 &gt;vi /etc/exports <span class="hljs-comment"><span class="hljs-comment">#     /export/vol1 *(rw,sync,no_root_squash,no_subtree_check) /export/vol2 *(rw,sync,no_root_squash,no_subtree_check) /export/vol3 *(rw,sync,no_root_squash,no_subtree_check) /export/vol4 *(rw,sync,no_root_squash,no_subtree_check) /export/vol5 *(rw,sync,no_root_squash,no_subtree_check) #:wq! ;) # -   &gt;exportfs -r</span></span></code> </pre></li><li>  Apr√®s toute cette magie, vous pouvez cr√©er du PV et du PVC dans notre cluster comme ceci: <br><br>  <b>PV</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs-vol1 spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: <span class="hljs-comment"><span class="hljs-comment"># Same IP as the NFS host we ssh'ed to earlier. server: 10.150.200.22 path: "/export/vol1" EOF</span></span></code> </pre><br>  <b>PVC</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany storageClassName: <span class="hljs-string"><span class="hljs-string">""</span></span> resources: requests: storage: 10Gi EOF</code> </pre></li></ol><br>  Sur ce, l'histoire de la cr√©ation d'un cluster se termine et l'histoire de son cycle de vie commence.  En prime, il existe deux autres commandes CSE utiles qui vous permettent d'√©conomiser des ressources parfois <s>ou non</s> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    8   &gt;cse cluster resize MyCluster --network k8s_cluster_net --nodes 8 #         &gt;vcd cse node delete MyCluster node-1a2v node-6685 --yes</span></span></code> </pre><br>  Merci √† tous pour votre temps, si vous avez des questions - posez-les dans les commentaires. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr472752/">https://habr.com/ru/post/fr472752/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr472738/index.html">Comment mettre √† niveau un projet existant d'ASP.NET MVC vers ASP.NET Core. Guide pratique</a></li>
<li><a href="../fr472744/index.html">Le MRP ne fonctionne pas ... Quelle est l'alternative?</a></li>
<li><a href="../fr472746/index.html">Serveur de terminal pour administrateur; Pas un seul √©cart SSH</a></li>
<li><a href="../fr472748/index.html">Navigateur s√©mantique ou vie sans sites</a></li>
<li><a href="../fr472750/index.html">OK, ai-je vraiment besoin de Kubernetes?</a></li>
<li><a href="../fr472754/index.html">Comment parler anglais en un mois. 9 √©tapes simples et √©prouv√©es</a></li>
<li><a href="../fr472758/index.html">Proposition: essayer - fonction int√©gr√©e de v√©rification des erreurs</a></li>
<li><a href="../fr472760/index.html">R√©duisez le temps de calcul de quelques ann√©es √† quelques minutes. Comprendre l'apprentissage automatique quantique</a></li>
<li><a href="../fr472762/index.html">Analyse technique de l'exploit checkm8</a></li>
<li><a href="../fr472766/index.html">Param√©trage √† partir d'un fichier dans py.test</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>