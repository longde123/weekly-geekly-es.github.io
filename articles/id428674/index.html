<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤·ğŸ¿ ğŸ ğŸ› ï¸ Membangun pencarian perutean / semantik klien di Profi.ru ğŸ™†ğŸ¿ ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Membangun perutean klien / pencarian semantik dan pengelompokan corpus eksternal yang sewenang-wenang di Profi.ru 
 TLDR 


 Ini adalah ringkasan ekse...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Membangun pencarian perutean / semantik klien di Profi.ru</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428674/"><h1 id="building-client-routing--semantic-search-and-clustering-arbitrary-external-corpuses-at-profiru">  Membangun perutean klien / pencarian semantik dan pengelompokan corpus eksternal yang sewenang-wenang di Profi.ru </h1><br><h2 id="tldr">  <strong>TLDR</strong> </h2><br><p>  Ini adalah ringkasan eksekutif yang sangat singkat (atau penggoda) tentang apa yang kami berhasil lakukan dalam waktu kurang lebih 2 bulan di departemen Profi.ru DS (saya ada di sana untuk sedikit lebih lama, tetapi memberi diri saya sendiri dan tim saya adalah hal yang terpisah untuk menjadi dilakukan pada awalnya). </p><a name="habracut"></a><br><h2 id="projected-goals">  Tujuan yang diproyeksikan </h2><br><ol><li> Memahami input / maksud klien dan merutekan klien sesuai (kami memilih untuk pengelompokan agnostik kualitas input pada akhirnya, meskipun kami juga mempertimbangkan model tingkat-depan tipe char dan model bahasa juga. Aturan kesederhanaan); </li><li>  Temukan layanan dan sinonim yang sama sekali baru untuk layanan yang ada; </li><li>  Sebagai sub-tujuan dari (2) - belajar untuk membangun kelompok yang tepat pada corpus eksternal yang sewenang-wenang; </li></ol><br><h2 id="achieved-goals">  Mencapai tujuan </h2><br><p>  Jelas beberapa hasil ini dicapai tidak hanya oleh tim kami, tetapi juga oleh beberapa tim (mis. Kami jelas tidak melakukan bagian pengikisan untuk corpus domain dan anotasi manual, meskipun saya yakin pengikisan juga dapat diselesaikan oleh tim kami - Anda hanya perlu cukup proxy + mungkin beberapa pengalaman dengan selenium). </p><br><p>  <strong>Tujuan bisnis:</strong> </p><br><ol><li> ~ <code>88+%</code> (vs ~ <code>60%</code> dengan pencarian elastis) akurasi pada routing klien / klasifikasi maksud (~ kelas <code>5k</code> ); </li><li>  Pencarian bersifat agnostik terhadap kualitas input (salah cetak / input parsial); </li><li>  Classifier menggeneralisasi, struktur morfologis bahasa dieksploitasi; </li><li>  Penggolong sangat efektif dalam berbagai tolok ukur (lihat di bawah); </li><li>  Untuk berada di sisi yang aman - setidaknya <code>1,000</code> layanan baru ditemukan + setidaknya <code>15,000</code> sinonim (vs keadaan saat ini <code>5,000</code> + ~ <code>30,000</code> ).  Saya berharap angka ini menjadi dua kali lipat bahkan tiga kali lipat; </li></ol><br><p>  Peluru terakhir adalah perkiraan rata-rata, tetapi yang konservatif. <br>  Juga tes AB akan mengikuti.  Tapi saya yakin dengan hasil ini. </p><br><p>  <strong>Tujuan "Ilmiah":</strong> </p><br><ol><li>  Kami benar-benar membandingkan banyak teknik penanaman kalimat modern menggunakan tugas klasifikasi hilir + KNN dengan database sinonim layanan; </li><li>  Kami berhasil mengalahkan pencarian yang lemah yang diawasi dengan lemah (pada dasarnya penggolong mereka adalah bag-of-ngrams) pada tolok ukur ini (lihat detail di bawah) menggunakan metode <strong>UNSUPERVISED</strong> ; </li><li>  Kami mengembangkan cara baru untuk membangun model NLP terapan (tas vanilla bi-LSTM + embeddings, pada dasarnya teks cepat bertemu RNN) - ini menjadikan morfologi bahasa Rusia menjadi pertimbangan dan digeneralisasikan dengan baik; </li><li>  Kami menunjukkan bahwa teknik penyematan akhir kami (lapisan leher botol dari pengklasifikasi terbaik) dikombinasikan dengan algoritma tanpa pengawasan yang canggih (UMAP + HDBSCAN) dapat menghasilkan gugus bintang; </li><li>  Kami menunjukkan dalam praktiknya kemungkinan, kelayakan dan kegunaan dari: <br><ul><li>  Distilasi pengetahuan; </li><li>  Augmentasi untuk data teks (sic!); </li></ul></li><li>  Pelatihan pengklasifikasi berbasis teks dengan augmentasi dinamis mengurangi waktu konvergensi secara drastis (10x) dibandingkan dengan menghasilkan dataset statis yang lebih besar (mis. CNN belajar untuk menggeneralisasikan kesalahan yang ditunjukkan dengan kalimat yang ditambah secara drastis); </li></ol><br><h2 id="overall-project-structure">  Struktur proyek secara keseluruhan </h2><br><p>  Ini tidak termasuk penggolong akhir. <br>  Juga pada akhirnya kami meninggalkan model RNN palsu dan triplet loss demi bottleneck classifier. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c2/449/157/1c24491576ed703ebc571dfd4d7d8da3.png"></p><br><h2 id="what-works-in-nlp-now">  Apa yang berfungsi di NLP sekarang? </h2><br><p>  Pandangan mata burung: <br><img src="https://habrastorage.org/getpro/habr/post_images/5a1/8f5/df1/5a18f5df1e133bef082edf9315011da7.png"></p><br><p>  Anda juga mungkin tahu bahwa NLP mungkin mengalami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">momen Imagenet sekarang</a> . </p><br><h2 id="large-scale-umap-hack">  Retasan UMAP skala besar </h2><br><p>  Saat membangun cluster, kami menemukan cara / hack untuk menerapkan UMAP ke level 100m + point (atau bahkan 1 miliar) pada dasarnya.  Pada dasarnya membangun grafik KNN dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">FAISS</a> dan kemudian hanya menulis ulang loop UMAP utama ke PyTorch menggunakan GPU Anda.  Kami tidak membutuhkan itu dan meninggalkan konsep (kami hanya memiliki 10-15m poin setelah semua), tetapi silakan ikuti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">utas</a> ini untuk detail. </p><br><h2 id="what-works-best">  Apa yang terbaik </h2><br><ul><li>  Untuk klasifikasi yang diawasi, teks-cepat memenuhi RNN (bi-LSTM) + set n-gram yang dipilih dengan cermat; </li><li>  Implementasi - python polos untuk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">n-gram</a> + PyTorch Embedding layer bag; </li><li>  Untuk pengelompokan - lapisan bottleneck dari model ini + UMAP + HDBSCAN; </li></ul><br><h2 id="best-classifier-benchmarks">  <strong>Tolok ukur classifier terbaik</strong> </h2><br><p>  <strong>Set dev beranotasi secara manual</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/04b/7cc/e7e/04b7cce7e8cee9cee4b066b6a353bed9.jpg"></p><br><p>  <strong>Kiri ke kanan:</strong> <br>  (Akurasi Top1) </p><br><ul><li>  Algoritma saat ini (pencarian elastis); </li><li>  RNN pertama; </li><li>  Anotasi baru; </li><li>  Tuning </li><li>  Lapisan tas penyisipan teks cepat; </li><li>  Menambahkan kesalahan ketik dan input parsial; </li><li>  Generasi kesalahan dinamis dan input parsial ( <strong>waktu pelatihan berkurang 10x</strong> ); </li><li>  Skor akhir; </li></ul><br><p>  <strong>Dev beranotasi secara manual menetapkan + 1-3 kesalahan per kueri</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ae2/a31/040/ae2a31040dbd77402d6b6dfee9eeba28.jpg"></p><br><p>  <strong>Kiri ke kanan:</strong> <br>  (Akurasi Top1) </p><br><ul><li>  Algoritma saat ini (pencarian elastis); </li><li>  Lapisan tas penyisipan teks cepat; </li><li>  Menambahkan kesalahan ketik dan input parsial; </li><li>  Generasi kesalahan dinamis dan input parsial; </li><li>  Skor akhir; </li></ul><br><p>  <strong>Set manual beranotasi + input parsial</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/c3c/680/681/c3c680681dd3166b95246930f1f1b1a8.jpg"></p><br><p>  <strong>Kiri ke kanan:</strong> <br>  (Akurasi Top1) </p><br><ul><li>  Algoritma saat ini (pencarian elastis); </li><li>  Lapisan tas penyisipan teks cepat; </li><li>  Menambahkan kesalahan ketik dan input parsial; </li><li>  Generasi kesalahan dinamis dan input parsial; </li><li>  Skor akhir; </li></ul><br><h2 id="large-scale-corpuses--n-gram-selection">  Koreksi skala besar / seleksi n-gram </h2><br><ul><li>  Kami mengumpulkan korpus terbesar untuk bahasa Rusia: <br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Areneum</a> - versi yang diproses tersedia di sini - pembuat dataset tidak membalas; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Taiga</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perayapan</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">wiki umum</a> - silakan ikuti artikel ini; </li></ul></li><li>  Kami mengumpulkan kamus kata <code>100m</code> menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perayapan 1TB</a> ; </li><li>  Gunakan juga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">peretasan</a> ini untuk mengunduh file seperti itu lebih cepat (semalam); </li><li>  Kami memilih rangkaian optimal <code>1m</code> n-gram untuk classifier kami untuk menggeneralisasi terbaik ( <code>500k</code> n-gram paling populer dari teks cepat yang dilatih di Wikipedia Rusia + <code>500k</code> n-gram paling populer pada data domain kami); </li></ul><br><p>  <strong>Tes stres 1M n-gram kami pada kosakata 100M:</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/198/1fe/38b/1981fe38b03b4cdf76022f4ff6ef0074.png" alt="gambar"></p><br><h2 id="text-augmentations">  Augmentasi teks </h2><br><p>  Singkatnya: </p><br><ul><li>  Ambil kamus besar dengan kesalahan (mis. 10-100 m kata unik); </li><li>  Menghasilkan kesalahan (menjatuhkan surat, menukar surat menggunakan probabilitas yang dihitung, memasukkan huruf acak, mungkin menggunakan tata letak keyboard, dll); </li><li>  Periksa apakah kata baru ada dalam kamus; </li></ul><br><p>  Kami brute memaksa banyak permintaan ke layanan seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> (dalam upaya untuk merekayasa balik dataset mereka), dan mereka memiliki kamus yang sangat kecil di dalamnya (juga layanan ini didukung oleh classifier pohon dengan fitur n-gram).  Agak lucu melihat bahwa <strong>mereka hanya mencakup 30-50% dari kata-kata yang kami miliki di beberapa corpus</strong> . </p><br><p>  <strong>Pendekatan kami jauh lebih unggul, jika Anda memiliki akses ke kosa kata domain yang besar</strong> . </p><br><h2 id="best-unsupervised--semi-supervised-results">  Hasil terbaik tanpa pengawasan / semi-diawasi </h2><br><p>  KNN digunakan sebagai patokan untuk membandingkan berbagai metode penanaman. </p><br><p>  (ukuran vektor) Daftar model yang diuji: </p><br><ul><li>  (512) Pendeteksi kalimat palsu berskala besar dilatih pada 200 GB data perayapan umum; </li><li>  (300) Pendeteksi kalimat palsu dilatih untuk memberi tahu kalimat acak dari Wikipedia dari suatu layanan; </li><li>  (300) Teks cepat diperoleh dari sini, dilatih sebelumnya tentang araneum corpus; </li><li>  (200) Teks cepat dilatih tentang data domain kami; </li><li>  (300) Teks cepat dilatih pada 200GB data Perayapan Umum; </li><li>  (300) Jaringan Siam dengan kehilangan triplet yang dilatih dengan layanan / sinonim / kalimat acak dari Wikipedia; </li><li>  (200) Iterasi pertama dari embedding tas RNN's embedding layer, kalimat dikodekan sebagai seluruh kantong embeddings; </li><li>  (200) Sama, tetapi pertama kalimat itu dibagi menjadi kata-kata, lalu setiap kata tertanam, kemudian rata-rata diambil; </li><li>  (300) Sama seperti di atas tetapi untuk model akhir; </li><li>  (300) Sama seperti di atas tetapi untuk model akhir; </li><li>  (250) Lapisan bottleneck dari model akhir (250 neuron); </li><li>  Baseline pencarian elastis yang diawasi dengan lemah; </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca1/e0b/e9c/ca1e0be9c152d092d4149f9986b87289.png" alt="standar"></p><br><p>  Untuk menghindari kebocoran, semua kalimat acak diambil secara acak.  Panjang kata-kata mereka sama dengan panjang layanan / sinonim yang dibandingkan dengan mereka.  Juga diambil langkah-langkah untuk memastikan bahwa model tidak hanya belajar dengan memisahkan kosa kata (embeddings dibekukan, Wikipedia undersampled untuk memastikan bahwa setidaknya ada satu kata domain dalam setiap kalimat Wikipedia). </p><br><h2 id="cluster-visualization">  Visualisasi cluster </h2><br><p>  <strong>3D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/4b7/f10/d19/4b7f10d19a785b5f690a28f2e2a039e6.gif"></p><br><p>  <strong>2D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ad7/0ad/441/ad70ad441ecae6f396c8bb76826484df.png"></p><br><h2 id="cluster-exploration-interface">  "Antarmuka" eksplorasi klaster </h2><br><p>  Hijau - kata / sinonim baru. <br>  Latar belakang abu-abu - kemungkinan kata baru. <br>  Teks abu-abu - sinonim yang ada. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cda/d17/00f/cdad1700ff0701ff6643a4aa14041d31.jpg"></p><br><h2 id="ablation-tests-and-what-works-what-we-tried-and-what-we-did-not">  Tes ablasi dan apa yang berhasil, apa yang kami coba dan apa yang tidak </h2><br><ol><li>  Lihat grafik di atas; </li><li>  Rata-rata polos / tf-idf rata-rata pernikahan teks cepat - <strong>garis dasar SANGAT tangguh</strong> ; </li><li>  Teks-cepat&gt; Word2Vec untuk Bahasa Rusia; </li><li>  Penyisipan kalimat dengan jenis deteksi kalimat palsu, tetapi artinya jika dibandingkan dengan metode lain; </li><li>  BPE (sentencepiece) tidak menunjukkan peningkatan pada domain kami; </li><li>  Model tingkat Char berjuang untuk menggeneralisasi, meskipun ada kertas dari google; </li><li>  Kami mencoba multi-head transformer (dengan classifier dan kepala pemodelan bahasa), tetapi pada penjelasan yang tersedia, kinerjanya kira-kira sama dengan model berbasis vanilla LSTM biasa.  Ketika kami bermigrasi untuk menyematkan pendekatan yang buruk, kami meninggalkan jalur penelitian ini karena kepraktisan transformer yang lebih rendah dan ketidakpraktisan memiliki kepala LM bersama dengan lapisan kantong penyematan; </li><li>  <strong>BERT</strong> - tampaknya berlebihan, juga beberapa orang mengklaim bahwa transformer berlatih secara harfiah selama berminggu-minggu; </li><li>  <strong>ELMO</strong> - menggunakan perpustakaan seperti AllenNLP tampaknya tidak produktif menurut pendapat saya baik di lingkungan penelitian / produksi dan pendidikan untuk alasan yang saya tidak akan berikan di sini; </li></ol><br><h2 id="deploy">  Sebarkan </h2><br><p>  Selesai menggunakan: </p><br><ul><li>  Wadah Docker dengan layanan web sederhana; </li><li>  Hanya CPU untuk inferensi sudah cukup; </li><li>  ~ <code>2.5 ms</code> per query pada CPU, tidak perlu batching; </li><li>  ~ Jejak memori RAM <code>1GB</code> ; </li><li>  Hampir tidak ada dependensi, selain dari <code>PyTorch</code> , <code>numpy</code> dan <code>pandas</code> (dan server web ofc). </li><li>  Meniru generasi n-gram teks cepat seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> ; </li><li>  Menanamkan lapisan tas + indeks karena baru saja disimpan dalam kamus; </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id428674/">https://habr.com/ru/post/id428674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id428662/index.html">Tugas pemrograman ritel</a></li>
<li><a href="../id428664/index.html">Boot kernel Linux. Bagian 1</a></li>
<li><a href="../id428666/index.html">Bagaimana saya membuat animasi yang mengubah suasana hati menggunakan topeng CSS</a></li>
<li><a href="../id428668/index.html">Blizzard mengumumkan rilis rilis ulang WarCraft III pada tahun 2019. Buka pre-order</a></li>
<li><a href="../id428672/index.html">Gambaran Umum QuietOn Active Squelch</a></li>
<li><a href="../id428676/index.html">Mematahkan fondasi dasar C #: mengalokasikan memori untuk tipe referensi pada stack</a></li>
<li><a href="../id428680/index.html">Membuat dan mengintegrasikan VK bot ke grup melalui VkBotLongPoll [Python]</a></li>
<li><a href="../id428682/index.html">Penghancuran Diri Beta Fallout 76</a></li>
<li><a href="../id428688/index.html">Menyiapkan lingkungan kerja di Docker untuk aplikasi kerangka-yii</a></li>
<li><a href="../id428690/index.html">Bagaimana cara mengajar pacar Anda bagaimana memprogram jika Anda bukan seorang guru, tetapi dia percaya pada Anda</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>