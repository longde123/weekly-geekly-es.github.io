<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëãüèº üßúüèæ üë©üèΩ‚ÄçüöÄ Migrar de Nagios a Icinga2 en Australia üê∑ ‚ÜïÔ∏è ü§ûüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos 


 Soy el administrador de sistemas de Linux, me mud√© de Rusia a Australia con una visa profesional independiente en 2015, pero el art√≠c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Migrar de Nagios a Icinga2 en Australia</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444060/"><p>  Hola a todos </p><br><p>  Soy el administrador de sistemas de Linux, me mud√© de Rusia a Australia con una visa profesional independiente en 2015, pero el art√≠culo no tratar√° sobre c√≥mo conseguir un lech√≥n y un tractor.  Dichos art√≠culos ya son suficientes (sin embargo, si hay inter√©s, tambi√©n escribir√© sobre ello), por lo que me gustar√≠a hablar sobre c√≥mo, en mi trabajo en Australia como ingeniero de linux-ops, fui el iniciador de la migraci√≥n desde un sistema seguimiento a otro.  Espec√≠ficamente - Nagios =&gt; Icinga2. </p><br><p>  El art√≠culo es en parte t√©cnico y en parte sobre comunicaci√≥n con personas y problemas asociados con la diferencia en cultura y m√©todos de trabajo. </p><a name="habracut"></a><br><p>  Desafortunadamente, la etiqueta "c√≥digo" no resalta el c√≥digo de Puppet y yaml, as√≠ que tuve que usar "texto sin formato". </p><br><p>  Nada anunciaba mal la ma√±ana del 21 de diciembre de 2016.  Como de costumbre, le√≠ a Habr con un an√≥nimo no registrado en la primera media hora de la jornada laboral, absorbiendo caf√© y me encontr√© con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> . </p><br><p>  Como Nagios fue utilizado en mi empresa, sin pensarlo dos veces, cre√© un ticket en Redmine y lanc√© el enlace al chat general, porque pens√© que era importante.  La iniciativa es punible incluso en Australia, por lo que el ingeniero principal me colg√≥ este problema desde que lo descubr√≠. </p><br><div class="spoiler">  <b class="spoiler_title">Pantalla de Redmine</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/ge/uc/gu/geucgutwhu3_zu4uwes2ozuwa3e.jpeg"></div></div><br><p>  En nuestro departamento, antes de exponer nuestra opini√≥n, es costumbre ofrecer al menos una alternativa, incluso si la elecci√≥n es obvia, as√≠ que comenc√© buscando en Google qu√© sistemas de monitoreo en general son relevantes actualmente, ya que en Rusia en el √∫ltimo lugar donde trabaj√©, ten√≠a mi propio sistema de grabaci√≥n personal, muy primitivo, pero sin embargo bastante funcional y cumpliendo todas las tareas que se le asignaron.  Python, el Polit√©cnico de San Petersburgo y la regla del metro.  No, el metro apesta.  Esto es personal (11 a√±os de trabajo) y merece un art√≠culo separado, pero no ahora. </p><br><p>  Un poco sobre las reglas para realizar cambios en la configuraci√≥n de la infraestructura en mi ubicaci√≥n actual.  Utilizamos Puppet, Gitlab y el principio de Infraestructura como C√≥digo, para que: </p><br><ul><li>  No hay cambios manuales a trav√©s de SSH modificando manualmente ning√∫n archivo en m√°quinas virtuales.  Durante tres a√±os de trabajo, recib√≠ un sombrero por esto muchas veces, el √∫ltimo hace una semana y no creo que fuera la √∫ltima vez.  Bueno, de hecho, arregle una l√≠nea en la configuraci√≥n, reinicie el servicio y vea si el problema se ha resuelto: 10 segundos.  Cree una nueva sucursal en Gitlab, empuje los cambios, espere a que r10k funcione en Puppetmaster, ejecute Puppet --environment = mybranch y espere un par de minutos m√°s hasta que todo esto funcione: 5 minutos como m√≠nimo. </li><li>  Cualquier cambio se realiza creando una Solicitud de fusi√≥n en Gitlab y debe obtener la aprobaci√≥n de al menos un miembro del equipo.  Los cambios importantes en el liderazgo del equipo requieren dos o tres aprobaciones. </li><li>  Todos los cambios son textuales de una forma u otra (dado que los manifiestos de Puppet, los scripts y los datos de Hiera son texto), los archivos binarios est√°n altamente desaconsejados y se necesitan buenas razones para aprobar dichos archivos. </li></ul><br><p>  Entonces, las opciones que mir√© son: </p><br><ul><li>  Munin: si hay m√°s de 10 servidores en la infraestructura, la administraci√≥n se convierte en un infierno (a partir de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> . No ten√≠a muchas ganas de comprobar esto, as√≠ que cre√≠ en mi palabra). </li><li>  Zabbix: ha estado observando durante mucho tiempo, de regreso en Rusia, pero luego fue redundante para mis tareas.  Aqu√≠, tuvo que descartarse debido al uso de Puppet como administrador de configuraci√≥n y Gitlab como sistema de control de versiones.  En ese momento, seg√∫n tengo entendido, Zabbix almacena la configuraci√≥n completa en una base de datos y, por lo tanto, no estaba claro c√≥mo administrar la configuraci√≥n en las condiciones actuales y c√≥mo rastrear los cambios. </li><li>  Prometheus es a lo que llegaremos al final, a juzgar por el estado de √°nimo en el departamento, pero en ese momento no pude dominarlo y no pude demostrar una muestra realmente funcional (Prueba de concepto), as√≠ que tuve que rechazarlo. </li><li>  Hab√≠a varias otras opciones que requer√≠an un redise√±o completo del sistema, o estaban en su infancia / abandonadas y por la misma raz√≥n fueron rechazadas. </li></ul><br><p>  Al final, me decid√≠ por Icinga2 por tres razones: </p><br><p>  1 - compatibilidad con Nrpe (un servicio de cliente que ejecuta verificaciones de comandos desde Nagios).  Esto era muy importante, porque en ese momento ten√≠amos 135 m√°quinas virtuales (ahora hay 165 de ellas en 2019) con un mont√≥n de servicios / cheques escritos por s√≠ mismos y rehacer todo esto ser√≠a una hemorroide terrible. <br>  2: todos los archivos de configuraci√≥n son texto, lo que facilita editar este asunto, crear solicitudes de fusi√≥n con la capacidad de ver lo que se ha agregado o eliminado. <br>  3 es un proyecto OpenSource animado y en crecimiento.  Somos muy aficionados a OpenSource y hacemos una contribuci√≥n viable al crear solicitudes de extracci√≥n y problemas para resolver problemas. </p><br><p>  Entonces vamos, Icinga2. </p><br><p>  Lo primero que tuve que enfrentar fue la inercia de mis colegas.  Todos est√°n acostumbrados a Nagios / Najios (aunque incluso aqu√≠ no pudieron comprometerse sobre c√≥mo pronunciar esto) y la interfaz CheckMK.  La interfaz de icinga se ve completamente diferente (era un punto negativo), pero es posible configurar de manera flexible lo que necesita ver con filtros literalmente por cualquier par√°metro (fue una ventaja, pero luch√© por ello notablemente). </p><br><div class="spoiler">  <b class="spoiler_title">Filtros</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/kx/bf/0e/kxbf0eqj-gh7bfkpgtzgjjgvhwm.jpeg"></div></div><br><p>  Estime la proporci√≥n del tama√±o de la barra de desplazamiento con respecto al tama√±o del campo de desplazamiento. </p><br><p>  El segundo: todos est√°n acostumbrados a ver toda la infraestructura en un monitor, porque CheckMk le permite trabajar con varios hosts Nagios, pero Icinga no sab√≠a c√≥mo hacerlo (de hecho, lo hizo, pero m√°s sobre eso a continuaci√≥n).  Una alternativa era una cosa llamada Thruk, pero su dise√±o caus√≥ v√≥mitos para todos los miembros del equipo, excepto uno, el que lo propuso (no yo). </p><br><div class="spoiler">  <b class="spoiler_title">Thruk Firebox - Decisi√≥n un√°nime del equipo</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/tv/1c/_l/tv1c_lkx-h9rbwgpdy63jdohnve.png"></div></div><br><p>  Despu√©s de un par de d√≠as de tormenta de ideas, propuse la idea del monitoreo de cl√∫ster, cuando hay un host maestro en la zona de producci√≥n y dos subordinados, uno en desarrollo / prueba y un host externo ubicado en otro proveedor para monitorear nuestros servicios desde el punto de vista de un cliente o un extra√±o. observador  Esta configuraci√≥n me permiti√≥ ver todos los problemas en una interfaz basada en la web y funcion√≥ para m√≠, pero Puppet ... El problema con Puppet era que el host maestro ahora ten√≠a que saber sobre todos los hosts y servicios / controles en el sistema y ten√≠a que distribuirlos entre zonas. (dev-test, staging-prod, ext), pero enviar los cambios a trav√©s de la API de Icinga lleva un par de segundos, pero compilar el directorio Puppet de todos los servicios para todos los hosts lleva un par de minutos.  Todav√≠a me culpan, aunque ya he explicado varias veces c√≥mo funciona todo y por qu√© todo lleva tanto tiempo. </p><br><p>  Tercero, un mont√≥n de copos de nieve (copos de nieve), cosas que quedan fuera del sistema general, porque tienen algo especial, por lo que las reglas generales no se aplican a ellos.  Fue decidido por un ataque frontal: si hay una alarma, pero de hecho todo est√° en orden, entonces aqu√≠ debe profundizar y comprender por qu√© me alerta, aunque no deber√≠a.  O viceversa: por qu√© Nagios est√° en p√°nico, pero Icinga no. </p><br><p>  Cuarto: Nagios trabaj√≥ aqu√≠ para m√≠ durante tres a√±os e inicialmente hab√≠a m√°s confianza en √©l que en mi novedoso sistema inconformista, por lo que cada vez que Icinga generaba p√°nico, nadie hac√≠a nada hasta que Nagios se entusiasmaba con el mismo tema.  Pero muy raramente, Icinga emiti√≥ alarmas reales antes que Nagios, y considero que esto es una jamba seria, que analizar√© en la secci√≥n "Conclusiones". </p><br><p>  Como resultado, la puesta en marcha se retras√≥ m√°s de 5 meses (planeado el 28 de junio de 2018, de hecho, el 3 de diciembre de 2018), principalmente debido a la "verificaci√≥n de paridad", esa basura cuando hay varios servicios en Nagios de los que nadie est√° hablando No he escuchado nada en los √∫ltimos a√±os, pero AHORA emitieron cr√≠ticas sin raz√≥n y tuve que explicar por qu√© no estaban en mi panel y tuve que agregarlos a Icinga para que "la verificaci√≥n de paridad est√© completa" (Todos los servicios / verificaciones en Nagios corresponden a servicios / controles en Icinga) </p><br><p>  Implementaci√≥n <br>  El primero es la guerra de C√≥digo vs Datos, como el estilo Puppet.  Todos los datos, aqu√≠ todo en general, deben estar en Hiera y nada m√°s.  Todo el c√≥digo est√° en archivos .pp.  Variables, abstracciones, funciones: todo va en pp. <br>  Como resultado, tenemos un mont√≥n de m√°quinas virtuales (165 en el momento de la escritura) y 68 aplicaciones web que necesitan ser monitoreadas para verificar el estado y la validez de los certificados SSL.  Pero debido a las hemorroides hist√≥ricas, la informaci√≥n para monitorear aplicaciones se toma de un repositorio de gitlab separado y el formato de datos no ha cambiado desde Puppet 3, lo que crea dificultades de configuraci√≥n adicionales. </p><br><div class="spoiler">  <b class="spoiler_title">C√≥digo de marionetas para aplicaciones, tenga cuidado</b> <div class="spoiler_text"><pre><code class="plaintext hljs">define profiles::services::monitoring::docker_apps( Hash $app_list, Hash $apps_accessible_from, Hash $apps_access_list, Hash $webhost_defaults, Hash $webcheck_defaults, Hash $service_overrides, Hash $targets, Hash $app_checks, ) { #### APPS #### $zone = $name $app_list.each | String $app_name, Hash $app_data | { $notify_group = { 'notify_group' =&gt; ($webcheck_defaults[$zone]['notify_group'] + pick($app_data['notify_group'], {} )) } # adds notifications for default group (systems) + any group defined in int/pm_docker_apps.eyaml $data = merge($webhost_defaults, $apps_accessible_from, $app_data) $site_domain = $app_data['site_domain'] $regexp = pick($app_data['check_regex'], 'html') # Pick a regex to check $check_url = $app_data['check_url'] ? { undef =&gt; { 'http_uri' =&gt; '/' }, default =&gt; { 'http_uri' =&gt; $app_data['check_url'] } } $check_regex = $regexp ?{ 'absent' =&gt; {}, default =&gt; {'http_expect_body_regex' =&gt; $regexp} } $site_domain.each | String $vhost, Hash $vdata | { # Split an app by domains if there are two or more $vhost_name = {'http_vhost' =&gt; $vhost} $vars = $data['vars'] + $vhost_name + $check_regex + $check_url $web_ipaddress = is_array($vdata['web_ipaddress']) ? { # Make IP-address an array if it's not, because askizzy has 2 ips and it's an array true =&gt; $vdata['web_ipaddress'], false =&gt; [$vdata['web_ipaddress']], } $access_from_zones = [$zone] + $apps_access_list[$data['accessible_from']] # Merge default zone (where the app is defined) and extra zones if they exist $web_ipaddress.each | String $ip_address | { # For each IP (if we have multiple) $suffix = length($web_ipaddress) ? { # If we have more than one - add IP as a suffix to this hostname to avoid duplicating resources 1 =&gt; '', default =&gt; "_${ip_address}" } $octets = split($ip_address, '\.') $ip_tag = "${octets[2]}.${octets[3]}" # Using last octet only causes a collision between nginx-vip 203.15.70.94 and ext. ip 49.255.194.94 $access_from_zones.each | $zone_prefix |{ $zone_target = $targets[$zone_prefix] $nginx_vip_name = "${zone_prefix}_nginx-vip-${ip_tag}" # If it's a host for ext - prefix becomes 'ext_' (ext_nginx-vip...) $nginx_host_vip = { $nginx_vip_name =&gt; { ensure =&gt; present, target =&gt; $zone_target, address =&gt; $ip_address, check_command =&gt; 'hostalive', groups =&gt; ['nginx_vip',], } } $ssl_vars = $app_checks['ssl'] $regex_vars = $app_checks['http'] + $vars + $webcheck_defaults[$zone] + $notify_group if !defined( Profiles::Services::Monitoring::Host[$nginx_vip_name] ) { ensure_resources('profiles::services::monitoring::host', $nginx_host_vip) } if !defined( Icinga2::Object::Service["${nginx_vip_name}_ssl"] ) { icinga2::object::service {"${nginx_vip_name}_ssl": ensure =&gt; $data['ensure'], assign =&gt; ["host.name == $nginx_vip_name",], groups =&gt; ['webchecks',], check_command =&gt; 'ssl', check_interval =&gt; $service_overrides['ssl']['check_interval'], target =&gt; $targets['services'], apply =&gt; true, vars =&gt; $ssl_vars } } if $regexp != 'absent'{ if !defined(Icinga2::Object::Service["${vhost}${$suffix} regex"]){ icinga2::object::service {"${vhost}${$suffix} regex": ensure =&gt; $data['ensure'], assign =&gt; ["match(*_nginx-vip-${ip_tag}, host.name)",], groups =&gt; ['webchecks',], check_command =&gt; 'http', check_interval =&gt; $service_overrides['regex']['check_interval'], target =&gt; $targets['services'], enable_flapping =&gt; true, apply =&gt; true, vars =&gt; $regex_vars } } } } } } } }</code> </pre> </div></div><br><p>  El c√≥digo de configuraci√≥n de host y servicio tambi√©n se ve horrible: </p><br><div class="spoiler">  <b class="spoiler_title">monitoreo / config.pp</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">class profiles::services::monitoring::config( Array $default_config, Array $hostgroups, Hash $hosts = {}, Hash $host_defaults, Hash $services, Hash $service_defaults, Hash $service_overrides, Hash $webcheck_defaults, Hash $servicegroups, String $servicegroup_target, Hash $user_defaults, Hash $users, Hash $oncall, Hash $usergroup_defaults, Hash $usergroups, Hash $notifications, Hash $notification_defaults, Hash $notification_commands, Hash $timeperiods, Hash $webhost_defaults, Hash $apps_access_list, Hash $check_commands, Hash $hosts_api = {}, Hash $targets = {}, Hash $host_api_defaults = {}, ) { # Profiles::Services::Monitoring::Hostgroup &lt;&lt;| |&gt;&gt; # will be enabled when we move to icinga completely #### APPS #### case $location { 'int', 'ext': { $apps_by_zone = {} } 'pm': { $int_apps = hiera('int_docker_apps') $int_app_defaults = hiera('int_docker_app_common') $st_apps = hiera('staging_docker_apps') $srs_apps = hiera('pm_docker_apps_srs') $pm_apps = hiera('pm_docker_apps') + $st_apps + $srs_apps $pm_app_defaults = hiera('pm_docker_app_common') $apps_by_zone = { 'int' =&gt; $int_apps, 'pm' =&gt; $pm_apps, } $app_access_by_zone = { 'int' =&gt; {'accessible_from' =&gt; $int_app_defaults['accessible_from']}, 'pm' =&gt; {'accessible_from' =&gt; $pm_app_defaults['accessible_from']}, } } default: { fail('Please ensure the node has $location fact set (int, pm, ext)') } } file { '/etc/icinga2/conf.d/': ensure =&gt; directory, recurse =&gt; true, purge =&gt; true, owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0750', notify =&gt; Service['icinga2'], } $default_config.each | String $file_name |{ file {"/etc/icinga2/conf.d/${file_name}": ensure =&gt; present, source =&gt; "puppet:///modules/profiles/services/monitoring/default_config/${file_name}", owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0640', } } $app_checks = { 'ssl' =&gt; $services['webchecks']['checks']['ssl']['vars'], 'http' =&gt; $services['webchecks']['checks']['http_regexp']['vars'] } $apps_by_zone.each | String $zone, Hash $app_list | { profiles::services::monitoring::docker_apps{$zone: app_list =&gt; $app_list, apps_accessible_from =&gt; $app_access_by_zone[$zone], apps_access_list =&gt; $apps_access_list, webhost_defaults =&gt; $webhost_defaults, webcheck_defaults =&gt; $webcheck_defaults, service_overrides =&gt; $service_overrides, targets =&gt; $targets, app_checks =&gt; $app_checks, } } #### HOSTS #### # Profiles::Services::Monitoring::Host &lt;&lt;| |&gt;&gt; # This is for spaceship invasion when it's ready. $hosts_has_large_disks = query_nodes('mountpoints.*.size_bytes &gt;= 1099511627776') $hosts.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_notify_group}, $vars_has_large_disks)) # Merging vars separately $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) profiles::services::monitoring::host{$host_name: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; $host_data['target'], check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } if !empty($hosts_api){ # All hosts managed by API $hosts_api.each | String $zone, Hash $hosts_api_zone | { # Split api hosts by zones $hosts_api_zone.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_api_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_api_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_api_notify_group}, $vars_has_large_disks)) $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) if defined(Profiles::Services::Monitoring::Host[$host_name]){ $hostname = "${host_name}_from_${zone}" } else { $hostname = $host_name } profiles::services::monitoring::host{$hostname: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; "${host_data['target_base']}/${zone}/hosts.conf", check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } } } #### END OF HOSTS #### #### SERVICES #### $services.each | String $service_group, Hash $s_list |{ # Service_group and list of services in that group $service_list = $s_list['checks'] # List of actual checks, separately from SG settings $service_list.each | String $service_name, Hash $data |{ $merged_defaults = merge($service_defaults, $s_list['settings']) # global service defaults + service group defaults $merged_data = merge($merged_defaults, $data) $settings_vars = pick($s_list['settings']['vars'], {}) $this_service_vars = pick($data['vars'], {}) $all_service_vars = delete_undef_values($service_defaults['vars'] + $settings_vars + $this_service_vars) # If we override default check_timeout, but not nrpe_timeout, make nrpe_timeout the same as check_timeout if ( $merged_data['check_timeout'] and ! $this_service_vars['nrpe_timeout'] ) { # NB: Icinga will convert 1m to 60 automatically! $nrpe = { 'nrpe_timeout' =&gt; $merged_data['check_timeout'] } } else { $nrpe = {} } # By default we use nrpe and all commands are run via nrpe. So vars.nrpe_command = $service_name is a default value # If it's server-side Icinga command - we don't need 'nrpe_command' # but there is no harm to have that var and the code is shorter if $merged_data['check_command'] == 'nrpe'{ $check_command = $merged_data['vars']['nrpe_command'] ? { undef =&gt; { 'nrpe_command' =&gt; $service_name }, default =&gt; { 'nrpe_command' =&gt; $merged_data['vars']['nrpe_command'] } } }else{ $check_command = {} } # Assembling $vars from Global Default service settings, servicegroup settings, this particular check settings and let's not forget nrpe settings. if $all_service_vars['graphite_template'] { $graphite_template = {'check_command' =&gt; $all_service_vars['graphite_template']} }else{ $graphite_template = {'check_command' =&gt; $service_name} } $service_notify = [] + pick($settings_vars['notify_group'], []) + pick($this_service_vars['notify_group'], []) # pick is required everywhere, otherwise becomes "The value '' cannot be converted to Numeric" $service_notify_group = $service_notify ? { [] =&gt; $service_defaults['vars']['notify_group'], default =&gt; $service_notify } # Assing default group (systems) if no other groups are defined $vars = $all_service_vars + $nrpe + $check_command + $graphite_template + {'notify_group' =&gt; $service_notify_group} # This needs to be merged separately, because merging it as part of MERGED_DATA overwrites arrays instead of merging them, so we lose some "assign" and "ignore" values $assign = delete_undef_values($service_defaults['assign'] + $s_list['settings']['assign'] + $data['assign']) $ignore = delete_undef_values($service_defaults['ignore'] + $s_list['settings']['ignore'] + $data['ignore']) icinga2::object::service {$service_name: ensure =&gt; $merged_data['ensure'], apply =&gt; $merged_data['apply'], enable_flapping =&gt; $merged_data['enable_flapping'], assign =&gt; $assign, ignore =&gt; $ignore, groups =&gt; [$service_group], check_command =&gt; $merged_data['check_command'], check_interval =&gt; $merged_data['check_interval'], check_timeout =&gt; $merged_data['check_timeout'], check_period =&gt; $merged_data['check_period'], display_name =&gt; $merged_data['display_name'], event_command =&gt; $merged_data['event_command'], retry_interval =&gt; $merged_data['retry_interval'], max_check_attempts =&gt; $merged_data['max_check_attempts'], target =&gt; $merged_data['target'], vars =&gt; $vars, template =&gt; $merged_data['template'], } } } #### END OF SERVICES #### #### OTHER BORING STUFF #### $servicegroups.each | $servicegroup, $description |{ icinga2::object::servicegroup{ $servicegroup: target =&gt; $servicegroup_target, display_name =&gt; $description } } $hostgroups.each| String $hostgroup |{ profiles::services::monitoring::hostgroup { $hostgroup:} } $notifications.each | String $name, Hash $settings |{ $assign = pick($notification_defaults['assign'], []) + $settings['assign'] $ignore = pick($notification_defaults['ignore'], []) + $settings['ignore'] $merged_settings = $settings + $notification_defaults icinga2::object::notification{$name: target =&gt; $merged_settings['target'], apply =&gt; $merged_settings['apply'], apply_target =&gt; $merged_settings['apply_target'], command =&gt; $merged_settings['command'], interval =&gt; $merged_settings['interval'], states =&gt; $merged_settings['states'], types =&gt; $merged_settings['types'], assign =&gt; delete_undef_values($assign), ignore =&gt; delete_undef_values($ignore), user_groups =&gt; $merged_settings['user_groups'], period =&gt; $merged_settings['period'], vars =&gt; $merged_settings['vars'], } } # Merging notification settings for users with other settings $users_oncall = deep_merge($users, $oncall) # Magic. Do not touch. create_resources('icinga2::object::user', $users_oncall, $user_defaults) create_resources('icinga2::object::usergroup', $usergroups, $usergroup_defaults) create_resources('icinga2::object::timeperiod',$timeperiods) create_resources('icinga2::object::checkcommand', $check_commands) create_resources('icinga2::object::notificationcommand', $notification_commands) profiles::services::sudoers { 'icinga_runs_ping_l2': ensure =&gt; present, sudoersd_template =&gt; 'profiles/os/redhat/centos7/sudoers/icinga.erb', } }</code> </pre> </div></div><br><p>  Todav√≠a estoy trabajando en este fideo y mejor√°ndolo siempre que sea posible.  Sin embargo, fue este c√≥digo el que hizo posible usar una sintaxis simple y clara en Hiera: </p><br><div class="spoiler">  <b class="spoiler_title">Datos</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">profiles::services::monitoring::config::services: perf_checks: settings: check_interval: '2m' assign: - 'host.vars.type == linux' checks: procs: {} load: {} memory: {} disk: check_interval: '5m' vars: notification_period: '24x7' disk_iops: vars: notifications: - 'silent' cpu: vars: notifications: - 'silent' dns_fqdn: check_interval: '15m' ignore: - 'xenserver in host.groups' vars: notifications: - 'silent' iftraffic_nrpe: vars: notifications: - 'silent' logging: settings: assign: - 'logserver in host.groups' checks: rsyslog: {} nginx_limit_req_other: {} nginx_limit_req_s2s: {} nginx_limit_req_s2x: {} nginx_limit_req_srs: {} logstash: {} logstash_api: vars: notifications: - 'silent'</code> </pre> </div></div><br><p>  Todas las verificaciones se dividen en grupos, cada grupo tiene configuraciones predeterminadas como d√≥nde y con qu√© frecuencia ejecutar estas verificaciones, qu√© notificaciones enviar y a qui√©n. </p><br><p>  En cada verificaci√≥n, puede anular cualquier opci√≥n, y todo esto finalmente se suma a la configuraci√≥n predeterminada de todas las verificaciones en su conjunto.  Por lo tanto, estos fideos se escriben en config.pp: hay una fusi√≥n de todas las configuraciones predeterminadas con las configuraciones de los grupos y luego con cada verificaci√≥n individual. </p><br><p>  Adem√°s, un cambio muy importante fue la capacidad de usar funciones en la configuraci√≥n, por ejemplo, la funci√≥n de cambiar el puerto, la direcci√≥n y la URL para verificar http_regex. </p><br><pre> <code class="plaintext hljs">http_regexp: assign: - 'host.vars.http_regex' - 'static_sites in host.groups' check_command: 'http' check_interval: '1m' retry_interval: '20s' max_check_attempts: 6 http_port: '{{ if(host.vars.http_port) { return host.vars.http_port } else { return 443 } }}' vars: notification_period: 'host.vars.notification_period' http_vhost: '{{ if(host.vars.http_vhost) { return host.vars.http_vhost } else { return host.name } }}' http_ssl: '{{ if(host.vars.http_ssl) { return false } else { return true } }}' http_expect_body_regex: 'host.vars.http_regex' http_uri: '{{ if(host.vars.http_uri) { return host.vars.http_uri } else { return "/" } }}' http_onredirect: 'follow' http_warn_time: 8 http_critical_time: 15 http_timeout: 30 http_sni: true</code> </pre> <br><p>  Esto significa que, si hay una variable <em>http_port</em> en la <em>definici√≥n de</em> host, <em>√∫sela</em> , de lo contrario 443. Por ejemplo, la interfaz web jabber se bloquea en 9090 y Unifi en 7443. <br>  <em>http_vhost</em> significa ignorar DNS y tomar esta direcci√≥n. <br>  Si se especifica uri en el host, contin√∫e, de lo contrario tome "/". </p><br><p>  Una historia divertida sali√≥ con http_ssl: esta infecci√≥n no quer√≠a desconectarse a pedido.  Est√∫pidamente me top√© con esta l√≠nea durante mucho tiempo, hasta que me di cuenta de que hab√≠a una variable en la definici√≥n del host: </p><br><pre> <code class="plaintext hljs">http_ssl: false</code> </pre> <br><p>  Sustituye en expresi√≥n </p><br><pre> <code class="plaintext hljs">if(host.vars.http_ssl) { return false } else { return true }</code> </pre> <br><p>  como <strong>falso</strong> y al final resulta </p><br><pre> <code class="plaintext hljs">if(false) { return false } else { return true }</code> </pre> <br><p>  es decir, la verificaci√≥n SSL siempre est√° activa.  Se decidi√≥ reemplazando la sintaxis: </p><br><pre> <code class="plaintext hljs">http_ssl: no</code> </pre> <br><p>  <strong>Conclusiones</strong> : </p><br><p>  Pros: </p><br><ul><li>  Ahora tenemos un sistema de monitoreo, y no dos, como fue en los √∫ltimos 7-8 meses, o uno, obsoleto y vulnerable. </li><li>  La estructura de datos de los hosts / servicios (verificaciones) ahora es (en mi opini√≥n) mucho m√°s legible y comprensible.  Para otros, esto no era tan obvio, as√≠ que tuve que cortar un par de p√°ginas en el wiki local para explicar c√≥mo funciona y d√≥nde editarlo. </li><li>  Es posible configurar comprobaciones de manera flexible utilizando variables y funciones, por ejemplo, para verificar http_regexp, el patr√≥n deseado, el c√≥digo de retorno, la url y el puerto se pueden establecer en la configuraci√≥n del host. </li><li>  Hay varios paneles, para cada uno de los cuales puede definir su propia lista de alarmas mostradas y administrar todo esto a trav√©s de Puppet y solicitudes de fusi√≥n. </li></ul><br><p>  Contras: </p><br><ul><li>  Inercia de los miembros del equipo: Nagios trabaj√≥, trabaj√≥ y trabaj√≥, y este Isinga constantemente molesta y ralentiza.  ¬øY c√≥mo puedo ver la historia?  Y, maldici√≥n, no se actualiza ... (El verdadero problema es que el historial de alarmas no se actualiza autom√°ticamente, solo por F5) </li><li>  La inercia del sistema, cuando hago clic en "verificar ahora" en la interfaz web, el resultado de la ejecuci√≥n depende del clima en Marte, especialmente en servicios complejos que tardan decenas de segundos en completarse.  Un resultado similar es algo normal. <img src="https://habrastorage.org/webt/ue/73/wa/ue73wa4yt4bhedebd1n66kizsf8.jpeg"></li><li>  En general, seg√∫n las estad√≠sticas semestrales de los dos sistemas trabajando uno al lado del otro, Nagios siempre trabaj√≥ m√°s r√°pido que Icinga y realmente me molest√≥.  Me parece que hay algo enga√±ado con temporizadores y un control de cinco minutos sobre el hecho cada 5:30 o algo as√≠. </li><li>  Si reinicia el servicio en cualquier momento (systemctl restart icinga2): todas las comprobaciones que estaban en curso en ese momento activar√°n una alarma cr√≠tica &lt;terminada por la se√±al 15&gt; en la pantalla y desde el lado parecer√° que todo ha ca√≠do ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">error confirmado</a> ) </li></ul><br><p>  Pero en general, funciona. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/444060/">https://habr.com/ru/post/444060/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444048/index.html">Consejos y trucos forenses digitales: c√≥mo detectar cambios en la pol√≠tica de grupo impulsados ‚Äã‚Äãpor intrusos</a></li>
<li><a href="../444050/index.html">Discusi√≥n: ¬øSe volver√° masivo el almacenamiento de ADN?</a></li>
<li><a href="../444052/index.html">C√≥mo en IntelliJ IDEA buscamos expresiones lambda</a></li>
<li><a href="../444056/index.html">Los proveedores de Internet en Crimea aumentaron dr√°sticamente los precios de los servicios.</a></li>
<li><a href="../444058/index.html">Cuando los ni√±os entienden que toda su vida ya est√° en l√≠nea</a></li>
<li><a href="../444062/index.html">Iluminar! Transformaciones nocturnas del Centro Lakhta</a></li>
<li><a href="../444064/index.html">Nuevas ideas para un nuevo futuro.</a></li>
<li><a href="../444068/index.html">Quien esta mirando</a></li>
<li><a href="../444070/index.html">Desarrollar un hex√°podo desde cero (parte 4) - trayectorias y secuencias matem√°ticas</a></li>
<li><a href="../444072/index.html">Compras en Android - Play Billing Library</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>