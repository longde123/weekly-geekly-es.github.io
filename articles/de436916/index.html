<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úåüèº üì∑ üëáüèº VShard - horizontale Skalierung in Tarantool üë®üèø‚Äçüè´ ü¶ë üîâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mein Name ist Vladislav, ich beteilige mich an der Entwicklung von Tarantool - DBMS und Anwendungsserver in einer Flasche. Und heute werde ich Ihnen e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - horizontale Skalierung in Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/436916/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Mein Name ist Vladislav, ich beteilige mich an der Entwicklung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarantool</a> - DBMS und Anwendungsserver in einer Flasche.  Und heute werde ich Ihnen erz√§hlen, wie wir die horizontale Skalierung in Tarantool mithilfe des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VShard-</a> Moduls <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">implementiert haben</a> . <br><a name="habracut"></a><br>  Zun√§chst eine kleine Theorie. <br><br>  Es gibt zwei Arten der Skalierung: horizontal und vertikal.  Horizontal wird in zwei Typen unterteilt: Replikation und Sharding.  Die Replikation wird zum Skalieren von Computern verwendet, das Sharding zum Skalieren von Daten. <br><br>  Sharding wird in zwei Typen unterteilt: Sharding nach Bereichen und Sharding nach Hashes. <br><br>  Beim Sharding mit Bereichen berechnen wir aus jedem Datensatz im Cluster einen Shard-Schl√ºssel.  Diese Shard-Schl√ºssel werden auf eine gerade Linie projiziert, die in Bereiche unterteilt ist, die wir verschiedenen physischen Knoten hinzuf√ºgen. <br><br>  Das Sharding mit Hashes ist einfacher: Von jedem Datensatz im Cluster, den wir als Hash-Funktion betrachten, f√ºgen wir die Eintr√§ge mit demselben Wert der Hash-Funktion einem physischen Knoten hinzu. <br><br>  Ich werde √ºber horizontale Skalierung mit Hash-Sharding sprechen. <br><br><h1>  Vorherige Implementierung </h1><br>  Das erste horizontale Skalierungsmodul, das wir hatten, war <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarantool Shard</a> .  Dies ist ein sehr einfaches Sharding durch Hashes, bei dem der Shard-Schl√ºssel aus dem Prim√§rschl√ºssel aller Eintr√§ge im Cluster ber√ºcksichtigt wird. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Aber dann entstand eine Aufgabe, die Tarantool Shard aus drei grundlegenden Gr√ºnden nicht bew√§ltigen konnte. <br><br>  Erstens war die <b>Lokalit√§t logisch verwandter Daten</b> erforderlich.  Wenn Daten logisch verbunden sind, m√∂chten wir sie immer auf demselben physischen Knoten speichern, unabh√§ngig davon, wie sich die Clustertopologie √§ndert oder der Ausgleich durchgef√ºhrt wird.  Und Tarantool Shard garantiert dies nicht.  Er betrachtet den Hash nur anhand von Prim√§rschl√ºsseln, und beim Neuausgleich k√∂nnen sogar Datens√§tze mit demselben Hash f√ºr einige Zeit getrennt werden - die √úbertragung ist nicht atomar. <br><br>  Das Problem der fehlenden Lokalit√§t der Daten hat uns am meisten verhindert.  Ich werde ein Beispiel geben.  Es gibt eine Bank, bei der der Kunde ein Konto er√∂ffnet hat.  Konto- und Kundendaten m√ºssen immer physisch zusammen gespeichert werden, damit sie in einer Anfrage gelesen und in einer Transaktion ausgetauscht werden k√∂nnen, beispielsweise beim √úberweisen von Geld von einem Konto.  Wenn Sie klassisches Sharding mit Tarantool Shard verwenden, unterscheiden sich die Werte der Shard-Funktionen f√ºr Konten und Kunden.  Daten k√∂nnen sich auf verschiedenen physischen Knoten befinden.  Dies erschwert sowohl das Lesen als auch die Transaktionsarbeit mit einem solchen Kunden erheblich. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre><br>  Im obigen Beispiel k√∂nnen die <code>id</code> Felder leicht nicht mit Konten und Kunden √ºbereinstimmen.  Sie sind √ºber das Kontofeld <code>customer_id</code> und <code>customer_id</code> <code>id</code> .  Das gleiche <code>id</code> Feld w√ºrde die Eindeutigkeit des Konto-Prim√§rschl√ºssels beeintr√§chtigen.  Und auf andere Weise kann Shard nicht scherben. <br><br>  Das n√§chste Problem ist das <b>langsame Resharding</b> .  Dies ist das klassische Problem aller Scherben auf Hashes.  Unter dem Strich √§ndern wir normalerweise die Shard-Funktion, wenn wir die Zusammensetzung eines Clusters √§ndern, da dies normalerweise von der Anzahl der Knoten abh√§ngt.  Wenn sich die Funktion √§ndert, m√ºssen Sie alle Eintr√§ge im Cluster durchgehen und die Shard-Funktion erneut neu berechnen.  Vielleicht ein paar Notizen √ºbertragen.  Und w√§hrend wir sie √ºbertragen, wissen wir nicht, ob die Daten, die f√ºr die n√§chste eingehende Anforderung ben√∂tigt werden, bereits √ºbertragen wurden. Vielleicht werden sie gerade √ºbertragen.  Daher ist es w√§hrend des Resharding erforderlich, dass bei jedem Lesen zwei Shard-Funktionen angefordert werden: die alte und die neue.  Anfragen werden doppelt so langsam und f√ºr uns war das inakzeptabel. <br><br>  Ein weiteres Merkmal von Tarantool Shard war, dass bei Ausfall einiger Knoten in den Replikats√§tzen eine <b>schlechte Lesezugriffsf√§higkeit</b> angezeigt wird. <br><br><h1>  Neue L√∂sung </h1><br>  Um die drei beschriebenen Probleme zu l√∂sen, haben wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarantool VShard erstellt</a> .  Der Hauptunterschied besteht darin, dass die Datenspeicherebene virtualisiert wird: Virtuelle Speicher werden √ºber physischen Speichern angezeigt, und Datens√§tze werden unter ihnen verteilt.  Diese Speicher werden Bucket'ami genannt.  Der Benutzer muss nicht dar√ºber nachdenken, was und auf welchem ‚Äã‚Äãphysischen Knoten liegt.  Bucket ist eine atomare unteilbare Dateneinheit, wie beim klassischen Sharding eines Tupels.  VShard speichert immer den gesamten Bucket auf einem physischen Knoten und √ºbertr√§gt beim Resharding alle Daten eines Buckets atomar.  Aus diesem Grund wird Lokalit√§t zur Verf√ºgung gestellt.  Wir m√ºssen nur die Daten in einen Bucket packen und k√∂nnen immer sicher sein, dass diese Daten zusammen mit allen √Ñnderungen im Cluster vorliegen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  Wie kann ich Daten in einen Eimer legen?  In dem Schema, das wir zuvor f√ºr den Bankkunden eingef√ºhrt haben, f√ºgen wir die <code>bucket id</code> gem√§√ü dem neuen Feld zu den Tabellen hinzu.  Wenn die verkn√ºpften Daten identisch sind, befinden sich die Datens√§tze im selben Bucket.  Der Vorteil ist, dass wir diese Datens√§tze mit derselben <code>bucket id</code> in verschiedenen R√§umen und sogar in verschiedenen Engines speichern k√∂nnen.  Die <code>bucket id</code> bereitgestellt, unabh√§ngig davon, wie diese Datens√§tze gespeichert sind. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre><br>  Warum sind wir so gespannt darauf?  Wenn wir klassisches Sharding haben, k√∂nnen sich die Daten √ºber alle physischen Speicher erstrecken, die wir nur haben.  Wenn Sie im Beispiel mit der Bank alle Konten eines Kunden anfordern, m√ºssen Sie sich an alle Knoten wenden.  Es stellt sich heraus, dass es schwierig ist, O (N) zu lesen, wobei N die Anzahl der physischen Speicher ist.  Schrecklich langsam. <br><br>  Dank Bucket'am und der Lokalit√§t nach <code>bucket id</code> wir unabh√§ngig von der Clustergr√∂√üe immer Daten von einem Knoten in einer Anforderung lesen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2bb/524/8b7/2bb5248b7757ea6249f47a3dca46a681.png"><br><br>  Sie m√ºssen die <code>bucket id</code> berechnen und dieselben Werte selbst zuweisen.  F√ºr einige ist dies ein Vorteil, f√ºr jemanden ein Nachteil.  Ich halte es f√ºr einen Vorteil, dass Sie die Funktion zur Berechnung der <code>bucket id</code> selbst ausw√§hlen k√∂nnen. <br><br>  Was ist der Hauptunterschied zwischen klassischem Sharding und virtuellem Sharding mit Bucket? <br><br>  Im ersten Fall, wenn wir die Zusammensetzung des Clusters √§ndern, haben wir zwei Zust√§nde: den aktuellen (alten) und den neuen, in den wir gehen m√ºssen.  W√§hrend des √úbergangsprozesses m√ºssen Sie nicht nur die Daten √ºbertragen, sondern auch die Hash-Funktionen f√ºr alle Datens√§tze neu berechnen.  Dies ist sehr unpraktisch, da wir zu einem bestimmten Zeitpunkt nicht wissen, welche Daten bereits √ºbertragen wurden und welche nicht.  Dar√ºber hinaus ist dies weder zuverl√§ssig noch atomar, da f√ºr die atomare √úbertragung eines Satzes von Datens√§tzen mit demselben Wert der Hash-Funktion der √úbertragungsstatus dauerhaft gespeichert werden muss, falls eine Wiederherstellung erforderlich ist.  Es gibt Konflikte, Fehler, Sie m√ºssen die Prozedur viele Male neu starten. <br><br>  Virtuelles Sharding ist viel einfacher.  Wir haben nicht zwei ausgew√§hlte Zust√§nde des Clusters, wir haben nur den Zustand des Buckets.  Der Cluster wird wendiger und bewegt sich allm√§hlich von einem Zustand in einen anderen.  Und jetzt gibt es mehr als zwei Staaten.  Dank eines reibungslosen √úbergangs k√∂nnen Sie das Guthaben im laufenden Betrieb √§ndern und den neu hinzugef√ºgten Speicher l√∂schen.  Das hei√üt, die Steuerbarkeit des Auswuchtens wird stark erh√∂ht, es wird granular. <br><br><h1>  Verwenden Sie </h1><br>  Angenommen, wir haben eine Funktion f√ºr die <code>bucket id</code> und so viele Daten in den Cluster eingegeben, dass kein Speicherplatz mehr vorhanden war.  Jetzt m√∂chten wir Knoten hinzuf√ºgen, damit die Daten selbst zu ihnen verschoben werden.  In VShard geschieht dies wie folgt.  Starten Sie zuerst neue Knoten und Tarantools und aktualisieren Sie dann die VShard-Konfiguration.  Es beschreibt alle Clustermitglieder, alle Replikate, Replikats√§tze, Master, zugewiesenen URIs und vieles mehr.  Wir f√ºgen der Konfiguration neue Knoten hinzu und verwenden sie mit der Funktion <code>VShard.storage.cfg</code> auf allen Knoten des Clusters. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Wie Sie sich erinnern, √§ndert sich beim klassischen Sharding mit einer √Ñnderung der Anzahl der Knoten auch die Shard-Funktion.  In VShard passiert dies nicht, wir haben eine feste Anzahl von virtuellen Speichern - Bucket'ov.  Dies ist die Konstante, die Sie beim Starten des Clusters ausw√§hlen.  Aus diesem Grund scheint die Skalierbarkeit eingeschr√§nkt zu sein, aber nicht wirklich.  Sie k√∂nnen eine gro√üe Anzahl von Bucket'ov, Zehntausenden und Hunderttausenden ausw√§hlen.  Die Hauptsache ist, dass mindestens zwei Gr√∂√üenordnungen mehr als die maximale Anzahl von Replikats√§tzen vorhanden sein sollten, die Sie jemals im Cluster haben werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  Da sich die Anzahl der virtuellen Speicher nicht √§ndert und die Shard-Funktion nur von diesem Wert abh√§ngt, k√∂nnen wir so viele physische Speicher wie n√∂tig hinzuf√ºgen, ohne die Shard-Funktion erneut anzugeben. <br><br>  Wie werden Bukets selbst auf physische Gesch√§fte verteilt?  Wenn VShard.storage.cfg auf einem der Knoten aufgerufen wird, wird der Neuausgleichsprozess aktiviert.  Dies ist ein Analyseprozess, der das perfekte Gleichgewicht in einem Cluster berechnet.  Er geht zu allen physischen Knoten, fragt, wer wie viele Bucket'ov hat, und erstellt Routen f√ºr ihre Bewegung, um die Verteilung zu mitteln.  Der Rebalancer sendet Routen zu √ºberf√ºllten Lagern, und sie beginnen, Eimer zu senden.  Nach einiger Zeit wird der Cluster ausgeglichen. <br><br>  In realen Projekten kann das Konzept der perfekten Balance jedoch anders sein.  Ich m√∂chte beispielsweise weniger Daten auf einem Replikatsatz als auf dem anderen speichern, da weniger Festplattenspeicher vorhanden ist.  VShard glaubt, dass alles gut ausbalanciert ist, und tats√§chlich ist mein Speicher kurz vor dem √úberlaufen.  Wir haben einen Mechanismus zum Anpassen der Ausgleichsregeln mithilfe von Gewichten bereitgestellt.  Jeder Replikatsatz und jedes Repository kann gewichtet werden.  Wenn der Balancer entscheidet, an wen er wie viele Eimer senden soll, ber√ºcksichtigt er die <b>Beziehung</b> aller Gewichtspaare. <br><br>  Zum Beispiel hat ein Gesch√§ft ein Gewicht von 100 und das andere 200. Dann speichert das erste zweimal weniger Eimer als das zweite.  Bitte beachten Sie, dass ich speziell √ºber das Gewichtsverh√§ltnis spreche.  Absolute Bedeutungen haben keine Wirkung.  Sie k√∂nnen Gewichte basierend auf einer 100% -Clusterverteilung ausw√§hlen: Ein Gesch√§ft hat 30%, ein anderes 70%.  Sie k√∂nnen die Speicherkapazit√§t in Gigabyte als Basis verwenden oder die Gewichte in der Anzahl der Bucket'ov messen.  Die Hauptsache ist, die Einstellung zu beobachten, die Sie brauchen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b4e/889/298/b4e889298991781b8c3ee01f4c066a6e.png"><br><br>  Ein solches System hat einen interessanten Nebeneffekt: Wenn Sie einem Gesch√§ft ein Gewicht von Null zuweisen, befiehlt der Balancer dem Gesch√§ft, alle seine Eimer zu verteilen.  Danach k√∂nnen Sie den gesamten Replikatsatz aus der Konfiguration entfernen. <br><br><h1>  Atomic Bucket Transfer </h1><br>  Wir haben einen Bucket, der Lese- und Schreibanforderungen akzeptiert, und dann fordert der Balancer auf, ihn in einen anderen Speicher zu √ºbertragen.  Bucket akzeptiert keine Aufzeichnungsanforderungen mehr, andernfalls k√∂nnen sie diese w√§hrend der √úbertragung aktualisieren. Dann haben sie Zeit, das tragbare Update, dann das tragbare Update-Update usw. auf unendlich zu aktualisieren.  Daher ist der Datensatz blockiert und Sie k√∂nnen immer noch aus dem Bucket lesen.  Die √úbertragung von Brocken an einen neuen Ort beginnt.  Nach Abschluss der √úbertragung nimmt der Bucket erneut Anforderungen an.  An der alten Stelle liegt es auch noch, aber es wurde bereits als M√ºll markiert und anschlie√üend wird es vom M√ºllsammler St√ºck f√ºr St√ºck gel√∂scht. <br><br>  Jeder Bucket ist Metadaten zugeordnet, die physisch auf der Festplatte gespeichert sind.  Alle oben genannten Schritte werden auf der Festplatte gespeichert. Unabh√§ngig davon, was mit dem Repository geschieht, wird der Status des Buckets automatisch wiederhergestellt. <br><br>  Sie k√∂nnten Fragen haben: <br><br><ul><li>  <b>Was passiert mit den Anforderungen, die mit dem Bucket funktionierten, als sie mit dem Portieren begannen?</b> <br><br>  In den Metadaten jedes Buckets gibt es zwei Arten von Links: Lesen und Schreiben.  Wenn der Benutzer eine Anfrage an den Bucket stellt, gibt er an, wie er damit arbeiten, schreibgesch√ºtzt oder schreibgesch√ºtzt schreiben wird.  F√ºr jede Anforderung wird der entsprechende Referenzz√§hler erh√∂ht. <br><br>  Warum brauche ich einen Referenzz√§hler zum Lesen von Anfragen?  Angenommen, der Bucket wird leise √ºbertragen, und hier kommt der Garbage Collector und m√∂chte diesen Bucket l√∂schen.  Er sieht, dass die Anzahl der Links gr√∂√üer als Null ist, sodass Sie sie nicht l√∂schen k√∂nnen.  Und wenn die Anforderungen verarbeitet werden, kann der Garbage Collector seine Arbeit abschlie√üen. <br><br>  Der Referenzz√§hler zum Schreiben von Anforderungen stellt sicher, dass der Bucket nicht einmal zu √ºbertragen beginnt, w√§hrend mindestens eine Schreibanforderung damit arbeitet.  Schreibanfragen k√∂nnen jedoch st√§ndig eingehen, und dann wird der Eimer niemals √ºbertragen.  Tatsache ist, dass, wenn der Balancer den Wunsch ge√§u√üert hat, ihn zu √ºbertragen, neue Aufzeichnungsanforderungen blockiert werden und das aktuelle System auf den Abschluss einer Zeit√ºberschreitung wartet.  Wenn die Anforderungen nicht in der zugewiesenen Zeit abgeschlossen werden, akzeptiert das System erneut neue Schreibanforderungen und verschiebt die √úbertragung des Buckets um einige Zeit.  Somit unternimmt der Balancer die √úbertragungsversuche, bis einer erfolgreich ist. <br><br>  VShard verf√ºgt √ºber eine Bucket_Ref-API auf niedriger Ebene, falls Sie nur wenige Funktionen auf hoher Ebene haben.  Wenn Sie wirklich etwas selbst tun m√∂chten, greifen Sie einfach √ºber den Code auf diese API zu. </li><li>  <b>Ist es m√∂glich, Datens√§tze √ºberhaupt nicht zu blockieren?</b> <br><br>  Es ist unm√∂glich.  Wenn der Bucket wichtige Daten enth√§lt, f√ºr die ein st√§ndiger Schreibzugriff erforderlich ist, m√ºssen Sie die √úbertragung vollst√§ndig blockieren.  <code>bucket_pin</code> gibt es eine Funktion <code>bucket_pin</code> , die den Bucket fest mit dem aktuellen Replikatsatz verbindet und dessen √úbertragung verhindert.  In diesem Fall kann sich der benachbarte Eimer ohne Einschr√§nkungen bewegen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Es gibt ein Tool, das noch leistungsf√§higer ist als <code>bucket_pin</code> - das Blockieren von Replikats√§tzen.  Dies geschieht nicht mehr im Code, sondern durch Konfiguration.  Das Blockieren verbietet die Bewegung von Bucket'ov aus diesem Replikatset'a und den Empfang neuer.  Dementsprechend stehen alle Daten st√§ndig zur Aufzeichnung zur Verf√ºgung. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h1>  VShard.router </h1><br>  VShard besteht aus zwei Submodulen: VShard.storage und VShard.router.  Sie k√∂nnen selbst in einer Instanz unabh√§ngig erstellt und skaliert werden.  Beim Zugriff auf den Cluster wissen wir nicht, wo sich der Bucket befindet, und VShard.router sucht nach der <code>bucket id</code> f√ºr uns. <br><br>  Schauen wir uns ein Beispiel an, wie das aussieht.  Wir kehren zum Bankcluster und zu den Kundenkonten zur√ºck.  Ich m√∂chte in der Lage sein, alle Konten eines bestimmten Clients aus dem Cluster zu ziehen.  Dazu schreibe ich die √ºbliche Funktion f√ºr die lokale Suche: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f78/e2b/df2/f78e2bdf2d977fcb9fb320b031592171.png"><br><br>  Sie sucht nach allen Kundenkonten anhand seiner ID.  Jetzt muss ich entscheiden, welches der Repositorys diese Funktion aufrufen soll.  Dazu berechne ich die <code>bucket id</code> aus der Client-ID in meiner Anfrage und fordere VShard.router auf, mir eine solche Funktion in dem Speicher aufzurufen, in dem der Bucket mit der resultierenden <code>bucket id</code> lebt.  Im Submodul befindet sich eine Routing-Tabelle, in der die Position des Buckets im Replikatsatz angegeben ist.  Und VShard.router vertritt meine Anfrage. <br><br>  Nat√ºrlich kann es vorkommen, dass zu diesem Zeitpunkt das Resharding begann und sich der Eimer in Bewegung setzte.  Der Router im Hintergrund aktualisiert die Tabelle schrittweise in gro√üen Bl√∂cken: Er fragt die Repositorys nach ihren aktuellen Bucket-Tabellen ab. <br><br>  Es kann sogar vorkommen, dass wir uns dem gerade verschobenen Bucket zuwenden und der Router seine Routing-Tabelle noch nicht aktualisiert hat.  Dann wendet er sich an das alte Repository und teilt dem Router entweder mit, wo er nach dem Bucket suchen soll, oder antwortet einfach, dass er nicht √ºber die erforderlichen Daten verf√ºgt.  Dann durchsucht der Router alle Speicher auf der Suche nach dem gew√ºnschten Bucket.  Und das alles ist f√ºr uns transparent, wir werden nicht einmal einen Fehler in der Routing-Tabelle bemerken. <br><br><h1>  Instabilit√§t lesen </h1><br>  Erinnern Sie sich daran, welche Probleme wir urspr√ºnglich hatten: <br><br><ul><li>  Es gab keine Datenlokalit√§t.  Wir haben uns entschieden, indem wir Bucket'ov hinzugef√ºgt haben. </li><li>  Resharding verlangsamte alles und verlangsamte sich.  Implementierte Atomic Data Transfer Bucket'ami, wurde das Nacherz√§hlen von Shard-Funktionen beseitigt. </li><li>  Instabiles Lesen. </li></ul><br>  Das letzte Problem wird von VShard.router mithilfe des automatischen Lesefailover-Subsystems gel√∂st. <br><br>  Der Router pingt regelm√§√üig den in der Konfiguration angegebenen Speicher an.  Und dann h√∂rten einige von ihnen auf zu pingen.  Der Router verf√ºgt √ºber eine Hot-Backup-Verbindung zu jedem Replikat. Wenn das aktuelle Replikat nicht mehr reagiert, wird es zu einem anderen weitergeleitet.  Die Leseanforderung wird normal verarbeitet, da wir Replikate lesen (aber nicht schreiben) k√∂nnen.  Wir k√∂nnen die Priorit√§t der Replikate festlegen, mit denen der Router das Failover f√ºr die Lesungen ausw√§hlen soll.  Wir machen das mit Zoning. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c3/5bf/dbd/5c35bfdbddd67fe8217f06730673bd43.png"><br><br>  Wir weisen jedem Replikat und jedem Router eine Zonennummer zu und legen eine Tabelle fest, in der wir den Abstand zwischen jedem Zonenpaar angeben.  Wenn der Router entscheidet, wohin eine Leseanforderung gesendet werden soll, w√§hlt er ein Replikat in der Zone aus, die seiner eigenen am n√§chsten liegt. <br><br>  Wie es in der Konfiguration aussieht: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/445/799/5ae/4457995ae5c7bc1761684cf7d4f3b2e4.png"><br><br>  Im allgemeinen Fall k√∂nnen Sie auf ein beliebiges Replikat verweisen. Wenn der Cluster jedoch gro√ü und komplex und sehr verteilt ist, ist das Zoning sehr n√ºtzlich.  Verschiedene Server-Racks k√∂nnen Zonen sein, um das Netzwerk nicht mit Datenverkehr zu belasten.  Oder es k√∂nnen Punkte sein, die geografisch voneinander entfernt sind. <br><br>  Zoning hilft auch bei unterschiedlichen Replikationsleistungen.  Zum Beispiel haben wir in jedem Replikatsatz ein Sicherungsreplikat, das keine Anforderungen akzeptieren sollte, sondern nur eine Kopie der Daten speichert.  Dann schaffen wir es in der Zone, die sehr weit von allen Routern in der Tabelle entfernt ist, und sie werden sich im extremsten Fall daran wenden. <br><br><h1>  Instabilit√§t der Aufnahme </h1><br>  Was ist mit dem Schreibfailover beim √Ñndern des Assistenten, da es sich um ein Lesefailover handelt?  Hier ist VShard nicht so rosig: Die Wahl eines neuen Meisters ist darin nicht umgesetzt, Sie m√ºssen es selbst tun.  Wenn wir es irgendwie ausgew√§hlt haben, ist es notwendig, dass diese Instanz jetzt die Autorit√§t des Masters √ºbernimmt.  Wir aktualisieren die Konfiguration, indem wir <code>master = false</code> f√ºr den alten Master und <code>master = true</code> f√ºr den neuen angeben. Wenden Sie sie √ºber VShard.storage.cfg an und rollen Sie sie in den Speicher.  Dann passiert alles automatisch.  Der alte Master akzeptiert keine Schreibanforderungen mehr und beginnt mit der Synchronisierung mit dem neuen Master, da m√∂glicherweise bereits Daten auf den alten Master angewendet wurden, der neue jedoch noch nicht eingetroffen ist.  Danach tritt der neue Master in die Rolle ein und beginnt, Anforderungen anzunehmen, und der alte Master wird zu einem Replikat.  So funktioniert Write Failover in VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><h1>  Wie kann man nun all diese verschiedenen Ereignisse verfolgen? </h1><br>  Im Allgemeinen reichen zwei Handles aus - <code>VShard.storage.info</code> und <code>VShard.router.info</code> . <br><br>  VShard.storage.info zeigt Informationen in mehreren Abschnitten an. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  Der erste ist der Replikationsabschnitt.  Der Status des Replikatsatzes, auf den Sie diese Funktion angewendet haben, wird angezeigt: Welche Replikationsverz√∂gerung hat sie, mit wem hat sie Verbindungen und mit wem ist sie nicht verf√ºgbar, wer ist verf√ºgbar und nicht verf√ºgbar, welcher Assistent ist f√ºr welche konfiguriert usw. <br><br>  Im Abschnitt Bucket k√∂nnen Sie in Echtzeit sehen, wie viele Bucket'ov derzeit auf das aktuelle Replikatset verschoben werden, wie viele es verlassen, wie viele derzeit daran arbeiten, wie viele als M√ºll markiert sind und wie viele angeh√§ngt sind. <br><br>  Der Alert-Bereich ist ein derartiges Durcheinander aller Probleme, die VShard unabh√§ngig feststellen konnte: Der Master ist nicht konfiguriert, die Redundanzstufe ist unzureichend, der Master ist vorhanden und alle Replikate sind fehlgeschlagen usw. <br><br>  Und der letzte Abschnitt ist ein Licht, das rot leuchtet, wenn es wirklich schlimm wird.  Es ist eine Zahl von null bis drei, je mehr desto schlechter. <br><br>  VShard.router.info hat die gleichen Abschnitte, aber sie bedeuten etwas anders. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  Der erste Abschnitt ist die Replikation.      ,    :    ,  replica set'  ,          ,   ,   replica set'  bucket'     ,     . <br><br>   Bucket    bucket',              ;    bucket'   ;  ,       replica set'. <br><br>   Alert,  ,   ,   failover,   bucket'. <br><br> ,         . <br><br><h1>     VShard? </h1><br>  ‚Äî    bucket'.       <code>int32_max</code> ?     bucket'   ‚Äî  30      16   .     bucket',     .           bucket',          bucket'.    ,          . <br><br>  ‚Äî   -   <code>bucket id</code> .    ,    -   ,   bucket ‚Äî           .      ,   bucket'   ,  VShard    bucket'.       -,      bucket'  bucket,  -.    . <br><br><h1>  Zusammenfassung </h1><br> Vshard : <br><br><ul><li>  ; </li><li>  ; </li><li>    ; </li><li>  read failover; </li><li>    bucket'. </li></ul><br> VShard   .  -    .  ‚Äî  <b>   </b> .     ,       .           . <br><br>  ‚Äî <b>lock-free  bucket'</b> .   ,       bucket'      .      ,     . <br><br>  ‚Äî <b>  </b> .          : -    ,   ,    ?        . <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436916/">https://habr.com/ru/post/de436916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436904/index.html">32K-Schwellenwert f√ºr Daten im AVR-Mikrocontroller-ROM</a></li>
<li><a href="../de436908/index.html">6 M√∂glichkeiten, Daten in einer Android-Anwendung auszublenden</a></li>
<li><a href="../de436910/index.html">Tipps zum Erstellen benutzerdefinierter Workflows in GitLab CI</a></li>
<li><a href="../de436912/index.html">2019 CRM-Trends: Spa√ü beim Lesen, gef√§hrlich zu glauben</a></li>
<li><a href="../de436914/index.html">Probleme mit dem Startwachstum - √úberwachung</a></li>
<li><a href="../de436918/index.html">Erstellen eines Spiels f√ºr Game Boy, Teil 2</a></li>
<li><a href="../de436920/index.html">PAS2JS-Transpiler von Pascal nach JavaScript: Inkompatibel mit Delphi und Problemumgehungen</a></li>
<li><a href="../de436922/index.html">Optimieren der Startzeit von Prometheus 2.6.0 mit pprof</a></li>
<li><a href="../de436924/index.html">Ein paar Worte zur Organisation von Roboterwettbewerben</a></li>
<li><a href="../de436926/index.html">Helden der Zwei-Faktor-Authentifizierung oder wie man "in den Schuhen anderer l√§uft"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>