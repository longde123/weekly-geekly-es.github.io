<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òëÔ∏è üë©üèº‚Äçü§ù‚Äçüë®üèΩ üëºüèª Wir bringen dem Computer bei, Ger√§usche zu unterscheiden: Lernen Sie den DCASE-Wettbewerb kennen und bauen Sie Ihren Audio-Klassifikator in 30 Minuten zusammen üåØ üê≤ ü§∞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel wurde in Verbindung mit ananaskelly geschrieben . 
 Einf√ºhrung 


 Hallo allerseits, Habr! In unserer Arbeit am Zentrum f√ºr Sprachtechn...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wir bringen dem Computer bei, Ger√§usche zu unterscheiden: Lernen Sie den DCASE-Wettbewerb kennen und bauen Sie Ihren Audio-Klassifikator in 30 Minuten zusammen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/speechpro/blog/437818/"><p> Dieser Artikel wurde in Verbindung mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">ananaskelly geschrieben</a> . </p><br><h3 id="vvedenie">  Einf√ºhrung </h3><br><p>  Hallo allerseits, Habr!  In unserer Arbeit am Zentrum f√ºr Sprachtechnologie in St. Petersburg haben wir ein wenig Erfahrung in der L√∂sung der Probleme der Klassifizierung und Erkennung akustischer Ereignisse gesammelt und beschlossen, diese mit Ihnen zu teilen.  Der Zweck dieses Artikels ist es, Ihnen einige Aufgaben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorzustellen</a> und √ºber den automatischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Soundverarbeitungswettbewerb DCASE 2018 zu</a> sprechen.  Wenn wir Ihnen von dem Wettbewerb erz√§hlen, werden wir auf <u>komplexe Formeln und Definitionen im</u> Zusammenhang mit maschinellem Lernen verzichten, damit die allgemeine Bedeutung des Artikels von einem <u>breiten Publikum</u> verstanden wird. </p><br><p>  F√ºr diejenigen, die von der <b>Versammlung</b> des <b>Klassifikators</b> angezogen wurden, haben wir einen kleinen Python-Code vorbereitet. √úber den Link auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github</a> finden Sie ein Notizbuch, in dem wir am Beispiel des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweiten Titels</a> des DCASE-Wettbewerbs ein einfaches Faltungsnetzwerk auf Keras erstellen, um Audiodateien zu klassifizieren.  Dort sprechen wir ein wenig √ºber das Netzwerk und die f√ºr das Training verwendeten Funktionen sowie √ºber die Verwendung einer einfachen Architektur, um ein Ergebnis nahe der Basislinie zu erzielen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MAP @ 3</a> = 0,6). </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cy/xk/ct/cyxkct2xgahwpajkvi4sxzvallu.png"></div><br><p>  Zus√§tzlich werden hier grundlegende Ans√§tze zur L√∂sung von Problemen (Baseline) beschrieben, die von den Organisatoren vorgeschlagen wurden.  Auch in Zukunft wird es mehrere Artikel geben, in denen wir sowohl √ºber unsere Erfahrungen bei der Teilnahme am Wettbewerb als auch √ºber die von anderen Wettbewerbsteilnehmern vorgeschlagenen L√∂sungen ausf√ºhrlicher und ausf√ºhrlicher sprechen werden.  Links zu diesen Artikeln werden nach und nach hier angezeigt. </p><a name="habracut"></a><br><p>  Sicherlich haben viele Menschen absolut keine Ahnung von einer Art <b>‚ÄûDCASE‚Äú</b> . Lassen Sie uns also herausfinden, um welche Art von Obst es sich handelt und <b>womit</b> es gegessen wird.  Der Wettbewerb ‚Äû <abbr title="Erkennung und Klassifizierung von akustischen Szenen und Ereignissen (Erkennung und Klassifizierung von akustischen Szenen und Ereignissen)">DCASE</abbr> ‚Äú findet j√§hrlich statt. Jedes Jahr werden verschiedene Aufgaben zur L√∂sung von Problemen im Bereich der Klassifizierung von Audioaufnahmen und der Erkennung akustischer Ereignisse behandelt.  Jeder kann am Wettbewerb teilnehmen, es ist kostenlos, daf√ºr reicht es aus, sich einfach als Teilnehmer auf der Seite zu registrieren.  Als Ergebnis des Wettbewerbs findet eine Konferenz zu denselben Themen statt, aber im Gegensatz zum Wettbewerb selbst ist die Teilnahme daran bereits bezahlt, und wir werden nicht mehr dar√ºber sprechen.  Belohnungen f√ºr die besten Entscheidungen werden normalerweise nicht herangezogen, aber es gibt Ausnahmen (zum Beispiel die 3. Aufgabe im Jahr 2018).  In diesem Jahr schlugen die Organisatoren die folgenden 5 Aufgaben vor: </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierung von akustischen Szenen (unterteilt in 3 Unteraufgaben)</a> <br>  A. Auf demselben Ger√§t aufgezeichnete Trainings- und Testdatens√§tze <br>  B. Trainings- und Testdatens√§tze, die auf verschiedenen Ger√§ten aufgezeichnet wurden <br>  C. Die Schulung ist unter Verwendung von Daten gestattet, die nicht vom Veranstalter bereitgestellt wurden </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierung akustischer Ereignisse</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vogelgesangserkennung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkennung von akustischen Ereignissen im Haushalt anhand eines schwach beschrifteten Datensatzes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierung der Haushaltsaktivit√§t im Raum nach Mehrkanalaufzeichnung</a> </li></ol><br><h4 id="o-detektirovanii-i-klassifikacii">  Informationen zur Erkennung und Klassifizierung </h4><br><p>  Wie wir sehen k√∂nnen, enthalten die Namen aller Aufgaben eines von zwei W√∂rtern: "Erkennung" oder "Klassifizierung".  Lassen Sie uns kl√§ren, was der Unterschied zwischen diesen Konzepten ist, damit keine Verwirrung entsteht. </p><br><p>  Stellen Sie sich vor, wir haben eine Audioaufnahme, auf der ein Hund in einem Moment bellt und eine Katze in einem anderen miaut, und es gibt einfach keine anderen Ereignisse dort.  Wenn wir dann genau verstehen wollen, wann diese Ereignisse auftreten, m√ºssen wir das Problem der Erkennung eines akustischen Ereignisses l√∂sen.  Das hei√üt, wir m√ºssen die Start- und Endzeiten f√ºr jede Veranstaltung herausfinden.  Nachdem wir das Erkennungsproblem gel√∂st haben, werden wir genau herausfinden, wann die Ereignisse auftreten, aber wir wissen nicht, von wem genau die gefundenen Ger√§usche erzeugt werden. Dann m√ºssen wir das Klassifizierungsproblem l√∂sen, dh bestimmen, was genau in dem angegebenen Zeitraum passiert ist. </p><br><p>  Um die Beschreibung der Aufgaben des Wettbewerbs zu verstehen, reichen diese Beispiele v√∂llig aus, was bedeutet, dass der einleitende Teil abgeschlossen ist und wir mit einer detaillierten Beschreibung der Aufgaben selbst fortfahren k√∂nnen. </p><br><hr><br><h3 id="anchortrack1anchortrack-1-klassifikaciya-akusticheskih-scen"><a name="Track1"></a>  Track 1. Klassifizierung von akustischen Szenen </h3><br><p>  Die erste Aufgabe besteht darin, die Umgebung (akustische Szene) zu bestimmen, in der das Audio aufgenommen wurde, z. B. "Metro Station", "Airport" oder "Pedestrian Street".  Die L√∂sung dieses Problems kann bei der Beurteilung der Umgebung mit einem k√ºnstlichen Intelligenzsystem hilfreich sein, beispielsweise in Autos mit Autopilot. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/db/4g/ff/db4gffctu9tgvd4upbse_eaaiwg.jpeg"></div><br><p>  In dieser Aufgabe wurden die von der Technischen Universit√§t Tampere (Finnland) erstellten Datens√§tze TUT Urban Acoustic Scenes 2018 und TUT Urban Acoustic Scenes 2018 Mobile f√ºr das Training vorgestellt.  Eine ausf√ºhrliche Beschreibung der Erstellung des Datensatzes sowie der Basisl√∂sung finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> . </p><br><p>  Insgesamt wurden 10 akustische Szenen f√ºr den Wettbewerb pr√§sentiert, die die Teilnehmer vorhersagen mussten. </p><br><h4 id="podzadacha-a">  Unteraufgabe A. </h4><br><p>  Wie bereits erw√§hnt, ist die Aufgabe in drei Unteraufgaben unterteilt, von denen sich jede in der Qualit√§t der Audioaufnahmen unterscheidet.  Beispielsweise wurden in Teilaufgabe A spezielle Mikrofone zur Aufzeichnung verwendet, die sich in den menschlichen Ohren befanden.  So wurde die Stereoaufnahme der menschlichen Wahrnehmung von Schall n√§her gebracht.  Die Teilnehmer hatten die M√∂glichkeit, diesen Ansatz f√ºr die Aufnahme zu verwenden, um die Erkennungsqualit√§t der akustischen Szene zu verbessern. </p><br><h4 id="podzadacha-v">  Unteraufgabe B. </h4><br><p>  In Unteraufgabe B wurden auch andere Ger√§te (z. B. Mobiltelefone) zur Aufzeichnung verwendet.  Die Daten aus Teilaufgabe A wurden in ein Monoformat konvertiert, die Abtastfrequenz wurde reduziert, es gibt keine Simulation der ‚ÄûH√∂rbarkeit‚Äú von Schall durch eine Person im Datensatz f√ºr diese Aufgabe, aber es gibt mehr Daten f√ºr das Training. </p><br><h4 id="podzadacha-s">  Unteraufgabe C. </h4><br><p>  Der Datensatz f√ºr Unteraufgabe C ist der gleiche wie in Unteraufgabe A, aber zur L√∂sung dieses Problems d√ºrfen alle externen Daten verwendet werden, die der Teilnehmer finden kann.  Das Ziel der L√∂sung dieses Problems besteht darin, herauszufinden, ob es m√∂glich ist, das in Teilaufgabe A erhaltene Ergebnis unter Verwendung von Daten von Drittanbietern zu verbessern. </p><br><p>  Die Qualit√§t der Entscheidungen auf dieser Strecke wurde anhand der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Genauigkeitsmetrik</a> bewertet. </p><br><p>  Grundlage f√ºr diese Aufgabe ist ein zweischichtiges <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungsnetzwerk,</a> das aus den Logarithmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kleiner Spektrogramme der</a> urspr√ºnglichen Audiodaten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lernt</a> .  Die vorgeschlagene Architektur verwendet die Standardtechniken BatchNormalization und Dropout.  Der Code auf GitHub ist hier zu sehen. </p><br><hr><br><h3 id="anchortrack2anchortrack-2-klassifikaciya-akusticheskih-sobytiy"><a name="Track2"></a>  Track 2. Klassifizierung von akustischen Ereignissen </h3><br><p>  In dieser Aufgabe wird vorgeschlagen, ein System zu erstellen, das akustische Ereignisse klassifiziert.  Ein solches System kann eine Erg√§nzung zu Smart Homes sein, die Sicherheit an √ºberf√ºllten Orten erh√∂hen oder Menschen mit H√∂rbehinderungen das Leben erleichtern. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ut/r5/9_/utr59_-ugehiq9jyhe_hgpgzbrm.jpeg"></div><br><p>  Das Dataset f√ºr diese Aufgabe besteht aus Dateien, die aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Freesound-Dataset</a> entnommen und mit Tags aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AudioSet</a> von Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gekennzeichnet wurden</a> .  Ausf√ºhrlicher wird der Prozess der Vorbereitung des Datensatzes in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem Artikel beschrieben</a> , der von den Organisatoren des Wettbewerbs erstellt wurde. </p><br><p>  Kehren wir zur eigentlichen Aufgabe zur√ºck, die mehrere Funktionen hat. </p><br><p>  Zun√§chst mussten die Teilnehmer ein Modell erstellen, mit dem Unterschiede zwischen akustischen Ereignissen sehr unterschiedlicher Art identifiziert werden k√∂nnen.  Der Datensatz ist in Klasse 41 unterteilt und enth√§lt verschiedene Musikinstrumente, Ger√§usche von Menschen, Tieren, Hausger√§usche und mehr. </p><br><p>  Zweitens gibt es neben dem √ºblichen Markup von Daten auch zus√§tzliche Informationen zum manuellen √úberpr√ºfen des Etiketts.  Das hei√üt, die Teilnehmer wissen, welche Dateien aus dem Datensatz von der Person auf √úbereinstimmung mit dem Etikett √ºberpr√ºft wurden und welche nicht.  Wie die Praxis gezeigt hat, haben Teilnehmer, die diese zus√§tzlichen Informationen irgendwie verwendet haben, Preise f√ºr die L√∂sung dieses Problems erhalten. </p><br><p>  Dar√ºber hinaus muss gesagt werden, dass die Dauer der Datens√§tze im Datensatz stark variiert: von 0,3 Sekunden bis 30 Sekunden.  Bei diesem Problem variiert auch die Datenmenge pro Klasse, f√ºr die das Modell trainiert werden muss, stark.  Dies wird am besten als Histogramm dargestellt, der Code f√ºr die Konstruktion, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von hier √ºbernommen wird</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/r4/gq/twr4gqwzkdivbpzkipwh6jesugi.jpeg"></div><br><p>  Wie Sie dem Histogramm entnehmen k√∂nnen, ist das manuelle Markup f√ºr die dargestellten Klassen ebenfalls unausgeglichen, was die Schwierigkeit erh√∂ht, wenn Sie diese Informationen beim Trainieren von Modellen verwenden m√∂chten. <br>  Die Ergebnisse in dieser Spur wurden unter Verwendung der Durchschnittsgenauigkeitsmetrik (Mean Mean Precision, MAP @ 3) ausgewertet. Eine ziemlich einfache Demonstration der Berechnung dieser Metrik mit Beispielen und Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . </p><br><hr><br><h3 id="anchortrack3anchortrack-3-detektirovanie-ptichego-peniya"><a name="Track3"></a>  Track 3. Erkennung von Vogelgesang </h3><br><p>  Der n√§chste Weg ist die Erkennung von Vogelgezwitscher.  Ein √§hnliches Problem tritt beispielsweise bei verschiedenen Systemen zur automatischen √úberwachung von Wildtieren auf - dies ist der erste Schritt bei der Verarbeitung von Daten vor beispielsweise der Klassifizierung.  Solche Systeme m√ºssen h√§ufig abgestimmt werden und sind gegen√ºber neuen akustischen Bedingungen instabil. Daher besteht das Ziel dieser Spur darin, die Kraft des maschinellen Lernens zur L√∂sung solcher Probleme zu nutzen. </p><br><p>  Dieser Track ist eine erweiterte Version des Wettbewerbs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûBird Audio Detection Challenge‚Äú</a> , der 2017/2018 von der St. Mary's University of London organisiert wurde.  F√ºr Interessierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">k√∂nnen</a> Sie den Artikel der Autoren des Wettbewerbs lesen, der Details zur Datenbildung, zur Organisation des Wettbewerbs selbst und zur Analyse der getroffenen Entscheidungen enth√§lt. </p><br><p>  Zur√ºck zur DCASE-Aufgabe.  Die Organisatoren stellten sechs Datens√§tze zur Verf√ºgung - drei f√ºr das Training, drei zum Testen - sie sind alle sehr unterschiedlich - aufgezeichnet unter verschiedenen akustischen Bedingungen mit verschiedenen Aufzeichnungsger√§ten, vor dem Hintergrund gibt es verschiedene Ger√§usche.  Die Hauptbotschaft lautet daher, dass das Modell nicht von der Umgebung abh√§ngen oder sich an diese anpassen kann.  Trotz der Tatsache, dass der Name ‚ÄûErkennung‚Äú bedeutet, besteht die Aufgabe nicht darin, die Grenzen des Ereignisses zu bestimmen, sondern in einer einfachen Klassifizierung - die endg√ºltige L√∂sung ist eine Art bin√§rer Klassifizierer, der einen kurzen Audioeintrag erh√§lt und entscheidet, ob ein Vogel darauf singt oder nicht .  Die AUC-Metrik wurde verwendet, um die Genauigkeit zu bewerten. </p><br><p>  Meistens versuchten die Teilnehmer, durch verschiedene Datenerweiterungen eine Verallgemeinerung und Anpassung zu erreichen.  Einer der Befehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beschreibt die</a> Anwendung verschiedener Techniken - √Ñndern der Frequenzaufl√∂sung in den extrahierten Merkmalen, vorl√§ufige Rauschreduzierung, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine Anpassungsmethode,</a> die auf der Ausrichtung von Statistiken zweiter Ordnung f√ºr verschiedene Datens√§tze basiert.  Solche Methoden sowie verschiedene Arten der Augmentation f√ºhren jedoch zu einer sehr geringen Steigerung gegen√ºber der Basisl√∂sung, wie viele Teilnehmer bemerken. </p><br><p>  Als Basisl√∂sung bereiteten die Autoren eine Modifikation der erfolgreichsten L√∂sung aus dem urspr√ºnglichen Wettbewerb ‚ÄûBird Audio Detection Challenge‚Äú vor.  Der Code ist wie gewohnt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf dem Github verf√ºgbar</a> . </p><br><hr><br><h3 id="anchortrack4anchortrack-4-detektirovanie-akusticheskih-sobytiy-v-bytovyh-usloviyah-s-ispolzovaniem-slabo-razmechennogo-nabora-dannyh"><a name="Track4"></a>  Track 4. Erkennung von akustischen Ereignissen im Haushalt anhand eines schwach beschrifteten Datensatzes. </h3><br><p>  In der vierten Spur ist das Erkennungsproblem bereits direkt gel√∂st.  Den Teilnehmern wurde ein relativ kleiner Datensatz mit markierten Daten zur Verf√ºgung gestellt - nur 1578 Audioaufnahmen von jeweils 10 Sekunden, die nur eine Klassenmarkierung aufweisen: Es ist bekannt, dass die Datei ein oder mehrere Ereignisse dieser Klassen enth√§lt, es gibt jedoch kein tempor√§res Markup.  Dar√ºber hinaus wurden zwei gro√üe Datens√§tze mit nicht zugewiesenen Daten bereitgestellt - 14412 Dateien mit Zielereignissen derselben Klasse wie in den Trainings- und Testbeispielen sowie 39999 Dateien mit beliebigen Ereignissen, die nicht in den Zielen enthalten waren.  Alle Daten sind eine Teilmenge des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">riesigen Audioset-Datensatzes, der von Google zusammengestellt wurde</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lx/td/h7/lxtdh7uaqxktdqu2bxzd4drrq0q.jpeg"></div><br><p>  Daher mussten die Teilnehmer ein Modell erstellen, das aus schwach beschrifteten Daten lernen kann, um Zeitstempel f√ºr den Beginn und das Ende von Ereignissen zu finden (Ereignisse k√∂nnen sich √ºberschneiden), und versuchen, es mit einer gro√üen Menge nicht markierter zus√§tzlicher Daten zu verbessern.  Dar√ºber hinaus ist anzumerken, dass in dieser Spur eine ziemlich starre Metrik verwendet wurde - es war notwendig, die Zeitbezeichnungen von Ereignissen mit einer Genauigkeit von 200 ms vorherzusagen.  Im Allgemeinen mussten die Teilnehmer eine ziemlich schwierige Aufgabe l√∂sen, ein angemessenes Modell zu erstellen, w√§hrend sie praktisch keine guten Daten f√ºr das Training hatten. <br>  Die meisten L√∂sungen basierten auf Faltungs-Wiederholungsnetzwerken - eine ziemlich beliebte Architektur auf dem Gebiet der Erkennung akustischer Ereignisse in letzter Zeit (ein Beispiel kann hier gelesen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> ). </p><br><p>  Die Grundl√∂sung der Autoren, auch auf faltungsrekursiven Netzwerken, basiert auf zwei Modellen.  Modelle haben fast die gleiche Architektur: drei Faltungsschichten und eine rekursive Schicht.  Der einzige Unterschied sind die Ausgangsnetzwerke.  Das erste Modell ist darauf trainiert, nicht zugewiesene Daten zu markieren, um den urspr√ºnglichen Datensatz zu erweitern. Daher sind an der Ausgabe Klassen in der Ereignisdatei vorhanden.  Die zweite dient zur direkten L√∂sung des Erkennungsproblems, dh am Ausgang erhalten wir eine tempor√§re Markierung f√ºr die Datei.  Code f√ºr den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> . </p><br><hr><br><h3 id="anchortrack5anchortrack-5-klassifikaciya-bytovoy-aktivnosti-v-pomeschenii-po-mnogokanalnoy-zapisi"><a name="Track5"></a>  Track 5. Klassifizierung der Haushaltsaktivit√§t im Raum nach Mehrkanalaufzeichnung. </h3><br><p>  Der letzte Titel unterschied sich von den anderen vor allem dadurch, dass den Teilnehmern Mehrkanalaufnahmen angeboten wurden.  Die Aufgabe selbst befand sich in der Klassifizierung: Es ist erforderlich, die Ereignisklasse vorherzusagen, die im Datensatz aufgetreten ist.  Im Gegensatz zum vorherigen Titel ist die Aufgabe etwas einfacher - es ist bekannt, dass nur ein Ereignis im Datensatz enthalten ist. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hr/l1/ud/hrl1udue-onkcjfot9kptycgcw8.jpeg"></div><br><p>  Der Datensatz wird durch ungef√§hr 200 Stunden Aufzeichnungen auf einem linearen Mikrofonarray von 4 Mikrofonen dargestellt.  Ereignisse sind alle Arten von allt√§glichen Aktivit√§ten - Kochen, Geschirr sp√ºlen, soziale Aktivit√§ten (Telefonieren, Besuchen und pers√∂nliche Gespr√§che) usw. Auch die Klasse der Abwesenheit von Ereignissen wird hervorgehoben. </p><br><p>  Die Autoren des Tracks betonen, dass die Bedingungen der Aufgabe relativ einfach sind, so dass sich die Teilnehmer direkt auf die Verwendung von r√§umlichen Informationen aus Mehrkanalaufzeichnungen konzentrieren.  Die Teilnehmer hatten auch die M√∂glichkeit, zus√§tzliche Daten und vorgefertigte Modelle zu verwenden.  Die Qualit√§t wurde nach dem F1-Ma√ü bewertet. </p><br><p>  Als grundlegende L√∂sung schlugen die Autoren des Tracks ein einfaches Faltungsnetzwerk mit zwei Faltungsschichten vor.  In ihrer L√∂sung wurden keine r√§umlichen Informationen verwendet - die Daten von vier Mikrofonen wurden f√ºr das unabh√§ngige Training verwendet, und die Vorhersagen wurden w√§hrend des Tests gemittelt.  Beschreibung und Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unter dem Link</a> . </p><br><hr><br><h3 id="zaklyuchenie">  Fazit </h3><br><p>  In dem Artikel haben wir versucht, kurz √ºber die Erkennung akustischer Ereignisse und √ºber einen Wettbewerb wie DCASE zu sprechen.  Vielleicht konnten sie jemanden f√ºr die Teilnahme im Jahr 2019 interessieren - der Wettbewerb beginnt im M√§rz. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437818/">https://habr.com/ru/post/de437818/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437808/index.html">Perf und Flammengraphen</a></li>
<li><a href="../de437810/index.html">Unternehmensrealit√§t</a></li>
<li><a href="../de437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Betas</a></li>
<li><a href="../de437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Beta-Versionen</a></li>
<li><a href="../de437816/index.html">MPLS ist √ºberall. Wie ist die Yandex.Cloud-Netzwerkinfrastruktur?</a></li>
<li><a href="../de437820/index.html">50 Farbt√∂ne Drupal-Sicherheit</a></li>
<li><a href="../de437824/index.html">Universelle 1C-Erweiterung f√ºr Google Sheets and Docs - nehmen und verwenden</a></li>
<li><a href="../de437826/index.html">Wie wir die Datenbank von Redis und Riak KV auf PostgreSQL migriert haben. Teil 1: Der Prozess</a></li>
<li><a href="../de437828/index.html">√ñffnen Sie das Webinar "SELECT-Abfrageausf√ºhrungsreihenfolge und Abfrageplan in MS SQL Server".</a></li>
<li><a href="../de437830/index.html">Zuverl√§ssige Programmierung nach Sprache - Noob Review. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>