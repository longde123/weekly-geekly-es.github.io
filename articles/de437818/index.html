<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☑️ 👩🏼‍🤝‍👨🏽 👼🏻 Wir bringen dem Computer bei, Geräusche zu unterscheiden: Lernen Sie den DCASE-Wettbewerb kennen und bauen Sie Ihren Audio-Klassifikator in 30 Minuten zusammen 🌯 🐲 🤰</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel wurde in Verbindung mit ananaskelly geschrieben . 
 Einführung 


 Hallo allerseits, Habr! In unserer Arbeit am Zentrum für Sprachtechn...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wir bringen dem Computer bei, Geräusche zu unterscheiden: Lernen Sie den DCASE-Wettbewerb kennen und bauen Sie Ihren Audio-Klassifikator in 30 Minuten zusammen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/speechpro/blog/437818/"><p> Dieser Artikel wurde in Verbindung mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">ananaskelly geschrieben</a> . </p><br><h3 id="vvedenie">  Einführung </h3><br><p>  Hallo allerseits, Habr!  In unserer Arbeit am Zentrum für Sprachtechnologie in St. Petersburg haben wir ein wenig Erfahrung in der Lösung der Probleme der Klassifizierung und Erkennung akustischer Ereignisse gesammelt und beschlossen, diese mit Ihnen zu teilen.  Der Zweck dieses Artikels ist es, Ihnen einige Aufgaben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorzustellen</a> und über den automatischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Soundverarbeitungswettbewerb DCASE 2018 zu</a> sprechen.  Wenn wir Ihnen von dem Wettbewerb erzählen, werden wir auf <u>komplexe Formeln und Definitionen im</u> Zusammenhang mit maschinellem Lernen verzichten, damit die allgemeine Bedeutung des Artikels von einem <u>breiten Publikum</u> verstanden wird. </p><br><p>  Für diejenigen, die von der <b>Versammlung</b> des <b>Klassifikators</b> angezogen wurden, haben wir einen kleinen Python-Code vorbereitet. Über den Link auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github</a> finden Sie ein Notizbuch, in dem wir am Beispiel des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweiten Titels</a> des DCASE-Wettbewerbs ein einfaches Faltungsnetzwerk auf Keras erstellen, um Audiodateien zu klassifizieren.  Dort sprechen wir ein wenig über das Netzwerk und die für das Training verwendeten Funktionen sowie über die Verwendung einer einfachen Architektur, um ein Ergebnis nahe der Basislinie zu erzielen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MAP @ 3</a> = 0,6). </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cy/xk/ct/cyxkct2xgahwpajkvi4sxzvallu.png"></div><br><p>  Zusätzlich werden hier grundlegende Ansätze zur Lösung von Problemen (Baseline) beschrieben, die von den Organisatoren vorgeschlagen wurden.  Auch in Zukunft wird es mehrere Artikel geben, in denen wir sowohl über unsere Erfahrungen bei der Teilnahme am Wettbewerb als auch über die von anderen Wettbewerbsteilnehmern vorgeschlagenen Lösungen ausführlicher und ausführlicher sprechen werden.  Links zu diesen Artikeln werden nach und nach hier angezeigt. </p><a name="habracut"></a><br><p>  Sicherlich haben viele Menschen absolut keine Ahnung von einer Art <b>„DCASE“</b> . Lassen Sie uns also herausfinden, um welche Art von Obst es sich handelt und <b>womit</b> es gegessen wird.  Der Wettbewerb „ <abbr title="Erkennung und Klassifizierung von akustischen Szenen und Ereignissen (Erkennung und Klassifizierung von akustischen Szenen und Ereignissen)">DCASE</abbr> “ findet jährlich statt. Jedes Jahr werden verschiedene Aufgaben zur Lösung von Problemen im Bereich der Klassifizierung von Audioaufnahmen und der Erkennung akustischer Ereignisse behandelt.  Jeder kann am Wettbewerb teilnehmen, es ist kostenlos, dafür reicht es aus, sich einfach als Teilnehmer auf der Seite zu registrieren.  Als Ergebnis des Wettbewerbs findet eine Konferenz zu denselben Themen statt, aber im Gegensatz zum Wettbewerb selbst ist die Teilnahme daran bereits bezahlt, und wir werden nicht mehr darüber sprechen.  Belohnungen für die besten Entscheidungen werden normalerweise nicht herangezogen, aber es gibt Ausnahmen (zum Beispiel die 3. Aufgabe im Jahr 2018).  In diesem Jahr schlugen die Organisatoren die folgenden 5 Aufgaben vor: </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierung von akustischen Szenen (unterteilt in 3 Unteraufgaben)</a> <br>  A. Auf demselben Gerät aufgezeichnete Trainings- und Testdatensätze <br>  B. Trainings- und Testdatensätze, die auf verschiedenen Geräten aufgezeichnet wurden <br>  C. Die Schulung ist unter Verwendung von Daten gestattet, die nicht vom Veranstalter bereitgestellt wurden </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierung akustischer Ereignisse</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vogelgesangserkennung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkennung von akustischen Ereignissen im Haushalt anhand eines schwach beschrifteten Datensatzes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierung der Haushaltsaktivität im Raum nach Mehrkanalaufzeichnung</a> </li></ol><br><h4 id="o-detektirovanii-i-klassifikacii">  Informationen zur Erkennung und Klassifizierung </h4><br><p>  Wie wir sehen können, enthalten die Namen aller Aufgaben eines von zwei Wörtern: "Erkennung" oder "Klassifizierung".  Lassen Sie uns klären, was der Unterschied zwischen diesen Konzepten ist, damit keine Verwirrung entsteht. </p><br><p>  Stellen Sie sich vor, wir haben eine Audioaufnahme, auf der ein Hund in einem Moment bellt und eine Katze in einem anderen miaut, und es gibt einfach keine anderen Ereignisse dort.  Wenn wir dann genau verstehen wollen, wann diese Ereignisse auftreten, müssen wir das Problem der Erkennung eines akustischen Ereignisses lösen.  Das heißt, wir müssen die Start- und Endzeiten für jede Veranstaltung herausfinden.  Nachdem wir das Erkennungsproblem gelöst haben, werden wir genau herausfinden, wann die Ereignisse auftreten, aber wir wissen nicht, von wem genau die gefundenen Geräusche erzeugt werden. Dann müssen wir das Klassifizierungsproblem lösen, dh bestimmen, was genau in dem angegebenen Zeitraum passiert ist. </p><br><p>  Um die Beschreibung der Aufgaben des Wettbewerbs zu verstehen, reichen diese Beispiele völlig aus, was bedeutet, dass der einleitende Teil abgeschlossen ist und wir mit einer detaillierten Beschreibung der Aufgaben selbst fortfahren können. </p><br><hr><br><h3 id="anchortrack1anchortrack-1-klassifikaciya-akusticheskih-scen"><a name="Track1"></a>  Track 1. Klassifizierung von akustischen Szenen </h3><br><p>  Die erste Aufgabe besteht darin, die Umgebung (akustische Szene) zu bestimmen, in der das Audio aufgenommen wurde, z. B. "Metro Station", "Airport" oder "Pedestrian Street".  Die Lösung dieses Problems kann bei der Beurteilung der Umgebung mit einem künstlichen Intelligenzsystem hilfreich sein, beispielsweise in Autos mit Autopilot. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/db/4g/ff/db4gffctu9tgvd4upbse_eaaiwg.jpeg"></div><br><p>  In dieser Aufgabe wurden die von der Technischen Universität Tampere (Finnland) erstellten Datensätze TUT Urban Acoustic Scenes 2018 und TUT Urban Acoustic Scenes 2018 Mobile für das Training vorgestellt.  Eine ausführliche Beschreibung der Erstellung des Datensatzes sowie der Basislösung finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> . </p><br><p>  Insgesamt wurden 10 akustische Szenen für den Wettbewerb präsentiert, die die Teilnehmer vorhersagen mussten. </p><br><h4 id="podzadacha-a">  Unteraufgabe A. </h4><br><p>  Wie bereits erwähnt, ist die Aufgabe in drei Unteraufgaben unterteilt, von denen sich jede in der Qualität der Audioaufnahmen unterscheidet.  Beispielsweise wurden in Teilaufgabe A spezielle Mikrofone zur Aufzeichnung verwendet, die sich in den menschlichen Ohren befanden.  So wurde die Stereoaufnahme der menschlichen Wahrnehmung von Schall näher gebracht.  Die Teilnehmer hatten die Möglichkeit, diesen Ansatz für die Aufnahme zu verwenden, um die Erkennungsqualität der akustischen Szene zu verbessern. </p><br><h4 id="podzadacha-v">  Unteraufgabe B. </h4><br><p>  In Unteraufgabe B wurden auch andere Geräte (z. B. Mobiltelefone) zur Aufzeichnung verwendet.  Die Daten aus Teilaufgabe A wurden in ein Monoformat konvertiert, die Abtastfrequenz wurde reduziert, es gibt keine Simulation der „Hörbarkeit“ von Schall durch eine Person im Datensatz für diese Aufgabe, aber es gibt mehr Daten für das Training. </p><br><h4 id="podzadacha-s">  Unteraufgabe C. </h4><br><p>  Der Datensatz für Unteraufgabe C ist der gleiche wie in Unteraufgabe A, aber zur Lösung dieses Problems dürfen alle externen Daten verwendet werden, die der Teilnehmer finden kann.  Das Ziel der Lösung dieses Problems besteht darin, herauszufinden, ob es möglich ist, das in Teilaufgabe A erhaltene Ergebnis unter Verwendung von Daten von Drittanbietern zu verbessern. </p><br><p>  Die Qualität der Entscheidungen auf dieser Strecke wurde anhand der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Genauigkeitsmetrik</a> bewertet. </p><br><p>  Grundlage für diese Aufgabe ist ein zweischichtiges <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungsnetzwerk,</a> das aus den Logarithmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kleiner Spektrogramme der</a> ursprünglichen Audiodaten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lernt</a> .  Die vorgeschlagene Architektur verwendet die Standardtechniken BatchNormalization und Dropout.  Der Code auf GitHub ist hier zu sehen. </p><br><hr><br><h3 id="anchortrack2anchortrack-2-klassifikaciya-akusticheskih-sobytiy"><a name="Track2"></a>  Track 2. Klassifizierung von akustischen Ereignissen </h3><br><p>  In dieser Aufgabe wird vorgeschlagen, ein System zu erstellen, das akustische Ereignisse klassifiziert.  Ein solches System kann eine Ergänzung zu Smart Homes sein, die Sicherheit an überfüllten Orten erhöhen oder Menschen mit Hörbehinderungen das Leben erleichtern. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ut/r5/9_/utr59_-ugehiq9jyhe_hgpgzbrm.jpeg"></div><br><p>  Das Dataset für diese Aufgabe besteht aus Dateien, die aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Freesound-Dataset</a> entnommen und mit Tags aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AudioSet</a> von Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gekennzeichnet wurden</a> .  Ausführlicher wird der Prozess der Vorbereitung des Datensatzes in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem Artikel beschrieben</a> , der von den Organisatoren des Wettbewerbs erstellt wurde. </p><br><p>  Kehren wir zur eigentlichen Aufgabe zurück, die mehrere Funktionen hat. </p><br><p>  Zunächst mussten die Teilnehmer ein Modell erstellen, mit dem Unterschiede zwischen akustischen Ereignissen sehr unterschiedlicher Art identifiziert werden können.  Der Datensatz ist in Klasse 41 unterteilt und enthält verschiedene Musikinstrumente, Geräusche von Menschen, Tieren, Hausgeräusche und mehr. </p><br><p>  Zweitens gibt es neben dem üblichen Markup von Daten auch zusätzliche Informationen zum manuellen Überprüfen des Etiketts.  Das heißt, die Teilnehmer wissen, welche Dateien aus dem Datensatz von der Person auf Übereinstimmung mit dem Etikett überprüft wurden und welche nicht.  Wie die Praxis gezeigt hat, haben Teilnehmer, die diese zusätzlichen Informationen irgendwie verwendet haben, Preise für die Lösung dieses Problems erhalten. </p><br><p>  Darüber hinaus muss gesagt werden, dass die Dauer der Datensätze im Datensatz stark variiert: von 0,3 Sekunden bis 30 Sekunden.  Bei diesem Problem variiert auch die Datenmenge pro Klasse, für die das Modell trainiert werden muss, stark.  Dies wird am besten als Histogramm dargestellt, der Code für die Konstruktion, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von hier übernommen wird</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/r4/gq/twr4gqwzkdivbpzkipwh6jesugi.jpeg"></div><br><p>  Wie Sie dem Histogramm entnehmen können, ist das manuelle Markup für die dargestellten Klassen ebenfalls unausgeglichen, was die Schwierigkeit erhöht, wenn Sie diese Informationen beim Trainieren von Modellen verwenden möchten. <br>  Die Ergebnisse in dieser Spur wurden unter Verwendung der Durchschnittsgenauigkeitsmetrik (Mean Mean Precision, MAP @ 3) ausgewertet. Eine ziemlich einfache Demonstration der Berechnung dieser Metrik mit Beispielen und Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . </p><br><hr><br><h3 id="anchortrack3anchortrack-3-detektirovanie-ptichego-peniya"><a name="Track3"></a>  Track 3. Erkennung von Vogelgesang </h3><br><p>  Der nächste Weg ist die Erkennung von Vogelgezwitscher.  Ein ähnliches Problem tritt beispielsweise bei verschiedenen Systemen zur automatischen Überwachung von Wildtieren auf - dies ist der erste Schritt bei der Verarbeitung von Daten vor beispielsweise der Klassifizierung.  Solche Systeme müssen häufig abgestimmt werden und sind gegenüber neuen akustischen Bedingungen instabil. Daher besteht das Ziel dieser Spur darin, die Kraft des maschinellen Lernens zur Lösung solcher Probleme zu nutzen. </p><br><p>  Dieser Track ist eine erweiterte Version des Wettbewerbs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„Bird Audio Detection Challenge“</a> , der 2017/2018 von der St. Mary's University of London organisiert wurde.  Für Interessierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">können</a> Sie den Artikel der Autoren des Wettbewerbs lesen, der Details zur Datenbildung, zur Organisation des Wettbewerbs selbst und zur Analyse der getroffenen Entscheidungen enthält. </p><br><p>  Zurück zur DCASE-Aufgabe.  Die Organisatoren stellten sechs Datensätze zur Verfügung - drei für das Training, drei zum Testen - sie sind alle sehr unterschiedlich - aufgezeichnet unter verschiedenen akustischen Bedingungen mit verschiedenen Aufzeichnungsgeräten, vor dem Hintergrund gibt es verschiedene Geräusche.  Die Hauptbotschaft lautet daher, dass das Modell nicht von der Umgebung abhängen oder sich an diese anpassen kann.  Trotz der Tatsache, dass der Name „Erkennung“ bedeutet, besteht die Aufgabe nicht darin, die Grenzen des Ereignisses zu bestimmen, sondern in einer einfachen Klassifizierung - die endgültige Lösung ist eine Art binärer Klassifizierer, der einen kurzen Audioeintrag erhält und entscheidet, ob ein Vogel darauf singt oder nicht .  Die AUC-Metrik wurde verwendet, um die Genauigkeit zu bewerten. </p><br><p>  Meistens versuchten die Teilnehmer, durch verschiedene Datenerweiterungen eine Verallgemeinerung und Anpassung zu erreichen.  Einer der Befehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beschreibt die</a> Anwendung verschiedener Techniken - Ändern der Frequenzauflösung in den extrahierten Merkmalen, vorläufige Rauschreduzierung, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine Anpassungsmethode,</a> die auf der Ausrichtung von Statistiken zweiter Ordnung für verschiedene Datensätze basiert.  Solche Methoden sowie verschiedene Arten der Augmentation führen jedoch zu einer sehr geringen Steigerung gegenüber der Basislösung, wie viele Teilnehmer bemerken. </p><br><p>  Als Basislösung bereiteten die Autoren eine Modifikation der erfolgreichsten Lösung aus dem ursprünglichen Wettbewerb „Bird Audio Detection Challenge“ vor.  Der Code ist wie gewohnt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf dem Github verfügbar</a> . </p><br><hr><br><h3 id="anchortrack4anchortrack-4-detektirovanie-akusticheskih-sobytiy-v-bytovyh-usloviyah-s-ispolzovaniem-slabo-razmechennogo-nabora-dannyh"><a name="Track4"></a>  Track 4. Erkennung von akustischen Ereignissen im Haushalt anhand eines schwach beschrifteten Datensatzes. </h3><br><p>  In der vierten Spur ist das Erkennungsproblem bereits direkt gelöst.  Den Teilnehmern wurde ein relativ kleiner Datensatz mit markierten Daten zur Verfügung gestellt - nur 1578 Audioaufnahmen von jeweils 10 Sekunden, die nur eine Klassenmarkierung aufweisen: Es ist bekannt, dass die Datei ein oder mehrere Ereignisse dieser Klassen enthält, es gibt jedoch kein temporäres Markup.  Darüber hinaus wurden zwei große Datensätze mit nicht zugewiesenen Daten bereitgestellt - 14412 Dateien mit Zielereignissen derselben Klasse wie in den Trainings- und Testbeispielen sowie 39999 Dateien mit beliebigen Ereignissen, die nicht in den Zielen enthalten waren.  Alle Daten sind eine Teilmenge des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">riesigen Audioset-Datensatzes, der von Google zusammengestellt wurde</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lx/td/h7/lxtdh7uaqxktdqu2bxzd4drrq0q.jpeg"></div><br><p>  Daher mussten die Teilnehmer ein Modell erstellen, das aus schwach beschrifteten Daten lernen kann, um Zeitstempel für den Beginn und das Ende von Ereignissen zu finden (Ereignisse können sich überschneiden), und versuchen, es mit einer großen Menge nicht markierter zusätzlicher Daten zu verbessern.  Darüber hinaus ist anzumerken, dass in dieser Spur eine ziemlich starre Metrik verwendet wurde - es war notwendig, die Zeitbezeichnungen von Ereignissen mit einer Genauigkeit von 200 ms vorherzusagen.  Im Allgemeinen mussten die Teilnehmer eine ziemlich schwierige Aufgabe lösen, ein angemessenes Modell zu erstellen, während sie praktisch keine guten Daten für das Training hatten. <br>  Die meisten Lösungen basierten auf Faltungs-Wiederholungsnetzwerken - eine ziemlich beliebte Architektur auf dem Gebiet der Erkennung akustischer Ereignisse in letzter Zeit (ein Beispiel kann hier gelesen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> ). </p><br><p>  Die Grundlösung der Autoren, auch auf faltungsrekursiven Netzwerken, basiert auf zwei Modellen.  Modelle haben fast die gleiche Architektur: drei Faltungsschichten und eine rekursive Schicht.  Der einzige Unterschied sind die Ausgangsnetzwerke.  Das erste Modell ist darauf trainiert, nicht zugewiesene Daten zu markieren, um den ursprünglichen Datensatz zu erweitern. Daher sind an der Ausgabe Klassen in der Ereignisdatei vorhanden.  Die zweite dient zur direkten Lösung des Erkennungsproblems, dh am Ausgang erhalten wir eine temporäre Markierung für die Datei.  Code für den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> . </p><br><hr><br><h3 id="anchortrack5anchortrack-5-klassifikaciya-bytovoy-aktivnosti-v-pomeschenii-po-mnogokanalnoy-zapisi"><a name="Track5"></a>  Track 5. Klassifizierung der Haushaltsaktivität im Raum nach Mehrkanalaufzeichnung. </h3><br><p>  Der letzte Titel unterschied sich von den anderen vor allem dadurch, dass den Teilnehmern Mehrkanalaufnahmen angeboten wurden.  Die Aufgabe selbst befand sich in der Klassifizierung: Es ist erforderlich, die Ereignisklasse vorherzusagen, die im Datensatz aufgetreten ist.  Im Gegensatz zum vorherigen Titel ist die Aufgabe etwas einfacher - es ist bekannt, dass nur ein Ereignis im Datensatz enthalten ist. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hr/l1/ud/hrl1udue-onkcjfot9kptycgcw8.jpeg"></div><br><p>  Der Datensatz wird durch ungefähr 200 Stunden Aufzeichnungen auf einem linearen Mikrofonarray von 4 Mikrofonen dargestellt.  Ereignisse sind alle Arten von alltäglichen Aktivitäten - Kochen, Geschirr spülen, soziale Aktivitäten (Telefonieren, Besuchen und persönliche Gespräche) usw. Auch die Klasse der Abwesenheit von Ereignissen wird hervorgehoben. </p><br><p>  Die Autoren des Tracks betonen, dass die Bedingungen der Aufgabe relativ einfach sind, so dass sich die Teilnehmer direkt auf die Verwendung von räumlichen Informationen aus Mehrkanalaufzeichnungen konzentrieren.  Die Teilnehmer hatten auch die Möglichkeit, zusätzliche Daten und vorgefertigte Modelle zu verwenden.  Die Qualität wurde nach dem F1-Maß bewertet. </p><br><p>  Als grundlegende Lösung schlugen die Autoren des Tracks ein einfaches Faltungsnetzwerk mit zwei Faltungsschichten vor.  In ihrer Lösung wurden keine räumlichen Informationen verwendet - die Daten von vier Mikrofonen wurden für das unabhängige Training verwendet, und die Vorhersagen wurden während des Tests gemittelt.  Beschreibung und Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unter dem Link</a> . </p><br><hr><br><h3 id="zaklyuchenie">  Fazit </h3><br><p>  In dem Artikel haben wir versucht, kurz über die Erkennung akustischer Ereignisse und über einen Wettbewerb wie DCASE zu sprechen.  Vielleicht konnten sie jemanden für die Teilnahme im Jahr 2019 interessieren - der Wettbewerb beginnt im März. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437818/">https://habr.com/ru/post/de437818/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437808/index.html">Perf und Flammengraphen</a></li>
<li><a href="../de437810/index.html">Unternehmensrealität</a></li>
<li><a href="../de437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Betas</a></li>
<li><a href="../de437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Beta-Versionen</a></li>
<li><a href="../de437816/index.html">MPLS ist überall. Wie ist die Yandex.Cloud-Netzwerkinfrastruktur?</a></li>
<li><a href="../de437820/index.html">50 Farbtöne Drupal-Sicherheit</a></li>
<li><a href="../de437824/index.html">Universelle 1C-Erweiterung für Google Sheets and Docs - nehmen und verwenden</a></li>
<li><a href="../de437826/index.html">Wie wir die Datenbank von Redis und Riak KV auf PostgreSQL migriert haben. Teil 1: Der Prozess</a></li>
<li><a href="../de437828/index.html">Öffnen Sie das Webinar "SELECT-Abfrageausführungsreihenfolge und Abfrageplan in MS SQL Server".</a></li>
<li><a href="../de437830/index.html">Zuverlässige Programmierung nach Sprache - Noob Review. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>