<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêó ü§úüèª üõÇ Fonctionnement du stockage S3 DataLine üîÑ üßúüèª üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 

 Ce n'est pas un secret que d'√©normes quantit√©s de donn√©es sont impliqu√©es dans le travail des applications modernes, et leur flux es...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Fonctionnement du stockage S3 DataLine</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dataline/blog/423853/"><img src="https://habrastorage.org/webt/r2/ri/yr/r2riyrsimmtddysut6vvaatcqtw.png"><br><br>  Bonjour, Habr! <br><br>  Ce n'est pas un secret que d'√©normes quantit√©s de donn√©es sont impliqu√©es dans le travail des applications modernes, et leur flux est en constante augmentation.  Ces donn√©es doivent √™tre stock√©es et trait√©es, souvent √† partir d'un grand nombre de machines, et ce n'est pas une t√¢che facile.  Pour le r√©soudre, il existe des magasins d'objets cloud.  Il s'agit g√©n√©ralement d'une impl√©mentation de la technologie de stockage d√©fini par logiciel. <br><br>  D√©but 2018, nous avons lanc√© (et lanc√©!) Notre propre stockage 100% compatible S3 bas√© sur Cloudian HyperStore.  Il s'est av√©r√© que le r√©seau a tr√®s peu de publications en russe sur Cloudian lui-m√™me, et encore moins sur l'application r√©elle de cette solution. <br><br>  Aujourd'hui, sur la base de l'exp√©rience de DataLine, je vais vous parler de l'architecture et de la structure interne du logiciel Cloudian, y compris la mise en ≈ìuvre Cloudian SDS bas√©e sur un certain nombre de solutions architecturales Apache Cassandra.  S√©par√©ment, nous consid√©rons le plus int√©ressant dans tout stockage SDS - la logique d'assurer la tol√©rance aux pannes et la distribution des objets. <br><br>  Si vous construisez votre stockage S3 ou √™tes occup√© √† le maintenir, cet article vous sera utile. <br><a name="habracut"></a><br>  Tout d'abord, je vais vous expliquer pourquoi notre choix s'est port√© sur Cloudian.  C'est simple: il y a tr√®s peu d'options valables dans ce cr√©neau.  Par exemple, il y a quelques ann√©es, lorsque nous pensions √† construire, il n'y avait que trois options: <br><br><ul><li>  Passerelle CEHP + RADOS; <br></li><li>  Minio <br></li><li>  Cloudian HyperStore. <br></li></ul><br>  Pour nous, en tant que fournisseur de services, les facteurs d√©cisifs ont √©t√©: un niveau √©lev√© de correspondance entre l'API de stockage et l'Amazon S3 d'origine, la disponibilit√© de la facturation int√©gr√©e, l'√©volutivit√© avec le support multir√©gionalit√© et la pr√©sence d'une troisi√®me ligne de support fournisseur.  Cloudian a tout cela. <br><br>  Et oui, le plus (certainement!) La chose la plus importante est que DataLine et Cloudian ont des couleurs d'entreprise similaires.  Vous devez admettre que nous n'avons pas pu r√©sister √† une telle beaut√©. <br><br><img src="https://habrastorage.org/webt/cj/mh/_u/cjmh_uot3g8v4spn8vgm4fvvngo.png"><br><br>  Malheureusement, Cloudian n'est pas le logiciel le plus courant, et il n'y a pratiquement aucune information √† ce sujet dans RuNet.  Aujourd'hui, nous allons corriger cette injustice et discuter avec vous des caract√©ristiques de l'architecture HyperStore, examiner ses composants les plus importants et traiter les principales nuances architecturales.  Commen√ßons par le plus √©l√©mentaire, √† savoir - qu'est-ce que Cloudian sous le capot? <br><br><h1>  Fonctionnement du stockage Cloudian HyperStore </h1><br>  Jetons un coup d'≈ìil au diagramme et voyons comment fonctionne la solution Cloudian. <br><br><img src="https://habrastorage.org/webt/8m/_n/0x/8m_n0xtx0vtlx50himh-wxrkjfm.jpeg"><br>  <i>Le sch√©ma de stockage des composants principaux.</i> <br><br>  Comme nous pouvons le voir, le syst√®me se compose de plusieurs composants principaux: <br><br><ul><li>  <b>Cloudian Management Control</b> - <i>console de gestion</i> ; </li><li>  <b>Service d'administration</b> - <i>module d'administration interne</i> ; </li><li>  <b>Service S3</b> - le <i>module responsable de la prise en charge du protocole S3</i> ; </li><li>  <b>Service HyperStore</b> - le <i>service de stockage r√©el</i> ; </li><li>  <b>Apache Cassandra</b> - un <i>r√©f√©rentiel centralis√© de donn√©es de service</i> ; </li><li>  <b>Redis</b> - <i>pour les donn√©es les plus fr√©quemment lues</i> . </li></ul><br>  Le plus grand int√©r√™t pour nous sera le travail des principaux services, le service S3 et le service HyperStore, puis nous examinerons attentivement leur travail.  Mais d'abord, il est logique de savoir comment la distribution des services dans le cluster est organis√©e et quelle est la tol√©rance aux pannes et la fiabilit√© du stockage des donn√©es de cette solution dans son ensemble. <br><br><img src="https://habrastorage.org/webt/hr/vq/su/hrvqsuhmqgexmgetlc72lsfwhqu.jpeg"><br><br><br>  Par <i>services communs</i> dans le diagramme ci-dessus, nous entendons les <b>services S3, HyperStore, CMC et Apache Cassandra</b> .  √Ä premi√®re vue, tout est beau et soign√©.  Mais en y regardant de plus pr√®s, il s'av√®re qu'une seule d√©faillance de n≈ìud unique est effectivement r√©solue.  Et la perte simultan√©e de deux n≈ìuds √† la fois peut √™tre fatale pour la disponibilit√© du cluster - Redis QoS (sur le n≈ìud 2) n'a qu'un seul esclave (sur le n≈ìud 3).  La m√™me image avec le risque de perdre la gestion du cluster - Puppet Server n'est que sur deux n≈ìuds (1 et 2).  Cependant, la probabilit√© de d√©faillance de deux n≈ìuds √† la fois est tr√®s faible et vous pouvez vivre avec. <br><br>  N√©anmoins, pour augmenter la fiabilit√© du stockage, nous utilisons 4 n≈ìuds dans la DataLine au lieu des trois minimum.  La r√©partition des ressources suivante est obtenue: <br><br><img src="https://habrastorage.org/webt/x0/ue/f2/x0uef2dubkivpngycrxidcjvx1u.png"><br><br>  Une nuance de plus frappe imm√©diatement: les <b>informations d'identification Redis</b> ne <b>sont</b> pas plac√©es sur chaque n≈ìud (comme on pourrait le supposer d'apr√®s le sch√©ma officiel ci-dessus), mais seulement sur 3 d'entre elles.  Dans ce cas, <b>Redis Credentials est</b> utilis√© pour chaque demande entrante.  Il s'av√®re qu'en raison de la n√©cessit√© d'aller au Redis de quelqu'un d'autre, il y a un certain d√©s√©quilibre dans les performances du quatri√®me n≈ìud. <br><br>  Pour nous, ce n'est pas encore significatif.  Pendant les tests de r√©sistance, des √©carts importants dans la vitesse de r√©ponse des n≈ìuds n'ont pas √©t√© remarqu√©s, mais sur de grands groupes de dizaines de n≈ìuds de travail, il est logique de corriger cette nuance. <br><br>  Voici √† quoi ressemble le sch√©ma de migration sur 6 n≈ìuds: <br><br><img src="https://habrastorage.org/webt/yc/df/1l/ycdf1lkqic3oh2rb6iu-yhyoiyo.jpeg"><br><br>  <i>Le diagramme montre comment la migration des services est impl√©ment√©e en cas de d√©faillance d'un n≈ìud.</i>  <i>Seule la panne d'un serveur de chaque r√¥le est prise en compte.</i>  <i>Si les deux serveurs tombent, une intervention manuelle sera requise.</i> <br><br>  Ici aussi, l'entreprise n'est pas sans subtilit√©s.  La migration des r√¥les utilise Puppet.  Par cons√©quent, si vous le perdez ou le cassez accidentellement, le basculement automatique peut ne pas fonctionner.  Pour la m√™me raison, vous ne devez pas modifier manuellement le manifeste de la marionnette int√©gr√©e.  Ce n'est pas enti√®rement s√ªr, les modifications peuvent √™tre subitement effiloch√©es, car les manifestes sont modifi√©s √† l'aide du panneau d'administration du cluster. <br><br>  Du point de vue de la s√©curit√© des donn√©es, tout est beaucoup plus int√©ressant.  Les m√©tadonn√©es d'objet sont stock√©es dans Apache Cassandra et chaque enregistrement est r√©pliqu√© sur 3 n≈ìuds sur 4.  Le facteur de r√©plication 3 est √©galement utilis√© pour stocker des donn√©es, mais vous pouvez en configurer un plus grand.  Cela garantit la s√©curit√© des donn√©es m√™me en cas de d√©faillance simultan√©e de 2 n≈ìuds sur 4.  Et si vous avez le temps de r√©√©quilibrer le cluster, vous ne pouvez rien perdre avec un n≈ìud restant.  L'essentiel est d'avoir suffisamment d'espace. <br><br><img src="https://habrastorage.org/webt/do/oo/ip/doooipch3gctjx0ruteyepc3n2w.jpeg"><br><br>  <i>C'est ce qui se produit lorsque deux n≈ìuds tombent en panne.</i>  <i>Le diagramme montre clairement que m√™me dans cette situation, les donn√©es restent en s√©curit√©</i> <br><br>  Dans le m√™me temps, la disponibilit√© des donn√©es et du stockage d√©pendra de la strat√©gie visant √† garantir la coh√©rence.  Pour les donn√©es, les m√©tadonn√©es, la lecture et l'√©criture, il est configur√© s√©par√©ment. <br><br>  Les options valides sont au moins un n≈ìud, un quorum ou tous les n≈ìuds. <br>  Ce param√®tre d√©termine le nombre de n≈ìuds qui doivent confirmer l'√©criture / la lecture pour que la demande soit consid√©r√©e comme r√©ussie.  Nous utilisons le quorum comme compromis raisonnable entre le temps n√©cessaire au traitement d'une demande et la fiabilit√© de l'√©criture / incoh√©rence de lecture.  Autrement dit, √† partir des trois n≈ìuds impliqu√©s dans l'op√©ration, pour un fonctionnement sans erreur, il suffit d'obtenir une r√©ponse coh√©rente de 2.  En cons√©quence, afin de rester √† flot en cas de d√©faillance de plusieurs n≈ìuds, vous devrez passer √† une seule strat√©gie d'√©criture / lecture. <br><br><h2>  Traitement des requ√™tes dans Cloudian </h2><br>  Ci-dessous, nous consid√©rerons deux sch√©mas de traitement des demandes entrantes dans Cloudian HyperStore, PUT et GET.  Il s'agit de la t√¢che principale pour S3 Service et HyperStore. <br><br>  Commen√ßons par le traitement de la demande d'√©criture: <br><br><img src="https://habrastorage.org/webt/ig/ae/g7/igaeg7v86nw9e1rgxt6xbdlhxb8.jpeg"><br><br>  Vous avez s√ªrement remarqu√© que chaque demande g√©n√®re beaucoup de v√©rifications et de r√©cup√©rations de donn√©es, au moins 6 hits d'un composant √† l'autre.  C'est √† partir d'ici que les retards d'enregistrement et la consommation √©lev√©e de temps CPU apparaissent lorsque vous travaillez avec de petits fichiers. <br><br>  Les fichiers volumineux sont transmis par morceaux.  Les blocs s√©par√©s ne sont pas consid√©r√©s comme des demandes distinctes et certains contr√¥les ne sont pas effectu√©s. <br><br>  Le n≈ìud qui a re√ßu la demande initiale d√©termine en outre ind√©pendamment o√π et quoi √©crire, m√™me s'il ne lui est pas √©crit directement.  Cela vous permet de masquer l'organisation interne du cluster du client final et d'utiliser des √©quilibreurs de charge externes.  Tout cela affecte positivement la facilit√© de maintenance et la tol√©rance aux pannes du stockage. <br><br><img src="https://habrastorage.org/webt/ss/tr/zj/sstrzjuve-nm6oyz2mj0yitmnts.jpeg"><br><br>  Comme vous pouvez le voir, la logique de lecture n'est pas trop diff√©rente de l'√©criture.  On y observe la m√™me sensibilit√© √©lev√©e de performance √† la taille des objets trait√©s.  Par cons√©quent, en raison d'√©conomies importantes dans l'utilisation des m√©tadonn√©es, il est beaucoup plus facile d'extraire un objet finement hach√© que de nombreux objets distincts du m√™me volume total. <br><br><h2>  Stockage et duplication de donn√©es </h2><br>  Comme vous pouvez le voir sur les diagrammes ci-dessus, Cloudian prend en charge divers sch√©mas de stockage et de duplication de donn√©es: <br><br>  <b>R√©plication</b> - √† l'aide de la r√©plication, il est possible de conserver un nombre personnalis√© de copies de chaque objet de donn√©es dans le syst√®me et de stocker chaque copie sur diff√©rents n≈ìuds.  Par exemple, √† l'aide de la r√©plication 3X, 3 copies de chaque objet sont cr√©√©es et chaque copie ¬´repose¬ª sur son propre n≈ìud. <br><br>  <b>Codage d'</b> effacement - Avec le codage d'effacement, chaque objet est cod√© en une quantit√© personnalis√©e (connue sous le nom de num√©ro K) de fragments de donn√©es plus une quantit√© personnalis√©e de code de redondance (num√©ro M).  Chaque fragment K + M d'un objet est unique et chaque fragment est stock√© sur son propre n≈ìud.  Un objet peut √™tre d√©cod√© √† l'aide de n'importe quels fragments K.  En d'autres termes, l'objet reste lisible, m√™me si M n≈ìuds sont inaccessibles. <br><br>  Par exemple, dans le codage d'effacement, selon la formule 4 + 2 (4 fragments de donn√©es plus 2 fragments de code de redondance), chaque objet est divis√© en 6 fragments uniques stock√©s sur six n≈ìuds diff√©rents, et cet objet peut √™tre restaur√© et lu si 4 fragments sur 6 sont disponibles . <br><br>  L'avantage du codage d'effacement par rapport √† la r√©plication est d'√©conomiser de l'espace, mais au prix d'une augmentation significative de la charge du processeur, d'une d√©t√©rioration de la vitesse de r√©ponse et de la n√©cessit√© de proc√©dures d'arri√®re-plan pour contr√¥ler la coh√©rence des objets.  Dans tous les cas, les m√©tadonn√©es sont stock√©es s√©par√©ment des donn√©es (dans Apache Cassandra), ce qui augmente la flexibilit√© et la fiabilit√© de la solution. <br><br><h2>  En bref sur les autres fonctions d'HyperStore </h2><br>  Comme je l'ai √©crit au d√©but de cet article, plusieurs outils utiles sont int√©gr√©s √† HyperStore.  Parmi eux: <br><br><ul><li>  Facturation flexible avec prise en charge de la modification du prix d'une ressource en fonction du volume et du plan tarifaire; <br></li><li>  Surveillance int√©gr√©e <br></li><li>  La capacit√© de limiter l'utilisation des ressources pour les utilisateurs et les groupes d'utilisateurs; <br></li><li>  Param√®tres QoS et proc√©dures int√©gr√©es pour √©quilibrer l'utilisation des ressources entre les n≈ìuds, ainsi que les proc√©dures r√©guli√®res de r√©√©quilibrage entre les n≈ìuds et les disques sur les n≈ìuds ou lors de la saisie de nouveaux n≈ìuds dans un cluster. <br></li></ul><br>  Cependant, Cloudian HyperStore n'est toujours pas parfait.  Par exemple, pour une raison quelconque, vous ne pouvez pas transf√©rer un compte existant vers un autre groupe ou affecter plusieurs groupes √† un enregistrement.  Il n'est pas possible de g√©n√©rer des rapports de facturation interm√©diaires - vous ne recevrez tous les rapports qu'apr√®s la cl√¥ture de la p√©riode de rapport.  Par cons√©quent, ni les clients ni nous ne pouvons savoir combien le compte a augment√© en temps r√©el. <br><br><h1>  Cloudian HyperStore Logic </h1><br>  Maintenant, nous allons plonger encore plus profond√©ment et regarder le plus int√©ressant de tout stockage SDS - la logique de la distribution des objets par n≈ìuds.  Dans le cas du stockage Cloudian, les m√©tadonn√©es sont stock√©es s√©par√©ment des donn√©es elles-m√™mes.  Pour les m√©tadonn√©es, Cassandra est utilis√©e, pour les donn√©es, la solution propri√©taire HyperStore. <br><br>  Malheureusement, jusqu'√† pr√©sent, il n'y a pas de traduction officielle de la documentation de Cloudian en russe sur Internet, donc ci-dessous je publierai ma traduction des parties les plus int√©ressantes de cette documentation. <br><br><h2>  Le r√¥le d'Apache Cassandra dans HyperStore </h2><br>  Dans HyperStore, Cassandra est utilis√©e pour stocker les m√©tadonn√©es d'objet, les informations de compte d'utilisateur et les donn√©es d'utilisation des services.  Dans un d√©ploiement typique sur chaque HyperStore, les donn√©es Cassandra sont stock√©es sur le m√™me lecteur que le syst√®me d'exploitation.  Le syst√®me prend √©galement en charge les donn√©es Cassandra sur un lecteur d√©di√© sur chaque n≈ìud.  Les donn√©es Cassandra ne sont pas stock√©es sur des disques de donn√©es HyperStore.  Lorsque des vNodes sont attribu√©s √† l'h√¥te, ils sont distribu√©s uniquement aux n≈ìuds de stockage HyperStore.  Les vNodes ne sont pas allou√©s au lecteur sur lequel les donn√©es Cassandra sont stock√©es. <br>  √Ä l'int√©rieur du cluster, les m√©tadonn√©es de Cassandra sont r√©pliqu√©es conform√©ment √† la strat√©gie (strat√©gie) de votre r√©f√©rentiel.  Cassandra Data Replication utilise les vNodes de cette fa√ßon: <br><br><ul><li>  Lors de la cr√©ation d'un nouvel objet Cassandra (cl√© primaire et ses valeurs correspondantes), il est hach√© et le hachage est utilis√© pour associer l'objet √† un vNode sp√©cifique.  Le syst√®me v√©rifie √† quel h√¥te ce vNode est affect√©, puis la premi√®re r√©plique de l'objet Cassandra est stock√©e sur le lecteur Cassandra sur cet h√¥te. <br></li><li>  Par exemple, supposons qu'un h√¥te se voit attribuer 96 vNodes r√©partis sur plusieurs disques de donn√©es HyperStore.  Les objets Cassandra dont les valeurs de hachage se situent dans les plages de jetons de l'un de ces 96 vNodes seront √©crits sur le lecteur Cassandra sur cet h√¥te. <br></li><li>  Des r√©pliques suppl√©mentaires de l'objet Cassandra (le nombre de r√©pliques d√©pend de votre configuration) sont associ√©es aux vNodes avec le num√©ro de s√©quence suivant et stock√©es sur le n≈ìud auquel ces vNodes sont affect√©s, √† condition que les vNodes soient ignor√©s si n√©cessaire, de sorte que chaque r√©plique de l'objet Cassandra soit stock√©e sur un autre machine h√¥te. <br></li></ul><br><h2>  Fonctionnement du stockage HyperStore </h2><br>  Le placement et la r√©plication des objets S3 dans un cluster HyperStore sont bas√©s sur un sch√©ma de mise en cache coh√©rent qui utilise un espace de jeton entier compris entre 0 et 2 <sup>127</sup> -1.  Les jetons entiers sont affect√©s aux n≈ìuds HyperStore.  Pour chaque objet S3, un hachage est calcul√© lors de son chargement dans le stockage.  L'objet est stock√© dans le n≈ìud auquel la valeur la plus faible du jeton a √©t√© affect√©e, sup√©rieure ou √©gale √† la valeur de hachage de l'objet.  La r√©plication est √©galement impl√©ment√©e en stockant l'objet sur les n≈ìuds auxquels des jetons ont √©t√© attribu√©s, qui ont une valeur minimale. <br><br>  Dans un stockage bas√© sur le hachage coh√©rent ¬´classique¬ª, un jeton est attribu√© √† un n≈ìud physique.  Le syst√®me Cloudian HyperStore utilise et √©tend les fonctionnalit√©s du ¬´n≈ìud virtuel¬ª (vNode) introduit dans Cassandra dans la version 1.2 - un grand nombre de jetons sont attribu√©s √† chaque h√¥te physique (256 au maximum).  En fait, le cluster de stockage se compose d'un tr√®s grand nombre de ¬´n≈ìuds virtuels¬ª avec un grand nombre de n≈ìuds virtuels (jetons) sur chaque h√¥te physique. <br><br>  Le syst√®me HyperStore attribue un ensemble distinct de jetons (n≈ìuds virtuels) √† chaque disque sur chaque h√¥te physique.  Chaque disque sur l'h√¥te est responsable de son propre ensemble de r√©pliques d'objets.  Une d√©faillance de disque affecte uniquement les r√©pliques d'objets qui s'y trouvent.  Les autres lecteurs de l'h√¥te continueront de fonctionner et s'acquitteront de leurs responsabilit√©s de stockage de donn√©es. <br><br>  Nous donnons un exemple et consid√©rons un cluster de 6 h√¥tes HyperStore, chacun ayant 4 disques de stockage S3.  Supposons que 32 jetons sont attribu√©s √† chaque h√¥te physique et qu'il y ait un espace de jeton simplifi√© de 0 √† 960, et la valeur de 192 jetons dans ce syst√®me (6 h√¥tes de 32 jetons) est de 0, 5, 10, 15, 20, etc. jusqu'√† 955. <br><br>  Le diagramme ci-dessous montre une distribution possible de jetons √† travers le cluster.  32 jetons de chaque h√¥te sont r√©partis uniform√©ment sur 4 disques (8 jetons par disque), et les jetons eux-m√™mes sont r√©partis de mani√®re al√©atoire sur le cluster. <br><br><img src="https://habrastorage.org/webt/wa/w2/9c/waw29ckv34avc3fdmqvfq4-a40a.jpeg"><br><br>  Supposons maintenant que vous ayez configur√© HyperStore sur des objets S3 r√©pliqu√©s 3X.  Soyons d'accord que l'objet S3 est charg√© dans le syst√®me, et l'algorithme de hachage appliqu√© √† son nom nous donne la valeur de hachage 322 (dans cet espace de hachage simplifi√©).  Le diagramme ci-dessous montre comment trois instances ou r√©pliques d'un objet seront stock√©es dans un cluster: <br><br><ul><li>  Avec sa valeur de hachage de nom 322, la premi√®re r√©plique de l'objet est stock√©e sur 325 jetons, car  il s'agit de la plus petite valeur de jeton sup√©rieure ou √©gale √† la valeur de hachage de l'objet.  325 jetons (surlign√©s en rouge sur le diagramme) sont affect√©s √† hyperstore2: Disk2.  En cons√©quence, la premi√®re r√©plique de l'objet y est stock√©e. <br></li></ul><br><ul><li>  La deuxi√®me r√©plique est stock√©e sur le disque auquel est affect√© le jeton suivant (330, surlign√© en orange), c'est-√†-dire sur l'hyperstore4: Disk2. <br></li><li>  La troisi√®me r√©plique est enregistr√©e sur le disque, √† laquelle est attribu√© le jeton suivant apr√®s 330 - 335 (jaune), sur l'hyperstore3: Disk3. <br></li></ul><br><img src="https://habrastorage.org/webt/xy/0w/k-/xy0wk-hrqt3lgppbyeohdpllsnq.jpeg"><br><blockquote>  <b>J'ajouterai un commentaire:</b> d'un point de vue pratique, cette optimisation (la distribution des jetons non seulement entre les n≈ìuds physiques, mais aussi entre les disques individuels) est n√©cessaire non seulement pour assurer l'accessibilit√©, mais aussi pour assurer une distribution uniforme des donn√©es entre les disques.  Dans ce cas, la matrice RAID n'est pas utilis√©e, toute la logique d'allocation des donn√©es sur les disques est contr√¥l√©e par HyperStore lui-m√™me.  D'une part, il est pratique et contr√¥l√©; si un disque est perdu, tout sera r√©√©quilibr√© ind√©pendamment.  D'un autre c√¥t√©, je fais personnellement confiance √† de meilleurs contr√¥leurs RAID - apr√®s tout, leur logique est optimis√©e depuis de nombreuses ann√©es.  Mais ce sont toutes mes pr√©f√©rences personnelles, nous n'avons jamais attrap√© HyperStore sur de vraies √©coles et probl√®mes, si nous suivons les recommandations du fournisseur lors de l'installation de logiciels sur des serveurs physiques.  Mais la tentative d'utiliser la virtualisation et les disques virtuels au-dessus de la m√™me lune sur le syst√®me de stockage a √©chou√©, lors de la surcharge du syst√®me de stockage pendant les tests de charge, HyperStore est devenu fou et a dispers√© les donn√©es de mani√®re totalement in√©gale, obstruant certains disques et n'en touchant pas d'autres. </blockquote><h2>  Conduire le p√©riph√©rique √† l'int√©rieur d'un cluster </h2><br>  Rappelez-vous que chaque h√¥te poss√®de 32 jetons et que les jetons de chaque h√¥te sont r√©partis uniform√©ment entre ses disques.  Examinons de plus pr√®s l'hyperstore2: Disk2 (dans le diagramme ci-dessous).  Nous voyons que les jetons 325, 425, 370 et ainsi de suite sont affect√©s √† ce disque. <br><br>  √âtant donn√© que le cluster est configur√© pour la r√©plication 3X, les √©l√©ments suivants seront stock√©s sur hyperstore2: Disk2: <br><br>  Conform√©ment √† 325 jetons de disque: <br><ul><li>  Les premi√®res r√©pliques d'objets avec une valeur de hachage de 320 (exclusivement) √† 325 (inclus); </li><li>  Deuxi√®mes r√©pliques d'objets avec une valeur de hachage de 315 (exclusivement) √† 320 (inclus); </li><li>  Troisi√®mes r√©pliques d'objets avec une valeur de hachage de 310 (exclusivement) √† 315 (inclus). </li></ul><br>  Selon le jeton de disque 425: <br><ul><li>  Les premi√®res r√©pliques d'objets avec une valeur de hachage de 420 (exclusivement) √† 425 (inclus); </li><li>  Deuxi√®mes r√©pliques d'objets avec une valeur de hachage de 415 (exclusivement) √† 420 (inclus); </li><li>  Troisi√®mes r√©pliques d'objets avec une valeur de hachage de 410 (exclusivement) √† 415 (inclus). </li></ul><br>  Et ainsi de suite. <br><br>  Comme indiqu√© pr√©c√©demment, lors du placement des deuxi√®me et troisi√®me r√©pliques, HyperStore peut dans certains cas transmettre des jetons afin de ne pas stocker plus d'une copie de l'objet sur un n≈ìud physique.  Cela √©limine l'utilisation d'hyperstore2: disk2 comme stockage pour les deuxi√®me ou troisi√®me r√©pliques du m√™me objet. <br><br><img src="https://habrastorage.org/webt/hl/rf/dq/hlrfdqwtqhiuwvdrrplv0mxt8dk.jpeg"><br><br>  Si le disque 2 se bloque sur les disques 1, 3 et 4, les donn√©es continueront d'√™tre stock√©es et les objets sur le disque 2 seront stock√©s dans le cluster, car  ont √©t√© r√©pliqu√©s sur d'autres h√¥tes. <br><blockquote>  <b>Commentaire:</b> par cons√©quent, la distribution de r√©pliques et / ou de fragments d'objets dans le cluster HyperStore est bas√©e sur la conception de Cassandra, qui a √©t√© d√©velopp√©e pour les besoins du stockage de fichiers.  ,    ,       ,     ,   ¬´¬ª  .          .                     .  ,      :        ,    ,     . </blockquote><h2>      </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voyons maintenant comment fonctionne HyperStore dans plusieurs centres de donn√©es et r√©gions. Dans notre cas, le mode multi-DPC diff√®re du mode multi-r√©gional en utilisant un ou plusieurs espaces de jetons. Dans le premier cas, l'espace des jetons est uniforme. Dans la seconde, chaque r√©gion aura un espace de jeton ind√©pendant avec (potentiellement) ses propres param√®tres pour le niveau de coh√©rence, la capacit√© et les configurations de stockage. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour comprendre comment cela fonctionne, revenons √† la traduction de la documentation, section </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬´D√©ploiements de centres de donn√©es multiples¬ª.</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Envisagez de d√©ployer HyperStore dans deux centres de donn√©es. Appelez-les DC1 et DC2. Chaque centre de donn√©es dispose de 3 n≈ìuds physiques. Comme dans nos exemples pr√©c√©dents, chaque n≈ìud physique poss√®de quatre disques, 32 jetons (vNodes) sont attribu√©s √† chaque h√¥te, et nous supposons un espace de jeton simplifi√© de 0 √† 960. Selon ce sc√©nario avec plusieurs centres de donn√©es, l'espace de jeton est divis√© en 192 jetons - 32 jetons pour chacun des 6 h√¥tes physiques. Les jetons sont distribu√©s par les h√¥tes de mani√®re absolument al√©atoire. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supposons √©galement que la r√©plication des objets S3 dans ce cas est configur√©e sur deux r√©pliques dans chaque centre de donn√©es. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voyons comment un objet S3 hypoth√©tique avec une valeur de hachage de 942 se r√©pliquera dans 2 centres de donn√©es:</font></font><br><br><ul><li>     vNode 945 (     ),    DC2,  hyperstore5:Disk3. <br></li><li>     vNode 950 (  ) DC2,  hyperstore6:Disk4. <br></li><li>  vNode 955   DC2,      ,   vNode . <br></li><li>     vNode 0 () ‚Äî  DC1, hyperstore2:Disk3.  ,       (955)       (0). <br></li><li>  vNode (5)   DC2,      ,   vNode . <br></li><li>       vNode 10 () ‚Äî  DC1, hyperstore3:Disk3. <br></li></ul><br><img src="https://habrastorage.org/webt/uu/vl/ak/uuvlakjhabsho_u_swahrrf8cz4.png"><br><blockquote> <b>:</b>        ,      ,   ,  ,           .     ,      . </blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ceci conclut notre pr√©sentation de l'architecture Cloudian et des fonctionnalit√©s cl√©s. </font><font style="vertical-align: inherit;">Quoi qu'il en soit, ce sujet est trop s√©rieux et trop vaste pour int√©grer le manuel exhaustif qu'il contient dans un article sur Habr√©. </font><font style="vertical-align: inherit;">Par cons√©quent, si vous √™tes int√©ress√© par les d√©tails que j'ai omis, vous avez des questions ou des suggestions pour la pr√©sentation du mat√©riel dans les prochains articles, je me ferai un plaisir de communiquer avec vous dans les commentaires. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans le prochain article, nous consid√©rerons l'impl√©mentation du stockage S3 dans DataLine, nous parlerons en d√©tail des technologies d'infrastructure et de tol√©rance aux pannes de r√©seau utilis√©es, et en bonus, je vous raconterai l'histoire de sa construction!</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr423853/">https://habr.com/ru/post/fr423853/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr423839/index.html">test sha256 par toothOK pour le r√©seau neuronal</a></li>
<li><a href="../fr423843/index.html">Si vous √™tes √† Kazan ou √† Novossibirsk et que vous souhaitez concevoir des micropuces, comme √† Cupertino</a></li>
<li><a href="../fr423845/index.html">Pr√©servatif corporatif</a></li>
<li><a href="../fr423847/index.html">Reconnaissance des couleurs et de la lumi√®re avec APDS-9960</a></li>
<li><a href="../fr423851/index.html">Pr√©sentation du nouveau plugin de Grafana - panneau Statusmap</a></li>
<li><a href="../fr423855/index.html">N√©buleuse Zyxel - la facilit√© de gestion comme base d'√©conomie</a></li>
<li><a href="../fr423857/index.html">6 d√©fis que vous rencontrerez lors de l'apprentissage de la programmation vous-m√™me</a></li>
<li><a href="../fr423861/index.html">Lanternes solaires - nous avons besoin de plus de lumi√®re</a></li>
<li><a href="../fr423863/index.html">Confrontation aux PHDays 8 - SOC View</a></li>
<li><a href="../fr423865/index.html">Roskomnadzor a rendu public</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>