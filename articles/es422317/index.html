<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚ÄçüöÄ üëí üëÑ ¬øPor qu√© los TPU son tan buenos para el aprendizaje profundo? üèöÔ∏è üë∏üèø üåã</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Procesador tensor de tercera generaci√≥n 

 Google Tensor Processor es un circuito integrado de prop√≥sito especial ( ASIC ) desarrollado desde cero por...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øPor qu√© los TPU son tan buenos para el aprendizaje profundo?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422317/"><img src="https://habrastorage.org/webt/hc/p7/cd/hcp7cda1npc6ylbq16nwwcsyxd4.jpeg"><br>  <i>Procesador tensor de tercera generaci√≥n</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Google Tensor Processor</a> es un circuito integrado de prop√≥sito especial ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ASIC</a> ) desarrollado desde cero por Google para realizar tareas de aprendizaje autom√°tico.  Trabaja en varios productos importantes de Google, incluidos Translate, Photos, Search Assistant y Gmail.  Cloud TPU ofrece los beneficios de escalabilidad y facilidad de uso a todos los desarrolladores y cient√≠ficos de datos que lanzan modelos de aprendizaje autom√°tico de vanguardia en Google Cloud.  En Google Next '18, anunciamos que Cloud TPU v2 ahora est√° disponible para todos los usuarios, incluidas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las cuentas de prueba gratuitas</a> , y Cloud TPU v3 est√° disponible para pruebas alfa. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/f34/b73/cee/f34b73ceecf379f47dd446f0cc8b1a7c.jpg"><br><br>  Pero mucha gente pregunta: ¬øcu√°l es la diferencia entre CPU, GPU y TPU?  Creamos un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sitio de demostraci√≥n</a> donde se encuentran la presentaci√≥n y la animaci√≥n que responden a esta pregunta.  En esta publicaci√≥n, me gustar√≠a hacer hincapi√© en ciertas caracter√≠sticas del contenido de este sitio. <br><br><h2>  ¬øC√≥mo funcionan las redes neuronales? </h2><br>  Antes de comenzar a comparar la CPU, la GPU y la TPU, veamos qu√© tipo de c√°lculos se requieren para el aprendizaje autom√°tico, y espec√≠ficamente, para las redes neuronales. <br><br>  Imagine, por ejemplo, que usamos una red neuronal de una sola capa para reconocer n√∫meros escritos a mano, como se muestra en el siguiente diagrama: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d6/bf4/585/2d6bf4585be28678f66652fd77ecb2f8.png"><br><br>  Si la imagen es una cuadr√≠cula de 28x28 p√≠xeles en escala de grises, se puede convertir en un vector de 784 valores (medidas).  Una neurona que reconoce el n√∫mero 8 toma estos valores y los multiplica con los valores de los par√°metros (l√≠neas rojas en el diagrama). <br><br>  El par√°metro funciona como un filtro, extrayendo caracter√≠sticas de los datos que indican la similitud de la imagen y la forma 8: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/141/559/0201415596ab1132ba07a3b430a2fa34.gif"><br><br>  Esta es la explicaci√≥n m√°s simple de la clasificaci√≥n de datos por redes neuronales.  Multiplicaci√≥n de datos con los par√°metros correspondientes a ellos (coloraci√≥n de puntos) y su suma (suma de puntos a la derecha).  El resultado m√°s alto indica la mejor coincidencia entre los datos ingresados ‚Äã‚Äãy el par√°metro correspondiente, que, lo m√°s probable, ser√° la respuesta correcta. <br><br>  En pocas palabras, las redes neuronales necesitan hacer una gran cantidad de multiplicaciones y adiciones de datos y par√°metros.  A menudo los organizamos en forma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">multiplicaci√≥n matricial</a> , que puedes encontrar en √°lgebra en la escuela.  Por lo tanto, el problema es realizar una gran cantidad de multiplicaciones de matrices lo m√°s r√°pido posible, gastando la menor energ√≠a posible. <br><br><h2>  ¬øC√≥mo funciona una CPU? </h2><br>  ¬øC√≥mo aborda la CPU esta tarea?  La CPU es un procesador de uso general basado en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">arquitectura von Neumann</a> .  Esto significa que la CPU funciona con software y memoria de la siguiente manera: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/963/029/620/96302962031a0a23ecf34b868d194161.gif"><br><br>  La principal ventaja de la CPU es la flexibilidad.  Gracias a la arquitectura von Neumann, puede descargar software completamente diferente para millones de prop√≥sitos diferentes.  La CPU se puede usar para procesamiento de texto, control de motores de cohetes, transacciones bancarias, clasificaci√≥n de im√°genes utilizando una red neuronal. <br><br>  Pero como la CPU es tan flexible, el equipo no siempre sabe de antemano cu√°l ser√° la pr√≥xima operaci√≥n hasta que lea las siguientes instrucciones del software.  La CPU necesita almacenar los resultados de cada c√°lculo en la memoria ubicada dentro de la CPU (los llamados registros o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cach√© L1</a> ).  El acceso a esta memoria se convierte en una desventaja de la arquitectura de la CPU, conocida como el cuello de botella de la arquitectura von Neumann.  Y aunque una gran cantidad de c√°lculos para redes neuronales hace que los pasos futuros sean predecibles, cada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dispositivo de l√≥gica aritm√©tica de la</a> CPU (ALU, un componente que almacena y controla multiplicadores y sumadores) realiza operaciones secuencialmente, accediendo a la memoria cada vez, lo que limita el rendimiento general y consume una cantidad significativa de energ√≠a . <br><br><h2>  C√≥mo funciona la GPU </h2><br>  Para aumentar el rendimiento en comparaci√≥n con la CPU, la GPU utiliza una estrategia simple: ¬øpor qu√© no integrar miles de ALU en el procesador?  La GPU moderna contiene alrededor de 2500 - 5000 ALU en el procesador, lo que hace posible realizar miles de multiplicaciones y adiciones a la vez. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbb/bcb/17b/fbbbcb17b7732e20d2658d5e76023beb.gif"><br><br>  Dicha arquitectura funciona bien con aplicaciones que requieren paralelizaci√≥n masiva, como, por ejemplo, la multiplicaci√≥n de matrices en una red neuronal.  Con una carga de entrenamiento t√≠pica de aprendizaje profundo (GO), el rendimiento en este caso aumenta en un orden de magnitud en comparaci√≥n con la CPU.  Por lo tanto, hoy la GPU es la arquitectura de procesador m√°s popular para GO. <br><br>  Pero la GPU sigue siendo un procesador de uso general que debe admitir un mill√≥n de aplicaciones y software diferentes.  Y esto nos lleva de vuelta al problema fundamental del cuello de botella de la arquitectura von Neumann.  Para cada c√°lculo en miles de ALU, GPU, es necesario recurrir a registros o memoria compartida para leer y guardar resultados de c√°lculo intermedios.  Debido a que la GPU realiza m√°s c√≥mputo paralelo en miles de sus ALU, tambi√©n gasta proporcionalmente m√°s energ√≠a en el acceso a la memoria y ocupa un √°rea grande. <br><br><h2>  ¬øC√≥mo funciona TPU? </h2><br>  Cuando desarrollamos TPU en Google, creamos una arquitectura dise√±ada para una tarea espec√≠fica.  En lugar de desarrollar un procesador de uso general, desarrollamos un procesador matricial especializado para trabajar con redes neuronales.  TPU no podr√° trabajar con un procesador de texto, controlar motores de cohetes o realizar transacciones bancarias, pero puede procesar una gran cantidad de multiplicaciones y adiciones para redes neuronales a una velocidad incre√≠ble, mientras consume mucha menos energ√≠a y se adapta a un volumen f√≠sico m√°s peque√±o. <br><br>  Lo principal que le permite hacer esto es la eliminaci√≥n radical del cuello de botella de la arquitectura von Neumann.  Dado que la tarea principal de TPU es el procesamiento matricial, los desarrolladores de circuitos estaban familiarizados con todos los pasos de c√°lculo necesarios.  Por lo tanto, pudieron colocar miles de multiplicadores y sumadores, y conectarlos f√≠sicamente, formando una gran matriz f√≠sica.  Esto se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">arquitectura de matriz canalizada</a> .  En el caso de Cloud TPU v2, se utilizan dos matrices de canalizaci√≥n de 128 x 128, lo que en total da 32.768 ALU para valores de punto flotante de 16 bits en un procesador. <br><br>  Veamos c√≥mo una matriz canalizada realiza c√°lculos para una red neuronal.  Primero, el TPU carga los par√°metros de la memoria en una matriz de multiplicadores y sumadores. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ec/de3/fc6/9ecde3fc6d69116db89aacd83bdf15e5.gif"><br><br>  El TPU luego carga los datos de la memoria.  Al completar cada multiplicaci√≥n, el resultado se transmite a los siguientes factores, mientras se realizan las sumas.  Por lo tanto, la salida ser√° la suma de todas las multiplicaciones de los datos y par√°metros.  A lo largo del proceso de computaci√≥n volum√©trica y transferencia de datos, el acceso a la memoria es completamente innecesario. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/04a/ef8/b31/04aef8b31b8eb550ba093df4eb811d58.gif"><br><br>  Por lo tanto, TPU demuestra un mayor rendimiento al calcular redes neuronales, consume mucha menos energ√≠a y ocupa menos espacio. <br><br><h2>  Ventaja: 5 veces menos costo </h2><br>  ¬øCu√°les son los beneficios de la arquitectura TPU?  Costo  Aqu√≠ est√° el costo de Cloud TPU v2 para agosto de 2018, en el momento de escribir esto: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e2f/340/d86/e2f340d861ef00ac9ceac0e7d6dc5f24.png"><br>  Costo de trabajo normal y de TPU para diferentes regiones de Google Cloud <br><br>  La Universidad de Stanford est√° distribuyendo un conjunto de pruebas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DAWNBench</a> que miden el rendimiento de los sistemas de aprendizaje profundo.  All√≠ puede ver varias combinaciones de tareas, modelos y plataformas inform√°ticas, as√≠ como los resultados de las pruebas correspondientes. <br><br>  Al final de la competencia, en abril de 2018, el costo m√≠nimo de capacitaci√≥n en procesadores con arquitectura distinta de TPU era de $ 72.40 (para la capacitaci√≥n de ResNet-50 con 93% de precisi√≥n en ImageNet en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">instancias puntuales</a> ).  Con Cloud TPU v2, esta capacitaci√≥n se puede realizar por $ 12.87.  Esto es menos de 1/5 del costo.  Tal es el poder de la arquitectura dise√±ada espec√≠ficamente para redes neuronales. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es422317/">https://habr.com/ru/post/es422317/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es422303/index.html">Best SQL Builder: use jOOQ en Android</a></li>
<li><a href="../es422305/index.html">Distribuci√≥n del n√∫mero de trabajadores rusos por salario basado en una gran encuesta en l√≠nea en una plataforma no especializada</a></li>
<li><a href="../es422309/index.html">C√≥mo proteger los datos en redes neuronales en la nube: se propone un nuevo m√©todo de cifrado</a></li>
<li><a href="../es422311/index.html">Interesante y utilidad de python. Parte 2</a></li>
<li><a href="../es422315/index.html">C√≥mo sobrevivir a un cazador de insectos: lucha diaria por los ingresos</a></li>
<li><a href="../es422319/index.html">Por primera vez, el equipo ruso se meti√≥ en el acelerador cient√≠fico m√°s grande IndieBio</a></li>
<li><a href="../es422321/index.html">Optimizaci√≥n del trabajo con prototipos en motores JavaScript.</a></li>
<li><a href="../es422323/index.html">Hackers: Rusia y China</a></li>
<li><a href="../es422325/index.html">DevDay sobre pruebas: Rel√°jate. Pru√©balo f√°cil</a></li>
<li><a href="../es422327/index.html">Cronograma del proyecto vs Backlog: Batalla sin posibilidades</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>