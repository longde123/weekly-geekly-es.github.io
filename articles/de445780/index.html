<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÆ üå≥ ‚ò¶Ô∏è Ensemble-Methoden. Auszug aus dem Buch ü§πüèΩ ‚õπüèª üë®üèª‚Äçüíª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Khabrozhiteli, wir haben der Druckerei ein neues Buch ‚ÄûMaschinelles Lernen: Algorithmen f√ºr Unternehmen‚Äú √ºbergeben . Hier ist ein Auszug √ºber En...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ensemble-Methoden. Auszug aus dem Buch</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/445780/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/vk/fr/zn/vkfrzn9ctkjsd2wjx8puqifp980.jpeg" alt="Bild"></a> <br><br>  Hallo Khabrozhiteli, wir haben der Druckerei ein neues Buch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûMaschinelles Lernen: Algorithmen f√ºr Unternehmen‚Äú √ºbergeben</a> .  Hier ist ein Auszug √ºber Ensemble-Methoden. Sie sollen erkl√§ren, was sie effektiv macht und wie h√§ufige Fehler vermieden werden k√∂nnen, die zu ihrem Missbrauch im Finanzbereich f√ºhren. <br><a name="habracut"></a><br><h3>  6.2.  Drei Fehlerquellen </h3><br>  MO-Modelle leiden normalerweise unter drei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fehlern</a> . <br><br>  1. Bias: Dieser Fehler wird durch unrealistische Annahmen verursacht.  Wenn die Verzerrung hoch ist, bedeutet dies, dass der MO-Algorithmus die wichtigen Beziehungen zwischen den Merkmalen und Ergebnissen nicht erkennen konnte.  In dieser Situation wird gesagt, dass der Algorithmus "nicht genehmigt" ist. <br><br>  2. Dispersion: Dieser Fehler wird durch die Empfindlichkeit gegen√ºber kleinen √Ñnderungen in der Trainingsuntermenge verursacht.  Wenn die Varianz hoch ist, bedeutet dies, dass der Algorithmus neu auf die Trainingsuntermenge ausgerichtet wird und daher selbst minimale √Ñnderungen in der Trainingsuntermenge schrecklich unterschiedliche Vorhersagen erzeugen k√∂nnen.  Anstatt allgemeine Muster in einer Trainingsuntermenge zu modellieren, nimmt der Algorithmus f√§lschlicherweise Rauschen f√ºr das Signal. <br><br>  3. Rauschen: Dieser Fehler wird durch die Streuung der beobachteten Werte verursacht, z. B. unvorhersehbare √Ñnderungen oder Messfehler.  Dies ist ein schwerwiegender Fehler, der von keinem Modell erkl√§rt werden kann. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bc/8g/ag/bc8gaguh6e1w07vd3aoqcqcz4m0.png" alt="Bild"></div><br>  Eine Ensemble-Methode ist eine Methode, die viele schwache Sch√ºler, die auf demselben Lernalgorithmus basieren, mit dem Ziel kombiniert, einen (st√§rkeren) Sch√ºler zu schaffen, dessen Leistung besser ist als die der einzelnen Sch√ºler.  Ensemble-Techniken helfen, Vorspannung und / oder Streuung zu reduzieren. <br><br><h3>  6.3.  Bootstrap-Aggregation </h3><br>  Das Absacken (Aggregation) oder Aggregation von Bootstrap-Beispielen ist ein effektiver Weg, um die Varianz in Prognosen zu verringern.  Es funktioniert wie folgt: Zun√§chst m√ºssen N Trainingsuntermengen von Daten unter Verwendung einer Zufallsstichprobe mit R√ºckgabe generiert werden.  Zweitens passen Sie N Evaluatoren an, einen f√ºr jede Trainingsuntermenge.  Diese Evaluatoren werden unabh√§ngig voneinander angepasst, daher k√∂nnen Modelle parallel angepasst werden.  Drittens ist die Ensemble-Vorhersage ein einfaches arithmetisches Mittel einzelner Vorhersagen aus N Modellen.  Bei kategorialen Variablen wird die Wahrscheinlichkeit, dass eine Beobachtung zu einer Klasse geh√∂rt, durch den Anteil der Bewerter bestimmt, die diese Beobachtung als Mitglied dieser Klasse klassifizieren (mit Stimmenmehrheit, dh mit Stimmenmehrheit).  Wenn der Basisgutachter Vorhersagen mit der Wahrscheinlichkeit der Vorhersage treffen kann, kann der eingepackte Klassifikator den Durchschnittswert der Wahrscheinlichkeiten erhalten. <br><br>  Wenn Sie die baggingClassifier-Klasse der sklearn-Bibliothek verwenden, um die Genauigkeit ohne Paket zu berechnen, sollten Sie diesen Fehler kennen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/scikit-learn/scikitlearn/issues/8933</a> .  Eine Problemumgehung besteht darin, Beschriftungen in einer ganzzahligen Reihenfolge umzubenennen. <br><br><h3>  6.3.1.  Dispersionsreduzierung </h3><br>  Der Hauptvorteil des Absackens besteht darin, dass die Varianz der Prognosen verringert wird, wodurch das Problem der √úberanpassung gel√∂st wird.  Die Varianz in der Bagged-Vorhersage (œÜi [c]) ist eine Funktion der Anzahl der Bagged-Gutachter (N), der durchschnittlichen Varianz der von einem Gutachter durchgef√ºhrten Vorhersage (œÉÃÑ) und der durchschnittlichen Korrelation zwischen ihren Vorhersagen (œÅÃÑ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pq/ly/9i/pqly9iyqdqu5m8ty8zahuoeomve.png" alt="Bild"></div><br>  Durch sequentielles Bootstraping (Kapitel 4) soll die Abtastung so unabh√§ngig wie m√∂glich gemacht werden, wodurch œÅÃÑ verringert wird, was die Streuung von eingepackten Klassifikatoren verringern sollte.  In Abb.  In 6.1 haben wir das Standardabweichungsdiagramm der Bagged-Vorhersage als Funktion von N ‚àà [5, 30], œÅÃÑ ‚àà [0, 1] und œÉÃÑ = 1 aufgetragen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/na/bm/8z/nabm8zbq6st62mg82lmhac45y0i.png" alt="Bild"></div><br><h3>  6.3.2.  Verbesserte Genauigkeit </h3><br>  Stellen Sie sich einen Bagged-Klassifikator vor, der eine Vorhersage f√ºr k Klassen mit Stimmenmehrheit unter N unabh√§ngigen Klassifikatoren macht.  Wir k√∂nnen Vorhersagen als {0,1} bezeichnen, wobei 1 korrekte Vorhersage bedeutet.  Die Genauigkeit des Klassifikators ist die Wahrscheinlichkeit p, die Vorhersage als 1 zu markieren. Im Durchschnitt erhalten wir Np-Vorhersagen, die als 1 mit einer Varianz von Np (1 - p) markiert sind.  Die Mehrheitsentscheidung macht die richtige Vorhersage, wenn die vorhersehbarste Klasse beobachtet wird.  Zum Beispiel machte der Bagged-Klassifikator f√ºr N = 10 und k = 3 die korrekte Vorhersage, wenn er beobachtet wurde <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g-/oq/oi/g-oqoilmsmjgpaukoouor90ndbo.png" alt="Bild"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/wk/cl/wxwkcl97h4cnn1n-yx8ljmdj14g.png" alt="Bild"></div><br>  Listing 6.1.  Die Richtigkeit des eingepackten Klassifikators <br><br><pre><code class="plaintext hljs">from scipy.misc import comb N,p,k=100,1./3,3. p_=0 for i in xrange(0,int(N/k)+1): p_+=comb(N,i)*p**i*(1-p)**(Ni) print p,1-p_</code> </pre> <br>  Dies ist ein starkes Argument f√ºr das Absacken eines Klassifikators im allgemeinen Fall, wenn die Rechenf√§higkeiten dies zulassen.  Im Gegensatz zum Boosten kann das Absacken jedoch die Genauigkeit schwacher Klassifikatoren nicht verbessern: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/fz/i8/9nfzi86m4gvaqru13iicjf1q0bg.png" alt="Bild"></div><br>  F√ºr eine detaillierte Analyse dieses Themas wird dem Leser empfohlen, sich an den Satz der Condorcet-Jury zu wenden.  Obwohl dieser Satz zum Zweck der Mehrheitsentscheidung in der Politikwissenschaft erhalten wurde, hat das in diesem Satz angesprochene Problem Gemeinsamkeiten mit dem oben beschriebenen. <br><br><h3>  6.3.3.  Redundanz der Beobachtungen </h3><br>  In Kapitel 4 haben wir einen der Gr√ºnde untersucht, warum finanzielle Beobachtungen nicht als gleichm√§√üig verteilt und voneinander unabh√§ngig angesehen werden k√∂nnen.  √úberm√§√üige Beobachtungen wirken sich nachteilig auf das Absacken aus.  Erstens ist es wahrscheinlicher, dass Proben, die mit R√ºckgabe entnommen wurden, nahezu identisch sind, auch wenn sie keine gemeinsamen Beobachtungen haben.  Es tut <img src="https://habrastorage.org/webt/sx/i4/u0/sxi4u0afpswnvbe5nzxexnp_k80.png" alt="Bild">  und das Absacken verringert die Varianz nicht, unabh√§ngig von N. Wenn beispielsweise jeder Fall bei t gem√§√ü einer finanziellen Rendite zwischen t und t + 100 markiert ist, m√ºssen wir 1% der F√§lle pro eingesacktem Gutachter ausw√§hlen, aber nicht mehr.  In Kapitel 4, Abschnitt 4.5, werden drei alternative L√∂sungen empfohlen, von denen eine max_samples = out ['tW']. Mean () bei der Implementierung der Bagged-Classifier-Klasse in der sklearn-Bibliothek war.  Eine andere (bessere) L√∂sung war die Anwendung der Methode der sequentiellen Bootstrap-Auswahl. <br><br>  Der zweite nachteilige Effekt der Beobachtungsredundanz besteht darin, dass die Genauigkeit zus√§tzlicher Pakete erh√∂ht wird.  Dies ist auf die Tatsache zur√ºckzuf√ºhren, dass die Zufallsstichprobe mit Stichproben zu den Stichproben der Trainingsuntermenge zur√ºckkehrt, die denen au√üerhalb des Pakets sehr √§hnlich sind.  In diesem Fall zeigt die korrekte geschichtete K-Block-Kreuzvalidierung ohne Mischen vor dem Teilen eine viel geringere Genauigkeit in der Testuntermenge als die, die au√üerhalb des Pakets bewertet wurde.  Aus diesem Grund wird bei Verwendung dieser sklearn-Bibliotheksklasse empfohlen, stratifiedKFold (n_splits = k, shuffle = False) festzulegen, den Bagged-Klassifikator zu √ºberpr√ºfen und die Ergebnisse der Nicht-Paket-Genauigkeit zu ignorieren.  Ein niedriges k ist einem hohen k vorzuziehen, da durch √úberspaltung wieder Muster in die Testuntermenge eingef√ºgt werden, die denen in der Trainingsuntermenge zu √§hnlich sind. <br><br><h3>  6.4.  Zuf√§lliger Wald </h3><br>  Entscheidungsb√§ume sind insofern bekannt, als sie zu √úberanpassungen neigen, was die Varianz der Prognosen erh√∂ht.  Um dieses Problem anzugehen, wurde eine Random Forest (RF) -Methode entwickelt, um Ensemble-Vorhersagen mit geringerer Varianz zu generieren. <br><br>  Eine zuf√§llige Gesamtstruktur weist einige Gemeinsamkeiten mit dem Absacken auf, da einzelne Evaluatoren unabh√§ngig voneinander in Bootstrap-Datenuntergruppen geschult werden.  Der Hauptunterschied zum Absacken besteht darin, dass eine zweite Ebene der Zuf√§lligkeit in zuf√§llige W√§lder eingebaut wird: W√§hrend der Optimierung jeder Knotenfragmentierung wird nur eine zuf√§llige Teilstichprobe (ohne R√ºckgabe) von Attributen ausgewertet, um die Bewerter weiter zu dekorrelieren. <br><br>  Wie beim Absacken reduziert ein zuf√§lliger Wald die Varianz von Prognosen ohne √úberanpassung (denken Sie daran, bis).  Der zweite Vorteil besteht darin, dass eine zuf√§llige Gesamtstruktur die Wichtigkeit von Attributen bewertet, auf die wir in Kapitel 8 ausf√ºhrlich eingehen werden. Der dritte Vorteil besteht darin, dass eine zuf√§llige Gesamtstruktur Sch√§tzungen der Genauigkeit au√üerhalb des Pakets liefert, bei Finanzanwendungen jedoch wahrscheinlich aufgeblasen wird (wie in beschrieben) Abschnitt 6.3.3).  Aber wie beim Absacken weist ein zuf√§lliger Wald nicht unbedingt eine geringere Tendenz auf als einzelne Entscheidungsb√§ume. <br><br>  Wenn eine gro√üe Anzahl von Stichproben redundant ist (nicht gleichm√§√üig verteilt und voneinander unabh√§ngig), erfolgt dennoch eine erneute Anpassung: Durch Zufallsstichproben mit R√ºckgabe wird eine gro√üe Anzahl nahezu identischer B√§ume erstellt (), wobei jeder Entscheidungsbaum √ºberpasst ist (ein Nachteil, aufgrund dessen Entscheidungsb√§ume ber√ºchtigt sind). .  Im Gegensatz zum Absacken legt eine zuf√§llige Gesamtstruktur die Gr√∂√üe der Bootstrap-Beispiele immer entsprechend der Gr√∂√üe der Trainingsuntermenge von Daten fest.  Schauen wir uns an, wie wir dieses Problem der Neuanpassung zuf√§lliger W√§lder in der sklearn-Bibliothek l√∂sen k√∂nnen.  Zur Veranschaulichung werde ich auf die Klassen der sklearn-Bibliothek verweisen.  Diese L√∂sungen k√∂nnen jedoch auf jede Implementierung angewendet werden: <br><br>  1. Setzen Sie den Parameter max_features auf einen niedrigeren Wert, um eine Diskrepanz zwischen den B√§umen zu erzielen. <br><br>  2. Vorzeitiger Stopp: Setzen Sie den Regularisierungsparameter min_weight_fraction_leaf auf einen ausreichend gro√üen Wert (z. B. 5%), damit die Genauigkeit au√üerhalb des Pakets gegen die Korrektheit au√üerhalb der Stichprobe (k-Block) konvergiert. <br><br>  3. Verwenden Sie den BaggingClassifier-Evaluator f√ºr den DecisionTreeClassifier-Basis-Evaluator, wobei max_samples auf die durchschnittliche Eindeutigkeit (avgU) zwischen den Samples festgelegt ist. <br><br><ul><li>  clf = DecisionTreeClassifier (Kriterium = 'Entropie', max_features = 'auto', class_weight = 'ausgeglichen') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  4. Verwenden Sie den BaggingClassifier-Evaluator f√ºr den Basis-RandomForestClassifier-Evaluator, wobei max_samples auf die durchschnittliche Eindeutigkeit (avgU) zwischen den Samples festgelegt ist. <br><br><ul><li>  clf = RandomForestClassifier (n_estimators = 1, Kriterium = 'Entropie', Bootstrap = False, class_weight = 'balance_subsample') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  5. √Ñndern Sie die zuf√§llige Gesamtstrukturklasse, um Standard-Bootstraps durch sequentielle Bootstraps zu ersetzen. <br><br>  Zusammenfassend zeigt Listing 6.2 drei alternative M√∂glichkeiten zum Konfigurieren einer zuf√§lligen Gesamtstruktur mit verschiedenen Klassen. <br><br>  Listing 6.2.  Drei M√∂glichkeiten, einen zuf√§lligen Wald einzurichten <br><br><pre> <code class="plaintext hljs">clf0=RandomForestClassifier(n_estimators=1000, class_weight='balanced_ subsample', criterion='entropy') clf1=DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced') clf1=BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU) clf2=RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample') clf2=BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)</code> </pre> <br>  Beim Anpassen von Entscheidungsb√§umen reduziert die Drehung des Merkmalsraums in der mit den Achsen √ºbereinstimmenden Richtung in der Regel die Anzahl der f√ºr den Baum erforderlichen Ebenen.  Aus diesem Grund schlage ich vor, dass Sie einen zuf√§lligen Baum in die PCA der Attribute einf√ºgen, da dies die Berechnungen beschleunigen und die Neuanpassung geringf√ºgig reduzieren kann (mehr dazu in Kapitel 8).  Wie in Kapitel 4, Abschnitt 4.8 beschrieben, hilft das Argument class_weight = 'balance_subsample' au√üerdem, zu verhindern, dass B√§ume Minderheitenklassen falsch klassifizieren. <br><br><h3>  6.5.  Boost </h3><br>  Kearns und Valiant [1989] waren unter den ersten, die fragten, ob schwache Bewerter kombiniert werden k√∂nnten, um die Realisierung eines hochpr√§zisen Bewerters zu erreichen.  Kurz darauf zeigte Schapire [1990] eine positive Antwort auf diese Frage mit einem Verfahren, das wir heute Boosten nennen (Boosten, Boosten, Verst√§rken).  Im Allgemeinen funktioniert es wie folgt: Generieren Sie zun√§chst eine Trainingsuntermenge durch zuf√§llige Auswahl mit R√ºckgabe gem√§√ü bestimmten Stichprobengewichten (initialisiert durch einheitliche Gewichte).  Zweitens passen Sie einen Bewerter mit dieser Trainingsuntermenge an.  Drittens, wenn ein einzelner Gutachter eine Genauigkeit erreicht, die den Akzeptanzschwellenwert √ºberschreitet (z. B. in einem bin√§ren Klassifikator sind es 50%, so dass der Klassifikator besser funktioniert als zuf√§llige Wahrsagerei), bleibt der Gutachter erhalten, andernfalls wird er verworfen.  Viertens geben Sie falsch klassifizierten Beobachtungen mehr Gewicht und korrekt klassifizierten Beobachtungen weniger Gewicht.  F√ºnftens wiederholen Sie die vorherigen Schritte, bis N Gutachter empfangen werden.  Sechstens ist die Ensemble-Vorhersage der gewichtete Durchschnitt der einzelnen Vorhersagen aus N Modellen, wobei die Gewichte durch die Genauigkeit der einzelnen Bewerter bestimmt werden.  Es gibt eine Reihe von Boosted-Algorithmen, von denen AdaBoost Adaptive Boosting einer der beliebtesten ist (Geron [2017]).  Abbildung 6.3 fasst den Entscheidungsfluss in der Standardimplementierung des AdaBoost-Algorithmus zusammen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1d/u-/cr/1du-crm0gpkjgvy7lk45jj9f9ig.png" alt="Bild"></div><br><h3>  6.6.  Bagging vs Finance Boosting </h3><br>  Aufgrund der obigen Beschreibung unterscheidet sich das Boosten durch verschiedene Aspekte grundlegend vom <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Absacken</a> : <br><br><ul><li>  Die Anpassung der einzelnen Klassifikatoren erfolgt nacheinander. </li><li>  Schlechte Klassifikatoren werden abgelehnt. </li><li>  Bei jeder Iteration werden die Beobachtungen unterschiedlich gewichtet. </li></ul><br>  Die Ensemble-Prognose ist der gewichtete Durchschnitt der einzelnen Sch√ºler. <br><br>  Der Hauptvorteil des Boostings besteht darin, dass sowohl die Varianz als auch die Verzerrung der Prognosen verringert werden.  Eine Vorspannungskorrektur tritt jedoch aufgrund eines h√∂heren Risikos einer √úberanpassung auf.  Es kann argumentiert werden, dass bei Finanzanwendungen das Absacken normalerweise dem Boosten vorzuziehen ist.  Das Absacken l√∂st das √úberanpassungsproblem, w√§hrend das Boosten das √úberanpassungsproblem l√∂st.  Eine √úberanpassung ist h√§ufig ein schwerwiegenderes Problem als eine Unteranpassung, da eine zu enge Anpassung des MO-Algorithmus an Finanzdaten aufgrund des geringen Signal-Rausch-Verh√§ltnisses √ºberhaupt nicht schwierig ist.  Dar√ºber hinaus kann das Absacken parallelisiert werden, w√§hrend das Boosten normalerweise eine sequentielle Ausf√ºhrung erfordert. <br><br><h3>  6.7.  Absacken f√ºr Skalierbarkeit </h3><br>  Wie Sie wissen, lassen sich einige g√§ngige MO-Algorithmen je nach Stichprobengr√∂√üe nicht sehr gut skalieren.  Die SVM-Methode (Support Vector Machines) ist ein Paradebeispiel.  Wenn Sie versuchen, den SVM-Evaluator √ºber eine Million Beobachtungen anzupassen, kann es lange dauern, bis der Algorithmus konvergiert.  Und selbst nach der Konvergenz gibt es keine Garantie daf√ºr, dass die L√∂sung ein globales Optimum darstellt oder nicht neu ausgerichtet wird. <br><br>  Ein praktischer Ansatz besteht darin, einen Bagged-Algorithmus zu erstellen, bei dem der Basisauswerter zu einer Klasse geh√∂rt, die mit der Stichprobengr√∂√üe nicht gut skaliert werden kann, z. B. SVM.  Bei der Definition dieses grundlegenden Gutachters f√ºhren wir eine strenge Bedingung f√ºr einen fr√ºhen Stopp ein.  Bei der Implementierung von SVMs (Support Vector Machines) in der sklearn-Bibliothek k√∂nnen Sie beispielsweise einen niedrigen Wert f√ºr den Parameter max_iter festlegen, z. B. 1E5-Iterationen.  Der Standardwert ist max_iter = -1, wodurch der Evaluator angewiesen wird, die Iteration fortzusetzen, bis die Fehler unter das Toleranzniveau fallen.  Andererseits k√∂nnen Sie das Toleranzniveau mit dem Parameter tol erh√∂hen, der standardm√§√üig tol = iE-3 ist.  Jede dieser beiden Optionen f√ºhrt zu einem vorzeitigen Stopp.  Sie k√∂nnen andere Algorithmen fr√ºhzeitig stoppen, indem Sie √§quivalente Parameter verwenden, z. B. die Anzahl der Ebenen in einer zuf√§lligen Gesamtstruktur (max_depth) oder den minimalen gewichteten Bruchteil der Gesamtsumme der Gewichte (alle Eingabebeispiele), die f√ºr einen Blattknoten erforderlich sind (min_weight_fraction_leaf). <br><br>  Da Bagged-Algorithmen parallelisiert werden k√∂nnen, wandeln wir eine gro√üe sequentielle Aufgabe in eine Reihe kleinerer Aufgaben um, die gleichzeitig ausgef√ºhrt werden.  Ein fr√ºhzeitiger Stopp erh√∂ht nat√ºrlich die Varianz der Ergebnisse einzelner Basisbewerter.  Diese Zunahme kann jedoch durch die Abnahme der Varianz, die mit dem Bagged-Algorithmus verbunden ist, mehr als ausgeglichen werden.  Sie k√∂nnen diese Reduzierung steuern, indem Sie neue unabh√§ngige Basisbewerter hinzuf√ºgen.  Auf diese Weise k√∂nnen Sie mit Bagging schnelle und zuverl√§ssige Sch√§tzungen f√ºr sehr gro√üe Datenmengen erhalten. <br><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  25% Rabatt f√ºr Khabrozhiteley Vorbestellungsb√ºcher auf einen Gutschein - <b>Maschinelles Lernen</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de445780/">https://habr.com/ru/post/de445780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de445762/index.html">Ist Mathematik logisch oder warum sind axiomatische Theorien paradox?</a></li>
<li><a href="../de445764/index.html">Meine Art, Masterkomponenten in Abbildung zu erstellen</a></li>
<li><a href="../de445766/index.html">Ehrlich gesagt √ºber das Rechenzentrum: Wie wir das Staubproblem in den Serverr√§umen des Rechenzentrums gel√∂st haben</a></li>
<li><a href="../de445772/index.html">Schnelles Zahlungssystem oder das Unm√∂gliche ist m√∂glich</a></li>
<li><a href="../de445778/index.html">10 neue kostenlose Kurse zu kognitiven Diensten und Azure</a></li>
<li><a href="../de445782/index.html">Eine Auswahl an Schraubendrehern und ungew√∂hnlichen Multitools von Leatherman bis Xiaomi</a></li>
<li><a href="../de445784/index.html">Berufliches Wachstum der Mitarbeiter - was ist das und warum ist es notwendig: Wir kommunizieren mit Dodo Pizza, Icons8 und Evil Martians</a></li>
<li><a href="../de445786/index.html">Kryptographie in Java. KeyStore-Klasse</a></li>
<li><a href="../de445788/index.html">Cloud-Video√ºberwachung zum Selbermachen: Neue Funktionen des Ivideon Web SDK</a></li>
<li><a href="../de445792/index.html">Wie wir Dokumentation in einem offenen Embox-Projekt entwickeln</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>