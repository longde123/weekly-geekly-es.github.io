<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§¶üèø ü•Ä ü§¶üèø Neurobugurt C√≥mo ense√±amos a la red neuronal a inventar memes un a√±o antes que Stanford üë©‚ÄçüöÄ üçÇ ‚úÖ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esta noticia (+ investigaci√≥n ) sobre la invenci√≥n del generador de memes por cient√≠ficos de la Universidad de Stanford me llev√≥ a escribir un art√≠cul...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neurobugurt C√≥mo ense√±amos a la red neuronal a inventar memes un a√±o antes que Stanford</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416379/"> Esta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">noticia</a> (+ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">investigaci√≥n</a> ) sobre la invenci√≥n del generador de memes por cient√≠ficos de la Universidad de Stanford me llev√≥ a escribir un art√≠culo.  En mi art√≠culo, intentar√© demostrar que no es necesario ser un cient√≠fico de Stanford para hacer cosas interesantes con las redes neuronales.  En el art√≠culo, describo c√≥mo en 2017 capacitamos a una red neuronal en un cuerpo de aproximadamente 30,000 textos y lo forzamos a generar nuevos memes y memes de Internet (signos de comunicaci√≥n) en el sentido sociol√≥gico de la palabra.  Describimos el algoritmo de aprendizaje autom√°tico que utilizamos, las dificultades t√©cnicas y administrativas que encontramos. <br><a name="habracut"></a><br>  Un poco de historia sobre c√≥mo llegamos a la idea de un neuro-escritor y en qu√© consist√≠a exactamente.  En 2017, realizamos un proyecto para un sitio web p√∫blico de Vkontakte, cuyo nombre y capturas de pantalla los moderadores de Habrahabr prohibieron publicar, considerando su menci√≥n como "auto" de relaciones p√∫blicas.  El p√∫blico existe desde 2013 y une publicaciones con la idea general de descomponer el humor a trav√©s de una l√≠nea y separar las l√≠neas con el s√≠mbolo "@": <br><br> <code> <br> @ <br>   <br> @ <br> </code> <br> <br>  El n√∫mero de l√≠neas puede variar, la trama puede ser cualquiera.  Muy a menudo, esto es humor o notas sociales agudas sobre los hechos rampantes de la realidad.  En general, este dise√±o se llama "buhurt". <br><br><img src="https://habrastorage.org/webt/g9/bb/4n/g9bb4nvtkhh2hkx7wybuutrzr3u.png" alt="imagen"><br><br>  <i>Uno de los buhurts t√≠picos.</i> <br><br>  A lo largo de los a√±os, el p√∫blico se ha convertido en una tradici√≥n interna (personajes, tramas, ubicaciones), y el n√∫mero de publicaciones ha excedido de 30,000. En el momento de su an√°lisis, el n√∫mero de l√≠neas de origen del texto exced√≠a de medio mill√≥n para las necesidades del proyecto. <br><br><h3>  Parte 0. El surgimiento de ideas y equipos. </h3><br>  A ra√≠z de la popularidad masiva de las redes neuronales, la idea de entrenar a ANN en nuestros textos estuvo en el aire durante unos seis meses, pero finalmente se formul√≥ utilizando E7su en diciembre de 2016. Al mismo tiempo, se invent√≥ el nombre ("Neurobugurt").  En ese momento, el equipo interesado en el proyecto constaba de solo tres personas.  Todos √©ramos estudiantes sin experiencia pr√°ctica en algoritmos y redes neuronales.  Peor a√∫n, ni siquiera ten√≠amos una sola GPU adecuada para el entrenamiento.  Todo lo que ten√≠amos era entusiasmo y confianza en que esta historia podr√≠a ser interesante. <br><br><h3>  Parte 1. La formulaci√≥n de la hip√≥tesis y las tareas. </h3><br>  Nuestra hip√≥tesis result√≥ ser la suposici√≥n de que si combina todos los textos publicados durante tres a√±os y medio y entrena la red neuronal en este edificio, puede obtener: <br><br>  a) m√°s creativo que las personas <br>  b) gracioso <br><br>  Incluso si las palabras o letras en el buhurt resultan ser confusas y ordenadas al azar, creemos que esto podr√≠a funcionar como un servicio de fan√°ticos y que a√∫n complacer√≠a a los lectores. <br><br>  La tarea se simplific√≥ enormemente por el hecho de que el formato de los buhurts es esencialmente textual.  Entonces, no necesit√°bamos sumergirnos en la visi√≥n artificial y otras cosas complejas.  Otra buena noticia es que todo el cuerpo de textos es muy similar.  Esto hizo posible no utilizar el aprendizaje reforzado, al menos en las primeras etapas.  Al mismo tiempo, entendimos claramente que crear un escritor de redes neuronales con salida legible m√°s de una vez no es tan f√°cil.  El riesgo de dar a luz a un monstruo que arrojar√≠a letras al azar era muy grande. <br><br><h3>  Parte 2. Preparaci√≥n del cuerpo de textos. </h3><br>  Se cree que la fase de preparaci√≥n puede llevar mucho tiempo, ya que est√° asociada con la recopilaci√≥n y limpieza de datos.  En nuestro caso, result√≥ ser bastante corto: se escribi√≥ un peque√±o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">analizador</a> que extrajo unas 30k publicaciones del muro de la comunidad y las coloc√≥ en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">archivo txt</a> . <br><br>  No borramos los datos antes del primer entrenamiento.  En el futuro, esto jug√≥ una broma cruel con nosotros, porque debido al error que apareci√≥ en esta etapa, no pudimos llevar los resultados a una forma legible durante mucho tiempo.  Pero m√°s sobre eso m√°s tarde. <br><br><img src="https://habrastorage.org/webt/jt/9v/uq/jt9vuqk2tdpoi7ycus5udn-lnpo.png" alt="imagen"><br><br>  <i>Archivo de pantalla con hamburguesas</i> <br><br><h3>  Parte 3. Anuncio, refinamiento de la hip√≥tesis, elecci√≥n del algoritmo. </h3><br>  Utilizamos un recurso accesible: una gran cantidad de suscriptores p√∫blicos.  Se supon√≠a que entre 300,000 lectores hay varios entusiastas que poseen redes neuronales en un nivel suficiente para llenar los vac√≠os en el conocimiento de nuestro equipo.  Partimos de la idea de anunciar ampliamente la competencia y atraer a los entusiastas del aprendizaje autom√°tico a la discusi√≥n del problema formulado.  Despu√©s de escribir los textos, le contamos a la gente sobre nuestra idea y esperamos una respuesta. <br><br><img src="https://habrastorage.org/webt/73/0b/jo/730bjoajceycgzoynywra6xcupy.png" alt="imagen"><br><br>  <i>Anuncio de discusi√≥n tem√°tica</i> <br><br>  La reacci√≥n de las personas super√≥ nuestras expectativas m√°s salvajes.  La discusi√≥n sobre el hecho de que vamos a entrenar una red neuronal extendi√≥ el holivar en casi 1000 comentarios.  La mayor√≠a de los lectores simplemente se desvanecieron e intentaron imaginar c√≥mo ser√≠a el resultado.  Alrededor de 6,000 personas se asomaron a la discusi√≥n tem√°tica, y m√°s de 50 aficionados interesados ‚Äã‚Äãdejaron comentarios para quienes les dimos un conjunto de prueba de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">814 l√≠neas buhurt</a> para realizar pruebas iniciales y capacitaci√≥n.  Cada persona interesada podr√≠a tomar un conjunto de datos y aprender el algoritmo que sea m√°s interesante para √©l, y luego discutirlo con nosotros y otros entusiastas.  Anunciamos de antemano que seguiremos trabajando con aquellos participantes cuyos resultados ser√°n m√°s legibles. <br><br>  El trabajo comenz√≥: alguien ensambl√≥ silenciosamente un generador en las cadenas de Markov, alguien prob√≥ varias implementaciones con un github, y la mayor√≠a se volvi√≥ loco en la discusi√≥n y nos convenci√≥ con espuma en la boca de que nada saldr√≠a de eso.  Esto comenz√≥ la parte t√©cnica del proyecto. <br><br><img src="https://habrastorage.org/webt/ie/cu/tz/iecutzyxvv__0a2qzo5ailsjk9s.png" alt="imagen"><br><br>  Algunas sugerencias de entusiastas <br><br>  Las personas ofrecieron docenas de opciones para la implementaci√≥n: <br><br><ul><li>  Cadenas de Markov. </li><li>  Encuentre una implementaci√≥n lista para usar de algo similar a GitHub y capac√≠telo. </li><li>  Un generador de frases al azar escrito en Pascal. </li><li>  Obtenga un negro literario que escriba tonter√≠as aleatorias, y lo pasaremos como una salida de red neuronal. </li></ul><br><img src="https://habrastorage.org/webt/ki/tr/ut/kitrutzq2yxrtm_weg1ngm4beow.png" alt="imagen"><br><br>  <i>Evaluaci√≥n de la complejidad del proyecto por parte de uno de los suscriptores.</i> <br><br>  La mayor√≠a de los comentaristas estuvieron de acuerdo en que nuestro proyecto est√° condenado al fracaso y que ni siquiera llegaremos a la etapa de prototipo.  Como entendimos m√°s tarde, la gente todav√≠a est√° inclinada a percibir las redes neuronales como alg√∫n tipo de magia negra que ocurre en la "cabeza de Zuckerberg" y las divisiones secretas de Google. <br><br><h3>  Parte 4. Selecci√≥n de algoritmos, capacitaci√≥n y expansi√≥n del equipo. </h3><br>  Despu√©s de un tiempo, la campa√±a que lanzamos para ideas de crowdsourcing para el algoritmo comenz√≥ a dar sus primeros frutos.  Obtuvimos unos 30 prototipos de trabajo, la mayor√≠a de los cuales emitieron tonter√≠as completamente ilegibles. <br><br>  En esta etapa, encontramos por primera vez una desmotivaci√≥n del equipo.  Todos los resultados fueron muy d√©bilmente similares a los buhurts y la mayor√≠a de las veces representaron un abracadabra de letras y s√≠mbolos.  El trabajo de docenas de entusiastas se convirti√≥ en polvo y esto los desmotiv√≥ tanto a ellos como a nosotros. <br><br>  El algoritmo basado en pyTorch se mostr√≥ mejor que otros.  Se decidi√≥ tomar esta implementaci√≥n y el algoritmo LSTM como base.  Reconocimos al suscriptor que lo propuso como el ganador y comenzamos a trabajar para mejorar el algoritmo junto con √©l.  Nuestro equipo distribuido ha crecido hasta cuatro personas.  El hecho curioso aqu√≠ es que el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ganador del concurso</a> , como result√≥, ten√≠a solo 16 a√±os.  La victoria fue su primer premio real en el campo de la ciencia de datos. <br><br>  Para el primer entrenamiento, se alquil√≥ un grupo de 8 tarjetas gr√°ficas GXT1080. <br><br><img src="https://habrastorage.org/webt/mb/_l/ql/mb_lqlodsgm8fyztu36ly0soqo4.jpeg" alt="imagen"><br><br>  <i>Consola de administraci√≥n de tarjetas agrupadas</i> <br><br>  El repositorio original y todos los manuales del proyecto Torch-rnn est√°n aqu√≠: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">github.com/jcjohnson/torch-rnn</a> .  M√°s tarde, sobre la base de esto, publicamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestro repositorio</a> , en el que est√°n nuestras fuentes, ReadMe para la instalaci√≥n, as√≠ como los propios neurobugurts terminados. <br><br>  Las primeras veces que entrenamos usando una configuraci√≥n preconfigurada en un cl√∫ster de GPU pagado.  La configuraci√≥n result√≥ no ser tan dif√≠cil: solo las instrucciones del desarrollador de Torch y la ayuda de la administraci√≥n de hosting, que est√° incluida en el pago, son suficientes. <br><br>  Sin embargo, muy r√°pidamente tuvimos dificultades: cada capacitaci√≥n cost√≥ el tiempo de alquiler de la GPU, lo que significa que simplemente no hab√≠a dinero en el proyecto.  Debido a esto, en enero-febrero de 2017, realizamos capacitaci√≥n en las instalaciones compradas e intentamos lanzar la generaci√≥n en nuestras m√°quinas locales. <br><br>  Cualquier texto es adecuado para el entrenamiento modelo.  Antes de entrenar, debe preprocesarlo, para lo cual Torch tiene un algoritmo especial preprocess.py que convierte su my_data.txt en dos archivos: HDF5 y JSON: <br><br>  El script de preprocesamiento se ejecuta as√≠: <br><br><pre> <code class="python hljs">python scripts/preprocess.py \ --input_txt my_data.txt \ --output_h5 my_data.h5 \ --output_json my_data.json</code> </pre> <br><img src="https://habrastorage.org/webt/8u/s8/kj/8us8kjppqy-1wqhpjp87hdiwqyi.png" alt="imagen"><br><br>  <i>Despu√©s del preprocesamiento, aparecen dos archivos en los que se capacitar√° la red neuronal en el futuro</i> <br><br>  <a href="">Aqu√≠</a> se describen los diversos indicadores que se pueden cambiar en la etapa de preprocesamiento.  Tambi√©n es posible ejecutar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Torch desde Docker</a> , pero el autor del art√≠culo no lo verific√≥. <br><br><h4>  Entrenamiento de redes neuronales </h4><br>  Despu√©s del preprocesamiento, puede continuar con la capacitaci√≥n del modelo.  En la carpeta con HDF5 y JSON, debe ejecutar la utilidad th, que apareci√≥ con usted si instal√≥ Torch correctamente: <br><br><pre> <code class="python hljs">th train.lua -input_h5 my_data.h5 -input_json my_data.json</code> </pre> <br>  La capacitaci√≥n lleva una gran cantidad de tiempo y genera archivos de la forma cv / checkpoint_1000.t7, que son los "pesos" de nuestra red neuronal.  Estos archivos pesan una cantidad impresionante de megabytes y contienen la fuerza de los enlaces entre letras espec√≠ficas en su conjunto de datos original. <br><br><img src="https://habrastorage.org/webt/qt/ts/it/qttsit4vulbqojnmxfcfdh1pdzm.png" alt="imagen"><br><br>  <i>Una red neuronal a menudo se compara con el cerebro humano, pero me parece una analog√≠a mucho m√°s clara con una funci√≥n matem√°tica que toma par√°metros en la entrada (su conjunto de datos) y da el resultado (nuevos datos) en la salida.</i> <br><br>  En nuestro caso, cada entrenamiento en un cl√∫ster de 8 GTX 1080 en un conjunto de datos de 500,000 l√≠neas tom√≥ aproximadamente una hora o dos, y un entrenamiento similar en un tipo de CPU i3-2120 tom√≥ aproximadamente 80-100 horas.  En el caso de un entrenamiento m√°s largo, la red neuronal comenz√≥ a volverse a entrenar r√≠gidamente: los s√≠mbolos se repet√≠an con demasiada frecuencia, cayendo en largos ciclos de preposiciones, conjunciones y palabras introductorias. <br><br>  Es conveniente que pueda elegir la frecuencia de los puntos de control y durante un entrenamiento obtendr√° inmediatamente muchos modelos: desde el menos entrenado (checkpoint_1000) hasta el reentrenado (checkpoint_1000000).  Solo suficiente espacio ser√≠a suficiente. <br><br><h4>  Nueva generaci√≥n de texto. </h4><br>  Despu√©s de haber recibido al menos un archivo listo con pesos (punto de control _ *******), puede pasar a la siguiente etapa m√°s interesante: comenzar a generar textos.  Para nosotros, fue un verdadero momento de verdad, porque por primera vez obtuvimos un resultado tangible: un error escrito por una m√°quina. <br><br>  En este punto, finalmente dejamos de usar el cl√∫ster y todas las generaciones se llevaron a cabo en nuestras m√°quinas de baja potencia.  Sin embargo, al intentar iniciar localmente, simplemente no logramos seguir las instrucciones e instalar Torch.  La primera barrera fue el uso de m√°quinas virtuales.  En Ubuntu 16 virtual, el palo no despega, olv√≠dalo.  StackOverflow a menudo acud√≠a al rescate, pero algunos errores no eran tan triviales que la respuesta solo se pod√≠a encontrar con gran dificultad. <br><br>  La instalaci√≥n de Torch en una m√°quina local detuvo el proyecto por un buen par de semanas: encontramos todo tipo de errores al instalar numerosos paquetes requeridos, tambi√©n tuvimos problemas con la virtualizaci√≥n (virtualenv .env) y finalmente no la usamos.  Varias veces el soporte fue demolido al nivel de sudo rm -rf y simplemente se instal√≥ nuevamente. <br><br>  Usando el archivo resultante con pesos, pudimos comenzar a generar textos en nuestra m√°quina local: <br><br><img src="https://habrastorage.org/webt/9z/ve/r_/9zver_8vvx-fknpdj_9-id70hfe.jpeg" alt="imagen"><br><br>  <i>Una de las primeras conclusiones.</i> <br><br><h3>  Parte 5. Borrar textos </h3><br>  Otra dificultad obvia fue que el tema de las publicaciones es muy diferente, y nuestro algoritmo no involucra ninguna divisi√≥n y considera todas las 500,000 l√≠neas como un solo texto.  Consideramos diferentes opciones para agrupar el conjunto de datos e incluso est√°bamos listos para dividir manualmente el cuerpo de textos por tema o colocar etiquetas en varios miles de buhurts (hab√≠a un recurso humano necesario para esto), pero constantemente enfrentamos dificultades t√©cnicas para enviar cl√∫steres al aprender LSTM.  Cambiar el algoritmo y realizar la competencia nuevamente no parec√≠a ser la idea m√°s sensata en t√©rminos de cronometraje del proyecto y la motivaci√≥n de los participantes. <br><br>  Parec√≠a que est√°bamos en un punto muerto: no pod√≠amos agrupar buhurts, y el entrenamiento en un solo gran conjunto de datos arroj√≥ resultados dudosos.  No quer√≠a dar un paso atr√°s y cambiar el algoritmo y la implementaci√≥n casi disparados: el proyecto simplemente podr√≠a caer en coma.  El equipo desesperadamente no ten√≠a suficiente conocimiento para resolver la situaci√≥n normalmente, pero el viejo SME-KAL-OCHK-A vino al rescate.  La soluci√≥n final a la <s>muleta</s> result√≥ ser simple: en el conjunto de datos original, separe los buhurts existentes entre s√≠ con l√≠neas vac√≠as y entrene LSTM nuevamente. <br><br>  Organizamos los latidos en 10 espacios verticales despu√©s de cada buhurt, repetimos el entrenamiento y durante la generaci√≥n establecimos un l√≠mite en el volumen de salida de 500 caracteres (la longitud promedio de un buhurt de "trama" en el conjunto de datos original). <br><br><img src="https://habrastorage.org/webt/da/2h/x4/da2hx4k9shkumjvxwickcnjfwnu.png" alt="imagen"><br><br>  <i>Como era.</i>  <i>Los intervalos entre textos son m√≠nimos.</i> <br><br><img src="https://habrastorage.org/webt/bb/y8/i5/bby8i5skh4u0trqrs94ppe2fal0.png" alt="imagen"><br><br>  <i>¬øC√≥mo se hizo?</i>  <i>Los intervalos de 10 l√≠neas permiten que LSTM "entienda" que un bogurt ha terminado y que otro ha comenzado.</i> <br><br>  Por lo tanto, fue posible lograr que aproximadamente el 60% de todos los buhurts generados comenzaron a tener una trama legible (aunque a menudo muy delirante) a lo largo de todo el buhurt de principio a fin.  La longitud de una parcela fue, en promedio, de 9 a 13 l√≠neas. <br><br><h3>  Parte 6. Reentrenamiento </h3><br>  Habiendo estimado la econom√≠a del proyecto, decidimos no gastar m√°s dinero en alquilar un grupo, sino invertir en comprar nuestras propias tarjetas.  El tiempo de aprendizaje aumentar√≠a, pero habiendo comprado una tarjeta una vez, podr√≠amos generar nuevos aumentos constantemente.  Al mismo tiempo, a menudo la capacitaci√≥n ya no era necesaria. <br><br><img src="https://habrastorage.org/webt/rr/_e/92/rr_e92il8tdd5ckjdglm7fy8cuy.jpeg" alt="imagen"><br><br>  <i>Configuraciones de lucha en la m√°quina local</i> <br><br><h3>  Parte 7. Resultados de equilibrio </h3><br>  A fines de marzo-abril de 2017, volvimos a entrenar la red neuronal, especificando los par√°metros de temperatura y la cantidad de eras de entrenamiento.  Como resultado, la calidad de la salida ha aumentado ligeramente. <br><br><img src="https://habrastorage.org/webt/ew/q8/qu/ewq8quy68ixemttz67jkmqeqvgw.png" alt="imagen"><br><br>  <i>Torch-rnn velocidad de aprendizaje en comparaci√≥n con char-rnn</i> <br><br>  Probamos los dos algoritmos que vienen con Torch: rnn y LSTM.  El segundo result√≥ ser mejor. <br><br><h3>  Parte 8. ¬øQu√© hemos logrado? </h3><br>  El primer neurobugurt se public√≥ el 17 de enero de 2017, inmediatamente despu√©s del entrenamiento en el cl√∫ster, y el primer d√≠a se recopilaron m√°s de 1000 comentarios. <br><br><img src="https://habrastorage.org/webt/t2/ng/i5/t2ngi5591hhqz7fh0n_snditsuo.png" alt="imagen"><br><br>  <i>Uno de los primeros neurobugurts</i> <br><br>  Los neurobugurts llegaron tan bien a la audiencia que se convirtieron en una secci√≥n separada, que durante todo el a√±o sali√≥ bajo el hashtag # neurobugurt y suscriptores divertidos.  En total, en 2017 y principios de 2018, generamos m√°s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">18,000 neurobugurts</a> , con un promedio de 500 caracteres cada uno.  Adem√°s, apareci√≥ todo un movimiento de parodias p√∫blicas, cuyos participantes representaban neurobugurs, reorganizando frases al azar en lugares. <br><br><img src="https://habrastorage.org/webt/ls/k2/jb/lsk2jbbloh3e7sg-1p-04k5muvq.jpeg" alt="imagen"><br><br><h3>  Parte 9. En lugar de una conclusi√≥n </h3><br>  Con este art√≠culo, quer√≠a mostrar que incluso si no tienes experiencia en redes neuronales, este dolor no es un problema.  No necesita trabajar en Stanford para hacer cosas simples pero interesantes con redes neuronales.  Todos los participantes en nuestro proyecto eran estudiantes comunes con sus tareas actuales, diplomas, trabajos, pero la causa com√∫n nos permiti√≥ llevar el proyecto a la final.  Gracias a la idea reflexiva, la planificaci√≥n y la energ√≠a de los participantes, pudimos obtener los primeros resultados sanos en menos de un mes despu√©s de la formulaci√≥n final de la idea (la mayor parte del trabajo t√©cnico y organizativo cay√≥ en las vacaciones de invierno de 2017). <br><br><img src="https://habrastorage.org/webt/pu/q2/yj/puq2yjjiq0ynj4ioncm_wn_obrm.png" alt="imagen"><br><br>  <i>M√°s de 18,000 buhurts generados por m√°quina</i> <br><br>  Espero que este art√≠culo ayude a alguien a planificar su propio proyecto ambicioso con redes neuronales.  Pido no juzgar estrictamente, ya que este es mi primer art√≠culo sobre Habr√©.  Si usted, como yo, entusiasta de ML, seamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">amigos</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es416379/">https://habr.com/ru/post/es416379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es416367/index.html">Comenzamos ReactOS con BTRFS de la secci√≥n</a></li>
<li><a href="../es416369/index.html">Casi complicado. Parte 2, crear una "casa inteligente" inal√°mbrica. Basado en tecnolog√≠a Linux, software Z-Wave y MajorDoMo</a></li>
<li><a href="../es416371/index.html">Luz de camping anal√≥gica</a></li>
<li><a href="../es416375/index.html">Conceptos b√°sicos de JavaScript para principiantes</a></li>
<li><a href="../es416377/index.html">Nos convertimos en magos en la programaci√≥n. Parte 1</a></li>
<li><a href="../es416381/index.html">Informe del Club de Roma 2018, Cap√≠tulo 3.13: Filantrop√≠a, Inversi√≥n, Crowdsourcing y Blockchain</a></li>
<li><a href="../es416385/index.html">Si la correlaci√≥n sale al 100%, en alg√∫n lugar se ha introducido un error: la experiencia de pasant√≠a en Rambler Group</a></li>
<li><a href="../es416387/index.html">Camarones: escale y comparta im√°genes HTTP en C ++ moderno con ImageMagic ++, SObjectizer y RESTinio</a></li>
<li><a href="../es416391/index.html">Optimizaci√≥n de la colocaci√≥n de m√°quinas virtuales en servidores.</a></li>
<li><a href="../es416393/index.html">Conferencia IIDF: las corporaciones no son vs startups</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>