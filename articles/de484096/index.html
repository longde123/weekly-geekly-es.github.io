<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§µüèª üéØ ü§ë Die Schlacht der beiden Yakozun oder Cassandra gegen HBase. Erfahrung im Sberbank-Team ü§ò ‚¨úÔ∏è üòè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dies ist nicht einmal ein Scherz, es scheint, dass dieses spezielle Bild die Essenz dieser Datenbanken am genauesten widerspiegelt, und am Ende wird k...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Schlacht der beiden Yakozun oder Cassandra gegen HBase. Erfahrung im Sberbank-Team</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/484096/">  Dies ist nicht einmal ein Scherz, es scheint, dass dieses spezielle Bild die Essenz dieser Datenbanken am genauesten widerspiegelt, und am Ende wird klar sein, warum: <br><br><img src="https://habrastorage.org/webt/i2/lk/zo/i2lkzo9tq7zpeprcbtgm3-mufk4.png"><br><br>  Laut DB-Engines Ranking sind Cassandra (im Folgenden CS) und HBase (HB) die beiden beliebtesten NoSQL-S√§ulenbasen. <br><br><img src="https://habrastorage.org/webt/su/rd/39/surd39n7bmrbnxgpn0512tnxamm.png"><br><br>  Nach dem Willen des Schicksals arbeitet unser Datenlademanagement-Team bei der Sberbank <a href="https://habr.com/ru/company/sberbank/blog/420425/">seit langer Zeit</a> eng mit HB zusammen.  In dieser Zeit haben wir seine St√§rken und Schw√§chen sehr gut studiert und gelernt, wie man es kocht.  Das st√§ndige Vorhandensein einer Alternative in Form von CS lie√ü mich jedoch zweifeln: Haben wir die richtige Wahl getroffen?  Dar√ºber hinaus haben die Ergebnisse des von DataStax durchgef√ºhrten <a href="https://www.datastax.com/products/compare/nosql-performance-benchmarks">Vergleichs</a> ergeben, dass CS HB leicht mit einem fast vernichtenden Ergebnis besiegt.  Auf der anderen Seite ist DataStax eine interessierte Person, und Sie sollten hier kein Wort verlieren.  Au√üerdem war eine kleine Menge an Informationen √ºber die Testbedingungen peinlich, sodass wir beschlossen, selbst herauszufinden, wer der K√∂nig von BigData NoSql ist, und die Ergebnisse waren sehr interessant. <br><a name="habracut"></a><br>  Bevor Sie jedoch zu den Ergebnissen der durchgef√ºhrten Tests √ºbergehen, m√ºssen Sie die wesentlichen Aspekte der Umgebungskonfigurationen beschreiben.  Tatsache ist, dass CS im Datenverlust-Toleranzmodus verwendet werden kann.  Das hei√üt  In diesem Fall ist nur ein Server (Knoten) f√ºr die Daten eines bestimmten Schl√ºssels verantwortlich. Wenn dieser aus irgendeinem Grund ausf√§llt, geht der Wert dieses Schl√ºssels verloren.  F√ºr viele Aufgaben ist dies nicht kritisch, f√ºr den Bankensektor jedoch eher die Ausnahme als die Regel.  In unserem Fall ist es wichtig, mehrere Kopien der Daten f√ºr eine zuverl√§ssige Speicherung zu haben. <br><br>  Daher wurde nur der CS-Modus der dreifachen Replikation in Betracht gezogen, d.h.  Die Fallerstellung wurde mit den folgenden Parametern durchgef√ºhrt: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> KEYSPACE ks <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> <span class="hljs-keyword"><span class="hljs-keyword">REPLICATION</span></span> = {<span class="hljs-string"><span class="hljs-string">'class'</span></span> : <span class="hljs-string"><span class="hljs-string">'NetworkTopologyStrategy'</span></span>, <span class="hljs-string"><span class="hljs-string">'datacenter1'</span></span> : <span class="hljs-number"><span class="hljs-number">3</span></span>};</code> </pre> <br>  Dar√ºber hinaus gibt es zwei M√∂glichkeiten, um das erforderliche Konsistenzniveau sicherzustellen.  Allgemeine Regel: <br>  NW + NR&gt; RF <br><br>  Dies bedeutet, dass die Anzahl der Best√§tigungen von den Knoten beim Schreiben (NW) plus die Anzahl der Best√§tigungen von den Knoten beim Lesen (NR) gr√∂√üer sein muss als der Replikationsfaktor.  In unserem Fall ist RF = 3 und daher sind die folgenden Optionen geeignet: <br>  2 + 2&gt; 3 <br>  3 + 1&gt; 3 <br><br>  Da es f√ºr uns von grundlegender Bedeutung ist, die Daten so zuverl√§ssig wie m√∂glich zu halten, wurde ein 3 + 1-Schema gew√§hlt.  Zus√§tzlich arbeitet HB auf einer √§hnlichen Basis, d.h.  Ein solcher Vergleich w√§re ehrlicher. <br><br>  Es sollte beachtet werden, dass DataStax bei seinen Recherchen das Gegenteil getan hat: Sie haben RF = 1 sowohl f√ºr CS als auch f√ºr HB festgelegt (f√ºr letztere durch √Ñndern der HDFS-Einstellungen).  Dies ist ein sehr wichtiger Aspekt, da die Auswirkungen auf die CS-Leistung in diesem Fall enorm sind.  Das folgende Bild zeigt zum Beispiel den Anstieg der Zeit, die zum Laden von Daten in CS erforderlich ist: <br><br><img src="https://habrastorage.org/webt/fw/az/r9/fwazr9muypegpgjpyaq4rnagg8u.png"><br><br>  Hier sehen wir Folgendes: Je mehr konkurrierende Threads Daten schreiben, desto l√§nger dauert es.  Dies ist nat√ºrlich, aber es ist wichtig, dass die Verschlechterung der Leistung f√ºr RF = 3 signifikant h√∂her ist.  Mit anderen Worten, wenn wir in 4 Tabellen in jeweils 5 Streams schreiben (insgesamt 20), verliert RF = 3 ungef√§hr zweimal (150 Sekunden RF = 3 gegen√ºber 75 f√ºr RF = 1).  Wenn wir jedoch die Last erh√∂hen, indem wir Daten in 8 Tabellen in jeweils 5 Streams laden (insgesamt 40), betr√§gt der Verlust von RF = 3 bereits das 2,7-fache (375 Sekunden gegen√ºber 138). <br><br>  Vielleicht ist dies zum Teil das Geheimnis erfolgreicher DataStax-Tests f√ºr CS-Lasttests, denn f√ºr HB an unserem Stand hatte die √Ñnderung des Replikationsfaktors von 2 auf 3 keine Auswirkung.  Das hei√üt  Discs sind in unserer Konfiguration nicht der Flaschenhals f√ºr HB.  Es gibt jedoch viele andere Fallstricke, da unsere Version von HB leicht gepatcht und verdeckt war, die Umgebungen v√∂llig anders sind usw.  Es ist auch erw√§hnenswert, dass ich vielleicht nicht wei√ü, wie ich CS richtig vorbereiten soll, und dass es einige effektivere M√∂glichkeiten gibt, damit zu arbeiten, und ich hoffe, dass wir in den Kommentaren darauf aufmerksam werden.  Aber das Wichtigste zuerst. <br><br>  Alle Tests wurden an einem Eisencluster durchgef√ºhrt, der aus 4 Servern mit jeweils einer Konfiguration bestand: <br><br>  <i>CPU: Xeon E5-2680 v4 @ 2.40GHz 64 Threads.</i> <i><br></i>  <i>Festplatten: 12 St√ºck SATA HDD</i> <i><br></i>  <i>Java-Version: 1.8.0_111</i> <i><br></i> <br><br>  CS Version: 3.11.5 <br><br><div class="spoiler">  <b class="spoiler_title">Parameter cassandra.yml</b> <div class="spoiler_text">  num_tokens: 256 <br>  hinted_handoff_enabled: true <br>  hinted_handoff_throttle_in_kb: 1024 <br>  max_hints_delivery_threads: 2 <br>  hints_directory: / data10 / cassandra / hints <br>  hints_flush_period_in_ms: 10000 <br>  max_hints_file_size_in_mb: 128 <br>  batchlog_replay_throttle_in_kb: 1024 <br>  Authentifikator: AllowAllAuthenticator <br>  authorizer: AllowAllAuthorizer <br>  role_manager: CassandraRoleManager <br>  role_validity_in_ms: 2000 <br>  permissions_validity_in_ms: 2000 <br>  credentials_validity_in_ms: 2000 <br>  Partitionierer: org.apache.cassandra.dht.Murmur3Partitioner <br>  data_file_directories: <br>  - / data1 / cassandra / data # Jedes dataN-Verzeichnis ist ein separates Laufwerk <br>  - / data2 / cassandra / data <br>  - / data3 / cassandra / data <br>  - / data4 / cassandra / data <br>  - / data5 / cassandra / data <br>  - / data6 / cassandra / data <br>  - / data7 / cassandra / data <br>  - / data8 / cassandra / data <br>  Commitlog-Verzeichnis: / data9 / cassandra / commitlog <br>  cdc_enabled: false <br>  disk_failure_policy: stop <br>  commit_failure_policy: Stop <br>  prepare_statements_cache_size_mb: <br>  thrift_prepared_statements_cache_size_mb: <br>  key_cache_size_in_mb: <br>  key_cache_save_period: 14400 <br>  row_cache_size_in_mb: 0 <br>  row_cache_save_period: 0 <br>  counter_cache_size_in_mb: <br>  counter_cache_save_period: 7200 <br>  gespeichertes_caches_verzeichnis: / data10 / cassandra / saved_caches <br>  commitlog_sync: periodisch <br>  commitlog_sync_period_in_ms: 10000 <br>  commitlog_segment_size_in_mb: 32 <br>  seed_provider: <br>  - Klassenname: org.apache.cassandra.locator.SimpleSeedProvider <br>  parameter: <br>  - Samen: "*, *" <br>  concurrent_reads: 256 # ausprobiert 64 - kein Unterschied bemerkt <br>  concurrent_writes: 256 # ausprobiert 64 - kein Unterschied bemerkt <br>  concurrent_counter_writes: 256 # versucht 64 - kein Unterschied bemerkt <br>  concurrent_materialized_view_writes: 32 <br>  memtable_heap_space_in_mb: 2048 # versuchte 16 GB - war langsamer <br>  memtable_allocation_type: heap_buffer <br>  index_summary_capacity_in_mb: <br>  index_summary_resize_interval_in_minutes: 60 <br>  trickle_fsync: false <br>  trickle_fsync_interval_in_kb: 10240 <br>  Speicherport: 7000 <br>  ssl_storage_port: 7001 <br>  listen_address: * <br>  broadcast_address: * <br>  listen_on_broadcast_address: true <br>  internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator <br>  start_native_transport: true <br>  native_transport_port: 9042 <br>  start_rpc: true <br>  rpc_address: * <br>  rpc_port: 9160 <br>  rpc_keepalive: wahr <br>  rpc_server_type: sync <br>  thrift_framed_transport_size_in_mb: 15 <br>  incremental_backups: false <br>  snapshot_before_compaction: false <br>  auto_snapshot: true <br>  column_index_size_in_kb: 64 <br>  column_index_cache_size_in_kb: 2 <br>  concurrent_compactors: 4 <br>  compaction_throughput_mb_per_sec: 1600 <br>  sstable_preemptive_open_interval_in_mb: 50 <br>  read_request_timeout_in_ms: 100000 <br>  range_request_timeout_in_ms: 200000 <br>  write_request_timeout_in_ms: 40000 <br>  counter_write_request_timeout_in_ms: 100000 <br>  cas_contention_timeout_in_ms: 20000 <br>  truncate_request_timeout_in_ms: 60000 <br>  request_timeout_in_ms: 200000 <br>  slow_query_log_timeout_in_ms: 500 <br>  cross_node_timeout: false <br>  endpoint_snitch: GossipingPropertyFileSnitch <br>  dynamic_snitch_update_interval_in_ms: 100 <br>  dynamic_snitch_reset_interval_in_ms: 600000 <br>  dynamic_snitch_badness_threshold: 0.1 <br>  request_scheduler: org.apache.cassandra.scheduler.NoScheduler <br>  server_encryption_options: <br>  internode_encryption: keine <br>  client_encryption_options: <br>  aktiviert: false <br>  internode_compression: dc <br>  inter_dc_tcp_nodelay: false <br>  tracetype_query_ttl: 86400 <br>  tracetype_repair_ttl: 604800 <br>  enable_user_defined_functions: false <br>  enable_scripted_user_defined_functions: false <br>  windows_timer_interval: 1 <br>  transparente_Daten_Verschl√ºsselungsoptionen: <br>  aktiviert: false <br>  tombstone_warn_threshold: 1000 <br>  tombstone_failure_threshold: 100000 <br>  batch_size_warn_threshold_in_kb: 200 <br>  batch_size_fail_threshold_in_kb: 250 <br>  unlogged_batch_across_partitions_warn_threshold: 10 <br>  compaction_large_partition_warning_threshold_mb: 100 <br>  gc_warn_threshold_in_ms: 1000 <br>  back_pressure_enabled: false <br>  enable_materialized_views: true <br>  enable_sasi_indexes: true <br></div></div><br>  GC-Einstellungen: <br><br><div class="spoiler">  <b class="spoiler_title">### CMS-Einstellungen</b> <div class="spoiler_text">  -XX: + UseParNewGC <br>  -XX: + UseConcMarkSweepGC <br>  -XX: + CMSParallelRemarkEnabled <br>  -XX: SurvivorRatio = 8 <br>  -XX: MaxTenuringThreshold = 1 <br>  -XX: CMSInitiatingOccupancyFraction = 75 <br>  -XX: + UseCMSInitiatingOccupancyOnly <br>  -XX: CMSWaitDuration = 10000 <br>  -XX: + CMSParallelInitialMarkEnabled <br>  -XX: + CMSEdenChunksRecordAlways <br>  -XX: + CMSClassUnloadingEnabled <br><br></div></div><br>  Dem Speicher jvm.options wurden 16 GB zugewiesen (immer noch 32 GB ausprobiert, es wurde kein Unterschied festgestellt). <br><br>  Das Erstellen von Tabellen wurde mit dem folgenden Befehl ausgef√ºhrt: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> ks.t1 (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span> <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span> PRIMARY <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>, title <span class="hljs-built_in"><span class="hljs-built_in">text</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> compression = {<span class="hljs-string"><span class="hljs-string">'sstable_compression'</span></span>: <span class="hljs-string"><span class="hljs-string">'LZ4Compressor'</span></span>, <span class="hljs-string"><span class="hljs-string">'chunk_length_kb'</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span>};</code> </pre> <br>  HB-Version: 1.2.0-cdh5.14.2 (in der Klasse org.apache.hadoop.hbase.regionserver.HRegion haben wir MetricsRegion ausgeschlossen, was zu GC mit mehr als 1000 Regionen auf RegionServer f√ºhrte) <br><br><div class="spoiler">  <b class="spoiler_title">Nicht standardm√§√üige HBase-Optionen</b> <div class="spoiler_text">  zookeeper.session.timeout: 120000 <br>  hbase.rpc.timeout: 2 Minute (n) <br>  hbase.client.scanner.timeout.period: 2 minute (n) <br>  hbase.master.handler.count: 10 <br>  hbase.regionserver.lease.period, hbase.client.scanner.timeout.period: 2 minute (n) <br>  hbase.regionserver.handler.count: 160 <br>  hbase.regionserver.metahandler.count: 30 <br>  hbase.regionserver.logroll.period: 4 Stunde (n) <br>  hbase.regionserver.maxlogs: 200 <br>  hbase.hregion.memstore.flush.size: 1 GiB <br>  hbase.hregion.memstore.block.multiplier: 6 <br>  hbase.hstore.compactionThreshold: 5 <br>  hbase.hstore.blockingStoreFiles: 200 <br>  hbase.hregion.majorcompaction: 1 Tag (e) <br>  HBase Service Advanced Configuration Snippet (Sicherheitsventil) f√ºr hbase-site.xml: <br>  hbase.regionserver.wal.codecorg.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec <br>  hbase.master.namespace.init.timeout3600000 <br>  hbase.regionserver.optionalcacheflushinterval18000000 <br>  hbase.regionserver.thread.compaction.large12 <br>  hbase.regionserver.wal.enablecompressiontrue <br>  hbase.hstore.compaction.max.size1073741824 <br>  hbase.server.compactchecker.interval.multiplier200 <br>  Java-Konfigurationsoptionen f√ºr HBase RegionServer: <br>  -XX: + UseParNewGC -XX: + UseConcMarkSweepGC -XX: CMSInitiatingOccupancyFraction = 70 -XX: + CMSParallelRemarkEnabled -XX: ReservedCodeCacheSize = 256m <br>  hbase.snapshot.master.timeoutMillis: 2 Minute (n) <br>  hbase.snapshot.region.timeout: 2 minute (n) <br>  hbase.snapshot.master.timeout.millis: 2 Minute (n) <br>  HBase REST Server Maximale Protokollgr√∂√üe: 100 MiB <br>  Maximale Anzahl von HBase REST Server-Protokolldateisicherungen: 5 <br>  HBase Thrift Server Maximale Protokollgr√∂√üe: 100 MiB <br>  Maximale Anzahl von HBase Thrift Server-Protokolldateisicherungen: 5 <br>  Master Max Log Size: 100 MiB <br>  Maximale Sicherung der Master-Protokolldatei: 5 <br>  RegionServer Max. Protokollgr√∂√üe: 100 MiB <br>  Maximale RegionServer-Protokolldateisicherungen: 5 <br>  HBase Active Master-Erkennungsfenster: 4 Minute (n) <br>  dfs.client.hedged.read.threadpool.size: 40 <br>  dfs.client.hedged.read.threshold.millis: 10 Millisekunden <br>  hbase.rest.threads.min: 8 <br>  hbase.rest.threads.max: 150 <br>  Maximale Prozessdateideskriptoren: 180.000 <br>  hbase.thrift.minWorkerThreads: 200 <br>  hbase.master.executor.openregion.threads: 30 <br>  hbase.master.executor.closeregion.threads: 30 <br>  hbase.master.executor.serverops.threads: 60 <br>  hbase.regionserver.thread.compaction.small: 6 <br>  hbase.ipc.server.read.threadpool.size: 20 <br>  Region Mover-Themen: 6 <br>  Client Java Heap Gr√∂√üe in Bytes: 1 GiB <br>  HBase REST Server-Standardgruppe: 3 GiB <br>  HBase Thrift Server Standardgruppe: 3 GiB <br>  Java-Heap-Gr√∂√üe des HBase-Masters in Bytes: 16 GiB <br>  Java-Heap-Gr√∂√üe von HBase RegionServer in Bytes: 32 GiB <br><br>  + ZooKeeper <br>  maxClientCnxns: 601 <br>  maxSessionTimeout: 120000 </div></div><br>  Tabellen erstellen: <br>  <i>hbase org.apache.hadoop.hbase.util.RegionSplitter ns: t1 UniformSplit -c 64 -f vgl</i> <i><br></i>  <i>alter 'ns: t1', {NAME =&gt; 'cf', DATA_BLOCK_ENCODING =&gt; 'FAST_DIFF', COMPRESSION =&gt; 'GZ'}</i> <br><br>  Es gibt einen wichtigen Punkt: In der DataStax-Beschreibung ist nicht angegeben, wie viele Regionen zum Erstellen der HB-Tabellen verwendet wurden, obwohl dies f√ºr gro√üe Volumes von entscheidender Bedeutung ist.  Daher wurde f√ºr die Tests die Zahl = 64 gew√§hlt, wodurch bis zu 640 GB gespeichert werden k√∂nnen, d. H.  mittelgro√üe Tabelle. <br><br>  Zum Zeitpunkt des Tests verf√ºgte HBase √ºber 22.000 Tabellen und 67.000 Regionen (dies w√§re f√ºr Version 1.2.0 t√∂dlich, wenn nicht der oben erw√§hnte Patch). <br><br>  Nun zum Code.  Da nicht klar war, welche Konfigurationen f√ºr eine bestimmte Datenbank vorteilhafter sind, wurden die Tests in verschiedenen Kombinationen durchgef√ºhrt.  Das hei√üt  In einigen Tests wurde die Last gleichzeitig auf 4 Tabellen verteilt (alle 4 Knoten wurden f√ºr die Verbindung verwendet).  In anderen Tests arbeiteten sie mit 8 verschiedenen Tischen.  In einigen F√§llen betrug die Chargengr√∂√üe 100, in anderen 200 (Chargenparameter - siehe Code unten).  Die Datengr√∂√üe f√ºr value betr√§gt 10 Byte oder 100 Byte (dataSize).  Insgesamt wurden jedes Mal 5 Millionen Datens√§tze in jede Tabelle geschrieben und abgezogen.  Gleichzeitig wurden 5 Streams in jede Tabelle geschrieben / gelesen (Stream-Nummer ist thNum), von denen jeder seinen eigenen Schl√ºsselbereich verwendete (Anzahl = 1 Million): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"BEGIN BATCH "</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); sb.append(<span class="hljs-string"><span class="hljs-string">"INSERT INTO "</span></span>) .append(tableName) .append(<span class="hljs-string"><span class="hljs-string">"(id, title) "</span></span>) .append(<span class="hljs-string"><span class="hljs-string">"VALUES ("</span></span>) .append(key) .append(<span class="hljs-string"><span class="hljs-string">", '"</span></span>) .append(value) .append(<span class="hljs-string"><span class="hljs-string">"');"</span></span>); key++; } sb.append(<span class="hljs-string"><span class="hljs-string">"APPLY BATCH;"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); session.execute(query); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"SELECT * FROM "</span></span>).append(tableName).append(<span class="hljs-string"><span class="hljs-string">" WHERE id IN ("</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { sb = sb.append(key); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i+<span class="hljs-number"><span class="hljs-number">1</span></span> &lt; batch) sb.append(<span class="hljs-string"><span class="hljs-string">","</span></span>); key++; } sb = sb.append(<span class="hljs-string"><span class="hljs-string">");"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); ResultSet rs = session.execute(query); } }</code> </pre><br>  Dementsprechend wurde f√ºr HB eine √§hnliche Funktionalit√§t bereitgestellt: <br><br><pre> <code class="java hljs">Configuration conf = getConf(); HTable table = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HTable(conf, keyspace + <span class="hljs-string"><span class="hljs-string">":"</span></span> + tableName); table.setAutoFlush(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); List&lt;Get&gt; lGet = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); List&lt;Put&gt; lPut = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] cf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"cf"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] qf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lPut.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Put p = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Put(makeHbaseRowKey(key)); String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); p.addColumn(cf, qf, value.getBytes()); lPut.add(p); key++; } table.put(lPut); table.flushCommits(); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lGet.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Get g = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Get(makeHbaseRowKey(key)); lGet.add(g); key++; } Result[] rs = table.get(lGet); } }</code> </pre><br>  Da der Client f√ºr die gleichm√§√üige Verteilung der Daten in HB sorgen muss, sah die Key-Salting-Funktion folgenderma√üen aus: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] makeHbaseRowKey(<span class="hljs-keyword"><span class="hljs-keyword">long</span></span> key) { <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] nonSaltedRowKey = Bytes.toBytes(key); CRC32 crc32 = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CRC32(); crc32.update(nonSaltedRowKey); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> crc32Value = crc32.getValue(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] salt = Arrays.copyOfRange(Bytes.toBytes(crc32Value), <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ArrayUtils.addAll(salt, nonSaltedRowKey); }</code> </pre><br>  Am interessantesten sind nun die Ergebnisse: <br><br><img src="https://habrastorage.org/webt/id/yd/pc/idydpc9plsmulsycf0i-wqy3c3c.png"><br><br>  Das selbe wie ein Graph: <br><br><img src="https://habrastorage.org/webt/72/ag/o1/72ago1u2gdnlufjanqwavk5p1jk.png"><br><br>  Der Vorteil von HB ist so erstaunlich, dass der Verdacht eines Engpasses in den CS-Einstellungen besteht.  Googeln und Torsion der offensichtlichsten Parameter (wie concurrent_writes oder memtable_heap_space_in_mb) f√ºhrten jedoch nicht zu einer Beschleunigung.  Gleichzeitig sind die Protokolle sauber, schw√∂ren auf nichts. <br><br>  Die Daten verteilen sich gleichm√§√üig auf die Knoten, die Statistiken aller Knoten sind ungef√§hr gleich. <br><br><div class="spoiler">  <b class="spoiler_title">Hier ist die Statistik in der Tabelle mit einem der Knoten</b> <div class="spoiler_text">  Tastenk√ºrzel: ks <br>  Read Count: 9383707 <br>  Leselatenz: 0.04287025042448576 ms <br>  Anzahl schreiben: 15462012 <br>  Schreibwartezeit: 0.1350068438699957 ms <br>  Ausstehende Flushes: 0 <br>  Tabelle: t1 <br>  SSTable-Anzahl: 16 <br>  Speicherplatz (live): 148,59 MiB <br>  Speicherplatz (gesamt): 148,59 MiB <br>  Von Snapshots belegter Speicherplatz (gesamt): 0 Byte <br>  Ausgenutzter Heapspeicher (gesamt): 5,17 MiB <br>  SSTabiles Kompressionsverh√§ltnis: 0,5720989576459437 <br>  Anzahl der Partitionen (Sch√§tzung): 3970323 <br>  Anzahl der Memtable-Zellen: 0 <br>  Speicherbare Datengr√∂√üe: 0 Bytes <br>  Speicher au√üerhalb des verwendeten Heapspeichers: 0 Byte <br>  Anzahl der Memtable-Schalter: 5 <br>  Lokale Lesez√§hlung: 2346045 <br>  Lokale Leselatenz: NaN ms <br>  Lokale Schreibanzahl: 3865503 <br>  Lokale Schreibwartezeit: NaN ms <br>  Ausstehende Sp√ºlungen: 0 <br>  Prozent repariert: 0.0 <br>  Bloom Filter falsch positiv: 25 <br>  Bloom Filter falsches Verh√§ltnis: 0,00000 <br>  Verwendeter Bloom-Filterraum: 4,57 MiB <br>  Bloom-Filter aus verwendetem Heap-Speicher: 4,57 MiB <br>  Indexzusammenfassung aus verwendetem Heap-Speicher: 590,02 KiB <br>  Komprimierungsmetadaten au√üerhalb des verwendeten Heapspeichers: 19,45 KiB <br>  Komprimierte Partition, Mindestbytes: 36 <br>  Komprimierte Partition, maximale Bytes: 42 <br>  Mittlere Bytes der komprimierten Partition: 42 <br>  Durchschnittliche Anzahl lebender Zellen pro Schicht (letzte f√ºnf Minuten): NaN <br>  Maximale Anzahl lebender Zellen pro Schicht (letzte f√ºnf Minuten): 0 <br>  Durchschnittliche Grabsteine ‚Äã‚Äãpro Scheibe (letzte f√ºnf Minuten): NaN <br>  Maximale Grabsteine ‚Äã‚Äãpro Scheibe (letzte f√ºnf Minuten): 0 <br>  Verworfene Mutationen: 0 Bytes <br></div></div><br>  Der Versuch, die Gr√∂√üe des Stapels zu reduzieren (bis zum Senden nacheinander), hatte keine Auswirkung, es wurde nur noch schlimmer.  Es ist m√∂glich, dass dies tats√§chlich die maximale Leistung f√ºr CS ist, da die mit CS erzielten Ergebnisse mit denen f√ºr DataStax vergleichbar sind - etwa Hunderttausende von Vorg√§ngen pro Sekunde.  Wenn Sie sich die Ressourcennutzung ansehen, werden Sie au√üerdem feststellen, dass CS viel mehr CPU und Festplatten ben√∂tigt: <br><br><img src="https://habrastorage.org/webt/us/fo/i4/usfoi4-mgkktzlosilmz2ogm7uu.png"><br>  <i>Die Abbildung zeigt die Auslastung w√§hrend des Durchlaufs aller Tests hintereinander f√ºr beide Datenbanken.</i> <br><br>  In Bezug auf die leistungsstarken Lesevorteile von HB.  Es ist ersichtlich, dass bei beiden Datenbanken die Festplattenauslastung w√§hrend des Lesens √§u√üerst gering ist (Lesetests sind der letzte Teil des Testzyklus f√ºr jede Datenbank, z. B. f√ºr CS von 15:20 bis 15:40 Uhr).  Im Fall von HB ist der Grund klar: Die meisten Daten h√§ngen im Speicher, im Memstore, und einige wurden im Blockcache zwischengespeichert.  Was CS betrifft, ist nicht ganz klar, wie es funktioniert. Die Festplattenauslastung ist jedoch ebenfalls nicht sichtbar. Es wurde jedoch nur f√ºr den Fall versucht, den row_cache_size_in_mb = 2048-Cache einzuschalten und caching = {'keys': 'ALL', 'rows_per_partition': ' 2.000.000 '}, aber das machte es noch ein bisschen schlimmer. <br><br>  Es lohnt sich auch noch einmal, einen signifikanten Punkt √ºber die Anzahl der Regionen in HB zu sagen.  In unserem Fall wurde der Wert 64 angezeigt.Wenn Sie ihn verringern und auf beispielsweise 4 einstellen, sinkt die Geschwindigkeit beim Lesen um das 2-fache.  Der Grund ist, dass der Memstore schneller verstopft und die Dateien h√§ufiger gel√∂scht werden und beim Lesen mehr Dateien verarbeitet werden m√ºssen, was f√ºr HB eine recht komplizierte Operation ist.  Unter realen Bedingungen kann dies durch √úberlegen der Strategie der Voraufteilung und Verdichtung behoben werden. Insbesondere verwenden wir ein selbst erstelltes Dienstprogramm, das den M√ºll sammelt und HFiles st√§ndig im Hintergrund komprimiert.  Es ist m√∂glich, dass f√ºr die DataStax-Tests im Allgemeinen 1 Region pro Tabelle zugewiesen wurde (was nicht korrekt ist), und dies w√ºrde etwas klarer machen, warum HB bei ihren Lesetests so viel verloren hat. <br><br>  Die vorl√§ufigen Schlussfolgerungen daraus sind wie folgt.  Unter der Annahme, dass beim Testen keine groben Fehler gemacht wurden, ist Cassandra wie ein Koloss mit Lehmf√º√üen.  Genauer gesagt, w√§hrend sie auf einem Bein balanciert, wie auf dem Bild am Anfang des Artikels, zeigt sie relativ gute Ergebnisse, aber wenn sie unter den gleichen Bedingungen k√§mpft, verliert sie geradezu.  Gleichzeitig haben wir unter Ber√ºcksichtigung der geringen CPU-Auslastung unserer Hardware gelernt, zwei RegionServer-HBs pro Host zu installieren und damit die Produktivit√§t zu verdoppeln.  Das hei√üt  Unter Ber√ºcksichtigung der Ressourcennutzung ist die Situation f√ºr CS noch bedauerlicher. <br><br>  Nat√ºrlich sind diese Tests sehr synthetisch und die Datenmenge, die hier verwendet wurde, ist relativ bescheiden.  Es ist m√∂glich, dass beim Umschalten auf Terabyte die Situation anders ist, aber wenn wir f√ºr HB Terabyte laden k√∂nnen, hat sich dies f√ºr CS als problematisch herausgestellt.  Auch bei diesen Datentr√§gern wurde h√§ufig eine OperationTimedOutException ausgel√∂st, obwohl die Parameter f√ºr die Antworterwartung im Vergleich zu den Standardwerten bereits um ein Vielfaches erh√∂ht wurden. <br><br>  Ich hoffe, dass wir durch gemeinsame Anstrengungen die CS-Engp√§sse finden und wenn wir es schaffen, sie zu beschleunigen, werde ich definitiv am Ende des Beitrags Informationen √ºber die Endergebnisse hinzuf√ºgen. <br><br>  <b>UPD: Die</b> folgenden Richtlinien wurden beim CS-Setup angewendet: <br><br>  <i>disk_optimization_strategy: drehen</i> <i><br></i>  <i>MAX_HEAP_SIZE = "32G"</i> <i><br></i>  <i>HEAP_NEWSIZE = "3200M"</i> <i><br></i>  <i>-Xms32G</i> <i><br></i>  <i>-Xmx32G</i> <i><br></i>  <i>-XX: + UseG1GC</i> <i><br></i>  <i>-XX: G1RSetUpdatingPauseTimePercent = 5</i> <i><br></i>  <i>-XX: MaxGCPauseMillis = 500</i> <i><br></i>  <i>-XX: InitiatingHeapOccupancyPercent = 70</i> <i><br></i>  <i>-XX: ParallelGCThreads = 32</i> <i><br></i>  <i>-XX: ConcGCThreads = 8</i> <br><br>  Da dies bei den Betriebssystemeinstellungen ein ziemlich langer und komplizierter Vorgang ist (Root abrufen, Server neu starten usw.), wurden diese Empfehlungen nicht angewendet.  Andererseits sind beide Datenbanken unter gleichen Bedingungen, sodass alles fair ist. <br><br>  Im Codeteil wird ein Connector f√ºr alle Threads erstellt, die in die Tabelle schreiben: <br><pre> <code class="java hljs">connector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CassandraConnector(); connector.connect(node, <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>, CL); session = connector.getSession(); session.getCluster().getConfiguration().getSocketOptions().setConnectTimeoutMillis(<span class="hljs-number"><span class="hljs-number">120000</span></span>); KeyspaceRepository sr = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> KeyspaceRepository(session); sr.useKeyspace(keyspace); prepared = session.prepare(<span class="hljs-string"><span class="hljs-string">"insert into "</span></span> + tableName + <span class="hljs-string"><span class="hljs-string">" (id, title) values (?, ?)"</span></span>);</code> </pre> <br><br>  Daten wurden verbindlich √ºbermittelt: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); session.execute(prepared.bind(key, value)); }</code> </pre> <br><br>  Dies hatte keinen wesentlichen Einfluss auf die Aufnahmeleistung.  Aus Gr√ºnden der Zuverl√§ssigkeit habe ich den Ladevorgang mit dem YCSB-Tool gestartet, absolut das gleiche Ergebnis.  Unten ist die Statistik f√ºr einen Thread (von 4): <br><br>  <i>2020-01-18 14: 41: 53: 180 315 Sek .: 10.000.000 Operationen;</i>  <i>21589,1 aktuelle Operationen / Sek .;</i>  <i>[CLEANUP: Count = 100, Max = 2236415, Min = 1, Avg = 22356.39, 90 = 4, 99 = 24, 99.9 = 2236415, 99.99 = 2236415] [INSERT: Count = 119551, Max = 174463, Min = 273, Durchschn. = 2582,71, 90 = 3491, 99 = 16767, 99,9 = 99711, 99,99 = 171263]</i> <i><br></i>  <i>[OVERALL], RunTime (ms), 315539</i> <i><br></i>  <i>[OVERALL], Throughput (ops / sec), 31691.803548848162</i> <i><br></i>  <i>[TOTAL_GCS_PS_Scavenge], Count, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_Scavenge], Zeit (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_Scavenge], Zeit (%), 0,7710615803434757</i> <i><br></i>  <i>[TOTAL_GCS_PS_MarkSweep], Count, 0</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_MarkSweep], Zeit (ms), 0</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_MarkSweep], Zeit (%), 0.0</i> <i><br></i>  <i>[TOTAL_GCs], Count, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME], Zeit (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME_%], Zeit (%), 0,7710615803434757</i> <i><br></i>  <i>[EINF√úGEN], Operationen, 10.000.000</i> <i><br></i>  <i>[INSERT], AverageLatency (us), 3114.2427012</i> <i><br></i>  <i>[INSERT], MinLatency (us), 269</i> <i><br></i>  <i>[EINF√úGEN], MaxLatency (us), 609279</i> <i><br></i>  <i>[EINF√úGEN], 95thPercentileLatency (us), 5007</i> <i><br></i>  <i>[INSERT], 99thPercentileLatency (us), 33439</i> <i><br></i>  <i>[INSERT], Return = OK, 10000000</i> <i><br></i> <br><br>  Hier sehen Sie, dass die Geschwindigkeit eines Streams ungef√§hr 32.000 Datens√§tze pro Sekunde betr√§gt, 4 Streams gearbeitet haben und 128.000.Anscheinend gibt es bei den aktuellen Einstellungen des Festplattensubsystems nichts mehr auszudr√ºcken. <br><br>  √úber das Lesen interessanter.  Dank der Ratschl√§ge der Genossen konnte er radikal beschleunigen.  Das Ablesen wurde nicht in 5, sondern in 100 Str√∂men durchgef√ºhrt. Eine Erh√∂hung auf 200 hatte keinen Effekt.  Ebenfalls zum Builder hinzugef√ºgt: <br>  .withLoadBalancingPolicy (neue TokenAwarePolicy (DCAwareRoundRobinPolicy.builder (). build ()) <br><br>  Als Ergebnis ergab der Test fr√ºher 159 644 Operationen (5 Streams, 4 Tabellen, 100 Batch), jetzt: <br>  100 Threads, 4 Tabellen, Batch = 1 (einzeln): 301 969 Ops <br>  100 Threads, 4 Tabellen, Batch = 10: 447 608 Ops <br>  100 Threads, 4 Tabellen, Batch = 100: 625 655 Ops <br><br>  Da die Ergebnisse bei Chargen besser sind, habe ich √§hnliche * Tests mit HB durchgef√ºhrt: <br><img src="https://habrastorage.org/webt/ct/bk/-y/ctbk-yrecbwegasrbpauq6f1vv8.png"><br>  <i>* Da bei der Arbeit mit 400 Threads die zuvor verwendete Funktion RandomStringUtils die CPU zu 100% belastete, wurde sie durch einen schnelleren Generator ersetzt.</i> <br><br>  Daher f√ºhrt eine Erh√∂hung der Anzahl der Threads beim Laden von Daten zu einer geringf√ºgigen Erh√∂hung der HB-Leistung. <br><br>  Was das Lesen betrifft, sind hier die Ergebnisse mehrerer Optionen.  Auf Anforderung von <a href="https://habr.com/ru/users/0x62ash/" class="user_link">0x62ash</a> wurde der Befehl flush vor dem Lesen ausgef√ºhrt, und es werden auch verschiedene andere Optionen zum Vergleich angegeben: <br>  Memstore - Lesen aus dem Speicher, d.h.  vor dem Sp√ºlen auf die Festplatte. <br>  HFile + zip - Lesen von Dateien, die mit dem GZ-Algorithmus komprimiert wurden. <br>  HFile + Upzip - Aus Dateien ohne Komprimierung lesen. <br><br>  Eine interessante Funktion ist bemerkenswert - kleine Dateien (siehe Feld ‚ÄûDaten‚Äú, in das 10 Bytes geschrieben werden) werden langsamer verarbeitet, insbesondere wenn sie komprimiert sind.  Dies ist nat√ºrlich nur bis zu einer bestimmten Gr√∂√üe m√∂glich. Offensichtlich wird eine 5-GB-Datei nicht schneller als 10 MB verarbeitet. Dies zeigt jedoch deutlich, dass in all diesen Tests immer noch kein Feld f√ºr die Erforschung verschiedener Konfigurationen vorhanden ist. <br><br>  Aus Interesse habe ich den YCSB-Code f√ºr die Arbeit mit HB-Chargen von 100 St√ºck korrigiert, um die Latenz und mehr zu messen.  Unten ist das Ergebnis der Arbeit von 4 Kopien, die in ihre Tabellen mit jeweils 100 Threads geschrieben haben.  Es stellte sich Folgendes heraus: <br><div class="spoiler">  <b class="spoiler_title">Eine Operation = 100 Datens√§tze</b> <div class="spoiler_text">  [OVERALL], RunTime (ms), 1165415 <br>  [OVERALL], Durchsatz (ops / sec), 858.06343662987 <br>  [TOTAL_GCS_PS_Scavenge], Count, 798 <br>  [TOTAL_GC_TIME_PS_Scavenge], Zeit (ms), 7346 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Time (%), 0.6303334005483026 <br>  [TOTAL_GCS_PS_MarkSweep], Count, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], Zeit (ms), 74 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Zeit (%), 0,006349669431061038 <br>  [TOTAL_GCs], Count, 799 <br>  [TOTAL_GC_TIME], Zeit (ms), 7420 <br>  [TOTAL_GC_TIME_%], Time (%), 0.6366830699793635 <br>  [EINF√úGEN], Operationen, 1.000.000 <br>  [INSERT], AverageLatency (us), 115893.891644 <br>  [INSERT], MinLatency (us), 14528 <br>  [INSERT], MaxLatency (us), 1470463 <br>  [EINF√úGEN], 95thPercentileLatency (us), 248319 <br>  [EINF√úGEN], 99thPercentileLatency (us), 445951 <br>  [INSERT], Return = OK, 1,000,000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Schlie√üen von zookeeper sessionid = 0x36f98ad0a4ad8cc <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Sitzung: 0x36f98ad0a4ad8cc geschlossen <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread wurde heruntergefahren <br>  [OVERALL], RunTime (ms), 1165806 <br>  [OVERALL], Durchsatz (ops / sec), 857.7756504941646 <br>  [TOTAL_GCS_PS_Scavenge], Count, 776 <br>  [TOTAL_GC_TIME_PS_Scavenge], Zeit (ms), 7517 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Time (%), 0.6447899564764635 <br>  [TOTAL_GCS_PS_MarkSweep], Count, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], Zeit (ms), 63 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Zeit (%), 0,005403986598113236 <br>  [TOTAL_GCs], Count, 777 <br>  [TOTAL_GC_TIME], Zeit (ms), 7580 <br>  [TOTAL_GC_TIME_%], Zeit (%), 0,6501939430745767 <br>  [EINF√úGEN], Operationen, 1.000.000 <br>  [INSERT], AverageLatency (us), 116042.207936 <br>  [INSERT], MinLatency (us), 14056 <br>  [INSERT], MaxLatency (us), 1462271 <br>  [EINF√úGEN], 95thPercentileLatency (us), 250239 <br>  [INSERT], 99thPercentileLatency (us), 446719 <br>  [INSERT], Return = OK, 1,000,000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Schlie√üen von zookeeper sessionid = 0x26f98ad07b6d67e <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Sitzung: 0x26f98ad07b6d67e geschlossen <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread wurde heruntergefahren <br>  [OVERALL], RunTime (ms), 1165999 <br>  [OVERALL], Durchsatz (ops / sec), 857.63366863951 <br>  [TOTAL_GCS_PS_Scavenge], Count, 818 <br>  [TOTAL_GC_TIME_PS_Scavenge], Zeit (ms), 7557 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6481137633908777 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 79 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.006775305982252128 <br> [TOTAL_GCs], Count, 819 <br> [TOTAL_GC_TIME], Time(ms), 7636 <br> [TOTAL_GC_TIME_%], Time(%), 0.6548890693731299 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116172.212864 <br> [INSERT], MinLatency(us), 7952 <br> [INSERT], MaxLatency(us), 1458175 <br> [INSERT], 95thPercentileLatency(us), 250879 <br> [INSERT], 99thPercentileLatency(us), 446463 <br> [INSERT], Return=OK, 1000000 <br><br> 20/01/19 13:19:17 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x36f98ad0a4ad8cd <br> 20/01/19 13:19:17 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cd closed <br> 20/01/19 13:19:17 INFO zookeeper.ClientCnxn: EventThread shut down <br> [OVERALL], RunTime(ms), 1166860 <br> [OVERALL], Throughput(ops/sec), 857.000839860823 <br> [TOTAL_GCS_PS_Scavenge], Count, 707 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7239 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6203829079752499 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 67 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.0057419056270675145 <br> [TOTAL_GCs], Count, 708 <br> [TOTAL_GC_TIME], Time(ms), 7306 <br> [TOTAL_GC_TIME_%], Time(%), 0.6261248136023173 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116230.849308 <br> [INSERT], MinLatency(us), 7352 <br> [INSERT], MaxLatency(us), 1443839 <br> [INSERT], 95thPercentileLatency(us), 250623 <br> [INSERT], 99thPercentileLatency(us), 447487 <br> [INSERT], Return=OK, 1000000 </div></div><br><br> ,    CS AverageLatency(us)   3114,   HB AverageLatency(us) = 1162 (,  1  = 100     ). <br><br>      ‚Äî        HBase.   ,  SSD       .   ,       ,    ,     4 ,  400    ,      .   :  ‚Äî  .  .   ScyllaDB     ,    ‚Ä¶ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de484096/">https://habr.com/ru/post/de484096/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de484076/index.html">Speicherort f√ºr Kryptow√§hrung: Besteuerung von Kryptow√§hrungen in verschiedenen L√§ndern</a></li>
<li><a href="../de484084/index.html">1C-Bitrix und ein Versuch, es einzuf√ºhren</a></li>
<li><a href="../de484088/index.html">Passwort-Hit-Parade (Analyse von ~ 5 Milliarden Passw√∂rtern nach Undichtigkeiten)</a></li>
<li><a href="../de484090/index.html">Neue IT-Infrastruktur f√ºr das Russian Post Data Center</a></li>
<li><a href="../de484092/index.html">Etwas gekleidete F√ºrsten und Adlige</a></li>
<li><a href="../de484100/index.html">Arbeiten mit der Oberfl√§che im Google Maps SDK f√ºr Android</a></li>
<li><a href="../de484102/index.html">PHP vs Python vs Ruby on Rails: Detaillierter Vergleich</a></li>
<li><a href="../de484106/index.html">MVCC in PostgreSQL-6. Vakuum</a></li>
<li><a href="../de484108/index.html">Etherblade.net-Encapsulator und Importersetzung f√ºr Netzwerkkomponenten (Teil 2)</a></li>
<li><a href="../de484112/index.html">Kann man ein Flugzeug hacken?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>