<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôè üôáüèø üò• Analyse des performances des machines virtuelles dans VMware vSphere. Partie 2: M√©moire ‚ú°Ô∏è üßó üë®üèø‚Äçü§ù‚Äçüë®üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Partie 1. √Ä propos du CPU 
 Partie 3. √Ä propos du stockage 

 Dans cet article, nous parlerons des compteurs de performances RAM dans vSphere. 
 Il se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analyse des performances des machines virtuelles dans VMware vSphere. Partie 2: M√©moire</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dataline/blog/455820/"><img src="https://habrastorage.org/webt/el/am/7y/elam7yyhc6vrowmmrt5ofxgj-r0.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1. √Ä propos du CPU</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 3. √Ä propos du stockage</a> <br><br>  Dans cet article, nous parlerons des compteurs de performances RAM dans vSphere. <br>  Il semble que la m√©moire soit de plus en plus sans ambigu√Øt√© qu'avec le processeur: s'il y a des probl√®mes de performances sur la VM, il est difficile de ne pas les remarquer.  Mais s'ils apparaissent, les traiter est beaucoup plus difficile.  Mais tout d'abord. <a name="habracut"></a><br><br><h3>  Un peu de th√©orie </h3><br>  La RAM des machines virtuelles est extraite de la m√©moire du serveur sur lequel les VM s'ex√©cutent.  C'est assez √©vident :).  Si la RAM du serveur n'est pas suffisante pour tout le monde, ESXi commence √† appliquer des techniques de r√©cup√©ration de m√©moire.  Sinon, les syst√®mes d'exploitation VM se bloqueraient avec des erreurs d'acc√®s √† la RAM. <br><br>  Quelles techniques utiliser ESXi d√©cide en fonction de la charge de RAM: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>√âtat de la m√©moire</b> <br></td><td>  <b>La fronti√®re</b> <br></td><td>  <b>Actions</b> <br></td></tr><tr><td> √âlev√© <br></td><td>  400% de minFree <br></td><td>  Apr√®s avoir atteint la limite sup√©rieure, les grandes pages de m√©moire sont divis√©es en petites (TPS fonctionne en mode standard). <br></td></tr><tr><td>  Clair <br></td><td>  100% de minFree <br></td><td>  Les grandes pages de m√©moire sont divis√©es en petits, TPS fonctionne de force. <br></td></tr><tr><td>  Doux <br></td><td>  64% de minFree <br></td><td>  TPS + ballon <br></td></tr><tr><td>  Dur <br></td><td>  32% de minFree <br></td><td>  TPS + Compress + Swap <br></td></tr><tr><td>  Faible <br></td><td>  16% de minFree <br></td><td>  Compresser + Swap + Block <br></td></tr></tbody></table></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Source</a> <br><br>  minFree est la RAM n√©cessaire au fonctionnement de l'hyperviseur. <br><br>  Avant ESXi 4.1 inclus, minFree √©tait fix√© par d√©faut - 6% de la RAM du serveur (le pourcentage pouvait √™tre modifi√© via l'option Mem.MinFreePct sur ESXi).  Dans les versions ult√©rieures, en raison de l'augmentation des volumes de m√©moire sur les serveurs minFree, il a commenc√© √† √™tre calcul√© en fonction de la taille de la m√©moire de l'h√¥te, et non en tant que valeur de pourcentage fixe. <br><br>  La valeur minFree (par d√©faut) est calcul√©e comme suit: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Pourcentage de m√©moire r√©serv√© pour minFree</b> <br></td><td>  <b>Plage de m√©moire</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 Go <br></td></tr><tr><td>  4% <br></td><td>  4-12 Go <br></td></tr><tr><td>  2% <br></td><td>  12-28 Go <br></td></tr><tr><td>  1% <br></td><td>  M√©moire restante <br></td></tr></tbody></table></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Source</a> <br><br>  Par exemple, pour un serveur avec 128 Go de RAM, la valeur MinFree serait: <br>  MinFree = 245,76 + 327,68 + 327,68 + 1024 = 1925,12 Mo = 1,88 Go <br>  La valeur r√©elle peut diff√©rer de quelques centaines de Mo, cela d√©pend du serveur et de la RAM. <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Pourcentage de m√©moire r√©serv√© pour minFree</b> <br></td><td>  <b>Plage de m√©moire</b> <br></td><td>  <b>Valeur pour 128 Go</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 Go <br></td><td>  245,76 Mo <br></td></tr><tr><td>  4% <br></td><td>  4-12 Go <br></td><td>  327,68 Mo <br></td></tr><tr><td>  2% <br></td><td>  12-28 Go <br></td><td>  327,68 Mo <br></td></tr><tr><td>  1% <br></td><td>  M√©moire restante (100 Go) <br></td><td>  1024 Mo <br></td></tr></tbody></table></div><br><br>  En r√®gle g√©n√©rale, pour les peuplements productifs, seul Elev√© peut √™tre consid√©r√© comme normal.  Pour les bancs d'essai et de d√©veloppement, des conditions claires / douces peuvent √™tre acceptables.  S'il reste moins de 64% MinFree de RAM sur l'h√¥te, les machines virtuelles ex√©cut√©es sur celui-ci rencontreront certainement des probl√®mes de performances. <br><br>  Dans chaque √©tat, certaines techniques de r√©cup√©ration de m√©moire sont appliqu√©es √† partir de TPS, ce qui n'affecte pratiquement pas les performances de la machine virtuelle, se terminant par Swapping.  Je vais vous en dire plus √† leur sujet. <br><br>  <b>Partage de page transparent (TPS).</b>  TPS est, en gros, la d√©duplication des pages de RAM des machines virtuelles sur un serveur. <br><br>  ESXi recherche des pages identiques de RAM de machine virtuelle, compte et compare la somme de hachage des pages et supprime les pages en double, en les rempla√ßant par des liens vers la m√™me page dans la m√©moire physique du serveur.  En cons√©quence, la consommation de m√©moire physique est r√©duite, et une certaine r√©abonnement de la m√©moire peut √™tre obtenue avec peu ou pas de perte de performances. <br><br><img src="https://habrastorage.org/webt/ul/fz/1i/ulfz1i0bomyhsarceziylov-o6i.jpeg"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Source</a> <br><br>  Ce m√©canisme ne fonctionne que pour les pages de 4 ko (petites pages).  Pages de 2 Mo de taille (grandes pages) l'hyperviseur n'essaie m√™me pas de d√©dupliquer: la chance de trouver des pages identiques de cette taille n'est pas grande. <br><br>  Par d√©faut, ESXi alloue de la m√©moire aux grandes pages.  La division de grandes pages en petites pages commence lorsque le seuil de l'√©tat Haut est atteint et est forc√©e lorsque l'√©tat Effacer est atteint (voir le tableau des √©tats de l'hyperviseur). <br><br>  Si vous voulez que TPS commence √† fonctionner sans attendre que la RAM h√¥te se remplisse, dans Advanced Options ESXi, vous devez d√©finir la valeur <i>¬´Mem.AllocGuestLargePage¬ª</i> sur 0 (la valeur par d√©faut est 1).  Ensuite, l'allocation de grandes pages de m√©moire pour les machines virtuelles sera d√©sactiv√©e. <br><br>  Depuis d√©cembre 2014, dans toutes les versions d'ESXi, le TPS entre les VM a √©t√© d√©sactiv√© par d√©faut, car une vuln√©rabilit√© a √©t√© trouv√©e qui permet th√©oriquement d'acc√©der √† la RAM d'une autre VM √† partir d'une VM.  D√©tails ici.  Informations sur la mise en ≈ìuvre pratique de l'exploitation de la vuln√©rabilit√© TPS que je n'ai pas rencontr√©es. <br><br>  La politique TPS est contr√¥l√©e via l'option avanc√©e <i>¬´Mem.ShareForceSalting¬ª</i> sur ESXi: <br>  0 - TPS inter-VM.  TPS fonctionne pour les pages de diff√©rentes VM; <br>  1 - TPS pour VM avec la m√™me valeur ¬´sched.mem.pshare.salt¬ª dans VMX; <br>  2 (par d√©faut) - TPS intra-VM.  TPS fonctionne pour les pages √† l'int√©rieur d'une machine virtuelle. <br><br>  Il est certainement judicieux de d√©sactiver les grandes pages et d'activer Inter-VM TPS sur les bancs de test.  Il peut √©galement √™tre utilis√© pour des stands avec un grand nombre de VM du m√™me type.  Par exemple, sur les supports avec VDI, les √©conomies de m√©moire physique peuvent atteindre des dizaines de pour cent. <br><br>  <b>Ballon de m√©moire.</b>  La montgolfi√®re n'est plus une technique aussi inoffensive et transparente pour le syst√®me d'exploitation VM que TPS.  Mais avec une utilisation appropri√©e avec Ballooning, vous pouvez vivre et m√™me travailler. <br><br>  Avec Vmware Tools, un pilote sp√©cial est install√© sur la machine virtuelle, appel√© Balloon Driver (alias vmmemctl).  Lorsque l'hyperviseur commence √† manquer de m√©moire physique et passe √† l'√©tat Soft, ESXi demande √† la machine virtuelle de renvoyer la RAM inutilis√©e via ce pilote de ballon.  Le pilote, √† son tour, fonctionne au niveau du syst√®me d'exploitation et lui demande de la m√©moire libre.  L'hyperviseur voit quelles pages de m√©moire physique Balloon Driver a prises, prend la m√©moire de la machine virtuelle et la retourne √† l'h√¥te.  Il n'y a aucun probl√®me avec le fonctionnement du syst√®me d'exploitation, car au niveau du syst√®me d'exploitation, la m√©moire est occup√©e par le pilote de ballon.  Par d√©faut, Balloon Driver peut occuper jusqu'√† 65% de la m√©moire de la machine virtuelle. <br><br>  Si VMware Tools n'est pas install√© sur la machine virtuelle ou si la bulle est d√©sactiv√©e (je ne le recommande pas, mais il y a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">KB</a> :), l'hyperviseur bascule imm√©diatement vers des m√©thodes plus strictes de suppression de m√©moire.  Conclusion: assurez-vous que VMware Tools sur la machine virtuelle l'est. <br><br><img src="https://habrastorage.org/webt/ey/pm/s1/eypms1ugdkmhr0r4odhga1locao.png"><br>  <i>Le fonctionnement de Balloon Driver peut √™tre v√©rifi√© √† partir du syst√®me d'exploitation via VMware Tools</i> . <br><br>  <b>Compression m√©moire</b>  Cette technique est utilis√©e lorsque ESXi atteint Hard.  Comme son nom l'indique, ESXi essaie de compresser 4 Ko de pages RAM en 2 Ko et ainsi lib√©rer de l'espace dans la m√©moire physique du serveur.  Cette technique augmente consid√©rablement le temps d'acc√®s au contenu des pages de la m√©moire RAM de la VM, car la page doit d'abord √™tre nettoy√©e.  Parfois, toutes les pages ne peuvent pas √™tre compress√©es et le processus lui-m√™me prend un certain temps.  Par cons√©quent, cette technique n'est pas tr√®s efficace en pratique. <br><br>  <b>√âchange de m√©moire.</b>  Apr√®s une courte phase, la compression de m√©moire ESXi est presque in√©vitablement (si les machines virtuelles ne sont pas all√©es sur d'autres h√¥tes ou ne se sont pas arr√™t√©es) passe au Swapping.  Et s'il reste tr√®s peu de m√©moire (√©tat bas), l'hyperviseur arr√™te √©galement d'allouer des pages de m√©moire de machine virtuelle, ce qui peut provoquer des probl√®mes dans les machines virtuelles invit√©es. <br><br>  Voici comment fonctionne le swapping.  Lorsque vous allumez la machine virtuelle, un fichier avec l'extension .vswp est cr√©√© pour elle.  En taille, elle est √©gale √† la RAM non r√©serv√©e de la VM: c'est la diff√©rence entre la m√©moire configur√©e et la m√©moire r√©serv√©e.  Lorsque vous travaillez avec Swapping, ESXi d√©charge les pages de m√©moire de la machine virtuelle dans ce fichier et commence √† travailler avec lui au lieu de la m√©moire physique du serveur.  Bien s√ªr, une telle m√©moire "RAM" est plus lente de plusieurs ordres de grandeur que la m√©moire r√©elle, m√™me si .vswp est en stockage rapide. <br><br>  Contrairement √† Ballooning, lorsque des pages inutilis√©es sont s√©lectionn√©es √† partir d'une machine virtuelle, les pages qui sont activement utilis√©es par le syst√®me d'exploitation ou les applications √† l'int√©rieur de la machine virtuelle peuvent aller sur le disque pendant l'√©change.  Par cons√©quent, les performances de la machine virtuelle diminuent jusqu'√† ce qu'elle se bloque.  La VM fonctionne officiellement et au moins elle peut √™tre correctement d√©sactiv√©e du syst√®me d'exploitation.  Si vous serez patient;) <br><br>  Si les machines virtuelles sont pass√©es √† Swap, il s'agit d'une situation anormale qu'il vaut mieux √©viter si possible. <br><br><h3>  Compteurs de performances de base de la m√©moire de la machine virtuelle </h3><br>  Nous sommes donc arriv√©s √† l'essentiel.  Pour surveiller l'√©tat de la m√©moire dans la machine virtuelle, les compteurs suivants sont disponibles: <br><br>  <b>Actif</b> - indique la quantit√© de RAM (kilo-octets) √† laquelle la machine virtuelle a acc√©d√© au cours de la p√©riode de mesure pr√©c√©dente. <br><br>  <b>L'utilisation</b> est identique √† Active, mais en pourcentage de la m√©moire de la VM configur√©e.  Il est calcul√© √† l'aide de la formule suivante: taille de m√©moire active √∑ machine virtuelle configur√©e. <br>  Une utilisation √©lev√©e et active, respectivement, ne sont pas toujours r√©v√©latrices de probl√®mes de performances des machines virtuelles.  Si une machine virtuelle utilise de mani√®re agressive la m√©moire (au moins y acc√®de), cela ne signifie pas qu'il n'y a pas assez de m√©moire.  C'est plut√¥t l'occasion de voir ce qui se passe dans le syst√®me d'exploitation. <br>  Il existe une alarme standard sur l'utilisation de la m√©moire pour les machines virtuelles: <br><br><img src="https://habrastorage.org/webt/8u/ca/n8/8ucan84mevajwvlnr-4ov9boyro.png"><br><br>  <b>Partag√©</b> - la quantit√© de RAM dans une machine virtuelle d√©dupliqu√©e √† l'aide de TPS (√† l'int√©rieur d'une machine virtuelle ou entre machines virtuelles). <br><br>  <b>Accord√©</b> - la quantit√© de m√©moire physique de l'h√¥te (kilo-octets) qui a √©t√© donn√©e √† la machine virtuelle.  Comprend partag√©. <br><br>  <b>Consomm√©</b> (accord√© - partag√©) - la quantit√© de m√©moire physique (kilo-octets) que la machine virtuelle consomme de l'h√¥te.  N'inclut pas Shared. <br><br>  Si une partie de la m√©moire de la machine virtuelle n'est pas allou√©e √† partir de la m√©moire physique de l'h√¥te, mais du fichier d'√©change ou que la m√©moire a √©t√© prise √† partir de la machine virtuelle via le pilote de ballon, ce montant n'est pas pris en compte dans Accord√© et Consomm√©. <br>  Les valeurs √©lev√©es de Accord√© et Consomm√© sont parfaitement normales.  Le syst√®me d'exploitation retire progressivement la m√©moire de l'hyperviseur et ne rend pas.  Au fil du temps, avec une machine virtuelle fonctionnant activement, les valeurs de ces compteurs approchent la quantit√© de m√©moire configur√©e et y restent. <br><br>  <b>Z√©ro</b> - la quantit√© de RAM dans la machine virtuelle (kilo-octets), qui contient des z√©ros.  Cette m√©moire est consid√©r√©e comme un hyperviseur libre et peut √™tre donn√©e √† d'autres machines virtuelles.  Apr√®s que l'OS invit√© l'ait re√ßu, il a √©crit quelque chose dans la m√©moire nulle, il va √† Consumed et ne revient pas. <br><br>  <b>Surcharge r√©serv√©e</b> - la quantit√© de RAM dans la VM, (Ko) r√©serv√©e par l'hyperviseur pour que la VM fonctionne.  C'est une petite quantit√©, mais elle doit √™tre disponible sur l'h√¥te, sinon la VM ne d√©marrera pas. <br><br>  <b>Ballon</b> - la quantit√© de RAM (Ko) saisie sur la machine virtuelle √† l'aide du pilote de ballon. <br><br>  <b>Compress√©</b> - la quantit√© de RAM (Ko) qui a pu √™tre compress√©e. <br><br>  <b>Swapped</b> - la quantit√© de RAM (Ko) qui, par manque de m√©moire physique sur le serveur, s'est d√©plac√©e sur le disque. <br>  Les compteurs de ballons et autres techniques de r√©cup√©ration de m√©moire sont nuls. <br><br>  Voici √† quoi ressemble le graphique avec les compteurs de m√©moire d'une machine virtuelle fonctionnant normalement avec 150 Go de RAM. <br><br><img src="https://habrastorage.org/webt/0l/pp/w3/0lppw3nz9iqzcnuergtxiseb67s.png"><br><br>  Sur le graphique ci-dessous, la machine virtuelle a des probl√®mes √©vidents.  Le graphique montre que pour cette machine virtuelle, toutes les techniques d√©crites pour travailler avec la RAM ont √©t√© utilis√©es.  Le ballon pour cette machine virtuelle est beaucoup plus grand que consomm√©.  En fait, la VM est probablement morte que vivante. <br><br><img src="https://habrastorage.org/webt/f4/ic/xk/f4icxkpxpykxua_gp-sirlzuu_u.png"><br><br><h3>  ESXTOP </h3><br>  Comme avec le CPU, si vous voulez √©valuer rapidement la situation sur l'h√¥te, ainsi que sa dynamique avec un intervalle allant jusqu'√† 2 secondes, cela vaut la peine d'utiliser ESXTOP. <br><br>  L'√©cran de m√©moire ESXTOP est appel√© avec la touche ¬´m¬ª et ressemble √† ceci (champs B, D, H, J, K, L, O s√©lectionn√©s): <br><br><img src="https://habrastorage.org/webt/rm/wj/4k/rmwj4krvvizdtcizkrid0zjuml8.png"><br><br>  Les param√®tres suivants nous int√©resseront: <br><br>  <b>Mem overcommit avg</b> - la valeur moyenne d'un sur-abonnement m√©moire sur un h√¥te pendant 1, 5 et 15 minutes.  Si au-dessus de z√©ro, c'est l'occasion de voir ce qui se passe, mais pas toujours un indicateur de la pr√©sence de probl√®mes. <br><br>  Dans les lignes <b>PMEM / MB</b> et <b>VMKMEM / MB</b> - informations sur la m√©moire physique du serveur et la m√©moire disponible pour VMkernel.  De l'int√©ressant ici, vous pouvez voir la valeur minfree (en Mo), l'√©tat de l'h√¥te de la m√©moire (dans notre cas, √©lev√©). <br><br>  Dans la ligne <b>NUMA / MB,</b> vous pouvez voir la r√©partition de la RAM par NUMA-n≈ìuds (sockets).  Dans cet exemple, la r√©partition est in√©gale, ce qui en principe n'est pas tr√®s bon. <br><br>  Voici un r√©sum√© des statistiques du serveur pour les techniques de r√©cup√©ration de m√©moire: <br><br>  <b>PSHARE / MB</b> est les statistiques TPS; <br><br>  <b>SWAP / MB</b> - statistiques sur l'utilisation de Swap; <br><br>  <b>ZIP / MB</b> - statistiques de compression des pages m√©moire; <br><br>  <b>MEMCTL / MB</b> - Statistiques d'utilisation du pilote de ballon. <br><br>  Pour les machines virtuelles individuelles, nous pouvons √™tre int√©ress√©s par les informations suivantes.  J'ai cach√© les noms des VM pour ne pas embarrasser le public :).  Si la m√©trique ESXTOP est la m√™me que le compteur dans vSphere, je cite le compteur correspondant. <br><br>  <b>MEMSZ</b> est la quantit√© de m√©moire configur√©e sur la machine virtuelle (Mo). <br>  MEMSZ = GRANT + MCTLSZ + SWCUR + intact. <br><br>  <b>SUBVENTION</b> - Accord√©e en Mo. <br><br>  <b>TCHD</b> - Actif en Mo. <br><br>  <b>MCTL?</b>  - est install√© sur VM Balloon Driver. <br><br>  <b>MCTLSZ</b> - Ballon en MB. <br><br>  <b>MCTLGT</b> est la quantit√© de RAM (Mo) qu'ESXi souhaite supprimer de la machine virtuelle via le pilote de ballon (cible Memctl). <br><br>  <b>MCTLMAX</b> - la quantit√© maximale de RAM (Mo) qu'ESXi peut supprimer de la machine virtuelle via le pilote de ballon. <br><br>  <b>SWCUR</b> - la quantit√© actuelle de RAM (Mo) donn√©e √† la machine virtuelle √† partir du fichier d'√©change. <br><br>  <b>SWGT</b> - la quantit√© de RAM (Mo) qu'ESXi veut donner aux VM √† partir d'un fichier Swap (Swap Target). <br><br>  √âgalement via ESXTOP, vous pouvez voir des informations plus d√©taill√©es sur la topologie NUMA VM.  Pour ce faire, s√©lectionnez les champs D, G: <br><br><img src="https://habrastorage.org/webt/ff/7y/zd/ff7yzdsjedyntnpj4duwv0c731m.png"><br><br>  <b>NHN</b> - n≈ìuds NUMA sur lesquels se trouve la machine virtuelle.  Ici, vous pouvez imm√©diatement remarquer un vm large qui ne tient pas sur un n≈ìud NUMA. <br><br>  <b>NRMEM</b> - combien de m√©gaoctets de m√©moire la VM prend du n≈ìud NUMA distant. <br><br>  <b>NLMEM</b> - combien de m√©gaoctets de m√©moire la VM prend du n≈ìud NUMA local. <br><br>  <b>N% L</b> - pourcentage de m√©moire VM sur le n≈ìud NUMA local (si moins de 80%, des probl√®mes de performances peuvent survenir). <br><br><h3>  M√©moire sur l'hyperviseur </h3><br>  Si les compteurs CPU sur l'hyperviseur ne sont g√©n√©ralement pas d'un int√©r√™t particulier, alors la situation est inverse avec la m√©moire.  Une utilisation √©lev√©e de la m√©moire sur la machine virtuelle n'indique pas toujours un probl√®me de performances, mais une utilisation √©lev√©e de la m√©moire sur l'hyperviseur d√©marre simplement le technicien de gestion de la m√©moire et provoque des probl√®mes avec les performances de la machine virtuelle.  Les alarmes d'utilisation de la m√©moire h√¥te doivent √™tre surveill√©es et les machines virtuelles ne doivent pas entrer dans Swap. <br><br><img src="https://habrastorage.org/webt/h2/x_/59/h2x_59kddpe84yzudcq03fq1rmc.png"><br><br><img src="https://habrastorage.org/webt/oc/w7/c9/ocw7c9vmbrqogjhmtpbotng4-6y.png"><br><br><h3>  Annuler </h3><br>  Si la machine virtuelle entre dans Swap, ses performances sont consid√©rablement r√©duites.  Les traces de montgolfi√®re et de compression disparaissent rapidement apr√®s l'apparition de RAM libre sur l'h√¥te, mais la machine virtuelle n'est pas press√©e de revenir de Swap √† la RAM du serveur. <br>  Avant ESXi 6.0, le seul moyen fiable et rapide de retirer les machines virtuelles de Swap √©tait de red√©marrer (plus pr√©cis√©ment, d'activer / d√©sactiver le conteneur).  √Ä partir d'ESXi 6.0, un moyen pas si officiel, mais fonctionnel et fiable pour sortir les machines virtuelles de Swap est apparu.  Lors d'une des conf√©rences, j'ai r√©ussi √† parler avec l'un des ing√©nieurs VMware responsable du Planificateur de CPU.  Il a confirm√© que la m√©thode fonctionne et est assez s√ªre.  D'apr√®s notre exp√©rience, aucun probl√®me avec lui n'a √©galement √©t√© constat√©. <br><br>  Duncan Epping a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©crit</a> les commandes r√©elles de sortie des machines virtuelles √† partir de Swap.  Je ne r√©p√©terai pas la description d√©taill√©e, donne juste un exemple de son utilisation.  Comme on peut le voir sur la capture d'√©cran, un certain temps apr√®s l'ex√©cution des commandes Swap sp√©cifi√©es sur la machine virtuelle dispara√Æt. <br><br><img src="https://habrastorage.org/webt/e5/lm/7e/e5lm7e0e6i_yxrrlm7dfwixptv0.png"><br><br><h3>  Conseils pour g√©rer la RAM sur ESXi </h3><br>  En conclusion, je vais vous donner quelques conseils pour vous aider √† √©viter les probl√®mes de performances des VM dus √† la RAM: <br><br><ul><li>  √âvitez de sursouscrire en RAM dans les clusters productifs.  Il est toujours conseill√© d'avoir environ 20-30% de m√©moire libre dans le cluster, afin que DRS (et l'administrateur) ait de la marge de man≈ìuvre, et que les machines virtuelles ne passent pas √† Swap pendant la migration.  N'oubliez pas non plus la marge de tol√©rance aux pannes.  Il est d√©sagr√©able lorsque, lorsqu'un serveur tombe en panne et que la machine virtuelle est red√©marr√©e √† l'aide de HA, certaines machines passent √©galement √† Swap. </li><li>  Dans les infrastructures hautement consolid√©es, essayez de NE PAS cr√©er de machines virtuelles avec plus de la moiti√© de la m√©moire h√¥te.  Ceci, encore une fois, aidera DRS √† distribuer les machines virtuelles entre les serveurs de cluster sans aucun probl√®me.  Cette r√®gle, bien s√ªr, n'est pas universelle :). </li><li>  Attention √† l'alarme d'utilisation de la m√©moire de l'h√¥te. </li><li>  N'oubliez pas de mettre VMware Tools sur la machine virtuelle et de ne pas d√©sactiver la montgolfi√®re. </li><li>  Pensez √† activer Inter-VM TPS et √† d√©sactiver les grandes pages dans les environnements VDI et de test. </li><li>  Si la machine virtuelle rencontre des probl√®mes de performances, v√©rifiez si elle utilise de la m√©moire √† partir d'un n≈ìud NUMA distant. </li><li>  Retirez les machines virtuelles de Swap le plus rapidement possible!  Entre autres choses, si la VM est en Swap, pour des raisons √©videntes, le syst√®me de stockage en souffre. </li></ul><br>  C'est tout pour la RAM.  Vous trouverez ci-dessous des articles connexes pour ceux qui souhaitent approfondir les d√©tails.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le prochain article</a> sera consacr√© √† l'histoire. <br><br><div class="spoiler">  <b class="spoiler_title">Liens utiles</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.yellow-bricks.com/2015/03/02/what-happens-at-which-vsphere-memory-state/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.yellow-bricks.com/2013/06/14/how-does-mem-minfreepct-work-with-vsphere-5-0-and-up/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.vladan.fr/vmware-transparent-page-sharing-tps-explained/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.yellow-bricks.com/2016/06/02/memory-pages-swapped-can-unswap/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://kb.vmware.com/s/article/1002586</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.vladan.fr/what-is-vmware-memory-ballooning/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://kb.vmware.com/s/article/2080735</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://kb.vmware.com/s/article/2017642</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://labs.vmware.com/vmtj/vmware-esx-memory-resource-management-swap</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://blogs.vmware.com/vsphere/2013/10/understanding-vsphere-active-memory.html</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/memory_counters.html</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://docs.vmware.com/en/VMware-vSphere/6.5/vsphere-esxi-vcenter-server-65-monitoring-performance-guide.pdf</a> <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr455820/">https://habr.com/ru/post/fr455820/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr455802/index.html">Comment assembler l'Olympic par le biais de newsletters par e-mail. Case Black Star</a></li>
<li><a href="../fr455806/index.html">Naissance et mort d'un album: nous comprenons comment les formats musicaux ont chang√© au cours des 100 derni√®res ann√©es</a></li>
<li><a href="../fr455808/index.html">Obtenez des extraits du registre sur le site Web de FTS en utilisant python</a></li>
<li><a href="../fr455812/index.html">Construire une architecture de microservices sur Golang et gRPC, partie 2 (docker)</a></li>
<li><a href="../fr455816/index.html">Comment cr√©er une action sympa pour Google Assistant. Lifehacks de Just AI</a></li>
<li><a href="../fr455826/index.html">Arrosage automatique t√©l√©command√©</a></li>
<li><a href="../fr455828/index.html">Les scientifiques ont d√©couvert de nouvelles formes de synchronisation exotiques</a></li>
<li><a href="../fr455830/index.html">Un regard sur Passer par les yeux d'un d√©veloppeur .NET. Semaine # 1</a></li>
<li><a href="../fr455832/index.html">Historique d'une seule enqu√™te SQL</a></li>
<li><a href="../fr455834/index.html">Benchmarks pour les serveurs Linux: 5 outils ouverts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>