<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👂 💭 👲🏿 Oh diese Newtonsche Methode 🍇 ↔️ 🎛️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es wurde viel über numerische Optimierungsmethoden geschrieben. Dies ist verständlich, insbesondere vor dem Hintergrund der Erfolge, die kürzlich durc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Oh diese Newtonsche Methode</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469877/">  Es wurde viel über numerische Optimierungsmethoden geschrieben.  Dies ist verständlich, insbesondere vor dem Hintergrund der Erfolge, die kürzlich durch tiefe neuronale Netze gezeigt wurden.  Und es ist sehr erfreulich, dass zumindest einige Enthusiasten nicht nur daran interessiert sind, wie sie ihr neuronales Netzwerk auf die Frameworks bombardieren können, die in diesem Internet an Popularität gewonnen haben, sondern auch, wie und warum alles funktioniert.  In letzter Zeit musste ich jedoch feststellen, dass bei der Beantwortung von Fragen zum Training neuronaler Netze (und nicht nur zum Training und nicht nur zu Netzen), einschließlich Habré, immer häufiger eine Reihe von „bekannten“ Aussagen für die Weiterleitung verwendet werden, deren Gültigkeit gelinde gesagt zweifelhaft.  Unter solchen zweifelhaften Aussagen: <br><br><ol><li>  Methoden der zweiten und weiteren Ordnung funktionieren beim Training neuronaler Netze nicht gut.  Weil. </li><li>  Die Newton-Methode erfordert eine positive Bestimmtheit der hessischen Matrix (zweite Ableitungen) und funktioniert daher nicht gut. <br></li><li>  Die Levenberg-Marquardt-Methode ist ein Kompromiss zwischen Gradientenabstieg und Newton-Methode und allgemein heuristisch. <br></li></ol><br>  usw.  Um diese Liste fortzusetzen, ist es besser, zur Sache zu kommen.  In diesem Beitrag werden wir die zweite Aussage betrachten, da ich ihn in Habré nur mindestens zweimal getroffen habe.  Ich werde auf die erste Frage nur in dem Teil eingehen, der die Newton-Methode betrifft, da sie viel umfangreicher ist.  Der dritte und der Rest bleiben bis zu besseren Zeiten. <br><a name="habracut"></a><br>  Im Mittelpunkt unserer Aufmerksamkeit steht die Aufgabe der bedingungslosen Optimierung <img src="https://habrastorage.org/getpro/habr/post_images/823/e50/c93/823e50c935e4cc19a175f53c17ca79af.gif" title="&quot;f (x) \ rightarrow \ min&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/bc7/838/e10/bc7838e10e143195c0381efbf0671cce.gif" title="&quot;x = (x_ {1}, x_ {2}, \ dots)&quot;">  - ein Punkt des Vektorraums oder einfach - ein Vektor.  Natürlich ist diese Aufgabe umso einfacher zu lösen, je mehr wir darüber wissen <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  .  Es wird normalerweise angenommen, dass es in Bezug auf jedes Argument differenzierbar ist <img src="https://habrastorage.org/getpro/habr/post_images/6d8/d4e/07d/6d8d4e07d259325d5dd652e4b3b97af6.gif" title="&quot;x_ {k}&quot;">  und so oft wie nötig für unsere schmutzigen Taten.  Es ist bekannt, dass eine notwendige Bedingung dafür an einem Punkt ist <img src="https://habrastorage.org/getpro/habr/post_images/8ae/e32/d7e/8aee32d7e93fb189b268894bf91622b0.gif" title="&quot;x ^ {*}&quot;">  Das Minimum ist erreicht, ist die Gleichheit des Gradienten der Funktion <img src="https://habrastorage.org/getpro/habr/post_images/735/1d5/454/7351d54544ca7acc4b7a9bff7a2c2f6a.gif" title="&quot;\ bigtriangledown f (x ^ {*})&quot;">  an diesem Punkt Null.  Von hier erhalten wir sofort die folgende Minimierungsmethode: <br><br>  Löse die Gleichung <img src="https://habrastorage.org/getpro/habr/post_images/d59/223/ac1/d59223ac159bfe660ea26a1d60f8f33f.gif" title="&quot;\ bigtriangledown f (x) = 0&quot;">  . <br><br>  Die Aufgabe ist, gelinde gesagt, nicht einfach.  Auf keinen Fall einfacher als das Original.  An dieser Stelle können wir jedoch sofort den Zusammenhang zwischen dem Minimierungsproblem und dem Problem der Lösung eines Systems nichtlinearer Gleichungen feststellen.  Diese Verbindung wird bei der Betrachtung der Levenberg-Marquardt-Methode (wenn wir dazu kommen) auf uns zurückkommen.  Denken Sie in der Zwischenzeit daran (oder finden Sie heraus), dass eine der am häufigsten verwendeten Methoden zum Lösen von Systemen nichtlinearer Gleichungen die Newtonsche Methode ist.  Es besteht in der Tatsache, dass die Gleichung zu lösen <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="F (x) = 0">  Wir gehen von einer anfänglichen Annäherung aus <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  Erstellen Sie eine Sequenz <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a0/23d/4a2/3a023d4a27cdff86f8cf3bc78d5b3a21.gif" title="x_ {i + 1} = x_ {i} -H ^ {- 1} (x_ {i}) F (x_ {i})">  - Newtons explizite Methode <br><br>  oder <br><br><img src="https://habrastorage.org/getpro/habr/post_images/116/6fa/27b/1166fa27b4038fed75d435daaaab53fe.gif" title="&quot;\ begin {Fälle} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + p_ {i} \ end {Fälle}&quot;">  - Newtons implizite Methode <br><br>  wo <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  - Matrix aus partiellen Ableitungen einer Funktion <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Im allgemeinen Fall, wenn uns das System der nichtlinearen Gleichungen einfach in Empfindungen gegeben wird, ist natürlich etwas von der Matrix erforderlich <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  wir sind nicht berechtigt.  In dem Fall, in dem die Gleichung eine Mindestbedingung für eine Funktion ist, können wir angeben, dass die Matrix <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  symmetrisch.  Aber nicht mehr. <br><br>  Newtons Methode zur Lösung nichtlinearer Gleichungssysteme wurde recht gut untersucht.  Und hier ist die Sache - für ihre Konvergenz ist die positive Bestimmtheit der Matrix nicht erforderlich <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  .  Ja, und kann nicht verlangt werden - sonst wäre er wertlos gewesen.  Stattdessen gibt es andere Bedingungen, die eine lokale Konvergenz dieser Methode gewährleisten und die wir hier nicht berücksichtigen werden, indem wir interessierte Personen zur Fachliteratur (oder in den Kommentaren) schicken.  Wir bekommen die Aussage 2 ist falsch. <br><br>  Also? <br><br>  Ja und nein.  Der Hinterhalt hier im Wort ist die lokale Konvergenz vor dem Wort.  Dies bedeutet, dass die anfängliche Annäherung <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  muss „nah genug“ an der Lösung sein, sonst werden wir bei jedem Schritt immer weiter von ihr entfernt.  Was tun?  Ich werde nicht näher darauf eingehen, wie dieses Problem für Systeme nichtlinearer Gleichungen allgemeiner Form gelöst wird.  Zurück zu unserer Optimierungsaufgabe.  Der erste Fehler von Aussage 2 ist tatsächlich, dass die Newton-Methode bei Optimierungsproblemen normalerweise ihre Modifikation bedeutet - die gedämpfte Newton-Methode, bei der die Folge von Approximationen gemäß der Regel konstruiert wird <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23d/ca8/404/23dca84042b77a1560b8cd2db607e8ae.gif" title="x_ {i + 1} = x_ {i} - \ alpha_ {i} H ^ {- 1} (x_ {i}) F (x_ {i})">  - Newtons explizite gedämpfte Methode <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9d4/67c/c96/9d467cc96266cf1179d3e553718f5bee.gif" title="&quot;\ begin {case} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + \ alpha_ {i} p_ {i} \ Ende {Fälle} &quot;">  - Newtons implizite gedämpfte Methode <br><br>  Hier ist die Reihenfolge <img src="https://habrastorage.org/getpro/habr/post_images/c70/738/fd1/c70738fd1eb4d9bfff34f20904f41bbf.gif" title="&quot;\ {\ alpha_ {i} \}&quot;">  ist ein Parameter der Methode und ihre Konstruktion ist eine separate Aufgabe.  Bei Minimierungsproblemen natürlich bei der Auswahl <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  es wird eine Anforderung bestehen, dass bei jeder Iteration der Wert der Funktion f abnimmt, d.h. <img src="https://habrastorage.org/getpro/habr/post_images/3ae/e45/ba0/3aee45ba0097ca8bdc8a23ef6a465f21.gif" title="f (x_ {i + 1}) &amp; lt; f (x_ {i})">  .  Es stellt sich eine logische Frage: Gibt es eine solche (positive) <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  ?  Und wenn die Antwort auf diese Frage positiv ist, dann <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  nannte die Richtung des Abstiegs.  Dann kann die Frage folgendermaßen gestellt werden: <br>  <i>Wann ist die nach Newtons Methode erzeugte Richtung die Abstiegsrichtung?</i> <br>  Und um es zu beantworten, müssen Sie das Minimierungsproblem von einer anderen Seite betrachten. <br><br><h2>  Abstiegsmethoden </h2><br>  Für das Minimierungsproblem erscheint dieser Ansatz ganz natürlich: Ausgehend von einem beliebigen Punkt wählen wir die Richtung p auf irgendeine Weise und machen einen Schritt in diese Richtung <img src="https://habrastorage.org/getpro/habr/post_images/fbf/01e/b21/fbf01eb21703831c5dd0e196a2efccc2.gif" title="&quot;\ alpha p&quot;">  .  Wenn <img src="https://habrastorage.org/getpro/habr/post_images/bd9/6f9/580/bd96f95806b05f65a5766db233a85653.gif" title="f (x + \ alpha p) &amp; lt; f (x)">  dann nimm <img src="https://habrastorage.org/getpro/habr/post_images/b87/e59/538/b87e59538ed10c96ec3db2e7bad8dc85.gif" title="&quot;x + \ alpha p&quot;">  als neuen Ausgangspunkt und wiederholen Sie den Vorgang.  Wenn die Richtung willkürlich gewählt wird, wird eine solche Methode manchmal als Random-Walk-Methode bezeichnet.  Es ist möglich, Einheitsbasisvektoren als Richtung zu verwenden - das heißt, um einen Schritt in nur einer Koordinate zu machen, wird diese Methode als Koordinatenabstiegsmethode bezeichnet.  Unnötig zu sagen, dass sie unwirksam sind?  Damit dieser Ansatz gut funktioniert, benötigen wir einige zusätzliche Garantien.  Dazu führen wir eine Hilfsfunktion ein <img src="https://habrastorage.org/getpro/habr/post_images/8bf/1d5/4e1/8bf1d54e1f36dd4c9dfd5720437af51c.gif" title="g (p) = f (x + p)">  .  Ich denke, es ist offensichtlich, dass die Minimierung <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  völlig gleichbedeutend mit Minimierung <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  .  Wenn <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  dann differenzierbar <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  kann dargestellt werden als <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/615/d31/e47615d310276ab67a9163889a2335a5.gif" title="g (p) = f (x) + \ bigtriangledown f ^ {T} (x) p + o (\ parallel p \ parallel ^ {2})"><br><br>  und wenn <img src="https://habrastorage.org/getpro/habr/post_images/2a7/342/acb/2a7342acbe0772f75af6eee281c247d0.gif" title="&quot;\ parallel p \ parallel&quot;">  klein genug dann <img src="https://habrastorage.org/getpro/habr/post_images/2ef/8a9/23f/2ef8a923f49cf84264effb5f3f703c31.gif" title="g (p) \ ungefähr \ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p">  .  Wir können jetzt versuchen, das Minimierungsproblem zu ersetzen <img src="https://habrastorage.org/getpro/habr/post_images/076/563/484/076563484d4e576c5c48098bfa94d45c.gif" title="g (p)">  die Aufgabe der Minimierung seiner Annäherung (oder <i>Modell</i> ) <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  .  Übrigens basieren alle Methoden auf der Verwendung des Modells <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  Gradient genannt.  Aber das Problem ist, <img src="https://habrastorage.org/getpro/habr/post_images/462/957/dda/462957dda265f4fb8be04327f1c12b0f.gif" title="&quot;\ bar {g}&quot;">  Ist eine lineare Funktion und hat daher kein Minimum.  Um dieses Problem zu lösen, fügen wir eine Einschränkung für die Länge des Schritts hinzu, den wir ausführen möchten.  In diesem Fall ist dies eine ganz natürliche Voraussetzung - denn unser Modell beschreibt die Zielfunktion nur in einer ausreichend kleinen Nachbarschaft mehr oder weniger korrekt.  Als Ergebnis erhalten wir ein zusätzliches Problem der bedingten Optimierung: <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/8a2/c43/279/8a2c4327974067619cfad20b7ea1e821.gif" title="\\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ parallel p \ parallel_ {2} = \ Delta"></a> <br><br>  Diese Aufgabe hat eine offensichtliche Lösung: <img src="https://habrastorage.org/getpro/habr/post_images/3ff/1f6/a21/3ff1f6a2117e5d9a99603bcc8fde4f69.gif" title="&quot;p = - \ beta \ bigtriangledown f (x)&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  - Faktor, der die Erfüllung der Bedingung garantiert.  Dann nehmen Iterationen der Abstiegsmethode die Form an <br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/987/c25/966987c257a50df1855a50ea363350dd.gif" title="&quot;x_ {i + 1} = x_ {i} - \ beta \ bigtriangledown f (x_ {i})&quot;">  , <br><br>  in dem wir die bekannte <b>Gradientenabstiegsmethode</b> lernen.  Parameter <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , die üblicherweise als Abstiegsgeschwindigkeit bezeichnet wird, hat nun eine vollständig verständliche Bedeutung erhalten, und ihr Wert wird aus der Bedingung bestimmt, dass der neue Punkt auf einer Kugel mit einem bestimmten Radius liegt, die um den alten Punkt herum umschrieben ist. <br><br>  Basierend auf den Eigenschaften des konstruierten Modells der Zielfunktion können wir argumentieren, dass es solche gibt <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  , auch wenn sehr klein, was wäre wenn <img src="https://habrastorage.org/getpro/habr/post_images/84b/5fd/00f/84b5fd00fe1f4ca32b7cd7bd095a1490.gif" title="&quot;\ bar {g} (p) &amp; lt; \ bar {g} (0)&quot;">  dann <img src="https://habrastorage.org/getpro/habr/post_images/553/80b/dc5/55380bdc5a434366df6d181078d6a8b7.gif" title="g (p) &amp; lt; g (0)">  .  Es ist bemerkenswert, dass in diesem Fall die Richtung, in die wir uns bewegen, nicht von der Größe des Radius dieser Kugel abhängt.  Dann können wir einen der folgenden Wege wählen: <br><br><ol><li>  Wählen Sie nach einer Methode den Wert aus <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  . </li><li>  Stellen Sie die Aufgabe ein, den entsprechenden Wert auszuwählen <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , was eine Abnahme des Wertes der Zielfunktion bewirkt. </li></ol><br>  Der erste Ansatz ist typisch für die <i>Methoden der Vertrauensregion</i> , der zweite führt zur Formulierung des Hilfsproblems der sogenannten  <i>lineare Suche (LineSearch)</i> .  In diesem speziellen Fall sind die Unterschiede zwischen diesen Ansätzen gering und werden nicht berücksichtigt.  Beachten Sie stattdessen Folgendes: <br><br>  <b><i>Warum suchen wir eigentlich nach einem Offset?</i></b> <b><i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i></b>  <b><i>genau auf der Kugel liegen?</i></b> <br><br>  Tatsächlich könnten wir diese Einschränkung gut durch die Anforderung ersetzen, dass beispielsweise p zur Oberfläche des Würfels gehört, d. H. <img src="https://habrastorage.org/getpro/habr/post_images/cf1/a35/92e/cf1a3592ebe97c9e262a083ea44c594c.gif" title="&quot;\ parallel p \ parallel _ {\ infty} = \ Delta&quot;">  (in diesem Fall ist es nicht zu vernünftig, aber warum nicht) oder eine elliptische Oberfläche?  Dies erscheint bereits ziemlich logisch, wenn wir uns an die Probleme erinnern, die bei der Minimierung von Gully-Funktionen auftreten.  Das Wesentliche des Problems ist, dass sich die Funktion entlang einiger Koordinatenlinien viel schneller ändert als entlang anderer.  Aus diesem Grund erhalten wir, dass, wenn das Inkrement zur Kugel gehören sollte, die Menge <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  bei dem der "Abstieg" vorgesehen ist, sollte sehr klein sein.  Und dies führt dazu, dass das Erreichen eines Minimums eine sehr große Anzahl von Schritten erfordert.  Aber wenn wir stattdessen eine geeignete Ellipse als Nachbarschaft nehmen, wird dieses Problem auf magische Weise zunichte gemacht. <br><br>  Durch die Bedingung, dass die Punkte der elliptischen Oberfläche gehören, kann es geschrieben werden als <img src="https://habrastorage.org/getpro/habr/post_images/6ff/1b4/930/6ff1b49309ec84aa656d848764359b4e.gif" title="&quot;\ parallel p \ parallel_ {B} = \ sqrt {p ^ {T} Bp} = \ Delta&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  Ist eine positive bestimmte Matrix, auch Metrik genannt.  Norm <img src="https://habrastorage.org/getpro/habr/post_images/ab8/b42/711/ab8b42711a932f9129bdb193b6a74360.gif" title="&quot;\ parallel \ cdot \ parallel_ {B}&quot;">  genannt die durch die Matrix induzierte elliptische Norm <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Was für eine Matrix ist das und woher kann man sie beziehen? Wir werden später darüber nachdenken, und jetzt kommen wir zu einer neuen Aufgabe. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/f4c/cc0/757/f4ccc0757e09fb304ff10a9a8c4751b6.gif" title="\\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ dfrac {1} {2} \ parallel p \ parallel_ {B} ^ {2} = \ Delta"></a> <br><br>  Das Quadrat der Norm und der 1/2 Faktor dienen hier nur der Einfachheit halber, um nicht mit den Wurzeln herumzuspielen.  Mit der Lagrange-Multiplikatormethode erhalten wir das gebundene Problem der bedingungslosen Optimierung <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/605/36d/d5e/60536dd5e2297580940a5b926760a3ce.gif" title="f (x) + \ bigtriangledown f ^ {T} (x) p + \ dfrac {\ lambda} {2} p ^ {T} Bp- \ lambda \ Delta \ rightarrow \ min"></a> <br><br>  Eine notwendige Bedingung für ein Minimum dafür ist <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/4c5/f7e/921/4c5f7e921637c73fcb992c5d9b9efcd6.gif" title="\ bigtriangledown f (x) + \ lambda Bp = 0"></a>  oder <img src="https://habrastorage.org/getpro/habr/post_images/a80/6c9/ff7/a806c9ff7ce22ea27c87b6a61a4c8fed.gif" title="&quot;B \ left (\ lambda p \ right) = - \ bigtriangledown f (x)&quot;">  von wo <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/b50/032/3a9/b500323a970c7ae821295450627bdad2.gif" title="p = - \ dfrac {1} {\ lambda} B ^ {- 1} \ bigtriangledown f (x) = \ dfrac {1} {\ lambda} \ bar {p}"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/697/906/5d7/6979065d729033e0093ffad8475e80a6.gif" title="\\\ dfrac {1} {\ lambda ^ {2}} \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) ^ {T} B \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} BB ^ {- 1} \ bigtriangledown f (x) = \ \ = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x) = \ Delta"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/41f/689/d89/41f689d890b92c84f05fdd0ede8a8114.gif" title="\ lambda = \ sqrt {\ dfrac {1} {\ Delta} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x)}> 0"></a> <br><br>  Wieder sehen wir, dass die Richtung <img src="https://habrastorage.org/getpro/habr/post_images/45b/686/bb0/45b686bb0219a74b212cfdeaf1998653.gif" title="&quot;\ bar {p} = - B ^ {- 1} \ bigtriangledown f (x)&quot;">  , in dem wir uns bewegen werden, hängt nicht vom Wert ab <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  - nur aus der Matrix <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Und wieder können wir entweder abholen <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  das ist mit der Notwendigkeit der Berechnung behaftet <img src="https://habrastorage.org/getpro/habr/post_images/99d/394/e7d/99d394e7d0b74248114405067e0ffd51.gif" title="&quot;\ lambda&quot;">  und explizite Matrixinversion <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  oder lösen Sie das Hilfsproblem, eine geeignete Vorspannung zu finden <img src="https://habrastorage.org/getpro/habr/post_images/b7b/6a7/371/b7b6a73716dc8f4e40a52c1c5ef0e6b4.gif" title="&quot;x_ {i + 1} = x_ {i} + \ beta \ bar {p} _ {i}&quot;">  .  Seit <img src="https://habrastorage.org/getpro/habr/post_images/0b8/52f/d1b/0b852fd1bbc20f2966bf757a56186312.gif" title="&quot;\ lambda &amp; gt; 0&quot;">  ist die Lösung für dieses Hilfsproblem garantiert. <br><br>  Was soll es also für Matrix B sein?  Wir beschränken uns auf spekulative Ideen.  Wenn das Ziel funktioniert <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  - quadratisch, das heißt, es hat die Form <img src="https://habrastorage.org/getpro/habr/post_images/974/f7f/cf6/974f7fcf6345b91ef8466f2cabba6efe.gif" title="f (x) = a + b ^ {T} x + x ^ {T} Hx">  wo <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  positiv definitiv ist es offensichtlich, dass der beste Kandidat für die Rolle der Matrix <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  ist hessisch <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  , da in diesem Fall eine Iteration der von uns konstruierten Abstiegsmethode erforderlich ist.  Wenn H nicht eindeutig positiv ist, kann es keine Metrik sein, und die damit konstruierten Iterationen sind Iterationen der gedämpften Newton-Methode, aber keine Iterationen der Abstiegsmethode.  Schließlich können wir eine strenge Antwort geben <br><br>  <b>Frage:</b> <i>Muss die hessische Matrix in der Newton-Methode positiv eindeutig sein?</i> <br>  <b>Antwort:</b> <i>Nein, es ist weder in der Standard- noch in der gedämpften Newton-Methode erforderlich.</i>  <i>Wenn diese Bedingung erfüllt ist, ist die gedämpfte Newton-Methode eine Abstiegsmethode und hat die Eigenschaft der <i>globalen</i> und nicht nur der lokalen Konvergenz.</i> <br><br>  Lassen Sie uns zur Veranschaulichung sehen, wie Vertrauensbereiche aussehen, wenn die bekannte Rosenbrock-Funktion mithilfe von Gradientenabstieg und Newtons Methoden minimiert wird, und wie sich die Form der Bereiche auf die Konvergenz des Prozesses auswirkt. <br><br><img src="https://habrastorage.org/webt/_x/30/nx/_x30nxs-eyrixuan0-diyvwixww.gif" width="600"><br><br>  So verhält sich die Abstiegsmethode mit einem sphärischen Vertrauensbereich, es ist auch ein Gradientenabstieg.  Alles ist wie ein Lehrbuch - wir stecken in einer Schlucht fest. <br><br><img src="https://habrastorage.org/webt/9x/ik/td/9xiktd4lapdka-uk010evfvlcdm.gif" width="600"><br><br>  Und das bekommen wir, wenn der Konfidenzbereich die Form einer Ellipse hat, die durch die hessische Matrix definiert ist.  Dies ist nichts weiter als eine Iteration der gedämpften Newton-Methode. <br><br>  Nur die Frage, was zu tun ist, wenn die hessische Matrix nicht eindeutig positiv ist, blieb ungelöst.  Es gibt viele Möglichkeiten.  Das erste ist zu punkten.  Vielleicht haben Sie Glück und Newtons Iterationen konvergieren ohne diese Eigenschaft.  Dies ist sehr real, insbesondere in den letzten Phasen des Minimierungsprozesses, wenn Sie bereits nahe genug an einer Lösung sind.  In diesem Fall können Iterationen der Standard-Newton-Methode verwendet werden, ohne sich um die Suche nach einer für den Abstieg zulässigen Nachbarschaft zu kümmern.  Oder verwenden Sie im Fall von Iterationen der gedämpften Newton-Methode <img src="https://habrastorage.org/getpro/habr/post_images/6da/2c0/bc5/6da2c0bc54434a64d7630c142d0c7bf9.gif" title="&quot;\ beta = 0&quot;">  Das heißt, wenn die erhaltene Richtung nicht die Abstiegsrichtung ist, ändern Sie sie beispielsweise in einen Anti-Gradienten.  <i>Sie müssen nur nicht explizit prüfen, ob der Hessische nach dem Sylvester-Kriterium eindeutig positiv ist</i> , wie hier gemacht <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde !!!</a>  .  Es ist verschwenderisch und sinnlos. <br>  Bei subtileren Methoden wird eine Matrix konstruiert, die der hessischen Matrix nahe kommt, aber die Eigenschaft der positiven Bestimmtheit besitzt, insbesondere durch Korrektur von Eigenwerten.  Ein separates Thema sind die quasi-Newtonschen Methoden oder variablen metrischen Methoden, die die positive Bestimmtheit der Matrix B garantieren und keine Berechnung der zweiten Ableitungen erfordern.  Im Allgemeinen geht eine ausführliche Diskussion dieser Themen weit über den Rahmen dieses Artikels hinaus. <br><br>  Ja, und übrigens folgt aus dem Gesagten, dass <i>die Newton-gedämpfte Methode mit positiver Bestimmtheit des Hessischen eine Gradientenmethode ist</i> .  Sowie quasi-Newtonsche Methoden.  Und viele andere, basierend auf einer separaten Wahl der Richtung und Schrittgröße.  Daher ist es falsch, die Newton-Methode der Gradiententerminologie gegenüberzustellen. <br><br><h2>  Zusammenfassend </h2><br>  Die Newton-Methode, an die bei der Erörterung von Minimierungsmethoden häufig erinnert wird, ist normalerweise nicht die Newton-Methode im klassischen Sinne, sondern die Abstiegsmethode mit der vom Hessischen der Zielfunktion angegebenen Metrik.  Und ja, es konvergiert global, wenn der Hessische überall positiv bestimmt ist.  Dies ist nur für konvexe Funktionen möglich, die in der Praxis viel seltener sind als wir möchten. Daher garantiert die Anwendung der Newton-Methode (wir werden uns nicht vom Kollektiv lösen und es weiterhin so nennen) im allgemeinen Fall ohne die entsprechenden Modifikationen nicht das richtige Ergebnis.  Das Erlernen neuronaler Netze, auch flacher, führt normalerweise zu nicht konvexen Optimierungsproblemen mit vielen lokalen Minima.  Und hier ist ein neuer Hinterhalt.  Newtons Methode konvergiert normalerweise schnell (wenn sie konvergiert).  Ich meine sehr schnell.  Und das ist seltsamerweise schlecht, weil wir in mehreren Iterationen zu einem lokalen Minimum kommen.  Und es kann für Funktionen mit komplexem Gelände viel schlimmer sein als das globale.  Der Gradientenabstieg mit linearer Suche konvergiert viel langsamer, überspringt jedoch eher die Rippen der Zielfunktion, was in den frühen Stadien der Minimierung sehr wichtig ist.  Wenn Sie den Wert der Zielfunktion bereits gut reduziert haben und sich die Konvergenz des Gradientenabfalls erheblich verlangsamt hat, kann eine Änderung der Metrik den Prozess durchaus beschleunigen, dies gilt jedoch für die Endphase. <br><br>  Natürlich ist dieses Argument nicht universell, nicht unbestreitbar und in einigen Fällen sogar falsch.  Ebenso wie die Aussage, dass Gradientenmethoden bei Lernproblemen am besten funktionieren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469877/">https://habr.com/ru/post/de469877/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469855/index.html">Gradle + LLVM</a></li>
<li><a href="../de469861/index.html">Datenstrukturen für Spielprogrammierer: Massendaten</a></li>
<li><a href="../de469867/index.html">Wie wird daraus ein Schüler (und was haben Brettspiele damit zu tun)</a></li>
<li><a href="../de469871/index.html">Als Tastaturen Tische waren</a></li>
<li><a href="../de469875/index.html">So schützen Sie Ihre Passwörter im Jahr 2019</a></li>
<li><a href="../de469879/index.html">Doppel-VPN mit einem Klick. So teilen Sie einfach die IP-Adresse eines Ein- und Ausstiegspunkts</a></li>
<li><a href="../de469881/index.html">Die ersten drei Tage des Lebens eines Postens auf Habré</a></li>
<li><a href="../de469883/index.html">Ist es möglich, Zufälligkeit zu programmieren?</a></li>
<li><a href="../de469889/index.html">SamsPcbGuide, Teil 12: Technologie - BGA-Gehäuse, Kunststoff und Raum II</a></li>
<li><a href="../de469893/index.html">Haus mit Hightech-Elementen für eine obdachlose Katze</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>