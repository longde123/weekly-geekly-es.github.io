<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÇ üí≠ üë≤üèø Oh diese Newtonsche Methode üçá ‚ÜîÔ∏è üéõÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es wurde viel √ºber numerische Optimierungsmethoden geschrieben. Dies ist verst√§ndlich, insbesondere vor dem Hintergrund der Erfolge, die k√ºrzlich durc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Oh diese Newtonsche Methode</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469877/">  Es wurde viel √ºber numerische Optimierungsmethoden geschrieben.  Dies ist verst√§ndlich, insbesondere vor dem Hintergrund der Erfolge, die k√ºrzlich durch tiefe neuronale Netze gezeigt wurden.  Und es ist sehr erfreulich, dass zumindest einige Enthusiasten nicht nur daran interessiert sind, wie sie ihr neuronales Netzwerk auf die Frameworks bombardieren k√∂nnen, die in diesem Internet an Popularit√§t gewonnen haben, sondern auch, wie und warum alles funktioniert.  In letzter Zeit musste ich jedoch feststellen, dass bei der Beantwortung von Fragen zum Training neuronaler Netze (und nicht nur zum Training und nicht nur zu Netzen), einschlie√ülich Habr√©, immer h√§ufiger eine Reihe von ‚Äûbekannten‚Äú Aussagen f√ºr die Weiterleitung verwendet werden, deren G√ºltigkeit gelinde gesagt zweifelhaft.  Unter solchen zweifelhaften Aussagen: <br><br><ol><li>  Methoden der zweiten und weiteren Ordnung funktionieren beim Training neuronaler Netze nicht gut.  Weil. </li><li>  Die Newton-Methode erfordert eine positive Bestimmtheit der hessischen Matrix (zweite Ableitungen) und funktioniert daher nicht gut. <br></li><li>  Die Levenberg-Marquardt-Methode ist ein Kompromiss zwischen Gradientenabstieg und Newton-Methode und allgemein heuristisch. <br></li></ol><br>  usw.  Um diese Liste fortzusetzen, ist es besser, zur Sache zu kommen.  In diesem Beitrag werden wir die zweite Aussage betrachten, da ich ihn in Habr√© nur mindestens zweimal getroffen habe.  Ich werde auf die erste Frage nur in dem Teil eingehen, der die Newton-Methode betrifft, da sie viel umfangreicher ist.  Der dritte und der Rest bleiben bis zu besseren Zeiten. <br><a name="habracut"></a><br>  Im Mittelpunkt unserer Aufmerksamkeit steht die Aufgabe der bedingungslosen Optimierung <img src="https://habrastorage.org/getpro/habr/post_images/823/e50/c93/823e50c935e4cc19a175f53c17ca79af.gif" title="&quot;f (x) \ rightarrow \ min&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/bc7/838/e10/bc7838e10e143195c0381efbf0671cce.gif" title="&quot;x = (x_ {1}, x_ {2}, \ dots)&quot;">  - ein Punkt des Vektorraums oder einfach - ein Vektor.  Nat√ºrlich ist diese Aufgabe umso einfacher zu l√∂sen, je mehr wir dar√ºber wissen <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  .  Es wird normalerweise angenommen, dass es in Bezug auf jedes Argument differenzierbar ist <img src="https://habrastorage.org/getpro/habr/post_images/6d8/d4e/07d/6d8d4e07d259325d5dd652e4b3b97af6.gif" title="&quot;x_ {k}&quot;">  und so oft wie n√∂tig f√ºr unsere schmutzigen Taten.  Es ist bekannt, dass eine notwendige Bedingung daf√ºr an einem Punkt ist <img src="https://habrastorage.org/getpro/habr/post_images/8ae/e32/d7e/8aee32d7e93fb189b268894bf91622b0.gif" title="&quot;x ^ {*}&quot;">  Das Minimum ist erreicht, ist die Gleichheit des Gradienten der Funktion <img src="https://habrastorage.org/getpro/habr/post_images/735/1d5/454/7351d54544ca7acc4b7a9bff7a2c2f6a.gif" title="&quot;\ bigtriangledown f (x ^ {*})&quot;">  an diesem Punkt Null.  Von hier erhalten wir sofort die folgende Minimierungsmethode: <br><br>  L√∂se die Gleichung <img src="https://habrastorage.org/getpro/habr/post_images/d59/223/ac1/d59223ac159bfe660ea26a1d60f8f33f.gif" title="&quot;\ bigtriangledown f (x) = 0&quot;">  . <br><br>  Die Aufgabe ist, gelinde gesagt, nicht einfach.  Auf keinen Fall einfacher als das Original.  An dieser Stelle k√∂nnen wir jedoch sofort den Zusammenhang zwischen dem Minimierungsproblem und dem Problem der L√∂sung eines Systems nichtlinearer Gleichungen feststellen.  Diese Verbindung wird bei der Betrachtung der Levenberg-Marquardt-Methode (wenn wir dazu kommen) auf uns zur√ºckkommen.  Denken Sie in der Zwischenzeit daran (oder finden Sie heraus), dass eine der am h√§ufigsten verwendeten Methoden zum L√∂sen von Systemen nichtlinearer Gleichungen die Newtonsche Methode ist.  Es besteht in der Tatsache, dass die Gleichung zu l√∂sen <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="F (x) = 0">  Wir gehen von einer anf√§nglichen Ann√§herung aus <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  Erstellen Sie eine Sequenz <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a0/23d/4a2/3a023d4a27cdff86f8cf3bc78d5b3a21.gif" title="x_ {i + 1} = x_ {i} -H ^ {- 1} (x_ {i}) F (x_ {i})">  - Newtons explizite Methode <br><br>  oder <br><br><img src="https://habrastorage.org/getpro/habr/post_images/116/6fa/27b/1166fa27b4038fed75d435daaaab53fe.gif" title="&quot;\ begin {F√§lle} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + p_ {i} \ end {F√§lle}&quot;">  - Newtons implizite Methode <br><br>  wo <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  - Matrix aus partiellen Ableitungen einer Funktion <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Im allgemeinen Fall, wenn uns das System der nichtlinearen Gleichungen einfach in Empfindungen gegeben wird, ist nat√ºrlich etwas von der Matrix erforderlich <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  wir sind nicht berechtigt.  In dem Fall, in dem die Gleichung eine Mindestbedingung f√ºr eine Funktion ist, k√∂nnen wir angeben, dass die Matrix <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  symmetrisch.  Aber nicht mehr. <br><br>  Newtons Methode zur L√∂sung nichtlinearer Gleichungssysteme wurde recht gut untersucht.  Und hier ist die Sache - f√ºr ihre Konvergenz ist die positive Bestimmtheit der Matrix nicht erforderlich <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  .  Ja, und kann nicht verlangt werden - sonst w√§re er wertlos gewesen.  Stattdessen gibt es andere Bedingungen, die eine lokale Konvergenz dieser Methode gew√§hrleisten und die wir hier nicht ber√ºcksichtigen werden, indem wir interessierte Personen zur Fachliteratur (oder in den Kommentaren) schicken.  Wir bekommen die Aussage 2 ist falsch. <br><br>  Also? <br><br>  Ja und nein.  Der Hinterhalt hier im Wort ist die lokale Konvergenz vor dem Wort.  Dies bedeutet, dass die anf√§ngliche Ann√§herung <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  muss ‚Äûnah genug‚Äú an der L√∂sung sein, sonst werden wir bei jedem Schritt immer weiter von ihr entfernt.  Was tun?  Ich werde nicht n√§her darauf eingehen, wie dieses Problem f√ºr Systeme nichtlinearer Gleichungen allgemeiner Form gel√∂st wird.  Zur√ºck zu unserer Optimierungsaufgabe.  Der erste Fehler von Aussage 2 ist tats√§chlich, dass die Newton-Methode bei Optimierungsproblemen normalerweise ihre Modifikation bedeutet - die ged√§mpfte Newton-Methode, bei der die Folge von Approximationen gem√§√ü der Regel konstruiert wird <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23d/ca8/404/23dca84042b77a1560b8cd2db607e8ae.gif" title="x_ {i + 1} = x_ {i} - \ alpha_ {i} H ^ {- 1} (x_ {i}) F (x_ {i})">  - Newtons explizite ged√§mpfte Methode <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9d4/67c/c96/9d467cc96266cf1179d3e553718f5bee.gif" title="&quot;\ begin {case} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + \ alpha_ {i} p_ {i} \ Ende {F√§lle} &quot;">  - Newtons implizite ged√§mpfte Methode <br><br>  Hier ist die Reihenfolge <img src="https://habrastorage.org/getpro/habr/post_images/c70/738/fd1/c70738fd1eb4d9bfff34f20904f41bbf.gif" title="&quot;\ {\ alpha_ {i} \}&quot;">  ist ein Parameter der Methode und ihre Konstruktion ist eine separate Aufgabe.  Bei Minimierungsproblemen nat√ºrlich bei der Auswahl <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  es wird eine Anforderung bestehen, dass bei jeder Iteration der Wert der Funktion f abnimmt, d.h. <img src="https://habrastorage.org/getpro/habr/post_images/3ae/e45/ba0/3aee45ba0097ca8bdc8a23ef6a465f21.gif" title="f (x_ {i + 1}) &amp; lt; f (x_ {i})">  .  Es stellt sich eine logische Frage: Gibt es eine solche (positive) <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  ?  Und wenn die Antwort auf diese Frage positiv ist, dann <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  nannte die Richtung des Abstiegs.  Dann kann die Frage folgenderma√üen gestellt werden: <br>  <i>Wann ist die nach Newtons Methode erzeugte Richtung die Abstiegsrichtung?</i> <br>  Und um es zu beantworten, m√ºssen Sie das Minimierungsproblem von einer anderen Seite betrachten. <br><br><h2>  Abstiegsmethoden </h2><br>  F√ºr das Minimierungsproblem erscheint dieser Ansatz ganz nat√ºrlich: Ausgehend von einem beliebigen Punkt w√§hlen wir die Richtung p auf irgendeine Weise und machen einen Schritt in diese Richtung <img src="https://habrastorage.org/getpro/habr/post_images/fbf/01e/b21/fbf01eb21703831c5dd0e196a2efccc2.gif" title="&quot;\ alpha p&quot;">  .  Wenn <img src="https://habrastorage.org/getpro/habr/post_images/bd9/6f9/580/bd96f95806b05f65a5766db233a85653.gif" title="f (x + \ alpha p) &amp; lt; f (x)">  dann nimm <img src="https://habrastorage.org/getpro/habr/post_images/b87/e59/538/b87e59538ed10c96ec3db2e7bad8dc85.gif" title="&quot;x + \ alpha p&quot;">  als neuen Ausgangspunkt und wiederholen Sie den Vorgang.  Wenn die Richtung willk√ºrlich gew√§hlt wird, wird eine solche Methode manchmal als Random-Walk-Methode bezeichnet.  Es ist m√∂glich, Einheitsbasisvektoren als Richtung zu verwenden - das hei√üt, um einen Schritt in nur einer Koordinate zu machen, wird diese Methode als Koordinatenabstiegsmethode bezeichnet.  Unn√∂tig zu sagen, dass sie unwirksam sind?  Damit dieser Ansatz gut funktioniert, ben√∂tigen wir einige zus√§tzliche Garantien.  Dazu f√ºhren wir eine Hilfsfunktion ein <img src="https://habrastorage.org/getpro/habr/post_images/8bf/1d5/4e1/8bf1d54e1f36dd4c9dfd5720437af51c.gif" title="g (p) = f (x + p)">  .  Ich denke, es ist offensichtlich, dass die Minimierung <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  v√∂llig gleichbedeutend mit Minimierung <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  .  Wenn <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  dann differenzierbar <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  kann dargestellt werden als <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/615/d31/e47615d310276ab67a9163889a2335a5.gif" title="g (p) = f (x) + \ bigtriangledown f ^ {T} (x) p + o (\ parallel p \ parallel ^ {2})"><br><br>  und wenn <img src="https://habrastorage.org/getpro/habr/post_images/2a7/342/acb/2a7342acbe0772f75af6eee281c247d0.gif" title="&quot;\ parallel p \ parallel&quot;">  klein genug dann <img src="https://habrastorage.org/getpro/habr/post_images/2ef/8a9/23f/2ef8a923f49cf84264effb5f3f703c31.gif" title="g (p) \ ungef√§hr \ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p">  .  Wir k√∂nnen jetzt versuchen, das Minimierungsproblem zu ersetzen <img src="https://habrastorage.org/getpro/habr/post_images/076/563/484/076563484d4e576c5c48098bfa94d45c.gif" title="g (p)">  die Aufgabe der Minimierung seiner Ann√§herung (oder <i>Modell</i> ) <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  .  √úbrigens basieren alle Methoden auf der Verwendung des Modells <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  Gradient genannt.  Aber das Problem ist, <img src="https://habrastorage.org/getpro/habr/post_images/462/957/dda/462957dda265f4fb8be04327f1c12b0f.gif" title="&quot;\ bar {g}&quot;">  Ist eine lineare Funktion und hat daher kein Minimum.  Um dieses Problem zu l√∂sen, f√ºgen wir eine Einschr√§nkung f√ºr die L√§nge des Schritts hinzu, den wir ausf√ºhren m√∂chten.  In diesem Fall ist dies eine ganz nat√ºrliche Voraussetzung - denn unser Modell beschreibt die Zielfunktion nur in einer ausreichend kleinen Nachbarschaft mehr oder weniger korrekt.  Als Ergebnis erhalten wir ein zus√§tzliches Problem der bedingten Optimierung: <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/8a2/c43/279/8a2c4327974067619cfad20b7ea1e821.gif" title="\\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ parallel p \ parallel_ {2} = \ Delta"></a> <br><br>  Diese Aufgabe hat eine offensichtliche L√∂sung: <img src="https://habrastorage.org/getpro/habr/post_images/3ff/1f6/a21/3ff1f6a2117e5d9a99603bcc8fde4f69.gif" title="&quot;p = - \ beta \ bigtriangledown f (x)&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  - Faktor, der die Erf√ºllung der Bedingung garantiert.  Dann nehmen Iterationen der Abstiegsmethode die Form an <br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/987/c25/966987c257a50df1855a50ea363350dd.gif" title="&quot;x_ {i + 1} = x_ {i} - \ beta \ bigtriangledown f (x_ {i})&quot;">  , <br><br>  in dem wir die bekannte <b>Gradientenabstiegsmethode</b> lernen.  Parameter <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , die √ºblicherweise als Abstiegsgeschwindigkeit bezeichnet wird, hat nun eine vollst√§ndig verst√§ndliche Bedeutung erhalten, und ihr Wert wird aus der Bedingung bestimmt, dass der neue Punkt auf einer Kugel mit einem bestimmten Radius liegt, die um den alten Punkt herum umschrieben ist. <br><br>  Basierend auf den Eigenschaften des konstruierten Modells der Zielfunktion k√∂nnen wir argumentieren, dass es solche gibt <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  , auch wenn sehr klein, was w√§re wenn <img src="https://habrastorage.org/getpro/habr/post_images/84b/5fd/00f/84b5fd00fe1f4ca32b7cd7bd095a1490.gif" title="&quot;\ bar {g} (p) &amp; lt; \ bar {g} (0)&quot;">  dann <img src="https://habrastorage.org/getpro/habr/post_images/553/80b/dc5/55380bdc5a434366df6d181078d6a8b7.gif" title="g (p) &amp; lt; g (0)">  .  Es ist bemerkenswert, dass in diesem Fall die Richtung, in die wir uns bewegen, nicht von der Gr√∂√üe des Radius dieser Kugel abh√§ngt.  Dann k√∂nnen wir einen der folgenden Wege w√§hlen: <br><br><ol><li>  W√§hlen Sie nach einer Methode den Wert aus <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  . </li><li>  Stellen Sie die Aufgabe ein, den entsprechenden Wert auszuw√§hlen <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , was eine Abnahme des Wertes der Zielfunktion bewirkt. </li></ol><br>  Der erste Ansatz ist typisch f√ºr die <i>Methoden der Vertrauensregion</i> , der zweite f√ºhrt zur Formulierung des Hilfsproblems der sogenannten  <i>lineare Suche (LineSearch)</i> .  In diesem speziellen Fall sind die Unterschiede zwischen diesen Ans√§tzen gering und werden nicht ber√ºcksichtigt.  Beachten Sie stattdessen Folgendes: <br><br>  <b><i>Warum suchen wir eigentlich nach einem Offset?</i></b> <b><i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i></b>  <b><i>genau auf der Kugel liegen?</i></b> <br><br>  Tats√§chlich k√∂nnten wir diese Einschr√§nkung gut durch die Anforderung ersetzen, dass beispielsweise p zur Oberfl√§che des W√ºrfels geh√∂rt, d. H. <img src="https://habrastorage.org/getpro/habr/post_images/cf1/a35/92e/cf1a3592ebe97c9e262a083ea44c594c.gif" title="&quot;\ parallel p \ parallel _ {\ infty} = \ Delta&quot;">  (in diesem Fall ist es nicht zu vern√ºnftig, aber warum nicht) oder eine elliptische Oberfl√§che?  Dies erscheint bereits ziemlich logisch, wenn wir uns an die Probleme erinnern, die bei der Minimierung von Gully-Funktionen auftreten.  Das Wesentliche des Problems ist, dass sich die Funktion entlang einiger Koordinatenlinien viel schneller √§ndert als entlang anderer.  Aus diesem Grund erhalten wir, dass, wenn das Inkrement zur Kugel geh√∂ren sollte, die Menge <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  bei dem der "Abstieg" vorgesehen ist, sollte sehr klein sein.  Und dies f√ºhrt dazu, dass das Erreichen eines Minimums eine sehr gro√üe Anzahl von Schritten erfordert.  Aber wenn wir stattdessen eine geeignete Ellipse als Nachbarschaft nehmen, wird dieses Problem auf magische Weise zunichte gemacht. <br><br>  Durch die Bedingung, dass die Punkte der elliptischen Oberfl√§che geh√∂ren, kann es geschrieben werden als <img src="https://habrastorage.org/getpro/habr/post_images/6ff/1b4/930/6ff1b49309ec84aa656d848764359b4e.gif" title="&quot;\ parallel p \ parallel_ {B} = \ sqrt {p ^ {T} Bp} = \ Delta&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  Ist eine positive bestimmte Matrix, auch Metrik genannt.  Norm <img src="https://habrastorage.org/getpro/habr/post_images/ab8/b42/711/ab8b42711a932f9129bdb193b6a74360.gif" title="&quot;\ parallel \ cdot \ parallel_ {B}&quot;">  genannt die durch die Matrix induzierte elliptische Norm <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Was f√ºr eine Matrix ist das und woher kann man sie beziehen? Wir werden sp√§ter dar√ºber nachdenken, und jetzt kommen wir zu einer neuen Aufgabe. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/f4c/cc0/757/f4ccc0757e09fb304ff10a9a8c4751b6.gif" title="\\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ dfrac {1} {2} \ parallel p \ parallel_ {B} ^ {2} = \ Delta"></a> <br><br>  Das Quadrat der Norm und der 1/2 Faktor dienen hier nur der Einfachheit halber, um nicht mit den Wurzeln herumzuspielen.  Mit der Lagrange-Multiplikatormethode erhalten wir das gebundene Problem der bedingungslosen Optimierung <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/605/36d/d5e/60536dd5e2297580940a5b926760a3ce.gif" title="f (x) + \ bigtriangledown f ^ {T} (x) p + \ dfrac {\ lambda} {2} p ^ {T} Bp- \ lambda \ Delta \ rightarrow \ min"></a> <br><br>  Eine notwendige Bedingung f√ºr ein Minimum daf√ºr ist <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/4c5/f7e/921/4c5f7e921637c73fcb992c5d9b9efcd6.gif" title="\ bigtriangledown f (x) + \ lambda Bp = 0"></a>  oder <img src="https://habrastorage.org/getpro/habr/post_images/a80/6c9/ff7/a806c9ff7ce22ea27c87b6a61a4c8fed.gif" title="&quot;B \ left (\ lambda p \ right) = - \ bigtriangledown f (x)&quot;">  von wo <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/b50/032/3a9/b500323a970c7ae821295450627bdad2.gif" title="p = - \ dfrac {1} {\ lambda} B ^ {- 1} \ bigtriangledown f (x) = \ dfrac {1} {\ lambda} \ bar {p}"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/697/906/5d7/6979065d729033e0093ffad8475e80a6.gif" title="\\\ dfrac {1} {\ lambda ^ {2}} \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) ^ {T} B \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} BB ^ {- 1} \ bigtriangledown f (x) = \ \ = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x) = \ Delta"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/41f/689/d89/41f689d890b92c84f05fdd0ede8a8114.gif" title="\ lambda = \ sqrt {\ dfrac {1} {\ Delta} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x)}> 0"></a> <br><br>  Wieder sehen wir, dass die Richtung <img src="https://habrastorage.org/getpro/habr/post_images/45b/686/bb0/45b686bb0219a74b212cfdeaf1998653.gif" title="&quot;\ bar {p} = - B ^ {- 1} \ bigtriangledown f (x)&quot;">  , in dem wir uns bewegen werden, h√§ngt nicht vom Wert ab <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  - nur aus der Matrix <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Und wieder k√∂nnen wir entweder abholen <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  das ist mit der Notwendigkeit der Berechnung behaftet <img src="https://habrastorage.org/getpro/habr/post_images/99d/394/e7d/99d394e7d0b74248114405067e0ffd51.gif" title="&quot;\ lambda&quot;">  und explizite Matrixinversion <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  oder l√∂sen Sie das Hilfsproblem, eine geeignete Vorspannung zu finden <img src="https://habrastorage.org/getpro/habr/post_images/b7b/6a7/371/b7b6a73716dc8f4e40a52c1c5ef0e6b4.gif" title="&quot;x_ {i + 1} = x_ {i} + \ beta \ bar {p} _ {i}&quot;">  .  Seit <img src="https://habrastorage.org/getpro/habr/post_images/0b8/52f/d1b/0b852fd1bbc20f2966bf757a56186312.gif" title="&quot;\ lambda &amp; gt; 0&quot;">  ist die L√∂sung f√ºr dieses Hilfsproblem garantiert. <br><br>  Was soll es also f√ºr Matrix B sein?  Wir beschr√§nken uns auf spekulative Ideen.  Wenn das Ziel funktioniert <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  - quadratisch, das hei√üt, es hat die Form <img src="https://habrastorage.org/getpro/habr/post_images/974/f7f/cf6/974f7fcf6345b91ef8466f2cabba6efe.gif" title="f (x) = a + b ^ {T} x + x ^ {T} Hx">  wo <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  positiv definitiv ist es offensichtlich, dass der beste Kandidat f√ºr die Rolle der Matrix <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  ist hessisch <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  , da in diesem Fall eine Iteration der von uns konstruierten Abstiegsmethode erforderlich ist.  Wenn H nicht eindeutig positiv ist, kann es keine Metrik sein, und die damit konstruierten Iterationen sind Iterationen der ged√§mpften Newton-Methode, aber keine Iterationen der Abstiegsmethode.  Schlie√ülich k√∂nnen wir eine strenge Antwort geben <br><br>  <b>Frage:</b> <i>Muss die hessische Matrix in der Newton-Methode positiv eindeutig sein?</i> <br>  <b>Antwort:</b> <i>Nein, es ist weder in der Standard- noch in der ged√§mpften Newton-Methode erforderlich.</i>  <i>Wenn diese Bedingung erf√ºllt ist, ist die ged√§mpfte Newton-Methode eine Abstiegsmethode und hat die Eigenschaft der <i>globalen</i> und nicht nur der lokalen Konvergenz.</i> <br><br>  Lassen Sie uns zur Veranschaulichung sehen, wie Vertrauensbereiche aussehen, wenn die bekannte Rosenbrock-Funktion mithilfe von Gradientenabstieg und Newtons Methoden minimiert wird, und wie sich die Form der Bereiche auf die Konvergenz des Prozesses auswirkt. <br><br><img src="https://habrastorage.org/webt/_x/30/nx/_x30nxs-eyrixuan0-diyvwixww.gif" width="600"><br><br>  So verh√§lt sich die Abstiegsmethode mit einem sph√§rischen Vertrauensbereich, es ist auch ein Gradientenabstieg.  Alles ist wie ein Lehrbuch - wir stecken in einer Schlucht fest. <br><br><img src="https://habrastorage.org/webt/9x/ik/td/9xiktd4lapdka-uk010evfvlcdm.gif" width="600"><br><br>  Und das bekommen wir, wenn der Konfidenzbereich die Form einer Ellipse hat, die durch die hessische Matrix definiert ist.  Dies ist nichts weiter als eine Iteration der ged√§mpften Newton-Methode. <br><br>  Nur die Frage, was zu tun ist, wenn die hessische Matrix nicht eindeutig positiv ist, blieb ungel√∂st.  Es gibt viele M√∂glichkeiten.  Das erste ist zu punkten.  Vielleicht haben Sie Gl√ºck und Newtons Iterationen konvergieren ohne diese Eigenschaft.  Dies ist sehr real, insbesondere in den letzten Phasen des Minimierungsprozesses, wenn Sie bereits nahe genug an einer L√∂sung sind.  In diesem Fall k√∂nnen Iterationen der Standard-Newton-Methode verwendet werden, ohne sich um die Suche nach einer f√ºr den Abstieg zul√§ssigen Nachbarschaft zu k√ºmmern.  Oder verwenden Sie im Fall von Iterationen der ged√§mpften Newton-Methode <img src="https://habrastorage.org/getpro/habr/post_images/6da/2c0/bc5/6da2c0bc54434a64d7630c142d0c7bf9.gif" title="&quot;\ beta = 0&quot;">  Das hei√üt, wenn die erhaltene Richtung nicht die Abstiegsrichtung ist, √§ndern Sie sie beispielsweise in einen Anti-Gradienten.  <i>Sie m√ºssen nur nicht explizit pr√ºfen, ob der Hessische nach dem Sylvester-Kriterium eindeutig positiv ist</i> , wie hier gemacht <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde !!!</a>  .  Es ist verschwenderisch und sinnlos. <br>  Bei subtileren Methoden wird eine Matrix konstruiert, die der hessischen Matrix nahe kommt, aber die Eigenschaft der positiven Bestimmtheit besitzt, insbesondere durch Korrektur von Eigenwerten.  Ein separates Thema sind die quasi-Newtonschen Methoden oder variablen metrischen Methoden, die die positive Bestimmtheit der Matrix B garantieren und keine Berechnung der zweiten Ableitungen erfordern.  Im Allgemeinen geht eine ausf√ºhrliche Diskussion dieser Themen weit √ºber den Rahmen dieses Artikels hinaus. <br><br>  Ja, und √ºbrigens folgt aus dem Gesagten, dass <i>die Newton-ged√§mpfte Methode mit positiver Bestimmtheit des Hessischen eine Gradientenmethode ist</i> .  Sowie quasi-Newtonsche Methoden.  Und viele andere, basierend auf einer separaten Wahl der Richtung und Schrittgr√∂√üe.  Daher ist es falsch, die Newton-Methode der Gradiententerminologie gegen√ºberzustellen. <br><br><h2>  Zusammenfassend </h2><br>  Die Newton-Methode, an die bei der Er√∂rterung von Minimierungsmethoden h√§ufig erinnert wird, ist normalerweise nicht die Newton-Methode im klassischen Sinne, sondern die Abstiegsmethode mit der vom Hessischen der Zielfunktion angegebenen Metrik.  Und ja, es konvergiert global, wenn der Hessische √ºberall positiv bestimmt ist.  Dies ist nur f√ºr konvexe Funktionen m√∂glich, die in der Praxis viel seltener sind als wir m√∂chten. Daher garantiert die Anwendung der Newton-Methode (wir werden uns nicht vom Kollektiv l√∂sen und es weiterhin so nennen) im allgemeinen Fall ohne die entsprechenden Modifikationen nicht das richtige Ergebnis.  Das Erlernen neuronaler Netze, auch flacher, f√ºhrt normalerweise zu nicht konvexen Optimierungsproblemen mit vielen lokalen Minima.  Und hier ist ein neuer Hinterhalt.  Newtons Methode konvergiert normalerweise schnell (wenn sie konvergiert).  Ich meine sehr schnell.  Und das ist seltsamerweise schlecht, weil wir in mehreren Iterationen zu einem lokalen Minimum kommen.  Und es kann f√ºr Funktionen mit komplexem Gel√§nde viel schlimmer sein als das globale.  Der Gradientenabstieg mit linearer Suche konvergiert viel langsamer, √ºberspringt jedoch eher die Rippen der Zielfunktion, was in den fr√ºhen Stadien der Minimierung sehr wichtig ist.  Wenn Sie den Wert der Zielfunktion bereits gut reduziert haben und sich die Konvergenz des Gradientenabfalls erheblich verlangsamt hat, kann eine √Ñnderung der Metrik den Prozess durchaus beschleunigen, dies gilt jedoch f√ºr die Endphase. <br><br>  Nat√ºrlich ist dieses Argument nicht universell, nicht unbestreitbar und in einigen F√§llen sogar falsch.  Ebenso wie die Aussage, dass Gradientenmethoden bei Lernproblemen am besten funktionieren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469877/">https://habr.com/ru/post/de469877/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469855/index.html">Gradle + LLVM</a></li>
<li><a href="../de469861/index.html">Datenstrukturen f√ºr Spielprogrammierer: Massendaten</a></li>
<li><a href="../de469867/index.html">Wie wird daraus ein Sch√ºler (und was haben Brettspiele damit zu tun)</a></li>
<li><a href="../de469871/index.html">Als Tastaturen Tische waren</a></li>
<li><a href="../de469875/index.html">So sch√ºtzen Sie Ihre Passw√∂rter im Jahr 2019</a></li>
<li><a href="../de469879/index.html">Doppel-VPN mit einem Klick. So teilen Sie einfach die IP-Adresse eines Ein- und Ausstiegspunkts</a></li>
<li><a href="../de469881/index.html">Die ersten drei Tage des Lebens eines Postens auf Habr√©</a></li>
<li><a href="../de469883/index.html">Ist es m√∂glich, Zuf√§lligkeit zu programmieren?</a></li>
<li><a href="../de469889/index.html">SamsPcbGuide, Teil 12: Technologie - BGA-Geh√§use, Kunststoff und Raum II</a></li>
<li><a href="../de469893/index.html">Haus mit Hightech-Elementen f√ºr eine obdachlose Katze</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>