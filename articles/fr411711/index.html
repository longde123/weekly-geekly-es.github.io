<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò¥ üê¥ üëéüèæ Comment Boston Dynamics a rendu BigDog autonome ‚ùì üî∏ üë©üèΩ‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La semaine derni√®re, nous avons d√©couvert le fonctionnement du l√©gendaire algorithme de coordination de la marche BigDog. Le robot n'√©tait pas encore ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment Boston Dynamics a rendu BigDog autonome</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/smileexpo/blog/411711/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/dp/1q/lt/dp1qltdljsroi6gvosqy6ptauoe.jpeg"></div><br>  <i>La semaine derni√®re, nous avons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©couvert</a> le fonctionnement du l√©gendaire algorithme de coordination de la marche BigDog.</i>  <i>Le robot n'√©tait pas encore autonome et ne pouvait traverser le terrain que sous le contr√¥le de l'op√©rateur.</i> <i><br><br></i>  <i>La plupart des lecteurs ont approuv√© la derni√®re fois l'id√©e d'une nouvelle traduction - sur la fa√ßon dont BigDog a appris √† se rendre ind√©pendamment au bon point et √† naviguer dans l'espace.</i>  <i>Eh bien, en fait, le voici.</i> <a name="habracut"></a><br><br>  Le syst√®me de navigation BigDog utilise une combinaison de balayage laser planaire, de vision st√©r√©o et de perception proprioceptive.  Avec son aide, l'emplacement du robot dans le monde environnant est d√©termin√©.  Elle d√©couvre les obstacles et les place dans un mod√®le bidimensionnel du monde.  Elle planifie ensuite le chemin et contr√¥le le robot pour suivre le chemin choisi.  Le planificateur de chemin est une variante de l'algorithme de recherche A * classique.  L'algorithme de lissage traite les r√©sultats et les transmet √† l'algorithme de suivi de chemin.  Il calcule les commandes de direction pour BigDog. <br><br>  Le syst√®me d√©crit a √©t√© test√© dans une zone foresti√®re avec de nombreux arbres, rochers et sous-bois.  En plus des territoires plats, il avait √©galement des pentes (angles allant jusqu'√† 11 degr√©s).  Au total, 26 tests ont √©t√© effectu√©s, dont 88% ont r√©ussi.  Le robot a "vu" le terrain dans un rayon de 130 m√®tres en se d√©pla√ßant √† une vitesse donn√©e et a surmont√© plus de 1,1 km. <br><br><h2>  √âquipement </h2><br>  <b>1) Capteurs proprioceptifs</b> <br><br>  Utilis√© pour contr√¥ler la d√©marche BigDog et la navigation autonome.  Chacun des 16 degr√©s de libert√© actifs et 4 passifs du robot est √©quip√© d'un capteur.  Ils fournissent des donn√©es sur la position et la charge actuelles.  Ces informations sont combin√©es avec des donn√©es IMU pour √©valuer l'√©tat de contact avec le sol, la hauteur du corps, la vitesse du corps.  En outre, un certain nombre de capteurs indiquent l'√©tat des syst√®mes de propulsion, de calcul, hydrauliques, thermiques et autres BigDog. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ul/d1/zt/uld1zt6ymgip1yrgosrwmepxl20.jpeg"></div><br>  <i>Capteurs BigDog: a) antenne GPS;</i>  <i>b) lidar;</i>  <i>c) cam√©ra st√©r√©o Bumblebee;</i>  <i>d) Honeywell IMU;</i>  <i>e) capteurs communs.</i> <br><br>  <b>2) Capteurs ext√©roceptifs</b> <br><br>  Le robot est √©quip√© de quatre capteurs externes: le lidar SICK LMS 291, la cam√©ra st√©r√©o Bumblebee PointGrey, le r√©cepteur GPS NovAtel et le Honeywell IMU.  Les donn√©es qu'ils contiennent p√©n√®trent dans le syst√®me illustr√© dans le diagramme ci-dessous. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k5/ic/-w/k5ic-wirg74tc4ynnp8_5dmossi.png"></div><br>  <b>3) Ordinateurs</b> <br><br>  Pour impl√©menter le syst√®me avec le sch√©ma ci-dessus, deux ordinateurs sont utilis√©s.  L'ordinateur principal BigDog est un PC104 avec un processeur Intel Pentium M monoc≈ìur (1,8 GHz).  Il interagit avec des capteurs proprioceptifs, contr√¥le l'√©quilibre et les mouvements du robot, calcule le mod√®le actuel de l'environnement et le chemin √† travers celui-ci, et contr√¥le √©galement la d√©marche. <br><br>  La vision est fournie par un ordinateur s√©par√© avec un processeur Intel CoreDuo (1,7 GHz).  Il communique avec une paire de cam√©ras, d√©tecte les incoh√©rences, √©value l'odom√©trie visuelle et prend en charge une carte du relief 3D.  Cet ordinateur transmet la carte et les donn√©es d'odom√©trie visuelle √† l'ordinateur h√¥te √† une fr√©quence de 15 Hz via le r√©seau local embarqu√©. <br><br>  L'avantage d'un tel syst√®me est la possibilit√© de simplifier la t√¢che de planification en la divisant en deux parties.  Les donn√©es du lidar et des capteurs sont en trois dimensions, mais nous pouvons compter sur l'auto-stabilisation du syst√®me de contr√¥le de la marche pour abandonner la perception et la planification 3D plus complexes. <br><br><h2>  Approche technique </h2><br>  Dans notre approche technique g√©n√©rale, nous utilisons les donn√©es de deux capteurs environnementaux pour d√©tecter les obstacles, calculer le chemin √† travers ou autour des obstacles, et commander au syst√®me de contr√¥le de la marche du robot de suivre un chemin donn√©. <br><br>  L'ensemble du processus peut √™tre divis√© en trois √©tapes.  Tout d'abord, les images du lidar et de la cam√©ra sont trait√©es pour obtenir une liste de points indiquant les obstacles dans l'environnement.  Ces points sont ensuite divis√©s en objets disjoints et suivis pendant un certain temps.  De plus, ces objets sont combin√©s dans la m√©moire temporaire pour le mappage.  Ce graphique est utilis√© pour planifier la direction du voyage vers une destination interm√©diaire.  L'ordonnanceur est con√ßu pour contr√¥ler que les trajectoires BigDog sont √† la bonne distance des obstacles et que les trajectoires sont stables dans l'espace pendant les it√©rations de l'ordonnanceur.  L'algorithme de mouvement le long d'une trajectoire donn√©e oblige le robot √† suivre le chemin pr√©vu, en envoyant des commandes de vitesse au syst√®me de contr√¥le de la marche.  Elle d√©place alternativement les membres du robot. <br><br><h3>  A. Collecte d'informations </h3><br>  <b>1) √âvaluation de la situation</b> <br><br>  Il existe deux sources d'informations odom√©triques: des capteurs cin√©matiques dans les jambes et un syst√®me de vision artificielle.  Les donn√©es obtenues sont combin√©es pour √©valuer l'emplacement du robot. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9e/qp/xk/9eqpxkkl0tmlthiev_xbvilijbe.png"></div><br><br>  Le syst√®me odom√©trique utilise les informations cin√©matiques des jambes pour calculer les mouvements du robot.  Et le syst√®me d'odom√©trie visuelle surveille les caract√©ristiques visuelles pour calculer le mouvement.  Ces deux outils utilisent un module de mesure inertielle (IMU) comme source d'information pour l'orientation spatiale.  Un compteur g√©n√©ral combine la sortie de ces deux odom√®tres, en se concentrant sur l'odom√©trie visuelle √† basse vitesse et la cin√©matique √† haute vitesse.  La combinaison de ces deux indicateurs √©limine les d√©fauts de chacun des compteurs: la d√©faillance √©ventuelle des syst√®mes st√©r√©o, la d√©rive des relev√©s du compteur kilom√©trique situ√©s dans les membres lors du fonctionnement en place, ainsi que les erreurs de ce capteur le long de l'axe vertical. <br><br>  Le lidar utilis√© dans le robot BigDog produit une nouvelle image toutes les 13 millisecondes.  Chaque image est transform√©e en un syst√®me de coordonn√©es externes avec le centre √† l'emplacement du robot.  Dans ce cas, les informations synchronis√©es dans le temps du compteur d'emplacement sont utilis√©es.  Le nuage de points 3D r√©sultant est ensuite transmis pour traitement par l'algorithme de segmentation d√©crit ci-dessous.  De m√™me, le syst√®me de vision st√©r√©oscopique recueille des cartes de non-conformit√© pendant un certain temps pour cr√©er une carte de terrain 3D dans un carr√© de 4 x 4 m√®tres centr√© sur le robot.  Le filtre spatial d√©termine les zones de changements d'altitude significatifs (c'est-√†-dire les obstacles potentiels) et transmet une liste de points appartenant √† ces zones √† l'algorithme de segmentation des nuages ‚Äã‚Äãde points. <br><br>  <b>2) Segmentation des nuages ‚Äã‚Äãde points et suivi des objets</b> <br><br>  En raison des irr√©gularit√©s de la terre et des mouvements du robot, une partie des donn√©es du scanner lidar comprendra des images de la terre.  Les r√©flexions des longs obstacles (tels que les murs) sont similaires en apparence aux r√©flexions de la surface de la terre.  Pour un fonctionnement r√©ussi, le syst√®me doit interpr√©ter ces r√©flexions de mani√®re √† pouvoir contr√¥ler le robot pr√®s des murs et ne pas avoir ¬´peur¬ª de la terre.  La premi√®re √©tape de ce processus est la segmentation des points d'obstacles fournis par le lidar et la carte du terrain en objets s√©par√©s.  Les nuages ‚Äã‚Äãde points 3D rares sont segment√©s en objets en fusionnant des points individuels s√©par√©s par une distance inf√©rieure √† 0,5 m√®tre. <br><br>  Les objets obtenus gr√¢ce √† l'algorithme de segmentation sont suivis pendant un certain temps.  Pour accomplir cette t√¢che, nous utilisons un algorithme it√©ratif gourmand avec des contraintes heuristiques.  L'objet dans l'image actuelle co√Øncide avec l'objet le plus proche de la derni√®re image, √† condition que les objets soient s√©par√©s par une distance ne d√©passant pas 0,7 m√®tre. <br><br>  En raison du fait que les nuages ‚Äã‚Äãde points sont segment√©s en objets et suivis pendant un certain temps, le robot peut se d√©placer de mani√®re ad√©quate dans l'environnement avec une in√©galit√© mod√©r√©e de la terre et divers types d'obstacles: arbres, pav√©s, billes tomb√©es, murs.  Les arbres et les murs sont d√©termin√©s principalement par un scanner lidar, et les pav√©s et les journaux sont d√©termin√©s par un syst√®me de vision st√©r√©oscopique. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/al/fy/twalfy7ylnbcl9a6gq1elkp79a8.jpeg"></div><br>  <i>Une s√©quence d'illustrations montrant un robot (rectangle jaune) avec: a) des donn√©es d'un lidar (points bleus) enregistr√©es en quelques secondes;</i>  <i>b) leurs installations respectives.</i>  <i>Les grands objets bruns sont des arbres.</i>  <i>Les r√©flexions au sol sont affich√©es transparentes et plates.</i>  <i>Le cylindre vert est le but;</i>  <i>la ligne bleue du chemin calcul√© y m√®ne.</i>  <i>c) Vue de dessus du cartogramme: les zones avec la valeur l√©tale la plus faible sont indiqu√©es en vert, et les zones avec la valeur la plus √©lev√©e sont indiqu√©es en violet.</i>  <i>Chaque unit√© de grille correspond √† 5 m√®tres.</i> <br><br><h3>  Planification de la navigation </h3><br>  Notre approche pour r√©soudre le probl√®me de navigation est g√©n√©ralement accept√©e dans la communaut√© robotique.  Les points d'obstacles (obtenus gr√¢ce aux processus de perception) sont trac√©s sur un cartogramme avec le centre √† l'emplacement du robot.  La cible finale du robot est projet√©e sur la fronti√®re du cartogramme, et une variante de l'algorithme A ‚àó lui est appliqu√©e.  Ce processus est r√©p√©t√© environ une fois par seconde. <br><br>  <b>1) M√©moire des obstacles suivis</b> <br><br>  En raison du champ de vision limit√© des deux capteurs du robot, il est imp√©ratif que le robot conserve une m√©moire pr√©cise des obstacles qu'il ne peut plus voir.  Comme la liste des objets est fournie par le syst√®me de suivi des objets, des objets individuels sont ajout√©s, mis √† jour ou supprim√©s dans la m√©moire d'objets du syst√®me de planification.  La taille de la liste d'objets est limit√©e, donc lorsque de nouveaux objets y sont ajout√©s, d'autres doivent √™tre supprim√©s. <br><br>  En d√©notant la liste actuelle des objets de la variable O, nous pouvons calculer deux sous-classes param√©tr√©es de O: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e3/k3/lu/e3k3lua8asqfffo9e8cda22bqb4.jpeg"></div><br><br>  Ici l'√¢ge (q) est la diff√©rence entre l'heure actuelle et l'heure de la derni√®re mesure de l'objet q, <br>  norm (q, r) inf - la distance minimale entre l'emplacement actuel du robot et la limite de l'objet q. <br><br>  <b>Les objets sont supprim√©s de O selon les crit√®res suivants:</b> <br><br><ul><li>  L'ensemble {P (30) ‚à© Q (15)} est soustrait de O. Il s'agit d'objets de plus de 30 secondes et situ√©s √† moins de 15 m√®tres du robot. </li><li>  L'ensemble {P (1800) ‚à© Q (10)} est soustrait de O. Ce sont des objets de plus d'une demi-heure et situ√©s √† moins de 10 m√®tres du robot. </li><li>  Les objets sont supprim√©s de O lorsque la limite de liste est atteinte.  La priorit√© de l'objet est d√©termin√©e par le temps pendant lequel il a √©t√© suivi avec succ√®s par le tracker.  En d'autres termes, les objets que le robot ¬´regardait¬ª plus longtemps sont stock√©s plus longtemps en m√©moire. </li><li>  Cependant, aucun objet suivi au cours des 10 derni√®res secondes n'est rejet√©. </li></ul><br>  Cette allocation de ressources m√©moire conduit au comportement suivant: lorsque les objets quittent le champ de vision des capteurs du robot, il oublie les objets supprim√©s et les objets qu'il n'a pas vus plusieurs fois.  Les objets en vue ou inaccessibles, mais situ√©s √† proximit√© du robot, ne sont pas oubli√©s. <br><br>  <b>2) Cr√©ation d'un cartogramme</b> <br><br>  Nous utilisons un cartogramme cr√©√© sur la base d'une grille 2D pour repr√©senter l'environnement entourant le robot.  Au lieu de cr√©er dynamiquement un cartogramme (lorsque le robot re√ßoit de nouvelles informations environnementales), un nouveau cartogramme est compil√© √† chaque it√©ration de la planification et rempli d'objets de la m√©moire de l'ordonnanceur.  Il s'ensuit que le planificateur d'itin√©raire dynamique ne peut pas √™tre utilis√© √† la place de l'algorithme A *.  √âtant donn√© que nous supposons que la taille des objets est limit√©e (que l'absence d'impasse dans l'environnement repr√©sente plus de la moiti√© de la carte), la port√©e de la t√¢che de planification et le temps pour calculer le chemin sont petits. <br><br>  Le cartogramme est rempli de valeurs de la liste d'objets conform√©ment √† l'algorithme suivant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vb/ld/cn/vbldcnab7njryzt9azsl-xt0soq.jpeg"></div><br><br>  Les cellules o√π se trouvent les objets se voient attribuer une valeur l√©tale tr√®s √©lev√©e.  L'indicateur pour les cellules proches de l'objet est r√©gl√© en fonction de la fonction f, qui prend en compte la distance actuelle jusqu'√† ce point.  Pour les r√©sultats des tests pr√©sent√©s ici, f √©tait simplement le cube inverse de la distance. <br><br>  L'effet de cette approche est qu'elle d√©cro√Æt progressivement des cellules de tr√®s grande valeur en s'√©loignant d'elles (et des objets qu'elles d√©signent). <br><br>  <b>3) Stabilit√© du chemin</b> <br><br>  Pour nous assurer que nous ne ¬´contr√¥lons¬ª pas BigDog de mani√®re al√©atoire et non syst√©matique, une attention particuli√®re est accord√©e √† la stabilit√© du chemin planifi√©.  Il doit √™tre aussi stable que possible gr√¢ce aux it√©rations du planificateur de trajet.  Ceci est fourni de trois mani√®res. <br><br>  Premi√®rement, le point de d√©part pass√© √† l'algorithme A * n'est pas la position actuelle du robot, mais la projection de sa position au point final du chemin donn√© pr√©c√©demment par l'algorithme A * (appelons ce point p).  Tant que BigDog suit le chemin pr√©vu, il peut en d√©vier l√©g√®rement lat√©ralement.  En projetant le point de d√©part sur le point du calcul pr√©c√©dent de l'algorithme A *, nous filtrons la fluctuation de la position du robot, et les trajets affich√©s par l'ordonnanceur deviennent plus stables.  Si le robot s'√©carte de la trajectoire plus que la valeur d√©finie (par d√©faut, il est de 3 m√®tres), le point p est simplement transf√©r√© √† la position actuelle du robot. <br><br>  Deuxi√®mement, afin de v√©rifier la continuit√© du planificateur de trajet, nous calculons q - la projection de la position du robot de 2,5 secondes dans le pass√© au dernier point calcul√© par l'algorithme A *.  Ensuite, le segment du dernier chemin planifi√© de q √† p est ajout√© au calcul du nouveau chemin.  En cons√©quence, le robot suit une petite distance d√©j√† parcourue.  Gr√¢ce √† cela, l'algorithme de suivi du chemin se montre mieux avec des violations importantes de la position, qui sont souvent rencontr√©es par des robots sur leurs pieds. <br><br>  Troisi√®mement, une partie de l'historique des trajets planifi√©s est stock√©e dans la m√©moire du robot.  Ces chemins sont utilis√©s pour r√©duire les valeurs des cellules du cartogramme o√π le robot est d√©j√† parti, tout en augmentant la valeur des cellules dans la zone environnante.  Par cons√©quent, en r√®gle g√©n√©rale, un nouveau chemin planifi√© dans la m√™me direction r√©p√©tera le chemin d√©j√† parcouru par le robot (mais sans garantie stricte). <br><br>  <b>4) Lissage de trajectoire</b> <br><br>  Le chemin calcul√©, puisqu'il est bas√© sur une grille r√©guli√®re, est un peu dentel√©.  Des changements de direction importants peuvent provoquer des commandes de contr√¥le ind√©sirables.  Pour √©viter cela, l'algorithme de lissage De Boer est appliqu√©. <br><br>  De plus, la planification de chemin bas√©e sur la grille conduit souvent √† des chemins techniquement optimaux mais moins souhaitables vers la cible.  Nous r√©solvons ce probl√®me en calculant un chemin liss√© pour chaque it√©ration de l'ordonnanceur.  Pour les it√©rations suivantes, les cellules du cartogramme o√π le chemin liss√© s'ex√©cute se voient attribuer une valeur inf√©rieure.  Cela fournit un chemin plus direct et plus fluide vers l'objectif. <br><br><h3>  C. Contr√¥le de la marche: mobilit√© et √©quilibre </h3><br>  Le syst√®me de planification de la navigation d√©termine un nouveau trajet environ une fois par seconde.  Un algorithme de suivi d'un chemin qui fonctionne √† une fr√©quence de 200 Hz guide le robot en fonction du dernier chemin pr√©vu.  Cet algorithme cr√©e un ensemble de commandes sous la forme des vitesses corporelles souhait√©es, y compris la vitesse d'avancement, la vitesse lat√©rale et la vitesse de lacet du corps.  Ces vitesses sont transmises au contr√¥leur de marche, qui contr√¥le le mouvement des jambes. <br><br>  En fonction de la distance entre le robot et le chemin, l'une des trois strat√©gies est utilis√©e.  Si le robot est situ√© pr√®s d'une section du chemin, il commence √† se d√©placer en diagonale jusqu'√† ce qu'il y p√©n√®tre par le c√¥t√©, se d√©pla√ßant √† pleine vitesse.  Si le robot est loin du chemin, il dirige exactement vers l'avant jusqu'au point souhait√©.  Dans une position interm√©diaire, une combinaison de ces strat√©gies est utilis√©e. <br><br>  Une description d√©taill√©e des algorithmes de contr√¥le de la marche d√©passe le cadre de cet article.  Cependant, en r√®gle g√©n√©rale, les vitesses corporelles agissent comme des entr√©es de contr√¥le pour les contr√¥leurs de d√©marche BigDog de bas niveau.  Le contr√¥leur de marche produit des commandes de force et de position pour chaque articulation pour assurer la stabilit√©, r√©pond aux anomalies et fournit les vitesses corporelles n√©cessaires.  Bien que les calculs de l'algorithme d'investigateur de trajectoire puissent √™tre utilis√©s pour n'importe quelle d√©marche BigDog, le trot est optimal en raison de la vitesse et de la capacit√© √† traverser un terrain accident√©. <br><br><h2>  R√©sultats des tests sur le terrain </h2><br>  Le capteur et le syst√®me de navigation d√©crits ci-dessus ont √©t√© install√©s sur BigDog et test√©s en dehors du laboratoire.  Les tests ont √©t√© effectu√©s dans une zone o√π il y avait beaucoup d'arbres, de rochers, de sous-bois, de collines avec des pentes allant jusqu'√† 11 degr√©s.  La figure 1 montre des exemples de paysage.  La figure 2 montre les donn√©es du lidar trait√©es par le robot. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o4/1y/ug/o41yug1warmbysmgjykntwplanc.jpeg"></div><br>  <i>Fig.</i>  <i>1. Le terrain o√π les tests ont √©t√© effectu√©s</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/w6/_d/mq/w6_dmqlx8c_czcoay_rnoiy_4o4.jpeg"></div><br>  <i>Fig.</i>  <i>2. Test, vue de dessus.</i>  <i>Image re√ßue du lidar et des cam√©ras st√©r√©o.</i>  <i>Les zones sombres sont des arbres et d'autres obstacles.</i>  <i>Le maillage est de 5 m√®tres.</i> <br><br>  Le syst√®me de navigation et le planificateur ont √©t√© d√©velopp√©s sur une p√©riode de 7 mois, avec des tests r√©guliers environ une fois toutes les cinq semaines.  Les r√©sultats des derniers tests sont d√©crits ici. <br><br>  Sur les 26 tests effectu√©s, 23 se sont termin√©s avec succ√®s: le robot a atteint le but, n'a rencontr√© aucun obstacle et n'√©tait pas pr√®s de cela.  Les r√©sultats de ces tests sont marqu√©s dans le tableau crois√© dynamique comme objectif.  Le robot est tomb√© √† la fin d'un seul test apr√®s avoir march√© sur une petite pierre.  Habituellement, le syst√®me de contr√¥le de la marche fait face √† de telles situations, mais pas cette fois (le r√©sultat est marqu√© dans le tableau comme Automne - ¬´Automne¬ª).  Lors de trois tests, le robot a rencontr√© de gros obstacles (plus de 20 m√®tres de large).  Le robot a calcul√© de quel c√¥t√© il √©tait pr√©f√©rable de contourner l'obstacle et n'a pas avanc√© dans un laps de temps donn√© (20 secondes).  Des obstacles de cette taille d√©passent le cadre pour lequel un syst√®me autonome a √©t√© d√©velopp√©.  Les r√©sultats de ces tests sont indiqu√©s dans le tableau comme Live-lock. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/an/hu/mi/anhumiw32wo1mu8oc7dtcbgsirs.jpeg"></div><br><br>  Dans ces 26 tests, le robot a √©t√© plac√© dans des sc√©narios assez similaires et au milieu de la for√™t.  √Ä mesure que l'environnement devient plus complexe, le nombre de r√©sultats de verrouillage en direct augmente et le robot s√©lectionne des chemins moins efficaces. <br><br>  <b>Plus int√©ressant - sur robo-hunter.com</b> : <br><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ; </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ; </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> . </li></ul><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   YouTube</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr411711/">https://habr.com/ru/post/fr411711/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr411701/index.html">La NASA r√©√©dite une tourn√©e lunaire en 4K</a></li>
<li><a href="../fr411703/index.html">Xiaomi, d√©placez-vous, Alfawise est venu</a></li>
<li><a href="../fr411705/index.html">Service en ligne comparant les changements de r√©ponse en fr√©quence et de pression acoustique en fonction de la tension appliqu√©e et de l'imp√©dance de la source</a></li>
<li><a href="../fr411707/index.html">Nakraudfandili: les meilleurs projets pour mars 2018</a></li>
<li><a href="../fr411709/index.html">Prix ‚Äã‚Äãdes produits spatiaux domestiques</a></li>
<li><a href="../fr411713/index.html">Si√®ge social cryptanarchiste et narguil√© pour les bitcoins: guide de crypto-monnaie √† Prague</a></li>
<li><a href="../fr411715/index.html">Cerveau, drogues et rock'n'roll: personnalit√© et recherche du stand-up le plus musical des neuroscientifiques</a></li>
<li><a href="../fr411717/index.html">Canon dans un baril. Essai 12. Monde d'id√©es</a></li>
<li><a href="../fr411719/index.html">Google d√©veloppe un microscope AR pour la d√©tection rapide du cancer</a></li>
<li><a href="../fr411721/index.html">LED clignotante du module du noyau Linux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>