<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüè≠ üö† ‚õ±Ô∏è Le r√©seau de neurones AttnGAN dessine des objets en plusieurs parties, en utilisant l'espace vectoriel non seulement des phrases, mais aussi des mots üç¶ ü§≥üèº ü§üüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Exemple de fonctionnement AttnGAN. Dans la rang√©e sup√©rieure se trouvent plusieurs images de diff√©rentes r√©solutions g√©n√©r√©es par un r√©seau neuronal. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le r√©seau de neurones AttnGAN dessine des objets en plusieurs parties, en utilisant l'espace vectoriel non seulement des phrases, mais aussi des mots</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/409747/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/lk/gs/w1/lkgsw1oaf838l4x7vrtlww4dcam.jpeg"></div><br>  <i><font color="gray">Exemple de fonctionnement AttnGAN.</font></i>  <i><font color="gray">Dans la rang√©e sup√©rieure se trouvent plusieurs images de diff√©rentes r√©solutions g√©n√©r√©es par un r√©seau neuronal.</font></i>  <i><font color="gray">Les deuxi√®me et troisi√®me lignes montrent le traitement des cinq mots les plus appropri√©s par deux mod√®les de l'attention du r√©seau neuronal pour dessiner les sections les plus pertinentes</font></i> <br><br>  La cr√©ation automatique d'images √† partir de descriptions de texte dans un langage naturel est un probl√®me fondamental pour de nombreuses applications, telles que la g√©n√©ration d'art et la conception informatique.  Ce probl√®me stimule √©galement les progr√®s dans le domaine de la formation √† l'IA multimodale avec une relation entre la vision et le langage. <br><br>  Des recherches r√©centes men√©es par des chercheurs dans ce domaine sont bas√©es sur des r√©seaux contradictoires g√©n√©ratifs (GAN).  L'approche g√©n√©rale consiste √† traduire la description textuelle enti√®re dans le vecteur de phrase global.  Cette approche d√©montre un certain nombre de r√©sultats impressionnants, mais elle pr√©sente les principaux inconv√©nients: le manque de d√©tails clairs au niveau des mots et l'impossibilit√© de g√©n√©rer des images haute r√©solution.  Une √©quipe de d√©veloppeurs de Lichai University, Rutgers University, Duke University (all - USA) et Microsoft ont propos√© leur solution au probl√®me: le nouveau r√©seau de neurones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Attentionn Generative Adversarial Network (AttnGAN)</a> est une am√©lioration de l'approche traditionnelle et permet un changement en plusieurs √©tapes de l'image g√©n√©r√©e, en changeant des mots individuels dans le texte description. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/2j/dv/5i/2jdv5ilcqwd6s4v5ymkjov4eq1i.png"><br><br>  <i><font color="gray">Architecture du r√©seau neuronal AttnGAN.</font></i>  <i><font color="gray">Chaque mod√®le d'attention re√ßoit automatiquement des conditions (c'est-√†-dire des vecteurs de vocabulaire correspondants) pour g√©n√©rer diff√©rentes zones de l'image.</font></i>  <i><font color="gray">Le module DAMSM offre une granularit√© suppl√©mentaire pour la fonction de perte de conformit√© dans la traduction de l'image au texte dans le r√©seau g√©n√©ratif</font></i> <br><br>  Comme vous pouvez le voir dans l'illustration d√©crivant l'architecture du r√©seau neuronal, le mod√®le AttnGAN pr√©sente deux innovations par rapport aux approches traditionnelles. <br><br>  Tout d'abord, il s'agit d'un r√©seau accusatoire, qui fait r√©f√©rence √† l'attention comme facteur d'apprentissage (Attentional Generative Adversarial Network).  Autrement dit, il met en ≈ìuvre le m√©canisme d'attention, qui d√©termine les mots les plus appropri√©s pour g√©n√©rer les parties correspondantes de l'image.  En d'autres termes, en plus de coder l'int√©gralit√© de la description textuelle dans l'espace vectoriel global des phrases, chaque mot individuel est √©galement cod√© en tant que vecteur textuel.  √Ä la premi√®re √©tape, le r√©seau neuronal g√©n√©ratif utilise l'espace vectoriel global des phrases pour rendre une image √† basse r√©solution.  Dans les √©tapes suivantes, elle utilise le vecteur d'image dans chaque r√©gion pour interroger les vecteurs de dictionnaire, en utilisant la couche d'attention pour former le vecteur de contexte de mot.  Ensuite, le vecteur d'image r√©gional est combin√© avec le vecteur de contexte de mot correspondant pour former un vecteur de contexte multimodal, sur la base duquel le mod√®le g√©n√®re de nouvelles caract√©ristiques d'image dans les r√©gions respectives.  Cela vous permet d'augmenter efficacement la r√©solution de l'image enti√®re dans son ensemble, car √† chaque √©tape, il y a de plus en plus de d√©tails. <br><br>  La deuxi√®me innovation de Microsoft en mati√®re de r√©seau neuronal est le module DAMSM (Deep Attentional Multimodal Similarity Model).  √Ä l'aide du m√©canisme d'attention, ce module calcule le degr√© de similitude entre l'image g√©n√©r√©e et la phrase de texte, en utilisant √† la fois des informations du niveau de l'espace vectoriel des phrases et un niveau bien d√©taill√© de vecteurs de dictionnaire.  Ainsi, DAMSM fournit une granularit√© suppl√©mentaire pour la perte de la fonction d'ajustement lors de la traduction de l'image au texte lors de la formation du g√©n√©rateur. <br><br>  Gr√¢ce √† ces deux innovations, le r√©seau de neurones AttnGAN affiche des r√©sultats nettement meilleurs que le meilleur des syst√®mes GAN traditionnels, √©crivent les d√©veloppeurs.  En particulier, le score de d√©marrage maximum connu pour les r√©seaux de neurones existants a √©t√© am√©lior√© de 14,14% (de 3,82 √† 4,36) sur l'ensemble de donn√©es CUB et am√©lior√© jusqu'√† 170,25% (de 9,58 √† 25,89) sur l'ensemble de donn√©es COCO plus sophistiqu√©. <br><br>  L'importance de cette √©volution est difficile √† surestimer.  Le r√©seau de neurones AttnGAN a montr√© pour la premi√®re fois qu'un r√©seau g√©n√©ratif-accusatoire multicouche, qui se r√©f√®re √† l'attention comme facteur d'apprentissage, est capable de d√©terminer automatiquement les conditions au niveau du mot pour g√©n√©rer des parties individuelles d'une image. <br><br>  L'article scientifique a √©t√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">publi√©</a> le 28 novembre 2017 sur le site de pr√©impression arXiv.org (arXiv: 1711.10485v1). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr409747/">https://habr.com/ru/post/fr409747/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr409735/index.html">Illusion au niveau du consensus</a></li>
<li><a href="../fr409739/index.html">Bloomberg: Telegram pr√©voit de gagner plus d'un milliard de dollars lors de l'ICO</a></li>
<li><a href="../fr409741/index.html">La Maison Blanche souhaite lancer Falcon Heavy</a></li>
<li><a href="../fr409743/index.html">Contr√¥leur DIY de panneau LED sur CPLD utilisant la modulation BAM</a></li>
<li><a href="../fr409745/index.html">Le sp√©cialiste de l'IA pr√©tend avoir r√©ussi √† comprendre dans quelle langue le manuscrit de Voynich est √©crit</a></li>
<li><a href="../fr409749/index.html">Chaudi√®re de pyrolyse √† domicile, ou lorsque le prix du gaz n'a pas d'importance</a></li>
<li><a href="../fr409751/index.html">Lettre AudioFilkina: musique blue tooth - pas pour le battage m√©diatique, mais pour le bien</a></li>
<li><a href="../fr409753/index.html">L'ONU pourrait terminer une exp√©rience al√©atoire √† grande √©chelle pour r√©duire le r√©chauffement climatique</a></li>
<li><a href="../fr409757/index.html">L'analyse scientifique des infrastructures Bitcoin et Ethereum montre une plus grande centralisation des r√©seaux</a></li>
<li><a href="../fr409759/index.html">Intel a averti les fournisseurs chinois des vuln√©rabilit√©s de fusion et de spectre devant le gouvernement am√©ricain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>