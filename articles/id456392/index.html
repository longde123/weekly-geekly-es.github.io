<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤽🏾 👩‍👦 👩‍🚒 Parsim 25TB dengan AWK dan R 🐾 🐤 👩‍👦‍👦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cara membaca artikel ini : Saya minta maaf atas kenyataan bahwa teksnya ternyata begitu panjang dan kacau. Untuk menghemat waktu Anda, saya memulai se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsim 25TB dengan AWK dan R</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/456392/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>Cara membaca artikel ini</b> : Saya minta maaf atas kenyataan bahwa teksnya ternyata begitu panjang dan kacau.</i>  <i>Untuk menghemat waktu Anda, saya memulai setiap bab dengan pengenalan "Apa yang Saya Pelajari," di mana saya menjelaskan esensi bab dalam satu atau dua kalimat.</i> <i><br><br></i>  <i><b>"Tunjukkan solusinya!"</b></i>  <i>Jika Anda hanya ingin melihat apa yang saya maksud, kemudian buka bab "Menjadi lebih inventif," tetapi saya pikir lebih menarik dan bermanfaat untuk membaca tentang kegagalan.</i> <br><br>  Baru-baru ini, saya diperintahkan untuk mengatur proses untuk memproses sejumlah besar urutan DNA asli (secara teknis, ini adalah chip SNP).  Itu perlu untuk mendapatkan data dengan cepat pada lokasi genetik yang diberikan (disebut SNP) untuk pemodelan berikutnya dan tugas-tugas lainnya.  Dengan bantuan R dan AWK, saya dapat membersihkan dan mengatur data dengan cara alami, sangat mempercepat pemrosesan permintaan.  Ini tidak mudah bagi saya dan membutuhkan banyak iterasi.  Artikel ini akan membantu Anda menghindari beberapa kesalahan saya dan menunjukkan apa yang saya lakukan pada akhirnya. <br><a name="habracut"></a><br>  Pertama, beberapa penjelasan pengantar. <br><br><h2>  Data </h2><br>  Pusat Pemrosesan Informasi Genetik Universitas kami telah memberi kami 25 TB data TSV.  Saya membaginya menjadi 5 paket yang dikompres oleh Gzip, yang masing-masing berisi sekitar 240 file empat gigabyte.  Setiap baris berisi data untuk satu SNP dari satu orang.  Secara total, data ~ 2,5 juta SNP dan ~ 60 ribu orang dikirimkan.  Selain informasi SNP, ada banyak kolom dalam file dengan angka yang mencerminkan berbagai karakteristik, seperti intensitas membaca, frekuensi alel yang berbeda, dll.  Ada sekitar 30 kolom dengan nilai unik. <br><br><h4>  Tujuan </h4><br>  Seperti halnya proyek manajemen data, hal terpenting adalah menentukan bagaimana data akan digunakan.  Dalam hal ini, <b>sebagian besar, kami akan memilih model dan alur kerja untuk SNP berdasarkan SNP</b> .  Artinya, pada saat yang sama kita akan membutuhkan data hanya untuk satu SNP.  Saya harus belajar cara mengekstrak semua catatan yang terkait dengan salah satu dari 2,5 juta SNP sesederhana mungkin, lebih cepat dan lebih murah. <br><br><h1>  Bagaimana tidak melakukannya </h1><br>  Saya akan mengutip klise yang sesuai: <br><br><blockquote>  Saya tidak gagal ribuan kali, saya baru saja menemukan seribu cara untuk tidak menguraikan banyak data dalam format yang sesuai untuk permintaan. </blockquote><br>
<h2>  Upaya pertama </h2><br>  <b>Apa yang saya pelajari</b> : Tidak ada cara murah untuk mengurai 25 TB sekaligus. <br><br>  Setelah mendengarkan subjek "Metode Pengolahan Data Besar Lanjut" di Vanderbilt University, saya yakin itu adalah topi.  Mungkin akan membutuhkan satu atau dua jam untuk mengkonfigurasi server Hive untuk menjalankan semua data dan melaporkan hasilnya.  Karena data kami disimpan di AWS S3, saya menggunakan layanan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Athena</a> , yang memungkinkan Anda untuk menerapkan kueri Hive SQL ke data S3.  Tidak perlu mengkonfigurasi / menaikkan Hive-cluster, dan bahkan hanya membayar untuk data yang Anda cari. <br><br>  Setelah saya menunjukkan data dan formatnya kepada Athena, saya melakukan beberapa tes dengan pertanyaan serupa: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  Dan dengan cepat mendapat hasil yang terstruktur dengan baik.  Selesai <br><br>  Sampai kami mencoba menggunakan data dalam pekerjaan ... <br><br>  Saya diminta untuk mengeluarkan semua informasi SNP untuk menguji model di atasnya.  Saya menjalankan kueri: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... dan menunggu.  Setelah delapan menit dan lebih dari 4 TB data yang diminta, saya mendapatkan hasilnya.  Athena membebankan biaya untuk jumlah data yang ditemukan, sebesar $ 5 per terabyte.  Jadi permintaan tunggal ini biaya $ 20 dan delapan menit menunggu.  Untuk menjalankan model sesuai dengan semua data, perlu menunggu 38 tahun dan membayar $ 50 juta. Jelas, ini tidak cocok untuk kita. <br><br><h2>  Itu perlu untuk menggunakan Parket ... </h2><br>  <b>Apa yang saya pelajari</b> : Hati-hati dengan ukuran file Parket Anda dan organisasi mereka. <br><br>  Pada awalnya saya mencoba untuk memperbaiki situasi dengan mengubah semua TSV ke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">file Parket</a> .  Mereka nyaman untuk bekerja dengan set data besar, karena informasi di dalamnya disimpan dalam bentuk kolom: setiap kolom terletak di segmen memori / disk sendiri, tidak seperti file teks di mana baris berisi elemen dari setiap kolom.  Dan jika Anda perlu menemukan sesuatu, maka baca saja kolom yang diperlukan.  Selain itu, rentang nilai disimpan di setiap file dalam kolom, jadi jika nilai yang diinginkan tidak berada dalam rentang kolom, Spark tidak akan membuang waktu memindai seluruh file. <br><br>  Saya menjalankan tugas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Lem AWS</a> sederhana untuk mengkonversi TSV kami ke Parket dan menjatuhkan file baru ke Athena.  Butuh sekitar 5 jam.  Tetapi ketika saya meluncurkan permintaan, dibutuhkan waktu yang hampir bersamaan dan sedikit uang untuk menyelesaikannya.  Faktanya adalah Spark, yang mencoba mengoptimalkan tugas, cukup membongkar satu TSV-chunk dan meletakkannya di Parquet-chunk sendiri.  Dan karena setiap potongan cukup besar dan berisi catatan lengkap banyak orang, semua SNP disimpan dalam setiap file, sehingga Spark harus membuka semua file untuk mengekstrak informasi yang diperlukan. <br><br>  Anehnya, tipe kompresi default (dan disarankan) di Parket - tajam - tidak dapat dipisahkan.  Oleh karena itu, setiap pelaksana terjebak pada tugas membongkar dan mengunduh dataset 3,5 GB penuh. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  Kami mengerti masalahnya </h2><br>  <b>Apa yang saya pelajari</b> : menyortir itu sulit, terutama jika datanya didistribusikan. <br><br>  Tampak bagi saya bahwa sekarang saya mengerti esensi masalah.  Yang harus saya lakukan adalah mengurutkan data berdasarkan kolom SNP, bukan oleh orang.  Kemudian beberapa SNP akan disimpan dalam potongan data yang terpisah, dan kemudian fungsi pintar Parket "terbuka hanya jika nilainya dalam kisaran" akan memanifestasikan dirinya dalam semua kemuliaan.  Sayangnya, memilah miliaran baris yang tersebar di sebuah cluster telah terbukti menjadi tugas yang menakutkan. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  AWS tentu saja tidak ingin mengembalikan uang itu karena "Saya seorang siswa yang linglung."  Setelah saya mulai menyortir di Lem Amazon, itu bekerja selama 2 hari dan jatuh. <br><br><h2>  Bagaimana dengan mempartisi? </h2><br>  <b>Apa yang saya pelajari</b> : Partisi dalam Spark harus seimbang. <br><br>  Kemudian muncul ide untuk membagi data pada kromosom.  Ada 23 di antaranya (dan beberapa lagi, diberi DNA mitokondria dan area yang belum dipetakan). <br>  Ini akan memungkinkan Anda untuk membagi data menjadi bagian-bagian yang lebih kecil.  Jika Anda hanya menambahkan satu baris <code>partition_by = "chr"</code> ke fungsi ekspor Spark di skrip Glue, maka data harus diurutkan menjadi kotak. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>Genom terdiri dari banyak fragmen yang disebut kromosom.</i> <br><br>  Sayangnya, ini tidak berhasil.  Kromosom memiliki ukuran yang berbeda, dan karena itu jumlah informasi yang berbeda.  Ini berarti bahwa tugas-tugas yang dikirim Spark ke pekerja tidak seimbang dan dilakukan perlahan-lahan, karena beberapa node selesai sebelumnya dan menganggur.  Namun, tugas itu selesai.  Tetapi ketika meminta satu SNP, ketidakseimbangan kembali menyebabkan masalah.  Biaya pemrosesan SNP pada kromosom yang lebih besar (yaitu, tempat kami ingin mendapatkan data) menurun hanya sekitar 10 kali.  Banyak, tetapi tidak cukup. <br><br><h2>  Dan jika Anda membaginya menjadi partisi yang lebih kecil? </h2><br>  <b>Apa yang saya pelajari</b> : tidak pernah mencoba melakukan 2,5 juta partisi sama sekali. <br><br>  Saya memutuskan untuk berjalan-jalan dan mempartisi setiap SNP.  Ini menjamin ukuran partisi yang sama.  <b>BURUK IDE</b> .  Saya mengambil keuntungan dari Lem dan menambahkan <code>partition_by = 'snp'</code> tidak bersalah.  Tugas dimulai dan mulai dijalankan.  Sehari kemudian, saya memeriksa dan melihat bahwa tidak ada yang ditulis dalam S3 sejauh ini, jadi saya membunuh tugas itu.  Sepertinya Lem sedang menulis file perantara ke tempat tersembunyi di S3, dan banyak file, mungkin beberapa juta.  Akibatnya, kesalahan saya menelan biaya lebih dari seribu dolar dan tidak menyenangkan mentor saya. <br><br><h2>  Partisi + penyortiran </h2><br>  <b>Apa yang saya pelajari</b> : menyortir masih sulit, seperti menyiapkan Spark. <br><br>  Upaya terakhir untuk mempartisi adalah saya mempartisi kromosom dan kemudian mengurutkan setiap partisi.  Secara teori, ini akan mempercepat setiap permintaan, karena data SNP yang diinginkan harus dalam beberapa potongan Parket dalam kisaran yang diberikan.  Sayangnya, menyortir bahkan data yang dipartisi telah terbukti menjadi tugas yang sulit.  Akibatnya, saya beralih ke ESDM untuk kluster khusus dan menggunakan delapan instance kuat (C5.4xl) dan Sparklyr untuk membuat alur kerja yang lebih fleksibel ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... namun, tugas itu masih belum selesai.  Saya menyetel dengan segala cara: Saya meningkatkan alokasi memori untuk setiap pelaksana kueri, menggunakan node dengan jumlah memori yang besar, menggunakan variabel penyiaran, tetapi setiap kali ternyata menjadi setengah ukuran, dan lambat laun para pemain mulai gagal, sampai semuanya berhenti. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Saya semakin inventif </h1><br>  <b>Apa yang saya pelajari</b> : terkadang data khusus memerlukan solusi khusus. <br><br>  Setiap SNP memiliki nilai posisi.  Ini adalah angka yang sesuai dengan jumlah basa yang terletak di sepanjang kromosomnya.  Ini adalah cara yang baik dan alami untuk mengatur data kami.  Pada awalnya saya ingin mempartisi berdasarkan wilayah setiap kromosom.  Misalnya, posisi 1 - 2000, 2001 - 4000, dll.  Tetapi masalahnya adalah SNP tidak terdistribusi secara merata di seluruh kromosom, oleh karena itu ukuran kelompok akan sangat bervariasi. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  Sebagai hasilnya, saya kemudian dibagi menjadi beberapa kategori (peringkat) posisi.  Menurut data yang sudah diunduh, saya menjalankan permintaan daftar SNP unik, posisi dan kromosom mereka.  Kemudian dia mengurutkan data di dalam setiap kromosom dan mengumpulkan SNP menjadi kelompok (bin) dengan ukuran tertentu.  Katakan masing-masing 1.000 SNP.  Ini memberi saya hubungan SNP dengan kelompok-dalam-kromosom. <br><br>  Pada akhirnya, saya membuat grup (bin) pada 75 SNP, saya akan menjelaskan alasannya di bawah ini. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  Pertama coba dengan Spark </h2><br>  <b>Apa yang saya pelajari</b> : Integrasi spark cepat, tetapi mempartisi masih mahal. <br><br>  Saya ingin membaca bingkai data kecil (2,5 juta baris) ini di Spark, menggabungkannya dengan data mentah, dan kemudian mempartisi dengan kolom <code>bin</code> baru ditambahkan. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  Saya menggunakan <code>sdf_broadcast()</code> , jadi Spark mengetahui bahwa itu harus mengirim bingkai data ke semua node.  Ini berguna jika datanya kecil dan diperlukan untuk semua tugas.  Jika tidak, Spark mencoba menjadi pintar dan mendistribusikan data sesuai kebutuhan, yang dapat menyebabkan rem. <br><br>  Dan lagi, ide saya tidak berhasil: tugas-tugasnya bekerja sebentar, menyelesaikan merger, dan kemudian, seperti para pelaksana yang diluncurkan dengan mempartisi, mereka mulai gagal. <br><br><h2>  Tambahkan AWK </h2><br>  <b>Apa yang saya pelajari</b> : jangan tidur ketika dasar mengajarkan Anda.  Tentunya seseorang sudah memecahkan masalah Anda di tahun 1980-an. <br><br>  Hingga saat ini, penyebab semua kegagalan saya dengan Spark adalah kebingungan data di cluster.  Mungkin situasinya dapat ditingkatkan dengan pra-pemrosesan.  Saya memutuskan untuk mencoba membagi data teks mentah menjadi kolom kromosom, jadi saya berharap untuk memberikan Spark dengan data "pra-partisi". <br><br>  Saya mencari di StackOverflow untuk cara memecah nilai kolom dan menemukan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">jawaban yang bagus.</a>  Menggunakan AWK, Anda dapat membagi file teks menjadi nilai kolom dengan menulis ke skrip, daripada mengirim hasilnya ke <code>stdout</code> . <br><br>  Untuk pengujian, saya menulis skrip Bash.  Saya mengunduh salah satu TSV yang dikemas, kemudian membukanya dengan <code>gzip</code> dan mengirimkannya ke <code>awk</code> . <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  Berhasil! <br><br><h2>  Mengisi inti </h2><br>  <b>Apa yang saya pelajari</b> : <code>gnu parallel</code> adalah hal yang ajaib, semua orang harus menggunakannya. <br><br>  Pemisahan itu agak lambat, dan ketika saya mulai <code>htop</code> untuk menguji penggunaan contoh EC2 yang kuat (dan mahal), ternyata saya hanya menggunakan satu inti dan sekitar 200 MB memori.  Untuk menyelesaikan masalah dan tidak kehilangan banyak uang, perlu dipikirkan cara memparalelkan pekerjaan.  Untungnya, dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Data Science</a> Jeron Janssens yang menakjubkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">di buku Command Line</a> , saya menemukan bab tentang paralelisasi.  Dari sini saya belajar tentang <code>gnu parallel</code> , metode yang sangat fleksibel untuk mengimplementasikan multithreading di Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  Ketika saya memulai partisi menggunakan proses baru, semuanya baik-baik saja, tetapi ada hambatan - mengunduh objek S3 ke disk tidak terlalu cepat dan tidak sepenuhnya paralel.  Untuk memperbaikinya, saya melakukan ini: <br><br><ol><li>  Saya menemukan bahwa adalah mungkin untuk mengimplementasikan langkah S3-unduh langsung di dalam pipa, sepenuhnya menghilangkan penyimpanan perantara pada disk.  Ini berarti bahwa saya dapat menghindari penulisan data mentah ke disk dan menggunakan lebih kecil, dan karenanya penyimpanan lebih murah di AWS. <br></li><li>  Perintah <code>aws configure set default.s3.max_concurrent_requests 50</code> sangat meningkatkan jumlah utas yang digunakan AWS CLI (ada 10 secara default). <br></li><li>  Saya beralih ke instance EC2 yang dioptimalkan untuk kecepatan jaringan, dengan huruf n pada namanya.  Saya menemukan bahwa hilangnya daya komputasi saat menggunakan n-instances lebih dari diimbangi dengan peningkatan kecepatan pengunduhan.  Untuk sebagian besar tugas, saya menggunakan c5n.4xl. <br></li><li>  Saya mengubah <code>gzip</code> menjadi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><code>pigz</code></a> , ini adalah alat gzip yang dapat melakukan hal-hal keren untuk memparalelkan tugas membongkar file yang awalnya tidak tertandingi (ini paling sedikit membantu). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  Langkah-langkah ini dikombinasikan satu sama lain sehingga semuanya bekerja sangat cepat.  Berkat peningkatan kecepatan pengunduhan dan penolakan penulisan ke disk, sekarang saya dapat memproses paket 5-terabyte hanya dalam beberapa jam. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Tweet ini seharusnya menyebutkan 'TSV'.  Sayang <br><br><h2>  Menggunakan data yang diurai ulang </h2><br>  <b>Apa yang saya pelajari</b> : Spark menyukai data yang tidak terkompresi dan tidak suka menggabungkan partisi. <br><br>  Sekarang datanya dalam S3 dalam format yang sudah dibongkar (baca, bagikan) dan semi-teratur, dan saya bisa kembali ke Spark lagi.  Kejutan menunggu saya: Saya kembali gagal mencapai yang diinginkan!  Sangat sulit untuk memberi tahu Spark bagaimana tepatnya data dipartisi.  Dan bahkan ketika saya melakukan ini, ternyata ada terlalu banyak partisi (95 ribu), dan ketika saya mengurangi jumlah mereka menjadi batas yang koheren dengan <code>coalesce</code> , itu merusak partisi saya.  Saya yakin ini bisa diperbaiki, tetapi dalam beberapa hari pencarian, saya tidak dapat menemukan solusi.  Pada akhirnya, saya menyelesaikan semua tugas di Spark, meskipun butuh beberapa saat, dan file parket saya tidak terlalu kecil (~ 200 Kb).  Namun, data di tempat itu dibutuhkan. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Terlalu kecil dan berbeda, luar biasa!</i> <br><br><h2>  Menguji permintaan Spark lokal </h2><br>  <b>Apa yang saya pelajari</b> : Spark memiliki terlalu banyak masalah dalam menyelesaikan masalah sederhana. <br><br>  Dengan mengunduh data dalam format cerdas, saya dapat menguji kecepatannya.  Saya mengatur skrip pada R untuk memulai server Spark lokal, dan kemudian saya memuat frame data Spark dari repositori tertentu dari grup Parket (bin).  Saya mencoba memuat semua data, tetapi tidak bisa membuat Sparklyr mengenali partisi. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  Eksekusi memakan waktu 29,415 detik.  Jauh lebih baik, tetapi tidak terlalu baik untuk pengujian massal apa pun.  Selain itu, saya tidak bisa mempercepat pekerjaan menggunakan caching, karena ketika saya mencoba untuk men-cache bingkai data dalam memori, Spark selalu macet, bahkan ketika saya mengalokasikan lebih dari 50 GB memori untuk dataset yang beratnya kurang dari 15. <br><br><h2>  Kembali ke AWK </h2><br>  <b>Apa yang saya pelajari</b> : array asosiatif AWK sangat efisien. <br><br>  Saya mengerti bahwa saya bisa mencapai kecepatan yang lebih tinggi.  Saya ingat bahwa dalam panduan AWK <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bruce Barnett yang</a> sangat baik, saya membaca tentang fitur keren yang disebut “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">array asosiatif</a> ”.  Sebenarnya, ini adalah pasangan kunci-nilai, yang karena alasan tertentu disebut berbeda di AWK, dan karena itu saya entah bagaimana tidak secara khusus menyebutkannya.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Roman Cheplyaka</a> mengingat bahwa istilah “array asosiatif” jauh lebih tua daripada istilah “pasangan nilai kunci”.  Bahkan jika Anda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mencari nilai kunci di Google Ngram</a> , Anda tidak akan melihat istilah ini di sana, tetapi Anda akan menemukan array asosiatif!  Selain itu, pasangan kunci-nilai paling sering dikaitkan dengan basis data, sehingga jauh lebih logis untuk membandingkan dengan hashmap.  Saya menyadari bahwa saya bisa menggunakan array asosiatif ini untuk menghubungkan SNP saya ke tabel bin dan data mentah tanpa menggunakan Spark. <br><br>  Untuk ini, dalam skrip AWK, saya menggunakan blok <code>BEGIN</code> .  Ini adalah bagian dari kode yang dieksekusi sebelum baris pertama data ditransfer ke badan utama skrip. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  Perintah <code>while(getline...)</code> memuat semua baris dari grup CSV (nampan), mengatur kolom pertama (nama SNP) sebagai kunci untuk array asosiatif <code>bin</code> dan nilai kedua (grup) sebagai nilainya.  Kemudian, di blok <code>{</code> <code>}</code> , yang diterapkan ke semua baris file utama, setiap baris dikirim ke file output, yang mendapatkan nama unik tergantung pada grupnya (bin): <code>..._bin_"bin[$1]"_...</code> <br><br>  <code>batch_num</code> dan <code>chunk_id</code> sesuai dengan data yang disediakan oleh pipeline, yang menghindari status ras, dan setiap utas eksekusi yang diluncurkan secara <code>parallel</code> menulis ke file uniknya sendiri. <br><br>  Karena saya menyebarkan semua data mentah ke folder pada kromosom yang tersisa setelah percobaan saya sebelumnya dengan AWK, sekarang saya bisa menulis skrip Bash lain untuk diproses pada kromosom sekaligus dan memberikan data yang dipartisi lebih dalam ke S3. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  Script memiliki dua bagian <code>parallel</code> . <br><br>  Bagian pertama membaca data dari semua file yang berisi informasi tentang kromosom yang diinginkan, kemudian data ini didistribusikan di seluruh aliran yang menyebarkan file ke grup yang sesuai (bin).  Untuk mencegah kondisi balapan terjadi ketika beberapa aliran ditulis ke satu file, AWK mentransfer nama file untuk menulis data ke tempat yang berbeda, misalnya, <code>chr_10_bin_52_batch_2_aa.csv</code> .  Akibatnya, banyak file kecil dibuat pada disk (untuk ini saya menggunakan volume EBS terabyte). <br><br>  Pipa dari bagian <code>parallel</code> kedua melewati kelompok (bin) dan menggabungkan file masing-masing ke CSV umum dengan <code>cat</code> , dan kemudian mengirimkannya untuk diekspor. <br><br><h2>  Disiarkan ke R? </h2><br>  <b>Apa yang saya pelajari</b> : Anda dapat mengakses <code>stdin</code> dan <code>stdout</code> dari skrip R, dan karenanya menggunakannya dalam pipeline. <br><br>  Dalam skrip Bash, Anda mungkin memperhatikan baris ini: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  Ini menerjemahkan semua file grup gabungan (nampan) ke dalam skrip R di bawah ini.  <code>{}</code> adalah teknik <code>parallel</code> khusus yang memasukkan data apa pun yang dikirim olehnya ke aliran yang ditentukan langsung ke perintah itu sendiri.  Opsi <code>{#}</code> memberikan ID utas unik, dan <code>{%}</code> mewakili nomor slot pekerjaan (diulangi, tetapi tidak pernah secara bersamaan).  Daftar semua opsi dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  Ketika variabel <code>file("stdin")</code> diteruskan ke <code>readr::read_csv</code> , data yang diterjemahkan ke dalam skrip-R dimuat ke dalam bingkai, yang kemudian ditulis langsung ke S3 sebagai file <code>aws.s3</code> menggunakan <code>aws.s3</code> . <br><br>  RDS sedikit seperti versi Parquet yang lebih muda, tanpa embel-embel penyimpanan kolom. <br><br>  Setelah menyelesaikan skrip Bash, saya menerima <code>.rds</code> file <code>.rds</code> terletak pada S3, yang memungkinkan saya untuk menggunakan kompresi yang efisien dan tipe <code>.rds</code> . <br><br>  Meski menggunakan rem R, semuanya bekerja sangat cepat.  Tidak mengherankan bahwa fragmen pada R yang bertanggung jawab untuk membaca dan menulis data dioptimalkan dengan baik.  Setelah menguji pada satu kromosom berukuran sedang, tugas diselesaikan pada contoh C5n.4xl dalam waktu sekitar dua jam. <br><br><h2>  Batasan S3 </h2><br>  <b>Apa yang saya pelajari</b> : berkat implementasi jalur yang cerdas, S3 dapat memproses banyak file. <br><br>  Saya khawatir jika S3 bisa menangani banyak file yang ditransfer ke sana.  Saya bisa membuat nama file bermakna, tetapi bagaimana S3 akan mencari mereka? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   —    . <br><br>          Amazon, ,    «-----»  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     —     . <br><br>       : «    ?»      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # … with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 × 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> …   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  Hasil </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     … <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Kesimpulan </h2><br>   —   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  «»  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> —   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     —     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id456392/">https://habr.com/ru/post/id456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id456380/index.html">Open Day dari Fakultas Pemrograman Netologi</a></li>
<li><a href="../id456382/index.html">Kolaborasi dan otomatisasi di frontend. Apa yang kami pelajari dari 13 sekolah</a></li>
<li><a href="../id456384/index.html">Grafik PVS-Studio dari Pengembangan Kemampuan Diagnostik</a></li>
<li><a href="../id456386/index.html">Buka perpustakaan untuk visualisasi konten audio</a></li>
<li><a href="../id456388/index.html">Bagan pengembangan diagnostik dalam PVS-Studio</a></li>
<li><a href="../id456394/index.html">Membuat Layar Splash di mana-mana di iOS</a></li>
<li><a href="../id456398/index.html">Plugin Vue-cli, bekerja dengan data kompleks dan pengujian berbasis properti - Pengumuman Panda-Meetup Frontend</a></li>
<li><a href="../id456402/index.html">Gigi Kebijaksanaan: Tarik-Tarik</a></li>
<li><a href="../id456404/index.html">Looper - Plugin untuk Sketch</a></li>
<li><a href="../id456406/index.html">Migrasikan kotak surat di antara kubah di Zimbra Collaboration Suite</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>