<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì≤ üèáüèæ üë®‚Äçüíª Probl√®mes d'analyse du Big Data üëÄ üòΩ üèõÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quels sont les d√©fis de l'analyse Big Data 
 Le Big Data cr√©e des fonctionnalit√©s qui ne sont pas partag√©es par les ensembles de donn√©es traditionnels...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Probl√®mes d'analyse du Big Data</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456088/"><h3>  Quels sont les d√©fis de l'analyse Big Data </h3><br>  Le Big Data cr√©e des fonctionnalit√©s qui ne sont pas partag√©es par les ensembles de donn√©es traditionnels.  Ces caract√©ristiques cr√©ent des probl√®mes importants pour l'analyse des donn√©es et motivent le d√©veloppement de nouvelles m√©thodes statistiques.  Contrairement aux ensembles de donn√©es traditionnels, o√π la taille de l'√©chantillon est g√©n√©ralement plus grande que la mesure, le Big Data se caract√©rise par une taille d'√©chantillon √©norme et une dimension √©lev√©e.  Tout d'abord, nous discuterons de l'effet des grandes tailles d'√©chantillon sur la compr√©hension de l'h√©t√©rog√©n√©it√©: d'une part, les grandes tailles d'√©chantillon nous permettent de d√©couvrir des mod√®les cach√©s associ√©s √† de petits sous-groupes de la population et une mauvaise g√©n√©ralit√© dans l'ensemble de la population.  D'autre part, la mod√©lisation de l'h√©t√©rog√©n√©it√© interne du Big Data n√©cessite des m√©thodes statistiques plus sophistiqu√©es.  Deuxi√®mement, nous discuterons de plusieurs ph√©nom√®nes uniques associ√©s √† une dimensionnalit√© √©lev√©e, notamment l'accumulation de bruit, la fausse corr√©lation et l'endog√©n√©it√© al√©atoire.  Ces caract√©ristiques uniques invalident les proc√©dures statistiques traditionnelles. <br><a name="habracut"></a><br><h3>  H√©t√©rog√©n√©it√© </h3><br>  Le Big Data est souvent cr√©√© en combinant plusieurs sources de donn√©es correspondant √† diff√©rents sous-groupes.  Chaque sous-groupe peut pr√©senter des fonctionnalit√©s uniques qui ne sont pas partag√©es par d'autres.  Dans des conditions classiques, lorsque la taille de l'√©chantillon est petite ou mod√©r√©e, les points de donn√©es de petites sous-populations sont g√©n√©ralement class√©s comme des ¬´√©carts¬ª, et il est syst√©matiquement difficile de mod√©liser en raison du nombre insuffisant d'observations.  Cependant, √† l'√®re du Big Data, la grande taille de l'√©chantillon nous permet de mieux comprendre l'h√©t√©rog√©n√©it√© en mettant en lumi√®re des √©tudes telles que l'√©tude de la relation entre certaines covariables (par exemple les g√®nes ou les SNP) et les r√©sultats rares (par exemple les maladies rares ou les maladies dans de petites populations) et la compr√©hension pourquoi certains traitements (comme la chimioth√©rapie) profitent √† une population et nuisent √† une autre.  Pour mieux illustrer ce point, nous introduisons le mod√®le suivant pour la population: <br><br><p><math> </math> $$ affiche $$ Œª1p1 (y; Œ∏1 (x)) + ‚ãØ + Œªmpm (y; Œ∏m (x)), Œª1p1 (y; Œ∏1 (x)) + ‚ãØ + Œªmpm (y; Œ∏m (x)), ( 1) $$ afficher $$ </p><br>  O√π Œªj ‚â• 0 repr√©sente la fraction du ji√®me sous-groupe, pj (y; Œ∏j (x)) est la distribution de probabilit√© de la r√©ponse du ji√®me sous-groupe, √©tant donn√© les covariables de x avec Œ∏j (x) comme vecteur de param√®tres.  En pratique, de nombreuses sous-populations sont rarement observ√©es, c'est-√†-dire que Œªj est tr√®s petit.  Lorsque la taille de l'√©chantillon n est mod√©r√©e, nŒªj peut √™tre petit, ce qui rend impossible de d√©river des param√®tres d√©pendants des covariables Œ∏j (x) en raison d'un manque d'informations.  Cependant, √©tant donn√© que le Big Data a une grande taille d'√©chantillon n, la taille d'√©chantillon nŒªj pour le j√®me groupe de population peut √™tre mod√©r√©ment grande, m√™me si Œªj est tr√®s petit.  Cela nous permet de tirer une conclusion plus pr√©cise sur les param√®tres de la sous-population Œ∏j (¬∑).  En bref, le principal avantage du Big Data est la compr√©hension de l'h√©t√©rog√©n√©it√© des sous-populations, comme les avantages de certains traitements personnalis√©s qui ne sont pas possibles avec un √©chantillon de petite ou moyenne taille. <br><br>  Le Big Data nous permet √©galement, en raison de la grande taille des √©chantillons, d'identifier une communaut√© faible parmi l'ensemble de la population.  Par exemple, √©valuer le b√©n√©fice cardiaque d'un verre de vin rouge par jour peut √™tre difficile sans un √©chantillon de grande taille.  De m√™me, les risques pour la sant√© associ√©s √† l'exposition √† certains facteurs environnementaux ne peuvent √™tre √©valu√©s de mani√®re plus convaincante que lorsque la taille des √©chantillons est suffisamment grande. <br><br>  En plus des avantages ci-dessus, l'h√©t√©rog√©n√©it√© du Big Data pose √©galement des d√©fis importants pour l'inf√©rence statistique.  La d√©rivation du mod√®le de m√©lange en (1) pour les grands ensembles de donn√©es n√©cessite des m√©thodes statistiques et informatiques complexes.  Dans les petites mesures, des m√©thodes standard telles que l'algorithme de maximisation de l'attente pour les mod√®les de m√©lange final peuvent √™tre utilis√©es.  √Ä grande √©chelle, cependant, nous devons soigneusement rationaliser la proc√©dure d'√©valuation pour √©viter le sur-ajustement ou l'accumulation de bruit et pour d√©velopper de bons algorithmes de calcul. <br><br><h3>  Accumulation de bruit </h3><br>  L'analyse des m√©gadonn√©es nous oblige √† √©valuer et √† v√©rifier de nombreux param√®tres en m√™me temps.  Les erreurs d'estimation s'accumulent lorsque la r√®gle de d√©cision ou de pr√©diction d√©pend d'un grand nombre de ces param√®tres.  Cet effet d'accumulation de bruit est particuli√®rement grave dans les grandes dimensions et peut m√™me dominer les vrais signaux.  Ceci est g√©n√©ralement g√©r√© par l'hypoth√®se de raret√©. <br><br>  Prenons, par exemple, une classification multidimensionnelle.  Une mauvaise classification est due √† la pr√©sence de nombreuses faiblesses qui ne contribuent pas √† la r√©duction des erreurs de classification.  Par exemple, consid√©rons le probl√®me de classification lorsque les donn√©es proviennent de deux classes: <br><br><p><math> </math> $$ affiche $$ X1 et Y1, ........ Xn‚àºNd (Œº1, Id), Yn‚àºNd (Œº2, Id) .X1, ..., Xn‚àºNd (Œº1, Id) et Y1, ..., Yn‚àº Nd (Œº2, Id).  (2) $$ afficher $$ </p><br>  Nous voulons construire une r√®gle de classification qui classe une nouvelle observation Z‚ààRdZ‚ààRd dans la premi√®re ou la deuxi√®me classe.  Pour illustrer l'effet de l'accumulation de bruit dans la classification, nous fixons n = 100 et d = 1000. Nous d√©finissons Œº1 = 0Œº1 = 0 et Œº2 comme clairsem√©s, c'est-√†-dire  seuls les 10 premiers enregistrements de Œº2 sont non nuls avec une valeur de 3, et tous les autres enregistrements sont nuls.  La figure 1 montre les deux premiers composants principaux en utilisant les premiers m = 2, 40, 200 √©l√©ments et jusqu'√† 1000 √©l√©ments.  Comme le montrent ces graphiques, lorsque m = 2, nous obtenons un degr√© √©lev√© de discrimination.  Cependant, le pouvoir discriminant devient tr√®s faible lorsque m est trop grand en raison de l'accumulation de bruit.  Les 10 premi√®res fonctions contribuent au classement, les autres non.  Par cons√©quent, lorsque m&gt; 10, les proc√©dures ne re√ßoivent pas de signaux suppl√©mentaires, mais accumulent du bruit: plus m, plus de bruit s'accumule, ce qui aggrave la proc√©dure de classification en raison de la dimensionnalit√©.  √Ä m = 40, les signaux accumul√©s compensent le bruit accumul√©, de sorte que les deux premiers composants principaux ont toujours une bonne capacit√© de reconnaissance.  Lorsque m = 200, le bruit accumul√© d√©passe le gain du signal. <br><br>  La discussion ci-dessus motive l'utilisation de mod√®les clairsem√©s et le choix de variables pour surmonter l'effet de l'accumulation de bruit.  Par exemple, dans le mod√®le de classification (2), au lieu d'utiliser toutes les fonctions, nous pourrions choisir un sous-ensemble des fonctionnalit√©s qui offrent le meilleur rapport signal / bruit.  Un tel mod√®le clairsem√© offre une efficacit√© de classification plus √©lev√©e.  En d'autres termes, le choix des variables joue un r√¥le cl√© pour surmonter l'accumulation de bruit dans la classification et la pr√©vision de la r√©gression.  Cependant, la s√©lection de variables de grandes dimensions est difficile en raison de la fausse corr√©lation, de l'endog√©n√©it√© al√©atoire, de l'h√©t√©rog√©n√©it√© et des erreurs de mesure. <br><br><h3>  Fausse corr√©lation </h3><br>  Une dimensionnalit√© √©lev√©e contient √©galement une fausse corr√©lation, citant le fait que de nombreuses variables al√©atoires non corr√©l√©es peuvent avoir des corr√©lations d'√©chantillonnage √©lev√©es dans de grandes dimensions.  Une fausse corr√©lation peut conduire √† des d√©couvertes scientifiques erron√©es et √† des conclusions statistiques incorrectes. <br><br>  Consid√©rons le probl√®me d'estimation du vecteur coefficient Œ≤ d'un mod√®le lin√©aire <br><br><p><math> </math> $$ afficher $$ y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, (3) $$ afficher $$ </p><br>  o√π y‚ààRny‚ààRn repr√©sente le vecteur de r√©ponse, X = [x1, ..., xn] T‚ààRn √ó dX = [x1, ..., xn] T‚ààRn √ó d repr√©sente la matrice de projection, ‚ààRnœµ‚ààRn repr√©sente le vecteur al√©atoire ind√©pendant le bruit et Id est la matrice d'identit√© d √ó d.  Pour faire face au probl√®me de l'accumulation de bruit, lorsque la taille d est comparable ou sup√©rieure √† la taille de l'√©chantillon n, on suppose que la r√©ponse ne donne qu'un petit nombre de variables, c'est-√†-dire que Œ≤ est un vecteur clairsem√©.  Conform√©ment √† cette hypoth√®se de raret√©, une variable peut √™tre s√©lectionn√©e pour √©viter l'accumulation de bruit, am√©liorer les performances de pr√©diction et am√©liorer l'interpr√©tabilit√© d'un mod√®le avec une repr√©sentation conservatrice. <br><br>  Pour les grandes tailles, m√™me pour un mod√®le aussi simple que (3), le choix des variables est difficile en raison de la pr√©sence d'une fausse corr√©lation.  En particulier, avec une dimensionnalit√© √©lev√©e, des variables importantes peuvent √™tre fortement corr√©l√©es avec plusieurs fausses variables qui ne sont pas scientifiquement li√©es.  Prenons un exemple simple illustrant ce ph√©nom√®ne.  Soit x1, ..., xn des observations ind√©pendantes d'un vecteur al√©atoire gaussien de dimension d X = (X1, ..., Xd) T‚àºNd (0, Id) X = (X1, ..., Xd) T‚àºNd (0, Id) ‚Å† .  Nous simulons √† plusieurs reprises les donn√©es avec n = 60 et d = 800 et 6400 1000 fois.  La figure 2a montre la distribution empirique du coefficient de corr√©lation absolu maximum de l'√©chantillon entre la premi√®re variable et les autres sont d√©finis comme <br><br><p><math> </math> $$ afficher $$ rÀÜ = maxj‚â•2 | CorrÀÜ (X1, Xj) |, r ^ = maxj‚â•2 | Corr ^ (X1, Xj) |, (4) $$ afficher $$ </p><br>  o√π Corr ^ (X1, Xj) Corr ^ (X1, Xj) est l'√©chantillon de corr√©lation entre les variables X1 et Xj.  Nous voyons que la corr√©lation absolue maximale de l'√©chantillon augmente avec l'augmentation de la dimension. <br><br>  De plus, nous pouvons calculer la corr√©lation multiple absolue maximale entre X1 et les combinaisons lin√©aires de plusieurs variables lat√©rales non pertinentes: <br><br><p><math> </math> $$ afficher $$ RÀÜ = max | S | = 4max {Œ≤j} 4j = 1‚à£‚à£‚à£‚à£CorrÀÜ (X1, ‚àëj‚ààSŒ≤jXj) ‚à£‚à£‚à£‚à£.R ^ = max | S | = 4max {Œ≤j} j = 14 | Corr ^ (X1, ‚àëj‚ààSŒ≤jXj) |.  (5) $$ afficher $$ </p><br>  En utilisant la configuration standard, la distribution empirique du coefficient absolu maximal de corr√©lation d'√©chantillon entre X1 et ‚àëj ‚àà SŒ≤jXj est donn√©e, o√π S est n'importe quel sous-ensemble de la quatri√®me taille de {2, ..., d} et Œ≤j est le coefficient de r√©gression des moindres carr√©s Xj lorsque X1 r√©gresse sur {Xj} j ‚àà S. Encore une fois, nous voyons que bien que X1 soit compl√®tement ind√©pendant de X2, ..., Xd, la corr√©lation entre X1 et la combinaison lin√©aire la plus proche de quatre variables quelconques de {Xj} j ‚â† 1 √† X1 peut √™tre tr√®s √©lev√©e. <br><br>  Une fausse corr√©lation a un effet significatif sur le choix des variables et peut conduire √† des d√©couvertes scientifiques erron√©es.  Soit XS = (Xj) j ‚àà S un vecteur al√©atoire index√© par S, et soit SÀÜS ^ l'ensemble s√©lectionn√© qui a une corr√©lation parasite plus √©lev√©e avec X1, comme dans la Fig.  2. Par exemple, lorsque n = 60 et d = 6400, nous voyons que X1 est pratiquement indiscernable de XSXS ^ pour l'ensemble SS ^  |  SÀÜ |  = 4 |  S ^ |  = 4‚Å†.  Si X1 repr√©sente le niveau d'expression du g√®ne responsable de la maladie, on ne peut pas le distinguer des quatre autres g√®nes de SS ^, qui ont un pouvoir pronostique similaire, bien que, d'un point de vue scientifique, cela n'a pas d'importance. <br><br>  Outre le choix des variables, une fausse corr√©lation peut √©galement conduire √† une conclusion statistique incorrecte.  Nous expliquons cela en consid√©rant √† nouveau le m√™me mod√®le lin√©aire que dans (3).  Ici, nous aimerions √©valuer l'erreur standard œÉ du reste, qui se manifeste de mani√®re notable dans les conclusions statistiques des coefficients de r√©gression, la s√©lection du mod√®le, le test de conformit√© et la r√©gression marginale.  Soit SÀÜS ^ l'ensemble des variables s√©lectionn√©es, et PSÀÜPS ^ la matrice de projection sur l'espace de colonne XSÀÜXS ^ ‚Å†.  Estimation standard de la variance r√©siduelle bas√©e sur des variables s√©lectionn√©es: <br><br><p><math> </math> $$ affiche $$ œÉÀÜ2 = yT (In - PSÀÜ) yn‚àí | SÀÜ | .œÉ ^ 2 = yT (In - PS ^) yn‚àí | S ^ |.  (6) $$ afficher $$ </p><br>  L'√©valuateur (6) est impartial lorsque les variables ne sont pas s√©lectionn√©es √† partir des donn√©es et que le mod√®le est correct.  Cependant, la situation est compl√®tement diff√©rente lorsque les variables sont s√©lectionn√©es en fonction des donn√©es.  En particulier, les auteurs ont montr√© que lorsqu'il y a beaucoup de fausses variables, œÉ2 est s√©rieusement sous-estim√©, cela conduit √† des conclusions statistiques erron√©es, y compris le choix de mod√®les ou de tests de signification, et des d√©couvertes scientifiques erron√©es, telles que la recherche de mauvais g√®nes pour les m√©canismes mol√©culaires.  Ils offrent √©galement une m√©thode avanc√©e de validation crois√©e pour att√©nuer le probl√®me. <br><br><h3>  Endog√©n√©it√© al√©atoire </h3><br>  L'endog√©n√©it√© al√©atoire est un autre probl√®me subtil d√©coulant de la haute dimensionnalit√©.  Dans le cadre de r√©gression, Y = ‚àëdj = 1Œ≤jXj + ŒµY = ‚àëj = 1dŒ≤jXj + Œµ‚Å†, le terme ¬´endog√©n√©it√©¬ª signifie que certains pr√©dicteurs {Xj} sont en corr√©lation avec le bruit r√©siduel Œµ.  Le mod√®le clairsem√© habituel suppose <br><br><p><math> </math> $$ affiche $$ Y = ‚àëjŒ≤jXj + Œµ, et E (ŒµXj) = 0 pour j = 1, ..., d, Y = ‚àëjŒ≤jXJ + Œµ, et E (ŒµXj) = 0 pour j = 1, ..., d , (7) $$ afficher $$ </p><br>  avec un petit ensemble S = {j: Œ≤j ‚â† 0}.  L'hypoth√®se exog√®ne (7) selon laquelle le bruit r√©siduel Œµ n'est pas corr√©l√© avec tous les pr√©dicteurs est cruciale pour la fiabilit√© de la plupart des m√©thodes statistiques existantes, y compris la coh√©rence dans le choix des variables.  Bien que cette hypoth√®se semble innocente, il est facile de la violer dans les grandes dimensions, car certaines variables {Xj} sont en corr√©lation al√©atoire avec Œµ, ce qui rend la plupart des proc√©dures multidimensionnelles statistiquement invalides. <br><br>  Pour expliquer le probl√®me d'endog√©n√©it√© plus en d√©tail, supposons que la r√©ponse inconnue Y soit associ√©e aux trois covariables comme suit: <br><br><p><math> </math> $$ affiche $$ Y = X1 + X2 + X3 + Œµ, avec EŒµXj = 0, pour j = 1, 2, 3.Y = X1 + X2 + X3 + Œµ, avecEŒµXj = 0, pour j = 1, 2, 3 . $$ afficher $$ </p><br>  Au stade de la collecte des donn√©es, nous ne connaissons pas le vrai mod√®le et donc nous collectons autant de covariables que potentiellement associ√©es √† Y dans l'espoir d'inclure tous les termes de S dans (7).  Soit dit en passant, certains de ces Xj (pour jj 1, 2, 3) peuvent √™tre associ√©s √† du bruit r√©siduel Œµ.  Cela r√©fute l'hypoth√®se d'une mod√©lisation exog√®ne dans (7).  En fait, plus il y a de covariables collect√©es ou mesur√©es, plus cette hypoth√®se est complexe. <br><br>  Contrairement √† la fausse corr√©lation, l'endog√©n√©it√© al√©atoire fait r√©f√©rence √† l'existence r√©elle de corr√©lations entre des variables involontaires.  La premi√®re est similaire au fait que deux personnes sont similaires, mais n'ont pas de lien g√©n√©tique, et la seconde est comme une connaissance qui se d√©roule facilement dans une grande ville.  Dans un sens plus g√©n√©ral, l'endog√©n√©it√© r√©sulte d'un biais de choix, d'erreurs de mesure et de variables manquantes.  Ces ph√©nom√®nes surviennent souvent lors de l'analyse du Big Data, principalement pour deux raisons: <br><br><ul><li>  Gr√¢ce aux nouvelles m√©thodes de mesure hautes performances, les scientifiques peuvent collecter autant de fonctions que possible et s'efforcer de le faire.  En cons√©quence, cela augmente la probabilit√© que certains d'entre eux soient corr√©l√©s avec du bruit r√©siduel. </li><li>  Le Big Data est g√©n√©ralement combin√© √† partir de plusieurs sources avec des sch√©mas de g√©n√©ration de donn√©es potentiellement diff√©rents.  Cela augmente la probabilit√© de biais dans les erreurs de s√©lection et de mesure, ce qui entra√Æne √©galement une endog√©n√©it√© al√©atoire potentielle. </li></ul><br>  L'endog√©n√©it√© al√©atoire appara√Æt-elle dans des ensembles de donn√©es r√©els et comment pouvons-nous tester cela dans la pratique?  Nous envisageons une √©tude de g√©nomique dans laquelle 148 √©chantillons de micror√©seaux sont t√©l√©charg√©s √† partir des bases de donn√©es GEO et ArrayExpress.  Ces √©chantillons ont √©t√© cr√©√©s sur la plateforme Affymetrix HGU133a pour les personnes atteintes d'un cancer de la prostate.  L'ensemble de donn√©es obtenu contient 22 283 sondes, ce qui correspond √† 12 719 g√®nes.  Dans cet exemple, nous nous int√©ressons √† un g√®ne appel√© ¬´membre de la famille des r√©cepteurs du domaine disco√Ødine 1¬ª (en abr√©g√© DDR1).  La DDR1 code pour les r√©cepteurs tyrosine kinases, qui jouent un r√¥le important dans la connexion des cellules avec leur microenvironnement.  Il est connu que DDR1 est √©troitement li√© au cancer de la prostate, et nous voulons √©tudier sa relation avec d'autres g√®nes chez les patients atteints de cancer.  Nous avons pris l'expression du g√®ne DDR1 comme variable de r√©ponse Y, et l'expression de tous les 12 718 g√®nes restants comme pr√©dicteurs.  Dans le volet gauche, fig.  La figure 3 montre la distribution empirique des corr√©lations entre la r√©ponse et les pr√©dicteurs individuels. <br><br>  Pour illustrer l'existence de l'endog√©n√©it√©, nous ajustons la r√©gression des moindres carr√©s L1 (Lasso) aux donn√©es, et la p√©nalit√© est automatiquement s√©lectionn√©e √† l'aide d'une validation crois√©e 10 fois (37 g√®nes s√©lectionn√©s).  Ensuite, nous r√©tablirons la r√©gression des moindres carr√©s habituelle pour le mod√®le s√©lectionn√© pour calculer le vecteur r√©siduel.  Dans le volet droit, fig.  3, nous construisons une distribution empirique des corr√©lations entre les pr√©dicteurs et les r√©sidus.  Nous voyons que le bruit r√©siduel est fortement corr√©l√© avec de nombreux pr√©dicteurs.  Pour nous assurer que ces corr√©lations ne sont pas provoqu√©es par une corr√©lation purement fausse, nous introduisons une ¬´distribution nulle¬ª de fausses corr√©lations en r√©arrangeant au hasard les ordres des lignes dans la matrice du projet, de sorte que les pr√©dicteurs sont vraiment ind√©pendants du bruit r√©siduel.  En comparant ces deux distributions, nous voyons que la distribution des corr√©lations entre les pr√©dicteurs et le bruit r√©siduel dans les donn√©es brutes (marqu√©es comme ¬´donn√©es brutes¬ª) a une queue plus lourde que dans les donn√©es r√©arrang√©es (marqu√©es comme ¬´donn√©es r√©arrang√©es¬ª).  Ce r√©sultat fournit des preuves solides de l'endog√©n√©it√©. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456088/">https://habr.com/ru/post/fr456088/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456076/index.html">Meetup PyDaCon au groupe Mail.ru: 22 juin</a></li>
<li><a href="../fr456078/index.html">Projection de conflit d'entreprise sur la connectivit√© r√©seau</a></li>
<li><a href="../fr456082/index.html">Comment nous d√©veloppons des recommandations de produits personnalis√©es</a></li>
<li><a href="../fr456084/index.html">Kubernetes 1.15: Aper√ßu des faits saillants</a></li>
<li><a href="../fr456086/index.html">Storyboards iOS: analyse des avantages et des inconv√©nients, meilleures pratiques</a></li>
<li><a href="../fr456090/index.html">Introduction aux tests unitaires dans Unity</a></li>
<li><a href="../fr456092/index.html">Sept signes troublants que vous d√©pendez des conditions m√©t√©orologiques, m√™me si vous ne le pensez pas</a></li>
<li><a href="../fr456094/index.html">Nous lisons les fiches techniques 2: SPI sur STM32; Temporisateurs et interruptions PWM sur le STM8</a></li>
<li><a href="../fr456096/index.html">Ce que fait le lecteur de geektimes moyen en planant dans les nuages</a></li>
<li><a href="../fr456100/index.html">Maintenant dans le nouvel emballage - Kingston A400 au format M.2 se pr√©cipite sur le march√©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>