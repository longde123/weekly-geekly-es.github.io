<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ÜñÔ∏è üë™ üîê Redes neuronales insensibles al peso (WANN) üë©üèΩ üö∂üèª ‚§¥Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El nuevo trabajo de Google ofrece una arquitectura de redes neuronales que puede simular los instintos y reflejos innatos de los seres vivos, seguido ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales insensibles al peso (WANN)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465369/"><p><img src="https://habrastorage.org/getpro/habr/post_images/ebf/c30/eca/ebfc30ecaa458eca2fe85d3c43956b47.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El</a> nuevo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo de Google</a> ofrece una arquitectura de redes neuronales que puede simular los instintos y reflejos innatos de los seres vivos, seguido de m√°s capacitaci√≥n durante toda la vida. </p><br><p>  Y tambi√©n reduce significativamente el n√∫mero de conexiones dentro de la red, aumentando as√≠ su velocidad. </p><a name="habracut"></a><br><p>  Las redes neuronales artificiales, aunque son similares en principio a las biol√≥gicas, a√∫n son muy diferentes de ellas para ser utilizadas en su forma pura para crear una IA fuerte.  Por ejemplo, ahora es imposible crear un modelo de una persona en un simulador (o un rat√≥n, o incluso un insecto), darle un "cerebro" en forma de una red neuronal moderna y entrenarlo.  Simplemente no funciona. </p><br><p>  Incluso descartando las diferencias en el mecanismo de aprendizaje (en el cerebro no existe un an√°logo exacto del algoritmo de propagaci√≥n de error de retroceso, por ejemplo) y la falta de correlaciones temporales de diferente escala, sobre la base de las cuales el cerebro biol√≥gico construye su trabajo, las redes neuronales artificiales tienen varios problemas m√°s que no les permiten simular suficientemente cerebro vivo  Probablemente debido a estos problemas inherentes al aparato matem√°tico utilizado ahora, el Aprendizaje de refuerzo, dise√±ado para imitar el entrenamiento de las criaturas vivientes sobre la base de la recompensa, no funciona tan bien como nos gustar√≠a en la pr√°ctica.  Aunque se basa en ideas realmente buenas y correctas.  Los desarrolladores mismos bromean que el cerebro es RNN + A3C (es decir, un algoritmo recurrente de red + actor-cr√≠tico para su entrenamiento). </p><br><p>  Una de las diferencias m√°s notables entre el cerebro biol√≥gico y las redes neuronales artificiales es que la estructura del cerebro vivo est√° preconfigurada por millones de a√±os de evoluci√≥n.  Aunque la neocorteza, que es responsable de la mayor actividad nerviosa en los mam√≠feros, tiene una estructura aproximadamente uniforme, la estructura general del cerebro est√° claramente definida por los genes.  Adem√°s, los animales que no sean mam√≠feros (aves, peces) no tienen una neocorteza en absoluto, pero al mismo tiempo exhiben un comportamiento complejo que las redes neuronales modernas no pueden lograr.  Una persona tambi√©n tiene limitaciones f√≠sicas en la estructura del cerebro, que son dif√≠ciles de explicar.  Por ejemplo, la resoluci√≥n de un ojo es de aproximadamente 100 megap√≠xeles (~ 100 millones de bastones y conos fotosensibles), lo que significa que desde dos ojos el flujo de video debe ser de aproximadamente 200 megap√≠xeles con una frecuencia de al menos 15 fotogramas por segundo.  Pero en realidad, el nervio √≥ptico puede pasar a trav√©s de s√≠ mismo no m√°s de 2-3 megap√≠xeles.  Y sus conexiones se dirigen no a la parte m√°s cercana del cerebro, sino a la parte occipital a la corteza visual. </p><br><p>  Por lo tanto, sin restarle importancia a la importancia de la neocorteza (en t√©rminos generales, puede considerarse al nacer como un an√°logo de redes neuronales modernas iniciadas al azar), los hechos sugieren que incluso en los humanos una estructura cerebral predefinida juega un papel muy importante.  Por ejemplo, si un beb√© tiene solo unos minutos para mostrar su lengua, entonces, gracias a las neuronas espejo, tambi√©n sacar√° la lengua.  Lo mismo sucede con la risa de los ni√±os.  Es bien sabido que los beb√©s desde el nacimiento han sido "cosidos" con un excelente reconocimiento de los rostros humanos.  Pero lo m√°s importante, el sistema nervioso de todos los seres vivos est√° optimizado para sus condiciones de vida.  El beb√© no llorar√° durante horas si tiene hambre.  Se cansar√°  O miedo a algo y c√°llate.  El zorro no alcanzar√° el agotamiento hasta que el hambre alcance las uvas inaccesibles.  Har√° varios intentos, decidir√° que √©l est√° amargado (s) y se ir√°.  Y este no es un proceso de aprendizaje, sino un comportamiento predefinido por la biolog√≠a.  Adem√°s, las diferentes especies tienen diferentes.  Algunos depredadores se apresuran inmediatamente por la presa, mientras que otros se quedan en una emboscada durante mucho tiempo.  Y aprendieron esto no a trav√©s de prueba y error, sino que tal es su biolog√≠a, dada por instintos.  Del mismo modo, muchos animales tienen programas de evitaci√≥n de depredadores por cable desde los primeros minutos de vida, aunque f√≠sicamente a√∫n no pudieron aprenderlos. </p><br><p>  Te√≥ricamente, los m√©todos modernos de entrenamiento de redes neuronales son capaces de crear una imagen de un cerebro tan pre-entrenado desde una red totalmente conectada, poniendo a cero conexiones innecesarias (de hecho, cort√°ndolas) y dejando solo las necesarias.  Pero esto requiere una gran cantidad de ejemplos, no se sabe c√≥mo entrenarlos y, lo que es m√°s importante, por el momento no hay buenas maneras de arreglar esta estructura "inicial" del cerebro.  El entrenamiento posterior cambia estos pesos y todo sale mal. </p><br><p>  Los investigadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de Google</a> tambi√©n hicieron esta pregunta.  ¬øEs posible crear una estructura cerebral inicial similar a la biol√≥gica, es decir, que ya est√© bien optimizada para resolver el problema y luego volver a entrenarla?  Te√≥ricamente, esto reducir√° dr√°sticamente el espacio de soluciones y le permitir√° entrenar r√°pidamente redes neuronales. </p><br><p>  Desafortunadamente, los algoritmos de optimizaci√≥n de estructura de red existentes, como la B√∫squeda de Arquitectura Neural (NAS), operan en bloques enteros.  Despu√©s de agregar o eliminar cu√°les, la red neuronal debe ser entrenada nuevamente desde cero.  Este es un proceso que requiere muchos recursos y no resuelve completamente el problema. </p><br><p>  Por lo tanto, los investigadores propusieron una versi√≥n simplificada, llamada "Redes neuronales agn√≥sticas de peso" (WANN).  La idea es reemplazar todos los pesos de una red neuronal con un peso "com√∫n".  Y en el proceso de aprendizaje, no se trata de seleccionar pesos entre las neuronas, como en las redes neuronales comunes, sino de seleccionar la estructura de la red en s√≠ (el n√∫mero y la ubicaci√≥n de las neuronas), que con los mismos pesos muestra los mejores resultados.  Y despu√©s de eso, optim√≠celo para que la red funcione bien con todos los valores posibles de este peso total (¬°com√∫n para todas las conexiones entre neuronas!). </p><br><p>  Como resultado, esto da la estructura de una red neuronal, que no depende de pesos espec√≠ficos, pero funciona bien con todos.  Porque funciona debido a la estructura general de la red.  Esto es similar al cerebro de un animal que a√∫n no se ha inicializado con escalas espec√≠ficas al nacer, pero que ya contiene instintos incrustados debido a su estructura general.  Y el ajuste posterior de las escalas durante el entrenamiento durante toda la vida, hace que esta red neuronal sea a√∫n mejor. </p><br><p>  Un efecto secundario positivo de este enfoque es una disminuci√≥n significativa en el n√∫mero de neuronas en la red (ya que solo quedan las conexiones m√°s importantes), lo que aumenta su velocidad.  A continuaci√≥n se muestra una comparaci√≥n de la complejidad de una red neuronal completamente conectada cl√°sica (izquierda) y una nueva red emparejada (derecha). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ebf/c30/eca/ebfc30ecaa458eca2fe85d3c43956b47.png"></p><br><p>  Para buscar dicha arquitectura, los investigadores utilizaron el algoritmo de b√∫squeda de topolog√≠a (NEAT).  Primero, se crea un conjunto de redes neuronales simples, y luego se realiza una de tres acciones: se agrega una nueva neurona a la conexi√≥n existente entre dos neuronas, se agrega una nueva conexi√≥n con otras aleatorias a otra neurona o la funci√≥n de activaci√≥n en la neurona cambia (ver las figuras a continuaci√≥n).  Y luego, a diferencia del NAS cl√°sico, donde se buscan los pesos √≥ptimos entre las neuronas, aqu√≠ todos los pesos se inicializan con un solo n√∫mero.  Y la optimizaci√≥n se lleva a cabo para encontrar la estructura de red que funciona mejor en una amplia gama de valores de este peso total.  Por lo tanto, se obtiene una red que no depende del peso espec√≠fico entre las neuronas, pero que funciona bien en todo el rango (pero todos los pesos siguen siendo iniciados por un n√∫mero, y no son diferentes como en las redes normales).  Adem√°s, como un objetivo adicional para la optimizaci√≥n, intentan minimizar la cantidad de neuronas en la red. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/097/a11/a58/097a11a5854d96c62b9af8c862c5f9a6.png"></p><br><p>  A continuaci√≥n se muestra un esquema general del algoritmo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c24/3ab/418/c243ab4183b3ac197c612382e60b8226.png"></p><br><ol><li>  crea una poblaci√≥n de redes neuronales simples </li><li>  cada red inicializa todos sus pesos con un n√∫mero y para un amplio rango de n√∫meros: w = -2 ... + 2 </li><li>  Las redes resultantes se ordenan por la calidad de la soluci√≥n al problema y por el n√∫mero de neuronas (abajo) </li><li>  en la parte de los mejores representantes, se agrega una neurona, una conexi√≥n o la funci√≥n de activaci√≥n en una neurona cambia </li><li>  Estas redes modificadas se utilizan como iniciales en el punto 1) </li></ol><br><p>  Todo esto es bueno, pero se han propuesto cientos, si no miles de ideas diferentes para redes neuronales.  ¬øFunciona esto en la pr√°ctica?  Si lo hace  A continuaci√≥n se muestra un ejemplo del resultado de b√∫squeda de dicha arquitectura de red para el problema cl√°sico del carro de p√©ndulo.  Como se puede ver en la figura, la red neuronal funciona bien con todas las variantes del peso total (mejor con +1.0, pero tambi√©n trata de levantar el p√©ndulo de -1.5).  Y despu√©s de optimizar este peso √∫nico, comienza a funcionar perfectamente (opci√≥n de pesos ajustados en la figura). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c7/faa/ce2/1c7faace27a7ba5298a3d13ab2863f4d.gif"></p><br><p>  Por lo general, puede volver a entrenar como este peso total √∫nico, ya que la selecci√≥n de la arquitectura se realiza en un n√∫mero discreto limitado de par√°metros (en el ejemplo anterior -2, -1,1,2).  Y puede obtener un par√°metro √≥ptimo m√°s preciso, digamos, 1.5.  Y puede utilizar el mejor peso total como punto de partida para el reentrenamiento de todos los pesos, como en el entrenamiento cl√°sico de redes neuronales. </p><br><p>  Esto es similar a c√≥mo se entrenan los animales.  Teniendo instintos que son casi √≥ptimos al nacer, y usando esta estructura cerebral dada por los genes como la inicial, durante el curso de su vida, los animales entrenan su cerebro en condiciones externas espec√≠ficas.  M√°s detalles en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo reciente en la revista Nature</a> . </p><br><p>  A continuaci√≥n se muestra un ejemplo de una red encontrada por WANN para una tarea de control de m√°quina basada en p√≠xeles.  Tenga en cuenta que este es un paseo basado en los "instintos pelados", con el mismo peso total en todas las articulaciones, sin el ajuste cl√°sico de todos los pesos.  Al mismo tiempo, la red neuronal es extremadamente simple en estructura. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ee0/ef6/da2/ee0ef6da26d15ba297396b2b4443a8a8.png"><img src="https://habrastorage.org/getpro/habr/post_images/9be/018/565/9be01856536220bfb09556265efb118a.gif"></p><br><p>  Los investigadores sugieren crear conjuntos de redes WANN como otro caso de uso para WANN.  Por lo tanto, la red neuronal inicializada aleatoriamente habitual en MNIST muestra una precisi√≥n de aproximadamente el 10%.  Una √∫nica red neuronal WANN seleccionada produce aproximadamente el 80%, pero un conjunto de WANN con diferentes pesos totales muestra ya&gt; 90%. </p><br><p>  Como resultado, el m√©todo propuesto por los investigadores de Google para buscar la arquitectura inicial de una red neuronal √≥ptima no solo imita el aprendizaje animal (nacimiento con instintos √≥ptimos incorporados y reentrenamiento durante la vida), sino que tambi√©n evita la simulaci√≥n de la vida animal completa con el aprendizaje completo de toda la red en algoritmos evolutivos cl√°sicos, creando Redes simples y r√°pidas a la vez.  Lo cual es suficiente para entrenar un poco y obtener una red neuronal completamente √≥ptima. </p><br><h3 id="ssylki">  Referencias </h3><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Entrada de blog de Google AI</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Un art√≠culo interactivo en el que puede cambiar el peso total y controlar el resultado.</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo de Nature sobre la importancia de los instintos incrustados al nacer</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/465369/">https://habr.com/ru/post/465369/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../465359/index.html">Huawei CloudCampus: infraestructura de servicio de alta nube</a></li>
<li><a href="../465361/index.html">Encrucijada digital de Kaz√°n: c√≥mo se introducen las tecnolog√≠as de seguridad vial en la ciudad</a></li>
<li><a href="../465363/index.html">Natas Web. Paso de la plataforma CTF destinada a explotar vulnerabilidades web. Parte 5</a></li>
<li><a href="../465365/index.html">Script de configuraci√≥n de Windows 10</a></li>
<li><a href="../465367/index.html">Qui√©n no se escondi√≥: no tengo la culpa (historial de secreto en la aviaci√≥n)</a></li>
<li><a href="../465371/index.html">C√°lculo de la hip√≥tesis nula, por ejemplo, el an√°lisis de salarios de programadores ucranianos.</a></li>
<li><a href="../465373/index.html">No es una revisi√≥n del ASUS ZenBook Pro 15 UX580GE: casi un a√±o con casi la parte superior</a></li>
<li><a href="../465375/index.html">Venta de servidores dedicados en Holanda y Mosc√∫</a></li>
<li><a href="../465377/index.html">Hazlo tu mismo Skype</a></li>
<li><a href="../465379/index.html">Control de bomba de insulina aut√≥nomo inal√°mbrico hecho en casa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>