<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôæ üç° üõ´ Livro ‚ÄúSite Confiabilidade Engenharia. Confiabilidade e confiabilidade como no Google ¬ª üéì üëÉüèº ‚å®Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Por quase 20 anos, o Google fornece sistemas inimaginavelmente complexos e de grande escala, sens√≠veis √†s solicita√ß√µes dos usu√°rios. O mecanismo de pe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livro ‚ÄúSite Confiabilidade Engenharia. Confiabilidade e confiabilidade como no Google ¬ª</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/420139/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/dx/hl/8t/dxhl8tgecl0xwqtx3pnxztosrom.jpeg" align="left" alt="imagem"></a>  Por quase 20 anos, o Google fornece sistemas inimaginavelmente complexos e de grande escala, sens√≠veis √†s solicita√ß√µes dos usu√°rios.  O mecanismo de pesquisa do Google encontra a resposta para todas as perguntas em uma fra√ß√£o de segundo, o Google Maps com a mais alta precis√£o reflete o cen√°rio terrestre, e o Google Mail est√° dispon√≠vel no modo 365/24/7 e, em ess√™ncia, se tornou o primeiro armazenamento em nuvem p√∫blica.  Esses sistemas s√£o perfeitos?  N√£o, eles tamb√©m falham, quebram e se tornam obsoletos, como qualquer equipamento.  N√≥s simplesmente n√£o percebemos.  O fato √© que h√° mais de dez anos, o Google desenvolve a exclusiva tecnologia de Engenharia de confiabilidade do site, que garante opera√ß√£o ininterrupta e desenvolvimento progressivo de sistemas de software de qualquer complexidade.  Este livro √© um dep√≥sito de experi√™ncia acumulada pelo Google ao longo de muitos anos, o trabalho coletivo de muitos especialistas destacados e um recurso indispens√°vel para qualquer engenheiro que queira desenvolver e manter qualquer produto com a mais alta qualidade e com mais efici√™ncia. <br><a name="habracut"></a><br><h3>  SRE do Google em termos de SRE </h3><br>  Os data centers do Google (data centers) s√£o significativamente diferentes dos data centers tradicionais e de "farms" de pequenos servidores.  Essas diferen√ßas introduzem problemas adicionais e oportunidades adicionais.  Este cap√≠tulo discute os desafios e oportunidades espec√≠ficos dos data centers do Google e apresenta a terminologia que ser√° usada ao longo do livro. <br><br>  <b>Equipamento</b> <br><br>  A maioria dos recursos de computa√ß√£o do Google est√° localizada em data centers projetados pela empresa, que possuem sistema de fornecimento de energia, sistema de refrigera√ß√£o, rede interna e equipamentos de computa√ß√£o [Barroso et al., 2013].  Ao contr√°rio dos datacenters t√≠picos fornecidos pelos fornecedores aos seus clientes, todos os datacenters do Google est√£o equipados com o mesmo1.  Para evitar confus√£o entre o hardware e o software do servidor, neste livro, usamos a seguinte terminologia: <br><br><ul><li>  <i>m√°quina (computador)</i> - uma unidade de equipamento (ou, possivelmente, uma m√°quina virtual); </li><li>  <i>servidor</i> - uma unidade de software que implementa um servi√ßo. </li></ul><br>  Qualquer servidor pode ser iniciado em m√°quinas, portanto, n√£o alocamos computadores espec√≠ficos para programas de servidor espec√≠ficos.  Por exemplo, n√£o temos uma m√°quina espec√≠fica na qual o servidor de correio esteja em execu√ß√£o.  Em vez disso, os recursos s√£o alocados pelo nosso sistema de gerenciamento de cluster Borg. <br><br>  Entendemos que esse uso do termo "servidor" n√£o √© padr√£o.  √â mais comum designar dois conceitos ao mesmo tempo: um programa que serve conex√µes de rede e, ao mesmo tempo, uma m√°quina que executa esses programas, mas quando falamos sobre o poder computacional do Google, a diferen√ßa entre os dois √© significativa.  Assim que voc√™ se acostumar com a nossa interpreta√ß√£o da palavra "servidor", ficar√° mais claro para voc√™ por que √© importante usar essa terminologia especializada, n√£o apenas diretamente no Google, mas ao longo deste livro. <br><br>  Na fig.  2.1 demonstrou a configura√ß√£o do data center do Google. <br><br><ul><li>  Dezenas de carros s√£o colocados nas prateleiras. </li><li>  As prateleiras est√£o em fileiras. </li><li>  Uma ou mais linhas formam um cluster. </li><li>  Geralmente, na constru√ß√£o de um data center (DPC), ou data center, v√°rios clusters est√£o localizados. </li><li>  V√°rios edif√≠cios de data center pr√≥ximos uns dos outros comp√µem o campus. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/no/2i/-3no2ios8vkdcddiea5ynqaaz1s.png" alt="imagem"></div><br>  Dentro de cada data center, todas as m√°quinas devem poder se comunicar efetivamente entre si, por isso criamos um switch virtual (switch) muito r√°pido com dezenas de milhares de portas.  Isso foi poss√≠vel conectando centenas de switches desenvolvidos pelo Google em uma ‚Äúf√°brica‚Äù baseada na topologia da rede Clos [Clos, 1953], chamada Jupiter [Singh et al., 2015].  Na sua configura√ß√£o m√°xima, o Jupiter suporta taxa de transfer√™ncia de 1,3 Pb / s entre servidores. <br><br>  Os data centers s√£o conectados entre si usando nossa rede global de backbone B4 [Jain et al., 2013].  O B4 possui uma arquitetura de rede configur√°vel por software e usa o protocolo de comunica√ß√£o aberta OpenFlow.  B4 fornece ampla largura de banda a um n√∫mero limitado de sistemas e usa controle flex√≠vel de largura de canal para maximizar seu valor m√©dio [Kumar et al., 2015]. <br><br><h3>  Software de sistema que "organiza" o equipamento </h3><br>  O software que fornece o gerenciamento e administra√ß√£o de nossos equipamentos deve ser capaz de lidar com sistemas enormes.  Falhas de hardware s√£o um dos principais problemas resolvidos com a ajuda do software.  Dado o grande n√∫mero de componentes de hardware em um cluster, eles acontecem com bastante frequ√™ncia.  Em cada cluster, milhares de m√°quinas geralmente falham em um ano e milhares de discos r√≠gidos falham.  Se voc√™ multiplicar esse n√∫mero pelo n√∫mero de clusters que operam em todo o mundo, o resultado √© impressionante.  Portanto, queremos isolar os usu√°rios de tais problemas, e as equipes envolvidas em nossos servi√ßos tamb√©m n√£o querem se distrair com problemas de hardware.  Cada campus do data center possui equipes respons√°veis ‚Äã‚Äãpelo suporte ao equipamento e √† infraestrutura do data center. <br><br><h3>  Gerenciamento de m√°quinas </h3><br>  Borg (Figura 2.2) √© um sistema de gerenciamento de cluster distribu√≠do [Verma et al., 2015], semelhante ao Apache Mesos.  A Borg gerencia trabalhos em n√≠vel de cluster. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tu/bv/iu/tubviu0qs-kyfob6r1kgodbttye.png" alt="imagem"></div>  Borg √© respons√°vel pelo lan√ßamento de trabalhos do usu√°rio.  Essas tarefas podem ser servi√ßos em execu√ß√£o constante ou processos em lote como o MapReduce [Dean e Ghemawat, 2004].  Eles podem consistir em v√°rias (√†s vezes milhares) de tarefas id√™nticas (tarefas) - por motivos de confiabilidade e porque um processo, via de regra, n√£o √© capaz de processar todo o tr√°fego do cluster.  Quando Borg inicia a tarefa, ele encontra as m√°quinas para executar suas tarefas e ordena que iniciem o programa do servidor.  Borg ent√£o monitora o status dessas tarefas.  Se a tarefa n√£o funcionar corretamente, ela ser√° destru√≠da e reiniciada, possivelmente em outra m√°quina. <br><br>  Como as tarefas s√£o distribu√≠das livremente entre m√°quinas, n√£o podemos usar endere√ßos IP e n√∫meros de porta para acess√°-los.  Esse problema √© resolvido por um n√≠vel adicional de abstra√ß√£o: ao iniciar uma tarefa, o Borg aloca um nome para a tarefa e um n√∫mero (√≠ndice) para cada tarefa usando o servi√ßo de nomes Borg (BNS).  Em vez de usar o endere√ßo IP e o n√∫mero da porta, outros processos se associam √†s tarefas da Borg pelo nome do BNS, que depois converte o BNS em endere√ßo IP e n√∫mero da porta.  Por exemplo, o caminho do BNS pode ser uma sequ√™ncia como / bns / &lt;cluster&gt; / &lt;user&gt; / &lt;task_name&gt; / &lt;task_number&gt;, que √© ent√£o traduzida (√© comum dizer "permitido" nas redes) no formato &lt;endere√ßo IP&gt;: &lt;port&gt; . <br><br>  Borg tamb√©m √© respons√°vel pela aloca√ß√£o de recursos para atribui√ß√µes.  Cada tarefa deve indicar quais recursos s√£o necess√°rios para conclu√≠-la (por exemplo, tr√™s n√∫cleos de processador, 2 GB de RAM).  Usando a lista de requisitos para todas as tarefas, o Borg pode distribuir tarefas de maneira ideal entre m√°quinas, levando em considera√ß√£o as considera√ß√µes de toler√¢ncia a falhas (por exemplo, o Borg n√£o iniciar√° todas as tarefas de uma tarefa no mesmo rack, pois a troca desse rack ser√° um ponto cr√≠tico em caso de falha) tarefas). <br><br>  Se uma tarefa tenta pegar mais recursos do que o solicitado, a Borg a destr√≥i e reinicia (j√° que geralmente √© prefer√≠vel ter uma tarefa que √†s vezes falha e reinicia do que n√£o √© reiniciada). <br><br><h3>  Armazenamento </h3><br>  Para acesso mais r√°pido aos dados, as tarefas podem usar o disco local das m√°quinas, mas temos v√°rias op√ß√µes para organizar o armazenamento persistente no cluster (e mesmo os dados armazenados localmente ser√£o movidos para o armazenamento em cluster).  Eles podem ser comparados aos sistemas de arquivos em cluster do Luster e do Hadoop Distributed File System (HDFS) com uma implementa√ß√£o de c√≥digo aberto. <br><br>  O armazenamento oferece aos usu√°rios a capacidade de acessar com facilidade e confiabilidade os dados dispon√≠veis para o cluster.  Como mostrado na fig.  2.3, o reposit√≥rio possui v√°rias camadas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/yf/7c/atyf7cemd0e2gczkjfgdnltw7zy.png" alt="imagem"></div><br>  1. A camada mais baixa √© chamada D (do disco, embora o n√≠vel D use os discos r√≠gidos tradicionais e as unidades flash).  D √© um servidor de arquivos que roda em praticamente todas as m√°quinas de cluster.  No entanto, os usu√°rios que desejam acessar seus dados n√£o gostariam de se lembrar em qual m√°quina est√£o armazenados, portanto a pr√≥xima camada est√° conectada aqui. <br><br>  2. A camada D acima √© a camada Colossus, que cria um sistema de arquivos no cluster que oferece a sem√¢ntica usual do sistema de arquivos, al√©m de replica√ß√£o e criptografia.  Colossus √© o sucessor do GFS, o sistema de arquivos do Google (Ghemawat et al., 2003). <br><br>  3. Em seguida, existem v√°rios servi√ßos semelhantes a bancos de dados criados acima do n√≠vel do Colossus. <br><br><ul><li>  Bigtable [Chang et al., 2006] √© um sistema de banco de dados n√£o relacional (NoSQL) capaz de trabalhar com bancos de dados do tamanho de petabytes.  O Bigtable √© um banco de dados disperso, tolerante a falhas, multidimensional e ordenado, indexado por chaves de linha, coluna e registro de data e hora;  cada valor do banco de dados √© uma matriz arbitr√°ria e n√£o interpretada de bytes.  O Bigtable tamb√©m suporta replica√ß√£o entre data centers. </li><li>  Spanner [Corbett et al., 2012] oferece uma interface semelhante a SQL para usu√°rios que exigem integridade e consist√™ncia de dados ao acessar de qualquer lugar do mundo. </li><li>  V√°rios outros sistemas de banco de dados est√£o dispon√≠veis, como o Blobstore.  Todos eles t√™m suas pr√≥prias for√ßas e fraquezas (ver cap√≠tulo 26). </li></ul><br><h3>  Rede </h3><br>  A rede do Google √© gerenciada de v√°rias maneiras.  Como mencionado anteriormente, usamos uma rede configur√°vel por software baseada no OpenFlow.  Em vez de roteadores inteligentes, usamos switches tolos n√£o t√£o caros em combina√ß√£o com um controlador central (duplicado), que pr√©-calcula a melhor rota na rede.  Isso permite que voc√™ use um equipamento de comuta√ß√£o mais simples, liberando-o da busca de rotas demorada. <br><br>  A largura de banda da rede deve ser alocada corretamente.  Como o Borg limita os recursos de computa√ß√£o que uma tarefa pode usar, o Bandwidth Enforcer (BwE) gerencia a largura de banda dispon√≠vel para maximizar o rendimento m√©dio.  A otimiza√ß√£o da largura de banda n√£o se relaciona apenas ao custo: o gerenciamento centralizado de tr√°fego resolve uma s√©rie de problemas extremamente dif√≠ceis de resolver por uma combina√ß√£o de roteamento distribu√≠do e gerenciamento convencional de tr√°fego (Kumar, 2015). <br><br>  Alguns servi√ßos t√™m trabalhos em execu√ß√£o em v√°rios clusters localizados em diferentes partes do mundo.  Para reduzir o tempo de atraso dos sistemas distribu√≠dos globalmente, gostar√≠amos de direcionar os usu√°rios para o data center mais pr√≥ximo que possua a capacidade adequada para isso.  Nosso Global Software Load Balancer (GSLB) realiza o balanceamento de carga em tr√™s n√≠veis: <br><br><ul><li>  balanceamento de carga geogr√°fica para consultas DNS (por exemplo, para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">www.google.com</a> ), √© descrito no cap√≠tulo 19; </li><li>  balanceamento de carga no n√≠vel dos servi√ßos do usu√°rio (por exemplo, YouTube ou Google Maps); </li><li>  balanceamento de carga no n√≠vel RPC (Remote Procedure Call), descrito no Cap√≠tulo 20. </li></ul><br>  Os propriet√°rios do servi√ßo especificam nomes simb√≥licos para eles, uma lista de endere√ßos BNS do servidor e o desempenho dispon√≠vel em cada site (geralmente √© medido em consultas por segundo - consultas por segundo, QPS).  Posteriormente, o GSLB roteia o tr√°fego para os endere√ßos BNS especificados. <br><br><h3>  Outro software do sistema </h3><br><br>  Existem outros componentes importantes no software do data center. <br><br>  <b>Servi√ßo de bloqueio</b> <br><br>  O Chubby Lock Service [Burrows, 2006] fornece uma API semelhante ao sistema de arquivos para servir bloqueios.  O Chubby lida com bloqueios em todos os datacenters.  Ele usa o protocolo Paxos para acessar de forma ass√≠ncrona o consenso (consulte o cap√≠tulo 23). <br><br>  O gordinho tamb√©m desempenha um papel importante na escolha de um assistente.  Se, para algum servi√ßo, cinco r√©plicas de uma tarefa s√£o fornecidas com o objetivo de aumentar a confiabilidade, mas em um determinado momento apenas uma delas faz o trabalho real, o Chubby √© usado para selecionar essa r√©plica. <br>  O Chubby √© √≥timo para dados que requerem confiabilidade de armazenamento.  Por esse motivo, o BNS usa o Chubby para armazenar a propor√ß√£o de caminhos do BNS para o endere√ßo IP: pares de portas. <br><br>  <b>Monitoramento e alerta</b> <br><br>  Queremos ter certeza de que todos os servi√ßos est√£o funcionando corretamente.  Portanto, estamos lan√ßando muitas inst√¢ncias do programa de monitoramento Borgmon (consulte o cap√≠tulo 10).  A Borgmon recebe regularmente valores de refer√™ncia dos servi√ßos monitorados.  Esses dados podem ser usados ‚Äã‚Äãimediatamente para notifica√ß√£o ou armazenados para processamento e an√°lise subsequentes, por exemplo, para constru√ß√£o de gr√°ficos.  Esse monitoramento pode ser usado para fins como: <br><br><ul><li>  configurar alertas para problemas urgentes; </li><li>  compara√ß√£o de comportamento: a atualiza√ß√£o do software acelerou o servidor; </li><li>  avalia√ß√£o da natureza das mudan√ßas no consumo de recursos ao longo do tempo, necess√°rias para o planejamento da capacidade. </li></ul><br><br><h3>  Nossa infraestrutura de software </h3><br>  A arquitetura do nosso software foi projetada para que seja poss√≠vel usar os recursos de hardware do sistema com mais efici√™ncia.  Nosso c√≥digo inteiro √© multiencadeado, portanto, uma tarefa pode usar facilmente v√°rios n√∫cleos.  Para suportar pain√©is, monitoramento e depura√ß√£o, cada servidor inclui uma implementa√ß√£o de servidor HTTP como uma interface atrav√©s da qual s√£o fornecidas informa√ß√µes e estat√≠sticas de diagn√≥stico para uma tarefa espec√≠fica. <br><br>  Todos os servi√ßos do Google se "comunicam" usando a infraestrutura RPC (chamada de procedimento remoto) chamada Stubby.  Existe uma vers√£o de c√≥digo aberto, chamada gRPC (consulte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">grpc.io</a> ).  Freq√ºentemente, uma chamada RPC √© feita mesmo para rotinas no programa local.  Isso permite reorientar o programa para as chamadas de outro servidor para obter maior modularidade ou conforme o c√≥digo do servidor original aumenta.  O GSLB pode executar o balanceamento de carga RPC da mesma maneira que nas interfaces de servi√ßo externas. <br><br>  O servidor recebe solicita√ß√µes de RPC do front-end e envia a RPC para o back-end.  Usando termos tradicionais, o front-end √© chamado de cliente e o back-end √© chamado de servidor. <br>  Os dados s√£o transferidos de e para o RPC usando o protocolo de serializa√ß√£o - os chamados buffers de protocolo ou, brevemente, protobufs.  Esse protocolo √© semelhante ao Thrift do Apache e possui v√°rias vantagens sobre o XML quando se trata de serializar dados estruturados: √© mais simples, tr√™s a dez vezes mais compacto, 20 a 100 vezes mais r√°pido e exclusivo. <br><br><h3>  Nosso ambiente de desenvolvimento </h3><br>  A velocidade do desenvolvimento do produto √© muito importante para o Google, por isso criamos um ambiente especial que aproveita ao m√°ximo nossa infraestrutura [Morgenthaler et al., 2012]. <br><br>  Com exce√ß√£o de alguns grupos cujos produtos s√£o de c√≥digo aberto e, portanto, usam seus pr√≥prios reposit√≥rios separados (por exemplo, Android e Chrome), os engenheiros de software do Google trabalham em um reposit√≥rio comum [Potvin, Levenberg, 2016].  Essa abordagem tem v√°rias aplica√ß√µes pr√°ticas que s√£o importantes para o nosso processo de produ√ß√£o. <br><br><ul><li>  Se um engenheiro encontrar um problema em um componente fora de seu projeto, ele poder√° corrigi-lo, enviar as altera√ß√µes propostas (‚Äúlista de altera√ß√µes‚Äù - lista de mudan√ßas, CL) ao propriet√°rio para considera√ß√£o e, em seguida, implementar as altera√ß√µes feitas na ramifica√ß√£o principal do programa. </li><li>  Altera√ß√µes no c√≥digo-fonte no projeto de um engenheiro requerem considera√ß√£o - realiza√ß√£o de uma auditoria (revis√£o).  Todo o software passa esse est√°gio antes da ado√ß√£o. </li></ul><br>  Quando o software est√° sendo montado, a solicita√ß√£o de montagem √© enviada para servidores especializados de data center.  At√© a cria√ß√£o de grandes projetos √© r√°pida, porque voc√™ pode usar v√°rios servidores para compila√ß√£o paralela.  Essa infraestrutura tamb√©m √© usada para testes cont√≠nuos.  Sempre que uma nova lista de altera√ß√µes (CL) aparece, s√£o executados testes de todos os softwares que podem ser afetados direta ou indiretamente por essas altera√ß√µes.  Se a estrutura detectar que as altera√ß√µes interromperam a opera√ß√£o de outras partes do sistema, notificar√° o propriet√°rio dessas altera√ß√µes.  Alguns projetos usam o sistema push-on-green ("envio com sucesso"), segundo o qual a nova vers√£o √© automaticamente enviada para opera√ß√£o comercial ap√≥s a aprova√ß√£o nos testes. <br><br><h3>  Shakespeare: exemplo de servi√ßo </h3><br>  Para demonstrar como o Google desenvolve um servi√ßo em um ambiente industrial, considere um exemplo de servi√ßo hipot√©tico que interage com as tecnologias do Google.  Suponha que desejemos oferecer um servi√ßo que permita determinar em quais obras de Shakespeare a palavra que voc√™ mencionou ocorre. <br><br>  Podemos dividir o sistema em duas partes. <br><br><ul><li>  Um componente de processamento em lote que l√™ todos os textos de Shakespeare, cria um √≠ndice e o grava no Bigtable.  Essa tarefa (mais precisamente, a tarefa) √© executada uma vez ou, possivelmente, ocasionalmente (afinal, algum novo texto de Shakespeare pode aparecer!). </li><li>  Um aplicativo front-end que processa solicita√ß√µes do usu√°rio final.  Essa tarefa est√° sempre em execu√ß√£o, pois a qualquer momento, um usu√°rio de qualquer fuso hor√°rio pode pesquisar nos livros de Shakespeare. </li></ul><br>  O componente de processamento em lote ser√° o servi√ßo MapReduce, cujo trabalho √© dividido em tr√™s fases. <br><br>  1. Na fase de mapeamento, os textos de Shakespeare s√£o lidos e divididos em palavras separadas.  Esta parte do trabalho ser√° conclu√≠da mais rapidamente se v√°rios processos de trabalho (tarefas) forem iniciados em paralelo. <br><br>  2. Na fase Aleat√≥ria, as entradas s√£o classificadas por palavra. <br><br>  3. Na fase Reduzir, as tuplas do formul√°rio (palavra, lista_produtos) s√£o criadas. <br><br>  Cada tupla √© escrita como uma string no Bigtable, a chave √© a palavra. <br><br><h3>  Solicitar ciclo de vida </h3><br>  Na fig.  2.4 mostra como a solicita√ß√£o do usu√°rio √© atendida.  Primeiro, o usu√°rio clica no link shakespeare.google.com no navegador.  Para obter o endere√ßo IP apropriado, o dispositivo do usu√°rio converte ("resolve") o endere√ßo usando o servidor DNS (1).  A consulta DNS acaba sendo encerrada no servidor DNS do Google, que interage com o GSLB.  Rastreando a carga de tr√°fego de todos os servidores front-end por regi√£o, o GSLB escolhe qual endere√ßo IP de qual servidor retornar ao usu√°rio. <br><br>  O navegador se conecta ao servidor HTTP no endere√ßo especificado.  Este servidor (chamado Google Frontend ou GFE) √© um servidor proxy "reverso" localizado na outra extremidade da conex√£o TCP do cliente (2).  O GFE pesquisa o servi√ßo necess√°rio (por exemplo, pode ser um servi√ßo de pesquisa, mapas ou - no nosso caso, o servi√ßo Shakespeare).  Acessando repetidamente o GSLB, o servidor encontra um servidor front-end Shakespeare dispon√≠vel e o acessa por meio de uma RPC (chamada de procedimento remoto), transmitindo uma solicita√ß√£o HTTP recebida do usu√°rio (3). <br><br>  O servidor Shakespeare analisa a solicita√ß√£o HTTP e cria um "buffer de protocolo" (protobuf) contendo as palavras a serem encontradas.  Agora, o servidor front-end de Shakespeare deve entrar em contato com o servidor back-end de Shakespeare: o primeiro entra em contato com o GSLB para obter o endere√ßo BNS de uma inst√¢ncia adequada e descarregada da segunda (4).  Em seguida, o servidor back-end de Shakespeare entra em contato com o servidor Bigtable para receber os dados solicitados (5). <br><br>  O resultado √© gravado no protobuf de resposta e retornado ao servidor de back-end de Shakespeare.  O back-end passa o protobuf com o resultado do servi√ßo para o servidor front-end de Shakespeare, que cria um documento HTML e o devolve como resposta ao usu√°rio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6b/nd/yo/6bndyo9cfcxu5k5sx4lieg5bfw8.png" alt="imagem"></div><br>  Toda essa cadeia de eventos acontece num piscar de olhos - em apenas algumas centenas de milissegundos!  Como muitos componentes est√£o envolvidos, h√° muitos locais onde um erro em potencial pode ocorrer;  em particular, uma falha no GSLB pode desorganizar todo o trabalho e levar ao colapso.   Google,   ,                 (   ),     ,    .   ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">www.google.com</a>  ,     . <br><br><h3>     </h3><br>   ,   -    100    (QPS).       ,       3470 QPS,       35 .    ,      37 ,  N + 2. <br><br><ul><li>        ,     36 . </li><li>         , -    35  ‚Äî  ,      . </li></ul><br>          : 1430 QPS    , 290 ‚Äî   , 1400 ‚Äî    350 ‚Äî    .      -   ,     :  ,  ,   .   N + 2   ,  17   , 16 ‚Äî     ‚Äî  .   , ,      ( ),    ‚Äî  N + 2  N + 1.                  :  GSLB    -       ,    20 % ,      .            2‚Äì3 . <br><br>  -      Bigtable,       .  -     Bigtable,   ,      ,    Bigtable   .        ,   Bigtable  ,        .   Bigtable           ,     ,         . <br><br> ,          .       ,         ,    . <br><br>  ¬ªMais informa√ß√µes sobre o livro podem ser encontradas no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site do editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Conte√∫do</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trecho</a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cupom de 20% de desconto para Habrozavitel - </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Site Confiabilidade Engenharia</font></font></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt420139/">https://habr.com/ru/post/pt420139/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt420125/index.html">Automa√ß√£o em finan√ßas: funcion√°rios banc√°rios podem ficar sem trabalho devido a rob√¥s</a></li>
<li><a href="../pt420129/index.html">Padr√µes ass√≠ncronos de rotina: fora aguardam</a></li>
<li><a href="../pt420131/index.html">M√©todo Probabil√≠stico de Minera√ß√£o de Bitcoin</a></li>
<li><a href="../pt420133/index.html">Modelagem de sistemas din√¢micos: como a lua se move?</a></li>
<li><a href="../pt420135/index.html">Isso tamb√©m √© Toshiba: produtos inesperados da corpora√ß√£o japonesa</a></li>
<li><a href="../pt420141/index.html">No MPP DBMS carregado - peppy Data Lake com ferramentas anal√≠ticas: compartilhe os detalhes da cria√ß√£o</a></li>
<li><a href="../pt420143/index.html">Desempenho do Kotlin no Android</a></li>
<li><a href="../pt420145/index.html">Como √© o dia √∫til dos membros do PC AppsConf</a></li>
<li><a href="../pt420147/index.html">C√≥digo aberto no Clojure</a></li>
<li><a href="../pt420151/index.html">Mais f√°cil do que parece. Cap√≠tulo 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>