<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è´ ü§∑üèø ‚ò™Ô∏è La rubrique ¬´Lisez des articles pour vous¬ª. Juillet - septembre 2019 üà∑Ô∏è üë©‚Äçüë©‚Äçüëß‚Äçüëß üñêÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Nous continuons √† publier des critiques d'articles scientifiques de membres de la communaut√© Open Data Science sur la cha√Æne #article_e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La rubrique ¬´Lisez des articles pour vous¬ª. Juillet - septembre 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/472672/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Bonjour, Habr!  Nous continuons √† publier des critiques d'articles scientifiques de membres de la communaut√© Open Data Science sur la cha√Æne #article_essense.  Si vous souhaitez les recevoir avant tout le monde - rejoignez la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">communaut√©</a> ! </p><br><p>  Articles pour aujourd'hui: </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La rotation des couches: un indicateur √©tonnamment puissant de g√©n√©ralisation dans les r√©seaux profonds?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">(Universit√© catholique de Louvain, Belgique, 2018)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage par transfert √† param√®tres efficaces pour la PNL (Google Research, Jagiellonian University, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RoBERTa: A Robustly Optimized BERT Pretraining Approach (Universit√© de Washington, Facebook AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">EfficientNet: repenser la mise √† l'√©chelle du mod√®le pour les r√©seaux de neurones convolutifs (Google Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment le cerveau passe de la perception consciente √† la perception subliminale (√âtats-Unis, Argentine, Espagne, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Grandes couches de m√©moire avec cl√©s de produit (Facebook AI Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Faisons-nous vraiment beaucoup de progr√®s?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Une analyse inqui√©tante des approches r√©centes de recommandation neuronale (Politecnico di Milano, Universit√© de Klagenfurt, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage des fonctionnalit√©s omnidimensionnelles pour la r√©-identification des personnes (Universit√© de Surrey, Universit√© Queen Mary, Samsung AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La reparam√©trie neuronale am√©liore l'optimisation structurelle (Google Research, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Liens vers les anciennes collections de la s√©rie:</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Janvier - juin 2019</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">F√©vrier - mars 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©cembre 2017 - janvier 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Octobre - novembre 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Septembre 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ao√ªt 2017</a> </li></ul></div></div><br><h3 id="1-layer-rotation-a-surprisingly-powerful-indicator-of-generalization-in-deep-networks">  1. La rotation des couches: un indicateur √©tonnamment puissant de g√©n√©ralisation dans les r√©seaux profonds? </h3><br><p>  Auteurs: Simon Carbonnelle, Christophe De Vleeschouwer (Universit√© catholique de Louvain, Belgique, 2018) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí Article original</a> <br>  Auteur de la revue: Svyatoslav Skoblov (in slack error_derivative) </p><br><img src="https://habrastorage.org/webt/tt/n5/g8/ttn5g8j27-ihyqnwk0rowhg8oie.png" width="500" height="250"><br><p><br>  Dans cet article, les auteurs ont attir√© l'attention sur une observation assez simple: la distance cosinus entre les poids des couches lors de l'initialisation et apr√®s la formation (le processus d'augmentation de la distance pendant la formation est appel√© rotation des couches).  Les messieurs disent que dans la plupart des exp√©riences, les r√©seaux qui ont atteint une distance de 1 dans toutes les couches sont constamment sup√©rieurs en pr√©cision aux autres configurations.  L'article pr√©sente √©galement l'algorithme <strong>Layca</strong> (Layer-level Controlled Amount of weight rotation), qui permet d'utiliser ce taux d'apprentissage par couche pour contr√¥ler cette m√™me rotation de couche.  En fait, il diff√®re de l'algorithme SGD habituel par la pr√©sence d'une projection orthogonale et d'une normalisation.  Une liste d√©taill√©e de l'algorithme avec le programme de formation peut √™tre trouv√©e dans l'article. </p><br><p>  L'id√©e principale que les auteurs en d√©duisent est la suivante: <strong>plus les rotations de couches sont importantes, meilleures sont les performances de g√©n√©ralisation</strong> .  La plupart de l'article est un enregistrement d'exp√©riences o√π divers sc√©narios de formation ont √©t√© √©tudi√©s: MNIST, CIFAR-10 / CIFAR-100, de minuscules ImageNet avec diff√©rentes architectures ont √©t√© utilis√©s, d'un r√©seau monocouche √† la famille ResNet. </p><br><p>  Une s√©rie d'exp√©riences s'est d√©roul√©e en plusieurs √©tapes: </p><br><ol><li>  <strong>Vanilla SGD</strong> Il <strong>s'est</strong> av√©r√© que, dans l'ensemble, le comportement des √©chelles co√Øncide avec l'hypoth√®se (de grands changements de distance correspondaient aux meilleures valeurs m√©triques), mais des probl√®mes ont √©galement √©t√© constat√©s: la rotation des couches s'est arr√™t√©e bien avant les valeurs souhait√©es;  une instabilit√© dans le changement de distance a √©galement √©t√© not√©e. </li><li>  <strong>SGD + d√©croissance</strong> du <strong>poids La</strong> diminution de la norme de <strong>poids a</strong> consid√©rablement am√©lior√© l'image de la formation: la plupart des couches ont atteint la distance maximale et les performances du test sont similaires √† celles du Layca propos√©.  L‚Äôavantage incontestable de la m√©thode de l‚Äôauteur est l‚Äôabsence d‚Äôun hyperparam√®tre suppl√©mentaire. </li><li>  <strong>√âchauffements LR</strong> Il s'est av√©r√© que l'√©chauffement aide SGD √† surmonter le probl√®me de la rotation des couches instable, cependant, cela n'a aucun effet sur Layca. </li><li>  <strong>M√©thodes adaptatives de gradient</strong> En plus de la v√©rit√© bien connue (en utilisant ces m√©thodes, il est plus difficile d'atteindre le niveau de g√©n√©ralisation que SGD + la d√©croissance de poids peut donner), il s'est av√©r√© que les effets de la rotation des couches sont tr√®s diff√©rents: la premi√®re augmente la rotation dans les derni√®res couches, tandis que la SGD dans les couches initiales .  Les auteurs sugg√®rent que cela peut √™tre la m√©chancet√© des m√©thodes adaptatives.  Et ils sugg√®rent d'utiliser Layca en conjonction avec eux (am√©liorant la capacit√© de g√©n√©raliser dans les m√©thodes adaptatives et acc√©l√©rant l'apprentissage dans SGD). </li></ol><br><p>  L'article se termine par une tentative d'interpr√©tation du ph√©nom√®ne.  Pour ce faire, les auteurs ont form√© un r√©seau avec 1 couche cach√©e sur une version d√©pouill√©e de MNIST, apr√®s quoi ils ont visualis√© des neurones al√©atoires, aboutissant √† une conclusion tout √† fait logique: un plus grand degr√© de rotation des couches correspond √† un moindre effet d'initialisation et √† une meilleure √©tude des caract√©ristiques, ce qui contribue √† une meilleure g√©n√©ralisation. </p><br><p>  Le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code de l'algorithme impl√©ment√© (tf / keras)</a> et le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code de reproduction des exp√©riences sont</a> t√©l√©charg√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">.</a> </p><br><h3 id="2-parameter-efficient-transfer-learning-for-nlp">  2. Apprentissage de transfert √† param√®tres efficaces pour la PNL </h3><br><p>  Auteurs de l'article: Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly (Google Research, Jagiellonian University, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí Article original</a> <br>  Auteur de la revue: Alexey Karnachev (in slack zhirzemli) </p><br><img src="https://habrastorage.org/webt/ka/lg/gp/kalggpbjkmd8zc7ep451lysxpc8.png"><br><p><br>  Ici, les messieurs offrent une technique de r√©glage fin simple mais efficace pour les mod√®les NLP (dans ce cas, BERT).  L'id√©e est d'incorporer des couches d'apprentissage (adaptateurs) directement dans le r√©seau.  Chacune de ces couches est un r√©seau avec un goulot d'√©tranglement, qui adapte les √©tats latents du mod√®le d'origine √† une t√¢che en aval sp√©cifique.  Les poids du mod√®le d'origine, quant √† eux, restent fig√©s. </p><br><p>  <strong>La motivation</strong> <br>  Dans les conditions de la formation en streaming (ou de la formation quasi en ligne), o√π il y a beaucoup de t√¢ches en aval, je n'ai pas vraiment envie de classer l'int√©gralit√© du mod√®le.  Premi√®rement, pendant longtemps, et deuxi√®mement, c'est difficile, et troisi√®mement, m√™me s'il est serr√©, le mod√®le doit √™tre en quelque sorte stock√©: pour vider ou pour garder en m√©moire.  Et nous ne serons pas en mesure de r√©utiliser ce mod√®le pour la t√¢che suivante: chaque fois que nous devons r√©gler une nouvelle mani√®re.  En cons√©quence, nous pouvons essayer d'adapter les √©tats de r√©seau cach√©s au probl√®me actuel.  De plus, le mod√®le d'origine reste intact et les adaptateurs eux-m√™mes sont beaucoup plus volumineux que le mod√®le principal (~ 4% du nombre total de param√®tres) </p><br><p>  <strong>Impl√©mentation</strong> <br>  Le probl√®me est r√©solu d'une mani√®re incroyablement simple: nous ajoutons 2 adaptateurs √† chaque couche du mod√®le.  Avant la normalisation des couches dans les mod√®les √† transformateur, une connexion par saut se produit: l'entr√©e transform√©e (√©tat masqu√© actuel) est ajout√©e √† l'entr√©e d'origine. </p><br><p>  Il y a 2 sections de ce type dans chaque couche de transformateur: une apr√®s une attention multi-t√™tes, la seconde apr√®s une alimentation directe.  Ainsi, les √©tats cach√©s de ces sections sont √©galement transmis via l'adaptateur: un r√©seau peu profond avec une couche cach√©e √† 1 goulot d'√©tranglement et avec une sortie de la m√™me dimension que l'entr√©e.  La non-lin√©arit√© est appliqu√©e √† l'√©tat de goulot d'√©tranglement et l'entr√©e (saut de connexion) est ajout√©e √† la sortie.  Il s'av√®re que le nombre total de param√®tres entra√Æn√©s est: 2md + m + d, o√π d est la dimension de l'√©tat cach√© du mod√®le d'origine, m est la taille de la couche d'adaptateur de goulot d'√©tranglement.  Il s'av√®re que pour le mod√®le de base BERT (12 couches, param√®tres 110M) et pour la taille de l'adaptateur bottlneck'a 128, nous obtenons 4,3% du nombre total de param√®tres </p><br><p>  <strong>R√©sultats</strong> <br>  La comparaison a √©t√© faite avec le r√©glage complet du mod√®le.  Pour toutes les t√¢ches, cette approche a montr√© une perte mineure de m√©triques (en moyenne moins de 1 point), avec le nombre de poids form√©s - 3% du total.  Je n'√©num√©rerai pas les t√¢ches elles-m√™mes, il y en a beaucoup, il y a une tablette dans l'article. </p><br><p>  <strong>R√©glage fin</strong> <br>  Dans ce mod√®le, seule la partie adaptateur est r√©gl√©e (+ le classificateur de sortie lui-m√™me).  Pour les √©chelles d'adaptateurs, ils proposent de faire une initialisation proche de l'identit√©.  Ainsi, un mod√®le non form√© ne changera en rien les √©tats de r√©seau cach√©s, ce qui permettra d√©j√† au processus de formation du mod√®le de d√©cider quels √©tats s'adapter √† la t√¢che et lesquels rester inchang√©s. </p><br><p>  Le taux d'apprentissage recommande de prendre plus qu'avec le r√©glage fin BERT standard.  Personnellement, sur ma t√¢che, 1e-04 lr a bien fonctionn√©.  De plus, (d√©j√† personnellement mon observation) pendant le processus de r√©glage, le mod√®le explose presque toujours les gradients, vous devez donc vous rappeler de faire un √©cr√™tage.  Optimiseur - Adam avec √©chauffement 10% </p><br><p>  <strong>Code</strong> <br>  Le code de leur article est joint.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Impl√©mentation sur Tensorflow</a> . <br>  Pour Torch, l'auteur de la revue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">fork-pytorch-transformers et a ajout√© une couche Adaptateur</a> (au d√©but du fichier README.md il y a un petit manuel de lancement) </p><br><h3 id="3-roberta-a-robustly-optimized-bert-pretraining-approach">  3. RoBERTa: une approche de pr√©-formation BERT robuste et optimis√©e </h3><br><p>  Auteurs de l'article: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov (University of Washington, Facebook AI, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí Article original</a> <br>  Auteur de la revue: Artem Rodichev (in slack fuckai) </p><br><p>  Am√©liore consid√©rablement la qualit√© des mod√®les BERT, premi√®re place sur le classement GLUE et SOTA sur de nombreuses t√¢ches PNL.  Ils ont sugg√©r√© un certain nombre de fa√ßons de former le mod√®le BERT le mieux possible sans aucune modification de l'architecture du mod√®le lui-m√™me. </p><br><p>  Diff√©rences cl√©s avec le BERT d'origine: </p><br><ol><li>  Augmentation de la construction des trains 10 fois, de 16 Go de texte brut √† 160 Go </li><li>  Masquage dynamique fait pour chaque √©chantillon </li><li>  Suppression de l'utilisation de la pr√©diction de la peine suivante de la perte </li><li>  Augmentation de la taille du mini-lot de 256 √©chantillons √† 8k </li><li>  Am√©lioration du codage BPE en convertissant la base de donn√©es d'Unicode en octets. </li></ol><br><p>  Le meilleur mod√®le final a √©t√© form√© sur 1024 cartes Nvidia V100 (128 serveurs DGX-1) pendant 5 jours. </p><br><p>  <strong>L'essence de l'approche:</strong> </p><br><p>  <em>Donn√©es.</em>  En plus des coquilles Wiki et BookCorpus (16 Go au total), qui enseignaient le BERT original, ils ont ajout√© 3 autres coquilles plus grandes, toutes en anglais: </p><br><ol><li>  SS-News 63 millions de nouvelles en 2,5 ans sur 76 Go </li><li>  OpenWebText est le cadre sur lequel OpenAI a appris le mod√®le GPT2.  Ce sont des articles analys√©s auxquels des liens ont √©t√© fournis dans des publications sur un reddit avec au moins trois mises √† jour.  38 Go de donn√©es </li><li>  Histoires - 31GB CommonCrawl Story Case </li></ol><br><p>  <em>Masquage dynamique.</em>  Dans le BERT d'origine, 15% des jetons sont masqu√©s dans chaque √©chantillon et ces jetons sont pr√©dits √† l'aide de la partie non masqu√©e de la s√©quence.  Un masque est g√©n√©r√© une fois pour chaque √©chantillon pendant le pr√©traitement et ne change pas.  Dans le m√™me temps, le m√™me √©chantillon dans le train peut se produire plusieurs fois, selon le nombre d'√©poques dans le corps.  L'id√©e du masquage dynamique est de cr√©er un nouveau masque pour la s√©quence √† chaque fois, plut√¥t que d'utiliser un masque fixe en pr√©traitement. </p><br><p>  <em>Objectif de pr√©diction de la phrase suivante.</em>  Disons simplement couper cet objektiv et voir si cela a empir√©?  Est-il devenu meilleur ou est-il √©galement rest√© - sur les t√¢ches SQuAD, MNLI, SST et RACE. </p><br><p>  <em>Augmentez la taille du mini-lot.</em>  Dans de nombreux endroits, en particulier dans la traduction automatique, il a √©t√© d√©montr√© que plus le mini-lot est grand, meilleurs sont les r√©sultats finaux du train.  Ils ont montr√© que si vous augmentez le minibatch de 256 √©chantillons, comme dans le BERT d'origine, √† 2k, puis √† 8k, alors la perplexit√© sur les baisses de validation et les m√©triques sur MNLI et SST-2 augmentent. </p><br><p>  <em>BPE</em>  Le BPE de l'impl√©mentation BERT d'origine utilise des caract√®res Unicode comme base pour les unit√©s de sous-mots.  Cela conduit au fait que dans des cas importants et divers, une partie importante du dictionnaire sera occup√©e par des caract√®res Unicode individuels.  OpenAI de retour dans GPT2 a sugg√©r√© d'utiliser non pas des caract√®res Unicode, mais des octets comme base pour les sous-mots.  Si nous utilisons un dictionnaire BPE de 50k, nous n'aurons pas de tokens inconnus.  Par rapport au BERT d'origine, la taille du mod√®le a augment√© de 15 M param√®tres pour le mod√®le de base et de 20 M pour les grands, soit 5 √† 10% de plus. </p><br><p>  <strong>R√©sultats:</strong> <br>  BERT-large et XLNet-large sont utilis√©s comme mod√®les de comparaison.  RoBERTa lui-m√™me est le m√™me en termes de param√®tres que BERT-large, ce qui lui a valu la premi√®re place sur le benchmark GLUE.  Nous avons utilis√© l'optimisation de fichiers √† t√¢che unique, contrairement √† de nombreuses autres approches du r√©f√©rentiel GLUE qui effectuent l'optimisation de fichiers √† t√¢ches multiples.  Sur les filles dans GLUE, les r√©sultats d'un mod√®le unique sont compar√©s, ils ont obtenu SOTA sur les 9 t√¢ches.  Sur l'ensemble de test, l'ensemble des mod√®les est compar√©, SOTA pour 4 des 9 t√¢ches et la vitesse de colle finale.  Sur deux versions de SQuAD sur le r√©seau de d√©veloppement SOTA, sur l'ensemble de test au niveau XLNet.  De plus, contrairement √† XLNet, ils ne se font pas prendre par des packages QA suppl√©mentaires avant de r√©soudre SQuAD. </p><br><img src="https://habrastorage.org/webt/5x/ue/u9/5xueu9hpmqwowfuf0yn1_zopqxy.png" width="500" height="250"><br><p><br>  SOTA sur la t√¢che RACE dans laquelle un morceau de texte est donn√©, une question sur ce texte et 4 options de r√©ponse o√π vous devez choisir la bonne.  Pour r√©soudre cette t√¢che, ils concat√©nent le texte, la question et la r√©ponse, parcourent BERT, obtiennent une repr√©sentation du jeton CLF, s'appliquent √† une couche enti√®rement connect√©e et pr√©disent si la r√©ponse est correcte.  Cette op√©ration est effectu√©e 4 fois - pour chacune des options de r√©ponse. </p><br><p>  Nous avons affich√© le code et la pr√©-formation du mod√®le <a href="">RoBERTa</a> dans <a href="">navet fairseq</a> .  Vous pouvez l'utiliser, tout semble net et simple. </p><br><h3 id="4-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks">  4. EfficientNet: repenser la mise √† l'√©chelle du mod√®le pour les r√©seaux de neurones convolutifs </h3><br><p>  Auteurs: Mingxing Tan, Quoc V. Le (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí Article original</a> <br>  Auteur de la revue: Alexander Denisenko (en mou Alexander Denisenko) </p><br><img src="https://habrastorage.org/webt/ey/se/0k/eyse0kouanmvflpgz9ev--x1oqm.png" width="500" height="250"><br><p><br>  Ils √©tudient la mise √† l'√©chelle (mise √† l'√©chelle) des mod√®les et l'√©quilibre entre eux-m√™mes la profondeur et la largeur (nombre de canaux) du r√©seau, ainsi que la r√©solution des images dans la grille.  Ils offrent une nouvelle m√©thode de mise √† l'√©chelle qui met uniform√©ment √† l'√©chelle la profondeur / largeur / r√©solution.  Montrez son efficacit√© sur MobileNet et ResNet. </p><br><p>  Ils utilisent √©galement Neural Architecture Search pour cr√©er un nouveau maillage et le mettre √† l'√©chelle, obtenant ainsi une classe de nouveaux mod√®les - EfficientNets.  Ils sont meilleurs et beaucoup plus √©conomiques que les grilles pr√©c√©dentes.  Sur ImageNet, EfficientNet-B7 atteint une pr√©cision de pointe de 84,4% dans le top 1 et 97,1% dans le top 5, soit 8,4 fois moins et 6,1 fois plus rapidement en inf√©rence que le ConvNet, le meilleur de sa cat√©gorie.  Il se transf√®re bien √† d'autres ensembles de donn√©es - ils ont obtenu SOTA sur 5 des 8 ensembles de donn√©es les plus populaires. </p><br><p>  <strong>Mise √† l'√©chelle du mod√®le compos√©</strong> <br>  La mise √† l'√©chelle est lorsque les op√©rations effectu√©es √† l'int√©rieur de la grille sont fixes et que la profondeur (nombre de r√©p√©titions des m√™mes modules) d, la largeur (nombre de canaux en convolution) w et la r√©solution r sont modifi√©es.  Dans le pageur, la mise √† l'√©chelle est formul√©e comme un probl√®me d'optimisation - nous voulons la pr√©cision maximale (Net (d, w, r)) malgr√© le fait que nous n'explorions pas de m√©moire et FLOPS. </p><br><p>  Nous avons men√© des exp√©riences et nous avons veill√© √† ce que cela aide √©galement √† l'√©chelle en profondeur et en r√©solution lors de l'√©chelle en largeur.  Avec les m√™mes FLOPS, nous obtenons un r√©sultat nettement meilleur sur ImageNet (voir l'image ci-dessus).  En g√©n√©ral, cela est raisonnable, car il semble qu'avec une augmentation de la r√©solution de l'image du r√©seau, plus de couches sont n√©cessaires en profondeur pour augmenter le champ r√©cepteur et plus de canaux afin de capturer tous les motifs de l'image avec une r√©solution plus √©lev√©e. </p><br><p>  L'essence de la mise √† l'√©chelle compos√©e: nous prenons le coefficient compos√© phi, qui met uniform√©ment √† l'√©chelle d, w et r avec ce coefficient: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="47.52ex" height="2.419ex" viewBox="0 -780.1 20460 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-3D" x="801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="2107" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6C" x="2637" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="2935" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="3439" y="0"></use><g transform="translate(4015,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="4895" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-69" x="5975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="6320" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-77" x="6765" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-3D" x="7760" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-62" x="9066" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-65" x="9495" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-74" x="9962" y="0"></use><g transform="translate(10323,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="11203" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="11706" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-69" x="12283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="12628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-72" x="13073" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-3D" x="13803" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-67" x="15109" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="15589" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6D" x="16119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6D" x="16998" y="0"></use><g transform="translate(17876,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="18756" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="19259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-69" x="19836" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="20181" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> d = \ alpha ^ \ phi, w = \ beta ^ \ phi, r = \ gamma ^ \ phi, </script>  o√π <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.278ex" height="2.419ex" viewBox="0 -780.1 9161.3 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6D" x="7753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="8631" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ alpha, \ beta, \ gamma </script>  - constantes obtenues √† partir d'une petite vue de grille sur la grille source. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ phi </script>  - coefficient caract√©risant la quantit√© de ressources informatiques disponibles. </p><br><p>  <strong>Filet efficace</strong> <br>  Pour cr√©er la grille, nous avons utilis√© la recherche d'architecture neuronale multi-objectifs, la pr√©cision optimis√©e et les FLOPS avec le param√®tre responsable du compromis entre eux.  Une telle recherche a donn√© EfficientNet-B0.  En bref - Conv suivi de plusieurs MBConv, √† la fin de Conv1x1, Pool, FC. </p><br><p>  Effectuez ensuite la mise √† l'√©chelle en deux √©tapes: </p><br><ol><li>  Pour commencer, nous r√©parons <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.152ex" height="2.419ex" viewBox="0 -780.1 3510.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-69" x="1330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-3D" x="1953" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-31" x="3009" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ phi = 1 </script>  , faire une recherche dans la grille pour rechercher <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.278ex" height="2.419ex" viewBox="0 -780.1 9161.3 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-6D" x="7753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-61" x="8631" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ alpha, \ beta, \ gamma </script>  . </li><li>  Mettez la grille √† l'√©chelle en utilisant les formules pour d, w et r.  Vous avez EffiientNet-B1.  De m√™me, l'augmentation <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiFbdtCtW9is6VuL_WevqZJZG3HBw#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ phi </script>  , obtenez EfficientNet-B2, ... B7. </li></ol><br><p>  Mis √† l'√©chelle pour diff√©rents ResNet et MobileNet, partout a re√ßu des am√©liorations significatives sur ImageNet, la mise √† l'√©chelle compos√©e a donn√© une augmentation significative par rapport √† la mise √† l'√©chelle dans une seule dimension.  Nous avons √©galement men√© des exp√©riences avec EfficientNet sur huit ensembles de donn√©es plus populaires, partout o√π nous avons obtenu SOTA ou un r√©sultat proche avec un nombre de param√®tres consid√©rablement plus petit. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> </p><br><h3 id="5-how-the-brain-transitions-from-conscious-to-subliminal-perception">  5. Comment le cerveau passe de la perception consciente √† la perception subliminale </h3><br><p>  Auteurs de l'article: Francesca Arese Lucini, Gino Del Ferraro, Mariano Sigman, Hernan A. Makse (√âtats-Unis, Argentine, Espagne, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí Article original</a> <br>  Auteur de la revue: Svyatoslav Skoblov (in slack error_derivative) </p><br><p>  Cet article est une continuation et une refonte du travail de <em>Dehaene, S, Naccache, L, Cohen, L, Le Bihan, D, Mangin, JF, Poline, JB, &amp; Rivie`re, D.M√©canismes c√©r√©braux de masquage de mots et d'amor√ßage de r√©p√©tition inconsciente</em> , dans que les auteurs ont essay√© de consid√©rer les modes de fonctionnement c√©r√©bral conscient et inconscient. </p><br><img src="https://habrastorage.org/webt/fi/o4/te/fio4terok3udte6bcmpajzfs_um.png" width="500" height="250"><br><p><br>  <strong>Exp√©rience:</strong> <br>  On montre aux volontaires des images (mots de 4 lettres, ou un √©cran vierge, ou des gribouillis).  Chacun d'eux est affich√© pendant 30 ms, en g√©n√©ral, l'action enti√®re dure 5 minutes. </p><br><ol><li>  Dans le mode ¬´conscient¬ª de l'exp√©rience, un √©cran vierge alterne avec des mots, ce qui permet √† une personne de percevoir consciemment le texte. </li><li>  Dans le mode ¬´inconscient¬ª, les mots alternent avec les gribouillis, ce qui interf√®re assez efficacement avec la perception du texte √† un niveau conscient. </li></ol><br><p>  <strong>Donn√©es:</strong> <br>  Au cours de cette pr√©sentation, les cerveaux de nos primates ont √©t√© scann√©s √† l'aide de l'IRMf.  Au total, les chercheurs avaient 15 volontaires, chacun r√©p√©t√© l'exp√©rience 5 fois, un total de 75 flux d'IRMf.  Il est √† noter que le scan du voxel s'est av√©r√© √™tre assez grand (tr√®s simplifi√©: le voxel est un cube 3D contenant un nombre assez important de cellules) - 4x4x4mm. </p><br><p>  <strong>Magie:</strong> <br>  Appelons le n≈ìud voxel actif de notre flux.  Le cerveau √©tant un gant de toilette modulaire, nous y introduisons deux types de connexions: externe et interne (correspondant √† la disposition spatiale des n≈ìuds).  Les connexions sont assembl√©es de mani√®re int√©ressante: nous construisons une matrice de corr√©lation crois√©e entre les n≈ìuds et connectons les n≈ìuds avec une connexion si la corr√©lation est sup√©rieure √† certains param√®tres adaptatifs lambda.  Ce param√®tre affecte la d√©charge de notre r√©seau. </p><br><p>  Le r√©glage des param√®tres s'effectue √† l'aide de la proc√©dure de "filtrage".  Si nous balan√ßons un peu notre lambda, des transitions nettes entre les dimensions finales du r√©seau deviennent perceptibles (c'est-√†-dire qu'un changement de param√®tre suffisamment petit correspond √† un grand incr√©ment de taille). </p><br><p>  Donc: les connexions internes sont activ√©es par la valeur lambda-1, qui correspond √† la valeur lambda juste avant une transition brusque.  Externe - valeur lambda-2 correspondant √† la valeur lambda imm√©diatement apr√®s une transition brusque. </p><br><p>  <strong>Magic 2:</strong> <br>  filtrage k-core.  Le concept k-core d√©crit la connectivit√© r√©seau et est formul√© simplement: le sous-r√©seau maximum, dont tous les n≈ìuds ont au moins k voisins.  Un tel sous-r√©seau peut √™tre obtenu par suppression it√©rative de n≈ìuds avec moins de k voisins.  √âtant donn√© que les n≈ìuds restants perdront des voisins, le processus se poursuit jusqu'√† ce qu'il n'y ait rien √† supprimer.  Reste le r√©seau k-core. </p><br><p>  <strong>R√©sultats:</strong> <br>  En appliquant cette artillerie √† notre cerveau, vous pouvez voir un certain nombre de caract√©ristiques tr√®s int√©ressantes. </p><br><ol><li>  Le nombre de n≈ìuds dans k-core avec k petit / tr√®s grand est extr√™mement grand.  Mais pour k moyen, au contraire, ce n'est pas suffisant.  Dans l'image, il ressemble √† une forme en U, √† savoir, une telle configuration de r√©seau donne la plus grande stabilit√© du syst√®me (r√©sistance aux erreurs locales et globales). </li><li>  <strong>et les</strong> n≈ìuds <strong>les plus importants</strong> appartenant au k-core avec un petit k peuvent √™tre vus dans presque tous les √©tats du r√©seau.  Mais un k-core avec un k tr√®s grand n'est caract√©ristique que pour les parties du cerveau qui sont actives √† l'√©tat inconscient de <em>gyrus fusiforme et de gyrus pr√©central gauche</em> .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Les m√™mes parties du cortex sont les plus actives et dans un √©tat conscient. </font></font></li></ol><br><p>           ,   rewiring,       (  ,      ).          k.  , U shape          ,       ,        . </p><br><p>  <strong>Conclusions:</strong> <br> ,  ,   ,              .     ,     ,      ,    -     (     , , ,     ). </p><br><p>  ,   ,         ,       ,        ,         , ,    - . , ,            qualia. </p><br><h3 id="6-large-memory-layers-with-product-keys"> 6. Large Memory Layers with Product Keys </h3><br><p>  : Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv√© J√©gou (Facebook AI Research, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí  </a> <br>  :   (  belerafon) </p><br><img src="https://habrastorage.org/webt/4b/_q/nm/4b_qnmw2tilj5zyscirkrogerrg.png"><br><p><br> ,       key-value        ,   . </p><br><p>     -    attention.    q,     k   v.  q,    k,    ,      value  .  ,       .    ,         .      ,         ,   .     -    q       (, -10).        .      . </p><br><p>    ‚Äî     q   k   .   ,    "Product Keys".      ,         q   ,     .        -10   , ,     O(N)    ""  ,   (sqrt(N)). </p><br><p>         key-value .      ,    (  ,     ). ,   BERT      28  . ,          ,     .  : 12-       2  ,  24-  ,    perplexity     . </p><br><p>            (      self-attention). ,     -         .  ,         multy-head attention.  C'est-√†-dire    query  ,      value,     .         -. </p><br><p>        , ,        ,    ,    BERT  .      . </p><br><h3 id="7-are-we-really-making-much-progress-a-worrying-analysis-of-recent-neural-recommendation-approaches"> 7. Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches </h3><br><p>  : Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach (Politecnico di Milano, University of Klagenfurt, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí  </a> <br>  :   (  netcitizen) </p><br><img src="https://habrastorage.org/webt/zk/89/wu/zk89wuyudxpggdl6-0tyxe2wwrw.png"><br><p><br>       DL    , ,             . </p><br><p> <strong></strong> <br>             DL       top-n.    DL      KDD, SIGIR, TheWebConf (WWW)  RecSys     : </p><br><ol><li>     </li><li>    -       </li><li>       </li></ol><br><p> <strong></strong> </p><br><ol><li>    7/18 (39%) </li><li>        ‚Äú‚Äù    train/test,     .,   , ,   . </li><li>     (Variational Autoencoders for Collaborative Filtering (Mult-VAE)  ¬±   )     KNN, SVD, PR. </li></ol><br><p> <strong></strong> <br>  DL,       CV, NLP      ,       . </p><br><h3 id="8-omni-scale-feature-learning-for-person-re-identification"> 8. Omni-Scale Feature Learning for Person Re-Identification </h3><br><p>  : Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang (University of Surrey, Queen Mary University, Samsung AI, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí  </a> <br>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> (  graviton) </p><br><p>  Person Re-Identification,    Face Recognition,    ,          .  (Kaiyang Zhou)        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">deep-person-reid</a>     ,      (OSNet),          Person Re-Identification.     . </p><br><p> <strong>  </strong> Person Re-Identification: </p><br><img src="https://habrastorage.org/webt/6d/rq/jv/6drqjvwsyg33s_e7zv2t4nov0ts.png" width="500" height="250"><br><p><br> <strong> :</strong> </p><br><ol><li>   conv1x1  deepwise conv3x3   conv3x3   (figure 3). </li><li>  ,      .    ResNeXt         ,     Inception      (figure 4). </li><li>      ‚Äúaggregation gate‚Äù       .  ,    Inception     . </li></ol><br><img src="https://habrastorage.org/webt/ic/tc/9z/ictc9z6mcdkv3o0i23swu_f5zty.png"><br><p><br>  OSNet       , ..      ,    :      (  ,  )   . </p><br><p> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les r√©sultats des tests ReID</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour OSNet (environ 2 millions de param√®tres) indiquent l'avantage de cette architecture par rapport aux autres mod√®les d'√©clairage (march√©: R1 93,6%, mAP 81,0% pour OSNet et R1 87,0%, mAP 69,5% pour MobileNetV2) et l'absence de diff√©rence significative de pr√©cision avec mod√®les lourds de ResNet et DenseNet (march√©: R1 94,8%, mAP 84,9% pour OSNet et R1 94,8%, mAP 86,0% pour ResNet).</font></font></p><br><p> Un autre d√©fi est l' <strong>adaptation du domaine</strong> : les mod√®les form√©s sur un ensemble de donn√©es sont de mauvaise qualit√© sur un autre.  OSNet montre √©galement de bons r√©sultats dans ce segment sans l'utilisation d'une ¬´adaptation de domaine non supervis√©e¬ª (en utilisant les donn√©es de test sous une forme non allou√©e pour uniformiser la distribution des donn√©es). </p><br><p>  L'architecture a √©galement √©t√© test√©e sur ImageNet, o√π elle a atteint une pr√©cision similaire avec MobileNetV2 avec moins de param√®tres, mais plus d'op√©rations. </p><br><h3 id="9-neural-reparameterization-improves-structural-optimization">  9. La reparam√©trie neuronale am√©liore l'optimisation structurelle </h3><br><p>  Auteurs: Stephan Hoyer, Jascha Sohl-Dickstein, Sam Greydanus (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">‚Üí Article original</a> <br>  Auteur de la revue: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Alexey</a> (in Arech slack) </p><br><img src="https://habrastorage.org/webt/ga/sd/hd/gasdhdqgy5febp9elrdgdhy9nqg.png"><br><p><br>  Dans la construction et d'autres technologies, il y a des t√¢ches d'optimisation de la structure / topologie d'une solution.  En gros, il s'agit d'une r√©ponse informatique √† une question comme, par exemple, comment concevoir un pont / b√¢timent / aile d'avion / aube de turbine / blablabla, de sorte que certaines restrictions soient respect√©es et que la structure soit suffisamment solide.  Il existe un ensemble de m√©thodes de solution ¬´standard¬ª - cela fonctionne, mais tout n'est pas toujours fluide. </p><br><p>  Qu'est-ce que ces gars de Google ont trouv√©?  Ils ont dit: g√©n√©rons une solution par un r√©seau de neurones (la partie de sur√©chantillonnage de UNet), puis en utilisant un mod√®le physique diff√©renciable, qui calculera le comportement d'une solution sous l'influence de toutes les forces et de la gravit√©, calculera la fonction objective - force (plus pr√©cis√©ment, l'inverse - conformit√©) ) dessins.  Puis, comme tout est automatiquement diff√©renciable, nous obtenons le gradient de la fonction objectif, qui est repouss√© √† travers toute la structure vers les poids et l'entr√©e du r√©seau neuronal.  Nous modifions les poids et l'entr√©e et continuons le cycle jusqu'√† la convergence vers une solution stable. </p><br><p>  Les r√©sultats se sont r√©v√©l√©s concerner de petits probl√®mes (en termes de taille de l'espace des solutions possibles) comparables aux m√©thodes traditionnelles d'optimisation des topologies, et pour les gros probl√®mes, ils sont nettement meilleurs que les probl√®mes traditionnels (surpoids dans 99 contre 66 sur 116 probl√®mes).  De plus, les solutions qui en r√©sultent sont souvent beaucoup plus technologiques et optimales que les d√©cisions de r√©f√©rence. </p><br><p>  C'est-√†-dire  en fait, ils ont utilis√© le NS comme un moyen d√©licat de param√©trer le mod√®le physique de la structure, qui implicitement (gr√¢ce √† l'architecture du NS) est capable d'imposer des restrictions utiles sur les valeurs des param√®tres (contr√¥l√©es en supprimant le NS de la m√©thode et en optimisant directement les valeurs des pixels). </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code source.</a> </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un aper√ßu plus d√©taill√© de cet article sur habr.</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr472672/">https://habr.com/ru/post/fr472672/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr472658/index.html">Presque tout sur le futur HolyJS 2019 Moscou</a></li>
<li><a href="../fr472660/index.html">Comment prototyper rapidement des appareils et pourquoi c'est important. Signaler Yandex.Taxi</a></li>
<li><a href="../fr472662/index.html">Avec qui partir pour l'exportation</a></li>
<li><a href="../fr472668/index.html">Pens√©e produit. Qu'est-ce que c'est et comment le d√©velopper</a></li>
<li><a href="../fr472670/index.html">Limely automne, Limely hiver ...</a></li>
<li><a href="../fr472674/index.html">Variables d'environnement pour les projets Python</a></li>
<li><a href="../fr472676/index.html">Nous cr√©ons le d√©partement de jones pour aider les √©quipes principales, en utilisant uniquement Slack, Jira et le ruban √©lectrique bleu</a></li>
<li><a href="../fr472682/index.html">Ralentir le vieillissement avec des synergies de m√©dicaments chez C. elegans</a></li>
<li><a href="../fr472684/index.html">Envoyer une surprise √† fsync () PostgreSQL</a></li>
<li><a href="../fr472686/index.html">Studio vid√©o bas√© sur i486</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>