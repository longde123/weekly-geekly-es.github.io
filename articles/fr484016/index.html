<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø‚Äç‚öïÔ∏è üë®üèæ üöÇ Les bases de l'apprentissage en profondeur sur l'exemple de l'auto-encodeur de d√©bogage, num√©ro de pi√®ce 1 üç∑ üê± üõÅ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Si vous lisez la formation sur les auto-encodeurs sur le site keras.io, alors l'un des premiers messages il y a quelque chose comme √ßa: dans la pratiq...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les bases de l'apprentissage en profondeur sur l'exemple de l'auto-encodeur de d√©bogage, num√©ro de pi√®ce 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/484016/"><p>  Si vous lisez la formation sur les auto-encodeurs sur le site keras.io, alors l'un des premiers messages il y a quelque chose comme √ßa: dans la pratique, les auto-encodeurs ne sont presque jamais utilis√©s, mais ils sont souvent √©voqu√©s dans les formations et les gens viennent, nous avons donc d√©cid√© d'√©crire notre propre tutoriel √† leur sujet: </p><br><p>  <em>Leur principale pr√©tention √† la gloire vient de leur pr√©sence dans de nombreux cours d'initiation √† l'apprentissage automatique disponibles en ligne.</em>  <em>En cons√©quence, beaucoup de nouveaux arrivants sur le terrain adorent les auto-encodeurs et n'en ont jamais assez.</em>  <em>C'est la raison pour laquelle ce tutoriel existe!</em> </p><br><p>  N√©anmoins, l'une des t√¢ches pratiques pour lesquelles elles peuvent s'appliquer √† soi-m√™me est la recherche d'anomalies, et j'en avais personnellement besoin dans le cadre du projet du soir. </p><br><p>  Sur Internet, il <strong>existe de</strong> nombreux tutoriels sur les auto-encodeurs, pourquoi en √©crire un de plus?  Eh bien, pour √™tre honn√™te, il y avait plusieurs raisons √† cela: </p><br><ul><li>  Il y avait le sentiment qu'en fait les tutoriels √©taient d'environ 3 ou 4, tout le reste a √©t√© r√©√©crit avec leurs propres mots; </li><li>  Presque tout - sur le MNIST'e qui souffre depuis longtemps avec des images 28x28; </li><li>  √Ä mon humble avis - ils ne d√©veloppent pas une intuition sur la fa√ßon dont tout cela devrait fonctionner, mais proposent simplement de r√©p√©ter; </li><li>  Et le facteur le plus important - personnellement, lorsque j'ai remplac√© MNIST par <strong>mon propre ensemble de donn√©es - tout a stupidement cess√© de fonctionner</strong> . </li></ul><br><p>  Ce qui suit d√©crit mon chemin sur lequel les c√¥nes sont farcis.  Si vous prenez l'un des mod√®les plats propos√©s (non convolutionnels) de la masse des tutoriels et que vous les collez stupidement, alors, rien, √©tonnamment, ne fonctionne.  Le but de l'article est de comprendre pourquoi et, il me semble, d'obtenir une sorte de compr√©hension intuitive de la fa√ßon dont tout cela fonctionne. </p><br><p>  Je ne suis pas un sp√©cialiste de l'apprentissage automatique et j'utilise les approches auxquelles je suis habitu√© dans le travail quotidien.  Pour les scientifiques de donn√©es exp√©riment√©s, cet article sera probablement compl√®tement sauvage, mais pour les d√©butants, il me semble, quelque chose de nouveau pourrait surgir. </p><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">quel genre de projet</b> <div class="spoiler_text"><p>  En bref sur le projet, bien que l'article ne parle pas de lui.  Il y a un r√©cepteur ADS-B, il capte les donn√©es des avions qui volent et les √©crit, les avions, les coordonn√©es √† la base.  Parfois, les avions se comportent de fa√ßon inhabituelle - ils tournent autour pour br√ªler du carburant avant l'atterrissage, ou tout simplement des vols priv√©s survolent des itin√©raires standard (couloirs).  Il est int√©ressant d'isoler d'environ un millier d'avions par jour ceux qui ne se sont pas comport√©s comme les autres.  J'admets pleinement que les √©carts de base peuvent √™tre calcul√©s plus facilement, mais j'√©tais int√©ress√© √† essayer <del>  la magie </del>  r√©seaux de neurones. </p></div></div><br><p>  Commen√ßons.  J'ai un ensemble de 4000 images en noir et blanc 64x64 pixels, cela ressemble √† ceci: </p><br><p><img src="https://habrastorage.org/webt/vw/5r/mh/vw5rmhetruoniuc7p4ksjulshde.png"></p><br><p>  Juste quelques lignes sur fond noir, et dans l'image 64x64 environ 2% des points sont remplis.  Si vous regardez beaucoup d'images, alors, bien s√ªr, il s'av√®re que la plupart des lignes sont assez similaires. </p><br><p>  Je n'entrerai pas dans les d√©tails de la fa√ßon dont l'ensemble de donn√©es a √©t√© charg√©, trait√©, car le but de l'article, encore une fois, n'est pas celui-ci.  Montrez simplement un morceau de code effrayant. </p><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># only for google colab %tensorflow_version 2.x import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os import zipfile import datetime import tensorflow_addons as tfa BATCH_SIZE = 128 AUTOTUNE=tf.data.experimental.AUTOTUNE def load_image(fpath): img_raw = tf.io.read_file(fpath) img = tf.io.decode_png(img_raw, channels=1, dtype=tf.uint8) return tf.image.convert_image_dtype(img, dtype=tf.float32) ## for splitting test/train def is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x,y) ## for image augmentation def random_flip_flop(img): return tf.image.random_flip_left_right(img) def transform_aug(shift_val): def random_transform(img): return tfa.image.translate(img,tf.random.uniform([2], -1*shift_val, shift_val)) return random_transform def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000, transform=0, flip=False): if cache: if isinstance(cache, str): ds = ds.cache(cache) else: ds = ds.cache() ds = ds.shuffle(buffer_size=shuffle_buffer_size) if transform != 0: ds = ds.map(transform_aug(transform)) if flip: ds = ds.map(random_flip_flop) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds def prepare_input_output(x): return (x, x) list_ds = tf.data.Dataset.list_files("/content/planes64/*") imgs_df = list_ds.map(load_image) train = imgs_df.enumerate().filter(is_train).map(lambda x,y: y) train_ds = prepare_for_training(train, transform=10, flip=True) train_ds = train_ds.map(prepare_input_output) val = imgs_df.enumerate().filter(is_test).map(lambda x, y: y) val_ds = val.map(prepare_input_output).batch(BATCH_SIZE, drop_remainder=True)</span></span></code> </pre> </div></div><br><p>  Voici, par exemple, le premier mod√®le propos√© avec keras.io, sur lequel ils ont travaill√© et form√© sur mnist: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># this is the size of our encoded representations encoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats # this is our input placeholder input_img = Input(shape=(784,)) # "encoded" is the encoded representation of the input encoded = Dense(encoding_dim, activation='relu')(input_img) # "decoded" is the lossy reconstruction of the input decoded = Dense(784, activation='sigmoid')(encoded)</span></span></code> </pre> <br><p>  Dans mon cas, le mod√®le est d√©fini comme ceci: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Il y a de l√©g√®res diff√©rences que j'aplatis et remod√®le directement dans le mod√®le, et que je ¬´compresse¬ª non pas 25 fois, mais seulement 10. Cela ne devrait rien affecter. </p><br><p>  En tant que fonction de perte - erreur quadratique moyenne, l'optimiseur n'est pas fondamental, laissez Adam.  Ci-apr√®s, nous formons 20 √©poques, 100 pas par √©poque. </p><br><p>  Si vous regardez les m√©triques - tout est en feu.  Pr√©cision == 0,993.  Si vous regardez les horaires de formation - tout est un peu plus triste, nous atteignons un plateau dans la r√©gion de la troisi√®me √®re. </p><br><p><img src="https://habrastorage.org/webt/fo/rq/r7/forqr7krelrj1xq1qis_jg2jeoq.png"></p><br><p>  Eh bien, si vous regardez directement le r√©sultat de l'encodeur, vous obtenez une image g√©n√©ralement triste (l'original est en haut et le r√©sultat de l'encodage-d√©codage est ci-dessous): </p><br><p><img src="https://habrastorage.org/webt/xo/mn/oq/xomnoqmjj80uaal0echobmal9xm.png"></p><br><p>  En g√©n√©ral, lorsque vous essayez de comprendre pourquoi quelque chose ne fonctionne pas, c'est une assez bonne approche pour diviser toutes les fonctionnalit√©s en gros blocs et v√©rifier chacun d'eux isol√©ment.  Alors faisons-le. </p><br><p>  Dans l'original du tutoriel - des donn√©es plates sont fournies √† l'entr√©e du mod√®le et elles sont prises √† la sortie.  Pourquoi ne pas v√©rifier mes actions sur aplatir et remodeler.  Voici un tel mod√®le sans op√©ration: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  R√©sultat: <br><img src="https://habrastorage.org/webt/vx/z0/aj/vxz0ajadu2qiktdq5ndzj5xje8w.png"></p><br><p>  Il n'y a rien √† enseigner ici.  Eh bien, en m√™me temps, cela a prouv√© que ma fonction de visualisation fonctionne √©galement. </p><br><p>  Ensuite, essayez de rendre le mod√®le non-op, mais aussi stupide que possible - il suffit de couper la couche de compression, de laisser une couche de la taille de l'entr√©e.  Comme ils le disent dans tous les tutoriels, disent-ils, il est tr√®s important que votre mod√®le apprenne des fonctionnalit√©s, et pas seulement une fonction d'identit√©.  Eh bien, c'est exactement ce que nous allons essayer d'obtenir, passons simplement l'image r√©sultante √† la sortie. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Elle apprend quelque chose, une pr√©cision == 0.995 et encore une fois elle tombe sur un plateau. <br><img src="https://habrastorage.org/webt/ro/t6/jk/rot6jkf2ertweb7weeh8d_layui.png"></p><br><p>  Mais, en g√©n√©ral, il est clair que cela ne fonctionne pas tr√®s bien.  Quoi qu'il en soit - quoi apprendre l√†-bas, passez l'entr√©e de la sortie et c'est tout. </p><br><p>  Si vous lisez la documentation des keras sur les couches denses, elle d√©crit ce qu'elles font: <code>output = activation(dot(input, kernel) + bias)</code> <br>  Pour que la sortie co√Øncide avec l'entr√©e, deux choses simples suffisent - biais = 0 et noyau - la matrice d'identit√© (il est important de ne pas laisser la matrice remplie d'unit√©s ici - ce sont des choses tr√®s diff√©rentes).  Heureusement, ceci et cela peuvent √™tre faits assez facilement √† partir de la documentation pour le m√™me <code>Dense</code> . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation = <span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer = tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Parce que  nous r√©glons le poids tout de suite, alors vous ne pouvez rien apprendre - tout de suite c'est bien: <br><img src="https://habrastorage.org/webt/dm/tk/5a/dmtk5ardo9xksg8c5v5febzikdo.png"></p><br><p>  Mais si vous commencez √† vous entra√Æner, cela commence, √† premi√®re vue, de mani√®re surprenante - le mod√®le commence avec une pr√©cision == 1.0, mais il tombe rapidement. <br>  √âvaluez le r√©sultat avant l'entra√Ænement: <code>8/Unknown - 1s 140ms/step - loss: 0.2488 - accuracy: 1.0000[0.24875330179929733, 1.0]</code> .  Formation: </p><br><pre> <code class="plaintext hljs">Epoch 1/20 100/100 [==============================] - 6s 56ms/step - loss: 0.1589 - accuracy: 0.9990 - val_loss: 0.0944 - val_accuracy: 0.9967 Epoch 2/20 100/100 [==============================] - 5s 51ms/step - loss: 0.0836 - accuracy: 0.9964 - val_loss: 0.0624 - val_accuracy: 0.9958 Epoch 3/20 100/100 [==============================] - 5s 50ms/step - loss: 0.0633 - accuracy: 0.9961 - val_loss: 0.0470 - val_accuracy: 0.9958 Epoch 4/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0520 - accuracy: 0.9961 - val_loss: 0.0423 - val_accuracy: 0.9961 Epoch 5/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0457 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9962</code> </pre> <br><p>  Oui, et ce n'est pas tr√®s clair, nous avons d√©j√† un mod√®le id√©al - l'image sort 1 en 1, et la perte (erreur quadratique moyenne) montre presque 0,25. </p><br><p>  Soit dit en passant, c'est une question fr√©quente sur les forums - la perte diminue, mais la pr√©cision n'augmente pas, comment cela peut-il √™tre? <br>  Ici, il convient de rappeler une fois de plus la d√©finition de la couche dense: <code>output = activation(dot(input, kernel) + bias)</code> et le mot activation qui y est mentionn√©, que j'ai si bien ignor√© ci-dessus.  Avec des poids de la matrice d'identit√© et sans biais, nous obtenons <code>output = activation(input)</code> . </p><br><p>  En fait, la fonction d'activation dans notre code source est d√©j√† indiqu√©e, sigmo√Øde, je l'ai copi√©e assez b√™tement et c'est tout.  Et dans les tutoriels, il est conseill√© de l'utiliser partout.  Mais vous devez le comprendre. </p><br><p>  Pour commencer, vous pouvez lire dans la documentation ce qu'ils en disent: <code>The sigmoid activation: (1.0 / (1.0 + exp(-x)))</code> .  Personnellement, cela ne me dit rien, car je ne suis pas fant√¥me une fois pour construire de tels graphiques dans ma t√™te. <br>  Mais vous pouvez construire avec des stylos: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.sigmoid(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/tp/bs/1b/tpbs1bjym9vqahdihjhm1aubrne.png"></p><br><p>  Et ici, il devient clair qu'√† z√©ro, le sigmo√Øde prend la valeur 0,5, et dans l'unit√© - environ 0,73.  Et les points que nous avons sont noirs (0,0) ou blancs (1,0).  Il s'av√®re donc que l'erreur quadratique moyenne de la fonction d'identit√© reste non nulle. </p><br><p>  Vous pouvez m√™me regarder les stylos, voici une ligne de l'image r√©sultante: </p><br><pre> <code class="python hljs">array([<span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> ], dtype=float32)</code> </pre> <br><p>  Et c'est tout, en fait, tr√®s cool, car plusieurs questions apparaissent en m√™me temps: </p><br><ul><li>  pourquoi cela n'√©tait-il pas visible dans la visualisation ci-dessus? </li><li>  pourquoi alors pr√©cision == 1.0, car les images originales sont 0 et 1. </li></ul><br><p>  Avec la visualisation, tout est √©tonnamment simple.  Pour afficher les images, j'ai utilis√© matplotlib: <code>plt.imshow(res_imgs[i][:, :, 0])</code> .  Et, comme d'habitude, si vous allez dans la documentation, tout y sera √©crit: l' <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code> <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code>  C'est-√†-dire  la biblioth√®que a soigneusement normalis√© mes 0,5 et 0,73 dans la plage de 0 √† 1. Modifiez le code: </p><br><pre> <code class="python hljs">plt.imshow(res_imgs[i][:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>], norm=matplotlib.colors.Normalize(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/rn/qn/ta/rnqntag1dohhikail8y6myq7siy.png"></p><br><p>  Et voici la question avec pr√©cision.  Pour commencer - par habitude, nous allons dans la documentation, lisons pour <code>tf.keras.metrics.Accuracy</code> et l√†, il semble qu'ils √©crivent compr√©hensible: </p><br><pre> <code class="plaintext hljs">For example, if y_true is [1, 2, 3, 4] and y_pred is [0, 2, 3, 4] then the accuracy is 3/4 or .75.</code> </pre> <br><p>  Mais dans ce cas, notre pr√©cision aurait d√ª √™tre 0. En cons√©quence, je me suis enterr√© dans la source et c'est assez clair pour moi: </p><br><pre> <code class="plaintext hljs"> When you pass the strings 'accuracy' or 'acc', we convert this to one of `tf.keras.metrics.BinaryAccuracy`, `tf.keras.metrics.CategoricalAccuracy`, `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.</code> </pre> <br><p>  De plus, dans la documentation sur le site pour une raison quelconque, ce paragraphe ne se trouve pas dans la description de <code>.compile</code> . </p><br><p>  Voici un morceau de code de <a href="https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py">https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py</a> </p><br><pre> <code class="python hljs">y_t_rank = len(y_t.shape.as_list()) y_p_rank = len(y_p.shape.as_list()) y_t_last_dim = y_t.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] y_p_last_dim = y_p.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] is_binary = y_p_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> is_sparse_categorical = ( y_t_rank &lt; y_p_rank <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> y_t_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> y_p_last_dim &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>, <span class="hljs-string"><span class="hljs-string">'acc'</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_binary: metric_obj = metrics_mod.binary_accuracy <span class="hljs-keyword"><span class="hljs-keyword">elif</span></span> is_sparse_categorical: metric_obj = metrics_mod.sparse_categorical_accuracy <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: metric_obj = metrics_mod.categorical_accuracy</code> </pre> <br><p>  <code>y_t</code> est y_true, ou la sortie attendue, <code>y_p</code> est y_predicted, ou le r√©sultat pr√©vu. <br>  Nous avons le format de donn√©es: <code>shape=(64,64,1)</code> , il s'av√®re donc que la pr√©cision est consid√©r√©e comme binary_accuracy.  Int√©r√™t pour la fa√ßon dont il est consid√©r√©: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">binary_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred, threshold=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> threshold = math_ops.cast(threshold, y_pred.dtype) y_pred = math_ops.cast(y_pred &gt; threshold, y_pred.dtype) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(math_ops.equal(y_true, y_pred), axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  C'est dr√¥le qu'ici, nous avons juste de la chance - par d√©faut, tout est consid√©r√© comme une unit√© sup√©rieure √† 0,5, et 0,5 et inf√©rieure - z√©ro.  La pr√©cision ressort donc √† cent pour cent pour notre mod√®le d'identit√©, bien qu'en r√©alit√© les chiffres ne soient pas du tout les m√™mes.  Eh bien, il est clair que si nous le voulons vraiment, nous pouvons corriger le seuil et r√©duire la pr√©cision √† z√©ro, par exemple, seulement ce n'est pas vraiment n√©cessaire.  Il s'agit d'une mesure, elle n'affecte pas la formation, il vous suffit de comprendre que vous pouvez la calculer de mille mani√®res diff√©rentes et obtenir des indicateurs compl√®tement diff√©rents.  √Ä titre d'exemple, vous pouvez extraire diverses mesures avec des stylos et leur transf√©rer nos donn√©es: </p><br><pre> <code class="python hljs">m = tf.keras.metrics.BinaryAccuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Nous donnera <code>1.0</code> . </p><br><p>  Et ici </p><br><pre> <code class="python hljs">m = tf.keras.metrics.Accuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Nous donnera <code>0.0</code> sur les m√™mes donn√©es. </p><br><p>  Soit dit en passant, le m√™me morceau de code peut √™tre utilis√© pour jouer avec les fonctions de perte et comprendre comment elles fonctionnent.  Si vous lisez les tutoriels sur les auto-encodeurs, ils sugg√®rent essentiellement d'utiliser l'une des deux fonctions de perte: soit l'erreur quadratique moyenne, soit ¬´binary_crossentropy¬ª.  Vous pouvez √©galement les regarder en m√™me temps. </p><br><p>  Je vous rappelle que pour <code>mse</code> j'ai d√©j√† donn√© des mod√®les d' <code>evaluate</code> : </p><br><pre> <code class="plaintext hljs">8/Unknown - 2s 221ms/step - loss: 0.2488 - accuracy: 1.0000[0.24876083992421627, 1.0]</code> </pre> <br><p>  C'est-√†-dire  perte == 0,2448.  Voyons pourquoi c'est.  Il me semble personnellement que c'est le plus simple et le plus compr√©hensible: la diff√©rence entre y_true et y_predict est soustraite pixel par pixel, chaque r√©sultat est carr√©, puis la moyenne est recherch√©e. </p><br><pre> <code class="python hljs">tf.keras.backend.mean(tf.math.squared_difference(x_batch[<span class="hljs-number"><span class="hljs-number">0</span></span>], res_imgs[<span class="hljs-number"><span class="hljs-number">0</span></span>]))</code> </pre> <br><p>  Et en sortie: </p><br><pre> <code class="plaintext hljs">&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.24826494&gt;</code> </pre> <br><p>  Ici, l'intuition est tr√®s simple - la majorit√© des pixels vides, le mod√®le en produit 0,5, ils obtiennent 0,25 - de diff√©rence au carr√© pour eux. </p><br><p>  Avec crossenttrtopy binaire, les choses sont un peu plus compliqu√©es, et il y a des articles entiers sur la fa√ßon dont cela fonctionne, mais personnellement, il m'a toujours √©t√© plus facile de lire les sources, et l√†, √ßa ressemble √† ceci: </p><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> from_logits: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> isinstance(output, (ops.EagerTensor, variables_module.Variable)): output = _backtrack_identity(output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output.op.type == <span class="hljs-string"><span class="hljs-string">'Sigmoid'</span></span>: <span class="hljs-comment"><span class="hljs-comment"># When sigmoid activation function is used for output operation, we # use logits from the sigmoid function directly to compute loss in order # to prevent collapsing zero when training. assert len(output.op.inputs) == 1 output = output.op.inputs[0] return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) # Compute cross entropy from probabilities. bce = target * math_ops.log(output + epsilon()) bce += (1 - target) * math_ops.log(1 - output + epsilon()) return -bce</span></span></code> </pre> <br><p>  Pour √™tre honn√™te, j'ai creus√© la t√™te sur ces quelques lignes de code pendant tr√®s longtemps.  Premi√®rement, il est imm√©diatement clair que deux impl√©mentations peuvent fonctionner: soit <code>sigmoid_cross_entropy_with_logits</code> sera appel√©, soit la derni√®re paire de lignes fonctionnera.  La diff√©rence est que <code>sigmoid_cross_entropy_with_logits</code> fonctionne avec les logits (comme son nom l'indique, doh), et le code principal fonctionne avec les probabilit√©s. </p><br><p>  Qui sont les logits?  Si vous lisez un million d'articles diff√©rents sur le sujet, ils mentionneront des d√©finitions math√©matiques, des formules, autre chose.  En pratique, tout semble √©tonnamment simple (corrigez-moi si je me trompe).  La sortie brute de la pr√©diction est des logits.  Eh bien, ou log-odds, les cotes logarithmiques qui sont mesur√©es en <strong>log</strong> istiques sur <strong>ses</strong> - perroquets logistiques. </p><br><div class="spoiler">  <b class="spoiler_title">Il y a une petite digression - pourquoi y a-t-il des logarithmes</b> <div class="spoiler_text"><p>  Les chances sont le rapport du nombre d'√©v√©nements dont nous avons besoin au nombre d'√©v√©nements dont nous n'avons pas besoin (contrairement √† la probabilit√©, qui est le rapport des √©v√©nements dont nous avons besoin au nombre de tous les √©v√©nements en g√©n√©ral).  Par exemple - le nombre de victoires de notre √©quipe au nombre de ses d√©faites.  Et il y a un probl√®me.  Poursuivant l'exemple avec les victoires des √©quipes, notre √©quipe peut √™tre mi-perdante et avoir une chance de gagner 1/2 (un √† deux), et peut-√™tre extr√™mement perdante - et avoir une chance de gagner 1/100.  Et dans la direction oppos√©e - moyennement raide et 2/1, plus raide que les plus hautes montagnes - puis 100/1.  Et il s'av√®re que toute la gamme des √©quipes perdantes est d√©crite par des nombres de 0 √† 1, et des √©quipes sympas - de 1 √† l'infini.  Par cons√©quent, il n'est pas pratique de comparer, il n'y a pas de sym√©trie, travailler avec cela en g√©n√©ral est g√™nant pour tout le monde, les math√©matiques sont laides.  Et si vous prenez le logarithme des cotes, alors tout devient sym√©trique: </p><br><pre> <code class="plaintext hljs">ln(1/2) == -0.69 ln(2/1) == 0.69 ln(1/100) == -4.6 ln(100/1) == 4.6</code> </pre> </div></div><br><p>  Dans le cas du tensorflow, cela est plut√¥t arbitraire, car, √† proprement parler, la sortie de la couche n'est math√©matiquement pas des cotes logarithmiques, mais elle est d√©j√† accept√©e.  Si la valeur brute est de -‚àû √† + ‚àû - alors logits.  Ils peuvent ensuite √™tre convertis en probabilit√©s.  Il y a deux options pour cela: softmax et son cas sp√©cial, sigmoid.  Softmax - Prenez un vecteur de logits et convertissez-les en un vecteur de probabilit√©s, et m√™me de sorte que la somme de la probabilit√© de tous les √©v√©nements qu'il contient se r√©v√®le √™tre 1. Sigmoid (dans le cas de tf) prend √©galement un vecteur de logits, mais convertit chacun d'eux en probabilit√©s s√©par√©ment, ind√©pendamment du reste. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1+ln(0.5) == 0.30685281944 tf.math.softmax(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.25, 0.5 , 0.25], dtype=float32)&gt; tf.math.sigmoid(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.57611686, 0.7310586 , 0.57611686], dtype=float32)&gt;</span></span></code> </pre> <br><p>  Vous pouvez le voir de cette fa√ßon.  Il y a des t√¢ches de classification multi-√©tiquettes, il y a des t√¢ches de classification multi-classes.  Multiclasse - c'est si vous avez besoin de d√©terminer les pommes dans l'image ou les oranges, et peut-√™tre m√™me les ananas.  Et multilabel, c'est quand il peut y avoir un vase de fruits sur la photo et vous devez dire qu'il y a des pommes et des oranges dessus, mais qu'il n'y a pas d'ananas.  Si nous voulons une multiclasse - nous avons besoin de softmax, si nous voulons une multi-√©tiquette - nous avons besoin de sigmoid. <br>  Ici, nous avons le cas du multilabel - il est n√©cessaire que chaque pixel individuel (classe) dise s'il est install√©. </p><br><p>  Revenons au tensorflow et pourquoi dans la crossentropie binaire (au moins dans les autres fonctions de crossentropie c'est √† peu pr√®s la m√™me), il y a deux branches globales.  La crossentropie fonctionne toujours avec des probabilit√©s, nous en reparlerons un peu plus tard.  Ensuite, il y a simplement deux fa√ßons: soit les probabilit√©s entrent d√©j√† dans l'entr√©e, soit les logits arrivent √† l'entr√©e - puis sigmo√Øde leur est d'abord appliqu√© pour obtenir la probabilit√©.  Il se trouve que l'application de sigmo√Øde et le calcul de l'entropie crois√©e se sont av√©r√©s meilleurs que le simple calcul de l'entropie crois√©e √† partir des probabilit√©s (la source de la fonction <code>sigmoid_cross_entropy_with_logits</code> a une conclusion math√©matique, plus pour les curieux, vous pouvez google `` entropie crois√©e de stabilit√© num√©rique ''), donc m√™me les d√©veloppeurs de tensorflow recommandent de ne pas transmettre la probabilit√© √† saisir des fonctions de crossentropie et rendre des logits bruts.  Eh bien, dans le code, les fonctions de perte sont v√©rifi√©es si la derni√®re couche est sigmo√Øde, puis elles la coupent et prennent l'entr√©e d'activation, plut√¥t que sa sortie, √† calculer, envoyant tout √† prendre en compte dans <code>sigmoid_cross_entropy_with_logits</code> . </p><br><p>  D'accord, tri√©, maintenant binary_crossentropy.  Il existe deux explications ¬´intuitives¬ª populaires qui mesurent l'entropie crois√©e. </p><br><p>  Plus formel: imaginez qu'il existe un certain mod√®le qui pour n classes conna√Æt la probabilit√© de leur occurrence (y <sub>0</sub> , y <sub>1</sub> , ..., y <sub>n</sub> ).  Et maintenant dans la vie, chacune de ces classes a surgi k <sub>n</sub> fois (k <sub>1</sub> , k <sub>1</sub> , ..., k <sub>n</sub> ).  La probabilit√© d'un tel √©v√©nement est le produit de la probabilit√© pour chaque classe individuelle - (y <sub>1</sub> ^ k <sub>1</sub> ) (y <sub>2</sub> ^ k <sub>2</sub> ) ... (y <sub>n</sub> ^ k <sub>n</sub> ).  En principe - il s'agit d√©j√† d'une d√©finition normale de l'entropie crois√©e - la probabilit√© d'un ensemble de donn√©es est exprim√©e en termes de probabilit√© d'un autre ensemble de donn√©es.  Le probl√®me avec cette d√©finition est qu'elle se r√©v√©lera √™tre de 0 √† 1 et sera souvent tr√®s petite; il n'est pas pratique de comparer de telles valeurs. <br>  Si nous prenons le logarithme de cela, alors k <sub>1</sub> log (y <sub>1</sub> ) + k <sub>2</sub> log (y <sub>2</sub> ) sortira et ainsi de suite.  La plage de valeurs devient de -‚àû √† 0. Multipliez tout cela par -1 / n - et la plage de 0 √† + ‚àû appara√Æt, en outre, parce que  il est exprim√© comme la somme des valeurs pour chaque classe, le changement dans chaque classe se refl√®te dans la valeur globale d'une mani√®re tr√®s pr√©visible. </p><br><p>  Plus simple: l'entropie crois√©e montre combien de bits suppl√©mentaires sont n√©cessaires pour exprimer l'√©chantillon en termes de mod√®le d'origine.  Si nous √©tions l√† pour faire un logarithme avec la base 2, alors nous irions directement en bits.  Nous utilisons des logarithmes naturels partout, donc ils montrent le nombre de nat ( <a href="https://en.wikipedia.org/wiki/Nat_(unit">https://en.wikipedia.org/wiki/Nat_(unit</a> )), pas de bits. </p><br><p>  L'entropie crois√©e binaire, √† son tour, est un cas particulier de l'entropie crois√©e ordinaire, lorsque le nombre de classes est de deux.  Ensuite, nous avons suffisamment de connaissances sur la probabilit√© d'occurrence d'une classe - y <sub>1</sub> , et la probabilit√© d'occurrence de la seconde sera (1-y <sub>1</sub> ). </p><br><p>  Mais, il me semble, un peu d√©rap√©.  Permettez-moi de vous rappeler que la derni√®re fois que nous avons essay√© de construire un encodeur automatique d'identit√©, il nous a montr√© une belle image et m√™me une pr√©cision de 1,0, mais en fait, les chiffres se sont r√©v√©l√©s horribles.  Pour les besoins de l'exp√©rience, vous pouvez effectuer quelques tests suppl√©mentaires: <br>  1) l'activation peut √™tre supprim√©e, il y aura une identit√© propre <br>  2) vous pouvez essayer d'autres fonctions d'activation, par exemple la m√™me relu </p><br><p>  Sans activation: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Nous obtenons le mod√®le d'identit√© parfait: </p><br><pre> <code class="python hljs">model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  La formation, en passant, ne m√®nera √† rien, car la perte == 0,0. </p><br><p>  Maintenant avec relu.  Son graphique ressemble √† ceci: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.relu(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">1</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/wq/ph/iw/wqphiwwmhtmxwfogld4nstyfp9w.png"></p><br><p>  En dessous de z√©ro - z√©ro, au dessus - y = x, c'est-√†-dire  en th√©orie, nous devrions obtenir le m√™me effet qu'en l'absence d'activation - un mod√®le id√©al. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">"adam"</span></span>, loss=<span class="hljs-string"><span class="hljs-string">"binary_crossentropy"</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>]) model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 158ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  D'accord, nous avons compris le mod√®le d'identit√©, m√™me avec une partie de la th√©orie, il est devenu plus clair.  Essayons maintenant de former le m√™me mod√®le pour qu'il devienne identit√©. </p><br><p>  Pour le plaisir, je vais mener cette exp√©rience sur trois fonctions d'activation.  Pour commencer - relu, car il s'est montr√© bien plus t√¥t (tout est comme avant, mais le kernel_initializer est supprim√©, donc par d√©faut ce sera <code>glorot_uniform</code> ): </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Il apprend √† merveille: </p><br><p><img src="https://habrastorage.org/webt/hv/wz/oo/hvwzoodkp6dxopq7pjkopeeuorw.png"></p><br><p>  Le r√©sultat √©tait assez bon, pr√©cision: 0,9999, perte (mse): 2e-04 apr√®s 20 √©poques et vous pouvez vous entra√Æner plus loin. </p><br><p><img src="https://habrastorage.org/webt/4y/io/jt/4yiojttsafqnf796ha6all0qmsy.png"></p><br><p>  Ensuite, essayez avec sigmoid: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  J'ai d√©j√† enseign√© quelque chose de similaire auparavant, √† la seule diff√©rence que le biais est d√©sactiv√© ici.  Il √©tudie abondamment, monte sur un plateau dans la r√©gion de la 50e √®re, pr√©cision: 0.9970, perte: 0.01 apr√®s 60 √©poques. </p><br><p>  Le r√©sultat n'est pas encore impressionnant: </p><br><p><img src="https://habrastorage.org/webt/_7/fl/o_/_7flo_xh5gkgh8oennqthe2ymhk.png"></p><br><p>  Eh bien, v√©rifiez √©galement tanh: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Le r√©sultat est comparable √† relu - pr√©cision: 0,9999, perte: 6e-04 apr√®s 20 √©poques, et vous pouvez vous entra√Æner davantage: </p><br><p><img src="https://habrastorage.org/webt/m4/ib/xc/m4ibxctlge5cxs7eqozjt5uwfsc.png"></p><br><p><img src="https://habrastorage.org/webt/sk/r3/p8/skr3p8etvlatcf-mnc6q9sabtfk.png"></p><br><p>  En fait, je suis tourment√© par la question de savoir si quelque chose peut √™tre fait pour que sigmo√Øde affiche un r√©sultat comparable.  Exclusivement par int√©r√™t sportif. </p><br><p>  Par exemple, vous pouvez essayer d'ajouter BatchNormalization: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.BatchNormalization()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Et puis une sorte de magie se produit.  √Ä la 13e √®re, pr√©cision: 1.0.  Et les r√©sultats ardents: </p><br><p><img src="https://habrastorage.org/webt/6x/md/di/6xmddijppdnstc8ire1mxlcwkca.png"></p><br><p>  III ... sur cette falaise, je terminerai la premi√®re partie, parce que le texte est trop dofig, et on ne sait pas si quelqu'un en a besoin ou non.  Dans la deuxi√®me partie, je vais comprendre ce qui s'est pass√©, exp√©rimenter diff√©rents optimiseurs, essayer de construire un encodeur-d√©codeur honn√™te, me cogner la t√™te sur la table.  J'esp√®re que quelqu'un √©tait int√©ress√© et utile. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr484016/">https://habr.com/ru/post/fr484016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr484004/index.html">@Pythonetc d√©cembre 2019</a></li>
<li><a href="../fr484006/index.html">Trucs et astuces de ma cha√Æne Telegram @pythonetc, d√©cembre 2019</a></li>
<li><a href="../fr484008/index.html">Qu'est-ce qu'un chef d'√©quipe</a></li>
<li><a href="../fr484012/index.html">Rationalisez le processus d'√©criture dans un bloc-notes</a></li>
<li><a href="../fr484014/index.html">10 mythes SEO √† laisser derri√®re eux en 2020</a></li>
<li><a href="../fr484018/index.html">C√¥t√© technique informatique du yachting</a></li>
<li><a href="../fr484020/index.html">Qui essayez-vous d'impressionner avec vos d√©lais?</a></li>
<li><a href="../fr484026/index.html">Partie 6: Portage de MemTest86 + vers RISC-V</a></li>
<li><a href="../fr484028/index.html">Horseshoe Bend - tablette convertible avec √©cran rabattable</a></li>
<li><a href="../fr484034/index.html">Mise en ≈ìuvre du plan de travail de stockage cibl√© des marchandises bas√© sur l'unit√© de comptabilit√© d'entrep√¥t ¬´1C Integrated Automation 2¬ª</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>