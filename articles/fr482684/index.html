<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîì ü§òüèΩ üíì J'ai cr√©√© mon propre dipfake en deux semaines et 552 $ üê∏ üêï üë©‚Äçüè´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En cr√©ant cette vid√©o, j'ai beaucoup appris 

 La technologie Dipfake utilise des r√©seaux de neurones profonds pour remplacer de mani√®re convaincante ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>J'ai cr√©√© mon propre dipfake en deux semaines et 552 $</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482684/"><h3>  En cr√©ant cette vid√©o, j'ai beaucoup appris </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/N6y0CA2Mcx8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <a href="https://ru.wikipedia.org/wiki/Deepfake" rel="nofollow">La</a> technologie <a href="https://ru.wikipedia.org/wiki/Deepfake" rel="nofollow">Dipfake</a> utilise <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BB%25D1%2583%25D0%25B1%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5" rel="nofollow">des r√©seaux de neurones profonds</a> pour remplacer de mani√®re convaincante une personne par une autre en vid√©o.  Cette technologie pr√©sente un potentiel √©vident d'utilisation malveillante, et elle devient de plus en plus courante.  Concernant les cons√©quences sociales et politiques de cette tendance, de nombreux bons articles ont d√©j√† √©t√© √©crits. <br><br>  Et ce n'est pas l'un d'eux.  Au lieu de cela, j‚Äôexaminerai de plus pr√®s cette technologie: comment fonctionne le logiciel diphey?  Est-il difficile de les cr√©er et √† quel point les r√©sultats sont-ils bons? <br><br>  J'ai d√©cid√© qu'il √©tait pr√©f√©rable de r√©pondre √† ces questions en cr√©ant ma propre vid√©o dip.  Les √©diteurs m'ont donn√© quelques jours pour jouer avec le logiciel et 1000 $ pour payer le cloud computing.  Apr√®s quelques semaines, j'ai obtenu le r√©sultat pr√©sent√© dans la vid√©o au d√©but de l'article.  J'ai commenc√© avec une vid√©o de Mark Zuckerberg s'adressant au Congr√®s et j'ai remplac√© son visage par le Lieutenant Commander Data (Brent Spiner) de Star Trek: The Next Generation.  Un total de 552 $ a √©t√© d√©pens√©. <br><a name="habracut"></a><br>  La vid√©o n'√©tait pas parfaite.  Tous les d√©tails du visage de Data ne sont pas transmis, et si vous regardez de pr√®s, des artefacts sont visibles sur les bords. <br><br>  Pourtant, il est tout √† fait remarquable qu'un nouveau venu comme moi puisse cr√©er une vid√©o convaincante, et ce, rapidement et √† moindre co√ªt.  Il y a tout lieu de croire que la technologie dipfeyk dans les ann√©es √† venir ne fera que s'am√©liorer, plus vite et moins cher. <br><br>  Dans cet article, je vais vous guider par la main dans mon parcours dipfake.  Je vais vous expliquer toutes les √©tapes √† suivre pour cr√©er une vid√©o deepfake.  En cours de route, je vais expliquer comment cette technologie fonctionne et quelles sont ses limites. <br><br><h2>  Les dipfeyks ont besoin de beaucoup de puissance de calcul et de donn√©es </h2><br>  Nous appelons ces vid√©os diphakes [¬´fausses profondes¬ª] parce qu'elles sont cr√©√©es en utilisant des r√©seaux de neurones profonds.  Au cours de la derni√®re d√©cennie, les informaticiens ont d√©couvert que les r√©seaux de neurones deviennent plus puissants avec l'ajout de couches suppl√©mentaires de neurones.  Mais pour lib√©rer le plein potentiel des r√©seaux de neurones profonds, vous avez besoin de beaucoup de donn√©es et d'une √©norme puissance de calcul. <br><br>  Il en va de m√™me pour les dipfakes.  Pour ce projet, j'ai lou√© une machine virtuelle avec quatre puissantes cartes graphiques.  Et m√™me avec tous ces chevaux, il m'a fallu pr√®s d'une semaine pour entra√Æner mon mannequin. <br><br>  J'avais aussi besoin d'une montagne d'images de Mark Zuckerberg et Data.  J'ai eu une vid√©o de 38 secondes, mais pour m'entra√Æner, j'avais besoin de vid√©os beaucoup plus longues, √† la fois Zuckerberg et Data. <br><br>  Pour ce faire, j'ai t√©l√©charg√© un tas de vid√©os contenant leurs visages: 14 clips avec des clips de Star Trek et neuf clips avec Mark Zuckerberg.  Parmi ces derniers figuraient des reportages officiels, plusieurs interviews √† la t√©l√©vision et m√™me une vid√©o dans laquelle Zuckerberg pr√©parait un barbecue dans sa cour. <br><br>  J'ai t√©l√©charg√© tous ces clips sur iMovie et supprim√© des images qui ne contenaient pas les visages de Zuckerberg et de Data.  J'ai √©galement coup√© en morceaux les passages les plus longs.  Un programme dipfake a besoin non seulement d'un grand nombre d'images, mais d'un grand nombre d'images diff√©rentes.  Nous avions besoin de photographier des visages sous diff√©rents angles, avec des expressions diff√©rentes et sous un √©clairage diff√©rent.  Une vid√©o d'une heure dans laquelle Zuckerberg lit le rapport ne peut pas produire de clich√©s plus pr√©cieux qu'un segment de cinq minutes, car il est tourn√© sous le m√™me angle, sous la m√™me lumi√®re et montre la m√™me expression faciale.  J'ai donc recadr√© quelques heures de vid√©o √† 9 minutes avec Data et jusqu'√† 7 minutes avec Zuckerberg. <br><br><h2>  Faceswap: un logiciel pour cr√©er des dipfakes </h2><br>  Ensuite, il est temps d'utiliser le logiciel pour dipheyka.  Au d√©but, j'ai essay√© d'utiliser le programme DeepFaceLab et j'ai pu cr√©er une vid√©o assez grossi√®re.  Puis j'ai demand√© conseil sur le forum SFWdeepfakes, puis quelques personnes m'ont conseill√© sur Faceswap.  Les gens ont not√© que ce programme a plus de fonctionnalit√©s, une meilleure documentation et un meilleur support en ligne.  J'ai d√©cid√© de suivre leurs conseils. <br><br>  Faceswap fonctionne sur Linux, Windows et Mac.  Le package contient des outils pour travailler √† toutes les √©tapes de la cr√©ation d'un dipfake, de l'importation des vid√©os originales √† la cr√©ation d'une vid√©o dipfake termin√©e.  Le logiciel n'est pas intuitif, mais il est accompagn√© d'un mat√©riel de formation d√©taill√© qui couvre toutes les √©tapes du processus.  Le mat√©riel a √©t√© √©crit par le cr√©ateur de Faceswap, Matt Torah, qui m'a √©galement beaucoup aid√© √† discuter sur la cha√Æne Deepfake de Discord. <br><br>  Faceswap n√©cessite une carte graphique puissante.  Je savais que mon MacBook Pro ne pouvait pas le supporter.  J'ai demand√© aux techniciens de notre r√©daction de me louer une machine virtuelle pour Linux aupr√®s d'un fournisseur leader de services cloud.  J'ai commenc√© avec une machine virtuelle avec un GPU Nvidia K80 et 12 Go de m√©moire vid√©o.  Quelques jours plus tard, je suis pass√© √† un mod√®le avec deux GPU, puis √† 4 GPU.  Elle avait quatre GPU Nvidia T4 Tensor Core avec 16 Go de m√©moire chacun (et encore 48 CPU et 192 RAM, qui √©taient pour la plupart inactifs). <br><br>  Apr√®s deux semaines de travail, j'ai re√ßu une facture de 522 $.  Bien s√ªr, j'ai d√©pens√© une somme assez importante pour la location d'un ordinateur.  La Torah m'a dit qu'en ce moment, l'option mat√©rielle la plus rentable pour un dipfake est une carte Nvidia GTX 1070 ou 1080 avec 8 Go de m√©moire.  Une telle carte utilis√©e vaut plusieurs centaines de dollars.  Une carte 1080 n'enseigne pas un r√©seau de neurones aussi vite que quatre de mes GPU, mais si vous √™tes pr√™t √† attendre quelques semaines, vous obtiendrez des r√©sultats similaires. <br><br>  Le workflow dans Faceswap se compose de trois √©tapes de base: <br><br><ul><li>  Extraction: coupez la vid√©o en images, trouvez des visages dans chaque image, affichez des images bien align√©es et soigneusement recadr√©es de chaque visage. </li><li>  Entra√Ænement: utilisez les images obtenues pour entra√Æner le r√©seau neuronal dipfake.  Il prend une image du visage d'une personne et produit une image du visage d'une autre personne avec la m√™me expression, l'√©clairage et la m√™me position. </li><li>  Transformation: appliquez le mod√®le form√© √† l'√©tape pr√©c√©dente √† une vid√©o sp√©cifique pour donner un dipfake.  Apr√®s avoir form√© le mod√®le, il peut √™tre appliqu√© √† n'importe quelle vid√©o dans laquelle ces personnes sont pr√©sentes sur les visages desquelles il a √©t√© form√©. </li></ul><br>  Pour chacune des trois √©tapes, un temps compl√®tement diff√©rent est requis de la personne et de la machine.  Le logiciel de r√©cup√©ration d'images fonctionne pendant plusieurs minutes, mais il peut prendre des heures √† une personne pour v√©rifier les r√©sultats.  Le logiciel note tous les visages de chaque image, ainsi que quelques faux positifs.  Pour obtenir de bons r√©sultats, une personne doit passer par tous les r√©sultats, en supprimant les visages inutiles et tout ce que le logiciel a pris pour une personne. <br><br>  L'apprentissage est facile √† mettre en place et ne n√©cessite pratiquement aucune implication humaine.  Cependant, cela peut prendre des jours, voire des semaines, pour obtenir de bons r√©sultats.  J'ai commenc√© √† entra√Æner mon mod√®le final le 7 d√©cembre, et cela a fonctionn√© jusqu'au 13 d√©cembre.  Il est possible qu'apr√®s une autre semaine de travail, la qualit√© de mon dipfake s'am√©liore.  Et j'ai √©galement utilis√© mon monstre cloud avec quatre cartes graphiques avanc√©es.  Si vous travaillez sur votre ordinateur avec un seul GPU de moindre puissance, cela peut prendre plusieurs semaines pour former un bon mod√®le. <br><br>  La derni√®re √©tape, la transformation, est rapide pour une personne et un ordinateur.  En recevant un mod√®le correctement form√©, vous pouvez livrer des vid√©os dipfake en moins d'une minute. <br><br><h2>  Comment fonctionnent les diphakes </h2><br>  Avant de d√©crire le processus d'apprentissage Faceswap, vous devez expliquer comment fonctionne la technologie sous-jacente. <br><br>  Au c≈ìur de Faceswap - et d'autres progiciels de premier plan pour cr√©er des diphas√©s - se trouve l'auto-encodeur.  Il s'agit d'un r√©seau neuronal form√© pour recevoir une image d'entr√©e et produire une image identique.  Cette comp√©tence en elle-m√™me n'est peut-√™tre pas si utile, mais, comme nous le verrons plus tard, c'est un √©l√©ment cl√© du processus de cr√©ation d'un dipfake. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/63f/e44/76b/63fe4476bd395c863cd0c9b71bf4cfbf.jpg"><br><br>  L'encodeur automatique est structur√© selon le principe de deux entonnoirs reli√©s par une extr√©mit√© √©troite.  D'un c√¥t√© du r√©seau se trouve un encodeur qui re√ßoit une image et la compresse en un petit nombre de variables.  Dans le mod√®le que j'ai utilis√© dans Faceswap, il s'agit de 1024 nombres √† virgule flottante 32 bits.  De l'autre c√¥t√© du r√©seau neuronal se trouve un d√©codeur.  Il prend cette repr√©sentation compacte, dite ¬´espace latent¬ª, et essaie de l'agrandir apr√®s avoir re√ßu l'image initiale. <br><br>  En limitant artificiellement la quantit√© de donn√©es transmises de l'encodeur au d√©codeur, ces deux r√©seaux d√©veloppent une repr√©sentation compacte du visage humain.  Un encodeur est quelque chose comme un algorithme de compression avec perte qui essaie d'enregistrer autant d'informations que possible sur un visage tout en limitant la quantit√© de stockage.  L'espace latent doit en quelque sorte extraire des d√©tails importants, par exemple, dans quelle direction le sujet regarde, ses yeux sont ouverts ou ferm√©s, il sourit ou fronce les sourcils. <br><br>  Il est important que l'auto-encodeur ne doive enregistrer que les traits du visage qui changent avec le temps.  Il n'a pas besoin de stocker des choses inchang√©es telles que la couleur des yeux ou la forme du nez.  S'il a les yeux bleus dans chaque photographie de Zuckerberg, alors son d√©codeur r√©seau apprendra √† dessiner automatiquement son visage aux yeux bleus.  Il n'est pas n√©cessaire d'entasser des informations dans un espace latent restreint qui ne change pas pendant la transition d'une image √† une autre.  Comme nous le verrons plus loin, le fait que les auto-encodeurs ont des attitudes diff√©rentes face aux caract√©ristiques faciales constantes et changeantes est extr√™mement important pour leur capacit√© √† √©mettre des diphfakes. <br><br>  Chaque algorithme de formation d'un r√©seau de neurones a besoin d'un moyen d'√©valuer la qualit√© du r√©seau afin qu'il puisse √™tre am√©lior√©.  Dans de nombreux cas, cela se fait par le biais d'une formation avec l'enseignant, lorsque la personne fournit la bonne r√©ponse pour chaque √©l√©ment √† partir de l'ensemble des donn√©es de formation.  Les encodeurs automatiques fonctionnent diff√©remment.  Puisqu'ils essaient simplement de reproduire leurs propres donn√©es d'entr√©e, le logiciel de formation peut juger automatiquement de leur qualit√© de travail.  Dans le jargon de l'apprentissage automatique, cela s'appelle l'apprentissage sans professeur. <br><br>  Comme tout r√©seau de neurones, les auto-encodeurs de Faceswap sont form√©s √† l'aide de la r√©tropropagation.  L'algorithme d'apprentissage alimente une image sp√©cifique dans le r√©seau neuronal et examine quels pixels de la sortie ne correspondent pas √† l'entr√©e.  Il calcule ensuite lequel des neurones de la derni√®re couche a le plus contribu√© aux erreurs et corrige l√©g√®rement les param√®tres de chaque neurone afin qu'il donne de meilleurs r√©sultats. <br><br>  Ensuite, ces erreurs se propagent √† la couche pr√©c√©dente, o√π les param√®tres de chaque neurone sont √† nouveau corrig√©s.  Les erreurs se propagent ainsi plus loin en arri√®re jusqu'√† ce que chacun des param√®tres du r√©seau neuronal - √† la fois le codeur et le d√©codeur - soit corrig√©. <br><br>  Ensuite, l'algorithme de formation alimente une autre image du r√©seau, et l'ensemble du processus est r√©p√©t√© √† nouveau.  Des centaines de milliers de ces r√©p√©titions peuvent √™tre n√©cessaires pour cr√©er un encodeur automatique qui reproduit bien sa propre entr√©e. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/335/661/bb1/335661bb13300b80164987a1aa1dd420.jpg"><br><br>  Le logiciel Dipfake fonctionne en entra√Ænant simultan√©ment deux auto-encodeurs, un pour le visage d'origine et le second pour le nouveau.  Pendant le processus de formation, chaque encodeur automatique re√ßoit des images d'une seule personne et il est form√© pour produire des images tr√®s similaires √† l'original. <br><br>  Il y a cependant un hic: les deux r√©seaux utilisent le m√™me encodeur.  Les d√©codeurs - les neurones du c√¥t√© droit du r√©seau - restent s√©par√©s, et chacun d'eux est form√© pour donner un visage diff√©rent.  Mais les neurones du c√¥t√© gauche du r√©seau ont des param√®tres communs qui changent chaque fois que l'un des auto-encodeurs est entra√Æn√©.  Lorsque le r√©seau Zuckerberg est form√© sur la face Zuckerberg, cela change la moiti√© du r√©seau appartenant √† l'encodeur et dans le r√©seau pour les donn√©es.  Chaque fois que le r√©seau de Data est form√© sur le visage de Data, l'encodeur Zuckerberg h√©rite de ces changements. <br><br>  Par cons√©quent, deux auto-encodeurs ont un encodeur commun qui peut ¬´lire¬ª le visage de Zuckerberg ou le visage de Data.  Le but de l'encodeur est d'utiliser la m√™me repr√©sentation de choses telles que l'angle de la t√™te ou l'emplacement des sourcils, qu'il ait re√ßu une photo de Zuckerberg ou une photo de Data en entr√©e.  Et cela, √† son tour, signifie que lorsque vous serrez votre visage avec l'encodeur, vous pouvez le d√©baller √† l'aide de n'importe quel d√©codeur. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce8/0b6/670/ce80b6670b49be4b905f9ce520810931.jpg"><br><br>  Donc, apr√®s avoir form√© quelques auto-encodeurs de cette mani√®re, il reste une √©tape simple de cr√©ation d'un faux-dip: vous √©changez des d√©codeurs.  Vous encodez une photo Zuckerberg, mais utilisez le d√©codeur de donn√©es √† l'√©tape de d√©codage.  Le r√©sultat est une photographie reconstruite de Data - mais avec la m√™me position de la t√™te et la m√™me expression faciale que la photographie originale de Zuckerberg. <br><br>  Rappelez-vous, j'ai mentionn√© que l'espace latent capture les caract√©ristiques faciales variables d'une personne - expression, direction de la vue, emplacement des sourcils - et des choses constantes comme la couleur des yeux ou la forme de la bouche donne le d√©codeur.  Cela signifie que si vous encodez l'image Zuckerberg, puis la d√©codez √† l'aide du d√©codeur de donn√©es, vous obtiendrez un visage avec des fonctionnalit√©s de donn√©es permanentes - par exemple, une forme de visage - mais avec l'expression et l'orientation de la face Zuckerberg d'origine. <br><br>  En appliquant cette technique aux images successives d'une vid√©o avec Zuckerberg, vous obtenez une nouvelle vid√©o o√π le visage de Data effectue les m√™mes mouvements - sourit, clignote, tourne la t√™te - que Zuckerberg a fait dans la vid√©o originale. <br><br>  Cette situation est sym√©trique.  Lorsque vous entra√Ænez un r√©seau de neurones pour recevoir une photo de Zuckerberg et √©mettre une photo de Data, vous l'entra√Ænez simultan√©ment pour recevoir une photo de Data et √©mettre une photo de Zuckerberg.  L'outil de conversion des vid√©os de Faceswap - la derni√®re √©tape du processus de cr√©ation d'un dipfake - comprend une case √† cocher ¬´swap models¬ª, permettant √† l'utilisateur d'√©changer les d√©codeurs.  En cons√©quence, au lieu de remplacer le visage de Data √† la place du visage de Zuckerberg, le programme fait le contraire, produisant des vid√©os tr√®s dr√¥les comme celle-ci: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Fi7sj7SU1Vg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Donn√©es d'entra√Ænement </h2><br>  En pratique, obtenir de bons r√©sultats lors de la cr√©ation d'un dipfake n'est pas facile. <br><br>  Comme je l'ai mentionn√©, j'ai obtenu sept minutes de vid√©o pour Data et neuf minutes pour Zuckerberg.  Ensuite, j'ai utilis√© l'outil d'extraction d'image Faceswap pour couper la vid√©o et obtenir des images recadr√©es des visages des deux hommes.  La vid√©o contient environ 30 images par seconde, mais je n'en ai extrait qu'une fois sur six - cette pratique est recommand√©e dans la documentation Faceswap.  En effet, une vari√©t√© d'images signifie plus que leur nombre, et l'enregistrement de chaque image entra√Ænerait un grand nombre d'images tr√®s similaires. <br><br>  L'outil d'extraction Faceswap a produit beaucoup de faux positifs.  Il a √©galement trouv√© de vrais visages √† l'arri√®re-plan de certains plans.  Pendant quelques heures, j'ai supprim√© manuellement toutes les photos extraites qui n'appartenaient √† aucun de mes deux sujets exp√©rimentaux.  En cons√©quence, j'ai obtenu 2598 images du visage de Data et 2224 images du visage de Zuckerberg. <br><br>  Et √† ce moment, enfin, il √©tait temps de passer √† une v√©ritable formation de mod√®le.  Maintenant Faceswap est livr√© avec 10 algorithmes dipfake diff√©rents qui prennent en charge diff√©rentes tailles d'image et n√©cessitent diff√©rentes puissances de calcul.  Parmi les plus sans pr√©tention, il existe un mod√®le ¬´l√©ger¬ª qui fonctionne avec des images de visage de 64 pixels maximum.  Il peut √™tre ex√©cut√© sur une machine avec pas plus de 2 Go de m√©moire vid√©o.  D'autres mod√®les fonctionnent avec des images de 128, 256 ou m√™me 512 pixels - cependant, ils n√©cessitent beaucoup plus de m√©moire vid√©o, ainsi que plus de temps d'entra√Ænement. <br><br>  J'ai commenc√© √† former le mod√®le DFL-SAE, d√©riv√© d'algorithmes de DeepFaceLab.  Cependant, il y avait un avertissement dans la documentation Faceswap que ce mod√®le souffre d'une ¬´fuite d'identit√©¬ª dans laquelle certaines caract√©ristiques d'une face peuvent s'infiltrer dans une autre.  Il m'a sembl√© que j'avais vu quelque chose comme √ßa dans quelques-unes des premi√®res vid√©os de test, donc un jour plus tard, je suis pass√© au mod√®le Villain, qui fonctionne avec des images √† 128 pixels.  Le manuel Faceswap le d√©crit comme tr√®s exigeant en VRAM, et comme "un bon choix pour ceux qui veulent obtenir un mod√®le de r√©solution plus √©lev√©e sans ajuster aucun param√®tre". <br><br>  Alors j'ai attendu.  Et il a attendu.  Le processus d'apprentissage n'√©tait pas encore termin√© lorsque ma date limite est venue vendredi - et ce apr√®s six jours de formation.  A cette √©poque, mon mod√®le a produit un tr√®s bon dipfake.  La vitesse de progression a ralenti, mais il est possible que j'aurais obtenu un meilleur r√©sultat si j'avais eu une autre semaine de temps informatique. <br><br>  Faceswap est bien adapt√© pour les longs travaux informatiques.  Si vous d√©marrez l'√©quipe de formation √† partir de l'interface graphique, l'interface du programme met r√©guli√®rement √† jour l'√©cran de pr√©visualisation, o√π vous pouvez voir des exemples de la fa√ßon dont le logiciel cr√©e des portraits de Data et Zuckerberg.  Si vous pr√©f√©rez effectuer une formation √† partir de la ligne de commande, cela est √©galement possible.  L'interface Faceswap poss√®de un bouton ¬´g√©n√©rer¬ª utile qui donne la commande exacte que vous devez ex√©cuter pour entra√Æner le mod√®le avec les param√®tres actuels d√©finis dans l'interface. <br><br><h2>  Quelle √©tait la qualit√© du dipfake? </h2><br>  Dans le processus d'apprentissage, Faceswap affiche en permanence une estimation num√©rique de la ¬´perte¬ª pour chacun des deux auto-encodeurs.  Ces estimations montrent dans quelle mesure l‚Äôencodeur automatique de Zuckerberg peut lire les photos de Zuckerberg - et dans quelle mesure l‚Äôencodeur automatique de Data peut lire les photos de Data.  Et ces chiffres √©taient toujours en baisse lorsque j'ai arr√™t√© d'apprendre vendredi, bien que la vitesse de progression se soit consid√©rablement ralentie. <br><br>  Bien s√ªr, en fait, il est important pour nous dans quelle mesure le d√©codeur de Data peut transformer le visage de Zuckerberg en Data.  Nous ne savons pas √† quoi devrait ressembler le ¬´r√©sultat final¬ª, il est donc impossible de mesurer la qualit√© du travail en nombre exact.  Le mieux que nous puissions faire est d'examiner la vid√©o et de d√©cider si elle semble r√©aliste. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zZEi1lHTdpY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La vid√©o ci-dessus montre la qualit√© du dipfake aux quatre √©tapes du processus d'apprentissage. Les vid√©os des 10 et 12 d√©cembre montrent le mod√®le Villain partiellement entra√Æn√©. La vid√©o du 6 d√©cembre en haut √† gauche est un premier test avec un mod√®le diff√©rent. Le coin inf√©rieur droit est le r√©sultat final. Au cours de l'entra√Ænement, les d√©tails de son visage sont devenus plus clairs et plus cr√©dibles. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le 9 d√©cembre, apr√®s trois jours de formation, j'ai publi√© une vid√©o pr√©liminaire sur la cha√Æne interne de la r√©daction de Slak. La vid√©o √©tait similaire √† ce qui se trouve dans le coin sup√©rieur gauche. Notre gourou du design, Aurich Lawson, a r√©agi avec sarcasme. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"En g√©n√©ral, cela semble mauvais", a-t-il √©crit, ajoutant qu'il "ne semble pas convaincant. J'attends une de ces vid√©os qui ne semblent pas fausses. "</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je pense que dans sa critique il y a un noyau rationnel. J'ai √©t√© surpris de la rapidit√© avec laquelle Faceswap a pu cr√©er des images de visages qui ressemblaient beaucoup √† Brent Spiner, plus qu'√† Zuckerberg. Cependant, si vous regardez attentivement, vous verrez les signes caract√©ristiques de la fraude num√©rique. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sur certaines images, la fronti√®re entre le faux visage de Data et la t√™te de Zuckerberg ne semble pas tout √† fait correcte. Parfois, le sourcil de Zuckerberg jette un coup d'≈ìil sous le visage de Data. Dans d'autres endroits, les bords du faux visage sont couverts de quelques pixels sur les oreilles de Zuckerberg. Il peut √™tre possible de r√©soudre ces probl√®mes avec la composition en post-traitement manuel par une personne - mais quelqu'un devra faire d√©filer la vid√©o image par image et ajuster le masque pour chacun.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/d8XknbnMs4c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, un probl√®me plus fondamental est que les algorithmes dipfake ne sont pas encore capables de reproduire suffisamment bien les moindres d√©tails des visages humains. Cela est assez √©vident lorsque vous regardez les vid√©os de d√©but et de fin en parall√®le. Faceswap a √©tonnamment bien traduit la structure globale du visage de Data. Mais m√™me apr√®s une semaine d'entra√Ænement, le visage est flou et ne contient pas suffisamment de d√©tails importants. Par exemple, les logiciels pour dipheykas peuvent difficilement faire face au dessin des dents humaines. Parfois, les dents deviennent clairement visibles et dans le cadre suivant, elles disparaissent, laissant la noirceur.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'une des principales raisons en est que la t√¢che Faceswap devient exponentiellement plus compliqu√©e avec des r√©solutions plus √©lev√©es. Les encodeurs automatiques font un bon travail avec des images 64x64 pixels. Mais reproduire les d√©tails les plus fins des images 128x128 pixels - sans parler des images de 256 pixels ou plus - est d√©j√† beaucoup plus difficile. C'est peut-√™tre l'une des raisons pour lesquelles les dipht√®ques les plus impressionnants ont un angle de vision assez large, sans gros plans de visages. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, vous ne devez pas consid√©rer cela comme une limitation fondamentale de la technologie diphasique. Dans les ann√©es √† venir, les chercheurs pourraient tr√®s bien d√©velopper des technologies capables de surmonter ces limites. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Souvent, la base d'un logiciel pour une dipheyka est d√©crite √† tort comme des r√©seaux g√©n√©ratifs-accusatoires (GSS), ou de tels r√©seaux de neurones qui permettent au logiciel de "repr√©senter"</font></font><a href="https://thispersondoesnotexist.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">personnes</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , objets ou paysages </font><a href="https://thispersondoesnotexist.com/" rel="nofollow"><font style="vertical-align: inherit;">inexistants</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">En fait, dipfeyki fonctionne avec des encodeurs automatiques. </font><font style="vertical-align: inherit;">Cependant, les derni√®res avanc√©es de la technologie GSS sugg√®rent que les dipfakes peuvent encore √™tre am√©lior√©s. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le GSS, qui est apparu pour la premi√®re fois en 2014, ne pouvait produire que des images grossi√®res √† basse r√©solution. </font><font style="vertical-align: inherit;">Mais r√©cemment, les chercheurs ont </font></font><a href="https://arxiv.org/abs/1710.10196" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d√©couvert</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> comment cr√©er un GSS qui produit des images photor√©alistes jusqu'√† 1024 pixels. </font><font style="vertical-align: inherit;">Les techniques sp√©cifiques utilis√©es dans ces travaux scientifiques peuvent ne pas √™tre applicables pour cr√©er un diphake, mais il est facile d'imaginer comment quelqu'un d√©veloppera une technologie similaire pour les auto-encodeurs - ou peut-√™tre une architecture de r√©seau de neurones compl√®tement nouvelle con√ßue pour remplacer les visages.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Perspective dipfake </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La mont√©e en popularit√© des dipfakes est √©videmment alarmante. Jusqu'√† r√©cemment, les gens pouvaient facilement prendre une vid√©o avec une personne √† sa valeur nominale. L'av√®nement du logiciel dipheyka et d'autres outils num√©riques nous a conduit au scepticisme √† propos des vid√©os maintenant. Si nous voyons une vid√©o dans laquelle une personne pr√©tend quelque chose de scandaleux - ou se d√©shabille - nous devrions envisager la possibilit√© que quelqu'un ait falsifi√© cette vid√©o afin de discr√©diter cette personne. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, mon exp√©rience met l'accent sur les limites de la technologie dipfake - au moins dans sa forme actuelle. Des connaissances et des efforts approfondis sont n√©cessaires pour cr√©er un visage virtuel pleinement convaincant. Je n'ai pas r√©ussi, et je ne suis pas s√ªr que quelqu'un ait d√©j√† pu produire une vid√©o dipfake qui soit vraiment indiscernable de la vraie.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De plus, aujourd'hui, des outils comme Faceswap ne traitent que les changements de visage. Ils ne changent pas le front, les cheveux, les bras et les jambes. Et m√™me si le visage est parfait, il sera possible de d√©terminer la vid√©o dipfake en fonction d'√©l√©ments qui ne semblent pas corrects. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, ces limitations de la technologie dipfake peuvent dispara√Ætre. Dans quelques ann√©es, le logiciel pourra apprendre √† produire des vid√©os indissociables des vraies. Et alors?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans ce cas, il sera utile de se rappeler que d'autres types de supports sont depuis longtemps faciles √† simuler. </font><font style="vertical-align: inherit;">La t√¢che triviale serait de prendre une capture d'√©cran d'un e-mail, o√π quelqu'un √©crit quelque chose qu'il n'a pas r√©ellement √©crit. </font><font style="vertical-align: inherit;">Et cela n'a pas entra√Æn√© une augmentation du nombre de carri√®res irr√©guli√®res en raison de courriels frauduleux, ni discr√©dit√© des captures d'√©cran de lettres comme preuves utilis√©es dans les discussions publiques.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais les gens savent que les e-mails peuvent √™tre truqu√©s et recherchent une confirmation suppl√©mentaire dans de tels cas. </font><font style="vertical-align: inherit;">Quelle cha√Æne d'√©v√©nements a attir√© l'attention du public sur les lettres? </font><font style="vertical-align: inherit;">D'autres personnes ont-elles re√ßu des copies de cet e-mail au moment o√π il √©tait cens√© √™tre √©crit? </font><font style="vertical-align: inherit;">L'auteur pr√©sum√© de la lettre a-t-il reconnu sa paternit√© ou revendique-t-il un faux? </font><font style="vertical-align: inherit;">Les r√©ponses √† ces questions aident les gens √† d√©cider dans quelle mesure ils peuvent prendre au s√©rieux une lettre publi√©e.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vous pouvez √™tre dupe une fois </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il en va de m√™me pour les vid√©os. </font><font style="vertical-align: inherit;">Peut-√™tre y aura-t-il une br√®ve p√©riode de temps o√π les trompeurs peuvent d√©truire la carri√®re d'une personne en publiant une vid√©o o√π elle dit ou fait quelque chose de scandaleux. </font><font style="vertical-align: inherit;">Mais bient√¥t, la soci√©t√© apprendra √† traiter les vid√©os avec scepticisme, √† moins que le clip vid√©o ne contienne des preuves documentaires, des t√©moins ou d'autres facteurs √† l'appui. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je pense que cela fonctionnera m√™me dans les cas d'abus les plus scandaleux de la technologie diphey: ins√©rer le visage d'une personne dans une vid√©o pornographique. </font><font style="vertical-align: inherit;">C'est √©videmment irrespectueux et inacceptable. </font><font style="vertical-align: inherit;">Mais les gens craignent que ces vid√©os ne d√©truisent leur r√©putation et leur carri√®re. </font><font style="vertical-align: inherit;">Je pense que ce n'est pas le cas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En effet, sur Internet, vous pouvez trouver des images compl√®tes de personnalit√©s c√©l√®bres (principalement des femmes) dont la t√™te est attach√©e au corps des stars du porno √† l'aide de Photoshop. La souffrance des femmes est compr√©hensible. Mais le public ne conclut pas automatiquement que ces femmes ont pos√© nues - nous connaissons l'existence de Photoshop et la possibilit√© de cr√©er de fausses photos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il en va de m√™me pour la pornographie profonde. √âvidemment, ce n'est pas bon de faire du faux porno avec votre participation. Mais la sortie d'une vid√©o dipfake avec une sorte de personne n'aura pas un effet aussi d√©vastateur qu'une vraie vid√©o de sexe. En l'absence de preuve de l'authenticit√© de la vid√©o, le public conclura qu'elle est fausse.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Matt Torah, l'auteur de Faceswap, me dit que cette consid√©ration a √©t√© l'une des composantes de sa motivation pour cr√©er le package. Il pense que des logiciels pour changer les gens seront in√©vitablement d√©velopp√©s. Il esp√®re qu'en cr√©ant un outil convivial pour changer les gens de l'open source, il contribuera √† √©liminer le voile du secret avec cette technologie et √† informer le public de ses capacit√©s et de ses limites. Et cela, √† son tour, nous aidera √† arriver rapidement au point o√π le public sera sceptique quant aux vid√©os qui pourraient se r√©v√©ler fausses.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√Ä long terme, nous risquons que le pendule des relations publiques oscille trop dans l'autre sens, et la possibilit√© de cr√©er des dipfakes d√©truira la croyance en la force probante des vid√©os. </font><font style="vertical-align: inherit;">Certains politiciens ont d√©j√† pris l'habitude de rejeter la critique des m√©dias comme une ¬´fausse nouvelle¬ª. </font><font style="vertical-align: inherit;">Cette tactique deviendra plus efficace avec une sensibilisation accrue du public √† la technologie dipheik.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr482684/">https://habr.com/ru/post/fr482684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr482674/index.html">¬´Imm√©diatement apr√®s les vacances¬ª: s√©minaires, master classes et concours technologiques √† l'Universit√© ITMO</a></li>
<li><a href="../fr482676/index.html">Tri efficace des donn√©es de type Struct</a></li>
<li><a href="../fr482678/index.html">Comment j'ai d√©cid√© de faire une qu√™te de texte pour iOS et ce qui en est r√©sult√©</a></li>
<li><a href="../fr482680/index.html">Crypter, XOR, piratage ZIP et PRSP non crypt√©s. R√©solution de probl√®mes avec r0ot-mi Crypto. 2e partie</a></li>
<li><a href="../fr482682/index.html">Automatisation de Qt Tool</a></li>
<li><a href="../fr482686/index.html">Les scientifiques automatisent la recherche sur le comportement animal pour d√©coder la fonction c√©r√©brale</a></li>
<li><a href="../fr482690/index.html">Cr√©ez votre √©diteur dans Combine</a></li>
<li><a href="../fr482692/index.html">Intel NUC Home pour le travail et la virtualisation</a></li>
<li><a href="../fr482694/index.html">Y a-t-il une vie apr√®s Windows ou o√π d√©velopper un administrateur / ing√©nieur syst√®me Windows en 2020?</a></li>
<li><a href="../fr482696/index.html">Chiffrement complet du disque des syst√®mes install√©s sous Windows Linux. Multiboot crypt√©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>