<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé≤ üé§ üî¢ R√©seaux de neurones, principes fondamentaux de fonctionnement, diversit√© et topologie üéê ü§ï üó£Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les r√©seaux de neurones ont r√©volutionn√© le domaine de la reconnaissance des formes, mais en raison de l'interpr√©tabilit√© non √©vidente du principe de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©seaux de neurones, principes fondamentaux de fonctionnement, diversit√© et topologie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416071/">  Les r√©seaux de neurones ont r√©volutionn√© le domaine de la reconnaissance des formes, mais en raison de l'interpr√©tabilit√© non √©vidente du principe de fonctionnement, ils ne sont pas utilis√©s dans des domaines tels que la m√©decine et l'√©valuation des risques.  Il n√©cessite une repr√©sentation visuelle du r√©seau, ce qui en fera non pas une bo√Æte noire, mais au moins ¬´translucide¬ª.  <b>Cristopher Olah, dans Neural Networks, Manifolds, and Topology, a d√©montr√© les principes de fonctionnement des r√©seaux de neurones et les a connect√©s √† la th√©orie math√©matique de la topologie et de la diversit√©, qui a servi de base √† cet article.</b>  Pour d√©montrer le fonctionnement d'un r√©seau de neurones, des r√©seaux de neurones profonds de faible dimension sont utilis√©s. <br><br>  Comprendre le comportement des r√©seaux de neurones profonds n'est g√©n√©ralement pas une t√¢che triviale.  Il est plus facile d'explorer des r√©seaux de neurones profonds de faible dimension - des r√©seaux dans lesquels il n'y a que quelques neurones dans chaque couche.  Pour les r√©seaux de faible dimension, vous pouvez cr√©er des visualisations pour comprendre le comportement et la formation de ces r√©seaux.  Cette perspective fournira une compr√©hension plus approfondie du comportement des r√©seaux de neurones et observera la connexion qui combine les r√©seaux de neurones avec un domaine math√©matique appel√© topologie. <br><br>  Un certain nombre de choses int√©ressantes en d√©coulent, notamment les limites inf√©rieures fondamentales de la complexit√© d'un r√©seau neuronal capable de classer certains ensembles de donn√©es. <br><br>  Consid√©rons le principe du r√©seau en utilisant un exemple <br><a name="habracut"></a><br>  Commen√ßons par un simple ensemble de donn√©es - deux courbes sur un plan.  La t√¢che r√©seau apprendra √† classer les points appartenant aux courbes. <br><br><img src="https://habrastorage.org/webt/m4/od/cv/m4odcvui3bls-vx-zzfhprjatzg.png"><br><br>  Une fa√ßon √©vidente de visualiser le comportement d'un r√©seau de neurones, pour voir comment l'algorithme classe tous les objets possibles (dans notre exemple, les points) d'un ensemble de donn√©es. <br><br>  Commen√ßons par la classe la plus simple de r√©seau neuronal, avec une couche d'entr√©e et de sortie.  Un tel r√©seau essaie de s√©parer deux classes de donn√©es en les divisant par une ligne. <br><br><img src="https://habrastorage.org/webt/ii/sz/eo/iiszeobw-_fe8mam77ku7o62g7g.png"><br><br>  Un tel r√©seau n'est pas utilis√© dans la pratique.  Les r√©seaux de neurones modernes ont g√©n√©ralement plusieurs couches entre leur entr√©e et leur sortie, appel√©es couches ¬´cach√©es¬ª. <br><br><img src="https://habrastorage.org/webt/ej/bp/dw/ejbpdwhjcvzouhxnir9a-zeavye.jpeg"><br><br><h3>  Diagramme de r√©seau simple </h3><br>  Nous visualisons le comportement de ce r√©seau, en observant ce qu'il fait avec diff√©rents points de son domaine.  Un r√©seau √† couche cach√©e s√©pare les donn√©es d'une courbe plus complexe qu'une ligne. <br><br><img src="https://habrastorage.org/webt/wp/cr/vf/wpcrvf_a0tiqrhftmxhddovy8tk.png"><br><br>  Avec chaque couche, le r√©seau transforme les donn√©es, cr√©ant une nouvelle vue.  Nous pouvons voir les donn√©es dans chacune de ces vues et comment le r√©seau avec une couche cach√©e les classe.  Lorsque l'algorithme atteint la pr√©sentation finale, le r√©seau neuronal tracera une ligne √† travers les donn√©es (ou dans des dimensions sup√©rieures - un hyperplan). <br><br>  Dans la visualisation pr√©c√©dente, les donn√©es d'une vue brute sont prises en compte.  Vous pouvez l'imaginer en regardant la couche d'entr√©e.  Maintenant, consid√©rez-le apr√®s sa conversion en premi√®re couche.  Vous pouvez l'imaginer en regardant la couche cach√©e. <br>  Chaque mesure correspond √† l'activation d'un neurone dans la couche. <br><br><img src="https://habrastorage.org/webt/fc/ha/ch/fchachtyqdnmtcgaxonj6d1xiu0.png"><br><br>  La couche cach√©e est entra√Æn√©e sur la vue afin que les donn√©es soient lin√©airement s√©parables. <br><br>  <b>Rendu de calque continu</b> <br><br>  Dans l'approche d√©crite dans la section pr√©c√©dente, nous apprenons √† comprendre les r√©seaux en regardant la pr√©sentation correspondant √† chaque couche.  Cela nous donne une liste discr√®te de vues. <br><br>  La partie non triviale consiste √† comprendre comment nous passons de l'un √† l'autre.  Heureusement, les niveaux de r√©seau neuronal ont des propri√©t√©s qui rendent cela possible. <br>  Il existe de nombreux types de couches diff√©rents utilis√©s dans les r√©seaux de neurones. <br><br>  Consid√©rons une couche tanh pour un exemple sp√©cifique.  La couche de Tanh-tanh (Wx + b) comprend: <br><br><ol><li>  La transformation lin√©aire de la matrice "poids" W </li><li>  Traduction √† l'aide du vecteur b </li><li>  Application ponctuelle de tanh. </li></ol><br>  Nous pouvons repr√©senter cela comme une transformation continue comme suit: <br><br><img src="https://habrastorage.org/webt/cn/vl/rb/cnvlrblwsnmp9mxxzbawagywuj0.gif"><br><br>  Ce principe de fonctionnement est tr√®s similaire √† d'autres couches standard consistant en une transformation affine, suivie de l'application ponctuelle d'une fonction d'activation monotone. <br>  Cette m√©thode peut √™tre utilis√©e pour comprendre des r√©seaux plus complexes.  Ainsi, le r√©seau suivant classe deux spirales l√©g√®rement emm√™l√©es √† l'aide de quatre couches cach√©es.  Au fil du temps, on peut voir que le r√©seau de neurones passe d'une vue brute √† un niveau sup√©rieur que le r√©seau a √©tudi√© pour classer les donn√©es.  Alors que les spirales sont initialement enchev√™tr√©es, vers la fin, elles sont lin√©airement s√©parables. <br><br><img src="https://habrastorage.org/webt/g0/-y/kg/g0-ykgrem8udnrx-xiua2rolthq.gif"><br><br>  En revanche, le prochain r√©seau, qui utilise √©galement plusieurs niveaux, mais ne peut pas classer deux spirales, qui sont plus enchev√™tr√©es. <br><br><img src="https://habrastorage.org/webt/gv/f3/wc/gvf3wc_-a7yw2h-3odoobbsie7u.gif"><br><br>  Il convient de noter que ces t√¢ches ont une complexit√© limit√©e, car des r√©seaux de neurones de faible dimension sont utilis√©s.  Si des r√©seaux plus larges √©taient utilis√©s, la r√©solution des probl√®mes √©tait simplifi√©e. <br><br><h3>  Couches Tang </h3><br>  Chaque couche √©tire et comprime l'espace, mais il ne coupe jamais, ne casse pas et ne le plie pas.  Intuitivement, nous voyons que les propri√©t√©s topologiques sont pr√©serv√©es sur chaque couche. <br><br>  De telles transformations qui n'affectent pas la topologie sont appel√©es homomorphismes (Wiki - Ceci est une cartographie du syst√®me alg√©brique A qui pr√©serve les op√©rations de base et les relations de base).  Formellement, ce sont des bijections qui sont des fonctions continues dans les deux sens.  Dans un mappage bijectif, chaque √©l√©ment d'un ensemble correspond exactement √† un √©l√©ment d'un autre ensemble, et un mappage inverse qui a la m√™me propri√©t√© est d√©fini. <br><br>  <b>Le th√©or√®me</b> <br><br>  Les couches √† N entr√©es et N sorties sont des homomorphismes si la matrice de poids W n'est pas d√©g√©n√©r√©e.  (Vous devez faire attention au domaine et √† la plage.) <br><br><div class="spoiler">  <b class="spoiler_title">Preuve:</b> <div class="spoiler_text">  1. Supposons que W ait un d√©terminant non nul.  Il s'agit alors d'une fonction lin√©aire bijective avec une inverse lin√©aire.  Les fonctions lin√©aires sont continues.  La multiplication par W est donc un hom√©omorphisme. <br>  2. Cartographies - homomorphismes <br>  3. tanh (sigmo√Øde et softplus, mais pas ReLU) sont des fonctions continues avec des inverses continus.  Ce sont des bijections si nous faisons attention √† la zone et √† la port√©e que nous envisageons.  Leur utilisation ponctuelle est un homomorphisme. <br><br>  Ainsi, si W a un d√©terminant non nul, la fibre est hom√©omorphe. <br></div></div><br><h3>  Topologie et classification </h3><br>  Consid√©rons un ensemble de donn√©es √† deux dimensions avec deux classes A, B‚äÇR2: <br><br>  A = {x |  d (x, 0) &lt;1/3} <br><br>  B = {x |  2/3 &lt;d (x, 0) &lt;1} <br><br><img src="https://habrastorage.org/webt/ow/2l/b1/ow2lb1ozxk5p4d-s4ho_lq6_ds8.png"><br><br>  A rouge, B bleu <br><br>  Condition: un r√©seau de neurones ne peut pas classer cet ensemble de donn√©es sans 3 couches cach√©es ou plus, quelle que soit la largeur. <br><br>  Comme mentionn√© pr√©c√©demment, la classification avec une fonction sigmo√Øde ou une couche softmax √©quivaut √† essayer de trouver l'hyperplan (ou dans ce cas la ligne) qui s√©pare A et B dans la repr√©sentation finale.  Avec seulement deux couches masqu√©es, le r√©seau est topologiquement incapable de partager des donn√©es de cette mani√®re et est vou√© √† l'√©chec dans cet ensemble de donn√©es. <br>  Dans la visualisation suivante, nous observons une vue latente pendant que le r√©seau s'entra√Æne avec la ligne de classification. <br><br><img src="https://habrastorage.org/webt/ap/nt/xe/apntxeprybgn8yf_jchwskwes44.gif"><br><br>  Pour ce r√©seau de formation, il ne suffit pas d'atteindre un r√©sultat √† cent pour cent. <br>  L'algorithme tombe dans un minimum local non productif, mais est capable d'atteindre une pr√©cision de classification de ~ 80%. <br><br>  Dans cet exemple, il n'y avait qu'une seule couche masqu√©e, mais cela n'a pas fonctionn√©. <br>  D√©claration.  Soit chaque couche est un homomorphisme, soit la matrice de poids de la couche a le d√©terminant 0. <br><br><div class="spoiler">  <b class="spoiler_title">Preuve:</b> <div class="spoiler_text">  S'il s'agit d'un homomorphisme, alors A est toujours entour√© de B, et la ligne ne peut pas les s√©parer.  Mais supposons qu'il ait un d√©terminant de 0: alors l'ensemble de donn√©es s'effondre sur un axe.  √âtant donn√© que nous avons affaire √† quelque chose d'hom√©omorphe √† l'ensemble de donn√©es d'origine, A est entour√© de B, et l'effondrement sur n'importe quel axe signifie que nous aurons quelques points de A et B m√©lang√©s, ce qui rend la distinction impossible. <br></div></div><br>  Si nous ajoutons un troisi√®me √©l√©ment cach√©, le probl√®me deviendra trivial.  Le r√©seau neuronal reconna√Æt la repr√©sentation suivante: <br><br><img src="https://habrastorage.org/webt/y1/p8/ol/y1p8olobp-zdo3shlaooa62hy8k.png"><br><br>  La vue permet de s√©parer les jeux de donn√©es avec un hyperplan. <br>  Pour mieux comprendre ce qui se passe, regardons un ensemble de donn√©es encore plus simple, qui est unidimensionnel: <br><br><img src="https://habrastorage.org/webt/ud/by/hr/udbyhrfnqgfdr9r9lvh4cg7apw0.png"><br><br>  A = [- 1 / 3,1 / 3] <br>  B = [- 1, ‚àí2 / 3] ‚à™ [2 / 3,1] <br>  Sans utiliser une couche de deux ou plusieurs √©l√©ments cach√©s, nous ne pouvons pas classer cet ensemble de donn√©es.  Mais, si nous utilisons un r√©seau √† deux √©l√©ments, nous apprendrons √† repr√©senter les donn√©es comme une bonne courbe qui nous permet de s√©parer les classes √† l'aide d'une ligne: <br><br><img src="https://habrastorage.org/webt/8_/w-/bo/8_w-boyjlwhtufafsnqpljuo8u0.gif"><br><br>  Que se passe-t-il?  Un √©l√©ment cach√© apprend √† tirer lorsque x&gt; -1/2, et l'autre apprend √† tirer lorsque x&gt; 1/2.  Lorsque le premier est d√©clench√©, mais pas le second, nous savons que nous sommes en A. <br><br><h3>  Conjecture de vari√©t√© </h3><br>  Cela s'applique-t-il aux ensembles de donn√©es du monde r√©el, tels que les ensembles d'images?  Si vous √™tes s√©rieux au sujet de l'hypoth√®se de la diversit√©, je pense que cela compte. <br><br>  L'hypoth√®se multidimensionnelle est que les donn√©es naturelles forment des vari√©t√©s de faible dimension dans l'espace d'implantation.  Il y a des raisons √† la fois th√©oriques [1] et exp√©rimentales [2] de croire que cela est vrai.  Si oui, alors la t√¢che de l'algorithme de classification est de s√©parer le faisceau de vari√©t√©s enchev√™tr√©es. <br><br>  Dans les exemples pr√©c√©dents, une classe a compl√®tement entour√© l'autre.  Cependant, il est peu probable que la vari√©t√© d'images de chiens soit compl√®tement entour√©e d'une collection d'images de chats.  Mais il existe d'autres situations topologiques plus plausibles qui peuvent encore survenir, comme nous le verrons dans la section suivante. <br><br><h3>  Connexions et homotopies </h3><br>  Un autre jeu de donn√©es int√©ressant est les deux tori connect√©s A et B. <br><br><img src="https://habrastorage.org/webt/wl/er/ns/wlernsr6ibyfnuq2cbkepwgxxq8.png"><br><br>  Comme les pr√©c√©dents ensembles de donn√©es que nous avons examin√©s, cet ensemble de donn√©es ne peut pas √™tre divis√© sans utiliser n + 1 dimensions, √† savoir la quatri√®me dimension. <br><br>  Les connexions sont √©tudi√©es dans la th√©orie des n≈ìuds, dans le domaine de la topologie.  Parfois, lorsque nous voyons une connexion, il n'est pas imm√©diatement clair s'il s'agit d'une incoh√©rence (beaucoup de choses qui s'emm√™lent mais peuvent √™tre s√©par√©es par une d√©formation continue) ou non. <br><br><img src="https://habrastorage.org/webt/lg/1h/bu/lg1hbu1mehjrj872e61gd73_3vi.png"><br><br>  Incoh√©rence relativement simple. <br><br>  Si un r√©seau neuronal utilisant des couches avec seulement trois unit√©s peut le classer, alors il est incoh√©rent.  (Question: toutes les incoh√©rences peuvent-elles √™tre class√©es sur le r√©seau avec seulement trois incoh√©rences, th√©oriquement?) <br><br>  Du point de vue de ce n≈ìud, la visualisation continue des repr√©sentations cr√©√©es par un r√©seau de neurones est une proc√©dure pour d√©nouer les connexions.  En topologie, nous appellerons cette isotopie ambiante entre le lien d'origine et les liens s√©par√©s. <br><br>  Formellement, l'isotopie de l'espace environnant entre les vari√©t√©s A et B est une fonction continue F: [0,1] √ó X ‚Üí Y telle que chaque Ft est un hom√©omorphisme de X √† sa gamme, F0 est une fonction d'identit√© et F1 mappe A √† B. T .e.  Ft passe continuellement de la carte A √† elle-m√™me, √† la carte A √† B. <br><br>  Th√©or√®me: il y a une isotopie de l'espace environnant entre l'entr√©e et la repr√©sentation du niveau du r√©seau si: a) W n'est pas d√©g√©n√©r√©, b) nous sommes pr√™ts √† transf√©rer des neurones vers la couche cach√©e et c) il y a plus d'un √©l√©ment cach√©. <br><br><div class="spoiler">  <b class="spoiler_title">Preuve:</b> <div class="spoiler_text">  1. La partie la plus difficile est la transformation lin√©aire.  Pour rendre cela possible, nous avons besoin de W pour avoir un d√©terminant positif.  Notre pr√©misse est qu'il n'est pas √©gal √† z√©ro, et nous pouvons inverser le signe s'il est n√©gatif en commutant deux neurones cach√©s, et donc nous pouvons garantir que le d√©terminant est positif.  L'espace des matrices d√©terminantes positives est connect√©, donc il existe p: [0,1] ‚Üí GLn ¬Æ5 tel que p (0) = Id et p (1) = W. On peut passer en continu de la fonction identit√© √† la transformation W en utilisant fonctions x ‚Üí p (t) x, multipliant x √† chaque instant t par une matrice passante continue p (t). <br>  2. On peut passer continuellement de la fonction d'identit√© √† la b-map en utilisant la fonction x ‚Üí x + tb. <br>  3. On peut passer continuellement de la fonction identique √† l'utilisation ponctuelle de œÉ avec la fonction: x ‚Üí (1-t) x + tœÉ (x) <br></div></div><br>  Jusqu'√† pr√©sent, il est peu probable que les relations dont nous avons parl√© apparaissent dans des donn√©es r√©elles, mais il existe des g√©n√©ralisations d'un niveau sup√©rieur.  Il est plausible que de telles fonctionnalit√©s puissent exister dans des donn√©es r√©elles. <br><br>  Les connexions et les n≈ìuds sont des vari√©t√©s unidimensionnelles, mais nous avons besoin de 4 dimensions pour que les r√©seaux puissent tous les d√©m√™ler.  De m√™me, un espace dimensionnel encore plus √©lev√© peut √™tre n√©cessaire pour pouvoir √©tendre les vari√©t√©s √† n dimensions.  Tous les collecteurs √† n dimensions peuvent √™tre √©tendus en 2n + 2 dimensions.  [3] <br><br><h3>  Sortie facile </h3><br>  Le moyen le plus simple est d'essayer de s√©parer les collecteurs et d'√©tirer les pi√®ces aussi emm√™l√©es que possible.  Bien que cela ne soit pas proche d'une v√©ritable solution, une telle solution peut atteindre une pr√©cision de classification relativement √©lev√©e et √™tre un minimum local acceptable. <br><br><img src="https://habrastorage.org/webt/7x/mf/fp/7xmffpy2eilztftxrcbv69xhsf4.png"><br><br>  De tels minima locaux sont absolument inutiles pour essayer de r√©soudre des probl√®mes topologiques, mais les probl√®mes topologiques peuvent fournir une bonne motivation pour √©tudier ces probl√®mes. <br><br>  D'un autre c√¥t√©, si nous voulons seulement obtenir de bons r√©sultats de classification, l'approche est acceptable.  Si un petit morceau d'un collecteur de donn√©es est pris sur un autre collecteur, est-ce un probl√®me?  Il est probable qu'il sera possible d'obtenir des r√©sultats de classement arbitrairement bons, malgr√© ce probl√®me. <br><br>  Des couches am√©lior√©es pour manipuler les collecteurs? <br><br>  Il est difficile d'imaginer que les couches standard avec des transformations affines sont vraiment bonnes pour manipuler les vari√©t√©s. <br><br>  Peut-√™tre qu'il est logique d'avoir une couche compl√®tement diff√©rente, que nous pouvons utiliser dans la composition avec des couches plus traditionnelles? <br><br>  L'√©tude d'un champ vectoriel avec une direction dans laquelle nous voulons d√©placer la vari√©t√© est prometteuse: <br><br><img src="https://habrastorage.org/webt/2z/iw/at/2ziwat9d2bjwlclnjrixwmm5llc.png"><br><br>  Et puis nous d√©formons l'espace en fonction du champ vectoriel: <br><br><img src="https://habrastorage.org/webt/ig/qb/dr/igqbdrqbcl3gy85kv4rzbhirflk.png"><br><br>  On pourrait √©tudier le champ vectoriel √† des points fixes (il suffit de prendre quelques points fixes de l'ensemble de donn√©es de test pour les utiliser comme ancres) et interpoler en quelque sorte. <br><br><div class="spoiler">  <b class="spoiler_title">Le champ vectoriel ci-dessus a la forme:</b> <div class="spoiler_text">  P (x) = (v0f0 (x) + v1f1 (x)) / (1 + 0 (x) + f1 (x)) <br></div></div><br>  O√π v0 et v1 sont des vecteurs, et f0 (x) et f1 (x) sont des Gaussiens √† n dimensions. <br><br><h3>  Couches K-voisins les plus proches </h3><br>  La s√©parabilit√© lin√©aire peut √™tre un besoin √©norme et peut-√™tre d√©raisonnable pour les r√©seaux de neurones.  Il est naturel d'utiliser la m√©thode des k plus proches voisins (k-NN).  Cependant, le succ√®s du k-NN d√©pend fortement de la pr√©sentation qu'il classe, donc une bonne pr√©sentation est requise avant que le k-NN puisse bien fonctionner. <br><br>  k-NN se diff√©rencie par la repr√©sentation sur laquelle il agit.  De cette fa√ßon, nous pouvons directement former le r√©seau pour classer k-NN.  Cela peut √™tre vu comme une sorte de couche de ¬´plus proche voisin¬ª qui agit comme une alternative √† softmax. <br>  Nous ne voulons pas mettre en garde contre l'ensemble de nos formations pour chaque mini-soir√©e, car ce sera une proc√©dure tr√®s co√ªteuse.  L'approche adapt√©e consiste √† classer chaque √©l√©ment du mini-lot en fonction des classes des autres √©l√©ments du mini-lot, en donnant √† chaque unit√© de poids divis√©e par la distance de la cible de classification. <br><br>  Malheureusement, m√™me avec des architectures complexes, l'utilisation de k-NN r√©duit la probabilit√© d'erreur - et l'utilisation d'architectures plus simples d√©grade les r√©sultats. <br><br><h3>  Conclusion </h3><br>  Les propri√©t√©s topologiques des donn√©es, telles que les relations, peuvent rendre impossible la division lin√©aire des classes en utilisant des r√©seaux de faible dimension, quelle que soit la profondeur.  M√™me dans les cas o√π cela est techniquement possible.  Par exemple, les spirales, qu'il peut √™tre tr√®s difficile de s√©parer. <br><br>  Pour une classification pr√©cise des donn√©es, les r√©seaux de neurones ont besoin de larges couches.  De plus, les couches traditionnelles du r√©seau neuronal sont mal adapt√©es pour repr√©senter des manipulations importantes avec des vari√©t√©s;  m√™me si nous r√©glons les poids manuellement, il serait difficile de repr√©senter de mani√®re compacte les transformations que nous voulons. <br><br><div class="spoiler">  <b class="spoiler_title">Liens vers des sources et explications</b> <div class="spoiler_text">  [1] Une grande partie des transformations naturelles que vous voudrez peut-√™tre effectuer sur une image, comme la translation ou la mise √† l'√©chelle d'un objet ou la modification de l'√©clairage, formeraient des courbes continues dans l'espace image si vous les ex√©cutiez en continu. <br><br>  [2] Carlsson et al.  ont constat√© que des taches locales d'images forment une bouteille de klein. <br>  [3] Ce r√©sultat est mentionn√© dans la sous-section de Wikipedia sur les versions d'isotopie. <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr416071/">https://habr.com/ru/post/fr416071/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr416059/index.html">Mobio s'entretient avec Daniil Shuleiko (Yandex.Taxi) sur la fusion avec Uber, le march√© des taxis et la concurrence</a></li>
<li><a href="../fr416061/index.html">Tant bien que mal, je vois tout</a></li>
<li><a href="../fr416063/index.html">Les n√©gociations des Russes n'ont nulle part o√π enregistrer</a></li>
<li><a href="../fr416067/index.html">Pr√©sentation de la vuln√©rabilit√© de Mikrotik Winbox. Ou un gros fichier</a></li>
<li><a href="../fr416069/index.html">Migration de donn√©es ElasticSearch sans perte</a></li>
<li><a href="../fr416073/index.html">Un robot de trading de crypto-monnaie simple</a></li>
<li><a href="../fr416075/index.html">Le FSB veut introduire la responsabilit√© de l'utilisation cach√©e des enregistreurs vocaux et des cam√©ras dans les smartphones [et pas seulement]</a></li>
<li><a href="../fr416077/index.html">PlantUML - Tout ce dont les analystes commerciaux ont besoin pour cr√©er des graphiques dans la documentation du logiciel</a></li>
<li><a href="../fr416079/index.html">Corona Native pour Android - utilisation de code Java personnalis√© dans un jeu √©crit en Corona</a></li>
<li><a href="../fr416081/index.html">Quelque chose ne va toujours pas avec le retour √† Habr</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>