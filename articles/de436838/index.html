<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçü§ù‚Äçüë©üèº üö£üèæ üë®‚Äçüëß Faltungsneurale Netze durch Visualisierungen in PyTorch verstehen üóÑÔ∏è üõÄüèΩ üîÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In unserer Zeit haben Maschinen eine Genauigkeit von 99% beim Verstehen und Definieren von Merkmalen und Objekten in Bildern erreicht. Wir sind jeden ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Faltungsneurale Netze durch Visualisierungen in PyTorch verstehen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436838/">  In unserer Zeit haben Maschinen eine Genauigkeit von 99% beim Verstehen und Definieren von Merkmalen und Objekten in Bildern erreicht.  Wir sind jeden Tag damit konfrontiert, zum Beispiel: Gesichtserkennung in der Kamera des Smartphones, die M√∂glichkeit, bei Google nach Fotos zu suchen, Text aus einem Barcode oder B√ºchern mit einer guten Geschwindigkeit zu scannen usw. Diese Effizienz der Maschine wurde durch ein spezielles neuronales Netzwerk erm√∂glicht, das als Faltungs-Neuronales bezeichnet wird das Netzwerk.  Wenn Sie ein Deep-Learning-Enthusiast sind, haben Sie wahrscheinlich davon geh√∂rt, und Sie k√∂nnten mehrere Bildklassifizierer entwickeln.  Moderne Deep-Learning-Frameworks wie Tensorflow und PyTorch vereinfachen das maschinelle Lernen von Bildern.  Es bleibt jedoch die Frage: Wie passieren die Daten die Schichten des neuronalen Netzwerks und wie lernt der Computer daraus?  Um eine klare Sicht von Grund auf zu erhalten, tauchen wir in eine Faltung ein und visualisieren das Bild jeder Ebene. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/2c6/958/8592c6958985979587858374abd08f98.png" alt="Bild"><br><a name="habracut"></a><br><h2>  Faltungsneurale Netze </h2><br>  Bevor Sie mit dem Studium von Convolutional Neural Networks (SNA) beginnen, m√ºssen Sie lernen, wie Sie mit neuronalen Netzen arbeiten.  Neuronale Netze ahmen das menschliche Gehirn nach, um komplexe Probleme zu l√∂sen und nach Mustern in Daten zu suchen.  In den letzten Jahren haben sie viele Algorithmen f√ºr maschinelles Lernen und Computer Vision ersetzt.  Das Grundmodell eines neuronalen Netzwerks besteht aus Neuronen, die in Schichten organisiert sind.  Jedes neuronale Netzwerk verf√ºgt √ºber eine Eingabe- und Ausgabeschicht und mehrere verborgene Schichten, die je nach Komplexit√§t des Problems hinzugef√ºgt werden.  Bei der √úbertragung von Daten durch Schichten werden Neuronen trainiert und erkennen Zeichen.  Diese Darstellung eines neuronalen Netzwerks wird als Modell bezeichnet.  Nachdem das Modell trainiert wurde, bitten wir das Netzwerk, Prognosen basierend auf Testdaten zu erstellen. <br><br>  Der SNS ist ein spezielles neuronales Netzwerk, das gut mit Bildern funktioniert.  Ian Lekun schlug sie 1998 vor, wo sie die im Eingabebild vorhandene Nummer erkannten.  SNA wird auch zur Spracherkennung, Bildsegmentierung und Textverarbeitung verwendet.  Vor der Schaffung von Faltungs-Neuronalen Netzen wurden mehrschichtige Perzeptrone bei der Konstruktion von Bildklassifikatoren verwendet.  Die Bildklassifizierung bezieht sich auf die Aufgabe, Klassen aus einem Mehrkanal-Rasterbild (Farbe, Schwarzwei√ü) zu extrahieren.  Mehrschichtige Perzeptrone ben√∂tigen viel Zeit, um nach Informationen in Bildern zu suchen, da jede Eingabe jedem Neuron in der n√§chsten Schicht zugeordnet werden muss.  Die SNA ging um sie herum und verwendete ein Konzept namens lokale Konnektivit√§t.  Dies bedeutet, dass wir jedes Neuron nur mit der lokalen Eingangsregion verbinden.  Dies minimiert die Anzahl der Parameter und erm√∂glicht es verschiedenen Teilen des Netzwerks, sich auf √ºbergeordnete Attribute wie Textur oder sich wiederholendes Muster zu spezialisieren.  Verwirrt?  Vergleichen wir, wie Bilder √ºber mehrschichtige Perzeptrone (MPs) und Faltungs-Neuronale Netze √ºbertragen werden. <br><br><h2>  Vergleich von MP und SNA </h2><br>  Die Gesamtzahl der Eintr√§ge in der Eingabeebene f√ºr das mehrschichtige Perzeptron betr√§gt 784, da das Eingabebild eine Gr√∂√üe von 28 x 28 = 784 hat (der MNIST-Datensatz wird ber√ºcksichtigt).  Das Netzwerk sollte in der Lage sein, die Anzahl im Eingabebild vorherzusagen. Dies bedeutet, dass die Ausgabe zu einer der folgenden Klassen im Bereich von 0 bis 9 geh√∂ren kann. In der Ausgabeschicht geben wir Klassensch√§tzungen zur√ºck, z. B. wenn diese Eingabe das Bild mit der Nummer ‚Äû3‚Äú ist. dann hat in der Ausgabeschicht das entsprechende Neuron "3" einen h√∂heren Wert als andere Neuronen.  Wieder stellt sich die Frage: "Wie viele versteckte Schichten brauchen wir und wie viele Neuronen sollten in jeder sein?"  Nehmen Sie zum Beispiel den folgenden MP-Code: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f8/efc/e14/3f8efce14418a7df0be2e813399def5d.png" alt="Bild"><br><br>  Der obige Code wird mithilfe eines Frameworks namens Keras implementiert.  Die erste verborgene Schicht hat 512 Neuronen, die mit der Eingangsschicht von 784 Neuronen verbunden sind.  Die n√§chste verborgene Schicht: die Ausschlussschicht, die das Problem der Umschulung l√∂st.  0,2 bedeutet, dass eine Wahrscheinlichkeit von 20% besteht, die Neuronen der vorherigen verborgenen Schicht nicht zu ber√ºcksichtigen.  Wir haben erneut eine zweite verborgene Schicht mit der gleichen Anzahl von Neuronen wie in der ersten verborgenen Schicht (512) und dann eine weitere exklusive Schicht hinzugef√ºgt.  Beenden Sie diesen Satz von Ebenen mit einer Ausgabeebene, die aus 10 Klassen besteht.  Die Klasse, die am wichtigsten ist, ist die vom Modell vorhergesagte Anzahl.  So sieht ein mehrschichtiges Netzwerk aus, nachdem alle Schichten identifiziert wurden.  Einer der Nachteile des mehrstufigen Perzeptrons besteht darin, dass es vollst√§ndig verbunden ist, was viel Zeit und Raum in Anspruch nimmt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db3/df6/605/db3df6605d0ddb868eb14b347227b963.png" alt="Bild"><br><br>  Convolts verwenden keine vollst√§ndig verbundenen Schichten.  Sie verwenden sp√§rliche Schichten, die Matrizen als Eingabe verwenden, was einen Vorteil gegen√ºber MP bietet.  In MP ist jeder Knoten daf√ºr verantwortlich, das gesamte Bild zu verstehen.  In der SNA teilen wir das Bild in Bereiche (kleine lokale Bereiche von Pixeln) auf.  Die Ausgabeschicht kombiniert die empfangenen Daten von jedem versteckten Knoten, um Muster zu finden.  Unten sehen Sie ein Bild davon, wie die Ebenen verbunden sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae6/f7f/d61/ae6f7fd618d5296a0deecabdd2e06e77.png" alt="Bild"><br><br>  Nun wollen wir sehen, wie die SNA Informationen auf Fotos findet.  Vorher m√ºssen wir verstehen, wie die Zeichen extrahiert werden.  In der SNA verwenden wir verschiedene Ebenen. Jede Ebene bewahrt die Zeichen des Bildes. Beispielsweise ber√ºcksichtigt sie das Bild des Hundes. Wenn das Netzwerk den Hund klassifizieren muss, muss es alle Zeichen wie Augen, Ohren, Zunge, Beine usw. identifizieren.  Diese Zeichen werden auf lokaler Netzwerkebene mithilfe von Filtern und Kernen unterbrochen und erkannt. <br><br><h2>  Wie sehen Computer ein Bild aus? </h2><br>  Eine Person, die ein Bild betrachtet und seine Bedeutung versteht, klingt sehr vern√ºnftig.  Nehmen wir an, Sie gehen und bemerken die vielen Landschaften um Sie herum.  Wie verstehen wir die Natur in diesem Fall?  Wir fotografieren die Umwelt mit unserem Hauptsinnesorgan - dem Auge - und senden es dann an die Netzhaut.  Es sieht alles ziemlich interessant aus, oder?  Stellen wir uns nun vor, dass ein Computer dasselbe tut.  In Computern werden Bilder mit einer Reihe von Pixelwerten interpretiert, die zwischen 0 und 255 liegen. Der Computer betrachtet diese Pixelwerte und versteht sie.  Auf den ersten Blick kennt er keine Objekte und Farben.  Es erkennt einfach die Pixelwerte und das Bild entspricht einer Reihe von Pixelwerten f√ºr den Computer.  Sp√§ter lernt er durch Analyse der Pixelwerte nach und nach, ob das Bild grau oder farbig ist.  Bilder in Graustufen haben nur einen Kanal, da jedes Pixel die Intensit√§t einer Farbe darstellt.  0 bedeutet schwarz und 255 bedeutet wei√ü, die anderen Varianten von schwarz und wei√ü, dh grau, liegen zwischen ihnen. <br><br>  Farbbilder haben drei Kan√§le: Rot, Gr√ºn und Blau.  Sie repr√§sentieren die Intensit√§t von 3 Farben (dreidimensionale Matrix), und wenn sich die Werte gleichzeitig √§ndern, ergibt sich eine gro√üe Anzahl von Farben, wirklich eine Farbpalette!  Danach erkennt der Computer die Kurven und Konturen von Objekten im Bild.  All dies kann im Faltungsnetzwerk untersucht werden.  Dazu verwenden wir PyTorch, um einen Datensatz zu laden und Filter auf Bilder anzuwenden.  Das Folgende ist ein Codeausschnitt. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load the libraries import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # Set the parameters num_workers = 0 batch_size = 20 # Converting the Images to tensors using Transforms transform = transforms.ToTensor() train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform) # Loading the Data train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # Peeking into dataset fig = plt.figure(figsize=(25, 4)) for image in np.arange(20): ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[image]), cmap='gray') ax.set_title(str(labels[image].item()))</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/304/163/1ad/3041631ad58d7300a35af90b39b94584.png" alt="Bild"><br><br>  Nun wollen wir sehen, wie ein einzelnes Bild in ein neuronales Netzwerk eingespeist wird. <br><br><pre> <code class="python hljs">img = np.squeeze(images[<span class="hljs-number"><span class="hljs-number">7</span></span>]) fig = plt.figure(figsize = (<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.imshow(img, cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) width, height = img.shape thresh = img.max()/<span class="hljs-number"><span class="hljs-number">2.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(width): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(height): val = round(img[x][y],<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y] !=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ax.annotate(str(val), xy=(y,x), color=<span class="hljs-string"><span class="hljs-string">'white'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y]&lt;thresh <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'black'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/264/f15/bff/264f15bffe653ae237f3e2fa1fc5c868.png" alt="Bild"><br><br>  Auf diese Weise wird die Zahl ‚Äû3‚Äú in Pixel aufgeteilt.  Aus dem Satz handgeschriebener Ziffern wird zuf√§llig ‚Äû3‚Äú ausgew√§hlt, in dem Pixelwerte angezeigt werden.  Hier normalisiert ToTensor () die tats√§chlichen Pixelwerte (0‚Äì255) und begrenzt sie auf einen Bereich von 0 bis 1. Warum ist das so?  Weil es die Berechnungen in den folgenden Abschnitten erleichtert, entweder um Bilder zu interpretieren oder um gemeinsame Muster zu finden, die in ihnen vorhanden sind. <br><br><h2>  Erstellen Sie Ihren eigenen Filter </h2><br>  Filter filtern, wie der Name schon sagt, Informationen.  Bei Faltungs-Neuronalen Netzen werden beim Arbeiten mit Bildern Informationen √ºber die Pixel gefiltert.  Warum sollten wir √ºberhaupt filtern?  Denken Sie daran, dass ein Computer einen Lernprozess durchlaufen muss, um Bilder zu verstehen, √§hnlich wie es ein Kind tut.  In diesem Fall brauchen wir jedoch nicht viele Jahre!  Kurz gesagt, er lernt von Grund auf neu und r√ºckt dann zum Ganzen vor. <br><br>  Daher sollte das Netzwerk zun√§chst alle groben Teile des Bildes kennen, n√§mlich die Kanten, Konturen und andere Elemente auf niedriger Ebene.  Sobald sie entdeckt wurden, ist der Weg f√ºr komplexe Symptome geebnet.  Um zu ihnen zu gelangen, m√ºssen wir zuerst die Attribute auf niedriger Ebene extrahieren, dann die Attribute auf mittlerer und dann auf h√∂herer Ebene.  Filter sind eine M√∂glichkeit, die Informationen zu extrahieren, die der Benutzer ben√∂tigt, und nicht nur die blinde Daten√ºbertragung, aufgrund derer der Computer die Strukturierung von Bildern nicht versteht.  Zu Beginn k√∂nnen Funktionen auf niedriger Ebene basierend auf einem bestimmten Filter extrahiert werden.  Der Filter ist hier auch eine Reihe von Pixelwerten, √§hnlich einem Bild.  Es kann als die Gewichte verstanden werden, die die Schichten in dem Faltungs-Neuronalen Netzwerk verbinden.  Diese Gewichte oder Filter werden mit Eingabewerten multipliziert, um Zwischenbilder zu erzeugen, die das Computerverst√§ndnis des Bildes darstellen.  Dann werden sie mit einigen weiteren Filtern multipliziert, um die Ansicht zu erweitern.  Dann erkennt es die sichtbaren Organe einer Person (vorausgesetzt, eine Person ist im Bild vorhanden).  Sp√§ter, mit mehreren weiteren Filtern und mehreren Schichten, ruft der Computer aus: ‚ÄûOh, ja!  Das ist ein Mann. " <br><br>  Wenn wir √ºber Filter sprechen, haben wir viele M√∂glichkeiten.  M√∂glicherweise m√∂chten Sie das Bild verwischen und dann einen Unsch√§rfefilter anwenden. Wenn Sie Sch√§rfe hinzuf√ºgen m√ºssen, hilft ein Sch√§rfefilter usw. <br><br>  Schauen wir uns einige Codefragmente an, um die Funktionalit√§t von Filtern zu verstehen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/679/6a4/bb4/6796a4bb4830bda29c6d14212274a286.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/752/a89/805/752a89805fba54b5d0f9e90073ca9fde.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/84f/8a1/f9c/84f8a1f9c92b1996b0e4eed4a2a7dd5b.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/142/635/038/142635038ecef3606d53d5d9c85b26f8.png" alt="Bild"><br><br>  So sieht das Bild nach dem Anwenden des Filters aus. In diesem Fall haben wir den Sobel-Filter verwendet. <br><br><h2>  Faltungsneurale Netze </h2><br>  Bisher haben wir gesehen, wie Filter verwendet werden, um Features aus Bildern zu extrahieren.  Um das gesamte neuronale Faltungsnetzwerk zu vervollst√§ndigen, m√ºssen wir alle Schichten kennen, die wir zum Entwerfen verwenden.  Die in der SNA verwendeten Schichten, <br><br><ol><li>  Faltungsschicht </li><li>  Pooling-Schicht </li><li>  Vollst√§ndig verklebte Schicht </li></ol><br>  Bei allen drei Ebenen sieht der Faltungsbildklassifizierer folgenderma√üen aus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/927/c31/8b4927c31b5f951d7026b30d68695bea.png" alt="Bild"><br><br>  Nun wollen wir sehen, was jede Ebene tut. <br><br>  <b>Die Faltungsschicht (CONV)</b> verwendet Filter, die Faltungsoperationen ausf√ºhren, indem sie das Eingabebild scannen.  Seine Hyperparameter umfassen eine Filtergr√∂√üe, die 2x2, 3x3, 4x4, 5x5 (aber nicht darauf beschr√§nkt) sein kann, und Schritt S. Das Ergebnis O wird als Feature-Map oder Aktivierungs Map bezeichnet, in der alle Features mithilfe von Eingabeebenen und Filtern berechnet werden.  Unten sehen Sie ein Bild der Generierung von Feature-Maps beim Anwenden der Faltung <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/14d/aab/a2a14daab68c91f8d92ba0c54509493b.png" alt="Bild"><br><br>  <b>Die Zusammenf√ºhrungsschicht (POOL) wird</b> verwendet, um die Merkmale zu komprimieren, die typischerweise nach der Faltungsschicht verwendet werden.  Es gibt zwei Arten von Gewerkschaftsoperationen: Dies ist die maximale und durchschnittliche Vereinigung, bei der die maximalen und durchschnittlichen Werte der Merkmale verwendet werden.  Das Folgende ist die Operation von Zusammenf√ºhrungsoperationen <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c2d/03f/2f8/c2d03f2f8734efade8cbc80d44d3767e.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/e5c/558/01ae5c558fa6647bfb9c19b9edabbb37.png" alt="Bild"><br><br>  <b>Vollst√§ndig verbundene Schichten (FCs)</b> arbeiten mit einem flachen Eingang, wobei jeder Eingang mit allen Neuronen verbunden ist.  Sie werden normalerweise am Ende des Netzwerks verwendet, um verborgene Schichten mit der Ausgabeschicht zu verbinden, wodurch die Klassenwerte optimiert werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d28/558/188/d285581882fa97824cdc0ad6ecb31873.png" alt="Bild"><br><br><h3>  SNA-Visualisierung in PyTorch </h3><br>  Nachdem wir nun die vollst√§ndige Ideologie zum Erstellen des SNA haben, implementieren wir den SNA mithilfe des PyTorch-Frameworks von Facebook. <br><br>  <b>Schritt 1</b> : Laden Sie das Eingabebild herunter, das √ºber das Netzwerk gesendet werden soll.  (Hier machen wir es mit Numpy und OpenCV) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline img_path = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> bgr_img = cv2.imread(img_path) gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"><span class="hljs-comment"># Normalise gray_img = gray_img.astype("float32")/255 plt.imshow(gray_img, cmap='gray') plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d0f/3d2/291d0f3d28f3091716c3aba41dc35c59.png" alt="Bild"><br><br>  <b>Schritt 2</b> : Filter rendern <br><br>  Lassen Sie uns die Filter visualisieren, um besser zu verstehen, welche wir verwenden werden. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np filter_vals = np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] ]) print(<span class="hljs-string"><span class="hljs-string">'Filter shape: '</span></span>, filter_vals.shape) <span class="hljs-comment"><span class="hljs-comment"># Defining the Filters filter_1 = filter_vals filter_2 = -filter_1 filter_3 = filter_1.T filter_4 = -filter_3 filters = np.array([filter_1, filter_2, filter_3, filter_4]) # Check the Filters fig = plt.figure(figsize=(10, 5)) for i in range(4): ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[]) ax.imshow(filters[i], cmap='gray') ax.set_title('Filter %s' % str(i+1)) width, height = filters[i].shape for x in range(width): for y in range(height): ax.annotate(str(filters[i][x][y]), xy=(y,x), color='white' if filters[i][x][y]&lt;0 else 'black')</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/75f/1fc/4c775f1fc19bd461679d5a45831f1e2e.png" alt="Bild"><br><br>  <b>Schritt 3</b> : Bestimmen Sie die SNA <br><br>  Dieser SNS hat eine Faltungsschicht und eine Poolschicht mit einer maximalen Funktion, und die Gewichte werden unter Verwendung der oben gezeigten Filter initialisiert <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, weight)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() <span class="hljs-comment"><span class="hljs-comment"># initializes the weights of the convolutional layer to be the weights of the 4 defined filters k_height, k_width = weight.shape[2:] # assumes there are 4 grayscale filters self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False) # initializes the weights of the convolutional layer self.conv.weight = torch.nn.Parameter(weight) # define a pooling layer self.pool = nn.MaxPool2d(2, 2) def forward(self, x): # calculates the output of a convolutional layer # pre- and post-activation conv_x = self.conv(x) activated_x = F.relu(conv_x) # applies pooling layer pooled_x = self.pool(activated_x) # returns all layers return conv_x, activated_x, pooled_x # instantiate the model and set the weights weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor) model = Net(weight) # print out the layer in the network print(model)</span></span></code> </pre><br><blockquote><pre> <code class="plaintext hljs">Net( (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) )</code> </pre> </blockquote>  <b>Schritt 4</b> : Filter rendern <br>  Ein kurzer Blick auf die verwendeten Filter, <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">viz_layer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(layer, n_filters= </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_filters): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_filters, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ax.imshow(np.squeeze(layer[<span class="hljs-number"><span class="hljs-number">0</span></span>,i].data.numpy()), cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Output %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) fig.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1.5</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0.8</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, i+<span class="hljs-number"><span class="hljs-number">1</span></span>, xticks=[], yticks=[]) ax.imshow(filters[i], cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Filter %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>).unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Filter: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/885/5c9/cac/8855c9cace448bed1d831c3dc4731828.png" alt="Bild"><br><br>  <b>Schritt 5</b> : Gefilterte Ergebnisse nach Ebene <br><br>  Die Bilder, die in der Ebene CONV und POOL angezeigt werden, werden unten angezeigt. <br><br><pre> <code class="python hljs">viz_layer(activated_layer) viz_layer(pooled_layer)</code> </pre><br>  Faltungsschichten <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c8/1bc/6bb4c81bc6ef16044dfc22e9e36bbaa6.png" alt="Bild"><br><br>  Ebenen b√ºndeln <br><br><img src="https://habrastorage.org/getpro/habr/post_images/789/278/823/78927882302ae10f6403ba3a498669fd.png" alt="Bild"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436838/">https://habr.com/ru/post/de436838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436798/index.html">Professionelle Deformation des Administrators</a></li>
<li><a href="../de436822/index.html">Android Robotics bis 2019: Die wahre Geschichte; in 5 Teilen; Teil 3</a></li>
<li><a href="../de436828/index.html">Der √úbergang zu Boost-1.65.1 und aufgetretene Fehler</a></li>
<li><a href="../de436830/index.html">Android Robotics bis 2019: Die wahre Geschichte; in 5 Teilen; Teil 5</a></li>
<li><a href="../de436836/index.html">Vorteile der Analyse von Level 7-Anwendungen in Firewalls. Teil 2. Sicherheit</a></li>
<li><a href="../de436840/index.html">Der Weg vom Glanz zur Neurowissenschaft: Ein thematischer Podcast √ºber Karrieren im Medien- und Content-Marketing</a></li>
<li><a href="../de436842/index.html">Veeam-L√∂sung zur Sicherung und Wiederherstellung virtueller Maschinen auf der Nutanix AHV-Plattform. Teil 2</a></li>
<li><a href="../de436846/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 348 (14. - 20. Januar 2019)</a></li>
<li><a href="../de436848/index.html">Die NSA k√ºndigt die Ver√∂ffentlichung eines internen Tools f√ºr das Reverse Engineering an</a></li>
<li><a href="../de436850/index.html">H√§ufige Fehler beim Schreiben von Komponententests. Yandex Vortrag</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>