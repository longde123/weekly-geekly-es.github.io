<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏾‍🤝‍👩🏼 🚣🏾 👨‍👧 Faltungsneurale Netze durch Visualisierungen in PyTorch verstehen 🗄️ 🛀🏽 🔮</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In unserer Zeit haben Maschinen eine Genauigkeit von 99% beim Verstehen und Definieren von Merkmalen und Objekten in Bildern erreicht. Wir sind jeden ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Faltungsneurale Netze durch Visualisierungen in PyTorch verstehen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436838/">  In unserer Zeit haben Maschinen eine Genauigkeit von 99% beim Verstehen und Definieren von Merkmalen und Objekten in Bildern erreicht.  Wir sind jeden Tag damit konfrontiert, zum Beispiel: Gesichtserkennung in der Kamera des Smartphones, die Möglichkeit, bei Google nach Fotos zu suchen, Text aus einem Barcode oder Büchern mit einer guten Geschwindigkeit zu scannen usw. Diese Effizienz der Maschine wurde durch ein spezielles neuronales Netzwerk ermöglicht, das als Faltungs-Neuronales bezeichnet wird das Netzwerk.  Wenn Sie ein Deep-Learning-Enthusiast sind, haben Sie wahrscheinlich davon gehört, und Sie könnten mehrere Bildklassifizierer entwickeln.  Moderne Deep-Learning-Frameworks wie Tensorflow und PyTorch vereinfachen das maschinelle Lernen von Bildern.  Es bleibt jedoch die Frage: Wie passieren die Daten die Schichten des neuronalen Netzwerks und wie lernt der Computer daraus?  Um eine klare Sicht von Grund auf zu erhalten, tauchen wir in eine Faltung ein und visualisieren das Bild jeder Ebene. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/2c6/958/8592c6958985979587858374abd08f98.png" alt="Bild"><br><a name="habracut"></a><br><h2>  Faltungsneurale Netze </h2><br>  Bevor Sie mit dem Studium von Convolutional Neural Networks (SNA) beginnen, müssen Sie lernen, wie Sie mit neuronalen Netzen arbeiten.  Neuronale Netze ahmen das menschliche Gehirn nach, um komplexe Probleme zu lösen und nach Mustern in Daten zu suchen.  In den letzten Jahren haben sie viele Algorithmen für maschinelles Lernen und Computer Vision ersetzt.  Das Grundmodell eines neuronalen Netzwerks besteht aus Neuronen, die in Schichten organisiert sind.  Jedes neuronale Netzwerk verfügt über eine Eingabe- und Ausgabeschicht und mehrere verborgene Schichten, die je nach Komplexität des Problems hinzugefügt werden.  Bei der Übertragung von Daten durch Schichten werden Neuronen trainiert und erkennen Zeichen.  Diese Darstellung eines neuronalen Netzwerks wird als Modell bezeichnet.  Nachdem das Modell trainiert wurde, bitten wir das Netzwerk, Prognosen basierend auf Testdaten zu erstellen. <br><br>  Der SNS ist ein spezielles neuronales Netzwerk, das gut mit Bildern funktioniert.  Ian Lekun schlug sie 1998 vor, wo sie die im Eingabebild vorhandene Nummer erkannten.  SNA wird auch zur Spracherkennung, Bildsegmentierung und Textverarbeitung verwendet.  Vor der Schaffung von Faltungs-Neuronalen Netzen wurden mehrschichtige Perzeptrone bei der Konstruktion von Bildklassifikatoren verwendet.  Die Bildklassifizierung bezieht sich auf die Aufgabe, Klassen aus einem Mehrkanal-Rasterbild (Farbe, Schwarzweiß) zu extrahieren.  Mehrschichtige Perzeptrone benötigen viel Zeit, um nach Informationen in Bildern zu suchen, da jede Eingabe jedem Neuron in der nächsten Schicht zugeordnet werden muss.  Die SNA ging um sie herum und verwendete ein Konzept namens lokale Konnektivität.  Dies bedeutet, dass wir jedes Neuron nur mit der lokalen Eingangsregion verbinden.  Dies minimiert die Anzahl der Parameter und ermöglicht es verschiedenen Teilen des Netzwerks, sich auf übergeordnete Attribute wie Textur oder sich wiederholendes Muster zu spezialisieren.  Verwirrt?  Vergleichen wir, wie Bilder über mehrschichtige Perzeptrone (MPs) und Faltungs-Neuronale Netze übertragen werden. <br><br><h2>  Vergleich von MP und SNA </h2><br>  Die Gesamtzahl der Einträge in der Eingabeebene für das mehrschichtige Perzeptron beträgt 784, da das Eingabebild eine Größe von 28 x 28 = 784 hat (der MNIST-Datensatz wird berücksichtigt).  Das Netzwerk sollte in der Lage sein, die Anzahl im Eingabebild vorherzusagen. Dies bedeutet, dass die Ausgabe zu einer der folgenden Klassen im Bereich von 0 bis 9 gehören kann. In der Ausgabeschicht geben wir Klassenschätzungen zurück, z. B. wenn diese Eingabe das Bild mit der Nummer „3“ ist. dann hat in der Ausgabeschicht das entsprechende Neuron "3" einen höheren Wert als andere Neuronen.  Wieder stellt sich die Frage: "Wie viele versteckte Schichten brauchen wir und wie viele Neuronen sollten in jeder sein?"  Nehmen Sie zum Beispiel den folgenden MP-Code: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f8/efc/e14/3f8efce14418a7df0be2e813399def5d.png" alt="Bild"><br><br>  Der obige Code wird mithilfe eines Frameworks namens Keras implementiert.  Die erste verborgene Schicht hat 512 Neuronen, die mit der Eingangsschicht von 784 Neuronen verbunden sind.  Die nächste verborgene Schicht: die Ausschlussschicht, die das Problem der Umschulung löst.  0,2 bedeutet, dass eine Wahrscheinlichkeit von 20% besteht, die Neuronen der vorherigen verborgenen Schicht nicht zu berücksichtigen.  Wir haben erneut eine zweite verborgene Schicht mit der gleichen Anzahl von Neuronen wie in der ersten verborgenen Schicht (512) und dann eine weitere exklusive Schicht hinzugefügt.  Beenden Sie diesen Satz von Ebenen mit einer Ausgabeebene, die aus 10 Klassen besteht.  Die Klasse, die am wichtigsten ist, ist die vom Modell vorhergesagte Anzahl.  So sieht ein mehrschichtiges Netzwerk aus, nachdem alle Schichten identifiziert wurden.  Einer der Nachteile des mehrstufigen Perzeptrons besteht darin, dass es vollständig verbunden ist, was viel Zeit und Raum in Anspruch nimmt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db3/df6/605/db3df6605d0ddb868eb14b347227b963.png" alt="Bild"><br><br>  Convolts verwenden keine vollständig verbundenen Schichten.  Sie verwenden spärliche Schichten, die Matrizen als Eingabe verwenden, was einen Vorteil gegenüber MP bietet.  In MP ist jeder Knoten dafür verantwortlich, das gesamte Bild zu verstehen.  In der SNA teilen wir das Bild in Bereiche (kleine lokale Bereiche von Pixeln) auf.  Die Ausgabeschicht kombiniert die empfangenen Daten von jedem versteckten Knoten, um Muster zu finden.  Unten sehen Sie ein Bild davon, wie die Ebenen verbunden sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae6/f7f/d61/ae6f7fd618d5296a0deecabdd2e06e77.png" alt="Bild"><br><br>  Nun wollen wir sehen, wie die SNA Informationen auf Fotos findet.  Vorher müssen wir verstehen, wie die Zeichen extrahiert werden.  In der SNA verwenden wir verschiedene Ebenen. Jede Ebene bewahrt die Zeichen des Bildes. Beispielsweise berücksichtigt sie das Bild des Hundes. Wenn das Netzwerk den Hund klassifizieren muss, muss es alle Zeichen wie Augen, Ohren, Zunge, Beine usw. identifizieren.  Diese Zeichen werden auf lokaler Netzwerkebene mithilfe von Filtern und Kernen unterbrochen und erkannt. <br><br><h2>  Wie sehen Computer ein Bild aus? </h2><br>  Eine Person, die ein Bild betrachtet und seine Bedeutung versteht, klingt sehr vernünftig.  Nehmen wir an, Sie gehen und bemerken die vielen Landschaften um Sie herum.  Wie verstehen wir die Natur in diesem Fall?  Wir fotografieren die Umwelt mit unserem Hauptsinnesorgan - dem Auge - und senden es dann an die Netzhaut.  Es sieht alles ziemlich interessant aus, oder?  Stellen wir uns nun vor, dass ein Computer dasselbe tut.  In Computern werden Bilder mit einer Reihe von Pixelwerten interpretiert, die zwischen 0 und 255 liegen. Der Computer betrachtet diese Pixelwerte und versteht sie.  Auf den ersten Blick kennt er keine Objekte und Farben.  Es erkennt einfach die Pixelwerte und das Bild entspricht einer Reihe von Pixelwerten für den Computer.  Später lernt er durch Analyse der Pixelwerte nach und nach, ob das Bild grau oder farbig ist.  Bilder in Graustufen haben nur einen Kanal, da jedes Pixel die Intensität einer Farbe darstellt.  0 bedeutet schwarz und 255 bedeutet weiß, die anderen Varianten von schwarz und weiß, dh grau, liegen zwischen ihnen. <br><br>  Farbbilder haben drei Kanäle: Rot, Grün und Blau.  Sie repräsentieren die Intensität von 3 Farben (dreidimensionale Matrix), und wenn sich die Werte gleichzeitig ändern, ergibt sich eine große Anzahl von Farben, wirklich eine Farbpalette!  Danach erkennt der Computer die Kurven und Konturen von Objekten im Bild.  All dies kann im Faltungsnetzwerk untersucht werden.  Dazu verwenden wir PyTorch, um einen Datensatz zu laden und Filter auf Bilder anzuwenden.  Das Folgende ist ein Codeausschnitt. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load the libraries import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # Set the parameters num_workers = 0 batch_size = 20 # Converting the Images to tensors using Transforms transform = transforms.ToTensor() train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform) # Loading the Data train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # Peeking into dataset fig = plt.figure(figsize=(25, 4)) for image in np.arange(20): ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[image]), cmap='gray') ax.set_title(str(labels[image].item()))</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/304/163/1ad/3041631ad58d7300a35af90b39b94584.png" alt="Bild"><br><br>  Nun wollen wir sehen, wie ein einzelnes Bild in ein neuronales Netzwerk eingespeist wird. <br><br><pre> <code class="python hljs">img = np.squeeze(images[<span class="hljs-number"><span class="hljs-number">7</span></span>]) fig = plt.figure(figsize = (<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.imshow(img, cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) width, height = img.shape thresh = img.max()/<span class="hljs-number"><span class="hljs-number">2.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(width): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(height): val = round(img[x][y],<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y] !=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ax.annotate(str(val), xy=(y,x), color=<span class="hljs-string"><span class="hljs-string">'white'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y]&lt;thresh <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'black'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/264/f15/bff/264f15bffe653ae237f3e2fa1fc5c868.png" alt="Bild"><br><br>  Auf diese Weise wird die Zahl „3“ in Pixel aufgeteilt.  Aus dem Satz handgeschriebener Ziffern wird zufällig „3“ ausgewählt, in dem Pixelwerte angezeigt werden.  Hier normalisiert ToTensor () die tatsächlichen Pixelwerte (0–255) und begrenzt sie auf einen Bereich von 0 bis 1. Warum ist das so?  Weil es die Berechnungen in den folgenden Abschnitten erleichtert, entweder um Bilder zu interpretieren oder um gemeinsame Muster zu finden, die in ihnen vorhanden sind. <br><br><h2>  Erstellen Sie Ihren eigenen Filter </h2><br>  Filter filtern, wie der Name schon sagt, Informationen.  Bei Faltungs-Neuronalen Netzen werden beim Arbeiten mit Bildern Informationen über die Pixel gefiltert.  Warum sollten wir überhaupt filtern?  Denken Sie daran, dass ein Computer einen Lernprozess durchlaufen muss, um Bilder zu verstehen, ähnlich wie es ein Kind tut.  In diesem Fall brauchen wir jedoch nicht viele Jahre!  Kurz gesagt, er lernt von Grund auf neu und rückt dann zum Ganzen vor. <br><br>  Daher sollte das Netzwerk zunächst alle groben Teile des Bildes kennen, nämlich die Kanten, Konturen und andere Elemente auf niedriger Ebene.  Sobald sie entdeckt wurden, ist der Weg für komplexe Symptome geebnet.  Um zu ihnen zu gelangen, müssen wir zuerst die Attribute auf niedriger Ebene extrahieren, dann die Attribute auf mittlerer und dann auf höherer Ebene.  Filter sind eine Möglichkeit, die Informationen zu extrahieren, die der Benutzer benötigt, und nicht nur die blinde Datenübertragung, aufgrund derer der Computer die Strukturierung von Bildern nicht versteht.  Zu Beginn können Funktionen auf niedriger Ebene basierend auf einem bestimmten Filter extrahiert werden.  Der Filter ist hier auch eine Reihe von Pixelwerten, ähnlich einem Bild.  Es kann als die Gewichte verstanden werden, die die Schichten in dem Faltungs-Neuronalen Netzwerk verbinden.  Diese Gewichte oder Filter werden mit Eingabewerten multipliziert, um Zwischenbilder zu erzeugen, die das Computerverständnis des Bildes darstellen.  Dann werden sie mit einigen weiteren Filtern multipliziert, um die Ansicht zu erweitern.  Dann erkennt es die sichtbaren Organe einer Person (vorausgesetzt, eine Person ist im Bild vorhanden).  Später, mit mehreren weiteren Filtern und mehreren Schichten, ruft der Computer aus: „Oh, ja!  Das ist ein Mann. " <br><br>  Wenn wir über Filter sprechen, haben wir viele Möglichkeiten.  Möglicherweise möchten Sie das Bild verwischen und dann einen Unschärfefilter anwenden. Wenn Sie Schärfe hinzufügen müssen, hilft ein Schärfefilter usw. <br><br>  Schauen wir uns einige Codefragmente an, um die Funktionalität von Filtern zu verstehen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/679/6a4/bb4/6796a4bb4830bda29c6d14212274a286.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/752/a89/805/752a89805fba54b5d0f9e90073ca9fde.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/84f/8a1/f9c/84f8a1f9c92b1996b0e4eed4a2a7dd5b.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/142/635/038/142635038ecef3606d53d5d9c85b26f8.png" alt="Bild"><br><br>  So sieht das Bild nach dem Anwenden des Filters aus. In diesem Fall haben wir den Sobel-Filter verwendet. <br><br><h2>  Faltungsneurale Netze </h2><br>  Bisher haben wir gesehen, wie Filter verwendet werden, um Features aus Bildern zu extrahieren.  Um das gesamte neuronale Faltungsnetzwerk zu vervollständigen, müssen wir alle Schichten kennen, die wir zum Entwerfen verwenden.  Die in der SNA verwendeten Schichten, <br><br><ol><li>  Faltungsschicht </li><li>  Pooling-Schicht </li><li>  Vollständig verklebte Schicht </li></ol><br>  Bei allen drei Ebenen sieht der Faltungsbildklassifizierer folgendermaßen aus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/927/c31/8b4927c31b5f951d7026b30d68695bea.png" alt="Bild"><br><br>  Nun wollen wir sehen, was jede Ebene tut. <br><br>  <b>Die Faltungsschicht (CONV)</b> verwendet Filter, die Faltungsoperationen ausführen, indem sie das Eingabebild scannen.  Seine Hyperparameter umfassen eine Filtergröße, die 2x2, 3x3, 4x4, 5x5 (aber nicht darauf beschränkt) sein kann, und Schritt S. Das Ergebnis O wird als Feature-Map oder Aktivierungs Map bezeichnet, in der alle Features mithilfe von Eingabeebenen und Filtern berechnet werden.  Unten sehen Sie ein Bild der Generierung von Feature-Maps beim Anwenden der Faltung <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/14d/aab/a2a14daab68c91f8d92ba0c54509493b.png" alt="Bild"><br><br>  <b>Die Zusammenführungsschicht (POOL) wird</b> verwendet, um die Merkmale zu komprimieren, die typischerweise nach der Faltungsschicht verwendet werden.  Es gibt zwei Arten von Gewerkschaftsoperationen: Dies ist die maximale und durchschnittliche Vereinigung, bei der die maximalen und durchschnittlichen Werte der Merkmale verwendet werden.  Das Folgende ist die Operation von Zusammenführungsoperationen <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c2d/03f/2f8/c2d03f2f8734efade8cbc80d44d3767e.png" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/e5c/558/01ae5c558fa6647bfb9c19b9edabbb37.png" alt="Bild"><br><br>  <b>Vollständig verbundene Schichten (FCs)</b> arbeiten mit einem flachen Eingang, wobei jeder Eingang mit allen Neuronen verbunden ist.  Sie werden normalerweise am Ende des Netzwerks verwendet, um verborgene Schichten mit der Ausgabeschicht zu verbinden, wodurch die Klassenwerte optimiert werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d28/558/188/d285581882fa97824cdc0ad6ecb31873.png" alt="Bild"><br><br><h3>  SNA-Visualisierung in PyTorch </h3><br>  Nachdem wir nun die vollständige Ideologie zum Erstellen des SNA haben, implementieren wir den SNA mithilfe des PyTorch-Frameworks von Facebook. <br><br>  <b>Schritt 1</b> : Laden Sie das Eingabebild herunter, das über das Netzwerk gesendet werden soll.  (Hier machen wir es mit Numpy und OpenCV) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline img_path = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> bgr_img = cv2.imread(img_path) gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"><span class="hljs-comment"># Normalise gray_img = gray_img.astype("float32")/255 plt.imshow(gray_img, cmap='gray') plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d0f/3d2/291d0f3d28f3091716c3aba41dc35c59.png" alt="Bild"><br><br>  <b>Schritt 2</b> : Filter rendern <br><br>  Lassen Sie uns die Filter visualisieren, um besser zu verstehen, welche wir verwenden werden. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np filter_vals = np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] ]) print(<span class="hljs-string"><span class="hljs-string">'Filter shape: '</span></span>, filter_vals.shape) <span class="hljs-comment"><span class="hljs-comment"># Defining the Filters filter_1 = filter_vals filter_2 = -filter_1 filter_3 = filter_1.T filter_4 = -filter_3 filters = np.array([filter_1, filter_2, filter_3, filter_4]) # Check the Filters fig = plt.figure(figsize=(10, 5)) for i in range(4): ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[]) ax.imshow(filters[i], cmap='gray') ax.set_title('Filter %s' % str(i+1)) width, height = filters[i].shape for x in range(width): for y in range(height): ax.annotate(str(filters[i][x][y]), xy=(y,x), color='white' if filters[i][x][y]&lt;0 else 'black')</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/75f/1fc/4c775f1fc19bd461679d5a45831f1e2e.png" alt="Bild"><br><br>  <b>Schritt 3</b> : Bestimmen Sie die SNA <br><br>  Dieser SNS hat eine Faltungsschicht und eine Poolschicht mit einer maximalen Funktion, und die Gewichte werden unter Verwendung der oben gezeigten Filter initialisiert <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, weight)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() <span class="hljs-comment"><span class="hljs-comment"># initializes the weights of the convolutional layer to be the weights of the 4 defined filters k_height, k_width = weight.shape[2:] # assumes there are 4 grayscale filters self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False) # initializes the weights of the convolutional layer self.conv.weight = torch.nn.Parameter(weight) # define a pooling layer self.pool = nn.MaxPool2d(2, 2) def forward(self, x): # calculates the output of a convolutional layer # pre- and post-activation conv_x = self.conv(x) activated_x = F.relu(conv_x) # applies pooling layer pooled_x = self.pool(activated_x) # returns all layers return conv_x, activated_x, pooled_x # instantiate the model and set the weights weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor) model = Net(weight) # print out the layer in the network print(model)</span></span></code> </pre><br><blockquote><pre> <code class="plaintext hljs">Net( (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) )</code> </pre> </blockquote>  <b>Schritt 4</b> : Filter rendern <br>  Ein kurzer Blick auf die verwendeten Filter, <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">viz_layer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(layer, n_filters= </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_filters): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_filters, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ax.imshow(np.squeeze(layer[<span class="hljs-number"><span class="hljs-number">0</span></span>,i].data.numpy()), cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Output %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) fig.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1.5</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0.8</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, i+<span class="hljs-number"><span class="hljs-number">1</span></span>, xticks=[], yticks=[]) ax.imshow(filters[i], cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Filter %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>).unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Filter: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/885/5c9/cac/8855c9cace448bed1d831c3dc4731828.png" alt="Bild"><br><br>  <b>Schritt 5</b> : Gefilterte Ergebnisse nach Ebene <br><br>  Die Bilder, die in der Ebene CONV und POOL angezeigt werden, werden unten angezeigt. <br><br><pre> <code class="python hljs">viz_layer(activated_layer) viz_layer(pooled_layer)</code> </pre><br>  Faltungsschichten <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c8/1bc/6bb4c81bc6ef16044dfc22e9e36bbaa6.png" alt="Bild"><br><br>  Ebenen bündeln <br><br><img src="https://habrastorage.org/getpro/habr/post_images/789/278/823/78927882302ae10f6403ba3a498669fd.png" alt="Bild"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436838/">https://habr.com/ru/post/de436838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436798/index.html">Professionelle Deformation des Administrators</a></li>
<li><a href="../de436822/index.html">Android Robotics bis 2019: Die wahre Geschichte; in 5 Teilen; Teil 3</a></li>
<li><a href="../de436828/index.html">Der Übergang zu Boost-1.65.1 und aufgetretene Fehler</a></li>
<li><a href="../de436830/index.html">Android Robotics bis 2019: Die wahre Geschichte; in 5 Teilen; Teil 5</a></li>
<li><a href="../de436836/index.html">Vorteile der Analyse von Level 7-Anwendungen in Firewalls. Teil 2. Sicherheit</a></li>
<li><a href="../de436840/index.html">Der Weg vom Glanz zur Neurowissenschaft: Ein thematischer Podcast über Karrieren im Medien- und Content-Marketing</a></li>
<li><a href="../de436842/index.html">Veeam-Lösung zur Sicherung und Wiederherstellung virtueller Maschinen auf der Nutanix AHV-Plattform. Teil 2</a></li>
<li><a href="../de436846/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 348 (14. - 20. Januar 2019)</a></li>
<li><a href="../de436848/index.html">Die NSA kündigt die Veröffentlichung eines internen Tools für das Reverse Engineering an</a></li>
<li><a href="../de436850/index.html">Häufige Fehler beim Schreiben von Komponententests. Yandex Vortrag</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>