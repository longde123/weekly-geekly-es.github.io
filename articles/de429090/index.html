<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙋🏿 👨🏿‍💼 🚲 Einführung in verstärktes Lernen 👩🏼‍🤝‍👨🏽 🕞 🖕</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! 

 Wir haben einen neuen Stream für den Kurs für maschinelles Lernen eröffnet. Warten Sie also in naher Zukunft auf Artikel, die sic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Einführung in verstärktes Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/">  Hallo allerseits! <br><br>  Wir haben einen neuen Stream für den Kurs für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">maschinelles Lernen</a> eröffnet. Warten Sie also in naher Zukunft auf Artikel, die sich sozusagen auf diese Disziplin beziehen.  Natürlich offene Seminare.  Schauen wir uns nun an, was Verstärkungslernen ist. <br><br>  Verstärktes Lernen ist eine wichtige Form des maschinellen Lernens, bei der ein Agent lernt, sich in einer Umgebung zu verhalten, indem er Aktionen ausführt und Ergebnisse sieht. <br><br>  In den letzten Jahren haben wir auf diesem faszinierenden Forschungsgebiet viele Erfolge erzielt.  Zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind und Deep Q Learning Architecture</a> im Jahr 2014, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sieg über den Go-Champion mit AlphaGo</a> im Jahr 2016, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI und PPO</a> im Jahr 2017, unter anderem. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  In dieser Artikelserie konzentrieren wir uns auf die verschiedenen Architekturen, die heute zur Lösung des Problems des verstärkten Lernens verwendet werden.  Dazu gehören Q-Learning, Deep Q-Learning, Policy Gradients, Actor Critic und PPO. <br><br>  In diesem Artikel erfahren Sie: <br><br><ul><li>  Was ist verstärkendes Lernen und warum Belohnungen eine zentrale Idee sind </li><li>  Drei Ansätze zum verstärkten Lernen </li><li>  Was „tief“ bedeutet, um tief zu lernen </li></ul><br>  Es ist sehr wichtig, diese Aspekte zu beherrschen, bevor Sie in die Implementierung von Verstärkungslernmitteln eintauchen. <br><br>  Die Idee des Verstärkungstrainings ist, dass der Agent aus der Umgebung lernt, indem er mit ihr interagiert und Belohnungen für die Durchführung von Aktionen erhält. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  Lernen durch Interaktion mit der Umwelt kommt aus unserer natürlichen Erfahrung.  Stellen Sie sich vor, Sie sind ein Kind im Wohnzimmer.  Sie sehen den Kamin und gehen dorthin. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  In der Nähe warm fühlen Sie sich gut (positive Belohnung +1).  Sie verstehen, dass Feuer eine positive Sache ist. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  Aber dann versuchst du das Feuer zu berühren.  Autsch!  Er verbrannte sich die Hand (negative Belohnung -1).  Sie haben gerade festgestellt, dass Feuer positiv ist, wenn Sie sich in ausreichender Entfernung befinden, weil es Wärme erzeugt.  Aber wenn Sie sich ihm nähern, werden Sie sich verbrennen. <br><br>  So lernen Menschen durch Interaktion.  Verstärktes Lernen ist einfach ein rechnerischer Ansatz zum Lernen durch Handeln. <br><br>  <b>Lernprozess zur Verstärkung</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  Stellen Sie sich als Beispiel einen Agenten vor, der lernt, Super Mario Bros. zu spielen.  Der Reinforcement Learning (RL) -Prozess kann als ein Zyklus modelliert werden, der wie folgt funktioniert: <br><br><ul><li>  Der Agent erhält den Status S0 von der Umgebung (in unserem Fall erhalten wir den ersten Frame des Spiels (Status) von Super Mario Bros (Umgebung)). </li><li>  Basierend auf diesem Zustand S0 ergreift der Agent die Aktion A0 (der Agent bewegt sich nach rechts). </li><li>  Die Umgebung wechselt in einen neuen Zustand S1 (neuer Frame) </li><li>  Die Umgebung gibt dem R1-Agenten eine Belohnung (nicht tot: +1) </li></ul><br>  Dieser RL-Zyklus erzeugt eine Folge von <b>Zuständen, Aktionen und Belohnungen.</b> <br>  Das Ziel des Agenten ist es, die erwarteten akkumulierten Belohnungen zu maximieren. <br><br>  <b>Zentrale Ideenprämienhypothesen</b> <br><br>  Warum ist es das Ziel eines Agenten, die erwarteten kumulierten Belohnungen zu maximieren?  Nun, das Lernen der Verstärkung basiert auf der Idee einer Belohnungshypothese.  Alle Ziele können durch Maximierung der erwarteten kumulierten Belohnungen beschrieben werden. <br><br>  <b>Um das beste Verhalten zu erzielen, müssen wir daher im Verstärkungstraining die erwarteten akkumulierten Belohnungen maximieren.</b> <br><br>  Die akkumulierte Belohnung zu jedem Zeitpunkt Schritt t kann wie folgt geschrieben werden: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  Dies entspricht: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  In Wirklichkeit können wir solche Belohnungen jedoch nicht einfach hinzufügen.  Belohnungen, die früher (zu Beginn des Spiels) eintreffen, sind wahrscheinlicher, weil sie in Zukunft vorhersehbarer sind als Belohnungen. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Angenommen, Ihr Agent ist eine kleine Maus und Ihr Gegner ist eine Katze.  Ihr Ziel ist es, die maximale Menge Käse zu essen, bevor die Katze Sie frisst.  Wie wir in der Abbildung sehen, frisst eine Maus eher Käse neben sich als Käse in der Nähe einer Katze (je näher wir ihm sind, desto gefährlicher ist er). <br><br>  Infolgedessen wird die Belohnung einer Katze, selbst wenn sie größer ist (mehr Käse), verringert.  Wir sind uns nicht sicher, ob wir es essen können.  Um die Vergütung zu reduzieren, gehen wir wie folgt vor: <br><br><ul><li>  Wir bestimmen den Abzinsungssatz namens Gamma.  Es sollte zwischen 0 und 1 liegen. </li><li>  Je größer das Gamma, desto geringer der Rabatt.  Dies bedeutet, dass sich der Lernagent mehr um langfristige Belohnungen kümmert. </li><li>  Andererseits ist der Rabatt umso größer, je kleiner das Gamma ist.  Dies bedeutet, dass kurzfristigen Belohnungen (nächster Käse) Vorrang eingeräumt wird. </li></ul><br>  Die kumulierte erwartete Gegenleistung unter Berücksichtigung der Diskontierung lautet wie folgt: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  Grob gesagt wird jede Belohnung mit dem Gamma auf die Zeitanzeige reduziert.  Mit zunehmendem Zeitschritt kommt die Katze uns näher, sodass die zukünftige Belohnung immer unwahrscheinlicher wird. <br><br>  <b>Gelegentliche oder kontinuierliche Aufgaben</b> <br><br>  Eine Aufgabe ist ein Beispiel für das Lernproblem mit Verstärkung.  Wir können zwei Arten von Aufgaben haben: episodische und kontinuierliche. <br><br>  <b>Episodische Aufgabe</b> <br><br>  In diesem Fall haben wir einen Startpunkt und einen Endpunkt <b>(Endzustand).</b>  <b>Dadurch wird eine Episode erstellt</b> : eine Liste von Zuständen, Aktionen, Belohnungen und neuen Zuständen. <br>  Nehmen wir zum Beispiel Super Mario Bros: Die Episode beginnt mit dem Start des neuen Mario und endet, wenn Sie getötet werden oder das Ende des Levels erreichen. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>Der Beginn einer neuen Episode</i> <br><br>  <b>Kontinuierliche Aufgaben</b> <br><br>  <b>Dies sind Aufgaben, die für immer andauern (ohne Terminalstatus)</b> .  In diesem Fall muss der Agent lernen, die besten Aktionen auszuwählen und gleichzeitig mit der Umgebung zu interagieren. <br><br>  Zum Beispiel ein Agent, der den automatisierten Aktienhandel durchführt.  Für diese Aufgabe gibt es keinen Startpunkt und keinen Endstatus.  <b>Der Agent arbeitet weiter, bis wir uns entschließen, ihn aufzuhalten.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>Monte-Carlo-Zeitdifferenzmethode</b> <br><br>  Es gibt zwei Möglichkeiten zu lernen: <br><br><ul><li>  Sammeln Sie am Ende der Episode Belohnungen und berechnen Sie dann die maximal erwarteten zukünftigen Belohnungen - Monte-Carlo-Ansatz </li><li>  Bewertung der Belohnungen bei jedem Schritt - ein vorübergehender Unterschied </li></ul><br>  <b>Monte Carlo</b> <br><br>  Wenn die Episode endet (der Agent erreicht einen „Endzustand“), überprüft der Agent die insgesamt angesammelte Belohnung, um festzustellen, wie gut er es gemacht hat.  Beim Monte-Carlo-Ansatz werden Belohnungen erst am Ende des Spiels erhalten. <br><br>  Dann starten wir ein neues Spiel mit erweitertem Wissen.  <b>Der Agent trifft bei jeder Iteration die besten Entscheidungen.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Hier ist ein Beispiel: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Wenn wir das Labyrinth als Umgebung betrachten: <br><br><ul><li>  Wir beginnen immer am selben Ausgangspunkt. </li><li>  Wir stoppen die Episode, wenn die Katze uns frisst oder wir&gt; 20 Schritte gehen. </li><li>  Am Ende der Episode haben wir eine Liste von Zuständen, Aktionen, Belohnungen und neuen Zuständen. </li><li>  Der Agent fasst die Gesamtbelohnung von Gt zusammen (um zu sehen, wie gut er es gemacht hat). </li><li>  Dann aktualisiert es V (st) gemäß der obigen Formel. </li><li>  Dann beginnt ein neues Spiel mit neuem Wissen. </li></ul><br>  In immer mehr Folgen <b>lernt</b> der <b>Agent, immer besser zu spielen.</b> <br><br>  <b>Zeitunterschiede: Lernen zu jedem Zeitschritt</b> <br><br>  Die TD-Methode (Temporal Difference Learning) wartet nicht auf das Ende der Episode, um die höchstmögliche Belohnung zu aktualisieren.  Er wird V abhängig von der gesammelten Erfahrung aktualisieren. <br><br>  Diese Methode wird als TD (0) oder <b>schrittweise TD bezeichnet (aktualisiert die Dienstprogrammfunktion nach jedem einzelnen Schritt).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  TD-Methoden erwarten nur, dass der nächste <b>Zeitschritt die Werte aktualisiert.</b>  Zum Zeitpunkt t + 1 <b>wird ein TD-Ziel unter Verwendung der Belohnung Rt + 1 und der aktuellen Bewertung V (St + 1) gebildet.</b> <br><br>  Das TD-Ziel ist eine Schätzung des erwarteten Wertes: Tatsächlich aktualisieren Sie die vorherige V (St) -Bewertung innerhalb eines Schritts auf das Ziel. <br><br>  <b>Kompromiss Exploration / Betrieb</b> <br><br>  Bevor wir verschiedene Strategien zur Lösung von Problemen mit dem Verstärkungstraining in Betracht ziehen, müssen wir ein weiteres sehr wichtiges Thema betrachten: den Kompromiss zwischen Exploration und Exploitation. <br><br><ul><li>  Intelligenz findet mehr Informationen über die Umgebung. </li><li>  Die Nutzung nutzt bekannte Informationen, um die Belohnungen zu maximieren. </li></ul><br>  Denken Sie daran, dass das Ziel unseres RL-Agenten darin besteht, die erwarteten akkumulierten Belohnungen zu maximieren.  Wir können jedoch in eine gemeinsame Falle tappen. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  In diesem Spiel kann unsere Maus unendlich viele kleine Käsestücke haben (jeweils +1).  Aber oben im Labyrinth befindet sich ein riesiges Stück Käse (+1000).  Wenn wir uns jedoch nur auf Belohnungen konzentrieren, wird unser Agent niemals einen riesigen Teil erreichen.  Stattdessen verwendet er nur die nächstgelegene Belohnungsquelle, auch wenn diese Quelle klein ist (Ausbeutung).  Aber wenn unser Agent ein wenig nachforscht, kann er eine große Belohnung finden. <br><br>  Dies ist ein Kompromiss zwischen Exploration und Exploitation.  Wir müssen eine Regel definieren, die bei der Bewältigung dieses Kompromisses hilft.  In zukünftigen Artikeln erfahren Sie verschiedene Möglichkeiten, dies zu tun. <br><br>  <b>Drei Ansätze zum verstärkten Lernen</b> <br><br>  Nachdem wir die Hauptelemente des verstärkten Lernens identifiziert haben, gehen wir zu drei Ansätzen zur Lösung des verstärkten Lernens über: kostenbasiert, politikbasiert und modellbasiert. <br><br>  <b>Basierend auf den Kosten</b> <br><br>  In der kostenbasierten RL besteht das Ziel darin, die Nutzfunktion V (s) zu optimieren. <br>  Eine Utility-Funktion ist eine Funktion, die uns über die maximal erwartete Belohnung informiert, die ein Agent in jedem Status erhält. <br><br>  Der Wert jedes Status ist der Gesamtbetrag der Belohnung, die der Agent ab diesem Status in Zukunft erwarten kann. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  Der Agent verwendet diese Dienstprogrammfunktion, um zu entscheiden, welcher Status bei jedem Schritt ausgewählt werden soll.  Der Agent wählt den Status mit dem höchsten Wert aus. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  Im Labyrinthbeispiel nehmen wir bei jedem Schritt den höchsten Wert: -7, dann -6, dann -5 (usw.), um das Ziel zu erreichen. <br><br>  <b>Richtlinienbasiert</b> <br><br>  In richtlinienbasiertem RL möchten wir die Richtlinienfunktion π (s) direkt optimieren, ohne die Dienstprogrammfunktion zu verwenden.  Eine Richtlinie bestimmt das Verhalten eines Agenten zu einem bestimmten Zeitpunkt. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>Aktion = Politik (Staat)</i> <br>  Wir untersuchen die Funktion der Politik.  Dies ermöglicht es uns, jeden Zustand mit der am besten geeigneten Aktion zu korrelieren. <br><br>  Es gibt zwei Arten von Richtlinien: <br><br><ul><li>  Deterministisch: Politik in einem bestimmten Staat wird immer die gleiche Aktion zurückgeben. </li><li>  Stochastisch: Zeigt die Wahrscheinlichkeit der Verteilung durch Aktion an. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  Wie Sie sehen können, gibt die Richtlinie direkt die beste Aktion für jeden Schritt an. <br><br>  <b>Basierend auf dem Modell</b> <br><br>  In modellbasiertem RL modellieren wir die Umgebung.  Dies bedeutet, dass wir ein Modell des Umweltverhaltens erstellen.  Das Problem ist, dass jede Umgebung eine andere Ansicht des Modells benötigt.  Aus diesem Grund werden wir uns in den folgenden Artikeln nicht besonders auf diese Art der Schulung konzentrieren. <br><br>  <b>Einführung in tiefgreifendes Lernen</b> <br><br>  Deep Reinforcement Learning führt tiefe neuronale Netze ein, um die Probleme des verstärkten Lernens zu lösen - daher der Name „tief“. <br>  Zum Beispiel werden wir im nächsten Artikel über Q-Learning (klassisches Verstärkungslernen) und Deep Q-Learning arbeiten. <br><br>  Sie werden den Unterschied in der Tatsache sehen, dass wir beim ersten Ansatz den traditionellen Algorithmus verwenden, um die Q-Tabelle zu erstellen, mit deren Hilfe wir herausfinden können, welche Maßnahmen für jeden Zustand zu ergreifen sind. <br><br>  Im zweiten Ansatz verwenden wir ein neuronales Netzwerk (um zustandsbasierte Belohnungen zu approximieren: q-Wert). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Udacity Inspired Q Design Chart</i> <i><br></i> <br><br>  Das ist alles.  Wie immer warten wir hier auf Ihre Kommentare oder Fragen, oder Sie können sie dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurslehrer Arthur Kadurin</a> in seiner <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offenen Lektion zum</a> Thema Networking stellen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de429090/">https://habr.com/ru/post/de429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de429078/index.html">So erstellen Sie prozedurale Kunst in weniger als 100 Codezeilen</a></li>
<li><a href="../de429082/index.html">Thailand ohne Stereotypen</a></li>
<li><a href="../de429084/index.html">Das zweite Leben des Elektroofens "Kharkov"</a></li>
<li><a href="../de429086/index.html">Backup Bierparty</a></li>
<li><a href="../de429088/index.html">Ausführen von GraphQL-Abfragen mit OdataToEntity</a></li>
<li><a href="../de429092/index.html">Warum ist Stealth im Weltraum immer noch da?</a></li>
<li><a href="../de429094/index.html">Directional Sound: Technologie, die Kopfhörer ersetzen kann - wie es funktioniert</a></li>
<li><a href="../de429096/index.html">Antiquitäten: ZX Spectrum, Kassettenprogramme und High Definition</a></li>
<li><a href="../de429102/index.html">Verkauf von Elektrofahrzeugen in Kanada</a></li>
<li><a href="../de429104/index.html">Hohe Daten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>