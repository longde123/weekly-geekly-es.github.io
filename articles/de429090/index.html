<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ™‹ğŸ¿ ğŸ‘¨ğŸ¿â€ğŸ’¼ ğŸš² EinfÃ¼hrung in verstÃ¤rktes Lernen ğŸ‘©ğŸ¼â€ğŸ¤â€ğŸ‘¨ğŸ½ ğŸ• ğŸ–•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! 

 Wir haben einen neuen Stream fÃ¼r den Kurs fÃ¼r maschinelles Lernen erÃ¶ffnet. Warten Sie also in naher Zukunft auf Artikel, die sic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>EinfÃ¼hrung in verstÃ¤rktes Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/">  Hallo allerseits! <br><br>  Wir haben einen neuen Stream fÃ¼r den Kurs fÃ¼r <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">maschinelles Lernen</a> erÃ¶ffnet. Warten Sie also in naher Zukunft auf Artikel, die sich sozusagen auf diese Disziplin beziehen.  NatÃ¼rlich offene Seminare.  Schauen wir uns nun an, was VerstÃ¤rkungslernen ist. <br><br>  VerstÃ¤rktes Lernen ist eine wichtige Form des maschinellen Lernens, bei der ein Agent lernt, sich in einer Umgebung zu verhalten, indem er Aktionen ausfÃ¼hrt und Ergebnisse sieht. <br><br>  In den letzten Jahren haben wir auf diesem faszinierenden Forschungsgebiet viele Erfolge erzielt.  Zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind und Deep Q Learning Architecture</a> im Jahr 2014, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sieg Ã¼ber den Go-Champion mit AlphaGo</a> im Jahr 2016, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI und PPO</a> im Jahr 2017, unter anderem. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  In dieser Artikelserie konzentrieren wir uns auf die verschiedenen Architekturen, die heute zur LÃ¶sung des Problems des verstÃ¤rkten Lernens verwendet werden.  Dazu gehÃ¶ren Q-Learning, Deep Q-Learning, Policy Gradients, Actor Critic und PPO. <br><br>  In diesem Artikel erfahren Sie: <br><br><ul><li>  Was ist verstÃ¤rkendes Lernen und warum Belohnungen eine zentrale Idee sind </li><li>  Drei AnsÃ¤tze zum verstÃ¤rkten Lernen </li><li>  Was â€tiefâ€œ bedeutet, um tief zu lernen </li></ul><br>  Es ist sehr wichtig, diese Aspekte zu beherrschen, bevor Sie in die Implementierung von VerstÃ¤rkungslernmitteln eintauchen. <br><br>  Die Idee des VerstÃ¤rkungstrainings ist, dass der Agent aus der Umgebung lernt, indem er mit ihr interagiert und Belohnungen fÃ¼r die DurchfÃ¼hrung von Aktionen erhÃ¤lt. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  Lernen durch Interaktion mit der Umwelt kommt aus unserer natÃ¼rlichen Erfahrung.  Stellen Sie sich vor, Sie sind ein Kind im Wohnzimmer.  Sie sehen den Kamin und gehen dorthin. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  In der NÃ¤he warm fÃ¼hlen Sie sich gut (positive Belohnung +1).  Sie verstehen, dass Feuer eine positive Sache ist. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  Aber dann versuchst du das Feuer zu berÃ¼hren.  Autsch!  Er verbrannte sich die Hand (negative Belohnung -1).  Sie haben gerade festgestellt, dass Feuer positiv ist, wenn Sie sich in ausreichender Entfernung befinden, weil es WÃ¤rme erzeugt.  Aber wenn Sie sich ihm nÃ¤hern, werden Sie sich verbrennen. <br><br>  So lernen Menschen durch Interaktion.  VerstÃ¤rktes Lernen ist einfach ein rechnerischer Ansatz zum Lernen durch Handeln. <br><br>  <b>Lernprozess zur VerstÃ¤rkung</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  Stellen Sie sich als Beispiel einen Agenten vor, der lernt, Super Mario Bros. zu spielen.  Der Reinforcement Learning (RL) -Prozess kann als ein Zyklus modelliert werden, der wie folgt funktioniert: <br><br><ul><li>  Der Agent erhÃ¤lt den Status S0 von der Umgebung (in unserem Fall erhalten wir den ersten Frame des Spiels (Status) von Super Mario Bros (Umgebung)). </li><li>  Basierend auf diesem Zustand S0 ergreift der Agent die Aktion A0 (der Agent bewegt sich nach rechts). </li><li>  Die Umgebung wechselt in einen neuen Zustand S1 (neuer Frame) </li><li>  Die Umgebung gibt dem R1-Agenten eine Belohnung (nicht tot: +1) </li></ul><br>  Dieser RL-Zyklus erzeugt eine Folge von <b>ZustÃ¤nden, Aktionen und Belohnungen.</b> <br>  Das Ziel des Agenten ist es, die erwarteten akkumulierten Belohnungen zu maximieren. <br><br>  <b>Zentrale IdeenprÃ¤mienhypothesen</b> <br><br>  Warum ist es das Ziel eines Agenten, die erwarteten kumulierten Belohnungen zu maximieren?  Nun, das Lernen der VerstÃ¤rkung basiert auf der Idee einer Belohnungshypothese.  Alle Ziele kÃ¶nnen durch Maximierung der erwarteten kumulierten Belohnungen beschrieben werden. <br><br>  <b>Um das beste Verhalten zu erzielen, mÃ¼ssen wir daher im VerstÃ¤rkungstraining die erwarteten akkumulierten Belohnungen maximieren.</b> <br><br>  Die akkumulierte Belohnung zu jedem Zeitpunkt Schritt t kann wie folgt geschrieben werden: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  Dies entspricht: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  In Wirklichkeit kÃ¶nnen wir solche Belohnungen jedoch nicht einfach hinzufÃ¼gen.  Belohnungen, die frÃ¼her (zu Beginn des Spiels) eintreffen, sind wahrscheinlicher, weil sie in Zukunft vorhersehbarer sind als Belohnungen. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Angenommen, Ihr Agent ist eine kleine Maus und Ihr Gegner ist eine Katze.  Ihr Ziel ist es, die maximale Menge KÃ¤se zu essen, bevor die Katze Sie frisst.  Wie wir in der Abbildung sehen, frisst eine Maus eher KÃ¤se neben sich als KÃ¤se in der NÃ¤he einer Katze (je nÃ¤her wir ihm sind, desto gefÃ¤hrlicher ist er). <br><br>  Infolgedessen wird die Belohnung einer Katze, selbst wenn sie grÃ¶ÃŸer ist (mehr KÃ¤se), verringert.  Wir sind uns nicht sicher, ob wir es essen kÃ¶nnen.  Um die VergÃ¼tung zu reduzieren, gehen wir wie folgt vor: <br><br><ul><li>  Wir bestimmen den Abzinsungssatz namens Gamma.  Es sollte zwischen 0 und 1 liegen. </li><li>  Je grÃ¶ÃŸer das Gamma, desto geringer der Rabatt.  Dies bedeutet, dass sich der Lernagent mehr um langfristige Belohnungen kÃ¼mmert. </li><li>  Andererseits ist der Rabatt umso grÃ¶ÃŸer, je kleiner das Gamma ist.  Dies bedeutet, dass kurzfristigen Belohnungen (nÃ¤chster KÃ¤se) Vorrang eingerÃ¤umt wird. </li></ul><br>  Die kumulierte erwartete Gegenleistung unter BerÃ¼cksichtigung der Diskontierung lautet wie folgt: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  Grob gesagt wird jede Belohnung mit dem Gamma auf die Zeitanzeige reduziert.  Mit zunehmendem Zeitschritt kommt die Katze uns nÃ¤her, sodass die zukÃ¼nftige Belohnung immer unwahrscheinlicher wird. <br><br>  <b>Gelegentliche oder kontinuierliche Aufgaben</b> <br><br>  Eine Aufgabe ist ein Beispiel fÃ¼r das Lernproblem mit VerstÃ¤rkung.  Wir kÃ¶nnen zwei Arten von Aufgaben haben: episodische und kontinuierliche. <br><br>  <b>Episodische Aufgabe</b> <br><br>  In diesem Fall haben wir einen Startpunkt und einen Endpunkt <b>(Endzustand).</b>  <b>Dadurch wird eine Episode erstellt</b> : eine Liste von ZustÃ¤nden, Aktionen, Belohnungen und neuen ZustÃ¤nden. <br>  Nehmen wir zum Beispiel Super Mario Bros: Die Episode beginnt mit dem Start des neuen Mario und endet, wenn Sie getÃ¶tet werden oder das Ende des Levels erreichen. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>Der Beginn einer neuen Episode</i> <br><br>  <b>Kontinuierliche Aufgaben</b> <br><br>  <b>Dies sind Aufgaben, die fÃ¼r immer andauern (ohne Terminalstatus)</b> .  In diesem Fall muss der Agent lernen, die besten Aktionen auszuwÃ¤hlen und gleichzeitig mit der Umgebung zu interagieren. <br><br>  Zum Beispiel ein Agent, der den automatisierten Aktienhandel durchfÃ¼hrt.  FÃ¼r diese Aufgabe gibt es keinen Startpunkt und keinen Endstatus.  <b>Der Agent arbeitet weiter, bis wir uns entschlieÃŸen, ihn aufzuhalten.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>Monte-Carlo-Zeitdifferenzmethode</b> <br><br>  Es gibt zwei MÃ¶glichkeiten zu lernen: <br><br><ul><li>  Sammeln Sie am Ende der Episode Belohnungen und berechnen Sie dann die maximal erwarteten zukÃ¼nftigen Belohnungen - Monte-Carlo-Ansatz </li><li>  Bewertung der Belohnungen bei jedem Schritt - ein vorÃ¼bergehender Unterschied </li></ul><br>  <b>Monte Carlo</b> <br><br>  Wenn die Episode endet (der Agent erreicht einen â€Endzustandâ€œ), Ã¼berprÃ¼ft der Agent die insgesamt angesammelte Belohnung, um festzustellen, wie gut er es gemacht hat.  Beim Monte-Carlo-Ansatz werden Belohnungen erst am Ende des Spiels erhalten. <br><br>  Dann starten wir ein neues Spiel mit erweitertem Wissen.  <b>Der Agent trifft bei jeder Iteration die besten Entscheidungen.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Hier ist ein Beispiel: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Wenn wir das Labyrinth als Umgebung betrachten: <br><br><ul><li>  Wir beginnen immer am selben Ausgangspunkt. </li><li>  Wir stoppen die Episode, wenn die Katze uns frisst oder wir&gt; 20 Schritte gehen. </li><li>  Am Ende der Episode haben wir eine Liste von ZustÃ¤nden, Aktionen, Belohnungen und neuen ZustÃ¤nden. </li><li>  Der Agent fasst die Gesamtbelohnung von Gt zusammen (um zu sehen, wie gut er es gemacht hat). </li><li>  Dann aktualisiert es V (st) gemÃ¤ÃŸ der obigen Formel. </li><li>  Dann beginnt ein neues Spiel mit neuem Wissen. </li></ul><br>  In immer mehr Folgen <b>lernt</b> der <b>Agent, immer besser zu spielen.</b> <br><br>  <b>Zeitunterschiede: Lernen zu jedem Zeitschritt</b> <br><br>  Die TD-Methode (Temporal Difference Learning) wartet nicht auf das Ende der Episode, um die hÃ¶chstmÃ¶gliche Belohnung zu aktualisieren.  Er wird V abhÃ¤ngig von der gesammelten Erfahrung aktualisieren. <br><br>  Diese Methode wird als TD (0) oder <b>schrittweise TD bezeichnet (aktualisiert die Dienstprogrammfunktion nach jedem einzelnen Schritt).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  TD-Methoden erwarten nur, dass der nÃ¤chste <b>Zeitschritt die Werte aktualisiert.</b>  Zum Zeitpunkt t + 1 <b>wird ein TD-Ziel unter Verwendung der Belohnung Rt + 1 und der aktuellen Bewertung V (St + 1) gebildet.</b> <br><br>  Das TD-Ziel ist eine SchÃ¤tzung des erwarteten Wertes: TatsÃ¤chlich aktualisieren Sie die vorherige V (St) -Bewertung innerhalb eines Schritts auf das Ziel. <br><br>  <b>Kompromiss Exploration / Betrieb</b> <br><br>  Bevor wir verschiedene Strategien zur LÃ¶sung von Problemen mit dem VerstÃ¤rkungstraining in Betracht ziehen, mÃ¼ssen wir ein weiteres sehr wichtiges Thema betrachten: den Kompromiss zwischen Exploration und Exploitation. <br><br><ul><li>  Intelligenz findet mehr Informationen Ã¼ber die Umgebung. </li><li>  Die Nutzung nutzt bekannte Informationen, um die Belohnungen zu maximieren. </li></ul><br>  Denken Sie daran, dass das Ziel unseres RL-Agenten darin besteht, die erwarteten akkumulierten Belohnungen zu maximieren.  Wir kÃ¶nnen jedoch in eine gemeinsame Falle tappen. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  In diesem Spiel kann unsere Maus unendlich viele kleine KÃ¤sestÃ¼cke haben (jeweils +1).  Aber oben im Labyrinth befindet sich ein riesiges StÃ¼ck KÃ¤se (+1000).  Wenn wir uns jedoch nur auf Belohnungen konzentrieren, wird unser Agent niemals einen riesigen Teil erreichen.  Stattdessen verwendet er nur die nÃ¤chstgelegene Belohnungsquelle, auch wenn diese Quelle klein ist (Ausbeutung).  Aber wenn unser Agent ein wenig nachforscht, kann er eine groÃŸe Belohnung finden. <br><br>  Dies ist ein Kompromiss zwischen Exploration und Exploitation.  Wir mÃ¼ssen eine Regel definieren, die bei der BewÃ¤ltigung dieses Kompromisses hilft.  In zukÃ¼nftigen Artikeln erfahren Sie verschiedene MÃ¶glichkeiten, dies zu tun. <br><br>  <b>Drei AnsÃ¤tze zum verstÃ¤rkten Lernen</b> <br><br>  Nachdem wir die Hauptelemente des verstÃ¤rkten Lernens identifiziert haben, gehen wir zu drei AnsÃ¤tzen zur LÃ¶sung des verstÃ¤rkten Lernens Ã¼ber: kostenbasiert, politikbasiert und modellbasiert. <br><br>  <b>Basierend auf den Kosten</b> <br><br>  In der kostenbasierten RL besteht das Ziel darin, die Nutzfunktion V (s) zu optimieren. <br>  Eine Utility-Funktion ist eine Funktion, die uns Ã¼ber die maximal erwartete Belohnung informiert, die ein Agent in jedem Status erhÃ¤lt. <br><br>  Der Wert jedes Status ist der Gesamtbetrag der Belohnung, die der Agent ab diesem Status in Zukunft erwarten kann. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  Der Agent verwendet diese Dienstprogrammfunktion, um zu entscheiden, welcher Status bei jedem Schritt ausgewÃ¤hlt werden soll.  Der Agent wÃ¤hlt den Status mit dem hÃ¶chsten Wert aus. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  Im Labyrinthbeispiel nehmen wir bei jedem Schritt den hÃ¶chsten Wert: -7, dann -6, dann -5 (usw.), um das Ziel zu erreichen. <br><br>  <b>Richtlinienbasiert</b> <br><br>  In richtlinienbasiertem RL mÃ¶chten wir die Richtlinienfunktion Ï€ (s) direkt optimieren, ohne die Dienstprogrammfunktion zu verwenden.  Eine Richtlinie bestimmt das Verhalten eines Agenten zu einem bestimmten Zeitpunkt. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>Aktion = Politik (Staat)</i> <br>  Wir untersuchen die Funktion der Politik.  Dies ermÃ¶glicht es uns, jeden Zustand mit der am besten geeigneten Aktion zu korrelieren. <br><br>  Es gibt zwei Arten von Richtlinien: <br><br><ul><li>  Deterministisch: Politik in einem bestimmten Staat wird immer die gleiche Aktion zurÃ¼ckgeben. </li><li>  Stochastisch: Zeigt die Wahrscheinlichkeit der Verteilung durch Aktion an. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  Wie Sie sehen kÃ¶nnen, gibt die Richtlinie direkt die beste Aktion fÃ¼r jeden Schritt an. <br><br>  <b>Basierend auf dem Modell</b> <br><br>  In modellbasiertem RL modellieren wir die Umgebung.  Dies bedeutet, dass wir ein Modell des Umweltverhaltens erstellen.  Das Problem ist, dass jede Umgebung eine andere Ansicht des Modells benÃ¶tigt.  Aus diesem Grund werden wir uns in den folgenden Artikeln nicht besonders auf diese Art der Schulung konzentrieren. <br><br>  <b>EinfÃ¼hrung in tiefgreifendes Lernen</b> <br><br>  Deep Reinforcement Learning fÃ¼hrt tiefe neuronale Netze ein, um die Probleme des verstÃ¤rkten Lernens zu lÃ¶sen - daher der Name â€tiefâ€œ. <br>  Zum Beispiel werden wir im nÃ¤chsten Artikel Ã¼ber Q-Learning (klassisches VerstÃ¤rkungslernen) und Deep Q-Learning arbeiten. <br><br>  Sie werden den Unterschied in der Tatsache sehen, dass wir beim ersten Ansatz den traditionellen Algorithmus verwenden, um die Q-Tabelle zu erstellen, mit deren Hilfe wir herausfinden kÃ¶nnen, welche MaÃŸnahmen fÃ¼r jeden Zustand zu ergreifen sind. <br><br>  Im zweiten Ansatz verwenden wir ein neuronales Netzwerk (um zustandsbasierte Belohnungen zu approximieren: q-Wert). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Udacity Inspired Q Design Chart</i> <i><br></i> <br><br>  Das ist alles.  Wie immer warten wir hier auf Ihre Kommentare oder Fragen, oder Sie kÃ¶nnen sie dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurslehrer Arthur Kadurin</a> in seiner <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offenen Lektion zum</a> Thema Networking stellen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de429090/">https://habr.com/ru/post/de429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de429078/index.html">So erstellen Sie prozedurale Kunst in weniger als 100 Codezeilen</a></li>
<li><a href="../de429082/index.html">Thailand ohne Stereotypen</a></li>
<li><a href="../de429084/index.html">Das zweite Leben des Elektroofens "Kharkov"</a></li>
<li><a href="../de429086/index.html">Backup Bierparty</a></li>
<li><a href="../de429088/index.html">AusfÃ¼hren von GraphQL-Abfragen mit OdataToEntity</a></li>
<li><a href="../de429092/index.html">Warum ist Stealth im Weltraum immer noch da?</a></li>
<li><a href="../de429094/index.html">Directional Sound: Technologie, die KopfhÃ¶rer ersetzen kann - wie es funktioniert</a></li>
<li><a href="../de429096/index.html">AntiquitÃ¤ten: ZX Spectrum, Kassettenprogramme und High Definition</a></li>
<li><a href="../de429102/index.html">Verkauf von Elektrofahrzeugen in Kanada</a></li>
<li><a href="../de429104/index.html">Hohe Daten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>