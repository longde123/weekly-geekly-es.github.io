<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ¿â€ğŸŒ¾ ğŸ“¦ ğŸ˜¡ Bildverarbeitung vs. menschliche Intuition: Algorithmen zur Unterbrechung des Betriebs von Objekterkennungsprogrammen ğŸ‘´ğŸ¾ ğŸ’‚ ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Logik der Maschinen ist einwandfrei, sie machen keine Fehler, wenn ihr Algorithmus korrekt funktioniert und die eingestellten Parameter den erford...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bildverarbeitung vs. menschliche Intuition: Algorithmen zur Unterbrechung des Betriebs von Objekterkennungsprogrammen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ua-hosting/blog/445372/"><img src="https://habrastorage.org/webt/s9/kf/68/s9kf68iyyoikag7yeexxfaxrp1e.jpeg"><br><br>  Die Logik der Maschinen ist einwandfrei, sie machen keine Fehler, wenn ihr Algorithmus korrekt funktioniert und die eingestellten Parameter den erforderlichen Standards entsprechen.  Bitten Sie das Auto, eine Route von Punkt A nach Punkt B zu wÃ¤hlen, und es wird die optimalste Route unter BerÃ¼cksichtigung der Entfernung, des Kraftstoffverbrauchs, des Vorhandenseins von Tankstellen usw. erstellen.  Dies ist eine reine Berechnung.  Das Auto wird nicht sagen: "Lass uns diese StraÃŸe entlang gehen, ich fÃ¼hle diese Route besser."  Vielleicht sind Autos in der Geschwindigkeit der Berechnungen besser als wir, aber die Intuition ist immer noch einer unserer TrÃ¼mpfe.  Die Menschheit hat Jahrzehnte damit verbracht, eine Maschine zu entwickeln, die dem menschlichen Gehirn Ã¤hnlich ist.  Aber haben sie so viel gemeinsam?  Heute werden wir eine Studie betrachten, in der Wissenschaftler, die an der unÃ¼bertroffenen "Vision" der Maschine auf der Grundlage von Faltungs-Neuronalen Netzen zweifelten, ein Experiment durchfÃ¼hrten, um ein Objekterkennungssystem mit einem Algorithmus zu tÃ¤uschen, dessen Aufgabe es war, "gefÃ¤lschte" Bilder zu erstellen.  Wie erfolgreich war die SabotageaktivitÃ¤t des Algorithmus, haben die Menschen die Erkennung besser bewÃ¤ltigt als Autos, und was wird diese Studie fÃ¼r die Zukunft dieser Technologie bringen?  Antworten finden wir im Bericht der Wissenschaftler.  Lass uns gehen. <a name="habracut"></a><br><br><h3>  Studienbasis </h3><br>  Objekterkennungstechnologien, die Faltungs-Neuronale Netze (SNS) verwenden, ermÃ¶glichen es der Maschine grob gesagt, einen Schwan von der Nummer 9 oder eine Katze von einem Fahrrad zu unterscheiden.  Diese Technologie entwickelt sich recht schnell und wird derzeit in verschiedenen Bereichen eingesetzt, von denen der offensichtlichste die Herstellung unbemannter Fahrzeuge ist.  Viele sind der Meinung, dass die SNA des Systems zur Erkennung von Objekten als Modell des menschlichen Sehens angesehen werden kann.  Diese Aussage ist jedoch aufgrund des menschlichen Faktors zu laut.  Die Sache ist, dass es sich als einfacher herausstellte, ein Auto zu tÃ¤uschen, als eine Person zu tÃ¤uschen (zumindest in Bezug auf die Objekterkennung).  SNA-Systeme sind sehr anfÃ¤llig fÃ¼r die Auswirkungen bÃ¶swilliger Algorithmen (wenn Sie dies wÃ¼nschen, feindlich), die sie in jeder Hinsicht daran hindern, ihre Aufgabe korrekt auszufÃ¼hren, und Bilder erstellen, die vom SNA-System falsch klassifiziert werden. <br><br>  Die Forscher teilen solche Bilder in zwei Kategorien ein: "Narren" (das Ziel vollstÃ¤ndig Ã¤ndern) und "peinlich" (das Ziel teilweise Ã¤ndern).  Die ersten sind bedeutungslose Bilder, die vom System als etwas Vertrautes erkannt werden.  Beispielsweise kann eine Reihe von Linien als "Baseball" und mehrfarbiges digitales Rauschen als "GÃ¼rteltier" klassifiziert werden.  Die zweite Kategorie von Bildern (â€peinlichâ€œ) sind Bilder, die unter normalen Bedingungen korrekt klassifiziert wÃ¼rden, aber der bÃ¶swillige Algorithmus verzerrt sie in den Augen des SNA-Systems leicht, Ã¼bertrieben Ã¼bertrieben.  Beispielsweise wird die handschriftliche Nummer 6 aufgrund eines kleinen Komplements von mehreren Pixeln als Nummer 5 klassifiziert. <br><br>  Stellen Sie sich vor, welchen Schaden solche Algorithmen anrichten kÃ¶nnen.  Es lohnt sich, die Klassifizierung der Verkehrszeichen gegen autonomen Verkehr auszutauschen, und UnfÃ¤lle sind unvermeidlich. <br><br>  Nachfolgend finden Sie die â€gefÃ¤lschtenâ€œ Bilder, die das SNA-System tÃ¤uschen, das darauf trainiert ist, Objekte zu erkennen, und wie ein Ã¤hnliches System sie klassifiziert. <br><br><img src="https://habrastorage.org/webt/bl/wf/ze/blwfzeyvqwjwhtqmk65ykrbulrc.jpeg"><br>  <i>Bild Nr. 1</i> <br><br>  SerienerklÃ¤rung: <br><br><ul><li>  <b>und</b> - indirekt verschlÃ¼sselte "betrÃ¼gerische" Bilder; </li><li>  <b>b</b> - direkt codierte "betrÃ¼gerische" Bilder; </li><li>  <b>c</b> - â€peinlicheâ€œ Bilder, die das System zwingen, eine Ziffer als eine andere zu klassifizieren; </li><li>  <b>d</b> - LaVAN-Angriffe (lokalisiertes und sichtbares gegnerisches / bÃ¶swilliges Rauschen) kÃ¶nnen zu einer falschen Klassifizierung fÃ¼hren, selbst wenn sich das â€Rauschenâ€œ nur an einem Punkt befindet (in der unteren rechten Ecke). </li><li>  <b>e</b> - dreidimensionale Objekte, die aus verschiedenen Winkeln falsch klassifiziert wurden. </li></ul><br>  Das MerkwÃ¼rdigste daran ist, dass eine Person mÃ¶glicherweise nicht dem Erliegen eines bÃ¶swilligen Algorithmus erliegt und Bilder basierend auf ihrer Intuition korrekt klassifiziert.  Bisher hat, wie Wissenschaftler sagen, niemand einen praktischen Vergleich der FÃ¤higkeiten einer Maschine und einer Person in einem Experiment durchgefÃ¼hrt, um bÃ¶swilligen Algorithmen gefÃ¤lschter Bilder entgegenzuwirken.  DafÃ¼r haben sich die Forscher entschieden. <br><br>  Zu diesem Zweck wurden mehrere Bilder erstellt, die mit bÃ¶swilligen Algorithmen erstellt wurden.  Den Probanden wurde gesagt, dass die Maschine diese (vorderen) Bilder als vertraute Objekte klassifizierte, d.h.  Die Maschine hat sie nicht richtig erkannt.  Die Aufgabe der Probanden bestand darin, genau zu bestimmen, wie die Maschine diese Bilder klassifizierte, d.h.  Was sie denken, dass die Maschine in den Bildern gesehen hat, ist diese Klassifizierung wahr usw. <br><br>  Insgesamt wurden 8 Experimente durchgefÃ¼hrt, bei denen 5 Arten von bÃ¶swilligen Bildern verwendet wurden, die ohne BerÃ¼cksichtigung des menschlichen Sehens erstellt wurden.  Mit anderen Worten, sie werden von Maschine zu Maschine erstellt.  Die Ergebnisse dieser Experimente erwiesen sich als sehr unterhaltsam, aber wir werden sie nicht verderben und alles in Ordnung bringen. <br><br><h3>  Versuchsergebnisse </h3><br><h2>  Experiment Nr. 1: Bilder mit ungÃ¼ltigen Tags tÃ¤uschen </h2><br>  Im ersten Experiment wurden 48 getÃ¤uschte Bilder verwendet, die vom Algorithmus erstellt wurden, um dem auf dem SNA namens AlexNet basierenden Erkennungssystem entgegenzuwirken.  Dieses System klassifizierte diese Bilder als "Zahnrad" und "Donut" ( <b>2a</b> ). <br><br><img src="https://habrastorage.org/webt/fg/yt/0h/fgyt0hewtb9akm-4nlbb-movqxo.jpeg"><br>  <i>Bild Nr. 2</i> <br><br>  WÃ¤hrend jedes Versuchs sah die Testperson, von der es 200 gab, ein betrÃ¼gerisches Bild und zwei Markierungen, d.h.  Klassifizierungsetiketten: System-SNS-Etikett und zufÃ¤llig aus den anderen 47 Bildern.  Die Probanden mussten das Etikett auswÃ¤hlen, das von der Maschine erstellt wurde. <br><br>  Infolgedessen entschieden sich die meisten Probanden fÃ¼r eine von der Maschine erstellte Beschriftung anstelle einer Beschriftung eines bÃ¶swilligen Algorithmus.  Klassifizierungsgenauigkeit, d.h.  Der Grad der Zustimmung des Probanden zur Maschine betrug 74%.  Statistisch gesehen wÃ¤hlten 98% der Probanden Maschinen-Tags auf einer hÃ¶heren Ebene als die statistische ZufÃ¤lligkeit ( <b>2d</b> , â€% der Probanden stimmen mit der Maschine Ã¼bereinâ€œ).  94% der Bilder zeigten eine sehr hohe Mensch-Maschine-Ausrichtung, dh von 48 wurden nur 3 Bilder von Personen anders als eine Maschine klassifiziert. <br><br>  So zeigten die Probanden, dass eine Person in der Lage ist, ein reales Bild und einen Narren zu teilen, dh gemÃ¤ÃŸ einem auf der SNA basierenden Programm zu handeln. <br><br><h2>  Experiment Nr. 2: erste Wahl gegen zweite </h2><br>  Die Forscher stellten die Frage - aufgrund dessen, welche Probanden Bilder so gut erkennen und von fehlerhaften Markierungen und betrÃ¼gerischen Bildern trennen konnten?  Vielleicht haben die Probanden den orange-gelben Ring als â€Donutâ€œ bezeichnet, weil der Donut in Wirklichkeit genau diese Form hat und ungefÃ¤hr dieselbe Farbe hat.  In Anerkennung kÃ¶nnten Assoziationen und intuitive Entscheidungen, die auf Erfahrung und Wissen basieren, einer Person helfen. <br><br>  Um dies zu Ã¼berprÃ¼fen, wurde das zufÃ¤llige Etikett durch das Etikett ersetzt, das von der Maschine als zweite mÃ¶gliche Klassifizierungsoption ausgewÃ¤hlt wurde.  Zum Beispiel klassifizierte AlexNet den orange-gelben Ring als â€Donutâ€œ, und die zweite Option fÃ¼r dieses Programm war â€Brezelâ€œ. <br><br>  Die Probanden standen vor der Aufgabe, die erste Markierung der Maschine oder diejenige zu wÃ¤hlen, die fÃ¼r alle 48 Bilder ( <b>2s</b> ) den zweiten Platz <b>einnahm</b> . <br><br>  Die Grafik in der Mitte von Bild <b>2d</b> zeigt die Ergebnisse dieses Tests: 91% der Probanden wÃ¤hlten die erste Version des Etiketts, und der Grad der Mensch-Maschine-Ãœbereinstimmung betrug 71%. <br><br><h2>  Experiment Nr. 3: Multithread-Klassifizierung </h2><br>  Die oben beschriebenen Experimente sind recht einfach, da die Probanden zwischen zwei mÃ¶glichen Antworten wÃ¤hlen kÃ¶nnen (Maschinen-Tag und Zufalls-Tag).  TatsÃ¤chlich durchlÃ¤uft die Maschine bei der Bilderkennung Hunderte und sogar Tausende von Optionen fÃ¼r Etiketten, bevor sie die am besten geeignete auswÃ¤hlt. <br><br>  Bei diesem Test befanden sich alle Markierungen fÃ¼r 48 Bilder unmittelbar vor den Probanden.  Sie mussten aus diesem Set das fÃ¼r jedes Bild am besten geeignete auswÃ¤hlen. <br><br>  Infolgedessen wÃ¤hlten 88% der Probanden genau die gleichen Etiketten wie die Maschine, und der Koordinationsgrad betrug 79%.  Eine interessante Tatsache ist, dass selbst bei der Auswahl des falschen Etiketts, das von der Maschine ausgewÃ¤hlt wurde, die Probanden in 63% dieser FÃ¤lle eines der Top-5-Etiketten auswÃ¤hlten.  Das heiÃŸt, alle Markierungen auf dem Auto sind in einer Liste von den am besten geeigneten bis zu den unangemessensten geordnet (Ã¼bertriebenes Beispiel: â€Bagelâ€œ, â€Brezelâ€œ, â€Gummiringâ€œ, â€Reifenâ€œ usw. bis hin zu â€Falke am Nachthimmelâ€œ). ) <br><br><h2>  Versuch Nr. 3b: "Was ist das?" </h2><br>  In diesem Test haben Wissenschaftler die Regeln leicht geÃ¤ndert.  Anstatt sie zu fragen, welches Etikett die Maschine fÃ¼r ein bestimmtes Bild wÃ¤hlen soll, wurden die Probanden einfach gefragt, was sie vor sich sehen. <br><br>  Erkennungssysteme, die auf Faltungs-Neuronalen Netzen basieren, wÃ¤hlen die geeignete Bezeichnung fÃ¼r ein bestimmtes Bild aus.  Dies ist ein ziemlich klarer und logischer Prozess.  In diesem Test zeigen die Probanden intuitives Denken. <br><br>  Infolgedessen wÃ¤hlten 90% der Probanden ein Etikett, das auch von der Maschine ausgewÃ¤hlt wurde.  Die Mensch-Maschine-Ausrichtung zwischen den Bildern betrug 81%. <br><br><h2>  Experiment 4: Statisches Fernsehrauschen </h2><br>  Wissenschaftler stellen fest, dass Bilder in frÃ¼heren Experimenten ungewÃ¶hnlich sind, aber unterscheidbare Merkmale aufweisen, die die Probanden dazu veranlassen kÃ¶nnen, die richtige (oder falsche) Wahl des Etiketts zu treffen.  Zum Beispiel ist das Bild â€Baseballâ€œ kein Ball, aber es gibt Linien und Farben, die auf einem echten Baseballball vorhanden sind.  Dies ist ein auffÃ¤lliges Unterscheidungsmerkmal.  Aber wenn das Bild keine solchen Merkmale aufweist, sondern im Wesentlichen statisches Rauschen ist, kann eine Person dann zumindest etwas darauf erkennen?  Das wurde beschlossen zu Ã¼berprÃ¼fen. <br><br><img src="https://habrastorage.org/webt/je/9o/sm/je9osmgxp99wmexrvqmzndviqpk.jpeg"><br>  <i>Bild Nr. 3a</i> <br><br>  In diesem Test befanden sich 8 Bilder mit Statik vor den Probanden, die das SNS-System als spezifisches Objekt erkennt (z. B. eine Vogel-Zaryanka).  AuÃŸerdem befanden sich vor den Probanden ein Etikett und damit verbundene normale Bilder (8 statische Bilder, 1 Etikett â€Zaryankaâ€œ und 5 Fotos dieses Vogels).  Die Testperson musste 1 von 8 statischen Bildern auswÃ¤hlen, die am besten zu dem einen oder anderen Etikett passen. <br><br>  Sie kÃ¶nnen sich selbst testen.  Oben sehen Sie ein Beispiel fÃ¼r einen solchen Test.  Welches der drei Bilder passt am besten zum Tag â€Zaryankaâ€œ und warum? <br><br>  81% der Probanden wÃ¤hlten das Etikett, das die Maschine wÃ¤hlte.  Gleichzeitig wurden 75% der Bilder von den Probanden mit der nach Ansicht der Maschine am besten geeigneten Beschriftung beschriftet (aus einer Reihe von Optionen, wie bereits erwÃ¤hnt). <br><br>  FÃ¼r diesen speziellen Test haben Sie mÃ¶glicherweise Fragen, genau wie meine.  Tatsache ist, dass ich in der vorgeschlagenen Statik (oben) persÃ¶nlich drei ausgeprÃ¤gte Merkmale sehe, die sie voneinander unterscheiden.  Und nur in einem Bild Ã¤hnelt dieses Merkmal stark der gleichen Zaryanka (ich denke, Sie verstehen, welches Bild der drei).  Daher ist meine persÃ¶nliche und sehr subjektive Meinung, dass ein solcher Test nicht besonders aussagekrÃ¤ftig ist.  Obwohl vielleicht unter anderem Optionen fÃ¼r statische Bilder wirklich nicht zu unterscheiden und nicht wiederzuerkennen waren. <br><br><h2>  Experiment Nr. 5: "zweifelhafte" Zahlen </h2><br>  Die oben beschriebenen Tests basierten auf Bildern, die nicht sofort vollstÃ¤ndig und ohne Zweifel als das eine oder andere Objekt klassifiziert werden kÃ¶nnen.  Es gibt immer einen Bruchteil der Zweifel.  Die tÃ¤uschenden Bilder sind in ihrer Arbeit ziemlich einfach - um das Bild bis zur Unkenntlichkeit zu verderben.  Es gibt jedoch eine zweite Art von bÃ¶swilligen Algorithmen, die nur ein kleines Detail im Bild hinzufÃ¼gen (oder entfernen), wodurch das Erkennungssystem des SNA-Systems vollstÃ¤ndig verletzt werden kann.  FÃ¼gen Sie ein paar Pixel hinzu, und die Zahl 6 wird auf magische Weise zur Zahl 5 ( <b>1s</b> ). <br><br>  Wissenschaftler halten solche Algorithmen fÃ¼r einen der gefÃ¤hrlichsten.  Sie kÃ¶nnen das Bild-Tag leicht Ã¤ndern, und das unbemannte Fahrzeug berÃ¼cksichtigt das Tempolimit-Zeichen (z. B. 75 statt 45) falsch, was zu traurigen Konsequenzen fÃ¼hren kann. <br><br><img src="https://habrastorage.org/webt/9k/wr/h9/9kwrh9bcsqbqyolfpdgc1crqcfg.jpeg"><br>  <i>Bild # 3b</i> <br><br>  In diesem Test schlugen die Wissenschaftler vor, dass die Probanden die falsche Antwort wÃ¤hlen, aber eher die falsche.  Im Test wurden 100 digitale Bilder verwendet, die durch einen bÃ¶swilligen Algorithmus geÃ¤ndert wurden (der LeNet SNA hat seine Klassifizierung geÃ¤ndert, dh der bÃ¶swillige Algorithmus hat erfolgreich funktioniert).  Die Probanden mussten sagen, welche Zahl ihrer Meinung nach die Maschine sah.  Wie erwartet haben 89% der Probanden diesen Test erfolgreich abgeschlossen. <br><br><h2>  Experiment 6: Fotos und lokalisierte "Verzerrung" </h2><br>  Wissenschaftler stellen fest, dass sich nicht nur Objekterkennungssysteme entwickeln, sondern auch bÃ¶swillige Algorithmen, die sie daran hindern.  Bisher war es fÃ¼r eine falsche Klassifizierung des Bildes erforderlich, 14% aller Pixel im Zielbild zu verzerren (Ã¤ndern, lÃ¶schen, beschÃ¤digen usw.).  Jetzt ist diese Zahl viel kleiner geworden.  Es reicht aus, ein kleines Bild in das Ziel einzufÃ¼gen, und die Klassifizierung wird verletzt. <br><br><img src="https://habrastorage.org/webt/6i/sz/1p/6isz1pvm_j-zzxt1mkeq2krmlts.jpeg"><br>  <i>Bild Nr. 4</i> <br><br>  In diesem Test wurde ein ziemlich neuer bÃ¶swilliger LaVAN-Algorithmus verwendet, der ein kleines Bild platziert, das an einer Stelle auf dem Zielfoto lokalisiert ist.  Infolgedessen kann das Objekterkennungssystem den U-Bahn-Zug als eine Dose Milch erkennen ( <b>4a</b> ).  Die wichtigsten Merkmale dieses Algorithmus sind genau der geringe Anteil beschÃ¤digter Pixel (nur 2%) des Zielbilds und das Fehlen der Notwendigkeit, es in seiner Gesamtheit oder im Hauptteil (hÃ¶chstwertigen Teil) zu verzerren. <br><br>  Im Test wurden 22 durch LaVAN beschÃ¤digte Bilder verwendet (das SNA-Erkennungssystem Inception V3 wurde von diesem Algorithmus erfolgreich gehackt).  Die Probanden sollten die bÃ¶swillige Beilage auf dem Foto klassifizieren.  87% der Probanden konnten dies erfolgreich tun. <br><br><h2>  Experiment 7: dreidimensionale Objekte </h2><br>  Die Bilder, die wir zuvor gesehen haben, sind zweidimensional, wie jedes Foto, Bild oder Zeitungsausschnitt.  Die meisten bÃ¶swilligen Algorithmen bearbeiten genau solche Bilder erfolgreich.  Diese SchÃ¤dlinge kÃ¶nnen jedoch nur unter bestimmten Bedingungen wirken, dh sie weisen eine Reihe von EinschrÃ¤nkungen auf: <br><br><ul><li>  KomplexitÃ¤t: nur zweidimensionale Bilder; </li><li>  Praktische Anwendung: BÃ¶swillige Ã„nderungen sind nur auf Systemen mÃ¶glich, die die empfangenen digitalen Bilder lesen, und nicht auf Bildern von Sensoren und Sensoren. </li><li>  StabilitÃ¤t: Ein bÃ¶swilliger Angriff verliert seine StÃ¤rke, wenn Sie ein zweidimensionales Bild drehen (GrÃ¶ÃŸe Ã¤ndern, zuschneiden, schÃ¤rfen usw.). </li><li>  Menschen: Wir sehen die Welt und die Objekte um uns herum in 3D unter verschiedenen Winkeln, bei Beleuchtung und nicht in Form von zweidimensionalen digitalen Bildern, die aus einem Winkel aufgenommen wurden. </li></ul><br><br>  Wie wir jedoch wissen, hat der Fortschritt bÃ¶swillige Algorithmen nicht verschont.  Unter ihnen erschien einer, der nicht nur zweidimensionale, sondern auch dreidimensionale Bilder verzerren kann, was zu einer falschen Klassifizierung durch das Objekterkennungssystem fÃ¼hrt.  Bei Verwendung von Software fÃ¼r dreidimensionale Grafiken fÃ¼hrt ein solcher Algorithmus Klassifizierer, die auf dem SNA (in diesem Fall dem Inception V3-Programm) basieren, aus unterschiedlichen Entfernungen und Betrachtungswinkeln in die Irre.  Das Ãœberraschendste ist, dass solche tÃ¤uschenden 3D-Bilder auf einem geeigneten Drucker gedruckt werden kÃ¶nnen, d.h.  Erstellen Sie ein reales physisches Objekt, und das Objekterkennungssystem klassifiziert es immer noch falsch (z. B. eine Orange als Bohrmaschine).  Und das alles dank geringfÃ¼giger Ã„nderungen in der Textur auf dem Zielbild ( <b>4b</b> ). <br><br>  FÃ¼r ein Objekterkennungssystem ist ein solcher bÃ¶swilliger Algorithmus ein schwerwiegender Gegner.  Aber der Mensch ist keine Maschine, er sieht und denkt anders.  In diesem Test gab es vor den Probanden Bilder von dreidimensionalen Objekten, in denen es die oben beschriebenen TexturÃ¤nderungen aus drei Winkeln gab.  Die Probanden erhielten auch die richtige und fehlerhafte Note.  Sie mussten feststellen, welche Etiketten korrekt sind, welche nicht und warum, d.h.  ob Testpersonen TexturÃ¤nderungen in Bildern sehen. <br><br>  Infolgedessen haben 83% der Probanden die Aufgabe erfolgreich abgeschlossen. <br><br>  FÃ¼r eine detailliertere Einarbeitung in die Nuancen der Studie empfehle ich dringend, dass Sie sich den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bericht von Wissenschaftlern</a> ansehen. <br><br>  Unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Link</a> finden Sie die Bild-, Daten- und Codedateien, die in der Studie verwendet wurden. <br><br><h3>  Nachwort </h3><br>  Die durchgefÃ¼hrten Arbeiten gaben Wissenschaftlern die MÃ¶glichkeit, eine einfache und ziemlich offensichtliche Schlussfolgerung zu ziehen - die menschliche Intuition kann eine Quelle sehr wichtiger Daten und ein Werkzeug sein, um die richtige Entscheidung und / oder Wahrnehmung von Informationen zu treffen.  Eine Person kann intuitiv verstehen, wie sich das Objekterkennungssystem verhÃ¤lt, welche Beschriftungen es wÃ¤hlt und warum. <br><br>  Die GrÃ¼nde, warum es fÃ¼r eine Person einfacher ist, ein reales Bild zu sehen und es richtig zu erkennen, sind mehrere.  Am offensichtlichsten ist die Methode, Informationen zu erhalten: Die Maschine empfÃ¤ngt ein Bild in digitaler Form, und eine Person sieht es mit eigenen Augen.  FÃ¼r eine Maschine ist ein Bild ein Datensatz, an dem Ã„nderungen vorgenommen werden, deren Klassifizierung Sie verzerren kÃ¶nnen.  FÃ¼r uns wird das Bild eines U-Bahn-Zuges immer ein U-Bahn-Zug sein, keine Dose Milch, weil wir es sehen. <br><br>  Wissenschaftler betonen auch, dass solche Tests schwer zu bewerten sind, weil eine Person keine Maschine ist und eine Maschine keine Person.  Zum Beispiel sprechen Forscher Ã¼ber den Test mit einem â€Donutâ€œ und einem â€Radâ€œ.  Diese Bilder Ã¤hneln dem â€Donutâ€œ und dem â€Radâ€œ, da das Erkennungssystem sie so klassifiziert.  Eine Person sieht, dass sie wie ein "Donut" und "Rad" aussieht, aber sie sind es nicht.  Dies ist der grundlegende Unterschied in der Wahrnehmung visueller Informationen zwischen einer Person und einem Programm. <br><br>  Vielen Dank fÃ¼r Ihre Aufmerksamkeit, bleiben Sie neugierig und haben Sie eine gute Arbeitswoche, Jungs. <br><br>  Vielen Dank fÃ¼r Ihren Aufenthalt bei uns.  GefÃ¤llt dir unser Artikel?  MÃ¶chten Sie weitere interessante Materialien sehen?  UnterstÃ¼tzen Sie uns, indem Sie eine Bestellung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufgeben</a> oder Ihren Freunden empfehlen, einen <b>Rabatt von 30% fÃ¼r Habr-Benutzer auf ein einzigartiges Analogon von Einstiegsservern, das wir fÃ¼r Sie erfunden haben:</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die ganze Wahrheit Ã¼ber VPS (KVM) E5-2650 v4 (6 Kerne) 10 GB DDR4 240 GB SSD 1 Gbit / s von $ 20 oder wie teilt man den Server?</a>  (Optionen sind mit RAID1 und RAID10, bis zu 24 Kernen und bis zu 40 GB DDR4 verfÃ¼gbar). <br><br>  <b>VPS (KVM) E5-2650 v4 (6 Kerne) 10 GB DDR4 240 GB SSD 1 Gbit / s bis zum Sommer kostenlos,</b> wenn Sie fÃ¼r einen Zeitraum von sechs Monaten bezahlen, kÃ¶nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> bestellen. <br><br>  <b>Dell R730xd 2 mal gÃ¼nstiger?</b>  Nur wir haben <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2 x Intel Dodeca-Core Xeon E5-2650v4 128 GB DDR4 6 x 480 GB SSD 1 Gbit / s 100 TV von 249 US-Dollar</a> in den Niederlanden und den USA!</b>  Lesen Sie mehr Ã¼ber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Aufbau eines InfrastrukturgebÃ¤udes.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klasse mit Dell R730xd E5-2650 v4 Servern fÃ¼r 9.000 Euro fÃ¼r einen Cent?</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de445372/">https://habr.com/ru/post/de445372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de445360/index.html">5 typische Aufgaben fÃ¼r JavaScript-Interviews: Analyse und LÃ¶sungen</a></li>
<li><a href="../de445362/index.html">Das Buch "Distributed Systems. Entwurfsmuster</a></li>
<li><a href="../de445366/index.html">So beschleunigen Sie die VerschlÃ¼sselung gemÃ¤ÃŸ GOST 28147-89 auf dem Baikal-T1-Prozessor aufgrund des SIMD-Blocks</a></li>
<li><a href="../de445368/index.html">Laden Sie ein Spiel mit ein paar Hunderttausenden von virtuellen Benutzern</a></li>
<li><a href="../de445370/index.html">TSDB-Analyse in Prometheus 2</a></li>
<li><a href="../de445378/index.html">Labyrinthe: Klassifizierung, Erzeugung, Suche nach LÃ¶sungen</a></li>
<li><a href="../de445380/index.html">Modernes PHP ist schÃ¶n und produktiv</a></li>
<li><a href="../de445384/index.html">Chang'e-4 Mission - wissenschaftliche AusrÃ¼stung auf dem Landemodul und dem Repeater-Satelliten</a></li>
<li><a href="../de445390/index.html">IDE einer normalen Person oder warum wir uns fÃ¼r Monaco entschieden haben</a></li>
<li><a href="../de445392/index.html">Dynamisches Remarketing von MyTarget: nicht persÃ¶nliche Produktempfehlungen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>