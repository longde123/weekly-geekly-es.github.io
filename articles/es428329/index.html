<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò§ ‚ù§Ô∏è ‚úä Entrenamiento de refuerzo: an√°lisis de videojuegos üöï ü§∑üèª üë©üèæ‚Äçü§ù‚Äçüë©üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En la Conferencia de IA, Vladimir Ivanov vivanov879 , Sr. , hablar√° sobre el uso del aprendizaje reforzado Ingeniero de aprendizaje profundo en Nvidia...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Entrenamiento de refuerzo: an√°lisis de videojuegos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/smileexpo/blog/428329/"><img align="left" src="https://habrastorage.org/webt/ls/va/9_/lsva9_rsrvkdmceogvse3sexdog.png"><br>  En la Conferencia de IA, <b>Vladimir Ivanov <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">vivanov879</a> , Sr.</b> , hablar√° sobre el uso del aprendizaje reforzado  <b>Ingeniero de aprendizaje profundo en Nvidia</b> .  El experto se dedica al aprendizaje autom√°tico en el departamento de pruebas: ‚ÄúAnalizo los datos que recopilamos durante las pruebas de videojuegos y hardware.  Para esto utilizo el aprendizaje autom√°tico y la visi√≥n por computadora.  La parte principal del trabajo es el an√°lisis de im√°genes, la limpieza de datos antes de la capacitaci√≥n, el marcado de datos y la visualizaci√≥n de las soluciones obtenidas ". <br><br>  En el art√≠culo de hoy, Vladimir explica por qu√© el aprendizaje reforzado se usa en autom√≥viles aut√≥nomos y habla sobre c√≥mo un agente est√° capacitado para actuar en un entorno cambiante, utilizando ejemplos de videojuegos. <br><br>  En los √∫ltimos a√±os, la humanidad ha acumulado una gran cantidad de datos.  Algunos conjuntos de datos se comparten y se presentan manualmente.  Por ejemplo, el conjunto de datos CIFAR, donde se firma cada imagen, a qu√© clase pertenece. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/ko/lp/16kolpeo1k3j1j9ppdh9vfjb6vc.jpeg"></div><br><br>  Hay conjuntos de datos en los que debe asignar una clase no solo a la imagen en su conjunto, sino a cada p√≠xel de la imagen.  Como, por ejemplo, en CityScapes. <br><br><img src="https://habrastorage.org/webt/eq/hz/b0/eqhzb0gt9v2demntxriobd5ewji.png"><br><br>  Lo que une estas tareas es que una red neuronal de aprendizaje solo necesita recordar los patrones en los datos.  Por lo tanto, con cantidades suficientemente grandes de datos, y en el caso de CIFAR son 80 millones de im√°genes, la red neuronal est√° aprendiendo a generalizar.  Como resultado, se las arregla bien con la clasificaci√≥n de im√°genes que nunca hab√≠a visto antes. <br><br>  Pero actuando dentro del marco de la t√©cnica de ense√±anza con el maestro, que funciona para marcar im√°genes, es imposible resolver problemas donde no queremos predecir la marca, sino tomar decisiones.  Como, por ejemplo, en el caso de la conducci√≥n aut√≥noma, donde la tarea es llegar de manera segura y confiable al punto final de la ruta. <a name="habracut"></a><br><br>  En los problemas de clasificaci√≥n, utilizamos la t√©cnica de ense√±anza con el maestro, cuando a cada imagen se le asigna una clase espec√≠fica.  Pero, ¬øqu√© pasa si no tenemos ese marcado, pero hay un agente y un entorno en el que puede realizar ciertas acciones?  Por ejemplo, que sea un videojuego, podemos hacer clic en las flechas de control. <br><br><img src="https://habrastorage.org/webt/bn/-c/ad/bn-cadnljvu7relew1mfes6p0xw.jpeg"><br><br>  Este tipo de problema debe resolverse con entrenamiento de refuerzo.  En el enunciado general del problema, queremos aprender a realizar la secuencia correcta de acciones.  Es fundamental que el agente tenga la capacidad de realizar acciones una y otra vez, explorando as√≠ el entorno en el que se encuentra.  Y en lugar de la respuesta correcta, qu√© hacer en una situaci√≥n particular, recibe una recompensa por una tarea completada correctamente.  Por ejemplo, en el caso de un taxi aut√≥nomo, el conductor recibir√° una bonificaci√≥n por cada viaje realizado. <br><br>  Volvamos a un ejemplo simple: un videojuego.  Tome algo simple, como el juego de tenis de mesa Atari. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hg/ev/0b/hgev0bl9kpvpzlllfzyyroentow.jpeg"></div><br><br>  Controlaremos la tableta a la izquierda.  Jugaremos contra el jugador de la computadora programado en las reglas de la derecha.  Dado que estamos trabajando con una imagen, y las redes neuronales son las m√°s exitosas en extraer informaci√≥n de las im√°genes, apliquemos una imagen a la entrada de una red neuronal de tres capas con un tama√±o de n√∫cleo de 3x3.  A la salida, tendr√° que elegir una de dos acciones: mover el tablero hacia arriba o hacia abajo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/o_/5z/mwo_5zeixn2llke6ethglty43ao.png"></div><br><br>  Entrenamos la red neuronal para realizar acciones que conduzcan a la victoria.  La t√©cnica de entrenamiento es la siguiente.  Dejamos que la red neuronal juegue algunas rondas de tenis de mesa.  Luego comenzamos a ordenar los juegos jugados.  En los juegos en los que gan√≥, marcamos las im√°genes con la etiqueta "Arriba" donde levant√≥ la raqueta y "Abajo" donde la baj√≥.  En los juegos perdidos, hacemos lo contrario.  Marcamos esas im√°genes donde baj√≥ el tablero con la etiqueta "Arriba", y donde lo levant√≥, "Abajo".  Por lo tanto, reducimos el problema al enfoque que ya conocemos: capacitaci√≥n con un maestro.  Tenemos un conjunto de im√°genes con etiquetas. <br><br><img src="https://habrastorage.org/webt/al/16/48/al1648vo1sfp72qj_8n2fpif3va.png"><br><br>  Usando esta t√©cnica de entrenamiento, en un par de horas, nuestro agente aprender√° a vencer a un jugador de computadora programado en las reglas. <br><br>  ¬øQu√© hacer con la conducci√≥n aut√≥noma?  El hecho es que el tenis de mesa es un juego muy simple.  Y puede producir miles de cuadros por segundo.  En nuestra red ahora solo hay 3 capas.  Por lo tanto, el proceso de aprendizaje es muy r√°pido.  El juego genera una gran cantidad de datos y los procesamos instant√°neamente.  En el caso de la conducci√≥n aut√≥noma, la recopilaci√≥n de datos es mucho m√°s larga y costosa.  Los autos son caros, y con un solo autom√≥vil recibiremos solo 60 cuadros por segundo.  Adem√°s, aumenta el precio del error.  En un videojuego, podr√≠amos permitirnos jugar juego tras juego al comienzo del entrenamiento.  Pero no podemos permitirnos estropear el auto. <br><br>  En este caso, ayudemos a la red neuronal al comienzo del entrenamiento.  Arreglamos la c√°mara en el autom√≥vil, colocamos un conductor experimentado y grabaremos fotos de la c√°mara.  Para cada imagen, suscribimos el √°ngulo de direcci√≥n del autom√≥vil.  Entrenaremos la red neuronal para copiar el comportamiento de un conductor experimentado.  Por lo tanto, nuevamente redujimos la tarea a la ense√±anza ya conocida con un maestro. <br><br><img src="https://habrastorage.org/webt/pl/d0/oc/pld0oc75oafeojutvborl_upb8a.png"><br><br>  Con un conjunto de datos lo suficientemente grande y diverso, que incluir√° diferentes paisajes, estaciones y condiciones clim√°ticas, la red neuronal aprender√° a controlar con precisi√≥n el autom√≥vil. <br><br>  Sin embargo, hubo un problema con los datos.  Son muy largos y caros de recolectar.  Usemos un simulador en el que se implementar√° toda la f√≠sica del movimiento del autom√≥vil, por ejemplo, DeepDrive.  Podemos aprenderlo sin temor a perder un autom√≥vil. <br><br><img src="https://habrastorage.org/webt/fe/c0/4v/fec04vmzbamtd60xxv3ypctf2ua.jpeg"><br><br>  En este simulador, tenemos acceso a todos los indicadores del autom√≥vil y del mundo.  Adem√°s, todas las personas, autom√≥viles, sus velocidades y distancias a ellos est√°n marcados alrededor. <br><br><img src="https://habrastorage.org/webt/hq/37/1k/hq371k3xvab9kfcworznwlkelai.jpeg"><br><br>  Desde el punto de vista del ingeniero, en dicho simulador, puede probar con seguridad nuevas t√©cnicas de entrenamiento.  ¬øQu√© debe hacer un investigador?  Por ejemplo, estudiar diferentes opciones para el descenso de gradiente en problemas de aprendizaje con refuerzo.  Para probar una hip√≥tesis simple, no quiero disparar gorriones desde un ca√±√≥n y ejecutar un agente en un mundo virtual complejo, y luego esperar d√≠as a la vez para obtener los resultados de la simulaci√≥n.  En este caso, usemos nuestra potencia inform√°tica de manera m√°s eficiente.  Deje que los agentes sean m√°s simples.  Tomemos, por ejemplo, un modelo de ara√±a de cuatro patas.  En el simulador de Mujoco, se ve as√≠: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yp/ad/dg/ypaddgwogfekvvjemaf3vseohdi.png"></div><br><br>  Le asignamos la tarea de correr a la mayor velocidad posible en una direcci√≥n determinada, por ejemplo, a la derecha.  El n√∫mero de par√°metros observados para una ara√±a es un vector de 39 dimensiones, que registra la posici√≥n y la velocidad de todas sus extremidades.  A diferencia de la red neuronal para el tenis de mesa, donde solo hab√≠a una neurona en la salida, hay ocho en la salida (ya que la ara√±a en este modelo tiene 8 articulaciones). <br><br>  En tales modelos simples, se pueden probar varias hip√≥tesis sobre la t√©cnica de ense√±anza.  Por ejemplo, comparemos la velocidad de aprendizaje para correr, seg√∫n el tipo de red neuronal.  Sea una red neuronal de una capa, una red neuronal de tres capas, una red convolucional y una red recurrente: <br><br><img src="https://habrastorage.org/webt/sq/zl/xu/sqzlxuj-k_nts13x7x5veh-56ay.png"><br><br>  La conclusi√≥n se puede extraer de la siguiente manera: dado que el modelo de ara√±a y la tarea son bastante simples, los resultados de la capacitaci√≥n son aproximadamente los mismos para diferentes modelos.  Una red de tres capas es demasiado compleja y, por lo tanto, aprende peor. <br><br>  A pesar del hecho de que el simulador funciona con un modelo de ara√±a simple, dependiendo de la tarea planteada a la ara√±a, el entrenamiento puede durar d√≠as.  En este caso, animemos varios cientos de ara√±as en una superficie al mismo tiempo en lugar de una y aprendamos de los datos que recibiremos de todos.  As√≠ que aceleraremos el entrenamiento varios cientos de veces.  Aqu√≠ hay un ejemplo del motor Flex. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/p4/6l/4j/p46l4jyulhhspwa9e5z_4oe40oq.png"></div><br><br>  Lo √∫nico que ha cambiado en t√©rminos de optimizaci√≥n de redes neuronales es la recopilaci√≥n de datos.  Cuando solo ejecutamos una ara√±a, recibimos datos secuencialmente.  Una carrera tras otra. <br><br><img src="https://habrastorage.org/webt/8t/cd/hr/8tcdhrcxqbx4mk3w99lacw2tfbu.png"><br><br>  Ahora puede suceder que algunas ara√±as reci√©n est√©n comenzando la carrera, mientras que otras han estado corriendo durante mucho tiempo. <br><br><img src="https://habrastorage.org/webt/6u/i6/jv/6ui6jv9y3zmbdy3cwfweuv1drt4.png"><br><br>  Tendremos esto en cuenta durante la optimizaci√≥n de la red neuronal.  De lo contrario, todo sigue igual.  Como resultado, obtenemos aceleraci√≥n en el entrenamiento cientos de veces, de acuerdo con la cantidad de ara√±as que est√°n simult√°neamente en la pantalla. <br><br>  Como tenemos un simulador efectivo, tratemos de resolver problemas m√°s complejos.  Por ejemplo, correr sobre terreno accidentado. <br><br><img src="https://habrastorage.org/webt/zo/sq/dn/zosqdnbygiz_2p-cvxl4hvtuvqw.png"><br><br>  Dado que el entorno en este caso se ha vuelto m√°s agresivo, cambiemos y compliquemos las tareas durante el entrenamiento.  Es dif√≠cil de aprender, pero f√°cil en la batalla.  Por ejemplo, cada pocos minutos para cambiar el terreno.  Adem√°s, dirijamos los agentes externos al agente.  Por ejemplo, vamos a lanzarle bolas y encender y apagar el viento.  Luego, el agente aprende a correr incluso en superficies que nunca ha conocido.  Por ejemplo, suba escaleras. <br><br><img src="https://habrastorage.org/webt/zc/-7/bq/zc-7bqzt5kcltfgrwfc9piw3mli.png"><br><br>  Como hemos aprendido tan efectivamente a correr en simulaciones, verifiquemos las t√©cnicas de entrenamiento de refuerzo en disciplinas competitivas.  Por ejemplo, en juegos de disparos.  La plataforma VizDoom ofrece un mundo en el que puedes disparar, recoger armas y reponer salud.  En este juego tambi√©n usaremos una red neuronal.  Solo que ahora tendr√° cinco salidas: cuatro para movimiento y una para disparar. <br><br>  Para que el entrenamiento sea efectivo, vamos a tomarlo gradualmente.  De simple a complejo.  En la entrada, la red neuronal recibe una imagen, y antes de comenzar a hacer algo consciente, debe aprender a comprender en qu√© consiste el mundo.  Al estudiar en escenarios simples, aprender√° a comprender qu√© objetos habitan el mundo y c√≥mo interactuar con ellos.  Comencemos con el gui√≥n: <br><br><img src="https://habrastorage.org/webt/99/cy/xm/99cyxm9xxkmd3wg9zryiguyqnoe.png"><br><br>  Una vez que domine este escenario, el agente comprender√° que hay enemigos, y que deber√≠an ser disparados, porque obtienes puntos por ellos.  Luego lo entrenaremos en un escenario donde la salud est√° disminuyendo constantemente y usted necesita reponerla. <br><br><img src="https://habrastorage.org/webt/06/gs/jr/06gsjr-tuvruizcpmy3da0gsyq4.jpeg"><br><br>  Aqu√≠ aprender√° que tiene salud y necesita reponerse, porque en caso de muerte, el agente recibe una recompensa negativa.  Adem√°s, aprender√° que si te mueves hacia el tema, puedes recogerlo.  En el primer escenario, el agente no pod√≠a moverse. <br><br>  Y en el tercer escenario final, dejemos que dispare con los robots programados en las reglas del juego para que pueda perfeccionar sus habilidades. <br><br><img src="https://habrastorage.org/webt/dn/pz/iq/dnpziqqditttkptyp7zflcdh4ag.png"><br><br>  Durante el entrenamiento en este escenario, la selecci√≥n correcta de las recompensas que recibe el agente es muy importante.  Por ejemplo, si otorga una recompensa solo por los rivales derrotados, la se√±al ser√° muy rara: si hay pocos jugadores alrededor, entonces recibiremos puntos cada pocos minutos.  Por lo tanto, usemos la combinaci√≥n de recompensas que exist√≠an antes.  El agente recibir√° una recompensa por cada acci√≥n √∫til, ya sea mejorar la salud, seleccionar cartuchos o golpear a un oponente. <br><br><blockquote>  Como resultado, un agente entrenado con recompensas bien elegidas es m√°s fuerte que sus oponentes m√°s exigentes computacionalmente.  En 2016, dicho sistema gan√≥ la competencia VizDoom con un margen de m√°s de la mitad de los puntos anotados desde el segundo lugar.  El segundo equipo tambi√©n utiliz√≥ una red neuronal, solo con una gran cantidad de capas e informaci√≥n adicional del motor del juego durante el entrenamiento.  Por ejemplo, informaci√≥n sobre si hay enemigos en el campo de visi√≥n del agente. </blockquote><br>  Hemos examinado enfoques para resolver problemas, donde es importante tomar decisiones.  Pero muchas tareas con este enfoque permanecer√°n sin resolver.  Por ejemplo, el juego de b√∫squeda Montezuma Revenge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s9/vp/_9/s9vp_9botnvsnfqcaxutbv-2pn0.jpeg"></div><br><br>  Aqu√≠ debe buscar las llaves para abrir las puertas de las habitaciones vecinas.  Raramente recibimos llaves, y abrimos habitaciones incluso con menos frecuencia.  Tambi√©n es importante no distraerse con objetos extra√±os.  Si entrena el sistema como lo hicimos en las tareas anteriores y da recompensas por enemigos vencidos, simplemente noquear√° el cr√°neo rodante una y otra vez y no examinar√° el mapa.  Si est√° interesado, puedo hablar sobre c√≥mo resolver estos problemas en un art√≠culo separado. <br><br>  <b>Puedes escuchar el discurso de Vladimir Ivanov en la Conferencia de AI el 22 de noviembre</b> .  Un programa detallado y entradas est√°n disponibles en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sitio web oficial del</a> evento. <br><br>  Lea la entrevista con Vladimir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428329/">https://habr.com/ru/post/es428329/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428313/index.html">Telegram en MacOS [presumiblemente] tambi√©n almacena localmente la correspondencia en una forma accesible</a></li>
<li><a href="../es428315/index.html">5 temores de desarrolladores que hemos superado</a></li>
<li><a href="../es428317/index.html">Reaccionar ganchos: ¬øganar o perder?</a></li>
<li><a href="../es428321/index.html">An√°lisis predictivo de datos: modelado y validaci√≥n</a></li>
<li><a href="../es428327/index.html">A qu√© mirar: Reglamento Europeo de Identificaci√≥n Electr√≥nica eIDAS</a></li>
<li><a href="../es428333/index.html">Resultados RAIF Hackathon AI 2018 Hackathon</a></li>
<li><a href="../es428335/index.html">Actualizaci√≥n de acceso directo de Siri</a></li>
<li><a href="../es428337/index.html">JavaScript entretenido: sin llaves</a></li>
<li><a href="../es428339/index.html">No lo automatice: malos consejos comerciales</a></li>
<li><a href="../es428341/index.html">Tecnolog√≠a Qsan RAID EE</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>