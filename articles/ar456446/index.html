<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>๐ฉโ๐ญ ๐ง๐ฝโ๐คโ๐ง๐ฝ โ Ceph - ูู "ุนูู ุงูุฑูุจุฉ" ุฅูู "ุฅูุชุงุฌ" โฆ๏ธ ๐ธ๐ฟ ๐๐ฝ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ุงุฎุชูุงุฑ CEPH. ุงูุฌุฒุก 1 


 ูุงู ูุฏููุง ุฎูุณุฉ ุฑููู ุ ุนุดุฑุฉ ููุงุชูุญ ุจุตุฑูุฉ ุ BGP ูููุฃุฉ ุ ุจุถุน ุนุดุฑุงุช ูู ูุญุฑูุงุช ุฃูุฑุงุต ุงูุญุงูุฉ ุงูุตูุจุฉ ููุฌููุนุฉ ูู ุฃูุฑุงุต SAS ูู ุฌููุน ุงู...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - ูู "ุนูู ุงูุฑูุจุฉ" ุฅูู "ุฅูุชุงุฌ"</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456446/" style=";text-align:right;direction:rtl"><h1 id="vybor-ceph-chast-1" style=";text-align:right;direction:rtl">  ุงุฎุชูุงุฑ CEPH.  ุงูุฌุฒุก 1 </h1><br><p style=";text-align:right;direction:rtl">  <em>ูุงู ูุฏููุง ุฎูุณุฉ ุฑููู ุ ุนุดุฑุฉ ููุงุชูุญ ุจุตุฑูุฉ ุ BGP ูููุฃุฉ ุ ุจุถุน ุนุดุฑุงุช ูู ูุญุฑูุงุช ุฃูุฑุงุต ุงูุญุงูุฉ ุงูุตูุจุฉ ููุฌููุนุฉ ูู ุฃูุฑุงุต SAS ูู ุฌููุน ุงูุฃููุงู ูุงูุฃุญุฌุงู ุ ููุฐูู proxmox ูุงูุฑุบุจุฉ ูู ูุถุน ูู ูุง ูู ุซุงุจุช ูู ุชุฎุฒูู S3 ุงูุฎุงุต ุจูุง.</em>  <em>ููุณ ูุฐุง ูู ูุง ูู ุถุฑูุฑู ูููุญุงูุงุฉ ุงูุงูุชุฑุงุถูุฉ ุ ูููู ุจูุฌุฑุฏ ุงูุจุฏุก ูู ุงุณุชุฎุฏุงู source ููุชูุญุฉ ุ ุซู ุงูุชูู ุฅูู ููุงูุชู.</em>  <em>ุงูุดูุก ุงููุญูุฏ ุงูุฐู ุฃุฒุนุฌูู ูู BGP.</em>  <em>ูุง ููุฌุฏ ุฃุญุฏ ูู ุงูุนุงูู ุนุงุฌุฒ ูุบูุฑ ูุณุคูู ูุบูุฑ ุฃุฎูุงูู ุนู ุชูุฌูู BGP ุงูุฏุงุฎูู.</em>  <em>ูููุช ุฃุนุฑู ุฃูู ูู ููุช ูุฑูุจ ุณูุบุฑู ููู.</em> </p><br><p style=";text-align:right;direction:rtl"><img src="https://habrastorage.org/getpro/habr/post_images/09e/a36/178/09ea3617814a9598a6aa9784abc14a76.jpg"></p><br><p style=";text-align:right;direction:rtl">  ูุงูุช ุงููููุฉ ุดุงุฆุนุฉ - ูุงู ููุงู CEPH ุ ูู ุชูุฌุญ ุจุดูู ุฌูุฏ.  ูุงู ูู ุงูุถุฑูุฑู ุฃู ุชูุนู "ุฌูุฏุง". <br>  ุงููุฌููุนุฉ ุงูุชู ุญุตูุช ุนูููุง ูุงูุช ุบูุฑ ูุชุฌุงูุณุฉ ุ ูููุถููููุฉ ุ ููุง ูุชู ุถุจุทูุง ุนูููุงู.  ูุงู ูุชุฃูู ูู ูุฌููุนุชูู ูู ุงูุนูุฏ ุงููุฎุชููุฉ ุ ูุน ุดุจูุฉ ูุดุชุฑูุฉ ูุงุญุฏุฉ ุชูุนุจ ุฏูุฑ ูู ูู ุงููุชูุฉ ูุงูุดุจูุฉ ุงูุนุงูุฉ.  ูุงูุช ุงูุนูุฏ ูุญููุฉ ุจุฃุฑุจุนุฉ ุฃููุงุน ูู ุงูุฃูุฑุงุต - ููุนุงู ูู ูุญุฑูุงุช ุฃูุฑุงุต ุงูุญุงูุฉ ุงูุซุงุจุชุฉ ุ ุชู ุชุฌููุนููุง ูู ูุงุนุฏุชูู ูููุตูุชูู ููููุถุน ูููุนูู ูู ูุญุฑูุงุช ุงูุฃูุฑุงุต ุงูุตูุจุฉ ุฐุงุช ุงูุฃุญุฌุงู ุงููุฎุชููุฉ ุ ุชู ุชุฌููุนููุง ูู ูุฌููุนุฉ ุซุงูุซุฉ.  ุชู ุญู ูุดููุฉ ุงูุฃุญุฌุงู ุงููุฎุชููุฉ ูู ุฎูุงู ุฃูุฒุงู OSD ูุฎุชููุฉ. </p><br><p style=";text-align:right;direction:rtl">  ุชู ุชูุณูู ุงูุฅุนุฏุงุฏ ููุณู ุฅูู ูุณููู - <strong>ุถุจุท ูุธุงู ุงูุชุดุบูู</strong> <strong>ูุถุจุท CEPH ููุณู</strong> ูุฅุนุฏุงุฏุงุชู. </p><a name="habracut"></a><br><h2 id="prokachka-os" style=";text-align:right;direction:rtl">  ุงูุชุณููุฉ OS </h2><br><h3 id="network" style=";text-align:right;direction:rtl">  ุดุจูุฉ </h3><br><p style=";text-align:right;direction:rtl">  ุงููููู ุงูุนุงูู ูุชุฃุซุฑ ุนูุฏ ุงูุชุณุฌูู ูุนูุฏ ุงูููุงุฒูุฉ.  ุนูุฏ ุงูุชุณุฌูู - ูุฃู ุงูุนููู ูุง ูุชููู ุงุณุชุฌุงุจุฉ ุญูู ุงูุชุณุฌูู ุงููุงุฌุญ ุญุชู ุชุคูุฏ ุงููุณุฎ ุงููุชูุงุซูุฉ ููุจูุงูุงุช ูู ูุฌููุนุงุช ุงูููุงุถุน ุงูุฃุฎุฑู ุงููุฌุงุญ.  ูุธุฑูุง ูุฃู ููุงุนุฏ ุชูุฒูุน ุงููุณุฎ ุงููุชูุงุซูุฉ ูู ุฎุฑูุทุฉ CRUSH ูุงูุช ูุฏููุง ูุณุฎุฉ ูุชูุงุซูุฉ ูุงุญุฏุฉ ููู ูุถูู ุ ููุฏ ูุงูุช ุงูุดุจูุฉ ุชุณุชุฎุฏู ุฏุงุฆููุง. </p><br><p style=";text-align:right;direction:rtl">  ูุฐูู ุ ุฃูู ุดูุก ูุฑุฑุช ุชูููู ุงูุดุจูุฉ ุงูุญุงููุฉ ููููุงู ุ ุจูููุง ุฃุญุงูู ุฅููุงุนู ุจุงูุงูุชูุงู ุฅูู ุดุจูุงุช ูููุตูุฉ. </p><br><p style=";text-align:right;direction:rtl">  ููุจุฏุก ุ ุงูููุชููุฉ ุฅุนุฏุงุฏุงุช ุจุทุงูุงุช ุงูุดุจูุฉ.  ุจุฏุฃุช ุจุฅุนุฏุงุฏ ููุงุฆู ุงูุงูุชุธุงุฑ: </p><br><p style=";text-align:right;direction:rtl">  ูุงุฐุง ูุงู: </p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">ethtool -l ens1f1</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"><code class="plaintext hljs">root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1 root@ceph01:~# ethtool -g ens1f1 Ring parameters for ens1f1: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1</code> </pre> </div></div><br><p style=";text-align:right;direction:rtl">  ูููู ููุงุญุธุฉ ุฃู ุงููุนููุงุช ุงูุญุงููุฉ ุจุนูุฏุฉ ุนู ุงูุญุฏ ุงูุฃูุตู.  ุฒูุงุฏุฉ: </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01:~#ethtool -G ens1f0 rx 4096 root@ceph01:~#ethtool -G ens1f0 tx 4096 root@ceph01:~#ethtool -L ens1f0 combined 63</code> </pre> <br><p style=";text-align:right;direction:rtl">  ุชุณุชุฑุดุฏ ููุงู ููุชุงุฒ </p><br><p style=";text-align:right;direction:rtl">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a> </p><br><p style=";text-align:right;direction:rtl">  ุฒูุงุฏุฉ ุทูู <strong>ูุงุฆูุฉ ุงูุชุธุงุฑ</strong> ุฅุฑุณุงู <strong>txqueuelen</strong> ูู 1000 ุฅูู 10ุ000 </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01:~#ip link set ens1f0 txqueuelen 10000</code> </pre> <br><p style=";text-align:right;direction:rtl">  ุญุณูุง ุ ุจุนุฏ ุชูุซูู ceph ููุณูุง </p><br><p style=";text-align:right;direction:rtl">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://ceph.com/geen-categorie/ceph-loves-jumbo-frames/</a> </p><br><p style=";text-align:right;direction:rtl">  ุฒูุงุฏุฉ <strong>MTU</strong> ุฅูู 9000. </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01:~#ip link set dev ens1f0 mtu 9000</code> </pre> <br><p style=";text-align:right;direction:rtl">  ุชูุช ุงูุฅุถุงูุฉ ุฅูู / etc / network / ูุงุฌูุงุช ุ ุจุญูุซ ูุชู ุชุญููู ูู ูุง ุณุจู ุนูุฏ ุจุฏุก ุงูุชุดุบูู </p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">ุงููุท / ุงูุฎ / ุดุจูุฉ / ูุงุฌูุงุช</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01:~# cat /etc/network/interfaces auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet manual post-up /sbin/ethtool -G ens1f0 rx 4096 post-up /sbin/ethtool -G ens1f0 tx 4096 post-up /sbin/ethtool -L ens1f0 combined 63 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 auto ens1f1 iface ens1f1 inet manual post-up /sbin/ethtool -G ens1f1 rx 4096 post-up /sbin/ethtool -G ens1f1 tx 4096 post-up /sbin/ethtool -L ens1f1 combined 63 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000</code> </pre> </div></div><br><p style=";text-align:right;direction:rtl">  ุจุนุฏ ุฐูู ุ ุจุนุฏ ููุณ ุงูููุงูุฉ ุ ุจุฏุฃ ูู ุงูุงูุชูุงุก ุจุนูุงูุฉ ูู ููุงุจุถ ุงูููุงุฉ 4.15.  ุจุงููุธุฑ ุฅูู ุฃูู ุนูู 128G RAM RAM ุ ุญุตููุง ุนูู ููู ุชูููู ูุนูู ูู <strong>sysctl</strong> </p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">cat /etc/sysctl.d/50-ceph.conf</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">net.core.rmem_max = 56623104 #        54M net.core.wmem_max = 56623104 #        54M net.core.rmem_default = 56623104 #        . 54M net.core.wmem_default = 56623104 #         54M #    net.ipv4.tcp_rmem = 4096 87380 56623104 # (,  , )    tcp_rmem #  3  ,      TCP. # :   TCP       #   .     #       (moderate memory pressure). #       8  (8192). #  :  ,    #   TCP  .     #  /proc/sys/net/core/rmem_default,   . #       ( ) #  87830 .     65535  #     tcp_adv_win_scale  tcp_app_win = 0, #  ,       tcp_app_win. # :   ,     #     TCP.     , #    /proc/sys/net/core/rmem_max.  ยซยป #     SO_RCVBUF     . net.ipv4.tcp_wmem = 4096 65536 56623104 net.core.somaxconn = 5000 #    ,  . net.ipv4.tcp_timestamps=1 #     (timestamps),    RFC 1323. net.ipv4.tcp_sack=1 #     TCP net.core.netdev_max_backlog=5000 ( 1000) #       ,  #    ,     . net.ipv4.tcp_max_tw_buckets=262144 #   ,    TIME-WAIT . #     โ ยซยป     #    . net.ipv4.tcp_tw_reuse=1 #   TIME-WAIT   , #     . net.core.optmem_max=4194304 #   - ALLOCATABLE #    (4096 ) net.ipv4.tcp_low_latency=1 #  TCP/IP      #     . net.ipv4.tcp_adv_win_scale=1 #          , #    TCP-    . #   tcp_adv_win_scale ,     #   : # Bytes- bytes\2  -tcp_adv_win_scale #  bytes โ     .   tcp_adv_win_scale # ,       : # Bytes- bytes\2  tcp_adv_win_scale #    .  - โ 2, # ..     ยผ  ,   # tcp_rmem. net.ipv4.tcp_slow_start_after_idle=0 #    ,     # ,       . #   SSR  ,    #  . net.ipv4.tcp_no_metrics_save=1 #    TCP      . net.ipv4.tcp_syncookies=0 #   syncookie net.ipv4.tcp_ecn=0 #Explicit Congestion Notification (   )  # TCP-.      ยซยป #       .     # -        #    . net.ipv4.conf.all.send_redirects=0 #   ICMP Redirect โฆ  .    #   ,        . #    . net.ipv4.ip_forward=0 #  .   ,     , #    . net.ipv4.icmp_echo_ignore_broadcasts=1 #   ICMP ECHO ,    net.ipv4.tcp_fin_timeout=10 #      FIN-WAIT-2   #   .  60 net.core.netdev_budget=600 # ( 300) #        , #          #  .    NIC ,    . # ,     SoftIRQs # ( )  CPU.    netdev_budget. #    300.    SoftIRQ  # 300   NIC     CPU net.ipv4.tcp_fastopen=3 # TFO TCP Fast Open #        TFO,      #    TCP .     ,  #  )</code> </pre> </div></div><br><p style=";text-align:right;direction:rtl">  ูุน <strong>ุดุจูุฉ ุงูููุนุงู ุ</strong> ุชู ุชุฎุตูุตูุง ุนูู ูุงุฌูุงุช ุดุจูุฉ ูููุตูุฉ ุจุณุฑุนุฉ 10 ุฌูุฌุงุจุช ูู ุงูุซุงููุฉ ูุดุจูุฉ ูุณุทุญุฉ ูููุตูุฉ.  ุชู ุชุณููู ุจุทุงูุงุช ุงูุดุจูุฉ ุซูุงุฆูุฉ ุงูููุงูุฐ <strong>Mellanox</strong> 10/25 ุฌูุฌุงุจุช ูู ุงูุซุงููุฉ ูุงูุชู <strong>ูุชู ุดุญููุง</strong> ูู ูุญูููู ูููุตููู ุจุณุฑุนุฉ 10 ุฌูุฌุงุจุช ูู ุงูุซุงููุฉ ุนูู ูู ุฌูุงุฒ.  ุชู ุชูููุฐ ุงูุชุฌููุน ุจุงุณุชุฎุฏุงู OSPF ุ ุญูุซ ุฃุธูุฑ ุงูุชุฑุงุจุท ูุน lacp ูุณุจุจ ูุง ุนุฑุถ ุงููุทุงู ุงูุชุฑุฏุฏู ุงูููู ุจุญุฏ ุฃูุตู 16 ุฌูุฌุงุจุช ูู ุงูุซุงููุฉ ุ ุจูููุง ุงุณุชุฎุฏู ospf ุจูุฌุงุญ ูููููุง ุงูุนุดุฑุงุช ุนูู ูู ุขูุฉ.  ููุงูุช ุงูุฎุทุท ุงููุณุชูุจููุฉ ูุงุณุชุฎุฏุงู ROCE ุนูู ูุฐู ุงููููุงูููุณุงุช ููุญุฏ ูู ุงููููู.  ููููุฉ ุชูููู ูุฐุง ุงูุฌุฒุก ูู ุงูุดุจูุฉ: </p><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ูุธุฑูุง ูุฃู ุงูุฃุฌูุฒุฉ ููุณูุง ูุฏููุง ุนูุงููู IP ุฎุงุฑุฌูุฉ ุนูู BGP ุ ูุญุชุงุฌ ุฅูู ุจุฑูุงูุฌ - <em>(ุฃู ุจุงูุฃุญุฑู ุ ูู ููุช ูุชุงุจุฉ ูุฐุง ุงูุชูุฑูุฑ ุ ูุงู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">frr = 6.0-1</a> ) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ููุฌูุฏูุง</a></em> ุจุงููุนู. </li><li style=";text-align:right;direction:rtl">  ูู ุงููุฌููุน ุ ูุงู ูุฏู ุงูุฃุฌูุฒุฉ ูุงุฌูุชุงู ููุดุจูุฉ ูุน ูุงุฌูุชูู - ูุง ูุฌููุนู 4 ููุงูุฐ.  ูุธุฑุช ุฅุญุฏู ุจุทุงูุงุช ุงูุดุจูุฉ ุฅูู ุงููุตูุน ูู ุฎูุงู ูููุฐูู ูุชู ุชูููู BGP ุนููู ุ ูุงูุซุงููุฉ - ูู ุงุซููู ูู ุงูููุงูุฐ ูุธุฑุช ุฅูู ููุชุงุญูู ูุฎุชูููู ูุชู ุถุจุท OSPF ุนููู </li></ol><br><p style=";text-align:right;direction:rtl">  ุชูุงุตูู ุฅุนุฏุงุฏ OSPF: ุงููููุฉ ุงูุฑุฆูุณูุฉ ูู ุชุฌููุน ุงุฑุชุจุงุทูู ูุงูุญุตูู ุนูู ุงูุชุณุงูุญ ูุน ุงูุฎุทุฃ. <br>  ุชู ุชูููู ูุงุฌูุงุช ุดุจูุฉ ูู ุดุจูุชูู ุซุงุจุชุชูู ุจุณูุทุชูู - 10.10.10.0/24 ู 10.10.20.0/24 </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">1: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.10.2/24 brd 10.10.10.255 scope global ens1f0 2: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.20.2/24 brd 10.10.20.255 scope global ens1f1</code> </pre> <br><p style=";text-align:right;direction:rtl">  ุงูุฐู ุงูุณูุงุฑุงุช ูุฑู ุจุนุถูุง ุงูุจุนุถ. </p><br><h3 id="disk" style=";text-align:right;direction:rtl">  DISK </h3><br><p style=";text-align:right;direction:rtl">  ูุงูุช ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุชุญุณูู ุฃุฏุงุก ุงูุฃูุฑุงุต.  ู SSD ููุฏ ุบูุฑุช ุฌุฏููุฉ ู <strong>noop</strong> ุ ู HDD - <strong>ุงูููุนุฏ ุงูููุงุฆู</strong> .  ุฅุฐุง ูุงู ุงูุฃูุฑ ุชูุฑูุจูุง - ูุฅู NOOP ูุนูู ููููุง ููุจุฏุฃ "ูู ูุงู ุฃููุงู - ูุงููุนุงู" ุ ูุงูุฐู ูุจุฏู ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ูุซู "FIFO (First In ุ First Out)".  ุงูุทูุจุงุช ูู ูุงุฆูุฉ ุงูุงูุชุธุงุฑ ุนูุฏูุง ุชุตุจุญ ูุชุงุญุฉ.  DEADLINE ุฃูุซุฑ ูุฑุงุกุฉ ููุท ุ ุจุงูุฅุถุงูุฉ ุฅูู ุฃู ุงูุนูููุฉ ูู ูุงุฆูุฉ ุงูุงูุชุธุงุฑ ุชุญุตู ุนูู ูุตูู ุญุตุฑู ุชูุฑูุจุง ุฅูู ุงููุฑุต ูู ููุช ุงูุชุดุบูู.  ูุฐุง ุฃูุฑ ุฑุงุฆุน ููุธุงููุง - ุจุนุฏ ูู ุดูุก ุ ููุงู ุนูููุฉ ูุงุญุฏุฉ ููุท ุชุนูู ูุน ูู ูุฑุต - OSD daemon. <br>  (ูููู ูุฃููุฆู ุงูุฐูู ูุฑุบุจูู ูู ุงูุงูุบูุงุณ ูู ุฌุฏููุฉ I / O ุฃู ููุฑุฃูุง ุนููุง ููุง: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> </p><br><p style=";text-align:right;direction:rtl">  ุชูุถู ุงููุฑุงุกุฉ ุจุงููุบุฉ ุงูุฑูุณูุฉ: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://www.opennet.ru/base/sys/linux_shedulers.txt.html</a> ) </p><br><p style=";text-align:right;direction:rtl">  ูู ุชูุตูุงุช ูุถุจุท ููุตุญ ููููุณ ูุฒูุงุฏุฉ nr_request ุฃูุถุง </p><br><blockquote style=";text-align:right;direction:rtl">  <em>nr_requests</em> <em><br></em>  <em>ุชุญุฏุฏ ูููุฉ nr_requests ููุฏุงุฑ ุทูุจุงุช ุงูุฅุฏุฎุงู / ุงูุฅุฎุฑุงุฌ ุงูุชู ูุชู ุชุฎุฒูููุง ูุคูุชูุง ูุจู ููุงู ุฌุฏููุฉ I / O ุจุฅุฑุณุงู / ุงุณุชูุจุงู ุงูุจูุงูุงุช ุฅูู ุฌูุงุฒ ุงูุญุธุฑ ุ ุฅุฐุง ููุช ุชุณุชุฎุฏู ุจุทุงูุฉ RAID / ุฌูุงุฒ ูุชูุฉ ููููู ุงูุชุนุงูู ูุน ูุงุฆูุฉ ุงูุชุธุงุฑ ุฃูุจุฑ ููุง ุญุฏุฏุชู I ุชู ุชุนููู ุฌุฏููุฉ ุงูุฅุฏุฎุงู / ุงูุฅุฎุฑุงุฌ ุฅูู ุ ูุฏ ูุณุงุนุฏ ุฑูุน ูููุฉ nr_requests ูู ุชุญุณูู ุจุงููุงูู ูุชูููู ุชุญููู ุงูุฎุงุฏู ุนูุฏ ุญุฏูุซ ูููุงุช ูุจูุฑุฉ ูู I / O ุนูู ุงูุฎุงุฏู.</em>  <em>ุฅุฐุง ููุช ุชุณุชุฎุฏู Deadline ุฃู CFQ ูุฌุฏูู ุฒููู ุ ููู ุงูููุชุฑุญ ุฃู ุชุถุจุท ูููุฉ nr_request ุนูู ุถุนูู ูููุฉ ุนูู ูุงุฆูุฉ ุงูุงูุชุธุงุฑ.</em> </blockquote><p style=";text-align:right;direction:rtl">  ููู!  ุงูููุงุทููู ุฃููุณูู ูู ูุทูุฑู CEPH ุงูุฐูู ูููุนูููุง ุจุฃู ูุธุงู ุฃููููุงุชูู ูุนูู ุจุดูู ุฃูุถู </p><br><p style=";text-align:right;direction:rtl"><img src="https://habrastorage.org/getpro/habr/post_images/b7e/fd3/bd0/b7efd3bd03fedc88307e200905c8c6a9.gif"></p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">WBThrottle ู / ุฃู nr_requests</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><blockquote style=";text-align:right;direction:rtl">  WBThrottle ู / ุฃู nr_requests <br>  ูุณุชุฎุฏู ุชุฎุฒูู ุงููููุงุช ูุฎุฒูุฉ ูุคูุชูุง I / O ูููุชุงุจุฉุ  ูุจุฐูู ูุฑุชูุน ุนุฏุฏ ูู ุงูููุงุฆุฏ ุฅุฐุง ูุงู ุณุฌู ุชุฎุฒูู ุงููููุงุช ุนูู ูุณุงุฆุท ุฃุณุฑุน.  ูุชู ุฅุนูุงู ุทูุจุงุช ุงูุนููู ุจูุฌุฑุฏ ูุชุงุจุฉ ุงูุจูุงูุงุช ูู ุงูุณุฌู ุ ุซู ูุชู ูุณุญูุง ุนูู ูุฑุต ุงูุจูุงูุงุช ููุณู ูู ููุช ูุงุญู ุจุงุณุชุฎุฏุงู ูุธุงุฆู Linux ุงูููุงุณูุฉ.  ูุฐุง ูุฌุนู ูู ุงููููู ููุฃูุฑุงุต ุงููุบุฒู OSD ุชูููุฑ ุฒูู ูุชุงุจุฉ ููุงุซู ู SSDs ุนูุฏ ุงููุชุงุจุฉ ูู ุญุฒู ุตุบูุฑุฉ.  ุชุชูุญ ูุฐู ุงููุชุงุจุฉ ุงููุชุฃุฎุฑุฉ ุฃูุถูุง ููููุงุฉ ููุณูุง ุฅุนุงุฏุฉ ุฅูุดุงุก ุทูุจุงุช ุงูุฅุฏุฎุงู / ุงูุฅุฎุฑุงุฌ ุฅูู ุงููุฑุต ุนูู ุฃูู ุฅูุง ุฏูุฌูุง ูุนูุง ุฃู ุงูุณูุงุญ ูุฑุคูุณ ุงูุฃูุฑุงุต ุงูููุฌูุฏุฉ ุจุงุฎุชูุงุฑ ูุณุงุฑ ุฃูุซุฑ ูุซุงููุฉ ุฃุนูู ุงูููุญุงุช ุงูุฎุงุตุฉ ุจูู.  ุงูุชุฃุซูุฑ ุงูููุงุฆู ูู ุฃูู ููููู ุถุบุท ุฅุฏุฎุงู / ุฅุฎุฑุงุฌ ุฃูุซุฑ ููููุงู ูู ูู ูุฑุต ููุง ุณูููู ูููููุง ุจุงุณุชุฎุฏุงู ุฅุฏุฎุงู / ุฅุฎุฑุงุฌ ูุจุงุดุฑ ุฃู ูุชุฒุงูู. </blockquote><p style=";text-align:right;direction:rtl">  ููุน ุฐูู ุ ุชูุดุฃ ูุดููุฉ ูุนููุฉ ุฅุฐุง ูุงู ุญุฌู ุงูุณุฌูุงุช ุงููุงุฑุฏุฉ ูู ูุชูุฉ Ceph ูุนููุฉ ูุจู ุฌููุน ุฅููุงููุงุช ุงูุฃูุฑุงุต ุงูุฃุณุงุณูุฉ.  ูู ูุซู ูุฐุง ุงูุณููุงุฑูู ุ ูููู ุฒูุงุฏุฉ ุงูุนุฏุฏ ุงูุฅุฌูุงูู ูููุงุฆู I / O ุงููุนููุฉ ุงููุนููุฉ ูููุชุงุจุฉ ุนูู ุงููุฑุต ุจุดูู ูุง ูููู ุงูุณูุทุฑุฉ ุนููู ูููุชุฌ ุนู ุฐูู ูุงุฆูุฉ ุงูุชุธุงุฑ ูุนูููุงุช I / O ุงูุชู ุชููุฃ ูุงูู ููุงุฆู ุงูุชุธุงุฑ ุงููุฑุต ู Ceph.  ุชุนูู ุทูุจุงุช ุงููุฑุงุกุฉ ุจุดูู ุณูุก ุจุดูู ุฎุงุต ุ ูุฃููุง ุชุชุนุทู ุจูู ุทูุจุงุช ุงููุชุงุจุฉ ุ ูุงูุชู ูุฏ ุชุณุชุบุฑู ุนุฏุฉ ุซูุงูู ููุชููู ุฅูู ุงููุฑุต ุงูุฑุฆูุณู. </p><br><p style=";text-align:right;direction:rtl">  ูุฏุญุฑ ูุฐู ุงููุดููุฉ ุ ูุฏู Ceph ุขููุฉ ุงุฎุชูุงู ูููุชุงุจุฉ ุชุฏุนู WBThrottle ูุฏูุฌุฉ ูู ุชุฎุฒูู ุงููููุงุช.  ุชู ุชุตูููู ููุญุฏ ูู ุฅุฌูุงูู ุนูููุงุช ุงูุฅุฏุฎุงู / ุงูุฅุฎุฑุงุฌ ูููุชุงุจุฉ ุงููุนููุฉ ุงูุชู ูููู ูุถุนูุง ูู ูุงุฆูุฉ ุงูุงูุชุธุงุฑ ูุจุฏุก ุนูููุฉ ุฅุนุงุฏุฉ ุงูุชุนููู ุงูุฎุงุตุฉ ุจูุง ูู ููุช ุณุงุจู ููุง ุณูุญุฏุซ ุจุดูู ุทุจูุนู ุจุณุจุจ ุงูุชุถููู ุจูุงุณุทุฉ kernel ููุณู.  ูุณูุก ุงูุญุธ ุ ููุถุญ ุงูุงุฎุชุจุงุฑ ุฃู ุงูููู ุงูุงูุชุฑุงุถูุฉ ูุฏ ูุง ุชุคุฏู ุฅูู ุชูููู ุงูุณููู ุงูุญุงูู ุฅูู ูุณุชูู ูููู ุฃู ูููู ูุฐุง ุงูุชุฃุซูุฑ ุนูู ุฒูู ุงูุชูุงู ุนูููุงุช ุงููุฑุงุกุฉ.  ูููู ุฃู ูุคุฏู ุงูุถุจุท ุฅูู ุชุบููุฑ ูุฐุง ุงูุณููู ูุชูููู ุงูุทูู ุงูููู ูููุงุฆู ุงูุงูุชุธุงุฑ ูุฌุนู ูุฐุง ุงูุชุฃุซูุฑ ุบูุฑ ููู.  ููุน ุฐูู ุ ููุงู ููุงุถูุฉ: ูู ุฎูุงู ุชูููู ุฅุฌูุงูู ุนุฏุฏ ุงูุฅุฏุฎุงูุงุช ุงููุณููุญ ุจูุง ูู ูุงุฆูุฉ ุงูุงูุชุธุงุฑ ุ ููููู ุชูููู ูุฏุฑุฉ ุงูููุงุฉ ููุณูุง ุนูู ุฒูุงุฏุฉ ููุงุกุชูุง ูู ุชุฑุชูุจ ุงูุทูุจุงุช ุงููุงุฑุฏุฉ.  ูุฌุฏุฑ ุงูุชูููุฑ ููููุงู ูู ุฃูู ุจุญุงุฌุฉ ุฅูู ุงููุฒูุฏ ูู ุฃุฌู ุชุทุจููู ุงููุญุฏุฏ ุ ูุนุจุก ุงูุนูู ุ ูุงูุชููู ูููุงุณุจูู. </p><br><p style=";text-align:right;direction:rtl">  ููุชุญูู ูู ุนูู ูุงุฆูุฉ ุงูุชุธุงุฑ ุงููุชุงุจุฉ ุงููุนููุฉ ูุฐู ุ ููููู ุฅูุง ุชุฎููุถ ุงูุญุฏ ุงูุฃูุตู ุงูุฅุฌูุงูู ูุนุฏุฏ ุนูููุงุช ุงูุฅุฏุฎุงู / ุงูุฅุฎุฑุงุฌ ุงููุนููุฉ ุจุงุณุชุฎุฏุงู ุฅุนุฏุงุฏุงุช WBThrottle ุ ุฃู ุชูููู ุงูุญุฏ ุงูุฃูุตู ููููุฉ ุงูุนูููุงุช ุงููุงุดูุฉ ุนูุฏ ูุณุชูู ุงููุชูุฉ ุฐุงุชู ูู kernel.  ูููู ููู ูู ุงูุขุฎุฑ ูุงูุขุฎุฑ ุงูุชุญูู ุจุดูู ูุนุงู ูู ููุณ ุงูุณููู ูุชูุถููุงุชู ูู ุงูุชู ุณุชููู ุงูุฃุณุงุณ ูุชูููุฐ ูุฐุง ุงูุฅุนุฏุงุฏ. <br>  ุชุฌุฏุฑ ุงูุฅุดุงุฑุฉ ุฃูุถูุง ุฅูู ุฃู ุฃููููุฉ ุชุดุบูู ูุธุงู Ceph ุชููู ุฃูุซุฑ ูุนุงููุฉ ููุงุณุชุนูุงูุงุช ุงูุฃูุตุฑ ุนูู ูุณุชูู ุงููุฑุต.  ุนูุฏ ุชูููุต ูุงุฆูุฉ ุงูุงูุชุธุงุฑ ุงูุฅุฌูุงููุฉ ุฅูู ูุฑุต ูุนูู ุ ููุชูู ูููุน ูุงุฆูุฉ ุงูุงูุชุธุงุฑ ุงูุฑุฆูุณู ุฅูู Ceph ุ ุญูุซ ูุชูุชุน ุจูุฒูุฏ ูู ุงูุชุญูู ูู ุงูุฃููููุฉ ุงูุชู ุชุชูุชุน ุจูุง ุนูููุฉ ุงูุฅุฏุฎุงู / ุงูุฅุฎุฑุงุฌ.  ุงููุธุฑ ูู ุงููุซุงู ุงูุชุงูู: </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">echo 8 &gt; /sys/block/sda/queue/nr_requests</code> </pre> <br><p style=";text-align:right;direction:rtl">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> </p></div></div><br><h3 id="common" style=";text-align:right;direction:rtl">  COMMON </h3><br><p style=";text-align:right;direction:rtl">  ูุนุฏุฏ ูููู ูู ุฅุนุฏุงุฏุงุช ุงูููุงุฉ ูุฌุนู <del>  ุณูุงุฑุชู ูููุฉ ูุญุฑูุฑู </del>  ุถุบุท ุจุนุถ ุงูุฃุฏุงุก ุฃูุซุฑ ูู ุงูุญุฏูุฏ </p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">cat /etc/sysctl.d/60-ceph2.conf</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"> kernel.pid_max = 4194303 #     25,       kernel.threads-max=2097152 # , , . vm.max_map_count=524288 #      . #        #         # malloc,    mmap, mprotect  madvise,     #  . fs.aio-max-nr=50000000 #   input-output #  Linux     - (AIO), #       - # ,    -  . #     , #      -. #  aio-max-nr     #  . vm.min_free_kbytes=1048576 #       . #  1Gb,       , #    OOM Killer   OSD.     #    ,      vm.swappiness=10 #       10% . #   128G ,  10%  12 .     . #    60%   ,   , #       vm.vfs_cache_pressure=1000 #    100.     #     . vm.zone_reclaim_mode=0 #         #  ,     . #     ,     . #       # ,    , zone_reclaim_mode #  ,   , # ,   ,   . vm.dirty_ratio=20 #   ,     ""  #    : #   128  . #   20  SSD,     CEPH  #     3G . #   40  HDD,      1G # 20%  128  25.6 . ,     , #    2.4G .         #    -    DevOps   . vm.dirty_background_ratio=3 #   ,    dirty pages  , #    pdflush/flush/kdmflush     fs.file-max=524288 #      ,,   ,    .</code> </pre> </div></div><br><h2 id="pogruzhenie-v--ceph" style=";text-align:right;direction:rtl">  ุงูุบูุต ูู CEPH </h2><br><p style=";text-align:right;direction:rtl">  ุงูุฅุนุฏุงุฏุงุช ุงูุชู ุฃูุฏ ุฃู ุฃุชูุงูููุง ุจูุฒูุฏ ูู ุงูุชูุงุตูู: </p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">ุงููุท / ุงูุฎ / ุงูุณูู / ุงูุณูู</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">osd: journal_aio: true #  ,  journal_block_align: true #  i/o journal_dio: true #   journal_max_write_bytes: 1073714824 #     #      journal_max_write_entries: 10000 #      journal_queue_max_bytes: 10485760000 journal_queue_max_ops: 50000 rocksdb_separate_wal_dir: true #    wal #       # NVMe bluestore_block_db_create: true #       bluestore_block_db_size: '5368709120 #5G' bluestore_block_wal_create: true bluestore_block_wal_size: '1073741824 #1G' bluestore_cache_size_hdd: '3221225472 # 3G' #     #     bluestore_cache_size_ssd: '9663676416 # 9G' keyring: /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap: '1073741824 #1G' osd_disk_thread_ioprio_class: idle osd_disk_thread_ioprio_priority: 7 osd_disk_threads: 2 #        osd_failsafe_full_ratio: 0.95 osd_heartbeat_grace: 5 osd_heartbeat_interval: 3 osd_map_dedup: true osd_max_backfills: 2 #       . osd_max_write_size: 256 osd_mon_heartbeat_interval: 5 osd_op_threads: 16 osd_op_num_threads_per_shard: 1 osd_op_num_threads_per_shard_hdd: 2 osd_op_num_threads_per_shard_ssd: 2 osd_pool_default_min_size: 1 #  .    osd_pool_default_size: 2 #  ,    #     #   osd_recovery_delay_start: 10.000000 osd_recovery_max_active: 2 osd_recovery_max_chunk: 1048576 osd_recovery_max_single_start: 3 osd_recovery_op_priority: 1 osd_recovery_priority: 1 #       osd_recovery_sleep: 2 osd_scrub_chunk_max: 4</code> </pre> </div></div><br><p style=";text-align:right;direction:rtl">  ุจุนุถ ุงููุนููุงุช ุงูุชู ุชู ุงุฎุชุจุงุฑูุง ุนูู ุถูุงู ุงูุฌูุฏุฉ ูู ุงูุฅุตุฏุงุฑ 12.2.12 ููููุฏุฉ ูู ceph ุงูุฅุตุฏุงุฑ 12.2.2 ุ ุนูู ุณุจูู ุงููุซุงู ุ <strong>osd_recovery_threads.</strong>  ูุฐูู ุ ุชุถููุช ุงูุฎุทุท ุชุญุฏูุซูุง ุนูู prod ุฅูู 12.2.12.  ุฃุธูุฑุช ุงูููุงุฑุณุฉ ุงูุชูุงูู ูู ูุฌููุนุฉ ูุงุญุฏุฉ ูู ุงูุฅุตุฏุงุฑุงุช 12.2.2 ู 12.2.12 ุ ูุงูุชู ุชุณูุญ ุจุงูุชุญุฏูุซ ุงููุณุชูุฑ. </p><br><h3 id="testovyy-klaster" style=";text-align:right;direction:rtl">  ูุชูุฉ ุงูุงุฎุชุจุงุฑ </h3><br><p style=";text-align:right;direction:rtl">  ูุจุทุจูุนุฉ ุงูุญุงู ุ ูุงู ูู ุงูุถุฑูุฑู ุงุฎุชุจุงุฑ ุงูุฅุตุฏุงุฑ ููุณู ููุง ูู ุงูุญุงู ูู ุงููุนุฑูุฉ ุ ูููู ูู ุงูููุช ุงูุฐู ุจุฏุฃุช ููู ุงูุนูู ูุน ุงููุฌููุนุฉ ูู ุงููุณุชูุฏุน ุ ูู ููู ููุงู ุณูู ุฅุตุฏุงุฑ ุฃุญุฏุซ.  ูุธุฑูุง ูุฃูู ูู ุงูุฅุตุฏุงุฑ ุงูุซุงููู ููุณุช ูุจูุฑุฉ ุฌุฏูุง ( <strong>1393</strong> ุณุทุฑูุง ูู ุงูุชููููุงุช ููุงุจู <strong>1436</strong> ุณุทุฑูุง ูู ุงูุฅุตุฏุงุฑ ุงูุฌุฏูุฏ) ุ ููุฏ ูุฑุฑูุง ุจุฏุก ุงุฎุชุจุงุฑ ูุงุญุฏ ุฌุฏูุฏ (ูุง ูุฒุงู ูุญุฏูุซูุง ุ ููุงุฐุง ุชุณุชูุฑ ูู ุงูููููุงุช ุงููุฏููุฉ) </p><br><p style=";text-align:right;direction:rtl">  ุงูุดูุก ุงููุญูุฏ ุงูุฐู ุญุงูููุง ุชุฑู ุงูุฅุตุฏุงุฑ ุงููุฏูู ูู ุญุฒูุฉ <strong>ูุดุฑ ceph ุ</strong> ุญูุซ ุชู <strong>ุชุญุณูู</strong> ุฌุฒุก ูู ุงูุฃุฏูุงุช ุงููุณุงุนุฏุฉ (ูุฌุฒุก ูู ุงูููุธููู) ูู ุฎูุงู ุจูุงุก ุงูุฌููุฉ.  ูุงู ุงูุฅุตุฏุงุฑ ุงูุฌุฏูุฏ ูุฎุชูููุง ุชูุงููุง ุ ูููู ูู ูุคุซุฑ ุนูู ุชุดุบูู ุงููุฌููุนุฉ ููุณูุง ุ <strong>ูุชุฑูุช</strong> ุงูุฅุตุฏุงุฑุงุช <strong>1.5.39</strong> </p><br><p style=";text-align:right;direction:rtl">  ูุธุฑูุง ูุฃู ุงูุฃูุฑ ceph-disk ููุถุญ ุจูุถูุญ ุฃูู ุชู ุฅููุงูู ูุงุณุชุฎุฏุงูู ุ ุฃููุง ุงูุนุฒูุฒ ุ ุงูุฃูุฑ ceph-volume ุ ุจุฏุฃูุง ูู ุฅูุดุงุก OSD ุจุงุณุชุฎุฏุงู ูุฐุง ุงูุฃูุฑ ุ ุฏูู ุฅุถุงุนุฉ ุงูููุช ุนูู ุงููุชูุงุฏู. </p><br><p style=";text-align:right;direction:rtl">  ูุงูุช ุงูุฎุทุฉ ูู ุฅูุดุงุก ูุณุฎุฉ ูุชุทุงุจูุฉ ูู ูุฑุตูู SSD ุ ูุถุน ุนููููุง ุณุฌูุงุช OSD ุ ูุงูุชู ุจุฏูุฑูุง ุชูุฌุฏ ุนูู SASs ุงููุบุฒู.  ุญุชู ูุชููู ูู ุญูุงูุฉ ุฃููุณูุง ูู ูุดุงูู ุงูุจูุงูุงุช ุนูุฏ ุณููุท ูุฑุต ุจุณุฌู. </p><br><p style=";text-align:right;direction:rtl">  ุฅูุดุงุก ูุฌููุนุฉ ูู ูุซุงุฆู ุงูุตูุจ </p><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">ุงููุท / ุงูุฎ / ุงูุณูู / ุงูุณูู</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-qa:~# cat /etc/ceph/ceph.conf #     [client] rbd_cache = true rbd_cache_max_dirty = 50331648 rbd_cache_max_dirty_age = 2 rbd_cache_size = 67108864 rbd_cache_target_dirty = 33554432 rbd_cache_writethrough_until_flush = true rbd_concurrent_management_ops = 10 rbd_default_format = 2 [global] auth_client_required = cephx auth_cluster_required = cephx auth_service_required = cephx cluster network = 10.10.10.0/24 debug_asok = 0/0 debug_auth = 0/0 debug_buffer = 0/0 debug_client = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_filer = 0/0 debug_filestore = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_journal = 0/0 debug_journaler = 0/0 debug_lockdep = 0/0 debug_mon = 0/0 debug_monc = 0/0 debug_ms = 0/0 debug_objclass = 0/0 debug_objectcatcher = 0/0 debug_objecter = 0/0 debug_optracker = 0/0 debug_osd = 0/0 debug_paxos = 0/0 debug_perfcounter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_rgw = 0/0 debug_throttle = 0/0 debug_timer = 0/0 debug_tp = 0/0 fsid = d0000000d-4000-4b00-b00b-0123qwe123qwf9 mon_host = ceph01-q, ceph02-q, ceph03-q mon_initial_members = ceph01-q, ceph02-q, ceph03-q public network = 8.8.8.8/28 #  ,  )) rgw_dns_name = s3-qa.mycompany.ru #     rgw_host = s3-qa.mycompany.ru #    [mon] mon allow pool delete = true mon_max_pg_per_osd = 300 #     #     #  , ,    , #     OSD.     PG #     -    mon_osd_backfillfull_ratio = 0.9 mon_osd_down_out_interval = 5 mon_osd_full_ratio = 0.95 #   SSD     #   -      #   5%   (   1.2Tb) #   ,     # bluestore_block_db_size     #   mon_osd_nearfull_ratio = 0.9 mon_pg_warn_max_per_osd = 520 [osd] bluestore_block_db_create = true bluestore_block_db_size = 5368709120 #5G bluestore_block_wal_create = true bluestore_block_wal_size = 1073741824 #1G bluestore_cache_size_hdd = 3221225472 # 3G bluestore_cache_size_ssd = 9663676416 # 9G journal_aio = true journal_block_align = true journal_dio = true journal_max_write_bytes = 1073714824 journal_max_write_entries = 10000 journal_queue_max_bytes = 10485760000 journal_queue_max_ops = 50000 keyring = /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap = 1073741824 #1G osd_disk_thread_ioprio_class = idle osd_disk_thread_ioprio_priority = 7 osd_disk_threads = 2 osd_failsafe_full_ratio = 0.95 osd_heartbeat_grace = 5 osd_heartbeat_interval = 3 osd_map_dedup = true osd_max_backfills = 4 osd_max_write_size = 256 osd_mon_heartbeat_interval = 5 osd_op_num_threads_per_shard = 1 osd_op_num_threads_per_shard_hdd = 2 osd_op_num_threads_per_shard_ssd = 2 osd_op_threads = 16 osd_pool_default_min_size = 1 osd_pool_default_size = 2 osd_recovery_delay_start = 10.0 osd_recovery_max_active = 1 osd_recovery_max_chunk = 1048576 osd_recovery_max_single_start = 3 osd_recovery_op_priority = 1 osd_recovery_priority = 1 osd_recovery_sleep = 2 osd_scrub_chunk_max = 4 osd_scrub_chunk_min = 2 osd_scrub_sleep = 0.1 rocksdb_separate_wal_dir = true</code> </pre> </div></div><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">#   root@ceph01-qa:~#ceph-deploy mon create ceph01-q #        root@ceph01-qa:~#ceph-deploy gatherkeys ceph01-q #   .       - ,       # mon_initial_members = ceph01-q, ceph02-q, ceph03-q #         root@ceph01-qa:~#ceph-deploy mon create-initial #        root@ceph01-qa:~#cat ceph.bootstrap-osd.keyring &gt; /var/lib/ceph/bootstrap-osd/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-mgr.keyring &gt; /var/lib/ceph/bootstrap-mgr/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-rgw.keyring &gt; /var/lib/ceph/bootstrap-rgw/ceph.keyring #      root@ceph01-qa:~#ceph-deploy admin ceph01-q #  ,   root@ceph01-qa:~#ceph-deploy mgr create ceph01-q</code> </pre> <br><p style=";text-align:right;direction:rtl"> ,        ceph-deploy    12.2.12 โ      OSD  db    - </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 blkid could not detect a PARTUUID for device: /dev/md1</code> </pre> <br><p style=";text-align:right;direction:rtl"> , blkid   PARTUUID,    : </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-qa:~#parted /dev/md0 mklabel GPT #   , #  GPT     #        = bluestore_block_db_size: '5368709120 #5G' #    20  OSD,     #    root@ceph01-qa:~#for i in {1..20}; do echo -e "n\n\n\n+5G\nw" | fdisk /dev/md0; done</code> </pre> <br><p style=";text-align:right;direction:rtl">   ,     OSD     (, ,    ) </p><br><p style=";text-align:right;direction:rtl">   OSD  bluestore     WAL,    db </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 stderr: 2019-04-12 10:39:27.211242 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid stderr: 2019-04-12 10:39:27.213185 7eff461b6e00 -1 bdev(0x55824c273680 /var/lib/ceph/osd/ceph-0//block.wal) open open got: (22) Invalid argument stderr: 2019-04-12 10:39:27.213201 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _open_db add block device(/var/lib/ceph/osd/ceph-0//block.wal) returned: (22) Invalid argument stderr: 2019-04-12 10:39:27.999039 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) mkfs failed, (22) Invalid argument stderr: 2019-04-12 10:39:27.999057 7eff461b6e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (22) Invalid argument stderr: 2019-04-12 10:39:27.999141 7eff461b6e00 -1 ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0/: (22) Invalid argumen</code> </pre> <br><p style=";text-align:right;direction:rtl">     -  (   ,  )      WAL      OSD โ     (    WAL,  , ,   ). </p><br><p style=";text-align:right;direction:rtl"> ,         WAL  NVMe,     . </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sdf --block.wal /dev/md0p2 --block.db /dev/md1p2</code> </pre> <br><p style=";text-align:right;direction:rtl">  ,   OSD.      ,        โ    SSD  ,     SAS. </p><br><p style=";text-align:right;direction:rtl">       20 ,     ,  โ . <br> , ,   : </p><br><div class="spoiler" style=";text-align:right;direction:rtl"> <b class="spoiler_title">ceph osd tree</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><p style=";text-align:right;direction:rtl"> root@eph01-q:~# ceph osd tree <br> ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF <br> -1 14.54799 root default <br> -3 9.09200 host ceph01-q <br> 0 ssd 1.00000 osd.0 up 1.00000 1.00000 <br> 1 ssd 1.00000 osd.1 up 1.00000 1.00000 <br> 2 ssd 1.00000 osd.2 up 1.00000 1.00000 <br> 3 ssd 1.00000 osd.3 up 1.00000 1.00000 <br> 4 hdd 1.00000 osd.4 up 1.00000 1.00000 <br> 5 hdd 0.27299 osd.5 up 1.00000 1.00000 <br> 6 hdd 0.27299 osd.6 up 1.00000 1.00000 <br> 7 hdd 0.27299 osd.7 up 1.00000 1.00000 <br> 8 hdd 0.27299 osd.8 up 1.00000 1.00000 <br> 9 hdd 0.27299 osd.9 up 1.00000 1.00000 <br> 10 hdd 0.27299 osd.10 up 1.00000 1.00000 <br> 11 hdd 0.27299 osd.11 up 1.00000 1.00000 <br> 12 hdd 0.27299 osd.12 up 1.00000 1.00000 <br> 13 hdd 0.27299 osd.13 up 1.00000 1.00000 <br> 14 hdd 0.27299 osd.14 up 1.00000 1.00000 <br> 15 hdd 0.27299 osd.15 up 1.00000 1.00000 <br> 16 hdd 0.27299 osd.16 up 1.00000 1.00000 <br> 17 hdd 0.27299 osd.17 up 1.00000 1.00000 <br> 18 hdd 0.27299 osd.18 up 1.00000 1.00000 <br> 19 hdd 0.27299 osd.19 up 1.00000 1.00000 <br> -5 5.45599 host ceph02-q <br> 20 ssd 0.27299 osd.20 up 1.00000 1.00000 <br> 21 ssd 0.27299 osd.21 up 1.00000 1.00000 <br> 22 ssd 0.27299 osd.22 up 1.00000 1.00000 <br> 23 ssd 0.27299 osd.23 up 1.00000 1.00000 <br> 24 hdd 0.27299 osd.24 up 1.00000 1.00000 <br> 25 hdd 0.27299 osd.25 up 1.00000 1.00000 <br> 26 hdd 0.27299 osd.26 up 1.00000 1.00000 <br> 27 hdd 0.27299 osd.27 up 1.00000 1.00000 <br> 28 hdd 0.27299 osd.28 up 1.00000 1.00000 <br> 29 hdd 0.27299 osd.29 up 1.00000 1.00000 <br> 30 hdd 0.27299 osd.30 up 1.00000 1.00000 <br> 31 hdd 0.27299 osd.31 up 1.00000 1.00000 <br> 32 hdd 0.27299 osd.32 up 1.00000 1.00000 <br> 33 hdd 0.27299 osd.33 up 1.00000 1.00000 <br> 34 hdd 0.27299 osd.34 up 1.00000 1.00000 <br> 35 hdd 0.27299 osd.35 up 1.00000 1.00000 <br> 36 hdd 0.27299 osd.36 up 1.00000 1.00000 <br> 37 hdd 0.27299 osd.37 up 1.00000 1.00000 <br> 38 hdd 0.27299 osd.38 up 1.00000 1.00000 <br> 39 hdd 0.27299 osd.39 up 1.00000 1.00000 <br> -7 6.08690 host ceph03-q <br> 40 ssd 0.27299 osd.40 up 1.00000 1.00000 <br> 41 ssd 0.27299 osd.41 up 1.00000 1.00000 <br> 42 ssd 0.27299 osd.42 up 1.00000 1.00000 <br> 43 ssd 0.27299 osd.43 up 1.00000 1.00000 <br> 44 hdd 0.27299 osd.44 up 1.00000 1.00000 <br> 45 hdd 0.27299 osd.45 up 1.00000 1.00000 <br> 46 hdd 0.27299 osd.46 up 1.00000 1.00000 <br> 47 hdd 0.27299 osd.47 up 1.00000 1.00000 <br> 48 hdd 0.27299 osd.48 up 1.00000 1.00000 <br> 49 hdd 0.27299 osd.49 up 1.00000 1.00000 <br> 50 hdd 0.27299 osd.50 up 1.00000 1.00000 <br> 51 hdd 0.27299 osd.51 up 1.00000 1.00000 <br> 52 hdd 0.27299 osd.52 up 1.00000 1.00000 <br> 53 hdd 0.27299 osd.53 up 1.00000 1.00000 <br> 54 hdd 0.27299 osd.54 up 1.00000 1.00000 <br> 55 hdd 0.27299 osd.55 up 1.00000 1.00000 <br> 56 hdd 0.27299 osd.56 up 1.00000 1.00000 <br> 57 hdd 0.27299 osd.57 up 1.00000 1.00000 <br> 58 hdd 0.27299 osd.58 up 1.00000 1.00000 <br> 59 hdd 0.89999 osd.59 up 1.00000 1.00000 </p></div></div><br><p style=";text-align:right;direction:rtl">          : </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket rack01 root #  root root@ceph01-q:~#ceph osd crush add-bucket ceph01-q host #   root@ceph01-q:~#ceph osd crush move ceph01-q root=rack01 #     root@ceph01-q:~#osd crush add 28 1.0 host=ceph02-q #     #       root@ceph01-q:~# ceph osd crush remove osd.4 root@ceph01-q:~# ceph osd crush remove rack01</code> </pre> <br><p style=";text-align:right;direction:rtl"> ,      <strong></strong> ,            โ  <strong>ceph osd crush move ceph01-host root=rack01</strong> ,      .    CTRL+C     . </p><br><p style=";text-align:right;direction:rtl">    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://tracker.ceph.com/issues/23386</a> </p><br><p style=";text-align:right;direction:rtl">    crushmap     <strong>rule replicated_ruleset</strong> </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd getcrushmap -o crushmap.row #     root@ceph01-prod:~#crushtool -d crushmap.row -o crushmap.txt #   root@ceph01-prod:~#vim crushmap.txt #,  rule replicated_ruleset root@ceph01-prod:~#crushtool -c crushmap.txt -o new_crushmap.row #  root@ceph01-prod:~#ceph osd setcrushmap -i new_crushmap.row #  </code> </pre> <br><p style=";text-align:right;direction:rtl"> <strong>:</strong>      placement group  OSD.    ,   . </p><br><p style=";text-align:right;direction:rtl">  ,        โ  ,     OSD ,        ,    root default. <br>  ,   ,      root  ssd     ,          default root.   OSD     . <br> <em>     ,     .     </em> </p><br><h3 id="kak-my-delali-razlichnye-gruppy-po-tipam-diskov" style=";text-align:right;direction:rtl">        . </h3><br><p style=";text-align:right;direction:rtl">     root- โ  ssd   hdd </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket ssd-root root root@ceph01-q:~#ceph osd crush add-bucket hdd-root root</code> </pre> <br><p style=";text-align:right;direction:rtl">        โ          </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"># : root@ceph01-q:~#ceph osd crush add-bucket ssd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack02 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack03 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack #  root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph03-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host</code> </pre> <br><p style=";text-align:right;direction:rtl">          </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root@ceph01-q:~#   0  3  SSD,   ceph01-q,     root@ceph01-q:~# ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 0 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 1 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 2 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 3 1 host=ssd-ceph01-q root-ceph01-q:~#    </code> </pre> <br><p style=";text-align:right;direction:rtl">     ssd-root  hdd-root   root-default ,     </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush remove default</code> </pre> <br><p style=";text-align:right;direction:rtl">     ,        โ      root          โ        ,     (    root,    ) </p><br><p style=";text-align:right;direction:rtl">        : <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules</a> </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush rule create-simple rule-ssd ssd-root host firstn root-ceph01-q:~#ceph osd crush rule create-simple rule-hdd hdd-root host firstn root-ceph01-q:~#    ,     root-ceph01-q:~#   -        , root-ceph01-q:~#       root-ceph01-q:~#  ,   ,    root-ceph01-q:~#        : root-ceph01-q:~# ##ceph osd crush rule create-simple rule-ssd ssd-root rack firstn</code> </pre> <br><p style=";text-align:right;direction:rtl">    ,            โ PROXMOX: </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"> root-ceph01-q:~# #ceph osd pool create {NAME} {pg_num} {pgp_num} root-ceph01-q:~# ceph osd pool create ssd_pool 1024 1024 root-ceph01-q:~# ceph osd pool create hdd_pool 1024 1024</code> </pre> <br><p style=";text-align:right;direction:rtl">         </p><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"> root-ceph01-q:~#ceph osd crush rule ls #    root-ceph01-q:~#ceph osd crush rule dump rule-ssd | grep rule_id # ID  root-ceph01-q:~#ceph osd pool set ssd_pool crush_rule 2</code> </pre><br><p style=";text-align:right;direction:rtl">               โ     ,    (    )   ,    . </p><br><p style=";text-align:right;direction:rtl">      300    ,        โ        10 Tb    10 PG โ      (pg)   โ           ). </p><br><p style=";text-align:right;direction:rtl">        PG โ         โ     . </p><br><p style=";text-align:right;direction:rtl">    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u="> </a> ,    CEPH. </p><br><p style=";text-align:right;direction:rtl">  : </p><br><p style=";text-align:right;direction:rtl"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://tracker.ceph.com/issues/23386</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">https://ceph.com/pgcalc/</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar456446/">https://habr.com/ru/post/ar456446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar456430/index.html">ุงููุถูุฉ ุงูุฃุจุฏูุฉ ููุฏูู ุงูููู</a></li>
<li><a href="../ar456432/index.html">ุฃุณุจูุน ุงูุฃูู 25: ุงูุถุนู Evernote ููุฆุงุช ูู ุงููุชุงุฌุฑ ุนูู ุงูุฅูุชุฑูุช ุงุฎุชุฑู</a></li>
<li><a href="../ar456434/index.html">ููู ุงููุณุชูุจู: "ูู ุณุชุนูู ุนูู ุงููุฑูุฎุ"</a></li>
<li><a href="../ar456436/index.html">ูููุฉ JS ูุตูุฑุฉ ูููู ุงูุงุซููู</a></li>
<li><a href="../ar456440/index.html">ูุบุงูุฑุงุช ุงููุฑุงูุบ ูุงููุงุฑู ุ ุงูุฌุฒุก ุงูุฃูู</a></li>
<li><a href="../ar456448/index.html">ููุงุนุฏ ูุงุฎุชูุงุฑ ุฅุทุงุฑ ุนูู JS</a></li>
<li><a href="../ar456450/index.html">DO-RA.Avia ูุฑุตุฏ ุงูุฅุดุนุงุน ุงููููู ูู ูุฌุงู ุงูุทูุฑุงู</a></li>
<li><a href="../ar456452/index.html">ุฃูุซูุฉ ุฑูุฒ C ++ ูุจู ูุจุนุฏ ุงููุทุงูุงุช</a></li>
<li><a href="../ar456462/index.html">ุชุฌููุน ููุชุจุฉ ุงูููููุงุช ุงูุฒุงูู ูููููุงุช ููุจ</a></li>
<li><a href="../ar456466/index.html">PHP ุงููุฑุงุซุฉ ุงูููู (ุฌูุฏูุง ุ ุชูุฑูุจูุง)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>