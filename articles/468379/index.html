<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ûó üòÉ üë©‚Äç‚úàÔ∏è Inteligencia artificial de uso general. TK, estado actual, perspectivas üçâ üñ≤Ô∏è üë®‚Äçüè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hoy en d√≠a, las palabras "inteligencia artificial" significan muchos sistemas diferentes, desde una red neuronal para el reconocimiento de im√°genes ha...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Inteligencia artificial de uso general. TK, estado actual, perspectivas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/468379/">  Hoy en d√≠a, las palabras "inteligencia artificial" significan muchos sistemas diferentes, desde una red neuronal para el reconocimiento de im√°genes hasta un bot para jugar Quake.  Wikipedia ofrece una maravillosa definici√≥n de IA: esta es "la propiedad de los sistemas inteligentes para realizar funciones creativas que tradicionalmente se consideran prerrogativas del hombre".  Es decir, se ve claramente en la definici√≥n: si una determinada funci√≥n se automatiz√≥ con √©xito, entonces deja de considerarse inteligencia artificial. <br><br>  Sin embargo, cuando se estableci√≥ por primera vez la tarea de "crear inteligencia artificial", AI significaba algo diferente.  Este objetivo ahora se llama IA fuerte o IA de prop√≥sito general. <br><a name="habracut"></a><br><h2>  Declaraci√≥n del problema. </h2><br>  Ahora hay dos formulaciones bien conocidas del problema.  El primero es la IA fuerte.  El segundo es una IA de prop√≥sito general (tambi√©n conocida como Inteligencia General Artificial, abreviada AGI). <br>  Upd.  En los comentarios, me dicen que esta diferencia es m√°s probable en el nivel del idioma.  En ruso, la palabra "inteligencia" no significa exactamente lo que la palabra "inteligencia" en ingl√©s <br><br>  <b>Una IA fuerte</b> es una IA hipot√©tica que podr√≠a hacer todo lo que una persona podr√≠a hacer.  Por lo general, se menciona que debe pasar la prueba de Turing en la configuraci√≥n inicial (hmm, ¬øla gente lo pasa?), Ser consciente de s√≠ mismo como una persona separada y ser capaz de lograr sus objetivos. <br><br>  Es decir, es algo as√≠ como una persona artificial.  En mi opini√≥n, la utilidad de tal IA es principalmente investigaci√≥n, porque las definiciones de una IA fuerte no dicen en ninguna parte cu√°les ser√°n sus objetivos. <br><br>  <b>AGI o IA de prop√≥sito general</b> es una "m√°quina de resultados".  Ella recibe un determinado objetivo en la entrada y da algunas acciones de control en motores / l√°seres / tarjeta de red / monitores.  Y el objetivo se logra.  Al mismo tiempo, AGI inicialmente no tiene conocimiento sobre el medio ambiente, solo sensores, actuadores y el canal a trav√©s del cual establece objetivos.  El sistema de gesti√≥n se considerar√° un AGI si puede alcanzar cualquier objetivo en cualquier entorno.  La llevamos a conducir un autom√≥vil y evitar accidentes, ella lo manejar√°.  La pusimos en control de un reactor nuclear para que haya m√°s energ√≠a, pero no explote, ella puede manejarlo.  Le daremos un buz√≥n de correo y le indicaremos que venda aspiradoras, tambi√©n lo veremos.  AGI es un solucionador de "problemas inversos".  Comprobar cu√°ntas aspiradoras se venden es una cuesti√≥n simple.  Pero descubrir c√≥mo convencer a una persona para que compre esta aspiradora ya es una tarea para el intelecto. <br><br>  En este art√≠culo hablar√© sobre AGI.  Sin pruebas de Turing, sin autoconciencia, sin personalidades artificiales: inteligencia artificial excepcionalmente pragm√°tica y no menos operadores pragm√°ticos. <br><br><h2>  Estado actual de las cosas </h2><br>  Ahora existe una clase de sistemas como el aprendizaje reforzado o el aprendizaje reforzado.  Esto es algo as√≠ como AGI, solo que sin versatilidad.  Son capaces de aprender y, debido a esto, alcanzar objetivos en una variedad de entornos.  Pero a√∫n est√°n muy lejos de lograr objetivos en cualquier entorno. <br><br>  En general, ¬øc√≥mo se organizan los sistemas de aprendizaje por refuerzo y cu√°les son sus problemas? <br><br><img src="https://habrastorage.org/webt/oj/3-/dl/oj3-dl6vkgtbhaxeilj7rkl0jxe.png"><br><br>  Cualquier RL est√° organizado de esta manera.  Hay un sistema de control, algunas se√±ales sobre la realidad circundante ingresan a trav√©s de los sensores (estado) y a trav√©s de los cuerpos gobernantes (acciones) que act√∫an sobre la realidad circundante.  La recompensa es una se√±al de refuerzo.  En los sistemas RL, el refuerzo se forma desde el exterior de la unidad de control e indica qu√© tan bien la IA se las arregla para lograr el objetivo.  Cu√°ntas aspiradoras vendidas en el √∫ltimo minuto, por ejemplo. <br>  Luego se forma una tabla de algo como esto (lo llamar√© la tabla SAR): <br><br><img src="https://habrastorage.org/webt/lp/oo/ek/lpooekbdpwktkkmvsilzsytsnbo.png"><br><br>  El eje del tiempo se dirige hacia abajo.  La tabla muestra todo lo que hizo la IA, todo lo que vio y todas las se√±ales de refuerzo.  Por lo general, para que RL haga algo significativo, primero necesita hacer movimientos aleatorios por un tiempo o mirar los movimientos de otra persona.  En general, RL comienza cuando ya hay al menos algunas l√≠neas en la tabla SAR. <br>  ¬øQu√© pasa despu√©s? <br><br><h3>  Sarsa </h3><br>  La forma m√°s simple de aprendizaje por refuerzo. <br><br>  Tomamos alg√∫n tipo de modelo de aprendizaje autom√°tico y, usando una combinaci√≥n de S y A (estado y acci√≥n), predecimos la R total para los pr√≥ximos ciclos de reloj.  Por ejemplo, veremos que (seg√∫n la tabla anterior) si le dice a una mujer "¬°sea un hombre, compre una aspiradora!", Entonces la recompensa ser√° baja, y si le dice lo mismo a un hombre, entonces alta. <br><br>  Qu√© modelos espec√≠ficos se pueden usar: lo describir√© m√°s adelante, por ahora solo dir√© que no se trata solo de redes neuronales.  Puede usar √°rboles de decisi√≥n o incluso definir una funci√≥n en forma de tabla. <br><br>  Y luego sucede lo siguiente.  AI recibe otro mensaje o enlace a otro cliente.  Todos los datos del cliente se ingresan en la IA desde el exterior: consideraremos la base de clientes y el contador de mensajes como parte del sistema de sensores.  Es decir, queda asignar algo de A (acci√≥n) y esperar refuerzos.  AI realiza todas las acciones posibles y, a su vez, predice (utilizando el mismo modelo de Machine Learning): ¬øqu√© suceder√° si hago eso?  ¬øY si es as√≠?  ¬øY cu√°nto refuerzo habr√° para esto?  Y luego RL realiza la acci√≥n para la cual se espera la recompensa m√°xima. <br><br>  Introduje un sistema tan simple y torpe en uno de mis juegos.  SARSA contrata unidades en el juego y se adapta en caso de un cambio en las reglas del juego. <br><br>  Adem√°s, en todos los tipos de entrenamiento reforzado, hay un descuento de recompensas y un dilema de exploraci√≥n / explotaci√≥n. <br><br>  Descontar los premios es un enfoque de este tipo cuando RL intenta maximizar no el monto de la recompensa para los pr√≥ximos N movimientos, sino el monto ponderado seg√∫n el principio "100 rublos ahora es mejor que 110 en un a√±o".  Por ejemplo, si el factor de descuento es 0.9, y el horizonte de planificaci√≥n es 3, entonces entrenaremos el modelo no en el R total para los pr√≥ximos 3 ciclos de reloj, sino en R1 * 0.9 + R2 * 0.81 + R3 * 0.729.  ¬øPor qu√© es esto necesario?  Entonces, esa IA, creando una ganancia en alg√∫n lugar all√≠ en el infinito, no la necesitamos.  Necesitamos una IA que genere ganancias aqu√≠ y ahora. <br>  Explorar / explotar dilema.  Si RL hace lo que su modelo considera √≥ptimo, nunca sabr√° si hubo mejores estrategias.  Exploit es una estrategia en la que RL hace lo que promete recompensas m√°ximas.  Explorar es una estrategia en la que RL hace algo para explorar el entorno en busca de mejores estrategias.  ¬øC√≥mo implementar una inteligencia efectiva?  Por ejemplo, puede realizar una acci√≥n aleatoria cada pocas medidas.  O puede hacer no un modelo predictivo, sino varios con configuraciones ligeramente diferentes.  Producir√°n resultados diferentes.  Cuanto mayor es la diferencia, mayor es el grado de incertidumbre de esta opci√≥n.  Puede realizar la acci√≥n para que tenga el valor m√°ximo: M + k * std, donde M es el pron√≥stico promedio de todos los modelos, std es la desviaci√≥n est√°ndar de los pron√≥sticos y k es el coeficiente de curiosidad. <br><br>  <b>¬øCu√°les son las desventajas?</b> <br><br>  Digamos que tenemos opciones.  Vaya a la meta (que est√° a 10 km de nosotros, y el camino es bueno) en autom√≥vil o a pie.  Y luego, despu√©s de esta elecci√≥n, tenemos opciones: movernos con cuidado o tratar de chocar contra cada pilar. <br><br>  La persona dir√° de inmediato que generalmente es mejor conducir un autom√≥vil y comportarse con cuidado. <br><br>  Pero SARSA ... Ver√° a lo que llev√≥ la decisi√≥n de ir en autom√≥vil antes.  Pero llev√≥ a esto.  En la etapa del conjunto inicial de estad√≠sticas, la IA condujo imprudentemente y se estrell√≥ en alg√∫n lugar en la mitad de los casos.  S√≠, √©l puede conducir bien.  Pero cuando elige ir en autom√≥vil, no sabe qu√© elegir√° para el pr√≥ximo movimiento.  Tiene estad√≠sticas: luego, en la mitad de los casos, eligi√≥ la opci√≥n adecuada, y en la mitad, suicida.  Por lo tanto, en promedio, es mejor caminar. <br><br>  SARSA cree que el agente se adherir√° a la misma estrategia que se utiliz√≥ para completar la tabla.  Y act√∫a sobre esta base.  Pero, ¬øqu√© sucede si suponemos lo contrario, que el agente se adherir√° a la mejor estrategia en los pr√≥ximos movimientos? <br><br><h3>  Q-learning </h3><br>  Este modelo calcula para cada estado la recompensa total m√°xima alcanzable de √©l.  Y lo escribe en una columna especial Q. Es decir, si del estado S puede obtener 2 puntos o 1, dependiendo del movimiento, Q (S) ser√° igual a 2 (con una profundidad de predicci√≥n de 1).  ¬øQu√© recompensa se puede obtener del estado S? Aprendemos del modelo predictivo Y (S, A).  (S - estado, A - acci√≥n). <br><br>  Luego creamos un modelo predictivo Q (S, A), es decir, a qu√© estado ir√° Q si realizamos la acci√≥n A desde S. Y creamos la siguiente columna en la tabla: Q2.  Es decir, el Q m√°ximo que se puede obtener del estado S (clasificamos todos los A posibles). <br><br>  Luego creamos un modelo de regresi√≥n Q3 (S, A), es decir, al estado con el que iremos Q2 si realizamos la acci√≥n A desde S. <br><br>  Y as√≠ sucesivamente.  Por lo tanto, podemos lograr una profundidad ilimitada de pron√≥stico. <br><br><img src="https://habrastorage.org/webt/ie/e_/gn/iee_gnm9ldz5uiv0dmyj4jni470.jpeg"><br><br>  En la imagen, R es el refuerzo. <br><br>  Y luego, cada movimiento seleccionamos la acci√≥n que promete el mayor Qn.  Si aplicamos este algoritmo al ajedrez, obtendr√≠amos algo as√≠ como un minimax ideal.  Algo casi equivalente a calcular mal movimientos a grandes profundidades. <br><br>  Un ejemplo com√∫n de comportamiento de q-learning.  El cazador tiene una lanza, y lo acompa√±a al oso, por iniciativa propia.  √âl sabe que la gran mayor√≠a de sus movimientos futuros tienen una recompensa negativa muy grande (hay muchas m√°s formas de perder que formas de ganar), √©l sabe que hay movimientos con una recompensa positiva.  El cazador cree que en el futuro har√° los mejores movimientos (y no se sabe cu√°les como en SARSA), y si hace los mejores movimientos, derrotar√° al oso.  Es decir, para ir al oso, es suficiente para que √©l pueda hacer todos los elementos necesarios para la caza, pero no es necesario tener una experiencia de √©xito inmediato. <br><br>  Si el cazador actuara al estilo SARSA, supondr√≠a que sus acciones en el futuro ser√≠an m√°s o menos las mismas que antes (a pesar de que ahora tiene un bagaje de conocimientos diferente), y solo ir√≠a al oso si ya fue a y gan√≥, por ejemplo, en&gt; 50% de los casos (bueno, o si otros cazadores ganaron en m√°s de la mitad de los casos, si aprende de su experiencia). <br><br>  <b>¬øCu√°les son las desventajas?</b> <br><br><ol><li>  El modelo no hace frente a la realidad cambiante.  Si toda nuestra vida hemos sido premiados por presionar el bot√≥n rojo, y ahora nos est√°n castigando, y no se han producido cambios visibles ... QL dominar√° este patr√≥n durante mucho tiempo. </li><li>  Qn puede ser una funci√≥n muy compleja.  Por ejemplo, para calcularlo, debe desplazarse por un ciclo de N iteraciones, y no funcionar√° m√°s r√°pido.  Y el modelo predictivo generalmente tiene una complejidad limitada: incluso una red neuronal grande tiene un l√≠mite de complejidad, y casi ning√∫n modelo de aprendizaje autom√°tico puede rotar los ciclos. </li><li>  La realidad generalmente tiene variables ocultas.  Por ejemplo, ¬øqu√© hora es ahora?  Es f√°cil saber si miramos el reloj, pero tan pronto como miramos hacia otro lado, ya es una variable oculta.  Para tener en cuenta estos valores no observables, es necesario que el modelo tenga en cuenta no solo el estado actual, sino tambi√©n alg√∫n tipo de historia.  En QL, puede hacer esto, por ejemplo, para alimentar no solo la S actual, sino tambi√©n varias anteriores, a la neurona o lo que est√° all√≠.  Esto se hace en RL, que juega juegos de Atari.  Adem√°s, puede usar una red neuronal recurrente para el pron√≥stico: deje que se ejecute secuencialmente en varios cuadros de la historia y calcule Qn. </li></ol><br><h3>  Sistemas basados ‚Äã‚Äãen modelos. </h3><br>  Pero, ¬øqu√© sucede si predecimos no solo R o Q, sino en general todos los datos sensoriales?  Constantemente tendremos una copia de bolsillo de la realidad y podremos verificar nuestros planes al respecto.  En este caso, estamos mucho menos preocupados por la dificultad de calcular la funci√≥n Q.  S√≠, requiere muchos relojes para calcular, bueno, de todos modos, para cada plan, ejecutaremos repetidamente el modelo de pron√≥stico.  ¬øPlaneando 10 movimientos hacia adelante?  Lanzamos el modelo 10 veces, y cada vez que alimentamos sus salidas a su entrada. <br><br>  <b>¬øCu√°les son las desventajas?</b> <br><br><ol><li>  Intensidad de recursos.  Supongamos que necesitamos elegir dos alternativas en cada medida.  Luego, durante 10 ciclos de reloj tendremos 2 ^ 10 = 1024 planes posibles.  Cada plan tiene 10 lanzamientos de modelos.  ¬øSi controlamos un avi√≥n con docenas de √≥rganos de gobierno?  ¬øY simulamos la realidad con un per√≠odo de 0.1 segundos?  ¬øDesea tener un horizonte de planificaci√≥n por al menos un par de minutos?  Tendremos que ejecutar el modelo muchas veces, hay muchos ciclos de reloj del procesador para una soluci√≥n.  Incluso si de alguna manera optimizas la enumeraci√≥n de planes, de todos modos, hay √≥rdenes de magnitud m√°s c√°lculos que en QL. </li><li>  El problema del caos.  Algunos sistemas est√°n dise√±ados para que incluso una peque√±a imprecisi√≥n de la simulaci√≥n de entrada provoque un gran error de salida.  Para contrarrestar esto, puede ejecutar varias simulaciones de la realidad, un poco diferente.  Producir√°n resultados muy diferentes, y de esto ser√° posible entender que estamos en la zona de tal inestabilidad. </li></ol><br><h2>  M√©todo de enumeraci√≥n de estrategia </h2><br>  Si tenemos acceso al entorno de prueba para IA, si lo ejecutamos no en realidad, sino en una simulaci√≥n, entonces podemos escribir de alguna forma la estrategia del comportamiento de nuestro agente.  Y luego elija, con evoluci√≥n u otra cosa, una estrategia que conduzca al m√°ximo beneficio. <br>  "Elegir una estrategia" significa que primero tenemos que aprender a escribir una estrategia de tal manera que pueda introducirse en el algoritmo de evoluci√≥n.  Es decir, podemos escribir la estrategia con el c√≥digo del programa, pero en algunos lugares dejamos los coeficientes y dejamos que la evoluci√≥n los recoja.  O podemos escribir una estrategia con una red neuronal y dejar que la evoluci√≥n recoja el peso de sus conexiones. <br><br>  Es decir, no hay pron√≥stico aqu√≠.  No hay mesa SAR.  Simplemente seleccionamos una estrategia, e inmediatamente da Acciones. <br><br>  Este es un m√©todo poderoso y efectivo, si desea probar RL y no sabe por d√≥nde comenzar, lo recomiendo.  Esta es una forma muy barata de "ver un milagro". <br><br>  <b>¬øCu√°les son las desventajas?</b> <br><br><ol><li>  Se requiere la capacidad de ejecutar los mismos experimentos muchas veces.  Es decir, deber√≠amos ser capaces de rebobinar la realidad hasta el punto de partida, decenas de miles de veces.  Para probar una nueva estrategia. <br><br>  La vida rara vez brinda tales oportunidades.  Por lo general, si tenemos un modelo del proceso que nos interesa, no podemos crear una estrategia astuta: simplemente podemos elaborar un plan, como en un enfoque basado en modelos, incluso con fuerza bruta contundente. </li><li>  Intolerancia a la experiencia.  ¬øTenemos una mesa SAR para a√±os de experiencia?  Podemos olvidarlo, no encaja en el concepto. </li></ol><br><h3>  Un m√©todo para enumerar estrategias, pero "en vivo" </h3><br>  La misma enumeraci√≥n de estrategias, pero sobre la realidad viva.  Intentamos 10 medidas de una estrategia.  Entonces 10 mide otro.  Luego 10 medidas de la tercera.  Luego seleccionamos aquel donde hab√≠a m√°s refuerzo. <br>  Los mejores resultados para caminar humanoides se obtuvieron mediante este m√©todo. <br><br><img src="https://habrastorage.org/webt/zh/v_/sn/zhv_snutr8ma1aeenqr3itjojvc.png"><br><br>  Para m√≠, esto suena algo inesperado: parece que el enfoque basado en el modelo QL + es matem√°ticamente ideal.  Pero nada de eso.  Las ventajas del enfoque son aproximadamente las mismas que las anteriores, pero son menos pronunciadas, ya que las estrategias no se prueban por mucho tiempo (bueno, no tenemos milenios en evoluci√≥n), lo que significa que los resultados son inestables.  Adem√°s, el n√∫mero de pruebas tampoco puede elevarse hasta el infinito, lo que significa que la estrategia deber√° buscarse en un espacio de opciones no muy complicado.  No solo tendr√° "plumas" que se pueden "torcer".  Bueno, la intolerancia a la experiencia no ha sido cancelada.  Y, en comparaci√≥n con QL o basado en modelos, estos modelos utilizan la experiencia de manera ineficiente.  Necesitan muchas m√°s interacciones con la realidad que los enfoques que utilizan el aprendizaje autom√°tico. <br><br>  Como puede ver, cualquier intento de crear un AGI en teor√≠a deber√≠a incluir el aprendizaje autom√°tico para pronosticar premios, o alguna forma de notaci√≥n param√©trica de una estrategia, para que pueda elegir esta estrategia con algo como la evoluci√≥n. <br><br>  Este es un fuerte ataque hacia las personas que ofrecen crear IA basada en bases de datos, l√≥gica y gr√°ficos conceptuales.  Si ustedes, los defensores del enfoque simb√≥lico, leen esto, bienvenidos a los comentarios, estar√© encantado de saber qu√© puede hacer AGI sin la mec√°nica descrita anteriormente. <br><br><h2>  Modelos de aprendizaje autom√°tico para RL </h2><br>  Casi cualquier modelo de ML se puede utilizar para el aprendizaje reforzado.  Las redes neuronales son, por supuesto, buenas.  Pero hay, por ejemplo, KNN.  Para cada par S y A, buscamos los m√°s similares, pero en el pasado.  ¬øY estamos buscando qu√© ser√° R. Stupid despu√©s de eso?  Si, pero funciona.  Hay √°rboles decisivos: aqu√≠ es mejor dar un paseo por las palabras clave "aumento de gradiente" y "bosque decisivo".  ¬øLos √°rboles son pobres para capturar dependencias complejas?  Utilice la ingenier√≠a de caracter√≠sticas.  ¬øQuieres tu IA m√°s cerca de General?  ¬°Usa FE autom√°tico!  Revise un mont√≥n de f√≥rmulas diferentes, env√≠elas como caracter√≠sticas para su impulso, descarte las f√≥rmulas que aumentan el error y deje las f√≥rmulas que mejoran la precisi√≥n.  Luego, env√≠e las mejores f√≥rmulas como argumentos para las nuevas f√≥rmulas, y as√≠ sucesivamente, evolucione. <br><br>  Puede usar regresiones simb√≥licas para pronosticar, es decir, simplemente ordenar f√≥rmulas en un intento de obtener algo que se aproxime a Q o R. Es posible intentar ordenar algoritmos; luego se obtiene una cosa llamada inducci√≥n de Solomonov, que es te√≥ricamente √≥ptima, pero casi muy dif√≠cil de entrenar aproximaciones de funciones. <br><br>  Pero las redes neuronales suelen ser un compromiso entre la expresividad y la complejidad del aprendizaje.  La regresi√≥n algor√≠tmica idealmente recoge cualquier dependencia, durante cientos de a√±os.  El √°rbol de decisi√≥n funcionar√° muy r√°pidamente, pero no podr√° extrapolar y = a + b.  Una red neuronal es algo intermedio. <br><br><h2>  Perspectivas de desarrollo. </h2><br>  ¬øCu√°les son las formas de hacer exactamente AGI ahora?  Al menos te√≥ricamente. <br><br><h3>  Evoluci√≥n </h3><br>  Podemos crear muchos entornos de prueba diferentes y comenzar la evoluci√≥n de algunas redes neuronales.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las configuraciones que obtengan m√°s puntos en total para todas las pruebas se multiplicar√°n. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La red neuronal debe tener memoria y ser√≠a deseable tener al menos parte de la memoria en forma de cinta, como una m√°quina de Turing o como en un disco duro. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El problema es que, con la ayuda de la evoluci√≥n, puede crecer algo como RL, por supuesto. </font><font style="vertical-align: inherit;">Pero, ¬øc√≥mo deber√≠a ser el lenguaje en el que RL parece compacto, para que la evoluci√≥n lo encuentre, y al mismo tiempo que la evoluci√≥n no encuentra soluciones como "pero crear√© una neurona para ciento cincuenta capas para que todos se vuelvan locos mientras lo ense√±o!" . </font><font style="vertical-align: inherit;">La evoluci√≥n es como una multitud de usuarios analfabetos: encontrar√° fallas en el c√≥digo y abandonar√° todo el sistema.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aixi </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Puede hacer un sistema basado en modelos basado en un paquete de muchas regresiones algor√≠tmicas. </font><font style="vertical-align: inherit;">Se garantiza que el algoritmo estar√° completo en Turing, lo que significa que no habr√° patrones que no se puedan recoger. </font><font style="vertical-align: inherit;">El algoritmo est√° escrito en c√≥digo, lo que significa que su complejidad se puede calcular f√°cilmente. </font><font style="vertical-align: inherit;">Esto significa que es posible refinar matem√°ticamente correctamente sus hip√≥tesis sobre el dispositivo de complejidad del mundo. </font><font style="vertical-align: inherit;">Con las redes neuronales, por ejemplo, este truco no funcionar√°; all√≠ la penalizaci√≥n por la complejidad es muy indirecta y heur√≠stica. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Solo queda aprender a entrenar r√°pidamente regresiones algor√≠tmicas. </font><font style="vertical-align: inherit;">Hasta ahora, lo mejor que hay para esto es la evoluci√≥n, y es imperdonablemente largo.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AI de semillas </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ser√≠a genial crear una IA que se mejore a s√≠ misma. Mejora tu capacidad para resolver problemas. Esto puede parecer una idea extra√±a, pero este problema </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ya se ha resuelto para sistemas de optimizaci√≥n est√°tica, como la evoluci√≥n</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Si logras darte cuenta de esto ... ¬øLo sabe todo sobre el expositor? Obtendremos una IA muy poderosa en muy poco tiempo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como hacerlo </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Puede intentar arreglar que en RL algunas de las acciones afecten la configuraci√≥n de RL. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O proporcione al sistema RL alguna herramienta para crear nuevos procesadores de datos previos y posteriores. Deje que RL sea tonto, pero podr√° crear calculadoras, computadoras port√°tiles y computadoras por s√≠ mismo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Otra opci√≥n es crear alg√∫n tipo de IA utilizando la evoluci√≥n, en la que parte de las acciones afectar√°n a su dispositivo a nivel de c√≥digo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero por el momento no he visto opciones viables para Seed AI, aunque muy limitadas. </font><font style="vertical-align: inherit;">¬øSe est√°n escondiendo los desarrolladores? </font><font style="vertical-align: inherit;">¬øO son estas opciones tan d√©biles que no merec√≠an atenci√≥n general y me pasaron por alto? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, ahora tanto Google como DeepMind trabajan principalmente con arquitecturas de redes neuronales. </font><font style="vertical-align: inherit;">Aparentemente, no quieren involucrarse en la enumeraci√≥n combinatoria y tratar de hacer que cualquiera de sus ideas sea adecuada para el m√©todo de propagaci√≥n del error. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que este art√≠culo de revisi√≥n haya resultado √∫til =) ¬°Los comentarios son bienvenidos, especialmente comentarios como "S√© c√≥mo mejorar AGI"!</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/468379/">https://habr.com/ru/post/468379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../468363/index.html">Mi magnum opus del mundo de los juegos m√≥viles</a></li>
<li><a href="../468367/index.html">Amazon anuncia plan de calentamiento global</a></li>
<li><a href="../468369/index.html">C√≥mo cre√© "WildMAN" - una parodia de muchos juegos de 8 bits y recientemente lo port√© a Android</a></li>
<li><a href="../468371/index.html">Dise√±o de juegos a la vida. Descarga sin problemas o inmersi√≥n completa en God of War 4</a></li>
<li><a href="../468377/index.html">8 historias sobre el interior de China. Lo que no se muestra a los extranjeros</a></li>
<li><a href="../468381/index.html">Regreso al futuro? Borrador Pendiente Cu√°ntico</a></li>
<li><a href="../468383/index.html">Ruby meme generator para atraer inter√©s en el idioma</a></li>
<li><a href="../468385/index.html">El escritorio est√° muerto, ¬°viva el escritorio! Colecciono habrastatistiki</a></li>
<li><a href="../468389/index.html">Artyom Galonsky, Oficina de STO de la Oficina: "Estoy en contra de un ingeniero de DevOps"</a></li>
<li><a href="../468393/index.html">Con un buen microcontrolador y el tiempo vuela r√°pido o un osciloscopio de fin de semana</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>