<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìö üõ°Ô∏è ‚ûø Travailler avec un cluster Proxmox: installation, configuration r√©seau, ZFS, r√©solution de probl√®mes courants üï¥üèΩ üíÉüèø üßò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Au cours des derni√®res ann√©es, j'ai travaill√© en √©troite collaboration avec les clusters Proxmox: de nombreux clients ont besoin de leur propre infras...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Travailler avec un cluster Proxmox: installation, configuration r√©seau, ZFS, r√©solution de probl√®mes courants</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/457894/"> Au cours des derni√®res ann√©es, j'ai travaill√© en √©troite collaboration avec les clusters Proxmox: de nombreux clients ont besoin de leur propre infrastructure pour d√©velopper leur projet.  C'est pourquoi je peux vous parler des erreurs et des probl√®mes les plus courants que vous pouvez √©galement rencontrer.  En plus de cela, nous allons bien s√ªr configurer un cluster de trois n≈ìuds √† partir de z√©ro. <br><img src="https://habrastorage.org/webt/jz/j-/lq/jzj-lqgwozo7rze1o8ij7bvzday.png"><br><a name="habracut"></a><br>  Un cluster Proxmox peut √™tre compos√© de deux serveurs ou plus.  Le nombre maximal de n≈ìuds dans un cluster est de 32 pi√®ces.  Notre propre cluster sera compos√© de trois n≈ìuds sur une multidiffusion (dans l'article, je d√©crirai √©galement comment √©lever un cluster sur l'unicit√© - cela est important si vous basez votre infrastructure de cluster sur Hetzner ou OVH, par exemple).  En bref, la multidiffusion permet le transfert de donn√©es vers plusieurs n≈ìuds simultan√©ment.  Avec la multidiffusion, nous ne pouvons pas penser au nombre de n≈ìuds dans le cluster (en se concentrant sur les limitations ci-dessus). <br><br>  Le cluster lui-m√™me est construit sur un r√©seau interne (il est important que les adresses IP soient sur le m√™me sous-r√©seau), les m√™mes Hetzner et OVH ont la possibilit√© de combiner des n≈ìuds dans diff√©rents centres de donn√©es en utilisant la technologie Virtual Switch (Hetzner) et vRack (OVH) - √† propos de Virtual Switch nous parlerons √©galement dans l'article.  Si votre h√©bergeur ne dispose pas de technologies similaires au travail, vous pouvez utiliser OVS (Open Virtual Switch), qui est pris en charge nativement par Proxmox, ou utiliser un VPN.  Cependant, dans ce cas, je recommande d'utiliser Unicast avec un petit nombre de n≈ìuds - des situations surviennent souvent o√π le cluster ¬´s'effondre¬ª simplement sur la base d'une telle infrastructure r√©seau et doit √™tre restaur√©.  Par cons√©quent, j'essaie d'utiliser OVH et Hetzner dans mon travail - j'ai vu moins de tels incidents, mais tout d'abord, √©tudiez le fournisseur d'h√©bergement qui sera h√©berg√©: a-t-il une technologie alternative, quelles solutions offre-t-il, prend-il en charge la multidiffusion, etc. . <br><br><h3>  Installer Proxmox </h3><br>  Proxmox peut √™tre install√© de deux mani√®res: installateur ISO et installation via shell.  Nous choisissons la deuxi√®me m√©thode, alors installez Debian sur le serveur. <br><br>  Nous proc√©dons directement √† l'installation de Proxmox sur chaque serveur.  L'installation est extr√™mement simple et est d√©crite dans la documentation officielle ici. <br><br>  Ajoutez le r√©f√©rentiel Proxmox et la cl√© de ce r√©f√©rentiel: <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://download.proxmox.com/debian/pve stretch pve-no-subscription"</span></span> &gt; /etc/apt/sources.list.d/pve-install-repo.list wget http://download.proxmox.com/debian/proxmox-ve-release-5.x.gpg -O /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg chmod +r /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg <span class="hljs-comment"><span class="hljs-comment"># optional, if you have a changed default umask</span></span></code> </pre> <br>  Mise √† jour des r√©f√©rentiels et du syst√®me lui-m√™me: <br><br><pre> <code class="bash hljs">apt update &amp;&amp; apt dist-upgrade</code> </pre> <br>  Apr√®s une mise √† jour r√©ussie, installez les packages Proxmox n√©cessaires: <br><br><pre> <code class="bash hljs">apt install proxmox-ve postfix open-iscsi</code> </pre> <br>  <b>Remarque</b> : Postfix et grub seront configur√©s pendant l'installation - l'un d'eux peut √©chouer.  Cela peut √™tre d√ª au fait que le nom d'h√¥te ne se r√©sout pas par son nom.  Modifiez les entr√©es des h√¥tes et effectuez la mise √† jour apt-get <br><br>  √Ä partir de maintenant, nous pouvons nous connecter √† l'interface Web de Proxmox √† l'adresse https: // &lt;adresse-ip-externe&gt;: 8006 (vous rencontrerez un certificat non approuv√© lors de la connexion). <br><br><img src="https://habrastorage.org/webt/e_/cg/mv/e_cgmvs9rrh3qwq0su222v2j0iw.png"><br>  <b>Image 1.</b> Interface Web du n≈ìud Proxmox <br><br><h3>  Installez Nginx et Let's Encrypt Certificate </h3><br>  Je n'aime pas vraiment la situation avec le certificat et l'adresse IP, donc je sugg√®re d'installer Nginx et de configurer le certificat Let's Encrypt.  Je ne d√©crirai pas l'installation de Nginx, je ne laisserai que les fichiers importants pour que le certificat Let's encrypt fonctionne: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/nginx/snippets/letsencrypt.conf</b> <div class="spoiler_text"><pre> <code class="nginx hljs"><span class="hljs-attribute"><span class="hljs-attribute">location</span></span><span class="hljs-regexp"><span class="hljs-regexp"> ^~</span></span> /.well-known/acme-challenge/ { <span class="hljs-attribute"><span class="hljs-attribute">allow</span></span> all; <span class="hljs-attribute"><span class="hljs-attribute">root</span></span> /var/lib/letsencrypt/; <span class="hljs-attribute"><span class="hljs-attribute">default_type</span></span> <span class="hljs-string"><span class="hljs-string">"text/plain"</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">try_files</span></span> <span class="hljs-variable"><span class="hljs-variable">$uri</span></span> =<span class="hljs-number"><span class="hljs-number">404</span></span>; }</code> </pre><br><br></div></div><br>  Commande pour l'√©mission du certificat SSL: <br><br><pre> <code class="bash hljs">certbot certonly --agree-tos --email sos@livelinux.info --webroot -w /var/lib/letsencrypt/ -d proxmox1.domain.name</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Configuration du site dans NGINX</b> <div class="spoiler_text"><pre> <code class="nginx hljs"><span class="hljs-attribute"><span class="hljs-attribute">upstream</span></span> proxmox1.domain.name { <span class="hljs-attribute"><span class="hljs-attribute">server</span></span> <span class="hljs-number"><span class="hljs-number">127.0.0.1:8006</span></span>; } <span class="hljs-section"><span class="hljs-section">server</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">listen</span></span> <span class="hljs-number"><span class="hljs-number">80</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">server_name</span></span> proxmox1.domain.name; <span class="hljs-attribute"><span class="hljs-attribute">include</span></span> snippets/letsencrypt.conf; <span class="hljs-attribute"><span class="hljs-attribute">return</span></span> <span class="hljs-number"><span class="hljs-number">301</span></span> https://<span class="hljs-variable"><span class="hljs-variable">$host</span></span><span class="hljs-variable"><span class="hljs-variable">$request_uri</span></span>; } <span class="hljs-section"><span class="hljs-section">server</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">listen</span></span> <span class="hljs-number"><span class="hljs-number">443</span></span> ssl; <span class="hljs-attribute"><span class="hljs-attribute">server_name</span></span> proxmox1.domain.name; <span class="hljs-attribute"><span class="hljs-attribute">access_log</span></span> /var/log/nginx/proxmox1.domain.name.access.log; <span class="hljs-attribute"><span class="hljs-attribute">error_log</span></span> /var/log/nginx/proxmox1.domain.name.<span class="hljs-literal"><span class="hljs-literal">error</span></span>.log; <span class="hljs-attribute"><span class="hljs-attribute">include</span></span> snippets/letsencrypt.conf; <span class="hljs-attribute"><span class="hljs-attribute">ssl_certificate</span></span> /etc/letsencrypt/live/proxmox1.domain.name/fullchain.pem; <span class="hljs-attribute"><span class="hljs-attribute">ssl_certificate_key</span></span> /etc/letsencrypt/live/proxmox1.domain.name/privkey.pem; <span class="hljs-attribute"><span class="hljs-attribute">location</span></span> / { <span class="hljs-attribute"><span class="hljs-attribute">proxy_pass</span></span> https://proxmox1.domain.name; <span class="hljs-attribute"><span class="hljs-attribute">proxy_next_upstream</span></span> <span class="hljs-literal"><span class="hljs-literal">error</span></span> timeout invalid_header http_500 http_502 http_503 http_504; <span class="hljs-attribute"><span class="hljs-attribute">proxy_redirect</span></span> <span class="hljs-literal"><span class="hljs-literal">off</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_buffering</span></span> <span class="hljs-literal"><span class="hljs-literal">off</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_set_header</span></span> Host <span class="hljs-variable"><span class="hljs-variable">$host</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_set_header</span></span> X-Real-IP <span class="hljs-variable"><span class="hljs-variable">$remote_addr</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_set_header</span></span> X-Forwarded-For <span class="hljs-variable"><span class="hljs-variable">$proxy_add_x_forwarded_for</span></span>; }</code> </pre> <br></div></div><br>  Apr√®s avoir install√© le certificat SSL, n'oubliez pas de le mettre √† renouvellement automatique via cron: <br><br><pre> <code class="bash hljs">0 */12 * * * /usr/bin/certbot -a \! -d /run/systemd/system &amp;&amp; perl -e <span class="hljs-string"><span class="hljs-string">'sleep int(rand(3600))'</span></span> &amp;&amp; certbot -q renew --renew-hook <span class="hljs-string"><span class="hljs-string">"systemctl reload nginx"</span></span></code> </pre> <br>  Super!  Nous pouvons maintenant acc√©der √† notre domaine via HTTPS. <br><br>  <b>Remarque</b> : pour d√©sactiver la fen√™tre d'informations d'abonnement, ex√©cutez cette commande: <br><br><pre> <code class="bash hljs">sed -i.bak <span class="hljs-string"><span class="hljs-string">"s/data.status !== 'Active'/false/g"</span></span> /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js &amp;&amp; systemctl restart pveproxy.service</code> </pre> <br>  <b>Param√®tres r√©seau</b> <br><br>  Avant de vous connecter au cluster, configurez les interfaces r√©seau sur l'hyperviseur.  Il convient de noter que la configuration des n≈ìuds restants n'est pas diff√©rente, √† l'exception des adresses IP et des noms de serveur, donc je ne dupliquerai pas leurs param√®tres. <br><br>  Cr√©ons un pont r√©seau pour le r√©seau interne afin que nos machines virtuelles (dans ma version il y aura un conteneur LXC pour plus de commodit√©), tout d'abord, elles sont connect√©es au r√©seau interne de l'hyperviseur et peuvent interagir entre elles.  Deuxi√®mement, un peu plus tard, nous ajouterons un pont pour le r√©seau externe afin que les machines virtuelles aient leur propre adresse IP externe.  En cons√©quence, les conteneurs seront en ce moment derri√®re NAT'om avec nous. <br><br>  Il existe deux fa√ßons de travailler avec la configuration r√©seau de Proxmox: via l'interface Web ou via le fichier de configuration / etc / network / interfaces.  Dans la premi√®re option, vous devrez red√©marrer le serveur (ou vous pouvez simplement renommer le fichier interfaces.new en interfaces et red√©marrer le service r√©seau via systemd).  Si vous commencez tout juste √† configurer et qu'il n'y a pas encore de machines virtuelles ou de conteneurs LXC, il est conseill√© de red√©marrer l'hyperviseur apr√®s les modifications. <br><br>  Cr√©ez maintenant un pont r√©seau appel√© vmbr1 dans l'onglet r√©seau du panneau Web Proxmox. <br><br><img src="https://habrastorage.org/webt/i3/6k/wp/i36kwpe0ky3khngufngwifulwcs.png"><br>  <b>Figure 2.</b> Interfaces r√©seau du n≈ìud proxmox1 <br><br><img src="https://habrastorage.org/webt/ro/k6/tg/rok6tgyuqyvte_dswvl-0xgvbxe.png"><br>  <b>Figure 3.</b> Cr√©ation d'un pont r√©seau <br><br><img src="https://habrastorage.org/webt/kx/xu/kg/kxxukgzgym97cjezlvrczgtji8g.png"><br>  <b>Figure 4.</b> Configuration de la configuration du r√©seau vmbr1 <br><br>  La configuration est extr√™mement simple - nous avons besoin de vmbr1 pour que les instances aient acc√®s √† Internet. <br><br>  Red√©marrez maintenant notre hyperviseur et v√©rifiez si l'interface a √©t√© cr√©√©e: <br><br><img src="https://habrastorage.org/webt/cx/b9/ga/cxb9ga2zhwn0fefphugyihuj6fg.png"><br>  <b>Figure 5.</b> Interface r√©seau vmbr1 dans la sortie d'une commande ip <br><br>  Remarque: J'ai d√©j√† l'interface ens19 - c'est l'interface avec le r√©seau interne, sur la base de laquelle un cluster sera cr√©√©. <br><br>  R√©p√©tez ces √©tapes sur les deux autres hyperviseurs, puis passez √† l'√©tape suivante - pr√©paration du cluster. <br><br>  En outre, une √©tape importante consiste d√©sormais √† activer le transfert de paquets - sans lui, les instances n'auront pas acc√®s au r√©seau externe.  Ouvrez le fichier sysctl.conf et modifiez la valeur du param√®tre net.ipv4.ip_forward √† 1, apr√®s quoi nous entrons la commande suivante: <br><br><pre> <code class="bash hljs">sysctl -p</code> </pre> <br>  Dans la sortie, vous devriez voir la directive net.ipv4.ip_forward (si vous ne l'avez pas modifi√©e auparavant) <br><br>  <b>Configuration d'un cluster Proxmox</b> <br><br>  Passons maintenant directement au cluster.  Chaque n≈ìud doit se r√©soudre et r√©soudre les autres n≈ìuds du r√©seau interne, pour cela, il est n√©cessaire de modifier les valeurs dans les enregistrements d'h√¥tes comme suit (chaque n≈ìud doit avoir un enregistrement sur les autres): <br><br><pre> <code class="bash hljs">172.30.0.15 proxmox1.livelinux.info proxmox1 172.30.0.16 proxmox2.livelinux.info proxmox2 172.30.0.17 proxmox3.livelinux.info proxmox3</code> </pre><br>  Il est √©galement n√©cessaire d'ajouter les cl√©s publiques de chaque n≈ìud aux autres - cela est n√©cessaire pour cr√©er un cluster. <br><br>  Cr√©ez un cluster via le panneau Web: <br><br><img src="https://habrastorage.org/webt/vl/rm/rh/vlrmrhkpwn5dle9gcnomfueoega.png"><br>  <b>Figure 6.</b> Cr√©ation d'un cluster via l'interface Web <br><br>  Apr√®s avoir cr√©√© le cluster, nous devons obtenir des informations √† ce sujet.  Acc√©dez au m√™me onglet du cluster et cliquez sur le bouton ¬´Join Information¬ª: <br><br><img src="https://habrastorage.org/webt/gj/ur/t2/gjurt2tqr_pgtlfsxv7l3hrz398.png"><br>  <b>Image 7.</b> Informations sur le cluster cr√©√© <br><br>  Ces informations nous sont utiles lors de la jonction des deuxi√®me et troisi√®me n≈ìuds du cluster.  Nous sommes connect√©s au deuxi√®me n≈ìud et dans l'onglet Cluster, cliquez sur le bouton "Join Cluster": <br><br><img src="https://habrastorage.org/webt/fo/8u/zh/fo8uzhx-lzxfyqkapqdqsfuoalq.png"><br>  <b>Figure 8.</b> Connexion √† un cluster de n≈ìuds <br><br>  Analysons plus en d√©tail les param√®tres de connexion: <br><br><ol><li>  <b>Adresse de l'homologue:</b> adresse IP du premier serveur (√† celui auquel nous nous connectons) </li><li>  <b>Mot de passe:</b> mot de passe du premier serveur </li><li>  <b>Empreinte digitale:</b> nous tirons cette valeur des informations du cluster </li></ol><br><img src="https://habrastorage.org/webt/l4/zp/eo/l4zpeodynxiuqubl1fjc4b9iona.png"><br>  <b>Figure 9.</b> √âtat du cluster apr√®s la connexion du deuxi√®me n≈ìud <br><br>  Le deuxi√®me n≈ìud est correctement connect√©!  Cependant, cela ne se produit pas toujours.  Si vous suivez les √©tapes de mani√®re incorrecte ou si des probl√®mes de r√©seau surviennent, alors la jointure du cluster √©chouera et le cluster lui-m√™me sera "divis√©".  La meilleure solution consiste √† d√©connecter le n≈ìud du cluster, √† supprimer toutes les informations sur le cluster, puis √† red√©marrer le serveur et √† v√©rifier les √©tapes pr√©c√©dentes.  Comment d√©connecter en toute s√©curit√© un n≈ìud d'un cluster?  Tout d'abord, supprimez-le du cluster sur le premier serveur: <br><br><pre> <code class="bash hljs">pvecm del proxmox2</code> </pre> <br>  Apr√®s quoi, le n≈ìud sera d√©connect√© du cluster.  Maintenant, allez sur le n≈ìud cass√© et d√©sactivez-y les services suivants: <br><br><pre> <code class="bash hljs">systemctl stop pvestatd.service systemctl stop pvedaemon.service systemctl stop pve-cluster.service systemctl stop corosync systemctl stop pve-cluster</code> </pre><br>  Le cluster Proxmox stocke des informations sur lui-m√™me dans la base de donn√©es sqlite, il doit √©galement √™tre effac√©: <br><br><pre> <code class="bash hljs">sqlite3 /var/lib/pve-cluster/config.db delete from tree <span class="hljs-built_in"><span class="hljs-built_in">where</span></span> name = <span class="hljs-string"><span class="hljs-string">'corosync.conf'</span></span>; .quit</code> </pre><br>  Les donn√©es sur l'√©corce sont supprim√©es avec succ√®s.  Supprimez les fichiers restants, pour cela, vous devez d√©marrer le syst√®me de fichiers du cluster en mode autonome: <br><br><pre> <code class="bash hljs">pmxcfs -l rm /etc/pve/corosync.conf rm /etc/corosync/* rm /var/lib/corosync/* rm -rf /etc/pve/nodes/*</code> </pre><br>  Nous red√©marrons le serveur (ce n'est pas n√©cessaire, mais nous sommes s√ªrs: tous les services doivent √™tre finis et fonctionner correctement √† la fin. Pour ne rien manquer, nous allons red√©marrer).  Apr√®s la mise sous tension, nous obtiendrons un n≈ìud vide sans aucune information sur le cluster pr√©c√©dent et nous pouvons recommencer la connexion. <br><br><h3>  Installer et configurer ZFS </h3><br>  ZFS est un syst√®me de fichiers qui peut √™tre utilis√© avec Proxmox.  Avec lui, vous pouvez vous permettre de r√©pliquer des donn√©es vers un autre hyperviseur, migrer le conteneur machine virtuelle / LXC, acc√©der au conteneur LXC √† partir du syst√®me h√¥te, etc.  L'installation est assez simple, passons √† l'analyse.  Trois SSD sont disponibles sur mes serveurs, que nous allons combiner en une matrice RAID. <br><br>  Ajouter des r√©f√©rentiels: <br><br><pre> <code class="bash hljs">nano /etc/apt/sources.list.d/stretch-backports.list deb http://deb.debian.org/debian stretch-backports main contrib deb-src http://deb.debian.org/debian stretch-backports main contrib nano /etc/apt/preferences.d/90_zfs Package: libnvpair1linux libuutil1linux libzfs2linux libzpool2linux spl-dkms zfs-dkms zfs-test zfsutils-linux zfsutils-linux-dev zfs-zed Pin: release n=stretch-backports Pin-Priority: 990</code> </pre><br>  Mise √† jour de la liste des packages: <br><br><pre> <code class="bash hljs">apt update</code> </pre> <br>  D√©finissez les d√©pendances requises: <br><br><pre> <code class="bash hljs"> apt install --yes dpkg-dev linux-headers-$(uname -r) linux-image-amd64</code> </pre> <br>  Installez ZFS lui-m√™me: <br><br><pre> <code class="bash hljs">apt-get install zfs-dkms zfsutils-linux</code> </pre> <br>  Si √† l'avenir vous obtenez une erreur fusermount: dispositif fusible introuvable, essayez d'abord 'modprobe fuse', puis ex√©cutez la commande suivante: <br><br><pre> <code class="bash hljs">modprobe fuse</code> </pre> <br>  Passons maintenant directement √† la configuration.  Nous devons d'abord formater les SSD et les configurer via parted: <br><br><div class="spoiler">  <b class="spoiler_title">Configurer / dev / sda</b> <div class="spoiler_text"><pre> <code class="bash hljs">parted /dev/sda (parted) <span class="hljs-built_in"><span class="hljs-built_in">print</span></span> Model: ATA SAMSUNG MZ7LM480 (scsi) Disk /dev/sda: 480GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 4296MB 4295MB primary raid 2 4296MB 4833MB 537MB primary raid 3 4833MB 37,0GB 32,2GB primary raid (parted) mkpart Partition <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>? primary/extended? primary File system <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>? [ext2]? zfs Start? 33GB End? 480GB Warning: You requested a partition from 33,0GB to 480GB (sectors 64453125..937500000). The closest location we can manage is 37,0GB to 480GB (sectors 72353792..937703087). Is this still acceptable to you? Yes/No? yes</code> </pre><br></div></div><br>  Des actions similaires doivent √™tre effectu√©es pour les autres lecteurs.  Une fois tous les disques pr√©par√©s, passez √† l'√©tape suivante: <br><br>  zpool create -f -o ashift = 12 rpool / dev / sda4 / dev / sdb4 / dev / sdc4 <br><br>  Nous choisissons ashift = 12 pour des raisons de performances - c'est la recommandation de zfsonlinux lui-m√™me, vous pouvez en lire plus sur leur wiki: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/zfsonlinux/zfs/wiki/faq#performance-considerations</a> <br><br>  Appliquer certains param√®tres pour ZFS: <br><br><pre> <code class="bash hljs">zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> atime=off rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> compression=lz4 rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> dedup=off rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> snapdir=visible rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> primarycache=all rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> aclinherit=passthrough rpool zfs inherit acltype rpool zfs get -r acltype rpool zfs get all rpool | grep compressratio</code> </pre><br>  Maintenant, nous devons calculer certaines variables pour calculer zfs_arc_max, je fais cela comme suit: <br><br><pre> <code class="bash hljs">mem =`free --giga | grep Mem | awk <span class="hljs-string"><span class="hljs-string">'{print $2}'</span></span>` partofmem=$((<span class="hljs-variable"><span class="hljs-variable">$mem</span></span>/10)) <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$setzfscache</span></span> &gt; /sys/module/zfs/parameters/zfs_arc_max grep c_max /proc/spl/kstat/zfs/arcstats zfs create rpool/data cat &gt; /etc/modprobe.d/zfs.conf &lt;&lt; EOL options zfs zfs_arc_max=<span class="hljs-variable"><span class="hljs-variable">$setzfscache</span></span> EOL <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$setzfscache</span></span> &gt; /sys/module/zfs/parameters/zfs_arc_max grep c_max /proc/spl/kstat/zfs/arcstats</code> </pre> <br>  Pour le moment, le pool a √©t√© cr√©√© avec succ√®s, nous avons √©galement cr√©√© un sous-pool de donn√©es.  Vous pouvez v√©rifier l'√©tat de votre pool avec la commande zpool status.  Cette action doit √™tre effectu√©e sur tous les hyperviseurs, puis passez √† l'√©tape suivante. <br><br>  Ajoutez maintenant ZFS √† Proxmox.  Nous allons dans les param√®tres du centre de donn√©es (√† savoir lui, et non un n≈ìud s√©par√©) dans la section "Stockage", cliquez sur le bouton "Ajouter" et s√©lectionnez l'option "ZFS", apr√®s quoi nous verrons les param√®tres suivants: <br><br>  ID: Nom de la centaine.  Je lui ai donn√© le nom local-zfs <br>  Pool ZFS: Nous avons cr√©√© rpool / data, et nous l'ajoutons ici. <br>  N≈ìuds: sp√©cifiez tous les n≈ìuds disponibles <br><br>  Cette commande cr√©e un nouveau pool avec les lecteurs que nous avons s√©lectionn√©s.  Sur chaque hyperviseur, un nouveau stockage doit appara√Ætre, appel√© local-zfs, apr√®s quoi vous pouvez migrer vos machines virtuelles du stockage local vers ZFS. <br><br><h3>  R√©plication d'instances vers un hyperviseur voisin </h3><br>  Le cluster Proxmox a la capacit√© de r√©pliquer des donn√©es d'un hyperviseur √† un autre: cette option vous permet de basculer l'instance d'un serveur √† un autre.  Les donn√©es seront pertinentes au moment de la derni√®re synchronisation - leur heure peut √™tre d√©finie lors de la cr√©ation de la r√©plication (15 minutes est d√©fini comme standard).  Il existe deux fa√ßons de migrer une instance vers un autre n≈ìud Proxmox: manuelle et automatique.  Examinons d'abord l'option manuelle, et √† la fin je vous donnerai un script Python qui vous permettra de cr√©er une machine virtuelle sur un hyperviseur accessible lorsque l'un des hyperviseurs n'est pas disponible. <br><br>  Pour cr√©er la r√©plication, acc√©dez au panneau Web Proxmox et cr√©ez une machine virtuelle ou un conteneur LXC.  Dans les paragraphes pr√©c√©dents, nous avons configur√© le pont vmbr1 avec NAT, ce qui nous permettra d'acc√©der au r√©seau externe.  Je vais cr√©er un conteneur LXC avec MySQL, Nginx et PHP-FPM avec un site de test pour tester la r√©plication.  Voici une instruction √©tape par √©tape. <br><br>  Nous chargeons le mod√®le appropri√© (allez dans stockage -&gt; Contenu -&gt; Mod√®les), un exemple dans la capture d'√©cran: <br><br><img src="https://habrastorage.org/webt/sd/bd/57/sdbd579lmmzxefsigiivaftvpce.png"><br>  <b>Image 10.</b> Stockage local avec des mod√®les de VM et des images <br><br>  Cliquez sur le bouton "Mod√®les" et chargez le mod√®le de conteneur LXC dont nous avons besoin: <br><br><img src="https://habrastorage.org/webt/qx/ug/he/qxughewqdsfniccmaamka9idie0.png"><br>  <b>Image 11.</b> S√©lection et chargement d'un mod√®le <br><br>  Nous pouvons maintenant l'utiliser lors de la cr√©ation de nouveaux conteneurs LXC.  S√©lectionnez le premier hyperviseur et cliquez sur le bouton ¬´Cr√©er CT¬ª dans le coin sup√©rieur droit: nous verrons le panneau pour cr√©er une nouvelle instance.  Les √©tapes d'installation sont assez simples et je ne donnerai que le fichier de configuration de ce conteneur LXC: <br><br><pre> <code class="bash hljs">arch: amd64 cores: 3 memory: 2048 nameserver: 8.8.8.8 net0: name=eth0,bridge=vmbr1,firewall=1,gw=172.16.0.1,hwaddr=D6:60:C5:39:98:A0,ip=172.16.0.2/24,<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=veth ostype: centos rootfs: <span class="hljs-built_in"><span class="hljs-built_in">local</span></span>:100/vm-100-disk-1.raw,size=10G swap: 512 unprivileged:</code> </pre><br>  Le conteneur a √©t√© cr√©√© avec succ√®s.  Vous pouvez vous connecter aux conteneurs LXC via la commande pct enter, j'ai √©galement ajout√© la cl√© SSH de l'hyperviseur avant l'installation pour vous connecter directement via SSH (il y a quelques probl√®mes avec l'affichage du terminal dans PCT).  J'ai pr√©par√© le serveur et y ai install√© toutes les applications serveur n√©cessaires, vous pouvez maintenant proc√©der √† la cr√©ation de la r√©plication. <br><br>  Nous cliquons sur le conteneur LXC et allons dans l'onglet ¬´R√©plication¬ª, o√π nous cr√©ons le param√®tre de r√©plication √† l'aide du bouton ¬´Ajouter¬ª: <br><br><img src="https://habrastorage.org/webt/ub/ac/si/ubacsivqghyu5w9np8dlnjdqe3g.png"><br>  <b>Figure 12.</b> Cr√©ation de r√©plication dans l'interface Proxmox <br><br><img src="https://habrastorage.org/webt/ea/mb/48/eamb489i0yqndxdcknvr2f1vefi.png"><br>  <b>Image 13.</b> Fen√™tre de cr√©ation de travail de r√©plication <br><br>  J'ai cr√©√© la t√¢che de r√©pliquer le conteneur sur le deuxi√®me n≈ìud, comme vous pouvez le voir dans la capture d'√©cran suivante, la r√©plication a r√©ussi - faites attention au champ "Statut", il informe de l'√©tat de la r√©plication, il convient √©galement de pr√™ter attention au champ "Dur√©e" pour savoir combien de temps la r√©plication des donn√©es prend. <br><br><img src="https://habrastorage.org/webt/wr/hd/t7/wrhdt7uk4szufdqrvboovxwr6t0.png"><br>  <b>Image 14.</b> Liste de synchronisation des VM <br><br>  Essayez maintenant de migrer la machine vers le deuxi√®me n≈ìud en utilisant le bouton ¬´Migrer¬ª <br><br>  La migration du conteneur va commencer, le journal peut √™tre consult√© dans la liste des t√¢ches - il y aura notre migration.  Apr√®s cela, le conteneur sera d√©plac√© vers le deuxi√®me n≈ìud. <br><br>  <b>Erreur ¬´√âchec de la v√©rification de la cl√© d'h√¥te¬ª</b> <br><br>  Parfois, lors de la configuration d'un cluster, un probl√®me similaire peut survenir: il emp√™che les machines de migrer et de cr√©er une r√©plication, ce qui √©limine les avantages des solutions de cluster.  Pour corriger cette erreur, supprimez le fichier known_hosts et connectez-vous via SSH au n≈ìud en conflit: <br><br><pre> <code class="bash hljs">/usr/bin/ssh -o <span class="hljs-string"><span class="hljs-string">'HostKeyAlias=proxmox2'</span></span> root@172.30.0.16</code> </pre><br>  Acceptez Hostkey et essayez d'entrer cette commande, elle devrait vous connecter au serveur: <br><br><pre> <code class="bash hljs">/usr/bin/ssh -o <span class="hljs-string"><span class="hljs-string">'BatchMode=yes'</span></span> -o <span class="hljs-string"><span class="hljs-string">'HostKeyAlias=proxmox2'</span></span> root@172.30.0.16</code> </pre><br><h3>  Caract√©ristiques des param√®tres r√©seau sur Hetzner </h3><br>  Acc√©dez au panneau Robot et cliquez sur le bouton ¬´Commutateurs virtuels¬ª.  Sur la page suivante, vous verrez un panneau pour cr√©er et g√©rer des interfaces de commutateur virtuel: vous devez d'abord le cr√©er, puis y ¬´connecter¬ª des serveurs d√©di√©s.  Dans la recherche, ajoutez les serveurs n√©cessaires pour vous connecter - ils n'ont pas besoin d'√™tre red√©marr√©s, il suffit d'attendre 10 √† 15 minutes lorsque la connexion au commutateur virtuel sera active. <br><br>  Apr√®s avoir ajout√© les serveurs √† Virtual Switch via le panneau Web, nous nous connectons aux serveurs et ouvrons les fichiers de configuration des interfaces r√©seau, o√π nous cr√©ons une nouvelle interface r√©seau: <br><br><pre> <code class="bash hljs">auto enp4s0.4000 iface enp4s0.4000 inet static address 10.1.0.11/24 mtu 1400 vlan-raw-device enp4s0</code> </pre> <br>  Examinons de plus pr√®s ce que c'est.  √Ä la base, il s'agit d'un VLAN qui se connecte √† une seule interface physique appel√©e enp4s0 (cela peut varier pour vous), avec le num√©ro de VLAN indiqu√© - il s'agit du num√©ro de commutateur virtuel que vous avez cr√©√© dans le panneau Web du robot Hetzner.  Vous pouvez sp√©cifier n'importe quelle adresse, tant qu'elle est locale. <br><br>  Je note que vous devez configurer enp4s0 comme d'habitude, en fait, il doit contenir une adresse IP externe qui a √©t√© d√©livr√©e √† votre serveur physique.  R√©p√©tez ces √©tapes sur d'autres hyperviseurs, puis red√©marrez le service r√©seau sur eux, envoyez une requ√™te ping √† un n≈ìud voisin √† l'aide de l'adresse IP du commutateur virtuel.  Si le ping a r√©ussi, vous avez r√©ussi √† √©tablir une connexion entre les serveurs √† l'aide de Virtual Switch. <br><br>  Je joindrai √©galement le fichier de configuration sysctl.conf, il sera n√©cessaire si vous avez des probl√®mes avec le package de transfert et d'autres param√®tres r√©seau: <br><br><pre> <code class="bash hljs">net.ipv6.conf.all.disable_ipv6=0 net.ipv6.conf.default.disable_ipv6 = 0 net.ipv6.conf.all.forwarding=1 net.ipv4.conf.all.rp_filter=1 net.ipv4.tcp_syncookies=1 net.ipv4.ip_forward=1 net.ipv4.conf.all.send_redirects=0</code> </pre><br>  <b>Ajout de sous-r√©seaux IPv4 √† Hetzner</b> <br><br>  Avant de commencer le travail, vous devez commander un sous-r√©seau dans Hetzner, vous pouvez le faire via le panneau Robot. <br><br>  Cr√©ez un pont r√©seau avec l'adresse qui proviendra de ce sous-r√©seau.  Exemple de configuration: <br><br><pre> <code class="bash hljs">auto vmbr2 iface vmbr2 inet static address ip-address netmask 29 bridge-ports none bridge-stp off bridge-fd 0</code> </pre> <br>  Acc√©dez maintenant aux param√®tres de la machine virtuelle dans Proxmox et cr√©ez une nouvelle interface r√©seau qui sera attach√©e au pont vmbr2.  J'utilise le conteneur LXC, sa configuration peut √™tre modifi√©e imm√©diatement dans Proxmox.  Configuration finale pour Debian: <br><br><pre> <code class="bash hljs">auto eth0 iface eth0 inet static address ip-address netmask 26 gateway bridge-address</code> </pre> <br>  Veuillez noter: j'ai sp√©cifi√© 26 masque, pas 29 - cela est n√©cessaire pour que le r√©seau fonctionne sur la machine virtuelle. <br><br>  <b>Ajout d'adresses IPv4 √† Hetzner</b> <br><br>  La situation avec une seule adresse IP est diff√©rente - g√©n√©ralement Hetzner nous donne une adresse suppl√©mentaire √† partir du sous-r√©seau du serveur.  Cela signifie qu'au lieu de vmbr2, nous devons utiliser vmbr0, mais pour le moment nous ne l'avons pas.  L'essentiel est que vmbr0 doit contenir l'adresse IP du serveur Iron (c'est-√†-dire, utiliser l'adresse qui a utilis√© l'interface r√©seau physique enp2s0).  L'adresse doit √™tre d√©plac√©e vers vmbr0, la configuration suivante convient √† cela (je vous conseille de commander KVM, auquel cas pour reprendre le fonctionnement du r√©seau): <br><br><pre> <code class="bash hljs">auto enp2s0 iface enp2s0 inet manual auto vmbr0 iface vmbr0 inet static address ip-address netmask 255.255.255.192 gateway ip-gateway bridge-ports enp2s0 bridge-stp off bridge-fd 0</code> </pre><br>  Red√©marrez le serveur, si possible (sinon, red√©marrez le service r√©seau), puis v√©rifiez les interfaces r√©seau via ip a: <br><br><pre> <code class="bash hljs">2: enp2s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master vmbr0 state UP group default qlen 1000 link/ether 44:8a:5b:2c:30:c2 brd ff:ff:ff:ff:ff:ff</code> </pre><br>  Comme vous pouvez le voir ici, enp2s0 est connect√© √† vmbr0 et n'a pas d'adresse IP, car il a √©t√© r√©affect√© √† vmbr0. <br><br>  Maintenant, dans les param√®tres de la machine virtuelle, ajoutez l'interface r√©seau qui sera connect√©e √† vmbr0.  Pour la passerelle, sp√©cifiez l'adresse attach√©e √† vmbr0. <br><br><h3>  √Ä la fin </h3><br>  J'esp√®re que cet article vous sera utile lorsque vous configurerez le cluster Proxmox √† Hetzner.  Si le temps le permet, je d√©velopperai l'article et ajouterai des instructions pour OVH - l√† aussi, tout n'est pas √©vident, comme cela semble √† premi√®re vue.  Le mat√©riel s'est av√©r√© assez volumineux, si vous trouvez des erreurs, alors veuillez √©crire dans les commentaires, je les corrigerai.  Merci √† tous pour votre attention. <br><br>  <i>Publi√© par Ilya Andreev, √©dit√© par Alexei Zhadan et Live Linux Team</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr457894/">https://habr.com/ru/post/fr457894/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr457876/index.html">Traduction: Norme IEEE 802.15.4z. Qu'est-ce qui nous attend dans le futur?</a></li>
<li><a href="../fr457884/index.html">Internet souverain - clarifier les ordonnances</a></li>
<li><a href="../fr457886/index.html">Authentification √† deux facteurs sur le site √† l'aide d'un jeton USB. Maintenant pour Linux</a></li>
<li><a href="../fr457888/index.html">Test de mutation: test des tests</a></li>
<li><a href="../fr457892/index.html">Professeur de roulette</a></li>
<li><a href="../fr457896/index.html">Zimbra et protection contre les surcharges du serveur</a></li>
<li><a href="../fr457900/index.html">Commission f√©d√©rale des communications des √âtats-Unis contre les m√©t√©orologues</a></li>
<li><a href="../fr457902/index.html">Mitap pour la science des donn√©es</a></li>
<li><a href="../fr457904/index.html">Radio atomique - la toute premi√®re √©mission musicale</a></li>
<li><a href="../fr457906/index.html">Les m√©decins pensent que dans un avenir proche, des dispositifs de fabrication de vaccins appara√Ætront dans les maisons et les pharmacies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>