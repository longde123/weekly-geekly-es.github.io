<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ±Ô∏è üÜî üßîüèΩ Apprentissage par renforcement ou strat√©gies √©volutives? - √Ä la fois cela et un autre ü§ΩüèΩ üë©üèº‚Äçüé® üë©‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 

 Nous d√©cidons rarement de publier ici des traductions de textes il y a deux ans, sans code et avec une orientation clairement acad√©m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apprentissage par renforcement ou strat√©gies √©volutives? - √Ä la fois cela et un autre</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/456160/">  Bonjour, Habr! <br><br>  Nous d√©cidons rarement de publier ici des traductions de textes il y a deux ans, sans code et avec une orientation clairement acad√©mique - mais aujourd'hui nous allons faire une exception.  Nous esp√©rons que le dilemme dans le titre de l'article pr√©occupe beaucoup de nos lecteurs, et que vous avez d√©j√† lu l'ouvrage original ou lirez l'ouvrage fondamental sur les strat√©gies √©volutives avec lesquelles ce post est pol√©mique.  Bienvenue au chat! <br><br><img src="https://habrastorage.org/webt/-n/u-/i6/-nu-i6enynr12ma1d7utan_7ml8.jpeg"><br><a name="habracut"></a><br>  En mars 2017, OpenAI a fait beaucoup de bruit dans la communaut√© du deep learning en publiant l'article ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a> ¬ª.  Dans ce travail, des r√©sultats impressionnants ont √©t√© d√©crits en faveur du fait que la lumi√®re n'a pas converg√© dans la formation avec renforcement (RL), et il est conseill√© d'essayer d'autres m√©thodes lors de la formation de r√©seaux neuronaux complexes.  Ensuite, une discussion a √©clat√© sur l'importance du renforcement de l'apprentissage et √† quel point il m√©rite le statut de technologie ¬´obligatoire¬ª pour apprendre √† r√©soudre des probl√®mes.  Ici, je veux parler du fait que vous ne devriez pas consid√©rer ces deux technologies comme concurrentes, dont l'une est clairement meilleure que l'autre;  au contraire, ils se compl√®tent finalement.  En effet, si vous pensez un peu √† ce qui est n√©cessaire pour cr√©er une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">IA commune</a> et de tels syst√®mes qui, tout au long de leur existence, seraient capables d'apprendre, de juger et de planifier, alors nous arriverons presque certainement √† la conclusion que telle ou telle solution combin√©e sera n√©cessaire .  Soit dit en passant, c'est la nature qui est venue √† une solution combin√©e, dot√©e de l'intelligence complexe des mammif√®res et des autres animaux sup√©rieurs au cours de l'√©volution. <br><br><h4>  Strat√©gies √©volutives </h4><br>  La th√®se principale de l'article OpenAI √©tait qu'au lieu d'utiliser l'apprentissage par renforcement combin√© √† la r√©tropropagation traditionnelle, ils ont r√©ussi √† former le r√©seau neuronal √† r√©soudre des probl√®mes complexes en utilisant la soi-disant ¬´strat√©gie √©volutive¬ª (ES).  Une telle approche ES consiste √† maintenir la distribution des valeurs de poids sur une √©chelle de r√©seau, de nombreux agents travaillant en parall√®le et utilisant des param√®tres s√©lectionn√©s dans cette distribution.  Chaque agent op√®re dans son propre environnement et √† la fin d'un nombre donn√© d'√©pisodes ou d'√©tapes d'un √©pisode, l'algorithme renvoie une r√©compense totale, exprim√©e sous forme de score de fitness.  Compte tenu de cette valeur, la distribution des param√®tres peut √™tre d√©plac√©e vers des agents plus performants, privant les moins performants.  Des millions de fois en r√©p√©tant une telle op√©ration impliquant des centaines d'agents, il est possible de d√©placer la distribution des poids dans un espace qui nous permettra de formuler une politique qualit√© pour que les agents r√©solvent leur t√¢che.  En effet, les r√©sultats pr√©sent√©s dans l'article sont impressionnants: il est d√©montr√© que si vous ex√©cutez un millier d'agents en parall√®le, alors le mouvement anthropomorphique sur deux jambes peut √™tre √©tudi√© en moins d'une demi-heure (alors que m√™me les m√©thodes RL les plus avanc√©es n√©cessitent plus d'une heure).  Pour une revue plus d√©taill√©e, je recommande de lire un excellent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> des auteurs de l'exp√©rience, ainsi que l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article scientifique</a> lui-m√™me. <br><br><img src="https://habrastorage.org/webt/0j/lp/ms/0jlpmsa3-jz8ono405nv8c79ve8.gif"><br><br>  <i>Diff√©rentes strat√©gies d'apprentissage pour la posture verticale anthropomorphe, √©tudi√©es √† l'aide de la m√©thode ES d'OpenAI.</i> <br><br><h4>  Bo√Æte noire </h4><br>  Le grand avantage de cette m√©thode est qu'elle est facile √† parall√©liser.  Alors que les m√©thodes RL, par exemple A3C, n√©cessitent l'√©change d'informations entre les workflows et le serveur de param√®tres, ES n'a besoin que d'estimations de validit√© et d'informations g√©n√©rales sur la distribution des param√®tres.  Gr√¢ce √† une telle simplicit√©, cette m√©thode contourne les m√©thodes RL modernes en termes d'√©volutivit√©.  Cependant, tout cela n'est pas vain: il faut optimiser le r√©seau sur le principe d'une bo√Æte noire.  Dans ce cas, la ¬´bo√Æte noire¬ª signifie que pendant la formation, la structure interne du r√©seau est compl√®tement ignor√©e, et seul le r√©sultat global (r√©compense pour l'√©pisode) est utilis√©, et cela d√©pend si les poids d'un r√©seau particulier seront h√©rit√©s par les g√©n√©rations futures.  Dans les situations o√π nous n'obtenons pas de r√©troaction prononc√©e de l'environnement - et dans la r√©solution de nombreuses t√¢ches traditionnelles li√©es au RL, le flux de r√©compense est tr√®s rar√©fi√© - le probl√®me passe d'une ¬´bo√Æte partiellement noire¬ª √† une ¬´bo√Æte compl√®tement noire¬ª.  Dans ce cas, il est possible d'augmenter s√©rieusement la productivit√©, donc, bien s√ªr, un tel compromis est justifi√©.  "Qui a besoin de gradients s'ils sont toujours d√©sesp√©r√©ment bruyants?"  - c'est l'opinion g√©n√©rale. <br><br>  Cependant, dans les situations o√π la r√©troaction est plus active, les probl√®mes ES commencent √† mal tourner.  L'√©quipe OpenAI d√©crit comment le r√©seau de classification simple MNIST a √©t√© form√© en utilisant ES, et cette fois la formation a √©t√© 1000 fois plus lente.  Le fait est que le signal de gradient dans la classification des images est extr√™mement instructif sur la fa√ßon d'enseigner au r√©seau une meilleure classification.  Ainsi, le probl√®me n'est pas tant associ√© √† la technique RL qu'√† des r√©compenses clairsem√©es dans des environnements qui produisent des gradients bruyants. <br><br><h4>  Solution trouv√©e par nature </h4><br>  Si vous essayez d'apprendre de l'exemple de la nature, en r√©fl√©chissant aux moyens de d√©velopper l'IA, dans certains cas, l'IA peut √™tre repr√©sent√©e comme une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">approche orient√©e vers les probl√®mes</a> .  En fin de compte, la nature fonctionne dans des limites que les informaticiens n'ont tout simplement pas.  Il existe une opinion selon laquelle une approche purement th√©orique pour r√©soudre un probl√®me particulier peut fournir des solutions plus efficaces que des alternatives empiriques.  N√©anmoins, je pense toujours qu'il conviendrait de v√©rifier comment un syst√®me dynamique fonctionnant dans des conditions de certaines restrictions (Terre) a form√© des agents (animaux, en particulier des mammif√®res), capables d'un comportement flexible et complexe.  Bien que certaines de ces limitations ne soient pas applicables dans les mondes simul√©s de la science des donn√©es, d'autres sont tout simplement tr√®s bonnes. <br><br>  Apr√®s avoir examin√© le comportement intellectuel des mammif√®res, nous voyons qu'il est form√© √† la suite de l'interaction complexe de deux processus √©troitement li√©s: <i>apprendre de l'exp√©rience des autres</i> et <i>apprendre de notre propre exp√©rience</i> .  Le premier est souvent identifi√© √† l'√©volution due √† la s√©lection naturelle, mais ici j'utilise un terme plus large pour prendre en compte l'√©pig√©n√©tique, les microbiomes et d'autres m√©canismes qui assurent l'√©change d'exp√©rience entre des organismes qui ne sont pas g√©n√©tiquement li√©s les uns aux autres.  Le deuxi√®me processus, l'apprentissage de premi√®re main, est toute l'information qu'un animal parvient √† assimiler tout au long de sa vie, et cette information est directement li√©e √† l'interaction de cet animal avec le monde ext√©rieur.  Cette cat√©gorie comprend tout, de l'apprentissage de la reconnaissance des objets √† la ma√Ætrise de la communication inh√©rente au processus √©ducatif. <br><br>  En gros, ces deux processus se produisant dans la nature peuvent √™tre compar√©s √† deux options pour optimiser les r√©seaux de neurones.  Les strat√©gies √©volutives, o√π les informations de gradient sont utilis√©es pour mettre √† jour les informations sur le corps, sont proches de l'apprentissage de l'exp√©rience de quelqu'un d'autre.  De m√™me, les m√©thodes de gradient, o√π la r√©ception d'une exp√©rience particuli√®re conduit √† l'un ou l'autre changement dans le comportement de l'agent, sont comparables √† l'apprentissage par l'exp√©rience.  Si vous pensez aux vari√©t√©s de comportements intellectuels ou aux capacit√©s que chacune de ces deux approches d√©veloppe chez les animaux, une telle comparaison est plus prononc√©e.  Dans les deux cas, les ¬´m√©thodes √©volutives¬ª contribuent √† l'√©tude des comportements r√©actifs qui permettent le d√©veloppement d'une certaine forme physique (suffisante pour rester en vie).  Apprendre √† marcher ou √† √©chapper √† la captivit√© dans de nombreux cas √©quivaut √† des comportements plus ¬´instinctifs¬ª qui sont ¬´c√¢bl√©s¬ª chez de nombreux animaux au niveau g√©n√©tique.  De plus, cet exemple confirme que les m√©thodes √©volutives sont applicables dans les cas o√π le signal-r√©compense est extr√™mement rare (comme, par exemple, le fait d'√©lever avec succ√®s un petit).  Dans un tel cas, il est impossible de corr√©ler la r√©compense avec un ensemble sp√©cifique d'actions qui pourraient avoir √©t√© commises plusieurs ann√©es avant le d√©but de ce fait.  D'un autre c√¥t√©, si nous consid√©rons le cas o√π l'ES √©choue, √† savoir la classification des images, les r√©sultats seront remarquablement comparables avec les r√©sultats de la formation animale r√©alis√©e au cours d'innombrables exp√©riences psychologiques comportementales men√©es sur plus de cent ans. <br><br><h4>  Entra√Ænement des animaux </h4><br>  Les m√©thodes utilis√©es dans l'apprentissage par renforcement sont dans de nombreux cas directement tir√©es de la litt√©rature psychologique sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le conditionnement op√©rant</a> , et le conditionnement op√©rant a √©t√© √©tudi√© sur la base de la psychologie animale.  Soit dit en passant, Richard Sutton, l'un des deux fondateurs de la formation en renforcement, d√©tient un baccalaur√©at en psychologie.  Dans le contexte du conditionnement op√©rant, les animaux apprennent √† associer r√©compense ou punition √† des sch√©mas comportementaux sp√©cifiques.  Les formateurs et les chercheurs peuvent en quelque sorte manipuler une telle association avec des r√©compenses, incitant les animaux √† faire preuve d'ing√©niosit√© ou de certains comportements.  Cependant, le conditionnement op√©rant utilis√© dans l'√©tude des animaux n'est rien de plus qu'une forme plus prononc√©e de ce conditionnement, sur la base duquel les animaux sont entra√Æn√©s tout au long de la vie.  Nous recevons constamment des signaux de renforcement positifs de l'environnement et ajustons notre comportement en cons√©quence.  En fait, de nombreux neurophysiologistes et scientifiques cognitifs pensent qu'en fait, les gens et les autres animaux agissent m√™me d'un niveau plus haut et apprennent constamment √† pr√©dire les r√©sultats de leur comportement dans des situations futures, en comptant sur les r√©compenses potentielles. <br><br>  Le r√¥le central de la pr√©vision dans l'auto-√©tude change la dynamique d√©crite ci-dessus de la mani√®re la plus significative.  Le signal qui √©tait auparavant consid√©r√© comme tr√®s rar√©fi√© (r√©compense √©pisodique) est tr√®s dense.  Th√©oriquement, la situation est approximativement la suivante: √† chaque instant, le cerveau des mammif√®res calcule les r√©sultats sur la base d'un flux complexe de stimuli et d'actions sensorielles, tandis que l'animal est simplement immerg√© dans ce flux.  Dans ce cas, le comportement final de l'animal donne un signal dense, qui doit √™tre guid√© par la correction des pr√©visions et le d√©veloppement du comportement.  Le cerveau utilise tous ces signaux afin d'optimiser les pr√©visions (et, par cons√©quent, la qualit√© des actions entreprises) √† l'avenir.  Un aper√ßu de cette approche est donn√© dans l'excellent livre ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Surfing Uncertainty</a> ¬ª du scientifique et philosophe cognitif Andy Clark.  Si nous extrapolons de tels arguments √† la formation d'agents artificiels, alors la formation par renforcement r√©v√®le un d√©faut fondamental: le signal utilis√© dans ce paradigme est d√©sesp√©r√©ment faible par rapport √† ce qu'il pourrait √™tre (ou devrait √™tre).  Dans les cas o√π il est impossible d'augmenter la saturation du signal (peut-√™tre parce qu'il est par d√©finition faible ou associ√© √† une r√©activit√© de bas niveau) - il est probablement pr√©f√©rable de pr√©f√©rer une m√©thode d'apprentissage bien parall√©lis√©e, par exemple ES. <br><br><h4>  Meilleur apprentissage des r√©seaux de neurones </h4><br>  Sur la base des principes d'une activit√© nerveuse plus √©lev√©e inh√©rente au cerveau des mammif√®res, qui est constamment engag√© dans la pr√©vision, il a √©t√© possible r√©cemment d'obtenir certains succ√®s dans la formation de renforcement, qui tient d√©sormais compte de l'importance de telles pr√©visions.  Je peux vous recommander deux travaux similaires: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprendre √† agir en pr√©disant l'avenir</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage par renforcement avec des t√¢ches auxiliaires non supervis√©es</a> </li></ul><br>  Dans ces deux articles, les auteurs compl√®tent les politiques de r√©seau de neurones par d√©faut typiques avec des r√©sultats pr√©visionnels concernant les conditions environnementales futures.  Dans le premier article, la pr√©vision est appliqu√©e √† une vari√©t√© de variables de mesure et dans le second, aux changements de l'environnement et du comportement de l'agent en tant que tel.  Dans les deux cas, le signal clairsem√© associ√© au renforcement positif devient beaucoup plus satur√© et informatif, fournissant √† la fois un apprentissage acc√©l√©r√© et l'assimilation de mod√®les comportementaux plus complexes.  De telles am√©liorations ne sont disponibles que lorsque vous travaillez avec des m√©thodes qui utilisent le signal de gradient, mais pas avec des m√©thodes qui fonctionnent sur le principe de la "bo√Æte noire", comme, par exemple, ES. <br><br>  De plus, l'apprentissage de premi√®re main et les m√©thodes de gradient sont beaucoup plus efficaces.  M√™me dans les cas o√π il √©tait possible d'√©tudier un probl√®me particulier par la m√©thode ES plut√¥t que d'utiliser une formation de renforcement, le gain a √©t√© obtenu en raison du fait que plusieurs fois plus de donn√©es √©taient impliqu√©es dans la strat√©gie ES qu'avec RL.  En r√©fl√©chissant dans ce cas aux principes de l'apprentissage chez les animaux, nous notons que le r√©sultat de la formation sur un exemple √©tranger se manifeste apr√®s plusieurs g√©n√©rations, alors que parfois un seul √©v√©nement v√©cu dans notre propre exp√©rience suffit pour que l'animal apprenne la le√ßon pour toujours.  Bien qu'une telle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation sans exemples</a> ne s'int√®gre pas encore pleinement dans les m√©thodes de gradient traditionnelles, elle est beaucoup plus intelligible que ES.  Il existe, par exemple, des approches telles que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">contr√¥le neuronal √©pisodique</a> , o√π les valeurs Q sont stock√©es pendant le processus de formation, apr√®s quoi le programme est v√©rifi√© avec elles avant d'effectuer des actions.  Il s'av√®re que la m√©thode du gradient vous permet d'apprendre √† r√©soudre les probl√®mes beaucoup plus rapidement qu'auparavant.  Dans l'article sur le contr√¥le neuronal √©pisodique, les auteurs mentionnent l'hippocampe humain, qui est capable de stocker des informations sur l'√©v√©nement m√™me apr√®s une exp√©rience une fois v√©cue et, par cons√©quent, joue un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√¥le essentiel</a> dans le processus de rappel.  De tels m√©canismes n√©cessitent un acc√®s √† l'organisation interne de l'agent, ce qui est √©galement par d√©finition impossible dans le paradigme ES. <br><br><h4>  Alors pourquoi ne pas les combiner? </h4><br>  La plupart de cet article aurait probablement pu donner l'impression que je pr√©conisais des m√©thodes RL.  Cependant, en fait, je pense qu'√† long terme, la meilleure solution serait une combinaison des deux m√©thodes, de sorte que chacune soit utilis√©e dans les situations o√π elle est la mieux adapt√©e.  De toute √©vidence, dans le cas de nombreuses politiques r√©actives ou dans des situations avec des signaux de renforcement positif tr√®s clairsem√©s, ES l'emporte, surtout si vous avez la puissance de calcul √† laquelle vous pouvez ex√©cuter une formation parall√®le en masse.  D'un autre c√¥t√©, les m√©thodes de gradient utilisant un apprentissage renforc√© ou une formation des enseignants seront utiles lorsque nous disposerons de nombreux commentaires et que la r√©solution du probl√®me doit √™tre apprise rapidement et avec moins de donn√©es. <br><br>  En ce qui concerne la nature, nous constatons que la premi√®re m√©thode, en substance, jette les bases de la seconde.  C'est pourquoi, au cours de l'√©volution, les mammif√®res ont d√©velopp√© un cerveau qui permet un apprentissage extr√™mement efficace de la mati√®re des signaux complexes provenant de l'environnement.  La question reste donc ouverte.  Peut-√™tre que les strat√©gies √©volutives nous aideront √† inventer des architectures d'apprentissage efficaces qui seront utiles pour les m√©thodes d'apprentissage √† gradient.  Apr√®s tout, la solution trouv√©e par la nature est en effet tr√®s r√©ussie. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456160/">https://habr.com/ru/post/fr456160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456150/index.html">Petite petite joie # 4: Radon - la qualit√© du code mesur√©e en chiffres</a></li>
<li><a href="../fr456152/index.html">Mod√®les de conception de niveaux pour les jeux 2D</a></li>
<li><a href="../fr456154/index.html">Fonctionnalit√©s UX de base et MVP lors de la cr√©ation d'un produit</a></li>
<li><a href="../fr456156/index.html">C'est pourquoi l'alg√®bre scolaire est n√©cessaire.</a></li>
<li><a href="../fr456158/index.html">Un peu sur les sources de combustible nucl√©aire</a></li>
<li><a href="../fr456162/index.html">Aurora, une entreprise fond√©e par des immigrants de Google, Tesla et Uber, a commenc√© √† travailler avec des entreprises automobiles</a></li>
<li><a href="../fr456164/index.html">Les ballons huards fournissent une connexion d'urgence au r√©seau et √† Internet au P√©rou apr√®s un tremblement de terre de magnitude - 8,0</a></li>
<li><a href="../fr456168/index.html">O√π √©tait ta maison il y a des millions d'ann√©es?</a></li>
<li><a href="../fr456170/index.html">Comment cr√©er une application financi√®re: 5 API pour aider le d√©veloppeur</a></li>
<li><a href="../fr456172/index.html">Partie 2: RocketChip: connexion de la RAM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>