<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòø üåæ üëÆ Motor AERODISK: Catastr√≥fico. Parte 1 ü§¥ üêì üë¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° leitores do Habr! O t√≥pico deste artigo ser√° a implementa√ß√£o da toler√¢ncia a desastres nos sistemas de armazenamento do AERODISK Engine. Inicialme...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Motor AERODISK: Catastr√≥fico. Parte 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/456348/"><p><img src="https://habrastorage.org/webt/2b/54/ub/2b54ub9jta3knw6ff5eseo1buyu.jpeg"></p><br><p> Ol√° leitores do Habr!  O t√≥pico deste artigo ser√° a implementa√ß√£o da toler√¢ncia a desastres nos sistemas de armazenamento do AERODISK Engine.  Inicialmente, quer√≠amos escrever em um artigo sobre os dois meios: replica√ß√£o e o cluster metro, mas, infelizmente, o artigo acabou sendo muito grande, ent√£o dividimos o artigo em duas partes.  Vamos do simples ao complexo.  Neste artigo, iremos configurar e testar a replica√ß√£o s√≠ncrona - descartar um data center e tamb√©m cortar o canal de comunica√ß√£o entre os data centers e ver o que acontece. </p><a name="habracut"></a><br><p>  Nossos clientes geralmente nos fazem perguntas diferentes sobre replica√ß√£o; portanto, antes de prosseguir com a configura√ß√£o e o teste da implementa√ß√£o de r√©plicas, falaremos um pouco sobre o que √© replica√ß√£o nos sistemas de armazenamento. </p><br><h2 id="nemnogo-teorii">  Pouco de teoria </h2><br><p>  A replica√ß√£o no armazenamento √© um processo cont√≠nuo de garantir a identidade dos dados em v√°rios sistemas de armazenamento simultaneamente.  Tecnicamente, a replica√ß√£o √© realizada por dois m√©todos. </p><br><p>  <strong>Replica√ß√£o s√≠ncrona</strong> √© a c√≥pia de dados do sistema de armazenamento principal para o de backup, seguida pela confirma√ß√£o obrigat√≥ria de ambos os sistemas de armazenamento de que os dados s√£o registrados e confirmados.  Ap√≥s a confirma√ß√£o de ambos os lados (nos dois sistemas de armazenamento), os dados s√£o considerados registrados e voc√™ pode trabalhar com eles.  Isso garante uma identidade de dados garantida em todos os sistemas de armazenamento participantes da r√©plica. </p><br><p>  As vantagens deste m√©todo: </p><br><ul><li>  Os dados s√£o sempre id√™nticos em todos os sistemas de armazenamento. </li></ul><br><p>  Contras: </p><br><ul><li>  Alto custo da solu√ß√£o (canais de comunica√ß√£o r√°pidos, fibra cara, transceptores de ondas longas, etc.) </li><li>  Restri√ß√µes de dist√¢ncia (dentro de algumas dezenas de quil√¥metros) </li><li>  N√£o h√° prote√ß√£o contra corrup√ß√£o de dados l√≥gicos (se os dados estiverem corrompidos (intencionalmente ou acidentalmente) no sistema de armazenamento principal, eles ser√£o automaticamente e imediatamente corrompidos no armazenamento de backup, pois os dados s√£o sempre id√™nticos (isso √© um paradoxo) </li></ul><br><p>  <strong>A replica√ß√£o ass√≠ncrona</strong> tamb√©m est√° copiando dados do armazenamento principal para o backup, mas com um certo atraso e sem a necessidade de confirmar o registro do outro lado.  Voc√™ pode trabalhar com os dados imediatamente ap√≥s gravar no armazenamento principal e, no armazenamento de backup, os dados estar√£o dispon√≠veis ap√≥s algum tempo.  A identidade dos dados nesse caso, √© claro, n√£o √© fornecida.  Os dados no armazenamento de backup est√£o sempre um pouco "no passado". </p><br><p>  Vantagens da replica√ß√£o ass√≠ncrona: </p><br><ul><li>  Baixo custo da solu√ß√£o (qualquer canal de comunica√ß√£o, √≥ptica opcional) </li><li>  Sem limite de dist√¢ncia </li><li>  Os dados no armazenamento de backup n√£o ser√£o corrompidos se estiverem corrompidos no principal (pelo menos por algum tempo); se os dados estiverem corrompidos, voc√™ sempre poder√° interromper a r√©plica para impedir a corrup√ß√£o de dados no armazenamento de backup. </li></ul><br><p>  Contras: </p><br><ul><li>  Os dados em diferentes data centers nem sempre s√£o id√™nticos </li></ul><br><p>  Portanto, a escolha do modo de replica√ß√£o depende das tarefas do neg√≥cio.  Se for cr√≠tico para voc√™ que o data center de backup tenha exatamente os mesmos dados que os dados principais (ou seja, requisito de neg√≥cios para RPO = 0), voc√™ ter√° que se esfor√ßar e suportar as limita√ß√µes da r√©plica s√≠ncrona.  E se o atraso no estado dos dados for permitido ou simplesmente n√£o houver dinheiro, ent√£o, definitivamente, voc√™ dever√° usar o m√©todo ass√≠ncrono. </p><br><p>  Tamb√©m distinguimos separadamente esse regime (mais precisamente, j√° uma topologia) como um cluster metropolitano.  O modo Metrocluster usa replica√ß√£o s√≠ncrona, mas, diferentemente de uma r√©plica regular, o metrocluster permite que ambos os sistemas de armazenamento funcionem no modo ativo.  I.e.  voc√™ n√£o possui uma separa√ß√£o de data centers em espera ativa.  Os aplicativos funcionam simultaneamente com dois sistemas de armazenamento localizados fisicamente em diferentes datacenters.  Os tempos de inatividade de acidentes em tal topologia s√£o muito pequenos (RTO, geralmente minutos).  Neste artigo, n√£o consideraremos nossa implementa√ß√£o do cluster metropolitano, pois esse √© um t√≥pico muito amplo e amplo; portanto, dedicaremos um artigo separado a seguir na continua√ß√£o deste. </p><br><p>  Muitas vezes, quando falamos de replica√ß√£o usando sistemas de armazenamento, muitos t√™m uma pergunta razo√°vel:&gt; "Muitos aplicativos t√™m suas pr√≥prias ferramentas de replica√ß√£o, por que usar a replica√ß√£o em sistemas de armazenamento?  Est√° melhor ou pior? </p><br><p>  N√£o h√° uma resposta √∫nica, ent√£o aqui est√£o os pr√≥s e contras: </p><br><p>  Argumentos PARA replica√ß√£o de armazenamento: </p><br><ul><li>  A simplicidade da solu√ß√£o.  De uma maneira, voc√™ pode replicar uma matriz inteira de dados, independentemente do tipo de carga ou aplicativo.  Se voc√™ usar uma r√©plica de aplicativos, precisar√° configurar cada aplicativo separadamente.  Se houver mais de dois deles, ser√° extremamente demorado e caro (a replica√ß√£o de aplicativos requer, em regra, uma licen√ßa separada e n√£o gratuita para cada aplicativo. Mas mais sobre isso abaixo). </li><li>  Voc√™ pode replicar qualquer coisa - qualquer aplicativo, qualquer dado - e eles sempre ser√£o consistentes.  Muitos (a maioria) aplicativos n√£o possuem recursos de replica√ß√£o, e as r√©plicas do lado do armazenamento s√£o a √∫nica maneira de fornecer prote√ß√£o contra desastres. </li><li>  N√£o √© necess√°rio pagar a mais pela funcionalidade de replica√ß√£o de aplicativos.  Como regra, custa muito, assim como licen√ßas para um sistema de armazenamento de r√©plicas.  Mas voc√™ precisa pagar pela licen√ßa de replica√ß√£o de armazenamento apenas uma vez e comprar a licen√ßa para a r√©plica do aplicativo para cada aplicativo separadamente.  Se houver muitos desses aplicativos, custar√° um centavo e o custo das licen√ßas para replica√ß√£o do armazenamento se tornar√° uma gota no balde. </li></ul><br><p>  Argumentos contra a replica√ß√£o de armazenamento: </p><br><ul><li>  A r√©plica que utiliza as ferramentas de aplicativo tem mais funcionalidade do ponto de vista dos pr√≥prios aplicativos; o aplicativo conhece melhor seus dados (o que √© √≥bvio); portanto, h√° mais op√ß√µes para trabalhar com eles. </li><li>  Os fabricantes de alguns aplicativos n√£o garantem a consist√™ncia de seus dados se a replica√ß√£o for feita por ferramentas de terceiros.  * </li></ul><br><p>  * - uma tese controversa.  Por exemplo, uma conhecida empresa de manufatura de DBMS declarou, por muito tempo, oficialmente que seu DBMS normalmente pode ser replicado apenas por seus meios, e o restante da replica√ß√£o (incluindo SHD-shnaya) "n√£o √© verdade".  Mas a vida mostrou que n√£o √© assim.  Provavelmente (mas isso n√£o √© exato), essa simplesmente n√£o √© a tentativa mais honesta de vender mais licen√ßas para os clientes. </p><br><p>  Como resultado, na maioria dos casos, a replica√ß√£o do lado do armazenamento √© melhor, porque  Essa √© uma op√ß√£o mais simples e mais barata, mas h√° casos complexos em que voc√™ precisa de funcionalidade espec√≠fica do aplicativo e precisa trabalhar com a replica√ß√£o no n√≠vel do aplicativo. </p><br><h2 id="s-teoriey-zakonchili-teper-praktika">  Com a teoria finalizada, agora pratique </h2><br><p>  Vamos configurar uma r√©plica em nosso laborat√≥rio.  No laborat√≥rio, emulamos dois data centers (de fato, dois racks adjacentes que parecem estar em edif√≠cios diferentes).  O suporte consiste em dois sistemas de armazenamento Engine N2, que s√£o interconectados por cabos √≥pticos.  Um servidor f√≠sico executando o Windows Server 2016 usando Ethernet de 10 Gb est√° conectado aos dois sistemas de armazenamento.  A posi√ß√£o √© bastante simples, mas n√£o muda a ess√™ncia. </p><br><p>  Esquematicamente, fica assim: </p><br><p><img src="https://habrastorage.org/webt/wj/u4/rc/wju4rcak9ilbms68pnffyvsb6ly.png"></p><br><p>  A replica√ß√£o l√≥gica √© organizada da seguinte maneira: </p><br><p><img src="https://habrastorage.org/webt/yf/yh/dy/yfyhdy19cbj3sc0gpzp8jx6liz0.jpeg"></p><br><p>  Agora, vejamos a funcionalidade de replica√ß√£o que temos agora. <br>  Dois modos s√£o suportados: ass√≠ncrono e s√≠ncrono.  √â l√≥gico que o modo s√≠ncrono seja limitado pela dist√¢ncia e pelo canal de comunica√ß√£o.  Em particular, o modo s√≠ncrono requer o uso de fibra como f√≠sica e Ethernet de 10 gigabits (ou superior). </p><br><p>  A dist√¢ncia suportada para replica√ß√£o s√≠ncrona √© de 40 quil√¥metros; o atraso do canal √≥ptico entre os data centers √© de at√© 2 milissegundos.  Em geral, ele funcionar√° com grandes atrasos, mas haver√° fortes freios durante a grava√ß√£o (o que tamb√©m √© l√≥gico); portanto, se voc√™ estiver pensando em replica√ß√£o s√≠ncrona entre data centers, verifique a qualidade da √≥tica e dos atrasos. </p><br><p>  Os requisitos de replica√ß√£o ass√≠ncrona n√£o s√£o t√£o s√©rios.  Mais precisamente, eles n√£o s√£o de todo.  Qualquer conex√£o Ethernet funcionando √© adequada. </p><br><p>  No momento, o armazenamento do AERODISK ENGINE suporta replica√ß√£o para dispositivos de bloco (LUNs) usando o protocolo Ethernet (cobre ou √≥ptica).  Para projetos que exigem necessariamente replica√ß√£o por meio da f√°brica SAN Fibre Channel, agora estamos concluindo a solu√ß√£o apropriada, mas at√© agora ela n√£o est√° pronta, portanto, no nosso caso, apenas Ethernet. </p><br><p>  A replica√ß√£o pode funcionar entre qualquer sistema de armazenamento da s√©rie ENGINE (N1, N2, N4), dos sistemas inferiores aos antigos e vice-versa. </p><br><p>  A funcionalidade dos dois modos de replica√ß√£o √© completamente id√™ntica.  Abaixo est√° mais sobre o que √©: </p><br><ul><li>  Replica√ß√£o "um para um" ou "um para um", ou seja, a vers√£o cl√°ssica com dois data centers, o principal e o de backup </li><li>  A replica√ß√£o √© "um para muitos" ou "um para muitos", ou seja,  um LUN pode ser replicado para v√°rios sistemas de armazenamento de uma s√≥ vez </li><li>  Ativa√ß√£o, desativa√ß√£o e "revers√£o" da replica√ß√£o, respectivamente, para ativar, desativar ou alterar a dire√ß√£o da replica√ß√£o </li><li>  A replica√ß√£o est√° dispon√≠vel para os conjuntos RDG (Raid Distributed Group) e DDP (Dynamic Disk Pool).  No entanto, o LUN do pool RDG s√≥ pode ser replicado para outro RDG.  C DDP √© semelhante. </li></ul><br><p>  Existem muitos outros recursos pequenos, mas list√°-los n√£o faz muito sentido; n√≥s os mencionaremos durante a instala√ß√£o. </p><br><h2 id="nastroyka-replikacii">  Configura√ß√£o de replica√ß√£o </h2><br><p>  O processo de instala√ß√£o √© bastante simples e consiste em tr√™s est√°gios. </p><br><ol><li>  Configura√ß√£o de rede </li><li>  Configura√ß√£o de armazenamento </li><li>  Configurando Regras (Links) e Mapeamento </li></ol><br><p>  Um ponto importante na configura√ß√£o da replica√ß√£o √© que os dois primeiros est√°gios devem ser repetidos em um sistema de armazenamento remoto, o terceiro est√°gio - apenas no principal. </p><br><h3 id="nastroyka-setevyh-resursov">  Configura√ß√£o de recursos de rede </h3><br><p>  A primeira etapa √© configurar as portas de rede atrav√©s das quais o tr√°fego de replica√ß√£o ser√° transmitido.  Para fazer isso, voc√™ precisa habilitar as portas e definir endere√ßos IP nelas na se√ß√£o Adaptadores front-end. </p><br><p>  Depois disso, precisamos criar um pool (no nosso caso, RDG) e um IP virtual para replica√ß√£o (VIP).  VIP √© um endere√ßo IP flutuante vinculado a dois endere√ßos "f√≠sicos" dos controladores de armazenamento (as portas que acabamos de configurar).  Ser√° a interface de replica√ß√£o principal.  Voc√™ tamb√©m pode operar n√£o com VIP, mas com VLAN, se precisar trabalhar com tr√°fego marcado. </p><br><p><img src="https://habrastorage.org/webt/di/ye/5f/diye5fvbzya3cvtebg7memcs5jo.jpeg"></p><br><p>  O processo de cria√ß√£o de um VIP para uma r√©plica n√£o √© muito diferente de criar um VIP para E / S (NFS, SMB, iSCSI).  Nesse caso, criamos um VIP (sem VLAN), mas devemos indicar que √© para replica√ß√£o (sem esse ponteiro, n√£o poderemos adicionar VIP √† regra na pr√≥xima etapa). </p><br><p><img src="https://habrastorage.org/webt/nd/i9/2d/ndi92dbmjvxuqjidju802r-vl7s.png"></p><br><p>  O VIP deve estar na mesma sub-rede que as portas IP entre as quais "flutua". </p><br><p><img src="https://habrastorage.org/webt/vm/no/9m/vmno9ms_uas_guk28o1etun7kg4.png"></p><br><p>  Repetimos essas configura√ß√µes no sistema de armazenamento remoto, com outro IP-shnik, por si s√≥. <br>  VIPs de diferentes sistemas de armazenamento podem estar em sub-redes diferentes, o principal √© que deve haver roteamento entre eles.  No nosso caso, este exemplo √© apenas mostrado (192.168.3.XX e 192.168.2.XX) </p><br><p><img src="https://habrastorage.org/webt/w5/r6/re/w5r6rexidxqry4rnfdrvgp5gzcq.jpeg"></p><br><p>  Com isso, a prepara√ß√£o da parte da rede √© conclu√≠da. </p><br><h3 id="nastraivaem-hranilischa">  Configurar armazenamento </h3><br><p>  A configura√ß√£o do armazenamento para uma r√©plica difere da usual apenas no mapeamento por meio do menu especial "Mapeamento de replica√ß√£o".  Caso contr√°rio, tudo ser√° o mesmo da configura√ß√£o usual.  Agora em ordem. </p><br><p>  No pool R02 criado anteriormente, voc√™ precisa criar um LUN.  Crie, chame-o de LUN1. </p><br><p><img src="https://habrastorage.org/webt/tg/l8/vk/tgl8vkdsqf-4oljacfssnh2_zus.jpeg"></p><br><p>  Tamb√©m precisamos criar o mesmo LUN em um sistema de armazenamento remoto de volume id√™ntico.  N√≥s criamos.  Para evitar confus√£o, o LUN remoto ser√° chamado LUN1R </p><br><p><img src="https://habrastorage.org/webt/xm/kl/v4/xmklv4deigknjz1vadluit9pdds.jpeg"></p><br><p>  Se precis√°ssemos usar um LUN que j√° existe, no momento da configura√ß√£o da r√©plica, esse LUN produtivo precisaria ser desmontado do host e, no sistema de armazenamento remoto, basta criar um LUN vazio de tamanho id√™ntico. </p><br><p>  A configura√ß√£o de armazenamento est√° conclu√≠da, prosseguimos para a cria√ß√£o da regra de replica√ß√£o. </p><br><h3 id="nastroyka-pravil-replikacii-ili-replikacionnyh-svyazey">  Configurar regras de replica√ß√£o ou links de replica√ß√£o </h3><br><p>  Depois de criar LUNs no armazenamento, que ser√° o principal no momento, configuramos a regra de replica√ß√£o LUN1 no SHD1 no LUN1R no SHD2. </p><br><p>  A configura√ß√£o √© feita no menu Replica√ß√£o Remota. </p><br><p>  Crie uma regra.  Para fazer isso, especifique o destinat√°rio da r√©plica.  Tamb√©m especificamos o nome da conex√£o e o tipo de replica√ß√£o (s√≠ncrona ou ass√≠ncrona). </p><br><p><img src="https://habrastorage.org/webt/xk/yu/8f/xkyu8ftgcubcwu4-ul_ux3vc7lq.jpeg"></p><br><p>  No campo "sistemas remotos", adicione nosso SHD2.  Para adicionar, voc√™ precisa usar o gerenciamento de armazenamento IP (MGR) e o nome do LUN remoto para o qual iremos replicar (no nosso caso, LUN1R).  O gerenciamento de IPs √© necess√°rio apenas no est√°gio de adi√ß√£o da comunica√ß√£o; o tr√°fego de replica√ß√£o por meio deles n√£o ser√° transmitido; para isso, o VIP configurado anteriormente ser√° usado. </p><br><p>  J√° nesta fase, podemos adicionar mais de um sistema remoto para a topologia ‚Äúum para muitos‚Äù: clique no bot√£o ‚Äúadicionar n√≥‚Äù, como na figura abaixo. </p><br><p><img src="https://habrastorage.org/webt/rv/xb/bh/rvxbbh4umovgds3gduoaxg3tfc8.jpeg"></p><br><p>  No nosso caso, o sistema remoto √© um, ent√£o estamos limitados a isso. </p><br><p>  A regra est√° pronta.  Observe que ele √© adicionado automaticamente a todos os participantes da replica√ß√£o (no nosso caso, existem dois).  Voc√™ pode criar quantas regras quiser, para qualquer n√∫mero de LUNs e em qualquer dire√ß√£o.  Por exemplo, para equilibrar a carga, podemos replicar parte dos LUNs de SHD1 para SHD2 e a outra parte, pelo contr√°rio, de SHD2 para SHD1. </p><br><p>  SHD1.  Imediatamente ap√≥s a cria√ß√£o, a sincroniza√ß√£o come√ßou. </p><br><p><img src="https://habrastorage.org/webt/y7/v8/gg/y7v8gg7bboqpit0zrow87pgvi5y.jpeg"></p><br><p>  SHD2.  Vemos a mesma regra, mas a sincroniza√ß√£o j√° terminou. </p><br><p><img src="https://habrastorage.org/webt/tb/dl/0k/tbdl0k_anxtcwmecg31bk0s7fmo.jpeg"></p><br><p>  LUN1 no SHD1 est√° no papel de Prim√°rio, ou seja, est√° ativo.  O LUN1R no SHD2 est√° no papel de Secund√°rio, ou seja, est√° em espera, em caso de falha do SHD1. <br>  Agora podemos conectar nosso LUN ao host. </p><br><p>  Faremos a conex√£o via iSCSI, embora isso possa ser feito via FC.  A configura√ß√£o do mapeamento para o iSCSI LUN em uma r√©plica praticamente n√£o √© diferente do cen√°rio usual, portanto, n√£o discutiremos isso em detalhes aqui.  Se houver, esse processo √© descrito no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instala√ß√£o r√°pida</a> . </p><br><p>  A √∫nica diferen√ßa √© que criamos o mapeamento no menu "Mapeamento de replica√ß√£o". </p><br><p><img src="https://habrastorage.org/webt/xn/uy/p9/xnuyp9dccwbpg93ahvefmhifmdq.jpeg"></p><br><p>  Configure o mapeamento, forne√ßa o LUN ao host.  O anfitri√£o viu um LUN. </p><br><p><img src="https://habrastorage.org/webt/qg/y3/vm/qgy3vmfark_2-pvl8liqtozb2iu.jpeg"></p><br><p>  Formate-o para o sistema de arquivos local. </p><br><p><img src="https://habrastorage.org/webt/zd/8l/qg/zd8lqglmv194u9-zsctatxrsuzk.jpeg"></p><br><p>  √â isso a√≠, a configura√ß√£o est√° conclu√≠da.  Em seguida ir√° testes. </p><br><h2 id="testirovanie">  Teste </h2><br><p>  Vamos testar tr√™s cen√°rios principais. </p><br><ol><li>  Fun√ß√µes de troca de equipe Secund√°rio&gt; Prim√°rio.  Uma troca de fun√ß√£o regular √© necess√°ria caso, por exemplo, precisemos principalmente de um data center para executar algumas opera√ß√µes preventivas e, durante esse per√≠odo, para que os dados estejam dispon√≠veis, transfira a carga para o data center de backup. </li><li>  Failover de fun√ß√µes Secund√°rio&gt; Prim√°rio (falha do datacenter).  Esse √© o cen√°rio principal para o qual existe replica√ß√£o, o que pode ajudar a sobreviver a uma falha completa do data center sem parar a empresa por um longo tempo. </li><li>  Canais de comunica√ß√£o quebrados entre data centers.  Verificar o comportamento correto de dois sistemas de armazenamento sob condi√ß√µes quando, por algum motivo, o canal de comunica√ß√£o entre os datacenters estiver indispon√≠vel (por exemplo, uma escavadeira cavou no lugar errado e rasgou a √≥tica escura). </li></ol><br><p>  Para come√ßar, come√ßaremos a gravar dados em nosso LUN (gravamos arquivos com dados aleat√≥rios).  Vimos imediatamente que o canal de comunica√ß√£o entre os sistemas de armazenamento est√° sendo utilizado.  Isso √© f√°cil de entender se voc√™ abrir o monitoramento de carga das portas respons√°veis ‚Äã‚Äãpela replica√ß√£o. </p><br><p><img src="https://habrastorage.org/webt/s7/99/bt/s799bttjt3v6q24uhvxoywfrwne.jpeg"></p><br><p>  Nos dois sistemas de armazenamento agora existem dados "√∫teis", podemos iniciar o teste. </p><br><p><img src="https://habrastorage.org/webt/r3/vs/dv/r3vsdvpsp9avchablad41pxrfdu.jpeg"></p><br><p>  Apenas no caso, vamos examinar as somas de hash de um dos arquivos e anot√°-las. </p><br><p><img src="https://habrastorage.org/webt/e1/zi/st/e1zistvzwlkimqbupxjtgnltc9o.jpeg"></p><br><h3 id="shtatnoe-pereklyuchenie-roley">  Troca de fun√ß√£o de equipe </h3><br><p>  A opera√ß√£o de alternar fun√ß√µes (alterando a dire√ß√£o da replica√ß√£o) pode ser feita a partir de qualquer sistema de armazenamento, mas voc√™ ainda precisa ir para ambos, pois ser√° necess√°rio desativar o mapeamento no Prim√°rio e ativ√°-lo no Secund√°rio (que se tornar√° Prim√°rio). </p><br><p>  Talvez agora surja uma pergunta razo√°vel: por que n√£o automatizar isso?  Respondemos: tudo √© simples, a replica√ß√£o √© uma ferramenta simples de toler√¢ncia a desastres baseada apenas em opera√ß√µes manuais.  Para automatizar essas opera√ß√µes, existe um modo de cluster de metr√¥, √© totalmente automatizado, mas sua configura√ß√£o √© muito mais complicada.  Escreveremos sobre a configura√ß√£o do cluster metro no pr√≥ximo artigo. </p><br><p>  Desative o mapeamento no armazenamento principal para garantir que a grava√ß√£o seja interrompida. </p><br><p><img src="https://habrastorage.org/webt/jk/j4/1l/jkj41ltsncz2hmqoclkrecqvguy.jpeg"></p><br><p>  Em um dos sistemas de armazenamento (n√£o importa, no principal ou no backup) no menu Replica√ß√£o remota, selecione nossa conex√£o REPL1 e clique em "Alterar fun√ß√£o". </p><br><p><img src="https://habrastorage.org/webt/dc/bb/8t/dcbb8tv24xxhofmg_avtodfelas.jpeg"></p><br><p>  Ap√≥s alguns segundos, o LUN1R (armazenamento de backup) se torna Prim√°rio. </p><br><p><img src="https://habrastorage.org/webt/-v/hf/l2/-vhfl2g0v20bnfnomxwupf0_9xk.jpeg"></p><br><p>  Fazemos o mapeamento de LUN1R com SHD2. </p><br><p><img src="https://habrastorage.org/webt/lh/uw/cy/lhuwcyscu0quljitysu2pkk35hg.jpeg"></p><br><p>  Depois disso, nosso drive E: se apega automaticamente ao host, s√≥ que desta vez "voou" com o LUN1R. </p><br><p>  Apenas no caso, compare os valores de hash. </p><br><p><img src="https://habrastorage.org/webt/g6/st/qh/g6stqhn-xr0yqlw84t7y4_y5sqm.png"></p><br><p>  Id√™ntico.  Teste aprovado. </p><br><h3 id="avariynoe-pereklyuchenie-otkaz-cod-a">  Failover  Falha no Data Center </h3><br><p>  No momento, o armazenamento principal ap√≥s a troca regular √© SHD2 e LUN1R, respectivamente.  Para simular um acidente, desligamos os dois controladores SHD2. <br>  O acesso a ele n√£o √© mais. </p><br><p>  Observamos o que est√° acontecendo no armazenamento 1 (backup no momento). </p><br><p><img src="https://habrastorage.org/webt/ai/oy/jt/aioyjtl8xqmmgngtidigkvoesai.jpeg"></p><br><p>  Vemos que o LUN Prim√°rio (LUN1R) n√£o est√° dispon√≠vel.  Uma mensagem de erro apareceu nos logs, no painel de informa√ß√µes e na pr√≥pria regra de replica√ß√£o.  Consequentemente, os dados do host est√£o indispon√≠veis no momento. </p><br><p>  Altere a fun√ß√£o do LUN1 para Prim√°rio. </p><br><p><img src="https://habrastorage.org/webt/ef/vr/wv/efvrwvemqzysnrtebprwvmqnxw8.jpeg"></p><br><p>  Assuntos mapeados para o host. </p><br><p><img src="https://habrastorage.org/webt/o9/es/oj/o9esojg6xcl-uv_wbbj6afkrz18.jpeg"></p><br><p>  Verifique se a unidade E aparece no host. </p><br><p><img src="https://habrastorage.org/webt/rg/kj/0s/rgkj0s-0rgoumtmnzk98bd-bgl4.jpeg"></p><br><p>  Verifique o hash. </p><br><p><img src="https://habrastorage.org/webt/hn/yb/yq/hnybyqjm7w_g1il4bowg1-xgq5y.jpeg"></p><br><p>  Est√° tudo bem.  O centro de armazenamento sofreu uma queda no data center, que estava ativo.  O tempo aproximado que gastamos na conex√£o da ‚Äúrevers√£o‚Äù da replica√ß√£o e na conex√£o do LUN do datacenter de backup foi de aproximadamente 3 minutos.  Est√° claro que no produto real tudo √© muito mais complicado e, al√©m de a√ß√µes com sistemas de armazenamento, voc√™ precisa executar muito mais opera√ß√µes na rede, em hosts e em aplicativos.  E na vida, esse per√≠odo ser√° muito mais longo. </p><br><p>  Aqui eu quero escrever que tudo, o teste foi conclu√≠do com sucesso, mas n√£o vamos nos apressar.  O armazenamento principal "mente", sabemos que quando ela "caiu", ela estava no papel de Prim√°ria.  O que acontece se ela ligar de repente?  Haver√° duas fun√ß√µes Prim√°rias, que s√£o iguais √† corrup√ß√£o de dados?  Vamos verificar agora. <br>  De repente, vamos ativar o armazenamento subjacente. </p><br><p>  Carrega por v√°rios minutos e depois retorna √† opera√ß√£o ap√≥s uma breve sincroniza√ß√£o, mas j√° no papel de Secund√°rio. </p><br><p><img src="https://habrastorage.org/webt/27/hu/q6/27huq6b6guby7o-g7_xkz7quugy.jpeg"></p><br><p>  Est√° tudo bem.  C√©rebro dividido n√£o aconteceu.  N√≥s pensamos sobre isso, e sempre ap√≥s a queda do sistema de armazenamento aumenta o papel de Secund√°rio, independentemente de qual papel ele tenha "na vida".  Agora podemos dizer com certeza que o teste de falha do data center foi bem-sucedido. </p><br><h3 id="otkaz-kanalov-svyazi-mezhdu-cod-ami">  Falha nos canais de comunica√ß√£o entre os data centers </h3><br><p>  A principal tarefa deste teste √© garantir que o sistema de armazenamento n√£o comece a surtar se perder temporariamente os canais de comunica√ß√£o entre os dois sistemas de armazenamento e reaparecer. <br>  Ent√£o  Desconectamos os fios entre os sistemas de armazenamento (imagine que uma escavadeira os tenha escavado). </p><br><p>  Na Prim√°ria, vemos que n√£o h√° conex√£o com a Secund√°ria. </p><br><p><img src="https://habrastorage.org/webt/yh/nf/ar/yhnfarhppjrnbaxotu4ds4szz5c.jpeg"></p><br><p>  No secund√°rio, vemos que n√£o h√° conex√£o com o prim√°rio. </p><br><p><img src="https://habrastorage.org/webt/f4/k9/7h/f4k97hzr11uh3cytxpsjlq2anly.jpeg"></p><br><p>  Tudo funciona bem, e continuamos a gravar dados no sistema de armazenamento principal, ou seja, eles j√° garantem que diferem do sistema de backup, ou seja, eles "sa√≠ram". </p><br><p>  Em alguns minutos, estamos consertando o canal de comunica√ß√£o.  Assim que os sistemas de armazenamento se v√™em, a sincroniza√ß√£o de dados √© ativada automaticamente.  N√£o √© necess√°rio nada do administrador. </p><br><p><img src="https://habrastorage.org/webt/wo/os/yy/woosyydo-vvbauzsd7lgu4qwfos.jpeg"></p><br><p>     . </p><br><p><img src="https://habrastorage.org/webt/up/ne/es/upneeslicidwf8manqfmlvcaohu.jpeg"></p><br><p>  ,        ,      . </p><br><h2 id="vyvody">  </h2><br><p>    ‚Äì    ,  ,   .       . </p><br><p>        ,  -    .      .   ,        . </p><br><p>                   active-active,       ,       . </p><br><p>   ,       . </p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vejo voc√™ de novo. </font></font></p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456348/">https://habr.com/ru/post/pt456348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456338/index.html">13 liners √∫nicos de JavaScript √∫teis</a></li>
<li><a href="../pt456340/index.html">Uma hist√≥ria sobre como uma equipe de freelancers grava aplicativos JavaScript de pilha completa</a></li>
<li><a href="../pt456342/index.html">Um idioma para governar tudo</a></li>
<li><a href="../pt456344/index.html">Por que ['1', '7', '11'] .Mapa (parseInt) retorna [1, NaN, 3] em Javascript?</a></li>
<li><a href="../pt456346/index.html">Roteiro interativo para alunos de desenvolvimento da Web</a></li>
<li><a href="../pt456350/index.html">Eventos digitais em Moscou, de 17 a 23 de junho</a></li>
<li><a href="../pt456352/index.html">M√≥dulo de comunica√ß√£o de objetos sem fio WISE-4000</a></li>
<li><a href="../pt456354/index.html">Como coletamos caixas de TV</a></li>
<li><a href="../pt456358/index.html">Os 13 artigos mais infames do ano passado</a></li>
<li><a href="../pt456362/index.html">Designer de n√≠vel 6: como motivamos e desenvolvemos designers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>