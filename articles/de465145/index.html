<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëâüèø ü¶Ç üßúüèº Gesichtserkennung auf Knieebene üê© üññüèæ üëÇüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Allgemeinen sieht die Gesichtserkennung und Identifizierung von Menschen anhand ihrer Ergebnisse f√ºr √Ñlteste wie Sex im Teenageralter aus - jeder s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Gesichtserkennung auf Knieebene</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465145/"> Im Allgemeinen sieht die Gesichtserkennung und Identifizierung von Menschen anhand ihrer Ergebnisse f√ºr √Ñlteste wie Sex im Teenageralter aus - jeder spricht viel √ºber ihn, aber nur wenige √ºben.  Es ist klar, dass wir nicht l√§nger √ºberrascht sind, dass Facebook / VK nach dem Herunterladen von Fotos von freundlichen Versammlungen vorschl√§gt, die auf dem Bild gefundenen Personen zu markieren. Hier wissen wir jedoch intuitiv, dass soziale Netzwerke eine gute Hilfe in Form eines Verbindungsdiagramms einer Person darstellen.  Und wenn es keinen solchen Graphen gibt?  Beginnen wir jedoch in der richtigen Reihenfolge. <br><br><img src="https://habrastorage.org/webt/m9/y6/h4/m9y6h4ogtivd9hcrktofvqcqzue.jpeg" alt="Ein verdammtes Ding versteckt sich mit einer Kiste in der T√ºr"><br><a name="habracut"></a><br>  Anf√§nglich entstand die Gesichtserkennung und Identifikation von ‚ÄûFreund / Feind‚Äú mit uns aus ausschlie√ülich h√§uslichen Bed√ºrfnissen - S√ºchtige betraten die T√ºr eines Kollegen und √ºberwachten st√§ndig das Bild von der installierten Videokamera und zerlegten sogar, wo der Nachbar und wo der Fremde keine Lust hatten nein <br><br>  Daher wurde in nur einer Woche ein Prototyp auf dem Knie montiert, der aus einer IP-Kamera, einem Einplatinenger√§t, einem Bewegungssensor und der Python-Erkennungsbibliothek face_recognition bestand.  Da sich die Python-Bibliothek auf einer Single-Plate-Hardware befand, die ziemlich leistungsf√§hig ist ... sagen wir es vorsichtig, nicht sehr schnell, haben wir beschlossen, den Verarbeitungsprozess wie folgt aufzubauen: <br><ul><li>  der Bewegungssensor bestimmt, ob sich in dem ihm anvertrauten Raum bewegt, und signalisiert seine Anwesenheit; </li><li>  Ein auf gstreamer basierender schriftlicher Dienst, der st√§ndig einen Stream von einer IP-Kamera empf√§ngt, unterbricht 5 Sekunden vor und 10 Sekunden nach der Erkennung und leitet ihn zur Analyse an die Erkennungsbibliothek weiter. </li><li>  Sie wiederum sieht sich das Video an, findet dort Gesichter, vergleicht sie mit bekannten Samples und gibt das Video, falls es unbekannt ist, an den Telegrammkanal weiter. Sp√§ter sollte es an derselben Stelle gesteuert werden, um Fehlalarme sofort auszuschalten - zum Beispiel, wenn ein Nachbar drehte sich zur Kamera auf der falschen Seite der Proben. </li></ul><br>  Der gesamte Prozess wurde von unserem geliebten Erlang zusammengeklebt, und beim Testen an Kollegen bewies er seine minimale Arbeitsf√§higkeit. <br><br>  Das zusammengebaute Modell fand jedoch keine Anwendung im wirklichen Leben - nicht wegen seiner technischen Unvollkommenheit, die zweifellos war - wie die Erfahrung zeigt, hat das Sammeln auf dem Knie unter Gew√§chshaus-B√ºrobedingungen eine sehr schlechte Tendenz, vor Ort und zum Zeitpunkt der Demonstration gegen√ºber dem Kunden zu brechen und wegen der organisatorischen lehnten die Bewohner des Eingangs die Mehrheit der Video√ºberwachung ab. <br><br>  Das Projekt ging ins Regal und stocherte regelm√§√üig mit einem Stock f√ºr Demonstrationen w√§hrend des Verkaufs und den Drang, im pers√∂nlichen Haushalt wiedergeboren zu werden. <br><br>  Alles hat sich ge√§ndert, seit wir ein spezifischeres und recht kommerzielles Projekt zum gleichen Thema hatten.  Da es nicht m√∂glich w√§re, Ecken mit einem Bewegungssensor direkt aus der Problemstellung herauszuschneiden, musste ich mich eingehender mit den Nuancen des Suchens und Erkennens von Gesichtern in drei K√∂pfen (okay, zweieinhalb, wenn Sie meine z√§hlen) direkt im Stream befassen.  Und dann geschah eine Offenbarung. <br><br>  Das Problem ist, dass die meisten Ergebnisse zu diesem Thema rein akademische Skizzen zum Thema sind: ‚ÄûIch musste einen Artikel in einer Zeitschrift zu einem modischen Thema schreiben und ein H√§kchen f√ºr die Ver√∂ffentlichung bekommen.‚Äú  Ich lenke nicht von den Verdiensten der Wissenschaftler ab - unter den Artikeln, die ich fand, gab es viele n√ºtzliche und interessante, aber leider muss ich zugeben, dass die Reproduzierbarkeit der Arbeit ihres auf Github ver√∂ffentlichten Codes zu w√ºnschen √ºbrig l√§sst oder wie ein zweifelhaftes Unterfangen mit Zeitverschwendung aussieht. <br><br>  Zahlreiche Frameworks f√ºr neuronale Netze und maschinelles Lernen waren oft schwer zu heben - die Gesichtserkennung war f√ºr sie eine separate enge Aufgabe, die f√ºr eine Vielzahl von Problemen, die sie l√∂sten, uninteressant war.  Mit anderen Worten, ein vorgefertigtes Beispiel zu nehmen und es auf der Zielhardware auszuf√ºhren, nur um zu √ºberpr√ºfen, wie es funktioniert und ob es funktioniert, hat nicht funktioniert.  Das war kein Beispiel, und die Notwendigkeit, es zu erhalten, deutete auf eine schlechte Suche bei der Zusammenstellung bestimmter Bibliotheken bestimmter Versionen f√ºr streng definierte Betriebssysteme hin.  Das hei√üt,  in Bewegung zu nehmen und zu fliegen - buchst√§blich Kr√ºmel wie die zuvor erw√§hnte face_recognition, die wir f√ºr das vorherige Handwerk verwendet haben. <br><br>  Gro√üe Unternehmen haben uns wie immer gerettet.  Sowohl Intel als auch Nvidia haben seit langem die wachsende Dynamik und kommerzielle Attraktivit√§t dieser Aufgabenklasse gesp√ºrt, aber als Anbieter von Ger√§ten verteilen sie in erster Linie ihre Frameworks zur kostenlosen L√∂sung spezifischer Anwendungsprobleme. <br><br>  Unser Projekt war eher nicht forschend, sondern experimentell, daher haben wir die L√∂sungen einzelner Anbieter nicht analysiert und verglichen, sondern lediglich das erste mit dem Ziel, einen vorgefertigten Prototyp zu sammeln und im Kampf zu testen und dabei die schnellstm√∂gliche Antwort zu erhalten.  Daher fiel die Wahl sehr schnell auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel OpenVINO</a> - eine Bibliothek f√ºr die praktische Anwendung des maschinellen Lernens in angewandten Aufgaben. <br><br>  Zu Beginn haben wir einen Stand zusammengestellt, der traditionell aus einem Nettop mit einem Intel Core i3-Prozessor und IP-Kameras chinesischer Anbieter auf dem Markt besteht.  Die Kamera war direkt mit dem Nettop verbunden und versorgte ihn mit einem RTSP-Stream mit einem nicht sehr gro√üen FPS, basierend auf der Annahme, dass die Leute immer noch nicht wie bei Wettbewerben davor laufen w√ºrden.  Die Verarbeitungsgeschwindigkeit eines Frames (Suchen und Erkennen von Gesichtern) schwankte im Bereich von zehn bis Hunderten von Millisekunden, was v√∂llig ausreichte, um den Suchmechanismus f√ºr Personen, die vorhandene Stichproben verwenden, einzubetten.  Dar√ºber hinaus hatten wir auch einen Backup-Plan - Intel verf√ºgt √ºber einen speziellen Coprozessor, um die Berechnungen von neuronalen Netzen des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neural Compute Stick 2</a> zu beschleunigen, die wir verwenden k√∂nnten, wenn wir keinen Allzweckprozessor h√§tten.  Aber - bisher ist nichts passiert. <br><br>  Nachdem wir die Montage abgeschlossen und die Funktionalit√§t der grundlegenden Beispiele √ºberpr√ºft hatten - eine Besonderheit des Intel SDK war eine schrittweise und sehr detaillierte Beispielanleitung -, begannen wir mit der Erstellung der Software. <br><br>  Die Hauptaufgabe f√ºr uns bestand darin, nach einer Person im Sichtfeld der Kamera zu suchen, sie zu identifizieren und rechtzeitig √ºber ihre Anwesenheit zu informieren.  Dementsprechend mussten wir nicht nur Gesichter erkennen und mit Mustern vergleichen (wie dies zu diesem Zeitpunkt nicht weniger Fragen verursachte als alles andere), sondern auch sekund√§re Dinge des Schnittstellenplans bereitstellen.  Wir m√ºssen n√§mlich Bilder mit den Gesichtern der notwendigen Personen von derselben Kamera f√ºr ihre sp√§tere Identifizierung erhalten.  Warum ich denke, dass es bei derselben Kamera auch ganz offensichtlich ist: Der Installationspunkt der Kamera auf dem Objekt und die Objektivoptik f√ºhren zu bestimmten Verzerrungen, die vermutlich die Qualit√§t der Erkennung beeintr√§chtigen k√∂nnen. Wir verwenden eine andere Quelle f√ºr Quelldaten als ein Tracking-Tool. <br><br>  Das hei√üt,  Zus√§tzlich zum Stream-Handler selbst ben√∂tigen wir mindestens ein Videoarchiv und einen Videodateianalysator, der alle erkannten Gesichter von der Aufzeichnung isoliert und die am besten geeigneten als Referenz speichert. <br><br>  Wie immer haben wir das bekannte Erlang und PostgreSQL als Bindeglied zwischen ffmpeg, Anwendungen auf OpenVINO und Telegram Bot API f√ºr Warnungen verwendet.  Dar√ºber hinaus ben√∂tigten wir eine Web-Benutzeroberfl√§che, um die Mindestanforderungen f√ºr die Verwaltung des Komplexes bereitzustellen, die unser Kollege frontender auf VueJS hochgeladen hatte. <br><br>  Die Logik der Arbeit war wie folgt: <br><ul><li>  Unter der Kontrolle einer Steuerebene (in Erlang) schreibt ffmpeg in f√ºnfmin√ºtigen Abschnitten einen Stream von der Kamera zum Video. Ein separater Prozess stellt sicher, dass die Aufzeichnungen in einem genau festgelegten Volumen gespeichert werden, und bereinigt die √§ltesten, wenn dieser Schwellenwert erreicht ist. </li><li>  √úber die Web-Benutzeroberfl√§che k√∂nnen Sie alle Datens√§tze anzeigen. Sie sind in chronologischer Reihenfolge angeordnet. Auf diese Weise k√∂nnen Sie das gew√ºnschte Fragment isolieren und zur Verarbeitung senden, obwohl dies nicht ohne Schwierigkeiten m√∂glich ist. </li><li>  Die Verarbeitung besteht darin, das Video zu analysieren und Frames mit erkannten Gesichtern zu extrahieren. Es handelt sich lediglich um die OpenVINO-basierte Software (ich muss sagen, hier haben wir es geschafft, den Winkel ein wenig zu verringern - die Software zum Analysieren des Streams und zum Analysieren von Dateien ist fast identisch, weshalb das meiste davon verwendet wurde eine gemeinsam genutzte Bibliothek, und die Dienstprogramme selbst unterscheiden sich nur in der Verarbeitungskette (a la modularer gstreamer).  Die Verarbeitung erfolgt auf Video, wobei die gefundenen Gesichter mithilfe eines speziell trainierten neuronalen Netzwerks isoliert werden.  Die resultierenden Fragmente des Rahmens, der Gesichter enth√§lt, fallen in ein anderes neuronales Netzwerk, das einen 256-Elemente-Vektor bildet, der tats√§chlich die Koordinaten der Referenzpunkte des Gesichts einer Person ist.  Dieser Vektor, der erkannte Rahmen und die Koordinaten des Rechtecks ‚Äã‚Äãder gefundenen Fl√§che werden in der Datenbank gespeichert. </li><li>  Nachdem die Verarbeitung abgeschlossen ist, √∂ffnet der Bediener die verschiedenen gezeichneten Rahmen, ist entsetzt √ºber ihre Anzahl und f√§hrt mit der Suche nach Zielpersonen fort.  Ausgew√§hlte Beispiele k√∂nnen einer vorhandenen Person hinzugef√ºgt oder eine neue erstellt werden.  Nach Abschluss der Verarbeitung der Aufgabe werden die Analyseergebnisse gel√∂scht, mit Ausnahme der gespeicherten Vektoren, die den Aufzeichnungen von Observablen zugeordnet sind. </li><li>  Dementsprechend k√∂nnen wir jederzeit sowohl Frames als auch Erkennungsvektoren betrachten und bearbeiten, wobei erfolglose Samples gel√∂scht werden. </li><li>  Parallel zum Erkennungszyklus im Hintergrund funktioniert der Stream-Analysedienst immer, was dasselbe tut, jedoch mit dem Stream von der Kamera.  Er w√§hlt Gesichter im beobachteten Fluss aus und vergleicht sie mit Proben aus der Datenbank, die auf der einfachen Annahme beruhen, dass die Vektoren einer Person n√§her beieinander liegen als alle anderen Vektoren.  Eine paarweise Berechnung des Abstands zwischen den Vektoren erfolgt, wenn der Schwellenwert erreicht ist, eine Aufzeichnung der Erkennung und ein Rahmen in die Datenbank gestellt werden.  Dar√ºber hinaus wird in naher Zukunft eine Person zur Stoppliste hinzugef√ºgt, wodurch mehrere Benachrichtigungen √ºber dieselbe Person vermieden werden. </li><li>  Die Steuerebene √ºberpr√ºft regelm√§√üig das Erkennungsprotokoll und benachrichtigt bei neuen Eintr√§gen mit einer Nachricht mit dem angeh√§ngten Foto und hebt das Gesicht durch den Bot f√ºr diejenigen hervor, die gem√§√ü den Einstellungen zugelassen sind. </li></ul><br><br>  Es sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/vc/jo/_h/vcjo_haxpwhnxoe1-37sozet9gs.jpeg" alt="So k√∂nnen wir in das Archiv st√∂bern"><br>  <i>Archiv anzeigen</i> <br><br><img src="https://habrastorage.org/webt/07/dq/x8/07dqx8o9vmcavh7qxj7rnduca8m.jpeg" alt="Und so - stecken Sie in die Tassen, die im Video beleuchtet sind"><br>  <i>Ergebnisse der Videoanalyse</i> <br><br><img src="https://habrastorage.org/webt/fp/ky/ct/fpkyctbrq3slopbowp5wurzoaug.jpeg" alt="Und hier sind sie, diese Jungs!"><br>  <i>Liste der beobachteten Pers√∂nlichkeiten</i> <br><br>  Die resultierende L√∂sung ist in vielerlei Hinsicht umstritten und manchmal sogar nicht optimal, sowohl hinsichtlich der Produktivit√§t als auch hinsichtlich der Verk√ºrzung der Reaktionszeit.  Ich wiederhole jedoch, wir hatten nicht das Ziel, sofort ein effektives System zu erhalten, sondern einfach diesen Weg zu gehen und die maximale Anzahl von Kegeln zu f√ºllen, enge Pfade und potenzielle nicht offensichtliche Probleme zu identifizieren. <br><br>  Das zusammengebaute System wurde eine Woche lang unter Gew√§chshausb√ºrobedingungen getestet.  W√§hrend dieser Zeit wurden die folgenden Beobachtungen notiert: <br><ul><li>  Es besteht eine eindeutige Abh√§ngigkeit der Erkennungsqualit√§t von der Qualit√§t der Originalproben.  Wenn die beobachtete Person die Beobachtungszone zu schnell passiert, hinterl√§sst sie mit hoher Wahrscheinlichkeit keine Daten f√ºr die Probenahme und wird nicht erkannt.  Ich denke jedoch, dass dies eine Frage der Feinabstimmung des Systems ist, einschlie√ülich der Beleuchtungs- und Videostream-Parameter. </li><li>  Da das System Gesichtselemente (Augen, Nase, Mund, Augenbrauen usw.) erkennt, kann es leicht get√§uscht werden, indem ein visuelles Hindernis zwischen Gesicht und Kamera (Haare, dunkle Brille, Kapuze usw.) gelegt wird - das Gesicht, h√∂chstwahrscheinlich wird es gefunden, aber der Vergleich mit den Proben wird aufgrund der starken Diskrepanz zwischen den Detektionsvektoren und den Proben nicht funktionieren; </li><li>  gew√∂hnliche Brillen wirken sich nicht zu stark aus - wir hatten Beispiele f√ºr positive Reaktionen bei Menschen mit Brille und falsch negative Reaktionen bei Menschen, die zum Testen eine Brille aufsetzten; </li><li>  Wenn sich der Bart auf den Originalproben befand und dann verschwunden war, wurde die Anzahl der Operationen reduziert (der Autor dieser Linien hat seinen Bart auf 2 mm gek√ºrzt und die Anzahl der Operationen darauf halbiert). </li><li>  Es fanden auch falsch positive Ergebnisse statt. Dies ist eine Gelegenheit f√ºr ein weiteres Eintauchen in die Mathematik des Problems und m√∂glicherweise eine L√∂sung f√ºr die Frage der partiellen Entsprechung der Vektoren und die optimale Methode zur Berechnung des Abstands zwischen ihnen.  Feldtests sollten diesbez√ºglich jedoch noch mehr Probleme aufzeigen. </li></ul><br>  Was soll kommen?  √úberpr√ºfen Sie das System im Kampf, optimieren Sie den Verarbeitungszyklus der Erkennung, vereinfachen Sie die Suche nach Ereignissen im Videoarchiv, f√ºgen Sie der Analyse weitere Daten hinzu (Alter, Geschlecht, Emotionen) und weitere 100.500 kleine und weniger wichtige Aufgaben, die noch erledigt werden m√ºssen.  Aber den ersten Schritt auf dem Weg von tausend Schritten haben wir bereits getan.  Wenn jemand seine Erfahrungen bei der L√∂sung solcher Probleme teilt oder interessante Links zu diesem Thema gibt, bin ich sehr dankbar. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465145/">https://habr.com/ru/post/de465145/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465135/index.html">DECT-Telefonie in Geb√§uden</a></li>
<li><a href="../de465137/index.html">Infrastruktur als Code: Erste Bekanntschaft</a></li>
<li><a href="../de465139/index.html">Gewusst wie: Wichtige Faktoren, die bei der Auswahl eines kostenlosen VPN f√ºr das Surfen im Internet √ºberpr√ºft werden m√ºssen</a></li>
<li><a href="../de465141/index.html">33+ Kubernetes Sicherheitstools</a></li>
<li><a href="../de465143/index.html">Wie finde ich den besten Standort f√ºr Unternehmen? Life Hack ohne Registrierung und SMS</a></li>
<li><a href="../de465149/index.html">'Hallo Welt' f√ºr dich in der Cloud</a></li>
<li><a href="../de465151/index.html">Installieren Sie Apache Cassandra unter Windows</a></li>
<li><a href="../de465153/index.html">Computer Vision sieht Emotionen, Puls, Atmung und L√ºgen - aber wie kann man darauf ein Startup aufbauen? Gespr√§ch mit Neurodata Lab</a></li>
<li><a href="../de465155/index.html">Zwei Einheiten der neurolinguistischen Programmierung</a></li>
<li><a href="../de465161/index.html">Quintett-Datenmodell und Hunderte von Gigabyte Daten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>