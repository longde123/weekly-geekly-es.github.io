<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍼 👩🏿‍🌾 🎙️ Restauration de photos à l'aide de réseaux de neurones 👨🏻‍🔧 🤲🏻 👩🏽‍🔧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut tout le monde, je travaille en tant que programmeur de recherche dans l'équipe de vision par ordinateur du groupe Mail.ru. Pour le Jour de la Vi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Restauration de photos à l'aide de réseaux de neurones</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/453872/"><img src="https://habrastorage.org/getpro/habr/post_images/333/44a/c78/33344ac788b63200841180799417f934.jpg"><br><br>  Salut tout le monde, je travaille en tant que programmeur de recherche dans l'équipe de vision par ordinateur du groupe Mail.ru.  Pour le Jour de la Victoire de cette année, nous avons décidé de réaliser un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">projet de restauration de photographies militaires</a> .  Qu'est-ce que la restauration photo?  Il se compose de trois étapes: <br><br><ul><li>  on retrouve tous les défauts d'image: cassures, éraflures, trous; <br></li><li>  peindre les défauts trouvés en fonction des valeurs de pixels qui les entourent; <br></li><li>  colorise l'image. <br></li></ul><br>  Dans cet article, je passerai en revue chacune des étapes de la restauration en détail et vous dirai comment et où nous avons pris les données, quels réseaux nous avons appris, ce que nous avons fait, quels râteaux nous avons empruntés. <br><a name="habracut"></a><br><h1>  Recherche de défauts </h1><br>  Nous voulons trouver tous les pixels liés aux défauts dans la photo téléchargée.  Tout d'abord, nous devons comprendre quel type de photographies des années de guerre les gens vont télécharger.  Nous nous sommes tournés vers les organisateurs du projet Immortal Regiment, qui ont partagé des données avec nous.  Après les avoir analysés, nous avons remarqué que les gens téléchargent souvent des portraits, seuls ou en groupe, qui présentent un nombre modéré ou important de défauts. <br><br>  Il a ensuite fallu prélever un échantillon de formation.  L'échantillon d'apprentissage pour la tâche de segmentation est une image et un masque sur lesquels tous les défauts sont marqués.  Le moyen le plus simple est de donner des photos aux marqueurs aux marqueurs.  Bien sûr, les gens sont capables de trouver des défauts, mais le problème est que le balisage est un processus très long. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d5d/a19/892/d5da1989220f10bcaac3944d27d64276.png"><br><br>  Cela peut prendre entre une heure et une journée entière de travail pour marquer des pixels liés à des défauts sur une photo, il est donc difficile de collecter un échantillon de plus de 100 photos en quelques semaines.  Par conséquent, nous avons essayé de compléter nos données en quelque sorte et avons écrit nous-mêmes les défauts: nous avons pris une photo propre, y avons appliqué des défauts artificiels et obtenu un masque nous indiquant quelles parties particulières de l'image étaient endommagées.  La partie principale de notre échantillon de formation était de 79 photos marquées manuellement, dont 11 ont été transférées à l'échantillon de test. <br><br>  L'approche la plus populaire pour le problème de segmentation: prenez Unet avec un encodeur pré-formé et minimisez la quantité <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">B</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">e</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.854ex" height="2.057ex" viewBox="0 -780.1 1659.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-63" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-65" x="1193" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-1"> Bce </script>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">entropie croisée binaire</a> ) et <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-5"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-7">I</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">C</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9">E</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.638ex" height="2.057ex" viewBox="0 -780.1 2858 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-44" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-49" x="828" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-43" x="1333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-45" x="2093" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> DICE </script>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Sørensen - Coefficient de dés</a> ). <br><br>  Quels problèmes se posent avec cette approche dans le problème de la segmentation des défauts? <br><br><ul><li>  Même s'il nous semble qu'il y a beaucoup de défauts sur la photo, qu'elle est très sale et très déchirée par le temps, la zone occupée par les défauts est encore beaucoup plus petite que la partie intacte de l'image.  Pour résoudre ce problème, vous pouvez augmenter le poids de la classe positive dans <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-10"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-11">B</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13">e</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.854ex" height="2.057ex" viewBox="0 -780.1 1659.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-63" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-65" x="1193" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-3"> Bce </script>  , et le poids optimal sera le rapport du nombre de pixels purs au nombre de pixels appartenant aux défauts. <br></li><li>  Le deuxième problème est que si nous utilisons Unet avec un encodeur pré-formé, par exemple Albunet-18, alors nous perdons beaucoup d'informations de position.  La première couche d'Albunet-18 consiste en une convolution avec un noyau de 5 et une foulée égale à deux.  Cela permet au réseau de fonctionner rapidement.  Nous avons sacrifié le temps réseau pour une meilleure localisation des défauts: nous avons supprimé le pool max après la première couche, réduit la foulée à 1 et réduit le noyau de convolution à 3. <br></li><li>  Si nous travaillons avec de petites images, par exemple, en compressant une image en 256 x 256 ou 512 x 512, les petits défauts disparaîtront simplement en raison de l'interpolation.  Par conséquent, vous devez travailler avec une grande image.  Maintenant en production, nous segmentons les défauts dans les photographies de 1024 x 1024. Par conséquent, il était nécessaire de former le réseau neuronal en grande récolte d'images de grande taille.  Et à cause de cela, il y a des problèmes avec la petite taille du lot sur une carte vidéo. <br></li><li>  Pendant la formation, nous avons environ 20 photos placées sur une carte.  Pour cette raison, l'estimation de la moyenne et de la variance dans les couches BatchNorm est inexacte.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BatchNorm sur place</a> nous aide à résoudre ce problème, qui, d'une part, économise de la mémoire, et d'autre part, il a une version de BatchNorm synchronisé, qui synchronise les statistiques entre toutes les cartes.  Maintenant, nous considérons la moyenne et la variance non pas par 20 images sur une carte, mais par 80 images de 4 cartes.  Cela améliore la convergence du réseau. <br></li></ul><br>  En fin de compte, l'augmentation du poids <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-14"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15">B</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17">e</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.854ex" height="2.057ex" viewBox="0 -780.1 1659.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-63" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-65" x="1193" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-4"> Bce </script>  En modifiant l'architecture et en utilisant BatchNorm sur place, nous avons commencé à rechercher des défauts sur la photo.  Mais à moindre coût, vous pourriez faire encore un peu mieux en ajoutant une augmentation de la durée du test.  Nous pouvons exécuter le réseau une fois dans l'image d'entrée, puis le mettre en miroir et exécuter à nouveau le réseau, cela peut nous aider à trouver de petits défauts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c75/95b/702/c7595b702dcb921dac612b0218ce13e4.png"><br><br>  En conséquence, notre réseau a convergé sur quatre GeForce 1080Ti en 18 heures.  L'inférence prend 290 ms.  Cela s'avère assez long, mais c'est le prix pour le fait que nous recherchons bien de petits défauts.  Validation <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-18"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20">I</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-21">C</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22">E</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.638ex" height="2.057ex" viewBox="0 -780.1 2858 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-44" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-49" x="828" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-43" x="1333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-45" x="2093" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-5"> DICE </script>  égal à 0,35, et <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-23"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24">R</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25">O</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26">C</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27">A</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28">U</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29">C</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.596ex" height="2.057ex" viewBox="0 -780.1 4562 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-52" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-4F" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-43" x="1523" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-41" x="2283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-55" x="3034" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhgoAD8aof8sFu4Rzf_zJ1nlQ-f7vg#MJMATHI-43" x="3801" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-6"> ROCAUC </script>  - 0,93. <br><br><h1>  Restauration de fragments </h1><br>  Unet nous a aidé à résoudre ce problème à nouveau.  Nous lui avons donné l'image d'origine et un masque à l'entrée, sur lesquels nous marquons les espaces propres avec des unités, et les pixels que nous voulons peindre avec des zéros.  Nous avons collecté les données comme suit: nous avons pris d'Internet un grand ensemble de données avec des images, par exemple, OpenImagesV4, et ajouté artificiellement des défauts de forme similaire à ceux trouvés dans la vie réelle.  Et après cela, ils ont formé le réseau pour réparer les pièces manquantes. <br><br>  Comment pouvons-nous modifier Unet pour cette tâche? <br><br>  Vous pouvez utiliser la convolution partielle au lieu de la convolution habituelle.  Son idée est que lorsque nous réduisons une région d'une image avec un noyau, nous ne prenons pas en compte les valeurs de pixels liées aux défauts.  Cela permet de rendre la peinture plus précise.  Un exemple d'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article NVIDIA</a> .  Dans l'image centrale, ils ont utilisé Unet avec la convolution habituelle, et à droite - avec Convolution partielle: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5a5/280/a4b/5a5280a4be71695d16ab08af876c5d23.png"><br><br>  Nous avons formé le réseau pendant 5 jours.  Le dernier jour, nous avons gelé BatchNorm, ce qui a contribué à rendre les bords de la partie peinte de l'image moins visibles. <br><br>  Le réseau traite une image de 512 x 512 en 50 ms.  Le PSNR de validation est de 26,4.  Cependant, les mesures ne peuvent pas être approuvées sans condition dans cette tâche.  Par conséquent, nous avons d'abord exécuté plusieurs bons modèles sur nos données, anonymisé les résultats, puis voté pour ceux que nous aimions le plus.  Nous avons donc choisi le modèle final. <br><br>  J'ai mentionné que nous avions artificiellement ajouté des défauts pour nettoyer les images.  Lors de la formation, vous devez surveiller attentivement la taille maximale des défauts superposés, car avec de très gros défauts que le réseau n'a jamais vus dans le processus d'apprentissage, il fantasmera énormément et donnera un résultat absolument inapplicable.  Donc, si vous devez peindre de gros défauts, appliquez également de gros défauts pendant la formation. <br><br>  Voici un exemple d'algorithme: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/abd/6cb/c8c/abd6cbc8c05396042bf83c00516b630e.png"><br><br><h1>  Coloration </h1><br>  Nous avons segmenté les défauts et les avons peints, la troisième étape est la reconstruction de la couleur.  Permettez-moi de vous rappeler que parmi les photographies de l '"Immortal Regiment" il y a beaucoup de portraits simples ou de groupe.  Et nous voulions que notre réseau fonctionne bien avec eux.  Nous avons décidé de faire notre propre colorisation, car aucun des services que nous connaissons ne peint les portraits rapidement et bien. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/015/bce/bf1/015bcebf15ef1ee069fdf08f16e00542.png"><br><br>  GitHub a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">référentiel</a> populaire pour colorier des photos.  En moyenne, il fait bien ce travail, mais il a plusieurs problèmes.  Par exemple, il aime peindre des vêtements en bleu.  Par conséquent, nous l'avons également rejeté. <br><br>  Nous avons donc décidé de créer un réseau neuronal pour la colorisation.  L'idée la plus évidente: prendre une image en noir et blanc et prévoir trois canaux, rouge, vert et bleu.  Mais, d'une manière générale, nous pouvons simplifier notre travail.  Nous pouvons travailler non pas avec la représentation RVB de la couleur, mais avec la représentation YCbCr.  Le composant Y est la luminosité (luma).  L'image en noir et blanc téléchargée est la chaîne Y, nous la réutiliserons.  Il restait à prédire Cb et Cr: Cb est la différence de couleur bleue et de luminosité, et Cr est la différence de couleur rouge et de luminosité. <br><br><img src="https://habrastorage.org/webt/k8/ke/l_/k8kel_xrjco6euolh8ypkchjm8g.jpeg"><br><br>  Pourquoi avons-nous choisi la vue YCbCr?  L'œil humain est plus sensible aux changements de luminosité qu'aux changements de couleur.  Par conséquent, nous réutilisons la composante Y (luminosité), à laquelle l'œil est initialement bien sensible, et prédisons le Cb et le Cr, dans lesquels nous pouvons faire un peu plus d'erreurs, car les gens remarquent moins de «fausses» couleurs.  Cette fonctionnalité a commencé à être activement utilisée à l'aube de la télévision couleur, lorsque la bande passante du canal n'était pas suffisante pour transmettre toutes les couleurs en totalité.  L'image a été transférée sur YCbCr, transférée sur le composant Y inchangée, et Cb et Cr ont été compressés deux fois. <br><br><h1>  Comment assembler la ligne de base </h1><br>  Vous pouvez à nouveau prendre Unet avec un encodeur pré-formé et minimiser la perte L1 entre le CbCr réel et le prédictif.  Nous voulons colorer des portraits, donc en plus des photos d'OpenImages, nous devons ajouter des photos spécifiques à notre tâche. <br><br>  Où puis-je obtenir des photographies en couleur de personnes en uniforme militaire?  Il y a des gens sur Internet qui peignent de vieilles photographies comme passe-temps ou pour commander.  Ils le font très soigneusement, essayant de se conformer pleinement à toutes les nuances.  Colorant l'uniforme, les épaulettes, les médailles, ils se tournent vers des documents d'archives, de sorte que le résultat de leur travail peut être fait confiance.  Au total, nous avons utilisé 200 photographies peintes à la main.  La deuxième source de données utiles est le site de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Armée rouge ouvrière et paysanne</a> .  L'un de ses créateurs a été photographié dans presque toutes les variantes possibles d'uniforme militaire pendant la Grande Guerre patriotique. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f72/999/7d9/f729997d9fd02755fd95967ff09d27ca.png"><br><br>  Dans certaines photographies, il a répété les poses de personnes à partir de photographies d'archives célèbres.  C’est particulièrement bien qu’il ait tiré sur un fond blanc, cela nous a permis d’augmenter très bien les données, en ajoutant divers objets naturels à l’arrière-plan.  Nous avons également utilisé des portraits modernes ordinaires de personnes, en les complétant d'insignes et d'autres attributs des vêtements de guerre. <br><br>  Nous avons formé AlbuNet-50 - c'est Unet, dans lequel AlbuNet-50 est utilisé comme encodeur.  Le réseau a commencé à donner des résultats adéquats: la peau est rose, les yeux sont gris-bleu, les bretelles sont jaunâtres.  Mais le problème est qu'elle a peint les tableaux avec des taches.  Cela est dû au fait que du point de vue de l'erreur L1, il est parfois plus rentable de ne rien faire que d'essayer de prédire une certaine couleur. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/435/4a9/63c/4354a963c4cd2789c434002e44fdda26.png"><br>  <i>Nous comparons notre résultat avec une photo de Ground Truth - colorisation manuelle de l'artiste sous le pseudo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Klimbim</a></i> <br><br>  Comment résoudre ce problème?  Nous avons besoin d'un discriminateur: un réseau de neurones, auquel nous fournirons des images à l'entrée, et il dira à quel point cette image est réaliste.  Ci-dessous, une photographie est peinte à la main, et la seconde par un réseau de neurones.  Lequel pensez-vous? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/253/86a/abd/25386aabd9080c14daf71b60d9968453.png"><br><br><div class="spoiler">  <b class="spoiler_title">La réponse</b> <div class="spoiler_text">  La photo de gauche est peinte manuellement. <br></div></div><br>  En tant que discriminateur, nous utilisons le discriminateur de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Self-Attention GAN</a> .  Il s'agit d'un petit réseau convolutionnel, dans les dernières couches dont la soi-disant auto-attention est intégrée.  Il vous permet de "faire plus attention" aux détails de l'image.  Nous utilisons également la normalisation spectrale.  L'explication et la motivation exactes se trouvent dans l'article.  Nous avons formé un réseau avec une combinaison de perte L1 et de l'erreur renvoyée par le discriminateur.  Maintenant, le réseau peint mieux les détails de l'image et l'arrière-plan est plus cohérent.  Un autre exemple: à gauche est le résultat du réseau formé uniquement avec une perte L1, à droite - avec une perte L1 et une erreur de discrimination. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa4/724/ada/aa4724ada4e0bbdc52dbcaadd5bb3fdc.png"><br><br>  Sur quatre Geforce 1080Ti, la formation a duré deux jours.  Le réseau a fonctionné en 30 ms dans l'image 512 x 512. Le MSE de validation était de 34,4.  Comme dans le cas du problème de peinture, les mesures ne peuvent pas être entièrement approuvées.  Par conséquent, nous avons sélectionné 6 modèles qui avaient les meilleures mesures pour la validation et avons voté aveuglément pour le meilleur modèle. <br><br>  Après avoir déployé le modèle en production, nous avons poursuivi les expériences et sommes arrivés à la conclusion qu'il était préférable de minimiser non pas la perte L1 par pixel, mais la perte perceptuelle.  Pour le calculer, vous devez exécuter la prédiction du réseau et la photo source via le réseau VGG-16, prendre les cartes d'attributs sur les couches inférieures et les comparer selon MSE.  Cette approche peint plus de zones et aide à obtenir une image plus colorée. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db9/303/509/db93035094c4906cf82c246151432cbc.jpg"><br><br><h1>  Conclusions et conclusion </h1><br>  Unet est un modèle sympa.  Dans le premier problème de segmentation, nous avons rencontré un problème de formation et de travail avec des images haute résolution, nous utilisons donc In-Place BatchNorm.  Dans la deuxième tâche (Inpainting), au lieu de la convolution habituelle, nous avons utilisé la convolution partielle, cela a aidé à obtenir de meilleurs résultats.  Dans le problème de colorisation pour Unet, nous avons ajouté un petit réseau de discriminateur qui a infligé une amende au générateur pour une image d'apparence irréaliste et utilisé une perte de perception. <br><br>  La deuxième conclusion est que les accesseurs sont importants.  Et pas seulement au stade du marquage des photos avant l'entraînement, mais aussi pour valider le résultat final, car en cas de problèmes de défauts de peinture ou de colorisation, il faut quand même valider le résultat avec l'aide d'une personne.  Nous donnons à l'utilisateur trois photos: l'original avec les défauts supprimés, colorisé avec les défauts supprimés, et juste la photo colorisée au cas où l'algorithme de recherche et de peinture des défauts serait erroné. <br><br>  Nous avons pris quelques photos du projet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Album militaire</a> et les avons traitées avec nos réseaux de neurones.  Voici les résultats obtenus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a51/ade/15e/a51ade15e9cee54ec8269dc1e22df0af.jpg"><br><br>  Et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici,</a> vous pouvez les voir dans la résolution d'origine et à chaque étape du traitement. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr453872/">https://habr.com/ru/post/fr453872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr453862/index.html">Pourquoi vous devriez utiliser pathlib</a></li>
<li><a href="../fr453864/index.html">Utiliser une souris et un clavier sur des consoles, c'est tricher?</a></li>
<li><a href="../fr453866/index.html">Demande d'API avec React Hooks, HOC ou Render Prop</a></li>
<li><a href="../fr453868/index.html">Mini interrupteur tactile avec panneau en verre sur nRF52832</a></li>
<li><a href="../fr453870/index.html">Nous écrivons le proxy Reverse socks5 sur PowerShell. Partie 1</a></li>
<li><a href="../fr453874/index.html">De la roulette russe au LOTO sécurisé: comment protéger le personnel du centre de données</a></li>
<li><a href="../fr453876/index.html">Comme dans Yandex.Practicum, le front desync a gagné: un numéro acrobatique avec Redux-Saga, postMessage et Jupyter</a></li>
<li><a href="../fr453882/index.html">Un excellent guide sur le métier d'architecte de solutions (+ liste de liens utiles)</a></li>
<li><a href="../fr453884/index.html">Remplacement de la caméra HYIP ou du reflex numérique?</a></li>
<li><a href="../fr453886/index.html">Le programme fonctionne</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>