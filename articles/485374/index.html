<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòµ üíß üÄÑÔ∏è La cach√© es el rey del rendimiento: los procesadores necesitan un cuarto nivel de cach√© üåÑ üêì ü§æüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La brecha entre la velocidad de los procesadores en el sentido general y la velocidad de la DRAM principal, tambi√©n en un sentido general, ha sido un ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La cach√© es el rey del rendimiento: los procesadores necesitan un cuarto nivel de cach√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485374/"><img src="https://habrastorage.org/getpro/habr/post_images/2ac/73c/219/2ac73c219095e9b2a9f772e767ba9cb7.jpg"><br><br>  La brecha entre la velocidad de los procesadores en el sentido general y la velocidad de la DRAM principal, tambi√©n en un sentido general, ha sido un problema en los √∫ltimos 30 a√±os; durante este per√≠odo, la brecha comenz√≥ a crecer realmente.  Y vale la pena decir honestamente que los ingenieros que desarrollaron tanto el equipo como los programas que crearon la jerarqu√≠a de cach√© y el software que podr√≠a aprovecharla lo hicieron de manera brillante.  Esta es una de las arquitecturas m√°s dif√≠ciles jam√°s concebidas por el hombre. <br><br>  Sin embargo, ahora que estamos al borde de una jerarqu√≠a de memoria en constante expansi√≥n, cuando comienza a aparecer memoria no vol√°til como Optane 3D XPoint (una variante de memoria con cambio de fase) en formatos DIMM y SSD, as√≠ como nuevos protocolos (CXL, OpenCAPI, CCIX, NVLink y Gen-Z), surge la pregunta: ¬øes hora de agregar un cach√© de cuarto nivel a los servidores?  Dado que el trabajo de tal cantidad de dispositivos depende del complejo de la CPU, algunos de los cuales est√°n m√°s cerca, mientras que otros est√°n m√°s lejos, es l√≥gico pensar si necesitamos otro nivel de cach√© que enmascare los retrasos de estos otros tipos de memoria y aumente el rendimiento de todo el sistema. <br><a name="habracut"></a><br>  Para presentar las oportunidades, hurgamos en nuestra propia memoria y, al mismo tiempo, hablamos con los desarrolladores de arquitectura de chips de IBM, Intel, AMD y Marvell para comprender lo que piensan sobre el uso de la cach√© L4 en los servidores.  El cach√© L4, por supuesto, no es una palabra nueva en velocidad, pero no es tan com√∫n en las arquitecturas de sistemas. <br><br>  Sin embargo, antes deber√≠amos repasar la historia del problema. <br><br>  Agregar un cach√© de primer nivel a los procesadores que ten√≠an solo un n√∫cleo en ese momento fue un compromiso en la d√©cada de 1980 que agreg√≥ latencia a los subsistemas de memoria al tiempo que reduc√≠a la latencia promedio de las solicitudes de datos y las instrucciones de los procesadores.  Las memorias cach√© L1 se ubicaron originalmente en la SRAM externa ubicada en las placas base y conectadas al complejo CPU-memoria.  Tal cach√© L1 estaba muy cerca del procesador, tanto en t√©rminos de frecuencia de reloj como en t√©rminos de espacio f√≠sico en la placa, e hizo posible aumentar la carga de la CPU.  Luego, estos cach√©s se dividieron para que los datos de uso frecuente pudieran almacenarse en un bloque y las instrucciones populares en el segundo, y esto aument√≥ ligeramente el rendimiento.  En alg√∫n momento del aumento en la velocidad del reloj del procesador y la brecha correspondiente en la velocidad de la CPU y la DRAM, se agregaron cach√©s L2 m√°s gordos, pero tambi√©n m√°s lentos (pero m√°s baratos en t√©rminos de ancho de banda), nuevamente al principio estaban fuera del gabinete de la CPU, pero luego integrado en √©l.  Y cuando se comenzaron a agregar m√°s y m√°s n√∫cleos a la CPU, as√≠ como m√°s y m√°s controladores DRAM para cargarlos, se agregaron bloques de cach√© L3 a√∫n m√°s grandes a la jerarqu√≠a. <br><br>  En su mayor parte, dicho sistema funcion√≥ bastante bien.  En algunos circuitos de CPU, incluso vemos ciertas reglas pr√°cticas que reflejan los niveles de la jerarqu√≠a de cach√©, lo que nos permitir√° estimar las posibilidades asociadas con el cuarto nivel. <br><br>  Chris Gianos, un ingeniero de chips y arquitecto de Intel, que ha liderado el desarrollo de muchas generaciones pasadas de procesadores Xeon, explica esto: ‚ÄúCon cada nivel de cach√©, generalmente necesitamos que crezcan lo suficientemente fuertes desde el nivel anterior, para que todo tenga sentido, porque Para lograr un aumento notable en el rendimiento del sistema, debe lograr una frecuencia bastante interesante de llamadas exitosas.  Si "cae" en los datos en cach√© en solo un peque√±o porcentaje de los casos, ser√° dif√≠cil notarlo.  Todo lo dem√°s ralentiza tu velocidad, y este aumento ser√° imperceptible.  Por lo tanto, se requieren cach√©s relativamente grandes, y cuando se trata de niveles m√°s altos, se necesitan cach√©s realmente enormes.  Hoy, L2 se mide en megabytes, L3 se mide en decenas o cientos de megabytes.  Por lo tanto, est√° claro que si comienzas a pensar en el cach√© L4, hablaremos de cientos de megabytes, si no gigabytes.  Y tal tama√±o definitivamente conducir√° a su alto costo.  Es necesario que existan ciertas condiciones, para que esta opci√≥n se vuelva interesante, y ciertamente no ser√° barata ". <br><br>  Los ingenieros de AMD que entrevistamos deseaban permanecer en el anonimato porque no quer√≠an dar la impresi√≥n de que la compa√±√≠a iba a agregar el cach√© L4 a la l√≠nea de procesadores Epyc, y, para ser precisos, AMD no prometi√≥ nada de eso.  Sin embargo, la compa√±√≠a reconoce que este es el pr√≥ximo paso obvio a considerar y, al igual que Intel, cree que todos los ingenieros est√°n pensando en implementar el cach√© L4.  En esencia, AMD dice que las compensaciones asociadas con los niveles de cach√© y las latencias se han estudiado ampliamente tanto en la industria como en la academia, y que con cada nuevo nivel que es m√°s grande y m√°s lento que el anterior, existe una compensaci√≥n al aumentar el camino general hacia DRAM.  Esto tambi√©n lo indica Intel Gianos, que habla sobre la necesidad de encontrar un equilibrio entre las solicitudes exitosas de cach√© y su volumen. <br><br>  IBM, por supuesto, agreg√≥ cach√© L4 a algunos de sus conjuntos de chips X86 en la d√©cada de 2000, y en la d√©cada de 2010 agreg√≥ L4 a los conjuntos de chips NUMA ( <a href="https://ru.wikipedia.org/wiki/Non-Uniform_Memory_Access" rel="nofollow">acceso de memoria desigual</a> ) en los mainframes System z11.  El procesador z11 tiene cuatro n√∫cleos, 64 KB de cach√© L1 para instrucciones y 128 KB de cach√© L1 para datos, m√°s 1.5 MB de cach√© L2 para cada n√∫cleo y 24 MB de cach√© L3 compartida para todos los n√∫cleos.  El chipset NUMA para z10 ten√≠a dos bancos de 96 MB de cach√© L4, es decir, 192 MB en total.  Al lanzar z12, IBM redujo el tama√±o del cach√© L1 a 98 KB por n√∫cleo, pero aument√≥ el cach√© L2 a 2 MB por n√∫cleo, dividi√©ndolo en dos partes, para instrucciones y datos, como en el caso de L1.  Tambi√©n duplic√≥ el tama√±o del cach√© L3 a 48 MB para seis n√∫cleos, y el tama√±o del cach√© L4 se increment√≥ a 384 MB para un par de chips en el conjunto de chips.  A medida que cambian las generaciones de procesadores System z, los tama√±os de cach√© crecieron, y para los procesadores z15 anunciados en septiembre, un par de cach√©s L1 pesar√°n 128 KB cada uno, un par de cach√©s L2 pesar√°n 4 MB cada uno, y un cach√© L3 compartido para 256 n√∫cleos tendr√° una capacidad de 256 MB.  El cach√© L4 en cada bah√≠a de mainframe es de 960 MB, y su tama√±o total para todo el sistema, que consta de cinco bah√≠as, es de 4.68 GB. <br><br>  Como <a href="https://www.nextplatform.com/2018/08/28/ibm-power-chips-blur-the-lines-to-memory-and-accelerators/" rel="nofollow">se√±alamos anteriormente</a> , los procesadores Power8 y Power9 tienen memoria intermedia, e IBM agreg√≥ 16 MB de cach√© L4 a cada b√∫fer Centaur, que es 128 MB de cach√© L4 por socket para 32 ranuras de memoria.  Las m√°quinas Power9 m√°s baratas no tienen un b√∫fer de memoria y, por lo tanto, no tienen cach√© L4.  Los arquitectos que desarrollaron el circuito Power10 estaban ocupados desarrollando el circuito para Power11 y, por lo tanto, no pudieron responder nuestras preguntas, pero William Stark, quien dirigi√≥ el desarrollo de Power10, encontr√≥ un poco de tiempo para nosotros y not√≥ lo siguiente: <br><br>  "En general, llegamos a la conclusi√≥n de que los cach√©s de gran nivel del √∫ltimo nivel son √∫tiles para aumentar la velocidad de los sistemas industriales", nos explic√≥ Stark por correo electr√≥nico.  "La alta latencia asociada con la memoria no vol√°til, en particular con la memoria de estado de fase, genera una solicitud de cach√©, posiblemente un cach√© de tipo L4, en la jerarqu√≠a de almacenamiento". <br><br>  Eso es exactamente lo que pensamos.  Y, por cierto, no afirmamos que la memoria cach√© L4 est√© necesariamente cerca de la memoria almacenada en el futuro DDR5 DIMM.  Quiz√°s sea mejor colocarlo entre el cach√© del procesador PCI-Express y el L3, y a√∫n mejor, en los b√∫feres de memoria y entre el cach√© del procesador PCI-Express y el L3.  Quiz√°s tenga que colocarse encima del controlador de E / S y la memoria en la futura arquitectura del servidor, que es un poco como <a href="https://www.nextplatform.com/2018/12/13/intel-bets-heavily-on-chip-stacking-for-the-future-of-compute/" rel="nofollow">la tecnolog√≠a Foveros de Intel</a> . <br><br>  Es posible ver esto desde un punto de vista diferente; por ejemplo, IBM tuvo la oportunidad de cambiar el tama√±o del cristal, y los ingenieros decidieron agregar el cach√© L4 al bus System z NUMA o al chip de memoria intermedia Power8 y Power9, no por su propio bien, sino simplemente porque a√∫n ten√≠an la oportunidad de agregar transistores despu√©s de implementar todas las funciones necesarias.  A veces nos parece que la cantidad de n√∫cleos en los procesadores Intel X86 depende del tama√±o de la memoria cach√© L3 que pueden pagar.  A veces parece que Intel asigna el tama√±o m√°ximo de la cach√© L3 a un cristal, y despu√©s de eso, los cristales Xeon de tres tama√±os diferentes simplemente se fabrican de acuerdo con estas especificaciones: en las √∫ltimas generaciones tienen 10, 18 o 28 n√∫cleos en un proceso de fabricaci√≥n de 14 nm. <br><br>  Todos estos, por supuesto, son cuestiones puramente acad√©micas, pero nos dan la motivaci√≥n potencial para que IBM y otros fabricantes de conjuntos de chips agreguen el cach√© L4.  Esto no solo puede ayudar en algunos casos, sino que es algo bastante obvio.  Creemos que en un monstruo de E / S como el mainframe System z, el cach√© L4 est√° en su lugar sin dudas y beneficia a todos los clientes, lo que aumenta el rendimiento de estas m√°quinas y les permite trabajar con una carga del procesador del 98-99%, ya que cu√°ntos n√∫cleos , y la escala de NUMA en mainframes ha crecido mucho √∫ltimamente. <br><br>  No hay ninguna raz√≥n para hacer el cach√© L4 exclusivamente en la DRAM incorporada (como lo hace IBM con sus chips) o en base a una SRAM mucho m√°s costosa; esto es lo que nos recuerda Rabin Sugumar, arquitecto de chips de Cray Research, Sun Microsystems, Oracle, Broadcom. , Cavium y Marvell: <br><br>  "Nuestros cach√©s L3 ya son lo suficientemente grandes", dice Sugumar.  - Por lo tanto, L4 en el caso de que est√© interesado debe hacerse utilizando una tecnolog√≠a diferente.  Quiz√°s eDRAM o incluso HBM o DRAM.  En este contexto, una implementaci√≥n de cach√© L4 basada en HBM parece una opci√≥n interesante, y este cach√© no resuelve tanto el problema de latencia como el ancho de banda.  Dado que la capacidad de HBM es limitada y el ancho de banda es alto, podemos obtener un cierto aumento en la velocidad, y en algunos casos especiales realmente vemos un aumento significativo en el ancho de banda ".  Sugumar agrega que para un n√∫mero bastante grande de aplicaciones, se observa un n√∫mero relativamente grande de errores de cach√©.  Sin embargo, debe calcular si valdr√° la pena agregar el siguiente nivel de cach√©. <br><br>  Otro posible caso de uso para algo como el cach√© L4, dice Sugumar, es usar DRAM local como cach√©.  ‚ÄúNo realizamos ning√∫n estudio de este tipo en el laboratorio, pero supongamos que tenemos una interfaz de gran ancho de banda en el chip, conectada a una memoria compartida en alg√∫n lugar del otro extremo del bucle, a una distancia de 500 ns a 1 Œºs.  Entonces, uno de los casos de uso ser√° crear un cach√© que mueva estos datos de la memoria compartida a la DRAM local.  Puede imaginar el trabajo de la m√°quina de estado administrando esta memoria, por lo que la mayor√≠a de las veces las llamadas ir√°n a la DRAM local, y puede minimizar el n√∫mero de llamadas a la DRAM distribuida general ". <br><br>  Esta opci√≥n nos parece un tipo de NUMA muy interesante.  Por cierto, Sugumar estaba trabajando en memoria distribuida para sistemas paralelos de alta velocidad en Sun Microsystems incluso antes de que apareciera la memoria no vol√°til.  Y uno de los problemas con estas diferentes variantes de la jerarqu√≠a de memoria fue que si una de ellas se pierde debido a una falla de la red o del bus, entonces toda la m√°quina se bloquear√°.  "En los sistemas de memoria distribuida, las fallas de la red deben manejarse de manera m√°s elegante, y esto causa muchos desaf√≠os de dise√±o". <br><br>  Otro punto es que queremos que cualquier cach√© de alto nivel, ni siquiera L4, se realice al m√°ximo con la ayuda del hierro y con el m√≠nimo con la ayuda del software.  Los n√∫cleos del sistema operativo y otro software siempre necesitan algo de tiempo para ponerse al d√≠a con el hardware, ya sea agregando nuevos n√∫cleos, o cach√©s L3 o L4, o memoria direccionable no vol√°til. <br><br>  "En alg√∫n momento, un nivel de cach√© adicional ser√° inevitable", dice Gianos.  - Obtuvimos el primer nivel de cach√©, y en alg√∫n momento apareci√≥ el segundo.  Y luego finalmente agregamos un tercero.  Y alg√∫n d√≠a tendremos un cuarto.  La √∫nica pregunta es cu√°ndo y por qu√©.  Y me parece que sus observaciones con respecto a las capacidades de este cach√© son bastante interesantes.  Pero Intel a√∫n no ha decidido cu√°ndo o por qu√© tales cosas se har√°n p√∫blicas.  Otras compa√±√≠as tambi√©n est√°n estudiando este tema;  Ser√≠a tonto no examinarlo.  Tarde o temprano esto suceder√°, pero pronto suceder√°, o no muy, a√∫n no est√° claro ‚Äù. </div></div><p>Source: <a href="https://habr.com/ru/post/485374/">https://habr.com/ru/post/485374/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../485354/index.html">Motocicleta hecha con impresi√≥n 3D SLS</a></li>
<li><a href="../485358/index.html">¬øEl desarrollo m√≥vil es f√°cil y aburrido? Informe Yandex</a></li>
<li><a href="../485364/index.html">C√≥mo dejar de perder el tiempo de los desarrolladores en deuda t√©cnica</a></li>
<li><a href="../485370/index.html">¬øC√≥mo puede un desarrollador ayudar a un gerente a hacer un trato?</a></li>
<li><a href="../485372/index.html">Sobre lo inmutable: historia del noveno lugar de la Copa AI rusa 2019</a></li>
<li><a href="../485376/index.html">¬øC√≥mo hacer que la interfaz sea tres veces m√°s r√°pida y cu√°ndo aplicar comandos en lugar de repositorios? Video</a></li>
<li><a href="../485378/index.html">Estudio de caso: C√≥mo aparecer en Google Play y adaptar ASO a diferentes pa√≠ses</a></li>
<li><a href="../485380/index.html">Artesan√≠a y √âxito de TI</a></li>
<li><a href="../485384/index.html">NeurIPS 2019: tendencias de ML que estar√°n con nosotros durante la pr√≥xima d√©cada</a></li>
<li><a href="../485386/index.html">Microbrowsers est√°n en todas partes. ¬øPero qu√© sabemos sobre ellos?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>