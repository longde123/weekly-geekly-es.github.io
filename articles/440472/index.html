<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêº üßõ ‚úåüèæ Implementaci√≥n de modelos seq2seq en Tensorflow üîà üë®üèΩ‚ÄçüöÄ üëàüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La generaci√≥n de datos utilizando una red neuronal recurrente se est√° convirtiendo en un m√©todo cada vez m√°s popular y se est√° utilizando en muchas √°r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implementaci√≥n de modelos seq2seq en Tensorflow</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440472/"><p>  La generaci√≥n de datos utilizando una red neuronal recurrente se est√° convirtiendo en un m√©todo cada vez m√°s popular y se est√° utilizando en muchas √°reas de la inform√°tica.  Desde el comienzo del nacimiento del concepto seq2seq en 2014, solo han pasado cinco a√±os, pero el mundo ha visto muchas aplicaciones, comenzando con los modelos cl√°sicos de traducci√≥n y reconocimiento de voz, y terminando con la generaci√≥n de descripciones de objetos en fotograf√≠as. </p><br><p> Por otro lado, con el tiempo, la biblioteca Tensorflow, lanzada por Google espec√≠ficamente para el desarrollo de redes neuronales, gan√≥ popularidad.  Naturalmente, los desarrolladores de Google no pod√≠an ignorar un paradigma tan popular como seq2seq, por lo que la biblioteca Tensorflow proporciona clases para el desarrollo dentro de este paradigma.  Este art√≠culo describe este sistema de clases. </p><a name="habracut"></a><br><h2 id="rekurentnye-seti">  Redes recurrentes </h2><br><p>  En la actualidad, las redes recurrentes son uno de los formalismos m√°s conocidos y pr√°cticos para construir redes neuronales profundas.  Las redes recursivas est√°n dise√±adas para procesar datos en serie, por lo tanto, a diferencia de una c√©lula normal (neurona), que recibe datos como entrada y genera el resultado de los c√°lculos, una c√©lula recursiva contiene dos entradas y dos salidas. </p><br><p>  Una de las entradas representa los datos del elemento actual de la secuencia, y la segunda entrada se llama <i>estado</i> y se transmite como resultado de los c√°lculos de celda en el elemento anterior de la secuencia. </p><br><img src="https://habrastorage.org/getpro/habr/post_images/684/601/aa6/684601aa63886d86a1b4dafcf8ab079c.png" width="100" alt="imagen"><br><p>  La figura muestra la celda A, para la cual se ingresan los datos de un elemento de secuencia <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.156ex" height="1.817ex" viewBox="0 -520.7 928.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-74" x="809" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-1"> x_t </script>  as√≠ como la condici√≥n no indicada aqu√≠ <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.017ex" height="1.937ex" viewBox="0 -520.7 1729.5 834" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-73" x="0" y="0"></use><g transform="translate(469,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMAIN-31" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> s_ {t-1} </script>  .  En la salida, la celda A da el estado <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.916ex" height="1.817ex" viewBox="0 -520.7 825.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-73" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-74" x="663" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> s_t </script>  y el resultado del c√°lculo <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.165ex" height="2.419ex" viewBox="0 -780.1 932.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-74" x="815" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-4"> h_t </script>  . </p><br><p>  En la pr√°ctica, la secuencia de datos generalmente se divide en subsecuencias de cierta longitud fija y se pasa al c√°lculo por subconjuntos completos (lotes).  En otras palabras, las subsecuencias son ejemplos de aprendizaje.  Las entradas, salidas y estados de celda de una red recursiva son secuencias de n√∫meros reales.  Para el c√°lculo de entrada <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mn>1</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.384ex" height="1.696ex" viewBox="0 -520.7 1026.4 730.2" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/440472/&amp;usg=ALkJrhjsnyGEFnuPcW0guhHALA30eA4JuQ#MJMAIN-31" x="809" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-5"> x_1 </script>  es necesario usar un estado que no fue el resultado de un c√°lculo en una secuencia de datos dada.  Tales estados se llaman estados iniciales.  Si la secuencia es lo suficientemente larga, entonces tiene sentido mantener el contexto de los c√°lculos en cada subsecuencia.  En este caso, es posible transmitir el √∫ltimo estado calculado en la secuencia anterior como el estado inicial.  Si la secuencia no es tan larga o la subsecuencia es el primer segmento, puede inicializar el estado inicial con ceros. </p><br><p>  Por el momento, para entrenar redes neuronales en casi todas partes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se utiliza</a> el algoritmo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">propagaci√≥n hacia atr√°s de errores</a> .  El resultado del c√°lculo en el conjunto de ejemplos transmitidos (en nuestro caso, el conjunto de subsecuencias) se compara con el resultado esperado (datos marcados).  La diferencia entre los valores reales y esperados se denomina error y este error se propaga a los pesos de la red en la direcci√≥n opuesta.  Por lo tanto, la red se adapta a los datos etiquetados y, como regla, el resultado de esta adaptaci√≥n funciona bien para los datos que la red no reuni√≥ en los ejemplos de entrenamiento inicial (hip√≥tesis de generalizaci√≥n). </p><br><p>  En el caso de una red recursiva, tenemos varias opciones sobre qu√© salidas considerar el error.  Aqu√≠ describiremos dos principales: </p><br><ol><li>  Puede considerar el error comparando la salida de la √∫ltima celda de la subsecuencia con la salida esperada.  Esto funciona bien para la tarea de clasificaci√≥n.  Por ejemplo, necesitamos determinar el color emocional de un tweet.  Para hacer esto, seleccionamos tweets y los marcamos en tres categor√≠as: negativo, positivo y neutral.  La salida de la celda ser√° de tres n√∫meros: pesos de categor√≠a.  El tweet tambi√©n se marcar√° con tres n√∫meros: las probabilidades de que el tweet pertenezca a la categor√≠a correspondiente.  Despu√©s de calcular el error en un subconjunto de datos, puede propagarlo a trav√©s de la salida o el estado que desee. </li><li>  Puede leer el error inmediatamente en las salidas del c√°lculo de celda para cada elemento de la subsecuencia.  Esto es muy adecuado para la tarea de predecir el siguiente elemento de una secuencia de los anteriores.  Este enfoque se puede utilizar, por ejemplo, en el problema de determinar anomal√≠as en series temporales de datos o en la tarea de predecir el siguiente car√°cter en el texto, para luego generarlo.  La propagaci√≥n de errores tambi√©n es posible a trav√©s de estados o salidas. </li></ol><br><p>  A diferencia de una red neuronal normal totalmente conectada, una red recursiva es profunda en el sentido de que el error se propaga no solo desde las salidas de la red a sus pesos, sino tambi√©n hacia la izquierda, a trav√©s de las conexiones entre estados.  La profundidad de la red est√° as√≠ determinada por la longitud de la subsecuencia.  Para propagar el error a trav√©s del estado de la red recursiva, hay un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">algoritmo</a> especial.  Su caracter√≠stica es que los gradientes de los pesos se multiplican entre s√≠, cuando el error se propaga de derecha a izquierda.  Si el error inicial es mayor que la unidad, como resultado, el error puede llegar a ser muy grande.  Por el contrario, si el error inicial es menor que la unidad, entonces, en alg√∫n lugar al comienzo de la secuencia, el error puede desvanecerse.  Esta situaci√≥n en la teor√≠a de las redes neuronales se llama el carrusel del error est√°ndar.  Para evitar tales situaciones durante el entrenamiento, se inventaron c√©lulas especiales que no tienen tales inconvenientes.  La primera de estas c√©lulas fue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LSTM</a> , ahora hay una amplia gama de alternativas, de las cuales la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GRU</a> m√°s popular. </p><br><p>  Una buena introducci√≥n a las redes de recurrencia se puede encontrar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en este art√≠culo</a> .  Otra fuente conocida es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un art√≠culo</a> del blog de Andrey Karpaty. </p><br><p>  La biblioteca Tensorflow tiene muchas clases y funciones para implementar redes recursivas.  Aqu√≠ hay un ejemplo de c√≥mo crear una red recursiva din√°mica basada en una celda del tipo GRU: </p><br><pre><code class="python hljs">cell = tf.contrib.rnn.GRUCell(dimension) outputs, state = tf.nn.dynamic_rnn(cell, input, sequence_length=input_length, dtype=tf.float32)</code> </pre> <br><p>  En este ejemplo, se crea una celda GRU, que luego se usa para crear una red recursiva din√°mica.  El tensor de datos de entrada y las longitudes reales de las subsecuencias se transmiten a la red.  Los datos de entrada siempre se especifican mediante un vector de n√∫meros reales.  Para un solo valor, por ejemplo, un c√≥digo de s√≠mbolo o una palabra, el llamado  incrustaci√≥n: asigna este c√≥digo a alguna secuencia de n√∫meros.  La funci√≥n de crear una red recursiva din√°mica devuelve un par de valores: una lista de salidas de red para todos los valores de la secuencia y el √∫ltimo estado calculado.  Como entrada, la funci√≥n toma una celda, datos de entrada y un tensor de longitud de subsecuencia. </p><br><p>  Una red recursiva din√°mica difiere de una est√°tica en que no crea una red de celdas de red para la subsecuencia por adelantado (en la etapa de determinaci√≥n del gr√°fico de c√°lculo), sino que lanza las celdas en las entradas din√°micamente durante el c√°lculo del gr√°fico en los datos de entrada.  Por lo tanto, esta funci√≥n necesita conocer las longitudes de subsecuencias de los datos de entrada para detenerse en el momento adecuado. </p><br><h2 id="porozhdayuschie-modeli-na-osnove-rekurentnyh-setey">  Generando modelos basados ‚Äã‚Äãen redes de recurrencia </h2><br><h3 id="porozhdayuschie-rekurentnye-seti">  Generando redes de recurrencia </h3><br><p>  Anteriormente, consideramos dos m√©todos para calcular los errores de las redes recursivas: en la √∫ltima salida o en todas las salidas para una secuencia dada.  Aqu√≠ consideramos el problema de generar secuencias.  La capacitaci√≥n de la red de generadores se basa en el segundo m√©todo de lo anterior. </p><br><p>  Con m√°s detalle, estamos tratando de entrenar una red recursiva para predecir el siguiente elemento de una secuencia.  Como se mencion√≥ anteriormente, la salida de una celda en una red recursiva es simplemente una secuencia de n√∫meros.  Este vector no es muy conveniente para el aprendizaje, por lo tanto, introducen otro nivel, que recibe este vector en la entrada, y en la salida da el peso de las predicciones.  Este nivel se llama <em>nivel de proyecci√≥n</em> y le permite comparar la salida de la celda en un elemento dado de la secuencia con la salida esperada en los datos etiquetados. </p><br><p>  Para ilustrar, considere la tarea de generar texto que se representa como una secuencia de caracteres.  La longitud del vector de salida del nivel de proyecci√≥n es igual al tama√±o del alfabeto del texto fuente.  El tama√±o del alfabeto generalmente no supera los 150 caracteres, si cuenta los caracteres de los idiomas ruso e ingl√©s, as√≠ como los signos de puntuaci√≥n.  La salida del nivel de proyecci√≥n es un vector con la longitud del alfabeto, donde cada s√≠mbolo corresponde a una determinada posici√≥n en este vector: el √≠ndice de este s√≠mbolo.  Los datos etiquetados tambi√©n son un vector que consiste en ceros, donde uno se encuentra en la posici√≥n del personaje que sigue la secuencia. </p><br><p>  Para el entrenamiento, utilizamos dos secuencias de datos: </p><br><ol><li>  Una secuencia de caracteres en el texto fuente, al principio de la cual se agrega un car√°cter especial que no forma parte del texto fuente.  Por lo general, se conoce como <em>ir</em> . </li><li>  La secuencia de caracteres del texto fuente tal cual, sin adiciones. </li></ol><br><p>  Ejemplo para el texto "mam√° lav√≥ el marco": </p><br><pre> <code class="python hljs">[<span class="hljs-string"><span class="hljs-string">'&lt;go&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span> <span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span> <span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">'] ['</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>]</code> </pre> <br><p>  Para el entrenamiento, generalmente se forman minibatches, que consisten en un peque√±o n√∫mero de ejemplos.  En nuestro caso, se trata de cadenas que pueden tener diferentes longitudes.  El c√≥digo descrito a continuaci√≥n utiliza el siguiente m√©todo para resolver el problema de diferentes longitudes.  De las muchas l√≠neas en este minipaquete, se calcula la longitud m√°xima.  Todas las dem√°s l√≠neas se rellenan con un car√°cter especial (relleno) para que todos los ejemplos en el minipacket tengan la misma longitud.  En el ejemplo de c√≥digo a continuaci√≥n, la cadena del <em>pad</em> se usa como dicho car√°cter.  Adem√°s, para una mejor generaci√≥n, al final del ejemplo, agregue el s√≠mbolo para el final de la oraci√≥n - <em>eos</em> .  Por lo tanto, en realidad, los datos del ejemplo se ver√°n un poco diferentes: </p><br><pre> <code class="python hljs">[<span class="hljs-string"><span class="hljs-string">'&lt;go&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span> <span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span> <span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span>&lt;eos&gt;<span class="hljs-string"><span class="hljs-string">', '</span></span>&lt;pad&gt;<span class="hljs-string"><span class="hljs-string">', '</span></span>&lt;pad&gt;<span class="hljs-string"><span class="hljs-string">', '</span></span>&lt;pad&gt;<span class="hljs-string"><span class="hljs-string">'] ['</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span><span class="hljs-string"><span class="hljs-string">', '</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">'&lt;eos&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>]</code> </pre> <br><p>  La primera secuencia se alimenta a la entrada de la red, y la segunda secuencia se usa como datos etiquetados.  El entrenamiento de predicci√≥n se basa en desplazar la secuencia original un personaje a la izquierda. </p><br><h3 id="obuchenie-i-porozhdenie">  Entrenamiento y desove </h3><br><h4 id="obuchenie">  Entrenamiento </h4><br><p>  El algoritmo de aprendizaje es bastante simple.  Para cada elemento de la secuencia de entrada, calculamos el vector de salida de su nivel de proyecci√≥n y lo comparamos con el marcado.  La √∫nica pregunta es c√≥mo calcular el error.  Puede usar el error cuadr√°tico medio, pero para calcular el error en esta situaci√≥n, es mejor usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrop√≠a cruzada</a> .  La biblioteca Tensorflow proporciona varias funciones para su c√°lculo, aunque no hay nada que detenga la implementaci√≥n de la f√≥rmula de c√°lculo directamente en el c√≥digo. </p><br><p>  Para mayor claridad, presentamos alguna notaci√≥n.  Por symbol_id indicaremos el identificador del s√≠mbolo (su n√∫mero de serie en el alfabeto).  El t√©rmino s√≠mbolo aqu√≠ es bastante arbitrario y simplemente significa un elemento del alfabeto.  El alfabeto puede no contener s√≠mbolos, sino palabras o incluso algunos conjuntos de atributos m√°s complejos.  El t√©rmino symbol_embedding se usar√° para denotar el vector de n√∫meros que corresponde a un elemento dado del alfabeto.  Por lo general, estos conjuntos de n√∫meros se almacenan en una tabla de tama√±os que coincide con el tama√±o del alfabeto. </p><br><p>  Tensorflow proporciona una funci√≥n que le permite acceder a la tabla de incrustaci√≥n y reemplazar los √≠ndices de caracteres con sus vectores de incrustaci√≥n.  Primero, definimos una variable para almacenar la tabla: </p><br><pre> <code class="python hljs">embedding_table = tf.Variable(tf.random_uniform([alphabet_size, embedding_size]))</code> </pre> <br><p>  Despu√©s de eso, puede convertir los tensores de entrada en tensores de inserci√≥n: </p><br><pre> <code class="python hljs">input_embeddings = tf.nn.embedding_lookup(embedding_table, input_ids)</code> </pre> <br><p>  El resultado de la llamada a la funci√≥n es un tensor de la misma dimensi√≥n que se transfiri√≥ a la entrada, pero como resultado, todos los √≠ndices de caracteres se reemplazan con las secuencias de incrustaci√≥n correspondientes. </p><br><h4 id="porozhdenie">  Engendrar </h4><br><p>  Para calcular, una celda de una red recursiva necesita un estado y el car√°cter actual.  El resultado del c√°lculo es una salida y un nuevo estado.  Si aplicamos el nivel de proyecci√≥n a la salida, podemos obtener un vector de pesos donde el peso en la posici√≥n correspondiente puede considerarse (muy condicionalmente) como la probabilidad de que este s√≠mbolo aparezca en la siguiente posici√≥n de la secuencia. </p><br><p>  Se pueden usar varias estrategias para seleccionar el siguiente s√≠mbolo en funci√≥n del vector de peso generado por el nivel de proyecci√≥n: </p><br><ul><li>  Estrategia de b√∫squeda codiciosa.  Cada vez que seleccionamos el s√≠mbolo con el mayor peso, es decir  muy probablemente en esta situaci√≥n, pero no necesariamente la m√°s apropiada en el contexto de toda la secuencia. </li><li>  Estrategia para elegir la mejor secuencia (b√∫squeda de haz).  No seleccionamos un s√≠mbolo a la vez, pero recordamos varias variantes de los s√≠mbolos m√°s probables.  Despu√©s de calcular todas estas opciones para todos los elementos de la secuencia generada, seleccionamos la secuencia de caracteres m√°s probable, teniendo en cuenta el contexto de toda la secuencia.  Por lo general, esto se implementa por medio de una matriz cuyo ancho es igual a la longitud de la secuencia y la altura al n√∫mero de anchos de generaci√≥n de haz.  Una vez completada la generaci√≥n de las variantes de secuencia, se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">utiliza</a> una de las variantes del algoritmo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Viterbi</a> para seleccionar la secuencia m√°s probable. </li></ul><br><h2 id="sistema-tipov-seq2seq-v-biblioteke-tensorflow">  Sistema de tipo seq2seq de la biblioteca Tensorflow </h2><br><p>  Dado lo anterior, est√° claro que la implementaci√≥n de modelos generativos basados ‚Äã‚Äãen redes de recurrencia es una tarea bastante dif√≠cil de codificar.  Por lo tanto, naturalmente, se propusieron sistemas de clase para facilitar la soluci√≥n de este problema.  Uno de estos sistemas se llama seq2seq, luego describimos la funcionalidad de sus tipos principales. </p><br><p>  Pero, antes que nada, algunas palabras sobre el nombre de la biblioteca.  El nombre seq2seq es la abreviatura de secuencia a secuencia (de secuencia a secuencia).  Se propuso la idea original de generar una secuencia para implementar un sistema de traducci√≥n.  La secuencia de entrada de palabras se aliment√≥ a la entrada de una red recursiva, llamada codificador en este sistema.  El resultado de esta red recursiva fue el estado del c√°lculo de la celda en el √∫ltimo car√°cter de la secuencia.  Este estado se present√≥ como el estado inicial de la segunda red recursiva, el decodificador, que fue entrenado para generar la siguiente palabra.  Las palabras fueron utilizadas como s√≠mbolos en ambas redes.  Los errores en el decorador se propagaron al codificador a trav√©s del estado transmitido.  El vector de estado mismo en esta terminolog√≠a se llamaba vector de pensamiento.  La presentaci√≥n intermedia se utiliz√≥ en los modelos de traducci√≥n tradicionales y, como regla, era un gr√°fico que representaba la estructura del texto de entrada para la traducci√≥n.  El sistema de traducci√≥n gener√≥ texto de salida basado en esta estructura intermedia. </p><br><p>  En realidad, la implementaci√≥n de seq2seq en Tensorflow pertenece a la parte del decodificador, sin afectar el codificador.  Por lo tanto, ser√≠a correcto llamar a la biblioteca 2seq, pero la fuerza de la tradici√≥n y la inercia de pensar aqu√≠ obviamente prevalecieron sobre el sentido com√∫n. </p><br><p>  Los dos metatipos principales en la biblioteca seq2seq son: </p><br><ol><li>  Clase <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">auxiliar</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Decodificador de</a> clase. </li></ol><br><p>  Los desarrolladores de la biblioteca identificaron estos tipos bas√°ndose en las siguientes consideraciones.  Consideremos el proceso de aprendizaje y el proceso de generaci√≥n, que describimos anteriormente, desde un √°ngulo ligeramente diferente. </p><br><p>  Para el entrenamiento necesitas: </p><br><ol><li>  Para cada car√°cter, pase el c√°lculo del estado actual y la incrustaci√≥n del car√°cter actual. </li><li>  Recuerde el estado de salida y la proyecci√≥n calculada para la salida. </li><li>  Obtenga el siguiente personaje en la secuencia y vaya al paso 1. </li></ol><br><p>  Despu√©s de eso, puede comenzar a contar los errores comparando los resultados de los c√°lculos con los siguientes caracteres de la secuencia. </p><br><p>  Para generarlo es necesario: </p><br><ol><li>  Para cada car√°cter, pase el c√°lculo del estado actual y la incrustaci√≥n del car√°cter actual. </li><li>  Recuerde el estado de salida y la proyecci√≥n calculada para la salida. </li><li>  Calcule el siguiente car√°cter como el m√°ximo de los √≠ndices de nivel de proyecci√≥n y vaya al paso 1. </li></ol><br><p>  Como se puede ver en la descripci√≥n, los algoritmos son muy similares.  Por lo tanto, los desarrolladores de la biblioteca decidieron encapsular el procedimiento para obtener el siguiente car√°cter en la clase Helper.  Para el entrenamiento, esto es solo leer el siguiente personaje de la secuencia, y para generarlo, seleccionar el personaje con el peso m√°ximo (por supuesto, para la b√∫squeda codiciosa). </p><br><p>  En consecuencia, la clase base Helper implementa el m√©todo next_inputs para obtener el siguiente car√°cter del estado y actual, as√≠ como el m√©todo de muestra para obtener √≠ndices de caracteres del nivel de proyecci√≥n.  La clase TrainingHelper se proporciona para la capacitaci√≥n, y la clase <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GreedyEmbeddingHelper</a> est√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">disponible</a> para la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">generaci√≥n</a> codiciosa.  Desafortunadamente, el modelo de b√∫squeda de haz no encaja en este tipo de sistema, por lo tanto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se</a> implementa una clase especial <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BeamSearchDecoder</a> en la biblioteca para esto.  sin usar Helper. </p><br><p>  La clase Decoder proporciona una interfaz para implementar un decodificador.  De hecho, la clase proporciona dos m√©todos: </p><br><ol><li>  inicializar para inicializar al comienzo del trabajo. </li><li>  paso para implementar un paso o generaci√≥n de aprendizaje.  El contenido de este paso est√° determinado por el Ayudante correspondiente. </li></ol><br><p>  La biblioteca implementa la clase <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BasicDecoder</a> , que se puede usar tanto para entrenar como para reproducirse con los asistentes TrainingHelper y GreedyEmbeddingHelper.  Estas tres clases suelen ser suficientes para implementar modelos de generaci√≥n basados ‚Äã‚Äãen redes de recurrencia. </p><br><p>  Finalmente, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las</a> funciones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dynamic_decode</a> se usan para organizar el paso a trav√©s de una entrada o secuencia generada. </p><br><p>  A continuaci√≥n, consideraremos un ejemplo ilustrativo, que muestra m√©todos para construir modelos de generaci√≥n para varios tipos de biblioteca seq2seq. </p><br><h2 id="illyustrativnyy-primer">  Ejemplo ilustrativo </h2><br><p>  En primer lugar, debe decirse que todos los ejemplos se implementan en Python 2.7.  Se puede encontrar una lista de bibliotecas adicionales en el archivo require.txt. </p><br><p>  Como ejemplo ilustrativo, considere parte de los datos para el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Desaf√≠o de normalizaci√≥n de texto:</a> concurso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">idioma ruso</a> realizado por Kaggle por Google en 2017.  El prop√≥sito de esta competencia fue convertir el texto ruso en una forma adecuada para leer.  El texto del concurso se desglos√≥ en expresiones mecanografiadas.  Los datos de entrenamiento se especificaron en un archivo CSV de la siguiente forma: </p><br><pre> <code class="plaintext hljs">"sentence_id","token_id","class","before","after" 0,0,"PLAIN","","" 0,1,"PLAIN","","" 0,2,"PLAIN","","" 0,3,"DATE","1862 ","    " 0,4,"PUNCT",".","." 1,0,"PLAIN","","" 1,1,"PLAIN","","" 1,2,"PLAIN","","" 1,3,"PLAIN","","" 1,4,"PLAIN","","" 1,5,"PLAIN","","" 1,6,"PLAIN","","" 1,7,"PLAIN","","" 1,8,"PLAIN","","" 1,9,"PUNCT",".","." ...</code> </pre> <br><p>  En el ejemplo anterior, una expresi√≥n de tipo DATE es interesante, en ella, "1862" se traduce en "mil ochocientos sesenta y dos a√±os".  Para ilustrar, consideramos los datos de tipo DATE solo como pares de la forma (expresi√≥n antes, expresi√≥n despu√©s).  Inicio del archivo de datos: </p><br><pre> <code class="plaintext hljs">before,after 1862 ,     1811 ,    12  2013,      15  2013,      1905 ,    17  2014,      7  2010 ,      1 ,  1843 ,     30  2007 ,      1846 ,     1996 ,     9 ,  ...</code> </pre> <br><p>  Construiremos el modelo generador utilizando la biblioteca seq2seq, en la que el codificador se implementar√° a nivel de s√≠mbolo (es decir, los elementos del alfabeto son s√≠mbolos), y el decodificador usar√° las palabras como alfabeto.  El c√≥digo de muestra, como los datos, est√° disponible en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio en Github</a> . </p><br><p>  Los datos de entrenamiento se dividen en tres subconjuntos: train.csv, test.csv y dev.csv, para entrenamiento, prueba y verificaci√≥n de reentrenamiento, respectivamente.  Los datos est√°n en el directorio de datos.  Se implementan tres modelos en el repositorio: seq2seq_greedy.py, seq2seq_attention.py y seq2seq_beamsearch.py.  Aqu√≠ miramos el c√≥digo para el modelo de b√∫squeda codicioso b√°sico. </p><br><p>  Todos los modelos usan la clase <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Estimator</a> para implementar.  El uso de esta clase le permite simplificar la codificaci√≥n sin distraerse con partes que no sean modelos.  Por ejemplo, no es necesario implementar un ciclo de transferencia de datos para el entrenamiento, crear sesiones para trabajar con Tensorflow, pensar en transferir datos a Tensorboard, etc.  Estimator requiere solo dos funciones para su implementaci√≥n: para la transferencia de datos y para construir un modelo.  Los ejemplos tambi√©n usan la clase <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dataset</a> para pasar datos para su procesamiento.  Esta implementaci√≥n moderna es mucho m√°s r√°pida que los diccionarios tradicionales para transferir datos del formulario feed_dict. </p><br><h3 id="formirovanie-dannyh">  Generacion de datos </h3><br><p>  Considere un c√≥digo de generaci√≥n de datos para capacitaci√≥n y generaci√≥n. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse_fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(line_before, line_after)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Encode in Bytes for TF source = [c.encode('utf8') for c in line_before.decode('utf8').rstrip('\n')] t = [w.encode('utf8') for w in nltk.word_tokenize(line_after.decode('utf8').strip())] learn_target = t + ['&lt;eos&gt;'] + ['&lt;pad&gt;'] target = ['&lt;go&gt;'] + t + ['&lt;eos&gt;'] return (source, len(source)), (target, learn_target, len(target)) def generator_fn(data_file): with open(data_file, 'rb') as f: reader = csv.DictReader(f, delimiter=',', quotechar='"') for row in reader: yield parse_fn(row['before'], row['after']) def input_fn(data_file, params=None): params = params if params is not None else {} shapes = (([None], ()), ([None], [None], ())) types = ((tf.string, tf.int32), (tf.string, tf.string, tf.int32)) defaults = (('&lt;pad&gt;', 0), ('&lt;pad&gt;', '&lt;pad&gt;', 0)) dataset = tf.data.Dataset.from_generator(functools.partial(generator_fn, data_file), output_shapes=shapes, output_types=types) dataset = dataset.repeat(params['epochs']) return (dataset.padded_batch(params.get('batch_size', 50), shapes, defaults).prefetch(1))</span></span></code> </pre> <br><p>  La funci√≥n input_fn se usa para crear una colecci√≥n de datos que el Estimador luego pasa al entrenamiento y la generaci√≥n.  El tipo de datos se establece primero.  Este es un par de la forma ((secuencia del codificador, longitud), (secuencia del decodificador, secuencia del decodificador con un prefijo, longitud)).  La cadena "" se usa como prefijos, cada secuencia de codificador termina con una palabra especial "".  Adem√°s, debido al hecho de que las secuencias (tanto de entrada como de salida) tienen una longitud desigual, se utiliza el s√≠mbolo de relleno con el valor "". <br></p><p>  El c√≥digo de preparaci√≥n de datos lee el archivo de datos, divide la cadena del codificador en caracteres y la cadena del decodificador en palabras, utilizando la biblioteca nltk para esto.  Una fila procesada de esta manera es un ejemplo de datos de entrenamiento.  La colecci√≥n generada se divide en mini paquetes y la cantidad de datos se clona de acuerdo con el n√∫mero de eras de entrenamiento (cada √©poca es un pase de datos). </p><br><h3 id="rabota-so-slovaryami">  Trabajar con diccionarios </h3><br><p>  Los diccionarios se almacenan como una lista en archivos, una l√≠nea para una palabra o car√°cter.  Para construir diccionarios, use el script build_vocabs.py.  Los diccionarios generados se encuentran en el directorio de datos como archivos de la forma vocab. *. Txt. </p><br><p>  C√≥digo para leer diccionarios: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Read vocabs and inputs dropout = params['dropout'] source, source_length = features training = (mode == tf.estimator.ModeKeys.TRAIN) vocab_source = tf.contrib.lookup.index_table_from_file(vocabulary_file=params['source_vocab_file'], num_oov_buckets=params['num_oov_buckets']) with open(params['source_vocab_file']) as f: num_sources = sum(1 for _ in f) + params['num_oov_buckets'] vocab_target = tf.contrib.lookup.index_table_from_file(vocabulary_file=params['target_vocab_file'], num_oov_buckets=params['num_oov_buckets']) with open(params['target_vocab_file']) as f: num_targets = sum(1 for _ in f) + params['num_oov_buckets']</span></span></code> </pre> <br><p>  Aqu√≠, probablemente, la funci√≥n index_table_from_file, que lee las entradas del diccionario de un archivo, es interesante, y su par√°metro num_oov_buckets es el n√∫mero de canastas de vocabulario.  Por defecto, este n√∫mero es igual a uno, es decir  todas las palabras que no est√°n en el diccionario tienen el mismo √≠ndice igual al tama√±o del diccionario + 1. Tenemos tres palabras desconocidas: "", "" y "", para las cuales queremos tener √≠ndices diferentes.  Por lo tanto, establezca este par√°metro en el n√∫mero tres.  Desafortunadamente, debe volver a leer el archivo de entrada para obtener el n√∫mero de palabras en el diccionario como una constante de tiempo para configurar el gr√°fico del modelo. <br></p><p>  Todav√≠a necesitamos crear una tabla para implementar la incrustaci√≥n - _source_embedding, as√≠ como para traducir cadenas de palabras a cadenas de identificaci√≥n: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># source embeddings matrix _source_embedding = tf.Variable(tf.random_uniform([num_sources, params['embedding_size']])) source_ids = vocab_source.lookup(source) source_embedding = tf.nn.embedding_lookup(_source_embedding, source_ids)</span></span></code> </pre> <br><h3 id="realizaciya-kodirovschika">  Implementaci√≥n del codificador </h3><br><p>  Para el codificador, utilizaremos una red recursiva bidireccional con varios niveles.     ,     ,      . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># add multilayer bidirectional RNN cell_fw = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(params['dim']) for _ in range(params['layers'])]) cell_bw = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(params['dim']) for _ in range(params['layers'])]) outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, source_embedding, sequence_length=source_length, dtype=tf.float32) # prepare output output = tf.concat(outputs, axis=-1) encoder_output = tf.layers.dense(output, params['dim']) # prepare state state_fw, state_bw = states cells = [] for fw, bw in zip(state_fw, state_bw): state = tf.concat([fw, bw], axis=-1) cells += [tf.layers.dense(state, params['dim'])] encoder_state = tuple(cells)</span></span></code> </pre> <br><p>       GRU,    MultiRNNCell,   ,   rnn.Cell.    , <br> sequence_length ‚Äî     ,     ,    . </p><br><p> ,       ,       ,           .      ,      128,        256.     ,        ,      128.        . </p><br><p>     .  Porque    , ,    bidirectional_dynamic_rnn,   ,     .           ,      .     , ..       . , ,  .            ,   ,       . </p><br><h3 id="realizaciya-dekodirovschika">   </h3><br><p>     ,    .           . </p><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment"># decoder RNN cell decoder_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(params['dim']) for _ in range(params['layers'])]) decoder_initial_state = encoder_state # projection layer projection_layer = tf.layers.Dense(num_targets, use_bias=False) # embedding table for targets target_embedding = tf.Variable(tf.random_uniform([num_targets, params['embedding_size']]))</span></span></code> </pre> <br><h4 id="obuchenie-1">  </h4><br><p>    TrainingHelper + BasicDecoder. </p><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment"># target embeddings matrix target, learn_target, target_length = labels target_ids = vocab_target.lookup(target) target_learn_ids = vocab_target.lookup(learn_target) # train encoder _target_embedding = tf.nn.embedding_lookup(target_embedding, target_ids) train_helper = tf.contrib.seq2seq.TrainingHelper(_target_embedding, target_length) train_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, train_helper, decoder_initial_state, output_layer=projection_layer) train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder) train_output = train_outputs.rnn_output train_sample_id = train_outputs.sample_id</span></span></code> </pre> <br><h4 id="porozhdenie-1">  </h4><br><p>        . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># prediction decoder prediction_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper( embedding=target_embedding, start_tokens=tf.fill([batch_size], tf.to_int32(vocab_target.lookup(tf.fill([], '&lt;go&gt;')))), end_token=tf.to_int32(vocab_target.lookup(tf.fill([], '&lt;eos&gt;')))) prediction_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, prediction_helper, decoder_initial_state, output_layer=projection_layer) prediction_output, _, _ = tf.contrib.seq2seq.dynamic_decode(prediction_decoder, maximum_iterations=params['max_iters']) # prepare prediction reverse_vocab_target = tf.contrib.lookup.index_to_string_table_from_file(params['target_vocab_file']) pred_strings = reverse_vocab_target.lookup(tf.to_int64(prediction_output.sample_id)) predictions = { 'ids': prediction_output.sample_id, 'text': pred_strings }</span></span></code> </pre> <br><p>     GreedyEmbeddingHelper       "",     "".        . , ,    dynamic_decode      .    ,    ,   . ,     ,        . <br><br></p><h4 id="funkciya-poter-i-optimizaciya">     </h4><br><p>     ,        seq2seq. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># loss masks = tf.sequence_mask(lengths=target_length, dtype=tf.float32) loss = tf.contrib.seq2seq.sequence_loss(logits=train_output, targets=target_learn_ids, weights=masks)</span></span></code> </pre> <br><p>    ,     ,      sequence_mask. </p><br><p>     Adam   ,   . </p><br><pre> <code class="python hljs">optimizer = tf.train.AdamOptimizer(learning_rate=params.get(<span class="hljs-string"><span class="hljs-string">'lr'</span></span>, <span class="hljs-number"><span class="hljs-number">.001</span></span>)) grads, vs = zip(*optimizer.compute_gradients(loss)) grads, gnorm = tf.clip_by_global_norm(grads, params.get(<span class="hljs-string"><span class="hljs-string">'clip'</span></span>, <span class="hljs-number"><span class="hljs-number">.5</span></span>)) train_op = optimizer.apply_gradients(zip(grads, vs), global_step=tf.train.get_or_create_global_step())</code> </pre> <br><h4 id="rezultaty-obucheniya">   </h4><br><p>         .     0.9   . , ,     ,    .   ,    . </p><br><pre> <code class="plaintext hljs">24  1944                 1  2003              1992 .           11  1927               1969            1  2016             1047          1863            17      22  2014              </code> </pre> <br><p>        .   ‚Äî   ,   ‚Äî  ,   ‚Äî  . </p><br><p>  ,    ‚Äî   .             .    ,    ( ),       .       .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>        ,     . </p><br><h2 id="zaklyuchenie">  Conclusi√≥n </h2><br><p>            seq2seq.      ,          ,     .    ,  . </p><br><p>           .  Tensorflow   ,   ,     .   ,         ,   .        ,        . ,      ,   padding  ,   embedding     ?       , ,       .         ‚Äî     . ,    ,    . ,    ,    ,    . ,       . ,          , , ,        . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/440472/">https://habr.com/ru/post/440472/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../440462/index.html">Las 7 mejores estrategias de marketing de contenidos que no debe perderse en 2019</a></li>
<li><a href="../440464/index.html">Trabaje con el servicio de bases de datos administradas de Digital Ocean en .NET Core</a></li>
<li><a href="../440466/index.html">Control remoto web UART</a></li>
<li><a href="../440468/index.html">2 veces m√°s, 10 veces m√°s r√°pido, durante todo el d√≠a, todo por el bien de las personas</a></li>
<li><a href="../440470/index.html">Incruste un int√©rprete de Python en una aplicaci√≥n Java utilizando el proyecto Panam√°</a></li>
<li><a href="../440474/index.html">Efectos de filtrado SVG. Parte 4. Im√°genes a dos colores con feComponentTransfer</a></li>
<li><a href="../440476/index.html">"Comienza con mitaps" o ¬øNecesitas cursos de programaci√≥n?</a></li>
<li><a href="../440478/index.html">3CX v16 Beta 1 con soporte para Raspberry Pi lanzado</a></li>
<li><a href="../440486/index.html">Precio de calidad: 7 principios para optimizar los costos de prueba</a></li>
<li><a href="../440488/index.html">Mapas de sombras reflectantes: parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>