<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì† ü§¥üèΩ üÖ±Ô∏è Die √úberschrift "Artikel f√ºr Sie lesen." Oktober - Dezember 2019 ü§∏üèø üòâ ‚§µÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Wir ver√∂ffentlichen weiterhin Rezensionen zu wissenschaftlichen Artikeln von Mitgliedern der Open Data Science-Community auf dem Kanal #art...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die √úberschrift "Artikel f√ºr Sie lesen." Oktober - Dezember 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/485122/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Hallo habr  Wir ver√∂ffentlichen weiterhin Rezensionen zu wissenschaftlichen Artikeln von Mitgliedern der Open Data Science-Community auf dem Kanal #article_essense.  Wenn Sie sie vor allen anderen erhalten m√∂chten - treten Sie der <a href="http://ods.ai/">Community bei</a> ! </p><br><p>  Artikel f√ºr heute: </p><br><ol><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Poly-Encoder: Transformatorarchitekturen und Strategien vor dem Training f√ºr schnelles und genaues Scoring mit mehreren S√§tzen (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Impliziter Diskriminator in variierendem Autoencoder (Indian Institute of Technology Ropar, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Selbsttraining mit Noisy Student verbessert die ImageNet-Klassifizierung (Google Research, Carnegie Mellon University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Momentum Contrast f√ºr unbeaufsichtigtes visuelles Repr√§sentationslernen (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Benchmarking der Robustheit neuronaler Netze gegen√ºber h√§ufigen St√∂rungen und St√∂rungen (University of California, Oregon State University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">DistilBERT, eine destillierte Version von BERT: kleiner, schneller, billiger und leichter (Hugging Face, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Plug-and-Play-Sprachmodelle: Ein einfacher Ansatz zur kontrollierten Texterstellung (Uber AI, Caltech, HKUST, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Deep Salience Representation f√ºr die F0-Sch√§tzung in polyphoner Musik (New York University, USA, 2017)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Analysieren und Verbessern der Bildqualit√§t von StyleGAN (NVIDIA, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Links zu fr√ºheren Sammlungen der Reihe:</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/company/ods/blog/472672/">Juli - September 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/471514/">Januar - Juni 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/352518/">Februar - M√§rz 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/352508/">Dezember 2017 - Januar 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/343822/">Oktober - November 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/339094/">September 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/336624/">August 2017</a> </li></ul></div></div><br><h3 id="1-poly-encoders-transformer-architectures-and-pre-training-strategies-for-fast-and-accurate-multi-sentence-scoring">  1. Polycodierer: Transformatorarchitekturen und Strategien vor dem Training f√ºr schnelles und genaues Scoring mit mehreren S√§tzen </h3><br><p>  Autoren: Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1905.01969">‚Üí Originalartikel</a> <br>  Autor der Rezension: Alexey (in slack zhirzemli) </p><br><p>  <strong>TLDR</strong> </p><br><p>  Der Artikel schl√§gt einen neuen Ansatz zur Bewertung von Satzpaaren (Aussagen) vor.  Dieses Verfahren ist sowohl bei der Vorhersage der Korrespondenz einer Antwort auf einen bedingten Kontext als auch bei Aufgaben wie der Vorhersage des n√§chsten Urteils relevant.  Die vorgeschlagene Poly-Encoder-Methode wird mit Bi-Encoder- und Cross-Encoder-Strategien verglichen.  Die Methode kombiniert den Vorteil von Bi-Encoder (die F√§higkeit, die Darstellung von Antworten zwischenzuspeichern) und Cross-Encoder (nicht unbedingtes Training von Kontext- und Antwortcodierern). </p><br><img src="https://habrastorage.org/webt/ax/qd/nl/axqdnlibzffxcyjtbfzeguhsdja.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Mehrsatzwertung</strong> </p><br><p>  (Eine kleine Erinnerung an Bi und Cross Encoder-Ans√§tze. F√ºr diejenigen, die vertraut sind, k√∂nnen Sie √ºberspringen) </p><br><p>  Die Aufgabe, die √úbereinstimmung des Kontexts (Benutzeranforderung oder -anweisung) mit der Menge der vorhandenen Antworten zu bestimmen, ist in Dialog- und Informationsabrufsystemen am relevantesten.  Es wird entweder durch Auffinden einer bestimmten Geschwindigkeit (Punktprodukt) zwischen den codierten Darstellungen des Kontexts und der Antwort oder durch gemeinsames Codieren des Kontexts und der Antwort in einen einzelnen Vektor mit anschlie√üender linearer Transformation in einen Skalar gel√∂st. </p><br><p>  Der erste Ansatz hei√üt Bi-Encoder und der offensichtliche Vorteil dieser Methode ist die M√∂glichkeit, die Darstellungen aller verf√ºgbaren Antworten offline zu z√§hlen.  Diese Ansichten werden zwischengespeichert, und w√§hrend der Inferenz m√ºssen Sie nur den Abfragevektor finden, ein Skalarprodukt mit Antwortvektoren erstellen und das Ergebnis anordnen.  Dar√ºber hinaus erm√∂glicht dieser Ansatz eine effizientere Negativabtastung in der Trainingsphase.  Innerhalb jeder Charge werden n√§mlich Darstellungen f√ºr positive Proben ber√ºcksichtigt, und negative Beispiele k√∂nnen direkt derselben Charge entnommen werden.  Verwenden Sie den Forward-Pass im Wesentlichen f√ºr positive und negative Beispiele.  Der Nachteil des Bi-Encoder-Ansatzes ist die Tatsache, dass Kontext- und Antwortdarstellungen fast unabh√§ngig voneinander lernen.  Der einzige Punkt, an dem zumindest ein Informationsfluss zwischen der Anforderungs- und der Antwortansicht m√∂glich ist, ist das Botnetz in Form des endg√ºltigen Skalarprodukts.  Auf der Ebene von Textfeatures werden Informationen nicht gefummelt. </p><br><p>  Der zweite Ansatz ist Cross-Encoder.  Es beinhaltet eine st√§rkere Interaktion von Kontext und Reaktion im Lernprozess und in der Schlussfolgerung.  Hier werden die Anforderungs- und Antwort-Token-Sequenzen zu einer verkettet.  Ein spezielles Trennzeichen wird zwischen ihnen platziert, und jedem Teil wird eine spezielle Einbettung hinzugef√ºgt (Anforderung, Antwort).  Tats√§chlich werden durch diese Einbettung die Eingabedarstellungen von Antworttoken um eine Konstante verschoben, sodass das Modell sie leichter von Anforderungstoken unterscheiden kann.  Infolgedessen lernt das Modell, eine gemeinsame Darstellung der Anforderung und Antwort zu finden, so dass die endg√ºltige lineare Ebene (Vektor -&gt; Skalar) einen gro√üen Logit-Wert f√ºr Paare von S√§tzen zur√ºckgibt, die miteinander √ºbereinstimmen, und einen kleinen Wert ansonsten.  Der Nachteil dieses Ansatzes ist die Unm√∂glichkeit, die Darstellungen von Antworten offline zu z√§hlen: Sie m√ºssen in der Inferenzphase zusammen mit einem bedingten Satz von Anforderungstoken ausgewertet werden.  Auch der Trick, die Ideen von negativen und positiven Beispielen in der Schulungsphase wiederzuverwenden, funktioniert hier nicht mehr.  Sie m√ºssen vor der Bildung der Charge negative Proben entnehmen. </p><br><p>  <strong>Motivation</strong> <br>  Die folgende L√∂sung erm√∂glicht es Ihnen, die M√§ngel zu beheben und die Vorteile von Bi- und Cross-Encoder-Ans√§tzen zu kombinieren.  Die Idee ist, dass wir einen Encoder trainieren m√∂chten, der einerseits die bedingte Abh√§ngigkeit von Antwort-Tokens von Anforderungs-Tokens ber√ºcksichtigt, und andererseits die Verwendung dieser Abh√§ngigkeit von vorbewerteten Darstellungen der Antwort und der Anforderung erfolgen sollte.  Geometrisch stelle ich mir das so vor: Verschieben Sie das Botnetz (das letzte Skalarprodukt der beiden Einreichungen) etwas tiefer in das Netzwerk.  Erstellen Sie eine Interaktion zwischen der Anforderungs- und der Antwortansicht.  Gleichzeitig ist die Implementierung einer solchen Interaktion nicht zu weit von der Endschicht entfernt, so dass der Hauptteil des Anforderungscodierers unabh√§ngig vom Antwortcodierer bleibt. </p><br><p>  <strong>Implementierung</strong> <br>  Die Umsetzung einer solchen Idee ist recht einfach: Der Encoder von Kandidaten funktioniert wie im Fall von Bi-Encoder: Wir erhalten die Darstellung der Sequenz in Vektorform ([CLS] -Token) unter Verwendung des transformatorbasierten Modells (BERT).  Diese Darstellungen werden nach dem Training des Modells zwischengespeichert. </p><br><p>  Der Kontextcodierer komprimiert seinerseits die Darstellung der Eingabesequenz nicht in einen einzelnen Vektor.  Hier belassen wir alle vom Modell codierten Sequenzvektoren. </p><br><p>  Um eine Bewertung der Konformit√§t des Kontexts (eine Menge von Vektoren) und des Kandidaten (ein Vektor) zu erhalten, wird der Aufmerksamkeitsmechanismus verwendet.  Der Kandidatenvektor ist in diesem Fall eine Anfrage, und der Kontextvektor ist der Schl√ºssel.  Es gilt als Skalarprodukt und weiter - Softmax nach den resultierenden Werten.  Kontextvektoren werden mit der resultierenden Verteilung gewichtet und aufsummiert.  Als Ergebnis erhalten wir die Kontextdarstellung in Form eines einzelnen Vektors.  Und au√üerdem betrachten wir, wie im √ºblichen Bi-Encoder, das Skalarprodukt des Kontexts und des Kandidaten. </p><br><p>  Der Artikel schlug auch eine Reihe von M√∂glichkeiten vor, um die Gewichtung von Kontextvektoren zu beschleunigen.  Die am besten funktionierende Option war ein solcher Vorgang des Z√§hlens der Aufmerksamkeit, bei dem nur die ersten m Vektoren der Kontextsequenz genommen wurden. </p><br><p>  <strong>Ergebnisse</strong> <br>  Als Ergebnis stellte sich heraus, dass Cross-Encoder immer noch am besten funktioniert.  Aber Poly-Encoder ist in Bezug auf Qualit√§tsmetriken nicht weit dahinter und in Bezug auf die Inferenzgeschwindigkeit arbeitet es hunderte Male schneller. </p><br><h3 id="2-implicit-discriminator-in-variational-autoencoder">  2. Impliziter Diskriminator in variierendem Autoencoder </h3><br><p>  Autoren: Prateek Munjal, Akanksha Paul, Narayanan C. Krishnan (Indisches Institut f√ºr Technologie Ropar, 2019) <br>  <a href="https://arxiv.org/abs/1909.13062">‚Üí Originalartikel</a> <br>  Autor der Rezension: Alex Chiron (in sliron shiron8bit) </p><br><p>  In dem Artikel schlugen die Autoren eine Architektur vor, die versucht, die Vorteile von VAE- und GAN-Ans√§tzen f√ºr die Bilderzeugung zu kombinieren, wobei die mit jedem Ansatz verbundenen Nachteile umgangen werden: Unsch√§rfe bei Autoencodern, Zusammenbruch des Modus / Fehlen des Modus bei gegnerischem Training.  Sie erreichen dies aufgrund der Gesamtgewichte zwischen dem Codierer und dem Diskriminator und dem gemeinsamen Generator / Decodierer, wodurch zum einen die Anzahl der Netzwerkgewichte verringert wird und zum anderen n√ºtzliche Informationen vom Diskriminator √ºber Gradienten abgerufen werden k√∂nnen, wenn der Generator / Decodierer nicht abf√§llt in die tats√§chliche Datenverteilung. </p><br><p>  <strong>Einleitung</strong> <br>  Bei den Erzeugungsproblemen spielt die √úbereinstimmung der Verteilung der erzeugten Daten Q mit der Verteilung der realen Daten P, die durch die Kullback-Leibler-Divergenz gemessen wird, eine wichtige Rolle.  Ein charakteristisches Merkmal dieses Ma√ües f√ºr die Entfernung von Verteilungen ist, dass es asymmetrisch ist.  Dementsprechend erhalten wir unterschiedliche Bilder, je nachdem, ob wir Div_KL (P || Q) oder Div_KL (Q || P) betrachten.  Wenn wir zwei Optionen f√ºr den Vergleich von Verteilungen betrachten (im Bild unten), dann ergibt die zweite Option mit Div_KL (P || Q) (aka forward-KL, aka zero avoiding) einen niedrigeren Wert und mit Div_KL (Q || P) Verteilungen aus der ersten Option werden als engere Verteilungen betrachtet.  Tats√§chlich sind die Ergebnisse von VAE und GAN sehr unterschiedlich: Durch den Verlust der Rekonstruktion (L2) wird die Vorw√§rts-KL-Divergenz minimiert (und somit werden alle Modi beibehalten, es entstehen jedoch unscharfe Bilder), und durch das Training mit einem Diskriminator wird die R√ºckw√§rts-KL-Divergenz minimiert (Bilder werden mehr erhalten) klar, aber es besteht die gefahr den mod zu √ºberspringen </p><br><img src="https://habrastorage.org/webt/y9/7k/cd/y97kcdipocff08h4dsoaqly3udq.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Architektur, Verluste und Ausbildung</strong> <br>  Wie bereits erw√§hnt, schlagen die Autoren vor, die Unzul√§nglichkeiten beider Modi zu ber√ºcksichtigen und beide Minimierungen aufgrund der Netzwerkarchitektur (im Bild unten) zu kombinieren, bei der die meisten Codierer- und Diskriminatorgewichte gemeinsam sind (nur vollst√§ndig verbundene K√∂pfe, die die ‚ÄûRealit√§t‚Äú des Bildes vorhersagen, und Parameter sind getrennt) mu, Sigma der VAE-Latentschicht) und auch aufgrund des Trainingsmodus.  Der Encoder und der Generator sind gleich. Die meisten verwendeten Verluste sind ziemlich normal: Beim L_enc-Encoder werden der L2-Fehler der Wiederherstellung und die Kullback-Leibler-Divergenz zu N (0,1) (L_prior) verwendet, der Rest ist gegnerisches Training (wir minimieren die Diskriminatorausgabe beim Trainieren des Diskriminators, maximieren sie) beim Erlernen eines Decoders / Generators), aber es gibt zwei Besonderheiten: </p><br><ul><li><p>  In den gegnerischen trainingsbezogenen F√§llen werden dem Diskriminator 2 verschiedene Arten von generierten Daten zugef√ºhrt: √ºber einen Codierer / Decodierer wiederhergestellt und von einem Generator / Decodierer aus Abtastwerten von N (0,1) generiert. </p><br></li><li><p>  In dem L_dec-Decodiererverlust gibt es ein Element, in dem Merkmale aus der vorletzten Schicht des Diskriminators (wiederum ist dies die letzte gemeinsame Schicht zwischen dem Diskriminator und dem Codierer) f√ºr echte und wiederhergestellte Bilder verglichen werden. </p><br></li></ul><br><img src="https://habrastorage.org/webt/-d/n5/jh/-dn5jh_obvrbb4am3ujm37hd9qs.png" width="500" height="250"><br><p>  <strong>Ergebnisse</strong> <br>  Die Autoren verglichen die Ergebnisse mit VAE und anderen Arbeiten, wobei auf die eine oder andere Weise versucht wurde, VAE und GANs (VAE-GAN, alpha-GAN und AGE von Dmitry Ulyanov und Victor Lempitsky) auf Celeba- und Cifar10-Datens√§tzen zu kombinieren (danke f√ºr nicht Mnist). Fast die besten Indikatoren bez√ºglich des Rekonstruktionsfehlers und der Frechet Inception Distance-Metrik erhalten (vergleicht Aktivierungsstatistiken f√ºr das vortrainierte Netz f√ºr echte und generierte Bilder).  Es wurde gesondert darauf hingewiesen, dass das Ranking nach FID stark von der gew√§hlten Architektur abh√§ngt. Daher ist es besser, das Ensemble der "Experten" (verschiedene Architekturen) zu √ºberpr√ºfen. </p><br><h3 id="3-self-training-with-noisy-student-improves-imagenet-classification">  3. Das Selbsttraining mit Noisy Student verbessert die ImageNet-Klassifizierung </h3><br><p>  Autoren: Qizhe Xie, Eduard Hovy, Minh-Thang Luong und Quoc V. Le (Google Research, Carnegie Mellon University, 2019) <br>  <a href="https://arxiv.org/abs/1911.04252">‚Üí Originalartikel</a> <br>  Autor der Rezension: Alexander Belsky (in slack belskikh) </p><br><p>  Google erhielt absolut beeindruckende 87,4% der Top1 und 98,2% der Top5 der Bildgenauigkeit.  Zayuzali verdeckt pseudo-dimmbare und sehr k√ºhne Netzwerke.  Der Ansatz wurde Noisy Student genannt. </p><br><img src="https://habrastorage.org/webt/es/s8/tm/ess8tmsezy4cjwydsqqfhjxclcu.png"><br><p><br></p><br><p>  <strong>Der Algorithmus</strong> sieht ungef√§hr so ‚Äã‚Äãaus: </p><br><ol><li>  Wir nehmen ein Lehrermodell, wir unterrichten ein normales Bild. </li><li>  Wir generieren weiche Psudo-Beschriftungen f√ºr Bilder aus dem JFT-Datensatz. </li><li>  Wir unterrichten das Sch√ºlermodell auf weichen Pseudoetiketten und greifen ein, sobald wir k√∂nnen: starke Augs, Aussetzer und stochastische Tiefe </li><li>  Nehmen Sie das Sch√ºlermodell, verwenden Sie es als Lehrer in Schritt 2 und wiederholen Sie den Vorgang. Der Datensatz wird wie folgt nach Klassen aufgeteilt.  Zun√§chst haben wir EfficientNet-B0 verwendet, auf das Bild trainiert und seine Vorhersagen auf dem JFT-Datensatz ausgef√ºhrt.  Dann nahmen sie die Beispiele, f√ºr die das maximale Vertrauen √ºber 0,3 liegt.  F√ºr jede Klasse wurden 130K-Bilder aufgenommen (wenn sie nach dem Filtern mit 0,3 Trashhold weniger - dupliziert, wenn mehr - aufgenommen wurden, entsprechend den G√ºltigkeitsbereichen der h√∂chsten Pr√§dikate).  Empfangene 130 Millionen Bilder, doppelte Emissionen, 81 Millionen √ºbrig </li></ol><br><p>  <strong>Architektur:</strong> <br>  EfficeintNet nimmt au√üerdem das Studentenmodell ein viel fetteres Lehrermodell.  Sie haben auch EfficientNet selbst in EfficientNet-L0 / L1 / L2 gescannt, was zu einem L2-Modell mit 480M-Parametern f√ºhrte (Resnet50 verf√ºgt zum Vergleich √ºber 26M-Parameter). </p><br><p>  <strong>Lernprozess:</strong> <br>  Butchesize 2048. Sota Modell L2 lehrte 350 Epochen.  Das gr√∂√üte L2-Modell, das in diesem Modus 3,5 Tage lang auf einem Cloud TPU v3 Pod mit 2048 Kernen getestet wurde. </p><br><p>  <strong>Iteratives Lernverfahren:</strong> <br>  Zun√§chst unterrichteten sie B7 sowohl als Sch√ºler als auch als Lehrer.  Dann unterrichteten sie mit B7 als Lehrer den fetten L0 als Sch√ºler.  Dann wechselten wir ihre Pl√§tze auf diese Weise und kamen zum L2-Modell, das wir am Ende als Lehrer f√ºr dasselbe L2-Modell verwendeten. Ergebnis: sota: mit 2-mal weniger Modellparametern im Vergleich zur vorherigen Zelle (FixRes ResNeXt-101 WSL) 829M Parameter) </p><br><p>  Erhielt auch sehr gute <strong>Ergebnisse</strong> auf ImageNet-A / C / P </p><br><img src="https://habrastorage.org/webt/ht/me/ce/htmeceti9jluqoyj84uqcy2fibo.png"><br><p><br></p><br><h3 id="4-momentum-contrast-for-unsupervised-visual-representation-learning">  4. Impuls-Kontrast f√ºr unbeaufsichtigtes Lernen der visuellen Darstellung </h3><br><p>  Autoren des Artikels: Kaiming He, Haoqi Fan, Yuxin Wu, Heiliger Xie, Ross Girshick (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1911.05722">‚Üí Originalartikel</a> <br>  Autor der Rezension: Arseny Kravchenko (in slack arsenyinfo) </p><br><p>  SotA ist ein unbeaufsichtigter Pretrain f√ºr verschiedene Computer Vision-Aufgaben (von der Klassifizierung bis zur Sch√§tzung dichter Posen), die an verschiedenen Datens√§tzen (imagenet, instagram) und Hauptaufgaben (imagenet, COCO, cityscapes, LVIS usw.) getestet wurden. </p><br><img src="https://habrastorage.org/webt/li/5p/bn/li5pbnzez-zowzce2movvxthfea.png"><br><p><br></p><br><p>  Wie funktioniert ein unbeaufsichtigter Vorlauf?  Wir haben eine Aufgabe, f√ºr die keine Labels ben√∂tigt werden, lernen den Encoder, fixieren ihn und l√∂sen dann das Hauptproblem, indem wir die fehlenden Layer hinzuf√ºgen (linear f√ºr die Klassifizierung, Decoder f√ºr die Segmentierung usw.).  Eine der beliebtesten Aufgaben in dieser Nische ist die Instanzdiskriminierung auf der Grundlage des Kontrastverlusts, d.h.  Wir m√∂chten, dass Merkmale verschiedener Vergr√∂√üerungen desselben Bildes nahe beieinander liegen (zum Beispiel in Bezug auf den Kosinusabstand), und Merkmale verschiedener sind weit entfernt. </p><br><p>  Sie k√∂nnen versuchen, diese Aufgabe durchg√§ngig zu vermitteln, aber vieles h√§ngt von der Gr√∂√üe des Stapels ab: Die Qualit√§t h√§ngt in hohem Ma√üe von der Vielfalt der Beispiele im Stapel ab.  Experimente zeigen, dass sich die Endqualit√§t mit zunehmender Chargengr√∂√üe verbessert.  Aber die Partie ist Moskau etwas √§hnlich: Es ist kein Gummi, es wird nicht lange funktionieren, um es in der Stirn zu vergr√∂√üern. </p><br><p>  Fr√ºhere Dudes, die sich in der N√§he von Zellen befanden, haben eine Speicherbank durcheinander gebracht: Merkmale fr√ºherer Stapel wurden separat im Speicher gespeichert und auch zur Erzeugung negativer, d.h.  ungleiche Proben.  Dies hat teilweise geholfen, aber auch unvollkommen: W√§hrend des Trainings √§ndern sich die Encodergewichte und alte Funktionen gehen verloren. </p><br><p>  Schlie√ülich die Idee des Artikels: </p><br><ol><li>  Ersetzen wir eine einfache Speicherbank durch eine Warteschlange, in der ziemlich neue Funktionen enthalten sind. </li><li>  Wir werden zwei Versionen des Encoders behalten: eine wird f√ºr die aktuelle Charge verwendet und ist trainiert, die andere ist stabiler, ihre Gewichte werden gegen√ºber der ersten Version aktualisiert, jedoch mit gro√üer Dynamik. </li><li>  Merkmale des Stapels werden als erster Encoder betrachtet, Merkmale in der Warteschlange werden vom zweiten Encoder gez√§hlt. </li></ol><br><p>  Dieser Ansatz erm√∂glicht eine Ann√§herung an die Qualit√§t des End-to-End-Trainings, erzielt jedoch dank der langen Warteschlange die potenziellen Ergebnisse einer unrealistisch gro√üen Charge.  Auf diese Weise erhalten Sie coole Metriken f√ºr verschiedene Aufgaben, einschlie√ülich  In einigen Gegenden ist es sogar ein bisschen besser als das traditionelle, √ºberwachte Image auf dem Imaginet. </p><br><h3 id="5-benchmarking-neural-network-robustness-to-common-corruptions-and-perturbations">  5. Benchmarking der Robustheit neuronaler Netze gegen√ºber h√§ufigen St√∂rungen und St√∂rungen </h3><br><p>  Autoren: Dan Hendrycks, Thomas Dietterich (Universit√§t von Kalifornien, Oregon State University, 2019) <br>  <a href="https://arxiv.org/abs/1903.12261">‚Üí Originalartikel</a> <br>  Autor der Rezension: Vladimir Iglovikov (in ternaus slack) </p><br><img src="https://habrastorage.org/webt/fy/p3/zn/fyp3znumddg9tstty7aukiiuvwg.png" width="500" height="250"><br><p><br></p><br><p>  Es wurde am ICLR 2019 angenommen und so wie ich es verstehe, ist dies eine der DL-Arbeiten, die in keinem Netzwerk geschult wurden. </p><br><p>  Die Aufgabe war wie folgt - aber lassen Sie uns eine Erweiterung f√ºr die ImageNet-Validierung versuchen, aber wir werden die ungebrochene trainieren.  Dar√ºber hinaus haben wir im Gegensatz zu adevrsarial nicht die Aufgabe, die Transformationen klein und f√ºr das Auge unsichtbar zu machen. </p><br><p>  <strong>Was wurde getan:</strong> </p><br><ol><li>  Eine Reihe von Erweiterungen wurde ausgew√§hlt.  Die Autoren sagen, dass dies am h√§ufigsten vorkommt, aber meiner Meinung nach l√ºgen sie. <br>  Sie verwendeten: GaussianNoise, ISONoise, Downscale, Defocus, MotionBlur, ZoomBlur, FrostedGlassBlur, JpegCompression, Schnee, Nebel, Regen, Elastic Transoform usw. </li><li>  Alle diese Transformationen wurden auf die ImageNet-Validierung angewendet.  Der resultierende Datensatz wurde ImageNet-C genannt </li><li>  Es wurde auch eine Variante mit dem Namen ImageNet-P vorgeschlagen, bei der auf jedes Bild Transformationss√§tze mit unterschiedlichen St√§rken angewendet wurden. </li><li>  Eine Metrik wurde vorgeschlagen, um die Stabilit√§t des Modells zu bewerten. </li><li>  Im Rahmen dieser Metrik wurden mehrere Modelle bewertet: AlexNet, VGG-11, VGG-19, Resnet-50, Resnet-18, VGG-19 + BN usw </li></ol><br><p>  <strong>Schlussfolgerungen:</strong> </p><br><ol><li>  Je st√§rker die Augmentation ist, desto mehr leidet die Genauigkeit des Modells.  : capitan_obvious: </li><li>  Je komplexer das Modell, desto stabiler. </li><li>  Das Auftragen von CLAHE in Bildern vor dem Ableiten hilft ein wenig. </li><li>  Feature-Aggregationsbl√∂cke wie DenseNet oder Resnext helfen. </li><li>  Netzwerke mit mehreren Skalen sind stabiler.  Ein Beispiel f√ºr solche Netzwerke ist MSDNet, Multigrid (von solchen Netzwerken habe ich noch nichts geh√∂rt). </li></ol><br><p>  <a href="https://github.com/hendrycks/robustness">Code</a> </p><br><h3 id="6-distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter">  6. DistilBERT, eine destillierte Version von BERT: kleiner, schneller, billiger und leichter </h3><br><p>  Autoren: Victor Sanh, Lysandre-Deb√ºt, Julien Chaumond, Thomas Wolf (Hugging Face, 2019) <br>  <a href="https://arxiv.org/abs/1910.01108">‚Üí Originalartikel</a> <br>  Autor der Rezension: Yuri Kashnitsky (in yorko slack) </p><br><p>  Der Artikel ist kurz, es ist sehr einfach zu lesen.  Am Anfang ein paar allgemeine Worte zum Wettr√ºsten in der NLP und zum √∂kologischen Fu√üabdruck.  Weiter die Idee der Destillation (und Hinton hat es auch hier getan). Bei der Aufgabe der Sprachmodellierung prognostizieren wir standardm√§√üig das n√§chste Wort im Kontext.  Normalerweise vergleicht der Kreuzentropieverlust den Vektor der vorhergesagten Wahrscheinlichkeiten (die L√§nge des gesamten W√∂rterbuchs) mit einem Bin√§rvektor, bei dem nur eine Einheit das reale Wort an einer bestimmten Stelle im Trainingssatz anzeigt.  Das hei√üt, die zweite, dritte usw.  Das Wort, das das Modell f√ºr angemessen h√§lt, wird vom Verlust ignoriert.  In dem Artikel wird ein Beispiel gegeben: "Ich denke, dies ist der Anfang einer sch√∂nen [MASKE]", anstatt [MASKE] will BERT zuerst den Alltag oder das Leben ersetzen, aber die Worte, die durch die zuk√ºnftige Wahrscheinlichkeit, Zukunft, Geschichte und Welt vorhergesagt werden, sind auch gut.  K√∂nnen wir irgendwie die Tatsache ber√ºcksichtigen, dass das Modell eine gute Wahrscheinlichkeitsverteilung erzeugt?  Grob gesagt, um das Modell f√ºr die Tatsache zu belohnen, dass es keine Murdock, Toleranz, Mutterschaft und andere wenige passende W√∂rter im oberen Bereich gibt. </p><br><img src="https://habrastorage.org/webt/wg/xd/rx/wgxdrxth-vykjuhkxszaxakxdke.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Die Idee der Destillation</strong> <br>  Die Idee eines bestimmten Lehrer-Sch√ºler-Modells ist, dass wir ein gro√ües Lehrermodell ( <strong>Lehrer</strong> , BERT) und ein kleineres Modell ( <strong>Sch√ºler</strong> , DistilBERT) haben, die "Wissen" aus dem Lehrermodell vermitteln.  Das Sch√ºlermodell optimiert den Destillationsverlust, n√§mlich den f√ºr die Wahrscheinlichkeitsverteilungen von Lehrer und Sch√ºler definierten Kreuzentropieverlust: L = Œ£ t_i * log (s_i).  Das hei√üt, f√ºr ein bestimmtes Wort, das durch das [MASK] -Symbol gel√∂scht wurde und das durch den Kontext vorhergesagt werden muss, vergleichen wir zwei Wahrscheinlichkeitsverteilungen des Auftretens jedes Wortes aus dem W√∂rterbuch: {t_i} und {s_i} - vorhergesagt durch das Lehrermodell bzw. das Lehrermodell Student.  Somit wird ein reichhaltiges Trainingssignal erhalten - das Studentenmodell f√ºr jedes Wort empf√§ngt ein Signal, das nicht nur durch Vergleichen seines Vorhersagevektors mit dem realen Wort in der Trainingsprobe berechnet wird, sondern durch Vergleichen mit dem Vorhersagevektor des Lehrermodells. </p><br><p>  <strong>DistilBERT-Modell</strong> <br>  Die ganze Idee ist, dass der Sch√ºler ein kleineres Modell als der Lehrer ist.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DistilBERT ist also im Allgemeinen das gleiche BERT, aber die H√§lfte der Schichten wurde weggeworfen. </font><font style="vertical-align: inherit;">Sie warfen auch tokenartige Einbettungen und Pooler, es gibt jedoch keine Details dar√ºber. </font><font style="vertical-align: inherit;">Aber das Wichtigste ist, dass DistilBERT 40% weniger ist - 66 Millionen Parameter im Vergleich zu 110 Millionen bei BERT</font></font></p><br><p> <strong> DistilBERT</strong> <br>  DistilBERT  distillation loss     ‚Äî   masked language modeling loss,    BERT   cosine embedding loss ‚Äî           ( ,  ,      "" -   ,  "" ). :   ablation studies, ,   masked language modeling loss,    , ..    distillation loss  cosine embedding loss.   ,    RoBERTa   next sentence prediction   dynamic masking. </p><br><p>      ,  BERT (eng. wiki + Toronto Book Corpus) 90   8 V100 (16 GB).   RoBERTa    1024 V100 (32 GB). </p><br><p>  <strong>Ergebnisse</strong> <br>     BERT ‚Äî "it performed surprisingly well",        DistilBERT ‚Äî  GLUE  surprisingly well ‚Äî     5  9   ,  BERT ,     SQuAD  IMDb ‚Äî  .   ,    DistilBERT   60% ‚Äî  . </p><br><p> <strong> </strong> <br>   DistilBERT  iPhone 7 Plus.   70% ,  BERT-base (  ),     200 .  ablation studies:     ,      ‚Äî distillation loss  cosine embedding loss. </p><br><p>      3          ,  DistilBERT ‚Äî     BERT,   40%  ,   60%    "97%   "    BERT (        ML). </p><br><p> -,      BERT,     . </p><br><p> <strong> :</strong> <br> <a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"> Jay Alammar</a> <br> <a href="https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews">  , DistilBERT + Catalyst:   </a> </p><br><h3 id="7-plug-and-play-language-models-a-simple-approach-to-controlled-text-generation"> 7. Plug and Play Language Models: A Simple Approach To Controlled Text Generation </h3><br><p>  : Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu (Uber AI, Caltech, HKUST, 2019) <br> <a href="https://arxiv.org/abs/1912.02164">‚Üí  </a> <br>  :   (  Egor Timofeev) </p><br><p>               . ,           / /      (, .  <a href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a> ).     ,         ,     , ,      . </p><br><p> <strong></strong> <br>       (   x_prev    ),        p(x),      conditional LM (,    ‚Äî CTRL)    p(x|a). </p><br><p>       : p(x|a) ‚àù p(x)p(a|x),  p(x)  ,    (, GPT2),  p(a|x) ‚Äî     .       ‚Äî       ,   /.     ,       ,    . </p><br><p>   <strong></strong> : </p><br><ol><li>    ,  log(p(a|x)) ( ).     hidden state  . </li><li>      ,  hidden state      log(p(a|x)).   H_new. </li><li>   :           p(x).    ,    : -,        KL(H, H_new),  -,  .. post-norm fusion ( <a href="https://arxiv.org/pdf/1809.00125.pdf">https://arxiv.org/pdf/1809.00125.pdf</a> ),   p(x)   non conditional LM  ,     . </li><li>      . </li></ol><br><p>           ,  p(a|x). </p><br><p>  <strong>Ergebnisse</strong> <br>       ,   -            topic relevance.    :  (GPT2) &lt;  +    &lt;&lt;       &lt;    + . </p><br><img src="https://habrastorage.org/webt/jx/zm/ye/jxzmyeaubsu6wtcp2np1zda32tk.png" width="500" height="250"><br><p><br></p><br><h3 id="8-deep-salience-representation-for-f0-estimation-in-polyphonic-music"> 8. Deep Salience Representation for F0 Estimation in Polyphonic Music </h3><br><p>  : Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, Juan Pablo Bello ( New York University, USA, 2017) <br> <a href="https://bmcfee.github.io/papers/ismir2017_salience.pdf">‚Üí  </a> <br>  :   (  nglaz) </p><br><p>    .  ,                .        ,     ‚Äì    .       ,   -   .   constant-Q ,          (      )          . </p><br><img src="https://habrastorage.org/webt/7l/6y/c0/7l6yc0irzsti2kbks7avmvrb7w4.png" width="500" height="250"><br><p>     .  constant-Q     -   f_min  -    F.    f_min   f_min * h,      ,    ,     .    h   {0.5, 1, 2, 3, 4, 5},        .   ,          3- ,        2-  3-    (, ,  ). ,    ,     ,    ,   (0.5f, f, 2f, 3f, 4f, 5f),    .     ( 55)      .         ,           ,  dilated-. </p><br><p>  , ,     constant-Q       F,           . </p><br><p>    F0 estimation,    ,          .  2017 ,   ,   state-of-the-art.           ,      . </p><br><h3 id="9-analyzing-and-improving-the-image-quality-of-stylegan"> 9. Analyzing and Improving the Image Quality of StyleGAN </h3><br><p>  : Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila (NVIDIA, 2019) <br> <a href="http://arxiv.org/abs/1912.04958">‚Üí  </a> <br>  :   (  shiron8bit) </p><br><p> GAN-      ,     ,         .     ,   ,      ,   ,    ( FID)   : </p><br><ul><li>   droplet-like  (    / ),  AdaIN. </li><li>   ,   ProGAN-    /       end-to-end     MSG-GAN.     ,        /,            . </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pfadl√§ngen-Regularisierung hinzugef√ºgt. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sie schlugen ein Verfahren zur Erkennung erzeugter Bilder vor: Wir finden die Projektion des Bildes im latenten Raum W, wenn das rekonstruierte Bild nahe am Original liegt, wird es h√∂chstwahrscheinlich durch stylegan2 erzeugt. </font></font></li></ul><br><img src="https://habrastorage.org/webt/f6/v3/7p/f6v37pcy3wcpw0epu5rz1-r24qk.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Tr√∂pfchenartefakte und AdaIN</strong> <br>  Die Autoren des Artikels sprechen sich mit folgendem Argument gegen die Verwendung des AdaIN-Layers aus: adain normalisiert jede Feature-Map und zerst√∂rt so Informationen √ºber die relativen Gr√∂√üenwerte. Droplets sind ein Versuch des Generators, diese Informationen auf eine andere Weise zu √ºbertragen.  Als Option zur Schw√§chung von AdaIN wurde Folgendes vorgeschlagen: Wir werden alle Skalierungen (Modulation / Demodulation) direkt in der Faltung durchf√ºhren, basierend auf dem von Block A kommenden Stil und dem Offset des ausgehenden Signals (anstelle von mu (y) / y_ {b, i} in AdaIN). Lassen Sie Block B Rauschen transformieren.  Diese Innovation erm√∂glichte es gleichzeitig, das Training unter den gleichen Bedingungen zu beschleunigen. </p><br><p>  <strong>Ausfall von ProGAN</strong> <br>  In dem Artikel √ºber MSG-GAN wurde vorgeschlagen, Sprungverbindungen zu verwenden, um passende Generatorbl√∂cke und Diskriminatorbl√∂cke durch Aufl√∂sung zu verbinden.  Die Autoren von Stylegan entwickelten diese Idee, indem sie die Ausgaben von Generatorbl√∂cken aller Aufl√∂sungen (mit Aufw√§rtsabtastung) aufsummierten und die entsprechende abw√§rtsabtastende Version des Bildes dem Eingang jedes Diskriminatorblocks zuf√ºhrten.  Es wurde vorgeschlagen, Restbl√∂cke als zweite Option zu verwenden, w√§hrend das √úberspringen von Verbindungen im Generator und von Restbl√∂cken im Diskriminator die besten Ergebnisse zeigten (der Diskriminator √§hnelt LAPGAN, aber ohne Diskriminatoren f√ºr jede Aufl√∂sung werden Feature-Maps weitergeleitet) Wie bei ProGAN tragen in den ersten Iterationen die Teile des Rasters, die f√ºr niedrigere Aufl√∂sungen verantwortlich sind, und das Gesamtbild st√§rker bei, und dann wird die Betonung auf kleine Details verlagert. </p><br><p>  <strong>Pfadl√§ngen-Regularisierung</strong> <br>  In Anbetracht der Tatsache, dass niedrige FID-Werte nicht immer qualitativ hochwertige Bilder liefern und auch eine Korrelation zwischen der Bildqualit√§t und der PPL-Metrik (Perceptual Path Length - urspr√ºnglich der Unterschied zwischen vgg-Merkmalen von Bildern mit kleinen Schritten in Z, aber der Unterschied wurde durch LPIPS ersetzt), schlugen die Autoren Path vor L√§ngenregulierung, um die Funktionalit√§t zu minimieren </p><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.258ex" height="2.78ex" viewBox="0 -883.9 9583.3 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-4A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-54" x="929" y="488"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-77" x="785" y="-212"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-79" x="1255" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMAIN-3D" x="2030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-6E" x="3336" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-61" x="3937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-62" x="4466" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-6C" x="4896" y="0"></use><g transform="translate(5194,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-77" x="748" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMAIN-28" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-67" x="6720" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMAIN-28" x="7200" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-77" x="7590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMAIN-29" x="8306" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMATHI-79" x="8696" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700301&amp;usg=ALkJrhjW5tH_Qulcw8MDMXZcEVmLdPi9Dw#MJMAIN-29" x="9193" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></mi></msubsup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><msub><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></mi></msub><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> J ^ T_w y = \ nabla_w (g (w) y) </script></p><br>  wo g der Generator selbst ist, ist J_w der Jacobian in latenten Raumvariablen.  Gleichzeitig k√∂nnen die Jacobi-Berechnungen √ºber Backprop durchgef√ºhrt werden. Um die Berechnungen zu vereinfachen, kann der Regularizer nur f√ºr jeweils 16 Chargen gez√§hlt werden.  Die Zahl a wird als exponentielles gleitendes Mittel der Jacobi-Norm berechnet. Die Verwendung der Pfadl√§ngen-Regularisierung erm√∂glicht eine "glattere" Interpolation des verborgenen Raums W, wodurch zus√§tzlich zur Verbesserung der Bildqualit√§t die Reversibilit√§t verbessert wird (d. H. Es wird w gefunden, das nach Durchlaufen des Generators ein bestimmtes Bild ergibt) er√∂ffnet auch Perspektiven in Bezug auf Animation und Interpolation zwischen Schl√ºsselbildern (in der neuen Architektur sollten zwischen Projektionen √§hnlicher Bilder Punkte vorhanden sein, die f√ºr das Schlie√üen von Bildern verantwortlich sind)  I).  Die Einf√ºhrung dieser Regularisierung hat auch dazu beigetragen, die Erkennung der von dieser Architektur erzeugten Bilder zu vereinfachen. <br><p>  Die Trainingszeit f√ºr 8 GPUs bei einer Aufl√∂sung von 1024 * 1024 betrug 2 bis 9 Tage f√ºr verschiedene Konfigurationen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de485122/">https://habr.com/ru/post/de485122/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de485104/index.html">Erstellen eines universellen RFID-Schl√ºssels f√ºr Sprechanlagen</a></li>
<li><a href="../de485108/index.html">Statistik der zertifizierten PMI-Spezialisten in Russland am 10.01.2020</a></li>
<li><a href="../de485110/index.html">Meine Erfahrung mit effektiver Fernarbeit</a></li>
<li><a href="../de485118/index.html">Clean Code von Robert Martin. Auszug. Wie schreibe ich klaren und sch√∂nen Code?</a></li>
<li><a href="../de485120/index.html">F√ºgen Sie unserer Anwendung eine sehr schnelle JSON-API hinzu.</a></li>
<li><a href="../de485124/index.html">Reine Tests in PHP und PHPUnit</a></li>
<li><a href="../de485126/index.html">Mu-mu, woof-woof, quack-quack: Evolution der akustischen Kommunikation</a></li>
<li><a href="../de485128/index.html">Sparen Sie bei Mikrotik CHR-Lizenzen</a></li>
<li><a href="../de485132/index.html">Nehmen Sie am Google Play Indie Games Festival teil</a></li>
<li><a href="../de485136/index.html">Istio Tracing and Monitoring: Microservices und das Unsicherheitsprinzip</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>