<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëãüèª üßí ü§πüèæ 7 anos de hype de redes neurais em gr√°ficos e perspectivas inspiradoras do Deep Learning 2020 üòñ ‚òëÔ∏è ‚öñÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O ano novo est√° se aproximando, os anos 2010 terminar√£o em breve, dando ao mundo o renascimento sensacional das redes neurais. Fiquei perturbado e pri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>7 anos de hype de redes neurais em gr√°ficos e perspectivas inspiradoras do Deep Learning 2020</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481844/"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/0a2/971/f4c0a297160b156ef22379a9555bd5fd.png"><br><br>  O ano novo est√° se aproximando, os anos 2010 terminar√£o em breve, dando ao mundo o renascimento sensacional das redes neurais.  Fiquei perturbado <s>e privado de sono com um</s> simples pensamento: ‚ÄúComo algu√©m pode estimar retrospectivamente a velocidade do desenvolvimento de redes neurais?‚Äù. Para ‚ÄúQuem conhece o passado conhece o futuro‚Äù.  Com que rapidez decolaram diferentes algoritmos?  Como se pode avaliar a velocidade do progresso nessa √°rea e estimar a velocidade do progresso na pr√≥xima d√©cada? <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/cad/e4a/11b/cade4a11b5be7a57182eddbf3765ba4c.png"><br><br>  √â claro que voc√™ pode calcular aproximadamente o n√∫mero de artigos em diferentes √°reas.  O m√©todo n√£o √© ideal, √© necess√°rio levar em considera√ß√£o subdom√≠nios, mas, em geral, voc√™ pode tentar.  Eu dou uma id√©ia, no <a href="https://scholar.google.com/scholar%3Fhl%3Den%26as_sdt%3D0%252C5%26q%3Dbatch%2Bnormalization" rel="nofollow">Google Scholar (BatchNorm)</a> √© bem real!  Voc√™ pode considerar novos conjuntos de dados, novos cursos.  Seu humilde servo, tendo <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">examinado</a> v√°rias op√ß√µes, <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">optou</a> pelo <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">Google Trends (BatchNorm)</a> . <br><br>  Meus colegas e eu recebemos solicita√ß√µes das principais tecnologias de ML / DL, por exemplo, <a href="https://en.wikipedia.org/wiki/Batch_normalization" rel="nofollow">Normaliza√ß√£o de lote</a> , como na figura acima, adicionamos a data de publica√ß√£o do artigo com um ponto e obtivemos uma linha do tempo para a popularidade do t√≥pico.  Mas n√£o para todos, o <s>caminho est√° repleto de rosas, a</s> decolagem √© t√£o √≥bvia e bonita, como o batnorm.  Alguns termos, como regulariza√ß√£o ou pular conex√µes, n√£o puderam ser constru√≠dos devido ao ru√≠do de dados.  Mas, em geral, conseguimos coletar tend√™ncias. <br><br>  Quem se importa com o que aconteceu - bem-vindo ao corte! <br><a name="habracut"></a><br><h1>  Em vez de introduzir ou reconhecer imagens </h1><br>  Ent√£o!  Os dados iniciais eram bastante barulhentos, √†s vezes havia picos acentuados. <img src="https://habrastorage.org/webt/wn/ej/-p/wnej-pvivjixvw84l9sibvriuno.png"><br>  <i>Fonte: <a href="https://twitter.com/karpathy/status/849338608297406465" rel="nofollow">Andrei Karpaty twitter - os alunos ficam nos corredores de uma enorme audi√™ncia para ouvir uma palestra sobre redes neurais convolucionais</a></i> <br><br>  Convencionalmente, foi o suficiente para <a href="https://en.wikipedia.org/wiki/Andrej_Karpathy" rel="nofollow">Andrey Karpaty</a> dar uma palestra do lend√°rio <a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n: Redes Neurais Convolucionais para Reconhecimento Visual</a> para 750 pessoas com a populariza√ß√£o do conceito de como est√° <a href="http://cs231n.stanford.edu/" rel="nofollow">ocorrendo</a> um pico agudo.  Portanto, os dados foram suavizados com um simples <a href="http://nghiaho.com/%3Fp%3D1159" rel="nofollow">filtro de caixa</a> (todas as sa√≠das suavizadas s√£o marcadas como Suavizadas no eixo).  Como est√°vamos interessados ‚Äã‚Äãem comparar a taxa de crescimento da popularidade - ap√≥s a suaviza√ß√£o, todos os dados foram normalizados.  Acabou bem engra√ßado.  Aqui est√° um gr√°fico das principais arquiteturas concorrentes no ImageNet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fa/4d6/f7b/0fa4d6f7bad7b232ec50e2f0bde6559b.png"><br>  <i>Fonte: doravante - os c√°lculos do autor de acordo com o Google Trends</i> <br><br>  O gr√°fico mostra muito claramente que, ap√≥s a sensacional publica√ß√£o da <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">AlexNet</a> , que produziu o mingau do hype atual das redes neurais no final de 2012, por quase dois anos estava fervendo, <s>ao contr√°rio das afirma√ß√µes do heap,</s> apenas um c√≠rculo relativamente restrito de especialistas <s>se uniu</s> .  O t√≥pico foi para o p√∫blico em geral apenas no inverno de 2014-2015.  Preste aten√ß√£o em como a programa√ß√£o se torna peri√≥dica a partir de 2017: novos picos a cada primavera.  <s>Na psiquiatria, isso √© chamado de exacerba√ß√£o da primavera ...</s> Esse √© um sinal claro de que agora o termo √© usado principalmente pelos estudantes e, em m√©dia, o interesse no AlexNet diminui em compara√ß√£o ao pico de popularidade. <br><br>  Al√©m disso, no segundo semestre de 2014, a <a href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="nofollow">VGG</a> apareceu.  A prop√≥sito, a <a href="" rel="nofollow">VGG</a> foi coautora do supervisor de <a href="" rel="nofollow">estudos da</a> minha ex-aluna <a href="https://scholar.google.com/citations%3Fuser%3DL7lMQkQAAAAJ%26hl%3Den" rel="nofollow">Karen Simonyan</a> , agora trabalhando no Google DeepMind ( <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> , <a href="https://en.wikipedia.org/wiki/AlphaZero" rel="nofollow">AlphaZero</a> , etc.).  Enquanto estudava na Universidade Estadual de Moscou, no terceiro ano, Karen implementou um bom <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">algoritmo de estimativa de movimento</a> , que serve de refer√™ncia para estudantes de 2 anos por 12 anos.  Al√©m disso, as tarefas s√£o um tanto indescritivelmente semelhantes.  Compare: <br><br><img width="50%" src="https://habrastorage.org/webt/mo/w0/9w/mow09w8lyvaxyw3b1ztxm2wxtxe.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/01a/c45/c94/01ac45c944275e9045557c9d453ff938.png"><br>  <i>Fonte: Fun√ß√£o de perda para tarefas de estimativa de movimento (materiais do autor) e <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">VGG-56</a></i> <br><br>  √Ä esquerda, voc√™ precisa encontrar o ponto mais profundo em uma superf√≠cie n√£o trivial, dependendo dos dados de entrada para o n√∫mero m√≠nimo de medi√ß√µes (muitos m√≠nimos locais s√£o poss√≠veis), e √† direita, voc√™ precisa encontrar um ponto mais baixo com c√°lculos m√≠nimos (e tamb√©m v√°rios m√≠nimos locais, e a superf√≠cie tamb√©m depende dos dados) .  √Ä esquerda, temos o vetor de movimento previsto e, √† direita, a rede treinada.  E a diferen√ßa √© que, √† esquerda, h√° apenas uma medida impl√≠cita do espa√ßo de cores, e √† direita h√° um par de medidas de centenas de milh√µes.  Bem, a complexidade computacional √† direita √© cerca de 12 ordens de magnitude (!) Superior.  Um pouco assim ... Mas o segundo ano, mesmo com uma tarefa simples, oscila assim ... [cortado pela censura].  E o n√≠vel de programa√ß√£o dos alunos de ontem por raz√µes desconhecidas nos √∫ltimos 15 anos caiu acentuadamente.  Eles precisam dizer: "Voc√™ far√° bem, eles o levar√£o ao DeepMind!"  Pode-se dizer "invente o VGG", mas "eles levar√£o para o DeepMind" por algum motivo motiva melhor.  Obviamente, este √© um an√°logo avan√ßado moderno do cl√°ssico "Voc√™ comer√° s√™mola, se tornar√° um astronauta!".  No entanto, no nosso caso, se contarmos o n√∫mero de crian√ßas no pa√≠s e o tamanho do corpo de cosmonautas, as chances s√£o milh√µes de vezes maiores, porque dois de n√≥s j√° trabalhamos no DeepMind em nosso laborat√≥rio. <br><br>  Em seguida, foi a <a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="nofollow">ResNet</a> , quebrando a barreira do n√∫mero de camadas e come√ßando a decolar ap√≥s seis meses.  E, finalmente, o DenseNet, que surgiu no come√ßo do hype <a href="https://towardsdatascience.com/densenet-2810936aeebb" rel="nofollow">,</a> decolou quase imediatamente, ainda mais do que o ResNet. <br><br>  Se estamos falando de popularidade, gostaria de acrescentar algumas palavras sobre as caracter√≠sticas da rede e o desempenho, das quais a popularidade tamb√©m depende.  Se voc√™ observar como a classe <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">ImageNet</a> √© prevista, dependendo do n√∫mero de opera√ß√µes na rede, o layout ser√° assim (mais alto e √† esquerda - melhor): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c08/f56/f11/c08f56f11908ffb9f78a0d5d66a71342.png"><br>  <i>Fonte: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">An√°lise comparativa das arquiteturas de redes neurais profundas representativas</a></i> <br><br>  O tipo AlexNet n√£o √© mais um bolo e eles governam redes baseadas no ResNet.  No entanto, se voc√™ observar a avalia√ß√£o pr√°tica do <abbr title="n√∫mero de quadros processados ‚Äã‚Äãpor segundo">FPS</abbr> mais perto do meu cora√ß√£o, poder√° ver claramente que o VGG est√° mais pr√≥ximo do ideal aqui e, em geral, o alinhamento muda visivelmente.  Incluindo o AlexNet inesperadamente no envelope ideal para Pareto (a escala horizontal √© logar√≠tmica, melhor acima e √† direita): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/3b9/412/6423b941235280be5ec1182b91cf6d6e.png"><br>  <i>Fonte: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">An√°lise comparativa das arquiteturas de redes neurais profundas representativas</a></i> <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  Nos pr√≥ximos anos, o alinhamento de arquiteturas com alta probabilidade mudar√° significativamente devido ao <a href="https://habr.com/post/455353/">progresso dos aceleradores de redes neurais</a> , quando algumas arquiteturas v√£o para cestas e outras decolam repentinamente, simplesmente porque √© melhor optar por um novo hardware.  Por exemplo, <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">no artigo mencionado</a> , √© feita uma compara√ß√£o no NVIDIA Titan X Pascal e na placa NVIDIA Jetson TX1, e o layout muda visivelmente.  Ao mesmo tempo, o progresso do TPU, NPU e outros est√° apenas come√ßando. <br></li><li>  Como profissional, n√£o posso deixar de notar que a compara√ß√£o no ImageNet √© feita por padr√£o no ImageNet-1k, e n√£o no ImageNet-22k, simplesmente porque a maioria treina suas redes no ImageNet-1k, onde h√° 22 vezes menos aulas (isso mais f√°cil e r√°pido).  A mudan√ßa para o ImageNet-22k, que √© mais relevante para muitas aplica√ß√µes pr√°ticas, tamb√©m mudar√° o alinhamento (para aqueles que s√£o agu√ßados por 1k - muito). <br></li></ul><br><h1>  Mais profundo em tecnologia e arquitetura </h1><br>  No entanto, de volta √† tecnologia.  O termo <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="nofollow">Dropout</a> como palavra de pesquisa √© bastante barulhento, mas o crescimento em cinco vezes est√° claramente associado a redes neurais.  E o decl√≠nio no interesse √© mais prov√°vel com uma <a href="https://patents.google.com/patent/US9406017B2/en" rel="nofollow">patente do Google</a> e o advento de novos m√©todos.  Observe que cerca de um ano e meio passou da publica√ß√£o do <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">artigo original</a> para o aumento do interesse no m√©todo: <br><img src="https://habrastorage.org/getpro/habr/post_images/162/fca/4e4/162fca4e4b15574f66f3df40f4230090.png"><br><br>  No entanto, se falarmos do per√≠odo anterior ao aumento da popularidade, em DL, um dos primeiros lugares √© claramente ocupado pelas <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">redes recorrentes</a> e pelo <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C" rel="nofollow">LSTM</a> : <br><img src="https://habrastorage.org/getpro/habr/post_images/9bf/423/789/9bf423789d111f7b95352dd9a7b062c1.png"><br><br>  Muito 20 anos antes do atual pico de popularidade, e agora, com seu uso, a tradu√ß√£o autom√°tica e a an√°lise do genoma foram drasticamente aprimoradas e, em um futuro pr√≥ximo (se voc√™ tirar da minha √°rea), o tr√°fego do YouTube, do Netflix com a mesma qualidade visual cair√° duas vezes.  Se voc√™ aprender corretamente as li√ß√µes da hist√≥ria, √© √≥bvio que parte das id√©ias do atual eixo de artigos ‚Äúdecolar√°‚Äù somente ap√≥s 20 anos.  Leve um estilo de vida saud√°vel, cuide-se e voc√™ o ver√° pessoalmente! <br><br>  Agora mais perto do hype prometido.  <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="nofollow">Os GANs</a> decolaram assim: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/80c/272/c0d/80c272c0d6207b79b2736559480aea3d.png"></a> <br><br>  Pode-se ver claramente que por quase um ano houve sil√™ncio total e somente em 2016, ap√≥s dois anos, um aumento acentuado come√ßou (os resultados foram visivelmente aprimorados).  Essa decolagem, um ano depois, deu o sensacional DeepFake, que, no entanto, tamb√©m decolou 1,5 anos.  Ou seja, mesmo tecnologias muito promissoras exigem uma quantidade significativa de tempo para passar de uma ideia a aplicativos que todos possam usar. <br><br>  Se voc√™ observar as imagens que a GAN gerou no <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">artigo original</a> e o que pode ser constru√≠do com o <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="nofollow">StyleGAN</a> , torna-se bastante √≥bvio por que houve tanto sil√™ncio.  Em 2014, apenas especialistas puderam avaliar o qu√£o legal foi - criar, em ess√™ncia, outra rede como uma fun√ß√£o de perda e trein√°-los juntos.  E em 2019, todo aluno pode apreciar o qu√£o legal isso √© (sem entender completamente como isso √© feito): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ee/6ca/51a/5ee6ca51ac69315327c2dc31f8f80c15.png"><br><br>  Atualmente, <a href="https://www.eff.org/ai/metrics" rel="nofollow">existem muitos</a> problemas diferentes resolvidos com sucesso pelas redes neurais. Voc√™ pode pegar as melhores redes e criar gr√°ficos de popularidade para cada dire√ß√£o, lidar com ru√≠dos e picos de consultas de pesquisa etc.  Para n√£o espalhar meus pensamentos sobre a √°rvore, encerraremos esta sele√ß√£o com o t√≥pico algoritmos de segmenta√ß√£o, onde as id√©ias de <a href="https://medium.com/%40sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="nofollow">convolu√ß√£o atrosa / dilatada</a> e <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" rel="nofollow">ASPP</a> no √∫ltimo ano e meio foram disparadas <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">por</a> si mesmas <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">no benchmark do algoritmo</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f52/6c8/6d5/f526c86d5b81454b2a62f77193be0335.png"><br>  Tamb√©m deve ser observado que, se o <a href="https://arxiv.org/pdf/1412.7062.pdf" rel="nofollow">DeepLabv1</a> mais de um ano "aguardava" o aumento da popularidade, o <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">DeepLabv2</a> decolava em um ano e o <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">DeepLabv3</a> quase que imediatamente.  I.e.  em geral, podemos falar sobre acelerar o crescimento do interesse ao longo do tempo (bem, ou acelerar o crescimento do interesse em tecnologias de autores respeit√°veis). <br><br>  Tudo isso levou √† cria√ß√£o do seguinte problema global - um aumento explosivo no n√∫mero de publica√ß√µes sobre o tema: <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/99b/bd7/66e/99bbd766ebd801adb403efa6ee5efb4e.png"><br>  <i>Fonte: <a href="http://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" rel="nofollow">Muitos documentos de aprendizado de m√°quina?</a></i> <br><br>  Este ano, temos cerca de 150 a 200 artigos por dia, pois nem todos s√£o publicados no arXiv-e.  Hoje, ler artigos mesmo em sua pr√≥pria sub√°rea √© completamente imposs√≠vel.  Como resultado, muitas id√©ias interessantes certamente ser√£o enterradas sob os escombros de novas publica√ß√µes, o que afetar√° o momento de sua "decolagem".  No entanto, tamb√©m o aumento <i>explosivo</i> do n√∫mero de especialistas competentes empregados na regi√£o d√° <s>poucas</s> esperan√ßas de lidar com o problema. <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  Al√©m do ImageNet e da hist√≥ria dos bastidores dos sucessos dos jogos DeepMind, as GANs deram origem a uma nova onda de populariza√ß√£o de redes neurais.  Com eles, era realmente poss√≠vel <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">‚Äúfilmar‚Äù atores</a> sem <a href="https://www.youtube.com/watch%3Fv%3DWm3squcz7Aw" rel="nofollow">usar uma c√¢mera</a> .  E se haver√° mais!  Sob esse ru√≠do informacional, ser√£o financiadas tecnologias de processamento e reconhecimento menos sonoras, mas bastante funcionais. <br></li><li>  Como existem muitas publica√ß√µes, estamos ansiosos para o surgimento de novos m√©todos de redes neurais para an√°lise r√°pida de artigos, porque somente eles nos salvar√£o (uma piada com uma fra√ß√£o de uma piada!). <br></li></ul><br><h1>  Rob√¥s de trabalho, homem feliz </h1><br>  H√° 2 anos, o AutoML vem ganhando popularidade <s>nas p√°ginas dos jornais</s> .  Tudo come√ßou tradicionalmente com o ImageNet, no qual, com a Precis√£o Top 1, ele come√ßou a tomar firmemente os primeiros lugares: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/9a1/498/e6b9a14988b3d708bdbc73aebbf567d2.png"><br>  A ess√™ncia do AutoML √© muito simples, um sonho centen√°rio de cientistas de dados se tornou realidade - para uma rede neural selecionar hiper par√¢metros.  A ideia foi recebida com um estrondo: <br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4f8/c12/56a/4f8c1256ab40bc119fa0c971a8f8bbc1.png"></div><br>  Abaixo no gr√°fico, vemos uma situa√ß√£o bastante rara quando, ap√≥s a publica√ß√£o dos artigos iniciais na <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">NASNet</a> e <a href="https://arxiv.org/pdf/1802.01548.pdf" rel="nofollow">AmoebaNet</a> , eles come√ßam a ganhar popularidade pelos padr√µes das id√©ias anteriores quase instantaneamente (um grande interesse no t√≥pico √© afetado): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0aa/81b/662/0aa81b6628967bc6e77468365cce8554.png"><br>  A imagem id√≠lica √© um pouco estragada por dois pontos.  Em primeiro lugar, qualquer conversa sobre o AutoML come√ßa com a frase: "Se voc√™ tem um dofigalion da GPU ...".  E esse √© o problema.  O Google, √© claro, alega que, com o <a href="https://cloud.google.com/automl/" rel="nofollow">Cloud AutoML,</a> isso √© facilmente resolvido, o <s>principal √© que voc√™ tem dinheiro suficiente</s> , mas nem todos concordam com essa abordagem.  Em segundo lugar, funciona at√© agora <a href="https://towardsdatascience.com/automl-is-overhyped-1b5511ded65f" rel="nofollow">imperfeitamente</a> .  Por outro lado, lembrando os GANs, cinco anos ainda n√£o se passaram e a ideia em si parece muito promissora. <br><br>  De qualquer forma, a principal decolagem do AutoML come√ßar√° com a pr√≥xima gera√ß√£o de aceleradores de hardware para redes neurais e, de fato, com algoritmos aprimorados. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ec/e67/3c5/7ece673c519b57348f556aa7f7b9dfa6.png"><br>  <i>Fonte: Imagem de Dmitry Konovalchuk, materiais do autor</i> <br><br>  <b>Total: De fato, os cientistas de dados n√£o ter√£o um feriado eterno, √© claro, porque por muito tempo permanecer√° uma grande dor de cabe√ßa com os dados.</b>  <b>Mas antes do Ano Novo e do in√≠cio da d√©cada de 2020, por que n√£o sonhar?</b> <br><br><h1>  Algumas palavras sobre ferramentas </h1><br>  A efic√°cia da pesquisa depende muito das ferramentas.  Se, para programar o AlexNet, voc√™ precisava de programa√ß√£o n√£o trivial, hoje essa rede pode ser coletada em v√°rias linhas em novas estruturas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/002/004/b7c0020046f4f5a5b021c6e4a943c9a5.png"><br>  √â claramente visto como a popularidade est√° mudando nas ondas.  Hoje, o mais popular (inclusive de <a href="https://paperswithcode.com/trends" rel="nofollow">acordo com PapersWithCode</a> ) √© o <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> .  E uma vez que o popular <a href="http://caffe.berkeleyvision.org/" rel="nofollow">Caffe</a> lindamente sai muito bem.  (Observa√ß√£o: t√≥pico e software significa que a filtragem de t√≥picos do Google foi usada na plotagem.) <br><br>  Bem, desde que abordamos as ferramentas de desenvolvimento, vale a pena mencionar bibliotecas para acelerar a execu√ß√£o da rede: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/de3/fb6/8af/de3fb68af88d9afabe23929e1b060309.png"></a> <br><br>  O mais antigo do t√≥pico √© (respeito √† NVIDIA) <a href="https://developer.nvidia.com/cudnn" rel="nofollow">cuDNN</a> e, felizmente para os desenvolvedores, nos √∫ltimos dois anos o n√∫mero de bibliotecas aumentou v√°rias vezes e o in√≠cio de sua popularidade tornou-se visivelmente mais acentuado.  E parece que tudo isso √© apenas o come√ßo. <br><br>  <b>Total: Mesmo nos √∫ltimos 3 anos, as ferramentas mudaram seriamente para melhor.</b>  <b>E h√° 3 anos, pelos padr√µes de hoje, eles n√£o eram de todo.</b>  <b>O progresso √© muito bom!</b> <br><br><h1>  Perspectivas de rede neural prometidas </h1><br>  Mas a divers√£o come√ßa depois.  Neste ver√£o, em um <a href="https://habr.com/post/455353/">grande artigo separado,</a> descrevi em detalhes por que a CPU e at√© a GPU n√£o s√£o eficientes o suficiente para trabalhar com redes neurais, por que bilh√µes de d√≥lares est√£o fluindo para o desenvolvimento de novos chips e quais s√£o as perspectivas.  Eu n√£o vou me repetir.  Abaixo est√° uma generaliza√ß√£o e adi√ß√£o do texto anterior. <br><br>  Para come√ßar, voc√™ precisa entender as diferen√ßas entre os c√°lculos de redes neurais e os c√°lculos na arquitetura familiar de von Neumann (na qual eles podem, √© claro, ser calculados, mas com menos efici√™ncia): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1c/b6a/2f7/a1cb6a2f730e5d5f66a0e29b3fa7d1ac.png"><br>  <i>Fonte: Imagem de Dmitry Konovalchuk, materiais do autor</i> <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Arquitetura Von Neumann</b> <br></td><td>  <b>Redes neurais</b> <br></td></tr><tr><td>  A maioria dos c√°lculos s√£o opera√ß√µes seq√ºenciais. <br></td><td>  Computa√ß√£o massivamente paralela (voc√™ precisa de uma arquitetura com um grande n√∫mero de m√≥dulos de computa√ß√£o e acelera√ß√£o da computa√ß√£o tensorial) <br></td></tr><tr><td>  O curso dos c√°lculos est√° mudando <br>  dependendo das condi√ß√µes ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C" rel="nofollow">superscalaridade</a> necess√°ria) <br></td><td>  A estrutura computacional √© quase sempre fixa e conhecida antecipadamente (a superescalabilidade √© ineficiente) <br></td></tr><tr><td>  H√° localidade de acordo com os dados (o cache funciona bem) <br></td><td>  Sem localidade de dados (o cache aquece o ar) <br></td></tr><tr><td>  C√°lculos precisos <br></td><td>  Os c√°lculos podem n√£o ser precisos. <br></td></tr><tr><td>  Os dados s√£o alterados de maneira diferente para algoritmos diferentes <br></td><td>  Dezenas de megabytes de coeficientes de rede permanecem inalterados quando os dados s√£o repetidamente executados atrav√©s de uma rede neural <br></td></tr></tbody></table></div><br>  Na √©poca anterior, a discuss√£o principal foi em torno de FPGA / ASIC, e os c√°lculos imprecisos passaram quase despercebidos, ent√£o vamos falar neles com mais detalhes.  As enormes perspectivas de reduzir os chips das pr√≥ximas gera√ß√µes est√£o precisamente na capacidade de ler de forma imprecisa (e armazenar dados de coeficientes localmente).  De fato, o espessamento tamb√©m √© usado na aritm√©tica exata, quando os pesos da rede s√£o convertidos em n√∫meros inteiros e quantizados, mas em um novo n√≠vel.  Como exemplo, considere um somador de um bit (o exemplo √© bastante abstrato): <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/959/bbf/f16/959bbff160d7f0b33e4d46ce7a5be453.png"><br>  <i>Fonte: <a href="https://www.researchgate.net/publication/270898651_A_High_Speed_and_Low_Power_8_Bit_x_8_Bit_Multiplier_Design_using_Novel_Two_Transistor_2T_XOR_Gates" rel="nofollow">Projeto de multiplicador de 8 bits x 8 bits de alta velocidade e baixa pot√™ncia usando novas portas XOR de dois transistores (2T)</a></i> <br><br>  Ele precisa de 6 transistores (existem abordagens diferentes, o n√∫mero de transistores necess√°rios pode ser cada vez menor, mas, em geral, algo assim).  Para 8 bits, <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">s√£o</a> necess√°rios aproximadamente <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">48 transistores</a> .  Nesse caso, o somador anal√≥gico requer apenas 2 (dois!) Transistores, ou seja,  24 vezes menos: <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f34/85f/2a5/f3485f2a5ac6fa241d2e59cd80ed1064.png"><br>  <i>Fonte: <a href="http://www.iitk.ac.in/eclub/ee381/AnalogMultipliers.pdf" rel="nofollow">Multiplicadores Anal√≥gicos (An√°lise e Projeto de Circuitos Anal√≥gicos Integrados)</a></i> <br><br>  Se a precis√£o for maior (por exemplo, equivalente a 10 ou 16 bits digitais), a diferen√ßa ser√° ainda maior.  Ainda mais interessante √© a situa√ß√£o com multiplica√ß√£o!  Se um multiplexador digital de 8 bits exigir cerca de <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">400 transistores</a> , um anal√≥gico 6, ou seja,  67 vezes (!) Menos.  Obviamente, transistores "anal√≥gicos" e "digitais" s√£o significativamente diferentes do ponto de vista dos circuitos, mas a id√©ia √© clara - se conseguirmos aumentar a precis√£o dos c√°lculos anal√≥gicos, chegaremos facilmente √† situa√ß√£o em que precisaremos de duas ordens de magnitude a menos de transistores.  E o objetivo n√£o √© tanto reduzir o tamanho (o que √© importante em conex√£o com a "desacelera√ß√£o da lei de Moore"), mas reduzir o consumo de eletricidade, o que √© essencial para as plataformas m√≥veis.  E para data centers, n√£o ser√° sup√©rfluo. <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/077/0b2/42b/0770b242bb2693acbe79a2165a4e8c68.png"><br>  <i>Fonte: <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/" rel="nofollow">IBM pensa em chips anal√≥gicos para acelerar o aprendizado de m√°quina</a></i> <br><br>  A chave do sucesso aqui ser√° uma redu√ß√£o na precis√£o e, novamente, aqui a IBM est√° na vanguarda: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a87/570/9f1/a875709f1c76de4e228a567f293a6618.png"><br>  <i>Fonte: <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/" rel="nofollow">IBM Research Blog: Precis√£o de 8 bits para o treinamento de sistemas de aprendizado profundo</a></i> <br><br>  Eles j√° est√£o envolvidos em ASICs especializados para redes neurais, que mostram mais de 10 vezes superior √† GPU e planejam atingir 100 vezes superior nos pr√≥ximos anos.  Parece extremamente encorajador, estamos realmente ansiosos por isso, porque, repito, isso ser√° um avan√ßo para os dispositivos m√≥veis. <br><br>  Enquanto isso, a situa√ß√£o n√£o √© t√£o m√°gica, embora haja s√©rios sucessos.  Aqui est√° um teste interessante dos atuais aceleradores de hardware m√≥vel das redes neurais (a imagem √© clic√°vel e novamente aquece a alma do autor, tamb√©m em imagens por segundo): <br><br> <a href="" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/175/fe0/10d/175fe010d6d9742ddabe6d20d50edb67.png"></a> <br>  <i>Fonte: <a href="https://www.groundai.com/project/ai-benchmark-all-about-deep-learning-on-smartphones-in-2019/1" rel="nofollow">Evolu√ß√£o do desempenho de aceleradores de IA m√≥veis: taxa de transfer√™ncia de imagem para o modelo float Inception-V3 (modelo FP16 usando TensorFlow Lite e NNAPI)</a></i> <br><br>  Verde indica chips m√≥veis, azul indica CPU, laranja indica GPU.  √â claramente visto que os chips m√≥veis atuais e, em primeiro lugar, o chip topo de linha da Huawei, j√° est√£o superando as CPUs dezenas de vezes maiores em tamanho (e consumo de energia).  E √© forte!  Com a GPU, at√© agora tudo n√£o √© t√£o m√°gico, mas haver√° algo mais.  Voc√™ pode ver os resultados com mais detalhes em um site separado <a href="http://ai-benchmark.com/" rel="nofollow">http://ai-benchmark.com/</a> , preste aten√ß√£o √† se√ß√£o de testes, eles escolheram um bom conjunto de algoritmos para compara√ß√£o. <br><br>  <b>Total: o progresso dos aceleradores anal√≥gicos hoje √© bastante dif√≠cil de avaliar.</b>  <b>H√° uma corrida.</b>  <b>Como os produtos ainda n√£o foram lan√ßados, existem <a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2Baccelerator%2Bfiletype%253Apdf" rel="nofollow">relativamente poucas</a> publica√ß√µes.</b>  <b>Voc√™ pode monitorar as patentes que aparecem com atraso (por exemplo, fluxo denso <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU" rel="nofollow">da IBM</a> ) ou <a href="https://patents.google.com/%3Fq%3Danalog%26q%3Ddnn%26q%3Daccelerator%26oq%3Danalog%2Bdnn%2Baccelerator" rel="nofollow">procurar patentes raras de</a> outros fabricantes.</b>  <b>Parece que ser√° uma revolu√ß√£o muito s√©ria, principalmente em smartphones e TPUs de servidores.</b> <br><br><h1>  Em vez de uma conclus√£o </h1><br>  Hoje, o ML / DL √© chamado de nova tecnologia de programa√ß√£o, quando n√£o escrevemos um programa, mas inserimos um bloco e o treinamos.  I.e.  Como no in√≠cio, havia um montador, depois C, depois C ++, e agora, ap√≥s 30 anos de espera, o pr√≥ximo passo √© o ML / DL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfd/8ae/5f7/cfd8ae5f7236a0c9781199044966d2c5.png"></div><br>  Isso faz sentido.  Recentemente, em empresas avan√ßadas, os locais de tomada de decis√£o nos programas s√£o substitu√≠dos por redes neurais.  I.e.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">se ontem houvesse solu√ß√µes "nos FIs" ou em heur√≠sticas que eram boas para o cora√ß√£o de um programador ou mesmo as equa√ß√µes de Lagrange (uau!) e outras realiza√ß√µes mais complexas de d√©cadas de desenvolvimento da teoria de controle foram usadas, hoje eles colocam uma rede neural simples com 3-5 camadas com v√°rias entradas e dezenas de chances. Ela aprende instantaneamente, trabalha significativamente com mais efici√™ncia e o desenvolvimento de c√≥digo se torna mais r√°pido. Se antes era necess√°rio sentar, xam√£ </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, ligar o c√©rebro</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , agora eu o enfiei, alimentei dados e funcionou, e voc√™ est√° ocupado com coisas de n√≠vel superior. Apenas algum tipo de feriado! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturalmente, a depura√ß√£o agora √© diferente. Se antes, quando algo n√£o funcionava, havia uma solicita√ß√£o: "Envie um exemplo em que n√£o funcione!" E ent√£o um </font><s><font style="vertical-align: inherit;">barbudo</font></s><font style="vertical-align: inherit;"> s√©rio e experiente</font></font><s><font style="vertical-align: inherit;"></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o programador conhecia o c√≥digo e as heur√≠sticas, determinou alguns coeficientes e, se ele adivinhou com uma generaliza√ß√£o do exemplo para todos esses casos e o corrigiu no lugar certo, outros exemplos semelhantes come√ßaram a funcionar (oh, felicidade!). Com um bloco de rede neural, esse foco n√£o funcionar√° e a solicita√ß√£o ser√°: ‚ÄúD√™ um exemplo e marque dados, pliz!‚Äù E, em seguida, haver√° um treinamento adicional com o controle de obter exemplos suficientes para todos os n√≥s potencialmente envolvidos na decis√£o errada. E ainda mais na produ√ß√£o, um grande bot√£o vermelho ‚ÄúRetrain‚Äù aparecer√° simplesmente com a mesma inscri√ß√£o grande e vermelha: ‚ÄúPressione n√£o mais que uma vez por m√™s!‚Äù (Para limitar a sintonia dos arquivos). E a economia global se tornar√° ainda mais eficiente. Aleluia!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, como uma ferramenta matem√°tica, ML / DL em geral, e redes neurais em particular, √© claramente algo mais do que a pr√≥xima tecnologia de programa√ß√£o. </font><font style="vertical-align: inherit;">As mesmas redes neurais agora s√£o encontradas simplesmente a cada passo:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O smartphone tira fotos do texto e o reconhece - s√£o redes neurais, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Um smartphone traduz bem rapidamente de um idioma para outro e fala uma tradu√ß√£o - redes neurais e mais uma vez redes neurais, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O navegador e o alto-falante inteligente reconhecem muito bem a fala - novamente redes neurais, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A TV mostra uma imagem de contraste brilhante de 8K do v√≠deo de entrada 2K - tamb√©m uma rede neural, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Os rob√¥s na produ√ß√£o tornaram-se mais precisos, come√ßaram a ver e reconhecer melhor as situa√ß√µes anormais - novamente redes neurais, </font></font><br></li><li>   10       ,       <s> </s>        ,    - 90- ‚Äî   , <br></li><li>             ‚Äî   , <br></li><li>    ‚Äî               -   <a href="https://www.cgevent.ru/archives/28741" rel="nofollow">     </a> ‚Äî    , <br></li><li>   ‚Äî    !  ) <br></li></ul><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f28/b47/999/f28b47999ec31c004bc0b917fed05633.png"><br><br>  Apenas quatro anos se passaram desde que as pessoas aprenderam a treinar redes neurais realmente profundas em muitos aspectos, gra√ßas a BatchNorm (2015) e pular conex√µes (2015), e tr√™s anos se passaram desde que eles "decolaram" e estamos realmente lendo os resultados de seu trabalho n√£o viu.  E agora eles chegar√£o aos produtos.  Algo nos diz que nos pr√≥ximos anos muitas coisas interessantes nos aguardam.  Especialmente quando os aceleradores "decolam" ... <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/a88/6b9/3dc/a886b93dc708002de50fac65c7d84a7a.png"><br><br>  Era uma vez, se algu√©m se lembra, Prometeu roubou fogo do Olimpo e o entregou √†s pessoas.  Zeus zangado com outros deuses criou a primeira beleza de uma mulher chamada Pandora, que foi dotada de muitas qualidades femininas maravilhosas <s>(de repente, percebi que a recontagem politicamente correta de alguns dos mitos da Gr√©cia Antiga √© extremamente dif√≠cil)</s> .  Pandora foi enviada √†s pessoas, mas Prometeu, que suspeitava que algo estava errado, resistiu ao feiti√ßo, e seu irm√£o Epimeteu n√£o.  Como presente para o casamento, Zeus enviou um belo caix√£o com Merc√∫rio e Merc√∫rio, uma alma bondosa, cumpriu a ordem - ele entregou o caix√£o a Epimeteu, mas avisou-o para n√£o abri-lo em nenhum caso.  Pandora curiosa roubou o caix√£o do marido, abriu-o, mas havia apenas pecados, doen√ßas, guerras e outros problemas da humanidade.  Ela tentou fechar o caix√£o, mas j√° era tarde demais: <br><br><img src="https://habrastorage.org/webt/fi/hk/wh/fihkwhcd3uam5uyejc6thklshcm.png"><br>  <i>Fonte: <a href="https://regnum.ru/pictures/2414568/6.html" rel="nofollow">Igreja do artista Frederick Stuart, caixa aberta de Pandora</a></i> <br><br>  Desde ent√£o, a frase "abrir a caixa de Pandora" foi, <s>por curiosidade</s> , realizar <s>uma</s> a√ß√£o irrevers√≠vel, cujas consequ√™ncias podem n√£o ser t√£o bonitas quanto as decora√ß√µes do caix√£o do lado de fora. <br><br>  Voc√™ sabe, quanto mais eu mergulho nas redes neurais, mais distinta √© a sensa√ß√£o de que essa √© outra caixa de Pandora.  No entanto, a humanidade tem a experi√™ncia mais rica em abrir essas caixas!  Desde o recente recente - isto √© energia nuclear e a Internet.  Ent√£o, acho que podemos lidar juntos.  N√£o √© de admirar que um bando de homens <s>barbudos</s> duros entre os abridores.  Bem, um caix√£o √© lindo, concorda!  E n√£o √© verdade que existem apenas problemas, eles j√° t√™m um monte de coisas boas.  Portanto, eles se uniram e ... n√≥s abrimos ainda mais! <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  <b>O artigo n√£o incluiu muitos t√≥picos interessantes, por exemplo, algoritmos cl√°ssicos de ML, aprendizado de transfer√™ncia, aprendizado por refor√ßo, popularidade de conjuntos de dados etc.</b>  <b>(Senhores, voc√™s podem continuar o t√≥pico!)</b> <b><br></b> </li><li>  <b>Para a pergunta sobre o caix√£o: pessoalmente, acho que <a href="https://habr.com/ru/post/411323/">os programadores do Google</a> que permitiram ao Google <a href="https://tproger.ru/news/google-drops-pentagon/" rel="nofollow">abandonar o contrato de US $ 10 bilh√µes com o Pent√°gono</a> s√£o √≥timos e bonitos.</b>  <b>Eles respeitam e respeitam.</b>  <b>No entanto, observe que algu√©m ganhou essa grande licita√ß√£o.</b> <b><br></b> </li></ul><br>  Leia tamb√©m: <br><br><ul><li>  <a href="https://habr.com/post/455353/">Acelera√ß√£o de hardware de redes neurais profundas: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP e outras letras</a> - o texto do autor sobre o estado atual e as perspectivas de acelera√ß√£o de hardware de redes neurais em compara√ß√£o com as abordagens atuais. <br></li><li>  <a href="https://habr.com/post/480348/">Deep Fake Science, a crise da reprodutibilidade e de onde v√™m os reposit√≥rios vazios</a> - sobre os problemas na ci√™ncia gerados pelo ML / DL. <br></li><li>  <a href="https://habr.com/post/451664/">Compara√ß√£o de codecs m√°gicos de rua.</a>  <a href="https://habr.com/post/451664/">N√≥s revelamos segredos</a> - um exemplo de falsifica√ß√£o baseada em redes neurais. <br></li></ul><br><h3>  Todo um grande n√∫mero de <i>novas descobertas interessantes</i> na d√©cada de 2020 em geral e no Ano Novo, em particular! </h3><br><h2>  Agradecimentos </h2><br>  Gostaria de agradecer cordialmente: <br><br><ul><li>  Laborat√≥rio de Computa√ß√£o Gr√°fica e Multim√≠dia Universidade Estadual de Moscou VMK  M.V.  Lomonosov por sua contribui√ß√£o ao desenvolvimento da aprendizagem profunda na R√∫ssia e n√£o apenas <br></li><li>  pessoalmente Konstantin Kozhemyakov e Dmitry Konovalchuk, que fizeram muito para tornar este artigo melhor e mais visual, <br></li><li>  e, finalmente, muito obrigado a Kirill Malyshev, Yegor Sklyarov, Nikolai Oplachko, Andrey Moskalenko, Ivan Molodetsky, Evgeny Lyapustin, Roman Kazantsev, Alexander Yakovenko e Dmitry Klepikov por muitos coment√°rios e corre√ß√µes √∫teis que tornaram este texto muito melhor! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt481844/">https://habr.com/ru/post/pt481844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt481828/index.html">A hist√≥ria do software educacional: sistemas de gerenciamento de aprendizado e a ascens√£o da educa√ß√£o on-line</a></li>
<li><a href="../pt481836/index.html">Pizza como servi√ßo: como a Amazon migrou para o Redshift</a></li>
<li><a href="../pt481838/index.html">WireGuard, configurando v√°rios clientes para NAT, e para onde vai o STUN?</a></li>
<li><a href="../pt481840/index.html">Proteja sua API GraphQL contra vulnerabilidades</a></li>
<li><a href="../pt481842/index.html">Mudando para o armazenamento puro: nosso novo armazenamento</a></li>
<li><a href="../pt481846/index.html">Usando o GitHub CI para projetos Elixir</a></li>
<li><a href="../pt481848/index.html">Treinamento experiente da equipe</a></li>
<li><a href="../pt481850/index.html">A Inquisi√ß√£o Espanhola e o rob√¥ da humilha√ß√£o: quais s√£o as confer√™ncias "predat√≥rias" por dinheiro</a></li>
<li><a href="../pt481852/index.html">Revis√£o da impressora 3D Anet N4 // Como colorir realisticamente um personagem de Dark Souls</a></li>
<li><a href="../pt481854/index.html">Testando id√©ias atrav√©s da cria√ß√£o de prot√≥tipos no painel</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>