<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë´ ü§¶ üé± Allgemeine Theorie und Arch√§ologie der Virtualisierung x86 ü•ä üõåüèΩ ‚öìÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Einleitung 
 Autorenteam 
 Gepostet von Anton Zhbankov ( AntonVirtual , cloudarchitect.cc ) 
 Mitautoren: Grigory Pryalukhin , Evgeny Parfenov 

 Allg...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Allgemeine Theorie und Arch√§ologie der Virtualisierung x86</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/474776/"><h2>  Einleitung </h2><br><h4>  Autorenteam </h4><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gepostet</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anton Zhbankov</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">AntonVirtual</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cloudarchitect.cc</a> ) <br>  Mitautoren: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grigory Pryalukhin</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Evgeny</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parfenov</a> <br><br><h3>  Allgemeine Virtualisierungskonzepte </h3><br>  Ich musste eine Menge Interpretationen von <i>Virtualisierung sehen</i> und mir viele Kontroversen anh√∂ren, um das praktische Ergebnis zu diskutieren.  Und wie Sie wissen, l√§uft das Argument zweier kluger Leute auf eine Debatte √ºber Definitionen hinaus.  Definieren wir, was Virtualisierung ist und was daraus entsteht. <br><br>  Die wahrscheinlich am n√§chsten liegende Definition f√ºr Virtualisierung ist die ‚ÄûAbstraktion‚Äú der objektorientierten Programmierung.  Wenn es ins normale Russisch √ºbersetzt wird, verbirgt dies die Implementierung hinter einer abstrakten Schnittstelle.  Was nat√ºrlich alles auf einmal erkl√§rte.  Versuchen wir es noch einmal, aber f√ºr diejenigen, die noch kein Programmierstudium absolviert haben. <br><blockquote>  Virtualisierung - Versteckt eine bestimmte Implementierung hinter einer universellen standardisierten Methode f√ºr den Zugriff auf Ressourcen / Daten. </blockquote><br>  Wenn Sie versuchen, diese Definition in die Praxis umzusetzen, stellt sich heraus, dass sie bei v√∂llig unerwarteten Themen funktioniert.  Sagen wir die Uhr.  So wurde vor mehreren tausend Jahren eine Sonnenuhr erfunden, und im Mittelalter wurde eine mechanische erfunden.  Was haben wir gemeinsam?  Die Sonne und ein paar Zahnr√§der?  Eine Art Unsinn.  Und dann Quarzoszillatoren und alles andere. <br>  Die Quintessenz ist, dass wir eine Standardschnittstelle haben - einen Zeiger oder einen digitalen Zeiger, der in einer universellen Standardform die aktuelle Zeit anzeigt.  Aber ist es uns wichtig, wie genau dieser Mechanismus in der Box implementiert ist, wenn die Zeit f√ºr uns mit ausreichender Genauigkeit angezeigt wird? <br>  ‚ÄûLassen Sie mich‚Äú, k√∂nnen Sie sagen, ‚Äûaber ich dachte, bei der Virtualisierung geht es um Maschinen, Prozessoren und so weiter! <br>  Ja, es geht um Autos und Prozessoren, aber dies ist nur ein Sonderfall.  Lassen Sie uns einen breiteren Blick werfen, da der Artikel mutig eine allgemeine Theorie behauptet. <br><a name="habracut"></a><br><h2>  POZOR! </h2><br><h3>  Uwaga!  Achtung!  Pozor! </h3><br>  Dieser Artikel hat ein <b>allgemeines p√§dagogisches</b> Ziel, eine ganze Reihe von Technologien und gruseligen W√∂rtern mit der Geschichte in einer bestimmten Struktur zu verkn√ºpfen, und enth√§lt aufgrund dieses Umstands eine erhebliche Anzahl <b>absichtlicher</b> Vereinfachungen.  Nat√ºrlich enth√§lt es auch eine gro√üe Anzahl nerviger Auslassungen und sogar abgedroschene Fehler bei Tippfehlern.  Konstruktive Kritik ist nur zu begr√º√üen, insbesondere in Form von "Lassen Sie mich an diesen Teil erinnern." <br><br><h2>  Arten der Virtualisierung </h2><br>  Kehren wir von v√∂llig abstrakten Konzepten zu den bekannten Begriffen unserer geliebten Computer zur√ºck. <br><br><h3>  Speichervirtualisierung </h3><br>  Die erste ist wahrscheinlich die Art der Virtualisierung, auf die ein Anf√§nger trifft - die Virtualisierung eines Datenspeichersystems.  In diesem Fall wird das Speichersystem nicht im Sinne eines gro√üen Arrays mit Festplatten verwendet, die √ºber einen Glasfaserkanal verbunden sind, sondern als logisches Subsystem, das f√ºr die langfristige Datenspeicherung verantwortlich ist. <br><br><h4>  FS -&gt; LBA -&gt; CHS </h4><br>  Nehmen Sie den einfachsten Fall eines Speichersystems auf einer einzelnen Festplatte.  Das √ºbliche Format f√ºr die Arbeit mit Daten sind die Dateien, die sich auf dem logischen Laufwerk befinden.  Die Datei kann ge√∂ffnet, gelesen, geschlossen werden.  Ein solches Objekt wie eine Datei existiert jedoch einfach nicht physisch - es gibt nur eine M√∂glichkeit, auf bestimmte Datenbl√∂cke mit der Adressierungsmethode des Formulars ‚ÄûLaufwerk: \ Ordner1 \ Ordner2 \ Datei‚Äú zuzugreifen.  Das hei√üt  Wir treffen auf die erste Ebene der Virtualisierung - von mnemonisch und f√ºr den Menschen verst√§ndlich, √ºbersetzen wir alles in systemverst√§ndliche Adressen.  In den Metadatentabellen sucht der Dateisystemtreiber nach der Art der Datenbl√∂cke, und wir erhalten die Adresse im LBA-System (Logical Block Addressing).  In dem LBA-System haben Bl√∂cke eine feste Gr√∂√üe und folgen linear aufeinander, d.h.  Irgendwie hat es vielleicht mit dem Speichern von Daten auf Magnetband zu tun, aber die Festplatte ist irgendwie ganz anders!  Und hier kommen wir zur zweiten Ebene der Virtualisierung - der √úbersetzung der LBA-Adressierung an CHS (Zylinder / Kopf / Sektor). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e95/940/bd4/e95940bd4389a0187c2bf3d82406e118.png" alt="Bild"><br><br>  CHS wiederum beginnt bereits im Festplattencontroller, sich in physikalische Parameter zum Lesen umzusetzen, aber das ist eine ganz andere Geschichte. <br>  Selbst bei einem einfachen Zugriff auf die Datei, um beispielsweise ein Video mit Memasics anzuzeigen, stie√üen wir sofort auf drei Virtualisierungsebenen. <br>  Alles w√§re zu einfach, wenn sich die Ebenen nicht in zuf√§lliger Reihenfolge und auf verschiedene Weise √ºberlappen w√ºrden. <br><br><h4>  RAID </h4><br>  Die n√§chste Virtualisierungsebene, die viele f√§lschlicherweise nicht als Virtualisierung bezeichnen, ist RAID (redundantes Array kosteng√ºnstiger / unabh√§ngiger Festplatten). <br><br>  Das Hauptmerkmal von RAID im Kontext der besprochenen Konzepte ist nicht die F√§higkeit, Daten vor dem Ausfall einer bestimmten physischen Festplatte zu sch√ºtzen.  RAID bietet eine zweite Ebene der LBA-Adressierung neben mehreren (manchmal sehr vielen) unabh√§ngigen LBA-Adressen.  Da wir unabh√§ngig vom RAID-Level genauso auf das RAID zugreifen k√∂nnen wie auf eine einzelne Festplatte ohne RAID, k√∂nnen wir mit Sicherheit sagen: <blockquote>  RAID ist Festplattenvirtualisierung. </blockquote><br>  Dar√ºber hinaus erstellt der RAID-Controller nicht nur eine gro√üe virtuelle Festplatte aus mehreren physischen Festplatten, sondern kann durch Hinzuf√ºgen einer weiteren Virtualisierungsebene eine beliebige Anzahl davon erstellen. <br><br><h3>  Virtualisierung anzeigen </h3><br>  Die n√§chste Art der Virtualisierung, die viele von uns fast t√§glich verwenden, jedoch nicht als Virtualisierung betrachten, ist eine Remoteverbindung zum Desktop. <br><br>  Terminalserver, VDI und sogar nur RDP √ºber VPN zum Server sind Sitzungsvirtualisierung.  √úber eine Standardschnittstelle (Monitor, Tastatur, Maus) arbeiten wir entweder mit einer realen Maschine oder mit einem unverst√§ndlichen Design von einem virtuellen Desktop auf einem verkn√ºpften Klon mit einer containerisierten Anwendung, von der wir Daten √ºber einen Puffer an eine Anwendung mit Streaming-Bereitstellung √ºbertragen.  Oder nicht, wer wird es herausfinden, au√üer dem, der es entworfen hat? <br><br><h2>  Einf√ºhrung in die x86-Virtualisierung </h2><br><h3>  Geschichte und √úbersicht der Prozessoren </h3><br><h4>  Programmausf√ºhrung </h4><br>  In der ersten Stunde eines speziellen Programmierkurses sagte Vladimir Denisovich Lelyukh (f√ºr ihn in Frieden ruhen) zu den Sch√ºlern: Der Computer kann trotz seines Namens nicht z√§hlen, er kann so tun, als k√∂nne er z√§hlen.  Aber wenn etwas wie eine Ente aussieht, wie eine Ente geht und wie eine Ente quakt, ist es aus praktischer Sicht eine Ente. <br><br>  Versuchen wir, uns dies f√ºr die weitere praktische Verwendung zu merken. <br><br>  Der Computer und insbesondere der Prozessor tun eigentlich nichts - er erwartet nur einige Eingabeparameter an bestimmten Stellen und liefert dann durch schreckliche schwarze Magie an bestimmten Stellen einige Ergebnisse. <br><br>  Ein Programm ist in diesem Fall ein bestimmter Strom von Befehlen, die streng sequentiell ausgef√ºhrt werden, wodurch wir ein bestimmtes Ergebnis erwarten. <br>  Aber wenn das Programm ausgef√ºhrt wird, wie k√∂nnen die Daten dann √ºberhaupt eingegeben werden?  Und im Allgemeinen irgendwie auf einem Computer interagieren? <br><br>  Hierf√ºr wurden Hardware-Interrupts erfunden.  Der Benutzer dr√ºckt eine Taste - der Tastatur-Controller signalisiert dies und die Ausf√ºhrung des aktuellen Code-Threads wird unterbrochen.  Die Adressen von Interrupt-Handlern werden in einem bestimmten Speicherbereich aufgezeichnet, und nach dem Speichern des aktuellen Zustands wird die Steuerung an den Interrupt-Handler √ºbertragen.  Im Gegenzug sollte der Handler theoretisch schnell alles verarbeiten, dann schreiben er und der Handler die im gew√ºnschten Puffer gedr√ºckte Taste auf und geben die Kontrolle zur√ºck.  Somit scheint die Anwendung zu laufen und wir k√∂nnen mit dem System interagieren. <br><br>  Interrupt-Handler (und der Haupttyp der Handler sind Ger√§tetreiber) haben die M√∂glichkeit, in einen speziellen Prozessormodus zu wechseln, wenn andere Interrupts vor dem Verlassen dieses Modus nicht implementiert werden k√∂nnen.  Was am Ende oft zu einem Aufh√§ngeproblem f√ºhrte - ein Fehler im Treiber erlaubte es nicht, die Unterbrechung zu beenden. <br><br><h4>  Multitasking </h4><br>  Was tun, wenn mehrere Programme (Codestreams mit ihren Daten- und Speicherstrukturen) gleichzeitig ausgef√ºhrt werden m√ºssen?  Wenn es mehr Codestreams gibt als Ger√§te, die sie ausf√ºhren k√∂nnen, ist dies offensichtlich ein Problem. <br><br>  Pseudo-Multitasking tritt auf, wenn eine Aufgabe ausgef√ºhrt wird, wenn direkt zu ihr gewechselt wird. <br><br>  In Zukunft wird eine kooperative (nicht pr√§emptives Multitasking) Aufgabe angezeigt. Die ausf√ºhrbare Aufgabe selbst erkennt, dass sie keine Prozessorressourcen mehr ben√∂tigt und die Steuerung anderen Personen √ºbergibt.  Aber das alles ist nicht genug. <br><br>  Und hier kommen wieder Unterbrechungen + die F√§higkeit vorzugeben, uns zu retten.  Es ist f√ºr den Benutzer nicht wirklich wichtig, dass sie ausschlie√ülich gleichzeitig ausgef√ºhrt werden. Es reicht aus, so auszusehen. <br>  Daher wird ein Handler einfach aufgelegt, um den Zeitgeber zu unterbrechen, der zu steuern beginnt, welcher Codestream als n√§chstes ausgef√ºhrt werden soll.  Wenn der Timer ziemlich oft ausgel√∂st wird (etwa 15 ms), sieht f√ºr den Benutzer alles wie ein Parallelbetrieb aus.  Und so gibt es ein modernes Verdr√§ngungs-Multitasking. <br><br><h4>  Realer Modus </h4><br>  Der reale Prozessormodus in diesem Artikel kann recht einfach beschrieben werden - der gesamte Speicher steht jedem zur Verf√ºgung.  Auf jede Anwendung, einschlie√ülich Malware (Malware, Schadsoftware), kann von √ºberall aus zugegriffen werden, sowohl zum Lesen als auch zum Schreiben. <br><br>  Dies ist die anf√§ngliche Betriebsart der Intel x86-Prozessorfamilie. <br><br><h4>  Gesch√ºtzter Modus </h4><br>  1982 erschien eine Innovation im Intel 80286-Prozessor (im Folgenden einfach 286) - eine gesch√ºtzte Betriebsart, die Innovationen bei der Organisation der Arbeit mit dem Speicher mit sich brachte (zum Beispiel die Zuweisung von Arten von Speichersegmenten - Code, Daten, Stapel).  Das Wichtigste, was der 286-Prozessor der x86-Welt gebracht hat, ist das Konzept der Schutzringe, die wir immer noch verwenden. <br><br>  Das Konzept der Schutzringe erschien urspr√ºnglich im Multics-Betriebssystem f√ºr den GE645-Mainframe (1967) mit einer teilweise Software-Implementierung und vollst√§ndiger Hardware bereits 1970 im Honeywell 6180-System. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a9/249/522/2a9249522d8bdb211dbb4eb0aec70b75.png" alt="Bild"><br><br>  Die Grundidee der Verteidigungsringe √§hnelt mehrstufigen mittelalterlichen Festungen, die wertvollste liegt genau in der Mitte hinter den mehreren Mauern.  In diesem Fall ist der uneingeschr√§nkte direkte Zugriff auf einen beliebigen RAM-Bereich und die Kontrolle √ºber alle Prozesse das Wertvollste.  Sie werden von Prozessen beherrscht, die im Null-Schutz-Ring arbeiten.  Hinter der Wand arbeiten im ersten Ring weniger wichtige Prozesse, wie z. B. Ger√§tetreiber, und in den allerletzten Benutzeranwendungen.  Das Prinzip ist einfach - von innen kann man nach au√üen gehen, aber von au√üen nach innen ist es verboten.  Das hei√üt  Kein Benutzerprozess kann auf den OS-Kernel-Speicher zugreifen, wie dies fr√ºher im Real-Modus m√∂glich war. <br><br>  Bei der allerersten vollst√§ndigen Implementierung des Honeywell 6180 wurden 8 Schutzringe implementiert, aber Intel entschied sich, die Schaltung auf 4 zu vereinfachen, von denen in der Praxis die Hersteller von Betriebssystemen nur zwei - null und drei - verwendeten. <br><br><h4>  32bit </h4><br>  1985 wurde ein weiterer √§u√üerst architektonisch wichtiger Prozessor in der x86-Reihe ver√∂ffentlicht - 80386 (im Folgenden 386), der eine 32-Bit-Speicheradressierung implementierte und 32-Bit-Befehle verwendete.  Und nat√ºrlich Speichervirtualisierung.  Wie bereits erw√§hnt, ist Virtualisierung das Verschleiern der tats√§chlichen Implementierung durch die Bereitstellung k√ºnstlicher ‚Äûvirtueller‚Äú Ressourcen.  In diesem Fall handelt es sich um eine Speicheradressierung.  Das Speichersegment hat eine eigene Adressierung, die nichts mit dem tats√§chlichen Ort der Speicherzellen zu tun hat. <br>  Der Prozessor war so gefragt, dass er vor 2007 produziert wurde. <br>  Die Architektur in Bezug auf Intel hei√üt IA32. <br><br><h4>  64bit </h4><br>  Selbst ohne Virtualisierung Mitte der 2000er Jahre stie√ü die Branche nat√ºrlich bereits an die Grenzen von 32 Bit.  Es gab teilweise Problemumgehungen in Form von PAE (Physical Address Extension), die den Code jedoch komplizierten und verlangsamten.  Der √úbergang zu 64 Bit war eine ausgemachte Sache. <br><br>  AMD stellte seine Version der Architektur vor, die AMD64 hei√üt.  Bei Intel hofften sie auf die IA64-Plattform (Intel Architecture 64), die wir auch unter dem Namen Itanium kennen.  Der Markt begegnete dieser Architektur jedoch ohne gro√üe Begeisterung, so dass Intel gezwungen war, eine eigene Unterst√ºtzung f√ºr AMD64-Anweisungen zu implementieren, die zuerst EM64T und dann nur Intel 64 hie√ü. <br><br>  Letztendlich kennen wir alle diese Architektur als AMD64, x86-64, x86_64 oder manchmal x64. <br><br>  Da die Hauptnutzung von Servern zu dieser Zeit physisch ohne Virtualisierung sein sollte, passierte mit den ersten 64-Bit-Prozessoren in der Virtualisierung eine technisch witzige Sache.  Verschachtelte Hypervisoren wurden h√§ufig als Laborserver verwendet, da sich nicht jeder mehrere Cluster physischer Server leisten konnte.  Am Ende stellte sich heraus, dass die Last-VM im eingebetteten Hypervisor nur im 32-Bit-Modus funktionieren konnte. <br><br>  In den ersten x86-64-Prozessoren haben Entwickler, w√§hrend sie die vollst√§ndige Kompatibilit√§t mit dem 32-Bit-Betriebsmodus aufrechterhalten, einen erheblichen Teil der Funktionalit√§t im 64-Bit-Modus verworfen.  In diesem Fall bestand das Problem darin, die Speichersegmentierung erheblich zu vereinfachen.  Die M√∂glichkeit, die Unverletzlichkeit eines kleinen Teils des Arbeitsspeichers in der VM zu gew√§hrleisten, in der der Hypervisor-Ausnahmehandler ausgef√ºhrt wurde, wurde entfernt.  Dementsprechend konnte das Gastbetriebssystem es √§ndern. <br>  In der Folge gab AMD die M√∂glichkeit zur√ºck, Segmente einzuschr√§nken, und Intel wartete einfach auf die Einf√ºhrung der Hardware-Virtualisierung. <br><br><h4>  UMA </h4><br>  X86-Multiprozessorsysteme begannen mit dem UMA-Modus (Uniform Memory Access) zu arbeiten, bei dem der Abstand zwischen einem Prozessor (Verz√∂gerung beim Zugriff auf eine Speicherzelle) und einer Speicherleiste gleich ist.  Bei Intel-Prozessoren wurde dieses Arbeitsschema auch nach dem Erscheinen von Multi-Core-Prozessoren bis zur 54xx-Generation (Harpertown) beibehalten.  Seit der 55xx-Generation (Nehalem) haben Prozessoren auf die NUMA-Architektur umgestellt. <br><br>  Aus Sicht der Ausf√ºhrungslogik sind dies zus√§tzliche Hardware-Threads, denen Sie Code-Streams zur parallelen Ausf√ºhrung zuweisen k√∂nnen. <br><br><h4>  NUMA </h4><br>  NUMA (Non Uniform Memory Access) - Architektur mit ungleichm√§√üigem Speicherzugriff.  Innerhalb dieser Architektur verf√ºgt jeder Prozessor √ºber einen eigenen lokalen Speicher, auf den mit geringer Latenz direkt zugegriffen wird.  Auf den Speicher anderer Prozessoren wird indirekt mit h√∂heren Verz√∂gerungen zugegriffen, was zu einer verringerten Leistung f√ºhrt. <br><br>  Bei Intel Xeon Scalable v2-Prozessoren f√ºr 2019 bleibt die interne Architektur weiterhin UMA innerhalb des Sockets und wird zu NUMA f√ºr andere Sockets (obwohl dies nicht wirklich der Fall ist und dies nur so tut, als w√§re es der Fall).  Die Opteron-Prozessoren von AMD verf√ºgten bereits zur Zeit des √§ltesten UMA Xeon √ºber eine NUMA-Architektur, und dann wurde NUMA bis zur letzten Generation von Rom, in der sie zu NUMA = socket zur√ºckkehrten, sogar in den Sockel integriert. <br><br><h3>  Virtuelle Maschine </h3><br>  Virtuelle Maschine (VM, von der englischen virtuellen Maschine) - Ein Software- und / oder Hardwaresystem, das die Hardware einer bestimmten Plattform emuliert (Ziel ist die Ziel- oder Gastplattform) und Programme f√ºr die Zielplattform auf der Hostplattform ausf√ºhrt (Host ist die Hostplattform) oder die Host-Plattform) oder die Virtualisierung einer Plattform und die Erstellung von Umgebungen, in denen Programme und sogar Betriebssysteme voneinander isoliert sind.  Wikipedia <br>  In diesem Artikel werden wir "virtuelle Maschine" sagen, was "virtuelle Systemmaschinen" bedeutet, wodurch alle Ressourcen und Hardware in Form von Softwarekonstrukten vollst√§ndig simuliert werden k√∂nnen. <br>  Es gibt zwei Haupttypen von Software zum Erstellen virtueller Maschinen - mit vollst√§ndiger und resp.  unvollst√§ndige Virtualisierung. <br><br>  <b>Die vollst√§ndige Virtualisierung</b> ist ein Ansatz, bei dem die gesamte Hardware, einschlie√ülich des Prozessors, emuliert wird.  Erm√∂glicht das Erstellen von hardwareunabh√§ngigen Umgebungen und das Ausf√ºhren von beispielsweise dem Betriebssystem und der Anwendungssoftware f√ºr die x86-Plattform auf SPARC-Systemen oder den bekannten Spectrum-Emulatoren mit dem Z80-Prozessor auf dem bekannten x86.  Die Kehrseite der v√∂lligen Unabh√§ngigkeit ist der hohe Overhead f√ºr die Virtualisierung des Prozessors und die geringe Gesamtleistung. <br><br>  <b>Unvollst√§ndige Virtualisierung</b> ist ein Ansatz, bei dem nicht 100% der Hardware virtualisiert wird.  Da unvollst√§ndige Virtualisierung in der Branche am h√§ufigsten vorkommt, werden wir dar√ºber sprechen.  Informationen zu Plattformen und Technologien f√ºr virtuelle Systemmaschinen mit unvollst√§ndiger Virtualisierung f√ºr die x86-Architektur.  In diesem Fall ist die Virtualisierung des Prozessors unvollst√§ndig, d.h.  Mit Ausnahme des teilweisen Ersetzens oder Versteckens bestimmter Systemaufrufe wird der Bin√§rcode der virtuellen Maschine direkt vom Prozessor ausgef√ºhrt. <br><br><h4>  Software-Virtualisierung </h4><br>  Die offensichtliche Konsequenz der Prozessorarchitektur und der Gewohnheiten von Betriebssystemen, im Nullring zu arbeiten, war das Problem - der Kernel des Gastbetriebssystems kann nicht an der √ºblichen Stelle arbeiten.  Der Nullring wird vom Hypervisor belegt, und Sie m√ºssen nur das Gastbetriebssystem dorthin lassen - zum einen sind wir mit allen Konsequenzen in den Real-Modus zur√ºckgekehrt, zum anderen erwartet das Gastbetriebssystem dort niemanden und zerst√∂rt sofort alle Datenstrukturen und l√§sst das Auto fallen. <br><br>  Aber alles war ganz einfach entschieden: Da f√ºr den Hypervisor das Gastbetriebssystem nur eine Reihe von Speicherseiten mit direktem Vollzugriff ist und der virtuelle Prozessor nur eine Reihe von Befehlen darstellt, warum nicht diese umschreiben?  Direkt im laufenden Betrieb wirft der Hypervisor alle Anweisungen aus der Warteschlange der Anweisungen zur Ausf√ºhrung auf dem virtuellen Prozessor aus und ersetzt sie durch weniger privilegierte Anweisungen.  Das Ergebnis dieser Anweisungen wird jedoch genauso dargestellt, als bef√§nde sich das Gastbetriebssystem im Nullring.  Auf diese Weise k√∂nnen Sie alles virtualisieren, bis ein Gastbetriebssystem vollst√§ndig fehlt. <br>  Dieser Ansatz wurde 1999 vom Entwicklerteam in das VMware Workstation-Produkt und 2001 in die GSX-Serverhypervisoren (zweiter Typ, wie Workstation) und ESX (erster Typ) implementiert. <br><br><h4>  Paravirtualisierung </h4><br>  <b>Die Paravirtualisierung</b> ist ein sehr einfaches Konzept, bei dem davon <b>ausgegangen</b> wird, dass das Gastbetriebssystem wei√ü, dass es sich in einer virtuellen Maschine befindet, und dass es wei√ü, wie auf das Hostbetriebssystem f√ºr bestimmte Systemfunktionen <b>zugegriffen</b> wird.        ‚Äî   ,         . <br>   x86   2003     Linux Xen. <br><br>                 ,         . ,  VMware ESXi     SCSI  PVSCSI,          ,    .         ( VMware Tools),      Linux (open-vm-tools). <br><br><h4>   </h4><br>                 ,          . <br><br>      ‚Äî     Intel VT-x  AMD-V ,     ,      .            . <br><br><h3>   </h3><br><h4>  2 (hosted) </h4><br>    ‚Äî  ,     .         .       ,   ,        ,        .                  .      ,         . ,     -         ,      . <br><br>    : VMware Workstation / Fusion, Oracle VM VirtualBox, Parallels Desktop, VMware Server (ex-GSX), Microsoft Virtual Server 2005 <br><br><h4>  1 (bare-metal) </h4><br><br>        ,    .     ,      ,    -.      -,      .                 .      x86    VMware ESX,        1+.  ‚Äú‚Äù       VMware ESXi ‚Äî  ESX,         RHEL. <br><br>      ESXi.      API ,    VMkernel.  ,      ,    .     ,             . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/361/f83/77d/361f8377d29345aa2498b31b9af66030.jpg" alt="Bild"><br><br>     :   ‚Äú‚Äù             ,       HCL (hardware compatibility list). <br><br><h4>  1+ ( ) </h4><br>    (   1+, 1a, 1.5)       ,     (parent partition   Microsoft Hyper-V)    (domain dom0   Xen). ,    ,              .               -. <br><br>  ,          .         :       ,     ESXi,  ,         HCL.               ,      . <br><br>     1+  : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89c/b74/eb3/89cb74eb3f97b6a8af063fbd447b1c87.jpg" alt="Bild"><br><br>     :  VMware ESX, Microsoft Hyper-V, Xen-based  (Citrix XenServer   Xen    Linux). ,  Citrix XenServer ‚Äì    RHEL-based OS,             Red-Hat Enterprise Linux.      Xen    :     Linux    Xen      dom0.    ,  Xen-based           1 . <br><br><h2>     </h2><br>      VMware,      .             .   ,      ,   .        ,   ,       . <br><br><h3> SLA </h3><br>   ,      SLA   (RPO / RTO). <br><br><h4> HA </h4><br> High Availability ‚Äî         .             . :  RTO    HA +   / . <br><br><h4> FT </h4><br> Fault Tolerance ‚Äî          .      ,        .            ,      .          . :  RTO  . <br><br><h3> TCO </h3><br>   ,      TCO. <br><br><h4> vMotion </h4><br> vMotion ‚Äî             .           ,     , ..      . :  RTO         ,  ,    . <br><br><h4> Storage vMotion </h4><br> Storage vMotion ‚Äî             .        ,     . :  RTO         ,  ,    . <br><br><h4> DPM </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distributed Power Management - Technologie zur Steuerung der Hostlast und des Ein- und Ausschaltens von Hosts, wenn sich die Last des Clusters √§ndert. </font><font style="vertical-align: inherit;">Ben√∂tigt DRS f√ºr seinen Betrieb. </font><font style="vertical-align: inherit;">Effekt: Gesamtreduzierung des Stromverbrauchs.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Verteilter vSwitch </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distributed vSwitch ist eine Technologie zur zentralen Verwaltung der Netzwerkeinstellungen von virtuellen Host-Switches. </font><font style="vertical-align: inherit;">Effekt: Reduzierung des Umfangs und der Komplexit√§t der Arbeit an der Neukonfiguration des Netzwerksubsystems, Verringerung des Fehlerrisikos.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> EVC </font></font></h4><br> Enhanced vMotion Compatibility ‚Äî ,          .             ,       . :         /   . <br><br><h3> QoS </h3><br>   ,      SLA   . <br><br><h4> vNUMA </h4><br> vNUMA ‚Äî          NUMA    (vCPU  vRAM &gt;  NUMA). :      ,  NUMA. <br><br><h4> Resource Pool </h4><br>   ‚Äî               . :  ,   . <br><br><h4> Limit / Reserve </h4><br>     /     ,                / . <br><br><h4> DRS </h4><br> Dynamic Resource Scheduler ‚Äî                     .   vMotion. <br><br><h4> Storage IO Control </h4><br> Storage IO control ‚Äî ,   ‚Äú ‚Äù,      ,          .   ‚Äî   /     . <br><br><h4> Network IO Control </h4><br> Network IO Control ‚Äî ,   ‚Äú ‚Äù,      . <br><br><h3> Storage Integration (VAAI etc) </h3><br>       : <br><ul><li>                /   ,      . </li><li>     ‚Äî VAAI, ODX.      ,        . ,      ,   ,    .        ,          . </li></ul><br><br><h3>  Sicherheit </h3><br><h4> Microsegmentation </h4><br>       ‚Äî      ,     .     . <br><br><h4> Agentless AV </h4><br>    .                  .        ,   ‚Äú ‚Äù. <br><br><h2>   </h2><br>  ,    ‚Äî    .             .  ,    . <br><br>         .        +  + .    .    ,      ‚Äú ‚Äù.          . <br><br><h3>  Architektur </h3><br>     ,           . <br><br>  ,  ,            ,        .       ‚Äî       SDS. <br><br>  Wir bekommen: <br><br><ul><li>   ‚Äî , ,       ,   /.    . </li><li>   ‚Äî    ,  ,  .       . </li></ul><br> , ,        .    ,   . <br><br> <b> </b> ‚Äî     . <br>         .         ,         SDS.         disaggregated HCI (  ).  , , NetApp             ,    . NetApp HCI   ( 2019) ‚Äî hybrid cloud infrastructure. <br><br><h3>   </h3><br>   ,      ,      . <br><br><ul><li> 1.  . SDS       ,  vSAN + ESXi </li><li> 1.5   . SDS        ,  S2D + Hyper-V </li><li> 2.  . SDS         . Nutanix, Cisco Hyperflex, HPE Simplivity. </li></ul><br> ,         ,         .   1 ,          ,     2      . <br><br><h2>  </h2><br>  ,        ,     . ,      OSI,   .      ‚Äî    ,    . <br><br>     ‚Äî     ,          .     ,   . <br><br><h3>  vs  </h3><br>  Die Vor- und Nachteile beider Ans√§tze sind recht einfach und direkt entgegengesetzt. <br><br>  Vollst√§ndige Virtualisierung (VM) bietet vollst√§ndige Unabh√§ngigkeit von der Eisenebene, einschlie√ülich vollst√§ndig unabh√§ngiger Betriebssystem-, Festplatten- und Netzwerkstacks.  Auf der anderen Seite ben√∂tigt jede Anwendung, da wir uns an Schema 1 Anwendung = 1 Server halten, ein eigenes Betriebssystem, eine eigene Festplatte und einen eigenen Netzwerkstapel.  d.h.  Es gibt eine Vielzahl von Ressourcen. <br><br>  Die Container verf√ºgen √ºber gemeinsame Festplatten- und Netzwerkstacks mit dem Host-Betriebssystem und verwenden alle zusammen denselben Kern auf dem gesamten physischen Server (nun ja, oder virtuell, seit einiger Zeit), sodass Sie in homogenen Landschaften ganz erheblich Ressourcen sparen k√∂nnen. <br><br>  In der Vergangenheit verf√ºgte x86 zun√§chst √ºber Container f√ºr alles sowie √ºber physische Server.  Nach dem Aufkommen der vollst√§ndigen Virtualisierung nahm die Bedeutung von Containern um fast 15 Jahre dramatisch ab, und in der Unternehmenswelt herrschten dicke VMs.  Zu dieser Zeit befanden sich Container bei Hostern, die Hunderte von Webservern des gleichen Typs bereitstellten, bei denen ihre Leichtigkeit gefragt war.  Aber in den letzten Jahren, seit etwa 2015, sind Container in Form von Cloud-nativen Anwendungen in die Unternehmensrealit√§t zur√ºckgekehrt. <br><br><h3>  Beh√§lter 0.1 </h3><br><h4>  Chroot </h4><br>  Der Prototyp von Containern war 1979 Chroot. <br><br>  ‚ÄûChroot ist das √Ñndern des Stammverzeichnisses unter Unix-√§hnlichen Betriebssystemen.  Ein Programm, das mit einem ge√§nderten Stammverzeichnis gestartet wird, hat nur Zugriff auf die in diesem Verzeichnis enthaltenen Dateien. ‚Äú <br><br>  Das hei√üt  Tats√§chlich erfolgt die Isolation nur auf Dateisystemebene, andernfalls ist dies nur ein normaler Vorgang im Betriebssystem. <br><br><h4>  Freebsd Gef√§ngnis </h4><br>  Deutlich weiter fortgeschritten war das 1999 erschienene Gef√§ngnis f√ºr freies BSD.  Mit Jail konnten Sie vollwertige virtuelle Betriebssysteminstanzen mit eigenen Anwendungen und Konfigurationsdateien erstellen, die auf der Basis-FreeBSD basieren.  Sicherlich gibt es diejenigen, die sagen - und was macht das Gef√§ngnis in Containern, denn das ist Paravirtualisierung!  Und sie werden teilweise recht haben. <br><br>  Vor der vollst√§ndigen Virtualisierung (und ihrer Variante in Form einer Paravirtualisierung) fehlt dem Gef√§ngnis jedoch die M√∂glichkeit, den Kernel einer anderen Version in der Gast-VM auszuf√ºhren und mit der Migration der VM auf ein anderes Hostsystem zu clustern. <br><br><h4>  Solaris-Zonen </h4><br>  Solaris Zones ist eine Betriebssystemvirtualisierungstechnologie (Containervirtualisierung), die 2004 in Sun Solaris eingef√ºhrt wurde.  Das Grundprinzip ist ein geringer Virtualisierungsaufwand. <br><br>  Die Migration auf OpenSolaris und darauf basierende Distributionen, die 2019 verf√ºgbar waren, erlangte kaum Popularit√§t. <br><br><h3>  Beh√§lter 1.0 </h3><br>  Im Zeitalter von Containern 1.0 sind zwei Hauptrichtungen der Containerisierung aufgetreten: kommerzielle Produkte f√ºr Hosting-Anbieter und Containerisierung von Anwendungen. <br><br><h4>  Virtuozzo / OpenVZ </h4><br>  Russische SWsoft stellte 2001 seine erste Version der Containervirtualisierung Virtuozzo vor, die auf den Markt von Hosting-Anbietern ausgerichtet ist.  Aufgrund der Entschlossenheit und der spezifischen kommerziellen Zielgruppe erwies sich das Produkt als recht erfolgreich und gewann an Popularit√§t.  Technologisch wurde 2002 der gleichzeitige Betrieb von 2500 Containern auf einem Server mit 8 Prozessoren demonstriert. <br><br>  2005 erschien eine offene Version von Virtuozzo-Containern f√ºr Linux mit dem Namen OpenVZ.  Und wurde fast zum Goldstandard f√ºr das Hosting von VPS. <br><br><h4>  Lxc </h4><br>  LinuX Containers (LXC) ist eine weitere bekannte Containervirtualisierung basierend auf Namespaces und cgroups, die 2008 ver√∂ffentlicht wurde. Sie liegt den derzeit beliebten Dockern usw. zugrunde. <br><br><h3>  Container 1.1 (Anwendungsvirtualisierung) </h3><br>  Wenn die verbleibenden Container das Basisbetriebssystem in Segmente unterteilen sollen, k√∂nnen Sie diese Schicht des Systems abrei√üen und in einer einzigen Box mit der Anwendung und der gesamten Umgebung verpacken.  Und dann kann dieses fertige Paket als normale Anwendung auf Benutzerebene gestartet werden. <br><br><h4>  App-v </h4><br>  Microsoft Application Virtualization (App-V), ehemals Softricity SoftGrid - Technologie zum Containerisieren bestimmter Anwendungen (der Container ist das Gegenteil) in einer isolierten Sandbox, dann Microsoft.  Im Jahr 2006 erwarb Microsoft das Softricity-Startup, das den Container tats√§chlich umdrehte. <br><br><h4>  Thinapp </h4><br>  VMware ThinApp (ehemals Thinstall) ist ein Application Containerization-Produkt von Jilt, das 2008 von VMware erworben wurde.  VMware sch√§tzt, dass 90-95% aller Anwendungen auf der Welt diese Technologie verwenden. <br><br><h3>  Beh√§lter 2.0 </h3><br>  Die Entstehungsgeschichte von Containern 2.0 ist stark mit einer Ver√§nderung des Softwareentwicklungsprozesses verbunden.  Der Wunsch des Unternehmens, einen so wichtigen Parameter wie Time-to-Market zu reduzieren, zwang die Entwickler, die Ans√§tze zur Erstellung von Softwareprodukten zu √ºberdenken.  Die Waterfall-Entwicklungsmethode (lange Release-Zyklen, die gesamte Anwendung wird aktualisiert) wird durch Agile ersetzt (kurze, zeitlich festgelegte Release-Zyklen, Anwendungskomponenten werden unabh√§ngig aktualisiert) und zwingt Entwickler, monolithische Anwendungen in Komponenten zu unterteilen.  W√§hrend die Komponenten von monolithischen Anwendungen immer noch recht gro√ü sind und nicht viele von ihnen in virtuellen Maschinen platziert werden k√∂nnen, sind virtuelle Maschinen nicht sehr geeignet, wenn eine Anwendung aus Dutzenden oder Hunderten von Komponenten besteht.  Dar√ºber hinaus tritt auch das Problem von zus√§tzlichen Softwareversionen, Bibliotheken und Abh√§ngigkeiten auf. Oft kommt es vor, dass unterschiedliche Komponenten unterschiedliche Versionen oder unterschiedlich konfigurierte Umgebungsvariablen erfordern.  Solche Komponenten m√ºssen auf verschiedene virtuelle Maschinen verteilt werden, weil  Es ist fast unm√∂glich, mehrere Softwareversionen gleichzeitig auf demselben Betriebssystem auszuf√ºhren.  Die Anzahl der VMs w√§chst wie eine Lawine.  Hier werden Container auf der B√ºhne angezeigt, sodass im Rahmen eines Gastbetriebssystems mehrere isolierte Umgebungen zum Starten von Anwendungskomponenten erstellt werden k√∂nnen.  Durch die Containerisierung von Anwendungen k√∂nnen Sie die Segmentierung einer monolithischen Anwendung in noch kleinere Komponenten fortsetzen und zum Paradigma einer Aufgabe = einer Komponente √ºbergehen - eines Containers, der als Microservice-Ansatz bezeichnet wird, und jede dieser Komponenten ist ein Microservice. <br><br><h4>  Beh√§lter unter der Haube </h4><br>  Wenn Sie sich den Container mit einem Blick des Systemadministrators ansehen, dann sind dies nur Linux-Prozesse mit eigenen Pids usw.  Was macht es m√∂glich, Prozesse, die in Containern ablaufen, voneinander zu isolieren und die Ressourcen des Gastbetriebssystems gemeinsam zu verbrauchen?  Zwei Standardmechanismen, die im Kernel jeder modernen Linux-Distribution vorhanden sind.  Der erste, Linux-Namespaces, der sicherstellt, dass jeder Prozess seine eigene Betriebssystemdarstellung (Dateisystem, Netzwerkschnittstellen, Hostname usw.) und der zweite, Linux-Kontrollgruppen (cgroups), sieht, dass der Prozess nur die Ressourcen des Gastbetriebssystems (CPU, Speicher) beansprucht Netzwerkbandbreite usw.). <br><br><h4>  Linux-Namespaces </h4><br>  Standardm√§√üig enth√§lt jedes Linux-System einen einzelnen Namespace.  Alle Systemressourcen wie Dateisysteme, Prozesskennungen (Prozess-IDs), Benutzer-IDs (Benutzer-IDs) und Netzwerkschnittstellen geh√∂ren zu diesem Namespace.  Aber niemand hindert uns daran, zus√§tzliche Namespaces zu erstellen und Systemressourcen zwischen ihnen neu zu verteilen. <br><br>  Wenn ein neuer Prozess gestartet wird, startet er in einem Namespace, einem Systemstandard oder einem der erstellten.  Bei diesem Vorgang werden nur die Ressourcen angezeigt, die in dem zum Ausf√ºhren verwendeten Namespace verf√ºgbar sind. <br><br>  Aber nicht alles ist so einfach, jeder Prozess geh√∂rt nicht zu einem einzelnen Namespace, sondern zu einem Namespace in jeder der Kategorien: <br><br><ul><li>  Mount (mnt) </li><li>  Prozess-ID (pid) </li><li>  Netzwerk (netto) </li><li>  Prozess√ºbergreifende Kommunikation (IPC) </li><li>  UTS </li><li>  Benutzer-ID (Benutzer) </li></ul><br>  Jeder Namespace-Typ isoliert eine entsprechende Ressourcengruppe.  Beispielsweise definiert der UTS-Bereich den f√ºr Prozesse sichtbaren Hostnamen und Dom√§nennamen.  Somit k√∂nnen zwei Prozesse innerhalb des Gastbetriebssystems davon ausgehen, dass sie auf verschiedenen Servern ausgef√ºhrt werden. <br><br>  Der Netzwerk-Namespace bestimmt die Sichtbarkeit der Netzwerkschnittstellen. Der Prozess darin sieht nur die Schnittstellen, die zu diesem Namespace geh√∂ren. <br><br><h4>  Linux-Kontrollgruppen (cgroups) </h4><br>  Linux Control Groups (cgroups) ist der Kernel-Systemmechanismus (Kernel) von Linux-Systemen, der den Verbrauch von Systemressourcen durch Prozesse begrenzt.  Jeder Prozess oder jede Gruppe von Prozessen kann nicht mehr Ressourcen (CPU, Speicher, Netzwerkbandbreite usw.) abrufen, als ihm zugewiesen ist, und kann nicht die "anderen" Ressourcen erfassen - die Ressourcen benachbarter Prozesse. <br><br><h3>  Docker </h3><br>  Wie oben erw√§hnt, hat Docker keine Container als solche erfunden.  Container gibt es schon seit vielen Jahren (auch auf LXC-Basis), doch Docker machte sie sehr beliebt, indem es das erste System entwickelte, mit dem Container einfach und problemlos zwischen verschiedenen Maschinen ausgetauscht werden konnten.  Docker hat ein Tool zum Erstellen von Containern erstellt - das Packen der Anwendung und ihrer Abh√§ngigkeiten sowie das Ausf√ºhren von Containern auf jedem Linux-System, auf dem Docker installiert ist. <br><br>  Ein wichtiges Merkmal von Docker ist die Portabilit√§t nicht nur der Anwendung selbst und ihrer Abh√§ngigkeiten zwischen v√∂llig verschiedenen Linux-Distributionen, sondern auch die Portabilit√§t der Umgebung und des Dateisystems.  Ein unter CentOS erstellter Container kann beispielsweise auf einem Ubuntu-System ausgef√ºhrt werden.  In diesem Fall wird das Dateisystem innerhalb des gestarteten Containers von CentOS geerbt, und die Anwendung geht davon aus, dass es auf CentOS ausgef√ºhrt wird.  Dies √§hnelt in gewisser Weise einem OVF-Image einer virtuellen Maschine, das Konzept eines Docker-Images verwendet jedoch Ebenen.  Dies bedeutet, dass beim Aktualisieren nur eines Teils des Image nicht das gesamte Image erneut heruntergeladen werden muss. Es reicht aus, nur die ge√§nderte Ebene herunterzuladen, als ob es im OVF-Image m√∂glich w√§re, das Betriebssystem zu aktualisieren, ohne das gesamte Image zu aktualisieren. <br><br>  Docker hat ein √ñkosystem zum Erstellen, Speichern, √úbertragen und Starten von Containern geschaffen.  Die Docker-Welt besteht aus drei Hauptkomponenten: <br><br><ul><li>  Bilder - ein Bild, dies ist die Entit√§t, die Ihre Anwendung, die erforderliche Umgebung und andere Metadaten enth√§lt, die zum Starten des Containers erforderlich sind. </li><li>  Register - Repository, Speicherort f√ºr Docker-Images.  Es gibt eine Vielzahl von Repositorys, die von offiziell - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hub.docker.com</a> bis hin zu privaten Repositorys reichen, die in der Infrastruktur des Unternehmens bereitgestellt werden. </li><li>  Container - Ein Container, ein Linux-Container, der aus einem Docker-Image erstellt wurde.  Wie oben erw√§hnt, handelt es sich um einen Linux-Prozess, der auf einem Linux-System mit installiertem Docker ausgef√ºhrt wird und von anderen Prozessen und dem Betriebssystem selbst isoliert ist. </li></ul><br>  Ber√ºcksichtigen Sie den Lebenszyklus des Containers.  Zun√§chst erstellt ein Entwickler mit seiner Anwendung (dem Docker-Build-Befehl) ein Docker-Image, das vollst√§ndig von Grund auf neu erstellt wurde oder bereits erstellte Images als Grundlage verwendet (denken Sie an Ebenen).  Au√üerdem kann dieses Image vom Entwickler direkt auf seinem eigenen Computer gestartet oder auf einen anderen Computer √ºbertragen werden - den Server.  Aus Gr√ºnden der Portabilit√§t werden h√§ufig Repositorys verwendet (Docker-Push-Befehl) - sie laden das Image in das Repository.  Danach kann das Image auf einen anderen Computer oder Server heruntergeladen werden (Docker Pull).  Erstellen Sie abschlie√üend einen Arbeitscontainer (Docker-Lauf) aus diesem Image. <br><br><h3>  Kubernetes </h3><br>  Wie wir bereits gesagt haben, bedeutet das Konzept der Mikrodienste, eine monolithische Anwendung in viele kleine Dienste zu unterteilen, die normalerweise eine einzige Funktion ausf√ºhren.  Nun, wenn es Dutzende solcher Dienste gibt, k√∂nnen sie immer noch manuell √ºber beispielsweise Docker verwaltet werden.  Aber was tun, wenn es Hunderte und Tausende solcher Dienste gibt?  Zus√§tzlich zur industriellen Umgebung ben√∂tigen Sie eine Testumgebung und zus√§tzliche Umgebungen f√ºr verschiedene Versionen des Produkts, d. H.  multiplizieren Sie mit 2, mit 3 oder noch mehr.  Google hatte die gleichen Probleme: Die Ingenieure von Google waren die ersten, die Container im industriellen Ma√üstab einsetzten.  So wurde Kubernetes (K8s) geboren, unter dem Namen Borg in den W√§nden von Google-Produkt erstellt, sp√§ter der breiten √ñffentlichkeit zug√§nglich gemacht und umbenannt. <br><br>  K8s ist ein System, mit dem Containeranwendungen (Microservices) einfach bereitgestellt, verwaltet und √ºberwacht werden k√∂nnen.  Wie wir bereits wissen, ist jeder Linux-Rechner zum Starten von Containern geeignet und die Container sind voneinander isoliert. K8s k√∂nnen verschiedene Server mit unterschiedlicher Hardware und unter der Kontrolle verschiedener Linux-Distributionen verwalten.  All dies hilft uns, die verf√ºgbare Hardware effektiv zu nutzen.  Wie bei der Virtualisierung bietet K8s einen gemeinsamen Ressourcenpool zum Starten, Verwalten und √úberwachen unserer Mikrodienste. <br><br>  Da dieser Artikel haupts√§chlich f√ºr Virtualisierungstechniker gedacht ist, um ein allgemeines Verst√§ndnis der Funktionsprinzipien und der Hauptkomponenten von K8s zu erhalten, empfehlen wir Ihnen, den Artikel zu lesen, in dem die Parallele zwischen K8s und VMware vSphere dargestellt wird: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://medium.com/@pryalukhin/kubernetes-introduction-for-vmware- users-232cc2f69c58</a> <br><br><h2>  Geschichte der X86 Industrial Virtualization </h2><br><h3>  VMware </h3><br>  VMware erschien 1998 und begann mit der Entwicklung eines zweiten Hypervisortyps, der sp√§ter als VMware Workstation bekannt wurde. <br><br>  Das Unternehmen trat 2001 mit zwei Hypervisoren in den Servermarkt ein - GSX (Ground Storm X, zweiter Typ) und ESX (Elastic Sky X, erster Typ).  Im Laufe der Zeit sind die Aussichten des zweiten Typs in Serveranwendungen offensichtlich geworden, d.h.  Keine.  Und die bezahlte GSX wurde zuerst in einen kostenlosen VMware-Server verwandelt und dann vollst√§ndig gestoppt und begraben. <br><br>  Im Jahr 2003 wurden das zentrale Managementsystem von Virtual Center, die vSMP-Technologie und die Livemigration von virtuellen Maschinen eingef√ºhrt. <br><br>  Im Jahr 2004 wurde VMware von EMC, einem Speichergiganten, √ºbernommen, jedoch betriebsunabh√§ngig gelassen. <br><br>  Im Jahr 2008, als VMware de facto zum Industriestandard wurde, stimulierte es das schnelle Wachstum wettbewerbsf√§higer Angebote - Citrix, Microsoft usw. Es wird deutlich, dass eine kostenlose Version des Hypervisors erforderlich ist, was unm√∂glich war - da ein √ºbergeordneter Abschnitt in ESX recht kommerzielles RHEL verwendete.  Das Projekt, RHEL durch etwas einfacheres und kostenloses zu ersetzen, wurde 2008 mit dem System busybox umgesetzt.  Das Ergebnis ist ESXi, das heute allen bekannt ist. <br><br>  Parallel dazu entwickelt sich das Unternehmen durch interne Projekte und Akquisitionen von Startups.  Vor einigen Jahren nahm eine Liste von VMware-Produkten einige A4-Seiten ein. Sagen wir einfach.  VMware f√ºr 2019 ist mit einem Marktanteil von mehr als 70% und einem absoluten Technologief√ºhrer nach wie vor der De-facto-Standard im On-Premise-Markt f√ºr die vollst√§ndige Virtualisierung von Unternehmen. Eine detaillierte √úberpr√ºfung der Geschichte verdient einen eigenen, sehr umfangreichen Artikel. <br><br><h3>  Connectix </h3><br>  Connectix wurde 1988 gegr√ºndet und arbeitete bis zur Einf√ºhrung der Virtualisierung an einer Vielzahl von Systemdienstprogrammen.  1997 wurde das erste VirtualPC-Produkt f√ºr den Apple Macintosh entwickelt, mit dem Windows auf einer virtuellen Maschine ausgef√ºhrt werden kann.  Die erste Version von VirtualPC f√ºr Windows erschien im Jahr 2001. <br><br>  Im Jahr 2003 kaufte Microsoft VirtualPC und im Einvernehmen mit Connectix wechselten die Entwickler zu Microsoft.  Danach wurde Connectix geschlossen. <br><br>  Das VHD-Format (Virtual Hard Disk) wurde von Connectix f√ºr VirtualPC entwickelt. Zur Erinnerung: Die virtuellen Festplatten von Hyper-V-Maschinen enthalten in ihrer Signatur ‚Äûconectix‚Äú. <br>  Wie Sie sich vorstellen k√∂nnen, ist der virtuelle PC ein klassischer Desktop-Hypervisor des zweiten Typs. <br><br><h3>  Microsoft </h3><br>  Die Reise von Microsoft in die industrielle Virtualisierung begann mit dem Kauf von Connectix und dem Rebranding von Connectix Virtual PC in Microsoft Virtual PC 2004. Der f√ºr eine Weile entwickelte virtuelle PC wurde unter Windows 7 unter dem Namen Windows Virtual PC aufgenommen. In Windows 8 und h√∂her wurde Virtual PC durch ersetzt Desktop-Version von Hyper-V. <br><br>  Basierend auf Virtual PC wurde der Virtual Server Server Hypervisor erstellt, der bis Anfang 2008 bestand.  Aufgrund des offensichtlichen technologischen Verlusts vor VMware ESX wurde beschlossen, die Entwicklung des zweiten Hypervisortyps zugunsten des eigenen ersten Hypervisortyps, Hyper-V, zu drosseln.  Es gibt eine inoffizielle Meinung in der Branche, dass Hyper-V Xen in der Architektur √ºberraschend √§hnlich ist.  Ungef√§hr das Gleiche wie .Net in Java. <br><blockquote>  "Nat√ºrlich k√∂nnte man denken, dass Microsoft die Idee von Java gestohlen hat."  Aber das stimmt nicht, Microsoft hat sie inspiriert!  - (aus einer Rede eines Microsoft-Vertreters bei der Pr√§sentation von Windows 2003 Server) </blockquote><br>  Aus den kuriosen Augenblicken ist zu ersehen, dass die Verwendung eigener Virtualisierungsprodukte innerhalb von Microsoft in den letzten null Jahren, gelinde gesagt, optional war.  Es gibt Screenshots von Technet aus Artikeln zur Virtualisierung, in denen das VMware Tools-Logo deutlich in der Taskleiste vorhanden ist.  Au√üerdem f√ºhrte Mark Russinovich auf der Plattform 2009 in Moskau eine Demonstration mit VMware Workstation durch. <br><br>  Um neue M√§rkte zu erschlie√üen, hat Microsoft mit Azure eine eigene √∂ffentliche Cloud erstellt, die einen stark modifizierten Nano-Server mit Hyper-V-, S2D- und SDN-Unterst√ºtzung als Plattform verwendet.  Es ist erw√§hnenswert, dass Azure anfangs an einigen Stellen weit hinter den On-Premise-Systemen zur√ºckblieb.  Beispielsweise wurde die Unterst√ºtzung f√ºr virtuelle Maschinen der zweiten Generation (mit Unterst√ºtzung f√ºr sicheren Start, Start von GPT-Partitionen, PXE-Start usw.) erst 2018 in Azure angezeigt.  In On-Premise sind VMs der zweiten Generation seit Windows Server 2012R2 bekannt.  Gleiches gilt f√ºr Portall√∂sungen: Bis 2017 verwendeten Azure und das Windows Azure Pack (mandantenf√§hige Cloud-L√∂sung mit SDN- und Shielded VM-Unterst√ºtzung, die 2013 den System Center App Controller ersetzte) dasselbe Portaldesign.  Nachdem Microsoft einen Kurs zu √∂ffentlichen Clouds angek√ºndigt hatte, trat Azure vor, um verschiedene Kenntnisse zu entwickeln und zu implementieren.  Um das Jahr 2016 herum kann man ein v√∂llig logisches Bild beobachten: Jetzt stammen alle Innovationen in Windows Server von Azure, aber nicht in die entgegengesetzte Richtung.  Die Tatsache, dass Teile der Dokumentation ‚Äûwie sie sind‚Äú von Azure in den On-Premise-Modus kopiert werden, weist darauf hin (siehe Dokumentation zu Azure SDN und Network Controller), was einerseits auf die Einstellung zu On-Premise-L√∂sungen hinweist und andererseits auf die Beziehung der L√∂sungen hinweist in Bezug auf Einheiten und Architektur.  Wer hat von wem kopiert und wie ist es wirklich - eine strittige Frage. <br><br>  Im M√§rz 2018 gab Satya Nadela (Microsoft-CEO) offiziell bekannt, dass die √∂ffentliche Cloud zur Priorit√§t des Unternehmens wird.  Dies ist offensichtlich ein Symbol f√ºr das allm√§hliche Zusammenfalten und Verblassen der Serverlinie f√ºr On-Premise-Produkte (eine Stagnation war jedoch bereits 2016 zu beobachten, wurde jedoch mit der ersten Betaversion von Windows Server und anderen On-Premise-Produktlinien best√§tigt), mit Ausnahme von Azure Edge - dem minimal erforderlichen Server Infrastruktur im B√ºro des Kunden f√ºr Dienste, die nicht in die Cloud √ºbertragen werden k√∂nnen. <br><br><h3>  Virtuelles Eisen </h3><br>   2003   Virtual Iron    Xen     ,       . <br>  2009   Oracle      Oracle VM     x86.   Oracle VM     SPARC. <br><br><h3> Innotek </h3><br>   2007 Innotek GmbH       VirtualBox,    .           . <br><br>  2008     Sun,   2010      Oracle. Oracle       . <br> VirtualBox      ‚Äî VDI (), VMDK (VMware), VHD (Microsoft).     , Windows, macOS, Linux, Solaris  OpenSolaris.   VirtualBox  FreeBSD. <br><br><h3>  Ibm </h3><br>  ‚Äî             ( :  60- 1     ). ,     :           .      -.   -        ,          .     (!)   (   ,     ,  ‚Äì  ).         ‚Äì    ,  .          -   ,        IBM   . <br><br>  60-  XX      ,      ,   .          .      (   Pay as you go   ,  ?).      .             . <br><br>               IBM    IBM System/360-67.   CP/CMS ,  ,      . CP (Control Program) ‚Äì  ,     ¬´ ¬ª (VM). CMS ( ‚Äì Cambridge Monitor System,    Conversational Monitor System)      . ,  CMS           z/VM.  ,        90-         (     ,       )            Time-Sharing.     ,     ‚Äì      ,     .   ,       ,      . <br><br>      CP/CMS   VM/370     System/370 2  1972 .       ‚Äì VM,       VM     IBM.      ,          (           ) ‚Äì         VM/370.  :         ()     System/370     VM/370     (   ! ‚Äì      ).        . <br><br> 80-      ¬´ ¬ª. VM      ,       .   ,             VM.         (Logical Partition Access Resources  LPAR),      .          ,  -     VM,    LPAR         VM  .   -   ,     .     ,  VM     ,    80-: <br><br> VM/SP ‚Äì        System z <br> VM/SP HPO (High Performance Option) ‚Äì   VM/SP     System z <br> VM/XA (extended architecture) ‚Äì  VM     S/370. <br><br>   90-     x86     ,     .      ,  ,    -     .     ,      ,        ,      .           ,  IBM ,     . <br><br><h3> Linux Xen </h3><br> Xen ( ) ‚Äî ,           (Ian Pratt)      GPL.      2003 .          ,   XenSource. <br>  2013  Xen    Linux Foundation. <br><br><h4> XenSource </h4><br>        XenServer  XenEnterprise,   2007     Citrix. <br><br><h4> Citrix XenServer </h4><br>  XenSource  500  , Citrix      .  ,     ,   XenServer    ,       .        VMware ESX      XenServer          2009 .      XenCenter  . <br>        Citrix  Microsoft    ,  ,      . <br><br>     , Citrix XenApp  XenDesktop       Xen. <br><br><h4>  Amazon </h4><br> Amazon      IaaS   EC2 (Elastic Compute)  2006 .   EC2   Xen,   Amazon     ,          ,         . <br><br>  2017       EC2  KVM   .  ,       EC2  KVM   . <br><br><h3> Linux QEMU / KVM </h3><br> QEMU (Quick EMUlator) ‚Äî        ,    GPL v2.  x86   ARM, MIPS, RISC-V, PowerPC, SPARC, SPARC64.       QEMU   ,    .    QEMU  x86    ,          KVM (Kernel-based Virtual Machine)  Qumranet. <br><br>  KVM ‚Äî  QEMU KVM,       qcow2 (QEMU copy-on-write 2)       KVM. <br>   ,   QEMU     , QEMU / KVM    . <br><br><h3> Qumranet </h3><br>  ,       KVM   SPICE.   2005 ,     KVM    Linux. 4  2008    Red Hat. <br><br><h3>  Roter Hut </h3><br>      GNU/Linux,  2010   Red Hat       Xen. ,        ,     .         ,    KVM.   Red Hat Enterprise Virtualization 2.2 (RHEV)    2010        VDI   Citrix  VMware     Qumranet,    .       , Live Migration,  2  (  RHEL). ,      , Red Hat   Xen    . <br><br> 28  2018   IBM    Red Hat. <br><br><h4> OpenStack </h4><br>   OpenStack     -   VMware      x86.    2010    Rackspace Hosting ( )  NASA (    Nebula).     ,   2012  VMware     OpenStack,     -. <br><br>       Canonical (Ubuntu Linux), Debian, SUSE, Red Hat, HP, Oracle. <br><br>     .  2012 NASA   ,     AWS.   2016 HPE     Helion   OpenStack. <br><br>    OpenStack      KVM.         OpenStack       ,   OpenStack    . <br><br>       OpenStack         .     ‚Äî           OpenStack. , ,              . <br><br>  OpenStack    ,                   .   OpenStack        ‚Äî              ,     . <br><br>   OpenStack             .       ,   ,   OpenStack    . <br><br><h4> Nutanix AHV </h4><br> Nutanix          VMware vSphere.   -      ,  -      VMware       ,           .       KVM,       AHV (Acropolis HyperVisor). <br><br><h4> Parallels </h4><br>  7  Virtuozzo       KVM. <br><br><h4> Proxmox </h4><br> Proxmox VE (Virtual Environment) ‚Äî        Proxmox Server Solutions GmbH   Debian Linux.     2008 . <br>       LXC ( OpenVZ),      KVM. <br><br><h4> Parallels / Virtuozzo /  </h4><br>   1999     SWsoft     .  2003    - Plesk. <br><br>  2004  SWsoft    Parallels      Parallels Workstation (     Windows). <br>      Parallels        Parallels Desktop  Mac (     MacOS). <br><br>           ,    .           Virtuozzo  OpenVZ,     .  Parallels             Parallels Bare Metal Server ( Parallels Hypervisor  Cloud Server,   Virtuozzo),     Cloud Storage.          . <br><br>  2015           ‚Äî  (    )   Virtuozzo,         .       Depo  IBS     -. <br><br>   7 Virtuozzo    ,  7      KVM. ,  ‚Äî     KVM. <br>   ,        2019 . <br><br> Parallels Desktop    Parallels   Corel.      Odin   IngramMicro.      Virtuozzo / . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de474776/">https://habr.com/ru/post/de474776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de474760/index.html">Wir befestigen ngx-translate in Angular-Anwendung. Praktische Anleitung</a></li>
<li><a href="../de474762/index.html">Seminar: Hybride IT-L√∂sungen f√ºr Unternehmen. 14. November Moskau</a></li>
<li><a href="../de474768/index.html">Open Broadcast der Haupthalle HighLoad ++ 2019</a></li>
<li><a href="../de474770/index.html">So f√ºhren wir Abrechnungsregressionstests in SAP HCM durch</a></li>
<li><a href="../de474772/index.html">Ein Startup, das in 21 Tagen mit AI ein Heilmittel entwickelt hat</a></li>
<li><a href="../de474782/index.html">√úberblick √ºber die Sprachsynthese-Technologie</a></li>
<li><a href="../de474784/index.html">Arcade Stick Story</a></li>
<li><a href="../de474788/index.html">Organisation von Routen in Laravel</a></li>
<li><a href="../de474790/index.html">Verhandlungsgeschichten</a></li>
<li><a href="../de474792/index.html">6. bis 8. Dezember - Rosbank Tech.Madness Hackathon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>