<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶é üë©üèº‚Äçüåæ üë©üèæ‚Äçü§ù‚Äçüë®üèª Almacenamiento en cl√∫ster para peque√±os cl√∫steres web basados ‚Äã‚Äãen drbd + ocfs2 üßíüèº üëä üàÇÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="De lo que hablaremos: 
 C√≥mo implementar r√°pidamente el almacenamiento compartido para dos servidores basado en soluciones drbd + ocfs2. 

 Para quien...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Almacenamiento en cl√∫ster para peque√±os cl√∫steres web basados ‚Äã‚Äãen drbd + ocfs2</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445612/">  <b>De lo que hablaremos:</b> <br>  C√≥mo implementar r√°pidamente el almacenamiento compartido para dos servidores basado en soluciones drbd + ocfs2. <br><br>  <b>Para quien ser√° √∫til:</b> <br>  El tutorial ser√° √∫til para los administradores del sistema y cualquier persona que elija un m√©todo de implementaci√≥n de almacenamiento o quiera probar una soluci√≥n. <br><br><h3>  ¬øQu√© decisiones hemos rechazado y por qu√©? </h3><br>  A menudo nos enfrentamos a una situaci√≥n en la que necesitamos implementar un almacenamiento com√∫n en un peque√±o cl√∫ster web con un buen rendimiento de lectura y escritura.  Probamos varias opciones para implementar un repositorio com√∫n para nuestros proyectos, pero poco pudo satisfacernos de inmediato para varios indicadores.  Ahora expliquemos por qu√©. <br><br><ul><li> Glusterfs no nos satisfizo con el rendimiento de lectura y escritura, hubo problemas con la lectura simult√°nea de una gran cantidad de archivos, hubo una gran carga en la CPU.  El problema con la lectura de archivos podr√≠a resolverse solicit√°ndolos directamente en el ladrillo, pero esto no siempre es aplicable y generalmente es incorrecto. </li></ul><br><ul><li>  A Ceph no le gust√≥ la excesiva complejidad, que puede ser da√±ina en proyectos con 2-4 servidores, especialmente si el proyecto es atendido posteriormente.  Nuevamente, existen serias limitaciones de rendimiento que lo obligan a construir cl√∫steres de almacenamiento separados, como ocurre con los glusterfs. </li></ul><br><ul><li>  El uso de un servidor nfs para implementar el almacenamiento compartido genera problemas de tolerancia a fallas. </li></ul><br><ul><li>  s3 es una excelente soluci√≥n popular para un cierto rango de tareas, pero no es un sistema de archivos, lo que reduce el alcance. </li></ul><a name="habracut"></a><br><ul><li>  lsyncd.  Si ya comenzamos a hablar de "sistemas que no son archivos", entonces vale la pena revisar esta popular soluci√≥n.  No solo no es adecuado para el intercambio bidireccional (sino que si realmente lo desea, puede hacerlo), sino que tampoco funciona de manera estable en una gran cantidad de archivos.  Una buena adici√≥n a todo ser√° que es de un solo subproceso.  La raz√≥n est√° en la arquitectura del programa: usa inotify para monitorear los objetos de trabajo que cuelga al inicio y durante el reescaneo.  Rsync se utiliza como medio de transmisi√≥n. </li></ul><br><h3>  Tutorial: c√≥mo implementar almacenamiento compartido basado en drbd + ocfs2 </h3><br>  Una de las soluciones m√°s convenientes para nosotros fue un mont√≥n de <b>ocfs2 + drbd</b> .  Ahora le mostraremos c√≥mo implementar r√°pidamente el almacenamiento compartido para dos servidores en funci√≥n de una base de datos de soluciones.  Pero primero, un poco sobre los componentes: <br><br>  <b>DRBD</b> es el sistema de almacenamiento Linux est√°ndar que permite que los datos se repliquen entre bloques de servidores.  La aplicaci√≥n principal es construir almacenamiento tolerante a fallas. <br><br>  <b>OCFS2</b> es un sistema de archivos que proporciona el uso compartido del mismo almacenamiento por m√∫ltiples sistemas.  Se incluye en la distribuci√≥n de Linux y es un m√≥dulo de kernel y herramientas de espacio de usuario para trabajar con FS.  OCFS2 puede usarse no solo sobre DRBD, sino tambi√©n sobre iSCSI con m√∫ltiples conexiones.  En nuestro ejemplo, usamos DRBD. <br><br>  Todas las acciones se realizan en el servidor ubuntu 18.04 en una configuraci√≥n m√≠nima. <br><br>  <b>Paso 1. Configurar DRBD:</b> <b><br></b> <br>  En el archivo /etc/drbd.d/drbd0.res describimos nuestro dispositivo de bloque virtual / dev / drbd0: <br><br><pre><code class="plaintext hljs">resource drbd0 { syncer { rate 1000M; } net { allow-two-primaries; after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } startup { become-primary-on both; } on drbd1 { meta-disk internal; device /dev/drbd0; disk /dev/vdb1; address 10.10.10.192:7789; } on drbd2 { meta-disk internal; device /dev/drbd0; disk /dev/vdb1; address 10.10.10.193:7789; } }</code> </pre> <br>  <i>meta-disco interno</i> : use los mismos dispositivos de bloque para almacenar metadatos <br>  <i>device / dev / drbd0</i> : use / dev / drbd0 como la ruta al volumen drbd. <br>  <i>disk / dev / vdb1</i> - use / dev / vdb1 <br>  <i>syncer {tasa 1000M;</i>  <i>}</i> - usa el ancho de banda del canal gigabit <br>  <i>allow-two-primaries</i> : una opci√≥n importante para permitir la aceptaci√≥n de cambios en dos servidores principales <br>  <i>after-sb-0pri, after-sb-1pri, after-sb-2pri</i> : opciones responsables de las acciones del nodo cuando se detecta splitbrain.  Consulte la documentaci√≥n para m√°s detalles. <br>  <i>Become-primary-on both</i> : establece ambos nodos en primario. <br><br>  En nuestro caso, tenemos dos m√°quinas virtuales absolutamente id√©nticas, con un ancho de banda de red virtual dedicado de 10 gigabits. <br><br>  En nuestro ejemplo, los nombres de red de dos nodos del cl√∫ster son drbd1 y drbd2.  Para un funcionamiento correcto, debe asignar los nombres y las direcciones IP de los nodos en / etc / hosts. <br><br><pre> <code class="bash hljs">10.10.10.192 drbd1 10.10.10.193 drbd2</code> </pre> <br>  <b>Paso 2. Configure los nodos:</b> <br><br>  En ambos servidores ejecutamos: <br><pre> <code class="bash hljs">drbdadm create-md drbd0</code> </pre> <br><img src="https://habrastorage.org/webt/8d/_s/cu/8d_scupzapinrfgfcfybxipxfbk.png" alt="imagen"><br><br><pre> <code class="bash hljs">modprobe drbd drbdadm up drbd0 cat /proc/drbd</code> </pre> <br>  Obtenemos lo siguiente: <br><br><img src="https://habrastorage.org/webt/c4/zp/kx/c4zpkxvdupcfqbbmfccp3bbjqws.png" alt="imagen"><br><br>  Puedes comenzar la sincronizaci√≥n.  En el primer nodo que necesita hacer: <br><pre> <code class="bash hljs">drbdadm primary --force drbd0</code> </pre> <br>  Nos fijamos en el estado: <br><pre> <code class="bash hljs">cat /proc/drbd</code> </pre> <br><img src="https://habrastorage.org/webt/vd/nn/xd/vdnnxdcrmmufhf7sdz_gxyzcezy.png" alt="imagen"><br><br>  Genial, la sincronizaci√≥n ha comenzado.  Estamos esperando el final y vemos la imagen: <br><br><img src="https://habrastorage.org/webt/fx/j1/5k/fxj15krpylof_f5uok4zq8xfznq.png" alt="imagen"><br><br>  <b>Paso 3. Comenzamos la sincronizaci√≥n en la segunda nota:</b> <br><br><pre> <code class="bash hljs">drbdadm primary --force drbd0</code> </pre><br>  Obtenemos lo siguiente: <br><br><img src="https://habrastorage.org/webt/zl/8f/_w/zl8f_ws9wacesr88mtoo9b_dxbm.png" alt="imagen"><br><br>  Ahora podemos escribir en drbd desde dos servidores. <br><br>  <b>Paso 4. Instalar y configurar ocfs2.</b> <br><br>  Usaremos una configuraci√≥n bastante trivial: <br><br><pre> <code class="plaintext hljs">cluster: node_count = 2 name = ocfs2cluster node: number = 1 cluster = ocfs2cluster ip_port = 7777 ip_address = 10.10.10.192 name = drbd1 node: number = 2 cluster = ocfs2cluster ip_port = 7777 ip_address = 10.10.10.193 name = drbd2</code> </pre><br>  Debe escribirse en <i>/etc/ocfs2/cluster.conf</i> en ambos nodos. <br><br>  Cree FS en drbd0 en cualquier nodo: <br><pre> <code class="bash hljs">mkfs.ocfs2 -L <span class="hljs-string"><span class="hljs-string">"testVol"</span></span> /dev/drbd0</code> </pre><br>  Aqu√≠ creamos un sistema de archivos etiquetado testVol en drbd0 usando la configuraci√≥n predeterminada. <br><br><img src="https://habrastorage.org/webt/9l/sr/gl/9lsrgldfbco5qrzkjhkoh4oefly.png" alt="imagen"><br><br>  En / etc / default / o2cb debe estar configurado (como en nuestro archivo de configuraci√≥n) <br><pre> <code class="bash hljs">O2CB_ENABLED=<span class="hljs-literal"><span class="hljs-literal">true</span></span> O2CB_BOOTCLUSTER=ocfs2cluster</code> </pre> <br>  y ejecutar en cada nodo: <br><pre> <code class="bash hljs">o2cb register-cluster ocfs2cluster</code> </pre> <br>  Despu√©s de eso, encienda y agregue al inicio todas las unidades que necesitamos: <br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> drbd o2cb ocfs2 systemctl start drbd o2cb ocfs2</code> </pre> <br>  Parte de esto ya se estar√° ejecutando durante el proceso de configuraci√≥n. <br><br>  <b>Paso 5. Agregue puntos de montaje a fstab en ambos nodos:</b> <br><br><pre> <code class="bash hljs">/dev/drbd0 /media/shared ocfs2 defaults,noauto,heartbeat=<span class="hljs-built_in"><span class="hljs-built_in">local</span></span> 0 0</code> </pre> <br>  El directorio <i>/ media / shared</i> debe crearse de antemano. <br><br>  Aqu√≠ usamos las opciones noauto, lo que significa que el FS no se montar√° al inicio (prefiero montar el FS de red a trav√©s de systemd) y heartbeat = local, lo que significa usar el servicio heartbeat en cada nodo.  Tambi√©n hay un latido global, que es m√°s adecuado para grandes grupos. <br><br>  A continuaci√≥n, puede montar <i>/ media / shared</i> y verificar la sincronizaci√≥n del contenido. <br><br>  <b>Hecho</b>  Como resultado, obtenemos un almacenamiento m√°s o menos tolerante a fallas con escalabilidad y rendimiento decente. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/445612/">https://habr.com/ru/post/445612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445594/index.html">Herramientas para crear un sitio receptivo sin acceso al sitio</a></li>
<li><a href="../445596/index.html">Consejos y trucos de Kubernetes: p√°ginas de error personalizadas en NGINX Ingress</a></li>
<li><a href="../445600/index.html">[Encuesta y maldad] Hosting, se equivoquen</a></li>
<li><a href="../445602/index.html">PHP Rusia 2019: su "estadio" para el idioma de la primera liga</a></li>
<li><a href="../445608/index.html">Juego terminado: los analistas informan un aumento en el n√∫mero de ataques DDoS en el segmento de juegos</a></li>
<li><a href="../445618/index.html">Escribimos un sistema operativo en Rust. Implementando memoria de p√°gina (nuevo)</a></li>
<li><a href="../445620/index.html">¬øQu√© hace un escritor de UX?</a></li>
<li><a href="../445622/index.html">Nuevo en Java 12: The Teeing Collector</a></li>
<li><a href="../445626/index.html">¬øQu√© tan profundo es la madriguera del conejo? CLRium # 5: recolector de basura</a></li>
<li><a href="../445632/index.html">Del analizador del p√≥ster del teatro Python al bot Telegram. Parte 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>