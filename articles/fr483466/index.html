<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üâë üêû üç• Pr√©sentation de la m√©thode de r√©tropropagation üòÆ üö£üèΩ ü§∑üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour √† tous! Les vacances du Nouvel An ont pris fin, ce qui signifie que nous sommes √† nouveau pr√™ts √† partager du mat√©riel utile avec vous. Une tr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pr√©sentation de la m√©thode de r√©tropropagation</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/483466/">  <i>Bonjour √† tous!</i>  <i>Les vacances du Nouvel An ont pris fin, ce qui signifie que nous sommes √† nouveau pr√™ts √† partager du mat√©riel utile avec vous.</i>  <i>Une traduction de cet article a √©t√© pr√©par√©e en pr√©vision du lancement d'un nouveau volet sur le cours <a href="https://otus.pw/h0mh/">"Algorithmes pour les d√©veloppeurs"</a> .</i> <i><br><br></i>  <i>C'est parti!</i> <br><br><img src="https://habrastorage.org/webt/kl/6d/cd/kl6dcdek8egee8jyp_p0_7hcz30.png"><br><br><hr><br>  La m√©thode d'erreur de propagation inverse est probablement l'√©l√©ment le plus fondamental d'un r√©seau neuronal.  Il a √©t√© d√©crit pour la premi√®re fois dans les ann√©es 1960 et pr√®s de 30 ans plus tard, il a √©t√© popularis√© par Rumelhart, Hinton et Williams dans un article intitul√© <a href="https://www.nature.com/articles/323533a0">¬´Apprendre les repr√©sentations par des erreurs de propagation inverse¬ª</a> . <a name="habracut"></a><br><br>  La m√©thode est utilis√©e pour entra√Æner efficacement un r√©seau de neurones en utilisant la r√®gle dite de la cha√Æne (la r√®gle de diff√©renciation d'une fonction complexe).  Autrement dit, apr√®s chaque passage √† travers le r√©seau, la propagation arri√®re effectue un passage dans la direction oppos√©e et ajuste les param√®tres du mod√®le (poids et d√©placements). <br><br>  Dans cet article, je voudrais examiner en d√©tail d'un point de vue math√©matique le processus d'apprentissage et d'optimisation d'un r√©seau neuronal simple √† 4 couches.  Je pense que cela aidera le lecteur √† comprendre le fonctionnement de la r√©tropropagation et √† en comprendre l'importance. <br><br><h3>  D√©finir un mod√®le de r√©seau neuronal </h3><br>  Le r√©seau neuronal √† quatre couches se compose de quatre neurones dans la couche d'entr√©e, de quatre neurones dans les couches cach√©es et d'un neurone dans la couche de sortie. <br><br><img src="https://habrastorage.org/webt/d3/1z/7q/d31z7q7wxug2d-435f_t1fi19ki.png"><br>  <i>Une image simple d'un r√©seau neuronal √† quatre couches.</i> <br><br><h3>  Couche d'entr√©e </h3><br>  Sur la figure, les neurones violets repr√©sentent l'entr√©e.  Ils peuvent √™tre de simples quantit√©s scalaires ou plus complexes - des vecteurs ou des matrices multidimensionnelles. <br><br><img src="https://habrastorage.org/webt/su/ba/e1/subae1x6bn1yju51obgv3qaephm.png"><br>  <i>√âquation d√©crivant les entr√©es xi.</i> <br><br>  Le premier ensemble d'activations (a) est √©gal aux valeurs d'entr√©e.  ¬´Activation¬ª est la valeur d'un neurone apr√®s application de la fonction d'activation.  Voir ci-dessous pour plus de d√©tails. <br><br><h3>  Couches masqu√©es </h3><br>  Les valeurs finales dans les neurones cach√©s (dans la figure verte) sont calcul√©es en utilisant les entr√©es pond√©r√©es z <sup>l</sup> dans la couche I et les activations a <sup>I</sup> dans la couche L. Pour les couches 2 et 3, les √©quations seront les suivantes: <br><br>  Pour l = 2: <br><br><img src="https://habrastorage.org/webt/wh/ix/ho/whixhogzr32hedjvb-rackpht1c.png"><br><br>  Pour l = 3: <br><br><img src="https://habrastorage.org/webt/yv/eb/qq/yvebqquzvpxg3iu3xwqbhli-fpu.png"><br><br>  W <sup>2</sup> et W <sup>3</sup> sont les poids sur les couches 2 et 3, et b <sup>2</sup> et b <sup>3</sup> sont les d√©calages sur ces couches. <br><br>  Les activations a <sup>2</sup> et a <sup>3</sup> sont calcul√©es √† l'aide de la fonction d'activation f.  Par exemple, cette fonction f est non lin√©aire (comme <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmo√Øde</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> et <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">tangente hyperbolique</a> ) et permet au r√©seau d'√©tudier des motifs complexes dans les donn√©es.  Nous ne nous attarderons pas sur le fonctionnement des fonctions d'activation, mais si vous √™tes int√©ress√©, je vous recommande fortement de lire ce merveilleux <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">article</a> . <br><br>  Si vous regardez attentivement, vous verrez que tous les x, z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> , a <sup>3</sup> , W <sup>1</sup> , W <sup>2</sup> , b <sup>1</sup> et b <sup>2</sup> n'ont pas d'indices repr√©sent√©s sur la figure d'un r√©seau neuronal √† quatre couches.  Le fait est que nous avons combin√© toutes les valeurs des param√®tres dans des matrices regroup√©es par couches.  C'est une fa√ßon standard de travailler avec les r√©seaux de neurones, et c'est assez confortable.  Cependant, je vais parcourir les √©quations pour qu'il n'y ait pas de confusion. <br><br>  Prenons l'exemple de la couche 2 et de ses param√®tres.  Les m√™mes op√©rations peuvent √™tre appliqu√©es √† n'importe quelle couche du r√©seau neuronal. <br>  W <sup>1</sup> est la matrice des poids de dimension <i>(n, m)</i> , o√π <i>n</i> est le nombre de neurones de sortie (neurones dans la couche suivante), et <i>m</i> est le nombre de neurones d'entr√©e (neurones dans la couche pr√©c√©dente).  Dans notre cas, <i>n = 2</i> et <i>m = 4</i> . <br><br><img src="https://habrastorage.org/webt/ez/pw/6j/ezpw6j_huyb2cr5zkxinl9ylku8.png"><br><br>  Ici, le premier nombre dans l'indice de l'un des poids correspond √† l'indice des neurones dans la couche suivante (dans notre cas, c'est la deuxi√®me couche cach√©e), et le deuxi√®me nombre correspond √† l'indice des neurones dans la couche pr√©c√©dente (dans notre cas, c'est la couche d'entr√©e). <br><br>  <i>x</i> est le vecteur d'entr√©e de dimension ( <i>m</i> , 1), o√π <i>m</i> est le nombre de neurones d'entr√©e.  Dans notre cas, <i>m</i> = 4. <br><br><img src="https://habrastorage.org/webt/5a/by/8a/5aby8acxjiohf0-f5jrmgbfbxsi.png"><br><br>  b <sup>1</sup> est le vecteur de d√©placement de dimension ( <i>n</i> , 1), o√π <i>n</i> est le nombre de neurones dans la couche actuelle.  Dans notre cas, <i>n</i> = 2. <br><br><img src="https://habrastorage.org/webt/2u/5t/9v/2u5t9vhiftmq9fou4khqqvkynhc.png"><br><br>  En suivant l'√©quation pour z <sup>2,</sup> nous pouvons utiliser les d√©finitions ci-dessus de W <sup>1</sup> , x et b <sup>1</sup> pour obtenir l'√©quation z <sup>2</sup> : <br><br><img src="https://habrastorage.org/webt/-5/bz/kz/-5bzkzalwngzrkhbuwq52fugpkc.png"><br><br>  Regardez maintenant attentivement l'illustration du r√©seau de neurones ci-dessus: <br><br><img src="https://habrastorage.org/webt/_e/sj/ld/_esjld_rfdemfxpuztceadijwns.png"><br><br>  Comme vous pouvez le voir, z <sup>2</sup> peut √™tre exprim√© en termes de z <sub>1</sub> <sup>2</sup> et z <sub>2</sub> <sup>2</sup> , o√π z <sub>1</sub> <sup>2</sup> et z <sub>2</sub> <sup>2</sup> sont les sommes des produits de chaque valeur d'entr√©e x <sup>i</sup> par le poids correspondant W <sub>ij</sub> <sup>1</sup> . <br><br>  Cela conduit √† la m√™me √©quation pour z <sup>2</sup> et prouve que les repr√©sentations matricielles z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> et a <sup>3</sup> sont vraies. <br><br><h3>  Couche de sortie </h3><br>  La derni√®re partie du r√©seau neuronal est la couche de sortie, qui donne la valeur pr√©dite.  Dans notre exemple simple, il se pr√©sente sous la forme d'un seul neurone color√© en bleu et calcul√© comme suit: <br><br><img src="https://habrastorage.org/webt/fy/vh/05/fyvh05jvkxbosdqhaqak-vbzn0k.png"><br><br>  Encore une fois, nous utilisons la repr√©sentation matricielle pour simplifier l'√©quation.  Vous pouvez utiliser les m√©thodes ci-dessus pour comprendre la logique sous-jacente. <br><br><h3>  Distribution directe et √©valuation </h3><br>  Les √©quations ci-dessus forment une distribution directe √† travers le r√©seau neuronal.  Voici un bref aper√ßu: <br><br><img src="https://habrastorage.org/webt/pe/ya/fr/peyafrffaxqvrnjeito3i-j-gpk.png"><br><br>  <i>(1) - couche d'entr√©e</i> <i><br></i>  <i>(2) - la valeur du neurone dans la premi√®re couche cach√©e</i> <i><br></i>  <i>(3) - valeur d'activation sur la premi√®re couche cach√©e</i> <i><br></i>  <i>(4) - la valeur du neurone dans la deuxi√®me couche cach√©e</i> <i><br></i>  <i>(5) - valeur d'activation au deuxi√®me niveau cach√©</i> <i><br></i>  <i>(6) - couche de sortie</i> <br><br>  La derni√®re √©tape du passage direct consiste √† √©valuer la valeur de sortie pr√©dite <i>s par</i> rapport √† la valeur de sortie attendue <i>y</i> . <br><br>  La sortie y fait partie de l'ensemble de donn√©es d'apprentissage (x, y), o√π <i>x</i> est l'entr√©e (comme nous le rappelons de la section pr√©c√©dente). <br><br>  L'estimation entre <i>s</i> et <i>y</i> s'effectue via la fonction de perte.  Elle peut √™tre simple comme <a href="https://en.wikipedia.org/wiki/Mean_squared_error">erreur standard</a> ou plus complexe comme <a href="http://neuralnetworksanddeeplearning.com/chap3.html">entropie crois√©e</a> . <br><br>  Nous appelons cette fonction de perte C et la notons comme suit: <br><br><img src="https://habrastorage.org/webt/eg/s7/wh/egs7whz63c-ryaazd_r-vvuxbae.png"><br><br>  O√π le <i>co√ªt</i> peut √™tre √©gal √† l'erreur standard, √† l'entropie crois√©e ou √† toute autre fonction de perte. <br><br>  Sur la base de la valeur de C, le mod√®le ¬´sait¬ª combien ses param√®tres doivent √™tre ajust√©s afin d'approcher la valeur de sortie attendue de <i>y</i> .  Cela se produit en utilisant la m√©thode de r√©tropropagation. <br><br><h3>  Propagation de l'erreur et calcul des gradients </h3><br>  Bas√©e sur un article de 1989, la m√©thode de r√©tropropagation: <br><br>  <i>Ajuste constamment les poids des connexions dans le r√©seau pour minimiser la mesure de la diff√©rence entre le vecteur de sortie r√©el du r√©seau et le vecteur de sortie souhait√©</i> . <br>  et <br>  <i>... permet de cr√©er de nouvelles fonctions utiles qui distinguent la r√©tropropagation des m√©thodes ant√©rieures et plus simples ...</i> <br><br>  En d'autres termes, la r√©tropropagation vise √† minimiser la fonction de perte en ajustant les poids et les d√©calages du r√©seau.  Le degr√© d'ajustement est d√©termin√© par les gradients de la fonction de perte par rapport √† ces param√®tres. <br><br>  Une question se pose: <i>pourquoi calculer des gradients</i> ? <br><br>  Pour r√©pondre √† cette question, nous devons d'abord r√©viser certains concepts de l'informatique: <br><br>  Le gradient de la fonction C (x <sup>1</sup> , x <sup>2</sup> , ..., x <sup>m</sup> ) en x est le <a href="https://en.wikipedia.org/wiki/Partial_derivative">vecteur des d√©riv√©es partielles de</a> C par <i>rapport</i> √† <i>x</i> . <br><br><img src="https://habrastorage.org/webt/km/eo/zl/kmeozlylfgdy7nknsa0cq6ytaei.png"><br><br>  La d√©riv√©e de la fonction C refl√®te la sensibilit√© √† un changement de la valeur de la fonction (valeur de sortie) par rapport au changement de son argument <i>x</i> ( <a href="https://en.wikipedia.org/wiki/Derivative">valeur d'entr√©e</a> ).  En d'autres termes, la d√©riv√©e nous indique dans quelle direction C. se d√©place. <br><br>  Le gradient montre combien il est n√©cessaire de changer le param√®tre <i>x</i> (dans le sens positif ou n√©gatif) afin de minimiser C. <br><br>  Ces gradients sont calcul√©s √† l'aide d'une m√©thode appel√©e <a href="https://en.wikipedia.org/wiki/Chain_rule">r√®gle de</a> cha√Æne. <br>  Pour un poids (w <sup>jk</sup> ) <sub>l, le</sub> gradient est: <br><br><img src="https://habrastorage.org/webt/y7/97/mu/y797mumguvia31hytpq6gzq3gvy.png"><br><br>  <i>(1) R√®gle de cha√Æne</i> <i><br></i>  <i>(2) Par d√©finition, m est le nombre de neurones par l - 1 couche</i> <i><br></i>  <i>(3) Calcul d√©riv√©</i> <i><br></i>  <i>(4) Valeur finale</i> <i><br></i>  <i>Un ensemble d'√©quations similaire peut √™tre appliqu√© √† (b <sup>j</sup> ) <sub>l</sub></i> : <br><br><img src="https://habrastorage.org/webt/oo/7_/gz/oo7_gzmr5wpgql73bxefnlfob5u.png"><br><br>  <i>(1) R√®gle de cha√Æne</i> <i><br></i>  <i>(2) Calcul d√©riv√©</i> <i><br></i>  <i>(3) Valeur finale</i> <br>  La partie commune des deux √©quations est souvent appel√©e ¬´gradient local¬ª et s'exprime comme suit: <br><br><img src="https://habrastorage.org/webt/k9/4a/nc/k94anc1xfk3sjjgk08qf9_48fam.png"><br><br>  Un ¬´gradient local¬ª peut √™tre facilement d√©termin√© √† l'aide d'une r√®gle de cha√Æne.  Je ne peindrai pas ce processus maintenant. <br><br>  Les d√©grad√©s permettent d'optimiser les param√®tres du mod√®le: <br><br>  Jusqu'√† ce que le crit√®re d'arr√™t soit atteint, les op√©rations suivantes sont effectu√©es: <br><br><img src="https://habrastorage.org/webt/xw/31/1s/xw311s5zex1_sdlnvvucd9qqubk.png"><br><br>  <i>Algorithme d'optimisation des poids et des d√©calages</i> (√©galement appel√© descente de gradient) <br><ul><li>  Les valeurs initiales de <i>w</i> et <i>b</i> sont s√©lectionn√©es au hasard. </li><li>  Epsilon (e) est la vitesse d'apprentissage.  Il d√©termine l'effet du d√©grad√©. </li><li>  <i>w</i> et <i>b</i> sont des repr√©sentations matricielles de poids et de d√©calages. </li><li>  La d√©riv√©e de C par rapport √† <i>w</i> ou <i>b</i> peut √™tre calcul√©e en utilisant des d√©riv√©es partielles de C par rapport aux poids ou compensations individuels. </li><li>  La condition de terminaison est satisfaite d√®s que la fonction de perte est minimis√©e. </li></ul><br><br>  Je veux consacrer la derni√®re partie de cette section √† un exemple simple dans lequel nous calculons le gradient C par rapport √† un poids (w <sup>22</sup> ) <sub>2</sub> . <br><br>  Zoomons sur le bas du r√©seau neuronal susmentionn√©: <br><br><img src="https://habrastorage.org/webt/l7/0w/6d/l70w6d7hhxqjm0wqxtwoj8y8nxq.png"><br><br>  <i>Repr√©sentation visuelle de la r√©tropropagation dans un r√©seau neuronal</i> <br>  Le poids (w <sup>22</sup> ) <sub>2</sub> relie (a <sup>2</sup> ) <sub>2</sub> et (z <sup>2</sup> ) <sub>2</sub> , donc le calcul du gradient n√©cessite d'appliquer la r√®gle de cha√Æne sur (z <sup>2</sup> ) <sub>3</sub> et (a <sup>2</sup> ) <sub>3</sub> : <br><br><img src="https://habrastorage.org/webt/n_/mz/nm/n_mznmzn_dt1lqe-nyx7qoplcsa.png"><br><br>  Le calcul de la valeur finale de la d√©riv√©e de C de (a <sup>2</sup> ) <sub>3</sub> n√©cessite la connaissance de la fonction C. Puisque C d√©pend de (a <sup>2</sup> ) <sub>3</sub> , le calcul de la d√©riv√©e doit √™tre simple. <br><br>  J'esp√®re que cet exemple a r√©ussi √† faire la lumi√®re sur les math√©matiques derri√®re le calcul des gradients.  Si vous voulez en savoir plus, je vous recommande fortement de consulter la s√©rie d'articles Stanford NLP, o√π Richard Socher fournit 4 excellentes explications pour la r√©tropropagation. <br><br><h3>  Remarque finale </h3><br>  Dans cet article, j'ai expliqu√© en d√©tail comment la r√©tropropagation d'une erreur fonctionne sous le capot en utilisant des m√©thodes math√©matiques telles que le calcul des gradients, la r√®gle de cha√Æne, etc.  Conna√Ætre les m√©canismes de cet algorithme renforcera votre connaissance des r√©seaux de neurones et vous permettra de vous sentir √† l'aise lorsque vous travaillez avec des mod√®les plus complexes.  Bonne chance dans votre parcours d'apprentissage en profondeur! <br><br>  <b><i>C‚Äôest tout.</i></b>  <b><i>Nous invitons tout le monde √† un webinaire gratuit sur le th√®me <a href="https://otus.pw/h0mh/">"Arbre de segments: simple et rapide"</a> .</i></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr483466/">https://habr.com/ru/post/fr483466/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr483448/index.html">Disney - Le plus grand double sens de l'histoire de l'humanit√©</a></li>
<li><a href="../fr483454/index.html">Passer de Mercurial √† GIT dans Atlassian Bitbucket avec enregistrer des fichiers en cyrillique</a></li>
<li><a href="../fr483458/index.html">Assistant de base de donn√©es GreenPig</a></li>
<li><a href="../fr483460/index.html">SQL HowTo: cr√©ation de cha√Ænes √† l'aide de fonctions de fen√™tre</a></li>
<li><a href="../fr483462/index.html">Tais-toi et prends mon argent</a></li>
<li><a href="../fr483468/index.html">Tests d'int√©gration Flutter - C'est facile</a></li>
<li><a href="../fr483470/index.html">Pose de tuiles efficacement (Pro CSS, SVG, motif et plus)</a></li>
<li><a href="../fr483472/index.html">Supprimer tout: comment effacer les donn√©es et restaurer le SSD NVMe aux param√®tres d'usine</a></li>
<li><a href="../fr483476/index.html">La morale du transport robotis√©: le probl√®me du chariot, risques et cons√©quences</a></li>
<li><a href="../fr483478/index.html">Soleil, vent et eau ver 0.1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>