<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüè´ ‚¨ÜÔ∏è üíñ Autoescalado y gesti√≥n de recursos en Kubernetes (revisi√≥n e informe de video) üíÆ üë®üèø‚Äçüåæ üßïüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El 27 de abril, en la conferencia Strike-2019 , en el marco de la secci√≥n DevOps, se realiz√≥ un informe titulado "Autoescalado y gesti√≥n de recursos e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Autoescalado y gesti√≥n de recursos en Kubernetes (revisi√≥n e informe de video)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/459326/">  El 27 de abril, en la conferencia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Strike-2019</a> , en el marco de la secci√≥n DevOps, se realiz√≥ un informe titulado "Autoescalado y gesti√≥n de recursos en Kubernetes".  Habla sobre c√≥mo usar K8 para garantizar una alta disponibilidad de aplicaciones y garantizar su m√°ximo rendimiento. <br><br><img src="https://habrastorage.org/webt/ol/sv/vf/olsvvfwmfrorzavctm_ipdufuxo.jpeg"><br><br>  Por tradici√≥n, nos complace presentar un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>video con un informe</b></a> (44 minutos, mucho m√°s informativo que el art√≠culo) y el extracto principal en forma de texto.  Vamos! <a name="habracut"></a><br><br>  Analizaremos el tema del informe por palabras y comenzaremos desde el final. <br><br><h2>  Kubernetes </h2><br>  Tengamos contenedores Docker en el host.  Por qu√©  Para garantizar la repetibilidad y el aislamiento, que a su vez permite una implementaci√≥n simple y buena, CI / CD.  Tenemos muchas m√°quinas con contenedores. <br><br>  ¬øQu√© le da en este caso a Kubernetes? <br><br><ol><li>  Dejamos de pensar en estas m√°quinas y comenzamos a trabajar con la "nube", un <b>grupo de contenedores</b> o pods (grupos de contenedores). </li><li>  Adem√°s, ni siquiera pensamos en pods individuales, sino que gestionamos grupos m√°s grandes.  Tales <b>primitivas de alto nivel</b> nos permiten decir que hay una plantilla para lanzar una determinada carga de trabajo, pero el n√∫mero requerido de instancias para su lanzamiento.  Si posteriormente cambiamos la plantilla, todas las instancias tambi√©n cambiar√°n. </li><li>  Usando la <b>API declarativa, en</b> lugar de ejecutar una secuencia de comandos espec√≠ficos, describimos el "dispositivo mundial" (en YAML) que crea Kubernetes.  Y de nuevo: cuando la descripci√≥n cambia, su visualizaci√≥n real tambi√©n cambiar√°. </li></ol><br><h2>  Gesti√≥n de recursos </h2><br><h3>  CPU </h3><br>  Ejecutemos nginx, php-fpm y mysql en el servidor.  Estos servicios tendr√°n incluso m√°s procesos en ejecuci√≥n, cada uno de los cuales requiere recursos inform√°ticos: <br><br><img src="https://habrastorage.org/webt/yu/v6/t8/yuv6t8a5q6txbhi25fe5mwv8f4k.png"><br>  <i>(los n√∫meros en la diapositiva son "loros", la necesidad abstracta de cada proceso de potencia inform√°tica)</i> <br><br>  Para que sea conveniente trabajar con esto, es l√≥gico combinar procesos en grupos (por ejemplo, todos los procesos nginx en un grupo "nginx").  Una manera simple y obvia de hacer esto es colocar cada grupo en un contenedor: <br><br><img src="https://habrastorage.org/webt/yu/en/fr/yuenfrw6vzvexx1xrkvy3dfgw3o.png"><br><br>  Para continuar, debe recordar qu√© es un contenedor (en Linux).  Su aparici√≥n fue posible gracias a tres caracter√≠sticas clave en el n√∫cleo, implementadas durante mucho tiempo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">capacidades</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">espacios de nombres</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cgroups</a> .  Y otras tecnolog√≠as (incluyendo "shells" convenientes como Docker) contribuyeron a un mayor desarrollo: <br><br><img src="https://habrastorage.org/webt/f-/nf/ua/f-nfuaos1_9xdwyblt2em4yp-gs.png"><br><br>  En el contexto del informe, solo estamos interesados ‚Äã‚Äãen <b>cgroups</b> , porque los grupos de control son parte de la funcionalidad de los contenedores (Docker, etc.) que implementa la gesti√≥n de recursos.  Los procesos, unidos en grupos, como quer√≠amos, son los grupos de control. <br><br>  Volvamos a los requisitos de CPU para estos procesos, y ahora para los grupos de procesos: <br><br><img src="https://habrastorage.org/webt/_s/cl/7v/_scl7v6nsak1ieo-dipaz9sgb_a.png"><br>  <i>(Repito que todos los n√∫meros son una expresi√≥n abstracta de los requisitos de recursos)</i> <br><br>  Al mismo tiempo, la CPU en s√≠ tiene un cierto recurso final <i>(en el ejemplo es 1000)</i> , que puede no ser suficiente para todos (la suma de las necesidades de todos los grupos es 150 + 850 + 460 = 1460).  ¬øQu√© pasar√° en este caso? <br><br>  El n√∫cleo comienza a distribuir recursos y lo hace "honestamente", dando la misma cantidad de recursos a cada grupo.  Pero en el primer caso hay m√°s de lo necesario (333&gt; 150), por lo que el exceso (333-150 = 183) permanece en reserva, que tambi√©n se distribuye por igual entre otros dos contenedores: <br><br><img src="https://habrastorage.org/webt/nm/qv/te/nmqvteopou65lsc_ku2b0-qmbco.gif"><br><br>  Como resultado: el primer contenedor ten√≠a suficientes recursos, el segundo, no era suficiente, el tercero, no era suficiente.  Este es el resultado del programador <b>"honesto" en Linux</b> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CFS</a> .  Su trabajo se puede regular asignando <b>peso a</b> cada uno de los contenedores.  Por ejemplo, as√≠: <br><br><img src="https://habrastorage.org/webt/z1/c_/_b/z1c__bumb8hy1k5zdrkcwo_aak0.gif"><br><br>  Veamos el caso de la falta de recursos en el segundo contenedor (php-fpm).  Todos los recursos del contenedor se distribuyen entre los procesos por igual.  Como resultado, el proceso maestro funciona bien y todos los trabajadores disminuyen la velocidad, recibiendo menos de la mitad de lo que se necesita: <br><br><img src="https://habrastorage.org/webt/9z/wi/d2/9zwid2g1znmdoslkl9bf9cogrnq.gif"><br><br>  As√≠ es como funciona el planificador CFS.  Los pesos que asignamos a los contenedores se denominar√°n <b>solicitudes</b> en el futuro.  Por qu√©, ver a continuaci√≥n. <br><br>  Echemos un vistazo a toda la situaci√≥n desde el otro lado.  Como saben, todos los caminos conducen a Roma, y ‚Äã‚Äãen el caso de una computadora a la CPU.  Una CPU, muchas tareas: necesita un sem√°foro.  La forma m√°s f√°cil de administrar los recursos es "sem√°foro": le dan a un proceso un tiempo de acceso fijo a la CPU, luego al siguiente, etc. <br><br><img src="https://habrastorage.org/webt/vf/af/qe/vfafqespiii6mnts87i4amfdyku.gif"><br><br>  Este enfoque se llama <i>limitaci√≥n dura</i> .  Recu√©rdalo solo como <b>l√≠mites</b> .  Sin embargo, si distribuye l√≠mites a todos los contenedores, surge un problema: mysql viajaba por el camino y en alg√∫n momento su necesidad de una CPU termin√≥, pero todos los dem√°s procesos se vieron obligados a esperar mientras la CPU estaba <b>inactiva</b> . <br><br><img src="https://habrastorage.org/webt/7o/1m/ps/7o1mps5khnoqkfywizrusxtug1q.png"><br><br>  Volvamos al kernel de Linux y su interacci√≥n con la CPU: la imagen general es la siguiente: <br><br><img src="https://habrastorage.org/webt/m7/xz/x8/m7xzx8brcdgbihu8qchb63rwjwc.png"><br><br>  Cgroup tiene dos configuraciones: de hecho, estos son dos simples "giros" que le permiten determinar: <br><br><ol><li>  el peso del contenedor (solicitud) es <b>acciones</b> ; </li><li>  un porcentaje del tiempo total de CPU para trabajar en tareas de contenedor (l√≠mites) es la <b>cuota</b> . </li></ol><br><h3>  ¬øC√≥mo medir la CPU? </h3><br>  Hay diferentes formas: <br><br><ol><li>  ¬øQu√© son los <i>loros?</i> Nadie lo sabe, cada vez que necesita estar de acuerdo. </li><li>  <i>El inter√©s es</i> m√°s claro, pero relativo: el 50% de un servidor con 4 n√∫cleos y 20 n√∫cleos son cosas completamente diferentes. </li><li>  Puede usar los <i>pesos</i> ya mencionados que Linux conoce, pero tambi√©n son relativos. </li><li>  La opci√≥n m√°s adecuada es medir los recursos inform√°ticos en <i>segundos</i> .  Es decir  en segundos de tiempo de procesador en relaci√≥n con segundos de tiempo real: dieron 1 segundo de tiempo de procesador en 1 segundo real; este es un n√∫cleo de CPU completo. </li></ol><br>  Para hacerlo a√∫n m√°s f√°cil de decir, comenzaron a medir directamente en los <i>n√∫cleos</i> , lo que significa el tiempo de CPU en relaci√≥n con el real.  Dado que Linux comprende los pesos en lugar de los n√∫cleos / tiempo del procesador, se necesitaba un mecanismo de traducci√≥n de uno a otro. <br><br>  Considere un ejemplo simple con un servidor con 3 n√∫cleos de CPU, donde tres pods seleccionar√°n pesos (500, 1000 y 1500) que se convertir√°n f√°cilmente a las partes correspondientes de los n√∫cleos asignados a ellos (0.5, 1 y 1.5). <br><br><img src="https://habrastorage.org/webt/mz/vl/1x/mzvl1xmzbtvlwgsqtf1-_n_rhns.png"><br><br>  Si toma un segundo servidor, donde habr√° el doble de n√∫cleos (6), y coloca los mismos pods all√≠, la distribuci√≥n de n√∫cleos se puede calcular f√°cilmente simplemente multiplicando por 2 (1, 2 y 3, respectivamente).  Pero ocurre un punto importante cuando aparece el cuarto m√≥dulo en este servidor, cuyo peso puede ser 3000 por conveniencia. Elimina algunos de los recursos de la CPU (la mitad de los n√∫cleos), y el resto de los m√≥dulos los cuentan (reducir a la mitad): <br><br><img src="https://habrastorage.org/webt/p3/1t/nd/p31tndrejrchgwgbnpk53q5qnig.gif"><br><br><h3>  Kubernetes y recursos de CPU </h3><br>  En Kubernetes, los recursos de la CPU generalmente se miden en <b>mili-n√∫cleos</b> , es decir.  Se toman 0.001 granos como el peso base.  <i>(Lo mismo en la terminolog√≠a de Linux / cgroups se llama recurso compartido de CPU, aunque, para ser m√°s precisos, 1000 CPU = 1024</i> recursos <i>compartidos de CPU).</i> K8s se asegura de no colocar m√°s pods en el servidor que recursos de CPU para la suma de pesos Todas las vainas. <br><br>  ¬øC√≥mo va esto?  Cuando se agrega un servidor a un cl√∫ster de Kubernetes, informa cu√°ntos n√∫cleos de CPU tiene disponibles.  Y al crear un nuevo pod, el programador de Kubernetes sabe cu√°ntos n√∫cleos necesita este pod.  Por lo tanto, el pod se definir√° en el servidor, donde hay suficientes n√∫cleos. <br><br>  ¬øQu√© suceder√° si <b>no se</b> especifica la solicitud (es decir, el pod no determina la cantidad de n√∫cleos que necesita)?  Veamos c√≥mo Kubernetes generalmente cuenta los recursos. <br><br>  El pod puede especificar tanto las solicitudes (planificador CFS) como los l√≠mites (¬ørecuerda el sem√°foro?): <br><br><ul><li>  Si son iguales, la clase de QoS garantizada se asigna al pod.  Tal cantidad de granos siempre disponibles para √©l est√° garantizada. </li><li>  Si la solicitud es inferior al l√≠mite, la clase de QoS es <b>burstable</b> .  Es decir  esperamos que el pod, por ejemplo, siempre use 1 n√∫cleo, pero este valor no es una limitaci√≥n: a <i>veces el</i> pod puede usar m√°s (cuando hay recursos libres en el servidor para esto). </li><li>  Tambi√©n existe la clase de QoS de <b>mejor esfuerzo</b> : los pods para los que no se especifica la solicitud pertenecen a ella.  Los recursos se les dan al final. </li></ul><br><h3>  El recuerdo </h3><br>  La situaci√≥n es similar con la memoria, pero un poco diferente: despu√©s de todo, la naturaleza de estos recursos es diferente.  En general, la analog√≠a es la siguiente: <br><br><img src="https://habrastorage.org/webt/hw/zu/ja/hwzuja_vf0ojiz8uai-hhtn23ys.png"><br><br>  Veamos c√≥mo se implementan las solicitudes en la memoria.  Deje que los pods vivan en el servidor, cambiando la memoria consumida, hasta que uno de ellos se vuelva tan grande que la memoria se agote.  En este caso, el asesino OOM aparece y mata el proceso m√°s grande: <br><br><img src="https://habrastorage.org/webt/mg/i0/at/mgi0atkc5o5m0xo-crxt3augbnm.gif"><br><br>  Esto no siempre nos conviene, por lo tanto, es posible regular qu√© procesos son importantes para nosotros y no deben ser eliminados.  Para hacer esto, use el par√°metro <b>oom_score_adj</b> . <br><br>  Volvamos a las clases de QoS de la CPU y dibujemos una analog√≠a con los valores oom_score_adj, que determinan las prioridades de consumo de memoria para los pods: <br><br><ul><li>  El valor m√°s bajo de oom_score_adj de un pod es -998, lo que significa que dicho pod deber√≠a ser eliminado en √∫ltimo lugar, esto est√° <b>garantizado</b> . </li><li>  El m√°s alto, 1000, es el <b>mejor esfuerzo</b> , esas c√°psulas se matan antes que nadie. </li><li>  Para calcular el resto de los valores ( <b>burstable</b> ), hay una f√≥rmula cuya esencia se reduce al hecho de que cuanto m√°s pod ha solicitado recursos, menos posibilidades hay de que se elimine. </li></ul><br><img src="https://habrastorage.org/webt/sc/yo/rm/scyorm9zwn_lltxminknyv-cbhy.png"><br><br>  El segundo "giro" - <b>limit_in_bytes</b> - para los l√≠mites.  Todo es m√°s simple: simplemente asignamos la cantidad m√°xima de memoria a emitir, y aqu√≠ (a diferencia de la CPU) no hay duda de en qu√© se mide (memoria). <br><br><h3>  Total </h3><br>  Se establecen solicitudes y <code>limits</code> para cada pod en Kubernetes, ambos par√°metros para la CPU y para la memoria: <br><br><ol><li>  seg√∫n las solicitudes, el planificador de Kubernetes funciona, que distribuye pods entre servidores; </li><li>  basado en todos los par√°metros, se determina la clase QodS del pod; </li><li>  Los pesos relativos se calculan en funci√≥n de las solicitudes de CPU; </li><li>  Seg√∫n las solicitudes de la CPU, se configura un planificador CFS; </li><li>  Seg√∫n las solicitudes de memoria, se configura el asesino OOM; </li><li>  Seg√∫n los l√≠mites de la CPU, se configura un "sem√°foro"; </li><li>  seg√∫n los l√≠mites de memoria, se establece un l√≠mite en cgroup. </li></ol><br><img src="https://habrastorage.org/webt/dr/1i/0_/dr1i0_troiqlcg4q_ki2fytefs0.png"><br><br>  En general, esta imagen responde a todas las preguntas sobre c√≥mo se lleva a cabo la parte principal de la gesti√≥n de recursos en Kubernetes. <br><br><h2>  Autoescalado </h2><br><h3>  K8S Cluster-Autoscaler </h3><br>  Imagine que todo el cl√∫ster ya est√° ocupado y se debe crear un nuevo pod.  Si bien el pod no puede aparecer, se cuelga en el estado <i>Pendiente</i> .  Para que parezca, podemos conectar un nuevo servidor al cl√∫ster o ... poner cluster-autoescaler, que lo har√° por nosotros: pedir una m√°quina virtual al proveedor de la nube (por solicitud de la API) y conectarla al cl√∫ster, despu√©s de lo cual se agregar√° el pod . <br><br><img src="https://habrastorage.org/webt/zu/va/dq/zuvadqhlpycxkqaw5q35bmr08_e.gif"><br><br>  Esta es la escala autom√°tica del cl√∫ster de Kubernetes, que funciona muy bien (en nuestra experiencia).  Sin embargo, como en otros lugares, hay algunos matices aqu√≠ ... <br><br>  Mientras est√°bamos aumentando el tama√±o del cl√∫ster, todo estaba bien, pero ¬øqu√© sucede cuando el cl√∫ster <b>comenz√≥ a liberarse</b> ?  El problema es que la migraci√≥n de pods (a hosts libres) es t√©cnicamente dif√≠cil y costosa en t√©rminos de recursos.  Kubernetes tiene un enfoque completamente diferente. <br><br>  Considere un cl√∫ster de 3 servidores en el que hay implementaci√≥n.  Tiene 6 pods: ahora son 2 para cada servidor.  Por alguna raz√≥n, quer√≠amos apagar uno de los servidores.  Para hacer esto, use el <code>kubectl drain</code> , que: <br><br><ul><li>  proh√≠be enviar nuevos pods a este servidor; </li><li>  eliminar pods existentes en el servidor. </li></ul><br>  Como Kubernetes supervisa el mantenimiento del n√∫mero de pods (6), simplemente los <b>recrear√°</b> en otros nodos, pero no en el desconectado, ya que ya est√° marcado como inaccesible para alojar nuevos pods.  Esta es la mec√°nica fundamental para Kubernetes. <br><br><img src="https://habrastorage.org/webt/l8/dw/jf/l8dwjfv1yyszva-knkk6p0hls_s.gif"><br><br>  Sin embargo, hay un matiz aqu√≠.  En una situaci√≥n similar para StatefulSet (en lugar de Implementaci√≥n) las acciones ser√°n diferentes.  Ahora ya tenemos una aplicaci√≥n con estado, por ejemplo, tres pods con MongoDB, uno de los cuales ten√≠a alg√∫n tipo de problema (los datos se volvieron incorrectos o alg√∫n otro error que impidi√≥ que el pod se iniciara correctamente).  Y nuevamente decidimos desconectar un servidor.  Que va a pasar <br><br><img src="https://habrastorage.org/webt/a1/dx/ck/a1dxckkad2wpckrygftdzcjbuyq.gif"><br><br>  MongoDB <i>podr√≠a</i> morir porque necesita un qu√≥rum: para un grupo de tres instalaciones, al menos dos deben funcionar.  Sin embargo, esto <i>no sucede</i> , gracias al <b>PodDisruptionBudget</b> .  Este par√°metro determina el n√∫mero m√≠nimo requerido de pods de trabajo.  Sabiendo que uno de los pods con MongoDB ya no funciona, y viendo que minAvailable est√° configurado para MongoDB en <code>minAvailable: 2</code> , Kubernetes no le permitir√° eliminar el pod. <br><br>  En pocas palabras: para mover correctamente (y volver a crear) pods cuando se libera el cl√∫ster, debe configurar PodDisruptionBudget. <br><br><h3>  Escala horizontal </h3><br>  Considera una situaci√≥n diferente.  Hay una aplicaci√≥n que se ejecuta como Implementaci√≥n en Kubernetes.  El tr√°fico de usuarios llega a sus pods (por ejemplo, hay tres de ellos), y medimos un cierto indicador en ellos (por ejemplo, carga de CPU).  Cuando aumenta la carga, lo arreglamos en el horario y aumentamos el n√∫mero de pods para distribuir solicitudes. <br><br>  Hoy en Kubernetes no necesita hacer esto manualmente: puede aumentar / disminuir autom√°ticamente el n√∫mero de pods seg√∫n los valores de los indicadores de carga medidos. <br><br><img src="https://habrastorage.org/webt/kj/fm/_t/kjfm_tu0u83c4mthfjayisabme0.gif"><br><br>  Las preguntas principales aqu√≠ son <b>qu√© medir exactamente</b> y <b>c√≥mo interpretar los</b> valores obtenidos (para tomar la decisi√≥n de cambiar el n√∫mero de pods).  Puedes medir mucho: <br><br><img src="https://habrastorage.org/webt/h-/tw/a8/h-twa8kqe49av8gwxwxeoadyalc.png"><br><br>  C√≥mo hacerlo t√©cnicamente: recopilar m√©tricas, etc.  - Habl√© en detalle en el informe sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Monitoreo y Kubernetes</a> .  ¬°Y el principal consejo para elegir los par√°metros √≥ptimos es <b>experimentar</b> ! <br><br>  Existe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un m√©todo USE</a> <i>(Saturaci√≥n de utilizaci√≥n y errores</i> ), cuyo significado es el siguiente.  ¬øSobre qu√© base tiene sentido escalar, por ejemplo, php-fpm?  Basado en el hecho de que los trabajadores terminan, es la <i>utilizaci√≥n</i> .  Y si los trabajadores terminaron y no se aceptan nuevas conexiones, esto es <i>saturaci√≥n</i> .  Ambos par√°metros deben medirse y, seg√∫n los valores, se debe llevar a cabo el escalado. <br><br><h2>  En lugar de una conclusi√≥n </h2><br>  El informe tiene una continuaci√≥n: sobre el escalamiento vertical y sobre c√≥mo elegir los recursos correctos.  Hablar√© de esto en futuros videos en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestro YouTube</a> : ¬°suscr√≠base para no perderse! <br><br><h2>  Videos y diapositivas </h2><br>  Video de la actuaci√≥n (44 minutos): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/10ZR-fbyuSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Presentaci√≥n del informe: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Otros informes de Kubernetes en nuestro blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Expandiendo y complementando a Kubernetes</a> " <i>(Andrey Polov; 8 de abril de 2019 en Saint HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bases de datos y Kubernetes</a> " <i>(Dmitry Stolyarov; 8 de noviembre de 2018 en HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Monitoreo y Kubernetes</a> " <i>(Dmitry Stolyarov; 28 de mayo de 2018 en RootConf)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mejores pr√°cticas de CI / CD con Kubernetes y GitLab</a> " <i>(Dmitry Stolyarov; 7 de noviembre de 2017 en HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Nuestra experiencia con Kubernetes en peque√±os proyectos</a> " <i>(Dmitry Stolyarov; 6 de junio de 2017 en RootConf)</i> . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/459326/">https://habr.com/ru/post/459326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../459314/index.html">Visualice y trate con Hash Match Join</a></li>
<li><a href="../459316/index.html">Hydra 2019: transmisi√≥n gratuita de la primera sala y un poco sobre lo que ser√° en la conferencia</a></li>
<li><a href="../459318/index.html">TypeScript y sprints cortos. C√≥mo hicimos la herramienta de variaci√≥n de entrevista frontal</a></li>
<li><a href="../459320/index.html">Operador Kubernetes en Python sin frameworks y SDK</a></li>
<li><a href="../459322/index.html">Editorial Peter. Venta de verano</a></li>
<li><a href="../459328/index.html">La mejor relaci√≥n calidad-precio - Mpow A5 (059)</a></li>
<li><a href="../459330/index.html">Bitrix para programador y gerente: amor y odio</a></li>
<li><a href="../459334/index.html">YouTrack 2019.2: un banner de todo el sistema, mejoras en la p√°gina de lista de tareas, nuevas opciones de b√∫squeda y m√°s</a></li>
<li><a href="../459336/index.html">Vive y aprende. Parte 1. Orientaci√≥n escolar y profesional</a></li>
<li><a href="../459338/index.html">Usar el verificador como un medio para modelar r√°pidamente proyectos RTL. Introducci√≥n a UVM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>