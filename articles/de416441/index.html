<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧤 🏾 🏴󠁧󠁢󠁷󠁬󠁳󠁿 Tonhöhenverfolgung oder Bestimmung der Tonhöhenfrequenz in der Sprache am Beispiel von Praat, YAAPT und YIN 👨🏿‍🤝‍👨🏼 👨🏾 👩🏿‍💻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Bereich der Emotionserkennung ist die Stimme nach dem Gesicht die zweitwichtigste Quelle emotionaler Daten. Die Stimme kann durch mehrere Parameter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tonhöhenverfolgung oder Bestimmung der Tonhöhenfrequenz in der Sprache am Beispiel von Praat, YAAPT und YIN</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neurodatalab/blog/416441/"><img src="https://habrastorage.org/getpro/habr/post_images/37d/3f1/975/37d3f19758eb7d646ccff079d37772f8.png" alt="Bild"><br><br>  Im Bereich der Emotionserkennung ist die Stimme nach dem Gesicht die zweitwichtigste Quelle emotionaler Daten.  Die Stimme kann durch mehrere Parameter charakterisiert werden.  Die Tonhöhe ist eine der Hauptmerkmale dieser Art. Auf dem Gebiet der Akustiktechnologie ist es jedoch korrekter, diesen Parameter als Grundfrequenz zu bezeichnen. <br><br>  Die Frequenz des Grundtons hängt direkt mit dem zusammen, was wir Intonation nennen.  Und Intonation ist zum Beispiel mit den emotional ausdrucksstarken Eigenschaften der Stimme verbunden. <br><br>  Dennoch ist die Bestimmung der Frequenz des Grundtons keine völlig triviale Aufgabe mit interessanten Nuancen.  In diesem Artikel werden wir die Merkmale von Algorithmen für ihre Bestimmung diskutieren und vorhandene Lösungen mit Beispielen für bestimmte Audioaufnahmen vergleichen. <br><a name="habracut"></a><br>  <b>Einführung</b> <br><br>  Erinnern wir uns zunächst daran, was im Wesentlichen die Frequenz des Grundtons ist und für welche Aufgaben er möglicherweise benötigt wird.  <i>Die Grundfrequenz</i> , die auch als CHOT, Grundfrequenz oder F0 bezeichnet wird, ist die Frequenz der Stimmbänder, wenn sie stimmhafte Töne aussprechen.  Wenn Sie nicht-tonige Töne aussprechen (stimmlos), z. B. flüsternd sprechen oder zischende und pfeifende Töne aussprechen, zögern die Bänder nicht, was bedeutet, dass diese Eigenschaft für sie nicht relevant ist. <br><br>  * Bitte beachten Sie, dass die Unterteilung in Ton- und Nicht-Ton-Klänge nicht der Unterteilung in Vokale und Konsonanten entspricht. <br><br>  Die Variabilität der Frequenz des Grundtons ist ziemlich groß und kann nicht nur zwischen Menschen stark variieren (für Männerstimmen mit niedrigerem Durchschnitt beträgt die Frequenz 70-200 Hz und für Frauenstimmen 400 Hz), sondern auch für eine Person, insbesondere bei emotionaler Sprache . <br><br>  Die Bestimmung der Frequenz des Grundtons wird verwendet, um eine breite Palette von Problemen zu lösen: <br><br><ul><li>  Erkennen von Emotionen, wie wir oben sagten; </li><li>  Geschlechtsbestimmung; </li><li>  Bei der Lösung des Problems, Audio mit mehreren Stimmen zu segmentieren oder Sprache in Phrasen zu unterteilen; </li><li>  In der Medizin zur Bestimmung der pathologischen Eigenschaften der Stimme (z. B. anhand der akustischen Parameter Jitter und Shimmer).  Zum Beispiel die Identifizierung von Anzeichen der Parkinson-Krankheit [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> ].  Jitter und Shimmer können auch verwendet werden, um Emotionen zu erkennen [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> ]. </li></ul><br>  Es gibt jedoch eine Reihe von Schwierigkeiten bei der Bestimmung von F0.  Beispielsweise ist es häufig möglich, F0 mit Harmonischen zu verwechseln, was zu sogenannten Tonhöhenverdopplungs- / Tonhöhenhalbierungseffekten führen kann [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> ].  Und bei Audioaufnahmen mit schlechter Qualität ist F0 ziemlich schwer zu berechnen, da die gewünschte Spitze bei niedrigen Frequenzen fast verschwindet. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erinnerst</a> du dich übrigens an die Geschichte von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laurel und Yanny</a> ?  Die Unterschiede in den Wörtern, die Menschen beim Anhören derselben Audioaufnahme hören, sind genau auf den Unterschied in der Wahrnehmung F0 zurückzuführen, der von vielen Faktoren beeinflusst wird: dem Alter des Hörers, dem Ermüdungsgrad und dem Wiedergabegerät.  Wenn Sie also Aufnahmen in Lautsprechern mit qualitativ hochwertiger Wiedergabe niedriger Frequenzen hören, hören Sie Laurel und in Audiosystemen, in denen niedrige Frequenzen schlecht wiedergegeben werden, Yanny.  Der Übergangseffekt ist beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> auf einem Gerät zu sehen.  In diesem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> fungiert das neuronale Netzwerk als Zuhörer.  In einem anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel können</a> Sie lesen, wie das Yanny / Laurel-Phänomen in Bezug auf die Sprachbildung erklärt wird. <br><br>  Da eine detaillierte Analyse aller Methoden zur Bestimmung von F0 zu umfangreich wäre, ist der Artikel übersichtlicher Natur und kann zur Navigation im Thema beitragen. <br><br>  <b>Methoden zur Bestimmung von F0</b> <br><br>  Methoden zur Bestimmung von F0 können in drei Kategorien unterteilt werden: basierend auf der Zeitdynamik des Signals oder der Zeitdomäne;  basierend auf der Frequenzstruktur oder dem Frequenzbereich sowie kombinierten Methoden.  Wir empfehlen Ihnen, sich mit dem Übersichtsartikel zum Thema vertraut zu machen, in dem die angegebenen Methoden zum Extrahieren von F0 detailliert analysiert werden. <br><br>  Beachten Sie, dass jeder der diskutierten Algorithmen aus drei Hauptschritten besteht: <br><br>  Vorverarbeitung (Filtern des Signals, Aufteilen in Frames) <br>  Suche nach möglichen Werten von F0 (Kandidaten) <br>  Tracking ist die Wahl der wahrscheinlichsten Flugbahn F0 (da wir für jeden Moment mehrere konkurrierende Kandidaten haben, müssen wir die wahrscheinlichste Spur unter ihnen finden). <br><br>  <b>Zeitbereich</b> <br><br>  Wir skizzieren einige allgemeine Punkte.  Vor der Anwendung der Zeitbereichsmethoden wird das Signal vorgefiltert, wobei nur niedrige Frequenzen übrig bleiben.  Schwellenwerte werden festgelegt - die minimalen und maximalen Frequenzen, beispielsweise von 75 bis 500 Hz.  Die Bestimmung von F0 erfolgt nur für Bereiche mit harmonischer Sprache, da dies für Pausen oder Rauschgeräusche nicht nur bedeutungslos ist, sondern auch Fehler in benachbarten Rahmen verursachen kann, wenn Interpolation und / oder Glättung angewendet werden.  Die Rahmenlänge wird so ausgewählt, dass sie mindestens drei Punkte enthält. <br><br>  Die Hauptmethode, auf deren Grundlage später eine ganze Familie von Algorithmen erschien, ist die Autokorrelation.  Der Ansatz ist recht einfach - es ist notwendig, die Autokorrelationsfunktion zu berechnen und ihr erstes Maximum zu nehmen.  Es wird die am stärksten ausgeprägte Frequenzkomponente im Signal angezeigt.  Was könnte die Schwierigkeit bei der Verwendung der Autokorrelation sein und warum ist es bei weitem nicht immer so, dass das erste Maximum der gewünschten Frequenz entspricht?  Selbst unter nahezu idealen Bedingungen bei Aufnahmen hoher Qualität kann das Verfahren aufgrund der komplexen Struktur des Signals falsch sein.  Unter realitätsnahen Bedingungen, bei denen unter anderem bei verrauschten Aufnahmen oder Aufnahmen von anfänglich geringer Qualität das Verschwinden des gewünschten Peaks auftreten kann, steigt die Anzahl der Fehler stark an. <br><br>  Trotz der Fehler ist die Autokorrelationsmethode aufgrund ihrer einfachen Einfachheit und Logik recht praktisch und attraktiv, weshalb sie in vielen Algorithmen, einschließlich YIN, als Grundlage dient.  Sogar der Name des Algorithmus verweist auf das Gleichgewicht zwischen der Bequemlichkeit und Ungenauigkeit der Autokorrelationsmethode: "Der Name YIN von" Yin "und" Yang "der orientalischen Philosophie spielt auf das Zusammenspiel von Autokorrelation und Aufhebung an."  [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">4</a> ] <br><br>  Die Macher von YIN versuchten, die Schwächen des Autokorrelationsansatzes zu beheben.  Die erste Änderung ist die Verwendung der Funktion Cumulative Mean Normalized Difference, die die Empfindlichkeit gegenüber Amplitudenmodulationen verringern und die Peaks stärker machen soll: <br><br>  \ begin {Gleichung} <br>  d'_t (\ tau) = <br>  \ begin {Fälle} <br>  1, &amp; \ tau = 0 \\ <br>  d_t (\ tau) \ bigg / \ bigg [\ frac {1} {\ tau} \ sum \ limit_ {j = 1} ^ {\ tau} d_t (j) \ bigg], &amp; \ text {else} <br>  \ end {Fälle} <br>  \ end {Gleichung} <br>  YIN versucht auch, Fehler zu vermeiden, die auftreten, wenn die Länge der Fensterfunktion nicht vollständig durch die Schwingungsdauer geteilt wird.  Hierzu wird eine parabolische Minimalinterpolation verwendet.  Im letzten Schritt der Audiosignalverarbeitung wird die Funktion "Beste lokale Schätzung" ausgeführt, um scharfe Sprünge in den Werten zu verhindern (ob gut oder schlecht - dies ist ein strittiger Punkt). <br><br>  <b>Frequenzbereich</b> <br><br>  Wenn wir über den Frequenzbereich sprechen, tritt die harmonische Struktur des Signals in den Vordergrund, dh das Vorhandensein von Spektralspitzen bei Frequenzen, die ein Vielfaches von F0 sind.  Sie können dieses periodische Muster mithilfe der Cepstral-Analyse zu einem klaren Peak „kollabieren“.  Cepstrum - Fourier-Transformation des Logarithmus des Leistungsspektrums;  Der Cepstral-Peak entspricht der periodischsten Komponente des Spektrums (man kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> darüber lesen). <br><br>  <b>Hybridmethoden zur Bestimmung von F0</b> <br><br>  Der nächste Algorithmus, der näher untersucht werden sollte, trägt den sprechenden Namen YAAPT - ein weiterer Algorithmus für die Tonhöhenverfolgung - und ist in der Tat hybride, da er sowohl Frequenz- als auch Zeitinformationen verwendet.  Eine vollständige Beschreibung finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> . Hier beschreiben wir nur die Hauptphasen. <br><br><img src="https://habrastorage.org/webt/r2/mu/uj/r2muujzlcxgdgp5a0bqem3t_iuu.png"><br>  <i>Abbildung 1. YAAPTalgo-Algorithmusdiagramm ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> )</i> . <br><br>  YAAPT besteht aus mehreren Hauptschritten, von denen der erste die Vorverarbeitung ist.  In diesem Stadium werden die Werte des ursprünglichen Signals quadriert und eine zweite Version des Signals erhalten.  Dieser Schritt verfolgt das gleiche Ziel wie die kumulative mittlere normalisierte Differenzfunktion in YIN - Verstärkung und Wiederherstellung von "gestauten" Autokorrelationsspitzen.  Beide Versionen des Signals werden gefiltert - normalerweise reichen sie von 50 bis 1500 Hz, manchmal von 50 bis 900 Hz. <br><br>  Dann wird die Basistrajektorie F0 aus dem Spektrum des umgewandelten Signals berechnet.  Kandidaten für F0 werden unter Verwendung der Funktion Spectral Harmonics Correlation (SHC) bestimmt. <br><br>  \ begin {Gleichung} <br>  SHC (t, f) = \ Summe \ Grenzen_ {f '= - WL / 2} ^ {WL / 2} \ Produkt \ Grenzen_ {r = 1} ^ {NH + 1} S (t, rf + f') <br>  \ end {Gleichung} <br>  Dabei ist S (t, f) das Betragsspektrum für den Rahmen t und die Frequenz f, WL die Fensterlänge in Hz, NH die Anzahl der Harmonischen (die Autoren empfehlen die Verwendung der ersten drei Harmonischen).  Die spektrale Leistung wird auch verwendet, um stimmhafte-stimmlose Frames zu bestimmen, wonach die optimalste Trajektorie gesucht wird, und die Möglichkeit der Tonhöhenverdopplung / Tonhöhenhalbierung wird berücksichtigt [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , Abschnitt II, C]. <br><br>  Ferner werden Kandidaten für F0 sowohl für das Anfangssignal als auch für das konvertierte Signal bestimmt, und anstelle der Autokorrelationsfunktion wird hier die normalisierte Kreuzkorrelation (NCCF) verwendet. <br><br>  \ begin {Gleichung} <br>  NCCF (m) = \ frac {\ sum \ begrenzt_ {n = 0} ^ {Nm-1} x (n) * x (n + m)} {\ sqrt {\ sum \ Grenzen_ {n = 0} ^ { Nm-1} x ^ 2 (n) * \ sum \ limit_ {n = 0} ^ {Nm-1} x ^ 2 (n + m)}} \ text {,} \ hspace {0,3 cm} 0 &lt;m &lt;M_ {0} <br>  \ end {Gleichung} <br>  Der nächste Schritt besteht darin, alle möglichen Kandidaten zu bewerten und ihre Signifikanz oder ihr Gewicht (Verdienst) zu berechnen.  Das Gewicht der aus dem Audiosignal erhaltenen Kandidaten hängt nicht nur von der Amplitude des NCCF-Peaks ab, sondern auch von ihrer Nähe zur aus dem Spektrum bestimmten Trajektorie F0.  Das heißt, der Frequenzbereich wird hinsichtlich der Genauigkeit als grob, aber als stabil angesehen [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , Abschnitt II, D]. <br><br>  Dann wird für alle Paare der verbleibenden Kandidaten die Übergangskostenmatrix berechnet - der Übergangspreis, zu dem sie letztendlich die optimale Flugbahn finden [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , Abschnitt II, E]. <br><br>  <b>Beispiele</b> <br><br>  Jetzt wenden wir alle oben genannten Algorithmen auf bestimmte Audioaufnahmen an.  Als Ausgangspunkt werden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Praat verwenden</a> , ein Werkzeug, das für viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachwissenschaftler von</a> grundlegender Bedeutung ist.  In Python werden wir uns dann die Implementierung von YIN und YAAPT ansehen und die erhaltenen Ergebnisse vergleichen. <br><br>  Als Audiomaterial können Sie jedes verfügbare Audio verwenden.  Wir haben mehrere Auszüge aus unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RAMAS-</a> Datenbank <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">entnommen</a> - ein multimodaler <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datensatz</a> , der unter Beteiligung von VGIK-Akteuren erstellt wurde.  Sie können auch Material aus anderen offenen Datenbanken wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LibriSpeech</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RAVDESS verwenden</a> . <br><br>  Als anschauliches Beispiel haben wir Auszüge aus mehreren Aufnahmen mit neutralen und emotional gefärbten Männer- und Frauenstimmen genommen und aus Gründen der Klarheit zu einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufnahme zusammengefasst</a> .  Schauen wir uns unser Signal, sein Spektrogramm, seine Intensität (orange Farbe) und F0 (blaue Farbe) an.  In Praat kann dies mit Strg + O (Öffnen - Aus Datei lesen) und dann mit der Schaltfläche Anzeigen und Bearbeiten erfolgen. <br><br><img src="https://habrastorage.org/webt/ir/ay/kh/iraykhpzwfetpahhiymdic6pdwi.png"><br>  <i>Abbildung 2. Spektrogramm, Intensität (orange Farbe), F0 (blaue Farbe) in Praat.</i> <br><br>  Das Audio zeigt ziemlich deutlich, dass in der emotionalen Sprache die Tonhöhe sowohl bei Männern als auch bei Frauen zunimmt.  Gleichzeitig kann F0 für emotionale männliche Sprache gut mit F0 einer weiblichen Stimme verglichen werden. <br><br>  <b>Tracking</b> <br><br>  Wählen Sie im Praat-Menü die Registerkarte Periodizität analysieren - bis Tonhöhe (ac), dh die Definition von F0 mithilfe der Autokorrelation.  Es erscheint ein Fenster zum Einstellen von Parametern, in dem 3 Parameter zum Bestimmen von Kandidaten für F0 und 6 weitere Parameter für den Pfadfinder-Algorithmus festgelegt werden können, der den wahrscheinlichsten Pfad F0 unter allen Kandidaten erstellt. <br><br><div class="spoiler">  <b class="spoiler_title">Viele Parameter (in Praat befindet sich ihre Beschreibung auch auf der Schaltfläche Hilfe)</b> <div class="spoiler_text"><ul><li>  Stille-Schwelle - Die Schwelle der relativen Amplitude des Signals zur Bestimmung der Stille. Der Standardwert beträgt 0,03. </li><li>  Sprachschwelle - das Gewicht des stimmlosen Kandidaten, der Maximalwert ist 1. Je höher dieser Parameter, desto mehr Frames werden als stimmlos definiert, dh ohne Tongeräusche.  In diesen Rahmen wird F0 nicht bestimmt.  Der Wert dieses Parameters ist der Schwellenwert für Spitzen der Autokorrelationsfunktion.  Der Standardwert ist 0,45. </li><li>  Oktavkosten - bestimmt, wie viel mehr Gewicht die Hochfrequenzkandidaten im Vergleich zu den Niederfrequenzkandidaten haben.  Je höher der Wert, desto mehr wird der Hochfrequenzkandidat bevorzugt.  Der Standardwert ist 0,01 pro Oktave. </li><li>  Oktavsprungkosten - Mit einer Erhöhung dieses Koeffizienten nimmt die Anzahl der scharfen sprungartigen Übergänge zwischen aufeinanderfolgenden Werten von F0 ab.  Der Standardwert ist 0,35. </li><li>  Voiced / Unvoiced-Kosten - Durch Erhöhen dieses Koeffizienten wird die Anzahl der Voiced / Unvoiced-Übergänge verringert.  Der Standardwert ist 0,14. </li><li>  Tonhöhenobergrenze (Hz) - Kandidaten über dieser Frequenz werden nicht berücksichtigt.  Der Standardwert ist 600 Hz. </li></ul><br></div></div><br>  Eine detaillierte Beschreibung des Algorithmus findet sich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem Artikel von</a> 1993. <br><br>  Wie das Ergebnis des Trackers (Pfadfinder) aussieht, können Sie sehen, indem Sie auf OK klicken und dann die resultierende Pitch-Datei anzeigen (Anzeigen &amp; Bearbeiten).  Es ist ersichtlich, dass es zusätzlich zu der ausgewählten Flugbahn noch ziemlich signifikante Kandidaten mit einer niedrigeren Frequenz gab. <br><br><img src="https://habrastorage.org/webt/wq/rq/vf/wqrqvf_cbnbn8orcajij6sfrasu.png"><br>  <i>Abbildung 3. PitchPath für die ersten 1,3 Sekunden der Audioaufnahme.</i> <br><br>  <b>Aber was ist mit Python?</b> <br><br>  Nehmen wir zwei Bibliotheken, die Pitch Tracking anbieten - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aubio</a> , in dem der Standardalgorithmus YIN ist, und die Bibliothek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AMFM_decompsition</a> , in der der YAAPT-Algorithmus implementiert ist.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fügen Sie</a> in der separaten Datei (Datei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PraatPitch.txt</a> ) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> F0-Werte von Praat ein (dies kann manuell erfolgen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wählen Sie die Audiodatei</a> aus, klicken Sie auf Anzeigen und Bearbeiten, wählen Sie die gesamte Datei aus und wählen Sie im oberen Menü die Liste Pitch-Pitch aus). <br><br>  Vergleichen Sie nun die Ergebnisse für alle drei Algorithmen (YIN, YAAPT, Praat). <br><br><div class="spoiler">  <b class="spoiler_title">Viel Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.basic_tools <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> basic <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> aubio <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> source, pitch <span class="hljs-comment"><span class="hljs-comment"># load audio signal = basic.SignalObj('/home/eva/Documents/papers/habr/media/audio.wav') filename = '/home/eva/Documents/papers/habr/media/audio.wav' # YAAPT pitches pitchY = pYAAPT.yaapt(signal, frame_length=40, tda_frame_length=40, f0_min=75, f0_max=600) # YIN pitches downsample = 1 samplerate = 0 win_s = 1764 // downsample # fft size hop_s = 441 // downsample # hop size s = source(filename, samplerate, hop_s) samplerate = s.samplerate tolerance = 0.8 pitch_o = pitch("yin", win_s, hop_s, samplerate) pitch_o.set_unit("midi") pitch_o.set_tolerance(tolerance) pitchesYIN = [] confidences = [] total_frames = 0 while True: samples, read = s() pitch = pitch_o(samples)[0] pitch = int(round(pitch)) confidence = pitch_o.get_confidence() pitchesYIN += [pitch] confidences += [confidence] total_frames += read if read &lt; hop_s: break # load PRAAT pitches praat = np.genfromtxt('/home/eva/Documents/papers/habr/PraatPitch.txt', filling_values=0) praat = praat[:,1] # plot fig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(12, 8)) ax1.plot(np.asarray(pitchesYIN), label='YIN', color='green') ax1.legend(loc="upper right") ax2.plot(pitchY.samp_values, label='YAAPT', color='blue') ax2.legend(loc="upper right") ax3.plot(praat, label='Praat', color='red') ax3.legend(loc="upper right") plt.show()</span></span></code> </pre> <br></div></div><br><br><img src="https://habrastorage.org/webt/tv/k7/uq/tvk7uqi50ctcnd3j3rjq0sjzl8s.png"><br>  <i>Abbildung 4. Vergleich der Funktionsweise der Algorithmen YIN, YAAPT und Praat.</i> <br><br>  Wir sehen, dass YIN mit den Standardparametern ziemlich ausgeschlagen ist, eine sehr flache Flugbahn mit Werten unter Praat erhält und die Übergänge zwischen männlichen und weiblichen Stimmen sowie zwischen emotionaler und nicht emotionaler Sprache vollständig verliert. <br><br>  YAAPT hat einen sehr hohen Ton in der emotionalen weiblichen Sprache gekürzt, aber insgesamt deutlich besser abgeschnitten.  Aufgrund seiner spezifischen Funktionen funktioniert YAAPT besser - Sie können natürlich nicht sofort antworten, aber Sie können davon ausgehen, dass die Rolle darin besteht, Kandidaten aus drei Quellen zu erhalten und ihr Gewicht genauer zu berechnen als in YIN. <br><br>  <b>Fazit</b> <br><br>  Da sich die Frage, die Frequenz des Grundtons (F0) in der einen oder anderen Form zu bestimmen, vor fast jedem stellt, der mit Klang arbeitet, gibt es viele Möglichkeiten, ihn zu lösen.  Die Frage nach der erforderlichen Genauigkeit und den Merkmalen des Audiomaterials bestimmt jeweils, wie sorgfältig Parameter ausgewählt werden müssen. In einem anderen Fall können Sie sich auf eine Basislösung wie YAAPT beschränken.  Wenn wir Praat als Standardalgorithmus für die Sprachverarbeitung verwenden (dennoch verwenden es eine große Anzahl von Forschern), können wir daraus schließen, dass YAAPT in erster Näherung zuverlässiger und genauer als YIN ist, obwohl sich unser Beispiel als kompliziert herausstellte. <br><br>  Gepostet von <b>Eva Kazimirova</b> , Neurodata Lab Researcher, Sprachverarbeitungsspezialistin. <br><br>  <font color="green"><b>Offtop</b></font> : Gefällt dir der Artikel?  Tatsächlich haben wir eine Reihe solcher interessanten Aufgaben in den Bereichen ML, Mathematik und Programmierung, und wir brauchen ein Gehirn.  Interessieren Sie sich dafür?  Komm zu uns!  E-Mail: hr@neurodatalab.com <br><br><div class="spoiler">  <b class="spoiler_title">Referenzen</b> <div class="spoiler_text"><ol><li>  Rusz, J., Cmejla, R., Ruzickova, H., Ruzicka, E. Quantitative akustische Messungen zur Charakterisierung von Sprach- und Stimmstörungen bei der frühen unbehandelten Parkinson-Krankheit.  Das Journal der Acoustical Society of America, vol.  129, Ausgabe 1 (2011), pp.  350-367.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li><li>  Farrús, M., Hernando, J., Ejarque, P. Jitter und Schimmermessungen für die Sprechererkennung.  Tagungsband der Jahreskonferenz der International Speech Communication Association, INTERSPEECH, vol.  2 (2007), pp.  1153-1156.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li><li>  Zahorian, S., Hu, HA.  Spektrale / zeitliche Methode zur robusten Grundfrequenzverfolgung.  Das Journal der Acoustical Society of America, vol.  123, Ausgabe 6 (2008), pp.  4559-4571.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li><li>  De Cheveigné, A., Kawahara, H. YIN, ein grundlegender Frequenzschätzer für Sprache und Musik.  Das Journal der Acoustical Society of America, vol.  111, Ausgabe 4 (2002), pp.  1917-1930.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li></ol></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416441/">https://habr.com/ru/post/de416441/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416431/index.html">Die besten Blockchain-Projekte. ICO Juli 2018 (Abstimmung)</a></li>
<li><a href="../de416433/index.html">Fragen Sie Ethan: Können Strahlungsverluste von Sternen dunkle Energie erklären?</a></li>
<li><a href="../de416435/index.html">Warum ist das menschliche Gehirn so effektiv?</a></li>
<li><a href="../de416437/index.html">Gibt es genug Chemikalien auf den eisigen Welten, um das Leben dort zu erhalten?</a></li>
<li><a href="../de416439/index.html">iOS 12: Benachrichtigungsgruppierung</a></li>
<li><a href="../de416443/index.html">9 Geheimnisse von ASP.NET Core</a></li>
<li><a href="../de416445/index.html">Skillbox-Webinare: die interessantesten - kostenlos</a></li>
<li><a href="../de416449/index.html">.NET Core + Docker auf Raspberry Pi. Ist das legal?</a></li>
<li><a href="../de416451/index.html">Microsoft Research-Datenbanken jetzt für alle verfügbar</a></li>
<li><a href="../de416453/index.html">Diebstahlschemata in RBS-Systemen und fünf Ebenen der Gegenwirkung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>