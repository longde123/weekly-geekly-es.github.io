<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üß§ üèæ üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø Tonh√∂henverfolgung oder Bestimmung der Tonh√∂henfrequenz in der Sprache am Beispiel von Praat, YAAPT und YIN üë®üèø‚Äçü§ù‚Äçüë®üèº üë®üèæ üë©üèø‚Äçüíª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Bereich der Emotionserkennung ist die Stimme nach dem Gesicht die zweitwichtigste Quelle emotionaler Daten. Die Stimme kann durch mehrere Parameter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tonh√∂henverfolgung oder Bestimmung der Tonh√∂henfrequenz in der Sprache am Beispiel von Praat, YAAPT und YIN</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neurodatalab/blog/416441/"><img src="https://habrastorage.org/getpro/habr/post_images/37d/3f1/975/37d3f19758eb7d646ccff079d37772f8.png" alt="Bild"><br><br>  Im Bereich der Emotionserkennung ist die Stimme nach dem Gesicht die zweitwichtigste Quelle emotionaler Daten.  Die Stimme kann durch mehrere Parameter charakterisiert werden.  Die Tonh√∂he ist eine der Hauptmerkmale dieser Art. Auf dem Gebiet der Akustiktechnologie ist es jedoch korrekter, diesen Parameter als Grundfrequenz zu bezeichnen. <br><br>  Die Frequenz des Grundtons h√§ngt direkt mit dem zusammen, was wir Intonation nennen.  Und Intonation ist zum Beispiel mit den emotional ausdrucksstarken Eigenschaften der Stimme verbunden. <br><br>  Dennoch ist die Bestimmung der Frequenz des Grundtons keine v√∂llig triviale Aufgabe mit interessanten Nuancen.  In diesem Artikel werden wir die Merkmale von Algorithmen f√ºr ihre Bestimmung diskutieren und vorhandene L√∂sungen mit Beispielen f√ºr bestimmte Audioaufnahmen vergleichen. <br><a name="habracut"></a><br>  <b>Einf√ºhrung</b> <br><br>  Erinnern wir uns zun√§chst daran, was im Wesentlichen die Frequenz des Grundtons ist und f√ºr welche Aufgaben er m√∂glicherweise ben√∂tigt wird.  <i>Die Grundfrequenz</i> , die auch als CHOT, Grundfrequenz oder F0 bezeichnet wird, ist die Frequenz der Stimmb√§nder, wenn sie stimmhafte T√∂ne aussprechen.  Wenn Sie nicht-tonige T√∂ne aussprechen (stimmlos), z. B. fl√ºsternd sprechen oder zischende und pfeifende T√∂ne aussprechen, z√∂gern die B√§nder nicht, was bedeutet, dass diese Eigenschaft f√ºr sie nicht relevant ist. <br><br>  * Bitte beachten Sie, dass die Unterteilung in Ton- und Nicht-Ton-Kl√§nge nicht der Unterteilung in Vokale und Konsonanten entspricht. <br><br>  Die Variabilit√§t der Frequenz des Grundtons ist ziemlich gro√ü und kann nicht nur zwischen Menschen stark variieren (f√ºr M√§nnerstimmen mit niedrigerem Durchschnitt betr√§gt die Frequenz 70-200 Hz und f√ºr Frauenstimmen 400 Hz), sondern auch f√ºr eine Person, insbesondere bei emotionaler Sprache . <br><br>  Die Bestimmung der Frequenz des Grundtons wird verwendet, um eine breite Palette von Problemen zu l√∂sen: <br><br><ul><li>  Erkennen von Emotionen, wie wir oben sagten; </li><li>  Geschlechtsbestimmung; </li><li>  Bei der L√∂sung des Problems, Audio mit mehreren Stimmen zu segmentieren oder Sprache in Phrasen zu unterteilen; </li><li>  In der Medizin zur Bestimmung der pathologischen Eigenschaften der Stimme (z. B. anhand der akustischen Parameter Jitter und Shimmer).  Zum Beispiel die Identifizierung von Anzeichen der Parkinson-Krankheit [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1</a> ].  Jitter und Shimmer k√∂nnen auch verwendet werden, um Emotionen zu erkennen [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2</a> ]. </li></ul><br>  Es gibt jedoch eine Reihe von Schwierigkeiten bei der Bestimmung von F0.  Beispielsweise ist es h√§ufig m√∂glich, F0 mit Harmonischen zu verwechseln, was zu sogenannten Tonh√∂henverdopplungs- / Tonh√∂henhalbierungseffekten f√ºhren kann [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> ].  Und bei Audioaufnahmen mit schlechter Qualit√§t ist F0 ziemlich schwer zu berechnen, da die gew√ºnschte Spitze bei niedrigen Frequenzen fast verschwindet. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erinnerst</a> du dich √ºbrigens an die Geschichte von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laurel und Yanny</a> ?  Die Unterschiede in den W√∂rtern, die Menschen beim Anh√∂ren derselben Audioaufnahme h√∂ren, sind genau auf den Unterschied in der Wahrnehmung F0 zur√ºckzuf√ºhren, der von vielen Faktoren beeinflusst wird: dem Alter des H√∂rers, dem Erm√ºdungsgrad und dem Wiedergabeger√§t.  Wenn Sie also Aufnahmen in Lautsprechern mit qualitativ hochwertiger Wiedergabe niedriger Frequenzen h√∂ren, h√∂ren Sie Laurel und in Audiosystemen, in denen niedrige Frequenzen schlecht wiedergegeben werden, Yanny.  Der √úbergangseffekt ist beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> auf einem Ger√§t zu sehen.  In diesem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> fungiert das neuronale Netzwerk als Zuh√∂rer.  In einem anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel k√∂nnen</a> Sie lesen, wie das Yanny / Laurel-Ph√§nomen in Bezug auf die Sprachbildung erkl√§rt wird. <br><br>  Da eine detaillierte Analyse aller Methoden zur Bestimmung von F0 zu umfangreich w√§re, ist der Artikel √ºbersichtlicher Natur und kann zur Navigation im Thema beitragen. <br><br>  <b>Methoden zur Bestimmung von F0</b> <br><br>  Methoden zur Bestimmung von F0 k√∂nnen in drei Kategorien unterteilt werden: basierend auf der Zeitdynamik des Signals oder der Zeitdom√§ne;  basierend auf der Frequenzstruktur oder dem Frequenzbereich sowie kombinierten Methoden.  Wir empfehlen Ihnen, sich mit dem √úbersichtsartikel zum Thema vertraut zu machen, in dem die angegebenen Methoden zum Extrahieren von F0 detailliert analysiert werden. <br><br>  Beachten Sie, dass jeder der diskutierten Algorithmen aus drei Hauptschritten besteht: <br><br>  Vorverarbeitung (Filtern des Signals, Aufteilen in Frames) <br>  Suche nach m√∂glichen Werten von F0 (Kandidaten) <br>  Tracking ist die Wahl der wahrscheinlichsten Flugbahn F0 (da wir f√ºr jeden Moment mehrere konkurrierende Kandidaten haben, m√ºssen wir die wahrscheinlichste Spur unter ihnen finden). <br><br>  <b>Zeitbereich</b> <br><br>  Wir skizzieren einige allgemeine Punkte.  Vor der Anwendung der Zeitbereichsmethoden wird das Signal vorgefiltert, wobei nur niedrige Frequenzen √ºbrig bleiben.  Schwellenwerte werden festgelegt - die minimalen und maximalen Frequenzen, beispielsweise von 75 bis 500 Hz.  Die Bestimmung von F0 erfolgt nur f√ºr Bereiche mit harmonischer Sprache, da dies f√ºr Pausen oder Rauschger√§usche nicht nur bedeutungslos ist, sondern auch Fehler in benachbarten Rahmen verursachen kann, wenn Interpolation und / oder Gl√§ttung angewendet werden.  Die Rahmenl√§nge wird so ausgew√§hlt, dass sie mindestens drei Punkte enth√§lt. <br><br>  Die Hauptmethode, auf deren Grundlage sp√§ter eine ganze Familie von Algorithmen erschien, ist die Autokorrelation.  Der Ansatz ist recht einfach - es ist notwendig, die Autokorrelationsfunktion zu berechnen und ihr erstes Maximum zu nehmen.  Es wird die am st√§rksten ausgepr√§gte Frequenzkomponente im Signal angezeigt.  Was k√∂nnte die Schwierigkeit bei der Verwendung der Autokorrelation sein und warum ist es bei weitem nicht immer so, dass das erste Maximum der gew√ºnschten Frequenz entspricht?  Selbst unter nahezu idealen Bedingungen bei Aufnahmen hoher Qualit√§t kann das Verfahren aufgrund der komplexen Struktur des Signals falsch sein.  Unter realit√§tsnahen Bedingungen, bei denen unter anderem bei verrauschten Aufnahmen oder Aufnahmen von anf√§nglich geringer Qualit√§t das Verschwinden des gew√ºnschten Peaks auftreten kann, steigt die Anzahl der Fehler stark an. <br><br>  Trotz der Fehler ist die Autokorrelationsmethode aufgrund ihrer einfachen Einfachheit und Logik recht praktisch und attraktiv, weshalb sie in vielen Algorithmen, einschlie√ülich YIN, als Grundlage dient.  Sogar der Name des Algorithmus verweist auf das Gleichgewicht zwischen der Bequemlichkeit und Ungenauigkeit der Autokorrelationsmethode: "Der Name YIN von" Yin "und" Yang "der orientalischen Philosophie spielt auf das Zusammenspiel von Autokorrelation und Aufhebung an."  [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">4</a> ] <br><br>  Die Macher von YIN versuchten, die Schw√§chen des Autokorrelationsansatzes zu beheben.  Die erste √Ñnderung ist die Verwendung der Funktion Cumulative Mean Normalized Difference, die die Empfindlichkeit gegen√ºber Amplitudenmodulationen verringern und die Peaks st√§rker machen soll: <br><br>  \ begin {Gleichung} <br>  d'_t (\ tau) = <br>  \ begin {F√§lle} <br>  1, &amp; \ tau = 0 \\ <br>  d_t (\ tau) \ bigg / \ bigg [\ frac {1} {\ tau} \ sum \ limit_ {j = 1} ^ {\ tau} d_t (j) \ bigg], &amp; \ text {else} <br>  \ end {F√§lle} <br>  \ end {Gleichung} <br>  YIN versucht auch, Fehler zu vermeiden, die auftreten, wenn die L√§nge der Fensterfunktion nicht vollst√§ndig durch die Schwingungsdauer geteilt wird.  Hierzu wird eine parabolische Minimalinterpolation verwendet.  Im letzten Schritt der Audiosignalverarbeitung wird die Funktion "Beste lokale Sch√§tzung" ausgef√ºhrt, um scharfe Spr√ºnge in den Werten zu verhindern (ob gut oder schlecht - dies ist ein strittiger Punkt). <br><br>  <b>Frequenzbereich</b> <br><br>  Wenn wir √ºber den Frequenzbereich sprechen, tritt die harmonische Struktur des Signals in den Vordergrund, dh das Vorhandensein von Spektralspitzen bei Frequenzen, die ein Vielfaches von F0 sind.  Sie k√∂nnen dieses periodische Muster mithilfe der Cepstral-Analyse zu einem klaren Peak ‚Äûkollabieren‚Äú.  Cepstrum - Fourier-Transformation des Logarithmus des Leistungsspektrums;  Der Cepstral-Peak entspricht der periodischsten Komponente des Spektrums (man kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> dar√ºber lesen). <br><br>  <b>Hybridmethoden zur Bestimmung von F0</b> <br><br>  Der n√§chste Algorithmus, der n√§her untersucht werden sollte, tr√§gt den sprechenden Namen YAAPT - ein weiterer Algorithmus f√ºr die Tonh√∂henverfolgung - und ist in der Tat hybride, da er sowohl Frequenz- als auch Zeitinformationen verwendet.  Eine vollst√§ndige Beschreibung finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> . Hier beschreiben wir nur die Hauptphasen. <br><br><img src="https://habrastorage.org/webt/r2/mu/uj/r2muujzlcxgdgp5a0bqem3t_iuu.png"><br>  <i>Abbildung 1. YAAPTalgo-Algorithmusdiagramm ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> )</i> . <br><br>  YAAPT besteht aus mehreren Hauptschritten, von denen der erste die Vorverarbeitung ist.  In diesem Stadium werden die Werte des urspr√ºnglichen Signals quadriert und eine zweite Version des Signals erhalten.  Dieser Schritt verfolgt das gleiche Ziel wie die kumulative mittlere normalisierte Differenzfunktion in YIN - Verst√§rkung und Wiederherstellung von "gestauten" Autokorrelationsspitzen.  Beide Versionen des Signals werden gefiltert - normalerweise reichen sie von 50 bis 1500 Hz, manchmal von 50 bis 900 Hz. <br><br>  Dann wird die Basistrajektorie F0 aus dem Spektrum des umgewandelten Signals berechnet.  Kandidaten f√ºr F0 werden unter Verwendung der Funktion Spectral Harmonics Correlation (SHC) bestimmt. <br><br>  \ begin {Gleichung} <br>  SHC (t, f) = \ Summe \ Grenzen_ {f '= - WL / 2} ^ {WL / 2} \ Produkt \ Grenzen_ {r = 1} ^ {NH + 1} S (t, rf + f') <br>  \ end {Gleichung} <br>  Dabei ist S (t, f) das Betragsspektrum f√ºr den Rahmen t und die Frequenz f, WL die Fensterl√§nge in Hz, NH die Anzahl der Harmonischen (die Autoren empfehlen die Verwendung der ersten drei Harmonischen).  Die spektrale Leistung wird auch verwendet, um stimmhafte-stimmlose Frames zu bestimmen, wonach die optimalste Trajektorie gesucht wird, und die M√∂glichkeit der Tonh√∂henverdopplung / Tonh√∂henhalbierung wird ber√ºcksichtigt [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , Abschnitt II, C]. <br><br>  Ferner werden Kandidaten f√ºr F0 sowohl f√ºr das Anfangssignal als auch f√ºr das konvertierte Signal bestimmt, und anstelle der Autokorrelationsfunktion wird hier die normalisierte Kreuzkorrelation (NCCF) verwendet. <br><br>  \ begin {Gleichung} <br>  NCCF (m) = \ frac {\ sum \ begrenzt_ {n = 0} ^ {Nm-1} x (n) * x (n + m)} {\ sqrt {\ sum \ Grenzen_ {n = 0} ^ { Nm-1} x ^ 2 (n) * \ sum \ limit_ {n = 0} ^ {Nm-1} x ^ 2 (n + m)}} \ text {,} \ hspace {0,3 cm} 0 &lt;m &lt;M_ {0} <br>  \ end {Gleichung} <br>  Der n√§chste Schritt besteht darin, alle m√∂glichen Kandidaten zu bewerten und ihre Signifikanz oder ihr Gewicht (Verdienst) zu berechnen.  Das Gewicht der aus dem Audiosignal erhaltenen Kandidaten h√§ngt nicht nur von der Amplitude des NCCF-Peaks ab, sondern auch von ihrer N√§he zur aus dem Spektrum bestimmten Trajektorie F0.  Das hei√üt, der Frequenzbereich wird hinsichtlich der Genauigkeit als grob, aber als stabil angesehen [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , Abschnitt II, D]. <br><br>  Dann wird f√ºr alle Paare der verbleibenden Kandidaten die √úbergangskostenmatrix berechnet - der √úbergangspreis, zu dem sie letztendlich die optimale Flugbahn finden [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3</a> , Abschnitt II, E]. <br><br>  <b>Beispiele</b> <br><br>  Jetzt wenden wir alle oben genannten Algorithmen auf bestimmte Audioaufnahmen an.  Als Ausgangspunkt werden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Praat verwenden</a> , ein Werkzeug, das f√ºr viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachwissenschaftler von</a> grundlegender Bedeutung ist.  In Python werden wir uns dann die Implementierung von YIN und YAAPT ansehen und die erhaltenen Ergebnisse vergleichen. <br><br>  Als Audiomaterial k√∂nnen Sie jedes verf√ºgbare Audio verwenden.  Wir haben mehrere Ausz√ºge aus unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RAMAS-</a> Datenbank <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">entnommen</a> - ein multimodaler <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datensatz</a> , der unter Beteiligung von VGIK-Akteuren erstellt wurde.  Sie k√∂nnen auch Material aus anderen offenen Datenbanken wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LibriSpeech</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RAVDESS verwenden</a> . <br><br>  Als anschauliches Beispiel haben wir Ausz√ºge aus mehreren Aufnahmen mit neutralen und emotional gef√§rbten M√§nner- und Frauenstimmen genommen und aus Gr√ºnden der Klarheit zu einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufnahme zusammengefasst</a> .  Schauen wir uns unser Signal, sein Spektrogramm, seine Intensit√§t (orange Farbe) und F0 (blaue Farbe) an.  In Praat kann dies mit Strg + O (√ñffnen - Aus Datei lesen) und dann mit der Schaltfl√§che Anzeigen und Bearbeiten erfolgen. <br><br><img src="https://habrastorage.org/webt/ir/ay/kh/iraykhpzwfetpahhiymdic6pdwi.png"><br>  <i>Abbildung 2. Spektrogramm, Intensit√§t (orange Farbe), F0 (blaue Farbe) in Praat.</i> <br><br>  Das Audio zeigt ziemlich deutlich, dass in der emotionalen Sprache die Tonh√∂he sowohl bei M√§nnern als auch bei Frauen zunimmt.  Gleichzeitig kann F0 f√ºr emotionale m√§nnliche Sprache gut mit F0 einer weiblichen Stimme verglichen werden. <br><br>  <b>Tracking</b> <br><br>  W√§hlen Sie im Praat-Men√º die Registerkarte Periodizit√§t analysieren - bis Tonh√∂he (ac), dh die Definition von F0 mithilfe der Autokorrelation.  Es erscheint ein Fenster zum Einstellen von Parametern, in dem 3 Parameter zum Bestimmen von Kandidaten f√ºr F0 und 6 weitere Parameter f√ºr den Pfadfinder-Algorithmus festgelegt werden k√∂nnen, der den wahrscheinlichsten Pfad F0 unter allen Kandidaten erstellt. <br><br><div class="spoiler">  <b class="spoiler_title">Viele Parameter (in Praat befindet sich ihre Beschreibung auch auf der Schaltfl√§che Hilfe)</b> <div class="spoiler_text"><ul><li>  Stille-Schwelle - Die Schwelle der relativen Amplitude des Signals zur Bestimmung der Stille. Der Standardwert betr√§gt 0,03. </li><li>  Sprachschwelle - das Gewicht des stimmlosen Kandidaten, der Maximalwert ist 1. Je h√∂her dieser Parameter, desto mehr Frames werden als stimmlos definiert, dh ohne Tonger√§usche.  In diesen Rahmen wird F0 nicht bestimmt.  Der Wert dieses Parameters ist der Schwellenwert f√ºr Spitzen der Autokorrelationsfunktion.  Der Standardwert ist 0,45. </li><li>  Oktavkosten - bestimmt, wie viel mehr Gewicht die Hochfrequenzkandidaten im Vergleich zu den Niederfrequenzkandidaten haben.  Je h√∂her der Wert, desto mehr wird der Hochfrequenzkandidat bevorzugt.  Der Standardwert ist 0,01 pro Oktave. </li><li>  Oktavsprungkosten - Mit einer Erh√∂hung dieses Koeffizienten nimmt die Anzahl der scharfen sprungartigen √úberg√§nge zwischen aufeinanderfolgenden Werten von F0 ab.  Der Standardwert ist 0,35. </li><li>  Voiced / Unvoiced-Kosten - Durch Erh√∂hen dieses Koeffizienten wird die Anzahl der Voiced / Unvoiced-√úberg√§nge verringert.  Der Standardwert ist 0,14. </li><li>  Tonh√∂henobergrenze (Hz) - Kandidaten √ºber dieser Frequenz werden nicht ber√ºcksichtigt.  Der Standardwert ist 600 Hz. </li></ul><br></div></div><br>  Eine detaillierte Beschreibung des Algorithmus findet sich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem Artikel von</a> 1993. <br><br>  Wie das Ergebnis des Trackers (Pfadfinder) aussieht, k√∂nnen Sie sehen, indem Sie auf OK klicken und dann die resultierende Pitch-Datei anzeigen (Anzeigen &amp; Bearbeiten).  Es ist ersichtlich, dass es zus√§tzlich zu der ausgew√§hlten Flugbahn noch ziemlich signifikante Kandidaten mit einer niedrigeren Frequenz gab. <br><br><img src="https://habrastorage.org/webt/wq/rq/vf/wqrqvf_cbnbn8orcajij6sfrasu.png"><br>  <i>Abbildung 3. PitchPath f√ºr die ersten 1,3 Sekunden der Audioaufnahme.</i> <br><br>  <b>Aber was ist mit Python?</b> <br><br>  Nehmen wir zwei Bibliotheken, die Pitch Tracking anbieten - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aubio</a> , in dem der Standardalgorithmus YIN ist, und die Bibliothek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AMFM_decompsition</a> , in der der YAAPT-Algorithmus implementiert ist.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F√ºgen Sie</a> in der separaten Datei (Datei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PraatPitch.txt</a> ) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> F0-Werte von Praat ein (dies kann manuell erfolgen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">W√§hlen Sie die Audiodatei</a> aus, klicken Sie auf Anzeigen und Bearbeiten, w√§hlen Sie die gesamte Datei aus und w√§hlen Sie im oberen Men√º die Liste Pitch-Pitch aus). <br><br>  Vergleichen Sie nun die Ergebnisse f√ºr alle drei Algorithmen (YIN, YAAPT, Praat). <br><br><div class="spoiler">  <b class="spoiler_title">Viel Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.basic_tools <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> basic <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> aubio <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> source, pitch <span class="hljs-comment"><span class="hljs-comment"># load audio signal = basic.SignalObj('/home/eva/Documents/papers/habr/media/audio.wav') filename = '/home/eva/Documents/papers/habr/media/audio.wav' # YAAPT pitches pitchY = pYAAPT.yaapt(signal, frame_length=40, tda_frame_length=40, f0_min=75, f0_max=600) # YIN pitches downsample = 1 samplerate = 0 win_s = 1764 // downsample # fft size hop_s = 441 // downsample # hop size s = source(filename, samplerate, hop_s) samplerate = s.samplerate tolerance = 0.8 pitch_o = pitch("yin", win_s, hop_s, samplerate) pitch_o.set_unit("midi") pitch_o.set_tolerance(tolerance) pitchesYIN = [] confidences = [] total_frames = 0 while True: samples, read = s() pitch = pitch_o(samples)[0] pitch = int(round(pitch)) confidence = pitch_o.get_confidence() pitchesYIN += [pitch] confidences += [confidence] total_frames += read if read &lt; hop_s: break # load PRAAT pitches praat = np.genfromtxt('/home/eva/Documents/papers/habr/PraatPitch.txt', filling_values=0) praat = praat[:,1] # plot fig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(12, 8)) ax1.plot(np.asarray(pitchesYIN), label='YIN', color='green') ax1.legend(loc="upper right") ax2.plot(pitchY.samp_values, label='YAAPT', color='blue') ax2.legend(loc="upper right") ax3.plot(praat, label='Praat', color='red') ax3.legend(loc="upper right") plt.show()</span></span></code> </pre> <br></div></div><br><br><img src="https://habrastorage.org/webt/tv/k7/uq/tvk7uqi50ctcnd3j3rjq0sjzl8s.png"><br>  <i>Abbildung 4. Vergleich der Funktionsweise der Algorithmen YIN, YAAPT und Praat.</i> <br><br>  Wir sehen, dass YIN mit den Standardparametern ziemlich ausgeschlagen ist, eine sehr flache Flugbahn mit Werten unter Praat erh√§lt und die √úberg√§nge zwischen m√§nnlichen und weiblichen Stimmen sowie zwischen emotionaler und nicht emotionaler Sprache vollst√§ndig verliert. <br><br>  YAAPT hat einen sehr hohen Ton in der emotionalen weiblichen Sprache gek√ºrzt, aber insgesamt deutlich besser abgeschnitten.  Aufgrund seiner spezifischen Funktionen funktioniert YAAPT besser - Sie k√∂nnen nat√ºrlich nicht sofort antworten, aber Sie k√∂nnen davon ausgehen, dass die Rolle darin besteht, Kandidaten aus drei Quellen zu erhalten und ihr Gewicht genauer zu berechnen als in YIN. <br><br>  <b>Fazit</b> <br><br>  Da sich die Frage, die Frequenz des Grundtons (F0) in der einen oder anderen Form zu bestimmen, vor fast jedem stellt, der mit Klang arbeitet, gibt es viele M√∂glichkeiten, ihn zu l√∂sen.  Die Frage nach der erforderlichen Genauigkeit und den Merkmalen des Audiomaterials bestimmt jeweils, wie sorgf√§ltig Parameter ausgew√§hlt werden m√ºssen. In einem anderen Fall k√∂nnen Sie sich auf eine Basisl√∂sung wie YAAPT beschr√§nken.  Wenn wir Praat als Standardalgorithmus f√ºr die Sprachverarbeitung verwenden (dennoch verwenden es eine gro√üe Anzahl von Forschern), k√∂nnen wir daraus schlie√üen, dass YAAPT in erster N√§herung zuverl√§ssiger und genauer als YIN ist, obwohl sich unser Beispiel als kompliziert herausstellte. <br><br>  Gepostet von <b>Eva Kazimirova</b> , Neurodata Lab Researcher, Sprachverarbeitungsspezialistin. <br><br>  <font color="green"><b>Offtop</b></font> : Gef√§llt dir der Artikel?  Tats√§chlich haben wir eine Reihe solcher interessanten Aufgaben in den Bereichen ML, Mathematik und Programmierung, und wir brauchen ein Gehirn.  Interessieren Sie sich daf√ºr?  Komm zu uns!  E-Mail: hr@neurodatalab.com <br><br><div class="spoiler">  <b class="spoiler_title">Referenzen</b> <div class="spoiler_text"><ol><li>  Rusz, J., Cmejla, R., Ruzickova, H., Ruzicka, E. Quantitative akustische Messungen zur Charakterisierung von Sprach- und Stimmst√∂rungen bei der fr√ºhen unbehandelten Parkinson-Krankheit.  Das Journal der Acoustical Society of America, vol.  129, Ausgabe 1 (2011), pp.  350-367.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li><li>  Farr√∫s, M., Hernando, J., Ejarque, P. Jitter und Schimmermessungen f√ºr die Sprechererkennung.  Tagungsband der Jahreskonferenz der International Speech Communication Association, INTERSPEECH, vol.  2 (2007), pp.  1153-1156.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li><li>  Zahorian, S., Hu, HA.  Spektrale / zeitliche Methode zur robusten Grundfrequenzverfolgung.  Das Journal der Acoustical Society of America, vol.  123, Ausgabe 6 (2008), pp.  4559-4571.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li><li>  De Cheveign√©, A., Kawahara, H. YIN, ein grundlegender Frequenzsch√§tzer f√ºr Sprache und Musik.  Das Journal der Acoustical Society of America, vol.  111, Ausgabe 4 (2002), pp.  1917-1930.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang</a> </li></ol></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416441/">https://habr.com/ru/post/de416441/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416431/index.html">Die besten Blockchain-Projekte. ICO Juli 2018 (Abstimmung)</a></li>
<li><a href="../de416433/index.html">Fragen Sie Ethan: K√∂nnen Strahlungsverluste von Sternen dunkle Energie erkl√§ren?</a></li>
<li><a href="../de416435/index.html">Warum ist das menschliche Gehirn so effektiv?</a></li>
<li><a href="../de416437/index.html">Gibt es genug Chemikalien auf den eisigen Welten, um das Leben dort zu erhalten?</a></li>
<li><a href="../de416439/index.html">iOS 12: Benachrichtigungsgruppierung</a></li>
<li><a href="../de416443/index.html">9 Geheimnisse von ASP.NET Core</a></li>
<li><a href="../de416445/index.html">Skillbox-Webinare: die interessantesten - kostenlos</a></li>
<li><a href="../de416449/index.html">.NET Core + Docker auf Raspberry Pi. Ist das legal?</a></li>
<li><a href="../de416451/index.html">Microsoft Research-Datenbanken jetzt f√ºr alle verf√ºgbar</a></li>
<li><a href="../de416453/index.html">Diebstahlschemata in RBS-Systemen und f√ºnf Ebenen der Gegenwirkung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>