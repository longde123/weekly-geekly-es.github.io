<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😢 🚖 📣 Was KI-Forscher über die damit verbundenen möglichen Risiken denken 🖖🏽 💤 👩🏾‍🎤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ich habe mich bereits 2007 für die mit KI verbundenen Risiken interessiert. Zu dieser Zeit war die Reaktion der meisten Menschen auf dieses Thema unge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Was KI-Forscher über die damit verbundenen möglichen Risiken denken</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/">  Ich habe mich bereits 2007 für die mit KI verbundenen Risiken interessiert.  Zu dieser Zeit war die Reaktion der meisten Menschen auf dieses Thema ungefähr so: "Es ist sehr lustig, zurück zu kommen, wenn jemand anderes als Internet-Idioten daran glauben wird." <br><br>  In den folgenden Jahren teilten mehrere äußerst kluge und einflussreiche Persönlichkeiten, darunter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stephen Hawking</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Elon Musk</a> , öffentlich ihre Besorgnis über die Risiken der KI, gefolgt von Hunderten anderer Intellektueller, von Oxford-Philosophen bis zu Kosmologen vom MIT und Investoren aus dem Silicon Valley .  Und wir sind zurück. <br><br>  Dann änderte sich die Reaktion zu: "Nun, einige Wissenschaftler und Geschäftsleute können das glauben, aber es ist unwahrscheinlich, dass sie echte Experten auf diesem Gebiet sind, die sich in der Situation wirklich auskennen." <br><br>  Von hier kamen Aussagen wie der populärwissenschaftliche Artikel „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bill Gates hat Angst vor KI, aber KI-Forscher sollten es wissen</a> “: <br><blockquote>  Nach einem Gespräch mit KI-Forschern - echten Forschern, die solche Systeme kaum zum Laufen bringen, ganz zu schweigen von ihrer guten Funktionsweise - wird deutlich, dass sie keine Angst haben, dass sich Superintelligenz plötzlich an sie heranschleicht, weder jetzt noch in Zukunft .  Trotz all der beängstigenden Geschichten, die Mask erzählt, haben die Forscher es nicht eilig, Schutzräume und Selbstzerstörung mit einem Countdown zu bauen. </blockquote><a name="habracut"></a><br>  Oder, wie sie auf Fusion.net im Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einspruch gegen Killerroboter von einer Person, die tatsächlich KI entwickelt</a> " geschrieben haben: <br><blockquote>  Andrew Angie entwickelt professionell KI-Systeme.  Er unterrichtete einen KI-Kurs in Stanford, entwickelte KI bei Google und wechselte dann zur chinesischen Baidu-Suchmaschine, um seine Arbeit an der Spitze der Anwendung von KI auf reale Probleme fortzusetzen.  Wenn er hört, wie Elon Musk oder Stephen Hawking - Leute, die mit moderner Technologie nicht direkt vertraut sind - über KI sprechen, die möglicherweise die Menschheit zerstören könnte, kann man fast hören, wie er sein Gesicht mit den Händen bedeckt. </blockquote><br>  Ramez Naam von Marginal Revolution wiederholt ungefähr dasselbe im Artikel „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Was denken Forscher über die Risiken von KI?</a> “: <br><blockquote>  Elon Musk, Stephen Hawking und Bill Gates haben kürzlich Bedenken geäußert, dass die Entwicklung der KI das Szenario „KI-Killer“ umsetzen und möglicherweise zum Aussterben der Menschheit führen könnte.  Sie sind keine KI-Forscher, und soweit ich weiß, haben sie nicht direkt mit KI gearbeitet.  Was denken echte KI-Forscher über die Risiken der KI? </blockquote><br>  Er zitiert die Worte speziell ausgewählter KI-Forscher, wie die Autoren anderer Geschichten - und hört dann auf, ohne andere Meinungen zu erwähnen. <br><br>  Aber sie existieren.  KI-Forscher, einschließlich führender Fachleute, haben von Anfang an aktiv Bedenken hinsichtlich der Risiken von KI und darüber hinaus geäußert.  Ich werde zunächst diese Personen trotz Naams Liste auflisten und dann weitermachen, warum ich dies nicht als "Diskussion" im klassischen Sinne betrachte, die von der Auflistung der Sterne erwartet wird. <br><br>  Die Kriterien meiner Liste lauten wie folgt: Ich erwähne nur die renommiertesten Forscher oder Professoren der Wissenschaft an guten Instituten mit vielen Zitaten wissenschaftlicher Arbeiten oder sehr angesehene Wissenschaftler aus der Industrie, die für große Unternehmen arbeiten und eine gute Erfolgsbilanz vorweisen können.  Sie beschäftigen sich mit KI und maschinellem Lernen.  Sie haben mehrere starke Aussagen zur Unterstützung eines bestimmten Standpunkts in Bezug auf das Einsetzen einer Singularität oder eines ernsthaften Risikos durch die KI in naher Zukunft.  Einige von ihnen haben Werke oder Bücher zu diesem Thema geschrieben.  Andere drückten einfach ihre Gedanken aus und glaubten, dass dies ein wichtiges Thema ist, das es wert ist, studiert zu werden. <br><br>  Wenn jemand mit der Aufnahme einer Person in diese Liste nicht einverstanden ist oder glaubt, dass ich etwas Wichtiges vergessen habe, lassen Sie es mich wissen. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stuart Russell</a> ist Professor für Informatik in Berkeley, Gewinner des IJCAI Computers And Thought Award, Forscher bei der Computer Mechanization Association, Forscher an der American Academy of Advanced Scientific Research, Direktor des Zentrums für Intelligente Systeme, Gewinner des Blaise Pascal-Preises usw.  usw.  Co-Autor von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI: A Modern Approach</a> , einem klassischen Lehrbuch, das an 1.200 Universitäten weltweit verwendet wird.  Auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">seiner Website</a> schreibt er: <br><blockquote>  Auf dem Gebiet der KI wurden 50 Jahre lang unter der Annahme gearbeitet, dass je klüger desto besser.  Die Sorge um das Wohl der Menschheit muss damit verbunden sein.  Das Argument ist einfach: <br><br>  1. Die KI wird wahrscheinlich erfolgreich erstellt. <br>  2. Unbegrenzter Erfolg führt zu großen Risiken und großen Vorteilen. <br>  3. Was können wir tun, um die Chancen zu erhöhen, Vorteile zu erzielen und Risiken zu vermeiden? <br><br>  Einige Organisationen arbeiten bereits an diesen Themen, darunter das Institut für die Zukunft der Menschheit in Oxford, das Zentrum für existenzielle Risikostudien in Cambridge (CSER), das Institut für das Studium der maschinellen Intelligenz in Berkeley und das Institut für das zukünftige Leben in Harvard / MIT (FLI).  Ich bin in Beiräten mit CSER und FLI. <br><br>  So wie Forscher der Kernfusion das Problem der Begrenzung von Kernreaktionen als eines der wichtigsten Probleme auf ihrem Gebiet betrachteten, wird die Entwicklung des Gebiets der KI unweigerlich Kontroll- und Sicherheitsprobleme aufwerfen.  Die Forscher beginnen bereits, Fragen zu stellen, von rein technischen (die Hauptprobleme der Rationalität und Nützlichkeit usw.) bis zu allgemein philosophischen. </blockquote><br>  Auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">edge.org</a> beschreibt er einen ähnlichen Standpunkt: <br><blockquote>  Wie von Steve Omohandro, Nick Bostrom und anderen erklärt, kann eine Diskrepanz in den Werten mit Entscheidungssystemen, deren Möglichkeiten ständig zunehmen, zu Problemen führen - vielleicht sogar zu Problemen des Ausmaßes des Aussterbens, wenn die Maschinen leistungsfähiger sind als Menschen.  Einige glauben, dass es in den kommenden Jahrhunderten keine vorhersehbaren Risiken für die Menschheit gibt, und vergessen vielleicht, dass der Zeitunterschied zwischen Rutherfords zuversichtlicher Aussage, dass Atomenergie niemals extrahiert werden kann, und weniger als 24 Stunden durch die Erfindung der Neutronen-initiierten Kernkettenreaktion vergangen ist . </blockquote><br>  Er versuchte auch, ein Vertreter dieser Ideen in der akademischen Gemeinschaft zu werden, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">und gab an</a> : <br><blockquote>  Ich finde, dass die wichtigsten Leute in dieser Branche, die noch nie zuvor Ängste geäußert haben, sich denken, dass dieses Problem sehr ernst genommen werden muss, und je früher wir es ernst nehmen, desto besser. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David McAllister</a> ist Professor und Senior Fellow am Toyota Institute of Technology der University of Chicago, der zuvor an den Fakultäten des MIT und des Cornell Institute gearbeitet hat.  Er ist Mitglied der American AI Association, hat mehr als hundert Werke veröffentlicht, Forschungen in den Bereichen maschinelles Lernen, Programmiertheorie, automatische Entscheidungsfindung, KI-Planung und Computerlinguistik durchgeführt und die Algorithmen des berühmten Deep Blue-Schachcomputers maßgeblich beeinflusst.  Laut einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> in der Pittsburgh Tribune Review: <br><blockquote>  Der Chicagoer Professor David McAllister hält es für unvermeidlich, dass vollautomatische intelligente Maschinen in der Lage sind, intelligentere Versionen von sich selbst zu entwerfen und zu erstellen, dh den Beginn eines Ereignisses, das als [technologische] Singularität bekannt ist.  Die Singularität wird es Maschinen ermöglichen, unendlich intelligent zu werden, was zu einem „unglaublich gefährlichen Szenario“ führt, sagt er. </blockquote><br>  In seinem Blog <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Thoughts on Cars</a> schreibt er: <br><blockquote>  Die meisten Informatiker weigern sich, über echte Erfolge in der KI zu sprechen.  Ich denke, es wäre vernünftiger zu sagen, dass niemand vorhersagen kann, wann eine mit dem menschlichen Verstand vergleichbare KI empfangen wird.  John MacArthy hat mir einmal erzählt, dass er, wenn er gefragt wird, wann die KI auf menschlicher Ebene erstellt wird, antwortet, dass sie fünf bis fünfhundert Jahre alt ist.  MacArthy war schlau.  Angesichts der Unsicherheiten in diesem Bereich ist es vernünftig, das Problem der freundlichen KI in Betracht zu ziehen ... <br><br>  In den frühen Stadien wird die allgemeine KI sicher sein.  Die frühen Stadien von OII werden jedoch ein ausgezeichneter Testort für KI als Diener oder andere freundliche KI-Optionen sein.  Ben Goertzel wirbt auch in einem guten Beitrag in seinem Blog für einen experimentellen Ansatz.  Wenn uns die Ära sicherer und nicht zu intelligenter OIIs erwartet, haben wir Zeit, über gefährlichere Zeiten nachzudenken. </blockquote><br>  Er war Mitglied der Expertengruppe des AAAI-Gremiums für langfristige KI-Futures, die sich den langfristigen Perspektiven der KI widmete, Vorsitzender des Ausschusses für langfristige Kontrolle und wird <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wie folgt beschrieben</a> : <br><blockquote>  Makalister sprach mit mir über den Ansatz der "Singularität", ein Ereignis, bei dem Computer schlauer werden als Menschen.  Er nannte nicht das genaue Datum seines Auftretens, sagte aber, dass dies in den nächsten paar Jahrzehnten passieren könnte und am Ende definitiv passieren würde.  Hier sind seine Ansichten zur Singularität.  Zwei wichtige Ereignisse werden eintreten: operative Intelligenz, bei der wir problemlos mit Computern sprechen können, und eine Kettenreaktion der KI, bei der sich der Computer ohne Hilfe verbessern und dann erneut wiederholen kann.  Das erste Ereignis werden wir in automatischen Assistenzsystemen bemerken, die uns wirklich helfen werden.  Später wird es wirklich interessant, mit Computern zu kommunizieren.  Und damit Computer alles können, was Menschen können, müssen Sie auf das Eintreten des zweiten Ereignisses warten. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hans Moravek</a> ist ehemaliger Professor am Institut für Robotik der Carnegie Mellon University und nach ihm das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Paradoxon von Moravec</a> , dem Gründer der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SeeGrid Corporation</a> , benannt, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sich</a> auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bildverarbeitungssysteme</a> für industrielle Anwendungen spezialisiert hat.  Seine Arbeit „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Synthese von Sensoren in den Sicherheitsgittern mobiler Roboter</a> “ wurde mehr als tausend Mal zitiert, und er wurde eingeladen, einen Artikel für die British Encyclopedia of Robotics zu schreiben, zu einer Zeit, als Artikel in Enzyklopädien von Experten auf diesem Gebiet und nicht von Hunderten anonymer Internetkommentatoren verfasst wurden. <br><br>  Er ist auch der Autor von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Robot: Von einer einfachen Maschine zu einem transzendentalen Geist</a> , den Amazon wie folgt beschreibt: <br><blockquote>  In diesem spannenden Buch sagt Hans Moravek voraus, dass sich Maschinen bis 2040 dem intellektuellen Niveau der Menschen nähern und uns bis 2050 übertreffen werden.  Aber während Moravec das Ende einer Ära menschlicher Dominanz vorhersagt, ist seine Vision dieses Ereignisses nicht so trostlos.  Er ist nicht von einer Zukunft eingezäunt, in der Maschinen die Welt regieren, sondern akzeptiert und beschreibt eine erstaunliche Sichtweise, nach der intelligente Roboter unsere evolutionären Nachkommen werden.  Moravec glaubt, dass sich am Ende dieses Prozesses "der riesige Cyberspace mit dem unmenschlichen Supermind vereinen und sich mit Angelegenheiten befassen wird, die weit entfernt von Menschen und weit entfernt von menschlichen Angelegenheiten von Bakterien sind". </blockquote><br>  Shane Leg ist Mitbegründer von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind Technologies</a> , einem KI-Startup, das 2014 von Google für 500 Millionen US-Dollar gekauft wurde.  Er promovierte am nach ihm benannten Institut für KI  Dale Moul in der Schweiz und arbeitete auch in der Abteilung für Computational Neurobiology.  Gatsby in London.  Am Ende seiner Dissertation „Maschinen-Superintelligenz“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schreibt er</a> : <br><blockquote>  Wenn jemals etwas auftaucht, das sich der absoluten Leistung annähern kann, wird es eine superintelligente Maschine sein.  Per Definition wird sie in der Lage sein, eine Vielzahl von Zielen in einer Vielzahl von Umgebungen zu erreichen.  Wenn wir uns im Voraus sorgfältig auf eine solche Gelegenheit vorbereiten, können wir nicht nur die Katastrophe vermeiden, sondern auch eine Ära des Wohlstands beginnen, anders als alles, was zuvor existierte. </blockquote><br>  In einem anschließenden Interview <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sagt er</a> : <br><blockquote>  AI ist jetzt dort, wo das Internet 1988 war.  In speziellen Anwendungen (Suchmaschinen wie Google, Hedge Funds und Bioinformatik) sind Anforderungen an maschinelles Lernen erforderlich, und ihre Zahl wächst von Jahr zu Jahr.  Ich denke, dass dieser Prozess Mitte des nächsten Jahrzehnts massiv und spürbar werden wird.  Der KI-Boom wird voraussichtlich um 2020 stattfinden, gefolgt von einem Jahrzehnt rascher Fortschritte, möglicherweise nach Marktkorrekturen.  KI auf menschlicher Ebene wird um Mitte 2020 geschaffen, obwohl viele Menschen den Beginn dieses Ereignisses nicht akzeptieren werden.  Danach werden die mit fortgeschrittener KI verbundenen Risiken in die Praxis umgesetzt.  Ich werde nicht über die "Singularität" sagen, aber sie erwarten, dass irgendwann nach der Schaffung der OII verrückte Dinge passieren werden.  Es ist irgendwo zwischen 2025 und 2040. </blockquote><br>  Er und seine Mitbegründer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Demis Khasabis</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mustafa Suleiman</a> unterzeichneten eine Petition für das Institut für zukünftiges Leben in Bezug auf KI-Risiken. Eine ihrer Bedingungen für den Beitritt zu Google war, dass das Unternehmen sich bereit erklärt, einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">KI-Ethikrat</a> zu organisieren, um diese Probleme zu untersuchen. <br><br>  Steve Omohundro ist ehemaliger Professor für Informatik an der Universität von Illinois, Gründer der Computer Vision- und Trainingsgruppe am Zentrum für das Studium komplexer Systeme und Erfinder verschiedener wichtiger Entwicklungen im Bereich maschinelles Lernen und Computer Vision.  Er arbeitete an Robotern, die Lippen lesen, der parallelen Programmiersprache StarLisp und geometrischen Lernalgorithmen.  Derzeit leitet er <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Self-Aware Systems</a> , "ein Team von Wissenschaftlern, die sich dafür einsetzen, dass intelligente Technologien der Menschheit zugute kommen".  Seine Arbeit „Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundlagen der KI-Motivation</a> “ trug dazu bei, die Domäne der Maschinenethik zu erweitern, da er feststellte, dass superintelligente Systeme auf potenziell gefährliche Ziele ausgerichtet sein werden.  Er schreibt: <br><blockquote>  Wir haben gezeigt, dass alle fortgeschrittenen KI-Systeme wahrscheinlich eine Reihe von Kernmotivationen haben.  Es ist unerlässlich, diese Motivationen zu verstehen, um Technologien zu schaffen, die eine positive Zukunft für die Menschheit gewährleisten.  Yudkovsky forderte eine "freundliche KI".  Dazu müssen wir einen wissenschaftlichen Ansatz für die „utilitaristische Entwicklung“ entwickeln, der es uns ermöglicht, sozial nützliche Funktionen zu entwickeln, die zu den gewünschten Sequenzen führen.  Die raschen Fortschritte im technologischen Fortschritt lassen darauf schließen, dass diese Probleme bald kritisch werden könnten. </blockquote><br>  Seine Artikel zum Thema "Rationale KI für das Gemeinwohl" finden Sie unter dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Murray Shanahan</a> hat einen Doktortitel in Informatik von Cambridge und ist jetzt Professor für kognitive Robotik am Imperial College London.  Er veröffentlichte Arbeiten in Bereichen wie Robotik, Logik, dynamische Systeme, Computational Neurobiology und Philosophie des Geistes.  Derzeit arbeitet er an dem Buch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Technological Singularity</a> , das im August veröffentlicht wird.  Die Werbeanmerkung von Amazon lautet wie folgt: <br><blockquote>  Shanahan beschreibt technologische Fortschritte in der KI, die beide unter dem Einfluss von Wissen aus der Biologie gemacht und von Grund auf neu entwickelt wurden.  Er erklärt, dass der Übergang zur superintelligenten KI sehr schnell vonstatten gehen wird, wenn die KI auf menschlicher Ebene geschaffen wird - eine theoretisch mögliche, aber schwierige Aufgabe.  Der Shanahan reflektiert, wozu die Existenz superintelligenter Maschinen in Bereichen wie Persönlichkeit, Verantwortung, Rechte und Individualität führen kann.  Einige Vertreter der superintelligenten KI können zum Wohle des Menschen geschaffen werden, andere können außer Kontrolle geraten.  (Das heißt, Siri oder HAL?) Die Singularität stellt für die Menschheit sowohl eine existenzielle Bedrohung als auch eine existenzielle Gelegenheit dar, ihre Grenzen zu überwinden.  Shanahan macht deutlich, dass wir uns beide Möglichkeiten vorstellen müssen, wenn wir ein besseres Ergebnis erzielen wollen. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Marcus Hutter</a> ist Professor für Informatikforschung an der National Australia University.  Zuvor arbeitete er am nach ihm benannten Institut für KI  Dale Moul in der Schweiz und am Nationalen Institut für Informatik und Kommunikation in Australien arbeitete außerdem an stimuliertem Lernen, Bayes'schen Befunden, Theorie der rechnerischen Komplexität, Solomons Theorie induktiver Vorhersagen, Computer Vision und genetischen Profilen.  Er schrieb auch viel über Singularität.  In dem Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kann Intelligenz explodieren?</a> " Schreibt er: <br><blockquote>  In diesem Jahrhundert kann es zu einer technologischen Explosion kommen, deren Ausmaß den Namen Singularität verdient.  Das Standardszenario ist eine Community interagierender intelligenter Personen in der virtuellen Welt, die auf Computern mit hyperbolisch wachsenden Computerressourcen simuliert wird.  Dies geht unweigerlich mit einer Explosion der Geschwindigkeit einher, gemessen an der physischen Zeit, aber nicht unbedingt mit einer Explosion der Intelligenz.  Wenn die virtuelle Welt von freien und interagierenden Individuen bevölkert wird, führt der evolutionäre Druck zur Entstehung von Individuen mit zunehmender Intelligenz, die um Computerressourcen konkurrieren.  Der Endpunkt dieser evolutionären Beschleunigung der Intelligenz kann die Gemeinschaft der intelligentesten Individuen sein.              .     ,        ,    , , ,     ,      . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jürgen Schmidhuber</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist Professor für KI an der Universität Lugano und ehemaliger Professor für kognitive Robotik an der Technischen Universität München. Er entwickelt einige der fortschrittlichsten neuronalen Netze der Welt, beschäftigt sich mit evolutionärer Robotik und der Theorie der rechnerischen Komplexität und ist wissenschaftlicher Mitarbeiter an der Europäischen Akademie der Wissenschaften und Künste. In seinem Buch " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hypotheses of Singularities</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " argumentiert er, dass "wir mit der Fortsetzung bestehender Trends in den nächsten Jahrzehnten einer intellektuellen Explosion gegenüberstehen werden". Als er direkt beim Reddit AMA nach den mit KI verbundenen Risiken gefragt wurde, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">antwortete er</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">finde eine Nische zum Überleben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . "</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist Professor und Mitglied des iCORE-Komitees an der University of Alberta. </font><font style="vertical-align: inherit;">Er ist wissenschaftlicher Mitarbeiter bei der Association for the Development of AI, Mitautor des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">beliebtesten Lehrbuchs über stimuliertes Lernen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dem Pionier der Methode der Zeitunterschiede, einem der wichtigsten auf diesem Gebiet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In seinem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bericht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf einer vom Institut für die Zukunft des Lebens organisierten KI-Konferenz argumentierte Saton, dass „es eine echte Chance gibt, dass auch mit unserem Leben“ eine KI geschaffen wird, die intellektuell mit Menschen vergleichbar ist, und fügte hinzu, dass diese KI „uns nicht gehorchen wird“, „wird um mit uns zu konkurrieren und zusammenzuarbeiten “und dass„ wenn wir super kluge Sklaven schaffen, bekommen wir super kluge Gegner. “ Abschließend sagte er, dass "wir Mechanismen (sozial, rechtlich, politisch, kulturell) durchdenken müssen, um das gewünschte Ergebnis zu erzielen", aber dass "unweigerlich normale Menschen an Bedeutung verlieren werden". Er erwähnte auch ähnliche Themen bei der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Präsentation des</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gadsby Institute. Auch in dem Buch von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt solche Zeilen: "Richard Saton, einer der größten Spezialisten für KI, sagt eine Explosion der Intelligenz irgendwo in der Mitte des Jahrhunderts voraus." </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist Professor für Bildverarbeitung am Imperial College London, führend in Robot Vision Groups und im Dyson Robotics Laboratory und Erfinder des computergestützten Lokalisierungs- und Markup-Systems MonoSLAM. Auf seiner Website </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">schreibt er</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         « » (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    « »    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> müssen nicht vorgestellt werden. Turing erfand die mathematischen Grundlagen der Computerwissenschaft und benannte nach ihm eine Turing-Maschine, Turing-Vollständigkeit und Turing-Test. Goode arbeitete mit Turing in Bletchley Park zusammen, half bei der Entwicklung eines der ersten Computer und erfand viele bekannte Algorithmen, zum Beispiel den schnellen diskreten Fourier-Transformationsalgorithmus, der als FFT-Algorithmus bekannt ist. Können digitale Autos in ihrer Arbeit denken? Turing schreibt:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nehmen wir an, dass solche Maschinen erstellt werden können, und betrachten Sie die Konsequenzen ihrer Erstellung. </font><font style="vertical-align: inherit;">Eine solche Tat wird zweifellos auf Feindseligkeit stoßen, es sei denn, wir haben seit Galileis Zeiten religiöse Toleranz erreicht. </font><font style="vertical-align: inherit;">Die Opposition wird aus Intellektuellen bestehen, die Angst haben, ihren Arbeitsplatz zu verlieren. </font><font style="vertical-align: inherit;">Aber es ist wahrscheinlich, dass sich Intellektuelle irren werden. </font><font style="vertical-align: inherit;">Es wird möglich sein, viele Dinge zu tun, um ihren Intellekt auf dem Niveau der von Maschinen festgelegten Standards zu halten, da es nach dem Start der Maschinenmethode nicht viel Zeit dauert, bis die Maschinen unsere unbedeutenden Fähigkeiten übertreffen. </font><font style="vertical-align: inherit;">Irgendwann sollten wir damit rechnen, dass die Maschinen die Kontrolle übernehmen.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Während seiner Arbeit im Atlas Computer Lab in den 60er Jahren entwickelte Goode diese Idee in „ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reasoning for the First Ultra-Intelligent Machine</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> “:</font></font><br><blockquote>   ,  ,       .     –     ,       .  ,   ,   « »,      . ,    –   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es stört mich, dass diese Liste den Eindruck eines gewissen Streits zwischen „Gläubigen“ und „Skeptikern“ in diesem Bereich erweckt, bei dem sie sich gegenseitig in Stücke schlagen. Aber das habe ich nicht gedacht. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn ich Artikel über Skeptiker lese, stoße ich immer auf zwei Argumente. Erstens sind wir immer noch sehr weit von der KI der menschlichen Ebene entfernt, ganz zu schweigen von der Superintelligenz, und es gibt keinen offensichtlichen Weg, solche Höhen zu erreichen. Zweitens, wenn Sie Verbote für KI-Forschung fordern, sind Sie ein Idiot. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich stimme beiden Punkten voll und ganz zu. Wie die Führer der KI-Risikobewegung. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Umfrage unter KI-Forschern ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) zeigten, dass sie im Durchschnitt 50% für die Tatsache geben, dass KI auf menschlicher Ebene bis 2040 Ode erscheinen wird, und 90% - dass sie bis 2075 erscheinen wird. Im Durchschnitt glauben 75% von ihnen, dass Superintelligenz („Maschinenintelligenz, die Fähigkeiten ernsthaft übertrifft) von jeder Person in den meisten Berufen “) wird innerhalb von 30 Jahren nach dem Aufkommen der KI auf menschlicher Ebene erscheinen. Und obwohl die Technik dieser Umfrage einige Zweifel aufwirft, stellt sich heraus, dass die meisten KI-Forscher der Meinung sind, dass in ein oder zwei Generationen etwas auftauchen wird, über das man sich Sorgen machen sollte, wenn man die Ergebnisse akzeptiert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Direktor des Instituts für Maschinenintelligenz, Luke Muelhauser, und der Direktor des Instituts für die Zukunft der Menschheit, Nick Bostrom, gaben jedoch an, dass ihre Vorhersagen für die Entwicklung der KI viel später sind als die Vorhersagen der an der Umfrage teilnehmenden Wissenschaftler. Wenn du studierst</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten zu den Vorhersagen der KI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von Stuart Armstrong zeigen, dass die Schätzungen der KI-Unterstützer zum Zeitpunkt des Auftretens der KI im Allgemeinen nicht von den Schätzungen der KI-Skeptiker abweichen. Darüber hinaus gehört die langfristige Vorhersage in dieser Tabelle Armstrong selbst. Derzeit arbeitet Armstrong jedoch am Institut für die Zukunft der Menschheit und macht </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">auf die Risiken der KI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und die Notwendigkeit aufmerksam, die Ziele der Superintelligenz zu erforschen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Unterschied zwischen Unterstützern und Skeptikern besteht nicht in ihrer Einschätzung, wann das Auftreten von KI auf menschlicher Ebene zu erwarten ist, sondern darin, wann wir uns darauf vorbereiten müssen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das bringt uns zum zweiten Punkt. Die Position der Skeptiker scheint zu sein, dass wir, obwohl wir wahrscheinlich ein paar kluge Leute schicken sollten, um an einer vorläufigen Bewertung des Problems zu arbeiten, absolut keinen Grund haben, in Panik zu geraten oder das Studium der KI zu verbieten. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KI-Fans bestehen darauf, dass wir zwar keine Panik brauchen oder KI-Forschung verbieten müssen, es sich aber wahrscheinlich lohnt, ein paar kluge Leute zu schicken, um an einer vorläufigen Bewertung des Problems zu arbeiten. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun ist wohl der leidenschaftlichste Skeptiker der KI-Risiken. Er wurde in einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel über Populärwissenschaft</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , in einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beitrag über die </font></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Randrevolution,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reichlich zitiert </font><font style="vertical-align: inherit;">und sprach auch mit </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">KDNuggets</font></a><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zu den "unvermeidlichen Fragen der Singularität", die er selbst als "so weit weg, dass Science-Fiction darüber geschrieben werden kann" beschreibt. </font><font style="vertical-align: inherit;">Als er jedoch gebeten wurde, seine Position zu klären, erklärte er:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk ist sehr besorgt über existenzielle Bedrohungen für die Menschheit (weshalb er Raketen baut, um Menschen zur Kolonisierung anderer Planeten zu schicken). </font><font style="vertical-align: inherit;">Und obwohl das Risiko einer KI-Rebellion sehr gering und sehr weit von der Zukunft entfernt ist, müssen wir darüber nachdenken und Vorsichtsmaßnahmen und Regeln entwickeln. </font><font style="vertical-align: inherit;">So wie das Bioethik-Komitee in den 1970er und 1980er Jahren vor dem weit verbreiteten Einsatz von Genetik erschien, brauchen wir KI-Ethik-Komitees. </font><font style="vertical-align: inherit;">Aber wie Yoshua Benjio schrieb, haben wir noch viel Zeit.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz ist ein weiterer Experte, der oft als das wichtigste Sprachrohr für Skepsis und Einschränkungen bezeichnet wird. </font><font style="vertical-align: inherit;">Sein Standpunkt wurde in Artikeln wie "Der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Direktor von Microsoft Research glaubt, dass außer Kontrolle geratene KI uns nicht töten wird</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " und " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz von Microsoft glaubt, dass KI keine Angst haben sollte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " beschrieben. </font><font style="vertical-align: inherit;">Aber hier ist, was er in einem längeren Interview mit NPR sagte:</font></font><br><blockquote> Keist: Horvitz bezweifelt, dass virtuelle Sekretärinnen jemals zu etwas werden, das die Welt erobern wird.  Er sagt, dass dies zu erwarten ist, dass sich ein Drachen zu einer Boeing 747 entwickelt. Bedeutet das, dass er sich über eine Singularität lustig macht? <br><br>  Horvitz: Nein.  Ich denke, dass es eine Mischung von Konzepten gab, und ich selbst habe auch gemischte Gefühle. <br><br>  Keist: Insbesondere aufgrund von Ideen wie Singularität versuchen Horvits und andere KI-Experten zunehmend, sich mit ethischen Fragen zu befassen, die in den kommenden Jahren bei eng gezielter KI auftreten können.  Sie stellen auch futuristischere Fragen.  Wie kann ich beispielsweise einen Notabschaltknopf für einen Computer erstellen, der sich selbst ändern kann? <br><br>  Horvits: Ich glaube wirklich, dass der Einsatz hoch genug ist, um Zeit und Energie aktiv nach Lösungen zu suchen, auch wenn die Wahrscheinlichkeit solcher Ereignisse gering ist. </blockquote><br>  Dies stimmt im Allgemeinen mit der Position vieler der leidenschaftlichsten KI-Risikorührer überein.  Mit solchen Freunden werden keine Feinde benötigt. <br><br>  Der Slate-Artikel „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hab keine Angst vor KI</a> “ bringt die Dinge überraschenderweise auch ins richtige Licht: <br><blockquote>  Wie Musk selbst feststellt, liegt die Lösung für das Risiko von KI in der nüchternen und rationalen Zusammenarbeit von Wissenschaftlern und Gesetzgebern.  Es ist jedoch ziemlich schwer zu verstehen, wie das Sprechen über "Dämonen" dazu beitragen kann, dieses edle Ziel zu erreichen.  Sie kann sie sogar behindern. <br><br>  Erstens gibt es große Lücken in der Idee des Skynet-Skripts.  Obwohl Forscher auf dem Gebiet der Informatik glauben, dass die Argumentation von Mask "nicht völlig verrückt" ist, sind sie immer noch zu weit von einer Welt entfernt, in der der Hype um KI eine etwas weniger KI-Realität verschleiert, mit der unsere Informatiker konfrontiert sind. <br><br>  Ian Lekun, der Leiter des KI-Labors bei Facebook, fasste diese Idee 2013 in einem Beitrag auf Google+ zusammen: Der Hype schadet der KI.  In den letzten fünf Jahrzehnten hat der Hype die KI viermal getötet.  Sie muss gestoppt werden. "Lekun und andere haben zu Recht Angst vor Hype. Die Nichterfüllung der hohen Erwartungen, die an Science-Fiction gestellt werden, führt zu ernsthaften Budgetkürzungen für die KI-Forschung. </blockquote><br>  Wissenschaftler, die an KI arbeiten, sind kluge Leute.  Sie sind nicht daran interessiert, in klassische politische Fallen zu geraten, in denen sie in Lager aufgeteilt werden und sich gegenseitig Panik oder Straußismus vorwerfen.  Anscheinend versuchen sie, ein Gleichgewicht zwischen der Notwendigkeit, Vorarbeiten im Zusammenhang mit der Bedrohung in der Ferne zu beginnen, und dem Risiko, ein so starkes Gefühl zu verursachen, das sie treffen wird, zu finden. <br><br>  Ich möchte nicht sagen, dass es keine Meinungsverschiedenheiten darüber gibt, wann Sie dieses Problem angehen müssen.  Grundsätzlich kommt es darauf an, ob man sagen kann, dass „wir das Problem lösen werden, wenn wir darauf stoßen“ oder einen solchen unerwarteten Start erwarten, aufgrund dessen alles außer Kontrolle gerät und auf den wir uns daher vorbereiten müssen im Voraus.  Ich sehe weniger Beweise als ich möchte, dass die Mehrheit der KI-Forscher mit ihrer eigenen Meinung die zweite Möglichkeit versteht.  Was kann ich sagen, auch wenn in einem Artikel über Marginal Revolution ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experte zitiert wird, der</a> sagt, dass Superintelligenz keine große Bedrohung darstellt, weil „intelligente Computer sich keine Ziele setzen können“, obwohl jeder, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bostrom liest, das</a> weiß Das ganze Problem ist. <br><br>  Es gibt noch viel zu tun.  Aber nur um nicht speziell Artikel auszuwählen, in denen "echte KI-Experten sich keine Sorgen um Superintelligenz machen". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de402379/">https://habr.com/ru/post/de402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de402367/index.html">Wie man aufhört, für Roaming zu bezahlen, oder mit einer Nummer auf der ganzen Welt</a></li>
<li><a href="../de402369/index.html">So messen Sie die Geschwindigkeit eines 3D-Druckers - sein heißes Ende. Und nicht nur Geschwindigkeit</a></li>
<li><a href="../de402373/index.html">Was gibt die "Genetik der Mikrobiota"</a></li>
<li><a href="../de402375/index.html">8-Kilowatt-4-Kanal-Wechselstromschalter mit Verbrauchsmessung. Teil 1</a></li>
<li><a href="../de402377/index.html">Was halten Ihre Smartphones vom USB-Laden im Auto?</a></li>
<li><a href="../de402381/index.html">Wie man Astronauten rekrutiert</a></li>
<li><a href="../de402383/index.html">Saw, Shura: Wie wir die mobile Mishiko Dog Tracker App entwickelt haben</a></li>
<li><a href="../de402385/index.html">Warum sollten Sie einen Boom im Bereich der Herstellung von Robotern für Geschäftsräume erwarten?</a></li>
<li><a href="../de402387/index.html">3D-Stift für 3D-Drucker</a></li>
<li><a href="../de402389/index.html">MPAA und RIAA planen, Daten von ausgefallenen Festplatten der Dateifreigabe Megaupload wiederherzustellen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>