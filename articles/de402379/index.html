<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò¢ üöñ üì£ Was KI-Forscher √ºber die damit verbundenen m√∂glichen Risiken denken üññüèΩ üí§ üë©üèæ‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ich habe mich bereits 2007 f√ºr die mit KI verbundenen Risiken interessiert. Zu dieser Zeit war die Reaktion der meisten Menschen auf dieses Thema unge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Was KI-Forscher √ºber die damit verbundenen m√∂glichen Risiken denken</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/">  Ich habe mich bereits 2007 f√ºr die mit KI verbundenen Risiken interessiert.  Zu dieser Zeit war die Reaktion der meisten Menschen auf dieses Thema ungef√§hr so: "Es ist sehr lustig, zur√ºck zu kommen, wenn jemand anderes als Internet-Idioten daran glauben wird." <br><br>  In den folgenden Jahren teilten mehrere √§u√üerst kluge und einflussreiche Pers√∂nlichkeiten, darunter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stephen Hawking</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Elon Musk</a> , √∂ffentlich ihre Besorgnis √ºber die Risiken der KI, gefolgt von Hunderten anderer Intellektueller, von Oxford-Philosophen bis zu Kosmologen vom MIT und Investoren aus dem Silicon Valley .  Und wir sind zur√ºck. <br><br>  Dann √§nderte sich die Reaktion zu: "Nun, einige Wissenschaftler und Gesch√§ftsleute k√∂nnen das glauben, aber es ist unwahrscheinlich, dass sie echte Experten auf diesem Gebiet sind, die sich in der Situation wirklich auskennen." <br><br>  Von hier kamen Aussagen wie der popul√§rwissenschaftliche Artikel ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bill Gates hat Angst vor KI, aber KI-Forscher sollten es wissen</a> ‚Äú: <br><blockquote>  Nach einem Gespr√§ch mit KI-Forschern - echten Forschern, die solche Systeme kaum zum Laufen bringen, ganz zu schweigen von ihrer guten Funktionsweise - wird deutlich, dass sie keine Angst haben, dass sich Superintelligenz pl√∂tzlich an sie heranschleicht, weder jetzt noch in Zukunft .  Trotz all der be√§ngstigenden Geschichten, die Mask erz√§hlt, haben die Forscher es nicht eilig, Schutzr√§ume und Selbstzerst√∂rung mit einem Countdown zu bauen. </blockquote><a name="habracut"></a><br>  Oder, wie sie auf Fusion.net im Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einspruch gegen Killerroboter von einer Person, die tats√§chlich KI entwickelt</a> " geschrieben haben: <br><blockquote>  Andrew Angie entwickelt professionell KI-Systeme.  Er unterrichtete einen KI-Kurs in Stanford, entwickelte KI bei Google und wechselte dann zur chinesischen Baidu-Suchmaschine, um seine Arbeit an der Spitze der Anwendung von KI auf reale Probleme fortzusetzen.  Wenn er h√∂rt, wie Elon Musk oder Stephen Hawking - Leute, die mit moderner Technologie nicht direkt vertraut sind - √ºber KI sprechen, die m√∂glicherweise die Menschheit zerst√∂ren k√∂nnte, kann man fast h√∂ren, wie er sein Gesicht mit den H√§nden bedeckt. </blockquote><br>  Ramez Naam von Marginal Revolution wiederholt ungef√§hr dasselbe im Artikel ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Was denken Forscher √ºber die Risiken von KI?</a> ‚Äú: <br><blockquote>  Elon Musk, Stephen Hawking und Bill Gates haben k√ºrzlich Bedenken ge√§u√üert, dass die Entwicklung der KI das Szenario ‚ÄûKI-Killer‚Äú umsetzen und m√∂glicherweise zum Aussterben der Menschheit f√ºhren k√∂nnte.  Sie sind keine KI-Forscher, und soweit ich wei√ü, haben sie nicht direkt mit KI gearbeitet.  Was denken echte KI-Forscher √ºber die Risiken der KI? </blockquote><br>  Er zitiert die Worte speziell ausgew√§hlter KI-Forscher, wie die Autoren anderer Geschichten - und h√∂rt dann auf, ohne andere Meinungen zu erw√§hnen. <br><br>  Aber sie existieren.  KI-Forscher, einschlie√ülich f√ºhrender Fachleute, haben von Anfang an aktiv Bedenken hinsichtlich der Risiken von KI und dar√ºber hinaus ge√§u√üert.  Ich werde zun√§chst diese Personen trotz Naams Liste auflisten und dann weitermachen, warum ich dies nicht als "Diskussion" im klassischen Sinne betrachte, die von der Auflistung der Sterne erwartet wird. <br><br>  Die Kriterien meiner Liste lauten wie folgt: Ich erw√§hne nur die renommiertesten Forscher oder Professoren der Wissenschaft an guten Instituten mit vielen Zitaten wissenschaftlicher Arbeiten oder sehr angesehene Wissenschaftler aus der Industrie, die f√ºr gro√üe Unternehmen arbeiten und eine gute Erfolgsbilanz vorweisen k√∂nnen.  Sie besch√§ftigen sich mit KI und maschinellem Lernen.  Sie haben mehrere starke Aussagen zur Unterst√ºtzung eines bestimmten Standpunkts in Bezug auf das Einsetzen einer Singularit√§t oder eines ernsthaften Risikos durch die KI in naher Zukunft.  Einige von ihnen haben Werke oder B√ºcher zu diesem Thema geschrieben.  Andere dr√ºckten einfach ihre Gedanken aus und glaubten, dass dies ein wichtiges Thema ist, das es wert ist, studiert zu werden. <br><br>  Wenn jemand mit der Aufnahme einer Person in diese Liste nicht einverstanden ist oder glaubt, dass ich etwas Wichtiges vergessen habe, lassen Sie es mich wissen. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stuart Russell</a> ist Professor f√ºr Informatik in Berkeley, Gewinner des IJCAI Computers And Thought Award, Forscher bei der Computer Mechanization Association, Forscher an der American Academy of Advanced Scientific Research, Direktor des Zentrums f√ºr Intelligente Systeme, Gewinner des Blaise Pascal-Preises usw.  usw.  Co-Autor von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI: A Modern Approach</a> , einem klassischen Lehrbuch, das an 1.200 Universit√§ten weltweit verwendet wird.  Auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">seiner Website</a> schreibt er: <br><blockquote>  Auf dem Gebiet der KI wurden 50 Jahre lang unter der Annahme gearbeitet, dass je kl√ºger desto besser.  Die Sorge um das Wohl der Menschheit muss damit verbunden sein.  Das Argument ist einfach: <br><br>  1. Die KI wird wahrscheinlich erfolgreich erstellt. <br>  2. Unbegrenzter Erfolg f√ºhrt zu gro√üen Risiken und gro√üen Vorteilen. <br>  3. Was k√∂nnen wir tun, um die Chancen zu erh√∂hen, Vorteile zu erzielen und Risiken zu vermeiden? <br><br>  Einige Organisationen arbeiten bereits an diesen Themen, darunter das Institut f√ºr die Zukunft der Menschheit in Oxford, das Zentrum f√ºr existenzielle Risikostudien in Cambridge (CSER), das Institut f√ºr das Studium der maschinellen Intelligenz in Berkeley und das Institut f√ºr das zuk√ºnftige Leben in Harvard / MIT (FLI).  Ich bin in Beir√§ten mit CSER und FLI. <br><br>  So wie Forscher der Kernfusion das Problem der Begrenzung von Kernreaktionen als eines der wichtigsten Probleme auf ihrem Gebiet betrachteten, wird die Entwicklung des Gebiets der KI unweigerlich Kontroll- und Sicherheitsprobleme aufwerfen.  Die Forscher beginnen bereits, Fragen zu stellen, von rein technischen (die Hauptprobleme der Rationalit√§t und N√ºtzlichkeit usw.) bis zu allgemein philosophischen. </blockquote><br>  Auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">edge.org</a> beschreibt er einen √§hnlichen Standpunkt: <br><blockquote>  Wie von Steve Omohandro, Nick Bostrom und anderen erkl√§rt, kann eine Diskrepanz in den Werten mit Entscheidungssystemen, deren M√∂glichkeiten st√§ndig zunehmen, zu Problemen f√ºhren - vielleicht sogar zu Problemen des Ausma√ües des Aussterbens, wenn die Maschinen leistungsf√§higer sind als Menschen.  Einige glauben, dass es in den kommenden Jahrhunderten keine vorhersehbaren Risiken f√ºr die Menschheit gibt, und vergessen vielleicht, dass der Zeitunterschied zwischen Rutherfords zuversichtlicher Aussage, dass Atomenergie niemals extrahiert werden kann, und weniger als 24 Stunden durch die Erfindung der Neutronen-initiierten Kernkettenreaktion vergangen ist . </blockquote><br>  Er versuchte auch, ein Vertreter dieser Ideen in der akademischen Gemeinschaft zu werden, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">und gab an</a> : <br><blockquote>  Ich finde, dass die wichtigsten Leute in dieser Branche, die noch nie zuvor √Ñngste ge√§u√üert haben, sich denken, dass dieses Problem sehr ernst genommen werden muss, und je fr√ºher wir es ernst nehmen, desto besser. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David McAllister</a> ist Professor und Senior Fellow am Toyota Institute of Technology der University of Chicago, der zuvor an den Fakult√§ten des MIT und des Cornell Institute gearbeitet hat.  Er ist Mitglied der American AI Association, hat mehr als hundert Werke ver√∂ffentlicht, Forschungen in den Bereichen maschinelles Lernen, Programmiertheorie, automatische Entscheidungsfindung, KI-Planung und Computerlinguistik durchgef√ºhrt und die Algorithmen des ber√ºhmten Deep Blue-Schachcomputers ma√ügeblich beeinflusst.  Laut einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> in der Pittsburgh Tribune Review: <br><blockquote>  Der Chicagoer Professor David McAllister h√§lt es f√ºr unvermeidlich, dass vollautomatische intelligente Maschinen in der Lage sind, intelligentere Versionen von sich selbst zu entwerfen und zu erstellen, dh den Beginn eines Ereignisses, das als [technologische] Singularit√§t bekannt ist.  Die Singularit√§t wird es Maschinen erm√∂glichen, unendlich intelligent zu werden, was zu einem ‚Äûunglaublich gef√§hrlichen Szenario‚Äú f√ºhrt, sagt er. </blockquote><br>  In seinem Blog <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Thoughts on Cars</a> schreibt er: <br><blockquote>  Die meisten Informatiker weigern sich, √ºber echte Erfolge in der KI zu sprechen.  Ich denke, es w√§re vern√ºnftiger zu sagen, dass niemand vorhersagen kann, wann eine mit dem menschlichen Verstand vergleichbare KI empfangen wird.  John MacArthy hat mir einmal erz√§hlt, dass er, wenn er gefragt wird, wann die KI auf menschlicher Ebene erstellt wird, antwortet, dass sie f√ºnf bis f√ºnfhundert Jahre alt ist.  MacArthy war schlau.  Angesichts der Unsicherheiten in diesem Bereich ist es vern√ºnftig, das Problem der freundlichen KI in Betracht zu ziehen ... <br><br>  In den fr√ºhen Stadien wird die allgemeine KI sicher sein.  Die fr√ºhen Stadien von OII werden jedoch ein ausgezeichneter Testort f√ºr KI als Diener oder andere freundliche KI-Optionen sein.  Ben Goertzel wirbt auch in einem guten Beitrag in seinem Blog f√ºr einen experimentellen Ansatz.  Wenn uns die √Ñra sicherer und nicht zu intelligenter OIIs erwartet, haben wir Zeit, √ºber gef√§hrlichere Zeiten nachzudenken. </blockquote><br>  Er war Mitglied der Expertengruppe des AAAI-Gremiums f√ºr langfristige KI-Futures, die sich den langfristigen Perspektiven der KI widmete, Vorsitzender des Ausschusses f√ºr langfristige Kontrolle und wird <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wie folgt beschrieben</a> : <br><blockquote>  Makalister sprach mit mir √ºber den Ansatz der "Singularit√§t", ein Ereignis, bei dem Computer schlauer werden als Menschen.  Er nannte nicht das genaue Datum seines Auftretens, sagte aber, dass dies in den n√§chsten paar Jahrzehnten passieren k√∂nnte und am Ende definitiv passieren w√ºrde.  Hier sind seine Ansichten zur Singularit√§t.  Zwei wichtige Ereignisse werden eintreten: operative Intelligenz, bei der wir problemlos mit Computern sprechen k√∂nnen, und eine Kettenreaktion der KI, bei der sich der Computer ohne Hilfe verbessern und dann erneut wiederholen kann.  Das erste Ereignis werden wir in automatischen Assistenzsystemen bemerken, die uns wirklich helfen werden.  Sp√§ter wird es wirklich interessant, mit Computern zu kommunizieren.  Und damit Computer alles k√∂nnen, was Menschen k√∂nnen, m√ºssen Sie auf das Eintreten des zweiten Ereignisses warten. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hans Moravek</a> ist ehemaliger Professor am Institut f√ºr Robotik der Carnegie Mellon University und nach ihm das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Paradoxon von Moravec</a> , dem Gr√ºnder der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SeeGrid Corporation</a> , benannt, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sich</a> auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bildverarbeitungssysteme</a> f√ºr industrielle Anwendungen spezialisiert hat.  Seine Arbeit ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Synthese von Sensoren in den Sicherheitsgittern mobiler Roboter</a> ‚Äú wurde mehr als tausend Mal zitiert, und er wurde eingeladen, einen Artikel f√ºr die British Encyclopedia of Robotics zu schreiben, zu einer Zeit, als Artikel in Enzyklop√§dien von Experten auf diesem Gebiet und nicht von Hunderten anonymer Internetkommentatoren verfasst wurden. <br><br>  Er ist auch der Autor von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Robot: Von einer einfachen Maschine zu einem transzendentalen Geist</a> , den Amazon wie folgt beschreibt: <br><blockquote>  In diesem spannenden Buch sagt Hans Moravek voraus, dass sich Maschinen bis 2040 dem intellektuellen Niveau der Menschen n√§hern und uns bis 2050 √ºbertreffen werden.  Aber w√§hrend Moravec das Ende einer √Ñra menschlicher Dominanz vorhersagt, ist seine Vision dieses Ereignisses nicht so trostlos.  Er ist nicht von einer Zukunft eingez√§unt, in der Maschinen die Welt regieren, sondern akzeptiert und beschreibt eine erstaunliche Sichtweise, nach der intelligente Roboter unsere evolution√§ren Nachkommen werden.  Moravec glaubt, dass sich am Ende dieses Prozesses "der riesige Cyberspace mit dem unmenschlichen Supermind vereinen und sich mit Angelegenheiten befassen wird, die weit entfernt von Menschen und weit entfernt von menschlichen Angelegenheiten von Bakterien sind". </blockquote><br>  Shane Leg ist Mitbegr√ºnder von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind Technologies</a> , einem KI-Startup, das 2014 von Google f√ºr 500 Millionen US-Dollar gekauft wurde.  Er promovierte am nach ihm benannten Institut f√ºr KI  Dale Moul in der Schweiz und arbeitete auch in der Abteilung f√ºr Computational Neurobiology.  Gatsby in London.  Am Ende seiner Dissertation ‚ÄûMaschinen-Superintelligenz‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schreibt er</a> : <br><blockquote>  Wenn jemals etwas auftaucht, das sich der absoluten Leistung ann√§hern kann, wird es eine superintelligente Maschine sein.  Per Definition wird sie in der Lage sein, eine Vielzahl von Zielen in einer Vielzahl von Umgebungen zu erreichen.  Wenn wir uns im Voraus sorgf√§ltig auf eine solche Gelegenheit vorbereiten, k√∂nnen wir nicht nur die Katastrophe vermeiden, sondern auch eine √Ñra des Wohlstands beginnen, anders als alles, was zuvor existierte. </blockquote><br>  In einem anschlie√üenden Interview <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sagt er</a> : <br><blockquote>  AI ist jetzt dort, wo das Internet 1988 war.  In speziellen Anwendungen (Suchmaschinen wie Google, Hedge Funds und Bioinformatik) sind Anforderungen an maschinelles Lernen erforderlich, und ihre Zahl w√§chst von Jahr zu Jahr.  Ich denke, dass dieser Prozess Mitte des n√§chsten Jahrzehnts massiv und sp√ºrbar werden wird.  Der KI-Boom wird voraussichtlich um 2020 stattfinden, gefolgt von einem Jahrzehnt rascher Fortschritte, m√∂glicherweise nach Marktkorrekturen.  KI auf menschlicher Ebene wird um Mitte 2020 geschaffen, obwohl viele Menschen den Beginn dieses Ereignisses nicht akzeptieren werden.  Danach werden die mit fortgeschrittener KI verbundenen Risiken in die Praxis umgesetzt.  Ich werde nicht √ºber die "Singularit√§t" sagen, aber sie erwarten, dass irgendwann nach der Schaffung der OII verr√ºckte Dinge passieren werden.  Es ist irgendwo zwischen 2025 und 2040. </blockquote><br>  Er und seine Mitbegr√ºnder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Demis Khasabis</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mustafa Suleiman</a> unterzeichneten eine Petition f√ºr das Institut f√ºr zuk√ºnftiges Leben in Bezug auf KI-Risiken. Eine ihrer Bedingungen f√ºr den Beitritt zu Google war, dass das Unternehmen sich bereit erkl√§rt, einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">KI-Ethikrat</a> zu organisieren, um diese Probleme zu untersuchen. <br><br>  Steve Omohundro ist ehemaliger Professor f√ºr Informatik an der Universit√§t von Illinois, Gr√ºnder der Computer Vision- und Trainingsgruppe am Zentrum f√ºr das Studium komplexer Systeme und Erfinder verschiedener wichtiger Entwicklungen im Bereich maschinelles Lernen und Computer Vision.  Er arbeitete an Robotern, die Lippen lesen, der parallelen Programmiersprache StarLisp und geometrischen Lernalgorithmen.  Derzeit leitet er <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Self-Aware Systems</a> , "ein Team von Wissenschaftlern, die sich daf√ºr einsetzen, dass intelligente Technologien der Menschheit zugute kommen".  Seine Arbeit ‚ÄûDie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundlagen der KI-Motivation</a> ‚Äú trug dazu bei, die Dom√§ne der Maschinenethik zu erweitern, da er feststellte, dass superintelligente Systeme auf potenziell gef√§hrliche Ziele ausgerichtet sein werden.  Er schreibt: <br><blockquote>  Wir haben gezeigt, dass alle fortgeschrittenen KI-Systeme wahrscheinlich eine Reihe von Kernmotivationen haben.  Es ist unerl√§sslich, diese Motivationen zu verstehen, um Technologien zu schaffen, die eine positive Zukunft f√ºr die Menschheit gew√§hrleisten.  Yudkovsky forderte eine "freundliche KI".  Dazu m√ºssen wir einen wissenschaftlichen Ansatz f√ºr die ‚Äûutilitaristische Entwicklung‚Äú entwickeln, der es uns erm√∂glicht, sozial n√ºtzliche Funktionen zu entwickeln, die zu den gew√ºnschten Sequenzen f√ºhren.  Die raschen Fortschritte im technologischen Fortschritt lassen darauf schlie√üen, dass diese Probleme bald kritisch werden k√∂nnten. </blockquote><br>  Seine Artikel zum Thema "Rationale KI f√ºr das Gemeinwohl" finden Sie unter dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Murray Shanahan</a> hat einen Doktortitel in Informatik von Cambridge und ist jetzt Professor f√ºr kognitive Robotik am Imperial College London.  Er ver√∂ffentlichte Arbeiten in Bereichen wie Robotik, Logik, dynamische Systeme, Computational Neurobiology und Philosophie des Geistes.  Derzeit arbeitet er an dem Buch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Technological Singularity</a> , das im August ver√∂ffentlicht wird.  Die Werbeanmerkung von Amazon lautet wie folgt: <br><blockquote>  Shanahan beschreibt technologische Fortschritte in der KI, die beide unter dem Einfluss von Wissen aus der Biologie gemacht und von Grund auf neu entwickelt wurden.  Er erkl√§rt, dass der √úbergang zur superintelligenten KI sehr schnell vonstatten gehen wird, wenn die KI auf menschlicher Ebene geschaffen wird - eine theoretisch m√∂gliche, aber schwierige Aufgabe.  Der Shanahan reflektiert, wozu die Existenz superintelligenter Maschinen in Bereichen wie Pers√∂nlichkeit, Verantwortung, Rechte und Individualit√§t f√ºhren kann.  Einige Vertreter der superintelligenten KI k√∂nnen zum Wohle des Menschen geschaffen werden, andere k√∂nnen au√üer Kontrolle geraten.  (Das hei√üt, Siri oder HAL?) Die Singularit√§t stellt f√ºr die Menschheit sowohl eine existenzielle Bedrohung als auch eine existenzielle Gelegenheit dar, ihre Grenzen zu √ºberwinden.  Shanahan macht deutlich, dass wir uns beide M√∂glichkeiten vorstellen m√ºssen, wenn wir ein besseres Ergebnis erzielen wollen. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Marcus Hutter</a> ist Professor f√ºr Informatikforschung an der National Australia University.  Zuvor arbeitete er am nach ihm benannten Institut f√ºr KI  Dale Moul in der Schweiz und am Nationalen Institut f√ºr Informatik und Kommunikation in Australien arbeitete au√üerdem an stimuliertem Lernen, Bayes'schen Befunden, Theorie der rechnerischen Komplexit√§t, Solomons Theorie induktiver Vorhersagen, Computer Vision und genetischen Profilen.  Er schrieb auch viel √ºber Singularit√§t.  In dem Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kann Intelligenz explodieren?</a> " Schreibt er: <br><blockquote>  In diesem Jahrhundert kann es zu einer technologischen Explosion kommen, deren Ausma√ü den Namen Singularit√§t verdient.  Das Standardszenario ist eine Community interagierender intelligenter Personen in der virtuellen Welt, die auf Computern mit hyperbolisch wachsenden Computerressourcen simuliert wird.  Dies geht unweigerlich mit einer Explosion der Geschwindigkeit einher, gemessen an der physischen Zeit, aber nicht unbedingt mit einer Explosion der Intelligenz.  Wenn die virtuelle Welt von freien und interagierenden Individuen bev√∂lkert wird, f√ºhrt der evolution√§re Druck zur Entstehung von Individuen mit zunehmender Intelligenz, die um Computerressourcen konkurrieren.  Der Endpunkt dieser evolution√§ren Beschleunigung der Intelligenz kann die Gemeinschaft der intelligentesten Individuen sein.              .     ,        ,    , , ,     ,      . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J√ºrgen Schmidhuber</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist Professor f√ºr KI an der Universit√§t Lugano und ehemaliger Professor f√ºr kognitive Robotik an der Technischen Universit√§t M√ºnchen. Er entwickelt einige der fortschrittlichsten neuronalen Netze der Welt, besch√§ftigt sich mit evolution√§rer Robotik und der Theorie der rechnerischen Komplexit√§t und ist wissenschaftlicher Mitarbeiter an der Europ√§ischen Akademie der Wissenschaften und K√ºnste. In seinem Buch " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hypotheses of Singularities</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " argumentiert er, dass "wir mit der Fortsetzung bestehender Trends in den n√§chsten Jahrzehnten einer intellektuellen Explosion gegen√ºberstehen werden". Als er direkt beim Reddit AMA nach den mit KI verbundenen Risiken gefragt wurde, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">antwortete er</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">finde eine Nische zum √úberleben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . "</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist Professor und Mitglied des iCORE-Komitees an der University of Alberta. </font><font style="vertical-align: inherit;">Er ist wissenschaftlicher Mitarbeiter bei der Association for the Development of AI, Mitautor des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">beliebtesten Lehrbuchs √ºber stimuliertes Lernen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dem Pionier der Methode der Zeitunterschiede, einem der wichtigsten auf diesem Gebiet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In seinem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bericht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf einer vom Institut f√ºr die Zukunft des Lebens organisierten KI-Konferenz argumentierte Saton, dass ‚Äûes eine echte Chance gibt, dass auch mit unserem Leben‚Äú eine KI geschaffen wird, die intellektuell mit Menschen vergleichbar ist, und f√ºgte hinzu, dass diese KI ‚Äûuns nicht gehorchen wird‚Äú, ‚Äûwird um mit uns zu konkurrieren und zusammenzuarbeiten ‚Äúund dass‚Äû wenn wir super kluge Sklaven schaffen, bekommen wir super kluge Gegner. ‚Äú Abschlie√üend sagte er, dass "wir Mechanismen (sozial, rechtlich, politisch, kulturell) durchdenken m√ºssen, um das gew√ºnschte Ergebnis zu erzielen", aber dass "unweigerlich normale Menschen an Bedeutung verlieren werden". Er erw√§hnte auch √§hnliche Themen bei der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pr√§sentation des</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gadsby Institute. Auch in dem Buch von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt solche Zeilen: "Richard Saton, einer der gr√∂√üten Spezialisten f√ºr KI, sagt eine Explosion der Intelligenz irgendwo in der Mitte des Jahrhunderts voraus." </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist Professor f√ºr Bildverarbeitung am Imperial College London, f√ºhrend in Robot Vision Groups und im Dyson Robotics Laboratory und Erfinder des computergest√ºtzten Lokalisierungs- und Markup-Systems MonoSLAM. Auf seiner Website </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">schreibt er</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         ¬´ ¬ª (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    ¬´ ¬ª    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> m√ºssen nicht vorgestellt werden. Turing erfand die mathematischen Grundlagen der Computerwissenschaft und benannte nach ihm eine Turing-Maschine, Turing-Vollst√§ndigkeit und Turing-Test. Goode arbeitete mit Turing in Bletchley Park zusammen, half bei der Entwicklung eines der ersten Computer und erfand viele bekannte Algorithmen, zum Beispiel den schnellen diskreten Fourier-Transformationsalgorithmus, der als FFT-Algorithmus bekannt ist. K√∂nnen digitale Autos in ihrer Arbeit denken? Turing schreibt:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nehmen wir an, dass solche Maschinen erstellt werden k√∂nnen, und betrachten Sie die Konsequenzen ihrer Erstellung. </font><font style="vertical-align: inherit;">Eine solche Tat wird zweifellos auf Feindseligkeit sto√üen, es sei denn, wir haben seit Galileis Zeiten religi√∂se Toleranz erreicht. </font><font style="vertical-align: inherit;">Die Opposition wird aus Intellektuellen bestehen, die Angst haben, ihren Arbeitsplatz zu verlieren. </font><font style="vertical-align: inherit;">Aber es ist wahrscheinlich, dass sich Intellektuelle irren werden. </font><font style="vertical-align: inherit;">Es wird m√∂glich sein, viele Dinge zu tun, um ihren Intellekt auf dem Niveau der von Maschinen festgelegten Standards zu halten, da es nach dem Start der Maschinenmethode nicht viel Zeit dauert, bis die Maschinen unsere unbedeutenden F√§higkeiten √ºbertreffen. </font><font style="vertical-align: inherit;">Irgendwann sollten wir damit rechnen, dass die Maschinen die Kontrolle √ºbernehmen.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">W√§hrend seiner Arbeit im Atlas Computer Lab in den 60er Jahren entwickelte Goode diese Idee in ‚Äû </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reasoning for the First Ultra-Intelligent Machine</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äú:</font></font><br><blockquote>   ,  ,       .     ‚Äì     ,       .  ,   ,   ¬´ ¬ª,      . ,    ‚Äì   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es st√∂rt mich, dass diese Liste den Eindruck eines gewissen Streits zwischen ‚ÄûGl√§ubigen‚Äú und ‚ÄûSkeptikern‚Äú in diesem Bereich erweckt, bei dem sie sich gegenseitig in St√ºcke schlagen. Aber das habe ich nicht gedacht. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn ich Artikel √ºber Skeptiker lese, sto√üe ich immer auf zwei Argumente. Erstens sind wir immer noch sehr weit von der KI der menschlichen Ebene entfernt, ganz zu schweigen von der Superintelligenz, und es gibt keinen offensichtlichen Weg, solche H√∂hen zu erreichen. Zweitens, wenn Sie Verbote f√ºr KI-Forschung fordern, sind Sie ein Idiot. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich stimme beiden Punkten voll und ganz zu. Wie die F√ºhrer der KI-Risikobewegung. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Umfrage unter KI-Forschern ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) zeigten, dass sie im Durchschnitt 50% f√ºr die Tatsache geben, dass KI auf menschlicher Ebene bis 2040 Ode erscheinen wird, und 90% - dass sie bis 2075 erscheinen wird. Im Durchschnitt glauben 75% von ihnen, dass Superintelligenz (‚ÄûMaschinenintelligenz, die F√§higkeiten ernsthaft √ºbertrifft) von jeder Person in den meisten Berufen ‚Äú) wird innerhalb von 30 Jahren nach dem Aufkommen der KI auf menschlicher Ebene erscheinen. Und obwohl die Technik dieser Umfrage einige Zweifel aufwirft, stellt sich heraus, dass die meisten KI-Forscher der Meinung sind, dass in ein oder zwei Generationen etwas auftauchen wird, √ºber das man sich Sorgen machen sollte, wenn man die Ergebnisse akzeptiert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Direktor des Instituts f√ºr Maschinenintelligenz, Luke Muelhauser, und der Direktor des Instituts f√ºr die Zukunft der Menschheit, Nick Bostrom, gaben jedoch an, dass ihre Vorhersagen f√ºr die Entwicklung der KI viel sp√§ter sind als die Vorhersagen der an der Umfrage teilnehmenden Wissenschaftler. Wenn du studierst</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten zu den Vorhersagen der KI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von Stuart Armstrong zeigen, dass die Sch√§tzungen der KI-Unterst√ºtzer zum Zeitpunkt des Auftretens der KI im Allgemeinen nicht von den Sch√§tzungen der KI-Skeptiker abweichen. Dar√ºber hinaus geh√∂rt die langfristige Vorhersage in dieser Tabelle Armstrong selbst. Derzeit arbeitet Armstrong jedoch am Institut f√ºr die Zukunft der Menschheit und macht </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">auf die Risiken der KI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und die Notwendigkeit aufmerksam, die Ziele der Superintelligenz zu erforschen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Unterschied zwischen Unterst√ºtzern und Skeptikern besteht nicht in ihrer Einsch√§tzung, wann das Auftreten von KI auf menschlicher Ebene zu erwarten ist, sondern darin, wann wir uns darauf vorbereiten m√ºssen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das bringt uns zum zweiten Punkt. Die Position der Skeptiker scheint zu sein, dass wir, obwohl wir wahrscheinlich ein paar kluge Leute schicken sollten, um an einer vorl√§ufigen Bewertung des Problems zu arbeiten, absolut keinen Grund haben, in Panik zu geraten oder das Studium der KI zu verbieten. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KI-Fans bestehen darauf, dass wir zwar keine Panik brauchen oder KI-Forschung verbieten m√ºssen, es sich aber wahrscheinlich lohnt, ein paar kluge Leute zu schicken, um an einer vorl√§ufigen Bewertung des Problems zu arbeiten. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun ist wohl der leidenschaftlichste Skeptiker der KI-Risiken. Er wurde in einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel √ºber Popul√§rwissenschaft</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , in einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beitrag √ºber die </font></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Randrevolution,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reichlich zitiert </font><font style="vertical-align: inherit;">und sprach auch mit </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">KDNuggets</font></a><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zu den "unvermeidlichen Fragen der Singularit√§t", die er selbst als "so weit weg, dass Science-Fiction dar√ºber geschrieben werden kann" beschreibt. </font><font style="vertical-align: inherit;">Als er jedoch gebeten wurde, seine Position zu kl√§ren, erkl√§rte er:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk ist sehr besorgt √ºber existenzielle Bedrohungen f√ºr die Menschheit (weshalb er Raketen baut, um Menschen zur Kolonisierung anderer Planeten zu schicken). </font><font style="vertical-align: inherit;">Und obwohl das Risiko einer KI-Rebellion sehr gering und sehr weit von der Zukunft entfernt ist, m√ºssen wir dar√ºber nachdenken und Vorsichtsma√ünahmen und Regeln entwickeln. </font><font style="vertical-align: inherit;">So wie das Bioethik-Komitee in den 1970er und 1980er Jahren vor dem weit verbreiteten Einsatz von Genetik erschien, brauchen wir KI-Ethik-Komitees. </font><font style="vertical-align: inherit;">Aber wie Yoshua Benjio schrieb, haben wir noch viel Zeit.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz ist ein weiterer Experte, der oft als das wichtigste Sprachrohr f√ºr Skepsis und Einschr√§nkungen bezeichnet wird. </font><font style="vertical-align: inherit;">Sein Standpunkt wurde in Artikeln wie "Der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Direktor von Microsoft Research glaubt, dass au√üer Kontrolle geratene KI uns nicht t√∂ten wird</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " und " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz von Microsoft glaubt, dass KI keine Angst haben sollte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " beschrieben. </font><font style="vertical-align: inherit;">Aber hier ist, was er in einem l√§ngeren Interview mit NPR sagte:</font></font><br><blockquote> Keist: Horvitz bezweifelt, dass virtuelle Sekret√§rinnen jemals zu etwas werden, das die Welt erobern wird.  Er sagt, dass dies zu erwarten ist, dass sich ein Drachen zu einer Boeing 747 entwickelt. Bedeutet das, dass er sich √ºber eine Singularit√§t lustig macht? <br><br>  Horvitz: Nein.  Ich denke, dass es eine Mischung von Konzepten gab, und ich selbst habe auch gemischte Gef√ºhle. <br><br>  Keist: Insbesondere aufgrund von Ideen wie Singularit√§t versuchen Horvits und andere KI-Experten zunehmend, sich mit ethischen Fragen zu befassen, die in den kommenden Jahren bei eng gezielter KI auftreten k√∂nnen.  Sie stellen auch futuristischere Fragen.  Wie kann ich beispielsweise einen Notabschaltknopf f√ºr einen Computer erstellen, der sich selbst √§ndern kann? <br><br>  Horvits: Ich glaube wirklich, dass der Einsatz hoch genug ist, um Zeit und Energie aktiv nach L√∂sungen zu suchen, auch wenn die Wahrscheinlichkeit solcher Ereignisse gering ist. </blockquote><br>  Dies stimmt im Allgemeinen mit der Position vieler der leidenschaftlichsten KI-Risikor√ºhrer √ºberein.  Mit solchen Freunden werden keine Feinde ben√∂tigt. <br><br>  Der Slate-Artikel ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hab keine Angst vor KI</a> ‚Äú bringt die Dinge √ºberraschenderweise auch ins richtige Licht: <br><blockquote>  Wie Musk selbst feststellt, liegt die L√∂sung f√ºr das Risiko von KI in der n√ºchternen und rationalen Zusammenarbeit von Wissenschaftlern und Gesetzgebern.  Es ist jedoch ziemlich schwer zu verstehen, wie das Sprechen √ºber "D√§monen" dazu beitragen kann, dieses edle Ziel zu erreichen.  Sie kann sie sogar behindern. <br><br>  Erstens gibt es gro√üe L√ºcken in der Idee des Skynet-Skripts.  Obwohl Forscher auf dem Gebiet der Informatik glauben, dass die Argumentation von Mask "nicht v√∂llig verr√ºckt" ist, sind sie immer noch zu weit von einer Welt entfernt, in der der Hype um KI eine etwas weniger KI-Realit√§t verschleiert, mit der unsere Informatiker konfrontiert sind. <br><br>  Ian Lekun, der Leiter des KI-Labors bei Facebook, fasste diese Idee 2013 in einem Beitrag auf Google+ zusammen: Der Hype schadet der KI.  In den letzten f√ºnf Jahrzehnten hat der Hype die KI viermal get√∂tet.  Sie muss gestoppt werden. "Lekun und andere haben zu Recht Angst vor Hype. Die Nichterf√ºllung der hohen Erwartungen, die an Science-Fiction gestellt werden, f√ºhrt zu ernsthaften Budgetk√ºrzungen f√ºr die KI-Forschung. </blockquote><br>  Wissenschaftler, die an KI arbeiten, sind kluge Leute.  Sie sind nicht daran interessiert, in klassische politische Fallen zu geraten, in denen sie in Lager aufgeteilt werden und sich gegenseitig Panik oder Strau√üismus vorwerfen.  Anscheinend versuchen sie, ein Gleichgewicht zwischen der Notwendigkeit, Vorarbeiten im Zusammenhang mit der Bedrohung in der Ferne zu beginnen, und dem Risiko, ein so starkes Gef√ºhl zu verursachen, das sie treffen wird, zu finden. <br><br>  Ich m√∂chte nicht sagen, dass es keine Meinungsverschiedenheiten dar√ºber gibt, wann Sie dieses Problem angehen m√ºssen.  Grunds√§tzlich kommt es darauf an, ob man sagen kann, dass ‚Äûwir das Problem l√∂sen werden, wenn wir darauf sto√üen‚Äú oder einen solchen unerwarteten Start erwarten, aufgrund dessen alles au√üer Kontrolle ger√§t und auf den wir uns daher vorbereiten m√ºssen im Voraus.  Ich sehe weniger Beweise als ich m√∂chte, dass die Mehrheit der KI-Forscher mit ihrer eigenen Meinung die zweite M√∂glichkeit versteht.  Was kann ich sagen, auch wenn in einem Artikel √ºber Marginal Revolution ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experte zitiert wird, der</a> sagt, dass Superintelligenz keine gro√üe Bedrohung darstellt, weil ‚Äûintelligente Computer sich keine Ziele setzen k√∂nnen‚Äú, obwohl jeder, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bostrom liest, das</a> wei√ü Das ganze Problem ist. <br><br>  Es gibt noch viel zu tun.  Aber nur um nicht speziell Artikel auszuw√§hlen, in denen "echte KI-Experten sich keine Sorgen um Superintelligenz machen". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de402379/">https://habr.com/ru/post/de402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de402367/index.html">Wie man aufh√∂rt, f√ºr Roaming zu bezahlen, oder mit einer Nummer auf der ganzen Welt</a></li>
<li><a href="../de402369/index.html">So messen Sie die Geschwindigkeit eines 3D-Druckers - sein hei√ües Ende. Und nicht nur Geschwindigkeit</a></li>
<li><a href="../de402373/index.html">Was gibt die "Genetik der Mikrobiota"</a></li>
<li><a href="../de402375/index.html">8-Kilowatt-4-Kanal-Wechselstromschalter mit Verbrauchsmessung. Teil 1</a></li>
<li><a href="../de402377/index.html">Was halten Ihre Smartphones vom USB-Laden im Auto?</a></li>
<li><a href="../de402381/index.html">Wie man Astronauten rekrutiert</a></li>
<li><a href="../de402383/index.html">Saw, Shura: Wie wir die mobile Mishiko Dog Tracker App entwickelt haben</a></li>
<li><a href="../de402385/index.html">Warum sollten Sie einen Boom im Bereich der Herstellung von Robotern f√ºr Gesch√§ftsr√§ume erwarten?</a></li>
<li><a href="../de402387/index.html">3D-Stift f√ºr 3D-Drucker</a></li>
<li><a href="../de402389/index.html">MPAA und RIAA planen, Daten von ausgefallenen Festplatten der Dateifreigabe Megaupload wiederherzustellen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>