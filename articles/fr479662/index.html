<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ›´ ğŸ’£ Comment Yandex a appris Ã  l'intelligence artificielle Ã  trouver des erreurs dans les actualitÃ©s ğŸ¤›ğŸ» ğŸ––ğŸ½ ğŸ¤ </title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous parlons souvent de technologies et de bibliothÃ¨ques qui ont vu le jour et se sont formÃ©es Ã  Yandex. En fait, nous appliquons et dÃ©veloppons au mo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment Yandex a appris Ã  l'intelligence artificielle Ã  trouver des erreurs dans les actualitÃ©s</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/479662/">  Nous parlons souvent de technologies et de bibliothÃ¨ques qui ont vu le jour et se sont formÃ©es Ã  Yandex.  En fait, nous appliquons et dÃ©veloppons au moins des solutions tierces. <br><br>  Aujourd'hui, je vais parler Ã  la communautÃ© Habr d'un de ces exemples.  Vous apprendrez pourquoi nous avons appris au rÃ©seau neuronal BERT Ã  trouver des fautes de frappe dans les titres de l'actualitÃ©, et n'avons pas utilisÃ© le modÃ¨le prÃªt Ã  l'emploi, pourquoi vous ne pouvez pas prendre et exÃ©cuter BERT sur plusieurs cartes vidÃ©o et comment nous avons utilisÃ© la caractÃ©ristique clÃ© de cette technologie - le mÃ©canisme d'attention. <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br><a name="habracut"></a><h2>  DÃ©fi </h2><br>  Yandex.News est un service qui recueille les actualitÃ©s des publications qui nous sont liÃ©es.  Il s'agit non seulement des nouvelles des mÃ©dias les plus lues et les plus citÃ©es sur la page principale, mais aussi <a href="https://yandex.ru/sport">des</a> sections <a href="https://yandex.ru/sport">thÃ©matiques</a> ou mÃªme des sÃ©lections personnelles de toutes les publications.  En tout cas, ce sont des milliers de sites et des millions de rubriques, Ã  partir desquelles la machine doit former une sÃ©lection toutes les quelques minutes. <br><br>  C'est la machine, car nous n'intervenons jamais dans l'image du jour: nous n'y ajoutons pas d'actualitÃ©s manuellement, nous n'en supprimons pas (peu importe combien nous le voudrions), nous n'Ã©ditons pas les gros titres.  Autour de cela a dÃ©jÃ  Ã©tÃ© brisÃ© de nombreuses copies.  Une approche entiÃ¨rement algorithmique prÃ©sente des avantages et des inconvÃ©nients.  Quelque chose que nous pouvons amÃ©liorer avec la technologie, quelque chose non.  MÃªme s'il y a des fautes d'orthographe ou des fautes de frappe dans les titres, nous ne les corrigeons pas.  Nous avons ajoutÃ© les favicons des publications aux gros titres pour qu'il soit clair d'oÃ¹ viennent les nouvelles.  Cela a aidÃ© en partie, mais nous n'avons pas acceptÃ© les erreurs et avons commencÃ© Ã  chercher un moyen de les Ã©liminer sans apporter de modifications au texte. <br><br>  S'il est impossible de corriger l'erreur, vous pouvez entraÃ®ner la machine Ã  trouver des en-tÃªtes qui, en raison d'erreurs, ne conviennent pas au sommet.  De plus, Yandex est spÃ©cialisÃ© dans la morphologie russe depuis l'Ã©poque oÃ¹ le nom n'avait pas encore Ã©tÃ© inventÃ©.  Il semblerait que nous prenions un rÃ©seau de neurones - et le point est dans le chapeau. <br><br><h2>  Les outils </h2><br>  Yandex dispose de la technologie <a href="https://yandex.ru/dev/speller/">Speller</a> pour rechercher et corriger les erreurs.  GrÃ¢ce Ã  la <a href="https://habr.com/ru/company/yandex/blog/333522/">bibliothÃ¨que d'</a> apprentissage automatique <a href="https://habr.com/ru/company/yandex/blog/333522/">CatBoost,</a> Speller peut dÃ©crypter des mots mÃ©connaissables (Â«adjectifsÂ» â†’ Â«camarades de classeÂ») et prendre en compte le contexte lors de la recherche de fautes de frappe (Â«miss musicÂ» â†’ Â«download musicÂ»).  Il peut sembler que Speller est idÃ©al pour notre tÃ¢che, mais non. <br><br>  L'orthographe (connu en interne sous le nom de gardien de recherche) est dÃ©jÃ  au niveau de l'architecture pour rÃ©soudre un problÃ¨me complÃ¨tement diffÃ©rent: pour aider les utilisateurs Ã  restaurer le bon formulaire de demande.  Dans la recherche, il n'est pas si important que le cas soit correctement sÃ©lectionnÃ©, une lettre majuscule ou une virgule soit posÃ©e.  LÃ , il est plus important pour la requÃªte de recherche "Haminguel" de deviner que la personne avait en tÃªte Hemingway. <br><br>  Des erreurs dans les gros titres sont faites par des personnes relativement instruites qui ont peu de chances d'Ã©crire Haminguel.  Mais une approbation incorrecte (Â«le vol a Ã©tÃ© retardÃ©Â»), des mots manquants (Â«le jeune homme a essayÃ© la voitureÂ») et des lettres majuscules supplÃ©mentaires (Â«prÃ©sident de la BanqueÂ») sont monnaie courante.  Enfin, il y a une phrase formellement correcte: Â«Je rÃ©parerai la rue Gorki Ã  PskovÂ», Ã  laquelle un tuteur normal ne sâ€™accroche pas (enfin, que se passe-t-il si câ€™est une promesse de lâ€™auteur?),  De plus, dans les News, la tÃ¢che n'Ã©tait pas la mÃªme que dans la Recherche: non pas pour corriger les fautes de frappe et les erreurs, mais pour les dÃ©tecter. <br><br>  Nous avions d'autres options, par exemple, des modÃ¨les basÃ©s sur DSSM (si intÃ©ressants, nous avons briÃ¨vement parlÃ© de cette approche dans un article sur <a href="https://habr.com/ru/company/yandex/blog/314222/">l'algorithme de Palekh</a> ), mais ils avaient aussi des limites.  Par exemple, l'ordre des mots n'a pas Ã©tÃ© parfaitement pris en compte. <br><br>  En gÃ©nÃ©ral, les outils prÃªts Ã  l'emploi n'Ã©taient pas adaptÃ©s Ã  notre tÃ¢che ou Ã©taient limitÃ©s.  Donc, vous devez crÃ©er le vÃ´tre - pour former votre modÃ¨le.  Et c'Ã©tait une bonne raison de travailler avec la technologie BERT, qui est devenue disponible aux dÃ©veloppeurs en 2018 et a donnÃ© des rÃ©sultats impressionnants. <br><br><h2>  PrÃ©sentation de BERT </h2><br>  Le principal problÃ¨me des problÃ¨mes modernes de traitement du langage naturel (PNL) est de trouver suffisamment d'exemples balisÃ©s par des personnes pour former un rÃ©seau neuronal.  Si vous avez besoin d'une croissance de qualitÃ©, alors l'Ã©chantillon de formation devrait Ãªtre trÃ¨s grand - des millions et des milliards d'exemples.  En mÃªme temps, la PNL comporte de nombreuses tÃ¢ches et elles sont toutes diffÃ©rentes.  La collecte de donnÃ©es en volumes similaires pour chaque tÃ¢che est longue, coÃ»teuse et souvent impossible.  MÃªme pour les plus grandes entreprises du monde. <br><br>  Mais il existe une option pour contourner ce problÃ¨me - Ã  l'aide d'une formation en deux Ã©tapes.  Dans un premier temps, le rÃ©seau neuronal apprend une structure de langage pendant une longue et coÃ»teuse durÃ©e sur un Ã©norme corps de milliards de mots (c'est la prÃ©-formation).  Ensuite, le rÃ©seau est rapidement et Ã  moindre coÃ»t tordu pour une tÃ¢che spÃ©cifique - par exemple, pour diviser les avis en bons et mauvais (c'est un rÃ©glage fin).  <a href="https://habr.com/ru/company/yandex/blog/305956/">Environ</a> 10 000 exemplaires balisÃ©s Ã  <a href="https://habr.com/ru/company/yandex/blog/305956/">Tolok</a> . <br><br>  La technologie BERT (ReprÃ©sentations des codeurs bidirectionnels des transformateurs) est basÃ©e sur cette idÃ©e.  L'idÃ©e elle-mÃªme n'est pas nouvelle et a dÃ©jÃ  Ã©tÃ© appliquÃ©e, mais il existe une diffÃ©rence significative.  Transformer est une telle architecture de rÃ©seau neuronal qui vous permet de prendre en compte tout le contexte Ã  la fois, y compris l'autre extrÃ©mitÃ© de la phrase et le roulement des participants quelque part au milieu.  Et c'est sa diffÃ©rence par rapport aux architectures Ã  la mode prÃ©cÃ©dentes, qui tenaient compte du contexte.  Par exemple, un rÃ©seau de neurones LSTM a une longueur de contexte de dizaines de mots au mieux, et ici c'est tout 200. <br><br>  Sur <a href="https://github.com/google-research/bert">GitHub</a> , le code source TensorFlow et mÃªme un modÃ¨le universel prÃ©-formÃ© en 102 langues sont disponibles, du russe au volapyuk.  Prenez, semble-t-il, la solution hors de la boÃ®te - et obtenez le rÃ©sultat immÃ©diatement.  Mais non. <br><br>  Il s'est avÃ©rÃ© que le modÃ¨le universel dans les textes russes a montrÃ© une qualitÃ© nettement infÃ©rieure Ã  celle du modÃ¨le anglais, battant des records dans les textes anglais (ce qui, vous voyez, est logique).  Dans les textes russes, elle a perdu face Ã  nos modÃ¨les internes sur DSSM. <br><br>  D'accord, vous pouvez vous prÃ©-Ã©duquer - heureusement, Yandex a suffisamment de textes russes et d'expÃ©rience en apprentissage automatique.  Mais il y a une nuance.  Il faut un an pour apprendre! <br><br>  Le fait est que BERT est conÃ§u pour les processeurs tenseurs de Google (TPU), donc hors de la boÃ®te, il ne peut fonctionner qu'avec une seule carte vidÃ©o (GPU).  Et il est impossible de parallÃ©liser son front avec nâ€™importe quel <a href="https://github.com/horovod">horovod</a> : transfÃ©rer 400 mÃ©gaoctets de donnÃ©es dâ€™une carte Ã  une autre Ã  chaque Ã©tape coÃ»te trÃ¨s cher, la parallÃ©lisation deviendra inutile.  Que faire <br><br><h2>  Optimisation </h2><br>  Ils ont commencÃ© Ã  rechercher des idÃ©es et des solutions susceptibles d'accÃ©lÃ©rer considÃ©rablement le dossier.  Tout d'abord, nous avons remarquÃ© que chaque nombre dans notre modÃ¨le occupait 32 bits de mÃ©moire (le flotteur standard pour les nombres dans l'ordinateur).  Il semble Ãªtre petit, mais lorsque vous avez 100 millions de poids, cela est essentiel.  Nous n'avions pas besoin d'une telle prÃ©cision partout, nous avons donc dÃ©cidÃ© de convertir partiellement les nombres au format 16 bits (c'est ce qu'on appelle un entraÃ®nement de prÃ©cision mixte). <br><br>  En cours de route, avec l'aide de nombreux fichiers et bÃ©quilles, nous avons vissÃ© la compilation XLA, en nous appuyant sur le <a href="https://github.com/google-research/bert/pull/255">commit</a> NVIDIA alors encore brut.  GrÃ¢ce Ã  cela, nos cartes NVIDIA Tesla V100 (un petit serveur d'entre elles se prÃ©sente comme un appartement dans un quartier peu coÃ»teux de Moscou) ont pu rÃ©vÃ©ler pleinement leur potentiel grÃ¢ce Ã  l'arithmÃ©tique 16 bits sur les cÅ“urs de tenseur. <br><br>  Nous ne nous intÃ©ressions qu'aux titres en russe, mais le modÃ¨le multilingue, que nous avons pris comme base, a Ã©tÃ© formÃ© dans des centaines de langues, y compris mÃªme un volapuk artificiel.  Les mots de toutes les langues traduits en espace vectoriel ont Ã©tÃ© stockÃ©s dans le modÃ¨le.  De plus, vous ne pouvez pas les prendre et simplement les retirer de lÃ  - j'ai dÃ» transpirer pour rÃ©duire la taille du dictionnaire. <br><br>  Et encore une chose.  Si vous Ãªtes un scientifique et que votre ordinateur est sous la table, vous pouvez tout reconfigurer pour chaque tÃ¢che spÃ©cifique.  Mais dans un vÃ©ritable cloud informatique, oÃ¹ des milliers de machines sont configurÃ©es de la mÃªme maniÃ¨re, il est assez problÃ©matique, par exemple, de reconstruire le noyau pour chaque nouvelle fonctionnalitÃ© TensorFlow.  Par consÃ©quent, nous avons consacrÃ© beaucoup d'efforts Ã  la collecte de telles versions de packages que toutes les puces nouvellement conÃ§ues peuvent faire, et ne nÃ©cessitent pas de mise Ã  jour et de reconfiguration radicales des cartes vidÃ©o dans le cloud. <br><br>  En gÃ©nÃ©ral, pressÃ© tous les jus partout oÃ¹ ils le pouvaient.  Et nous l'avons fait.  L'annÃ©e s'est transformÃ©e en une semaine. <br><br><h2>  La formation </h2><br>  Construire le bon jeu de donnÃ©es est gÃ©nÃ©ralement la partie la plus difficile du travail.  Tout d'abord, nous avons appris le classificateur sur trois millions de rubriques marquÃ©es de tolokers.  Cela semble Ãªtre beaucoup, mais seulement 30 000 d'entre eux - avec des fautes de frappe.  OÃ¹ trouver plus d'exemples? <br><br>  Nous avons dÃ©cidÃ© de voir quelles rubriques les mÃ©dias eux-mÃªmes corrigent.  Il y en a plus de 2 millions dans l'histoire de Yandex.  Bingo!  Bien qu'il soit trop tÃ´t pour se rÃ©jouir. <br><br>  Il s'est avÃ©rÃ© que trÃ¨s souvent les mÃ©dias refont les gros titres non Ã  cause d'erreurs.  De nouveaux dÃ©tails sont apparus - et l'Ã©diteur a remplacÃ© une formulation correcte par une autre.  Par consÃ©quent, nous nous sommes limitÃ©s aux corrections avec une diffÃ©rence entre les versions de jusqu'Ã  trois lettres (bien qu'il y ait encore du bruit ici: il a Ã©tÃ© "trouvÃ© deux" - il est devenu "trouvÃ© trois").  Nous avons donc marquÃ© un million de fautes de frappe.  Nous avons Ã©tudiÃ© d'abord sur cette grande sÃ©lection avec bruit, puis sur un petit marquage tolker sans bruit. <br><br><h2>  La qualitÃ© </h2><br>  Dans de telles tÃ¢ches, il est habituel de mesurer l'exactitude et l'exhaustivitÃ©.  Dans notre cas, la prÃ©cision est la proportion de verdicts corrects parmi tous les verdicts concernant une erreur dans l'en-tÃªte.  ExhaustivitÃ© - la proportion d'en-tÃªtes d'erreur que nous avons dÃ©tectÃ©s parmi tous les en-tÃªtes d'erreur.  Cela et un autre dans le monde idÃ©al devraient aspirer Ã  100%.  Mais dans les tÃ¢ches d'apprentissage automatique, ces indicateurs ont tendance Ã  entrer en conflit.  Autrement dit, plus nous tordons la prÃ©cision, plus l'exhaustivitÃ© diminue.  Et vice versa. <br><br>  Dans notre approche prÃ©cÃ©dente basÃ©e sur DSSM, nous avons dÃ©jÃ  atteint une prÃ©cision de 95% (soit 5% de verdicts faux positifs).  C'est dÃ©jÃ  un indicateur assez Ã©levÃ©.  Par consÃ©quent, nous avons dÃ©cidÃ© de maintenir le mÃªme niveau de prÃ©cision et de voir comment l'intÃ©gralitÃ© change avec le nouveau modÃ¨le.  Et elle est passÃ©e de 21 Ã  78%.  Et c'est dÃ©finitivement un succÃ¨s. <br><br>  Ici, il serait possible d'y mettre un terme, mais je me souviens de la promesse de parler d'attention. <br><br><h2>  RÃ©seau neuronal avec feutre </h2><br>  Il est gÃ©nÃ©ralement admis qu'un rÃ©seau de neurones est une telle boÃ®te noire.  Nous alimentons quelque chose Ã  l'entrÃ©e et obtenons quelque chose Ã  la sortie.  Pourquoi et comment est un mystÃ¨re. <br><br>  Cette limitation vise Ã  contourner les rÃ©seaux de neurones interprÃ©tÃ©s.  BERT est l'un d'entre eux.  Son interprÃ©tabilitÃ© rÃ©side dans le mÃ©canisme d'attention.  En gros, dans chaque couche du rÃ©seau neuronal, nous rÃ©pÃ©tons la mÃªme technique: nous regardons les mots voisins avec une Â«attentionÂ» diffÃ©rente et prenons en compte l'interaction avec eux.  Par exemple, lorsqu'un rÃ©seau de neurones traite le pronom Â«ilÂ», il Â«regarde attentivementÂ» le nom auquel Â«ilÂ» fait rÃ©fÃ©rence. <br><br>  L'image ci-dessous montre dans diffÃ©rentes nuances de rouge les mots que le jeton "regarde", ce qui accumule des informations sur l'intÃ©gralitÃ© du titre pour la couche de classificateur final.  Si une faute de frappe dans le mot - l'attention le met en Ã©vidence, si les mots sont incohÃ©rents - alors les deux (et, Ã©ventuellement, en dÃ©pendent). <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br>  Ã€ cet endroit, d'ailleurs, on peut discerner tout le potentiel des rÃ©seaux de neurones.  Ã€ aucun stade de la formation, notre modÃ¨le ne sait exactement oÃ¹ se trouve la faute de frappe dans l'exemple: il sait seulement que l'intÃ©gralitÃ© du titre est incorrect.  Et pourtant, elle apprend qu'il est incorrect d'Ã©crire Â«une Ã©cole pour 1224 placesÂ» en raison d'un chiffre incohÃ©rent, et elle met spÃ©cifiquement en Ã©vidence le chiffre 4. <br><br>  Nous ne nous sommes pas arrÃªtÃ©s aux fautes de frappe et avons commencÃ© Ã  appliquer une nouvelle approche non seulement pour rechercher les erreurs, mais aussi pour identifier les en-tÃªtes obsolÃ¨tes.  Mais c'est une histoire complÃ¨tement diffÃ©rente avec laquelle nous espÃ©rons revenir Ã  Habr dans un proche avenir. <br><br><h2>  Liens utiles pour ceux qui veulent approfondir le sujet </h2><br><ul><li>  <a href="https://github.com/google-research/bert">Code TensorFlow et modÃ¨les prÃ©-formÃ©s pour BERT</a> </li><li>  <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">Open Sourcing BERT: PrÃ©-formation de pointe pour le traitement du langage naturel</a> </li><li>  <a href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPU vs GPU pour transformateurs (BERT)</a> </li><li>  <a href="http://jalammar.github.io/illustrated-transformer/">Le transformateur illustrÃ©</a> </li><li>  <a href="https://news.developer.nvidia.com/nvidia-achieves-4x-speedup-on-bert-neural-network/">NVIDIA obtient une accÃ©lÃ©ration 4X sur le rÃ©seau neuronal BERT</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr479662/">https://habr.com/ru/post/fr479662/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr479644/index.html">Mots simples sur le programmatique</a></li>
<li><a href="../fr479650/index.html">Top 12 des infographies dynamiques dynamiques informatiques les plus intÃ©ressantes</a></li>
<li><a href="../fr479654/index.html">GÃ©nÃ©rateur de vue Django</a></li>
<li><a href="../fr479656/index.html">PostgreSQL Antipatterns: statistiques autour de la tÃªte</a></li>
<li><a href="../fr479660/index.html">3. Analyse des logiciels malveillants Ã  l'aide de Check Point forensics. Mobile de sablage</a></li>
<li><a href="../fr479664/index.html">Fonctionnement des kubernetes gÃ©rÃ©s et d'OpenShift gÃ©rÃ© dans IBM Cloud. Partie 1 - Architecture et sÃ©curitÃ©</a></li>
<li><a href="../fr479666/index.html">Golang: Sur quoi s'appuie un spÃ©cialiste Go dans une mer de spÃ©cialitÃ©s informatiques?</a></li>
<li><a href="../fr479668/index.html">QA pour les dÃ©butants: comment tester une fusÃ©e ou un avion?</a></li>
<li><a href="../fr479672/index.html">PEUT renifler</a></li>
<li><a href="../fr479676/index.html">ExtJS 7 et Spring Boot 2. Comment construire un SPA qui interagit avec votre API et les plugins ReactJS externes?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>