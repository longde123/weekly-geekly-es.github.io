<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåè üë®‚Äçüë©‚Äçüëß üéπ Ceph - de "no joelho" a "produ√ß√£o" parte 2 ‚òùüèø ü¶Ö üßïüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(primeira parte aqui: https://habr.com/en/post/456446/ ) 
 Ceph 
 1. Introdu√ß√£o 


 Como a rede √© um dos elementos principais da Ceph e √© um pouco esp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - de "no joelho" a "produ√ß√£o" parte 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (primeira parte aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  1. Introdu√ß√£o </h3><br><p>  Como a rede √© um dos elementos principais da Ceph e √© um pouco espec√≠fica em nossa empresa, primeiro falaremos um pouco sobre ela. <br>  Haver√° muito menos descri√ß√µes do pr√≥prio Ceph, principalmente uma infraestrutura de rede.  Apenas servidores Ceph e alguns recursos dos servidores de virtualiza√ß√£o Proxmox ser√£o descritos. </p><a name="habracut"></a><br><p>  Portanto: a pr√≥pria topologia de rede √© criada como <strong>Leaf-Spine.</strong>  A arquitetura cl√°ssica de tr√™s camadas √© uma rede em que h√° <strong>Core</strong> (roteadores principais), <strong>Agrega√ß√£o</strong> (roteadores de agrega√ß√£o) e diretamente conectados aos clientes do <strong>Access</strong> (roteadores de acesso): </p><br><p>  <strong>Esquema de tr√™s n√≠veis</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  A topologia Leaf-Spine consiste em dois n√≠veis: <strong>Spine</strong> (grosso modo o roteador principal) e <strong>Leaf</strong> (ramifica√ß√µes). </p><br><p>  <strong>Esquema de dois n√≠veis</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Todo o roteamento interno e externo √© constru√≠do no BGP.  O principal sistema que lida com controle de acesso, an√∫ncios e muito mais √© o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>XCloud.</strong></a> <br>  Os servidores para reserva de canal (e tamb√©m para sua expans√£o) est√£o conectados a dois comutadores L3 (a maioria dos servidores est√° conectada aos comutadores Leaf, mas alguns servidores com maior carga de rede s√£o conectados diretamente √† coluna do comutador) e, por meio do BGP, anunciam seu endere√ßo unicast, assim como qualquer endere√ßo de broadcast para o servi√ßo, se v√°rios servidores atenderem ao tr√°fego do servi√ßo e o balanceamento do ECMP for suficiente para eles.  Um recurso separado desse esquema, que nos permitiu economizar endere√ßos, mas tamb√©m exigia que os engenheiros se familiarizassem com o mundo IPv6, era o uso do padr√£o n√£o numerado BGP baseado na RFC 5549. Por algum tempo, Quagga foi usado para servidores no BGP para esse esquema para servidores e periodicamente houve problemas com a perda de festas e conectividade.  Por√©m, depois de mudar para o FRRouting (cujos colaboradores ativos s√£o nossos fornecedores de equipamentos de rede: Cumulus e XCloudNetworks), n√£o observamos mais esses problemas. </p><br><p>  Por conveni√™ncia, chamamos todo esse esquema geral de "f√°brica". </p><br><h2 id="poisk-puti">  Procure uma maneira </h2><br><p>  Op√ß√µes de configura√ß√£o de rede de cluster: </p><br><p>  1) Segunda rede no BGP </p><br><p>  2) A segunda rede em dois switches empilhados separados com LACP </p><br><p>  3) Segunda rede em dois comutadores isolados separados com OSPF </p><br><h3 id="testy">  Testes </h3><br><p>  Os testes foram realizados em dois tipos: </p><br><p>  a) rede usando iperf, qperf, utilit√°rios nuttcp </p><br><p>  b) testes internos Ceph ceph-gobench, banco de rados, criou rbd e testou-os usando dd em um ou v√°rios threads, usando fio </p><br><p>  Todos os testes foram realizados em m√°quinas de teste com discos SAS.  Os n√∫meros no desempenho da rbd n√£o foram muito analisados, foram usados ‚Äã‚Äãapenas para compara√ß√£o.  Interessado em altera√ß√µes, dependendo do tipo de conex√£o. </p><br><h3 id="pervyy-variant">  Primeira op√ß√£o </h3><br><p>  <strong>As placas de rede est√£o conectadas √† f√°brica, configurada BGP.</strong> </p><br><p>  O uso desse esquema para a rede interna n√£o foi considerado a melhor op√ß√£o: </p><br><p>  Em primeiro lugar, o n√∫mero excessivo de elementos intermedi√°rios na forma de comutadores, fornecendo lat√™ncia adicional (esse foi o principal motivo). <br>  Em segundo lugar, inicialmente, para transmitir est√°tica no s3, eles usaram o endere√ßo anycast gerado em v√°rias m√°quinas com o radosgateway.  Isso resultou no fato de que o tr√°fego das m√°quinas front-end para o RGW n√£o foi distribu√≠do uniformemente, mas passou pela rota mais curta - ou seja, o Nginx do front-end sempre virou para o mesmo n√≥ com o RGW que estava conectado √† folha compartilhada com ele (isso, √© claro, era n√£o √© o argumento principal - simplesmente recusamos posteriormente os endere√ßos anycast para retornar est√°tico).  Mas, pela pureza do experimento, eles decidiram realizar testes nesse esquema para obter dados para compara√ß√£o. </p><br><p>  Est√°vamos com medo de executar testes para toda a largura de banda, j√° que a f√°brica √© usada por servidores de produtos e, se bloquearmos os links entre a folha e a coluna, isso prejudicaria algumas das vendas. <br>  Na verdade, esse foi outro motivo para rejeitar esse esquema. <br>  Os testes Iperf com um limite de 3Gbps de 1, 10 e 100 fluxos de BW foram utilizados para compara√ß√£o com outros esquemas. <br>  Os testes mostraram os seguintes resultados: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  em <strong>1</strong> fluxo, aproximadamente <strong>9,30 - 9,43 Gbits / s</strong> (nesse caso, o n√∫mero de retransmiss√µes aumenta fortemente, para <strong>39148</strong> ).  A figura que se mostrou pr√≥xima ao m√°ximo de uma interface sugere que uma das duas √© usada.  O n√∫mero de retransmiss√µes √© de aproximadamente <strong>500 a 600.</strong> <br>  <strong>10</strong> fluxos de <strong>9,63 Gbits / s</strong> por interface, enquanto o n√∫mero de retransmiss√µes cresceu para uma m√©dia de <strong>17045.</strong> <br>  em <strong>100</strong> threads, o resultado foi pior que em <strong>10</strong> , enquanto o n√∫mero de retransmiss√µes √© menor: o valor m√©dio √© <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Segunda op√ß√£o </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Havia dois comutadores Juniper EX4500.  Eles os coletaram na pilha, conectaram o servidor com os primeiros links para um switch, o segundo para o segundo. <br>  A configura√ß√£o inicial da liga√ß√£o foi a seguinte: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Os testes iperf e qperf mostraram Bw de at√© 16 <strong>Gbits / s.</strong>  Decidimos comparar diferentes tipos de mod: <br>  <strong>rr, balance-xor e 802.3ad.</strong>  Tamb√©m comparamos diferentes tipos de hash <strong>layer2 + 3 e layer3 + 4</strong> (na esperan√ßa de obter uma vantagem na computa√ß√£o em hash). <br>  Tamb√©m comparamos os resultados para diferentes valores sysctl da vari√°vel <strong>net.ipv4.fib_multipath_hash_policy,</strong> (bem, brincamos um pouco com <strong>net.ipv4.tcp_congestion_control</strong> , embora isso n√£o tenha nada a ver com <strong>liga√ß√£o</strong> . H√° um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bom artigo sobre ValdikSS para</a> essa vari√°vel). </p><br><p>  Mas em todos os testes, n√£o funcionou para superar o limite de 18 <strong>Gbits / s</strong> (esse valor foi alcan√ßado usando o <strong>balance-xor e o 802.3ad</strong> , n√£o havia muita diferen√ßa entre os resultados dos testes) e esse valor foi atingido "in jump" por rajadas. </p><br><h3 id="tretiy-variant">  Terceira op√ß√£o </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Para configurar esta op√ß√£o, o LACP foi removido dos comutadores (o empilhamento foi deixado, mas foi usado apenas para gerenciamento).  Em cada switch, eles coletaram uma vlan separada para um grupo de portas (de olho no futuro em que os servidores QA e PROD ficar√£o presos nos mesmos switches). </p><br><p>  Configurou duas redes privadas planas para cada vlan (uma interface por switch).  No topo desses endere√ßos est√° o an√∫ncio de outro endere√ßo da terceira rede privada, que √© a rede de cluster do CEPH. </p><br><p>  Como a <em>rede p√∫blica</em> (atrav√©s da qual usamos SSH) funciona no BGP, usamos frr para configurar o OSPF, que j√° est√° no sistema. </p><br><p>  <strong>10.10.10.0/24 e 20.20.20.0/24</strong> - duas redes planas nos comutadores </p><br><p>  <strong>172.16.1.0/24</strong> - rede para an√∫ncio </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Configura√ß√£o da m√°quina: <br>  interfaces <strong>ens1f0 ens1f1</strong> olham para uma rede privada <br>  interfaces <strong>ens4f0 ens4f1</strong> olhar para a rede p√∫blica </p><br><p>  A configura√ß√£o de rede na m√°quina √© assim: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  As configura√ß√µes de Frr s√£o assim: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  Nessas configura√ß√µes, a rede testa iperf, qperf etc.  mostrou utiliza√ß√£o m√°xima de ambos os canais em <strong>19,8 Gbit / s,</strong> enquanto a lat√™ncia caiu para <strong>20us</strong> </p><br><p>  <em><strong>Campo ID do roteador Bgp:</strong> usado para identificar o n√≥ ao processar informa√ß√µes de roteamento e criar rotas.</em>  <em>Se n√£o especificado na configura√ß√£o, um dos endere√ßos IP do host √© selecionado.</em>  <em>Diferentes fabricantes de hardware e software podem ter algoritmos diferentes; no nosso caso, a FRR usou o maior endere√ßo IP de loopback.</em>  <em>Isso levou a dois problemas:</em> <em><br></em>  <em>1) Se tentarmos desligar outro endere√ßo (por exemplo, privado da rede 172.16.0.0) mais do que o atual, isso causou uma altera√ß√£o no <strong>ID</strong> do <strong>roteador</strong> e, consequentemente, reinstalou as sess√µes atuais.</em>  <em>Isso significa uma pequena interrup√ß√£o e perda de conectividade de rede.</em> <em><br></em>  <em>2) Se tentarmos desligar qualquer endere√ßo de broadcast compartilhado por v√°rias m√°quinas e ele tiver sido selecionado como um <strong>ID de roteador</strong> , dois n√≥s com o mesmo <strong>ID de roteador</strong> aparecer√£o na rede <strong>.</strong></em> </p><br><h2 id="chast-2">  Parte 2 </h2><br><p>  Ap√≥s testar o controle de qualidade, come√ßamos a atualizar o Ceph de combate. </p><br><h3 id="network">  REDE </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Passando de uma rede para duas </h3><br><p>  O par√¢metro de rede do cluster √© um daqueles que n√£o podem ser alterados em tempo real, especificando o OSD via <strong>ceph tell osd. * Injectargs.</strong>  Mud√°-lo na configura√ß√£o e reiniciar o cluster inteiro √© uma solu√ß√£o toler√°vel, mas eu realmente n√£o queria ter um pequeno tempo de inatividade.  Tamb√©m √© imposs√≠vel reiniciar um OSD com um novo par√¢metro de rede - em algum momento, ter√≠amos dois meio-clusters - OSDs antigos na rede antiga, novos no novo.  Felizmente, o par√¢metro de rede do cluster (assim como public_network, a prop√≥sito) √© uma lista, ou seja, voc√™ pode especificar v√°rios valores.  Decidimos mudar gradualmente - primeiro adicione uma nova rede √†s configura√ß√µes e remova a antiga.  O Ceph percorre a lista de redes sequencialmente - o OSD come√ßa a trabalhar primeiro com a rede listada primeiro. </p><br><p>  A dificuldade era que a primeira rede funcionava atrav√©s do bgp e estava conectada a um switch, e a segunda - ao ospf e conectada a outros que n√£o estavam fisicamente conectados ao primeiro.  No momento da transi√ß√£o, era necess√°rio ter acesso tempor√°rio √† rede entre as duas redes.  A peculiaridade de montar nossa f√°brica era que as ACLs n√£o podem ser configuradas na rede se n√£o estiverem na lista de anunciadas (neste caso, s√£o "externas" e as ACLs s√≥ podem ser criadas externamente. Foi criada em spains, mas n√£o chegou nas folhas). </p><br><p>  A solu√ß√£o era uma muleta, complicada, mas funcionava: anunciar a rede interna via bgp, simultaneamente com ospf. </p><br><p>  A sequ√™ncia de transi√ß√£o √© a seguinte: </p><br><p>  1) Configure a rede de cluster para ceph em duas redes: atrav√©s do bgp e do ospf <br>  Nas configura√ß√µes frr, n√£o era necess√°rio alterar nada, uma linha </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  n√£o nos limita nos endere√ßos anunciados, o endere√ßo da rede interna √© elevado na interface de loopback, basta configurar a recep√ß√£o do an√∫ncio desse endere√ßo nos roteadores. </p><br><p>  2) Adicione uma nova rede √† configura√ß√£o <strong>ceph.conf</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  e comece a reiniciar o OSD, um de cada vez, at√© que todos <strong>mudem para a</strong> rede <strong>172.16.1.0/24.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Em seguida, removemos o excesso de rede da configura√ß√£o </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  e repita o procedimento. </p><br><p>  Isso √© tudo, passamos sem problemas para uma nova rede. </p><br><p>  Refer√™ncias: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt458390/">https://habr.com/ru/post/pt458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt458376/index.html">N√£o √© outra linguagem de programa√ß√£o. Parte 1: L√≥gica do Dom√≠nio</a></li>
<li><a href="../pt458378/index.html">Usando o Avocode para o layout do site. Revis√£o para iniciantes. B√¥nus - registre um per√≠odo de avalia√ß√£o de 30 dias</a></li>
<li><a href="../pt458382/index.html">Por que estamos ensinando isso?</a></li>
<li><a href="../pt458384/index.html">An√°lise e teste do HP 3D Structured Light Scanner Pro S3</a></li>
<li><a href="../pt458388/index.html">Deep (Learning + Random) Floresta e an√°lise de artigo</a></li>
<li><a href="../pt458394/index.html">Protegendo protocolos sem fio usando o LoRaWAN como exemplo</a></li>
<li><a href="../pt458396/index.html">Como eu tornei o desenvolvimento no Vue.js conveniente com a renderiza√ß√£o no servidor</a></li>
<li><a href="../pt458398/index.html">Higiene do trabalho remoto ou os benef√≠cios da telepatia</a></li>
<li><a href="../pt458400/index.html">Arquitetura e implementa√ß√£o de microsservi√ßos passo a passo, parte 1</a></li>
<li><a href="../pt458404/index.html">Transi√ß√£o do mon√≥lito para microsservi√ßos: hist√≥ria e pr√°tica</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>