<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëº ‚è≤Ô∏è üë©üèø‚Äçü§ù‚Äçüë®üèæ D√©ploiement d'Elasticsearch sur AWS avec Kubernetes en 10 √©tapes üßîüèø üõÖ üë∞üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kubernetes aka k8s est un syst√®me open source pour automatiser le d√©ploiement, la mise √† l'√©chelle et la gestion des applications conteneuris√©es. Dans...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>D√©ploiement d'Elasticsearch sur AWS avec Kubernetes en 10 √©tapes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/416643/"><p><img src="https://habrastorage.org/webt/rt/no/ud/rtnouda08zlh-unwdbcvqlb8fts.jpeg"></p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes</a> aka <code>k8s</code> est un syst√®me open source pour automatiser le d√©ploiement, la mise √† l'√©chelle et la gestion des applications conteneuris√©es.  Dans cet article, je vais vous montrer comment configurer un cluster Kubernetes et d√©ployer un cluster Elasticsearch sur AWS sur celui-ci.  Ces param√®tres fonctionnent √©galement sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GCE</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Azure</a> . </p><a name="habracut"></a><br><h3 id="konfigurirovanie-kubernetes-na-aws">  Configuration de Kubernetes sur AWS </h3><br><p>  Pour commencer, obtenez un acc√®s administratif aux services AWS suivants: <strong>S3, EC2, Route53, IAM</strong> et <strong>VPC</strong> . </p><br><p>  <strong>1. Installation:</strong> je vais montrer l'installation de la CLI pour Linux.  Si vous avez un syst√®me d'exploitation diff√©rent, suivez les liens ci-dessous pour les instructions d'installation de votre syst√®me d'exploitation. </p><br><p>  Tout d'abord, d√©finissez l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AWS CLI</a> pour acc√©der √† AWS via la CLI.  Si vous avez d√©j√† Python et pip, ex√©cutez la commande: </p><br><pre> <code class="plaintext hljs">pip install awscli --upgrade --user</code> </pre> <br><p>  Ensuite, nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kops</a> , un outil en ligne de commande qui nous guide √† travers la configuration du cluster de production K8S. <br>  Installez les binaires <strong>Kops</strong> directement depuis github. </p><br><pre> <code class="plaintext hljs">wget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/</code> </pre> <br><p>  Enfin, nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubectl</a> - CLI pour g√©rer le cluster K8S (si vous avez utilis√© docker, cela est similaire √† la <strong>docker</strong> CLI).  La derni√®re version est install√©e par la commande: </p><br><pre> <code class="plaintext hljs">wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl</code> </pre> <br><p>  <strong>Remarque:</strong> vous pouvez d√©marrer le cluster Kubernetes et suivre les instructions de cet article sur une machine domestique avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">minikube</a> . </p><br><p>  <strong>2.Cr√©ez des utilisateurs IAM:</strong> pour cr√©er des clusters dans AWS, nous allons cr√©er un utilisateur IAM distinct pour <code>kops</code> .  Pour <code>kops</code> besoin d'un compte API.  Cr√©ez un utilisateur et configurez un compte via l'interface utilisateur de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">console AWS</a> .  L'utilisateur <code>kops</code> aura besoin de l'autorisation IAM suivante: </p><br><ul><li>  AmazonEC2FullAccess </li><li>  AmazonRoute53FullAccess </li><li>  AmazonS3FullAccess </li><li>  IAMFullAccess </li><li>  AmazonVPCFullAccess </li></ul><br><p><img src="https://habrastorage.org/webt/93/ll/zh/93llzhkqarlmyjtluwo3xiqpjwa.jpeg"></p><br><p>  Alternativement, vous pouvez faire la m√™me chose √† partir de la CLI en appliquant les commandes suivantes: </p><br><pre> <code class="plaintext hljs">aws iam create-group --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops aws iam create-user --user-name kops aws iam add-user-to-group --user-name kops --group-name kops aws iam create-access-key --user-name kops</code> </pre> <br><p>  Notez les <code>AccessKeyID</code> <code>SecretAccessKey</code> et <code>AccessKeyID</code> dans <code>kops</code> . </p><br><p>  Configurez l'AWS CLI pour utiliser votre compte avec <code>aws configure</code> . </p><br><p>  Assurez-vous que l'utilisateur que vous avez cr√©√© figure dans la <code>aws iam list-users</code> . </p><br><p>  Nous exportons le compte AWS en tant que variables d'environnement suivantes afin que les <code>kops</code> CLI puissent les utiliser. </p><br><pre> <code class="plaintext hljs">export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)</code> </pre> <br><blockquote>  <em>Si vous utilisez Kops 1.6.2 ou une version ult√©rieure, la configuration d'un DNS est facultative.</em>  <em>Vous pouvez cr√©er un cluster de potins.</em>  <em>Seule condition: le nom du cluster doit se terminer par <code>.k8s.local</code> .</em> </blockquote><br><h3 id="nastroyka-dns">  Configuration DNS </h3><br><p>  Si vous avez d√©j√† h√©berg√© votre domaine via AWS et pr√©voyez de l'utiliser, rien ne doit √™tre fait.  Autre option: si vous souhaitez utiliser un sous-domaine de votre domaine, cr√©ez une deuxi√®me zone d'h√©bergement publique pour ce sous-domaine.  Dans ce manuel, nous travaillerons avec une zone d'h√©bergement priv√©e.  D√©finissez la zone sous n'importe quel nom.  Utilisez ce nom pour cr√©er des clusters Kubernetes.  <a href="">En savoir plus</a> sur la configuration du DNS <a href="">ici</a> . </p><br><p>  <strong>3. Cr√©ation d'un compartiment S3:</strong> pour enregistrer l'√©tat et l'apparence de notre cluster K8S, vous devez cr√©er un compartiment S3 distinct pour les <code>kops</code> .  Ce compartiment deviendra une source de donn√©es fiables pour le cluster de configuration. </p><br><pre> <code class="plaintext hljs">aws s3api create-bucket \ --bucket &lt;your-unique-bucket-name&gt; \ --region us-east-1</code> </pre> <br><p>  <strong>Remarque:</strong> si vous mettez votre compartiment en service dans une zone autre que <code>us-east-1</code> , en plus de d√©finir <code>- region</code> passez √† la zone souhait√©e et ajoutez <code>LocationConstraint</code> √† la m√™me zone.  Ce qui suit montre la commande de construction de compartiment dans la r√©gion <code>us-west-1</code> . </p><br><pre> <code class="plaintext hljs">aws s3api create-bucket \ --bucket &lt;your-unique-bucket-name&gt; \ --region us-west-1 \ --create-bucket-configuration LocationConstraint=us-west-1</code> </pre> <br><p>  Pour configurer le stockage des versions de compartiment S3 pour la r√©cup√©ration, utilisez la commande suivante: </p><br><pre> <code class="plaintext hljs">aws s3api put-bucket-versioning \ --bucket &lt;your-unique-bucket-name&gt; \ --versioning-configuration Status=Enabled</code> </pre> <br><p>  <strong>4. Cr√©ation du premier cluster Kubernetes:</strong> vous √™tes donc pr√™t √† cr√©er votre premier cluster!  Tout d'abord, configurez les variables d'environnement pour simplifier le processus.  Si vous avez ignor√© la configuration DNS (apr√®s l'√©tape 2), ajoutez <code>.k8s.local</code> √† la valeur <code>NAME</code> . </p><br><pre> <code class="plaintext hljs">export NAME=myfirstcluster.example.com export KOPS_STATE_STORE=s3://your-bucket-name</code> </pre> <br><p>  N'oubliez pas de garder une trace des zones r√©gionales √† votre disposition.  Dans cet exemple, nous d√©ploierons un cluster dans la r√©gion <strong>us-east-2</strong> . </p><br><pre> <code class="plaintext hljs">aws ec2 describe-availability-zones --region us-east-2</code> </pre> <br><p>  Si vous utilisez une zone d'h√©bergement publique, cr√©ez un cluster √† l'aide de la commande suivante: </p><br><pre> <code class="plaintext hljs">kops create cluster \ --zones us-east-2c \ --node-count 3 \ ${NAME}</code> </pre> <br><p>  Si vous utilisez une zone d'h√©bergement priv√©e, proc√©dez comme suit: </p><br><pre> <code class="plaintext hljs">kops create cluster \ --zones us-east-2c \ --node-count 3 \ --dns private ${NAME}</code> </pre> <br><p>  Cette commande vous fournira le journal de configuration du cluster K8S.  Le d√©marrage du cluster prend du temps, car il cr√©e de nouvelles machines EC2 pour les n≈ìuds ma√Ætres des sbires. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 test]$ kops create cluster \ &gt; --dns private \ &gt; --zones us-east-2c \ &gt; --node-count 3 \ &gt; ${NAME} --yes I0306 09:45:29.636834 20628 create_cluster.go:439] Inferred --cloud=aws from zone "us-east-2c" I0306 09:45:29.637030 20628 create_cluster.go:971] Using SSH public key: /home/ec2-user/.ssh/id_rsa.pub I0306 09:45:29.850021 20628 subnets.go:184] Assigned CIDR 172.20.32.0/19 to subnet us-east-2c I0306 09:45:31.118837 20628 dns.go:92] Private DNS: skipping DNS validation I0306 09:45:46.986963 20628 executor.go:91] Tasks: 73 done / 73 total; 0 can run I0306 09:45:46.987101 20628 dns.go:153] Pre-creating DNS records I0306 09:45:47.668392 20628 update_cluster.go:248] Exporting kubecfg for cluster kops has set your kubectl context to k8s.appbase Cluster is starting. It should be ready in a few minutes.</code> </pre> <br><p>  Voila!  Le cluster K8s devrait d√©j√† fonctionner. </p><br><p>  <strong>5. V√©rification du cluster:</strong> toutes les instances cr√©√©es par <code>kops</code> sont en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ASG (Auto Scaling Groups)</a> .  En cas d'√©chec, les instances ASG sont v√©rifi√©es et reconstruites automatiquement. </p><br><p>  Pour modifier la configuration du cluster, ex√©cutez la commande suivante: </p><br><pre> <code class="plaintext hljs">kops edit cluster ${NAME}</code> </pre> <br><p>  Chaque fois que vous modifiez la configuration du cluster, vous devrez cr√©er un cluster en ex√©cutant la commande suivante: </p><br><pre> <code class="plaintext hljs">kops update cluster ${NAME} --yes</code> </pre> <br><p>  Vous verrez quelque chose comme √ßa. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 examples]$ kops update cluster --yes Using cluster from kubectl context: k8s.appbase I0216 05:09:06.074467 2158 dns.go:92] Private DNS: skipping DNS validation I0216 05:09:07.699380 2158 executor.go:91] Tasks: 73 done / 73 total; 0 can run I0216 05:09:07.699486 2158 dns.go:153] Pre-creating DNS records I0216 05:09:07.961703 2158 update_cluster.go:248] Exporting kubecfg for cluster kops has set your kubectl context to k8s.appbase Cluster changes have been applied to the cloud.</code> </pre> <br><p>  V√©rifiez le cluster. </p><br><pre> <code class="plaintext hljs">kops validate cluster</code> </pre> <br><p>  Assurez-vous que le cluster est op√©rationnel. </p><br><pre> <code class="plaintext hljs">Using cluster from kubectl context: k8s.appbase Validating cluster k8s.appbase INSTANCE GROUPS NAME ROLE MACHINETYPE MIN MAX SUBNETS master-us-east-2c Master t2.large 1 1 us-east-2c nodes Node t2.medium 3 3 us-east-2c NODE STATUS NAME ROLE READY ip-172-20-44-33.us-east-2.compute.internal master True ip-172-20-52-48.us-east-2.compute.internal node True ip-172-20-62-30.us-east-2.compute.internal node True ip-172-20-64-53.us-east-2.compute.internal node True Your cluster k8s.appbase is ready</code> </pre> <br><p>  <strong>D√©couvrez vos nouveaux k8!</strong> </p><br><p>  Avec un simple appel √† l'API Kubernetes, vous pouvez v√©rifier si l'API est en ligne et √† l'√©coute.  Utilisez <code>kubectl</code> pour v√©rifier les n≈ìuds. </p><br><pre> <code class="plaintext hljs">kubectl get nodes</code> </pre> <br><p>  Cela donnera des informations sur vos n≈ìuds et leur √©tat actuel. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 elasticsearch]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-172-20-44-33.us-east-2.compute.internal Ready master 1m v1.8.6 ip-172-20-52-48.us-east-2.compute.internal Ready node 3m v1.8.6 ip-172-20-62-30.us-east-2.compute.internal Ready node 2m v1.8.6 ip-172-20-64-53.us-east-2.compute.internal Ready node 4m v1.8.6</code> </pre> <br><p>  Un sous-marin Kubernetes est une abstraction qui repr√©sente un groupe d'un ou plusieurs conteneurs d'applications (tels que Docker) et plusieurs ressources partag√©es pour ces conteneurs.  Sous se d√©roule sur le n≈ìud.  Si vous devez faire √©voluer l'application, ajoutez des n≈ìuds au K8S d√©ploy√©. </p><br><p>  Pour en savoir plus sur les modules disponibles: </p><br><pre> <code class="plaintext hljs">kubectl get pods</code> </pre> <br><p>  Cette commande r√©pertorie les foyers disponibles dans le cluster. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE es-5967f5d99c-5vcpb 1/1 Running 0 3h es-5967f5d99c-cqk88 1/1 Running 0 3h es-5967f5d99c-lp789 1/1 Running 0 3h</code> </pre> <br><h3 id="razvertyvanie-elasticsearch-v-klastere-k8s">  D√©ploiement d'Elasticsearch dans le cluster K8S </h3><br><p><img src="https://habrastorage.org/webt/ot/1m/he/ot1mhep0o9nltuo2ueq0puexg7g.png"></p><br><p>  Si vous n'√™tes pas familier avec Kubernetes, je recommande <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la formation en ligne k8s</a> . </p><br><p>  Pour le moment, nous avons cr√©√© dans le cluster K8S: le n≈ìud principal et deux n≈ìuds d'agent.  Le r√¥le du n≈ìud principal est de transf√©rer des commandes de d√©ploiement vers des applications s'ex√©cutant dans les pods des agents de n≈ìud. </p><br><p>  Les d√©ploiements d'applications K8S sont d√©claratifs et sont configur√©s via des fichiers JSON / YAML.  Choisissez un contr√¥leur en fonction du type d'application ou du syst√®me que vous d√©ployez.  Elasticsearch √©tant une application avec √©tat, nous utiliserons le contr√¥leur StatefulSet. </p><br><p>  <strong>6. D√©ploiement via StatefulSet.</strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>StatefulSet</strong></a> g√®re les pods en fonction de la sp√©cification de conteneurs identiques.  Il g√®re le d√©ploiement et la mise √† l'√©chelle de l'ensemble de foyers et assure l'ordre et l'unicit√© de ces foyers.  Le contr√¥leur <strong>StatefulSet</strong> facilite √©galement l'association de l'application √† un volume persistant, ce qui est important pour Elasticsearch. </p><br><p>  Cr√©ez un fichier appel√© <code>es-stateful set. yaml</code> .  Il contiendra la sp√©cification Elasticsearch.  N'h√©sitez pas √† modifier la configuration.  Pour une liste des variables d'environnement qui peuvent √™tre transmises √† votre image Elasticsearch, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">voir ici</a> . </p><br><p>  <strong>7. Services:</strong> <code>Service</code> Kubernetes - une abstraction qui d√©finit un ensemble logique de <code></code> et y acc√®de.  Cela permet √† l'application conteneur d'identifier une autre application conteneur ou sa propre instance dans un autre foyer. </p><br><p><img src="https://habrastorage.org/webt/di/wn/yq/diwnyqsurqxa0813hatxpfhfgco.png"></p><br><p>  <code>LoadBalancer</code> est un type de service sp√©cial qui fournit des pods aux r√©seaux externes et distribue la charge.  Nous l'utiliserons pour cr√©er une adresse IP externe √† travers laquelle n'importe qui pourra contacter le cluster Elasticsearch.  Nous utiliserons ce service pour les n≈ìuds ES comme un moyen de se d√©couvrir. </p><br><p>  Cr√©ez un fichier appel√© <code>es-svc.yaml</code> .  Modifiez-le et sp√©cifiez le service d'√©quilibrage de charge. </p><br><pre> <code class="plaintext hljs">apiVersion: v1 #API Version of the resource kind: Service #Type of resource metadata: #Contains metadata of this resource. name: elasticsearch #Name of this resource labels: #Additional identifier to put on pods component: elasticsearch #puts component = elasticsearch spec: #Specifications of this resource type: LoadBalancer #type of service selector: #will distribute load on pods which component: elasticsearch #have label `component = elasticsearch` ports: #Port on which LoadBalancer will listen - name: http #Name given to port port: 9200 #Port number protocol: TCP #Protocol supported - name: transport #Name given to port port: 9300 #Port number protocol: TCP #Protocol supported</code> </pre> <br><p>  <strong>8. Cr√©er une application:</strong> c'est tout ce dont nous avons besoin.  D√©ployez notre cluster Elasticsearch sur K8S √† l'aide des commandes suivantes. </p><br><pre> <code class="plaintext hljs">kubectl create -f es-statefulset.yaml kubectl create -f es-svc.yaml</code> </pre> <br><p>  'Cr√©er' est une commande universelle pour cr√©er n'importe quelle ressource dans K8S. </p><br><p>  Notre cluster Elasticsearch √† 3 n≈ìuds (vous vous souvenez des <code>replicas = 3</code> dans la configuration StatefulSet?) Sera lanc√© instantan√©ment. </p><br><p>  Nous pouvons v√©rifier les pods Elasticsearch avec cette commande: </p><br><pre> <code class="plaintext hljs">kubectl get pods</code> </pre> <br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 test]$ kubectl get pods,svc,deployment NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 23m es-1 1/1 Running 0 17m es-2 1/1 Running 0 23m</code> </pre> <br><p>  <strong>9. Test du cluster Elasticsearch:</strong> v√©rifiez si Elasticsearch est correctement configur√© et fonctionne.  Obtenez l'adresse IP externe pour vous connecter √† Elasticsearch.  Il sera situ√© dans le service <strong>LoadBalancer que</strong> nous avons cr√©√©.  Utilisez la commande suivante pour d√©crire le <strong>LoadBalancer</strong> : </p><br><pre> <code class="plaintext hljs">kubectl describe service elasticsearch</code> </pre> <br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 examples]$ kubectl describe service elasticsearch Name: elasticsearch Namespace: default Labels: component=elasticsearch Annotations: &lt;none&gt; Selector: component=elasticsearch Type: LoadBalancer IP: 100.70.114.146 LoadBalancer Ingress: http://a4d0c157d212811e898430af47d23da1-952261901.us-east-2.elb.amazonaws.com Port: http 9200/TCP TargetPort: 9200/TCP NodePort: http 31358/TCP Endpoints: 100.96.4.28:9200 Port: transport 9300/TCP TargetPort: 9300/TCP NodePort: transport 31767/TCP Endpoints: 100.96.4.28:9300 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 1m service-controller Ensuring load balancer Normal EnsuredLoadBalancer 1m service-controller Ensured load balancer [ec2-user@ip-172-31-35-145 examples]$</code> </pre> <br><p>  Notez la valeur de <code>LoadBalancer Ingress</code> .  Ouvrez un navigateur avec un URI et un num√©ro de suffixe du port externe Elasticsearch: <code>9200</code> .  Vous verrez ceci: </p><br><p><img src="https://habrastorage.org/webt/el/b_/zl/elb_zlrvyocf5b7phv5pd9zubri.jpeg"></p><br><p>  Vous pouvez v√©rifier la fonctionnalit√© des n≈ìuds Elasticsearch en ajoutant: <code>9200/_cluster /health?pretty</code> √† l'adresse IP externe. </p><br><p><img src="https://habrastorage.org/webt/hh/ye/aa/hhyeaabyci4yvxnwqjkxacghek4.jpeg"></p><br><p>  <strong>10. Kubernetes Healing Testing:</strong> StatefulSets a la <strong>capacit√© de</strong> stocker le nombre sp√©cifi√© de r√©pliques.  De cette fa√ßon, si un sous tombe, StatefulSet d√©marre un nouveau sous. </p><br><p>  Nous allons le tester en simulant un √©chec (en supprimant tous les pods sur lesquels nos instances ES s'ex√©cutent) pour voir si notre cluster ES peut automatiquement sauvegarder avec des donn√©es intactes. </p><br><p><img src="https://habrastorage.org/webt/l1/vh/nb/l1vhnbrmqhhumqzqi1c7uczvdaa.gif"></p><br><p>  √âtant donn√© que StatefulSet ex√©cute un foyer √† la fois, il faut du temps pour restaurer tous les conteneurs. </p><br><p>  On voit qu'apr√®s la r√©cup√©ration des foyers, un dossier index√© est √† notre disposition en l'√©tat avant la panne ES. </p><br><h3 id="rekomenduem-sleduyuschie-shagi">  Prochaines √©tapes recommand√©es </h3><br><p>  Avant d'utiliser ces param√®tres en production, veuillez noter: </p><br><ol><li>  Configurez les sauvegardes.  Aide √† r√©cup√©rer les donn√©es perdues.  Ce processus est mieux automatis√©. </li><li>  Configuration des autorisations.  Nous voulons prot√©ger le cluster Elasticsearch.  La configuration d'une authentification ou d'une autorisation de base bas√©e sur un jeton de m√©dia assurera la s√©curit√©. </li><li>  Certificats TLS.  Configurez LetsEncrypt / autres fournisseurs TLS de certificats de mappage de domaine personnel pour notre cluster ES et prot√©gez toutes les demandes qui lui sont envoy√©es. </li></ol><br><p>  Bien que l'article ne traite pas de cela, mais sachez: Kubernetes peut faire tout cela. </p><br><p>  Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©ployez Elasticsearch avec Kubernetes sur AWS en 10 √©tapes</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr416643/">https://habr.com/ru/post/fr416643/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr416633/index.html">QUIC, TLS 1.3, DNS sur HTTPS, puis partout</a></li>
<li><a href="../fr416635/index.html">De droite √† gauche. Comment transformer l'interface du site sous RTL</a></li>
<li><a href="../fr416637/index.html">Musique √† partir de papier et de carton: une br√®ve histoire du variophone et du ¬´son dessin√©¬ª</a></li>
<li><a href="../fr416639/index.html">Entretien avec un pionnier du rajeunissement</a></li>
<li><a href="../fr416641/index.html">8 √©tapes du processus de d√©veloppement d'une interface d'application mobile</a></li>
<li><a href="../fr416645/index.html">MIS. Mod√®les de recherche</a></li>
<li><a href="../fr416647/index.html">Les agences gouvernementales r√™vent-elles des risques √©lectriques?</a></li>
<li><a href="../fr416651/index.html">1M HTTP rps sur 1 c≈ìur de processeur. DPDK au lieu de nginx + noyau Linux TCP / IP</a></li>
<li><a href="../fr416653/index.html">Tri de biblioth√®que</a></li>
<li><a href="../fr416657/index.html">Les deux tiers des cartes m√©moire utilis√©es contiennent des donn√©es personnelles d'anciens propri√©taires</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>