<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üè≥Ô∏è‚Äçüåà üßëüèø‚Äçü§ù‚Äçüßëüèª üõ¨ Oh este m√©todo de Newton üè° üÜó üëãüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se ha escrito mucho sobre los m√©todos de optimizaci√≥n num√©rica. Esto es comprensible, especialmente en el contexto de los √©xitos recientemente demostr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Oh este m√©todo de Newton</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469877/">  Se ha escrito mucho sobre los m√©todos de optimizaci√≥n num√©rica.  Esto es comprensible, especialmente en el contexto de los √©xitos recientemente demostrados por las redes neuronales profundas.  Y es muy gratificante que al menos algunos entusiastas est√©n interesados ‚Äã‚Äãno solo en c√≥mo bombardear su red neuronal en los marcos que han ganado popularidad en Internet, sino tambi√©n en c√≥mo y por qu√© todo funciona.  Sin embargo, recientemente tuve que se√±alar que al plantear problemas relacionados con el entrenamiento de las redes neuronales (y no solo con el entrenamiento, y no solo con las redes), incluso en Habr√©, cada vez con m√°s frecuencia se utilizan varias declaraciones "conocidas" para el reenv√≠o, cuya validez para decirlo suavemente, dudoso.  Entre tales declaraciones dudosas: <br><br><ol><li>  Los m√©todos de la segunda y m√°s √≥rdenes no funcionan bien en las tareas de entrenamiento de redes neuronales.  Porque eso </li><li>  El m√©todo de Newton requiere una definici√≥n positiva de la matriz de Hesse (segundas derivadas) y, por lo tanto, no funciona bien. <br></li><li>  El m√©todo de Levenberg-Marquardt es un compromiso entre el descenso de gradiente y el m√©todo de Newton y generalmente es heur√≠stico. <br></li></ol><br>  etc.  Que continuar con esta lista, es mejor ponerse manos a la obra.  En esta publicaci√≥n consideraremos la segunda declaraci√≥n, ya que solo lo conoc√≠ al menos dos veces en Habr√©.  Tocar√© la primera pregunta solo en la parte relativa al m√©todo de Newton, ya que es mucho m√°s extensa.  El tercero y el resto quedar√°n hasta tiempos mejores. <br><a name="habracut"></a><br>  El foco de nuestra atenci√≥n ser√° la tarea de optimizaci√≥n incondicional. <img src="https://habrastorage.org/getpro/habr/post_images/823/e50/c93/823e50c935e4cc19a175f53c17ca79af.gif" title="&quot;f (x) \ rightarrow \ min&quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/bc7/838/e10/bc7838e10e143195c0381efbf0671cce.gif" title="&quot;x = (x_ {1}, x_ {2}, \ puntos)&quot;">  - un punto de espacio vectorial, o simplemente - un vector.  Naturalmente, esta tarea es m√°s f√°cil de resolver, cuanto m√°s sepamos sobre <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  .  Por lo general, se supone que es diferenciable con respecto a cada argumento <img src="https://habrastorage.org/getpro/habr/post_images/6d8/d4e/07d/6d8d4e07d259325d5dd652e4b3b97af6.gif" title="&quot;x_ {k}&quot;">  , y tantas veces como sea necesario para nuestros actos sucios.  Es bien sabido que una condici√≥n necesaria para eso en un punto <img src="https://habrastorage.org/getpro/habr/post_images/8ae/e32/d7e/8aee32d7e93fb189b268894bf91622b0.gif" title="&quot;x ^ {*}&quot;">  se alcanza el m√≠nimo, es la igualdad del gradiente de la funci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/735/1d5/454/7351d54544ca7acc4b7a9bff7a2c2f6a.gif" title="&quot;\ bigtriangledowndown f (x ^ {*})&quot;">  en este punto cero  Desde aqu√≠ obtenemos instant√°neamente el siguiente m√©todo de minimizaci√≥n: <br><br>  Resuelve la ecuaci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/d59/223/ac1/d59223ac159bfe660ea26a1d60f8f33f.gif" title="&quot;\ bigtriangledowndown f (x) = 0&quot;">  . <br><br>  La tarea, por decirlo suavemente, no es f√°cil.  Definitivamente no es m√°s f√°cil que el original.  Sin embargo, en este punto, podemos notar de inmediato la conexi√≥n entre el problema de minimizaci√≥n y el problema de resolver un sistema de ecuaciones no lineales.  Esta conexi√≥n volver√° a nosotros cuando consideremos el m√©todo Levenberg-Marquardt (cuando lleguemos a √©l).  Mientras tanto, recuerde (o descubra) que uno de los m√©todos m√°s utilizados para resolver sistemas de ecuaciones no lineales es el m√©todo de Newton.  Consiste en el hecho de que para resolver la ecuaci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  partimos de una aproximaci√≥n inicial <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  construir una secuencia <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a0/23d/4a2/3a023d4a27cdff86f8cf3bc78d5b3a21.gif" title="&quot;x_ {i + 1} = x_ {i} -H ^ {- 1} (x_ {i}) F (x_ {i})&quot;">  - M√©todo expl√≠cito de Newton <br><br>  o <br><br><img src="https://habrastorage.org/getpro/habr/post_images/116/6fa/27b/1166fa27b4038fed75d435daaaab53fe.gif" title="&quot;\ begin {cases} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + p_ {i} \ end {cases}&quot;">  - M√©todo impl√≠cito de Newton <br><br>  donde <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  - matriz compuesta de derivadas parciales de una funci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Naturalmente, en el caso general, cuando el sistema de ecuaciones no lineales simplemente se nos da en sensaciones, requiere algo de la matriz <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  No tenemos derecho.  En el caso de que la ecuaci√≥n sea una condici√≥n m√≠nima para alguna funci√≥n, podemos afirmar que la matriz <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  sim√©trico  Pero no mas. <br><br>  El m√©todo de Newton para resolver sistemas de ecuaciones no lineales se ha estudiado bastante bien.  Y aqu√≠ est√° la cosa: para su convergencia no se requiere la definici√≥n positiva de la matriz <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  .  S√≠, y no puede ser requerido; de lo contrario, no habr√≠a servido para nada.  En cambio, hay otras condiciones que aseguran la convergencia local de este m√©todo y que no consideraremos aqu√≠, enviando a las personas interesadas a literatura especializada (o en el comentario).  Obtenemos que la afirmaci√≥n 2 es falsa. <br><br>  Entonces? <br><br>  Si y no  La emboscada aqu√≠ en la palabra es la convergencia local antes de la palabra.  Significa que la aproximaci√≥n inicial <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  debe estar "lo suficientemente cerca" de la soluci√≥n, de lo contrario, en cada paso seremos m√°s y m√°s alejados de ella.  Que hacer  No entrar√© en detalles sobre c√≥mo se resuelve este problema para sistemas de ecuaciones no lineales de forma general.  En cambio, volvamos a nuestra tarea de optimizaci√≥n.  El primer error de la declaraci√≥n 2 es que, en general, hablar del m√©todo de Newton en problemas de optimizaci√≥n significa su modificaci√≥n: el m√©todo de Newton amortiguado, en el que la secuencia de aproximaciones se construye de acuerdo con la regla <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23d/ca8/404/23dca84042b77a1560b8cd2db607e8ae.gif" title="&quot;x_ {i + 1} = x_ {i} - \ alpha_ {i} H ^ {- 1} (x_ {i}) F (x_ {i})&quot;">  - M√©todo amortiguado expl√≠cito de Newton <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9d4/67c/c96/9d467cc96266cf1179d3e553718f5bee.gif" title="&quot;\ begin {cases} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + \ alpha_ {i} p_ {i} \ end {cases} &quot;">  - M√©todo amortiguado impl√≠cito de Newton <br><br>  Aqu√≠ esta la secuencia <img src="https://habrastorage.org/getpro/habr/post_images/c70/738/fd1/c70738fd1eb4d9bfff34f20904f41bbf.gif" title="&quot;\ {\ alpha_ {i} \}&quot;">  es un par√°metro del m√©todo y su construcci√≥n es una tarea separada.  En problemas de minimizaci√≥n, natural al elegir <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  habr√° un requisito de que, en cada iteraci√≥n, el valor de la funci√≥n f disminuya, es decir, <img src="https://habrastorage.org/getpro/habr/post_images/3ae/e45/ba0/3aee45ba0097ca8bdc8a23ef6a465f21.gif" title="&quot;f (x_ {i + 1}) &amp; lt; f (x_ {i})&quot;">  .  Surge una pregunta l√≥gica: ¬øexiste tal (positivo) <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  ?  Y si la respuesta a esta pregunta es positiva, entonces <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  llamado la direcci√≥n de descenso.  Entonces la pregunta puede plantearse de esta manera: <br>  <i>¬øCu√°ndo es la direcci√≥n generada por el m√©todo de Newton la direcci√≥n de descenso?</i> <br>  Y para responderlo, tendr√° que mirar el problema de minimizaci√≥n desde otro lado. <br><br><h2>  M√©todos de descenso </h2><br>  Para el problema de minimizaci√≥n, este enfoque parece bastante natural: a partir de alg√∫n punto arbitrario, elegimos la direcci√≥n p de alguna manera y damos un paso en esa direcci√≥n. <img src="https://habrastorage.org/getpro/habr/post_images/fbf/01e/b21/fbf01eb21703831c5dd0e196a2efccc2.gif" title="&quot;\ alpha p&quot;">  .  Si <img src="https://habrastorage.org/getpro/habr/post_images/bd9/6f9/580/bd96f95806b05f65a5766db233a85653.gif" title="&quot;f (x + \ alpha p) &amp; lt; f (x)&quot;">  entonces toma <img src="https://habrastorage.org/getpro/habr/post_images/b87/e59/538/b87e59538ed10c96ec3db2e7bad8dc85.gif" title="&quot;x + \ alpha p&quot;">  como un nuevo punto de partida y repita el procedimiento.  Si la direcci√≥n se elige arbitrariamente, dicho m√©todo a veces se denomina m√©todo de caminata aleatoria.  Es posible tomar vectores base unitarios como una direcci√≥n, es decir, dar un paso en una sola coordenada, este m√©todo se llama m√©todo de descenso de coordenadas.  No hace falta decir que son ineficaces?  Para que este enfoque funcione bien, necesitamos algunas garant√≠as adicionales.  Para hacer esto, presentamos una funci√≥n auxiliar <img src="https://habrastorage.org/getpro/habr/post_images/8bf/1d5/4e1/8bf1d54e1f36dd4c9dfd5720437af51c.gif" title="&quot;g (p) = f (x + p)&quot;">  .  Creo que es obvio que la minimizaci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  completamente equivalente a minimizar <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  .  Si <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  entonces diferenciable <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  puede ser representado como <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/615/d31/e47615d310276ab67a9163889a2335a5.gif" title="&quot;g (p) = f (x) + \ bigtriangledown f ^ {T} (x) p + o (\ parallel p \ parallel ^ {2})&quot;"><br><br>  y si <img src="https://habrastorage.org/getpro/habr/post_images/2a7/342/acb/2a7342acbe0772f75af6eee281c247d0.gif" title="&quot;\ paralela p \ paralela&quot;">  lo suficientemente peque√±o entonces <img src="https://habrastorage.org/getpro/habr/post_images/2ef/8a9/23f/2ef8a923f49cf84264effb5f3f703c31.gif" title="&quot;g (p) \ aprox \ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p&quot;">  .  Ahora podemos intentar reemplazar el problema de minimizaci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/076/563/484/076563484d4e576c5c48098bfa94d45c.gif" title="&quot;g (p)&quot;">  la tarea de minimizar su aproximaci√≥n (o <i>modelo</i> ) <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  .  Por cierto, todos los m√©todos basados ‚Äã‚Äãen el uso del modelo <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  llamado gradiente  Pero el problema es que <img src="https://habrastorage.org/getpro/habr/post_images/462/957/dda/462957dda265f4fb8be04327f1c12b0f.gif" title="&quot;\ bar {g}&quot;">  Es una funci√≥n lineal y, por lo tanto, no tiene un m√≠nimo.  Para resolver este problema, agregamos una restricci√≥n en la longitud del paso que queremos dar.  En este caso, este es un requisito completamente natural, porque nuestro modelo describe m√°s o menos correctamente la funci√≥n objetivo solo en un vecindario lo suficientemente peque√±o.  Como resultado, obtenemos un problema adicional de optimizaci√≥n condicional: <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/8a2/c43/279/8a2c4327974067619cfad20b7ea1e821.gif" title="\\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ parallel p \ parallel_ {2} = \ Delta"></a> <br><br>  Esta tarea tiene una soluci√≥n obvia: <img src="https://habrastorage.org/getpro/habr/post_images/3ff/1f6/a21/3ff1f6a2117e5d9a99603bcc8fde4f69.gif" title="&quot;p = - \ beta \ bigtriangledown f (x)&quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  - factor que garantiza el cumplimiento de la restricci√≥n.  Luego, las iteraciones del m√©todo de descenso toman la forma <br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/987/c25/966987c257a50df1855a50ea363350dd.gif" title="&quot;x_ {i + 1} = x_ {i} - \ beta \ bigtriangledowndown f (x_ {i})&quot;">  , <br><br>  en el que aprendemos el conocido <b>m√©todo de descenso de gradiente</b> .  Par√°metro <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , que generalmente se denomina velocidad de descenso, ahora ha adquirido un significado comprensible, y su valor se determina a partir de la condici√≥n de que el nuevo punto se encuentre en una esfera de un radio dado, circunscrito alrededor del punto anterior. <br><br>  Con base en las propiedades del modelo construido de la funci√≥n objetivo, podemos argumentar que existe tal <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  , incluso si es muy peque√±o, ¬øy si <img src="https://habrastorage.org/getpro/habr/post_images/84b/5fd/00f/84b5fd00fe1f4ca32b7cd7bd095a1490.gif" title="&quot;\ bar {g} (p) &amp; lt; \ bar {g} (0)&quot;">  entonces <img src="https://habrastorage.org/getpro/habr/post_images/553/80b/dc5/55380bdc5a434366df6d181078d6a8b7.gif" title="&quot;g (p) &amp; lt; g (0)&quot;">  .  Es de destacar que, en este caso, la direcci√≥n en la que nos moveremos no depende del tama√±o del radio de esta esfera.  Entonces podemos elegir una de las siguientes formas: <br><br><ol><li>  Seleccione de acuerdo con alg√∫n m√©todo el valor <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  . </li><li>  Establecer la tarea de elegir el valor apropiado <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , proporcionando una disminuci√≥n en el valor de la funci√≥n objetivo. </li></ol><br>  El primer enfoque es t√≠pico para los <i>m√©todos de la regi√≥n de confianza</i> , el segundo conduce a la formulaci√≥n del problema auxiliar del llamado  <i>b√∫squeda lineal (LineSearch)</i> .  En este caso particular, las diferencias entre estos enfoques son peque√±as y no las consideraremos.  En cambio, preste atenci√≥n a lo siguiente: <br><br>  <b><i>¬øpor qu√©, de hecho, estamos buscando una compensaci√≥n</i></b> <b><i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i></b>  <b><i>acostado exactamente en la esfera?</i></b> <br><br>  De hecho, podr√≠amos reemplazar esta restricci√≥n con el requisito, por ejemplo, que p pertenezca a la superficie del cubo, es decir, <img src="https://habrastorage.org/getpro/habr/post_images/cf1/a35/92e/cf1a3592ebe97c9e262a083ea44c594c.gif" title="&quot;\ parallel p \ parallel _ {\ infty} = \ Delta&quot;">  (en este caso, no es demasiado razonable, pero ¬øpor qu√© no?) o alguna superficie el√≠ptica?  Esto ya parece bastante l√≥gico, si recordamos los problemas que surgen al minimizar las funciones de barranco.  La esencia del problema es que, en algunas l√≠neas de coordenadas, la funci√≥n cambia mucho m√°s r√°pido que en otras.  Debido a esto, obtenemos que si el incremento debe pertenecer a la esfera, entonces la cantidad <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  en el que se proporciona el "descenso" debe ser muy peque√±o.  Y esto lleva al hecho de que lograr un m√≠nimo requerir√° una gran cantidad de pasos.  Pero si, en cambio, tomamos una elipse adecuada como vecindario, entonces este problema m√°gicamente quedar√° en nada. <br><br>  Por la condici√≥n de que pertenezcan los puntos de la superficie el√≠ptica, se puede escribir como <img src="https://habrastorage.org/getpro/habr/post_images/6ff/1b4/930/6ff1b49309ec84aa656d848764359b4e.gif" title="&quot;\ parallel p \ parallel_ {B} = \ sqrt {p ^ {T} Bp} = \ Delta&quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  Es una matriz definida positiva, tambi√©n llamada m√©trica.  Norma <img src="https://habrastorage.org/getpro/habr/post_images/ab8/b42/711/ab8b42711a932f9129bdb193b6a74360.gif" title="&quot;\ parallel \ cdot \ parallel_ {B}&quot;">  llamada la norma el√≠ptica inducida por la matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  ¬øQu√© tipo de matriz es esta y de d√≥nde obtenerla? Consideraremos m√°s adelante, y ahora llegamos a una nueva tarea. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/f4c/cc0/757/f4ccc0757e09fb304ff10a9a8c4751b6.gif" title="\\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ dfrac {1} {2} \ parallel p \ parallel_ {B} ^ {2} = \ Delta"></a> <br><br>  El cuadrado de la norma y el factor 1/2 est√°n aqu√≠ √∫nicamente por conveniencia, para no interferir con las ra√≠ces.  Aplicando el m√©todo multiplicador de Lagrange, obtenemos el problema de la optimizaci√≥n incondicional. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/605/36d/d5e/60536dd5e2297580940a5b926760a3ce.gif" title="f (x) + \ bigtriangledown f ^ {T} (x) p + \ dfrac {\ lambda} {2} p ^ {T} Bp- \ lambda \ Delta \ rightarrow \ min"></a> <br><br>  Una condici√≥n necesaria para un m√≠nimo para ello es <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/4c5/f7e/921/4c5f7e921637c73fcb992c5d9b9efcd6.gif" title="\ bigtriangledown f (x) + \ lambda Bp = 0"></a>  o <img src="https://habrastorage.org/getpro/habr/post_images/a80/6c9/ff7/a806c9ff7ce22ea27c87b6a61a4c8fed.gif" title="&quot;B \ left (\ lambda p \ right) = - \ bigtriangledown f (x)&quot;">  de donde <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/b50/032/3a9/b500323a970c7ae821295450627bdad2.gif" title="p = - \ dfrac {1} {\ lambda} B ^ {- 1} \ bigtriangledown f (x) = \ dfrac {1} {\ lambda} \ bar {p}"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/697/906/5d7/6979065d729033e0093ffad8475e80a6.gif" title="\\\ dfrac {1} {\ lambda ^ {2}} \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) ^ {T} B \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} BB ^ {- 1} \ bigtriangledown f (x) = \ \ = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x) = \ Delta"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/41f/689/d89/41f689d890b92c84f05fdd0ede8a8114.gif" title="\ lambda = \ sqrt {\ dfrac {1} {\ Delta} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x)}> 0"></a> <br><br>  De nuevo vemos que la direcci√≥n <img src="https://habrastorage.org/getpro/habr/post_images/45b/686/bb0/45b686bb0219a74b212cfdeaf1998653.gif" title="&quot;\ bar {p} = - B ^ {- 1} \ bigtriangledown f (x)&quot;">  , en el que nos moveremos, no depende del valor <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  - solo de la matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Y de nuevo, podemos recoger <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  eso est√° plagado de la necesidad de calcular <img src="https://habrastorage.org/getpro/habr/post_images/99d/394/e7d/99d394e7d0b74248114405067e0ffd51.gif" title="&quot;\ lambda&quot;">  y la inversi√≥n expl√≠cita de la matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  o resolver el problema auxiliar de encontrar un sesgo adecuado <img src="https://habrastorage.org/getpro/habr/post_images/b7b/6a7/371/b7b6a73716dc8f4e40a52c1c5ef0e6b4.gif" title="&quot;x_ {i + 1} = x_ {i} + \ beta \ bar {p} _ {i}&quot;">  .  Desde <img src="https://habrastorage.org/getpro/habr/post_images/0b8/52f/d1b/0b852fd1bbc20f2966bf757a56186312.gif" title="&quot;\ lambda &amp; gt; 0&quot;">  , se garantiza que la soluci√≥n a este problema auxiliar existe. <br><br>  Entonces, ¬øqu√© deber√≠a ser para la matriz B?  Nos restringimos a las ideas especulativas.  Si la funci√≥n objetivo <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  - cuadr√°tico, es decir, tiene la forma <img src="https://habrastorage.org/getpro/habr/post_images/974/f7f/cf6/974f7fcf6345b91ef8466f2cabba6efe.gif" title="&quot;f (x) = a + b ^ {T} x + x ^ {T} Hx&quot;">  donde <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  positiva definida, es obvio que el mejor candidato para el papel de la matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  es arpillera <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  , ya que en este caso se requiere una iteraci√≥n del m√©todo de descenso que hemos construido.  Si H no es definitivo positivo, entonces no puede ser una m√©trica, y las iteraciones construidas con √©l son iteraciones del m√©todo de Newton amortiguado, pero no son iteraciones del m√©todo de descenso.  Finalmente, podemos dar una respuesta rigurosa a <br><br>  <b>Pregunta:</b> <i>¬øLa matriz de Hesse en el m√©todo de Newton tiene que ser positiva definida?</i> <br>  <b>Respuesta:</b> <i>no, no se requiere ni en el m√©todo est√°ndar ni en el m√©todo de Newton amortiguado.</i>  <i>Pero si se cumple esta condici√≥n, entonces el m√©todo de Newton amortiguado es un m√©todo de descenso y tiene la propiedad de convergencia <i>global</i> y no solo local.</i> <br><br>  A modo de ilustraci√≥n, veamos c√≥mo se ven las regiones de confianza al minimizar la conocida funci√≥n de Rosenbrock utilizando el descenso de gradiente y los m√©todos de Newton, y c√≥mo la forma de las regiones afecta la convergencia del proceso. <br><br><img src="https://habrastorage.org/webt/_x/30/nx/_x30nxs-eyrixuan0-diyvwixww.gif" width="600"><br><br>  As√≠ es como el m√©todo de descenso se comporta con una regi√≥n de confianza esf√©rica, tambi√©n es un descenso de gradiente.  Todo es como un libro de texto: estamos atrapados en un ca√±√≥n. <br><br><img src="https://habrastorage.org/webt/9x/ik/td/9xiktd4lapdka-uk010evfvlcdm.gif" width="600"><br><br>  Y esto lo obtenemos si la regi√≥n de confianza tiene la forma de una elipse definida por la matriz de Hesse.  Esto no es m√°s que una iteraci√≥n del m√©todo de Newton amortiguado. <br><br>  Solo queda por resolver la cuesti√≥n de qu√© hacer si la matriz de Hesse no es definitiva.  Hay muchas opciones  El primero es anotar.  Tal vez tengas suerte y las iteraciones de Newton converjan sin esta propiedad.  Esto es bastante real, especialmente en las etapas finales del proceso de minimizaci√≥n, cuando ya est√° lo suficientemente cerca de una soluci√≥n.  En este caso, las iteraciones del m√©todo est√°ndar de Newton se pueden usar sin molestarse con la b√∫squeda de un vecindario admisible para el descenso.  O utilice iteraciones del m√©todo de Newton amortiguado en el caso de <img src="https://habrastorage.org/getpro/habr/post_images/6da/2c0/bc5/6da2c0bc54434a64d7630c142d0c7bf9.gif" title="&quot;\ beta = 0&quot;">  , es decir, en el caso de que la direcci√≥n obtenida no sea la direcci√≥n de descenso, c√°mbiela, por ejemplo, a un anti-gradiente.  <i>¬°Simplemente no tiene que verificar expl√≠citamente si el Hessian es positivo definido de acuerdo con el criterio de Sylvester</i> , como se hizo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠!</a>  .  Es derrochador y sin sentido. <br>  Los m√©todos m√°s sutiles implican la construcci√≥n de una matriz, en un sentido cercano a la matriz de Hesse, pero que posee la propiedad de definici√≥n positiva, en particular, corrigiendo valores propios.  Un tema separado son los m√©todos cuasi-newtonianos, o m√©todos m√©tricos variables, que garantizan la definici√≥n positiva de la matriz B y no requieren el c√°lculo de las segundas derivadas.  En general, una discusi√≥n detallada de estos temas va mucho m√°s all√° del alcance de este art√≠culo. <br><br>  S√≠, y por cierto, se deduce de lo que se ha dicho que <i>el m√©todo amortiguado de Newton con una definici√≥n positiva del hessiano es un m√©todo de gradiente</i> .  As√≠ como los m√©todos cuasi-newtonianos.  Y muchos otros, basados ‚Äã‚Äãen una elecci√≥n separada de direcci√≥n y tama√±o de paso.  Por lo tanto, contrastar el m√©todo de Newton con la terminolog√≠a de gradiente es incorrecto. <br><br><h2>  Para resumir </h2><br>  El m√©todo de Newton, que a menudo se recuerda cuando se discuten los m√©todos de minimizaci√≥n, generalmente no es el m√©todo de Newton en su sentido cl√°sico, sino el m√©todo de descenso con la m√©trica especificada por el hessiano de la funci√≥n objetivo.  Y s√≠, converge globalmente si el hessiano es positivo en todas partes.  Esto es posible solo para funciones convexas, que son mucho menos comunes en la pr√°ctica de lo que nos gustar√≠a, por lo que, en el caso general, sin las modificaciones apropiadas, la aplicaci√≥n del m√©todo de Newton (no nos separaremos del colectivo y continuaremos llam√°ndolo as√≠) no garantiza el resultado correcto.  El aprendizaje de redes neuronales, incluso las superficiales, generalmente conduce a problemas de optimizaci√≥n no convexos con muchos m√≠nimos locales.  Y aqu√≠ hay una nueva emboscada.  El m√©todo de Newton generalmente converge (si converge) r√°pidamente.  Me refiero a muy r√°pido.  Y esto, curiosamente, es malo, porque llegamos a un m√≠nimo local en varias iteraciones.  Y para funciones con terreno complejo puede ser mucho peor que el global.  El descenso de gradiente con b√∫squeda lineal converge mucho m√°s lentamente, pero es m√°s probable que se "salte" las crestas de la funci√≥n objetivo, lo cual es muy importante en las primeras etapas de minimizaci√≥n.  Si ya ha reducido bien el valor de la funci√≥n objetivo, y la convergencia del descenso del gradiente ha disminuido significativamente, entonces un cambio en la m√©trica puede acelerar el proceso, pero esto es para las etapas finales. <br><br>  Por supuesto, este argumento no es universal, no es indiscutible y, en algunos casos, incluso incorrecto.  Adem√°s de la afirmaci√≥n de que los m√©todos de gradiente funcionan mejor en los problemas de aprendizaje. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/469877/">https://habr.com/ru/post/469877/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../469865/index.html">700 empleados y m√∫ltiples continentes: c√≥mo Alconost construy√≥ un modelo de negocio sin oficina</a></li>
<li><a href="../469867/index.html">C√≥mo se convierte en un ni√±o de escuela (y qu√© tienen que ver los juegos de mesa con √©l)</a></li>
<li><a href="../469869/index.html">Por qu√© deber√≠as overclockear RAM (¬°es f√°cil!)</a></li>
<li><a href="../469871/index.html">Cuando los teclados eran tablas</a></li>
<li><a href="../469875/index.html">C√≥mo proteger tus contrase√±as en 2019</a></li>
<li><a href="../469879/index.html">Doble VPN en un clic. C√≥mo dividir f√°cilmente la direcci√≥n IP de un punto de entrada y salida</a></li>
<li><a href="../469881/index.html">Los primeros tres d√≠as de la vida de un post en Habr√©</a></li>
<li><a href="../469885/index.html">Deshabilite la consola local cuando use x11vnc</a></li>
<li><a href="../469889/index.html">SamsPcbGuide, parte 12: Tecnolog√≠a - Cajas tipo BGA, pl√°stico y espacio II</a></li>
<li><a href="../469893/index.html">Casa con elementos de alta tecnolog√≠a para un gato sin hogar.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>