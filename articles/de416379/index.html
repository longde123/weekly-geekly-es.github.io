<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò° ‚úãüèª ü•† Neurobugurt. Wie wir dem neuronalen Netzwerk beigebracht haben, Meme ein Jahr fr√ºher als Stanford zu erfinden üë∑üèº üê© üíß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Diese Nachricht (+ Forschung ) √ºber die Erfindung des Meme-Generators durch Wissenschaftler der Stanford University veranlasste mich, einen Artikel zu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neurobugurt. Wie wir dem neuronalen Netzwerk beigebracht haben, Meme ein Jahr fr√ºher als Stanford zu erfinden</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416379/"> Diese <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachricht</a> (+ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Forschung</a> ) √ºber die Erfindung des Meme-Generators durch Wissenschaftler der Stanford University veranlasste mich, einen Artikel zu schreiben.  In meinem Artikel werde ich versuchen zu zeigen, dass Sie kein Stanford-Wissenschaftler sein m√ºssen, um interessante Dinge mit neuronalen Netzen zu tun.  In dem Artikel beschreibe ich, wie wir 2017 ein neuronales Netzwerk auf einem K√∂rper von ungef√§hr 30.000 Texten trainiert und es gezwungen haben, neue Internet-Memes und Memes (Kommunikationszeichen) im soziologischen Sinne des Wortes zu generieren.  Wir beschreiben den von uns verwendeten Algorithmus f√ºr maschinelles Lernen sowie die technischen und administrativen Schwierigkeiten, auf die wir gesto√üen sind. <br><a name="habracut"></a><br>  Ein kleiner Hintergrund dar√ºber, wie wir auf die Idee eines Neuroautors gekommen sind und woraus sie genau bestand.  Im Jahr 2017 haben wir ein Projekt f√ºr eine √∂ffentliche Vkontakte-Website erstellt, deren Namen und Screenshots die Habrahabr-Moderatoren nicht ver√∂ffentlichen durften, da sie als "Selbst" -PR erw√§hnt wurden.  Public existiert seit 2013 und vereint Posts mit der allgemeinen Idee, Humor durch eine Linie zu zerlegen und die Linien durch das Symbol ‚Äû@‚Äú zu trennen: <br><br> <code> <br> @ <br>   <br> @ <br> </code> <br> <br>  Die Anzahl der Linien kann variieren, die Darstellung kann beliebig sein.  Meistens sind dies Humor oder scharfe soziale Notizen √ºber die weit verbreiteten Tatsachen der Realit√§t.  Im Allgemeinen wird dieses Design "buhurt" genannt. <br><br><img src="https://habrastorage.org/webt/g9/bb/4n/g9bb4nvtkhh2hkx7wybuutrzr3u.png" alt="Bild"><br><br>  <i>Einer der typischen Buhurts</i> <br><br>  Im Laufe der Jahre hat sich die √ñffentlichkeit zu einer internen √úberlieferung (Charaktere, Handlungen, Orte) entwickelt, und die Anzahl der Beitr√§ge hat 30.000 √ºberschritten. Zum Zeitpunkt ihrer Analyse f√ºr die Anforderungen des Projekts betrug die Anzahl der Quellzeilen des Textes mehr als eine halbe Million. <br><br><h3>  Teil 0. Die Entstehung von Ideen und Teams </h3><br>  Im Zuge der gro√üen Beliebtheit neuronaler Netze lag die Idee, ANN in unseren Texten zu trainieren, etwa sechs Monate in der Luft, wurde jedoch im Dezember 2016 schlie√ülich mit E7su formuliert. Gleichzeitig wurde der Name erfunden (‚ÄûNeurobugurt‚Äú).  Zu diesem Zeitpunkt bestand das an dem Projekt interessierte Team nur aus drei Personen.  Wir waren alle Studenten ohne praktische Erfahrung in Algorithmen und neuronalen Netzen.  Schlimmer noch, wir hatten nicht einmal eine einzige geeignete GPU f√ºr das Training.  Wir hatten nur Begeisterung und Zuversicht, dass diese Geschichte interessant sein k√∂nnte. <br><br><h3>  Teil 1. Die Formulierung der Hypothese und Aufgaben </h3><br>  Unsere Hypothese stellte sich als die Annahme heraus, dass Sie, wenn Sie alle √ºber dreieinhalb Jahre ver√∂ffentlichten Texte mischen und das neuronale Netzwerk in diesem Geb√§ude trainieren, Folgendes erhalten k√∂nnen: <br><br>  a) kreativer als Menschen <br>  b) lustig <br><br>  Selbst wenn sich herausstellen sollte, dass die W√∂rter oder Buchstaben im Buhurt maschinell verwirrt und zuf√§llig angeordnet sind, waren wir der Meinung, dass dies als Fan-Service funktionieren k√∂nnte und den Lesern dennoch gefallen w√ºrde. <br><br>  Die Aufgabe wurde erheblich vereinfacht, da das Format der Buhurts im Wesentlichen textuell ist.  Wir mussten uns also nicht auf Bildverarbeitung und andere komplexe Dinge einlassen.  Eine weitere gute Nachricht war, dass der gesamte Textk√∂rper sehr √§hnlich ist.  Dies erm√∂glichte es, zumindest in den fr√ºhen Stadien kein verst√§rktes Lernen einzusetzen.  Gleichzeitig haben wir klar verstanden, dass es nicht so einfach ist, einen neuronalen Netzwerkschreiber mit mehr als einmal lesbarer Ausgabe zu erstellen.  Das Risiko, ein Monster zur Welt zu bringen, das zuf√§llig Buchstaben schleudert, war sehr gro√ü. <br><br><h3>  Teil 2. Vorbereitung des Textk√∂rpers </h3><br>  Es wird angenommen, dass die Vorbereitungsphase sehr lange dauern kann, da sie mit der Erfassung und Bereinigung von Daten verbunden ist.  In unserem Fall stellte sich heraus, dass es ziemlich kurz war: Es wurde ein kleiner <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parser</a> geschrieben, der ungef√§hr 30.000 Posts aus der Community-Wand <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">herauspumpte</a> und sie in eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">txt-Datei legte</a> . <br><br>  Wir haben die Daten vor dem ersten Training nicht gel√∂scht.  In Zukunft war dies ein grausamer Witz f√ºr uns, da wir aufgrund des Fehlers, der sich zu diesem Zeitpunkt eingeschlichen hatte, die Ergebnisse lange Zeit nicht in lesbare Form bringen konnten.  Aber dazu sp√§ter mehr. <br><br><img src="https://habrastorage.org/webt/jt/9v/uq/jt9vuqk2tdpoi7ycus5udn-lnpo.png" alt="Bild"><br><br>  <i>Bildschirmdatei mit Burgern</i> <br><br><h3>  Teil 3. Ank√ºndigung, Verfeinerung der Hypothese, Wahl des Algorithmus </h3><br>  Wir haben eine zug√§ngliche Ressource verwendet - eine gro√üe Anzahl √∂ffentlicher Abonnenten.  Die Annahme war, dass es unter 300.000 Lesern mehrere Enthusiasten gibt, die neuronale Netze auf einem ausreichenden Niveau besitzen, um die Wissensl√ºcken unseres Teams zu schlie√üen.  Wir gingen von der Idee aus, den Wettbewerb umfassend anzuk√ºndigen und Enthusiasten des maschinellen Lernens f√ºr die Diskussion des formulierten Problems zu gewinnen.  Nachdem wir die Texte geschrieben hatten, erz√§hlten wir den Leuten von unserer Idee und hofften auf eine Antwort. <br><br><img src="https://habrastorage.org/webt/73/0b/jo/730bjoajceycgzoynywra6xcupy.png" alt="Bild"><br><br>  <i>Ank√ºndigung einer thematischen Diskussion</i> <br><br>  Die Reaktion der Menschen hat unsere wildesten Erwartungen √ºbertroffen.  Die Diskussion √ºber die Tatsache, dass wir ein neuronales Netzwerk trainieren werden, verbreitete den Holivar um fast 1000 Kommentare.  Die meisten Leser verblassten einfach und versuchten sich vorzustellen, wie das Ergebnis aussehen w√ºrde.  Ungef√§hr 6.000 Menschen schauten in die thematische Diskussion ein, und mehr als 50 interessierte Amateure hinterlie√üen Kommentare, f√ºr die wir einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Testsatz</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">814 Buhurt-Linien</a> f√ºr die Durchf√ºhrung von ersten Tests und Schulungen gaben.  Jede interessierte Person k√∂nnte einen Datensatz nehmen und den f√ºr sie interessantesten Algorithmus lernen und dann mit uns und anderen Enthusiasten diskutieren.  Wir haben im Voraus angek√ºndigt, dass wir weiterhin mit den Teilnehmern zusammenarbeiten werden, deren Ergebnisse am besten lesbar sind. <br><br>  Die Arbeit begann: Jemand baute stillschweigend einen Generator an Markov-Ketten zusammen, jemand versuchte verschiedene Implementierungen mit einem Github und die meisten wurden in der Diskussion einfach verr√ºckt und √ºberzeugten uns mit Schaum im Mund, dass nichts daraus werden w√ºrde.  Damit begann der technische Teil des Projekts. <br><br><img src="https://habrastorage.org/webt/ie/cu/tz/iecutzyxvv__0a2qzo5ailsjk9s.png" alt="Bild"><br><br>  Einige Vorschl√§ge von Enthusiasten <br><br>  Die Leute boten Dutzende von Optionen f√ºr die Implementierung an: <br><br><ul><li>  Markov-Ketten. </li><li>  Finden Sie eine vorgefertigte Implementierung von etwas √§hnlichem wie GitHub und trainieren Sie es. </li><li>  Ein in Pascal geschriebener Zufallsgenerator. </li><li>  Holen Sie sich einen literarischen Neger, der zuf√§lligen Unsinn schreibt, und wir werden dies als neuronale Netzwerkausgabe ausgeben. </li></ul><br><img src="https://habrastorage.org/webt/ki/tr/ut/kitrutzq2yxrtm_weg1ngm4beow.png" alt="Bild"><br><br>  <i>Bewertung der Komplexit√§t des Projekts von einem der Abonnenten</i> <br><br>  Die meisten Kommentatoren waren sich einig, dass unser Projekt zum Scheitern verurteilt ist und wir nicht einmal das Prototypenstadium erreichen werden.  Wie wir sp√§ter verstanden haben, neigen die Menschen immer noch dazu, neuronale Netze als eine Art schwarze Magie wahrzunehmen, die im ‚ÄûKopf von Zuckerberg‚Äú und in geheimen Abteilungen von Google vorkommt. <br><br><h3>  Teil 4. Algorithmusauswahl, Training und Teamerweiterung </h3><br>  Nach einiger Zeit begann die Kampagne, die wir f√ºr Crowdsourcing-Ideen f√ºr den Algorithmus gestartet hatten, erste Fr√ºchte zu tragen.  Wir haben ungef√§hr 30 funktionierende Prototypen, von denen die meisten v√∂llig unlesbaren Unsinn gaben. <br><br>  Zu diesem Zeitpunkt stie√üen wir zun√§chst auf eine Demotivation des Teams.  Alle Ergebnisse waren buhurts sehr schwach √§hnlich und stellten am h√§ufigsten Abrakadabra aus Buchstaben und Symbolen dar.  Die Arbeit von Dutzenden von Enthusiasten ging zu Staub und dies demotivierte sowohl sie als auch uns. <br><br>  Der pyTorch-basierte Algorithmus zeigte sich besser als andere.  Es wurde beschlossen, diese Implementierung und den LSTM-Algorithmus als Grundlage zu nehmen.  Wir haben den Abonnenten, der es vorgeschlagen hat, als Gewinner erkannt und gemeinsam mit ihm an der Verbesserung des Algorithmus gearbeitet.  Unser verteiltes Team ist auf vier Personen angewachsen.  Die lustige Tatsache hier ist, dass der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gewinner des Wettbewerbs</a> , wie sich herausstellte, erst 16 Jahre alt war.  Der Sieg war sein erster wirklicher Preis auf dem Gebiet der Datenwissenschaft. <br><br>  F√ºr das erste Training wurde ein Cluster von 8 Grafikkarten GXT1080 gemietet. <br><br><img src="https://habrastorage.org/webt/mb/_l/ql/mb_lqlodsgm8fyztu36ly0soqo4.jpeg" alt="Bild"><br><br>  <i>Kartencluster-Verwaltungskonsole</i> <br><br>  Das Original-Repository und alle Torch-rnn-Projekthandb√ºcher finden Sie hier: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/jcjohnson/torch-rnn</a> .  Auf dieser Grundlage haben wir sp√§ter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unser Repository ver√∂ffentlicht</a> , in dem sich unsere Quellen, ReadMe f√ºr die Installation, sowie die fertigen Neurobugurts selbst befinden. <br><br>  Die ersten Male haben wir mit einer vorkonfigurierten Konfiguration auf einem kostenpflichtigen GPU-Cluster trainiert.  Das Einrichten stellte sich als nicht so schwierig heraus - nur Anweisungen des Torch-Entwicklers und Hilfe der Hosting-Administration, die in der Zahlung enthalten ist, reichen aus. <br><br>  Sehr schnell stie√üen wir jedoch auf Schwierigkeiten: Jedes Training kostete die GPU-Mietzeit - was bedeutet, dass das Projekt einfach kein Geld enthielt.  Aus diesem Grund haben wir von Januar bis Februar 2017 Schulungen in den gekauften Einrichtungen durchgef√ºhrt und versucht, die Erzeugung auf unseren lokalen Maschinen zu starten. <br><br>  Jeder Text ist f√ºr das Modelltraining geeignet.  Vor dem Training m√ºssen Sie es vorverarbeiten, f√ºr das Torch einen speziellen preprocess.py-Algorithmus hat, der Ihre my_data.txt in zwei Dateien konvertiert: HDF5 und JSON: <br><br>  Das Vorverarbeitungsskript l√§uft folgenderma√üen ab: <br><br><pre> <code class="python hljs">python scripts/preprocess.py \ --input_txt my_data.txt \ --output_h5 my_data.h5 \ --output_json my_data.json</code> </pre> <br><img src="https://habrastorage.org/webt/8u/s8/kj/8us8kjppqy-1wqhpjp87hdiwqyi.png" alt="Bild"><br><br>  <i>Nach der Vorverarbeitung erscheinen zwei Dateien, in denen das neuronale Netzwerk in Zukunft trainiert wird</i> <br><br>  Hier werden die verschiedenen Flags beschrieben, die in der Vorverarbeitungsphase ge√§ndert <a href="">werden k√∂nnen</a> .  Es ist auch m√∂glich, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Torch von Docker aus</a> auszuf√ºhren, aber der Autor des Artikels hat es nicht √ºberpr√ºft. <br><br><h4>  Neuronales Netzwerktraining </h4><br>  Nach der Vorverarbeitung k√∂nnen Sie mit dem Training des Modells fortfahren.  In dem Ordner mit HDF5 und JSON m√ºssen Sie das Dienstprogramm th ausf√ºhren, das bei korrekter Installation von Torch mit Ihnen angezeigt wurde: <br><br><pre> <code class="python hljs">th train.lua -input_h5 my_data.h5 -input_json my_data.json</code> </pre> <br>  Das Training nimmt viel Zeit in Anspruch und generiert Dateien in der Form cv / checkpoint_1000.t7, die die ‚ÄûGewichte‚Äú unseres neuronalen Netzwerks darstellen.  Diese Dateien wiegen beeindruckend viele Megabyte und enthalten die St√§rke der Verkn√ºpfungen zwischen bestimmten Buchstaben in Ihrem Originaldatensatz. <br><br><img src="https://habrastorage.org/webt/qt/ts/it/qttsit4vulbqojnmxfcfdh1pdzm.png" alt="Bild"><br><br>  <i>Ein neuronales Netzwerk wird oft mit dem menschlichen Gehirn verglichen, aber es scheint mir eine viel klarere Analogie zu einer mathematischen Funktion zu sein, die Parameter am Eingang (Ihren Datensatz) verwendet und das Ergebnis (neue Daten) am Ausgang liefert.</i> <br><br>  In unserem Fall dauerte jedes Training auf einem Cluster von 8 GTX 1080 in einem Datensatz von 500.000 Zeilen ungef√§hr ein oder zwei Stunden, und ein √§hnliches Training auf einer Art CPU i3-2120 dauerte ungef√§hr 80-100 Stunden.  Bei l√§ngerem Training begann sich das neuronale Netzwerk starr neu zu trainieren - die Symbole wiederholten sich zu oft und fielen in lange Zyklen von Pr√§positionen, Konjunktionen und einleitenden W√∂rtern. <br><br>  Es ist praktisch, dass Sie die H√§ufigkeit der Checkpoints ausw√§hlen k√∂nnen. W√§hrend eines Trainings erhalten Sie sofort viele Modelle: vom am wenigsten trainierten (checkpoint_1000) bis zum umgeschulten (checkpoint_1000000).  Nur genug Platz w√§re genug. <br><br><h4>  Neue Texterzeugung </h4><br>  Nachdem Sie mindestens eine vorgefertigte Datei mit Gewichten erhalten haben (Pr√ºfpunkt _ *******), k√∂nnen Sie mit der n√§chsten und interessantesten Phase fortfahren: Beginnen Sie mit der Generierung von Texten.  F√ºr uns war es ein echter Moment der Wahrheit, denn zum ersten Mal haben wir ein greifbares Ergebnis erzielt - einen Bugurt, der von einer Maschine geschrieben wurde. <br><br>  Zu diesem Zeitpunkt haben wir die Verwendung des Clusters endg√ºltig eingestellt und alle Generationen wurden auf unseren Maschinen mit geringem Stromverbrauch ausgef√ºhrt.  Beim Versuch, lokal zu starten, ist es uns jedoch nicht gelungen, die Anweisungen zu befolgen und Torch zu installieren.  Das erste Hindernis war der Einsatz virtueller Maschinen.  Unter virtuellem Ubuntu 16 hebt der Stick nicht ab - vergessen Sie es.  StackOverflow kam oft zur Rettung, aber einige Fehler waren so trivial, dass die Antwort nur schwer zu finden war. <br><br>  Die Installation von Torch auf einem lokalen Computer hat das Projekt f√ºr ein paar Wochen blockiert: Wir haben alle Arten von Fehlern bei der Installation zahlreicher erforderlicher Pakete festgestellt, wir hatten auch Probleme mit der Virtualisierung (virtualenv .env) und haben sie schlie√ülich nicht verwendet.  Mehrmals wurde der Stand auf das Niveau von sudo rm -rf abgerissen und einfach wieder installiert. <br><br>  Mit der resultierenden Datei mit Gewichten konnten wir beginnen, Texte auf unserem lokalen Computer zu generieren: <br><br><img src="https://habrastorage.org/webt/9z/ve/r_/9zver_8vvx-fknpdj_9-id70hfe.jpeg" alt="Bild"><br><br>  <i>Eine der ersten Schlussfolgerungen</i> <br><br><h3>  Teil 5. Texte l√∂schen </h3><br>  Eine weitere offensichtliche Schwierigkeit bestand darin, dass das Thema der Beitr√§ge sehr unterschiedlich ist und unser Algorithmus keine Unterteilung beinhaltet und alle 500.000 Zeilen als einen einzigen Text betrachtet.  Wir haben verschiedene Optionen f√ºr das Clustering des Datensatzes in Betracht gezogen und waren sogar bereit, den Textk√∂rper manuell nach Themen- oder Platzierungs-Tags in mehreren tausend Buhurts aufzuteilen (hierf√ºr war eine notwendige Personalressource erforderlich), hatten jedoch beim Erlernen von LSTM st√§ndig technische Schwierigkeiten beim Einreichen von Clustern.  Eine √Ñnderung des Algorithmus und eine erneute Durchf√ºhrung des Wettbewerbs schienen im Hinblick auf das Timing des Projekts und die Motivation der Teilnehmer nicht die sinnvollste Idee zu sein. <br><br>  Es schien, als w√§ren wir in einer Sackgasse - wir konnten keine Buhurts gruppieren, und das Training an einem einzigen riesigen Datensatz f√ºhrte zu zweifelhaften Ergebnissen.  Ich wollte keinen Schritt zur√ºcktreten und den fast rasanten Algorithmus und die Implementierung √§ndern - das Projekt k√∂nnte einfach ins Koma fallen.  Das Team hatte verzweifelt nicht genug Wissen, um die Situation normal zu l√∂sen, aber der gute alte KMU-KAL-OCHK-A kam zur Rettung.  Die endg√ºltige L√∂sung f√ºr die <s>Kr√ºcke</s> erwies sich als genial einfach: Trennen Sie im urspr√ºnglichen Datensatz die vorhandenen Buhurts mit leeren Zeilen voneinander und trainieren Sie LSTM erneut. <br><br>  Wir haben die Beats nach jedem Buhurt in 10 vertikalen R√§umen angeordnet, das Training wiederholt und w√§hrend der Generierung eine Grenze f√ºr das Ausgabevolumen von 500 Zeichen festgelegt (die durchschnittliche L√§nge eines Buhurt-Plots im Originaldatensatz). <br><br><img src="https://habrastorage.org/webt/da/2h/x4/da2hx4k9shkumjvxwickcnjfwnu.png" alt="Bild"><br><br>  <i>Wie es war.</i>  <i>Die Intervalle zwischen den Texten sind minimal.</i> <br><br><img src="https://habrastorage.org/webt/bb/y8/i5/bby8i5skh4u0trqrs94ppe2fal0.png" alt="Bild"><br><br>  <i>Wie ist es geworden?</i>  <i>Intervalle von 10 Zeilen lassen LSTM ‚Äûverstehen‚Äú, dass ein Bogurt vorbei ist und ein anderer begonnen hat.</i> <br><br>  Auf diese Weise konnte erreicht werden, dass etwa 60% aller erzeugten Buhurts von Anfang bis Ende eine lesbare (wenn auch oft sehr wahnhafte) Darstellung √ºber die gesamte L√§nge des Buhurts aufwiesen.  Die L√§nge einer Parzelle betrug durchschnittlich 9 bis 13 Linien. <br><br><h3>  Teil 6. Umschulung </h3><br>  Nachdem wir die Wirtschaftlichkeit des Projekts gesch√§tzt hatten, beschlossen wir, kein Geld mehr f√ºr die Anmietung eines Clusters auszugeben, sondern in den Kauf eigener Karten zu investieren.  Die Lernzeit w√ºrde sich erh√∂hen, aber wenn wir einmal eine Karte gekauft haben, k√∂nnen wir st√§ndig neue Buurts generieren.  Gleichzeitig war es oft nicht mehr n√∂tig, Schulungen durchzuf√ºhren. <br><br><img src="https://habrastorage.org/webt/rr/_e/92/rr_e92il8tdd5ckjdglm7fy8cuy.jpeg" alt="Bild"><br><br>  <i>Kampfeinstellungen auf dem lokalen Computer</i> <br><br><h3>  Teil 7. Ergebnisse ausgleichen </h3><br>  Von M√§rz bis April 2017 haben wir das neuronale Netz neu trainiert und dabei die Temperaturparameter und die Anzahl der Trainingszeiten angegeben.  Infolgedessen hat sich die Qualit√§t der Ausgabe leicht erh√∂ht. <br><br><img src="https://habrastorage.org/webt/ew/q8/qu/ewq8quy68ixemttz67jkmqeqvgw.png" alt="Bild"><br><br>  <i>Fackel-Lerngeschwindigkeit im Vergleich zu Charakter</i> <br><br>  Wir haben beide mit Torch gelieferten Algorithmen getestet: rnn und LSTM.  Der zweite erwies sich als besser. <br><br><h3>  Teil 8. Was haben wir erreicht? </h3><br>  Der erste Neurobugurt wurde am 17. Januar 2017 ver√∂ffentlicht - unmittelbar nach dem Training im Cluster - und am ersten Tag wurden mehr als 1000 Kommentare gesammelt. <br><br><img src="https://habrastorage.org/webt/t2/ng/i5/t2ngi5591hhqz7fh0n_snditsuo.png" alt="Bild"><br><br>  <i>Einer der ersten Neurobugurts</i> <br><br>  Neurobugurts erreichten das Publikum so gut, dass sie zu einer separaten Sektion wurden, die das ganze Jahr √ºber unter dem Hashtag # neurobugurt herauskam und Abonnenten am√ºsierte.  Insgesamt haben wir 2017 und Anfang 2018 mehr als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">18.000 Neurobugurts</a> mit durchschnittlich jeweils 500 Zeichen generiert.  Dar√ºber hinaus erschien eine ganze Bewegung √∂ffentlicher Parodien, deren Teilnehmer Neurobuguren darstellten und Phrasen an bestimmten Stellen zuf√§llig neu anordneten. <br><br><img src="https://habrastorage.org/webt/ls/k2/jb/lsk2jbbloh3e7sg-1p-04k5muvq.jpeg" alt="Bild"><br><br><h3>  Teil 9. Anstelle einer Schlussfolgerung </h3><br>  Mit diesem Artikel wollte ich zeigen, dass diese Trauer kein Problem ist, auch wenn Sie keine Erfahrung mit neuronalen Netzen haben.  Sie m√ºssen nicht bei Stanford arbeiten, um einfache, aber interessante Dinge mit neuronalen Netzen zu tun.  Alle Teilnehmer an unserem Projekt waren normale Studenten mit ihren aktuellen Aufgaben, Diplomen und Arbeiten, aber die gemeinsame Sache erm√∂glichte es uns, das Projekt ins Finale zu bringen.  Dank der durchdachten Idee, Planung und Energie der Teilnehmer konnten wir die ersten vern√ºnftigen Ergebnisse in weniger als einem Monat nach der endg√ºltigen Formulierung der Idee erzielen (der gr√∂√üte Teil der technischen und organisatorischen Arbeit fiel in die Winterferien 2017). <br><br><img src="https://habrastorage.org/webt/pu/q2/yj/puq2yjjiq0ynj4ioncm_wn_obrm.png" alt="Bild"><br><br>  <i>√úber 18.000 maschinengenerierte Buhurts</i> <br><br>  Ich hoffe, dieser Artikel hilft jemandem, sein eigenes ehrgeiziges Projekt mit neuronalen Netzen zu planen.  Ich bitte nicht streng zu urteilen, da dies mein erster Artikel √ºber Habr√© ist.  Wenn Sie, wie ich, ein ML-Enthusiast sind, lassen Sie uns <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Freunde sein</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416379/">https://habr.com/ru/post/de416379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416367/index.html">Wir starten ReactOS mit BTRFS des Abschnitts</a></li>
<li><a href="../de416369/index.html">Fast kompliziert. Teil 2, Schaffung eines drahtlosen ‚ÄûSmart Home‚Äú. Basierend auf Linux-Technologie, Z-Wave- und MajorDoMo-Software</a></li>
<li><a href="../de416371/index.html">Analoges Campinglicht</a></li>
<li><a href="../de416375/index.html">JavaScript-Grundlagen f√ºr Anf√§nger</a></li>
<li><a href="../de416377/index.html">Wir werden zu Assistenten in der Programmierung. Teil 1</a></li>
<li><a href="../de416381/index.html">Bericht des Club of Rome 2018, Kapitel 3.13: Philanthropie, Investition, Crowdsourcing und Blockchain</a></li>
<li><a href="../de416385/index.html">Wenn die Korrelation zu 100% herauskommt, hat sich irgendwo ein Fehler eingeschlichen: die Praktikumserfahrung bei der Rambler Group</a></li>
<li><a href="../de416387/index.html">Shrimp: Skalieren und teilen Sie HTTP-Bilder in modernem C ++ mit ImageMagic ++, SObjectizer und RESTinio</a></li>
<li><a href="../de416391/index.html">Optimierung der Platzierung virtueller Maschinen auf Servern</a></li>
<li><a href="../de416393/index.html">IIDF-Konferenz: Unternehmen sind nicht gegen Startups</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>