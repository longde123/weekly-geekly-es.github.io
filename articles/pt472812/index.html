<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõë üçö üë©üèΩ‚Äçü§ù‚Äçüë®üèæ O servidor "extingue" se o teste de fuma√ßa do data center "pegar fogo"? üêã üë©üèø‚Äçüé® ü§üüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O que voc√™ sentiria se, em um belo dia de ver√£o, um data center com seu equipamento se pareceria com isso? 



 Ol√° pessoal! Meu nome √© Dmitry Samsono...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O servidor "extingue" se o teste de fuma√ßa do data center "pegar fogo"?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/472812/">  O que voc√™ sentiria se, em um belo dia de ver√£o, um data center com seu equipamento se pareceria com isso? <br><br><img src="https://habrastorage.org/webt/b4/5i/by/b45ibyn7ljfqivfhmjhqb-hyjmq.jpeg"><br><br>  Ol√° pessoal!  Meu nome √© Dmitry Samsonov, trabalho como administrador de sistemas l√≠der em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Odnoklassniki</a> .  A foto mostra um dos quatro data centers em que o equipamento que atende ao nosso projeto est√° instalado.  Por tr√°s dessas paredes existem cerca de 4 mil unidades de equipamentos: servidores, sistema de armazenamento de dados, equipamentos de rede, etc.  - quase ‚Öì de todos os nossos equipamentos. <br>  A maioria dos servidores √© Linux.  Existem v√°rias dezenas de servidores Windows (MS SQL) - nosso legado, que recusamos sistematicamente h√° muitos anos. <br>  Assim, em 5 de junho de 2019 √†s 14:35, os engenheiros de um de nossos data centers relataram um alarme de inc√™ndio. <br><a name="habracut"></a><br><h4>  Nega√ß√£o </h4><br>  14:45.  Incidentes menores de fuma√ßa nos data centers s√£o mais comuns do que parecem.  Os indicadores dentro dos corredores eram normais, ent√£o nossa primeira rea√ß√£o foi relativamente calma: introduzimos uma proibi√ß√£o de trabalhar com produ√ß√£o, ou seja, qualquer altera√ß√£o na configura√ß√£o, lan√ßando novas vers√µes etc., exceto pelo trabalho relacionado √† corre√ß√£o de algo. <br><br><h4>  Raiva </h4><br>  Alguma vez voc√™ j√° tentou descobrir com os bombeiros exatamente onde havia um inc√™ndio no telhado ou subir em um telhado em chamas para avaliar a situa√ß√£o?  Qual ser√° o grau de confian√ßa nas informa√ß√µes recebidas por cinco pessoas? <br><br>  14:50.  <b>Foi recebida informa√ß√£o de que o inc√™ndio est√° se aproximando do sistema de refrigera√ß√£o</b> .  Mas isso vir√°?  O administrador do sistema de plant√£o exibe tr√°fego externo das frentes desse data center. <br><blockquote>  No momento, as frentes de todos os nossos servi√ßos s√£o duplicadas em tr√™s datacenters, √© usado o balanceamento no n√≠vel DNS, que permite remover os endere√ßos de um datacenter do DNS, protegendo os usu√°rios de poss√≠veis problemas com o acesso aos servi√ßos.  Caso j√° tenham ocorrido problemas no datacenter, ele sai automaticamente da rota√ß√£o.  Mais detalhes podem ser encontrados aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Balanceamento de carga e toler√¢ncia a falhas em Odnoklassniki.</a> </blockquote><br>  O inc√™ndio ainda n√£o nos afetou de forma alguma - nem usu√°rios nem equipamentos foram afetados.  √â um acidente?  A primeira se√ß√£o do documento "Plano de A√ß√£o para Acidentes" define o conceito de "Acidente" e a se√ß√£o termina da seguinte forma: <br>  <b>‚ÄúEm <u>caso de d√∫vida, um acidente ou n√£o, ent√£o √© um acidente!</u></b>  <b>"</b> <br><br>  14:53.  Um coordenador de acidentes √© nomeado. <br><blockquote>  Um coordenador √© uma pessoa que controla a comunica√ß√£o entre todos os participantes, estima a escala do acidente, usa o "Plano de A√ß√£o para Acidentes", atrai a equipe necess√°ria, monitora a conclus√£o do reparo e, mais importante, delega todas as tarefas.  Em outras palavras, essa √© a pessoa que gerencia todo o processo de elimina√ß√£o do acidente. </blockquote><br><h4>  Negocia√ß√£o </h4><br>  15:01.  Come√ßamos a desconectar servidores que n√£o est√£o vinculados √† produ√ß√£o. <br>  15:03.  Desative todos os servi√ßos reservados corretamente. <br>  Isso inclui n√£o apenas as frentes (nas quais os usu√°rios n√£o est√£o mais conectados) e seus servi√ßos auxiliares (l√≥gica de neg√≥cios, caches, etc.), mas tamb√©m v√°rios bancos de dados com fator de replica√ß√£o 2 ou mais ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cassandra</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">armazenamento de dados bin√°rios</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">armazenamento a frio</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NewSQL</a> , etc.). <br>  15:06.  <b>Foi recebida informa√ß√£o de que um inc√™ndio amea√ßa um dos corredores do data center.</b>  N√£o temos equipamentos nesta sala, mas o fato de o fogo se espalhar do telhado para as salas muda muito a imagem do que est√° acontecendo. <br>  (Mais tarde, verificou-se que n√£o havia amea√ßa f√≠sica ao sal√£o, uma vez que estava hermeticamente isolado do telhado. A amea√ßa era apenas para o sistema de refrigera√ß√£o desse sal√£o.) <br>  15:07.  Permitimos a execu√ß√£o de comandos em servidores no modo acelerado sem verifica√ß√µes adicionais ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sem a nossa calculadora favorita</a> ). <br>  15:08.  A temperatura nos quartos est√° dentro dos limites normais. <br>  15:12.  <b>Foi registrado um aumento de temperatura nos corredores.</b> <br>  15:13.  Mais da metade dos servidores no data center est√£o desativados.  N√≥s continuamos. <br>  15:16.  Foi decidido desligar todos os equipamentos. <br>  15:21  Come√ßamos a desligar os servidores sem estado sem desligar corretamente o aplicativo e os SOs. <br>  15:23.  Um grupo de respons√°veis ‚Äã‚Äãpelo MS SQL √© destacado (existem poucos, a depend√™ncia de servi√ßos neles n√£o √© grande, mas o procedimento de recupera√ß√£o leva mais tempo e √© mais complicado do que, por exemplo, Cassandra). <br><br><h4>  Depress√£o </h4><br>  15:25.  <b>Foram recebidas informa√ß√µes sobre como desligar a energia em quatro das 16 salas (N¬∫ 6, 7, 8, 9).</b>  Nos s√©timo e oitavo sal√µes est√£o nossos equipamentos.  N√£o h√° mais informa√ß√µes sobre nossos dois quartos (n¬∫s 1 e 3). <br>  Normalmente, durante inc√™ndios, a energia √© desligada imediatamente, mas, neste caso, gra√ßas ao trabalho coordenado dos bombeiros e do pessoal t√©cnico do data center, ela foi desligada n√£o em todos os lugares e n√£o imediatamente, mas se necess√°rio. <br>  (Mais tarde, verificou-se que a energia nos corredores 8 e 9 n√£o foi desligada.) <br>  15:28.  Come√ßamos a implantar bancos de dados MS SQL a partir de backups em outros data centers. <br>  Quanto tempo vai demorar?  Existe largura de banda de rede suficiente para toda a rota? <br>  15:37.  <b>Bloqueou algumas partes da rede.</b> <br>  A rede de gerenciamento e produ√ß√£o √© fisicamente isolada uma da outra.  Se a rede de produ√ß√£o estiver dispon√≠vel, voc√™ poder√° acessar o servidor, parar o aplicativo e desligar o sistema operacional.  Se n√£o estiver dispon√≠vel, voc√™ poder√° acessar o IPMI, parar o aplicativo e desligar o sistema operacional.  Se n√£o houver rede, voc√™ n√£o poder√° fazer nada.  "Obrigado cap!" Voc√™ vai pensar. <br>  "De qualquer forma, h√° muita confus√£o", voc√™ tamb√©m pode pensar. <br>  O problema √© que os servidores, mesmo sem inc√™ndio, geram uma quantidade enorme de calor.  Mais precisamente, quando h√° resfriamento, eles geram calor, e quando n√£o h√°, eles criam um inferno infernal, que na melhor das hip√≥teses derreter√° alguns dos equipamentos e desligar√° o outro, e na pior das hip√≥teses ... causar√° um inc√™ndio dentro do sal√£o, que quase certamente destruir√° tudo. <br><br><img src="https://habrastorage.org/webt/fp/m6/zg/fpm6zg2uwewoewqfwgocfbkamky.jpeg"><br><br>  15:39.  Corrigimos problemas com o banco de dados conf. <br><blockquote>  O banco de dados conf √© o back-end do servi√ßo com o mesmo nome, usado por todos os aplicativos de produ√ß√£o para alterar rapidamente as configura√ß√µes.  Sem essa base, n√£o podemos controlar o portal, mas o pr√≥prio portal pode funcionar. </blockquote><br>  15:41.  Os sensores de temperatura nos equipamentos da rede Core registram leituras pr√≥ximas ao m√°ximo permitido.  Essa √© uma caixa que ocupa um rack inteiro e garante a opera√ß√£o de todas as redes dentro do data center. <br><br><img src="https://habrastorage.org/webt/hk/pu/ju/hkpujukttnjdf651rnnhpof0qxw.jpeg"><br><br>  15:42.  O rastreador de problemas e o wiki n√£o est√£o dispon√≠veis, alterne para o modo de espera. <br>  Isso n√£o √© produ√ß√£o, mas em um acidente, a disponibilidade de qualquer base de conhecimento pode ser cr√≠tica. <br>  15:50.  Um dos sistemas de monitoramento foi desconectado. <br>  Existem v√°rios deles, e eles s√£o respons√°veis ‚Äã‚Äãpor v√°rios aspectos do trabalho dos servi√ßos.  Alguns deles est√£o configurados para funcionar autonomamente dentro de cada data center (ou seja, monitoram apenas o data center), enquanto outros consistem em componentes distribu√≠dos que sobrevivem de forma transparente √† perda de qualquer data center. <br>  Nesse caso, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sistema para detectar anomalias nos indicadores de l√≥gica de neg√≥cios</a> que funciona no modo de espera principal parou de funcionar.  Mude para o modo de espera. <br><br><h4>  Aceita√ß√£o </h4><br>  15:51.  Por meio do IPMI, todos os servidores, exceto o MS SQL, foram desativados sem um desligamento correto. <br>  Voc√™ est√° pronto para o gerenciamento de servidores em massa atrav√©s do IPMI, se necess√°rio? <br><br><hr>  <i>O exato momento em que o resgate de equipamentos no data center, nesta fase, √© conclu√≠do.</i>  <i>Tudo o que poderia ser feito foi feito.</i>  <i>Alguns colegas podem relaxar.</i> <hr><br>  16:13.  <b>Havia informa√ß√µes de que os tubos freon dos aparelhos de ar-condicionado estouravam no teto - isso atrasaria o lan√ßamento do data center ap√≥s a elimina√ß√£o do inc√™ndio.</b> <br>  16:19.  De acordo com dados recebidos da equipe t√©cnica do data center, o aumento de temperatura nos corredores parou. <br>  17:10.  Restaurou o trabalho do banco de dados conf.  Agora podemos alterar as configura√ß√µes do aplicativo. <br>  Por que √© t√£o importante se tudo √© tolerante a falhas e funciona mesmo sem um data center? <br>  Primeiro, nem tudo √© tolerante a falhas.  Existem v√°rios servi√ßos secund√°rios que n√£o sobrevivem √† falha do data center suficientemente bem e existem bases no modo de espera principal.  A capacidade de gerenciar as configura√ß√µes permite que voc√™ fa√ßa todo o necess√°rio para minimizar o impacto das consequ√™ncias do acidente nos usu√°rios, mesmo em condi√ß√µes dif√≠ceis. <br>  Em segundo lugar, ficou claro que, nas pr√≥ximas horas, o datacenter n√£o se recuperar√° completamente; portanto, era necess√°rio tomar medidas para que a indisponibilidade a longo prazo de r√©plicas n√£o levasse a problemas adicionais, como estouros de disco nos demais datacenters. <br>  17:29.  Hora da pizza!  Temos pessoas trabalhando, n√£o rob√¥s. <br><br><img src="https://habrastorage.org/webt/3x/ft/hl/3xfthlpehimklv9yd_oua6phnis.png"><br><br><h4>  Reabilita√ß√£o </h4><br>  18:02.  Nos quartos n ¬∞ 8 (nosso), 9, 10 e 11, a temperatura se estabilizou.  Em um dos que permanecem desconectados (n¬∫ 7), nossos equipamentos est√£o localizados e a temperatura continua aumentando. <br>  18:31.  Eles deram luz verde para lan√ßar equipamentos nos corredores Nos. 1 e 3 - o fogo n√£o afetou esses corredores. <br><br><hr>  <i>Atualmente, os servidores est√£o sendo lan√ßados nos corredores n¬∫ 1, 3, 8, come√ßando pelos mais cr√≠ticos.</i>  <i>Verifica a opera√ß√£o correta de todos os servi√ßos em execu√ß√£o.</i>  <i>Ainda h√° problemas com o quarto 7.</i> <hr><br><br>  18:44.  A equipe t√©cnica do data center descobriu que no hall n√∫mero 7 (onde apenas nosso equipamento est√° localizado), muitos servidores n√£o est√£o desligados.  Segundo nossos dados, 26 servidores permanecem l√°.  Ap√≥s a verifica√ß√£o, encontramos 58 servidores. <br>  20:18.  A equipe t√©cnica do data center sopra ar na sala sem ar-condicionado atrav√©s de dutos m√≥veis dispostos nos corredores. <br>  23:08.  Eles deixaram o primeiro administrador ir para casa.  Algu√©m precisa dormir √† noite para continuar trabalhando amanh√£.  Em seguida, lan√ßamos outra parte dos administradores e desenvolvedores. <br>  02:56.  Lan√ßamos tudo o que poderia ser lan√ßado.  Fazemos uma grande verifica√ß√£o de todos os servi√ßos com autotestes. <br><br><img src="https://habrastorage.org/webt/90/pq/ii/90pqii3vxt6p5hzomjdymkebp3a.jpeg"><br><br>  03:02  Ar condicionado no √∫ltimo, s√©timo sal√£o restaurado. <br>  03:36.  Eles colocaram as frentes do centro de dados em rota√ß√£o no DNS.  A partir deste momento, o tr√°fego do usu√°rio come√ßa a chegar. <br>  Dissolvemos a maioria da equipe de administradores dom√©sticos.  Mas deixamos algumas pessoas. <br><blockquote>  Pequenas FAQ: <br>  P: O que aconteceu das 18:31 √†s 02:56? <br>  R: Seguindo o ‚ÄúPlano de A√ß√£o para Acidentes‚Äù, lan√ßamos todos os servi√ßos, come√ßando pelos mais importantes.  Ao mesmo tempo, o coordenador do bate-papo presta um servi√ßo a um administrador gratuito, que verifica se o SO e o aplicativo foram iniciados, se h√° algum erro ou se os indicadores est√£o normais.  Ap√≥s a conclus√£o do lan√ßamento, ele informa ao bate-papo que est√° livre e recebe um novo servi√ßo do coordenador. <br>  O processo √© adicionalmente inibido pelo ferro com falha.  Mesmo que o desligamento e desativa√ß√£o do sistema operacional dos servidores estejam corretos, alguns deles n√£o retornam devido a falhas de unidades, mem√≥ria ou chassi com falha repentina.  Com a perda de energia, a taxa de falha aumenta. <br>  P: Por que voc√™ n√£o pode simplesmente iniciar tudo de uma vez e depois reparar o que sai do monitoramento? <br>  R: Tudo precisa ser feito gradualmente, porque existem depend√™ncias entre os servi√ßos.  E tudo deve ser verificado imediatamente, sem esperar pelo monitoramento - porque √© melhor lidar com os problemas imediatamente, n√£o esperar pelo seu agravamento. </blockquote><br>  7:40 da manh√£  O √∫ltimo administrador (coordenador) foi para a cama.  O trabalho do primeiro dia est√° conclu√≠do. <br>  8:09 da manh√£  Os primeiros desenvolvedores, engenheiros nos datacenters e administradores (incluindo o novo coordenador) come√ßaram os trabalhos de restaura√ß√£o. <br>  09:37.  Come√ßamos a elevar o n√∫mero 7 do sal√£o (o √∫ltimo). <br>  Ao mesmo tempo, continuamos restaurando o que n√£o terminamos em outras salas: substituindo discos / mem√≥ria / servidores, consertando tudo o que est√° pegando fogo no monitoramento, alternando fun√ß√µes de invers√£o em circuitos de espera principal e outras pequenas coisas, que s√£o, no entanto, bastante. <br>  17:08.  Permita todo o trabalho regular com a produ√ß√£o. <br>  21:45.  O trabalho do segundo dia est√° conclu√≠do. <br>  09:45.  Hoje √© sexta-feira.  Ainda existem alguns problemas menores no monitoramento.  Antes do fim de semana, todo mundo quer relaxar.  Continuamos a reparar massivamente tudo o que √© poss√≠vel.  Tarefas administrativas normais que podem ser adiadas s√£o adiadas.  O coordenador √© novo. <br>  15:40.  De repente, metade da pilha principal de equipamentos de rede no OTHER data center foi reiniciada.  Frentes removidas da rota√ß√£o para minimizar riscos.  N√£o h√° efeito para os usu√°rios.  Mais tarde, descobriu-se que era um chassi ruim.  O coordenador trabalha com a corre√ß√£o de duas falhas ao mesmo tempo. <br>  17:17.  A rede em outro data center √© restaurada, tudo est√° verificado.  O centro de dados est√° em rota√ß√£o. <br>  18:29.  O trabalho do terceiro dia e, em geral, a recupera√ß√£o do acidente est√° conclu√≠do. <br><br><h4>  Posf√°cio </h4><br>  04/04/2013, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no dia do erro 404</a> , Odnoklassniki <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sobreviveu a um acidente grave</a> - por tr√™s dias o portal ficou total ou parcialmente indispon√≠vel.  Durante esse per√≠odo, mais de 100 pessoas de diferentes cidades, de diferentes empresas (muito obrigado novamente!) Remotamente e diretamente nos data centers, repararam manual e automaticamente milhares de servidores. <br>  Tiramos conclus√µes.  Para evitar que isso aconte√ßa novamente, realizamos e continuamos a realizar um extenso trabalho at√© hoje. <br><br>  Quais s√£o as principais diferen√ßas entre o acidente atual e o 404? <br><br><ul><li>  Temos um "Plano de A√ß√£o de Emerg√™ncia".  Uma vez por trimestre, realizamos exerc√≠cios - realizamos uma emerg√™ncia, que um grupo de administradores (por sua vez) deve eliminar usando o "Plano de A√ß√£o de Emerg√™ncia".  Os principais administradores de sistema se revezam na defini√ß√£o do papel de coordenador. </li><li>  Trimestralmente no modo de teste, isole os datacenters (todos por sua vez) pela LAN e WAN, o que permite identificar gargalos em tempo h√°bil. </li><li>  Menos unidades defeituosas, porque refor√ßamos os padr√µes: menos horas de funcionamento, limites mais r√≠gidos para o SMART, </li><li>  BerkeleyDB completamente abandonado - um banco de dados antigo e inst√°vel que exigia muito tempo para se recuperar da reinicializa√ß√£o do servidor. </li><li>  Reduza o n√∫mero de servidores com MS SQL e reduza a depend√™ncia do restante. </li><li>  Temos nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√≥pria nuvem - uma nuvem</a> , onde temos migrado ativamente todos os servi√ßos nos √∫ltimos dois anos.  A nuvem simplifica bastante todo o ciclo de trabalho com o aplicativo e, em caso de acidente, fornece ferramentas exclusivas como: <br><ul><li>  correto interrompa todos os aplicativos em um clique; </li><li>  migra√ß√£o simples de aplicativos de servidores com falha; </li><li>  classifica√ß√£o autom√°tica (em ordem de prioridade dos servi√ßos), lan√ßando um data center inteiro. </li></ul></li></ul><br>  O acidente descrito neste artigo se tornou o maior desde o dia 404.  Claro, nem tudo correu bem.  Por exemplo, durante a indisponibilidade do gravador de data center em outro data center, um disco travou em um dos servidores, ou seja, apenas uma das tr√™s r√©plicas no cluster Cassandra estava dispon√≠vel, devido √† qual 4,2% dos usu√°rios de aplicativos m√≥veis n√£o puderam efetuar login .  Ao mesmo tempo, os usu√°rios j√° conectados continuaram trabalhando.  No total, mais de 30 problemas foram identificados de acordo com os resultados do acidente - de bugs banais a falhas de arquitetura de servi√ßo. <br><br>  Mas a diferen√ßa mais importante entre o acidente atual e o 404¬∫ √© que, enquanto eliminamos as consequ√™ncias do inc√™ndio, os usu√°rios ainda se correspondiam e faziam videochamadas em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tamtam</a> , jogavam jogos, ouviam m√∫sica, davam presentes uns aos outros, assistiam v√≠deos, programas de TV e canais de TV em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OK</a> e tamb√©m transmitido para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OK Live</a> . <br><br>  Como est√£o indo seus acidentes? </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt472812/">https://habr.com/ru/post/pt472812/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt472792/index.html">Computador baseado em v√°lvulas NOR: dentro do computador de controle de bordo Apollo</a></li>
<li><a href="../pt472796/index.html">YES recua FAANG * ou [guia pr√°tico] em busca de emprego nos EUA / Europa para especialista em TI</a></li>
<li><a href="../pt472798/index.html">Mapas Yandex para aplica√ß√£o de t√°xi</a></li>
<li><a href="../pt472802/index.html">O MIRO √© uma plataforma de rob√¥ indoor aberta. Parte 2 - Design de Rob√¥</a></li>
<li><a href="../pt472810/index.html">Para o administrador do sistema iniciante: como fazer o pedido sair do caos</a></li>
<li><a href="../pt472814/index.html">Minha primeira m√°quina virtual: como n√£o mexer</a></li>
<li><a href="../pt472816/index.html">Padr√µes elegantes em JavaScript moderno (ciclo da equipe Bill Sourour)</a></li>
<li><a href="../pt472818/index.html">Movimento coletivo: como os cientistas estudam corti√ßa de formiga</a></li>
<li><a href="../pt472822/index.html">Quando a Academia Russa de Ci√™ncias √© impotente</a></li>
<li><a href="../pt472826/index.html">Microintera√ß√µes e seu uso em interfaces de usu√°rio</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>