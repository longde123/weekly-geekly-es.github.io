<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèáüèª üë©üèø‚Äçüåæ üë©üèΩ‚Äçüéì Richard Hamming: Cap√≠tulo 10. Teor√≠a de la codificaci√≥n - I üèª üõãÔ∏è üòì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content=""El objetivo de este curso es prepararte para tu futuro t√©cnico". 
 Hola Habr ¬øRecuerdas el incre√≠ble art√≠culo "T√∫ y tu trabajo" (+219, 2442 marcadore...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Richard Hamming: Cap√≠tulo 10. Teor√≠a de la codificaci√≥n - I</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417109/"><blockquote>  "El objetivo de este curso es prepararte para tu futuro t√©cnico". </blockquote><br><img src="https://habrastorage.org/getpro/habr/post_images/d67/6ff/9ea/d676ff9eadd2a38b0948de76bbf27fd4.jpg" alt="imagen" align="right">  Hola Habr  ¬øRecuerdas el incre√≠ble art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"T√∫ y tu trabajo"</a> (+219, 2442 marcadores, 394k lecturas)? <br><br>  Entonces, Hamming (s√≠, s√≠, los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digos de Hamming que se</a> autoverifican y corrigen a s√≠ mismos) tiene un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">libro</a> completo escrito basado en sus conferencias.  Lo estamos traduciendo, porque el hombre est√° hablando de negocios. <br><br>  Este libro no es solo sobre TI, es un libro sobre el estilo de pensamiento de personas incre√≠blemente geniales.  <i>‚ÄúEsto no es solo una carga de pensamiento positivo;</i>  <i>describe condiciones que aumentan las posibilidades de hacer un gran trabajo ".</i> <br><br>  Ya hemos traducido 28 (de 30) cap√≠tulos.  Y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estamos trabajando</a> en una edici√≥n en papel. <br><br><h3>  Teor√≠a de la codificaci√≥n - I </h3><br>  Habiendo considerado las computadoras y el principio de su trabajo, ahora consideraremos el tema de la informaci√≥n: c√≥mo las computadoras representan la informaci√≥n que queremos procesar.  El significado de cualquier car√°cter puede depender de c√≥mo se procese, la m√°quina no tiene un significado espec√≠fico para el bit utilizado.  Cuando discutimos la historia del software, Cap√≠tulo 4, consideramos un lenguaje de programaci√≥n sint√©tico, en el que el c√≥digo de la instrucci√≥n break coincid√≠a con el c√≥digo de otras instrucciones.  Esta situaci√≥n es t√≠pica para la mayor√≠a de los idiomas, el significado de la instrucci√≥n est√° determinado por el programa correspondiente. <br><br>  Para simplificar el problema de presentar informaci√≥n, consideramos el problema de transmitir informaci√≥n de punto a punto.  Esta pregunta est√° relacionada con el tema de la informaci√≥n de conservaci√≥n.  Los problemas de transmisi√≥n de informaci√≥n en el tiempo y el espacio son id√©nticos.  La figura 10.1 muestra un modelo est√°ndar para transmitir informaci√≥n. <br><br><img src="https://habrastorage.org/webt/ba/-h/xi/ba-hxivlgh-iaa6jippqrpar6lm.jpeg" alt="imagen"><br><br>  <i>Figura 10.1</i> <br><a name="habracut"></a><br>  A la izquierda en la figura 10.1 se encuentra la fuente de informaci√≥n.  Cuando consideramos el modelo, no nos importa la naturaleza de la fuente.  Puede ser un conjunto de s√≠mbolos del alfabeto, n√∫meros, f√≥rmulas matem√°ticas, notas musicales, s√≠mbolos con los que podemos representar movimientos de baile; la naturaleza de la fuente y el significado de los s√≠mbolos almacenados en ella no forma parte del modelo de transmisi√≥n.  Consideramos solo la fuente de informaci√≥n, con tal restricci√≥n obtenemos una poderosa teor√≠a general que puede extenderse a muchas √°reas.  Es una abstracci√≥n de muchas aplicaciones. <br><br>  Cuando Shannon cre√≥ la teor√≠a de la informaci√≥n a fines de la d√©cada de 1940, se cre√≠a que deber√≠a llamarse la teor√≠a de la comunicaci√≥n, pero insisti√≥ en el t√©rmino informaci√≥n.  Este t√©rmino se ha convertido en una causa constante de un mayor inter√©s y una constante decepci√≥n en teor√≠a.  Los investigadores quer√≠an construir "teor√≠as de informaci√≥n" completas, degeneraron en una teor√≠a de un conjunto de personajes.  Volviendo al modelo de transmisi√≥n, tenemos una fuente de datos que necesita ser codificada para la transmisi√≥n. <br><br>  El codificador consta de dos partes, la primera parte se llama codificador fuente, el nombre exacto depende del tipo de fuente.  Las fuentes de diferentes tipos de datos corresponden a diferentes tipos de codificadores. <br><br>  La segunda parte del proceso de codificaci√≥n se denomina codificaci√≥n de canal y depende del tipo de canal para transmitir datos.  Por lo tanto, la segunda parte del proceso de codificaci√≥n es consistente con el tipo de canal de transmisi√≥n.  Por lo tanto, cuando se utilizan interfaces est√°ndar, los datos de la fuente se codifican inicialmente de acuerdo con los requisitos de la interfaz, y luego de acuerdo con los requisitos del canal de datos utilizado. <br><br>  Seg√∫n el modelo, en la figura 10.1, el canal de datos est√° expuesto a "ruido aleatorio adicional".  Todo el ruido en el sistema se combina en este punto.  Se supone que el codificador acepta todos los caracteres sin distorsi√≥n, y el decodificador realiza su funci√≥n sin errores.  Esta es una idealizaci√≥n, pero para muchos prop√≥sitos pr√°cticos est√° cerca de la realidad. <br><br>  La fase de decodificaci√≥n tambi√©n consta de dos etapas: canal - est√°ndar, est√°ndar - receptor de datos.  Al final de la transferencia de datos se transmite al consumidor.  Y nuevamente, no estamos considerando c√≥mo el consumidor interpreta estos datos. <br><br>  Como se se√±al√≥ anteriormente, un sistema de transmisi√≥n de datos, por ejemplo, mensajes telef√≥nicos, radio, programas de televisi√≥n, presenta datos en forma de un conjunto de n√∫meros en los registros de una computadora.  Repito nuevamente, la transmisi√≥n en el espacio no es diferente de la transmisi√≥n en el tiempo o el almacenamiento de informaci√≥n.  ¬øTiene informaci√≥n que se requerir√° despu√©s de un tiempo, luego debe codificarse y almacenarse en la fuente de almacenamiento de datos.  Si es necesario, la informaci√≥n se decodifica.  Si el sistema de codificaci√≥n y decodificaci√≥n es el mismo, transmitimos datos a trav√©s del canal de transmisi√≥n sin cambios. <br><br>  La diferencia fundamental entre la teor√≠a presentada y la teor√≠a habitual en f√≠sica es la suposici√≥n de que no hay ruido en la fuente y el receptor.  De hecho, se producen errores en cualquier equipo.  En mec√°nica cu√°ntica, el ruido ocurre en cualquier etapa de acuerdo con el principio de incertidumbre, y no como una condici√≥n inicial;  En cualquier caso, el concepto de ruido en la teor√≠a de la informaci√≥n no es equivalente a un concepto similar en mec√°nica cu√°ntica. <br>  Para mayor claridad, consideraremos la forma binaria de representaci√≥n de datos en el sistema.  Otros formularios se procesan de manera similar, por simplicidad no los consideraremos. <br><br>  Comenzamos nuestra consideraci√≥n de los sistemas con caracteres codificados de longitud variable, como en el c√≥digo Morse cl√°sico de puntos y guiones, en el que los caracteres que aparecen con frecuencia son cortos y los raros son largos.  Este enfoque le permite lograr una alta eficiencia del c√≥digo, pero vale la pena se√±alar que el c√≥digo Morse es ternario, no binario, ya que contiene un espacio entre puntos y guiones.  Si todos los caracteres en el c√≥digo tienen la misma longitud, dicho c√≥digo se llama c√≥digo de bloque. <br><br>  La primera propiedad obvia necesaria del c√≥digo es la capacidad de decodificar de manera √∫nica el mensaje en ausencia de ruido, al menos esta parece ser la propiedad deseada, aunque en algunas situaciones este requisito puede ser descuidado.  Los datos del canal de transmisi√≥n para el receptor se ven como una secuencia de caracteres de ceros y unos. <br><br>  Llamaremos a dos caracteres adyacentes una extensi√≥n doble, tres caracteres adyacentes a una extensi√≥n triple y, en el caso general, si reenviamos N caracteres, el receptor ve adiciones al c√≥digo base de N caracteres.  El receptor, sin conocer el valor de N, debe dividir la secuencia en bloques de difusi√≥n.  O, en otras palabras, el receptor debe poder descomponer la secuencia de manera √∫nica para restaurar el mensaje original. <br><br>  Considere un alfabeto de una peque√±a cantidad de caracteres, generalmente los alfabetos son mucho m√°s grandes.  Los alfabetos de idiomas comienzan de 16 a 36 caracteres, incluidos may√∫sculas y min√∫sculas, signos num√©ricos, signos de puntuaci√≥n.  Por ejemplo, en la tabla ASCII 128 = 2 ^ 7 caracteres. <br>  Considere un c√≥digo especial que consta de 4 caracteres s1, s2, s3, s4 <br><br>  <b>s1 = 0;</b>  <b>s2 = 00;</b>  <b>s3 = 01;</b>  <b>s4 = 11.</b> <br><br>  ¬øC√≥mo debe interpretar el receptor la siguiente expresi√≥n recibida? <br><br>  <b>0011</b> ? <br><br>  ¬øC√≥mo <b>s1s1s4</b> o c√≥mo <b>s2s4</b> ? <br><br>  No puede dar una respuesta inequ√≠voca a esta pregunta, este c√≥digo definitivamente no est√° decodificado, por lo tanto, no es satisfactorio.  C√≥digo, por otro lado <br><br>  <b>s1 = 0;</b>  <b>s2 = 10;</b>  <b>s3 = 110;</b>  <b>s4 = 111</b> <br><br>  decodifica el mensaje de una manera √∫nica.  Tome una cadena arbitraria y considere c√≥mo la decodificar√° el receptor.  Necesita construir un √°rbol de decodificaci√≥n De acuerdo con el formulario en la Figura 10.II.  Cadena <br><br>  <b>1101000010011011100010100110</b> ... <br><br>  puede dividirse en bloques de caracteres <br><br>  <b>110, 10, 0, 10, 0, 110, 111, 0, 0, 0, 10, 10, 0, 110,</b> ... <br><br>  de acuerdo con la siguiente regla para construir un √°rbol de decodificaci√≥n: <br><br><blockquote>  Si est√°s en la parte superior del √°rbol, lee el siguiente personaje.  Cuando alcanzas la hoja de un √°rbol, conviertes la secuencia a un personaje y vuelves al inicio. </blockquote><br>  La raz√≥n de la existencia de tal √°rbol es que ning√∫n personaje es un prefijo del otro, por lo que siempre se sabe cu√°ndo debe volver al comienzo del √°rbol de decodificaci√≥n. <br><br>  Es necesario prestar atenci√≥n a lo siguiente.  En primer lugar, la decodificaci√≥n es un proceso estrictamente continuo, en el que cada bit se examina solo una vez.  En segundo lugar, los protocolos generalmente incluyen caracteres que son un marcador del final del proceso de decodificaci√≥n y son necesarios para indicar el final de un mensaje. <br><br>  No utilizar un car√°cter final es un error com√∫n en el dise√±o del c√≥digo.  Por supuesto, puede proporcionar un modo de decodificaci√≥n constante, en cuyo caso no se necesita el car√°cter final. <br><br><img src="https://habrastorage.org/webt/jx/zn/py/jxznpy6gu3tgjcqw_kcicihl49w.jpeg" alt="imagen"><br><br>  <i>Figura 10.II</i> <br><br>  La siguiente pregunta son los c√≥digos para la decodificaci√≥n de flujo (instant√°neo).  Considere el c√≥digo que se obtiene del anterior mostrando caracteres <br><br>  <b>s1 = 0;</b>  <b>s2 = 01;</b>  <b>s3 = 011;</b>  <b>s4 = 111.</b> <br><br>  Supongamos que obtenemos la secuencia <b>011111 ... 111</b> .  La √∫nica forma en que puede decodificar el texto del mensaje es agrupar bits del final de 3 en un grupo y seleccionar grupos con un cero inicial delante de unos, despu√©s de lo cual puede decodificar.  Tal c√≥digo se decodifica de una manera √∫nica, ¬°pero no al instante!  ¬°Para decodificar, debe esperar el final de la transferencia!  En la pr√°ctica, este enfoque elimina la tasa de decodificaci√≥n (teorema de Macmillan), por lo tanto, es necesario buscar m√©todos de decodificaci√≥n instant√°nea. <br><br>  Considere dos formas de codificar el mismo car√°cter, Si: <br><br>  <b>s1 = 0;</b>  <b>s2 = 10;</b>  <b>s3 = 110;</b>  <b>s4 = 1110, s5 = 1111,</b> <br><br>  El √°rbol de decodificaci√≥n de este m√©todo se muestra en la Figura 10.III. <br><br><img src="https://habrastorage.org/webt/ox/bv/on/oxbvonl2ljgm0bgliqfgdy-qxsi.jpeg" alt="imagen"><br><br>  <i>Figura 10.III</i> <br><br>  Segunda forma <br><br>  <b>s1 = 00;</b>  <b>s2 = 01;</b>  <b>s3 = 100;</b>  <b>s4 = 110, s5 = 111</b> , <br><br>  El √°rbol de decodificaci√≥n de este cuidado se muestra en la Figura 10.IV. <br><br>  La forma m√°s obvia de medir la calidad del c√≥digo es la longitud promedio de un conjunto de mensajes.  Para esto, es necesario calcular la longitud del c√≥digo de cada car√°cter multiplicado por la probabilidad correspondiente de ocurrencia de pi.  Por lo tanto, se obtiene la longitud del c√≥digo completo.  La f√≥rmula para la longitud promedio L del c√≥digo para un alfabeto de q caracteres es la siguiente <br><br><img src="https://habrastorage.org/webt/o9/t8/hp/o9t8hprkvogq4j7tapnejfcqi5e.jpeg" alt="imagen"><br><br>  donde pi es la probabilidad de aparici√≥n del s√≠mbolo si, li es la longitud correspondiente del s√≠mbolo codificado.  Para un c√≥digo eficiente, el valor de L debe ser lo m√°s peque√±o posible.  Si P1 = 1/2, p2 = 1/4, p3 = 1/8, p4 = 1/16 y p5 = 1/16, entonces para el c√≥digo # 1 obtenemos el valor de longitud del c√≥digo <br><br><img src="https://habrastorage.org/webt/gm/dh/mm/gmdhmmak1wygwdg2bzrhpwjw5ek.jpeg" alt="imagen"><br><br>  Y para el c√≥digo # 2 <br><br><img src="https://habrastorage.org/webt/wl/gx/w_/wlgxw_sxrvguf4x7zxy5ppsjdeg.jpeg" alt="imagen"><br><br>  Los valores obtenidos indican la preferencia del primer c√≥digo. <br>  Si todas las palabras del alfabeto tienen la misma probabilidad de ocurrencia, entonces el segundo c√≥digo ser√° m√°s preferible.  Por ejemplo, con pi = 1/5, c√≥digo de longitud # 1 <br><br><img src="https://habrastorage.org/webt/fq/m_/5e/fqm_5ew-iiumvzfzhfa-dzqrvp0.jpeg" alt="imagen"><br><br>  y longitud del c√≥digo # 2 <br><br><img src="https://habrastorage.org/webt/so/h4/8h/soh48hms46bmymyvq2zfdjvhum0.jpeg" alt="imagen"><br><br>  Este resultado muestra una preferencia por 2 c√≥digos.  Por lo tanto, cuando se desarrolla un c√≥digo "bueno", es necesario tener en cuenta la probabilidad de aparici√≥n de caracteres. <br><br><img src="https://habrastorage.org/webt/fl/z0/ta/flz0taltdxjpeh7d83nylufdezi.jpeg" alt="imagen"><br><br>  <i>Figura 10.IV</i> <br><br><img src="https://habrastorage.org/webt/ub/0r/oc/ub0roc6s1bd8_tbjmbbi5knnmsy.jpeg" alt="imagen"><br><br>  <i>Figura 10.V</i> <br><br>  Considere la desigualdad de Kraft, que determina el valor l√≠mite de la longitud del c√≥digo del s√≠mbolo li.  En la base 2, la desigualdad se representa como <br><br><img src="https://habrastorage.org/webt/uf/fm/3x/uffm3x64fdcwpfqm0t59-58ojm8.jpeg" alt="imagen"><br><br>  Esta desigualdad sugiere que el alfabeto no puede tener demasiados caracteres cortos, de lo contrario la suma ser√° bastante grande. <br><br>  Para demostrar la desigualdad de Kraft para cualquier c√≥digo decodificado √∫nico r√°pido, construimos un √°rbol de decodificaci√≥n y aplicamos el m√©todo de inducci√≥n matem√°tica.  Si un √°rbol tiene una o dos hojas, como se muestra en la Figura 10.V, entonces la desigualdad es sin duda cierta.  Adem√°s, si el √°rbol tiene m√°s de dos hojas, dividimos el √°rbol de largo m en dos sub√°rboles.  De acuerdo con el principio de inducci√≥n, suponemos que la desigualdad es verdadera para cada rama de altura m -1 o menos.  Seg√∫n el principio de inducci√≥n, aplicando desigualdad a cada rama.  Denote las longitudes de los c√≥digos de las ramas K 'y K' '.  Cuando se combinan dos ramas de un √°rbol, la longitud de cada una aumenta en 1, por lo tanto, la longitud del c√≥digo consiste en las sumas K '/ 2 y K' '/ 2, <br><br><img src="https://habrastorage.org/webt/g9/-s/vv/g9-svvn75bjqfxxcuzrxf-vz_h0.jpeg" alt="imagen"><br><br>  El teorema est√° probado. <br><br>  Considere la prueba del teorema de Macmillan.  Aplicamos la desigualdad de Kraft para decodificar c√≥digos sin hilos.  La prueba se basa en el hecho de que para cualquier n√∫mero K&gt; 1, la en√©sima potencia del n√∫mero es obviamente m√°s que una funci√≥n lineal de n, donde n es un n√∫mero bastante grande.  Elevamos la desigualdad de Kraft a la en√©sima potencia y presentamos la expresi√≥n como una suma <br><br><img src="https://habrastorage.org/webt/zv/kx/lm/zvkxlmytcctyyvabj5iwh8x2nui.jpeg" alt="imagen"><br><br>  donde Nk es el n√∫mero de caracteres de longitud k, la suma comienza con la longitud m√≠nima de la en√©sima representaci√≥n del car√°cter y termina con la longitud m√°xima nl, donde l es la longitud m√°xima del car√°cter codificado.  Del requisito de decodificaci√≥n √∫nica se deduce eso.  El importe se presenta como <br><br><img src="https://habrastorage.org/webt/2b/pj/pe/2bpjpe5u9p6epynv-sfxvfym2ji.jpeg" alt="imagen"><br><br>  Si K&gt; 1, entonces es necesario establecer n lo suficientemente grande como para que la desigualdad se vuelva falsa.  Por lo tanto, k &lt;= 1;  El teorema de Macmillan est√° probado. <br><br>  Considere algunos ejemplos de la aplicaci√≥n de la desigualdad de Kraft.  ¬øPuede existir un c√≥digo decodificado de forma √∫nica con longitudes 1, 3, 3, 3?  Si desde <br><br><img src="https://habrastorage.org/webt/wc/y3/gp/wcy3gpqx-cl-e60fb3-im8ec6n4.jpeg" alt="imagen"><br><br>  ¬øQu√© pasa con las longitudes 1, 2, 2, 3?  Calcular de acuerdo con la f√≥rmula <br><br><img src="https://habrastorage.org/webt/ix/aa/qz/ixaaqzsj_qvuc0ug-nys8ezqexe.jpeg" alt="imagen"><br><br>  ¬°Desigualdad violada!  Hay demasiados caracteres cortos en este c√≥digo. <br><br>  Los c√≥digos de coma son c√≥digos que consisten en los caracteres 1, que terminan en un car√°cter 0, con la excepci√≥n del √∫ltimo car√°cter que consiste en todos.  Uno de los casos especiales es el c√≥digo. <br><br>  <b>s1 = 0;</b>  <b>s2 = 10;</b>  <b>s3 = 110;</b>  <b>s4 = 1110;</b>  <b>s5 = 11111.</b> <br><br>  Para este c√≥digo, obtenemos la expresi√≥n para la desigualdad de Kraft <br><br><img src="https://habrastorage.org/webt/dv/x5/pj/dvx5pjumdoxckpby4xczhlf8yso.jpeg" alt="imagen"><br><br>  En este caso, logramos la igualdad.  Es f√°cil ver que para los c√≥digos de puntos, la desigualdad de Kraft degenera en igualdad. <br><br>  Al crear un c√≥digo, debe prestar atenci√≥n a la cantidad de Kraft.  Si la cantidad de Kraft comienza a exceder 1, entonces esta es una se√±al sobre la necesidad de incluir un car√°cter de una longitud diferente para reducir la longitud promedio del c√≥digo. <br><br>  Cabe se√±alar que la desigualdad de Kraft no significa que este c√≥digo sea decodificable de manera √∫nica, sino que existe un c√≥digo con caracteres de tal longitud que est√° decodificado de manera √∫nica.  Para construir un c√≥digo decodificado √∫nico, puede asignar la longitud correspondiente en bits li con un n√∫mero binario.  Por ejemplo, para longitudes 2, 2, 3, 3, 4, 4, 4, 4, obtenemos la desigualdad de Kraft <br><br><img src="https://habrastorage.org/webt/dz/mf/ce/dzmfcev61jjvbppxsaqxblyetn4.jpeg" alt="imagen"><br><br>  Por lo tanto, puede existir un c√≥digo de flujo decodificado √∫nico. <br><br>  <b>s1 = 00;</b>  <b>s2 = 01;</b>  <b>s3 = 100;</b>  <b>s4 = 101;</b> <b><br><br></b>  <b>s5 = 1100;</b>  <b>s6 = 1101;</b>  <b>s7 = 1110;</b>  <b>s8 = 1111;</b> <br><br>  Quiero prestar atenci√≥n a lo que realmente sucede cuando intercambiamos ideas.  Por ejemplo, en este momento quiero transferir la idea de mi cabeza a la tuya.  Pronuncio algunas palabras a trav√©s de las cuales, como creo, puedes entender (entender) esta idea. <br><br>  Pero cuando un poco m√°s tarde desee transmitir esta idea a su amigo, seguramente pronunciar√° palabras completamente diferentes.  De hecho, el significado o significado no est√° encerrado en ninguna palabra espec√≠fica.  Us√© algunas palabras, y puedes usar palabras completamente diferentes para transmitir la misma idea.  Por lo tanto, diferentes palabras pueden transmitir la misma informaci√≥n.  Pero, tan pronto como le diga a su interlocutor que no comprende el mensaje, entonces, por regla general, el interlocutor seleccionar√° un conjunto diferente de palabras, la segunda o incluso la tercera, para transmitir el significado.  Por lo tanto, la informaci√≥n no est√° encerrada en un conjunto de palabras espec√≠ficas.  Tan pronto como reciba estas o esas palabras, har√° mucho trabajo al traducir las palabras a la idea que el interlocutor quiere transmitirle. <br><br>  Aprendemos a seleccionar palabras para adaptarnos al interlocutor.  En cierto sentido, elegimos palabras que coincidan con nuestros pensamientos y el nivel de ruido en el canal, aunque dicha comparaci√≥n no refleja con precisi√≥n el modelo que uso para representar el ruido en el proceso de decodificaci√≥n.  En organizaciones grandes, un problema grave es la incapacidad del interlocutor para escuchar lo que otra persona ha dicho.  En puestos superiores, los empleados escuchan lo que "quieren escuchar".  En algunos casos, debe recordar esto cuando asciende en la escala profesional.  La presentaci√≥n de informaci√≥n en una forma formal es un reflejo parcial de los procesos de nuestras vidas y ha encontrado una amplia aplicaci√≥n m√°s all√° de los l√≠mites de las reglas formales en las aplicaciones inform√°ticas. <br><br>  <i>Continuar√° ...</i> <br><br>  <i>¬øQui√©n quiere ayudar con la traducci√≥n, el dise√±o y la publicaci√≥n del libro? Escriba en un correo electr√≥nico personal o correo magisterludi2016@yandex.ru</i> <br><br>  Por cierto, tambi√©n lanzamos la traducci√≥n de otro libro genial: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"La m√°quina de los sue√±os: la historia de la revoluci√≥n inform√°tica"</a> ) <br><br><div class="spoiler">  <b class="spoiler_title">Contenido del libro y cap√≠tulos traducidos</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pr√≥logo</a> <br><ol><li>  Introducci√≥n al arte de hacer ciencia e ingenier√≠a: aprender a aprender (28 de marzo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Traducci√≥n: Cap√≠tulo 1</a> </li><li>  "Fundamentos de la revoluci√≥n digital (discreta)" (30 de marzo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 2. Fundamentos de la revoluci√≥n digital (discreta)</a> </li><li>  "Historia de las computadoras: hardware" (31 de marzo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 3.</a> Historia de las computadoras: hardware </li><li>  "Historia de las computadoras - Software" (4 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 4. Historia de las computadoras - Software</a> </li><li>  Historia de las computadoras: aplicaciones (6 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 5. Historia de las computadoras: aplicaci√≥n pr√°ctica</a> </li><li>  "Inteligencia artificial - Parte I" (7 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 6. Inteligencia artificial - 1</a> </li><li>  "Inteligencia artificial - Parte II" (11 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 7. Inteligencia artificial - II</a> </li><li>  "Inteligencia Artificial III" (13 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 8. Inteligencia Artificial-III</a> </li><li>  "Espacio N-Dimensional" (14 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 9. Espacio N-Dimensional</a> </li><li>  "Teor√≠a de la codificaci√≥n - La representaci√≥n de la informaci√≥n, Parte I" (18 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 10. Teor√≠a de la codificaci√≥n - I</a> </li><li>  "Teor√≠a de la codificaci√≥n - La representaci√≥n de la informaci√≥n, Parte II" (20 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 11. Teor√≠a de la codificaci√≥n - II</a> </li><li>  "C√≥digos de correcci√≥n de errores" (21 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 12. C√≥digos de correcci√≥n de errores</a> </li><li>  "Teor√≠a de la informaci√≥n" (25 de abril de 1995) <i>(el traductor desapareci√≥: ((()</i> </li><li>  Filtros digitales, Parte I (27 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 14. Filtros digitales - 1</a> </li><li>  Filtros digitales, Parte II (28 de abril de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 15. Filtros digitales - 2</a> </li><li>  Filtros digitales, Parte III (2 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 16. Filtros digitales - 3</a> </li><li>  Filtros digitales, Parte IV (4 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 17. Filtros digitales - IV</a> </li><li>  "Simulaci√≥n, Parte I" (5 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 18. Modelado - I</a> </li><li>  "Simulaci√≥n, Parte II" (9 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 19. Modelado - II</a> </li><li>  "Simulaci√≥n, Parte III" (11 de mayo de 1995) </li><li>  Fibra √≥ptica (12 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 21. Fibra √≥ptica</a> </li><li>  ‚ÄúInstrucci√≥n asistida por computadora‚Äù (16 de mayo de 1995) <i>(el traductor desapareci√≥: ((()</i> </li><li>  Matem√°ticas (18 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 23. Matem√°ticas</a> </li><li>  Mec√°nica cu√°ntica (19 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 24. Mec√°nica cu√°ntica</a> </li><li>  Creatividad (23 de mayo de 1995).  Traducci√≥n: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 25. Creatividad</a> </li><li>  "Expertos" (25 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 26. Expertos</a> </li><li>  ‚ÄúDatos no confiables‚Äù (26 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 27. Datos no v√°lidos</a> </li><li>  Ingenier√≠a de sistemas (30 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 28. Ingenier√≠a de sistemas</a> </li><li>  "Obtiene lo que mide" (1 de junio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 29.</a> Obtiene lo que mide </li><li>  "C√≥mo sabemos lo que sabemos" (2 de junio de 1995) el <i>traductor desapareci√≥: (((</i> </li><li>  Hamming, "Usted y su investigaci√≥n" (6 de junio de 1995).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Traducci√≥n: usted y su trabajo</a> </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ¬øQui√©n quiere ayudar con la traducci√≥n, el dise√±o y la publicaci√≥n del libro? Escriba en un correo electr√≥nico personal o correo magisterludi2016@yandex.ru </font></font><br><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es417109/">https://habr.com/ru/post/es417109/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es417099/index.html">¬øC√≥mo escrib√≠ la biblioteca est√°ndar de C ++ 11 o por qu√© boost es tan aterrador? Cap√≠tulo 2</a></li>
<li><a href="../es417101/index.html">Definici√≥n de Listo: lo que olvidamos contar</a></li>
<li><a href="../es417103/index.html">Spark SQL. Un poco sobre el optimizador de consultas</a></li>
<li><a href="../es417105/index.html">Impresi√≥n en una impresora 3D. Experiencias secretas de 3Dtool</a></li>
<li><a href="../es417107/index.html">Creador del juego mientras es verdadero: aprenda () sobre la programaci√≥n de gamedev, los problemas de realidad virtual y las simulaciones de ML</a></li>
<li><a href="../es417111/index.html">Conferencias en l√≠nea: transmisi√≥n vs seminario web</a></li>
<li><a href="../es417113/index.html">Impresora italiana 3D en Rusia: Raise3D N1 Dual - modelado y creaci√≥n de prototipos</a></li>
<li><a href="../es417115/index.html">Para enterrar o quemar Flutter.io?</a></li>
<li><a href="../es417117/index.html">Ingenier√≠a inversa del emulador NES en el juego para GameCube</a></li>
<li><a href="../es417119/index.html">Paginaci√≥n en Vue.js</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>