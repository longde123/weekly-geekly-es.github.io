<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçü§ù‚Äçüë©üèª üë©üèø‚ÄçüöÄ üíå La derni√®re fronti√®re de l'AQ en mati√®re de d√©fense: la d√©tection automatique des erreurs üöù üï∞Ô∏è üéÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut Je m'appelle Askhat Nuryev, je suis l'un des principaux ing√©nieurs en automatisation de DINS. 

 Je travaille chez Dino Systems depuis 7 ans. Pe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La derni√®re fronti√®re de l'AQ en mati√®re de d√©fense: la d√©tection automatique des erreurs</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dins/blog/473456/"><img src="https://habrastorage.org/webt/6g/16/7z/6g167zqsfurjcekgnb-vsvxauwm.png"><br><br>  Salut  Je m'appelle Askhat Nuryev, je suis l'un des principaux ing√©nieurs en automatisation de DINS. <br><br>  Je travaille chez Dino Systems depuis 7 ans.  Pendant ce temps, j'ai d√ª faire face √† diff√©rentes t√¢ches: de l'√©criture de tests fonctionnels automatis√©s aux tests de performances et de haute disponibilit√©.  Petit √† petit, je suis devenu plus impliqu√© dans l'organisation des tests et l'optimisation des processus en g√©n√©ral. <br><br>  Dans cet article, je dirai: <br><br><ul><li>  Que se passe-t-il si des bogues ont d√©j√† fui en production? </li><li>  Comment rivaliser pour la qualit√© du syst√®me, si vous ne pouvez pas compter les erreurs avec vos mains et ne r√©visez pas vos yeux? </li><li>  Quels sont les pi√®ges de la gestion automatique des erreurs? </li><li>  Quels bonus puis-je obtenir en analysant les statistiques des requ√™tes? </li></ul><a name="habracut"></a><br>  DINS est le centre de d√©veloppement de RingCentral, un leader du march√© parmi les fournisseurs de cloud pour les communications unifi√©es.  Ringentral fournit tout pour les communications d'entreprise de la t√©l√©phonie classique, SMS, r√©unions aux fonctionnalit√©s des centres de contact et des produits pour un travail d'√©quipe complexe (√† la Slack).  Cette solution cloud est situ√©e dans ses propres centres de donn√©es, et le client n'a besoin que de s'abonner au site. <br><br>  Le syst√®me, au d√©veloppement duquel nous participons, dessert 2 millions d'utilisateurs actifs et traite plus de 275 millions de demandes par jour.  L'√©quipe sur laquelle je travaille d√©veloppe l'API. <br>  Le syst√®me poss√®de une API assez compliqu√©e.  Avec lui, vous pouvez envoyer des SMS, passer des appels, collecter des vid√©oconf√©rences, configurer des comptes et m√™me envoyer des fax (bonjour, 2019).  Sous une forme simplifi√©e, le sch√©ma d'interaction des services ressemble √† ceci.  Je ne plaisante pas. <br><br><img src="https://habrastorage.org/webt/yh/qv/fh/yhqvfhbzgk-qluiiymhsfgchl2o.png"><br><br>  Il est clair qu'un syst√®me aussi complexe et tr√®s charg√© cr√©e un grand nombre d'erreurs.  Par exemple, il y a un an, nous recevions des dizaines de milliers d'erreurs par semaine.  Ce sont des milli√®mes de pour cent par rapport au nombre total de demandes, mais toujours autant d'erreurs sont un g√¢chis.  Nous les avons rattrap√©s gr√¢ce au service de support d√©velopp√©, cependant, ces erreurs affectent les utilisateurs.  De plus, le syst√®me √©volue constamment, le nombre de clients augmente.  Et le nombre d'erreurs aussi. <br><br>  Tout d'abord, nous avons essay√© de r√©soudre le probl√®me de mani√®re classique. <br>  Nous avons rassembl√©, demand√© des journaux de production, corrig√© quelque chose, oubli√© quelque chose, cr√©√© des tableaux de bord dans Kibana et Sumologic.  Mais dans l'ensemble, cela n'a pas aid√©.  Les bugs ont quand m√™me fui, se sont plaints les utilisateurs.  Il est devenu clair que quelque chose n'allait pas. <br><br><h3>  Automatisation </h3><br>  Bien s√ªr, nous avons commenc√© √† comprendre et √† constater que 90% du temps consacr√© √† la correction de l'erreur est consacr√© √† la collecte d'informations √† ce sujet.  Voici quoi exactement: <br><br><ul><li>  Obtenez les informations manquantes des autres services. </li><li>  Examinez les journaux du serveur. </li><li>  Enqu√™ter sur le comportement de notre syst√®me. </li><li>  Comprenez si tel ou tel comportement du syst√®me est erron√©. </li></ul><br>  Et seuls les 10% restants ont √©t√© d√©pens√©s directement pour le d√©veloppement. <br><br>  Nous avons pens√© - mais que se passe-t-il si nous cr√©ons un syst√®me qui trouve lui-m√™me des erreurs, leur donne une priorit√© et affiche toutes les donn√©es n√©cessaires pour y rem√©dier? <br><br>  Je dois dire que l'id√©e m√™me d'un tel service a suscit√© quelques inqui√©tudes. <br>  Quelqu'un a dit: "Si nous trouvons nous-m√™mes tous les bogues, alors pourquoi avons-nous besoin de l'assurance qualit√©?" <br>  D'autres ont dit le contraire: "Vous allez vous noyer dans ce tas de punaises!". <br>  En un mot, cela valait la peine de rendre un service, ne serait-ce que pour comprendre lequel d'entre eux a raison. <div class="spoiler">  <b class="spoiler_title">spoiler</b> <div class="spoiler_text">  (les deux groupes de sceptiques se sont tromp√©s) </div></div><br><br><h3>  Solutions pr√™tes √† l'emploi </h3><br>  Tout d'abord, nous avons d√©cid√© de voir lesquels des syst√®mes similaires sont d√©j√† sur le march√©.  Il s'est av√©r√© qu'il y en avait beaucoup.  Vous pouvez mettre en √©vidence Raygun, Sentry, Airbrake, il existe d'autres services. <br>  Mais aucun d'eux ne nous convenait, et voici pourquoi: <br><br><ul><li>  Certains services nous ont oblig√©s √† apporter des modifications trop importantes √† l'infrastructure existante, y compris des modifications sur le serveur.  Airbrake.io devrait affiner des dizaines, des centaines de composants du syst√®me. </li><li>  D'autres ont collect√© des donn√©es sur nos propres erreurs et les ont envoy√©es quelque part sur le c√¥t√©.  Notre politique de s√©curit√© ne le permet pas - les donn√©es des utilisateurs et des erreurs doivent rester avec nous. </li><li>  Eh bien, ils sont √©galement assez chers. </li></ul><br><br><h3>  Nous faisons notre </h3><br>  Il est devenu √©vident que nous devions rendre notre service, d'autant plus que nous avions d√©j√† construit une tr√®s bonne infrastructure pour cela: <br><br><ul><li>  Tous les services ont d√©j√† √©crit des journaux dans un r√©f√©rentiel unique - Elastic.  Dans les journaux, des identifiants uniformes des demandes de tous les services ont √©t√© jet√©s. </li><li>  Des statistiques de performances ont √©galement √©t√© enregistr√©es dans Hadoop.  Nous avons travaill√© avec des journaux en utilisant Impala et Metabase. </li></ul><br>  De toutes les erreurs de serveur ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">selon la classification des codes d'√©tat HTTP</a> ), le code 500 est le plus prometteur en termes d'analyse d'erreurs.  En r√©ponse aux erreurs 502, 503 et 504, dans certains cas, vous pouvez simplement r√©p√©ter la demande apr√®s un certain temps sans m√™me montrer la r√©ponse √† l'utilisateur.  Et selon les recommandations de l'API RC Platform, les utilisateurs doivent contacter le support s'ils re√ßoivent le code d'√©tat 500 en r√©ponse √† un appel. <br><br>  La premi√®re version du syst√®me a collect√© les journaux d'ex√©cution des requ√™tes, toutes les traces de pile qui se sont produites, les donn√©es utilisateur et ont plac√© le bogue dans le tracker, dans notre cas c'√©tait JIRA. <br><br>  Juste apr√®s le test, nous avons remarqu√© que le syst√®me cr√©e un nombre important d'erreurs en double.  Cependant, parmi ces doublons, beaucoup avaient presque les m√™mes traces de pile. <br><br>  Il a fallu changer la m√©thode d'identification des m√™mes erreurs.  De l'analyse de donn√©es purement statistiques, passez √† la recherche de la cause premi√®re de l'erreur.  Les traces de pile caract√©risent bien le probl√®me, mais elles sont assez difficiles √† comparer les unes aux autres - les num√©ros de ligne changent d'une version √† l'autre, les donn√©es utilisateur et d'autres parasites y p√©n√®trent.  De plus, ils n'entrent pas toujours dans le journal - pour certaines demandes abandonn√©es, ils n'existent tout simplement pas. <br>  Dans sa forme la plus pure, les traces de pile sont peu pratiques √† utiliser pour le suivi des erreurs. <br><br>  Il √©tait n√©cessaire de s√©lectionner des mod√®les, des mod√®les de traces de pile et de les effacer des informations qui changeaient souvent.  Apr√®s une s√©rie d'exp√©riences, nous avons d√©cid√© d'utiliser des expressions r√©guli√®res pour effacer les donn√©es. <br><br>  En cons√©quence, nous avons publi√© une nouvelle version, dans laquelle les erreurs ont √©t√© identifi√©es par ces mod√®les uniques, si des traces de pile √©taient disponibles.  Et s'ils n'√©taient pas disponibles, alors √† l'ancienne, par la m√©thode http et le groupe d'API. <br><br>  Et apr√®s cela, il n'y avait pratiquement plus de doublons.  Cependant, de nombreuses erreurs uniques ont √©t√© trouv√©es. <br><br>  L'√©tape suivante consiste √† comprendre comment hi√©rarchiser les erreurs, lesquelles doivent √™tre corrig√©es plus t√¥t.  Nous avons prioris√© par: <br><br><ul><li>  La fr√©quence de l'erreur. </li><li>  Le nombre d'utilisateurs qu'elle est concern√©e. </li></ul><br>  Sur la base des statistiques collect√©es, nous avons commenc√© √† publier des rapports hebdomadaires.  Ils ressemblent √† ceci: <br><br><img src="https://habrastorage.org/webt/sg/gy/lj/sggyljismpvupxa43mcjkz_pkko.png"><br><br>  Ou, par exemple, les 10 principales erreurs par semaine.  Fait int√©ressant, ces 10 bogues dans jira ont repr√©sent√© 90% des erreurs de service: <br><br><img src="https://habrastorage.org/webt/y8/hz/rm/y8hzrmloopmi_ncgweivbkvjfig.png"><br><br>  Nous avons envoy√© ces rapports aux d√©veloppeurs et aux chefs d'√©quipe. <br>  Quelques mois apr√®s le lancement du syst√®me, le nombre de probl√®mes est devenu nettement moins important.  M√™me notre petit MVP (produit minimalement viable) a aid√© √† mieux trier les erreurs. <br><br><h3>  Le probl√®me </h3><br>  Peut-√™tre que nous nous arr√™terions ici, sinon pour un accident. <br>  Une fois que je suis arriv√© au travail, j'ai remarqu√© que le syst√®me rivetait les bogues comme des petits pains: un par un.  Apr√®s une courte enqu√™te, il est devenu clair que des dizaines de ces erreurs provenaient d'un seul service.  Pour savoir quel est le probl√®me, je suis all√© dans la salle de discussion de l'√©quipe de d√©ploiement.  Il y avait des gars qui √©taient impliqu√©s dans l'installation de nouvelles versions de services en production et s'assuraient qu'ils fonctionnaient comme pr√©vu. <br>  J'ai demand√©: "Les gars, que s'est-il pass√© avec ce service?". <br>  Et ils r√©pondent: "Il y a une heure, nous y avons install√© une nouvelle version." <br>  √âtape par √©tape, nous avons identifi√© le probl√®me et trouv√© une solution temporaire, en d'autres termes, red√©marr√© le serveur. <br><br>  Il est devenu clair que le syst√®me "erron√©" est n√©cessaire non seulement aux d√©veloppeurs et ing√©nieurs responsables de la qualit√©.  Les ing√©nieurs qui sont responsables de l'√©tat des serveurs en production, ainsi que les gars qui installent de nouvelles versions sur les serveurs, s'y int√©ressent √©galement.  Le service que nous d√©veloppons montrera exactement quelles erreurs se produisent dans la production lors des modifications du syst√®me, telles que l'installation de serveurs, l'application d'une nouvelle configuration, etc. <br><br>  Et nous avons d√©cid√© de faire une autre it√©ration de d√©veloppement. <br><br>  Dans le processus de gestion des erreurs, nous avons ajout√© un enregistrement des statistiques de lecture des probl√®mes √† la base de donn√©es et aux tableaux de bord dans Grafana.  Voici √† quoi ressemble la distribution graphique des erreurs par jour dans tout le syst√®me: <br><br><img src="https://habrastorage.org/webt/xj/q_/gk/xjq_gkdt72qdxqocv-rv7hmku_0.png"><br><br>  Et donc - des erreurs dans les services individuels. <br><br><img src="https://habrastorage.org/webt/x-/5t/kx/x-5tkxagwpv16lhfg2l0fvpzgeg.png"><br><br>  Nous avons √©galement viss√© les d√©clencheurs avec escalades aux √©quipes d'ing√©nierie responsables - au cas o√π il y aurait beaucoup d'erreurs.  Nous avons √©galement mis en place une collecte de donn√©es toutes les 30 minutes (au lieu d'une fois par jour, comme auparavant). <br>  Le processus de notre syst√®me a commenc√© √† ressembler √† ceci: <br><br><img src="https://habrastorage.org/webt/pl/gz/qg/plgzqgh9rsamo-ztprr2zbgjxug.png"><br><br><h3>  Erreurs client </h3><br>  Cependant, les utilisateurs n'ont pas seulement souffert d'erreurs de serveur.  Il est √©galement arriv√© que l'erreur se soit produite en raison de la mise en ≈ìuvre des applications clientes. <br>  Pour g√©rer les erreurs des clients, nous avons d√©cid√© de construire un autre processus de recherche et d'analyse.  Pour ce faire, nous avons choisi 2 types d'erreurs qui affectent les entreprises: les erreurs d'autorisation et les erreurs de limitation. <br><br><blockquote>  La limitation est un moyen de prot√©ger le syst√®me contre les surcharges.  Si l'application ou l'utilisateur d√©passe leur quota de demandes, le syst√®me renvoie un code d'erreur 429 et un en-t√™te Retry-After, la valeur de l'en-t√™te indique le d√©lai apr√®s lequel la demande doit √™tre r√©p√©t√©e pour une ex√©cution r√©ussie. <br><br>  Les applications peuvent rester limit√©es ind√©finiment si elles cessent d'envoyer de nouvelles demandes.  Les utilisateurs finaux ne peuvent pas distinguer ces erreurs des autres.  En cons√©quence, cela provoque des plaintes aupr√®s du service d'assistance. </blockquote><br><br>  Heureusement, l'infrastructure et le syst√®me de statistiques permettent de suivre m√™me les erreurs des clients.  Nous pouvons le faire car les d√©veloppeurs d'applications qui utilisent notre API doivent se pr√©-enregistrer et recevoir leur cl√© unique.  Chaque demande du client doit contenir un jeton d'autorisation, sinon le client recevra une erreur.  √Ä l'aide de ce jeton, nous calculons l'application. <br><br>  Voici √† quoi ressemble la surveillance des erreurs de limitation.  Les pics d'erreurs correspondent aux jours de la semaine et le week-end - au contraire, il n'y a pas d'erreurs: <br><br><img src="https://habrastorage.org/webt/pd/xk/se/pdxksedgs0yum5fmcl5o1rjzosw.png"><br><br>  De la m√™me mani√®re que dans le cas d'erreurs internes, sur la base des statistiques de Hadoop, nous avons trouv√© des applications suspectes.  Premi√®rement, par rapport au nombre de demandes r√©ussies par rapport au nombre de demandes qui se sont termin√©es avec le code 429. Si nous avons re√ßu plus de la moiti√© de ces demandes, nous pensions que l'application ne fonctionnait pas correctement. <br>  Plus tard, nous avons commenc√© √† analyser le comportement des applications individuelles avec des utilisateurs sp√©cifiques.  Parmi les applications suspectes, nous avons trouv√© le p√©riph√©rique sp√©cifique sur lequel l'application s'ex√©cute et regard√© la fr√©quence √† laquelle elle ex√©cute les demandes apr√®s avoir re√ßu la premi√®re erreur de limitation.  Si la fr√©quence des demandes n'a pas diminu√©, l'application n'a pas trait√© l'erreur comme pr√©vu. <br><br>  Une partie des applications a √©t√© d√©velopp√©e dans notre entreprise.  Par cons√©quent, nous avons pu trouver imm√©diatement des ing√©nieurs responsables et corriger rapidement les erreurs.  Et nous avons d√©cid√© d'envoyer les erreurs restantes √† une √©quipe qui a contact√© des d√©veloppeurs externes et les a aid√©s √† corriger leur application. <br><br>  Pour chacune de ces applications, nous: <br><br><ul><li>  Nous cr√©ons une t√¢che dans JIRA. </li><li>  Nous enregistrons des statistiques dans Influx. </li><li>  Nous pr√©parons des d√©clencheurs d'intervention chirurgicale en cas de forte augmentation du nombre d'erreurs. </li></ul><br>  Le syst√®me de gestion des erreurs client ressemble √† ceci: <br><br><img src="https://habrastorage.org/webt/5j/yf/s8/5jyfs8mn48kpqhefrw3bnvyfynm.png"><br><br>  Une fois par semaine, nous collectons des rapports sur les 10 pires applications en fonction du nombre d'erreurs. <br><br><h3>  Ne pas attraper, mais avertir </h3><br>  Ainsi, nous avons appris √† trouver des erreurs dans le syst√®me de production, appris √† travailler avec les erreurs de serveur et les erreurs de client.  Tout semble aller bien, mais ... <br><br>  Mais en fait, nous r√©pondons trop tard - les bogues affectent d√©j√† les utilisateurs! <br><br>  Pourquoi ne pas essayer de trouver des erreurs plus t√¥t? <br>  Bien s√ªr, il serait cool de tout trouver dans des environnements de test.  Mais les environnements de test sont des espaces de bruit blanc.  Ils sont en d√©veloppement actif, chaque jour plusieurs versions de serveurs fonctionnent.  Il est trop t√¥t pour d√©tecter les erreurs de mani√®re centralis√©e.  Il y a trop d'erreurs, trop souvent tout change. <br><br>  Cependant, la soci√©t√© dispose d'environnements sp√©ciaux dans lesquels tous les assemblages stables sont int√©gr√©s pour v√©rifier les performances, la r√©gression manuelle centralis√©e et les tests de haute disponibilit√©.  En r√®gle g√©n√©rale, ces environnements ne sont pas encore suffisamment stables.  Cependant, certaines √©quipes sont int√©ress√©es √† analyser les probl√®mes avec ces environnements. <br><br>  Mais il y a un obstacle de plus - Hadoop ne collecte pas de donn√©es de ces environnements!  Nous ne pouvons pas utiliser la m√™me m√©thode pour d√©tecter les erreurs; nous devons rechercher une source de donn√©es diff√©rente. <br><br>  Apr√®s une courte recherche, nous avons d√©cid√© de traiter le streaming des statistiques, en lisant les donn√©es de la file d'attente dans laquelle nos services √©crivent pour le transfert vers Hadoop.  Il suffisait d'accumuler des erreurs uniques et de les traiter par lots, par exemple une fois toutes les 30 minutes.  Il est facile de mettre en place un syst√®me de mise en file d'attente qui d√©livre des donn√©es - il ne restait plus qu'√† affiner la r√©ception et le traitement. <br><br>  Nous avons commenc√© √† observer le comportement des erreurs trouv√©es apr√®s d√©tection.  Il s'est av√©r√© que la plupart des erreurs trouv√©es et non corrig√©es apparaissent plus tard en production.  Donc, nous les trouvons correctement. <br><br>  Ainsi, nous avons construit un prototype du syst√®me, des institutions et des erreurs de suivi.  D√©j√† sous sa forme actuelle, il vous permet d'am√©liorer la qualit√© du syst√®me, de constater et de corriger les erreurs avant que les utilisateurs ne les connaissent.  Si, auparavant, nous traitions des dizaines de milliers de demandes erron√©es par semaine, ce nombre n'est plus que de 2 √† 3 000.  Et nous les corrigeons beaucoup plus rapidement. <br><br><h3>  Et ensuite </h3><br>  Bien s√ªr, nous ne nous arr√™terons pas l√† et continuerons d'am√©liorer le syst√®me de recherche et de suivi des erreurs.  Nous avons des plans: <br><br><ul><li>  Analyse de plus d'erreurs API. </li><li>  Int√©gration avec des tests fonctionnels. </li><li>  Fonctionnalit√©s suppl√©mentaires pour enqu√™ter sur les incidents dans notre syst√®me. </li></ul><br>  Mais plus √† ce sujet la prochaine fois. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr473456/">https://habr.com/ru/post/fr473456/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr473442/index.html">Quelque chose va mal tourner, et c'est tr√®s bien: comment gagner un hackathon avec une √©quipe de trois</a></li>
<li><a href="../fr473444/index.html">Accord de non-concurrence: l'essentiel est de ne pas avoir peur</a></li>
<li><a href="../fr473446/index.html">DevOops 2019 et C ++ Russia 2019 Piter gratuits</a></li>
<li><a href="../fr473452/index.html">Intelligence artificielle Robotique InterSystems IRIS</a></li>
<li><a href="../fr473454/index.html">Automatisation des tests √† l'aide de Selenide via Selenoid dans un conteneur Docker</a></li>
<li><a href="../fr473460/index.html">A la question des math√©matiques</a></li>
<li><a href="../fr473462/index.html">Semaine de la s√©curit√© 44: NordVPN, TorGuard et Half Hack</a></li>
<li><a href="../fr473468/index.html">Comprendre le th√©or√®me de Bayes</a></li>
<li><a href="../fr473470/index.html">GitLab 12.4 avec une am√©lioration de la demande de fusion et des d√©pendances de l'API d'audit</a></li>
<li><a href="../fr473476/index.html">V√©rification de l'encapsuleur OpenCvSharp sur OpenCV avec PVS-Studio</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>