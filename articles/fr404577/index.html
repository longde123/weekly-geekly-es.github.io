<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñêüèº üßí ‚è© Evolution de l'approche de d√©ploiement de code dans Reddit üêû üôçüèæ ü•£</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous, au sein de l' √©quipe du service de paiement blockchain de Wirex , connaissons l'exp√©rience de la n√©cessit√© d'affiner et d'am√©liorer constamment ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Evolution de l'approche de d√©ploiement de code dans Reddit</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/wirex/blog/404577/"><img src="https://habrastorage.org/web/ff7/6e9/159/ff76e9159228455b8f91663d289c8e2f.png" alt="image"><br><br>  <i>Nous, au sein de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©quipe du</a> service de paiement blockchain de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wirex</a> , connaissons l'exp√©rience de la n√©cessit√© d'affiner et d'am√©liorer constamment la solution technologique existante.</i>  <i>L'auteur du mat√©riel ci-dessous parle de l'histoire de l'√©volution du d√©ploiement de code de la c√©l√®bre plateforme de nouvelles sociales Reddit.</i> <br><br><blockquote>  ¬´Il est important de suivre la direction de votre d√©veloppement afin de pouvoir l'envoyer dans le bon sens dans les d√©lais.¬ª </blockquote><br>  L'√©quipe Reddit d√©ploie constamment du code.  Tous les membres de l'√©quipe de d√©veloppement √©crivent r√©guli√®rement du code qui est rev√©rifi√© par l'auteur lui-m√™me, et test√© de l'ext√©rieur, pour qu'il puisse ensuite aller √† la "production".  Chaque semaine, nous effectuons au moins 200 ¬´d√©ploiements¬ª, dont chacun prend g√©n√©ralement un total de moins de 10 minutes. <br><br>  Le syst√®me qui fournit tout cela a √©volu√© au fil des ans.  Voyons ce qui a chang√© en elle tout ce temps et ce qui est rest√© inchang√©. <br><br><h3>  D√©but de l'histoire: d√©ploiements stables et r√©currents (2007-2010) </h3><br>  L'ensemble du syst√®me que nous avons aujourd'hui est pass√© d'une graine - un script Perl appel√© push.  Il a √©t√© √©crit il y a longtemps, √† des moments tr√®s diff√©rents pour Reddit.  Toute notre √©quipe technique √©tait si petite √† l'√©poque qu'elle <a href="">s'int√©grait</a> tranquillement <a href="">dans une petite ¬´salle de r√©union¬ª</a> .  Nous n'avons pas utilis√© AWS √† l'√©poque.  Le site fonctionnait sur un nombre fini de serveurs et toute capacit√© suppl√©mentaire devait √™tre ajout√©e manuellement.  Tout fonctionnait sur une seule grande application Python monolithique appel√©e r2. <br><a name="habracut"></a><br>  Une chose au fil des ans est rest√©e inchang√©e.  Les demandes ont √©t√© class√©es dans l'√©quilibreur de charge et r√©parties entre les "pools" contenant des serveurs d'applications plus ou moins identiques.  Par exemple, les pages de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rubrique</a> et de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">liste de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">commentaires</a> sont trait√©es par diff√©rents pools de serveurs.  En fait, tout processus r2 peut g√©rer tout type de demande, cependant, la division en pools vous permet de prot√©ger chacun d'eux contre les sauts brusques de trafic dans les pools voisins.  Ainsi, en cas de croissance du trafic, la panne ne menace pas l'ensemble du syst√®me, mais ses pools individuels. <br><br><img src="https://habrastorage.org/web/63d/7b1/8e7/63d7b18e768d4b15a6310ecf6f0e15d5.png" alt="image"><br><br>  La liste des serveurs cibles a √©t√© √©crite manuellement dans le code de l'outil push et le processus de d√©ploiement a fonctionn√© avec un syst√®me monolithique.  L'outil a parcouru la liste des serveurs, s'est connect√© via SSH, a ex√©cut√© l'une des s√©quences de commandes pr√©d√©finies qui ont mis √† jour la copie actuelle du code √† l'aide de git et red√©marr√© tous les processus d'application.  L'essence du processus (le code est grandement simplifi√© pour une compr√©hension g√©n√©rale): <br><br><pre><code class="hljs mel">#            <span class="hljs-string"><span class="hljs-string">`make -C /home/reddit/reddit static`</span></span> <span class="hljs-string"><span class="hljs-string">`rsync /home/reddit/reddit/static public:/var/www/`</span></span> #    app-        #    ,   foreach $h (@hostlist) { <span class="hljs-string"><span class="hljs-string">`git push $h:/home/reddit/reddit master`</span></span> <span class="hljs-string"><span class="hljs-string">`ssh $h make -C /home/reddit/reddit`</span></span> <span class="hljs-string"><span class="hljs-string">`ssh $h /bin/restart-reddit.sh`</span></span> }</code> </pre> <br>  Le d√©ploiement s'est d√©roul√© s√©quentiellement, un serveur apr√®s l'autre.  Pour toute sa simplicit√©, le sch√©ma avait un avantage important: il est tr√®s similaire au ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©ploiement canari</a> ¬ª.  En d√©ployant le code sur plusieurs serveurs et en remarquant des erreurs, vous vous √™tes imm√©diatement rendu compte qu'il y avait des bogues, vous pouviez interrompre (Ctrl-C) le processus et revenir en arri√®re avant que des probl√®mes ne surviennent avec toutes les demandes √† la fois.  La facilit√© de d√©ploiement a permis de v√©rifier facilement et sans cons√©quences graves les √©l√©ments en production et de les annuler s'ils ne fonctionnaient pas.  De plus, il √©tait pratique de d√©terminer quel d√©ploiement particulier a provoqu√© des erreurs, o√π sp√©cifiquement et ce qui doit √™tre annul√©. <br><br>  Un tel m√©canisme a fait un bon travail pour assurer la stabilit√© et le contr√¥le pendant le d√©ploiement.  L'outil a fonctionn√© assez rapidement.  Les choses se sont bien pass√©es. <br><br><h3>  Notre r√©giment est arriv√© (2011) </h3><br>  Ensuite, nous avons embauch√© plus de personnes, il y avait maintenant six d√©veloppeurs et notre nouvelle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´salle de r√©union¬ª est devenue plus spacieuse</a> .  Nous avons commenc√© √† r√©aliser que le processus de d√©ploiement de code n√©cessitait d√©sormais plus de coordination, en particulier lorsque les coll√®gues travaillaient √† domicile.  L'utilitaire push a √©t√© mis √† jour: maintenant, il a annonc√© le d√©but et la fin des d√©ploiements √† l'aide du chatbot IRC, qui s'est simplement assis dans l'IRC et a annonc√© les √©v√©nements.  Les processus effectu√©s pendant les d√©ploiements n'ont subi pratiquement aucun changement, mais maintenant le syst√®me a tout fait pour le d√©veloppeur et a inform√© tout le monde des modifications apport√©es. <br><br>  √Ä partir de ce moment, l'utilisation du chat a commenc√© dans le flux de travail de d√©ploiement.  Les discussions sur la gestion du d√©ploiement √† partir des chats √©taient assez populaires √† l'√©poque, cependant, puisque nous utilisions des serveurs IRC tiers, nous ne pouvions pas faire confiance au chat √† cent pour cent dans la gestion de l'environnement de production, et donc le processus restait au niveau d'un flux d'informations √† sens unique. <br><br>  √Ä mesure que le trafic vers le site augmentait, l'infrastructure qui le soutenait augmentait √©galement.  De temps en temps, nous devions constamment lancer un nouveau groupe de serveurs d'applications et les mettre en service.  Le processus n'√©tait toujours pas automatis√©.  En particulier, la liste d'h√¥tes en push devait encore √™tre mise √† jour manuellement. <br><br>  La puissance des pools √©tait g√©n√©ralement augment√©e en ajoutant plusieurs serveurs √† la fois.  En cons√©quence, le fait de pousser successivement dans la liste a r√©ussi √† effectuer des modifications sur tout un groupe de serveurs dans le m√™me pool, sans affecter les autres, c'est-√†-dire qu'il n'y avait pas de diversification par pools. <br><br><img src="https://habrastorage.org/web/ccd/063/580/ccd0635803d647b7b0161762c2aa5ff7.png" alt="image"><br><br>  UWSGI √©tait utilis√© pour contr√¥ler les processus de travail, et donc lorsque nous avons donn√© √† l'application une commande de red√©marrage, il a tu√© tous les processus existants en m√™me temps, les rempla√ßant par de nouveaux.  Les nouveaux processus ont mis du temps √† se pr√©parer pour traiter les demandes.  Dans le cas d'un red√©marrage involontaire d'un groupe de serveurs situ√©s dans le m√™me pool, la combinaison de ces deux circonstances a s√©rieusement affect√© la capacit√© de ce pool √† r√©pondre aux demandes.  Nous avons donc rencontr√© une limite sur la vitesse de d√©ploiement s√©curis√© du code sur tous les serveurs.  √Ä mesure que le nombre de serveurs augmentait, la dur√©e de toute la proc√©dure augmentait. <br><br><h3>  D√©ploiement des instruments de recyclage (2012) </h3><br>  Nous avons enti√®rement repens√© l'outil de d√©ploiement.  Et bien que son nom, malgr√© une alt√©ration compl√®te, soit rest√© le m√™me (push), cette fois il a √©t√© √©crit en Python.  La nouvelle version a connu quelques am√©liorations majeures. <br><br>  Tout d'abord, il a pris la liste des h√¥tes du DNS, et non de la s√©quence cod√©e en dur dans le code.  Cela a permis de mettre √† jour uniquement la liste, sans avoir √† mettre √† jour le code push.  Les d√©buts d'un syst√®me de d√©couverte de services ont √©merg√©. <br><br>  Pour r√©soudre le probl√®me des red√©marrages successifs, nous avons m√©lang√© la liste des h√¥tes avant les d√©ploiements.  Le brassage r√©duit les risques et permet d'acc√©l√©rer le processus. <br><br><img src="https://habrastorage.org/web/b13/1ec/1ee/b131ec1eec994fa0bc9900bd9d8f7766.PNG" alt="image"><br><br>  La version originale a m√©lang√© la liste au hasard √† chaque fois, cependant, cela a rendu difficile une restauration rapide, car chaque fois la liste du premier groupe de serveurs √©tait diff√©rente.  Par cons√©quent, nous avons corrig√© le m√©lange: il g√©n√©rait d√©sormais un certain ordre qui pouvait √™tre utilis√© lors du d√©ploiement r√©p√©t√© apr√®s le rollback. <br><br>  Un autre changement, petit mais important, a √©t√© le d√©ploiement constant d'une version fixe du code.  La version pr√©c√©dente de l'outil mettait toujours √† jour la branche principale sur l'h√¥te cible, mais que se passe-t-il si le ma√Ætre change √† droite pendant le d√©ploiement en raison du lancement par erreur du code par quelqu'un?  Le d√©ploiement d'une r√©vision git donn√©e au lieu d'appeler par nom de branche a permis de s'assurer que la m√™me version de code √©tait utilis√©e sur chaque serveur de production. <br><br>  Enfin, le nouvel outil distingue son code (il fonctionne principalement avec une liste d'h√¥tes et y acc√®de via SSH) et les commandes ex√©cut√©es sur les serveurs.  Cela d√©pendait toujours beaucoup des besoins de R2, mais avait quelque chose comme un prototype d'API.  Cela a permis √† r2 de suivre ses propres √©tapes de d√©ploiement, ce qui a facilit√© les changements de roulement et lib√©r√© le flux.  Voici un exemple de commandes ex√©cut√©es sur un serveur distinct.  Le code, encore une fois, n'est pas le code exact, mais dans l'ensemble, cette s√©quence d√©crit bien le flux de travail r2: <br><br><pre> <code class="hljs pgsql">sudo /opt/reddit/deploy.py <span class="hljs-keyword"><span class="hljs-keyword">fetch</span></span> reddit sudo /opt/reddit/deploy.py deploy reddit f3bbbd66a6 sudo /opt/reddit/deploy.py <span class="hljs-keyword"><span class="hljs-keyword">fetch</span></span>-names sudo /opt/reddit/deploy.py <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> <span class="hljs-keyword"><span class="hljs-keyword">all</span></span></code> </pre> <br>  Les noms de r√©cup√©ration sont particuli√®rement int√©ressants: cette instruction est propre √† r2. <br><br><h3>  Autoscaling (2013) </h3><br>  Ensuite, nous avons finalement d√©cid√© de passer √† un cloud avec une mise √† l'√©chelle automatique (un sujet pour un article s√©par√©).  Cela nous a permis d'√©conomiser beaucoup d'argent dans les moments o√π le site n'√©tait pas charg√© de trafic et d'augmenter automatiquement la capacit√© pour faire face √† toute forte augmentation des demandes. <br><br>  Les am√©liorations pr√©c√©dentes, en chargeant automatiquement la liste des h√¥tes √† partir du DNS, ont fait de cette transition une √©vidence.  La liste des h√¥tes a chang√© plus souvent qu'auparavant, mais du point de vue de l'outil de d√©ploiement, cela n'a jou√© aucun r√¥le.  Le changement, qui a √©t√© introduit √† l'origine comme une am√©lioration de la qualit√©, est devenu l'un des composants cl√©s n√©cessaires pour ex√©cuter la mise √† l'√©chelle automatique. <br><br>  Cependant, l'autoscaling a conduit √† quelques cas limites int√©ressants.  Il fallait contr√¥ler les lancements.  Que se passe-t-il si le serveur d√©marre juste pendant le d√©ploiement?  Nous devions nous assurer que chaque nouveau serveur en cours d'ex√©cution v√©rifiait la disponibilit√© du nouveau code et le prenait, le cas √©ch√©ant.  Nous ne pouvions pas oublier que les serveurs se d√©connectaient au moment du d√©ploiement.  L'outil devait devenir plus intelligent et apprendre √† d√©terminer que le serveur √©tait hors ligne dans le cadre de la proc√©dure, et non √† la suite d'une erreur survenue pendant le d√©ploiement.  Dans ce dernier cas, il a d√ª avertir fortement tous les coll√®gues impliqu√©s dans le probl√®me. <br><br>  En m√™me temps, nous avons, en passant, et pour diverses raisons, pass√© de uWSGI √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gunicorn</a> .  Cependant, du point de vue du sujet de ce billet, une telle transition n'a pas entra√Æn√© de changements significatifs. <br><br>  Cela a donc fonctionn√© pendant un certain temps. <br><br><h3>  Trop de serveurs (2014) </h3><br>  Au fil du temps, le nombre de serveurs n√©cessaires pour r√©pondre aux pics de trafic a augment√©.  Cela a conduit au fait que les d√©ploiements n√©cessitaient de plus en plus de temps.  Dans le pire des cas, un d√©ploiement normal a pris environ une heure - un mauvais r√©sultat. <br><br>  Nous avons r√©√©crit l'outil afin qu'il puisse prendre en charge le travail parall√®le avec les h√¥tes.  La nouvelle version est appel√©e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rouleau √† p√¢tisserie</a> .  L'ancienne version n√©cessitait beaucoup de temps pour initialiser les connexions ssh et attendre la fin de toutes les commandes, donc la parall√©lisation dans des limites raisonnables nous a permis d'acc√©l√©rer le d√©ploiement.  Le temps de d√©ploiement a de nouveau √©t√© r√©duit √† cinq minutes. <br><br><img src="https://habrastorage.org/web/efb/7ba/c3d/efb7bac3df5a4905897b462dea14bf62.PNG" alt="image"><br><br>  Pour r√©duire l'impact du red√©marrage simultan√© de plusieurs serveurs, le composant de mixage de l'outil est devenu plus intelligent.  Au lieu de m√©langer aveugl√©ment la liste, il a tri√© les pools de serveurs afin que les h√¥tes d'un pool soient <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">aussi √©loign√©s que possible</a> . <br><br>  Le changement le plus important dans le nouvel outil a √©t√© que l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">API entre l'outil de d√©ploiement et les outils sur chaque serveur a</a> √©t√© d√©finie beaucoup plus clairement et s√©par√©e des besoins de r2.  Initialement, cela a √©t√© fait par d√©sir de rendre le code plus orient√© open-source, mais bient√¥t cette approche a √©t√© tr√®s utile d'une autre mani√®re.  Voici un exemple de d√©ploiement avec la s√©lection de commandes d'API lanc√©es √† distance: <br><br><img src="https://habrastorage.org/web/a8b/2d4/750/a8b2d4750f65444998587581f1d1132f.png" alt="image"><br><br><h3>  Trop de monde (2015) </h3><br>  Soudain, un moment est venu o√π, en fin de compte, beaucoup de gens travaillaient d√©j√† sur r2.  C'√©tait cool, et en m√™me temps signifiait qu'il y aurait encore plus de d√©ploiements.  Le respect de la r√®gle d'un d√©ploiement √† la fois est devenu de plus en plus difficile.  Les d√©veloppeurs devaient se mettre d'accord sur la proc√©dure de d√©livrance du code.  Pour optimiser la situation, nous avons ajout√© un autre √©l√©ment au chatbot qui coordonne la file d'attente de d√©ploiement.  Les ing√©nieurs ont demand√© une r√©serve de d√©ploiement et l'ont re√ßue, ou leur code a √©t√© "mis en file d'attente".  Cela a aid√© √† rationaliser les d√©ploiements, et ceux qui voulaient les terminer pouvaient calmement attendre leur tour. <br><br>  Un autre ajout important √† mesure que l'√©quipe grandissait √©tait de suivre les d√©ploiements en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un seul endroit</a> .  Nous avons chang√© l'outil de d√©ploiement pour envoyer des m√©triques √† Graphite.  Cela a permis de suivre facilement la corr√©lation entre les d√©ploiements et les changements de m√©trique. <br><br><h3>  De nombreux (deux) services (√©galement en 2015) </h3><br>  Tout √† coup, le moment de la sortie du deuxi√®me service en ligne est venu.  C'√©tait une version mobile du site Web avec sa propre pile compl√®tement diff√©rente, ses propres serveurs et le processus de construction.  Il s'agissait du premier v√©ritable test d'une API d'outil de d√©ploiement divis√©.  En y ajoutant la capacit√© de travailler sur toutes les √©tapes de montage dans diff√©rents ¬´emplacements¬ª pour chaque projet, il a pu supporter la charge et faire face √† la maintenance de deux services au sein d'un m√™me syst√®me. <br><br><h3>  25 services (2016) </h3><br>  L'ann√©e suivante, nous avons assist√© √† l'expansion rapide de l'√©quipe.  Au lieu de deux services, deux douzaines sont apparues, au lieu de deux √©quipes de d√©veloppement, quinze.  La plupart des services ont √©t√© construits soit sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Baseplate</a> , notre infrastructure principale, soit sur des applications clientes, similaires au Web mobile.  L'infrastructure derri√®re les d√©ploiements est la m√™me pour tout le monde.  Bient√¥t, de nombreux autres nouveaux services seront disponibles en ligne, et tout cela est largement d√ª √† la polyvalence du rouleau √† p√¢tisserie.  Il vous permet de simplifier le lancement de nouveaux services √† l'aide d'outils familiers aux personnes. <br><br><h3>  Airbag (2017) </h3><br>  √Ä mesure que le nombre de serveurs dans le monolithe augmentait, le temps de d√©ploiement augmentait.  Nous voulions augmenter consid√©rablement le nombre de d√©ploiements parall√®les, mais cela entra√Ænerait trop de red√©marrages simultan√©s des serveurs d'applications.  De telles choses, bien s√ªr, entra√Ænent une baisse du d√©bit et une perte de la capacit√© de traiter les demandes entrantes en raison de la surcharge des serveurs restants. <br><br>  Le processus principal de Gunicorn a utilis√© le m√™me mod√®le que uWSGI, rechargeant tous les travailleurs en m√™me temps.  Les nouveaux processus de travail n'ont pas pu r√©pondre aux demandes tant qu'ils n'ont pas √©t√© enti√®rement charg√©s.  Le temps de lancement de notre monolithe variait de 10 √† 30 secondes.  Cela signifie que pendant cette p√©riode, nous ne pourrions pas du tout traiter les demandes.  Pour trouver un moyen de sortir de cette situation, nous avons remplac√© le processus principal de gunicorn par le gestionnaire de travail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Einhorn</a> de Stripe, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tout en pr√©servant la pile HTTP Gunicorn et le conteneur WSGI</a> .  Pendant le red√©marrage, Einhorn cr√©e un nouveau travailleur, attend qu'il soit pr√™t, se d√©barrasse d'un ancien travailleur et r√©p√®te le processus jusqu'√† la fin de la mise √† jour.  Cela cr√©e un airbag et nous permet de maintenir la bande passante √† un niveau pendant les d√©ploiements. <br><br>  Le nouveau mod√®le a cr√©√© un autre probl√®me.  Comme mentionn√© pr√©c√©demment, le remplacement d'un travailleur par un nouveau et enti√®rement termin√© a pris jusqu'√† 30 secondes.  Cela signifiait que s'il y avait un bogue dans le code, il ne s'affichait pas imm√©diatement et r√©ussissait √† se d√©ployer sur de nombreux serveurs avant d'√™tre d√©tect√©.  Pour √©viter cela, nous avons introduit un m√©canisme de blocage de la transition de la proc√©dure de d√©ploiement vers le nouveau serveur, qui √©tait en vigueur jusqu'au red√©marrage de tous les processus de travail.  Il a √©t√© mis en ≈ìuvre simplement - en sondant l'√©tat d'Einhorn et en attendant la disponibilit√© de tous les nouveaux travailleurs.  Pour maintenir la vitesse au m√™me niveau, nous avons augment√© le nombre de serveurs trait√©s en parall√®le, ce qui √©tait compl√®tement s√ªr dans les nouvelles conditions. <br><br>  Un tel m√©canisme nous permet de d√©ployer simultan√©ment sur un nombre beaucoup plus important de machines, et le temps de d√©ploiement, couvrant environ 800 serveurs, est r√©duit √† 7 minutes, en tenant compte des pauses suppl√©mentaires pour v√©rifier les bogues. <br><br><h3>  En regardant en arri√®re </h3><br>  L'infrastructure de d√©ploiement d√©crite ici est un produit n√© de nombreuses ann√©es d'am√©liorations coh√©rentes, plut√¥t que d'un effort cibl√© unique.  Les √©chos des d√©cisions prises une fois et prises aux premiers stades des compromis se font encore sentir dans le syst√®me actuel, et cela a toujours √©t√© le cas √† toutes les √©tapes.  Une telle approche √©volutive a ses avantages et ses inconv√©nients: elle n√©cessite un minimum d'efforts √† tout moment, cependant, il y a un risque t√¥t ou tard de s'arr√™ter.  Il est important de suivre la direction de votre d√©veloppement afin de pouvoir l'envoyer dans le bon sens dans les d√©lais. <br><br><h3>  Le futur </h3><br>  L'infrastructure Reddit devrait √™tre pr√™te pour le soutien continu de l'√©quipe √† mesure qu'elle grandit et lance de nouvelles choses.  Le taux de croissance de l'entreprise est plus rapide que jamais, et nous travaillons sur des projets encore plus int√©ressants et de grande envergure que tout ce que nous faisions auparavant.  Les probl√®mes auxquels nous sommes confront√©s aujourd'hui sont de double nature: d'une part, il est n√©cessaire d'augmenter l'autonomie des d√©veloppeurs, d'autre part, de maintenir la s√©curit√© de l'infrastructure de production et d'am√©liorer l'airbag, ce qui permet aux d√©veloppeurs d'effectuer rapidement et en toute confiance des d√©ploiements. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/files/4bd/bf6/597/4bdbf659775744b1bdbb4d8a00a0a980.png" alt="image"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr404577/">https://habr.com/ru/post/fr404577/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr404567/index.html">Editeur Peter. Soldes d'√©t√©</a></li>
<li><a href="../fr404569/index.html">Particules, antiparticules et leur annihilation</a></li>
<li><a href="../fr404571/index.html">Les r√©seaux sociaux - une nouvelle source s√©rieuse de cybermenaces</a></li>
<li><a href="../fr404573/index.html">Gagner le ¬´monstre math√©matique¬ª: ce n'est pas une question de nombres, mais d'apprendre √† penser</a></li>
<li><a href="../fr404575/index.html">L'IA de Microsoft a battu le record d'un homme chez Mme Pac-man</a></li>
<li><a href="../fr404579/index.html">Polybius ICO a lev√© 20 millions de dollars en deux semaines</a></li>
<li><a href="../fr404583/index.html">Facebook exp√©rimente un bot qui peut n√©gocier et mentir</a></li>
<li><a href="../fr404585/index.html">Comme nous dans le village de chalets fourni Internet</a></li>
<li><a href="../fr404587/index.html">Fiction et fantaisie en deux ans et demi, pr√®s d'une centaine de bons livres</a></li>
<li><a href="../fr404589/index.html">Le monde des drones: des appareils portables aux anti-drones</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>