<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚵🏽 👩🏼‍🔬 🐬 LSTM - ANN Dynamische Preisgestaltung im Haushaltswareneinzelhandel 🤖 🐔 💇🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es ist kein Geheimnis, dass Methoden des maschinellen Lernens überall in verschiedene Geschäftsbereiche vordrangen und neue Geschäftsprozesse optimier...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>LSTM - ANN Dynamische Preisgestaltung im Haushaltswareneinzelhandel</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/421429/"> Es ist kein Geheimnis, dass Methoden des maschinellen Lernens überall in verschiedene Geschäftsbereiche vordrangen und neue Geschäftsprozesse optimierten, verbesserten und sogar schufen.  Einer der wichtigen Bereiche ist das Problem der Festlegung des Warenpreises. Hier hilft das MO mit genügend Daten dabei, das zu tun, was zuvor schwer zu erreichen war - die Multi-Faktor-Nachfragekurve aus den Daten wiederherzustellen.  Dank der wiederhergestellten Nachfragekurve konnten dynamische Preissysteme aufgebaut werden, die eine Optimierung der Preise je nach Zweck der Preisgestaltung ermöglichen - um Umsatz oder Gewinn zu steigern.  Dieser Artikel ist eine Zusammenstellung meiner Dissertation, in der das dynamische Preismodell LSTM-ANN für 4 Wochen für eine der Waren des Haushaltswarenhändlers entwickelt und in der Praxis getestet wurde. <br><a name="habracut"></a><br>  Ich möchte sofort darauf hinweisen, dass ich in diesem Artikel nicht den Namen des Unternehmens offenlegen werde, in dem die Studie durchgeführt wurde (dies ist jedoch eines der Unternehmen aus der Liste in den Räumlichkeiten), sondern einfach den Namen "Einzelhändler". <br><br><h2>  Hintergrund </h2><br>  Auf dem Einzelhandelsmarkt für Haushaltswaren gibt es einen Preisführer, Leroy Merlin.  Das Verkaufsvolumen dieses Netzwerks ermöglicht es ihnen, eine Mindestpreisstrategie für die gesamte Produktpalette beizubehalten, was zu einem Preisdruck auf andere Marktteilnehmer führt. <br><br>  Umsatz und Gewinn der wichtigsten Einzelhändler in St. Petersburg zum 31. Dezember 2017 <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b56/KxDRU-7Yq0c.jpg" alt="Bild"><br><br>  In dieser Hinsicht verfolgt der Einzelhändler einen anderen Preisansatz: <br><br><ul><li>  Der Preis wird auf den niedrigsten der Wettbewerber festgelegt. </li><li>  Preisbeschränkung von unten: Kaufpreis + Mindestprämie, die die ungefähren Kosten pro Wareneinheit widerspiegelt. </li></ul><br>  Dieser Ansatz ist eine Kombination aus der kostspieligen Preismethode und der Preisgestaltung der Wettbewerber.  Es ist jedoch nicht perfekt - es berücksichtigt nicht direkt die Verbrauchernachfrage. <br><br>  Aufgrund der Tatsache, dass das dynamische Preismodell viele Faktoren (Nachfrage, Saisonalität, Werbeaktionen, Preise der Wettbewerber) berücksichtigt und es Ihnen auch ermöglicht, den vorgeschlagenen Preis einzuschränken (z. B. von unten nach unten abdeckenden Kosten), wird dieses System möglicherweise alle Einseitigkeit und Unbedenklichkeit beseitigen Nachteile anderer Preismethoden. <br><br><h2>  Daten </h2><br>  Für die Studie lieferte das Unternehmen Daten von Januar 2015 bis Juli 2017 (920 Tage / 131 Wochen).  Diese Daten enthalten: <br><br><ul><li>  Tagesverkauf, einschließlich Wochenenden, für 470 Produkte (16 Produktgruppen); </li><li>  Werbetage im Laden; </li><li>  Tage, an denen Rabatte auf Waren gewährt wurden; </li><li>  Preise für jedes der 470 Produkte; </li><li>  Tägliche Daten zur Anzahl der Kontrollen im gesamten Netzwerk in St. Petersburg; </li><li>  Preise der Hauptkonkurrenten für die meisten der 470 Produkte (Daten wurden einmal pro Woche aufgenommen). </li></ul><br>  Zusätzlich zu diesen Daten habe ich auch Kalender-Dummy-Variablen hinzugefügt: <br><br><ul><li>  Jahreszeit (Herbst / Winter / Sommer / Frühling); </li><li>  Monat </li><li>  Viertel; </li><li>  Wochentag; </li><li>  Feiertage; </li></ul><br>  Auch Wettervariablen: <br><br><ul><li>  Niederschlag - Dummy; </li><li>  Temperatur </li><li>  Die Abweichung der Temperatur vom Durchschnitt der Saison. </li></ul><br>  Bei der direkten Analyse des täglichen Warenverkaufs stellte ich Folgendes fest: <br>  Nur etwa 30% der Waren wurden ständig verkauft, alle anderen Waren wurden entweder später als 2015 zum Verkauf angeboten oder früher als 2017 aus dem Verkauf genommen, was zu einer erheblichen Einschränkung der Auswahl von Waren für Forschungs- und Preisexperimente führte.  Dies führt uns auch zu der Tatsache, dass es aufgrund des ständigen Warenwechsels in der Linie des Geschäfts schwierig wird, ein integriertes Rohstoffpreissystem zu schaffen. Es gibt jedoch einige Möglichkeiten, um dieses Problem zu umgehen, auf das später eingegangen wird. <br><br><h2>  Preissystem </h2><br>  Um ein System von Preisempfehlungen für die Waren für den nächsten Zeitraum auf der Grundlage eines Modells aufzubauen, das die Nachfrage vorhersagt, habe ich das folgende Schema entwickelt: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125ba/ZaHF_uL_FD4.jpg" alt="Bild"><br><br>  Nachdem wir das Modell anhand der Daten trainiert haben, erhalten wir ein Modell, das die Multi-Faktor-Nachfragekurve wiederherstellt, indem wir dem Input verschiedene Warenpreise liefern. In Abhängigkeit von diesem Preis erhalten wir den geschätzten Umsatz.  So können wir eine Preisoptimierung durchführen, um das gewünschte Ergebnis zu erzielen - den erwarteten Umsatz oder den erwarteten Gewinn maximieren.  Es bleibt nur ein Modell zu trainieren, das den Umsatz gut vorhersagen kann. <br><br>  Was hat nicht geklappt <br>  Nachdem ich eines der Produkte für die Forschung ausgewählt hatte, verwendete ich XGBoost, bevor ich direkt zum LSTM-Modell überging. <br><br>  Ich habe dies in der Erwartung getan, dass XGBoost mir hilft, viele unnötige Faktoren zu verwerfen (dies geschieht automatisch), und die verbleibenden sollten für LSTM-Modelle verwendet werden.  Ich habe diesen Ansatz bewusst gewählt, weil ich, um unnötige Fragen zur Verteidigung der Dissertation zu vermeiden, eine starke und gleichzeitig einfache Rechtfertigung der Wahl der Faktoren für das Modell einerseits und andererseits eine Vereinfachung der Entwicklung erhalten wollte.  Außerdem erhielt ich ein fertiges, grobes Modell, an dem verschiedene Ideen in der Studie schnell getestet werden konnten.  Und nachdem Sie ein endgültiges Verständnis dafür erhalten haben, was funktionieren wird und was nicht, erstellen Sie das endgültige LSTM-Modell. <br><br>  Um das Prognoseproblem zu verstehen, gebe ich einen täglichen Verkaufsplan für das erste ausgewählte Produkt an: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125c4/rwun1M2dbAI.jpg" alt="Bild"><br><br>  Die gesamte Zeitreihe der Verkäufe in der Grafik wurde in durchschnittliche Verkäufe für den Zeitraum unterteilt, um nicht die tatsächlichen Werte preiszugeben, sondern das Erscheinungsbild beizubehalten. <br><br>  Im Allgemeinen viel Lärm, während es ausgeprägte Bursts gibt - dies ist die Durchführung von Werbeaktionen auf Netzwerkebene. <br><br>  Da es für mich die erste Erfahrung beim Erstellen von Modellen für maschinelles Lernen war, musste ich ziemlich viel Zeit mit verschiedenen Artikeln und Dokumentationen verbringen, damit letztendlich etwas für mich klappte. <br><br>  Eine erste Liste von Faktoren, die vermutlich den Umsatz beeinflussen: <br><br><ul><li>  Daten zum täglichen Verkauf anderer Waren dieser Gruppe, zum Gesamtumsatz in der Gruppe in Stücken und zur Anzahl der Schecks für alle Geschäfte in St. Petersburg mit Verzögerungen 1, 2, 3, 7, 14, 21, 28; </li><li>  Angaben zu Preisen anderer Waren der Gruppe; </li><li>  Das Verhältnis des Preises des untersuchten Produkts zu den Preisen anderer Waren aus der Gruppe; </li><li>  Der niedrigste Preis unter allen Wettbewerbern (die Daten wurden einmal pro Woche erhoben, und ich ging davon aus, dass diese Preise für die nächste Woche gültig sein werden); </li><li>  Das Verhältnis des Preises des untersuchten Produkts zum niedrigsten Preis der Wettbewerber; </li><li>  Verkaufsverzögerungen nach Gruppen (in Stücken); </li><li>  Einfacher Durchschnitt und RSI basierend auf Verkaufsverzögerungen von Waren der Gruppe, Gesamtverkäufen in der Gruppe und Anzahl der Schecks. </li></ul><br>  Insgesamt 380 Faktoren.  (2,42 Beobachtungen pro Faktor).  Daher war das Problem, unwesentliche Faktoren abzuschneiden, sehr hoch. XGBoost half jedoch dabei, dies zu bewältigen, und reduzierte die Anzahl der Faktoren signifikant auf 23 (40 Beobachtungen pro Faktor). <br><br>  Das beste Ergebnis, das ich mit der Gier-Suche erzielen konnte, ist wie folgt: <br><br><img src="https://pp.userapi.com/c850120/v850120780/126b5/CfMx4e8ESwc.jpg" alt="Bild"><br>  R ^ 2-adj = 0,4 an der Testprobe <br><br>  Die Daten wurden ohne Mischen in Trainings- und Testproben unterteilt (da dies eine Zeitreihe ist).  Als Metrik habe ich den Indikator R ^ 2 bewusst angepasst, da die Präsentation der Endergebnisse der Arbeit vor der Kommission stattfinden musste, inkl.  Es bestand aus Wirtschaftsvertretern und wurde daher als das bekannteste und am einfachsten zu verstehende verwendet. <br><br>  Die endgültigen Ergebnisse schmälerten meinen Glauben an den Erfolg, da das Ergebnis von R ^ 2-adj 0,4 nur bedeutete, dass das Vorhersagesystem die Nachfrage am nächsten Tag nicht gut vorhersagen konnte und die Empfehlung für den Preis sich nicht wesentlich vom "Finger am Himmel" -System unterscheiden würde. <br><br>  Außerdem habe ich mich entschlossen zu prüfen, wie effektiv die Verwendung von XGBoost für die Vorhersage des täglichen Umsatzes einer Warengruppe (in Witzen) und für die Vorhersage der Anzahl der Schecks im Allgemeinen über das Netzwerk sein wird. <br><br>  Umsatz nach Produktgruppen: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125df/mvImj91seQs.jpg" alt="Bild"><br>  R ^ 2-adj = 0,71 <br><br>  Schecks: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125e9/9cv4Fstpybc.jpg" alt="Bild"><br>  R ^ 2-adj = 0,86 <br><br>  Ich denke, der Grund, warum die Verkaufsdaten für ein bestimmtes Produkt nicht vorhergesagt werden konnten, geht aus den dargestellten Grafiken hervor - Rauschen.  Der individuelle Verkauf von Waren erwies sich als zu anfällig für Zufälle, so dass die Methode zur Erstellung einer Regression nicht effektiv war.  Gleichzeitig haben wir durch die Aggregation der Daten den Einfluss der Zufälligkeit beseitigt und gute Vorhersagefähigkeiten erhalten. <br><br>  Um endlich sicherzustellen, dass die Vorhersage der Nachfrage für einen Tag im Voraus sinnlos ist, habe ich das SARIMAX-Modell (Statistikmodellpaket für Python) für den täglichen Verkauf verwendet: <br><br><img src="https://pp.userapi.com/c850120/v850120780/12605/d2HKylyPqKY.jpg" alt="Bild"><br><br><img src="https://pp.userapi.com/c850120/v850120341/11b45/FNzCYjeSuMM.jpg" alt="Bild"><br><br>  Tatsächlich unterscheiden sich die Ergebnisse in keiner Weise von denen, die mit XGBoost erhalten wurden, was darauf hindeutet, dass die Verwendung eines komplexen Modells in diesem Fall nicht gerechtfertigt ist. <br><br>  Gleichzeitig möchte ich auch darauf hinweisen, dass weder XGBoost- noch SARIMAX-Wetterfaktoren signifikant waren. <br><br><h2>  Das endgültige Modell bauen </h2><br>  Die Lösung für das Problem der Vorhersagequalität bestand darin, Daten auf wöchentlicher Ebene zu aggregieren.  Dies ermöglichte es uns, den Einfluss von Zufallsfaktoren zu reduzieren, reduzierte jedoch die Menge der beobachteten Daten signifikant: Wenn es 920 tägliche Daten gab, nur 131 wöchentliche Daten. Die Situation verschlechterte sich, weil die Anzahl der Faktoren nahezu unverändert blieb (Dummies für die Wochentage wurden ausgeschlossen), aber die Anzahl der Beobachtungen der Zielvariablen stark abgenommen. <br><br>  Darüber hinaus wurde meine Aufgabe durch die Tatsache erschwert, dass das Unternehmen zu diesem Zeitpunkt beschloss, das Produkt zu ändern, das das Experiment unter Verwendung des Modells anwenden soll, sodass ich ein Modell von Grund auf neu entwickeln musste. <br><br>  Der Warenwechsel erfolgte bei Waren mit ausgeprägter Saisonalität: <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b3d/ytgwPwTAb9k.jpg" alt="Bild"><br><br>  Aufgrund der Umstellung auf Wochenverkäufe stellte sich eine logische Frage: Ist es angemessen, das LSTM-Modell überhaupt mit einer so geringen Datenmenge zu verwenden?  Ich habe mich entschlossen, dies in der Praxis herauszufinden und vor allem die Anzahl der Faktoren zu reduzieren (auch wenn dies ein potenzieller Schaden bei der Reduzierung wichtiger Informationen war).  Ich warf alle Faktoren aus, die auf der Grundlage von Verkaufsverzögerungen (Durchschnitt, RSI), Wetterfaktoren berechnet wurden (bei den täglichen Daten spielte das Wetter keine Rolle, und die Übertragung auf das wöchentliche Niveau verlor umso mehr an Sinn).  Danach habe ich traditionell XGBoost verwendet, um andere unbedeutende Faktoren abzuschneiden.  Später habe ich zusätzlich mehrere weitere Faktoren basierend auf dem LSTM-Modell unterteilt, indem ich die Faktoren einfach einzeln ausgeschlossen, das Modell erneut trainiert und die Ergebnisse verglichen habe. <br><br>  Die endgültige Liste der Faktoren lautet wie folgt: <br><br><ul><li>  Das Verhältnis des Preises pro Kilogramm des untersuchten Produkts und des Primers CERESIT ST 17 10 l.; </li><li>  Das Verhältnis des Preises des untersuchten Produkts und des Produkts und der Grundierung CERESIT ST 17 10 l; </li><li>  Das Verhältnis des Preises des untersuchten Produkts und des Primers EURO PRIMER 3 l; </li><li>  Das Verhältnis des Preises des untersuchten Produkts und des Mindestpreises der Wettbewerber; </li><li>  Dummy-Variablen für drei Werbeaktionen auf Netzwerkebene; </li><li>  Dummy-Variablen der Frühlings-, Sommer- und Herbstsaison; </li><li>  Protokolliert 1 - 5 wöchentliche Verkäufe des untersuchten Produkts. </li></ul><br>  Nur 15 Faktoren (9 Beobachtungen pro Faktor). <br><br>  Das endgültige LSTM-Modell wurde unter Verwendung von Keras geschrieben, enthielt 2 verborgene Schichten (25 bzw. 20 Neuronen) und der Aktivator war ein Sigmoid. <br><br>  Der Code für das endgültige LSTM-Modell mit Keras: <br><br><pre><code class="python hljs">model = Sequential() model.add(LSTM(<span class="hljs-number"><span class="hljs-number">25</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">1</span></span>, trainX.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>]))) model.add(LSTM(<span class="hljs-number"><span class="hljs-number">20</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>) model.fit(trainX, trainY, epochs=<span class="hljs-number"><span class="hljs-number">40</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'LSTM_W.h5'</span></span>)</code> </pre> <br>  Ergebnis: <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b4d/p2WlWsInzls.jpg" alt="Bild"><br><br><img src="https://pp.userapi.com/c850120/v850120780/12618/brScDD1suqE.jpg" alt="Bild"><br><br>  Die Qualität der Vorhersage auf der Testprobe sah durch die Metrik ziemlich überzeugend aus, erreichte jedoch meiner Meinung nach immer noch nicht das Ideal, da Bursts in einzelnen Wochen trotz der ziemlich genauen Bestimmung des durchschnittlichen Umsatzniveaus erheblich von "abweichen konnten." durchschnittliches Umsatzniveau, das an einzelnen Tagen eine starke Abweichung der Umsatzprognose von der Realität ergab (bis zu 50%).  Trotzdem habe ich dieses Modell direkt für das Experiment in der Praxis verwendet. <br><br>  Interessant ist auch, wie die wiederhergestellte Nachfragekurve preislich aussieht.  Zu diesem Zweck habe ich das Modell über die Preisspanne geführt und basierend auf den prognostizierten Verkäufen eine Nachfragekurve erstellt: <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b34/bxcLRJ0MYwc.jpg" alt="Bild"><br><br><h2>  Ein Experiment </h2><br>  Jede Woche lieferte das Netzwerk Verkaufsdaten für die Vorwoche in St. Petersburg sowie Preise von Wettbewerbern.  Basierend auf diesen Daten habe ich eine Preisoptimierung durchgeführt, um die erwarteten Gewinne zu maximieren, sagte der Preis, den das Netzwerk für die nächste Woche festlegen sollte, was es auch tat.  Dies dauerte 4 Wochen (die Laufzeit wurde mit dem Händler vereinbart). <br><br>  Die Gewinnmaximierung wurde mit Einschränkungen durchgeführt: Der Mindestpreis war der Kaufpreis + Fix.  Aufpreis wurde der Höchstpreis durch den Preis der Grundierung desselben Herstellers nur in einer 10l Packung begrenzt. <br><br>  Die experimentellen Ergebnisse sind in den folgenden Tabellen dargestellt (alle Zahlen werden durch einen bestimmten Wert geteilt, um die absoluten Werte nicht preiszugeben): <br><br>  Umsatzprognose: <br><br><img src="https://pp.userapi.com/c850120/v850120780/1264e/YRDIUbPG9xQ.jpg" alt="Bild"><br><br>  Gewinnprognose: <br><br><img src="https://pp.userapi.com/c850120/v850120780/12656/vcdgOqaU4B4.jpg" alt="Bild"><br><br>  Um die Auswirkungen des neuen Preissystems auf den Umsatz zu bewerten, habe ich den Umsatz für denselben Zeitraum nur für die Vorjahre verglichen. <br><br>  4 Wochen Zusammenfassung Ergebnisse: <br><br><img src="https://pp.userapi.com/c850120/v850120780/1265e/n4R8YcCFEqg.jpg" alt="Bild"><br><br>  Als Ergebnis erhalten wir ein zweifaches Bild: völlig unrealistische Umsatzprognosen, aber gleichzeitig rein positive Ergebnisse für Wirtschaftsindikatoren (sowohl in Bezug auf Gewinn als auch Umsatz). <br><br>  Die Erklärung ist meiner Meinung nach, dass in diesem Fall das Modell, das den Umsatz falsch vorhersagte, dennoch die richtige Idee hatte - die Preiselastizität für dieses Produkt lag unter 1, was bedeutet, dass der Preis erhöht werden könnte. ohne Angst vor einem Umsatzrückgang, den wir gesehen haben (der Absatz in Einheiten blieb in etwa auf dem Niveau des Vorjahres und des Vorjahres). <br><br>  Vergessen Sie jedoch nicht, dass 4 Wochen ein kurzfristiger Zeitraum sind und das Experiment nur an einem Produkt durchgeführt wurde.  Langfristig führt eine Überbewertung der Waren in einem Geschäft in der Regel zu einem Umsatzrückgang für das gesamte Geschäft.  Um meine Vermutung darüber zu bestätigen, habe ich mich mit XGBoost entschlossen, zu prüfen, ob die Verbraucher über ein „Gedächtnis“ für die Preise früherer Perioden verfügen (wenn es in der Vergangenheit „insgesamt“ teurer war als die Wettbewerber, geht der Verbraucher zu den Wettbewerbern).  Das heißt,  ob das durchschnittliche Preisniveau für die Gruppe in den letzten 1, 3 und 6 Monaten für den Verkauf nach Produktgruppen angegeben wird. <br><br><img src="https://pp.userapi.com/c850120/v850120780/12698/nJHagmFwIwA.jpg" alt="Bild"><br><br>  In der Tat wurde die Vermutung bestätigt: Auf die eine oder andere Weise beeinflusst das durchschnittliche Preisniveau für frühere Perioden den Umsatz in der aktuellen Periode.  Dies bedeutet, dass es nicht ausreicht, in der aktuellen Periode eine Preisoptimierung für ein einzelnes Produkt durchzuführen - es ist auch notwendig, das allgemeine Preisniveau langfristig zu berücksichtigen.  Dies führt im Allgemeinen zu einer Situation, in der Taktiken (Gewinnmaximierung jetzt) ​​Strategien widersprechen (Überleben im Wettbewerb).  Dies ist jedoch bereits besser den Vermarktern überlassen. <br><br>  Angesichts der Ergebnisse und Erfahrungen könnte ein auf einer Umsatzprognose basierendes Preissystem meiner Meinung nach am besten so aussehen: <br><br><ol><li>  Um einen Schritt weiter von der Produktpalette zu gehen, müssen Sie eine Clusteranalyse durchführen und bedingte Schraubendreher nach Ähnlichkeit und Umsatzprognose gruppieren und den Preis nicht für einen einzelnen Schraubendreher, sondern für diese Untergruppe festlegen, damit das Problem des ständigen Entfernens und Hinzufügens von Produktnomenklaturen vermieden wird. </li><li>  Preisoptimierung im Komplex durchführen - nicht nur für einzelne Warengruppen, sondern auch unter Berücksichtigung langfristiger Effekte.  Zu diesem Zweck können Sie das Modell verwenden, das den Umsatz als Ganzes über das Netzwerk vorhersagt. Glücklicherweise hat es sich selbst bei täglichen Verkäufen als beeindruckend genau herausgestellt. </li></ol><br>  Zusammenfassend möchte ich sagen, dass es für mich als nicht erfahrene Person in der Entwicklung im Allgemeinen und in den Methoden von MO im Besonderen schwierig war, jedoch alles als machbar herauszustellen.  Es war auch interessant, selbst zu überprüfen, wie diese Methoden in der Realität anwendbar sind.  Nachdem ich schon viele Artikel gelesen hatte, brannten meine Augen vor der Tatsache, dass ich versuchen würde, alles selbst zu tun, und ich war in der Erwartung, dass ich hervorragende Ergebnisse erzielen würde.  Die Praxis erwies sich als hart - eine kleine Anzahl von Waren mit einer langen Verkaufsgeschichte, verrauschten täglichen Daten, fehlenden Vorhersagen bei den Verkaufsmengen und der Verwendung komplexer Modelle ist nicht immer gerechtfertigt.  Trotzdem habe ich ein unvergessliches Erlebnis und gelernt, was es bedeutet, Analytik in die Praxis umzusetzen. <br><br>  → Basierend auf der geleisteten Arbeit habe ich ein Projekt in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository</a> vorbereitet <br><br>  Im Repository finden Sie einen Datensatz, der auf der Grundlage von Abhängigkeiten aus realen Daten generiert wurde, sowie ein Python-Skript, mit dem Sie ein virtuelles Experiment mit diesen generierten Daten durchführen können. So können Sie Ihr Glück versuchen und das Modell entsprechend Ihrem Gewinn überholen und den Preis für das Produkt festlegen.  Sie müssen lediglich das Skript herunterladen und ausführen. <br><br>  Ich hoffe, meine Erfahrung wird dazu beitragen, die Grenzen des Einsatzes von MO-Methoden zu bestimmen, und zeigt, dass Geduld und Ausdauer Ergebnisse erzielen können, auch wenn Sie in keinem Bereich ein Profi sind. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de421429/">https://habr.com/ru/post/de421429/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de421417/index.html">Kaffeemaschine für Kaffeesüchtige oder mobile Wacaco-Kaffeemaschine</a></li>
<li><a href="../de421419/index.html">Ode an „geschäumtes“ Nickel, nicht existierende Saphire und den stellvertretenden sowjetischen Minister: die Ikone OTTO SX-P1 in Japan, den USA und der UdSSR</a></li>
<li><a href="../de421421/index.html">LAppS: Eine halbe Million 1 KB WebSocket-Nachrichten pro Sekunde mit TLS auf einer CPU</a></li>
<li><a href="../de421423/index.html">Enterprise DevOps: Wie ein großes Unternehmen Microservices sammelt</a></li>
<li><a href="../de421425/index.html">Zähmen und konsolidieren: Die Geschichte des Wechsels zu Oracle Supercluster</a></li>
<li><a href="../de421431/index.html">Zeitmanagement oder effektives Chaosmanagement</a></li>
<li><a href="../de421433/index.html">Es dauerte genau einen Tag, bis der Server gestartet wurde.</a></li>
<li><a href="../de421435/index.html">"Warum machen wir das alle?" - Prisma-Schöpfer und ehemaliger VK-Projektleiter über sein neues geheimes Projekt</a></li>
<li><a href="../de421439/index.html">Die Japaner stellten einen Prototyp-Prozessor für Exaflops-Supercomputer vor: Wie der Chip funktioniert</a></li>
<li><a href="../de421441/index.html">Embox beginnt den Mount Elbrus zu besteigen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>