<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐗 🌔 🎓 Identifizieren Sie Betrug mithilfe des Enron-Datensatzes. Teil 1, Datenaufbereitung und Auswahl der Zulassungen 🔰 🍠 👇🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Enron Corporation ist eine der bekanntesten Persönlichkeiten der amerikanischen Wirtschaft in den 2000er Jahren. Dies wurde nicht durch ihren Täti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Identifizieren Sie Betrug mithilfe des Enron-Datensatzes. Teil 1, Datenaufbereitung und Auswahl der Zulassungen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/424891/"><p> Die Enron Corporation ist eine der bekanntesten Persönlichkeiten der amerikanischen Wirtschaft in den 2000er Jahren.  Dies wurde nicht durch ihren Tätigkeitsbereich (Elektrizität und Verträge für seine Lieferung) erleichtert, sondern durch die Resonanz aufgrund von Betrug.  Seit 15 Jahren ist das Unternehmenseinkommen schnell gewachsen, und die Arbeit darin versprach ein gutes Gehalt.  Aber alles endete genauso flüchtig: im Zeitraum 2000-2001.  Der Aktienkurs fiel von 90 USD / Stück auf fast Null, was auf aufgedeckten Betrug mit deklariertem Einkommen zurückzuführen war.  Seitdem ist das Wort "Enron" ein Begriff im Haushalt und dient als Bezeichnung für Unternehmen, die nach einem ähnlichen Muster arbeiten. </p><br><p>  Während des Prozesses wurden 18 Personen (einschließlich der größten Angeklagten in diesem Fall: Andrew Fastov, Jeff Skilling und Kenneth Lay) verurteilt. </p><br><p><img src="https://habrastorage.org/webt/te/rh/1l/terh1lsenbtg26n8nhjbhv3opfi.jpeg" alt="Bild! [Bild] (http: // https: //habrastorage.org/webt/te/rh/1l/terh1lsenbtg26n8nhjbhv3opfi.jpeg)"></p><br><p>  Gleichzeitig wurden ein Archiv der elektronischen Korrespondenz zwischen Mitarbeitern des Unternehmens, besser bekannt als Enron Email Dataset, und Insiderinformationen über das Einkommen der Mitarbeiter dieses Unternehmens veröffentlicht. </p><br><p>  In dem Artikel werden die Quellen dieser Daten untersucht und ein darauf basierendes Modell erstellt, um festzustellen, ob eine Person des Betrugs verdächtigt wird.  Klingt interessant?  Dann willkommen im Habrakat. <a name="habracut"></a></p><br><h1 id="opisanie-dataseta">  Beschreibung des Datensatzes </h1><br><p>  Enron-Datensatz (Datensatz) ist ein zusammengesetzter Satz offener Daten, der Aufzeichnungen von Personen enthält, die in einem denkwürdigen Unternehmen mit dem entsprechenden Namen arbeiten. <br>  Es kann 3 Teile unterscheiden: </p><br><ul><li>  Zahlungen_Funktionen - eine Gruppe, die finanzielle Bewegungen charakterisiert; </li><li>  stock_features - eine Gruppe, die die mit Aktien verbundenen Zeichen widerspiegelt; </li><li>  email_features - Eine Gruppe, die Informationen über die E-Mails einer bestimmten Person in aggregierter Form wiedergibt. </li></ul><br><p>  Natürlich gibt es auch eine Zielvariable, die angibt, ob die Person des Betrugs verdächtigt wird (das Zeichen <abbr title="Person von Interesse">„Poi“ <abbr>).</abbr></abbr> <abbr title="Person von Interesse"><br></abbr> </p><p>  Laden Sie unsere Daten herunter und beginnen Sie mit ihnen zu arbeiten: </p><br><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pickle <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>("final_project/enron_dataset.pkl", "rb") <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> data_file: data_dict = pickle.<span class="hljs-keyword"><span class="hljs-keyword">load</span></span>(data_file)</code> </pre> <br><p>  Danach verwandeln wir den <strong>Datensatz data_dict</strong> in einen Pandas-Datenrahmen, um das Arbeiten mit Daten zu vereinfachen: </p><br><pre> <code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings warnings.filterwarnings('<span class="hljs-title"><span class="hljs-title">ignore'</span></span>) source_df = pd.DataFrame.from_dict(<span class="hljs-title"><span class="hljs-title">data_dict</span></span>, <span class="hljs-title"><span class="hljs-title">orient</span></span> = '<span class="hljs-title"><span class="hljs-title">index'</span></span>) source_df.drop('<span class="hljs-type"><span class="hljs-type">TOTAL</span></span>',<span class="hljs-title"><span class="hljs-title">inplace</span></span>=<span class="hljs-type"><span class="hljs-type">True</span></span>)</code> </pre> <br><p>  Wir gruppieren die Zeichen gemäß den zuvor angegebenen Typen.  Dies sollte die Arbeit mit Daten danach erleichtern: </p><br><pre> <code class="hljs powershell">payments_features = [<span class="hljs-string"><span class="hljs-string">'salary'</span></span>, <span class="hljs-string"><span class="hljs-string">'bonus'</span></span>, <span class="hljs-string"><span class="hljs-string">'long_term_incentive'</span></span>, <span class="hljs-string"><span class="hljs-string">'deferred_income'</span></span>, <span class="hljs-string"><span class="hljs-string">'deferral_payments'</span></span>, <span class="hljs-string"><span class="hljs-string">'loan_advances'</span></span>, <span class="hljs-string"><span class="hljs-string">'other'</span></span>, <span class="hljs-string"><span class="hljs-string">'expenses'</span></span>, <span class="hljs-string"><span class="hljs-string">'director_fees'</span></span>, <span class="hljs-string"><span class="hljs-string">'total_payments'</span></span>] stock_features = [<span class="hljs-string"><span class="hljs-string">'exercised_stock_options'</span></span>, <span class="hljs-string"><span class="hljs-string">'restricted_stock'</span></span>, <span class="hljs-string"><span class="hljs-string">'restricted_stock_deferred'</span></span>,<span class="hljs-string"><span class="hljs-string">'total_stock_value'</span></span>] email_features = [<span class="hljs-string"><span class="hljs-string">'to_messages'</span></span>, <span class="hljs-string"><span class="hljs-string">'from_poi_to_this_person'</span></span>, <span class="hljs-string"><span class="hljs-string">'from_messages'</span></span>, <span class="hljs-string"><span class="hljs-string">'from_this_person_to_poi'</span></span>, <span class="hljs-string"><span class="hljs-string">'shared_receipt_with_poi'</span></span>] target_field = <span class="hljs-string"><span class="hljs-string">'poi'</span></span></code> </pre> <br><h2 id="finansovye-dannye">  Finanzdaten </h2><br><p>  In diesem Datensatz ist vielen ein NaN bekannt, der die übliche Lücke in den Daten ausdrückt.  Mit anderen Worten, der Autor des Datensatzes konnte keine Informationen zu einem bestimmten Attribut finden, das einer bestimmten Zeile im Datenrahmen zugeordnet ist.  Infolgedessen können wir annehmen, dass NaN 0 ist, da es keine Informationen über ein bestimmtes Merkmal gibt. </p><br><pre> <code class="hljs powershell">payments = source_df[<span class="hljs-type"><span class="hljs-type">payments_features</span></span>] payments = payments.replace(<span class="hljs-string"><span class="hljs-string">'NaN'</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><h3 id="proverka-dannyh">  Datenüberprüfung </h3><br><p>  Beim Vergleich mit dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Original-PDF</a> , das dem Datensatz zugrunde liegt, stellte sich heraus, dass die Daten leicht verzerrt sind, da <em>das</em> Feld <em>total_payments</em> nicht für alle Zeilen im <strong>Zahlungsdatenrahmen</strong> die Summe aller Finanztransaktionen einer bestimmten Person ist.  Sie können dies wie folgt überprüfen: </p><br><pre> <code class="hljs powershell">errors = payments[<span class="hljs-type"><span class="hljs-type">payments</span></span>[<span class="hljs-type"><span class="hljs-type">payments_features</span></span>[:-<span class="hljs-number"><span class="hljs-number">1</span></span>]]<span class="hljs-type"><span class="hljs-type">.sum</span></span>(<span class="hljs-type"><span class="hljs-type">axis</span></span>=<span class="hljs-string"><span class="hljs-string">'columns'</span></span>) != <span class="hljs-type"><span class="hljs-type">payments</span></span>[<span class="hljs-string"><span class="hljs-string">'total_payments'</span></span>]] errors.head()</code> </pre> <br><p><img src="https://habrastorage.org/webt/eo/79/ye/eo79ye27iirsiwcuickrxz4on30.png" alt="2 ungültige Zeilen"><br>  Wir sehen, dass BELFER ROBERT und BHATNAGAR SANJAY falsche Zahlungsbeträge haben. </p><br><p>  Sie können diesen Fehler beheben, indem Sie die Daten in den Fehlerzeilen nach links oder rechts verschieben und die Summe aller Zahlungen erneut zählen: </p><br><pre> <code class="hljs powershell">import numpy as np shifted_values = payments.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, <span class="hljs-type"><span class="hljs-type">payments_features</span></span>[<span class="hljs-number"><span class="hljs-number">1</span></span>:]].values expected_payments = shifted_values.sum() shifted_values = np.append(shifted_values, expected_payments) payments.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, <span class="hljs-type"><span class="hljs-type">payments_features</span></span>] = shifted_values shifted_values = payments.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, <span class="hljs-type"><span class="hljs-type">payments_features</span></span>[:-<span class="hljs-number"><span class="hljs-number">1</span></span>]].values payments.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, <span class="hljs-type"><span class="hljs-type">payments_features</span></span>] = np.insert(shifted_values, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><h2 id="dannye-po-akciyam">  Bestandsdaten </h2><br><pre> <code class="hljs powershell">stocks = source_df[<span class="hljs-type"><span class="hljs-type">stock_features</span></span>] stocks = stocks.replace(<span class="hljs-string"><span class="hljs-string">'NaN'</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  Führen Sie auch in diesem Fall eine Validierungsprüfung durch: </p><br><pre> <code class="hljs powershell">errors = stocks[<span class="hljs-type"><span class="hljs-type">stocks</span></span>[<span class="hljs-type"><span class="hljs-type">stock_features</span></span>[:-<span class="hljs-number"><span class="hljs-number">1</span></span>]]<span class="hljs-type"><span class="hljs-type">.sum</span></span>(<span class="hljs-type"><span class="hljs-type">axis</span></span>=<span class="hljs-string"><span class="hljs-string">'columns'</span></span>) != <span class="hljs-type"><span class="hljs-type">stocks</span></span>[<span class="hljs-string"><span class="hljs-string">'total_stock_value'</span></span>]] errors.head()</code> </pre> <br><p><img src="https://habrastorage.org/webt/sd/28/mz/sd28mzikmhurh_b0mqwibnje8fa.png" alt="Bild"></p><br><p>  Wir werden den Fehler in den Beständen auf ähnliche Weise beheben: </p><br><pre> <code class="hljs powershell">shifted_values = stocks.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, <span class="hljs-type"><span class="hljs-type">stock_features</span></span>[<span class="hljs-number"><span class="hljs-number">1</span></span>:]].values expected_payments = shifted_values.sum() shifted_values = np.append(shifted_values, expected_payments) stocks.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, <span class="hljs-type"><span class="hljs-type">stock_features</span></span>] = shifted_values shifted_values = stocks.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, <span class="hljs-type"><span class="hljs-type">stock_features</span></span>[:-<span class="hljs-number"><span class="hljs-number">1</span></span>]].values stocks.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, <span class="hljs-type"><span class="hljs-type">stock_features</span></span>] = np.insert(shifted_values, <span class="hljs-number"><span class="hljs-number">0</span></span>, shifted_values[-<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre> <br><h2 id="svodnye-dannye-po-elektronnoy-perepiske">  E-Mail-Korrespondenz </h2><br><p>  Wenn für diese Finanzen oder Aktien NaN gleich 0 war und dies in das Endergebnis für jede dieser Gruppen passt, ist es im Fall von E-Mail sinnvoller, NaN durch einen Standardwert zu ersetzen.  Dazu können Sie Imputer verwenden: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">from</span></span> sklearn.impute <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SimpleImputer imp = SimpleImputer()</code> </pre> <br><p>  Gleichzeitig werden wir den Standardwert für jede Kategorie (unabhängig davon, ob wir eine Person des Betrugs vermuten) separat betrachten: </p><br><pre> <code class="hljs markdown">target = source<span class="hljs-emphasis"><span class="hljs-emphasis">_df[target_</span></span>field] email<span class="hljs-emphasis"><span class="hljs-emphasis">_data = source_</span></span>df[<span class="hljs-string"><span class="hljs-string">email_features</span></span>] email<span class="hljs-emphasis"><span class="hljs-emphasis">_data = pd.concat([email_</span></span>data, target], axis=1) email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>poi = email<span class="hljs-emphasis"><span class="hljs-emphasis">_data[email_</span></span>data[<span class="hljs-string"><span class="hljs-string">target_field</span></span>]][<span class="hljs-string"><span class="hljs-string">email_features</span></span>] email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>nonpoi = email<span class="hljs-emphasis"><span class="hljs-emphasis">_data[email_</span></span>data[<span class="hljs-string"><span class="hljs-string">target_field</span></span>] == False][email<span class="hljs-emphasis"><span class="hljs-emphasis">_features] email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_poi[email_</span></span>features] = imp.fit<span class="hljs-emphasis"><span class="hljs-emphasis">_transform(email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_poi) email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_nonpoi[email_</span></span>features] = imp.fit<span class="hljs-emphasis"><span class="hljs-emphasis">_transform(email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_nonpoi) email_</span></span>data = email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>poi.append(email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>nonpoi)</code> </pre> <br><p>  Endgültiger Datensatz nach Korrektur: </p><br><pre> <code class="hljs cs">df = payments.<span class="hljs-keyword"><span class="hljs-keyword">join</span></span>(stocks) df = df.<span class="hljs-keyword"><span class="hljs-keyword">join</span></span>(email_data) df = df.astype(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)</code> </pre> <br><h2 id="vybrosy">  Emissionen </h2><br><p>  Im letzten Schritt dieser Phase werden alle Ausreißer entfernt, die das Training verzerren können.  Gleichzeitig stellt sich immer die Frage: Wie viele Daten können wir aus der Stichprobe entfernen und trotzdem als trainiertes Modell nicht verlieren?  Ich folgte dem Rat eines der Dozenten des ML-Kurses (Maschinelles Lernen) zu Udacity - „10 entfernen und erneut auf Emissionen prüfen.“ </p><br><pre> <code class="hljs powershell">first_quartile = df.quantile(q=<span class="hljs-number"><span class="hljs-number">0.25</span></span>) third_quartile = df.quantile(q=<span class="hljs-number"><span class="hljs-number">0.75</span></span>) IQR = third_quartile - first_quartile outliers = df[(<span class="hljs-type"><span class="hljs-type">df</span></span> &gt; (<span class="hljs-type"><span class="hljs-type">third_quartile</span></span> + <span class="hljs-number"><span class="hljs-number">1.5</span></span> * <span class="hljs-type"><span class="hljs-type">IQR</span></span>)) | (<span class="hljs-type"><span class="hljs-type">df</span></span> &lt; (<span class="hljs-type"><span class="hljs-type">first_quartile</span></span> - <span class="hljs-number"><span class="hljs-number">1.5</span></span> * <span class="hljs-type"><span class="hljs-type">IQR</span></span>))].count(axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) outliers.sort_values(axis=<span class="hljs-number"><span class="hljs-number">0</span></span>, ascending=False, inplace=True) outliers = outliers.head(<span class="hljs-number"><span class="hljs-number">10</span></span>) outliers</code> </pre> <br><p>  Gleichzeitig werden keine Datensätze gelöscht, die Ausreißer sind und des Betrugs verdächtigt werden.  Der Grund ist, dass es nur 18 Zeilen mit solchen Daten gibt, und wir können sie nicht opfern, da dies zu einem Mangel an Trainingsbeispielen führen kann.  Infolgedessen entfernen wir nur diejenigen, die nicht des Betrugs verdächtigt werden, aber gleichzeitig eine Vielzahl von Anzeichen haben, anhand derer Emissionen beobachtet werden: </p><br><pre> <code class="hljs powershell">target_for_outliers = target.loc[<span class="hljs-type"><span class="hljs-type">outliers.index</span></span>] outliers = pd.concat([<span class="hljs-type"><span class="hljs-type">outliers</span></span>, <span class="hljs-type"><span class="hljs-type">target_for_outliers</span></span>], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) non_poi_outliers = outliers[<span class="hljs-type"><span class="hljs-type">np.logical_not</span></span>(<span class="hljs-type"><span class="hljs-type">outliers.poi</span></span>)] df.drop(non_poi_outliers.index, inplace=True)</code> </pre> <br><h2 id="privedenie-k-itogovom-vidu">  Finalisieren </h2><br><p>  Wir normalisieren unsere Daten: </p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scale df[df.<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>] = scale(df)</code> </pre> <br><p>  Lässt die Zielvariable auf eine kompatible Ansicht abzielen: </p><br><pre> <code class="hljs cmake"><span class="hljs-keyword"><span class="hljs-keyword">target</span></span>.drop(non_poi_outliers.index, inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">target</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">target</span></span>.map({<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>}) <span class="hljs-keyword"><span class="hljs-keyword">target</span></span>.value_counts()</code> </pre> <br><p><img src="https://habrastorage.org/webt/nr/mn/fi/nrmnfi0bdldw36fg9uy-tcxinsa.png" alt="Bild"><br>  Infolgedessen 18 Verdächtige und 121 diejenigen, die nicht verdächtigt wurden. </p><br><h1 id="otbor-priznakov">  Funktionsauswahl </h1><br><p>  Vielleicht ist einer der wichtigsten Punkte vor dem Erlernen eines Modells die Auswahl der wichtigsten Merkmale. </p><br><h2 id="proverka-na-multikollinearnost">  Multikollinearitätstest </h2><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns %matplotlib <span class="hljs-keyword"><span class="hljs-keyword">inline</span></span> sns.<span class="hljs-keyword"><span class="hljs-keyword">set</span></span>(style="whitegrid") corr = df.corr() * <span class="hljs-number"><span class="hljs-number">100</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">Select</span></span> upper triangle <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> correlation matrix mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span> up the matplotlib figure f, ax = plt.subplots(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">11</span></span>)) # Generate a custom diverging colormap cmap = sns.diverging_palette(<span class="hljs-number"><span class="hljs-number">220</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) # Draw the heatmap <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the mask <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> correct aspect ratio sns.heatmap(corr, mask=mask, cmap=cmap, center=<span class="hljs-number"><span class="hljs-number">0</span></span>, linewidths=<span class="hljs-number"><span class="hljs-number">1</span></span>, cbar_kws={"shrink": <span class="hljs-number"><span class="hljs-number">.7</span></span>}, annot=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, fmt=".2f")</code> </pre> <br><p><img src="https://habrastorage.org/webt/kw/66/ta/kw66tahpopi7zjc6tvaj1uzq9qe.png" alt="Bild"><br>  Wie Sie auf dem Bild sehen können, besteht eine ausgeprägte Beziehung zwischen 'credit_advanced' und 'total_payments' sowie zwischen 'total_stock_value' und 'beschränkter_stock'.  Wie bereits erwähnt, sind 'total_payments' und 'total_stock_value' nur das Ergebnis der Addition aller Indikatoren in einer bestimmten Gruppe.  Daher können sie gelöscht werden: </p><br><pre> <code class="hljs pgsql">df.<span class="hljs-keyword"><span class="hljs-keyword">drop</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>=[<span class="hljs-string"><span class="hljs-string">'total_payments'</span></span>, <span class="hljs-string"><span class="hljs-string">'total_stock_value'</span></span>], inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><h2 id="sozdanie-novyh-priznakov">  Neue Eigenschaften schaffen </h2><br><p>  Es besteht auch die Annahme, dass die Verdächtigen häufiger an Komplizen als an Mitarbeiter geschrieben haben, die daran nicht beteiligt waren.  Infolgedessen sollte der Anteil solcher Nachrichten größer sein als der Anteil der Nachrichten an normale Mitarbeiter.  Basierend auf dieser Aussage können Sie neue Zeichen erstellen, die den Prozentsatz der eingehenden / ausgehenden Personen im Zusammenhang mit Verdächtigen widerspiegeln: </p><br><pre> <code class="hljs powershell">df[<span class="hljs-string"><span class="hljs-string">'ratio_of_poi_mail'</span></span>] = df[<span class="hljs-string"><span class="hljs-string">'from_poi_to_this_person'</span></span>]/df[<span class="hljs-string"><span class="hljs-string">'to_messages'</span></span>] df[<span class="hljs-string"><span class="hljs-string">'ratio_of_mail_to_poi'</span></span>] = df[<span class="hljs-string"><span class="hljs-string">'from_this_person_to_poi'</span></span>]/df[<span class="hljs-string"><span class="hljs-string">'from_messages'</span></span>]</code> </pre> <br><h2 id="otsev-lishnih-priznakov">  Unnötige Zeichen herausfiltern </h2><br><p>  Im Toolkit der mit ML verbundenen Personen gibt es viele hervorragende Tools zur Auswahl der wichtigsten Funktionen (SelectKBest, SelectPercentile, VarianceThreshold usw.).  In diesem Fall wird RFECV verwendet, da es eine Kreuzvalidierung enthält, mit der Sie die wichtigsten Merkmale berechnen und in allen Teilmengen der Stichprobe überprüfen können: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split X_train, X_test, y_train, y_test = train_test_split(<span class="hljs-title"><span class="hljs-title">df</span></span>, <span class="hljs-title"><span class="hljs-title">target</span></span>, <span class="hljs-title"><span class="hljs-title">test_size</span></span>=0.2, <span class="hljs-title"><span class="hljs-title">random_state</span></span>=42)</code> </pre> <br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RFECV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier forest = RandomForestClassifier(random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>) rfecv = RFECV(estimator=forest, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>, scoring=<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>) rfecv = rfecv.fit(X_train, y_train) plt.figure() plt.xlabel("Number of features selected") plt.ylabel("Cross validation score of number of selected features") plt.plot(range(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(rfecv.grid_scores_) + <span class="hljs-number"><span class="hljs-number">1</span></span>), rfecv.grid_scores_, <span class="hljs-string"><span class="hljs-string">'--o'</span></span>) indices = rfecv.get_support() <span class="hljs-keyword"><span class="hljs-keyword">columns</span></span> = X_train.<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>[indices] print(<span class="hljs-string"><span class="hljs-string">'The most important columns are {}'</span></span>.format(<span class="hljs-string"><span class="hljs-string">','</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">join</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>)))</code> </pre> <br><p><img src="https://habrastorage.org/webt/fm/-7/oo/fm-7oo9gnbxhmpiw_vwldkgpwpi.png" alt="Bild"><br>  Wie Sie sehen können, hat RandomForestClassifier berechnet, dass nur 7 der 18 Attribute von Bedeutung sind.  Die Verwendung des Restes verringert die Genauigkeit des Modells. </p><br><pre> <code class="hljs pgsql">The most important <span class="hljs-keyword"><span class="hljs-keyword">columns</span></span> are bonus, deferred_income, other, exercised_stock_options, shared_receipt_with_poi, ratio_of_poi_mail, ratio_of_mail_to_poi</code> </pre> <br><p>  Diese 7 Funktionen werden in Zukunft verwendet, um das Modell zu vereinfachen und das Risiko einer Umschulung zu verringern: </p><br><ul><li>  Bonus </li><li>  deferred_income </li><li>  andere </li><li>  ausgeübte_Stock_Optionen </li><li>  shared_receipt_with_poi </li><li>  ratio_of_poi_mail </li><li>  ratio_of_mail_to_poi </li></ul><br><p>  Ändern Sie die Struktur der Trainings- und Testmuster für das zukünftige Modelltraining: </p><br><pre> <code class="hljs powershell">X_train = X_train[<span class="hljs-type"><span class="hljs-type">columns</span></span>] X_test = X_test[<span class="hljs-type"><span class="hljs-type">columns</span></span>]</code> </pre> <br><p>  Dies ist das Ende des ersten Teils, in dem die Verwendung des Enron-Datensatzes als Beispiel für eine Klassifizierungsaufgabe in ML beschrieben wird.  Basierend auf den Materialien aus dem Kurs Einführung in maschinelles Lernen über Udacity.  Es gibt auch ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Notizbuch</a> , das die gesamte Abfolge der Aktionen widerspiegelt. </p><br><blockquote>  Der zweite Teil ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> <br></blockquote><p></p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de424891/">https://habr.com/ru/post/de424891/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de424877/index.html">Kühlsystem für flüssige Bremsen</a></li>
<li><a href="../de424879/index.html">Schnittstellenverfügbarkeit Yandex Vortrag</a></li>
<li><a href="../de424881/index.html">Newtoo - 2018 eine vollwertige Browser-Engine von Grund auf neu entwickeln?</a></li>
<li><a href="../de424887/index.html">Worüber Lida schweigt: Der Beginn der Karriere eines Entwicklers. Prinzipien oder wie man ein Middl wird</a></li>
<li><a href="../de424889/index.html">Blick in den Intel 8087 Coprozessor</a></li>
<li><a href="../de424893/index.html">Der Handwerker erstellte ein WiFi-Modul für den Macintosh SE / 30, ein Modell von 1989</a></li>
<li><a href="../de424895/index.html">Tippen: Ungültige Zustände unaussprechlich machen</a></li>
<li><a href="../de424897/index.html">Ein unerwartetes Treffen. Kapitel 18</a></li>
<li><a href="../de424899/index.html">Was Sie über Audio hören sollten: 15 Podcasts</a></li>
<li><a href="../de424901/index.html">Die Zusammenfassung interessanter Materialien für den mobilen Entwickler # 272 (24. September - 30. September)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>