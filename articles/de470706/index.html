<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçüè≠ üçú ‚úã Python + Keras + LSTM: Erstellen Sie in einer halben Stunde einen Text√ºbersetzer ‚è´ üßó ‚òπÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr. 

 Im vorherigen Teil habe ich mich mit der Erstellung einer einfachen Texterkennung auf der Grundlage eines neuronalen Netzwerks befasst....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTM: Erstellen Sie in einer halben Stunde einen Text√ºbersetzer</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/">  Hallo Habr. <br><br>  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Teil habe</a> ich mich mit der Erstellung einer einfachen Texterkennung auf der Grundlage eines neuronalen Netzwerks befasst.  Heute werden wir einen √§hnlichen Ansatz verfolgen und einen automatischen √úbersetzer von Texten vom Englischen ins Deutsche schreiben. <br><br><img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br><br>  F√ºr diejenigen, die daran interessiert sind, wie dies funktioniert, sind Details unter dem Schnitt. <br><a name="habracut"></a><br>  <i>Hinweis</i> : Dieses Projekt zur Verwendung eines neuronalen Netzwerks f√ºr die √úbersetzung ist ausschlie√ülich p√§dagogisch, daher wird die Frage ‚ÄûWarum‚Äú nicht ber√ºcksichtigt.  Nur zum Spa√ü.  Ich wollte nicht beweisen, dass diese oder jene Methode besser oder schlechter ist, es war nur interessant zu √ºberpr√ºfen, was passiert.  Die unten verwendete Methode ist nat√ºrlich vereinfacht, aber ich hoffe, niemand hofft, dass wir in einer halben Stunde einen zweiten Lingvo schreiben werden. <br><br><h2>  Datenerfassung </h2><br>  Als Quelldatensatz wurde eine im Netzwerk gefundene Datei verwendet, die englische und deutsche Phrasen enth√§lt, die durch Tabulatoren getrennt sind.  Eine Reihe von Phrasen sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><pre><code class="python hljs">Hi. Hallo! Hi. Gr√º√ü Gott! Run! Lauf! Wow! Potzdonner! Wow! Donnerwetter! Fire! Feuer! Help! Hilfe! Help! Zu H√ºlf! Stop! Stopp! Wait! Warte! Go on. Mach weiter. Hello! Hallo! I ran. Ich rannte. I see. Ich verstehe. ...</code> </pre> <br>  Die Datei enth√§lt 192.000 Zeilen und hat eine Gr√∂√üe von 13 MB.  Wir laden den Text in den Speicher und teilen die Daten in zwei Bl√∂cke f√ºr englische und deutsche W√∂rter auf. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, mode=<span class="hljs-string"><span class="hljs-string">'rt'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> file: text = file.read() sents = text.strip().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sents] data = read_text(<span class="hljs-string"><span class="hljs-string">"deutch.txt"</span></span>) deu_eng = np.array(data) deu_eng = deu_eng[:<span class="hljs-number"><span class="hljs-number">30000</span></span>,:] print(<span class="hljs-string"><span class="hljs-string">"Dictionary size:"</span></span>, deu_eng.shape) <span class="hljs-comment"><span class="hljs-comment"># Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower()</span></span></code> </pre><br>  Wir haben auch alle W√∂rter in Kleinbuchstaben umgewandelt und die Satzzeichen entfernt. <br><br>  Der n√§chste Schritt besteht darin, die Daten f√ºr das neuronale Netzwerk vorzubereiten.  Das Netzwerk wei√ü nicht, was W√∂rter sind, und arbeitet ausschlie√ülich mit Zahlen.  Zum Gl√ºck ist in Keras bereits die Tokenizer-Klasse integriert, die W√∂rter in S√§tzen durch digitale Codes ersetzt. <br><br>  Die Verwendung wird einfach anhand eines Beispiels veranschaulicht: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences s = <span class="hljs-string"><span class="hljs-string">"To be or not to be"</span></span> eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts([s]) seq = eng_tokenizer.texts_to_sequences([s]) seq = pad_sequences(seq, maxlen=<span class="hljs-number"><span class="hljs-number">8</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'post'</span></span>) print(seq)</code> </pre><br>  Der Ausdruck "sein oder nicht sein" wird durch das Array [1 2 3 4 1 2 0 0] ersetzt, wobei es nicht schwer zu erraten ist, 1 = bis, 2 = sein, 3 = oder 4 = nicht.  Wir k√∂nnen diese Daten bereits an das neuronale Netzwerk senden. <br><br><h2>  Neuronales Netzwerktraining </h2><br>  Unsere Daten sind digital bereit.  Wir teilen das Array in zwei Bl√∂cke f√ºr Eingabe- (englische Zeilen) und Ausgabedaten (deutsche Zeilen).  Wir werden auch eine separate Einheit zur Validierung des Lernprozesses vorbereiten. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])</span></span></code> </pre><br>  Jetzt k√∂nnen wir ein Modell eines neuronalen Netzwerks erstellen und mit dem Training beginnen.  Wie Sie sehen k√∂nnen, enth√§lt das neuronale Netzwerk LSTM-Schichten mit Speicherzellen.  Obwohl es wahrscheinlich in einem ‚Äûnormalen‚Äú Netzwerk funktionieren w√ºrde, k√∂nnen diejenigen, die dies w√ºnschen, selbst nachsehen. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_vocab, out_vocab, in_timesteps, out_timesteps, n)</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(n)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(out_vocab, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> eng_length, deu_length = <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span> model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number"><span class="hljs-number">512</span></span>) num_epochs = <span class="hljs-number"><span class="hljs-number">40</span></span> model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], trainY.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>), epochs=num_epochs, batch_size=<span class="hljs-number"><span class="hljs-number">512</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, callbacks=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>)</code> </pre><br>  Das Training selbst sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br><br>  Wie Sie sehen, ist der Vorgang nicht schnell und dauert auf einer Core i7 + GeForce 1060 f√ºr einen Satz von 30.000 Zeilen etwa eine halbe Stunde.  Am Ende des Trainings (es muss nur einmal durchgef√ºhrt werden) wird das Modell in einer Datei gespeichert und kann dann wiederverwendet werden. <br><br>  Um die √úbersetzung zu erhalten, verwenden wir die Funktion Predict_classes, deren Eingabe wir einige einfache S√§tze √ºbermitteln.  Die Funktion get_word wird verwendet, um W√∂rter in Zahlen umzukehren. <br><br><pre> <code class="python hljs">model = load_model(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_word</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n, tokenizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.word_index.items(): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index == n: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string"><span class="hljs-string">"the weather is nice today"</span></span>, <span class="hljs-string"><span class="hljs-string">"my name is tom"</span></span>, <span class="hljs-string"><span class="hljs-string">"how old are you"</span></span>, <span class="hljs-string"><span class="hljs-string">"where is the nearest shop"</span></span>]) preds = model.predict_classes(phrs_enc) print(<span class="hljs-string"><span class="hljs-string">"Preds:"</span></span>, preds.shape) print(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer))</code> </pre><br><h2>  Ergebnisse </h2><br>  Das Merkw√ºrdigste sind nun die Ergebnisse.  Es ist interessant zu sehen, wie das neuronale Netzwerk die Entsprechung zwischen englischen und deutschen Phrasen lernt und sich daran erinnert.  Ich habe speziell 2 S√§tze leichter und 2 schwerer genommen, um den Unterschied zu erkennen. <br><br>  <b>5 Minuten Training</b> <br><br>  "Das Wetter ist heute sch√∂n" - "das ist ist tom" <br>  "Mein Name ist Tom" - "wie f√ºr Tom Tom" <br>  "Wie alt bist du" - "wie geht ist es" <br>  "Wo ist das n√§chste Gesch√§ft" - "wo ist der" <br><br>  Wie Sie sehen, gibt es bisher nur wenige ‚ÄûTreffer‚Äú.  Ein Fragment des Ausdrucks ‚ÄûWie alt bist du?‚Äú Verwechselte das neuronale Netzwerk mit dem Ausdruck ‚ÄûWie geht es dir?‚Äú Und produzierte die √úbersetzung ‚ÄûWie geht es?‚Äú (Wie geht es dir?).  In der Phrase "wo ist ..." identifizierte das neuronale Netzwerk nur das Verb wo und produzierte die √úbersetzung "wo ist der" (wo ist es?), Was im Prinzip nicht ohne Bedeutung ist.  Im Allgemeinen ungef√§hr das Gleiche wie ein Neuling in der Gruppe A1 ins Deutsche √ºbersetzt;) <br><br>  <b>10 Minuten Training</b> <br><br>  "Das Wetter ist heute sch√∂n" - "das haus ist bereit" <br>  "Mein Name ist tom" - "mein hei√üer hei√üer tom" <br>  "Wie alt bist du" - "wie alt sind sie" <br>  "Wo ist das n√§chste Gesch√§ft" - "wo ist paris" <br><br>  Einige Fortschritte sind sichtbar.  Der erste Satz ist v√∂llig fehl am Platz.  In der zweiten Phrase ‚Äûlernte‚Äú das neuronale Netz das Verb hei√ü (genannt), aber ‚Äûmein hei√üer hei√üer Tom‚Äú ist immer noch falsch, obwohl Sie die Bedeutung bereits erraten k√∂nnen.  Der dritte Satz ist bereits richtig.  Im vierten Teil lautet der richtige erste Teil ‚Äûwo ist‚Äú, aber der n√§chste Laden wurde aus irgendeinem Grund durch Paris ersetzt. <br><br>  <b>30 Minuten Training</b> <br><br>  "Das Wetter ist heute sch√∂n" - "das ist ist aus" <br>  "Mein Name ist Tom" - "" Tom "ist mein Name" <br>  "Wie alt bist du" - "wie alt sind sie" <br>  "Wo ist das n√§chste Gesch√§ft" - "wo ist der" <br><br>  Wie Sie sehen k√∂nnen, ist der zweite Satz korrekt geworden, obwohl das Design etwas ungew√∂hnlich aussieht.  Die dritte Phrase ist richtig, aber die 1. und 4. Phrase wurden noch nicht ‚Äûgelernt‚Äú.  Damit habe <s>ich</s> den Prozess abgeschlossen <s>, um Strom zu sparen</s> . <br><br><h2>  Fazit </h2><br>  Wie Sie sehen, funktioniert dies im Prinzip.  Ich m√∂chte mir eine neue Sprache mit solcher Geschwindigkeit merken :) Nat√ºrlich ist das Ergebnis bisher nicht perfekt, aber das Training mit 190.000 Zeilen w√ºrde mehr als eine Stunde dauern. <br><br>  F√ºr diejenigen, die alleine experimentieren m√∂chten, befindet sich der Quellcode unter dem Spoiler.  Das Programm kann theoretisch jedes Sprachpaar verwenden, nicht nur Englisch und Deutsch (die Datei sollte in UTF-8-Codierung vorliegen).  Das Thema √úbersetzungsqualit√§t bleibt ebenfalls offen, es gibt etwas zu testen. <br><br><div class="spoiler">  <b class="spoiler_title">keras_translate.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Force CPU os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed import string import re import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM, Embedding, RepeatVector from keras.preprocessing.text import Tokenizer from keras.callbacks import ModelCheckpoint from keras.preprocessing.sequence import pad_sequences from keras.models import load_model from keras import optimizers from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', 200) # Read raw text file def read_text(filename): with open(filename, mode='rt', encoding='utf-8') as file: text = file.read() sents = text.strip().split('\n') return [i.split('\t') for i in sents] data = read_text("deutch.txt") deu_eng = np.array(data) deu_eng = deu_eng[:30000,:] print("Dictionary size:", deu_eng.shape) # Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # Convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower() # Prepare English tokenizer eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts(deu_eng[:, 0]) eng_vocab_size = len(eng_tokenizer.word_index) + 1 eng_length = 8 # Prepare Deutch tokenizer deu_tokenizer = Tokenizer() deu_tokenizer.fit_on_texts(deu_eng[:, 1]) deu_vocab_size = len(deu_tokenizer.word_index) + 1 deu_length = 8 # Encode and pad sequences def encode_sequences(tokenizer, length, lines): # integer encode sequences seq = tokenizer.texts_to_sequences(lines) # pad sequences with 0 values seq = pad_sequences(seq, maxlen=length, padding='post') return seq # Split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # Prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # Prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1]) # Build NMT model def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n): model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(n)) model.add(Dropout(0.3)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=True)) model.add(Dropout(0.3)) model.add(Dense(out_vocab, activation='softmax')) model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy') return model print("deu_vocab_size:", deu_vocab_size, deu_length) print("eng_vocab_size:", eng_vocab_size, eng_length) # Model compilation (with 512 hidden units) model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512) # Train model num_epochs = 250 history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1) # plt.plot(history.history['loss']) # plt.plot(history.history['val_loss']) # plt.legend(['train','validation']) # plt.show() model.save('en-de-model.h5') # Load model model = load_model('en-de-model.h5') def get_word(n, tokenizer): if n == 0: return "" for word, index in tokenizer.word_index.items(): if index == n: return word return "" phrs_enc = encode_sequences(eng_tokenizer, eng_length, ["the weather is nice today", "my name is tom", "how old are you", "where is the nearest shop"]) print("phrs_enc:", phrs_enc.shape) preds = model.predict_classes(phrs_enc) print("Preds:", preds.shape) print(preds[0]) print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer)) print(preds[1]) print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer)) print(preds[2]) print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer)) print(preds[3]) print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer)) print()</span></span></code> </pre><br></div></div><br>  Das W√∂rterbuch selbst ist zu gro√ü, um es an den Artikel anzuh√§ngen. Der Link befindet sich in den Kommentaren. <br><br>  Wie immer alle erfolgreichen Experimente. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de470706/">https://habr.com/ru/post/de470706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de470688/index.html">Was ist √ºber VMworld 2019 bekannt?</a></li>
<li><a href="../de470692/index.html">Wie wir eine neue Rosbank-Website erstellt haben und was daraus wurde</a></li>
<li><a href="../de470694/index.html">Auswahl einer E-Mail-Marketing-Plattform: Was ist f√ºr russische Unternehmen zu beachten?</a></li>
<li><a href="../de470696/index.html">Warum ist Kaldi gut f√ºr die Spracherkennung? (aktualisiert am 25.12.2019)</a></li>
<li><a href="../de470700/index.html">Tischplatte. Metallic Lautlos Ihre</a></li>
<li><a href="../de470710/index.html">Maschinelles Lernen f√ºr Ihre Wohnungssuche. Teil 2</a></li>
<li><a href="../de470714/index.html">Wie ich zum Digital Breakthrough Finale kam</a></li>
<li><a href="../de470718/index.html">"Algebraische Effekte" in der menschlichen Sprache</a></li>
<li><a href="../de470720/index.html">Wie schreibe ich einen intelligenten Vertrag mit Python √ºber Ontologie? Teil 2: Speicher-API</a></li>
<li><a href="../de470722/index.html">Wie schreibe ich einen intelligenten Vertrag mit Python √ºber Ontologie? Teil 3: Laufzeit-API</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>