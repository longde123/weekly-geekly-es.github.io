<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçû üë¥üèΩ üõåüèº Creaci√≥n de infraestructura de TI tolerante a fallas. Parte 1: preparaci√≥n para implementar el cl√∫ster oVirt 4.3 üê¶ üåê üë®üèø‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se invita a los lectores a familiarizarse con los principios de construir una infraestructura tolerante a fallas de una peque√±a empresa dentro de un c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Creaci√≥n de infraestructura de TI tolerante a fallas. Parte 1: preparaci√≥n para implementar el cl√∫ster oVirt 4.3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lenvendo/blog/483980/"><p>  Se invita a los lectores a familiarizarse con los principios de construir una infraestructura tolerante a fallas de una peque√±a empresa dentro de un centro de datos, que se examinar√° en detalle en una breve serie de art√≠culos. </p><a name="habracut"></a><br><h2 id="vvodnaya-chast">  Introduccion </h2><br><p>  Bajo el <strong>DPC</strong> (Centro de procesamiento de datos) se puede entender: </p><br><ul><li>  propio bastidor en su "sala de servidores" en el territorio de la empresa que cumple con los requisitos m√≠nimos para proporcionar equipos de alimentaci√≥n y refrigeraci√≥n, as√≠ como tener acceso a Internet a trav√©s de dos proveedores independientes; </li><li>  bastidor alquilado con su propio equipo ubicado en este centro de datos, el llamado  colocaci√≥n, que cumple con el est√°ndar Tier III o IV, y que garantiza un suministro de energ√≠a confiable, enfriamiento y proporciona acceso a Internet tolerante a fallas; </li><li>  equipo totalmente alquilado en un centro de datos de nivel III o IV. </li></ul><br><p>  Qu√© opci√≥n de alojamiento elegir: en cada caso, todo es individual y generalmente depende de varios factores principales: </p><br><ul><li>  ¬øPor qu√© la empresa tiene su propia infraestructura de TI? </li><li>  qu√© quiere exactamente la empresa de la infraestructura de TI (confiabilidad, escalabilidad, capacidad de administraci√≥n, etc.); </li><li>  la cantidad de inversi√≥n inicial en infraestructura de TI, as√≠ como qu√© tipo de costos es: capital (que significa comprar su equipo) u operativo (el equipo generalmente se alquila); </li><li>  Planificaci√≥n del horizonte de la propia empresa. </li></ul><br><p>  Se puede escribir mucho sobre los factores que influyen en la decisi√≥n de la empresa de crear y usar su infraestructura de TI, pero nuestro objetivo es mostrar en la pr√°ctica c√≥mo crear esta infraestructura para que sea tolerante a fallas y sea posible ahorrar dinero. - Reduzca el costo de adquirir software comercial, o incluso ev√≠telos. </p><br><p>  Como muestra la pr√°ctica, no vale la pena ahorrar en hardware, ya que los avaros pagan dos veces, e incluso mucho m√°s.  Pero, de nuevo, buen hardware, esto es solo una recomendaci√≥n y, en √∫ltima instancia, qu√© comprar exactamente y cu√°nto depende de las capacidades de la empresa y la "codicia" de su gesti√≥n.  Adem√°s, la palabra "avaricia" debe entenderse en el buen sentido de la palabra, ya que es mejor invertir en hierro en la etapa inicial, para que luego no haya problemas serios en su mayor apoyo y escala, ya que inicialmente una planificaci√≥n incorrecta y un ahorro excesivo pueden conducir al futuro m√°s caro que al comenzar un proyecto. </p><br><p>  Entonces, los datos iniciales para el proyecto: </p><br><ul><li>  hay una empresa que decidi√≥ crear su propio portal web y poner sus actividades en Internet; </li><li>  la compa√±√≠a decidi√≥ alquilar un bastidor para colocar su equipo en un buen centro de datos certificado de acuerdo con el est√°ndar Tier III; </li><li>  la compa√±√≠a decidi√≥ no ahorrar mucho en hardware y, por lo tanto, compr√≥ el siguiente equipo con garant√≠as y soporte extendidos: </li></ul><br><div class="spoiler">  <b class="spoiler_title">Lista de equipos</b> <div class="spoiler_text"><blockquote><ul><li>  dos servidores f√≠sicos Dell PowerEdge R640 de la siguiente manera: </li><li>  <em>dos procesadores Intel Xeon Gold 5120</em> </li><li>  <em>512 Gb de RAM</em> </li><li>  <em>dos discos SAS en RAID1, para la instalaci√≥n del sistema operativo</em> </li><li>  <em>tarjeta de red incorporada de 4 puertos 1G</em> </li><li>  <em>dos tarjetas de red 10G de 2 puertos</em> </li><li>  <em>un FC HBA 16G de 2 puertos.</em> </li><li>  Sistema de almacenamiento de 2 controladores Dell MD3820f, conectado a trav√©s de FC 16G directamente a hosts Dell; </li><li>  dos interruptores del segundo nivel: Cisco WS-C2960RX-48FPS-L apilados; </li><li>  dos conmutadores de capa 3: Cisco WS-C3850-24T-E, apilados; </li><li>  Rack, UPS, PDU, servidores de consola, proporcionados por el centro de datos. </li></ul><br></blockquote></div></div><br><p>  Como podemos ver, el equipo existente tiene buenas perspectivas de escalamiento horizontal y vertical, si la compa√±√≠a puede competir con otras compa√±√≠as de un perfil similar en Internet, y comienza a obtener ganancias que pueden invertirse en la expansi√≥n de recursos para una mayor competencia y crecimiento de ganancias. </p><br><p>  Qu√© equipo podemos agregar si la empresa decide aumentar el rendimiento de nuestro cl√∫ster inform√°tico: </p><br><ul><li>  tenemos una gran reserva para la cantidad de puertos en los conmutadores 2960X, lo que significa que puede agregar m√°s servidores de hardware; </li><li>  compre dos conmutadores FC para conectar sistemas de almacenamiento y servidores adicionales a ellos; </li><li>  los servidores ya existentes pueden actualizarse: agregue memoria, reemplace los procesadores por otros m√°s eficientes, conecte los adaptadores de red existentes a una red 10G; </li><li>  Puede agregar estantes de disco adicionales al almacenamiento con el tipo de discos necesarios: SAS, SATA o SSD, seg√∫n la carga planificada; </li><li>  despu√©s de agregar conmutadores FC, puede comprar otro sistema de almacenamiento para agregar a√∫n m√°s capacidad de disco, y si compra la opci√≥n especial de Replicaci√≥n remota, puede configurar la replicaci√≥n de datos entre sistemas de almacenamiento tanto dentro del mismo centro de datos como entre centros de datos (pero esto ya est√° m√°s all√° el alcance del art√≠culo); </li><li>  tambi√©n hay conmutadores de tercer nivel: el Cisco 3850, que puede usarse como n√∫cleo de la red tolerante a fallas para el enrutamiento de alta velocidad entre redes internas.  Esto ser√° de gran ayuda en el futuro, a medida que la infraestructura interna crezca.  El 3850 tambi√©n tiene puertos 10G que se pueden activar m√°s tarde al actualizar el equipo de red a 10G. </li></ul><br><p>  Como ahora no hay ning√∫n lugar sin virtualizaci√≥n, entonces, por supuesto, estaremos en tendencia, m√°s a√∫n, esta es una excelente manera de reducir el costo de comprar servidores caros para ciertos elementos de infraestructura (servidores web, bases de datos, etc.), que no siempre son √≥ptimos usado en caso de baja carga, y eso es exactamente lo que ser√° al comienzo del lanzamiento del proyecto. </p><br><p>  Adem√°s, la virtualizaci√≥n tiene muchas otras ventajas que pueden ser muy √∫tiles para nosotros: tolerancia a fallas de VM por fallas del servidor de hardware, migraci√≥n en vivo entre nodos de hardware del cl√∫ster para su mantenimiento, equilibrio de carga manual o autom√°tico entre nodos de cl√∫ster, etc. </p><br><p>  Para el hardware adquirido por la empresa, se sugiere la implementaci√≥n del cl√∫ster VMware vSphere altamente accesible, pero dado que cualquier software de VMware es conocido por sus precios a caballo, usaremos un software de administraci√≥n de virtualizaci√≥n absolutamente gratuito, <a href="https://ru.wikipedia.org/wiki/OVirt"><strong>oVirt</strong></a> , sobre la base de lo cual es bien conocido pero Producto ya comercial - <a href="https://ru.bmstu.wiki/RHEV_(Red_Hat_Enterprise_Virtualization)"><strong>RHEV</strong></a> . </p><br><p>  El software <strong>oVirt</strong> es necesario para combinar todos los elementos de infraestructura para poder trabajar c√≥modamente con m√°quinas virtuales altamente accesibles: bases de datos, aplicaciones web, servidores proxy, equilibradores, servidores para recopilar registros y an√°lisis, etc. n., es decir, en qu√© consiste el portal web de nuestra empresa. </p><br><p>  Resumiendo esta introducci√≥n, nos esperan los siguientes art√≠culos, que en la pr√°ctica mostrar√°n c√≥mo implementar toda la infraestructura de hardware y software de la empresa: </p><br><div class="spoiler">  <b class="spoiler_title">Lista de art√≠culos</b> <div class="spoiler_text"><ul><li>  <strong>Parte 1.</strong> Preparaci√≥n para la implementaci√≥n del cl√∫ster oVirt 4.3. </li><li>  <strong>Parte 2.</strong> Instalaci√≥n y configuraci√≥n del oVirt cluster 4.3. </li><li>  <strong>Parte 3.</strong> Organizaci√≥n del enrutamiento tolerante a fallas en los enrutadores virtuales VyOS. </li><li>  <strong>Parte 4.</strong> Configuraci√≥n de la pila Cisco 3850, organizaci√≥n del enrutamiento de la intranet. </li></ul></div></div><br><h2 id="chast-1-podgotovka-k-razvyortyvaniyu-klastera-ovirt-43">  Parte 1. Preparaci√≥n para implementar el cl√∫ster oVirt 4.3 </h2><br><h3 id="bazovaya-nastroyka-hostov">  Configuraci√≥n b√°sica del host </h3><br><p>  Instalar y configurar el sistema operativo es el paso m√°s f√°cil.  Hay muchos art√≠culos sobre c√≥mo instalar y configurar correctamente el sistema operativo, por lo que no tiene sentido tratar de dar algo exclusivo sobre esto. </p><br><p>  Por lo tanto, tenemos dos hosts Dell PowerEdge R640, en los que necesita instalar el sistema operativo y realizar configuraciones previas, para usarlos como hipervisores para ejecutar m√°quinas virtuales en el cl√∫ster oVirt 4.3. </p><br><p>  Dado que planeamos utilizar el software gratuito no comercial oVirt, <strong>se</strong> eligi√≥ <strong>CentOS 7.7</strong> para la implementaci√≥n de hosts, aunque tambi√©n se pueden instalar otros sistemas operativos en hosts para oVirt: </p><br><ul><li>  construcci√≥n especial basada en RHEL, el llamado  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html"><strong>oVirt Node</strong></a> ; </li><li>  OS Oracle Linux, en el verano de 2019, <a href="https://blogs.oracle.com/virtualization/announcing-oracle-linux-virtualization-manager">se anunci√≥ el</a> soporte para la operaci√≥n de oVirt en √©l. </li></ul><br><p>  Antes de instalar el sistema operativo, se recomienda: </p><br><ul><li>  Configure la interfaz de red de iDRAC en ambos hosts </li><li>  actualizar el firmware para BIOS e iDRAC a las √∫ltimas versiones; </li><li>  es deseable configurar el servidor de Perfil del sistema en modo Rendimiento; </li><li>  configurar RAID desde discos locales (se recomienda RAID1) para instalar el sistema operativo en el servidor. </li></ul><br><p>  Luego instalamos el sistema operativo en el disco creado anteriormente a trav√©s de iDRAC: el proceso de instalaci√≥n es normal, no hay momentos especiales en √©l.  El acceso a la consola del servidor para comenzar a instalar el sistema operativo tambi√©n se puede obtener a trav√©s de iDRAC, aunque no hay nada que le impida conectar el monitor, el teclado y el mouse directamente al servidor e instalar el sistema operativo desde una unidad flash. </p><br><p>  Despu√©s de instalar el sistema operativo, realice su configuraci√≥n inicial: </p><br><pre><code class="plaintext hljs">systemctl enable network.service systemctl start network.service systemctl status network.service</code> </pre> <br><pre> <code class="plaintext hljs">systemctl stop NetworkManager systemctl disable NetworkManager systemctl status NetworkManager</code> </pre> <br><pre> <code class="plaintext hljs">yum install -y ntp systemctl enable ntpd.service systemctl start ntpd.service</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysconfig/selinux SELINUX=disabled SELINUXTYPE=targeted</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysctl.conf vm.max_map_count = 262144 vm.swappiness = 1</code> </pre> <br><p>  <u>Instale el conjunto b√°sico de software</u> </p><br><p>  Para la configuraci√≥n inicial del sistema operativo, debe configurar cualquier interfaz de red en el servidor para que pueda acceder a Internet, actualizar el sistema operativo e instalar los paquetes de software necesarios.  Esto se puede hacer tanto durante la instalaci√≥n del sistema operativo como despu√©s. </p><br><pre> <code class="plaintext hljs">yum -y install epel-release yum update yum -y install bind-utils yum-utils net-tools git htop iotop nmon pciutils sysfsutils sysstat mc nc rsync wget traceroute gzip unzip telnet</code> </pre> <br><p>  Todas las configuraciones anteriores y un conjunto de software es una cuesti√≥n de preferencia personal, y este conjunto es solo una recomendaci√≥n. </p><br><p>  Dado que nuestro host desempe√±ar√° el papel de un hipervisor, habilitaremos el perfil de rendimiento deseado: </p><br><pre> <code class="plaintext hljs">systemctl enable tuned systemctl start tuned systemctl status tuned</code> </pre> <br><pre> <code class="plaintext hljs">tuned-adm profile tuned-adm profile virtual-host</code> </pre> <br><p>  Puede leer m√°s sobre el perfil de rendimiento aqu√≠: " <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-virtualization_tuning_optimization_guide-tuned">Cap√≠tulo 4. tuned and tuned-adm</a> ". </p><br><p>  Despu√©s de instalar el sistema operativo, pasamos a la siguiente parte: configurar las interfaces de red en los hosts y la pila de conmutadores Cisco 2960X. </p><br><h3 id="nastroyka-steka-kommutatorov-cisco-2960x">  Configuraci√≥n de la pila del switch Cisco 2960X </h3><br><p>  Nuestro proyecto utilizar√° los siguientes n√∫meros de VLAN, o dominios de difusi√≥n aislados entre s√≠, para separar diferentes tipos de tr√°fico: </p><br><p>  <strong>VLAN 10</strong> - Internet <br>  <strong>VLAN 17</strong> - Administraci√≥n (iDRAC, almacenamiento, administraci√≥n de conmutadores) <br>  <strong>VLAN 32</strong> : red de producci√≥n de VM <br>  <strong>VLAN 33</strong> - red de interconexi√≥n (a contratistas externos) <br>  <strong>VLAN 34</strong> : red de prueba de VM <br>  <strong>VLAN 35</strong> : red de desarrolladores de VM <br>  <strong>VLAN 40</strong> - Red de monitoreo </p><br><p>  Antes de comenzar a trabajar, presentamos un diagrama en el nivel L2, al que finalmente deber√≠amos llegar: </p><br><p><img src="https://habrastorage.org/webt/in/ak/8a/inak8aoyty8yujo2zagwdehdytg.jpeg"></p><br><p>  Para la interacci√≥n de red entre los hosts oVirt y las m√°quinas virtuales entre s√≠, as√≠ como para administrar nuestro almacenamiento, debe configurar la pila de conmutadores Cisco 2960X. </p><br><p>  Los hosts Dell tienen tarjetas de red de 4 puertos incorporadas, por lo tanto, es aconsejable organizar su conexi√≥n al Cisco 2960X utilizando una conexi√≥n de red tolerante a fallas, utilizando la agrupaci√≥n de puertos de red f√≠sicos en una interfaz l√≥gica y el protocolo LACP (802.3ad): </p><br><ul><li>  los primeros dos puertos en el host se configuran en modo de enlace y se conectan al conmutador 2960X: en esta interfaz l√≥gica, se configurar√° un <strong><em>puente</em></strong> con una direcci√≥n para administrar el host, monitorear, comunicarse con otros hosts en el cl√∫ster oVirt, tambi√©n se usar√° para la migraci√≥n en vivo de m√°quinas virtuales; </li><li>  los dos segundos puertos en el host tambi√©n se configuran en modo de enlace y se conectan al 2960X; en esta interfaz l√≥gica que usa oVirt, se crear√°n puentes posteriores (en las VLAN correspondientes) a los que se conectar√°n las m√°quinas virtuales. </li><li>  ambos puertos de red, dentro de la misma interfaz l√≥gica, estar√°n activos, es decir  El tr√°fico en ellos se puede transmitir simult√°neamente, en modo de equilibrio. </li><li>  la configuraci√≥n de red en los nodos del cl√∫ster debe ser absolutamente IGUAL, con la excepci√≥n de las direcciones IP. </li></ul><br><p>  <u>Configuraci√≥n b√°sica de la <strong>pila de</strong> conmutadores <strong>2960X</strong> y sus puertos</u> </p><br><p>  Anteriormente, nuestros interruptores deber√≠an ser: </p><br><ul><li>  montado en un estante; </li><li>  conectado por dos cables especiales de la longitud deseada, por ejemplo, CAB-STK-E-1M; </li><li>  conectado a la fuente de alimentaci√≥n; </li><li>  conectado a la estaci√≥n de trabajo del administrador a trav√©s del puerto de la consola, para su configuraci√≥n inicial. </li></ul><br><p>  La gu√≠a necesaria para esto est√° disponible en <a href="https://www.cisco.com/c/en/us/support/switches/catalyst-2960-x-series-switches/products-installation-guides-list.html">la p√°gina oficial</a> del fabricante. </p><br><p>  Despu√©s de realizar los pasos anteriores, configure los interruptores. <br>  No se debe descifrar lo que significa cada equipo en el marco de este art√≠culo; si es necesario, toda la informaci√≥n se puede encontrar de forma independiente. <br>  Nuestro objetivo es configurar la pila de conmutadores lo m√°s r√°pido posible y conectarle los hosts y las interfaces de administraci√≥n de almacenamiento. </p><br><p>  1) Con√©ctese al interruptor maestro, entre en modo privilegiado, luego entre en modo de configuraci√≥n y realice ajustes b√°sicos. </p><br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n b√°sica del interruptor:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> enable configure terminal hostname 2960X no service pad service timestamps debug datetime msec service timestamps log datetime localtime show-timezone msec no service password-encryption service sequence-numbers switch 1 priority 15 switch 2 priority 14 stack-mac persistent timer 0 clock timezone MSK 3 vtp mode transparent ip subnet-zero vlan 17 name Management vlan 32 name PROD vlan 33 name Interconnect vlan 34 name Test vlan 35 name Dev vlan 40 name Monitoring spanning-tree mode rapid-pvst spanning-tree etherchannel guard misconfig spanning-tree portfast bpduguard default spanning-tree extend system-id spanning-tree vlan 1-40 root primary spanning-tree loopguard default vlan internal allocation policy ascending port-channel load-balance src-dst-ip errdisable recovery cause loopback errdisable recovery cause bpduguard errdisable recovery interval 60 line con 0 session-timeout 60 exec-timeout 60 0 logging synchronous line vty 5 15 session-timeout 60 exec-timeout 60 0 logging synchronous ip http server ip http secure-server no vstack interface Vlan1 no ip address shutdown exit</code> </pre></div></div><br><p>  Guardamos la configuraci√≥n con el comando <strong>wr mem</strong> y recargamos la pila del switch con el comando <strong>reload</strong> en el switch maestro 1. </p><br><p>  2) Configure los puertos de red del conmutador en modo de acceso en la VLAN 17, para conectar las interfaces de administraci√≥n de los servidores de almacenamiento e iDRAC. </p><br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n del puerto de administraci√≥n:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface GigabitEthernet1/0/5 description iDRAC - host1 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet1/0/6 description Storage1 - Cntr0/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/5 description iDRAC - host2 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/6 description Storage1 ‚Äì Cntr1/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge exit</code> </pre> </div></div><br><p>  3) Despu√©s de reiniciar la pila, verifique que funcione correctamente: </p><br><div class="spoiler">  <b class="spoiler_title">Comprobaci√≥n de la operaci√≥n de la pila:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">2960X#show switch stack-ring speed Stack Ring Speed : 20G Stack Ring Configuration: Full Stack Ring Protocol : FlexStack 2960X#show switch stack-ports Switch # Port 1 Port 2 -------- ------ ------ 1 Ok Ok 2 Ok Ok 2960X#show switch neighbors Switch # Port 1 Port 2 -------- ------ ------ 1 2 2 2 1 1 2960X#show switch detail Switch/Stack Mac Address : 0cd0.f8e4. Mac persistency wait time: Indefinite H/W Current Switch# Role Mac Address Priority Version State ---------------------------------------------------------- *1 Master 0cd0.f8e4. 15 4 Ready 2 Member 0029.c251. 14 4 Ready Stack Port Status Neighbors Switch# Port 1 Port 2 Port 1 Port 2 -------------------------------------------------------- 1 Ok Ok 2 2 2 Ok Ok 1 1</code> </pre> </div></div><br><p>  4) Configuraci√≥n del acceso SSH a la pila 2960X </p><br><p>  Para la administraci√≥n remota de la pila a trav√©s de SSH, utilizaremos IP 172.20.1.10 configurado en SVI (interfaz virtual de conmutaci√≥n) <strong>VLAN17</strong> . </p><br><p>  Aunque para fines de administraci√≥n es aconsejable utilizar un puerto dedicado especial en el conmutador, pero esto es una cuesti√≥n de preferencia personal y oportunidad. </p><br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n del acceso SSH a la pila de conmutadores:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">ip default-gateway 172.20.1.2 interface vlan 17 ip address 172.20.1.10 255.255.255.0 hostname 2960X ip domain-name hw.home-lab.ru no ip domain-lookup clock set 12:47:04 06 Dec 2019 crypto key generate rsa ip ssh version 2 ip ssh time-out 90 line vty 0 4 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh line vty 5 15 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh aaa new-model aaa authentication login default local username cisco privilege 15 secret my_ssh_password</code> </pre> </div></div><br><p>  Configure una contrase√±a para ingresar al modo privilegiado: </p><br><pre> <code class="plaintext hljs">enable secret *myenablepassword* service password-encryption</code> </pre> <br><p>  Configurar NTP: </p><br><pre> <code class="plaintext hljs">ntp server 85.21.78.8 prefer ntp server 89.221.207.113 ntp server 185.22.60.71 ntp server 192.36.143.130 ntp server 185.209.85.222 show ntp status show ntp associations show clock detail</code> </pre> <br><p>  5) Configure las interfaces l√≥gicas Etherchannel y los puertos f√≠sicos conectados a los hosts.  Para facilitar la configuraci√≥n, se permitir√°n todas las VLAN disponibles en todas las interfaces l√≥gicas, pero generalmente se recomienda configurar solo lo que necesita: </p><br><div class="spoiler">  <b class="spoiler_title">Configure las interfaces Etherchannel:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface Port-channel1 description EtherChannel with Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel2 description EtherChannel with Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel3 description EtherChannel with Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel4 description EtherChannel with Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface GigabitEthernet1/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet1/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet1/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet1/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active interface GigabitEthernet2/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet2/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet2/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet2/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active</code> </pre> </div></div><br><p>  <u>Configuraci√≥n inicial de interfaces de red para m√°quinas virtuales en <strong>Host1</strong> y <strong>Host2</strong></u> </p><br><p>  Verificamos la disponibilidad de los m√≥dulos necesarios para la uni√≥n en el sistema, instalamos el m√≥dulo para gestionar puentes: </p><br><pre> <code class="plaintext hljs">modinfo bonding modinfo 8021q yum install bridge-utils</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n de la interfaz l√≥gica BOND1 para m√°quinas virtuales en hosts y sus interfaces f√≠sicas:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1 #DESCRIPTION - management DEVICE=bond1 NAME=bond1 TYPE=Bond IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em2 #DESCRIPTION - management DEVICE=em2 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em3 #DESCRIPTION - management DEVICE=em3 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Despu√©s de completar la configuraci√≥n en la pila <strong>2960X</strong> y los hosts, reiniciamos la red en los hosts y verificamos la interfaz l√≥gica. </p><br><ul><li>  en el host: </li></ul><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: IEEE 802.3ad Dynamic link aggregation Transmit Hash Policy: layer2+3 (2) MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 ... 802.3ad info LACP rate: fast Min links: 0 Aggregator selection policy (ad_select): stable System priority: 65535 ... Slave Interface: em2 MII Status: up Speed: 1000 Mbps Duplex: full ... Slave Interface: em3 MII Status: up Speed: 1000 Mbps Duplex: full</code> </pre> <br><ul><li>  en la <strong>pila de</strong> conmutadores <strong>2960X</strong> : </li></ul><br><pre> <code class="plaintext hljs">2960X#show lacp internal Flags: S - Device is requesting Slow LACPDUs F - Device is requesting Fast LACPDUs A - Device is in Active mode P - Device is in Passive mode Channel group 1 LACP port Admin Oper Port Port Port Flags State Priority Key Key Number State Gi1/0/1 SA bndl 32768 0x1 0x1 0x102 0x3D Gi2/0/1 SA bndl 32768 0x1 0x1 0x202 0x3D 2960X#sh etherchannel summary Flags: D - down P - bundled in port-channel I - stand-alone s - suspended H - Hot-standby (LACP only) R - Layer3 S - Layer2 U - in use N - not in use, no aggregation f - failed to allocate aggregator M - not in use, minimum links not met m - not in use, port not aggregated due to minimum links not met u - unsuitable for bundling w - waiting to be aggregated d - default port A - formed by Auto LAG Number of channel-groups in use: 11 Number of aggregators: 11 Group Port-channel Protocol Ports ------+-------------+-----------+----------------------------------------------- 1 Po1(SU) LACP Gi1/0/1(P) Gi2/0/1(P)</code> </pre> <br><p>  Configuraci√≥n inicial de interfaces de red para administrar recursos de cl√∫ster en <strong>Host1</strong> y <strong>Host2</strong> </p><br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n de la interfaz l√≥gica BOND1 para la administraci√≥n y sus interfaces f√≠sicas en los hosts:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond0 #DESCRIPTION - management DEVICE=bond0 NAME=bond0 TYPE=Bond BONDING_MASTER=yes IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em0 #DESCRIPTION - management DEVICE=em0 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em1 #DESCRIPTION - management DEVICE=em1 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Despu√©s de completar la configuraci√≥n en la pila <strong>2960X</strong> y los hosts, reiniciamos la red en los hosts y verificamos la interfaz l√≥gica. </p><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 2960X#show lacp internal 2960X#sh etherchannel summary</code> </pre> <br><p>  Configuramos la interfaz de red de control en cada host en la <strong>VLAN 17</strong> y la vinculamos a la interfaz l√≥gica BOND1: </p><br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n de VLAN17 en Host1:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.163 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Configuraci√≥n de VLAN17 en Host2:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.164 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><p>  Reiniciamos la red en los hosts y verificamos su visibilidad entre ellos. </p><br><p>  Esto completa la configuraci√≥n de la pila de conmutadores Cisco 2960X, y si todo se hizo correctamente, ahora tenemos la conectividad de red de todos los elementos de infraestructura entre s√≠ en el nivel L2. </p><br><h3 id="nastroyka-shd-dell-md3820f">  Configurar el almacenamiento Dell MD3820f </h3><br><p>  Antes de comenzar a trabajar en la configuraci√≥n del sistema de almacenamiento, ya debe estar conectado a la pila de conmutadores Cisco <strong>2960X</strong> mediante las interfaces de administraci√≥n, as√≠ como a los <strong>hosts Host1</strong> y <strong>Host2 a</strong> trav√©s de FC. </p><br><p>  El esquema general de c√≥mo se debe conectar el almacenamiento a la pila de conmutadores se dio en el cap√≠tulo anterior. </p><br><p>  El esquema de conexi√≥n de almacenamiento en FC a hosts deber√≠a verse as√≠: </p><br><p><img src="https://habrastorage.org/webt/el/k8/5a/elk85aqc6ilmxeiowmh6acrkwri.jpeg"></p><br><p>  Durante la conexi√≥n, es necesario registrar direcciones WWPN para hosts FC HBA conectados a puertos FC en el sistema de almacenamiento; esto ser√° necesario para la configuraci√≥n posterior de la vinculaci√≥n de hosts a LUN en el sistema de almacenamiento. </p><br><p>  En la estaci√≥n de trabajo del administrador, descargue e instale la utilidad de administraci√≥n de almacenamiento Dell MD3820f: <strong>PowerVault Modular Disk Storage Manager</strong> ( <strong>MDSM</strong> ). <br>  Nos conectamos a √©l a trav√©s de sus direcciones IP predeterminadas y luego configuramos nuestras direcciones desde <strong>VLAN17</strong> para controlar los controladores a trav√©s de TCP / IP: </p><br><p>  <strong>Almacenamiento1</strong> : </p><br><pre> <code class="plaintext hljs">ControllerA IP - 172.20.1.13, MASK - 255.255.255.0, Gateway - 172.20.1.2 ControllerB IP - 172.20.1.14, MASK - 255.255.255.0, Gateway - 172.20.1.2</code> </pre> <br><p>  Despu√©s de configurar las direcciones, vaya a la interfaz de administraci√≥n de almacenamiento y establezca una contrase√±a, establezca la hora, actualice el firmware de los controladores y discos, si es necesario, etc. <br>  C√≥mo se hace esto se describe en <a href="https://www.dell.com/support/manuals/ru/ru/rubsdc/powervault-md3820f/mdseriesagpub/introduction%3Fguid%3Dguid-51208376-0df9-48a8-be66-63c1b06512ae%26lang%3Den-us">la gu√≠a de administraci√≥n de</a> almacenamiento. </p><br><p>  Despu√©s de completar la configuraci√≥n anterior, necesitaremos realizar algunas acciones: </p><br><ol><li>  Configurar <strong>identificadores de host</strong> FC. </li><li>  Cree un grupo de host: grupo de <strong>host</strong> y agregue nuestros dos hosts de Dell. </li><li>  Cree un grupo de discos y discos virtuales (o LUN) en √©l, que se presentar√°n a los hosts. </li><li>  Configure la presentaci√≥n de discos virtuales (o LUN) para hosts. </li></ol><br><p>  La adici√≥n de nuevos hosts e identificadores de enlace de los puertos FC del host a ellos se realiza a trav√©s del men√∫ - <strong>Asignaciones de host</strong> -&gt; <strong>Definir</strong> -&gt; <strong>Hosts ...</strong> <br>  Las direcciones WWPN de los hosts FC HBA se pueden encontrar, por ejemplo, en un servidor iDRAC. </p><br><p>  Como resultado, deber√≠amos obtener algo como esto: </p><br><p><img src="https://habrastorage.org/webt/p_/uk/u4/p_uku4o-ewgqpyi0xudxqjesfjc.png"></p><br><p>  Agregar un nuevo grupo de hosts y vincular hosts se realiza a trav√©s del men√∫ - <strong>Asignaciones de host</strong> -&gt; <strong>Definir</strong> -&gt; <strong>Grupo de host ...</strong> <br>  Para los hosts, seleccione el tipo de sistema operativo: <strong><em>Linux (DM-MP)</em></strong> . </p><br><p>  Despu√©s de crear el grupo host, a trav√©s de la pesta√±a <strong>Servicios de almacenamiento y copia</strong> , cree un grupo de <strong>discos: Grupo de discos</strong> , cuyo tipo depender√° de los requisitos de tolerancia a fallas, por ejemplo, RAID10, y en √©l discos virtuales del tama√±o correcto: </p><br><p><img src="https://habrastorage.org/webt/ue/g2/kn/ueg2kn0m3usv1kb4tygvx2vajz0.png"></p><br><p>  Y finalmente, la etapa final es la presentaci√≥n de discos virtuales (o LUN) para los hosts. <br>  Para hacer esto, a trav√©s del men√∫ - <strong>Mapeos de host</strong> -&gt; <strong>Mapeo Lun</strong> -&gt; <strong>Agregar ...</strong> hacemos el enlace de discos virtuales a los hosts, asign√°ndoles n√∫meros. </p><br><p>  Todo deber√≠a salir como en esta captura de pantalla: </p><br><p><img src="https://habrastorage.org/webt/db/a-/xq/dba-xqbacgrjxrutupvhvmua7ro.png"></p><br><p>  Hemos terminado con la configuraci√≥n de los sistemas de almacenamiento, y si todo se hizo correctamente, los hosts deber√≠an ver los LUN que se les presentan a trav√©s de sus HBA FC. <br>  Haga que el sistema actualice la informaci√≥n sobre las unidades asignadas: </p><br><pre> <code class="plaintext hljs">ls -la /sys/class/scsi_host/ echo "- - -" &gt; /sys/class/scsi_host/host[0-9]/scan</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Veamos qu√© dispositivos son visibles en nuestros servidores:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /proc/scsi/scsi Attached devices: Host: scsi0 Channel: 02 Id: 00 Lun: 00 Vendor: DELL Model: PERC H330 Mini Rev: 4.29 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 lsscsi [0:2:0:0] disk DELL PERC H330 Mini 4.29 /dev/sda [15:0:0:0] disk DELL MD38xxf 0825 - [15:0:0:1] disk DELL MD38xxf 0825 /dev/sdb [15:0:0:4] disk DELL MD38xxf 0825 /dev/sdc [15:0:0:11] disk DELL MD38xxf 0825 /dev/sdd [15:0:0:31] disk DELL Universal Xport 0825 - [18:0:0:0] disk DELL MD38xxf 0825 - [18:0:0:1] disk DELL MD38xxf 0825 /dev/sdi [18:0:0:4] disk DELL MD38xxf 0825 /dev/sdj [18:0:0:11] disk DELL MD38xxf 0825 /dev/sdk [18:0:0:31] disk DELL Universal Xport 0825 -</code> </pre> </div></div><br><p>  Tambi√©n puede configurar <strong>rutas m√∫ltiples</strong> en los hosts, y aunque al instalar oVirt puede hacerlo √©l mismo, es mejor verificar el MP por s√≠ mismo con anticipaci√≥n. </p><br><p>  <u>Instalar y configurar DM Multipath</u> </p><br><pre> <code class="plaintext hljs">yum install device-mapper-multipath mpathconf --enable --user_friendly_names y cat /etc/multipath.conf | egrep -v "^\s*(#|$)" defaults { user_friendly_names yes find_multipaths yes } blacklist { wwid 26353900f02796769 devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*" devnode "^hd[az]" }</code> </pre> <br><p>  Instale el servicio MP en inicio autom√°tico y ejec√∫telo: </p><br><pre> <code class="plaintext hljs">systemctl enable multipathd &amp;&amp; systemctl restart multipathd</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Verificaci√≥n de informaci√≥n sobre m√≥dulos cargados para operaci√≥n MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">lsmod | grep dm_multipath dm_multipath 27792 6 dm_service_time dm_mod 124407 139 dm_multipath,dm_log,dm_mirror modinfo dm_multipath filename: /lib/modules/3.10.0-957.12.2.el7.x86_64/kernel/drivers/md/dm-multipath.ko.xz license: GPL author: Sistina Software &lt;dm-devel@redhat.com&gt; description: device-mapper multipath target retpoline: Y rhelversion: 7.6 srcversion: 985A03DCAF053D4910E53EE depends: dm-mod intree: Y vermagic: 3.10.0-957.12.2.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: A3:2D:39:46:F2:D3:58:EA:52:30:1F:63:37:8A:37:A5:54:03:00:45 sig_hashalgo: sha256</code> </pre> </div></div><br><p>  Consulte el resumen de la configuraci√≥n de rutas m√∫ltiples existente: </p><br><pre> <code class="plaintext hljs">mpathconf multipath is enabled find_multipaths is disabled user_friendly_names is disabled dm_multipath module is loaded multipathd is running</code> </pre> <br><p>  Despu√©s de agregar un nuevo LUN al almacenamiento y presentarlo al host, es necesario escanearlo conectado al host HBA. </p><br><pre> <code class="plaintext hljs">systemctl reload multipathd multipath -v2</code> </pre> <br><p>  Y, por √∫ltimo, verificamos si todos los LUN se presentaron en el almacenamiento para hosts y si todos tienen dos rutas. </p><br><div class="spoiler">  <b class="spoiler_title">Verifique el funcionamiento del MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">multipath -ll 3600a098000e4b4b3000003175cec1840 dm-2 DELL ,MD38xxf size=2.0T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:1 sdb 8:16 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:1 sdi 8:128 active ready running 3600a098000e4b48f000002ab5cec1921 dm-6 DELL ,MD38xxf size=10T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 18:0:0:11 sdk 8:160 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 15:0:0:11 sdd 8:48 active ready running 3600a098000e4b4b3000003c95d171065 dm-3 DELL ,MD38xxf size=150G features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:4 sdc 8:32 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:4 sdj 8:144 active ready running</code> </pre> </div></div><br><p>  Como puede ver, los tres discos virtuales en el sistema de almacenamiento son visibles de dos maneras.  Por lo tanto, todo el trabajo preparatorio se ha completado, lo que significa que podemos pasar a la parte principal: configurar el cl√∫ster oVirt, que se discutir√° en el pr√≥ximo art√≠culo. </p></div></div><p>Source: <a href="https://habr.com/ru/post/483980/">https://habr.com/ru/post/483980/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../483964/index.html">No tengas miedo de JSON o tu primera aplicaci√≥n API</a></li>
<li><a href="../483972/index.html">C√≥mo usar Quora para promocionar su negocio</a></li>
<li><a href="../483974/index.html">Ceph a trav√©s de iSCSI - o esquiar mientras est√° parado en una hamaca</a></li>
<li><a href="../483976/index.html">2020 ciberseguridad y amenazas: lo que nos espera despu√©s de las vacaciones</a></li>
<li><a href="../483978/index.html">Comprender el concepto de desarrollo moderno de aplicaciones web en 2020</a></li>
<li><a href="../483986/index.html">Controlando pensamientos de robots con Emotiv Insight</a></li>
<li><a href="../483988/index.html">MicroSPA, o c√≥mo inventar una rueda cuadrada</a></li>
<li><a href="../483992/index.html">¬øPor qu√© algunos planetas comen su cielo?</a></li>
<li><a href="../483994/index.html">Reubicaci√≥n de TI en un yate. De Suecia a Espa√±a</a></li>
<li><a href="../484004/index.html">@Pythonetc Diciembre 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>