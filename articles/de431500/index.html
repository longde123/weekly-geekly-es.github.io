<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧕🏽 📢 💇 Datenbanken und Kubernetes (Rückblick und Videobericht) ✍🏼 ✍🏼 📘</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Am 8. November wurde in der Haupthalle der HighLoad ++ 2018- Konferenz im Rahmen des Abschnitts DevOps and Operations ein Bericht mit dem Titel Databa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Datenbanken und Kubernetes (Rückblick und Videobericht)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/431500/">  Am 8. November wurde in der Haupthalle der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HighLoad ++ 2018-</a> Konferenz im Rahmen des Abschnitts DevOps and Operations ein Bericht mit dem Titel Databases and Kubernetes erstellt.  Es geht um die hohe Verfügbarkeit von Datenbanken und Ansätze zur Fehlertoleranz gegenüber Kubernetes und damit sowie um praktische Optionen für die Platzierung von DBMS in Kubernetes-Clustern und bestehende Lösungen dafür (einschließlich Stolon für PostgreSQL). <br><br><img src="https://habrastorage.org/webt/oq/mh/kp/oqmhkpy4pxg-olk9yybf_julwvu.jpeg"><br><br>  Aus Tradition freuen wir uns, ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Video mit einem Bericht</b></a> (ungefähr eine Stunde, <b>viel</b> informativer <b>als der</b> Artikel) und dem Hauptdruck in Textform zu präsentieren.  Lass uns gehen! <a name="habracut"></a><br><br><h2>  Theorie </h2><br>  Dieser Bericht erschien als Antwort auf eine der beliebtesten Fragen, die uns in den letzten Jahren unermüdlich an verschiedenen Orten gestellt wurden: Kommentare im Hub oder auf YouTube, in sozialen Netzwerken usw.  Es klingt einfach: "Ist es möglich, die Datenbank in Kubernetes auszuführen?", Und wenn wir normalerweise mit "allgemein ja, aber ..." geantwortet haben, gab es eindeutig nicht genug Erklärungen für diese "allgemein" und "aber", sondern um sie anzupassen in einer kurzen Nachricht nicht erfolgreich. <br><br>  Für den Anfang fasse ich das Problem jedoch von der "Datenbank [Daten]" bis zum Stateful als Ganzes zusammen.  Ein DBMS ist nur ein Sonderfall von zustandsbehafteten Entscheidungen, von denen eine vollständigere Liste wie folgt dargestellt werden kann: <br><br><img src="https://habrastorage.org/webt/px/ps/2_/pxps2_ff80ru5qfth8bduiu_kdw.png"><br><br>  Bevor ich mich mit bestimmten Fällen befasse, werde ich auf drei wichtige Merkmale der Arbeit / Verwendung von Kubernetes eingehen. <br><br><h3>  1. Kubernetes Hochverfügbarkeitsphilosophie </h3><br>  Jeder kennt die Analogie „Haustiere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gegen Rinder</a> “ und versteht, dass klassische DBMS nur Haustiere sind, wenn Kubernetes eine Geschichte aus der Herdenwelt ist. <br><br>  Und wie sah die Architektur der „Haustiere“ in der „traditionellen“ Version aus?  Ein klassisches Beispiel für die Installation von MySQL ist die Replikation auf zwei Iron-Servern mit redundanter Stromversorgung, Festplatte, Netzwerk ... und allem anderen (einschließlich eines Ingenieurs und verschiedener Hilfstools), um sicherzustellen, dass der MySQL-Prozess nicht fehlschlägt und wenn ein Problem mit einem der kritischen Probleme auftritt Für seine Komponenten wird die Fehlertoleranz eingehalten: <br><br><img src="https://habrastorage.org/webt/0m/qg/s3/0mqgs3e3cco1zukmlah1jg2ubjs.png"><br><br>  Wie wird das gleiche in Kubernetes aussehen?  Hier gibt es normalerweise viel mehr Eisenserver, sie sind einfacher und sie haben keine redundante Stromversorgung und kein redundantes Netzwerk (in dem Sinne, dass der Verlust einer Maschine nichts beeinflusst) - all dies wird zu einem Cluster zusammengefasst.  Die Fehlertoleranz wird von der Software bereitgestellt: Wenn dem Knoten etwas passiert, erkennt Kubernetes die erforderlichen Instanzen auf dem anderen Knoten und startet sie. <br><br>  Was sind die Mechanismen für eine hohe Verfügbarkeit in K8? <br><br><img src="https://habrastorage.org/webt/n2/gw/mh/n2gwmhiogm3uzv1m5igrzqpyifq.png"><br><br><ol><li>  Controller  Es gibt viele, aber zwei Hauptprobleme: <code>Deployment</code> (für zustandslose Anwendungen) und <code>StatefulSet</code> (für zustandsbehaftete Anwendungen).  Sie speichern die gesamte Logik der Aktionen, die im Falle eines Knotenabsturzes ausgeführt werden (Pod-Unzugänglichkeit). </li><li>  <code>PodAntiAffinity</code> - Die Möglichkeit, bestimmte Pods so anzugeben, dass sie sich nicht auf demselben Knoten befinden. </li><li>  <code>PodDisruptionBudgets</code> - <code>PodDisruptionBudgets</code> die Anzahl der Pod-Instanzen, die bei geplanten Arbeiten gleichzeitig <code>PodDisruptionBudgets</code> können. </li></ol><br><h3>  2. Kubernetes Konsistenzgarantien </h3><br>  Wie funktioniert das bekannte Single-Master-Fehlertoleranzschema?  Zwei Server (Master und Standby), auf die ständig von der Anwendung zugegriffen wird, die wiederum über den Load Balancer verwendet wird.  Was passiert bei einem Netzwerkproblem? <br><br><img src="https://habrastorage.org/webt/6p/1k/ve/6p1kvelnrzyrphtu6sb_agftgaa.gif"><br><br>  Klassisches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><i>Split-Brain</i></a> : Die Anwendung beginnt mit dem Zugriff auf beide DBMS-Instanzen, von denen sich jede als die Hauptinstanz betrachtet.  Um dies zu vermeiden, wurde keepalived durch corosync mit bereits drei Instanzen ersetzt, um bei der Abstimmung für den Master ein Quorum zu erreichen.  Selbst in diesem Fall gibt es jedoch Probleme: Wenn eine heruntergefallene DBMS-Instanz versucht, sich auf jede mögliche Weise "umzubringen" (IP-Adresse entfernen, Datenbank in schreibgeschützt übersetzen ...), weiß der andere Teil des Clusters nicht, was mit dem Master passiert ist - es kann passieren, dass dieser Knoten tatsächlich noch funktioniert und Anforderungen an ihn gelangen, was bedeutet, dass wir den Assistenten immer noch nicht wechseln können. <br><br>  Um diese Situation zu lösen, gibt es einen Mechanismus zum Isolieren des Knotens, um den gesamten Cluster vor fehlerhaften Vorgängen zu schützen. Dieser Vorgang wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><i>Fencing bezeichnet</i></a> .  Die praktische Essenz läuft darauf hinaus, dass wir mit externen Mitteln versuchen, das heruntergefallene Auto zu "töten".  Die Ansätze können unterschiedlich sein: vom Ausschalten des Computers über IPMI und Blockieren des Ports am Switch bis zum Zugriff auf die API des Cloud-Anbieters usw.  Und erst nach diesem Vorgang können Sie den Assistenten wechseln.  Dies gewährleistet eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><i>höchstens einmalige</i></a> Garantie, die uns <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konsistenz</a></i> garantiert. <br><br><img src="https://habrastorage.org/webt/sl/xb/9r/slxb9rwf-lk8mcdaeqiuankz3z8.png"><br><br>  Wie kann man das auch in Kubernetes erreichen?  Zu diesem Zweck gibt es bereits erwähnte Controller, deren Verhalten im Falle einer Unzugänglichkeit eines Knotens unterschiedlich ist: <br><br><ol><li>  <code>Deployment</code> : "Mir wurde gesagt, dass es 3 Pods geben soll, und jetzt gibt es nur noch 2 Pods - ich werde einen neuen erstellen"; </li><li>  <code>StatefulSet</code> : "Pod weg?"  Ich werde warten: Entweder wird dieser Knoten zurückkehren oder sie werden uns sagen, dass wir ihn töten sollen. "  Die Container selbst (ohne Bedienereingriff) werden nicht neu erstellt.  Auf diese Weise wird höchstens die gleiche Garantie erreicht. </li></ol><br>  Im letzteren Fall ist jedoch ein Zaun erforderlich: Wir benötigen einen Mechanismus, der bestätigt, dass dieser Knoten definitiv verschwunden ist.  Die automatische Ausführung ist zum einen sehr schwierig (viele Implementierungen sind erforderlich), und zum anderen werden Knoten normalerweise nur langsam getötet (der Zugriff auf IPMI kann Sekunden, zehn Sekunden oder sogar Minuten dauern).  Nur wenige Leute sind mit der Wartezeit pro Minute zufrieden, um die Basis auf den neuen Master umzustellen.  Es gibt jedoch einen anderen Ansatz, für den kein Zaunmechanismus erforderlich ist ... <br><br>  Ich werde seine Beschreibung außerhalb von Kubernetes beginnen.  Es verwendet einen speziellen Load Balancer, über den Backends auf das DBMS zugreifen.  Seine Spezifität liegt in der Tatsache, dass es die Eigenschaft der Konsistenz hat, d.h.  Schutz vor Netzwerkfehlern und Split-Brain, da Sie damit alle Verbindungen zum aktuellen Master entfernen, auf die Synchronisierung (Replikat) auf einem anderen Knoten warten und zu diesem wechseln können.  Ich habe keinen etablierten Begriff für diesen Ansatz gefunden und ihn als <i>konsistente Umschaltung bezeichnet</i> . <br><br><img src="https://habrastorage.org/webt/ct/oq/lk/ctoqlkp3efyctjlvsyiheesk2s4.gif"><br><br>  Die Hauptfrage bei ihm ist, wie man es universell macht und sowohl Cloud-Anbieter als auch private Installationen unterstützt.  Zu diesem Zweck werden den Anwendungen Proxyserver hinzugefügt.  Jeder von ihnen akzeptiert Anfragen aus seiner Bewerbung (und leitet sie an das DBMS weiter), und von allen wird ein Quorum eingeholt.  Sobald ein Teil des Clusters ausfällt, entfernen die Proxys, die das Quorum verloren haben, sofort ihre Verbindungen zum DBMS. <br><br><img src="https://habrastorage.org/webt/nj/y0/-t/njy0-tahnwp8uxeyevxot_tlntq.png"><br><br><h3>  3. Datenspeicherung und Kubernetes </h3><br>  Der Hauptmechanismus ist das <i>Network Block Device</i> (auch bekannt als SAN) -Netzwerklaufwerk in verschiedenen Implementierungen für die gewünschten Cloud-Optionen oder Bare Metal.  Das Einfügen einer geladenen Datenbank (z. B. MySQL, für das 50.000 IOPS erforderlich sind) in die Cloud (AWS EBS) funktioniert jedoch aufgrund der <i>Latenz nicht</i> . <br><br><img src="https://habrastorage.org/webt/qq/wp/r7/qqwpr7fae-nbek0y2o6ex9lbzsa.png"><br><br>  Kubernetes kann in solchen Fällen eine lokale Festplatte anschließen - <i>Local Storage</i> .  Wenn ein Fehler auftritt (die Festplatte ist nicht mehr im Pod verfügbar), müssen wir diesen Computer reparieren - ähnlich wie beim klassischen Schema bei einem Ausfall eines zuverlässigen Servers. <br><br>  Beide Optionen ( <i>Netzwerkblockgerät</i> und <i>lokaler Speicher</i> ) gehören zur Kategorie <i>ReadWriteOnce</i> : Der Speicher kann nicht an zwei Stellen (Pods) <i>bereitgestellt werden.</i> Für diese Skalierung müssen Sie eine neue Festplatte erstellen und mit einem neuen Pod verbinden (hierfür ist ein K8s-Mechanismus integriert). und füllen Sie dann die erforderlichen Daten aus (bereits von unseren Streitkräften erstellt). <br><br>  Wenn wir den <i>ReadWriteMany-</i> Modus benötigen, stehen <i>Implementierungen des</i> <i>Network File System</i> (oder NAS) zur Verfügung: Für die öffentliche Cloud sind dies <code>AzureFile</code> und <code>AWSElasticFileSystem</code> sowie für deren Installationen CephFS und Glusterfs für Fans verteilter Systeme sowie NFS. <br><br><img src="https://habrastorage.org/webt/eq/y6/gp/eqy6gpf2duz9ljtzc342bj9upg4.png"><br><br><h2>  Übe </h2><br><h3>  1. Standalone </h3><br>  Diese Option ist der Fall, wenn Sie durch nichts daran gehindert werden, das DBMS im separaten Servermodus mit lokalem Speicher zu starten.  Von Hochverfügbarkeit ist keine Rede ... obwohl sie bis zu einem gewissen Grad (d. H. Ausreichend für diese Anwendung) auf Eisenebene implementiert werden kann.  Es gibt viele Fälle für diese Anwendung.  Zuallererst sind dies alle Arten von Staging- und Entwicklungsumgebungen, aber nicht nur: Auch sekundäre Dienste fallen hierher. Das Deaktivieren für 15 Minuten ist nicht kritisch.  In Kubernetes wird dies von <code>StatefulSet</code> mit einem Pod implementiert: <br><br><img src="https://habrastorage.org/webt/hl/xk/ha/hlxkhaodepe50imvuobtoilrz6o.png"><br><br>  Im Allgemeinen ist dies eine praktikable Option, die aus meiner Sicht keine Nachteile hat, verglichen mit der Installation eines DBMS auf einer separaten virtuellen Maschine. <br><br><h3>  2. Repliziertes Paar mit manueller Umschaltung </h3><br>  <code>StatefulSet</code> erneut verwendet, aber das allgemeine Schema sieht folgendermaßen aus: <br><br><img src="https://habrastorage.org/webt/vq/_i/to/vq_itonyvigrh0uezqek_lwtlt4.png"><br><br>  Wenn einer der Knoten abstürzt ( <code>mysql-a-0</code> ), geschieht kein Wunder, aber wir haben eine Replik ( <code>mysql-b-0</code> ), auf die wir den Verkehr umschalten können.  In diesem Fall ist es wichtig, nicht nur vor dem Umschalten des Datenverkehrs zu vergessen, nicht nur DBMS-Anforderungen aus dem <code>mysql</code> Dienst zu entfernen, sondern sich auch manuell beim DBMS anzumelden und sicherzustellen, dass alle Verbindungen hergestellt sind (sie zu beenden). Außerdem müssen Sie vom DBMS zum zweiten Knoten wechseln und das Replikat neu konfigurieren in die entgegengesetzte Richtung. <br><br>  Wenn Sie derzeit die klassische Version mit zwei Servern (Master + Standby) ohne automatisches <i>Failover verwenden</i> , entspricht diese Lösung Kubernetes.  Geeignet für MySQL, PostgreSQL, Redis und andere Produkte. <br><br><h3>  3. Skalieren der Leselast </h3><br>  In der Tat ist dieser Fall nicht zustandsbehaftet, weil wir nur über das Lesen sprechen.  Hier befindet sich der Haupt-DBMS-Server außerhalb des betrachteten Schemas, und im Rahmen von Kubernetes wird eine "Farm von Slave-Servern" erstellt, die schreibgeschützt sind.  Der allgemeine Mechanismus - die Verwendung von Init-Containern zum Füllen von DBMS-Daten in jedem neuen Pod dieser Farm (unter Verwendung eines Hot-Dumps oder des üblichen Dumps mit zusätzlichen Aktionen usw. - hängt vom verwendeten DBMS ab).  Um sicherzustellen, dass jede Instanz nicht zu weit vom Master entfernt ist, können Sie Lebendigkeitstests verwenden. <br><br><img src="https://habrastorage.org/webt/nz/pd/uk/nzpdukumat3zbax7vnkcs5jtsvs.png"><br><br><h3>  4. Smart Client </h3><br>  Wenn Sie ein <code>StatefulSet</code> aus drei Memcaches erstellen, bietet Kubernetes einen speziellen Service, der keine Anforderungen ausgleicht, sondern jeden Pod für seine eigene Domain erstellt.  Der Client kann mit ihnen arbeiten, wenn er selbst in der Lage ist, Shards und Replikationen durchzuführen. <br><br>  Für ein Beispiel müssen Sie nicht weit gehen: So funktioniert der Sitzungsspeicher in PHP sofort.  Für jede Sitzungsanforderung werden Anforderungen gleichzeitig an alle Server gestellt, wonach die relevanteste Antwort aus ihnen ausgewählt wird (ähnlich wie bei einem Datensatz). <br><br><img src="https://habrastorage.org/webt/t8/iz/26/t8iz261adru0mbdw7cd2o5i3y8o.png"><br><br><h3>  5. Cloud Native-Lösungen </h3><br>  Es gibt viele Lösungen, die sich anfänglich auf den Ausfall von Knoten konzentrieren, d. H.  Sie selbst können ein <i>Failover</i> und eine Wiederherstellung von Knoten durchführen und bieten <i>Konsistenzgarantien</i> .  Dies ist keine vollständige Liste von ihnen, sondern nur ein Teil der populären Beispiele: <br><br><img src="https://habrastorage.org/webt/9u/ah/qz/9uahqzayfbdsokgyod153jwfjfs.png"><br><br>  Alle von ihnen werden einfach in <code>StatefulSet</code> platziert, wonach sich die Knoten finden und einen Cluster bilden.  Die Produkte selbst unterscheiden sich darin, wie sie drei Dinge implementieren: <br><br><ol><li>  Wie lernen Knoten voneinander?  Es gibt Methoden wie die Kubernetes-API, DNS-Einträge, statische Konfiguration, spezialisierte Knoten (Seed), die Erkennung von Diensten von Drittanbietern ... </li><li>  Wie verbindet sich der Client?  Durch einen Load Balancer, der an Hosts verteilt wird, oder der Client muss über alle Hosts Bescheid wissen, und er wird entscheiden, wie er vorgehen soll. </li><li>  Wie erfolgt die horizontale Skalierung?  Auf keinen Fall voll oder schwierig / mit Einschränkungen. </li></ol><br>  Unabhängig von den gewählten Lösungen für diese Probleme funktionieren alle diese Produkte gut mit Kubernetes, da sie ursprünglich als "Herde" <i>(Vieh) geschaffen wurden</i> . <br><br><h3>  6. Stolon PostgreSQL </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mit Stolon</a> können Sie PostgreSQL, das als <i>Haustier erstellt wurde</i> , in <i>Vieh</i> verwandeln.  Wie wird das erreicht? <br><br><img src="https://habrastorage.org/webt/g_/z4/pc/g_z4pcehfw6p985duphcgrbukzo.png"><br><br><ul><li>  Erstens benötigen wir eine Serviceerkennung, in deren Rolle möglicherweise <b>etcd steht</b> (andere Optionen sind verfügbar) - ein Cluster von ihnen wird in einem <code>StatefulSet</code> abgelegt. </li><li>  Ein weiterer Teil der Infrastruktur ist <code>StatefulSet</code> mit PostgreSQL-Instanzen.  Neben dem eigentlichen DBMS befindet sich neben jeder Installation auch eine Komponente namens <b>Keeper</b> , die die DBMS-Konfiguration ausführt. </li><li>  Eine andere Komponente, <b>Sentinel,</b> wird als <code>Deployment</code> bereitgestellt und überwacht die Konfiguration des Clusters.  Er entscheidet, wer Master und Standby sein wird, schreibt diese Informationen an etcd.  Der Keeper liest Daten aus etcd und führt mit einer Instanz von PostgreSQL Aktionen aus, die dem aktuellen Status entsprechen. </li><li>  Eine weitere Komponente, die in <code>Deployment</code> bereitgestellt wird und PostgreSQL-Instanzen gegenübersteht, <b>Proxy,</b> ist eine Implementierung des bereits erwähnten <i>Consistent Switchover-</i> Musters.  Diese Komponenten sind mit etcd verbunden, und wenn diese Verbindung unterbrochen wird, beendet der Proxy sofort die ausgehenden Verbindungen, da er von diesem Moment an die Rolle seines Servers nicht kennt (ist er jetzt Master oder Standby?). </li><li>  Schließlich stehen Proxy-Instanzen dem üblichen <code>LoadBalancer</code> LoadBalancer gegenüber. </li></ul><br><h2>  Schlussfolgerungen </h2><br>  Ist es also möglich, in Kubernetes zu stationieren?  Ja, natürlich ist es in einigen Fällen möglich ... Und wenn es angebracht ist, wird es so gemacht (siehe Stolon-Workflow) ... <br><br>  Jeder weiß, dass sich die Technologie in Wellen entwickelt.  Anfangs kann jedes neue Gerät sehr schwierig zu bedienen sein, aber im Laufe der Zeit ändert sich alles: Technologie wird verfügbar.  Wohin gehen wir?  Ja, es wird so drinnen bleiben, aber wir werden nicht wissen, wie es funktionieren wird.  Kubernetes entwickelt aktiv <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Betreiber</a> .  Bisher gibt es nicht so viele von ihnen und sie sind nicht so gut, aber es gibt Bewegung in diese Richtung. <br><br><h2>  Videos und Folien </h2><br>  Video von der Aufführung (ca. eine Stunde): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BnegHj53pW4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Präsentation des Berichts: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  PS Wir haben auch im Internet einen sehr (!) Kurzen Textdruck aus diesem Bericht gefunden - danke an Nikolai Volynkin. <br><br><h2>  PPS </h2><br>  Weitere Berichte in unserem Blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Überwachung und Kubernetes</a> ";  <i>(Dmitry Stolyarov; 28. Mai 2018 bei RootConf)</i> ; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beste CI / CD-Praktiken mit Kubernetes und GitLab</a> “;  <i>(Dmitry Stolyarov; 7. November 2017 bei HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unsere Erfahrung mit Kubernetes in kleinen Projekten</a> ";  <i>(Dmitry Stolyarov; 6. Juni 2017 bei RootConf)</i> ; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mit dapp sammeln wir Docker-Images für CI / CD schnell und bequem</a> “ <i>(Dmitry Stolyarov; 8. November 2016 bei HighLoad ++)</i> ; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kontinuierliche Lieferpraktiken mit Docker</a> “ <i>(Dmitry Stolyarov; 31. Mai 2016 bei RootConf)</i> . </li></ul><br>  Sie könnten auch an folgenden Veröffentlichungen interessiert sein: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tipps und Tricks von Kubernetes: Beschleunigen des Bootstraps großer Datenbanken</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CockroachDB DBMS Orchestration in Kubernetes</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de431500/">https://habr.com/ru/post/de431500/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de431488/index.html">Klangmodulation</a></li>
<li><a href="../de431490/index.html">Extern - GUI für Golang</a></li>
<li><a href="../de431492/index.html">Reagieren Sie auf dem HolyJs 2018 am HeadHunter-Stand auf das Quiz des Wettbewerbs</a></li>
<li><a href="../de431496/index.html">Wie Technologie speziellen Klassenlehrern hilft</a></li>
<li><a href="../de431498/index.html">WebP wird das Web bald übernehmen, aber es wird nicht lange dauern</a></li>
<li><a href="../de431502/index.html">Konferenz für iOS-Entwickler Kolesa Mobile 3.0. Videobericht</a></li>
<li><a href="../de431504/index.html">Phishing - funktioniert. Chronik des Diebstahls des iPhone XS, gefolgt von iCloud-Datendiebstahl</a></li>
<li><a href="../de431506/index.html">Xcode und erweitertes Debugging in LLDB: Teil 1</a></li>
<li><a href="../de431508/index.html">Effizientes Transaktionsmanagement im Frühjahr</a></li>
<li><a href="../de431510/index.html">So sammeln Sie Informationen aus der Kontur. Einkauf mit Selen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>