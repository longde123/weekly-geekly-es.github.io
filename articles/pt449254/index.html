<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üà∂ üîà üàÅ FAQ sobre arquitetura e trabalho VKontakte üë¶üèæ üêΩ ‚õîÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A hist√≥ria do VKontakte est√° na Wikipedia, foi contada pelo pr√≥prio Pavel. Parece que todo mundo j√° a conhece. Pavel falou sobre o interior, a arquite...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>FAQ sobre arquitetura e trabalho VKontakte</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/449254/">  A hist√≥ria do VKontakte est√° na Wikipedia, foi contada pelo pr√≥prio Pavel.  Parece que todo mundo j√° a conhece.  Pavel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">falou</a> sobre o interior, a arquitetura e o design do site no HighLoad ++ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">em 2010</a> .  Muitos servidores vazaram desde ent√£o; portanto, atualizaremos as informa√ß√µes: dissecamos, extra√≠mos o interior, pesamos - examinamos o dispositivo VK de um ponto de vista t√©cnico. <br><br><img src="https://habrastorage.org/webt/_x/zc/wp/_xzcwpb5ze_4e-yx_jw_-8nvnei.jpeg"><br><br>  <strong>Alexey Akulovich</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">AterCattus</a> ) √© desenvolvedor de back-end da equipe VKontakte.  A transcri√ß√£o deste relat√≥rio √© uma resposta coletiva a perguntas freq√ºentes sobre o funcionamento da plataforma, infraestrutura, servidores e a intera√ß√£o entre eles, mas n√£o sobre desenvolvimento, principalmente <strong>sobre hardware</strong> .  Separadamente - sobre bancos de dados e o que a VK possui em seu lugar, sobre a coleta de logs e o monitoramento de todo o projeto como um todo.  Detalhes sob o corte. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_GqcriadL-s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Por mais de quatro anos, venho realizando todos os tipos de tarefas relacionadas ao back-end. <br><br><ul><li>  Download, armazenamento, processamento, distribui√ß√£o de m√≠dia: v√≠deo, transmiss√£o ao vivo, √°udio, fotos, documentos. </li><li>  Infraestrutura, plataforma, monitoramento de desenvolvedor, logs, caches regionais, CDN, protocolo RPC propriet√°rio. </li><li>  Integra√ß√£o com servi√ßos externos: envio por correio, an√°lise de links externos, feed RSS. </li><li>  Ajude os colegas em v√°rias quest√µes, para obter as respostas nas quais voc√™ precisa mergulhar em um c√≥digo desconhecido. </li></ul><br>  Durante esse per√≠odo, participei de muitos componentes do site.  Eu quero compartilhar essa experi√™ncia. <br><br><h2>  Arquitetura geral </h2><br>  Tudo, como sempre, come√ßa com um servidor ou um grupo de servidores que aceitam solicita√ß√µes. <br><br><h3>  Servidor frontal </h3><br>  O servidor frontal aceita solicita√ß√µes por HTTPS, RTMP e WSS. <br><br>  <strong>HTTPS</strong> s√£o solicita√ß√µes para as vers√µes web principal e m√≥vel do site: vk.com e m.vk.com e outros clientes oficiais e n√£o oficiais da nossa API: clientes m√≥veis, mensageiros instant√¢neos.  Temos uma recep√ß√£o de tr√°fego <strong>RTMP</strong> para transmiss√µes ao vivo com servidores frontais separados e conex√µes <strong>WSS</strong> para a API de Streaming. <br><br>  Para HTTPS e WSS, o <strong>nginx √©</strong> instalado nos servidores.  Para transmiss√µes RTMP, recentemente mudamos para nossa pr√≥pria solu√ß√£o <strong>kive</strong> , mas ela est√° al√©m do escopo do relat√≥rio.  Para toler√¢ncia a falhas, esses servidores anunciam endere√ßos IP comuns e agem como grupos para que, no caso de um problema em um dos servidores, as solicita√ß√µes do usu√°rio n√£o sejam perdidas.  Para HTTPS e WSS, esses mesmos servidores criptografam o tr√°fego para fazer parte da carga da CPU. <br><br>  Al√©m disso, n√£o falaremos sobre WSS e RTMP, mas apenas sobre solicita√ß√µes HTTPS padr√£o, que geralmente s√£o associadas a um projeto da web. <br><br><h3>  Backend </h3><br>  Atr√°s da frente, geralmente est√£o os servidores back-end.  Eles lidam com solicita√ß√µes que o servidor frontal recebe dos clientes. <br><br>  Esses s√£o <strong>servidores kPHP</strong> executando o daemon HTTP porque o HTTPS j√° est√° descriptografado.  O kPHP √© um servidor que funciona de acordo com o <strong>modelo prefork</strong> : inicia o processo mestre, v√°rios processos filhos, passa soquetes de escuta para eles e eles processam seus pedidos.  Ao mesmo tempo, os processos n√£o s√£o reiniciados entre cada solicita√ß√£o do usu√°rio, mas simplesmente redefinem seu estado para o estado inicial de valor zero - solicita√ß√£o por solicita√ß√£o, em vez de reiniciar. <br><br><h4>  Compartilhamento de carga </h4><br>  Todos os nossos back-end n√£o s√£o um grande conjunto de m√°quinas que podem lidar com qualquer solicita√ß√£o.  N√≥s os <strong>dividimos em grupos separados</strong> : geral, m√≥vel, API, v√≠deo, teste ... O problema em um grupo separado de m√°quinas n√£o afetar√° todos os outros.  Em caso de problemas com o v√≠deo, o usu√°rio que est√° ouvindo m√∫sica nem conhece os problemas.  Qual backend para o qual enviar a solicita√ß√£o √© resolvido pelo nginx na frente da configura√ß√£o. <br><br><h4>  Coleta e reequil√≠brio de m√©tricas </h4><br>  Para entender quantos carros voc√™ precisa em cada grupo, <strong>n√£o confiamos no QPS</strong> .  Os back-end s√£o diferentes, eles t√™m solicita√ß√µes diferentes, cada solicita√ß√£o tem uma complexidade de c√°lculo QPS diferente.  Portanto, usamos o <strong>conceito de carga no servidor como um todo - na CPU e no perf</strong> . <br><br>  Temos milhares desses servidores.  O grupo kPHP est√° sendo executado em cada servidor f√≠sico para utilizar todos os kernels (porque o kPHP √© de thread √∫nico). <br><br><h3>  Servidor de conte√∫do </h3><br>  <strong>O CS ou o Content Server √© armazenamento</strong> .  O CS √© um servidor que armazena arquivos e tamb√©m processa arquivos enviados, todos os tipos de tarefas em segundo plano s√≠ncronas que o principal front-end da Web representa para ele. <br><br>  Temos dezenas de milhares de servidores f√≠sicos que armazenam arquivos.  Os usu√°rios adoram fazer upload de arquivos, e n√≥s gostamos de armazenar e compartilh√°-los.  Alguns desses servidores s√£o fechados por servidores pu / pp especiais. <br><br><h3>  pu / pp </h3><br>  Se voc√™ abriu a guia de rede no VK, viu pu / pp. <br><br><img src="https://habrastorage.org/webt/fd/am/xo/fdamxolkfxlplnc5h5flbihru3g.png"><br><br>  O que √© pu / pp?  Se fecharmos um servidor ap√≥s o outro, h√° duas op√ß√µes para carregar e baixar um arquivo em um servidor que foi fechado: <strong>diretamente</strong> por meio de <code>http://cs100500.userapi.com/path</code> ou <strong>por um servidor intermedi√°rio</strong> - <code>http://pu.vk.com/c100500/path</code> . <br><br>  <strong>Pu √© o nome hist√≥rico para upload de fotos e pp √© proxy de fotos</strong> .  Ou seja, um servidor para enviar fotos e outro - para dar.  Agora, n√£o apenas as fotos s√£o carregadas, mas o nome foi preservado. <br><br>  Esses servidores <strong>encerram as sess√µes HTTPS</strong> para remover a carga do processador do armazenamento.  Al√©m disso, como os arquivos do usu√°rio s√£o processados ‚Äã‚Äãnesses servidores, as informa√ß√µes menos confidenciais s√£o armazenadas nessas m√°quinas, melhor.  Por exemplo, chaves de criptografia HTTPS. <br><br>  Como as m√°quinas s√£o fechadas por outras m√°quinas, podemos dar ao luxo de n√£o lhes dar IPs externos ‚Äúbrancos‚Äù e de IPs <strong>‚Äúcinzas‚Äù</strong> .  Assim, economizamos no pool de IP e garantimos a prote√ß√£o das m√°quinas contra o acesso externo - simplesmente n√£o h√° IP para acess√°-lo. <br><br>  <strong>Toler√¢ncia a falhas atrav√©s de IP compartilhado</strong> .  Em termos de toler√¢ncia a falhas, o esquema funciona da mesma maneira - v√°rios servidores f√≠sicos t√™m um IP f√≠sico comum, e o peda√ßo de ferro √† sua frente escolhe para onde enviar a solicita√ß√£o.  Mais tarde vou falar sobre outras op√ß√µes. <br><br>  O ponto controverso √© que, nesse caso, o <strong>cliente mant√©m menos conex√µes</strong> .  Se houver o mesmo IP em v√°rias m√°quinas - com o mesmo host: pu.vk.com ou pp.vk.com, o navegador do cliente ter√° um limite no n√∫mero de solicita√ß√µes simult√¢neas para um host.  Mas durante o onipresente HTTP / 2, acredito que esse n√£o seja mais o caso. <br><br>  O menos √≥bvio do esquema √© que voc√™ precisa <strong>bombear todo o tr√°fego</strong> que vai para o armazenamento por outro servidor.  Como bombeamos o tr√°fego pelos carros, ainda n√£o podemos bombear o tr√°fego pesado da mesma maneira, por exemplo, v√≠deo.  Transferimos diretamente - uma conex√£o direta separada para reposit√≥rios individuais especificamente para v√≠deo.  N√≥s transmitimos conte√∫do mais leve atrav√©s de um proxy. <br><br>  N√£o faz muito tempo, temos uma vers√£o aprimorada do proxy.  Agora vou dizer como eles diferem dos comuns e por que isso √© necess√°rio. <br><br><h3>  Sol </h3><br>  Em setembro de 2017, a Oracle, que havia comprado a Sun, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">demitiu um grande n√∫mero de funcion√°rios da Sun.</a>  Podemos dizer que, neste momento, a empresa deixou de existir.  Escolhendo um nome para o novo sistema, nossos administradores decidiram prestar homenagem e respeito a essa empresa e nomearam o novo sistema Sun.  Entre n√≥s, chamamos simplesmente de "luz do sol". <br><br><img src="https://habrastorage.org/webt/d3/6f/0j/d36f0jjqbwlst9mk2-lcncltkq4.png"><br><br>  Pp teve alguns problemas.  <strong>Um IP por grupo √© um cache ineficiente</strong> .  V√°rios servidores f√≠sicos t√™m um endere√ßo IP comum e n√£o h√° como controlar em qual servidor a solicita√ß√£o chegar√°.  Portanto, se diferentes usu√°rios buscarem o mesmo arquivo, se houver um cache nesses servidores, o arquivo ser√° instalado no cache de cada servidor.  Este √© um esquema muito ineficiente, mas nada pode ser feito. <br><br>  Como resultado, <strong>n√£o podemos compartilhar conte√∫do</strong> , porque n√£o podemos selecionar um servidor espec√≠fico para este grupo - eles t√™m um IP comum.  Al√©m disso, por alguns motivos internos, <strong>n√£o tivemos a oportunidade de colocar esses servidores nas regi√µes</strong> .  Eles ficaram apenas em S√£o Petersburgo. <br><br>  Com os s√≥is, mudamos o sistema de sele√ß√£o.  Agora temos o <strong>roteamento anycast</strong> : <strong>roteamento</strong> din√¢mico, anycast, daemon de autoverifica√ß√£o.  Cada servidor tem seu pr√≥prio IP individual, mas ao mesmo tempo uma sub-rede comum.  Tudo √© configurado de tal maneira que, no caso de perda de um servidor, o tr√°fego √© espalhado automaticamente para outros servidores do mesmo grupo.  Agora √© poss√≠vel selecionar um servidor espec√≠fico, <strong>n√£o h√° armazenamento em cache excessivo</strong> e a confiabilidade n√£o √© afetada. <br><br>  <strong>Suporte de peso</strong> .  Agora, podemos dar ao luxo de colocar carros de diferentes capacidades, conforme necess√°rio, e tamb√©m em caso de problemas tempor√°rios, alterar os pesos dos ‚Äús√≥is‚Äù que trabalham para reduzir a carga sobre eles, para que ‚Äúdescansem‚Äù e trabalhem novamente. <br><br>  <strong>Fragmento por ID do conte√∫do</strong> .  O engra√ßado do sharding √© que geralmente compartilhamos o conte√∫do, para que usu√°rios diferentes sigam o mesmo arquivo pelo mesmo "sol" para que eles tenham um cache comum. <br><br>  Lan√ßamos recentemente o aplicativo Clover.  Este √© um teste de transmiss√£o ao vivo on-line, onde o apresentador faz perguntas e os usu√°rios respondem em tempo real, escolhendo as op√ß√µes.  O aplicativo tem um bate-papo onde os usu√°rios podem inundar.  <strong>Mais de 100 mil pessoas</strong> podem se conectar simultaneamente √† transmiss√£o.  Todos eles escrevem mensagens enviadas a todos os participantes, junto com a mensagem e outro avatar.  Se 100 mil pessoas procuram um avatar em um "sol", √†s vezes ele pode rolar sobre uma nuvem. <br><br>  Para suportar explos√µes de solicita√ß√µes do mesmo arquivo, √© para algum tipo de conte√∫do que inclu√≠mos um esquema burro que espalha arquivos por todos os "s√≥is" dispon√≠veis na regi√£o. <br><br><h4>  Sol por dentro </h4><br>  Proxy reverso para nginx, cache em discos r√°pidos RAM ou Optane / NVMe.  Exemplo: <code>http://sun4-2.userapi.com/c100500/path</code> - link para o "sol", que est√° na quarta regi√£o, o segundo grupo de servidores.  Ele fecha o arquivo de caminho, que fica fisicamente no servidor 100500. <br><br><h3>  Cache </h3><br>  Adicionamos mais um n√≥ ao nosso esquema arquitetural - o ambiente de armazenamento em cache. <br><br><img src="https://habrastorage.org/webt/jp/4p/xv/jp4pxvydhstms-z9bpbmpjy0htk.png"><br><br>  Abaixo est√° o layout dos <strong>caches regionais</strong> , existem cerca de 20 deles.  Estes s√£o os locais em que exatamente os caches e "s√≥is" est√£o localizados, os quais podem armazenar em cache o tr√°fego por si mesmos. <br><br><img src="https://habrastorage.org/webt/yh/9i/qk/yh9iqkv3dyqd3uox9wgd2cmgqba.png"><br><br>  Isso √© cache de conte√∫do multim√≠dia, os dados do usu√°rio n√£o s√£o armazenados aqui - apenas m√∫sica, v√≠deo, fotos. <br><br>  Para determinar a regi√£o do usu√°rio, <strong>coletamos os prefixos de rede BGP anunciados nas regi√µes</strong> .  No caso de fallback, ainda temos a an√°lise da base geogr√°fica, se n√£o conseguirmos encontrar o IP por prefixos.  <strong>Com base no IP do usu√°rio, determinamos a regi√£o</strong> .  No c√≥digo, podemos observar uma ou mais regi√µes do usu√°rio - aqueles pontos nos quais ele √© geograficamente mais pr√≥ximo. <br><br><h4>  Como isso funciona? </h4><br>  <strong>Consideramos a popularidade dos arquivos por regi√£o</strong> .  H√° um n√∫mero de cache regional em que o usu√°rio est√° localizado e um identificador de arquivo - pegamos esse par e incrementamos a classifica√ß√£o de cada download. <br><br>  Ao mesmo tempo, dem√¥nios - servi√ßos nas regi√µes - ocasionalmente acessam a API e dizem: "Eu tenho esse e esse cache, me d√™ uma lista dos arquivos mais populares da minha regi√£o que ainda n√£o tenho".  A API fornece v√°rios arquivos classificados por classifica√ß√£o, o daemon os distribui, os leva para as regi√µes e os arquivos a partir da√≠.  Essa √© uma diferen√ßa fundamental entre pu / pp e Sun dos caches: eles fornecem o arquivo imediatamente, mesmo que o arquivo n√£o exista no cache, e o cache primeiro bombeia o arquivo para si pr√≥prio e, em seguida, come√ßa a distribu√≠-lo. <br><br>  Ao mesmo tempo, <strong>aproximamos o conte√∫do dos usu√°rios</strong> e diminu√≠mos a carga da rede.  Por exemplo, somente do cache de Moscou distribu√≠mos mais de 1 Tbit / s durante o hor√°rio de pico. <br><br>  Mas h√° problemas - os <strong>servidores de cache n√£o s√£o de borracha</strong> .  Para conte√∫do super popular, √†s vezes n√£o h√° rede suficiente em um servidor separado.  Temos servidores de cache de 40 a 50 Gbit / s, mas h√° conte√∫do que obstrui completamente esse canal.  Estamos nos esfor√ßando para realizar o armazenamento de mais de uma c√≥pia de arquivos populares na regi√£o.  Espero que possamos perceber isso at√© o final do ano. <br><br>  Examinamos a arquitetura geral. <br><br><ul><li>  Servidores frontais que aceitam solicita√ß√µes. </li><li>  Back-end que manipulam solicita√ß√µes. </li><li>  Cofres fechados por dois tipos de proxies. </li><li>  Caches regionais. </li></ul><br>  O que est√° faltando nesse esquema?  Obviamente, os bancos de dados nos quais armazenamos dados. <br><br><h2>  Bancos de dados ou mecanismos </h2><br>  N√≥s os chamamos n√£o de bancos de dados, mas de mecanismos de motores, porque, no sentido geralmente aceito, praticamente n√£o temos bancos de dados. <br><br><img src="https://habrastorage.org/webt/n6/zm/lj/n6zmlj5pwxsnqoqp0xgfhxza_ic.png"><br><br>  <strong>Esta √© uma medida necess√°ria</strong> .  Isso aconteceu porque, em 2008-2009, quando o VK teve um crescimento explosivo em popularidade, o projeto funcionou totalmente no MySQL e Memcache, e houve problemas.  O MySQL gostava de cair e arruinar arquivos, ap√≥s o que n√£o aumentou, e o Memcache gradualmente diminuiu o desempenho e teve que ser reiniciado. <br><br>  Acontece que no projeto que estava ganhando popularidade havia um armazenamento persistente que corrompeu os dados e um cache que diminuiu a velocidade.  Em tais condi√ß√µes, √© dif√≠cil desenvolver um projeto em crescimento.  Decidiu-se tentar reescrever as coisas cr√≠ticas nas quais o projeto repousava em suas pr√≥prias motos. <br><br>  <strong>A solu√ß√£o foi bem sucedida</strong> .  A capacidade de fazer isso era, como era uma necessidade urgente, porque outros m√©todos de dimensionamento n√£o existiam naquele momento.  N√£o havia um monte de bases, o NoSQL ainda n√£o existia, havia apenas MySQL, Memcache, PostrgreSQL - e isso √© tudo. <br><br>  <strong>Opera√ß√£o universal</strong> .  O desenvolvimento foi liderado por nossa equipe de desenvolvedores C, e tudo foi feito da mesma maneira.  Independentemente do mecanismo, em todos os lugares havia aproximadamente o mesmo formato dos arquivos gravados no disco, os mesmos par√¢metros de inicializa√ß√£o, os sinais eram processados ‚Äã‚Äãda mesma forma e se comportavam da mesma maneira em caso de situa√ß√µes e problemas de borda.  Com o crescimento dos mecanismos, √© conveniente para os administradores operar o sistema - n√£o h√° zool√≥gico que precise ser mantido e aprender a operar novamente cada nova base de terceiros, o que possibilitou aumentar seu n√∫mero de maneira r√°pida e conveniente. <br><br><h3>  Tipos de motores </h3><br>  A equipe escreveu alguns mecanismos.  Aqui est√£o apenas alguns deles: amigo, dicas, imagem, ipdb, cartas, listas, logs, memcached, meowdb, not√≠cias, nostradamus, foto, listas de reprodu√ß√£o, pmemcached, sandbox, pesquisa, armazenamento, curtidas, tarefas, ... <br><br>  Para cada tarefa que requer uma estrutura de dados espec√≠fica ou processa solicita√ß√µes at√≠picas, a equipe C escreve um novo mecanismo.  Porque n√£o <br><br>  Temos um mecanismo <strong>memcached</strong> separado, que √© semelhante ao habitual, mas com muitos p√£es e que n√£o diminui a velocidade.  N√£o ClickHouse, mas tamb√©m funciona.  H√° <strong>pmemcached</strong> separadamente - √© um <strong>memcached persistente</strong> que pode armazenar dados tamb√©m no disco e mais do que entra na RAM para n√£o perder dados ao reiniciar.  Existem v√°rios mecanismos para tarefas individuais: filas, listas, conjuntos - tudo o que √© exigido pelo nosso projeto. <br><br><h3>  Clusters </h3><br>  Do ponto de vista do c√≥digo, n√£o h√° necessidade de imaginar mecanismos ou bancos de dados como certos processos, entidades ou inst√¢ncias.  O c√≥digo funciona especificamente com clusters, com grupos de mecanismos - <strong>um tipo por cluster</strong> .  Digamos que exista um cluster armazenado em cache - √© apenas um grupo de m√°quinas. <br><br><blockquote>  O c√≥digo n√£o precisa saber a localiza√ß√£o f√≠sica, tamanho e n√∫mero de servidores.  Ele vai para o cluster por algum identificador. </blockquote><br>  Para que isso funcione, voc√™ precisa adicionar outra entidade, localizada entre o c√≥digo e os mecanismos - <strong>proxy</strong> . <br><br><h3>  Proxy RPC </h3><br>  Proxy - um <strong>barramento de conex√£o</strong> , que roda quase todo o site.  Ao mesmo tempo, <strong>n√£o temos descoberta de servi√ßo</strong> - em vez disso, h√° uma configura√ß√£o desse proxy, que sabe a localiza√ß√£o de todos os clusters e todos os shards deste cluster.  Isso √© feito pelos administradores. <br><br>  Os programadores geralmente n√£o se importam com quanto, onde e quanto custa - eles apenas acessam o cluster.  Isso nos permite muito.  Ap√≥s o recebimento da solicita√ß√£o, o proxy a redireciona, sabendo onde - determina isso. <br><br><img src="https://habrastorage.org/webt/7k/pf/ia/7kpfiagxzy2a4mrosc_f4otqnw8.png"><br><br>  Ao mesmo tempo, o proxy √© um ponto de prote√ß√£o contra falhas no servi√ßo.  Se algum mecanismo diminuir a velocidade ou travar, o proxy entender√° isso e responder√° adequadamente ao lado do cliente.  Isso permite remover o tempo limite - o c√≥digo n√£o espera o mecanismo responder, mas entende que ele n√£o funciona e voc√™ precisa se comportar de maneira diferente.  O c√≥digo deve estar preparado para o fato de que os bancos de dados nem sempre funcionam. <br><br><h4>  Implementa√ß√µes espec√≠ficas </h4><br>  √Äs vezes, ainda queremos ter algum tipo de solu√ß√£o personalizada como mecanismo.  Ao mesmo tempo, foi decidido n√£o usar nosso rpc-proxy pronto, criado especificamente para nossos mecanismos, mas criar um proxy separado para a tarefa. <br><br>  Para o MySQL, que ainda temos em alguns lugares, usamos db-proxy e para ClickHouse - <strong>Kittenhouse</strong> . <br><br>  Isso funciona globalmente assim.  H√° um servidor, o kPHP, Go, Python em execu√ß√£o - em geral, qualquer c√≥digo que possa seguir o nosso protocolo RPC.  O c√≥digo vai localmente para o proxy RPC - em cada servidor onde h√° c√≥digo, seu pr√≥prio proxy local √© iniciado.  Mediante solicita√ß√£o, o proxy entende para onde ir. <br><br><img src="https://habrastorage.org/webt/f-/dx/ro/f-dxrox3o97ckejzygz8mgf4tcs.png"><br><br>  Se um mecanismo deseja ir para outro, mesmo que seja um vizinho, ele passa por um proxy, porque o vizinho pode estar em um data center diferente.  O mecanismo n√£o deve estar vinculado a saber a localiza√ß√£o de algo que n√£o seja ele pr√≥prio - n√≥s temos esta solu√ß√£o padr√£o.  Mas √© claro que h√° exce√ß√µes :) <br><br>  Um exemplo de um esquema TL segundo o qual todos os mecanismos funcionam. <br><br><pre> <code class="plaintext hljs">memcache.not_found = memcache.Value; memcache.strvalue value:string flags:int = memcache.Value; memcache.addOrIncr key:string flags:int delay:int value:long = memcache.Value; tasks.task fields_mask:# flags:int tag:%(Vector int) data:string id:fields_mask.0?long retries:fields_mask.1?int scheduled_time:fields_mask.2?int deadline:fields_mask.3?int = tasks.Task; tasks.addTask type_name:string queue_id:%(Vector int) task:%tasks.Task = Long;</code> </pre> <br>  Este √© um protocolo bin√°rio, cujo an√°logo mais pr√≥ximo √© o <strong>protobuf.</strong>  O esquema descreve antecipadamente campos opcionais, tipos complexos - extens√µes de escalares internos e consultas.  Tudo funciona de acordo com este protocolo. <br><br><h4>  RPC sobre TL sobre TCP / UDP ... UDP? </h4><br>  Temos um protocolo RPC para consultar o mecanismo, que √© executado sobre o esquema TL.  Tudo isso funciona em cima da conex√£o TCP / UDP.  TCP - est√° claro por que muitas vezes nos perguntam sobre o UDP. <br><br>  O UDP ajuda a <strong>evitar o problema de um grande n√∫mero de conex√µes entre servidores</strong> .  Se houver um proxy RPC em cada servidor e, em geral, ele for para qualquer mecanismo, voc√™ receber√° dezenas de milhares de conex√µes TCP com o servidor.  H√° uma carga, mas √© in√∫til.  No caso do UDP, isso n√£o √© um problema. <br><br>  <strong>Nenhum aperto de m√£o TCP redundante</strong> .  Esse √© um problema t√≠pico: quando um novo mecanismo ou um novo servidor √© ativado, muitas conex√µes TCP s√£o estabelecidas ao mesmo tempo.  Para pequenas solicita√ß√µes leves, por exemplo, carga √∫til UDP, toda a comunica√ß√£o entre o c√≥digo e o mecanismo √© de <strong>dois pacotes UDP:</strong> um voa em uma dire√ß√£o e o outro voa na outra.  Uma viagem de ida e volta - e o c√≥digo recebeu uma resposta do mecanismo sem um aperto de m√£o. <br><br>  Sim, tudo funciona apenas <strong>com uma porcentagem muito pequena de perda de pacotes</strong> .  O protocolo tem suporte para retransmit√™ncias, tempos limite, mas se perdermos muito, obtemos praticamente o TCP, o que n√£o √© lucrativo.  Nos oceanos, n√£o dirija UDP. <br><br>  Temos milhares desses servidores e o mesmo esquema existe: um pacote de mecanismos √© colocado em cada servidor f√≠sico.  Basicamente, eles s√£o de rosca √∫nica para trabalhar o mais r√°pido poss√≠vel sem bloquear e s√£o fragmentados como solu√ß√µes de rosca √∫nica.  Ao mesmo tempo, n√£o temos nada mais confi√°vel que esses mecanismos e muita aten√ß√£o √© dada ao armazenamento persistente de dados. <br><br><h3>  Armazenamento de dados persistente </h3><br>  <strong>Os motores escrevem binlogs</strong> .  Um binlog √© um arquivo no final do qual um evento √© adicionado para alterar um estado ou dados.  Em diferentes solu√ß√µes, isso √© chamado de maneira diferente: log bin√°rio, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">WAL</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AOF</a> , mas o princ√≠pio √© um. <br><br>  Para que o mecanismo n√£o releia todo o binlog durante uma reinicializa√ß√£o durante muitos anos, os mecanismos gravam <strong>instant√¢neos - o status atual</strong> .  Se necess√°rio, eles primeiro l√™em e depois l√™em no binlog.  Todos os binlogs s√£o gravados no mesmo formato bin√°rio - de acordo com o esquema TL, para que os administradores possam administr√°-los igualmente com suas ferramentas.  N√£o h√° necessidade de capturas instant√¢neas.  Existe um cabe√ßalho geral que indica cujo instant√¢neo √© o int, a m√°gica do mecanismo e qual corpo n√£o √© importante para ningu√©m.  Esse √© o problema do mecanismo que registrou o instant√¢neo. <br><br>  Descreverei brevemente o princ√≠pio do trabalho.  H√° um servidor no qual o mecanismo est√° sendo executado.  Ele abre um novo binlog vazio para grava√ß√£o, escreve um evento de mudan√ßa nele. <br><br><img src="https://habrastorage.org/webt/dd/w9/9p/ddw99p7g6upg9hci9ou6aln6d_c.png"><br><br>  Em algum momento, ele decide tirar uma foto instant√¢nea ou recebe um sinal.  O servidor cria um novo arquivo, grava completamente seu estado nele, anexa o tamanho atual do log de bin - compensado no final do arquivo e continua a escrever mais.  Um novo binlog n√£o √© criado. <br><br><img src="https://habrastorage.org/webt/ec/fq/yt/ecfqytibh2tsm5ncd8mfli-b1ta.png"><br><br>  Em algum momento, quando o mecanismo reiniciar, haver√° um binlog e um instant√¢neo no disco.  O mecanismo l√™ em instant√¢neo completo, eleva seu estado em um determinado ponto. <br><br><img src="https://habrastorage.org/webt/bg/ph/-u/bgph-uu68nqedhby4a2kf3r9c5u.png"><br><br>  Subtrai a posi√ß√£o que estava no momento em que o instant√¢neo foi criado e o tamanho do binlog. <br><br><img src="https://habrastorage.org/webt/ar/gs/lq/argslqv8ewosmtic8-zaobq4g5o.png"><br><br>  L√™ o final do binlog para obter o estado atual e continua a gravar outros eventos.  Este √© um esquema simples, todos os nossos mecanismos trabalham nele. <br><br><h4>  Replica√ß√£o de dados </h4><br>  Como resultado, a replica√ß√£o de dados √© <strong>baseada em instru√ß√µes</strong> - n√£o estamos escrevendo nenhuma altera√ß√£o de p√°gina no binlog, mas <strong>solicitando altera√ß√µes</strong> .  Muito parecido com o que vem pela rede, apenas um pouco mudou. <br><br>  O mesmo esquema √© usado n√£o apenas para replica√ß√£o, mas tamb√©m <strong>para criar backups</strong> .  Temos um mecanismo - um mestre de escrita que escreve em um binlog.  Em qualquer outro local em que os administradores configurem, a c√≥pia desse binlog aumenta e √© tudo - temos um backup. <br><br><img src="https://habrastorage.org/webt/og/al/sz/ogalszm0wfe3f_064sbjnpo4p9c.png"><br><br>  Se voc√™ precisar de uma <strong>r√©plica de leitura</strong> para reduzir a carga de leitura na CPU, o mecanismo de leitura apenas aumentar√°, que l√™ o final do binlog e executa esses comandos localmente. <br><br>  O atraso aqui √© muito pequeno e h√° uma oportunidade de descobrir quanto a r√©plica est√° por tr√°s do mestre. <br><br><h3>  Compartilhamento de dados no proxy RPC </h3><br>  Como o sharding funciona?  Como o proxy entende para qual shard de cluster enviar?  O c√≥digo n√£o diz: "Enviar para 15 shard!"  - n√£o, ele faz um proxy. <br><br>  <strong>O esquema mais simples √© firstint</strong> , o primeiro n√∫mero na solicita√ß√£o. <br><br> <code>get(photo100_500) =&gt; 100 % N.</code> <br> <br>  Este √© um exemplo para um protocolo de texto em memcached simples, mas √© claro que as solicita√ß√µes s√£o complexas e estruturadas.  O exemplo pega o primeiro n√∫mero na consulta e o restante da divis√£o pelo tamanho do cluster. <br><br>  Isso √© √∫til quando queremos ter a localidade dos dados de uma entidade.  Digamos que 100 seja um ID de usu√°rio ou grupo e queremos que todos os dados de uma entidade estejam no mesmo fragmento para consultas complexas. <br><br>  Se n√£o nos importamos como as solicita√ß√µes s√£o espalhadas pelo cluster, h√° outra op√ß√£o: fazer o <strong>hash de todo o fragmento</strong> . <br><br> <code>hash(photo100_500) =&gt; 3539886280 % N</code> <br> <br>  Tamb√©m obtemos o hash, o restante da divis√£o e o n√∫mero do shard. <br><br>  Ambas as op√ß√µes funcionam apenas se estivermos preparados para o fato de que, quando aumentarmos o tamanho do cluster, o dividiremos ou o aumentaremos v√°rias vezes.  Por exemplo, tivemos 16 shards, estamos perdendo, queremos mais - voc√™ pode obter 32 com seguran√ßa sem tempo de inatividade.  Se queremos construir v√°rias vezes, haver√° um tempo de inatividade, porque n√£o ser√° poss√≠vel esmagar tudo com cuidado, sem perdas.  Essas op√ß√µes s√£o √∫teis, mas nem sempre. <br><br>  Se precisarmos adicionar ou remover um n√∫mero arbitr√°rio de servidores, <strong>um hash consistente no anel √† la Ketama ser√° usado</strong> .  Mas, ao mesmo tempo, perdemos completamente a localidade dos dados, precisamos fazer uma solicita√ß√£o de mesclagem para o cluster para que cada parte retorne sua pequena resposta e j√° combine as respostas ao proxy. <br><br>  Existem consultas superespec√≠ficas.   : RPC-proxy  , ,       .     , ,     ,      .    proxy. <br><br><img src="https://habrastorage.org/webt/jx/6t/f9/jx6tf9jlkkmva1qfifzmwrx58wc.png"><br><br><h2>  </h2><br>     .     ‚Äî <strong>   memcache</strong> . <br><br> <code>ring-buffer: prefix.idx = line</code> <br> <br>    ‚Äî  , ,      ‚Äî  .     0     1.   memcache ‚Äî       .        . <br><br>    ,   <strong>Multi Get</strong>  ,   ,         .  ,   -      ,   ,         ,      . <br><br>         <strong>logs-engine</strong> .      ,       .       600   . <br><br>   ,  ,    6‚Äì7 .    ,    , ,    ClickHouse   . <br><br><h3>    ClickHouse </h3><br>   ,      . <br><br><img src="https://habrastorage.org/webt/jm/-j/s0/jm-js04tjh8lb8pii1_dzl_sfa4.png"><br><br>  ,   RPC    RPC-proxy,   ,    .       ClickHouse,        : <br><br><ul><li>  -   ClickHouse; </li><li>  RPC-proxy,      ClickHouse,  - ,  ,   RPC. </li></ul><br>    ‚Äî          ClickHouse. <br><br>     ClickHouse,   <strong>KittenHouse</strong> .      KittenHouse  ClickHouse ‚Äî   .   ,  HTTP-     .   ,    ClickHouse <strong>  reverse proxy</strong> ,   ,     .         . <br><br><img src="https://habrastorage.org/webt/zj/fy/5y/zjfy5yuay9-6wqe3nrgjkeznvny.png"><br><br>      RPC-   , ,  nginx.   KittenHouse      UDP. <br><br><img src="https://habrastorage.org/webt/hq/wl/v_/hqwlv_vnujb-maxakxksbmrf6xo.png"><br><br>         ,    UDP-      .       RPC     ,      UDP.      . <br><br><h2>  Monitoramento </h2><br>     : ,        ,     .     : <strong>  </strong> . <br><br><h3>   </h3><br>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Netdata</a> ,        <strong>Graphite Carbon</strong> .      ClickHouse,   Whisper, .       ClickHouse,   <strong>Grafana</strong>  ,   .  ,   Netdata  Grafana  . <br><br><h3>   </h3><br>      . ,    ,    Counts, UniqueCounts   ,   - . <br><br><pre> <code class="plaintext hljs">statlogsCountEvent ( 'stat_name', $key1, $key2, ‚Ä¶) statlogsUniqueCount ( 'stat_name', $uid, $key1, $key2, ‚Ä¶) statlogsValuetEvent ( 'stat_name', $value, $key1, $key2, ‚Ä¶) $stats = statlogsStatData($params)</code> </pre><br>      ,    ,     ‚Äî  ,  Wathdogs. <br><br>    <strong> ,</strong>    600   1   .       <strong>   </strong> ,     .     ‚Äî  ,     . ,      . <br><br>    ,     <strong>  memcache</strong> ,    .         <strong>stats-daemon</strong>   .         <strong>logs-collectors</strong> ,       ,      . <br><br><img src="https://habrastorage.org/webt/ih/ab/oy/ihaboy4luh5hriorej9seodbx6u.png"><br><br>        logs-collectors. <br><br><img src="https://habrastorage.org/webt/fq/ta/bj/fqtabjgq556wqfdz5_kfq3mj94c.png"><br><br>          stas-daemom ‚Äî   ,      collector.  ,    -        memcache stats-daemon,   ,    . <br><br>  logs-collectors    <strong>meowDB</strong> ‚Äî   ,      . <br><br><img src="https://habrastorage.org/webt/v_/gb/_y/v_gb_ya-9ywkra7xdh5h_qtqsc4.png"><br><br>      ¬´-SQL¬ª  . <br><br><img src="https://habrastorage.org/webt/1q/gw/wp/1qgwwpyj3ewcwuonshvty_zcfhc.png"><br><br><h3>  </h3><br>  2018     ,          -,      ClickHouse.      ClickHouse ‚Äî    ? <br><br><img src="https://habrastorage.org/webt/wg/mz/kl/wgmzklw41x7ilj0-5hbr_kfdif8.png"><br><br>    ,     KittenHouse. <br><br><img src="https://habrastorage.org/webt/kq/s7/uj/kqs7ujzbhnqzt5f8djepldmwxia.png"><br><br>   <strong>     ¬´*House¬ª</strong> ,        ,       UDP.   *House    inserts,  ,   KittenHouse.        ClickHouse,     . <br><br><img src="https://habrastorage.org/webt/ff/k3/th/ffk3thypln9exuhyuhr-nuj9hr4.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><img src="https://habrastorage.org/webt/r4/g3/e9/r4g3e9yakpzbx5gscmgyl6keqsa.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><ul><li>     ,     StatsHouse. </li><li> StatsHouse   KittenHouse UDP-,    SQL-inserts, . </li><li> KittenHouse    ClickHouse. </li><li>     ,      StatsHouse ‚Äî   ClickHouse  SQL. </li></ul><br>    <strong></strong> ,   ,  .    , , ,    .     . <br><br>  <strong>  </strong> .   ,    stats-daemons  logs-collectors,  ClickHouse   ,  ,     . <strong>  ,       </strong> . <br><br><h2>  </h2><br>     PHP.    <strong>git</strong> :  <strong>GitLab</strong>  <strong>TeamCity</strong>  .     -,       ,   ‚Äî  . <br><br>        ,     diff  ‚Äî : , , .     binlog   copyfast,          .     ,  <strong>gossip replication</strong> ,       ,  ‚Äî  ,   .            .      ,       <strong>  </strong> .       . <br><br>     kPHP         <strong>git</strong>   .    <strong> HTTP-</strong> ,      diff ‚Äî     .     ‚Äî    <strong>binlog copyfast</strong> .     ,      .  <strong>  </strong> .  copyfast' ,   binlog   ,     gossip replication     ,    -,      .   <strong>graceful </strong>   . <br><br>   ,     ,   : <br><br><ul><li> git master branch; </li><li>   <strong>.deb</strong> ; </li><li>    binlog copyfast; </li><li>   ; </li><li>     .dep; </li><li> <strong>dpkg -i</strong> ; </li><li> graceful    . </li></ul><br>   ,        <strong>.deb</strong> ,     <strong>dpkg -i</strong>   .    kPHP  ,   ‚Äî dpkg?  .  ‚Äî  . <br><br> <b> :</b> <br><br><ul><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´  Vkontakte. ?¬ª</a>    copyfast  gossip. </li><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´ VK    CLickHouse    ¬ª</a> . </li><li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´     ¬ª</a> ,     ,   . </li></ul><br><blockquote>     ,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PHP Russia</a>  17          PHP-. ,     ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> (     PHP!) ‚Äî ,      PHP,   . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt449254/">https://habr.com/ru/post/pt449254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt449236/index.html">Ok Google: como fa√ßo para passar pelo captcha?</a></li>
<li><a href="../pt449240/index.html">A hist√≥ria de um jovem servi√ßo Daida (arte de assinatura)</a></li>
<li><a href="../pt449246/index.html">AX200 - Intel Wi-Fi 6</a></li>
<li><a href="../pt449248/index.html">IDE moderno. Definitivamente D, at√© certo ponto E, e certamente n√£o</a></li>
<li><a href="../pt449252/index.html">Projetos zumbis - mesclar dados do usu√°rio mesmo ap√≥s sua morte</a></li>
<li><a href="../pt449256/index.html">Eu li 80 curr√≠culos, tenho perguntas</a></li>
<li><a href="../pt449260/index.html">O que √© aprendizado autom√°tico de m√°quina (AutoML)</a></li>
<li><a href="../pt449262/index.html">IRM mais recente - atualiza√ß√£o do Siebel para IP17 +</a></li>
<li><a href="../pt449264/index.html">Criando um sistema de relat√≥rios para 1C: ERP baseado em OLAP e Excel</a></li>
<li><a href="../pt449266/index.html">3 relat√≥rios com RusCrypto: confer√™ncias com experi√™ncia</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>