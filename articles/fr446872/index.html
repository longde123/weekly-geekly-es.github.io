<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎅 👩🏽‍⚖️ 💔 Explorer OpenCV sur StereoPi: carte de profondeur à partir de la vidéo 👋 🤶🏽 👆🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aujourd'hui, nous voulons partager une série d'exemples sur Python pour les apprenants OpenCV sur le Raspberry Pi, à savoir la carte StereoPi à double...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Explorer OpenCV sur StereoPi: carte de profondeur à partir de la vidéo</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446872/"><img src="https://habrastorage.org/webt/hb/xt/po/hbxtpox_r6mswlkshq4w3cpovr4.gif"><br><br>  Aujourd'hui, nous voulons partager une série d'exemples sur Python pour les apprenants OpenCV sur le Raspberry Pi, à savoir la carte StereoPi à double chambre.  Le code fini (plus l'image Raspbian) vous aidera à passer par toutes les étapes, en commençant par la capture d'une image et en terminant par l'obtention d'une carte de profondeur de la vidéo capturée. <br><a name="habracut"></a><br><h3>  Introduction </h3><br>  Je dois souligner tout de suite que ces exemples sont pour une immersion confortable dans le sujet, et non pour une solution de production.  Si vous êtes un utilisateur avancé d'OpenCV et que vous avez eu affaire à des framboises, alors vous savez que pour un travail à part entière, il est conseillé de coder une bouchée, et même d'utiliser un GPU framboise.  À la fin de l'article, j'aborderai plus en détail les «goulots d'étranglement» de la solution python et les performances globales. <br><br><h3>  Avec quoi travaillons-nous </h3><br>  Nous avons une configuration telle que le fer: <br><br><img src="https://habrastorage.org/webt/or/pd/9u/orpd9ufeuctr0lbmsk0kfogroao.jpeg"><br><br>  Carte StereoPi à bord du Raspberry Pi Compute Module 3+.  Les deux caméras les plus simples sont connectées pour la version Raspberry Pi V1 (sur le capteur ov5647). <br><br>  Ce qui est installé: <br><br><ul><li>  Raspbian Stretch (noyau 4.14.98-v7 +) </li><li>  Python 3.5.3 </li><li>  OpenCV 3.4.4 (précompilé, «pip» de Python Wheels) </li><li>  Picamera 1.13 </li><li>  StereoVision lib 1.0.3 (https://github.com/erget/StereoVision) </li></ul><br>  Le processus d'installation de tous les logiciels dépasse le cadre de cet article, et nous vous suggérons simplement de télécharger l'image Raspbian finie (liens vers le github à la fin de l'article). <br><br><h3>  Première étape: capturer une image </h3><br>  Pour ce faire, utilisez le script 1_test.py <br><br>  Ouvrez la console, allez du dossier d'accueil au dossier avec des exemples: <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> stereopi-tutorial</code> </pre> <br>  Exécutez le script: <br><br><pre> <code class="bash hljs">python 1_test.py</code> </pre> <br>  Après le démarrage, un aperçu de notre image stéréo s'affiche à l'écran.  Le processus peut être interrompu en appuyant sur le bouton Q. Cela enregistrera la dernière image capturée, qui sera utilisée dans l'un des scripts suivants pour configurer la carte de profondeur. <br><br>  Ce script vous permet de vous assurer que tout le matériel fonctionne correctement, ainsi que d'obtenir la première image pour une utilisation future. <br><br>  Voici à quoi ressemble le premier script: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wllLrNUw3SE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Deuxième étape: collecter des images pour l'étalonnage </h3><br>  Si nous parlons d'un cheval sphérique dans le vide, pour obtenir une carte de profondeur de bonne qualité, nous devons avoir deux caméras absolument identiques, dont les axes vertical et optique sont parfaitement parallèles et les axes horizontaux coïncident.  Mais dans le monde réel, toutes les caméras sont légèrement différentes et il n'est pas possible de les organiser parfaitement.  Par conséquent, une astuce d'étalonnage logiciel a été inventée.  À l'aide de deux caméras du monde réel, un grand nombre de photos d'un objet précédemment connu sont prises (nous avons une photo avec un échiquier), puis un algorithme spécial calcule toutes les «imperfections» et essaie de corriger les photos afin qu'elles soient proches de l'idéal. <br><br>  Ce script fait la première étape du travail, à savoir qu'il aide à faire une série de photos pour l'étalonnage. <br><br>  Avant chaque photo, le script démarre un compte à rebours de 5 secondes.  Cette fois, en règle générale, il suffit de déplacer la carte dans une nouvelle position, pour s'assurer que sur les deux caméras, elle ne rampe pas sur les bords et ne fixe pas sa position (afin qu'il n'y ait pas de flou sur la photo).  Par défaut, la taille de la série est définie sur 30 photos. <br><br>  Lancement: <br><br><pre> <code class="bash hljs">python 2_chess_cycle.py</code> </pre> <br>  Processus: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1XCAlU3k-xs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  En conséquence, nous avons une série de photos dans le dossier / scènes. <br><br><h3>  Nous coupons les images en paires </h3><br>  Le troisième script 3_pairs_cut.py coupe les photos prises en images «gauche» et «droite» et les enregistre dans le dossier / pairs.  En fait, nous pourrions exclure ce script et faire la découpe à la volée, mais il est très utile dans d'autres expériences.  Par exemple, vous pouvez enregistrer des tranches de différentes séries, utiliser vos scripts pour travailler avec ces paires, ou même supprimer les photos prises sur d'autres caméras stéréo par paires. <br><br>  De plus, avant de couper chaque image, le script affiche son image, ce qui vous permet souvent de voir les photos ayant échoué avant la prochaine étape de calibrage et de simplement les supprimer. <br><br>  Exécutez le script: <br><br><pre> <code class="bash hljs">python 3_pairs_cut.py</code> </pre> <br>  Petite vidéo: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/95DWmPECbDc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Dans l'image finale, il y a un ensemble de photographies et de paires de coupes que nous avons utilisées pour nos expériences. <br><br><h3>  Calibration </h3><br>  Le script 4_calibration.py dessine toutes les paires avec les échiquiers et calcule les corrections nécessaires pour corriger les images.  Dans le scénario, un rejet automatique des photographies sur lesquelles aucun échiquier n'a été trouvé a été effectué, de sorte qu'en cas de photographies infructueuses, le travail ne s'arrête pas.  Une fois les 30 paires d'images téléchargées, le calcul démarre.  Cela nous prend environ une minute et demie.  Une fois terminé, le script prend l'une des paires stéréo et, sur la base des paramètres d'étalonnage calculés, les «corrige», affichant une image rectifiée à l'écran.  À ce stade, vous pouvez évaluer la qualité de l'étalonnage. <br><br>  Exécuté par la commande: <br><br><pre> <code class="bash hljs">python 4_calibration.py</code> </pre> <br>  Script d'étalonnage en cours: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/vtPhu23tKGo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Configuration de la carte de profondeur </h3><br>  Le script 5_dm_tune.py charge l'image prise par le premier script et les résultats de l'étalonnage.  Ensuite, une interface s'affiche qui vous permet de modifier les paramètres de la carte de profondeur et de voir ce qui change.  Astuce: avant de régler les paramètres, prenez un cadre dans lequel vous aurez simultanément des objets à différentes distances: proche (30-40 centimètres), à une distance moyenne (mètre ou deux) et à distance.  Cela vous permettra de choisir les paramètres dans lesquels les objets proches seront rouges et les objets distants seront bleu foncé. <br><br>  L'image contient un fichier avec nos paramètres de carte de profondeur.  Vous pouvez charger nos paramètres dans un script en cliquant simplement sur le bouton «Charger les paramètres» <br><br>  Nous lançons: <br><br><pre> <code class="bash hljs">python 5_dm_tune.py</code> </pre> <br>  Voici à quoi ressemble le processus de configuration: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Z4j3NrMyeGE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Carte de profondeur en temps réel </h3><br>  Le dernier script 6_dm_video.py construit une carte de profondeur à partir de la vidéo en utilisant les résultats des scripts précédents (calibration et paramétrage de la carte de profondeur). <br><br>  Lancement: <br><br><pre> <code class="bash hljs">python 6_dm_video.py</code> </pre> <br>  En fait, le résultat: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/f29arVstfZA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Nous espérons que nos scripts vous seront utiles dans vos expériences! <br><br>  Juste au cas où, j'ajouterai que tous les scripts ont un traitement de frappe, et vous pouvez interrompre le travail en appuyant sur le bouton Q. Si vous arrêtez "grossièrement", par exemple, Ctrl + C, le processus d'interaction de Python avec la caméra peut se casser et un redémarrage de la framboise sera nécessaire. <br><br><h3>  Pour avancé </h3><br><ul><li>  Le premier script du processus affiche le temps moyen entre les captures d'image et, à la fin, le FPS moyen.  Il s'agit d'un outil simple et pratique pour sélectionner de tels paramètres d'image dans lesquels le python ne "s'étouffe" toujours pas.  Avec lui, nous avons ramassé 1280x480 à 20 FPS, dans lequel la vidéo est rendue sans délai. </li><li>  Vous remarquerez peut-être que nous capturons une paire stéréo en résolution 1280x480, puis la redimensionnons à 640x240. <br><br>  Une question raisonnable: pourquoi tout cela, et pourquoi ne pas saisir immédiatement la vignette et ne pas charger notre python en le réduisant? <br><br>  Réponse: avec la capture directe à très basse résolution, il y a toujours des problèmes dans le cœur de framboise (l'image se casse).  Par conséquent, nous prenons une résolution plus grande, puis réduisons l'image.  Ici, nous utilisons une petite astuce: l'image n'est pas mise à l'échelle avec python, mais avec l'aide du GPU, donc il n'y a pas de charge sur le noyau du bras. </li><li>  Pourquoi capturer une vidéo au format BGRA, pas BGR? <br>  Nous utilisons des ressources GPU pour réduire la taille de l'image, et le natif du module de redimensionnement est le format BGRA.  Si nous utilisons BGR au lieu de BGRA, nous aurons deux inconvénients.  Le premier est légèrement inférieur au FPS final (dans nos tests - 20%).  Le second est le portage constant dans la console "PiCameraAlfaStripping: utiliser l'alpha-stripping pour convertir au format non alpha;  vous pouvez trouver un format alpha équivalent plus rapidement ».  La recherche sur Google a conduit à la section de documentation de Picamera, qui décrit cette astuce. </li><li>  Où est le PiRGBArray? <br><br>  C'est comme la classe native Picamera pour travailler avec l'appareil photo, mais ici, elle n'est pas utilisée.  Il s'est déjà avéré que dans nos tests, travailler avec un tableau numpy «fait à la main» est beaucoup plus rapide (une fois et demie) que d'utiliser PiRGBArray.  Cela ne signifie pas que PiRGBArray est mauvais, ce sont probablement nos mains tordues. </li><li>  Quel est le pourcentage chargé dans le calcul de la carte de profondeur? <br>  Répondons avec une photo: <br><br><img src="https://habrastorage.org/webt/nn/ez/ef/nnezefyxuiuxx7difz1xctii16w.jpeg"><br><br>  On voit que sur les 4 cœurs du processeur, en fait, un seul est chargé, soit 70%.  Et cela malgré le fait que nous travaillons avec une interface graphique et que nous fournissons des images et des cartes de profondeur à l'utilisateur.  Cela signifie qu'il y a une bonne marge de performance, et un réglage fin d'OpenCV avec OpenMP et d'autres goodies en C, ainsi qu'un mode «combat» sans interface graphique peuvent donner des résultats très intéressants. </li><li>  Quelle est la carte de profondeur FPS maximale obtenue avec ces paramètres? <br><br>  Le maximum atteint par nous était de 17 FPS, lors de la capture de 20 images par seconde de la caméra.  Les paramètres de vitesse les plus «réactifs» dans les paramètres de la carte de profondeur sont MinDisparity et NumOfDisparities.  Ceci est logique, car ils déterminent le nombre "d'étapes" effectuées au sein de l'algorithme par la fenêtre de recherche pour comparer les trames.  Le deuxième plus réactif est preFilterCap, il affecte, en particulier, la «fluidité» de la carte de profondeur. </li><li>  Qu'en est-il de la température du processeur? <br><br>  Sur Compute Module 3+ Lite (une nouvelle série, avec un «cap» de fer sur le processus), il montre à peu près les résultats suivants: <br><br><img src="https://habrastorage.org/webt/ba/p7/kw/bap7kwdbbhd0y2bmvebpqzimqpa.jpeg"></li><li>  Comment utiliser le GPU? <br><br>  Au minimum, il peut être utilisé pour l'istorisation et la rectification d'images en temps réel, car il existe des exemples ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici sur WebGL</a> ), Python <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pi3d</a> , ainsi que le projet Processing ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exemples pour les framboises</a> ). <br><br>  Il y a un autre développement intéressant de Koichi Nakamura, appelé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">py-videocore</a> .  Dans notre correspondance avec lui, il a exprimé l'idée que pour accélérer StereoBM, vous pouvez utiliser ses tris core et OpenCV <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">avec le support Cuda</a> .  En général, pour l'optimisation - un bord intact, comme on dit. </li></ul><br>  Merci de votre attention et voici le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien promis vers la source</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr446872/">https://habr.com/ru/post/fr446872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr446860/index.html">Histoire de 3dfx Voodoo1</a></li>
<li><a href="../fr446862/index.html">À quoi s'attendent les concepteurs à DUMP-2019: Aperçu de la section Conception</a></li>
<li><a href="../fr446864/index.html">Énergie, chaleur et eau</a></li>
<li><a href="../fr446866/index.html">Systèmes d'exploitation: trois pièces faciles. Partie 2: Abstraction: Processus (traduction)</a></li>
<li><a href="../fr446870/index.html">Systèmes de particules: une histoire de Noël</a></li>
<li><a href="../fr446876/index.html">Moscou, 18 avril - QIWI SERVER PARTY 4.0</a></li>
<li><a href="../fr446880/index.html">Graphiques incorrects: notre expérience</a></li>
<li><a href="../fr446882/index.html">Le MIPT a reçu le droit d'accueillir la Coupe du monde de programmation ICPC en 2020 à Moscou</a></li>
<li><a href="../fr446884/index.html">Que lire et regarder de la science-fiction fraîche: Mars, les cyborgs et l'IA rebelle</a></li>
<li><a href="../fr446886/index.html">Les meilleurs experts de l'Expo 3D: Sunny Wong. Plus de 25 millions d'entorses peuvent être évitées</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>