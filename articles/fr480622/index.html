<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧕🏻 🍅 👨‍❤️‍👨 Comment utiliser correctement la capacité de stockage disponible 🍑 📼 👩🏽‍🎤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous utilisons les services cloud depuis longtemps: courrier, stockage, réseaux sociaux, messagerie instantanée. Ils fonctionnent tous à distance - no...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment utiliser correctement la capacité de stockage disponible</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/480622/">  Nous utilisons les services cloud depuis longtemps: courrier, stockage, réseaux sociaux, messagerie instantanée.  Ils fonctionnent tous à distance - nous envoyons des messages et des fichiers, et ils sont stockés et traités sur des serveurs distants.  Le cloud gaming fonctionne également: l'utilisateur se connecte au service, sélectionne le jeu et se lance.  Ceci est pratique pour le joueur, car les jeux démarrent presque instantanément, ne prennent pas de mémoire et n'ont pas besoin d'un ordinateur de jeu puissant. <br><br><img src="https://habrastorage.org/webt/ej/k4/oy/ejk4oyjjh1r3riqgzzd239qu_va.jpeg"><br><br>  Pour un service cloud, tout est différent - il a des problèmes de stockage de données.  Chaque jeu peut peser des dizaines ou des centaines de gigaoctets, par exemple, "The Witcher 3" prend 50 Go, et "Call of Duty: Black Ops III" - 113. En même temps, les joueurs n'utiliseront pas le service avec 2-3 jeux, au moins plusieurs dizaines sont nécessaires .  En plus de stocker des centaines de jeux, le service doit décider de la quantité de stockage à allouer par joueur et évoluer lorsqu'il y en a des milliers. <br><br>  Tout cela devrait-il être stocké sur leurs serveurs: combien en ont-ils besoin, où placer les centres de données, comment «synchroniser» les données entre plusieurs centres de données à la volée?  Acheter des "nuages"?  Utiliser des machines virtuelles?  Est-il possible de stocker 5 fois les données utilisateur avec compression et de les fournir en temps réel?  Comment exclure toute influence des utilisateurs les uns sur les autres lors d'une utilisation cohérente de la même machine virtuelle? <br><br>  Toutes ces tâches ont été résolues avec succès dans Playkey.net - une plate-forme de jeu basée sur le cloud.  <strong>Vladimir Ryabov</strong> ( <a href="https://habr.com/ru/users/graymansama/" class="user_link">Graymansama</a> ) - chef du département d'administration système - parlera en détail de la technologie ZFS pour FreeBSD, qui a aidé à cela, et de sa nouvelle version de ZOL (ZFS sur Linux). <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SssLwMbMrQ4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Un millier de serveurs de l'entreprise sont situés dans des centres de données distants à Moscou, Londres et Francfort.  Il y a plus de 250 jeux dans le service, qui sont joués par 100 000 joueurs par mois. <br><br><img src="https://habrastorage.org/webt/s6/mv/nw/s6mvnwi5z_b5ltjl_vico2x7ro4.jpeg"><br><br>  Le service fonctionne comme ceci: le jeu s'exécute sur les serveurs de l'entreprise, l'utilisateur reçoit un flux de commandes du clavier, de la souris ou de la manette de jeu, et un flux vidéo est envoyé en réponse.  Cela vous permet de jouer à des jeux haut de gamme modernes sur des ordinateurs avec un matériel faible, des ordinateurs portables avec vidéo intégrée ou sur des Mac pour lesquels ces jeux ne sont pas du tout commercialisés. <br><br><h2>  Les jeux doivent être stockés et mis à jour </h2><br>  Les principales données du service de jeu en nuage sont les distributions de jeux, qui peuvent dépasser des centaines de Go, et les économies des utilisateurs. <br><br>  Quand nous étions petits, nous n'avions qu'une douzaine de serveurs et un modeste catalogue de 50 jeux.  Nous avons stocké toutes les données localement sur les serveurs, mis à jour manuellement, tout allait bien.  Mais le temps est venu de grandir et nous sommes partis <strong>pour les nuages ​​AWS</strong> . <br><br>  Avec AWS, nous avons obtenu plusieurs centaines de serveurs, mais l'architecture n'a pas changé.  Ils étaient également des serveurs, mais maintenant virtuels, avec des disques locaux sur lesquels reposaient les distributions de jeux.  Cependant, la mise à jour manuelle sur une centaine de serveurs échouera. <br><br>  Nous avons commencé à chercher une solution.  Au début, nous avons essayé de mettre à jour via <strong>rsync</strong> .  Mais il s'est avéré que cela est extrêmement lent et que la charge sur le nœud principal est trop importante.  Mais ce n'est même pas le pire: lorsque nous avions une faible connexion en ligne, nous avons éteint certaines des machines virtuelles afin de ne pas les payer, et lors de la mise à jour, les données n'étaient pas versées sur les serveurs éteints.  Tous ont été laissés sans mises à jour. <br><br>  La solution était des torrents - le programme <strong>BTSync</strong> .  Il vous permet de synchroniser un dossier sur un grand nombre de nœuds sans spécifier explicitement un nœud central. <br><br><h2>  Problèmes de croissance </h2><br>  Pendant un certain temps, tout cela a fonctionné à merveille.  Mais le service se développait, il y avait plus de jeux et de serveurs.  Le nombre de stockages locaux a également augmenté, nous avons dû payer de plus en plus.  Dans les nuages, c'est cher, surtout pour les SSD.  À un moment donné, même l'indexation habituelle d'un dossier pour démarrer sa synchronisation a commencé à prendre plus d'une heure, et tous les serveurs pouvaient être mis à jour pendant plusieurs jours. <br><br>  BTSync a créé un autre problème avec un trafic réseau excessif.  À cette époque, chez Amazon, il était payé même entre des réseaux virtuels internes.  Si le lanceur de jeu classique apporte de petites modifications aux gros fichiers, alors BTSync croit immédiatement que le fichier entier a changé et commence à le transférer entièrement à tous les nœuds.  Par conséquent, même une mise à niveau de 15 Mo pourrait générer des dizaines de Go de trafic de synchronisation. <br><br>  La situation est devenue critique lorsque le stockage est passé à 1 To.  Je viens de sortir un nouveau jeu World of Warships.  Sa distribution comptait plusieurs centaines de milliers de petits fichiers.  BTSync n'a pas pu le digérer et le distribuer à tous les autres serveurs - cela a ralenti la distribution des autres jeux. <br><br>  Tous ces facteurs ont créé deux problèmes: <br><br><ul><li>  produire un stockage local est coûteux, peu pratique et difficile à mettre à jour; </li><li>  les nuages ​​étaient très chers. </li></ul><br>  Nous avons décidé de revenir au concept de nos serveurs physiques. <br><br><h2>  Propre système de stockage </h2><br>  Avant de passer aux serveurs physiques, nous devons nous débarrasser du stockage local.  Cela nécessite son propre <strong>système de stockage - le stockage</strong> .  Il s'agit d'un système qui stocke toutes les distributions et les distribue de manière centrale sur tous les serveurs. <br><br>  Il semble que la tâche soit simple - elle a déjà été résolue à plusieurs reprises.  Mais avec les jeux, il y a des nuances.  Par exemple, la plupart des jeux refusent tout simplement de fonctionner s'ils disposent d'un accès en lecture seule.  Même avec la start-up habituelle, ils aiment écrire quelque chose dans leurs fichiers, et sans cela ils refusent de travailler.  Au contraire, si un grand nombre d'utilisateurs ont accès à un ensemble de distributions, ils commencent à battre les fichiers les uns des autres avec un accès compétitif. <br><br>  Nous avons réfléchi au problème, vérifié plusieurs solutions et <strong>sommes</strong> arrivés à <strong>ZFS - Zettabyte File System sur FreeBSD</strong> . <br><br><h2>  ZFS sur FreeBSD </h2><br>  Ce n'est pas un système de fichiers ordinaire.  Les systèmes classiques sont initialement installés sur un seul appareil, et pour travailler avec plusieurs disques nécessitent déjà un gestionnaire de volume. <br><blockquote>  ZFS a été initialement construit sur des pools virtuels. </blockquote>  Ils sont appelés <strong>zpool</strong> et se composent de groupes de disques ou de matrices RAID.  Le volume entier de ces disques est disponible pour tout système de fichiers dans zpool.  C'est parce que ZFS a été initialement développé comme un système qui fonctionnera avec de grandes quantités de données. <br><br><h3>  Comment ZFS a aidé à résoudre nos problèmes </h3><br>  Ce système possède un merveilleux <strong>mécanisme pour créer des instantanés et des clones</strong> .  Ils sont créés <strong>instantanément</strong> et ne pèsent que quelques Ko.  Lorsque nous apportons des modifications à l'un des clones, il augmente du volume de ces modifications.  Dans le même temps, les données des clones restants ne changent pas et restent uniques.  Cela vous permet de distribuer un disque de <strong>10 To</strong> avec un accès exclusif à l'utilisateur final, en ne dépensant que quelques Ko. <br><br>  Si des clones se développent en cours de modification d'une session de jeu, ne prendront-ils pas autant d'espace que tous les jeux?  Non, nous avons constaté que même dans des sessions de jeu assez longues, l'ensemble des changements dépasse rarement 100 à 200 Mo - ce n'est pas critique.  Par conséquent, nous pouvons donner un accès complet à un disque dur à haute capacité à part entière à plusieurs centaines d'utilisateurs en même temps, en ne dépensant que 10 To avec une queue. <br><br><h3>  Fonctionnement de ZFS </h3><br>  La description semble compliquée, mais ZFS fonctionne tout simplement.  Analysons son travail avec un exemple simple - créer des <code>zpool data</code> partir des disques <code>zpool create data /dev/da /dev/db /dev/dc</code> disponibles <code>zpool create data /dev/da /dev/db /dev/dc</code> . <br><br>  <em>Remarque</em>  <em>Ce n'est pas nécessaire pour la production, car si au moins un disque meurt, l'ensemble du pool passera dans l'oubli avec lui.</em>  <em>Mieux utiliser les groupes RAID.</em> <br><br>  Nous créons le système de fichiers <code>zfs create data/games</code> , et en lui un périphérique de bloc avec le nom <code>data/games/disk</code> de 10 To.  L'appareil est disponible dans <code>/dev/zvol/data/games/disk</code> comme un disque normal - vous pouvez effectuer les mêmes manipulations avec lui. <br><br>  Ensuite, le plaisir commence.  Nous remettons ce disque via <strong>iSCSI à</strong> notre assistant de mise <strong>à</strong> jour - une machine virtuelle classique exécutant Windows.  Nous connectons le disque et y mettons les jeux simplement depuis Steam, comme sur un ordinateur personnel ordinaire. <br><br>  Remplissez le disque avec des jeux.  Il reste maintenant à distribuer ces données à <strong>200 serveurs</strong> pour les utilisateurs finaux. <br><br><ul><li>  Créez un instantané de ce disque et appelez-le la première version - <code>zfs snapshot data/games/disk@ver1</code> .  <strong>Créez son clone</strong> <code>zfs clone data/games/disk@ver1 data/games/disk-vm1</code> , qui ira à la première machine virtuelle. </li><li>  Nous donnons le clone via iSCSI et <strong>KVM lance une</strong> machine virtuelle <strong>avec ce disque</strong> .  Il se charge, entre dans un pool de serveurs accessibles aux utilisateurs et attend un joueur. </li><li>  Une fois la session utilisateur terminée, nous prenons toutes les sauvegardes utilisateur de cette machine virtuelle et les <strong>plaçons sur un serveur distinct</strong> .  Nous <code>zfs destroy data/games/disk-vm1</code> <strong>la machine</strong> virtuelle <strong>et détruisons le clone</strong> - <code>zfs destroy data/games/disk-vm1</code> . </li><li>  Nous revenons à la première étape, créons à nouveau un clone et démarrons la machine virtuelle. </li></ul><br>  Cela nous permet de fournir à chaque utilisateur suivant une <strong>machine toujours propre</strong> , sur laquelle il n'y a aucun changement par rapport au lecteur précédent.  Le disque après chaque session utilisateur est supprimé et l'espace qu'il occupait sur le système de stockage est libéré.  Nous effectuons également des opérations similaires avec le disque système et avec toutes nos machines virtuelles. <br><br>  Récemment, je suis tombé sur une vidéo sur YouTube, où un utilisateur satisfait lors d'une session de jeu a formaté nos disques durs sur des serveurs, et était très heureux qu'il ait tout cassé.  Oui, s'il vous plaît, juste pour payer - il peut jouer et se faire plaisir.  Dans tous les cas, le prochain utilisateur obtiendra toujours une machine virtuelle propre et fonctionnelle, quoi que fasse le précédent. <br><br>  Dans ce cadre, les jeux sont distribués à seulement 200 serveurs.  Nous avons calculé le nombre 200 expérimentalement: c'est le nombre de serveurs sur lesquels les charges critiques sur les disques de stockage ne se produisent pas.  En effet, les <strong>jeux ont un profil de chargement assez spécifique</strong> : ils lisent beaucoup au stade du lancement ou au niveau du chargement, et pendant le jeu, au contraire, n'utilisent pratiquement pas de disque.  Si votre profil de charge est différent, le chiffre sera différent. <br><br>  Dans l'ancien schéma, pour une maintenance simultanée de 200 utilisateurs, nous avions besoin de 2 000 To de stockage local.  Maintenant, nous pouvons dépenser un peu plus de 10 To pour l'ensemble de données principal, et il reste encore 0,5 To en stock pour les changements d'utilisateurs.  Bien que ZFS aime quand il a au moins 15% d'espace libre dans sa piscine, il me semble que nous avons considérablement économisé. <br><br><h3>  Et si nous avons plusieurs centres de données? </h3><br>  Ce mécanisme ne fonctionnera qu'à l'intérieur d'un centre de données, où les serveurs avec un système de stockage sont connectés par au moins 10 interfaces gigabits.  Que faire s'il y a plusieurs DC?  Comment mettre à jour le disque principal avec des jeux (jeu de données) entre eux? <br><br>  Pour cela, ZFS a sa propre solution - <strong>le mécanisme d'envoi / réception</strong> .  La commande d'exécution est très simple: <br><pre> <code class="bash hljs">zfs send -v data/games/disk@ver1 | ssh myzfsuser@myserverip zfs receive data/games/disk</code> </pre> <br>  Le mécanisme vous permet de transférer d'un système de stockage à un autre un instantané du système principal.  Pour la première fois, vous devrez envoyer les 10 téraoctets de données écrites sur le nœud maître vers un système de stockage vide.  Mais avec les prochaines mises à jour, nous n'enverrons les modifications qu'à partir du moment où nous avons créé l'instantané précédent. <br><br>  En conséquence, nous obtenons: <br><br><ul><li>  <strong>Toutes les modifications sont effectuées de manière centralisée sur un seul système de stockage</strong> .  Ensuite, ils se dispersent dans tous les autres centres de données en n'importe quelle quantité, et les données sur tous les nœuds sont toujours identiques. </li><li>  <strong>Le mécanisme d'envoi / réception n'a pas peur d'une déconnexion</strong> .  Les données ne sont pas appliquées à l'ensemble de données principal tant qu'elles n'ont pas été entièrement transmises au nœud esclave.  Si la connexion est perdue, il est impossible d'endommager les données et répétez simplement la procédure d'envoi. </li><li>  <strong>Tout nœud peut facilement devenir un nœud maître</strong> lors d'un accident en quelques minutes, car les données de tous les nœuds sont toujours identiques. </li></ul><br><h3>  Déduplication et sauvegardes </h3><br>  ZFS a une autre fonctionnalité utile - la <strong>déduplication</strong> .  Cette fonction permet de <strong>ne pas stocker deux blocs de données identiques</strong> .  Au lieu de cela, seul le premier bloc est stocké et à la place du second, un lien vers le premier est stocké.  Deux fichiers identiques prendront de l'espace en un seul et s'ils correspondent à 90%, ils rempliront 110% du volume d'origine. <br><br>  La fonction nous a beaucoup aidés à stocker les sauvegardes des utilisateurs.  Dans un jeu, différents utilisateurs ont une sauvegarde similaire, de nombreux fichiers sont identiques.  Grâce à la déduplication, nous pouvons stocker cinq fois plus de données.  Notre taux de déduplication est de 5,22.  Physiquement, nous avons 4,43 téraoctets, nous multiplions par un facteur et nous obtenons près de 23 téraoctets de données réelles.  Cela économise de l'espace en évitant le stockage en double. <br><div class="scrollable-table"><table><tbody><tr><td>  NOM </td><td>  La taille </td><td>  ALLOC </td><td>  GRATUIT </td><td>  DEDUP </td></tr><tr><td>  les données </td><td>  7,16 To </td><td>  4,43 To </td><td>  2,73 To </td><td>  5.22x </td></tr></tbody></table></div>  <strong>Les instantanés sont bons pour les sauvegardes</strong> .  Nous utilisons cette technologie sur nos stockages de fichiers.  Par exemple, si vous enregistrez une image chaque jour pendant un mois, vous pouvez déployer un clone à tout moment n'importe quel jour de ce mois et extraire les fichiers perdus ou endommagés.  Cela élimine le besoin de restaurer l'ensemble du stockage ou d'en déployer une copie complète. <br><br>  <strong>Nous utilisons des clones pour aider nos développeurs</strong> .  Par exemple, ils veulent vivre une migration potentiellement dangereuse sur une base de combat.  Il n'est pas rapide de déployer une sauvegarde classique d'une base de données qui approche les 1 To.  Par conséquent, nous supprimons simplement le clone du disque de base et l'ajoutons instantanément à la nouvelle instance.  Les développeurs peuvent désormais tout tester en toute sécurité. <br><br><h3>  API ZFS </h3><br>  Bien sûr, tout cela doit être automatisé.  Pourquoi grimper sur les serveurs, travailler avec vos mains, écrire des scripts, si cela peut être donné aux programmeurs?  Par conséquent, nous avons écrit notre <a href="https://github.com/drook/zfsapi">API Web</a> simple. <br><br>  Nous y avons inclus toutes les fonctions ZFS standard, coupé l'accès à celles qui sont potentiellement dangereuses et pourraient casser tout le système de stockage, et avons donné tout cela aux programmeurs.  Désormais, <strong>toutes les opérations sur disque sont strictement centralisées</strong> et effectuées par code, et nous <strong>connaissons toujours l'état de chaque disque</strong> .  Tout fonctionne très bien. <br><br><h2>  ZoL - ZFS sur Linux </h2><br>  Nous avons centralisé le système et pensé, est-ce si bon?  En effet, maintenant pour toute extension, nous devons immédiatement acheter plusieurs racks de serveurs: ils sont liés aux systèmes de stockage, et il est irrationnel de diviser le système.  Que faire lorsque nous décidons de déployer un petit stand de démonstration pour montrer la technologie à des partenaires dans d'autres pays? <br><br>  En pensant, nous sommes arrivés à la vieille idée - d' <strong>utiliser des disques locaux</strong> , mais seulement avec toute l'expérience et les connaissances que nous avons reçues.  Si vous développez l'idée plus globalement, alors pourquoi ne pas donner à nos utilisateurs la possibilité non seulement d'utiliser nos serveurs, mais aussi de louer leurs ordinateurs? <br><br>  La fourchette relativement récente de <strong>ZFS sur Linux - ZoL</strong> nous a beaucoup aidés à cet <strong>égard</strong> . <br><blockquote>  Désormais, chaque serveur dispose de son propre stockage. </blockquote>  Seulement, il ne stocke pas 10 téraoctets de données, comme dans le cas d'une installation centralisée, mais seulement 1-2 distributions des jeux qu'il sert.  Un SSD suffit pour cela.  Tout cela fonctionne bien: chaque utilisateur suivant obtient toujours une machine virtuelle propre, ainsi qu'une installation de combat. <br><br>  Cependant, nous avons rencontré ici deux problèmes. <br><br><h3>  Comment mettre à jour? </h3><br>  <strong>Mettre à jour de manière centralisée via SSH, comme nous le faisons dans les centres de données ne fonctionnera pas</strong> .  Les utilisateurs peuvent être connectés au réseau local ou simplement désactivés, contrairement aux systèmes de stockage, et vous ne voulez pas augmenter autant de connexions SSH. <br><br>  Nous avons rencontré les mêmes problèmes que lors de l'utilisation de rsync.  Cependant, les torrents au-dessus de ZFS ne peuvent plus être obtenus.  Nous avons soigneusement réfléchi au fonctionnement du mécanisme d'envoi: il envoie tous les blocs de données modifiés vers le stockage final, où Receive les applique à l'ensemble de données actuel.  Pourquoi ne pas écrire les données dans un fichier, au lieu de les envoyer à l'utilisateur final? <br><br>  Le résultat est ce que nous appelons <strong>diff</strong> .  Il s'agit d'un fichier dans lequel tous les blocs modifiés entre les deux derniers instantanés sont écrits séquentiellement.  Nous avons mis ce diff sur un CDN et l'avons envoyé à tous nos utilisateurs via HTTP: il a allumé la machine, a vu qu'il y avait des mises à jour, l'a dégonflé et l'a appliqué à l'ensemble de données local à l'aide de la réception. <br><br><h3>  Que faire des chauffeurs? </h3><br>  Les serveurs centralisés ont la même configuration et les <strong>utilisateurs finaux ont toujours des ordinateurs et des cartes vidéo différents</strong> .  Même si nous remplissons la distribution du système d'exploitation avec tous les pilotes possibles autant que possible, la première fois qu'elle démarre, elle voudra toujours installer ces pilotes, puis elle redémarrera, puis, éventuellement, à nouveau.  Puisque chaque fois que nous fournissons un clone propre, tout ce carrousel se produira après chaque session utilisateur - c'est mauvais. <br><br>  Nous voulions faire un peu d'initialisation: attendre que Windows démarre, installe tous les pilotes, fasse tout ce qu'elle veut, puis seulement opère sur ce lecteur.  Mais le problème est que si vous apportez des modifications à l'ensemble de données principal, les mises à jour seront interrompues, car les données sur la source et sur le récepteur seront différentes et les différences ne s'appliqueront tout simplement pas. <br><br>  Cependant, ZFS est un système flexible et nous a permis de faire une petite béquille. <br><br><ul><li>  Comme d'habitude, créez un snapshot: <code>zfs snapshot data/games/os@init</code> . </li><li>  Créez son clone - <code>zfs clone data/games/os@init data/games/os-init</code> - et exécutez-le en mode d'initialisation. </li><li>  Nous attendons que tous les pilotes soient installés et tout redémarrera. </li><li>  Éteignez la machine virtuelle et reprenez un instantané.  Mais cette fois, pas à partir du jeu de données d'origine, mais à partir du clone d'initialisation: <code>zfs snapshot data/games/os-init@ver1</code> . </li><li>  Nous créons un clone de l'instantané avec tous les pilotes installés.  Il ne redémarrera plus: <code>zfs clone data/games/os-init@ver1 data/games/os-vm1</code> . </li><li>  Ensuite, nous travaillons sur le bouquet classique. </li></ul><br>  Maintenant, ce système est au stade des tests alpha.  Nous le testons sur de vrais utilisateurs sans connaissance de Linux, mais ils parviennent à tout déployer à la maison.  Notre objectif ultime est que tout utilisateur branche simplement un lecteur flash USB amorçable sur son ordinateur, connecte un lecteur SSD supplémentaire et le loue sur notre plateforme cloud. <br><br>  Nous n'avons discuté que d'une petite partie de la fonctionnalité ZFS.  Ce système peut faire des choses beaucoup plus intéressantes et différentes, mais peu de gens connaissent ZFS - les utilisateurs ne veulent pas en parler.  J'espère qu'après cet article de nouveaux utilisateurs apparaîtront dans la communauté ZFS. <br><br><blockquote>  Abonnez-vous à un <a href="https://t.me/DevOpsConfChannel">canal télégramme</a> ou à une <a href="http://eepurl.com/bN_0E1">newsletter</a> pour en savoir plus sur les nouveaux articles et vidéos de la conférence <a href="https://devopsconf.io/">DevOpsConf</a> .  En plus de la newsletter, nous collectons des informations sur les conférences à venir et disons, par exemple, ce qui sera intéressant pour les fans de DevOps à <a href="https://www.highload.ru/spb/2020">Saint HighLoad ++</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr480622/">https://habr.com/ru/post/fr480622/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr480610/index.html">Vtables C ++. Partie 2 (héritage virtuel + code généré par le compilateur)</a></li>
<li><a href="../fr480612/index.html">Apportez ces modifications pour respecter les normes d'accessibilité de la conception Web.</a></li>
<li><a href="../fr480614/index.html">ENUM rapide</a></li>
<li><a href="../fr480618/index.html">Jeu électronique Tic Tac Toe. Où suis-je venu</a></li>
<li><a href="../fr480620/index.html">SD-WAN et DNA pour aider l'administrateur: caractéristiques des architectures et de la pratique</a></li>
<li><a href="../fr480626/index.html">Héritage des systèmes et processus hérités ou Les 90 premiers jours dans le rôle de CTO</a></li>
<li><a href="../fr480642/index.html">Introduction aux ELF Linux: Comprendre et analyser</a></li>
<li><a href="../fr480644/index.html">Le manifeste sur l'abolition de 146 du Code pénal et le boycott de la Sberbank et des détenteurs de droits d'auteur-parasites. Pour l'open source et nginx</a></li>
<li><a href="../fr480646/index.html">Habr - meilleurs articles, auteurs et statistiques 2019</a></li>
<li><a href="../fr480650/index.html">Dont les cheveux sont plus forts: morphologie des cheveux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>