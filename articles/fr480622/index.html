<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßïüèª üçÖ üë®‚Äç‚ù§Ô∏è‚Äçüë® Comment utiliser correctement la capacit√© de stockage disponible üçë üìº üë©üèΩ‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous utilisons les services cloud depuis longtemps: courrier, stockage, r√©seaux sociaux, messagerie instantan√©e. Ils fonctionnent tous √† distance - no...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment utiliser correctement la capacit√© de stockage disponible</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/480622/">  Nous utilisons les services cloud depuis longtemps: courrier, stockage, r√©seaux sociaux, messagerie instantan√©e.  Ils fonctionnent tous √† distance - nous envoyons des messages et des fichiers, et ils sont stock√©s et trait√©s sur des serveurs distants.  Le cloud gaming fonctionne √©galement: l'utilisateur se connecte au service, s√©lectionne le jeu et se lance.  Ceci est pratique pour le joueur, car les jeux d√©marrent presque instantan√©ment, ne prennent pas de m√©moire et n'ont pas besoin d'un ordinateur de jeu puissant. <br><br><img src="https://habrastorage.org/webt/ej/k4/oy/ejk4oyjjh1r3riqgzzd239qu_va.jpeg"><br><br>  Pour un service cloud, tout est diff√©rent - il a des probl√®mes de stockage de donn√©es.  Chaque jeu peut peser des dizaines ou des centaines de gigaoctets, par exemple, "The Witcher 3" prend 50 Go, et "Call of Duty: Black Ops III" - 113. En m√™me temps, les joueurs n'utiliseront pas le service avec 2-3 jeux, au moins plusieurs dizaines sont n√©cessaires .  En plus de stocker des centaines de jeux, le service doit d√©cider de la quantit√© de stockage √† allouer par joueur et √©voluer lorsqu'il y en a des milliers. <br><br>  Tout cela devrait-il √™tre stock√© sur leurs serveurs: combien en ont-ils besoin, o√π placer les centres de donn√©es, comment ¬´synchroniser¬ª les donn√©es entre plusieurs centres de donn√©es √† la vol√©e?  Acheter des "nuages"?  Utiliser des machines virtuelles?  Est-il possible de stocker 5 fois les donn√©es utilisateur avec compression et de les fournir en temps r√©el?  Comment exclure toute influence des utilisateurs les uns sur les autres lors d'une utilisation coh√©rente de la m√™me machine virtuelle? <br><br>  Toutes ces t√¢ches ont √©t√© r√©solues avec succ√®s dans Playkey.net - une plate-forme de jeu bas√©e sur le cloud.  <strong>Vladimir Ryabov</strong> ( <a href="https://habr.com/ru/users/graymansama/" class="user_link">Graymansama</a> ) - chef du d√©partement d'administration syst√®me - parlera en d√©tail de la technologie ZFS pour FreeBSD, qui a aid√© √† cela, et de sa nouvelle version de ZOL (ZFS sur Linux). <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SssLwMbMrQ4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Un millier de serveurs de l'entreprise sont situ√©s dans des centres de donn√©es distants √† Moscou, Londres et Francfort.  Il y a plus de 250 jeux dans le service, qui sont jou√©s par 100 000 joueurs par mois. <br><br><img src="https://habrastorage.org/webt/s6/mv/nw/s6mvnwi5z_b5ltjl_vico2x7ro4.jpeg"><br><br>  Le service fonctionne comme ceci: le jeu s'ex√©cute sur les serveurs de l'entreprise, l'utilisateur re√ßoit un flux de commandes du clavier, de la souris ou de la manette de jeu, et un flux vid√©o est envoy√© en r√©ponse.  Cela vous permet de jouer √† des jeux haut de gamme modernes sur des ordinateurs avec un mat√©riel faible, des ordinateurs portables avec vid√©o int√©gr√©e ou sur des Mac pour lesquels ces jeux ne sont pas du tout commercialis√©s. <br><br><h2>  Les jeux doivent √™tre stock√©s et mis √† jour </h2><br>  Les principales donn√©es du service de jeu en nuage sont les distributions de jeux, qui peuvent d√©passer des centaines de Go, et les √©conomies des utilisateurs. <br><br>  Quand nous √©tions petits, nous n'avions qu'une douzaine de serveurs et un modeste catalogue de 50 jeux.  Nous avons stock√© toutes les donn√©es localement sur les serveurs, mis √† jour manuellement, tout allait bien.  Mais le temps est venu de grandir et nous sommes partis <strong>pour les nuages ‚Äã‚ÄãAWS</strong> . <br><br>  Avec AWS, nous avons obtenu plusieurs centaines de serveurs, mais l'architecture n'a pas chang√©.  Ils √©taient √©galement des serveurs, mais maintenant virtuels, avec des disques locaux sur lesquels reposaient les distributions de jeux.  Cependant, la mise √† jour manuelle sur une centaine de serveurs √©chouera. <br><br>  Nous avons commenc√© √† chercher une solution.  Au d√©but, nous avons essay√© de mettre √† jour via <strong>rsync</strong> .  Mais il s'est av√©r√© que cela est extr√™mement lent et que la charge sur le n≈ìud principal est trop importante.  Mais ce n'est m√™me pas le pire: lorsque nous avions une faible connexion en ligne, nous avons √©teint certaines des machines virtuelles afin de ne pas les payer, et lors de la mise √† jour, les donn√©es n'√©taient pas vers√©es sur les serveurs √©teints.  Tous ont √©t√© laiss√©s sans mises √† jour. <br><br>  La solution √©tait des torrents - le programme <strong>BTSync</strong> .  Il vous permet de synchroniser un dossier sur un grand nombre de n≈ìuds sans sp√©cifier explicitement un n≈ìud central. <br><br><h2>  Probl√®mes de croissance </h2><br>  Pendant un certain temps, tout cela a fonctionn√© √† merveille.  Mais le service se d√©veloppait, il y avait plus de jeux et de serveurs.  Le nombre de stockages locaux a √©galement augment√©, nous avons d√ª payer de plus en plus.  Dans les nuages, c'est cher, surtout pour les SSD.  √Ä un moment donn√©, m√™me l'indexation habituelle d'un dossier pour d√©marrer sa synchronisation a commenc√© √† prendre plus d'une heure, et tous les serveurs pouvaient √™tre mis √† jour pendant plusieurs jours. <br><br>  BTSync a cr√©√© un autre probl√®me avec un trafic r√©seau excessif.  √Ä cette √©poque, chez Amazon, il √©tait pay√© m√™me entre des r√©seaux virtuels internes.  Si le lanceur de jeu classique apporte de petites modifications aux gros fichiers, alors BTSync croit imm√©diatement que le fichier entier a chang√© et commence √† le transf√©rer enti√®rement √† tous les n≈ìuds.  Par cons√©quent, m√™me une mise √† niveau de 15 Mo pourrait g√©n√©rer des dizaines de Go de trafic de synchronisation. <br><br>  La situation est devenue critique lorsque le stockage est pass√© √† 1 To.  Je viens de sortir un nouveau jeu World of Warships.  Sa distribution comptait plusieurs centaines de milliers de petits fichiers.  BTSync n'a pas pu le dig√©rer et le distribuer √† tous les autres serveurs - cela a ralenti la distribution des autres jeux. <br><br>  Tous ces facteurs ont cr√©√© deux probl√®mes: <br><br><ul><li>  produire un stockage local est co√ªteux, peu pratique et difficile √† mettre √† jour; </li><li>  les nuages ‚Äã‚Äã√©taient tr√®s chers. </li></ul><br>  Nous avons d√©cid√© de revenir au concept de nos serveurs physiques. <br><br><h2>  Propre syst√®me de stockage </h2><br>  Avant de passer aux serveurs physiques, nous devons nous d√©barrasser du stockage local.  Cela n√©cessite son propre <strong>syst√®me de stockage - le stockage</strong> .  Il s'agit d'un syst√®me qui stocke toutes les distributions et les distribue de mani√®re centrale sur tous les serveurs. <br><br>  Il semble que la t√¢che soit simple - elle a d√©j√† √©t√© r√©solue √† plusieurs reprises.  Mais avec les jeux, il y a des nuances.  Par exemple, la plupart des jeux refusent tout simplement de fonctionner s'ils disposent d'un acc√®s en lecture seule.  M√™me avec la start-up habituelle, ils aiment √©crire quelque chose dans leurs fichiers, et sans cela ils refusent de travailler.  Au contraire, si un grand nombre d'utilisateurs ont acc√®s √† un ensemble de distributions, ils commencent √† battre les fichiers les uns des autres avec un acc√®s comp√©titif. <br><br>  Nous avons r√©fl√©chi au probl√®me, v√©rifi√© plusieurs solutions et <strong>sommes</strong> arriv√©s √† <strong>ZFS - Zettabyte File System sur FreeBSD</strong> . <br><br><h2>  ZFS sur FreeBSD </h2><br>  Ce n'est pas un syst√®me de fichiers ordinaire.  Les syst√®mes classiques sont initialement install√©s sur un seul appareil, et pour travailler avec plusieurs disques n√©cessitent d√©j√† un gestionnaire de volume. <br><blockquote>  ZFS a √©t√© initialement construit sur des pools virtuels. </blockquote>  Ils sont appel√©s <strong>zpool</strong> et se composent de groupes de disques ou de matrices RAID.  Le volume entier de ces disques est disponible pour tout syst√®me de fichiers dans zpool.  C'est parce que ZFS a √©t√© initialement d√©velopp√© comme un syst√®me qui fonctionnera avec de grandes quantit√©s de donn√©es. <br><br><h3>  Comment ZFS a aid√© √† r√©soudre nos probl√®mes </h3><br>  Ce syst√®me poss√®de un merveilleux <strong>m√©canisme pour cr√©er des instantan√©s et des clones</strong> .  Ils sont cr√©√©s <strong>instantan√©ment</strong> et ne p√®sent que quelques Ko.  Lorsque nous apportons des modifications √† l'un des clones, il augmente du volume de ces modifications.  Dans le m√™me temps, les donn√©es des clones restants ne changent pas et restent uniques.  Cela vous permet de distribuer un disque de <strong>10 To</strong> avec un acc√®s exclusif √† l'utilisateur final, en ne d√©pensant que quelques Ko. <br><br>  Si des clones se d√©veloppent en cours de modification d'une session de jeu, ne prendront-ils pas autant d'espace que tous les jeux?  Non, nous avons constat√© que m√™me dans des sessions de jeu assez longues, l'ensemble des changements d√©passe rarement 100 √† 200 Mo - ce n'est pas critique.  Par cons√©quent, nous pouvons donner un acc√®s complet √† un disque dur √† haute capacit√© √† part enti√®re √† plusieurs centaines d'utilisateurs en m√™me temps, en ne d√©pensant que 10 To avec une queue. <br><br><h3>  Fonctionnement de ZFS </h3><br>  La description semble compliqu√©e, mais ZFS fonctionne tout simplement.  Analysons son travail avec un exemple simple - cr√©er des <code>zpool data</code> partir des disques <code>zpool create data /dev/da /dev/db /dev/dc</code> disponibles <code>zpool create data /dev/da /dev/db /dev/dc</code> . <br><br>  <em>Remarque</em>  <em>Ce n'est pas n√©cessaire pour la production, car si au moins un disque meurt, l'ensemble du pool passera dans l'oubli avec lui.</em>  <em>Mieux utiliser les groupes RAID.</em> <br><br>  Nous cr√©ons le syst√®me de fichiers <code>zfs create data/games</code> , et en lui un p√©riph√©rique de bloc avec le nom <code>data/games/disk</code> de 10 To.  L'appareil est disponible dans <code>/dev/zvol/data/games/disk</code> comme un disque normal - vous pouvez effectuer les m√™mes manipulations avec lui. <br><br>  Ensuite, le plaisir commence.  Nous remettons ce disque via <strong>iSCSI √†</strong> notre assistant de mise <strong>√†</strong> jour - une machine virtuelle classique ex√©cutant Windows.  Nous connectons le disque et y mettons les jeux simplement depuis Steam, comme sur un ordinateur personnel ordinaire. <br><br>  Remplissez le disque avec des jeux.  Il reste maintenant √† distribuer ces donn√©es √† <strong>200 serveurs</strong> pour les utilisateurs finaux. <br><br><ul><li>  Cr√©ez un instantan√© de ce disque et appelez-le la premi√®re version - <code>zfs snapshot data/games/disk@ver1</code> .  <strong>Cr√©ez son clone</strong> <code>zfs clone data/games/disk@ver1 data/games/disk-vm1</code> , qui ira √† la premi√®re machine virtuelle. </li><li>  Nous donnons le clone via iSCSI et <strong>KVM lance une</strong> machine virtuelle <strong>avec ce disque</strong> .  Il se charge, entre dans un pool de serveurs accessibles aux utilisateurs et attend un joueur. </li><li>  Une fois la session utilisateur termin√©e, nous prenons toutes les sauvegardes utilisateur de cette machine virtuelle et les <strong>pla√ßons sur un serveur distinct</strong> .  Nous <code>zfs destroy data/games/disk-vm1</code> <strong>la machine</strong> virtuelle <strong>et d√©truisons le clone</strong> - <code>zfs destroy data/games/disk-vm1</code> . </li><li>  Nous revenons √† la premi√®re √©tape, cr√©ons √† nouveau un clone et d√©marrons la machine virtuelle. </li></ul><br>  Cela nous permet de fournir √† chaque utilisateur suivant une <strong>machine toujours propre</strong> , sur laquelle il n'y a aucun changement par rapport au lecteur pr√©c√©dent.  Le disque apr√®s chaque session utilisateur est supprim√© et l'espace qu'il occupait sur le syst√®me de stockage est lib√©r√©.  Nous effectuons √©galement des op√©rations similaires avec le disque syst√®me et avec toutes nos machines virtuelles. <br><br>  R√©cemment, je suis tomb√© sur une vid√©o sur YouTube, o√π un utilisateur satisfait lors d'une session de jeu a format√© nos disques durs sur des serveurs, et √©tait tr√®s heureux qu'il ait tout cass√©.  Oui, s'il vous pla√Æt, juste pour payer - il peut jouer et se faire plaisir.  Dans tous les cas, le prochain utilisateur obtiendra toujours une machine virtuelle propre et fonctionnelle, quoi que fasse le pr√©c√©dent. <br><br>  Dans ce cadre, les jeux sont distribu√©s √† seulement 200 serveurs.  Nous avons calcul√© le nombre 200 exp√©rimentalement: c'est le nombre de serveurs sur lesquels les charges critiques sur les disques de stockage ne se produisent pas.  En effet, les <strong>jeux ont un profil de chargement assez sp√©cifique</strong> : ils lisent beaucoup au stade du lancement ou au niveau du chargement, et pendant le jeu, au contraire, n'utilisent pratiquement pas de disque.  Si votre profil de charge est diff√©rent, le chiffre sera diff√©rent. <br><br>  Dans l'ancien sch√©ma, pour une maintenance simultan√©e de 200 utilisateurs, nous avions besoin de 2 000 To de stockage local.  Maintenant, nous pouvons d√©penser un peu plus de 10 To pour l'ensemble de donn√©es principal, et il reste encore 0,5 To en stock pour les changements d'utilisateurs.  Bien que ZFS aime quand il a au moins 15% d'espace libre dans sa piscine, il me semble que nous avons consid√©rablement √©conomis√©. <br><br><h3>  Et si nous avons plusieurs centres de donn√©es? </h3><br>  Ce m√©canisme ne fonctionnera qu'√† l'int√©rieur d'un centre de donn√©es, o√π les serveurs avec un syst√®me de stockage sont connect√©s par au moins 10 interfaces gigabits.  Que faire s'il y a plusieurs DC?  Comment mettre √† jour le disque principal avec des jeux (jeu de donn√©es) entre eux? <br><br>  Pour cela, ZFS a sa propre solution - <strong>le m√©canisme d'envoi / r√©ception</strong> .  La commande d'ex√©cution est tr√®s simple: <br><pre> <code class="bash hljs">zfs send -v data/games/disk@ver1 | ssh myzfsuser@myserverip zfs receive data/games/disk</code> </pre> <br>  Le m√©canisme vous permet de transf√©rer d'un syst√®me de stockage √† un autre un instantan√© du syst√®me principal.  Pour la premi√®re fois, vous devrez envoyer les 10 t√©raoctets de donn√©es √©crites sur le n≈ìud ma√Ætre vers un syst√®me de stockage vide.  Mais avec les prochaines mises √† jour, nous n'enverrons les modifications qu'√† partir du moment o√π nous avons cr√©√© l'instantan√© pr√©c√©dent. <br><br>  En cons√©quence, nous obtenons: <br><br><ul><li>  <strong>Toutes les modifications sont effectu√©es de mani√®re centralis√©e sur un seul syst√®me de stockage</strong> .  Ensuite, ils se dispersent dans tous les autres centres de donn√©es en n'importe quelle quantit√©, et les donn√©es sur tous les n≈ìuds sont toujours identiques. </li><li>  <strong>Le m√©canisme d'envoi / r√©ception n'a pas peur d'une d√©connexion</strong> .  Les donn√©es ne sont pas appliqu√©es √† l'ensemble de donn√©es principal tant qu'elles n'ont pas √©t√© enti√®rement transmises au n≈ìud esclave.  Si la connexion est perdue, il est impossible d'endommager les donn√©es et r√©p√©tez simplement la proc√©dure d'envoi. </li><li>  <strong>Tout n≈ìud peut facilement devenir un n≈ìud ma√Ætre</strong> lors d'un accident en quelques minutes, car les donn√©es de tous les n≈ìuds sont toujours identiques. </li></ul><br><h3>  D√©duplication et sauvegardes </h3><br>  ZFS a une autre fonctionnalit√© utile - la <strong>d√©duplication</strong> .  Cette fonction permet de <strong>ne pas stocker deux blocs de donn√©es identiques</strong> .  Au lieu de cela, seul le premier bloc est stock√© et √† la place du second, un lien vers le premier est stock√©.  Deux fichiers identiques prendront de l'espace en un seul et s'ils correspondent √† 90%, ils rempliront 110% du volume d'origine. <br><br>  La fonction nous a beaucoup aid√©s √† stocker les sauvegardes des utilisateurs.  Dans un jeu, diff√©rents utilisateurs ont une sauvegarde similaire, de nombreux fichiers sont identiques.  Gr√¢ce √† la d√©duplication, nous pouvons stocker cinq fois plus de donn√©es.  Notre taux de d√©duplication est de 5,22.  Physiquement, nous avons 4,43 t√©raoctets, nous multiplions par un facteur et nous obtenons pr√®s de 23 t√©raoctets de donn√©es r√©elles.  Cela √©conomise de l'espace en √©vitant le stockage en double. <br><div class="scrollable-table"><table><tbody><tr><td>  NOM </td><td>  La taille </td><td>  ALLOC </td><td>  GRATUIT </td><td>  DEDUP </td></tr><tr><td>  les donn√©es </td><td>  7,16 To </td><td>  4,43 To </td><td>  2,73 To </td><td>  5.22x </td></tr></tbody></table></div>  <strong>Les instantan√©s sont bons pour les sauvegardes</strong> .  Nous utilisons cette technologie sur nos stockages de fichiers.  Par exemple, si vous enregistrez une image chaque jour pendant un mois, vous pouvez d√©ployer un clone √† tout moment n'importe quel jour de ce mois et extraire les fichiers perdus ou endommag√©s.  Cela √©limine le besoin de restaurer l'ensemble du stockage ou d'en d√©ployer une copie compl√®te. <br><br>  <strong>Nous utilisons des clones pour aider nos d√©veloppeurs</strong> .  Par exemple, ils veulent vivre une migration potentiellement dangereuse sur une base de combat.  Il n'est pas rapide de d√©ployer une sauvegarde classique d'une base de donn√©es qui approche les 1 To.  Par cons√©quent, nous supprimons simplement le clone du disque de base et l'ajoutons instantan√©ment √† la nouvelle instance.  Les d√©veloppeurs peuvent d√©sormais tout tester en toute s√©curit√©. <br><br><h3>  API ZFS </h3><br>  Bien s√ªr, tout cela doit √™tre automatis√©.  Pourquoi grimper sur les serveurs, travailler avec vos mains, √©crire des scripts, si cela peut √™tre donn√© aux programmeurs?  Par cons√©quent, nous avons √©crit notre <a href="https://github.com/drook/zfsapi">API Web</a> simple. <br><br>  Nous y avons inclus toutes les fonctions ZFS standard, coup√© l'acc√®s √† celles qui sont potentiellement dangereuses et pourraient casser tout le syst√®me de stockage, et avons donn√© tout cela aux programmeurs.  D√©sormais, <strong>toutes les op√©rations sur disque sont strictement centralis√©es</strong> et effectu√©es par code, et nous <strong>connaissons toujours l'√©tat de chaque disque</strong> .  Tout fonctionne tr√®s bien. <br><br><h2>  ZoL - ZFS sur Linux </h2><br>  Nous avons centralis√© le syst√®me et pens√©, est-ce si bon?  En effet, maintenant pour toute extension, nous devons imm√©diatement acheter plusieurs racks de serveurs: ils sont li√©s aux syst√®mes de stockage, et il est irrationnel de diviser le syst√®me.  Que faire lorsque nous d√©cidons de d√©ployer un petit stand de d√©monstration pour montrer la technologie √† des partenaires dans d'autres pays? <br><br>  En pensant, nous sommes arriv√©s √† la vieille id√©e - d' <strong>utiliser des disques locaux</strong> , mais seulement avec toute l'exp√©rience et les connaissances que nous avons re√ßues.  Si vous d√©veloppez l'id√©e plus globalement, alors pourquoi ne pas donner √† nos utilisateurs la possibilit√© non seulement d'utiliser nos serveurs, mais aussi de louer leurs ordinateurs? <br><br>  La fourchette relativement r√©cente de <strong>ZFS sur Linux - ZoL</strong> nous a beaucoup aid√©s √† cet <strong>√©gard</strong> . <br><blockquote>  D√©sormais, chaque serveur dispose de son propre stockage. </blockquote>  Seulement, il ne stocke pas 10 t√©raoctets de donn√©es, comme dans le cas d'une installation centralis√©e, mais seulement 1-2 distributions des jeux qu'il sert.  Un SSD suffit pour cela.  Tout cela fonctionne bien: chaque utilisateur suivant obtient toujours une machine virtuelle propre, ainsi qu'une installation de combat. <br><br>  Cependant, nous avons rencontr√© ici deux probl√®mes. <br><br><h3>  Comment mettre √† jour? </h3><br>  <strong>Mettre √† jour de mani√®re centralis√©e via SSH, comme nous le faisons dans les centres de donn√©es ne fonctionnera pas</strong> .  Les utilisateurs peuvent √™tre connect√©s au r√©seau local ou simplement d√©sactiv√©s, contrairement aux syst√®mes de stockage, et vous ne voulez pas augmenter autant de connexions SSH. <br><br>  Nous avons rencontr√© les m√™mes probl√®mes que lors de l'utilisation de rsync.  Cependant, les torrents au-dessus de ZFS ne peuvent plus √™tre obtenus.  Nous avons soigneusement r√©fl√©chi au fonctionnement du m√©canisme d'envoi: il envoie tous les blocs de donn√©es modifi√©s vers le stockage final, o√π Receive les applique √† l'ensemble de donn√©es actuel.  Pourquoi ne pas √©crire les donn√©es dans un fichier, au lieu de les envoyer √† l'utilisateur final? <br><br>  Le r√©sultat est ce que nous appelons <strong>diff</strong> .  Il s'agit d'un fichier dans lequel tous les blocs modifi√©s entre les deux derniers instantan√©s sont √©crits s√©quentiellement.  Nous avons mis ce diff sur un CDN et l'avons envoy√© √† tous nos utilisateurs via HTTP: il a allum√© la machine, a vu qu'il y avait des mises √† jour, l'a d√©gonfl√© et l'a appliqu√© √† l'ensemble de donn√©es local √† l'aide de la r√©ception. <br><br><h3>  Que faire des chauffeurs? </h3><br>  Les serveurs centralis√©s ont la m√™me configuration et les <strong>utilisateurs finaux ont toujours des ordinateurs et des cartes vid√©o diff√©rents</strong> .  M√™me si nous remplissons la distribution du syst√®me d'exploitation avec tous les pilotes possibles autant que possible, la premi√®re fois qu'elle d√©marre, elle voudra toujours installer ces pilotes, puis elle red√©marrera, puis, √©ventuellement, √† nouveau.  Puisque chaque fois que nous fournissons un clone propre, tout ce carrousel se produira apr√®s chaque session utilisateur - c'est mauvais. <br><br>  Nous voulions faire un peu d'initialisation: attendre que Windows d√©marre, installe tous les pilotes, fasse tout ce qu'elle veut, puis seulement op√®re sur ce lecteur.  Mais le probl√®me est que si vous apportez des modifications √† l'ensemble de donn√©es principal, les mises √† jour seront interrompues, car les donn√©es sur la source et sur le r√©cepteur seront diff√©rentes et les diff√©rences ne s'appliqueront tout simplement pas. <br><br>  Cependant, ZFS est un syst√®me flexible et nous a permis de faire une petite b√©quille. <br><br><ul><li>  Comme d'habitude, cr√©ez un snapshot: <code>zfs snapshot data/games/os@init</code> . </li><li>  Cr√©ez son clone - <code>zfs clone data/games/os@init data/games/os-init</code> - et ex√©cutez-le en mode d'initialisation. </li><li>  Nous attendons que tous les pilotes soient install√©s et tout red√©marrera. </li><li>  √âteignez la machine virtuelle et reprenez un instantan√©.  Mais cette fois, pas √† partir du jeu de donn√©es d'origine, mais √† partir du clone d'initialisation: <code>zfs snapshot data/games/os-init@ver1</code> . </li><li>  Nous cr√©ons un clone de l'instantan√© avec tous les pilotes install√©s.  Il ne red√©marrera plus: <code>zfs clone data/games/os-init@ver1 data/games/os-vm1</code> . </li><li>  Ensuite, nous travaillons sur le bouquet classique. </li></ul><br>  Maintenant, ce syst√®me est au stade des tests alpha.  Nous le testons sur de vrais utilisateurs sans connaissance de Linux, mais ils parviennent √† tout d√©ployer √† la maison.  Notre objectif ultime est que tout utilisateur branche simplement un lecteur flash USB amor√ßable sur son ordinateur, connecte un lecteur SSD suppl√©mentaire et le loue sur notre plateforme cloud. <br><br>  Nous n'avons discut√© que d'une petite partie de la fonctionnalit√© ZFS.  Ce syst√®me peut faire des choses beaucoup plus int√©ressantes et diff√©rentes, mais peu de gens connaissent ZFS - les utilisateurs ne veulent pas en parler.  J'esp√®re qu'apr√®s cet article de nouveaux utilisateurs appara√Ætront dans la communaut√© ZFS. <br><br><blockquote>  Abonnez-vous √† un <a href="https://t.me/DevOpsConfChannel">canal t√©l√©gramme</a> ou √† une <a href="http://eepurl.com/bN_0E1">newsletter</a> pour en savoir plus sur les nouveaux articles et vid√©os de la conf√©rence <a href="https://devopsconf.io/">DevOpsConf</a> .  En plus de la newsletter, nous collectons des informations sur les conf√©rences √† venir et disons, par exemple, ce qui sera int√©ressant pour les fans de DevOps √† <a href="https://www.highload.ru/spb/2020">Saint HighLoad ++</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr480622/">https://habr.com/ru/post/fr480622/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr480610/index.html">Vtables C ++. Partie 2 (h√©ritage virtuel + code g√©n√©r√© par le compilateur)</a></li>
<li><a href="../fr480612/index.html">Apportez ces modifications pour respecter les normes d'accessibilit√© de la conception Web.</a></li>
<li><a href="../fr480614/index.html">ENUM rapide</a></li>
<li><a href="../fr480618/index.html">Jeu √©lectronique Tic Tac Toe. O√π suis-je venu</a></li>
<li><a href="../fr480620/index.html">SD-WAN et DNA pour aider l'administrateur: caract√©ristiques des architectures et de la pratique</a></li>
<li><a href="../fr480626/index.html">H√©ritage des syst√®mes et processus h√©rit√©s ou Les 90 premiers jours dans le r√¥le de CTO</a></li>
<li><a href="../fr480642/index.html">Introduction aux ELF Linux: Comprendre et analyser</a></li>
<li><a href="../fr480644/index.html">Le manifeste sur l'abolition de 146 du Code p√©nal et le boycott de la Sberbank et des d√©tenteurs de droits d'auteur-parasites. Pour l'open source et nginx</a></li>
<li><a href="../fr480646/index.html">Habr - meilleurs articles, auteurs et statistiques 2019</a></li>
<li><a href="../fr480650/index.html">Dont les cheveux sont plus forts: morphologie des cheveux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>