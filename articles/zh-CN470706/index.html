<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸŒ¡ï¸ ğŸ ğŸ•– Python + Keras + LSTMï¼šåŠå°æ—¶å†…å®Œæˆæ–‡æœ¬ç¿»è¯‘ ğŸ˜¡ ğŸ¦ˆ ğŸŒ¦ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="å“ˆHa 

 åœ¨ä¸Šä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ç€çœ¼äºåŸºäºç¥ç»ç½‘ç»œåˆ›å»ºç®€å•çš„æ–‡æœ¬è¯†åˆ«ã€‚ ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œå¹¶ç¼–å†™ä»è‹±è¯­åˆ°å¾·è¯­çš„æ–‡æœ¬è‡ªåŠ¨ç¿»è¯‘å™¨ã€‚ 



 å¯¹äºé‚£äº›å¯¹å®ƒçš„å·¥ä½œæ–¹å¼æ„Ÿå…´è¶£çš„äººï¼Œè¯·æ³¨æ„ç»†èŠ‚ã€‚ 

 æ³¨æ„ ï¼šè¿™ä¸ªä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œç¿»è¯‘çš„é¡¹ç›®å®Œå…¨æ˜¯æ•™è‚²æ€§çš„ï¼Œå› æ­¤ä¸è€ƒè™‘â€œä¸ºä»€ä¹ˆâ€é—®é¢˜ã€‚ åªæ˜¯ä¸ºäº†å¥½ç©ã€‚ æˆ‘...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTMï¼šåŠå°æ—¶å†…å®Œæˆæ–‡æœ¬ç¿»è¯‘</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/"> å“ˆHa <br><br> åœ¨<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ä¸Šä¸€éƒ¨åˆ†ä¸­ï¼Œ</a>æˆ‘ç€çœ¼äºåŸºäºç¥ç»ç½‘ç»œåˆ›å»ºç®€å•çš„æ–‡æœ¬è¯†åˆ«ã€‚ ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œå¹¶ç¼–å†™ä»è‹±è¯­åˆ°å¾·è¯­çš„æ–‡æœ¬è‡ªåŠ¨ç¿»è¯‘å™¨ã€‚ <br><br><img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br><br> å¯¹äºé‚£äº›å¯¹å®ƒçš„å·¥ä½œæ–¹å¼æ„Ÿå…´è¶£çš„äººï¼Œè¯·æ³¨æ„ç»†èŠ‚ã€‚ <br><a name="habracut"></a><br>  <i>æ³¨æ„</i> ï¼šè¿™ä¸ªä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œç¿»è¯‘çš„é¡¹ç›®å®Œå…¨æ˜¯æ•™è‚²æ€§çš„ï¼Œå› æ­¤ä¸è€ƒè™‘â€œä¸ºä»€ä¹ˆâ€é—®é¢˜ã€‚ åªæ˜¯ä¸ºäº†å¥½ç©ã€‚ æˆ‘æ²¡æœ‰å¼€å§‹è¯æ˜è¿™ç§æ–¹æ³•çš„ä¼˜åŠ£ï¼Œåªæ˜¯æ£€æŸ¥å‘ç”Ÿçš„äº‹æƒ…å¾ˆæœ‰è¶£ã€‚ å½“ç„¶ï¼Œä¸‹é¢ä½¿ç”¨çš„æ–¹æ³•å¾—åˆ°äº†ç®€åŒ–ï¼Œä½†æ˜¯æˆ‘å¸Œæœ›æ²¡æœ‰äººå¸Œæœ›æˆ‘ä»¬åœ¨åŠå°æ—¶å†…å†™å‡ºç¬¬äºŒæœ¬Lingvoã€‚ <br><br><h2> èµ„æ–™æ”¶é›† </h2><br> åœ¨ç½‘ç»œä¸Šæ‰¾åˆ°çš„åŒ…å«ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„è‹±è¯­å’Œå¾·è¯­çŸ­è¯­çš„æ–‡ä»¶ç”¨ä½œæºæ•°æ®é›†ã€‚ ä¸€ç»„çŸ­è¯­å¦‚ä¸‹æ‰€ç¤ºï¼š <br><br><pre><code class="python hljs">Hi. Hallo! Hi. GrÃ¼ÃŸ Gott! Run! Lauf! Wow! Potzdonner! Wow! Donnerwetter! Fire! Feuer! Help! Hilfe! Help! Zu HÃ¼lf! Stop! Stopp! Wait! Warte! Go on. Mach weiter. Hello! Hallo! I ran. Ich rannte. I see. Ich verstehe. ...</code> </pre> <br> è¯¥æ–‡ä»¶åŒ…å«19.2ä¸‡è¡Œï¼Œå¤§å°ä¸º13 MBã€‚ æˆ‘ä»¬å°†æ–‡æœ¬åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¹¶å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªå—ï¼Œåˆ†åˆ«ç”¨äºè‹±è¯­å’Œå¾·è¯­å•è¯ã€‚ <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, mode=<span class="hljs-string"><span class="hljs-string">'rt'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> file: text = file.read() sents = text.strip().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sents] data = read_text(<span class="hljs-string"><span class="hljs-string">"deutch.txt"</span></span>) deu_eng = np.array(data) deu_eng = deu_eng[:<span class="hljs-number"><span class="hljs-number">30000</span></span>,:] print(<span class="hljs-string"><span class="hljs-string">"Dictionary size:"</span></span>, deu_eng.shape) <span class="hljs-comment"><span class="hljs-comment"># Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower()</span></span></code> </pre><br> æˆ‘ä»¬è¿˜å°†æ‰€æœ‰å•è¯éƒ½è½¬æ¢ä¸ºå°å†™å¹¶åˆ é™¤äº†æ ‡ç‚¹ç¬¦å·ã€‚ <br><br> ä¸‹ä¸€æ­¥æ˜¯ä¸ºç¥ç»ç½‘ç»œå‡†å¤‡æ•°æ®ã€‚ ç½‘ç»œä¸çŸ¥é“ä»€ä¹ˆæ˜¯å•è¯ï¼Œåªèƒ½ä¸æ•°å­—ä¸€èµ·ä½¿ç”¨ã€‚ å¯¹æˆ‘ä»¬æ¥è¯´å¹¸è¿çš„æ˜¯ï¼Œkeraså·²ç»å†…ç½®äº†Tokenizerç±»ï¼Œè¯¥ç±»ç”¨æ•°å­—ä»£ç æ›¿æ¢äº†å¥å­ä¸­çš„å•è¯ã€‚ <br><br> é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥ç®€å•è¯´æ˜å…¶ç”¨æ³•ï¼š <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences s = <span class="hljs-string"><span class="hljs-string">"To be or not to be"</span></span> eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts([s]) seq = eng_tokenizer.texts_to_sequences([s]) seq = pad_sequences(seq, maxlen=<span class="hljs-number"><span class="hljs-number">8</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'post'</span></span>) print(seq)</code> </pre><br> çŸ­è¯­â€œè¦æˆä¸ºæˆ–ä¸åº”è¯¥â€å°†ç”±æ•°ç»„[1 2 3 4 1 2 0 0]ä»£æ›¿ï¼Œåœ¨è¯¥æ•°ç»„ä¸­ä¸éš¾çŒœæµ‹ï¼Œ1 = toï¼Œ2 = beï¼Œ3 =æˆ–4 = notã€‚ æˆ‘ä»¬å·²ç»å¯ä»¥å°†è¿™äº›æ•°æ®æäº¤åˆ°ç¥ç»ç½‘ç»œã€‚ <br><br><h2> ç¥ç»ç½‘ç»œè®­ç»ƒ </h2><br> æˆ‘ä»¬çš„æ•°æ®å·²ç»è¿‡æ•°å­—åŒ–å¤„ç†ã€‚ æˆ‘ä»¬å°†æ•°ç»„åˆ†ä¸ºä¸¤ä¸ªå—ï¼Œåˆ†åˆ«ç”¨äºè¾“å…¥ï¼ˆè‹±è¯­è¡Œï¼‰å’Œè¾“å‡ºï¼ˆå¾·è¯­è¡Œï¼‰æ•°æ®ã€‚ æˆ‘ä»¬è¿˜å°†å‡†å¤‡ä¸€ä¸ªå•ç‹¬çš„å•å…ƒæ¥éªŒè¯å­¦ä¹ è¿‡ç¨‹ã€‚ <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])</span></span></code> </pre><br> ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹å¹¶å¼€å§‹å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚ å¦‚æ‚¨æ‰€è§ï¼Œç¥ç»ç½‘ç»œåŒ…å«å…·æœ‰å­˜å‚¨å•å…ƒçš„LSTMå±‚ã€‚ å°½ç®¡å®ƒå¯èƒ½ä¼šåœ¨â€œå¸¸è§„â€ç½‘ç»œä¸Šè¿è¡Œï¼Œä½†æ˜¯é‚£äº›å¸Œæœ›çš„äººå¯ä»¥è‡ªè¡Œæ£€æŸ¥ã€‚ <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_vocab, out_vocab, in_timesteps, out_timesteps, n)</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(n)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(out_vocab, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> eng_length, deu_length = <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span> model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number"><span class="hljs-number">512</span></span>) num_epochs = <span class="hljs-number"><span class="hljs-number">40</span></span> model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], trainY.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>), epochs=num_epochs, batch_size=<span class="hljs-number"><span class="hljs-number">512</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, callbacks=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>)</code> </pre><br> åŸ¹è®­æœ¬èº«çœ‹èµ·æ¥åƒè¿™æ ·ï¼š <br><br><img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br><br> å¦‚æ‚¨æ‰€è§ï¼Œè¯¥è¿‡ç¨‹å¹¶ä¸å¿«ï¼Œå¹¶ä¸”åœ¨Core i7 + GeForce 1060ä¸ŠèŠ±è´¹äº†å¤§çº¦åŠå°æ—¶çš„æ—¶é—´æ¥å»ºç«‹3ä¸‡è¡Œã€‚ è®­ç»ƒç»“æŸæ—¶ï¼ˆåªéœ€æ‰§è¡Œä¸€æ¬¡ï¼‰ï¼Œæ¨¡å‹å°†ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œç„¶åå¯ä»¥é‡ç”¨ã€‚ <br><br> ä¸ºäº†è·å¾—ç¿»è¯‘ï¼Œæˆ‘ä»¬ä½¿ç”¨predict_classeså‡½æ•°ï¼Œåœ¨è¯¥å‡½æ•°çš„è¾“å…¥ä¸­æäº¤ä¸€äº›ç®€å•çš„çŸ­è¯­ã€‚  get_wordå‡½æ•°ç”¨äºå°†å•è¯è½¬æ¢ä¸ºæ•°å­—ã€‚ <br><br><pre> <code class="python hljs">model = load_model(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_word</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n, tokenizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.word_index.items(): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index == n: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string"><span class="hljs-string">"the weather is nice today"</span></span>, <span class="hljs-string"><span class="hljs-string">"my name is tom"</span></span>, <span class="hljs-string"><span class="hljs-string">"how old are you"</span></span>, <span class="hljs-string"><span class="hljs-string">"where is the nearest shop"</span></span>]) preds = model.predict_classes(phrs_enc) print(<span class="hljs-string"><span class="hljs-string">"Preds:"</span></span>, preds.shape) print(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer))</code> </pre><br><h2> ç»“æœ </h2><br> ç°åœ¨ï¼Œå®é™…ä¸Šï¼Œæœ€å¥‡æ€ªçš„æ˜¯ç»“æœã€‚ æœ‰è¶£çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œå¦‚ä½•å­¦ä¹ å’Œâ€œè®°ä½â€è‹±è¯­å’Œå¾·è¯­çŸ­è¯­ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚ æˆ‘ä¸“é—¨èŠ±äº†2ä¸ªè¯ç»„æ¯”è¾ƒå®¹æ˜“ï¼Œè€Œç”¨2ä¸ªè¯ç»„åˆ™å¾ˆéš¾çœ‹åˆ°å®ƒä»¬ä¹‹é—´çš„åŒºåˆ«ã€‚ <br><br>  <b>5åˆ†é’Ÿçš„è®­ç»ƒ</b> <br><br>  â€œä»Šå¤©å¤©æ°”å¾ˆå¥½â€-â€œ das ist ist tomâ€ <br>  â€œæˆ‘å«æ±¤å§†â€-â€œæ±¤å§†æ±¤å§†â€ <br>  â€œæ‚¨å‡ å²äº†â€-â€œæ‚¨çš„å¹´é¾„â€ <br>  â€œæœ€è¿‘çš„å•†åº—åœ¨å“ªé‡Œï¼Ÿâ€â€œ wo ist derâ€ <br><br> å¦‚æ‚¨æ‰€è§ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œâ€œç‚¹å‡»æ•°â€å¾ˆå°‘ã€‚ çŸ­è¯­â€œæ‚¨å‡ å²â€çš„ä¸€ä¸ªç‰‡æ®µå°†ç¥ç»ç½‘ç»œä¸çŸ­è¯­â€œæ‚¨å¥½å—â€ç›¸æ··æ·†ï¼Œå¹¶äº§ç”Ÿäº†ç¿»è¯‘â€œ wie geht ist es esâ€ï¼ˆæ‚¨å¥½å—ï¼Ÿï¼‰ã€‚ ç¥ç»ç½‘ç»œåœ¨çŸ­è¯­â€œå“ªé‡Œæ˜¯â€¦â€¦â€ä¸­ä»…è¯†åˆ«åŠ¨è¯â€œå“ªé‡Œâ€ï¼Œå¹¶äº§ç”Ÿç¿»è¯‘â€œ wo ist derâ€ï¼ˆå®ƒåœ¨å“ªé‡Œï¼Ÿï¼‰ï¼Œè¿™åœ¨åŸåˆ™ä¸Šå¹¶éæ²¡æœ‰æ„ä¹‰ã€‚ é€šå¸¸ï¼Œå®ƒè¿˜ä¼šåœ¨A1ç»„ä¸­ç¿»è¯‘æˆå¾·å›½æ–°æ‰‹ï¼›ï¼‰ <br><br>  <b>10åˆ†é’Ÿçš„è®­ç»ƒ</b> <br><br>  â€œä»Šå¤©å¤©æ°”å¾ˆå¥½â€-â€œ das haus ist bereitâ€ <br>  â€œæˆ‘çš„åå­—å«æ±¤å§†â€-â€œ meinheiÃŸeheiÃŸetomâ€ <br>  â€œä½ å‡ å²äº†â€-â€œæ€€ç‰¹Â·è¾›å¾·Â·è¥¿â€ <br>  â€œæœ€è¿‘çš„å•†åº—åœ¨å“ªé‡Œâ€-â€œå·´é»å¦‡å¥³â€ <br><br> å¯è§ä¸€äº›è¿›å±•ã€‚ ç¬¬ä¸€ä¸ªçŸ­è¯­å®Œå…¨ä¸åˆé€‚ã€‚ åœ¨ç¬¬äºŒä¸ªçŸ­è¯­ä¸­ï¼Œç¥ç»ç½‘ç»œâ€œå­¦ä¹ â€äº†åŠ¨è¯heiÃŸenï¼ˆç§°ä¸ºï¼‰ï¼Œä½†â€œ meinheiÃŸeheiÃŸetomâ€ä»ç„¶æ˜¯é”™è¯¯çš„ï¼Œå°½ç®¡æ‚¨å·²ç»å¯ä»¥çŒœæµ‹å…¶å«ä¹‰äº†ã€‚ ç¬¬ä¸‰ä¸ªçŸ­è¯­å·²ç»æ­£ç¡®ã€‚ åœ¨ç¬¬å››éƒ¨åˆ†ä¸­ï¼Œæ­£ç¡®çš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯â€œå¥³å£«â€ï¼Œä½†æœ€è¿‘çš„å•†åº—ç”±äºæŸç§åŸå› è¢«å·´é»æ‰€å–ä»£ã€‚ <br><br>  <b>30åˆ†é’Ÿçš„è®­ç»ƒ</b> <br><br>  â€œä»Šå¤©å¤©æ°”å¾ˆå¥½â€-â€œ das ist ist ausâ€ <br>  â€œæˆ‘çš„åå­—å«æ±¤å§†â€-â€œâ€æ±¤å§†â€œ ist mein nameâ€ <br>  â€œä½ å‡ å²äº†â€-â€œæ€€ç‰¹Â·è¾›å¾·Â·è¥¿â€ <br>  â€œæœ€è¿‘çš„å•†åº—åœ¨å“ªé‡Œï¼Ÿâ€â€œ wo ist derâ€ <br><br> å¦‚æ‚¨æ‰€è§ï¼Œå°½ç®¡è®¾è®¡çœ‹èµ·æ¥æœ‰äº›ä¸åŒï¼Œä½†ç¬¬äºŒå¥è¯å·²ç»å˜å¾—æ­£ç¡®ã€‚ ç¬¬ä¸‰ä¸ªçŸ­è¯­æ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯ç¬¬ä¸€ä¸ªå’Œç¬¬å››ä¸ªçŸ­è¯­å°šæœªâ€œå­¦ä¹ â€ã€‚  <s>ä¸ºäº†èŠ‚çœç”µåŠ›ï¼Œæˆ‘</s>å®Œæˆäº†è¯¥è¿‡ç¨‹ã€‚ <br><br><h2> ç»“è®º </h2><br> å¦‚æ‚¨æ‰€è§ï¼ŒåŸåˆ™ä¸Šè¿™æ˜¯å¯è¡Œçš„ã€‚ æˆ‘æƒ³ä»¥è¿™æ ·çš„é€Ÿåº¦è®°ä½ä¸€ç§æ–°è¯­è¨€ï¼šï¼‰å½“ç„¶ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œç»“æœè¿˜ä¸æ˜¯å¾ˆå®Œç¾ï¼Œä½†æ˜¯åœ¨å…¨å¥—19ä¸‡è¡Œçš„è®­ç»ƒä¸Šå°†èŠ±è´¹ä¸€ä¸ªå¤šå°æ—¶ã€‚ <br><br> å¯¹äºé‚£äº›æƒ³è‡ªå·±å°è¯•çš„äººï¼Œæºä»£ç ä½äºç ´åè€…çš„ä¸‹é¢ã€‚ è¯¥ç¨‹åºç†è®ºä¸Šå¯ä»¥ä½¿ç”¨ä»»ä½•ä¸€å¯¹è¯­è¨€ï¼Œä¸ä»…æ˜¯è‹±è¯­å’Œå¾·è¯­ï¼ˆæ–‡ä»¶åº”é‡‡ç”¨UTF-8ç¼–ç ï¼‰ã€‚ ç¿»è¯‘è´¨é‡çš„é—®é¢˜ä¹Ÿä»ç„¶å­˜åœ¨ï¼Œæœ‰å¾…æ£€éªŒã€‚ <br><br><div class="spoiler">  <b class="spoiler_title">keras_translate.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Force CPU os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed import string import re import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM, Embedding, RepeatVector from keras.preprocessing.text import Tokenizer from keras.callbacks import ModelCheckpoint from keras.preprocessing.sequence import pad_sequences from keras.models import load_model from keras import optimizers from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', 200) # Read raw text file def read_text(filename): with open(filename, mode='rt', encoding='utf-8') as file: text = file.read() sents = text.strip().split('\n') return [i.split('\t') for i in sents] data = read_text("deutch.txt") deu_eng = np.array(data) deu_eng = deu_eng[:30000,:] print("Dictionary size:", deu_eng.shape) # Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # Convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower() # Prepare English tokenizer eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts(deu_eng[:, 0]) eng_vocab_size = len(eng_tokenizer.word_index) + 1 eng_length = 8 # Prepare Deutch tokenizer deu_tokenizer = Tokenizer() deu_tokenizer.fit_on_texts(deu_eng[:, 1]) deu_vocab_size = len(deu_tokenizer.word_index) + 1 deu_length = 8 # Encode and pad sequences def encode_sequences(tokenizer, length, lines): # integer encode sequences seq = tokenizer.texts_to_sequences(lines) # pad sequences with 0 values seq = pad_sequences(seq, maxlen=length, padding='post') return seq # Split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # Prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # Prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1]) # Build NMT model def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n): model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(n)) model.add(Dropout(0.3)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=True)) model.add(Dropout(0.3)) model.add(Dense(out_vocab, activation='softmax')) model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy') return model print("deu_vocab_size:", deu_vocab_size, deu_length) print("eng_vocab_size:", eng_vocab_size, eng_length) # Model compilation (with 512 hidden units) model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512) # Train model num_epochs = 250 history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1) # plt.plot(history.history['loss']) # plt.plot(history.history['val_loss']) # plt.legend(['train','validation']) # plt.show() model.save('en-de-model.h5') # Load model model = load_model('en-de-model.h5') def get_word(n, tokenizer): if n == 0: return "" for word, index in tokenizer.word_index.items(): if index == n: return word return "" phrs_enc = encode_sequences(eng_tokenizer, eng_length, ["the weather is nice today", "my name is tom", "how old are you", "where is the nearest shop"]) print("phrs_enc:", phrs_enc.shape) preds = model.predict_classes(phrs_enc) print("Preds:", preds.shape) print(preds[0]) print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer)) print(preds[1]) print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer)) print(preds[2]) print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer)) print(preds[3]) print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer)) print()</span></span></code> </pre><br></div></div><br> è¯å…¸æœ¬èº«å¤ªå¤§ï¼Œæ— æ³•é™„åŠ åˆ°æ–‡ç« ä¸­ï¼Œé“¾æ¥ä½äºæ³¨é‡Šä¸­ã€‚ <br><br> ä¸å¾€å¸¸ä¸€æ ·ï¼Œæ‰€æœ‰æˆåŠŸçš„å®éªŒã€‚ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN470706/">https://habr.com/ru/post/zh-CN470706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN470688/index.html">å…³äºVMworld 2019çš„äº†è§£</a></li>
<li><a href="../zh-CN470692/index.html">æˆ‘ä»¬å¦‚ä½•å»ºç«‹æ–°çš„ç½—æ–¯é“¶è¡Œç½‘ç«™ï¼Œä»¥åŠå®ƒçš„æ¥é¾™å»è„‰</a></li>
<li><a href="../zh-CN470694/index.html">é€‰æ‹©ç”µå­é‚®ä»¶è¥é”€å¹³å°ï¼šä¿„ç½—æ–¯å…¬å¸åº”æ³¨æ„çš„äº‹é¡¹</a></li>
<li><a href="../zh-CN470696/index.html">Kaldiä¸ºä»€ä¹ˆå¯¹è¯­éŸ³è¯†åˆ«æœ‰å¥½å¤„ï¼Ÿ ï¼ˆæ›´æ–°12.25.2019ï¼‰</a></li>
<li><a href="../zh-CN470700/index.html">æ¡Œé¢ é‡‘å±çš„ æ— å£°çš„ ä½ çš„</a></li>
<li><a href="../zh-CN470710/index.html">æœºå™¨å­¦ä¹ åŠ©æ‚¨ä¸€è‡‚ä¹‹åŠ›ã€‚ ç¬¬äºŒéƒ¨åˆ†</a></li>
<li><a href="../zh-CN470714/index.html">æˆ‘å¦‚ä½•å‚åŠ æ•°å­—çªç ´å†³èµ›</a></li>
<li><a href="../zh-CN470718/index.html">äººç±»è¯­è¨€ä¸­çš„â€œä»£æ•°æ•ˆåº”â€</a></li>
<li><a href="../zh-CN470720/index.html">å¦‚ä½•ä½¿ç”¨Python on Ontologyç¼–å†™æ™ºèƒ½åˆçº¦ï¼Ÿ ç¬¬2éƒ¨åˆ†ï¼šå­˜å‚¨API</a></li>
<li><a href="../zh-CN470722/index.html">å¦‚ä½•ä½¿ç”¨Python on Ontologyç¼–å†™æ™ºèƒ½åˆçº¦ï¼Ÿ ç¬¬3éƒ¨åˆ†ï¼šè¿è¡Œæ—¶API</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>