<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍👩‍👦 🎸 👨🏿‍🔧 Redes neurais e aprendizagem profunda, capítulo 1: usando redes neurais para reconhecer números manuscritos 🧝 👨🏼‍💻 🚷</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota 
 Aqui está uma tradução do livro on-line gratuito de Michael Nielsen, Neural Networks and Deep Learning, distribuído sob a Licença Unported Crea...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neurais e aprendizagem profunda, capítulo 1: usando redes neurais para reconhecer números manuscritos</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456738/"><h3>  Nota </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/d1f/7ac/ee5/d1f7acee5600a381c43f05c7df9c439a.jpg" alt="Michael nielsen" align="left">  Aqui está uma tradução do livro on-line gratuito de Michael Nielsen, Neural Networks and Deep Learning, distribuído sob a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Licença Unported Creative Commons Attribution-NonCommercial 3.0</a> .  A motivação para sua criação foi a experiência bem-sucedida da tradução de um livro didático de programação, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Expressive JavaScript</a> .  O livro sobre redes neurais também é bastante popular: autores de artigos em inglês o citam ativamente.  Não encontrei as traduções dela, exceto a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tradução do começo do primeiro capítulo com abreviações</a> . <br><br>  Quem quiser agradecer ao autor do livro pode fazer isso em sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">página oficial</a> , por transferência via PayPal ou Bitcoin.  Para apoiar o tradutor em Habré, existe um formulário "para apoiar o autor". <br><br><div class="spoiler">  <b class="spoiler_title">Conteúdo</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Capítulo 1: usando redes neurais para reconhecer números manuscritos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Capítulo 2: como o algoritmo de retropropagação funciona</a> </li><li>  Capítulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1: aprimorando o método de treinamento de redes neurais</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: Por que a regularização ajuda a reduzir a reciclagem?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 3: como escolher hiperparâmetros de redes neurais?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Capítulo 4: prova visual de que as redes neurais são capazes de computar qualquer função</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Capítulo 5: por que as redes neurais profundas são tão difíceis de treinar?</a> </li><li>  Capítulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1: Aprendizado Profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: progresso recente no reconhecimento de imagens</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Posfácio: existe um algoritmo simples para criar inteligência?</a> </li></ul></div></div><br><h2>  1. Introdução </h2><br>  Este tutorial apresentará detalhes sobre conceitos como: <br><br><ul><li>  Redes neurais - um excelente paradigma de software, criado sob a influência da biologia e permitindo que o computador aprenda com base em observações. </li><li>  O aprendizado profundo é um poderoso conjunto de técnicas de treinamento em redes neurais. </li></ul><br>  As redes neurais (NS) e o aprendizado profundo (GO) hoje oferecem a melhor solução para muitos problemas nas áreas de reconhecimento de imagem, processamento de voz e linguagem natural.  Este tutorial ensinará muitos dos principais conceitos que sustentam o NS e o GO. <br><a name="habracut"></a><br><h2>  Sobre o que é este livro </h2><br>  O NS é um dos melhores paradigmas de software já inventados pelo homem.  Com uma abordagem de programação padrão, dizemos ao computador o que fazer, dividimos grandes tarefas em muitas pequenas e determinamos com precisão as tarefas que o computador executará facilmente.  No caso da Assembléia Nacional, pelo contrário, não dizemos ao computador como resolver o problema.  Ele mesmo aprende isso com base em "observações" dos dados, "inventando" sua própria solução para o problema. <br><br>  O aprendizado automatizado baseado em dados parece promissor.  No entanto, até 2006, não sabíamos como treinar a Assembléia Nacional para que eles pudessem transcender as abordagens mais tradicionais, com exceção de alguns casos especiais.  Em 2006, técnicas de treinamento dos chamados  redes neurais profundas (GNS).  Agora, essas técnicas são conhecidas como aprendizado profundo (GO).  Eles continuaram sendo desenvolvidos e hoje o GNS e o GO alcançaram resultados surpreendentes em muitas tarefas importantes relacionadas à visão por computador, reconhecimento de fala e processamento de linguagem natural.  Em larga escala, eles estão sendo implantados por empresas como Google, Microsoft e Facebook. <br><br>  O objetivo deste livro é ajudá-lo a dominar os principais conceitos de redes neurais, incluindo técnicas modernas de GO.  Depois de trabalhar com o tutorial, você escreverá um código que usa NS e GO para resolver problemas complexos de reconhecimento de padrões.  Você terá uma base para usar o NS e a defesa civil na abordagem para resolver seus próprios problemas. <br><br><h3>  Abordagem Baseada em Princípios </h3><br>  Uma das crenças subjacentes ao livro é que é melhor adquirir uma sólida compreensão dos princípios-chave da Assembléia Nacional e da Sociedade Civil do que obter conhecimento de uma longa lista de idéias diferentes.  Se você tiver um bom entendimento das ideias-chave, entenderá rapidamente outro material novo.  Na linguagem do programador, podemos dizer que estudaremos a sintaxe básica, as bibliotecas e as estruturas de dados da nova linguagem.  Você pode reconhecer apenas uma pequena fração de todo o idioma - muitos idiomas têm imensas bibliotecas padrão - no entanto, é possível entender novas bibliotecas e estruturas de dados de maneira rápida e fácil. <br><br>  Portanto, este livro não é categoricamente material educacional sobre como usar qualquer biblioteca específica para a Assembléia Nacional.  Se você quer apenas aprender a trabalhar com a biblioteca - não leia o livro!  Encontre a biblioteca que você precisa e trabalhe com materiais e documentação de treinamento.  Mas lembre-se: embora essa abordagem tenha a vantagem de solucionar o problema instantaneamente, se você quiser entender exatamente o que está acontecendo dentro da Assembléia Nacional, se quiser dominar idéias que serão relevantes em muitos anos, não será suficiente apenas estudar algumas biblioteca de moda.  Você precisa entender as idéias confiáveis ​​e de longo prazo subjacentes ao trabalho da Assembléia Nacional.  A tecnologia vem e vai, e as idéias duram para sempre. <br><br><h3>  Abordagem prática </h3><br>  Estudaremos os princípios básicos pelo exemplo de uma tarefa específica: ensinar um computador a reconhecer números manuscritos.  Usando abordagens de programação tradicionais, essa tarefa é extremamente difícil de resolver.  No entanto, podemos resolvê-lo muito bem com um NS simples e várias dezenas de linhas de código, sem nenhuma biblioteca especial.  Além disso, melhoraremos gradualmente esse programa, incluindo consistentemente nele mais e mais idéias importantes sobre a Assembléia Nacional e a Defesa Civil. <br><br>  Essa abordagem prática significa que você precisará de alguma experiência em programação.  Mas você não precisa ser um programador profissional.  Eu escrevi o código python (versão 2.7) que deve ficar claro mesmo se você não tiver escrito programas python.  No processo de estudo, criaremos nossa própria biblioteca para a Assembléia Nacional, que você poderá usar para experimentos e treinamento adicional.  Todo o código pode ser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">baixado aqui</a> .  Depois de terminar o livro, ou no processo de leitura, você pode escolher uma das bibliotecas mais completas para a Assembléia Nacional, adaptada para uso nesses projetos. <br><br>  Os requisitos matemáticos para entender o material são bastante médios.  A maioria dos capítulos possui partes matemáticas, mas geralmente são álgebra elementar e gráficos de funções.  Às vezes, uso matemática mais avançada, mas estruturei o material para que você possa entendê-lo, mesmo que alguns detalhes lhe escapem.  A maior parte da matemática é usada no capítulo 2, que requer um pouco de análise analítica e álgebra linear.  Para aqueles a quem eles não estão familiarizados, começo o capítulo 2 com uma introdução à matemática.  Se você achar difícil, pule o capítulo até o interrogatório.  De qualquer forma, não se preocupe com isso. <br><br>  Um livro raramente é ao mesmo tempo orientado para a compreensão de princípios e uma abordagem prática.  Mas acredito que é melhor estudar com base nas idéias fundamentais da Assembléia Nacional.  Vamos escrever código de trabalho, e não apenas estudar a teoria abstrata, e você pode explorar e estender esse código.  Dessa forma, você entenderá o básico, tanto a teoria quanto a prática, e poderá aprender mais. <br><br><h3>  Exercícios e tarefas </h3><br>  Os autores de livros técnicos geralmente alertam o leitor que ele simplesmente precisa concluir todos os exercícios e resolver todos os problemas.  Ao ler esses avisos para mim, eles sempre parecem um pouco estranhos.  Algo ruim vai acontecer comigo se eu não realizar exercícios e resolver problemas?  Não é claro.  Apenas economizarei tempo com uma compreensão menos profunda.  Às vezes vale a pena.  Às vezes não. <br><br>  O que vale a pena fazer com este livro?  Aconselho que você tente concluir a maioria dos exercícios, mas não tente resolver a maioria das tarefas. <br><br>  A maioria dos exercícios precisa ser concluída, pois são verificações básicas para o entendimento adequado do material.  Se você não pode executar o exercício com relativa facilidade, deve ter perdido algo fundamental.  Claro, se você está realmente preso a algum tipo de exercício - deixe de lado, talvez isso seja algum tipo de pequeno mal-entendido, ou talvez eu tenha formulado algo mal.  Mas se a maioria dos exercícios lhe causar dificuldades, é provável que você precise reler o material anterior. <br><br>  Tarefas são outra questão.  Eles são mais difíceis do que exercícios, e com alguns você terá dificuldade.  Isso é irritante, mas é claro que a paciência diante de tanta decepção é a única maneira de realmente entender e absorver o assunto. <br><br>  Portanto, eu não recomendo resolver todos os problemas.  Melhor ainda - escolha seu próprio projeto.  Você pode usar o NS para classificar sua coleção de músicas.  Ou para prever o valor dos estoques.  Ou algo mais.  Mas encontre um projeto interessante para você.  E então você pode ignorar as tarefas do livro ou usá-las apenas como inspiração para trabalhar em seu projeto.  Problemas com seu próprio projeto ensinam mais do que trabalhar com inúmeras tarefas.  O envolvimento emocional é um fator chave na conquista do domínio. <br><br>  Obviamente, embora você não tenha um projeto como esse.  Isso é normal.  Resolva tarefas para as quais você sente motivação intrínseca.  Use o material do livro para ajudá-lo a encontrar idéias para projetos criativos pessoais. <br><br><h2>  Capítulo 1 </h2><br>  O sistema visual humano é uma das maravilhas do mundo.  Considere a seguinte sequência de números manuscritos: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  A maioria das pessoas os lê com facilidade, como 504192. Mas essa simplicidade é enganadora.  Em cada hemisfério do cérebro, uma pessoa tem um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">córtex visual primário</a> , também conhecido como V1, que contém 140 milhões de neurônios e dezenas de bilhões de conexões entre eles.  Ao mesmo tempo, não apenas a V1 está envolvida na visão humana, mas toda uma sequência de regiões do cérebro - V2, V3, V4 e V5 - envolvidas no processamento de imagens cada vez mais complexo.  Temos em nossas cabeças um supercomputador sintonizado pela evolução por centenas de milhões de anos e perfeitamente adaptado para entender o mundo visível.  Reconhecer números manuscritos não é tão fácil.  É que nós, surpreendentemente, surpreendentemente bem, reconhecemos o que nossos olhos nos mostram.  Mas quase todo esse trabalho é realizado inconscientemente.  E geralmente não atribuímos importância à tarefa difícil que nossos sistemas visuais resolvem. <br><br>  A dificuldade de reconhecer padrões visuais se torna aparente quando você tenta escrever um programa de computador para reconhecer números como os acima.  O que parece fácil em nossa execução de repente acaba sendo extremamente complexo.  O conceito simples de como reconhecemos as formas - “o nove tem um loop no topo e a barra vertical no canto inferior direito” - não é tão simples para uma expressão algorítmica.  Ao tentar articular essas regras claramente, você rapidamente fica preso em um atoleiro de exceções, armadilhas e ocasiões especiais.  A tarefa parece sem esperança. <br><br>  Abordagem NS para resolver o problema de uma maneira diferente.  A idéia é pegar os muitos números manuscritos conhecidos como exemplos de ensino, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a67/2ff/15c/a672ff15c58d9672b9f5d2b427a20eb6.png"><br><br>  e desenvolver um sistema que possa aprender com esses exemplos.  Em outras palavras, a Assembléia Nacional usa exemplos para construir automaticamente regras de reconhecimento de dígitos manuscritas.  Além disso, aumentando o número de exemplos de treinamento, a rede pode aprender mais sobre números manuscritos e melhorar sua precisão.  Portanto, embora eu tenha citado acima apenas 100 estudos de caso, talvez possamos criar um melhor sistema de reconhecimento de escrita usando milhares ou mesmo milhões e bilhões de estudos de caso. <br><br>  Neste capítulo, escreveremos um programa de computador que implementa o NS aprendendo a reconhecer números manuscritos.  O programa terá apenas 74 linhas e não utilizará bibliotecas especiais para a Assembléia Nacional.  No entanto, este pequeno programa poderá reconhecer números manuscritos com uma precisão de mais de 96%, sem a necessidade de intervenção humana.  Além disso, em capítulos futuros, desenvolveremos idéias que podem melhorar a precisão para 99% ou mais.  De fato, os melhores NSs comerciais fazem um trabalho tão bom que são usados ​​pelos bancos para processar cheques e o serviço postal para reconhecer endereços. <br><br>  Nós nos concentramos no reconhecimento de manuscrito, pois esse é um grande protótipo de uma tarefa para o estudo da NS.  Esse protótipo é ideal para nós: é uma tarefa difícil (reconhecer números manuscritos não é uma tarefa fácil), mas não é tão complicado que requer uma solução extremamente complexa ou imenso poder de computação.  Além disso, essa é uma ótima maneira de desenvolver técnicas mais complexas, como o GO.  Portanto, no livro, retornaremos constantemente à tarefa de reconhecimento de manuscrito.  Posteriormente discutiremos como essas idéias podem ser aplicadas a outras tarefas de visão computacional, reconhecimento de fala, processamento de linguagem natural e outras áreas. <br><br>  Obviamente, se o objetivo deste capítulo fosse apenas escrever um programa para reconhecer números manuscritos, o capítulo seria muito mais curto!  No entanto, no processo, desenvolveremos muitas idéias-chave relacionadas ao SN, incluindo dois tipos importantes de neurônios artificiais ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perceptron</a> e neurônio sigmóide) e o algoritmo padrão de aprendizado do NS, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descida do gradiente estocástico</a> .  No texto, concentro-me em explicar por que tudo é feito dessa maneira e em moldar sua compreensão da Assembléia Nacional.  Isso requer uma conversa mais longa do que se eu tivesse acabado de apresentar a mecânica básica do que está acontecendo, mas custa um entendimento mais profundo que você terá.  Entre outras vantagens - até o final do capítulo, você entenderá o que é uma defesa civil e por que é tão importante. <br><br><h3>  Perceptrons </h3><br>  O que é uma rede neural?  Para começar, falarei sobre um tipo de neurônio artificial chamado perceptron.  Os Perceptrons foram inventados pelo cientista <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Frank Rosenblatt</a> nos anos 50 e 60, inspirados nos primeiros trabalhos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Warren McCallock</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Walter Pitts</a> .  Hoje, outros modelos de neurônios artificiais são usados ​​com mais frequência - neste livro, e a maioria dos trabalhos modernos sobre NS usam principalmente o modelo sigmóide do neurônio.  Nós a encontraremos em breve.  Mas, para entender por que os neurônios sigmóides são definidos dessa maneira, vale a pena gastar tempo analisando o perceptron. <br><br>  Então, como os perceptrons funcionam?  O perceptron recebe vários números binários x <sub>1</sub> , x <sub>2</sub> , ... e fornece um número binário: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/293/1f3/f5c/2931f3f5c6125d14262954583868f959.png"><br><br>  Neste exemplo, o perceptron possui três números de entrada, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Em geral, pode haver mais ou menos deles.  Rosenblatt propôs uma regra simples para o cálculo do resultado.  Ele introduziu pesos, w <sub>1</sub> , w <sub>2</sub> , números reais, expressando a importância dos números de entrada correspondentes para os resultados.  A saída de um neurônio, 0 ou 1, é determinada pelo fato de uma soma ponderada ser menor ou maior que um determinado limiar [limiar] <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mtext" id="MJXp-Span-2">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-5"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-7" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-8"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-10" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-13" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.764ex" height="2.057ex" viewBox="0 -520.7 4634.5 885.9" role="img" focusable="false" style="vertical-align: -0.848ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(2562,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(3670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-1"> \ sum_j w_jx_j </script>  .  Como pesos, o limiar é um número real, um parâmetro de um neurônio.  Em termos matemáticos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-14"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mo" id="MJXp-Span-21" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27"><font style="vertical-align: inherit;">n </font></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;">c </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;">um </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31"><font style="vertical-align: inherit;">s </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32"><font style="vertical-align: inherit;">e </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33"><font style="vertical-align: inherit;">s</font></span></span><span class="MJXp-mn" id="MJXp-Span-34"><font style="vertical-align: inherit;"> 0 </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;">s </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40"><font style="vertical-align: inherit;">s </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41"><font style="vertical-align: inherit;">u </font></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">m </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">J </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">w </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-47" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-49" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">x </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-50" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52"><font style="vertical-align: inherit;"> l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54"><font style="vertical-align: inherit;">m </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58"><font style="vertical-align: inherit;">d </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60"><font style="vertical-align: inherit;">L </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62"><font style="vertical-align: inherit;">Q</font></span></font><span class="MJXp-mtext" id="MJXp-Span-22">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mn" id="MJXp-Span-34"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-35">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-38">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-39">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-47" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-49" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-50" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-51">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mspace" id="MJXp-Span-63" style="width: 0em; height: 0em;"></span><span class="MJXp-mn" id="MJXp-Span-64">1</span><span class="MJXp-mtext" id="MJXp-Span-65">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-66">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67">f</span><span class="MJXp-mtext" id="MJXp-Span-68">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-69">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-71">u</span><span class="MJXp-msubsup" id="MJXp-Span-72"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-73" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-74" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-75"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-76" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-77" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-78"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-79" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-80" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-81" style="margin-left: 0.333em; margin-right: 0.333em;">&gt;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-82">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-83">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-84">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-85">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-86">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87">e</span><span class="MJXp-mtext" id="MJXp-Span-88">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-90">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-91">d</span><span class="MJXp-mrow" id="MJXp-Span-92"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-93">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-94">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-95">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-96">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-97">s</span></span><span class="MJXp-mtext" id="MJXp-Span-98">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-99">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-100">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-101">g</span><span class="MJXp-mrow" id="MJXp-Span-102"><span class="MJXp-mn" id="MJXp-Span-103">1</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="6.033ex" viewBox="0 -780.1 33095.6 2597.7" role="img" focusable="false" style="vertical-align: -4.222ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(6115,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6F" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-75" x="485" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="1058" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-70" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-75" x="1923" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="2495" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-3D" x="3134" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-62" x="4441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="4870" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-67" x="5337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-69" x="5817" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6E" x="6163" y="0"></use><g transform="translate(6763,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-61" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="963" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="1432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="1899" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-30" x="9132" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="9882" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="10352" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="11318" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-75" x="11788" y="0"></use><g transform="translate(12360,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(13630,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(14738,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6C" x="15953" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-69" x="16251" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6D" x="16597" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-69" x="17475" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="17821" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="18182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-64" x="18649" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="19172" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6C" x="19639" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="19937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-71" x="20404" y="0"></use></g><g transform="translate(8101,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-31" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-69" x="750" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-66" x="1096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="2146" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-75" x="2616" y="0"></use><g transform="translate(3188,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(4458,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(5566,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-3E" x="6808" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6C" x="7865" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-69" x="8163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6D" x="8509" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-69" x="9387" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="9733" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="10094" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="10811" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6E" x="11277" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-64" x="11878" y="0"></use><g transform="translate(12401,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-61" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="963" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="1432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="1899" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="15020" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-61" x="15381" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-67" x="15911" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-31" x="16391" y="0"></use></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> output = \ begin {cases} 0 ~ se ~ \ sum_j w_jx_j \ limite de leq \\ 1 ~ if ~ \ sum_j w_jx_j> limite \ end {cases} \ tag {1} </script></p><br><br>  Essa é a descrição completa do perceptron! <br><br>  Este é o modelo matemático básico.  Um perceptron pode ser pensado como um tomador de decisão, pesando as evidências.  Deixe-me dar um exemplo não muito realista, mas simples.  Digamos que o fim de semana está chegando, e você ouviu que um festival de queijos será realizado em sua cidade.  Você gosta de queijo e tenta decidir se vai ao festival ou não.  Você pode tomar uma decisão ponderando três fatores: <br><br><ol><li>  O tempo está bom? </li><li>  Seu parceiro quer ir com você? </li><li>  O festival está longe do transporte público?  (Você não tem carro). </li></ol><br>  Esses três fatores podem ser representados como variáveis ​​binárias x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Por exemplo, x <sub>1</sub> = 1 se o tempo estiver bom e 0 se estiver ruim.  x <sub>2</sub> = 1 se o seu parceiro deseja ir e 0 se não.  Mesmo para x <sub>3</sub> . <br><br>  Agora, digamos que você é tão fã de queijo que está pronto para ir ao festival, mesmo que seu parceiro não esteja interessado e seja difícil chegar a ele.  Mas talvez você odeie o mau tempo e, em caso de mau tempo, não irá ao festival.  Você pode usar o perceptrons para modelar esse processo de tomada de decisão.  Uma maneira é escolher o peso w <sub>1</sub> = 6 para o clima e w <sub>2</sub> = 2, w <sub>3</sub> = 2 para outras condições.  Um valor maior de w <sub>1</sub> significa que o tempo importa muito mais para você do que se o seu parceiro irá acompanhá-lo ou a proximidade do festival para uma parada.  Finalmente, suponha que você selecione o limite 5. Para o perceptron. Com essas opções, o perceptron implementa o modelo de decisão desejado, fornecendo 1 quando o tempo está bom e 0 quando está ruim.  O desejo do parceiro e a proximidade da parada não afetam o valor da saída. <br><br>  Ao alterar pesos e limites, podemos obter diferentes modelos de tomada de decisão.  Por exemplo, digamos que aceitamos o limiar 3. Então o perceptron decide que você precisa ir ao festival, quando o tempo está bom ou quando o festival está perto de uma parada de ônibus e seu parceiro concorda em ir com você.  Em outras palavras, o modelo é diferente.  Baixar o limiar significa que você deseja ir mais ao festival. <br><br>  Obviamente, o perceptron não é um modelo completo de tomada de decisão humana!  Mas este exemplo mostra como um perceptron pode pesar diferentes tipos de evidência para tomar decisões.  Parece possível que uma rede complexa de perceptrons possa tomar decisões muito complexas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f1/853/0d8/5f18530d8e17d640b3146924f7666032.png"><br><br>  Nesta rede, a primeira coluna de perceptrons - o que chamamos de primeira camada de perceptrons - toma três decisões muito simples, ponderando as evidências de entrada.  E os perceptrons da segunda camada?  Cada um deles toma uma decisão, avaliando os resultados da primeira camada de tomada de decisão.  Dessa maneira, o perceptron da segunda camada pode tomar uma decisão em um nível mais complexo e abstrato em comparação com o perceptron da primeira camada.  E decisões ainda mais complexas podem ser tomadas por perceptrons na terceira camada.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dessa maneira, uma rede multicamada de perceptrons pode lidar com decisões complexas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A propósito, quando determinei o perceptron, eu disse que ele tinha apenas um valor de saída. Mas na rede no topo, os perceptrons parecem ter vários valores de saída. De fato, eles têm apenas uma saída. Muitas setas de saída são apenas uma maneira conveniente de mostrar que a saída do perceptron é usada como entrada de vários outros perceptrons. Isso é menos complicado do que desenhar uma única saída ramificada. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos simplificar a descrição dos perceptrons. Condição</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-104"><span class="MJXp-msubsup" id="MJXp-Span-105"><span class="MJXp-mo" id="MJXp-Span-106" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Σ </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-107" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-108"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-109" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-110" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-111"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-112" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-113" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-114" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> &gt; </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-115"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-117"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um e </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-118"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-120"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-121"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-122"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> estranho, e que pode concordar em duas mudanças para a gravação da sua simplicidade. </font><font style="vertical-align: inherit;">O primeiro é gravar</font></font><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="20.1ex" height="3.142ex" viewBox="0 -832 8654.3 1352.7" role="img" focusable="false" style="vertical-align: -1.209ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJSZ1-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1494" y="-405"></use><g transform="translate(1614,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(2723,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-3E" x="3964" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="5021" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-72" x="5382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="5834" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="6300" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-68" x="6770" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6F" x="7346" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6C" x="7832" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-64" x="8130" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-3">\sum_j w_jx_j > treshold</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-123"><span class="MJXp-msubsup" id="MJXp-Span-124"><span class="MJXp-mo" id="MJXp-Span-125" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Σ </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-126" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-127"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-128" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-129" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-130"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-131" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-132" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> como o produto escalar,</font></font><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.564ex" height="3.142ex" viewBox="0 -832 3687.2 1352.7" role="img" focusable="false" style="vertical-align: -1.209ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJSZ1-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1494" y="-405"></use><g transform="translate(1614,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(2723,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-4">\sum_j w_jx_j</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-133"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-135" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">⋅ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-136"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mo" id="MJXp-Span-137" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-138"><span class="MJXp-mo" id="MJXp-Span-139" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Σ </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-140" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-141"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-142" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-143" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-144"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-145" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-146" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , em que w e x - um vector, cujos componentes são o peso e os dados de entrada, respectivamente. </font><font style="vertical-align: inherit;">O segundo é transferir o limiar para outra parte da desigualdade e substituí-lo por um valor conhecido como deslocamento de perceptron [viés],</font></font><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="16.335ex" height="3.142ex" viewBox="0 -832 7033.2 1352.7" role="img" focusable="false" style="vertical-align: -1.209ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-22C5" x="938" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="1439" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-3D" x="2289" y="0"></use><g transform="translate(3346,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJSZ1-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1494" y="-405"></use></g><g transform="translate(4960,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(6069,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-5">w \cdot x = \sum_j w_jx_j</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-147"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-148"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b </font></font></span><span class="MJXp-mo" id="MJXp-Span-149" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">≡ </font></font></span><span class="MJXp-mo" id="MJXp-Span-150" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-151"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-153"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-156"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-158"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-159"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="15.681ex" height="2.178ex" viewBox="0 -780.1 6751.6 937.7" role="img" focusable="false" style="vertical-align: -0.366ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-62" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-2261" x="707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMAIN-2212" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-74" x="2542" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-68" x="2903" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-72" x="3480" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-65" x="3931" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-73" x="4398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-68" x="4867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6F" x="5444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-6C" x="5929" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgfmyxTZ0G3eIr1GiV7IJ3KhKP_2g#MJMATHI-64" x="6228" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-6">b \equiv −threshold</script>  .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Usando deslocamento em vez de um limite, podemos reescrever a regra do perceptron: </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-160"><span class="MJXp-mtable" id="MJXp-Span-161"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-162" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-163" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-165"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-167"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-168"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mo" id="MJXp-Span-170" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mrow" id="MJXp-Span-171"><span class="MJXp-mo" id="MJXp-Span-172" style="margin-left: 0em; margin-right: 0em; vertical-align: -0.472em;"><span class="MJXp-right MJXp-scale5" style="font-size: 2.889em; margin-left: -0.22em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">{ </font></font></span></span><span class="MJXp-mtable" id="MJXp-Span-173"><span><span class="MJXp-mtr" id="MJXp-Span-174" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-175" style="text-align: left;"><span class="MJXp-mn" id="MJXp-Span-176"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-178"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">⋅ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-183"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-185"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-186" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">≤ </font></span><span class="MJXp-mn" id="MJXp-Span-187"><font style="vertical-align: inherit;">0 </font></span></font><span class="MJXp-mtext" id="MJXp-Span-177">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-178"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-180">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-183"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-185"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-186" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mn" id="MJXp-Span-187"><font style="vertical-align: inherit;"></font></span></span></span><span class="MJXp-mtr" id="MJXp-Span-188" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-189" style="padding-top: 0.2em; text-align: left;"><span class="MJXp-mn" id="MJXp-Span-190"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1 </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-192"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-196" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">⋅ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-198" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-199"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-200" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">&gt; </font></span><span class="MJXp-mn" id="MJXp-Span-201"><font style="vertical-align: inherit;">0</font></span></font><span class="MJXp-mtext" id="MJXp-Span-191">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-192"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-194">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-196" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-198" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-199"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-200" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mn" id="MJXp-Span-201"><font style="vertical-align: inherit;"></font></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-202" style="margin-left: 0em; margin-right: 0em;"></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-7"> output = \begin{cases} 0 ~ if ~ w \cdot x + b \leq 0 \\ 1 ~ if ~ w \cdot x + b > 0 \end{cases} \tag{2} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O deslocamento pode ser representado como uma medida de quão fácil é obter um valor de 1 na saída do perceptron. Ou, em termos biológicos, o deslocamento é uma medida de quão fácil é obter o perceptron para ativar. Um perceptron com um viés muito grande é extremamente fácil de fornecer 1. Mas com um viés negativo muito grande, isso é difícil de executar. Obviamente, a introdução do viés é uma pequena mudança na descrição dos perceptrons, mas mais tarde veremos que isso leva a uma maior simplificação da gravação. Portanto, ainda não usaremos o limite, mas sempre usaremos o deslocamento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Descrevi perceptrons em termos do método de pesar evidências para a tomada de decisão. Outro método de uso é o cálculo de funções lógicas elementares, que geralmente consideramos os principais cálculos, como AND, OR e NAND. Suponha, por exemplo, que tenhamos um perceptron com duas entradas, o peso de cada uma das quais é -2 e seu deslocamento é 3. Aqui está: A </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/530/d45/9a9/530d459a9262c475caf057106a500ddc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entrada 00 fornece a saída 1, porque (−2) ∗ 0 + (- 2 ) ∗ 0 + 3 = 3 é maior que zero. Os mesmos cálculos dizem que as entradas 01 e 10 dão 1. Mas 11 na entrada dá 0 na saída, pois (−2) ∗ 1 + (- 2) ∗ 1 + 3 = −1, menor que zero. Portanto, nosso perceptron implementa a função NAND!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este exemplo mostra que perceptrons podem ser usados ​​para calcular funções lógicas básicas. De fato, podemos usar redes perceptron para calcular quaisquer funções lógicas em geral. O fato é que o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">portão lógico</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> NAND </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">é</font></a><font style="vertical-align: inherit;"> universal para cálculos - é possível construir qualquer cálculo com base em ele. Por exemplo, você pode usar portas NAND para criar um circuito que adiciona dois bits, x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para fazer isso, calcule a soma bit a bit</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-203"><span class="MJXp-msubsup" id="MJXp-Span-204"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-205" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-206" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-207" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ⊕ </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-208"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-209" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-210" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , bem como</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">o sinalizador de transporte</font></a><font style="vertical-align: inherit;">, que é 1 quando ambos x</font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;">e x</font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">são 1 - ou seja, o sinalizador de transporte é simplesmente o resultado da multiplicação por bits x</font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;">x</font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">:</font><font style="vertical-align: inherit;">Para obter a rede equivalente de perceptrons, substituímos todos As portas NAND são perceptrons com duas entradas, o peso de cada uma delas é -2 e com um deslocamento de 3. Aqui está a rede resultante. Observe que movi o perceptron correspondente à válvula inferior direita, apenas para tornar mais conveniente desenhar setas:</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-8"> x_1 \oplus x_2 </script><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d25/af6/afe/d25af6afec499649a0b338f2ccc67f63.png"><br><br><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/222/5b4/71b/2225b471b11b7357777ebc024bd287c2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um aspecto digno de nota dessa rede perceptron é que a saída da mais à esquerda é usada duas vezes como entrada na parte inferior. Definindo o modelo do perceptron, não mencionei a admissibilidade de um esquema de saída dupla no mesmo local. De fato, isso realmente não importa. Se não queremos permitir isso, podemos simplesmente combinar duas linhas com pesos de -2 em uma com um peso de -4. (Se isso não lhe parecer óbvio, pare e prove para si mesmo). Após essa alteração, a rede tem a seguinte aparência, com todos os pesos não alocados iguais a -2, todos os deslocamentos iguais a 3 e um peso -4 é marcado: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/151/d84/dbd/151d84dbdf944cc9016ed656ba70e424.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esse registro de perceptrons que possuem uma saída, mas sem entradas:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/41c/8e3/e46/41c8e3e46587bf536bab96b8427e9bcd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é apenas uma abreviação. Isso não significa que ele não tem entradas. Para entender isso, suponha que tenhamos um perceptron sem entradas. Então a soma ponderada ∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sempre seria zero, portanto o perceptron daria 1 para b&gt; 0 e 0 para b ≤ 0. Ou seja, o perceptron daria apenas um valor fixo, e não o que precisamos (x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no exemplo acima). É melhor considerar os perceptrons de entrada não como perceptrons, mas como unidades especiais que são simplesmente definidas de modo a produzir os valores desejados x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ...</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O exemplo do somador demonstra como uma rede perceptron pode ser usada para simular um circuito contendo muitos portões NAND. E como esses portões são universais para cálculos, portanto, os perceptrons são universais para cálculos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A versatilidade computacional dos perceptrons é encorajadora e decepcionante. É encorajador, garantir que a rede perceptron possa ser tão poderosa quanto qualquer outro dispositivo de computação. Decepcionante, dando a impressão de que perceptrons são apenas um novo tipo de porta lógica NAND. Descoberta mais ou menos!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, a situação é realmente melhor. </font><font style="vertical-align: inherit;">Acontece que podemos desenvolver algoritmos de treinamento que podem ajustar automaticamente os pesos e deslocamentos da rede dos neurônios artificiais. </font><font style="vertical-align: inherit;">Esse ajuste ocorre em resposta a estímulos externos, sem a intervenção direta de um programador. </font><font style="vertical-align: inherit;">Esses algoritmos de aprendizado nos permitem usar neurônios artificiais de uma maneira radicalmente diferente dos portões lógicos comuns. </font><font style="vertical-align: inherit;">Em vez de registrar explicitamente um circuito de portas NAND e outros, nossas redes neurais podem simplesmente aprender a resolver problemas, às vezes aqueles para os quais seria extremamente difícil projetar diretamente um circuito regular.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Neurônios sigmóides </font></font></h3><br>  Os algoritmos de aprendizado são ótimos.  No entanto, como desenvolver esse algoritmo para uma rede neural?  Suponha que tenhamos uma rede de perceptrons que queremos usar para nos treinar na solução de um problema.  Suponha que a entrada na rede possa ser pixels de uma imagem digitalizada de um dígito manuscrito.  E queremos que a rede conheça os pesos e compensações necessárias para classificar corretamente os números.  Para entender como esse treinamento pode funcionar, vamos imaginar que estamos mudando um pouco de peso (ou viés) na rede.  Queremos que essa pequena alteração leve a uma pequena alteração na saída da rede.  Como veremos em breve, essa propriedade torna possível o aprendizado.  Esquematicamente, queremos o seguinte (obviamente, essa rede é muito simples para reconhecer a escrita à mão!): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cf/d97/76f/2cfd9776fdb27a7106bdf2a94d76eb46.png"><br><br>  Se uma pequena alteração no peso (ou viés) levasse a uma pequena alteração no resultado da saída, poderíamos alterar os pesos e os desvios para que nossa rede se comporte um pouco mais perto do que queremos.  Por exemplo, digamos que a rede atribuiu incorretamente a imagem a "8", embora devesse ter sido a "9".  Poderíamos descobrir como fazer uma pequena alteração no peso e no deslocamento para que a rede se aproxime um pouco mais da classificação da imagem como “9”.  E então repetiríamos isso, alterando pesos e mudanças várias vezes para obter o melhor e o melhor resultado.  A rede aprenderia. <br><br>  O problema é que, se houver perceptrons na rede, isso não acontece.  Uma pequena mudança nos pesos ou deslocamento de qualquer perceptron pode às vezes levar a uma mudança na sua saída para o oposto, digamos, de 0 a 1. Essa mudança pode mudar o comportamento do resto da rede de uma maneira muito complicada.  E mesmo que agora o nosso "9" seja corretamente reconhecido, o comportamento da rede com todas as outras imagens provavelmente mudou completamente de uma maneira que é difícil de controlar.  Por isso, é difícil imaginar como podemos ajustar gradualmente pesos e compensações para que a rede se aproxime gradualmente do comportamento desejado.  Talvez haja uma maneira inteligente de contornar esse problema.  Mas não há uma solução simples para o problema de aprender uma rede de perceptrons. <br><br>  Esse problema pode ser contornado com a introdução de um novo tipo de neurônio artificial chamado neurônio sigmóide.  Eles são semelhantes aos perceptrons, mas modificados para que pequenas alterações nos pesos e compensações resultem em apenas pequenas alterações na saída.  Esse é um fato básico que permitirá que a rede de neurônios sigmóides aprenda. <br><br>  Deixe-me descrever um neurônio sigmóide.  Vamos desenhá-los da mesma maneira que perceptrons: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cd/0b3/ccf/2cd0b3ccf36d0dabf96fd15649c29f90.png"><br><br>  Possui a mesma entrada x <sub>1</sub> , x <sub>2</sub> , ... Mas, em vez de serem iguais a 0 ou 1, essas entradas podem ter qualquer valor no intervalo de 0 a 1. Por exemplo, um valor de 0,638 será uma entrada válida para neurônio sigmóide (CH).  Assim como o perceptron, o SN tem pesos para cada entrada, w <sub>1</sub> , w <sub>2</sub> , ... e o viés total b.  Mas seu valor de saída não será 0 ou 1. Ele será σ (w⋅x + b), onde σ é o sigmóide. <br><br>  A propósito, σ às vezes é chamado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">função logística</a> , e essa classe de neurônios é chamada de neurônios logísticos.  É útil lembrar dessa terminologia, pois esses termos são usados ​​por muitas pessoas que trabalham com redes neurais.  No entanto, aderiremos à terminologia sigmóide. <br><br>  A função é definida da seguinte maneira: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-211"><span class="MJXp-mtext" id="MJXp-Span-212">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-213">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-214">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-216">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-217">a</span><span class="MJXp-mo" id="MJXp-Span-218" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-219">z</span><span class="MJXp-mo" id="MJXp-Span-220" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-221">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">q</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-224">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-225">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-226">v</span><span class="MJXp-mtext" id="MJXp-Span-227">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-228">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-229">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-230">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-231">c</span><span class="MJXp-mrow" id="MJXp-Span-232"><span class="MJXp-mn" id="MJXp-Span-233">1</span></span><span class="MJXp-mrow" id="MJXp-Span-234"><span class="MJXp-mn" id="MJXp-Span-235">1</span><span class="MJXp-mo" id="MJXp-Span-236" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-237"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-238" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-239" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-240">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-241">z</span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-242">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-244">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-245">g</span><span class="MJXp-mrow" id="MJXp-Span-246"><span class="MJXp-mn" id="MJXp-Span-247">3</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> \ sigma (z) \ equiv \ frac {1} {1 + e ^ {- z}} \ tag {3} </script></p><br><br>  No nosso caso, o valor de saída do neurônio sigmóide com dados de entrada x <sub>1</sub> , x <sub>2</sub> , ... pelos pesos w <sub>1</sub> , w <sub>2</sub> , ... e deslocamento b será considerado como: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-248"><span class="MJXp-mtext" id="MJXp-Span-249">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-252">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-253">c</span><span class="MJXp-mrow" id="MJXp-Span-254"><span class="MJXp-mn" id="MJXp-Span-255">1</span></span><span class="MJXp-mrow" id="MJXp-Span-256"><span class="MJXp-mn" id="MJXp-Span-257">1</span><span class="MJXp-mo" id="MJXp-Span-258" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-259">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-260">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-261">p</span><span class="MJXp-mo" id="MJXp-Span-262" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-263" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mtext" id="MJXp-Span-264">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-265">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266">u</span><span class="MJXp-msubsup" id="MJXp-Span-267"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-268" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-269" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-270"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-272" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-273"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-274" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-275" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-276" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-277">b</span><span class="MJXp-mo" id="MJXp-Span-278" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mtext" id="MJXp-Span-279">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-281">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-282">g</span><span class="MJXp-mrow" id="MJXp-Span-283"><span class="MJXp-mn" id="MJXp-Span-284">4</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-10"> \ frac {1} {1 + exp (- \ sum_j w_jx_j - b)} \ tag {4} </script></p><br><br>  À primeira vista, o CH parece completamente diferente dos neurônios.  A aparência algébrica de um sigmóide pode parecer confusa e obscura se você não estiver familiarizado com ele.  De fato, existem muitas semelhanças entre perceptrons e SN, e a forma algébrica de um sigmóide acaba sendo mais um detalhe técnico do que uma séria barreira à compreensão. <br><br>  Para entender as semelhanças com o modelo perceptron, suponha que z ≡ w ⋅ x + b seja um número positivo grande.  Então e - z ≈ 0, portanto, σ (z) ≈ 1. Em outras palavras, quando z = w ⋅ x + b é grande e positivo, o rendimento SN é aproximadamente 1, como no perceptron.  Suponha que z = w ⋅ x + b seja grande com um sinal de menos.  Então e - z → ∞ e σ (z) ≈ 0. Portanto, para z grande com sinal de menos, o comportamento do SN também se aproxima do perceptron.  E somente quando w ⋅ x + b tem um tamanho médio, há sérios desvios do modelo de perceptron observado. <br><br>  E a forma algébrica de σ?  Como o entendemos?  De fato, a forma exata de σ não é tão importante - a forma da função no gráfico é importante.  Aqui está: <br><br><img src="https://habrastorage.org/webt/sm/u4/jv/smu4jvbwuriryrfojrpt-ukdr6w.png"><br><br>  Esta é uma versão suave da função step: <br><br><img src="https://habrastorage.org/webt/2i/p0/a-/2ip0a-3cmpstyiwrglx_michl3m.png"><br><br>  Se σ fosse escalonado, o SN seria um perceptron, pois teria saída 0 ou 1 dependendo do sinal w ⋅ x + b (bem, de fato, em z = 0, o perceptron fornece 0 e a função step 1 , portanto, nesse ponto, a função precisaria ser alterada). <br><br>  Usando a função real σ, obtemos um perceptron suavizado.  E o principal aqui é a suavidade da função, não sua forma exata.  Suavidade significa que pequenas alterações Δw <sub>j</sub> pesos e δb deslocamentos fornecerão pequenas alterações Δde saída da saída.  A álgebra nos diz que a saída é bem aproximada da seguinte maneira: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-285"><span class="MJXp-mtext" id="MJXp-Span-286">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">S</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">a</span><span class="MJXp-mrow" id="MJXp-Span-289"><span class="MJXp-mo" id="MJXp-Span-290" style="margin-left: 0em; margin-right: 0em;">í</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-291">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-293">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-297">a</span><span class="MJXp-mtext" id="MJXp-Span-298">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-299">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-302">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-303">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304">x</span><span class="MJXp-mtext" id="MJXp-Span-305">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-306">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307">u</span><span class="MJXp-msubsup" id="MJXp-Span-308"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-309" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-310" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-311">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-314">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-315">c</span><span class="MJXp-mrow" id="MJXp-Span-316"><span class="MJXp-mtext" id="MJXp-Span-317">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-318">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319">a</span><span class="MJXp-mrow" id="MJXp-Span-320"><span class="MJXp-mo" id="MJXp-Span-321" style="margin-left: 0em; margin-right: 0em;">í</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-322">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-323">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-328">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-329">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-330">l</span></span><span class="MJXp-mrow" id="MJXp-Span-331"><span class="MJXp-mtext" id="MJXp-Span-332">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-337">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-338">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-339">l</span><span class="MJXp-msubsup" id="MJXp-Span-340"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-341" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-342" style="vertical-align: -0.4em;">j</span></span></span><span class="MJXp-mtext" id="MJXp-Span-343">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-344">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-345">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-347">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-348">a</span><span class="MJXp-msubsup" id="MJXp-Span-349"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-350" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-351" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-352" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-353">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-354">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-355">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-356">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-357">c</span><span class="MJXp-mrow" id="MJXp-Span-358"><span class="MJXp-mtext" id="MJXp-Span-359">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-361">a</span><span class="MJXp-mrow" id="MJXp-Span-362"><span class="MJXp-mo" id="MJXp-Span-363" style="margin-left: 0em; margin-right: 0em;">í</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-364">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-365"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-366"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-368"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-369"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-370"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-371"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-372"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span></span><font style="vertical-align: inherit;"><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375"><font style="vertical-align: inherit;">p </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377"><font style="vertical-align: inherit;">r </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378"><font style="vertical-align: inherit;">c </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379"><font style="vertical-align: inherit;">i </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-381"><font style="vertical-align: inherit;">l </font></span></span><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382"><font style="vertical-align: inherit;">b</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384"><font style="vertical-align: inherit;"> D </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386"><font style="vertical-align: inherit;">l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-388"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-392"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-393"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mrow" id="MJXp-Span-394"><span class="MJXp-mn" id="MJXp-Span-395"><font style="vertical-align: inherit;">5</font></span></span></font><span class="MJXp-mrow" id="MJXp-Span-373"><span class="MJXp-mtext" id="MJXp-Span-374">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-381"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-383">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-388"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-390">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-392"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-393"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-394"><span class="MJXp-mn" id="MJXp-Span-395"><font style="vertical-align: inherit;"></font></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-11"> \ Saída delta \ approx \ sum_j \ frac {\ saída parcial} {\ parcial w_j} \ Delta w_j + \ frac {\ saída parcial} {\ parcial b} \ Delta b \ tag {5} </script></p><br><br>  Onde a soma está acima de todos os pesos wj, e ∂ saída / ∂w <sub>j</sub> e ∂ saída / ∂b denotam as derivadas parciais da saída em relação a w <sub>j</sub> e b, respectivamente.  Não entre em pânico se você se sentir inseguro na companhia de derivativos privados!  Embora a fórmula pareça complicada, com todas essas derivadas parciais, na verdade ela diz algo bastante simples (e útil): Δ Saída é uma função linear que depende dos pesos e desvios de Δw <sub>j</sub> e Δb.  Sua linearidade facilita a seleção de pequenas alterações de pesos e compensações para atingir qualquer pequeno viés de saída desejado.  Portanto, embora os SNs sejam semelhantes aos perceptrons no comportamento qualitativo, eles facilitam a compreensão de como a saída pode ser alterada alterando pesos e deslocamentos. <br><br>  Se a forma geral σ é importante para nós, e não sua forma exata, então por que usamos essa fórmula (3)?  De fato, mais tarde consideraremos neurônios cuja saída é f (w ⋅ x + b), onde f () é outra função de ativação.  O principal que muda quando a função muda é o valor das derivadas parciais na equação (5).  Acontece que, quando calculamos essas derivadas parciais, o uso de σ simplifica muito a álgebra, pois os expoentes têm propriedades muito boas ao diferenciar.  De qualquer forma, σ é frequentemente usado no trabalho com redes neurais e, mais frequentemente, neste livro, usaremos essa função de ativação. <br><br>  Como interpretar o resultado do trabalho de CH?  Obviamente, a principal diferença entre os perceptrons e o CH é que o CH não fornece apenas 0 ou 1. Sua saída pode ser qualquer número real de 0 a 1, portanto valores como 0,173 ou 0,669 são válidos.  Isso pode ser útil, por exemplo, se você desejar que o valor de saída indique, por exemplo, o brilho médio dos pixels da imagem recebidos na entrada do NS.  Mas, às vezes, pode ser inconveniente.  Suponha que queremos que a saída de rede diga que “a imagem 9 foi inserida” ou “a imagem de entrada não é 9”.  Obviamente, seria mais fácil se os valores de saída fossem 0 ou 1, como um perceptron.  Mas, na prática, podemos concordar que qualquer valor de saída de pelo menos 0,5 significaria "9" na entrada e qualquer valor menor que 0,5 significaria que "não é 9".  Eu sempre indicarei explicitamente a existência de tais acordos. <br><br>  Exercícios <br><br><ul><li>  CH simulando perceptrons, parte 1 </li></ul><br>  Suponha que tomemos todos os pesos e compensações de uma rede de perceptrons e os multipliquemos por uma constante positiva c&gt; 0.  Mostre que o comportamento da rede não muda. <br><br><ul><li>  CH simulando perceptrons, parte 2 </li></ul><br>  Suponha que tenhamos a mesma situação do problema anterior - uma rede de perceptrons.  Suponha também que os dados de entrada para a rede estejam selecionados.  Não precisamos de um valor específico, o principal é que ele seja fixo.  Suponha que os pesos e deslocamentos sejam tais que w⋅x + b b 0, onde x é o valor de entrada de qualquer percepção da rede.  Agora substituímos todos os perceptrons na rede por SN e multiplicamos os pesos e deslocamentos pela constante positiva c&gt; 0.  Mostre que no limite c → ∞ o comportamento da rede a partir do SN será exatamente o mesmo que as redes de perceptrons.  Como esta afirmação será violada se, para um dos perceptrons, w⋅x + b = 0? <br><br><h3>  Arquitetura de rede neural </h3><br>  Na próxima seção, apresentarei uma rede neural capaz de uma boa classificação de números manuscritos.  Antes disso, é útil explicar a terminologia que nos permite apontar para diferentes partes da rede.  Digamos que temos a seguinte rede: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/472/521/00b/47252100b91b8e1a527796217a6ed0fa.png"><br><br>  Como mencionei, a camada mais à esquerda na rede é chamada de camada de entrada e seus neurônios são chamados de neurônios de entrada.  A camada mais à direita, ou saída, contém neurônios de saída ou, como no nosso caso, um neurônio de saída.  A camada do meio é chamada oculta, porque seus neurônios não são entrada nem saída.  O termo "oculto" pode parecer um pouco misterioso - quando o ouvi pela primeira vez, decidi que deveria ter uma profunda importância filosófica ou matemática - no entanto, significa apenas "não entrar nem sair".  A rede acima tem apenas uma camada oculta, mas algumas redes têm várias camadas ocultas.  Por exemplo, na rede de quatro camadas a seguir, existem duas camadas ocultas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/26e/be9/c5f/26ebe9c5f4c7f61413cfa43e67151734.png"><br><br>  Isso pode ser confuso, mas, por razões históricas, essas redes multicamadas são algumas vezes chamadas de perceptrons multicamadas, MLPs, apesar de consistirem em neurônios sigmóides e não em perceptrons.  Não vou usar essa terminologia porque é confusa, mas devo advertir sobre sua existência. <br><br>  Projetar camadas de entrada e saída às vezes é uma tarefa simples.  Por exemplo, digamos que estamos tentando determinar se o número manuscrito significa "9" ou não.  Um circuito de rede natural codifica o brilho dos pixels da imagem nos neurônios de entrada.  Se a imagem for em preto e branco, com 64x64 pixels de tamanho, teremos 64x64 = 4096 neurônios de entrada, com brilho no intervalo de 0 a 1. A camada de saída conterá apenas um neurônio, cujo valor menor que 0,5 significa que "em a entrada não era 9 ", mas valores mais significam que" a entrada era 9 ". <br><br>  E embora projetar camadas de entrada e saída geralmente seja uma tarefa simples, projetar camadas ocultas pode ser uma arte difícil.  Em particular, não é possível descrever o processo de desenvolvimento de camadas ocultas com algumas regras simples.  Pesquisadores da Assembléia Nacional desenvolveram muitas regras heurísticas para o design de camadas ocultas que ajudam a obter o comportamento desejado das redes neurais.  Por exemplo, essa heurística pode ser usada para entender como alcançar um compromisso entre o número de camadas ocultas e o tempo disponível para o treinamento da rede.  Mais tarde, encontraremos algumas dessas regras. <br><br>  Até agora, discutimos NSs em que a saída de uma camada é usada como entrada para a próxima.  Tais redes são chamadas de redes neurais de distribuição direta.  Isso significa que não há loops na rede - as informações sempre avançam e nunca são realimentadas.  Se tivéssemos loops, encontraríamos situações nas quais a entrada sigmóide dependeria da saída.  Seria difícil de entender e não permitimos esses loops. <br><br>  No entanto, existem outros modelos de NSs artificiais nos quais é possível usar loops de feedback.  Esses modelos são chamados de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">redes neurais recorrentes</a> (RNS).  A idéia dessas redes é que seus neurônios sejam ativados por períodos limitados de tempo.  Essa ativação pode estimular outros nêutrons, que podem ser ativados um pouco mais tarde, também por tempo limitado.  Isso leva à ativação dos seguintes neurônios e, com o tempo, obtemos uma cascata de neurônios ativados.  Loops nesses modelos não apresentam problemas, uma vez que a saída de um neurônio afeta sua entrada posteriormente, e não imediatamente. <br><br>  Os RNSs não foram tão influentes quanto os NSs de distribuição direta, principalmente porque os algoritmos de treinamento para RNSs até agora têm menos potencial.  No entanto, o RNS ainda permanece extremamente interessante.  No espírito do trabalho, eles estão muito mais próximos do cérebro do que os NS de distribuição direta.  É possível que o RNS seja capaz de resolver problemas importantes que, com a ajuda da distribuição direta NS, possam ser resolvidos com grandes dificuldades.  No entanto, a fim de limitar o escopo de nosso estudo, nos concentraremos no SN de distribuição direta mais amplamente utilizado. <br><br><h3>  Rede simples de classificação de tinta </h3><br>  Tendo definido as redes neurais, retornaremos ao reconhecimento de manuscrito.  A tarefa de reconhecer números manuscritos pode ser dividida em duas subtarefas.  Primeiro, queremos encontrar uma maneira de dividir uma imagem contendo muitos dígitos em uma sequência de imagens individuais, cada uma contendo um dígito.  Por exemplo, gostaríamos de dividir a imagem <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  em seis separados <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br>  Nós, humanos, podemos resolver facilmente esse problema de segmentação, mas é difícil para um programa de computador dividir a imagem corretamente.  Após a segmentação, o programa precisa classificar cada dígito individual.  Por exemplo, queremos que nosso programa reconheça que o primeiro dígito <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5a/c2a/808/e5ac2a808e18ac02ba0f09b2052fff4f.png"><br><br>  são 5. <br><br>  Vamos nos concentrar na criação de um programa para resolver o segundo problema, a classificação de números individuais.  Acontece que o problema da segmentação não é tão difícil de resolver assim que encontramos uma boa maneira de classificar dígitos individuais.  Existem muitas abordagens para resolver o problema de segmentação.  Uma delas é tentar muitas maneiras diferentes de segmentação de imagens usando o classificador de dígitos individuais, avaliando cada tentativa.  A segmentação de avaliação é muito apreciada se o classificador de dígitos individuais estiver confiante na classificação de todos os segmentos e baixa se houver problemas em um ou mais segmentos.  A idéia é que, se o classificador tiver problemas em algum lugar, isso provavelmente significa que a segmentação está incorreta.  Essa ideia e outras opções podem ser usadas para uma boa solução para o problema de segmentação.  Portanto, em vez de nos preocuparmos com a segmentação, nos concentraremos no desenvolvimento de um NS capaz de resolver uma tarefa mais interessante e complexa, a saber, o reconhecimento de números manuscritos individuais. <br><br>  Para reconhecer dígitos individuais, usaremos o NS de três camadas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/350/3dc/e6b3503dcc1e2848980d17ad8c193018.png"><br><br>  A camada da rede de entrada contém neurônios que codificam vários valores dos pixels de entrada.  Como será indicado na próxima seção, nossos dados de treinamento consistirão em muitas imagens de dígitos manuscritos digitalizados com tamanho de 28x28 pixels, portanto a camada de entrada contém 28x28 = 784 neurônios.  Por simplicidade, não indiquei a maioria dos 784 neurônios no diagrama.  Os pixels recebidos são preto e branco, com um valor de 0,0 indicando branco, 1,0 indicando preto e valores intermediários indicando tons de cinza cada vez mais escuros. <br><br>  A segunda camada da rede está oculta.  Denotamos o número de neurônios nessa camada n e experimentaremos diferentes valores de n.  O exemplo acima mostra uma pequena camada oculta contendo apenas n = 15 neurônios. <br><br>  Existem 10 neurônios na camada de saída da rede.  Se o primeiro neurônio estiver ativado, ou seja, seu valor de saída for ± 1, isso indica que a rede acredita que a entrada foi 0. Se o segundo neurônio estiver ativado, a rede acredita que a entrada foi 1. E assim por diante.  A rigor, numeramos os neurônios de saída de 0 a 9 e observamos quais deles tinham o valor máximo de ativação.  Se este é, digamos, o neurônio nº 6, nossa rede acredita que a entrada foi o número 6. E assim por diante. <br><br>  Você pode se perguntar por que precisamos usar dez neurônios.  Afinal, queremos saber qual dígito de 0 a 9 corresponde à imagem de entrada.  Seria natural usar apenas 4 neurônios de saída, cada um deles com um valor binário, dependendo de seu valor de saída ser mais próximo de 0 ou 1. Quatro neurônios seriam suficientes, já que 2 <sup>4</sup> = 16, mais de 10 valores possíveis.  Por que nossa rede deve usar 10 neurônios?  Isso é ineficaz?  A base para isso é empírica;  podemos tentar as duas variantes da rede e, para essa tarefa, uma rede com 10 neurônios de saída é melhor treinada para reconhecer números do que uma rede com 4.  No entanto, a questão permanece: por que 10 neurônios de saída são melhores?  Existe alguma heurística que nos diga com antecedência que 10 neurônios de saída devem ser usados ​​em vez de 4? <br><br>  Para entender o porquê, é útil pensar no que uma rede neural faz.  Primeiro, considere a opção com 10 neurônios de saída.  Nós nos concentramos no primeiro neurônio de saída, que está tentando decidir se a imagem recebida é zero.  Ele faz isso pesando as evidências obtidas de uma camada oculta.  O que os neurônios ocultos fazem?  Suponha que o primeiro neurônio na camada oculta determine se há algo assim na figura: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/650/c35/402/650c3540204f98b406e25078ddc8742a.png"><br><br>  Ele pode fazer isso atribuindo pesos grandes a pixels correspondentes a esta imagem e pesos pequenos ao restante.  Da mesma forma, suponha que o segundo, terceiro e quarto neurônios na camada oculta procurem se existem fragmentos semelhantes na imagem: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d94/4c3/cea/d944c3cea54d91106d2ee2198d04c801.png"><br><br>  Como você deve ter adivinhado, todos esses quatro fragmentos juntos apresentam a imagem 0, que vimos anteriormente: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f99/a20/1e5/f99a201e557bacdbe39190a6f913b49e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Portanto, se os quatro neurônios ocultos forem ativados, podemos concluir que o número é 0. Claro, essa não é a única evidência de que 0 foi exibido lá - podemos obter 0 de muitas outras maneiras (mudando ligeiramente essas imagens ou distorcendo-as levemente). </font><font style="vertical-align: inherit;">No entanto, podemos dizer com certeza que, pelo menos nesse caso, podemos concluir que havia 0 na entrada.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se assumirmos que a rede funciona assim, podemos dar uma explicação plausível do porquê é melhor usar 10 neurônios de saída em vez de 4. Se tivéssemos 4 neurônios de saída, o primeiro neurônio tentaria decidir qual é o bit mais significativo do dígito recebido. E não há uma maneira fácil de associar o bit mais significativo aos formulários simples fornecidos acima. É difícil imaginar quaisquer razões históricas pelas quais partes da forma de um dígito estariam de alguma forma relacionadas ao bit mais significativo da saída.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, todas as opções acima são suportadas apenas por heurísticas. </font><font style="vertical-align: inherit;">Nada fala a favor do fato de que uma rede de três camadas deve funcionar como eu disse, e os neurônios ocultos devem encontrar componentes simples de formas. </font><font style="vertical-align: inherit;">Talvez o complicado algoritmo de aprendizado encontre pesos que nos permitam usar apenas 4 neurônios de saída. </font><font style="vertical-align: inherit;">No entanto, como heurística, meu método funciona bem e pode economizar um tempo considerável no desenvolvimento de uma boa arquitetura NS.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exercícios </font></font></h3><br><ul><li>      ,      .          ,     .         . ,   3   ,       (  )     0,99,       0,01. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/f5b/4af/2e9/f5b4af2e9acf846ab8cc5c60dac20c03.png"><br><br><h3>     </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Então, nós temos o esquema de NA - como aprender a reconhecer números? A primeira coisa que precisamos é treinar dados, os chamados conjunto de dados de treinamento. Usaremos o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kit MNIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> contendo dezenas de milhares de imagens digitalizadas de números manuscritos e sua classificação correta. O nome MNIST recebeu devido ao fato de ser um subconjunto modificado dos dois conjuntos de dados coletados pelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , o Instituto Nacional de Padrões e Tecnologia dos EUA. Aqui estão algumas imagens do MNIST: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estes são os mesmos números que foram fornecidos no início do capítulo como uma tarefa de reconhecimento. Obviamente, ao verificar o NS, solicitaremos que ela reconheça as imagens erradas que já estavam no conjunto de treinamento!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os dados do MNIST consistem em duas partes. O primeiro contém 60.000 imagens destinadas ao treinamento. São manuscritos digitalizados de 250 pessoas, metade das quais eram funcionários do US Census Bureau e a outra metade eram estudantes do ensino médio. As imagens são em preto e branco, medindo 28x28 pixels. A segunda parte do conjunto de dados MNIST é de 10.000 imagens para testar a rede. Esta também é uma imagem em preto e branco de 28x28 pixels. Usaremos esses dados para avaliar quão bem a rede aprendeu a reconhecer números. Para melhorar a qualidade da avaliação, esses números foram extraídos de outras 250 pessoas que não participaram da gravação do conjunto de treinamento (embora também fossem funcionários do Bureau e estudantes do ensino médio). Isso nos ajuda a garantir que nosso sistema possa reconhecer a letra manuscrita de pessoas que não encontrou durante o treinamento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A entrada de treinamento será indicada por x. Será conveniente tratar cada imagem de entrada x como um vetor com medidas 28x28 = 784. Cada valor dentro do vetor indica o brilho de um pixel na imagem. Denotaremos o valor de saída como y = y (x), onde y é um vetor de dez dimensões. Por exemplo, se uma determinada imagem de treinamento x contiver 6, y (x) = (0,0,0,0,0,0,1,0,0,0) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> será o vetor de que precisamos. T é uma operação de transposição que transforma um vetor de linha em um vetor de coluna. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Queremos encontrar um algoritmo que permita procurar tais pesos e compensações para que a saída da rede se aproxime de y (x) para toda a entrada de treinamento x. Para quantificar a aproximação desse objetivo, definimos uma função de custo (às vezes chamada de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">função de perda)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">; no livro, usaremos a função cost, mas lembre-se de outro nome):</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-396"><span class="MJXp-mtable" id="MJXp-Span-397"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-398" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-399" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400">C</span><span class="MJXp-mo" id="MJXp-Span-401" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-402">w</span><span class="MJXp-mo" id="MJXp-Span-403" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-404">b</span><span class="MJXp-mo" id="MJXp-Span-405" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-406" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mfrac" id="MJXp-Span-407" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-408">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-409">2</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-410">n</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-411"><span class=""><span class="MJXp-mo" id="MJXp-Span-412" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">∑</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-413" style="margin-left: 0px;">x</span></span></span><span class="MJXp-mrow" id="MJXp-Span-414"><span class="MJXp-mo" id="MJXp-Span-415" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mrow" id="MJXp-Span-416"><span class="MJXp-mo" id="MJXp-Span-417" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-418">y</span><span class="MJXp-mo" id="MJXp-Span-419" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-420">x</span><span class="MJXp-mo" id="MJXp-Span-421" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-422" style="margin-left: 0em; margin-right: 0.222em;">–</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-423">a</span><span class="MJXp-mrow" id="MJXp-Span-424"><span class="MJXp-mo" id="MJXp-Span-425" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-msubsup" id="MJXp-Span-426"><span class="MJXp-mrow" id="MJXp-Span-427" style="margin-right: 0.05em;"><span class="MJXp-mo" id="MJXp-Span-428" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-429" style="vertical-align: 0.5em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-12"> C(w, b) = \frac{1}{2n} \sum_x || y(x) – a ||^2 \tag{6} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqui w denota um conjunto de pesos de rede, b é um conjunto de compensações, n é o número de dados de entrada de treinamento, a é o vetor de dados de saída quando x é um dado de entrada e a soma passa por toda a entrada de treinamento x. A saída, é claro, depende de x, we eb, mas por simplicidade, não designei essa dependência. A notação || v || significa o comprimento do vetor v. Vamos chamar C de função de custo quadrático; às vezes também é chamado de erro padrão, ou MSE. Se você olhar atentamente para C, poderá ver que não é negativo, pois todos os membros da soma não são negativos. Além disso, o custo de C (w, b) se torna pequeno, ou seja, C (w, b) ≈ 0, precisamente quando y (x) é aproximadamente igual ao vetor de saída a para todos os dados de entrada de treinamento x. Portanto, nosso algoritmo funcionou bem se conseguíssemos encontrar pesos e compensações tais que C (w, b) ≈ 0. E vice-versa, funcionou mal quando C (w,b) grande - significa que y (x) não corresponde à saída para uma grande quantidade de entrada. Acontece que o objetivo do algoritmo de treinamento é minimizar o custo de C (w, b) em função de pesos e compensações. Em outras palavras, precisamos encontrar um conjunto de pesos e compensações que minimizem o valor do custo. Faremos isso usando um algoritmo chamado descida de gradiente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por que precisamos de um valor quadrático? Não estamos interessados ​​principalmente no número de imagens corretamente reconhecidas pela rede? É possível simplesmente maximizar esse número diretamente e não minimizar o valor intermediário do valor quadrático? O problema é que o número de imagens reconhecidas corretamente não é uma função suave dos pesos e compensações da rede. Na maioria das vezes, pequenas alterações nos pesos e deslocamentos não alteram o número de imagens reconhecidas corretamente. Por isso, é difícil entender como alterar pesos e desvios para melhorar a eficiência. Se usarmos uma função de custo suave, será fácil entender como fazer pequenas alterações nos pesos e compensações, a fim de melhorar o custo. Portanto, primeiro focaremos no valor quadrático e, em seguida, estudaremos a precisão da classificação.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mesmo considerando que queremos usar uma função de custo suave, você ainda pode estar interessado em saber por que escolhemos a função quadrática para a equação (6)? Não é possível escolher arbitrariamente? Talvez se escolhêssemos uma função diferente, teríamos um conjunto completamente diferente de pesos e compensações minimizadores? Uma pergunta razoável e, posteriormente, examinaremos novamente a função de custo e faremos algumas correções. No entanto, a função de custo quadrático funciona muito bem para entender as coisas básicas na aprendizagem de NS, portanto, por enquanto, continuaremos com ela.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resumindo: nosso objetivo no treinamento de NS é encontrar pesos e compensações que minimizem a função de custo quadrático C (w, b). A tarefa é bem colocada, mas até agora tem muitas estruturas que distraem - a interpretação de w e b como pesos e compensações, a função σ oculta em segundo plano, a escolha da arquitetura de rede, MNIST e assim por diante. Acontece que podemos entender muito, ignorando grande parte dessa estrutura e concentrando-nos apenas no aspecto da minimização. Então, por enquanto, esqueceremos a forma especial da função de custo, a comunicação com a Assembléia Nacional e assim por diante. Em vez disso, vamos imaginar que apenas temos uma função com muitas variáveis ​​e queremos minimizá-la. Vamos desenvolver uma tecnologia chamada descida de gradiente, que pode ser usada para resolver esses problemas. E então voltamos a uma determinada função,que queremos minimizar para a Assembléia Nacional.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bem, digamos que estamos tentando minimizar algumas funções C (v). Pode ser qualquer função com valores reais de muitas variáveis ​​v = v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ... Observe que substituí a notação w e b por v para mostrar que ela pode ser qualquer função - não estamos mais obcecados com HC. É útil imaginar que uma função C tenha apenas duas variáveis ​​- v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fff/752/1ad/fff7521ad0e339cb68eceace0f200697.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gostaríamos de descobrir onde C atinge um mínimo global. Obviamente, com a função desenhada acima, podemos estudar o gráfico e encontrar o mínimo. Nesse sentido, eu posso ter lhe dado uma função muito simples! No caso geral, C pode ser uma função complexa de muitas variáveis, e geralmente é impossível apenas olhar o gráfico e encontrar o mínimo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma maneira de resolver o problema é usar a álgebra para encontrar o mínimo analiticamente. Podemos calcular as derivadas e tentar usá-las para encontrar o extremo. Se tivermos sorte, isso funcionará quando C for uma função de uma ou duas variáveis. Mas com um grande número de variáveis, isso se transforma em um pesadelo. E para os NSs, muitas vezes precisamos de muito mais variáveis ​​- para os maiores NSs, o custo funciona de maneira complexa e depende de bilhões de pesos e deslocamentos. O uso de álgebra para minimizar essas funções falhará!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Tendo declarado que seria mais conveniente considerar C como uma função de duas variáveis, eu disse duas vezes em dois parágrafos “sim, mas e se for uma função de um número muito maior de variáveis?” Peço desculpas. Acredite, será realmente útil representar C como uma função duas variáveis, é que às vezes essa imagem se desfaz, e é por isso que os dois parágrafos anteriores foram necessários. Para o raciocínio matemático, muitas vezes é necessário manipular várias representações intuitivas, aprendendo ao mesmo tempo quando a representação pode ser usada e quando não ZYA).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ok, isso significa que álgebra não vai funcionar. Felizmente, existe uma grande analogia que oferece um algoritmo que funciona bem. Imaginamos nossa função como algo como um vale. Com a programação mais recente, não será tão difícil de fazer. E imaginamos uma bola rolando ao longo da encosta do vale. Nossa experiência nos diz que a bola acabará deslizando para o fundo. Talvez possamos usar essa idéia para encontrar o mínimo de uma função? Selecionamos aleatoriamente o ponto de partida para uma bola imaginária e, em seguida, simulamos o movimento da bola, como se ela estivesse rolando para o fundo do vale. Podemos usar essa simulação simplesmente contando as derivadas (e, possivelmente, as segundas derivadas) de C - elas nos dirão tudo sobre a forma local do vale e, portanto, sobre como nossa bola irá rolar.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com base no que você escreveu, você pode pensar que vamos escrever as equações de movimento de Newton para a bola, considerar os efeitos do atrito e da gravidade e assim por diante. De fato, não estaremos tão perto de seguir essa analogia com a bola - estamos desenvolvendo um algoritmo para minimizar C, e não uma simulação exata das leis da física! Essa analogia deve estimular nossa imaginação e não limitar nosso pensamento. Então, em vez de mergulhar nos detalhes complexos da física, vamos fazer a pergunta: se fôssemos nomeados deus por um dia, e criaríamos nossas próprias leis da física, dizendo à bola como aplicar qual lei ou leis de movimento escolheríamos, para que a bola sempre role fundo do vale? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para esclarecer a questão, pensaremos no que acontece se movermos a bola uma pequena distância Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> na direção da v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e uma pequena distância Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> na direção de v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">A álgebra nos diz que C muda da seguinte maneira:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-430"><span class="MJXp-mtable" id="MJXp-Span-431"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-432" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-433" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-434">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-435">C</span><span class="MJXp-mo" id="MJXp-Span-436" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mfrac" id="MJXp-Span-437" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-438">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-439">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-440">∂</span><span class="MJXp-msubsup" id="MJXp-Span-441"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-442" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-443" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-444">Δ</span><span class="MJXp-msubsup" id="MJXp-Span-445"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-446" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-447" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-448" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mfrac" id="MJXp-Span-449" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-450">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-451">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-452">∂</span><span class="MJXp-msubsup" id="MJXp-Span-453"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-454" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-455" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-456">Δ</span><span class="MJXp-msubsup" id="MJXp-Span-457"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-458" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-459" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 \tag{7} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Encontraremos uma maneira de escolher tais Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para que ΔC seja menor que zero; </font><font style="vertical-align: inherit;">isto é, vamos selecioná-los para que a bola role para baixo. </font><font style="vertical-align: inherit;">Para entender como fazer isso, é útil definir Δv como o vetor de alterações, ou seja, Δv ≡ (Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , onde T é a operação de transposição que transforma vetores de linha em vetores de coluna. </font><font style="vertical-align: inherit;">Nós também definir o gradiente C como derivados parciais (∂S / ∂V </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ∂S / ∂V </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Denotamos o vetor gradiente por ∇:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-460"><span class="MJXp-mtable" id="MJXp-Span-461"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-462" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-463" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-464">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-465">C</span><span class="MJXp-mo" id="MJXp-Span-466" style="margin-left: 0.333em; margin-right: 0.333em;">≡</span><span class="MJXp-mo" id="MJXp-Span-467" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-468" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-469">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-470">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-471">∂</span><span class="MJXp-msubsup" id="MJXp-Span-472"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-473" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-474" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-475" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-476" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-477">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-478">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-479">∂</span><span class="MJXp-msubsup" id="MJXp-Span-480"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-481" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-482" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-483"><span class="MJXp-mo" id="MJXp-Span-484" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-485" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-14"> \nabla C \equiv (\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T \tag{8} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em breve, reescreveremos a mudança de ΔC a Δv e o gradiente ∇C. Enquanto isso, quero esclarecer uma coisa, por causa da qual as pessoas costumam ficar no gradiente. Quando se encontraram pela primeira vez com ∇C, as pessoas às vezes não entendem como deveriam perceber o símbolo ∇. O que isso significa especificamente? De fato, você pode considerar com segurança ∇C um único objeto matemático - um vetor definido anteriormente - que é simplesmente escrito usando dois caracteres. Desse ponto de vista, ∇ é como acenar uma bandeira informando que "∇C é um vetor gradiente". Existem pontos de vista mais avançados dos quais ∇ pode ser considerado uma entidade matemática independente (por exemplo, como um operador de diferenciação), mas não precisamos deles. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com essas definições, a expressão (7) pode ser reescrita como:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-486"><span class="MJXp-mtable" id="MJXp-Span-487"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-488" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-489" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-490">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-491">C</span><span class="MJXp-mo" id="MJXp-Span-492" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mi" id="MJXp-Span-493">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-494">C</span><span class="MJXp-mo" id="MJXp-Span-495" style="margin-left: 0.267em; margin-right: 0.267em;">⋅</span><span class="MJXp-mi" id="MJXp-Span-496">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-497">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-15"> \Delta C \approx \nabla C \cdot \Delta v \tag{9} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essa equação ajuda a explicar por que ∇C é chamado de vetor gradiente: conecta as mudanças em v com as mudanças em C, exatamente como esperado de uma entidade chamada gradiente. [eng. gradiente - desvio / aprox. transl.] No entanto, é mais interessante que essa equação nos permita ver como escolher Δv para que ΔC seja negativo. Digamos que escolhemos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-498"><span class="MJXp-mtable" id="MJXp-Span-499"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-500" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-501" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-502">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-503">v</span><span class="MJXp-mo" id="MJXp-Span-504" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-505" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-506">η</span><span class="MJXp-mi" id="MJXp-Span-507">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-508">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-16"> \Delta v = - \eta \nabla C \tag{10} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">onde η é um pequeno parâmetro positivo (velocidade de aprendizado). Então a equação (9) nos diz que ΔC ≈ - η ∇C ⋅ ∇C = - η || ∇C || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Desde || ∇C || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ≥ 0, isso garante que ΔC ≤ 0, ou seja, C diminua o tempo todo se mudarmos v, conforme prescrito em (10) (é claro, como parte da aproximação da equação (9)). E é exatamente disso que precisamos! Portanto, tomamos a equação (10) para determinar a "lei do movimento" da bola em nosso algoritmo de descida de gradiente. Ou seja, usaremos a equação (10) para calcular o valor de Δv e depois moveremos a bola para esse valor:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-509"><span class="MJXp-mtable" id="MJXp-Span-510"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-511" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-512" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-513">v</span><span class="MJXp-mo" id="MJXp-Span-514" style="margin-left: 0.333em; margin-right: 0.333em;">→</span><span class="MJXp-msup" id="MJXp-Span-515"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-516" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-517" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-518" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-519">v</span><span class="MJXp-mo" id="MJXp-Span-520" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-521">η</span><span class="MJXp-mi" id="MJXp-Span-522">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-523">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-17"> v \rightarrow v' = v - \eta \nabla C \tag{11} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em seguida, aplicamos novamente esta regra para a próxima jogada. Continuando a repetição, baixaremos C até que, esperançosamente, alcancemos um mínimo global. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resumindo, a descida do gradiente funciona através do cálculo seqüencial do gradiente ∇ C e subsequente deslocamento na direção oposta, o que leva a uma “queda” ao longo da encosta do vale. Isso pode ser visualizado da seguinte maneira: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f7/495/966/5f749596634bc20923f5f8a3e49a3b9f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Observe que, com essa regra, a descida do gradiente não reproduz o movimento físico real. Na vida real, a bola tem um impulso que pode rolar pela encosta ou até rolar por algum tempo. Somente após o trabalho da força de atrito é que a bola rolará pelo vale. Nossa regra de seleção Δv apenas diz "desça". Uma boa regra para encontrar o mínimo!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para que a descida do gradiente funcione corretamente, precisamos escolher um valor suficientemente pequeno da velocidade de aprendizado η para que a equação (9) seja uma boa aproximação. Caso contrário, pode resultar que ΔC&gt; 0 - nada de bom! Ao mesmo tempo, não é necessário que η seja muito pequeno, pois as alterações em Δv serão pequenas e o algoritmo funcionará muito lentamente. Na prática, η muda para que a equação (9) forneça uma boa aproximação e o algoritmo não funcione muito lentamente. Mais tarde veremos como funciona. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Expliquei a descida do gradiente quando a função C dependia de apenas duas variáveis. Mas tudo funciona da mesma maneira se C é uma função de muitas variáveis. Suponha que ela tenha m variáveis, v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Então a mudança em ΔC causada por uma pequena mudança em Δv = (Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., Δv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> será</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-524"><span class="MJXp-mtable" id="MJXp-Span-525"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-526" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-527" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-528">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-529">C</span><span class="MJXp-mo" id="MJXp-Span-530" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mi" id="MJXp-Span-531">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-532">C</span><span class="MJXp-mo" id="MJXp-Span-533" style="margin-left: 0.267em; margin-right: 0.267em;">⋅</span><span class="MJXp-mi" id="MJXp-Span-534">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-535">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-18"> \Delta C \approx \nabla C \cdot \Delta v \tag{12} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> onde o gradiente ∇C é o vetor </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-536"><span class="MJXp-mtable" id="MJXp-Span-537"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-538" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-539" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-540">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-541">C</span><span class="MJXp-mo" id="MJXp-Span-542" style="margin-left: 0.333em; margin-right: 0.333em;">≡</span><span class="MJXp-mo" id="MJXp-Span-543" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-544" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-545">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-546">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-547">∂</span><span class="MJXp-msubsup" id="MJXp-Span-548"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-549" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-550" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-551" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-552" style="margin-left: 0em; margin-right: 0em;">…</span><span class="MJXp-mo" id="MJXp-Span-553" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-554" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-555">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-556">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-557">∂</span><span class="MJXp-msubsup" id="MJXp-Span-558"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-559" style="margin-right: 0.05em;">v</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-560" style="vertical-align: -0.4em;">m</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-561"><span class="MJXp-mo" id="MJXp-Span-562" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-563" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-19"> \nabla C \equiv (\frac{\partial C}{\partial v_1},…, \frac{\partial C}{\partial v_m})^T \tag{13} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tal como acontece com duas variáveis, podemos escolher </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-564"><span class="MJXp-mtable" id="MJXp-Span-565"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-566" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-567" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-568">Δ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-569">v</span><span class="MJXp-mo" id="MJXp-Span-570" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-571" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-572">η</span><span class="MJXp-mi" id="MJXp-Span-573">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-574">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-20"> \Delta v = - \eta \nabla C \tag{14} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e assegure que nossa expressão aproximada (12) para ΔC seja negativa. </font><font style="vertical-align: inherit;">Isso nos permite percorrer o gradiente ao mínimo, mesmo quando C é uma função de muitas variáveis, aplicando a regra de atualização repetidamente.</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-575"><span class="MJXp-mtable" id="MJXp-Span-576"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-577" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-578" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-579">v</span><span class="MJXp-mo" id="MJXp-Span-580" style="margin-left: 0.333em; margin-right: 0.333em;">→</span><span class="MJXp-msup" id="MJXp-Span-581"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-582" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-583" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-584" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-585">v</span><span class="MJXp-mo" id="MJXp-Span-586" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-587">η</span><span class="MJXp-mi" id="MJXp-Span-588">∇</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-589">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-21"> v \rightarrow v' = v - \eta \nabla C \tag{15} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta regra de atualização pode ser considerada o algoritmo de descida de gradiente definidor. Ele nos fornece um método de alterar repetidamente a posição de v em busca do mínimo da função C. Essa regra nem sempre funciona - várias coisas podem dar errado, impedindo que a descida do gradiente encontre o mínimo global de C - retornaremos a esse ponto nos próximos capítulos. Mas, na prática, a descida gradiente geralmente funciona muito bem e veremos que na Assembléia Nacional essa é uma maneira eficaz de minimizar a função de custo e, portanto, treinar a rede.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em certo sentido, a descida do gradiente pode ser considerada a melhor estratégia de pesquisa mínima. </font><font style="vertical-align: inherit;">Suponha que estamos tentando mover Δv para uma posição para minimizar C. Isso é equivalente a minimizar ΔC ≈ ∇C ⋅ Δv. </font><font style="vertical-align: inherit;">Limitaremos o tamanho da etapa para que || Δv || </font><font style="vertical-align: inherit;">= ε para alguma constante pequena ε&gt; 0. Em outras palavras, queremos mover uma pequena distância de tamanho fixo e tentar encontrar a direção do movimento que diminua C o máximo possível. </font><font style="vertical-align: inherit;">Pode-se provar que a escolha de Δv que minimiza ∇C ⋅ Δv é Δv = -η∇C, onde η = ε / || ∇C || é determinada pela restrição || Δv || </font><font style="vertical-align: inherit;">= ε. </font><font style="vertical-align: inherit;">Portanto, a descida em gradiente pode ser considerada uma maneira de dar pequenos passos na direção que diminui mais a C.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exercícios </font></font></h3><br><ul><li>     . :      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  — </a> , ,  ,      . </li><li>     ,      ,       .  ,       ?            ? </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As pessoas estudaram muitas opções para a descida do gradiente, incluindo aquelas que reproduzem com mais precisão uma bola física real. Essas opções têm suas vantagens, mas também uma grande desvantagem: a necessidade de calcular as segundas derivadas parciais de C, que podem consumir muitos recursos. Para entender isso, suponha que precisamos calcular todas as segundas derivadas parciais ∂ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ∂v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ∂v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Se as variáveis </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vj são</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> milhões, precisamos calcular cerca de um trilhão (um milhão ao quadrado) das segundas derivadas parciais (na verdade, meio trilhão, uma vez que </font><font style="vertical-align: inherit;">C </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ∂v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ∂v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = ∂ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ∂v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ∂v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Mas você pegou a essência). Isso exigirá muitos recursos de computação. Existem truques para evitar isso, e a busca de alternativas à descida do gradiente é uma área de pesquisa ativa. No entanto, neste livro, usaremos a descida gradiente e suas variantes como a principal abordagem para o aprendizado da SN. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como aplicamos a descida gradiente ao aprendizado de NA? Precisamos usá-lo para procurar pesos </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wk</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e deslocamentos b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> que minimizem a equação de custo (6). Vamos voltar a gravar regra de actualização gradiente de descida, mudança de variáveis v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pesos e deslocamentos. Em outras palavras, agora nossa “posição” possui os componentes w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , e o vetor gradiente ∇C tem os componentes correspondentes ∂C / ∂w</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e ∂C / ∂b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Depois de escrever nossa regra de atualização com novos componentes, obtemos:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-590"><span class="MJXp-mtable" id="MJXp-Span-591"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-592" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-593" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-594"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-595" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-596" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-597" style="margin-left: 0.333em; margin-right: 0.333em;">→</span><span class="MJXp-msubsup" id="MJXp-Span-598"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-599" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-601">′</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-600">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-602" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-603"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-604" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-605" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-606" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-607">η</span><span class="MJXp-mfrac" id="MJXp-Span-608" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-609">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-610">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-611">∂</span><span class="MJXp-msubsup" id="MJXp-Span-612"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-613" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-614" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-22"> w_k \rightarrow w'_k = w_k - \eta \frac{\partial C}{\partial w_k} \tag{16} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-615"><span class="MJXp-mtable" id="MJXp-Span-616"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-617" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-618" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-619"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-620" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-621" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-622" style="margin-left: 0.333em; margin-right: 0.333em;">→</span><span class="MJXp-msubsup" id="MJXp-Span-623"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-624" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-626">′</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-625">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-627" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-628"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-629" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-630" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-631" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-632">η</span><span class="MJXp-mfrac" id="MJXp-Span-633" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-634">∂</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-635">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-636">∂</span><span class="MJXp-msubsup" id="MJXp-Span-637"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-638" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-639" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-23"> b_l \rightarrow b'_l = b_l - \eta \frac{\partial C}{\partial b_l} \tag{17} </script></p><br><br>  Ao reaplicar esta regra de atualização, podemos "rolar ladeira abaixo" e, com alguma sorte, encontrar a função de custo mínimo.  Em outras palavras, essa regra pode ser usada para treinar a Assembléia Nacional. <br><br>  Existem vários obstáculos para aplicar a regra de descida do gradiente.  Nós os estudaremos em mais detalhes nos próximos capítulos.  Mas, por enquanto, quero mencionar apenas um problema.  Para entendê-lo, voltemos ao valor quadrático na equação (6).  Observe que essa função de custo se parece com C = 1 / n C <sub>x</sub> C <sub>x</sub> , ou seja, é o custo médio C <sub>x</sub> ≡ (|| y (x) −a || <sup>2</sup> ) / 2 para exemplos de treinamento individuais.  Na prática, para calcular o gradiente ∇C, precisamos calcular os gradientes ∇C <sub>x</sub> separadamente para cada entrada de treinamento x e depois calculá-los como média, averageC = 1 / n ∑ <sub>x</sub> ∇C <sub>x</sub> .  Infelizmente, quando a quantidade de entrada for muito grande, levará muito tempo e esse treinamento será lento. <br><br>  Para acelerar o aprendizado, você pode usar a descida estocástica do gradiente.  A idéia é calcular aproximadamente o gradiente de byC calculando ∇C <sub>x</sub> para uma pequena amostra aleatória de entrada de treinamento.  Ao calcular sua média, podemos obter rapidamente uma boa estimativa do gradiente verdadeiro ∇C, e isso ajuda a acelerar a descida do gradiente e, portanto, o treinamento. <br><br>  Formulando com mais precisão, a descida do gradiente estocástico funciona através da amostragem aleatória de um pequeno número de m dados de entrada de treinamento.  Nós chamaremos esses dados aleatórios de X <sub>1</sub> , X <sub>2</sub> , .., X me chamaremos de minipacote.  Se o tamanho da amostra m for grande o suficiente, o valor médio de ∇C <sub>X <sub>j</sub></sub> será próximo o suficiente da média de todos os ∇Cx, ou seja, <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-640"><span class="MJXp-mtext" id="MJXp-Span-641">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-642">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-643">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-644">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-645">c</span><span class="MJXp-mrow" id="MJXp-Span-646"><span class="MJXp-mtext" id="MJXp-Span-647">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-648">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-649">u</span><span class="MJXp-msubsup" id="MJXp-Span-650"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-651" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-656">m</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-652"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-653">j</span><span class="MJXp-mo" id="MJXp-Span-654">=</span><span class="MJXp-mn" id="MJXp-Span-655">1</span></span></span></span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-657">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-658">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-659">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-660">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-661">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-662">a</span><span class="MJXp-msubsup" id="MJXp-Span-663"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-664" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-665" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-666"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-667" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-668" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-669"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-670">m</span></span><span class="MJXp-mtext" id="MJXp-Span-671">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-672">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-673">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-674">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-675">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-676">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-677">x</span><span class="MJXp-mtext" id="MJXp-Span-678">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-679">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-680">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-681">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-682">c</span><span class="MJXp-mrow" id="MJXp-Span-683"><span class="MJXp-mtext" id="MJXp-Span-684">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-685">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-686">u</span><span class="MJXp-msubsup" id="MJXp-Span-687"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-688" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-689" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mtext" id="MJXp-Span-690">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-691">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-692">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-693">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-694">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-695">a</span><span class="MJXp-msubsup" id="MJXp-Span-696"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-697" style="margin-right: 0.05em;">C</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-698" style="vertical-align: -0.4em;">x</span></span></span><span class="MJXp-mrow" id="MJXp-Span-699"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-700">n</span></span><span class="MJXp-mo" id="MJXp-Span-701" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-702">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-703">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-704">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-705">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-706">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-707">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-708">C</span><span class="MJXp-mtext" id="MJXp-Span-709">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-710">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-711">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-712">g</span><span class="MJXp-mrow" id="MJXp-Span-713"><span class="MJXp-mn" id="MJXp-Span-714">18</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-24"> \ frac {\ sum ^ m_ {j = 1} \ nabla C_ {X_j}} {m} \ approx \ frac {\ sum_x \ nabla C_x} {n} = \ nabla C \ tag {18} </script></p><br><br>  onde o segundo valor passa por todo o conjunto de dados de treinamento.  Ao trocar peças, obtemos <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-715"><span class="MJXp-mtext" id="MJXp-Span-716">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-717">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-718">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-719">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-720">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-721">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-722">C</span><span class="MJXp-mtext" id="MJXp-Span-723">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-724">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-725">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-726">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-727">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-728">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-729">x</span><span class="MJXp-mtext" id="MJXp-Span-730">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-731">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-732">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-733">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-734">c</span><span class="MJXp-mrow" id="MJXp-Span-735"><span class="MJXp-mn" id="MJXp-Span-736">1</span></span><span class="MJXp-mrow" id="MJXp-Span-737"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-738">m</span></span><span class="MJXp-mtext" id="MJXp-Span-739">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-740">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-741">u</span><span class="MJXp-msubsup" id="MJXp-Span-742"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-743" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-748">m</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-744"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-745">j</span><span class="MJXp-mo" id="MJXp-Span-746">=</span><span class="MJXp-mn" id="MJXp-Span-747">1</span></span></span></span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-749">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-750">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-751">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-752">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-753">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-754">a</span><span class="MJXp-msubsup" id="MJXp-Span-755"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-756" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-757" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-758"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-759" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-760" style="vertical-align: -0.4em;">j</span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-761">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-762">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-763">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-764">g</span><span class="MJXp-mrow" id="MJXp-Span-765"><span class="MJXp-mn" id="MJXp-Span-766">19</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-25"> \ nabla C \ approx \ frac {1} {m} \ sum ^ m_ {j = 1} \ nabla C_ {X_j} \ tag {19} </script></p><br><br>  o que confirma que podemos estimar o gradiente geral calculando os gradientes para um minipack selecionado aleatoriamente. <br><br>  Para relacionar isso diretamente ao treinamento de NS, vamos supor que w <sub>k</sub> e b <sub>l</sub> denotem os pesos e deslocamentos de nosso NS.  Em seguida, a descida do gradiente estocástico seleciona um mini-pacote aleatório de dados de entrada e aprende com eles <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-767"><span class="MJXp-msubsup" id="MJXp-Span-768"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-769" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-770" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mtext" id="MJXp-Span-771">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-772">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-773">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-774">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-775">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-776">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-777">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-778">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-779">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-780">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-781">w</span><span class="MJXp-msubsup" id="MJXp-Span-782"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-783" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-785">′</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-784">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-786" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-787"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-788" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-789" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-790" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mtext" id="MJXp-Span-791">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-792">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-793">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-794">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-795">c</span><span class="MJXp-mrow" id="MJXp-Span-796"><span class="MJXp-mtext" id="MJXp-Span-797">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-798">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-799">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-800">a</span></span><span class="MJXp-mrow" id="MJXp-Span-801"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-802">m</span></span><span class="MJXp-mtext" id="MJXp-Span-803">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-804">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-805">u</span><span class="MJXp-msubsup" id="MJXp-Span-806"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-807" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-808" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-809">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-810">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-811">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-812">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-813">c</span><span class="MJXp-mrow" id="MJXp-Span-814"><span class="MJXp-mtext" id="MJXp-Span-815">&nbsp;</span><span class="MJXp-msubsup" id="MJXp-Span-816"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-817" style="margin-right: 0.05em;">C</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-818" style="vertical-align: -0.4em;">p</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-819">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-820">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-821">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-822">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-823">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-824">l</span><span class="MJXp-mrow" id="MJXp-Span-825"><span class="MJXp-msubsup" id="MJXp-Span-826"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-827" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-828" style="vertical-align: -0.4em;">j</span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-829"><span class="MJXp-mtext" id="MJXp-Span-830">&nbsp;</span><span class="MJXp-msubsup" id="MJXp-Span-831"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-832" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-833" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-834">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-835">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-836">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-837">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-838">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-839">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-840">l</span></span><span class="MJXp-mtext" id="MJXp-Span-841">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-842">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-843">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-844">g</span><span class="MJXp-mrow" id="MJXp-Span-845"><span class="MJXp-mn" id="MJXp-Span-846">20</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-26"> w_k \ rightarrow w'_k = w_k - \ frac {\ eta} {m} \ sum_j \ frac {\ C_ parcial {X_j}} {\ w_k parcial} \ tag {20} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-847"><span class="MJXp-msubsup" id="MJXp-Span-848"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-849" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-850" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mtext" id="MJXp-Span-851">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-852">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-853">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-854">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-855">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-856">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-857">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-858">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-859">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-860">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-861">w</span><span class="MJXp-msubsup" id="MJXp-Span-862"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-863" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-865">′</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-864">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-866" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-867"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-868" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-869" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-870" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mtext" id="MJXp-Span-871">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-872">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-873">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-874">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-875">c</span><span class="MJXp-mrow" id="MJXp-Span-876"><span class="MJXp-mtext" id="MJXp-Span-877">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-878">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-879">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-880">a</span></span><span class="MJXp-mrow" id="MJXp-Span-881"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-882">m</span></span><span class="MJXp-mtext" id="MJXp-Span-883">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-884">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-885">u</span><span class="MJXp-msubsup" id="MJXp-Span-886"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-887" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-888" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-889">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-890">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-891">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-892">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-893">c</span><span class="MJXp-mrow" id="MJXp-Span-894"><span class="MJXp-mtext" id="MJXp-Span-895">&nbsp;</span><span class="MJXp-msubsup" id="MJXp-Span-896"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-897" style="margin-right: 0.05em;">C</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-898" style="vertical-align: -0.4em;">p</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-899">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-900">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-901">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-902">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-903">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-904">l</span><span class="MJXp-mrow" id="MJXp-Span-905"><span class="MJXp-msubsup" id="MJXp-Span-906"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-907" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-908" style="vertical-align: -0.4em;">j</span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-909"><span class="MJXp-mtext" id="MJXp-Span-910">&nbsp;</span><span class="MJXp-msubsup" id="MJXp-Span-911"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-912" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-913" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-914">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-915">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-916">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-917">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-918">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-919">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-920">l</span></span><span class="MJXp-mtext" id="MJXp-Span-921">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-922">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-923">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-924">g</span><span class="MJXp-mrow" id="MJXp-Span-925"><span class="MJXp-mn" id="MJXp-Span-926">21</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-27"> b_l \ rightarrow b'_l = b_l - \ frac {\ eta} {m} \ sum_j \ frac {\ C_ parcial {X_j}} {\ b_l parcial} \ tag {21} </script></p><br><br>  onde está a soma de todos os exemplos de treinamento X <sub>j</sub> no <sub>minipacote</sub> atual.  Depois, selecionamos outro mini-pacote aleatório e estudamos sobre ele.  E assim por diante, até esgotarmos todos os dados de treinamento, chamados de fim da era do treinamento.  Neste momento, estamos iniciando novamente uma nova era de aprendizado. <br><br>  A propósito, vale a pena notar que os acordos referentes à escala da função de custo e à atualização dos pesos e compensações diferem em um minipacote.  Na equação (6), escalamos a função de custo 1 / n vezes.  Às vezes, as pessoas omitem 1 / n somando os custos de exemplos de treinamento individuais, em vez de calcular a média.  Isso é útil quando o número total de exemplos de treinamento não é conhecido antecipadamente.  Isso pode acontecer, por exemplo, quando dados adicionais aparecerem em tempo real.  Da mesma forma, as regras de atualização do mini-pacote (20) e (21) às vezes omitem o membro de 1 / m na frente da soma.  Conceitualmente, isso não afeta nada, pois é equivalente a uma mudança na velocidade de aprendizado η.  No entanto, vale a pena prestar atenção ao comparar vários trabalhos. <br><br>  Uma descida estocástica de gradiente pode ser vista como um voto político: é muito mais fácil coletar uma amostra na forma de um minipacote do que aplicar uma descida de gradiente a uma amostra completa - assim como uma pesquisa na saída de um site é mais fácil de realizar do que uma eleição completa.  Por exemplo, se o nosso conjunto de treinamento tiver um tamanho de n = 60.000, como o MNIST, e fizermos uma amostra de um mini-pacote de tamanho m = 10, aceleraremos a estimativa do gradiente em 6000 vezes!  Obviamente, a estimativa não será ideal - haverá flutuação estatística nela - mas não precisará ser ideal: precisamos apenas avançar na direção que diminui C, o que significa que não precisamos calcular o gradiente com precisão.  Na prática, a descida estocástica do gradiente é uma técnica de ensino comum e poderosa para a Assembléia Nacional, e a base da maioria das tecnologias de ensino que iremos desenvolver como parte do livro. <br><br><h3>  Exercícios </h3><br><ul><li>  A versão extrema da descida em gradiente usa o tamanho de minipacote igual a 1. Ou seja, com a entrada x, atualizamos nossos pesos e compensações de acordo com as regras w <sub>k</sub> → w ′ <sub>k</sub> = w <sub>k</sub> - η /C <sub>x</sub> / ∂w <sub>k</sub> eb b <sub>l</sub> → b ′ <sub>L</sub> = b <sub>l</sub> - ηC <sub>x</sub> / ∂b <sub>l</sub> .  Em seguida, selecionamos outro exemplo de entrada de treinamento e atualizamos novamente os pesos e compensações.  E assim por diante  Este procedimento é conhecido como aprendizado online ou incremental.  No aprendizado on-line, o NS estuda com base em uma cópia de treinamento dos dados de entrada de cada vez (como pessoas).  Quais são as vantagens e desvantagens do aprendizado on-line em comparação com a descida gradiente estocástica com um tamanho de minipacote de 20? </li></ul><br>  Deixe-me terminar esta seção com uma discussão sobre um tópico que às vezes incomoda as pessoas que encontraram pela primeira vez uma descida de gradiente.  No NS, o valor de C é uma função de muitas variáveis ​​- todos os pesos e compensações - e, em certo sentido, determina a superfície em um espaço muito multidimensional.  As pessoas começam a pensar: "Vou ter que visualizar todas essas dimensões adicionais".  E eles começam a se preocupar: "Não consigo navegar em quatro dimensões, sem mencionar cinco (ou cinco milhões)".  Eles têm alguma qualidade especial que a supermatemática "real" tem?  Claro que não.  Até matemáticos profissionais não conseguem visualizar muito bem o espaço quadridimensional - se é que o fazem.  Eles fazem truques, desenvolvendo outras maneiras de representar o que está acontecendo.  Fizemos exatamente isso: usamos a representação algébrica (e não visual) de ΔC para entender como se mover para que C diminua.  Pessoas que fazem um bom trabalho com um grande número de dimensões têm em mente uma grande biblioteca de técnicas semelhantes;  nosso truque algébrico é apenas um exemplo.  Essas técnicas podem não ser tão simples como estamos acostumados ao visualizar três dimensões, mas quando você cria uma biblioteca de técnicas semelhantes, começa a pensar bem em dimensões superiores.  Não entrarei em detalhes, mas se você estiver interessado, talvez goste da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">discussão de algumas dessas técnicas por</a> matemáticos profissionais que estão acostumados a pensar em dimensões mais elevadas.  Embora algumas das técnicas discutidas sejam bastante complexas, a maioria das melhores respostas é intuitiva e acessível a todos. <br><br><h3>  Implementando uma Rede para Classificar Números </h3><br>  Ok, agora vamos escrever um programa que aprenda a reconhecer dígitos manuscritos usando descida estocástica de gradiente e dados de treinamento do MNIST.  Faremos isso com um pequeno programa em python 2.7, composto por apenas 74 linhas!  A primeira coisa que precisamos é fazer o download dos dados do MNIST.  Se você usa git, pode obtê-los clonando o repositório deste livro: <br><br> <code>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git</code> <br> <br>  Caso contrário, faça o download do código <a href="">no link</a> . <br><br>  A propósito, quando mencionei os dados do MNIST anteriormente, eu disse que eles são divididos em 60.000 imagens de treinamento e 10.000 imagens de teste.  Esta é a descrição oficial do MNIST.  Vamos quebrar os dados um pouco diferente.  Deixaremos as imagens de verificação inalteradas, mas dividiremos o conjunto de treinamento em duas partes: 50.000 imagens, que usaremos para treinar a Assembléia Nacional, e 10.000 imagens individuais para confirmação adicional.  Embora não os utilizemos, mais tarde eles serão úteis quando entenderemos a configuração de alguns hiper parâmetros do NS - a velocidade de aprendizado etc. - que nosso algoritmo não seleciona diretamente.  Embora os dados de corroboração não façam parte da especificação original do MNIST, muitos usam o MNIST dessa maneira e, no campo do HC, o uso de dados de corroboração é comum.  Agora, falando dos "dados de treinamento do MNIST", quero dizer nossos 50.000 karitnoks, não os 60.000 originais. <br><br>  Além dos dados MNIST, também precisamos de uma biblioteca python chamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Numpy</a> para cálculos rápidos de álgebra linear.  Se você não o possui, pode obtê-lo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no link</a> . <br><br>  Antes de fornecer todo o programa, deixe-me explicar os principais recursos do código para o NS.  O local central é ocupado pela classe Rede, que usamos para representar a Assembléia Nacional.  Aqui está o código de inicialização do objeto Rede: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  A matriz de tamanhos contém o número de neurônios nas camadas correspondentes.  Portanto, se queremos criar um objeto de rede com dois neurônios na primeira camada, três neurônios na segunda camada e um neurônio na terceira, então o escreveremos assim: <br><br><pre> <code class="python hljs">net = Network([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre> <br>  As compensações e pesos no objeto Rede são inicializados aleatoriamente usando a função numpy np.random.randn, que gera uma distribuição Gaussiana com uma expectativa matemática de 0 e um desvio padrão de 1. Essa inicialização aleatória fornece um ponto de partida ao nosso algoritmo de descida de gradiente estocástico.  Nos capítulos seguintes, encontraremos as melhores maneiras de inicializar pesos e compensações, mas por enquanto isso é suficiente.  Observe que o código de inicialização de rede pressupõe que a primeira camada de neurônios será inserida e não lhes atribui um viés, pois eles são usados ​​apenas para calcular a saída. <br><br>  Observe também que deslocamentos e pesos são armazenados como uma matriz de matrizes Numpy.  Por exemplo, net.weights [1] é uma matriz Numpy que armazena pesos que conectam a segunda e a terceira camadas de neurônios (essa não é a primeira e a segunda camadas, pois em python a numeração dos elementos da matriz vem do zero).  Como levará muito tempo para escrever net.weights [1], denotamos essa matriz como w.  Essa é uma matriz que w <sub>jk</sub> é o peso da conexão entre o k-ésimo neurônio na segunda camada e o j-ésimo neurônio na terceira.  Essa ordem de índices j e k pode parecer estranha - não seria mais lógico trocá-los?  Mas a grande vantagem desse registro será que o vetor de ativação da terceira camada de neurônios é obtido: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-927"><span class="MJXp-msup" id="MJXp-Span-928"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-929" style="margin-right: 0.05em;">a</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-930" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-931" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-932">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-933">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-934">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-935">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-936">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-937">a</span><span class="MJXp-mo" id="MJXp-Span-938" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-939">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-940">a</span><span class="MJXp-mo" id="MJXp-Span-941" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-942">b</span><span class="MJXp-mo" id="MJXp-Span-943" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-944">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-945">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-946">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-947">g</span><span class="MJXp-mrow" id="MJXp-Span-948"><span class="MJXp-mn" id="MJXp-Span-949">22</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-28"> a '= \ sigma (wa + b) \ tag {22} </script></p><br><br>  Vamos olhar para esta equação bastante rica.  a é o vetor de ativação da segunda camada de neurônios.  Para obter a ', multiplicamos a pela matriz de pesos w e adicionamos o vetor de deslocamento b.  Em seguida, aplicamos o elemento sigmóide σ por elemento a cada elemento do vetor wa + b (isso é chamado de vetorização da função σ).  É fácil verificar que a equação (22) fornece o mesmo resultado que a regra (4) para calcular um neurônio sigmóide. <br><br><h3>  Exercício </h3><br><ul><li>  Escreva a equação (22) na forma de componente e verifique se ela fornece o mesmo resultado da regra (4) para calcular um neurônio sigmóide. </li></ul><br>  Com tudo isso em mente, é fácil escrever código que calcula a saída de um objeto de rede.  Começamos definindo um sigmóide: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sigmoid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0</span></span>/(<span class="hljs-number"><span class="hljs-number">1.0</span></span>+np.exp(-z))</code> </pre> <br>  Observe que, quando o parâmetro z for um vetor ou matriz Numpy, o Numpy aplicará automaticamente o elemento sigmóide, ou seja, na forma de vetor. <br><br>  Adicione um método de propagação direta à classe Network, que recebe a entrada a da rede e retorna a saída correspondente.  Supõe-se que o parâmetro a seja (n, 1) Numpy ndarray, não um vetor (n,).  Aqui n é o número de neurônios de entrada.  Se você tentar usar o vetor (n), obterá resultados estranhos. <br><br>  O método simplesmente aplica a equação (22) a cada camada: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">feedforward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, a)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       "a"""</span></span><span class="hljs-string"><span class="hljs-string">" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a</span></span></code> </pre> <br>  Claro, basicamente nós, dos objetos de Rede, precisamos deles para aprender.  Para isso, forneceremos a eles o método SGD, que implementa a descida estocástica do gradiente.  Aqui está o código dele.  Em alguns lugares, é bastante misterioso, mas a seguir iremos analisá-lo em mais detalhes. <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SGD</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta, test_data=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    -    . training_data –   "(x, y)",       .       .  test_data ,          ,     .     ,    . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: n_test = len(test_data) n = len(training_data) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n, mini_batch_size)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> mini_batch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batches: self.update_mini_batch(mini_batch, eta) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0}: {1} / {2}"</span></span>.format( j, self.evaluate(test_data), n_test) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0} complete"</span></span>.format(j)</code> </pre> <br>  training_data é uma lista de tuplas "(x, y)" que representam a entrada de treinamento e a saída desejada.  As variáveis ​​epochs e mini_batch_size são o número de épocas a serem aprendidas e o tamanho dos minipacotes a serem usados.  eta - velocidade de aprendizagem, η.  Se test_data estiver definido, a rede será avaliada com relação aos dados de verificação após cada era e o progresso atual será exibido.  Isso é útil para acompanhar o progresso, mas diminui significativamente o trabalho. <br><br>  O código funciona assim.  Em cada época, ele começa misturando acidentalmente os dados de treinamento e os divide em mini-pacotes do tamanho certo.  Essa é uma maneira fácil de criar uma amostra de dados de treinamento.  Em seguida, para cada mini_batch, aplicamos uma etapa de descida de gradiente.  Isso é feito pelo código self.update_mini_batch (mini_batch, eta), que atualiza os pesos e compensações da rede de acordo com uma iteração de descida de gradiente único usando apenas dados de treinamento no mini_batch.  Aqui está o código para o método update_mini_batch: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_mini_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, mini_batch, eta)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    ,          -. mini_batch –    (x, y),  eta –  ."""</span></span> nabla_b = [np.zeros(b.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.biases] nabla_w = [np.zeros(w.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.weights] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nb, dnb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nw, dnw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w, nw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b, nb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.biases, nabla_b)]</code> </pre> <br>  A maior parte do trabalho é feita pela linha. <br><br><pre> <code class="python hljs"> delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code> </pre> <br>  Invoca um algoritmo de retropropagação - uma maneira rápida de calcular o gradiente de uma função de custo.  Portanto, update_mini_batch simplesmente calcula esses gradientes para cada exemplo de treinamento de mini_batch e atualiza self.weights e self.biases. <br><br>  Até agora, não demonstrarei código para self.backprop.  O trabalho de distribuição reversa será explorado no próximo capítulo, e haverá um código self.backprop.  Por enquanto, suponha que ele se comporte como indicado, retornando um gradiente apropriado para o custo associado ao exemplo de treinamento x. <br><br>  Vamos dar uma olhada em todo o programa, incluindo comentários explicativos.  Com exceção da função self.backprop, o programa fala por si - o trabalho principal é realizado por self.SGD e self.update_mini_batch.  O método self.backprop usa várias funções adicionais para calcular o gradiente, ou seja, sigmoid_prime, que calcula a derivada do sigmoide, e self.cost_derivative, que não descreverei aqui.  Você pode ter uma idéia deles observando o código e os comentários.  No próximo capítulo, vamos considerá-los com mais detalhes.  Lembre-se de que, embora o programa pareça longo, a maior parte do código são comentários que facilitam o entendimento.  De fato, o próprio programa consiste em apenas 74 linhas não de código - não vazias e nem comentários.  Todo o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">código está disponível no GitHub</a> . <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" network.py ~~~~~~~~~~           .      .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #   import random #   import numpy as np class Network(object): def __init__(self, sizes): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].               0    1. ,      ,       ,        . """ self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): """   ,  ``a`` -  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """    -    . training_data –   "(x, y)",       .       .  test_data ,          ,     .     ,    . """ if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch {0}: {1} / {2}".format( j, self.evaluate(test_data), n_test) else: print "Epoch {0} complete".format(j) def update_mini_batch(self, mini_batch, eta): """    ,          -. mini_batch –    (x, y),  eta –  .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) #   delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """ l      ,      . l = 1    , l = 2 – ,   .    ,   python      .""" for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """    ,      .    –          .""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): """    ( C_x /  a)   .""" return (output_activations-y) ####   def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Quão bem o programa reconhece números manuscritos?  Vamos começar carregando os dados do MNIST.  Faremos isso usando o pequeno programa auxiliar mnist_loader.py, que descreverei abaixo.  Execute os seguintes comandos no shell python: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper()</code> </pre> <br>  Obviamente, isso pode ser feito em um programa separado, mas se você trabalhar em paralelo com um livro, será mais fácil. <br><br>  Depois de baixar os dados do MNIST, configure uma rede de 30 neurônios ocultos.  Faremos isso depois de importar o programa descrito acima, chamado de rede: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network &gt;&gt;&gt; net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br>  Finalmente, usamos descida de gradiente estocástico para treinamento em dados de treinamento por 30 eras, com tamanho de minipacote de 10 e velocidade de aprendizado de η = 3,0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Se você estiver executando um código paralelamente à leitura de um livro, lembre-se de que levará alguns minutos para executar.  Sugiro que você comece tudo, continue lendo e verifique periodicamente o que o programa produz.  Se você estiver com pressa, poderá reduzir o número de eras, reduzindo o número de neurônios ocultos ou usando apenas parte dos dados de treinamento.  O código de trabalho final funcionará mais rápido: esses scripts python foram projetados para fazer você entender como a rede funciona e não possui alto desempenho!  E, é claro, após o treinamento, a rede pode funcionar muito rapidamente em praticamente qualquer plataforma de computação.  Por exemplo, quando ensinamos à rede uma boa seleção de pesos e compensações, ela pode ser transportada facilmente para trabalhar em JavaScript em um navegador da Web ou como um aplicativo nativo em um dispositivo móvel.  De qualquer forma, aproximadamente a mesma conclusão é feita pelo programa que treina a rede neural.  Ela escreve o número de imagens de teste reconhecidas corretamente após cada era de treinamento.  Como você pode ver, mesmo após uma era, atinge uma precisão de 9.129 em 10.000, e esse número continua a crescer: <br><br> <code>Epoch 0: 9129 / 10000 <br> Epoch 1: 9295 / 10000 <br> Epoch 2: 9348 / 10000 <br> ... <br> Epoch 27: 9528 / 10000 <br> Epoch 28: 9542 / 10000 <br> Epoch 29: 9534 / 10000</code> <br> <br>  Acontece que a rede treinada fornece uma porcentagem da classificação correta de cerca de 95 - 95,42% no máximo!  Uma primeira tentativa bastante promissora.  Eu aviso que seu código não necessariamente produzirá exatamente os mesmos resultados, pois inicializamos a rede com pesos e compensações aleatórios.  Para este capítulo, eu escolhi a melhor de três tentativas. <br><br>  Vamos reiniciar o experimento alterando o número de neurônios ocultos para 100. Como antes, se você executar o código ao mesmo tempo que lê, lembre-se de que leva muito tempo (na minha máquina, cada época leva várias dezenas de segundos), então é melhor ler em paralelo com execução de código. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Naturalmente, isso melhora o resultado para 96,59%.  Pelo menos nesse caso, o uso de mais neurônios ocultos ajuda a obter melhores resultados. <br><br>  O feedback dos leitores sugere que os resultados deste experimento variam muito e alguns resultados de aprendizado são muito piores.  Usando técnicas do Capítulo 3 para reduzir seriamente a diversidade da eficiência do trabalho de uma execução para outra. <br><br>  Obviamente, para alcançar tal precisão, eu tive que escolher um certo número de épocas para aprender, o tamanho do mini-pacote e a velocidade de aprendizado η.  Como mencionei acima, eles são chamados hiperparâmetros de nossa Assembléia Nacional - para distingui-los de parâmetros simples (pesos e compensações) que o algoritmo ajusta durante o treinamento.  Se escolhermos mal os hiperparâmetros, obteremos resultados ruins.  Suponha, por exemplo, que tenhamos escolhido a taxa de aprendizado η = 0,001: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, test_data=test_data)</code> </pre> <br>  Os resultados são muito menos impressionantes: <br><br> <code>Epoch 0: 1139 / 10000 <br> Epoch 1: 1136 / 10000 <br> Epoch 2: 1135 / 10000 <br> ... <br> Epoch 27: 2101 / 10000 <br> Epoch 28: 2123 / 10000 <br> Epoch 29: 2142 / 10000</code> <br> <br>  No entanto, você pode ver que a eficiência da rede está crescendo lentamente ao longo do tempo.  Isso sugere que você pode tentar aumentar a velocidade de aprendizado, por exemplo, para 0,01.  Nesse caso, os resultados serão melhores, o que indica a necessidade de aumentar ainda mais a velocidade (se a mudança melhorar a situação, mude ainda mais!).  Se você fizer isso várias vezes, chegaremos a η = 1,0 (e às vezes até 3,0), o que está próximo de nossas experiências anteriores.  Portanto, embora inicialmente tenhamos selecionado mal parâmetros hiperparâmetro, pelo menos reunimos informações suficientes para melhorar nossa escolha de parâmetros. <br><br>  Em geral, a depuração de NA é uma questão complicada.  Isso ocorre especialmente quando a escolha dos hiperparâmetros iniciais produz resultados que não excedem o ruído aleatório.  Suponha que tentemos usar uma arquitetura bem-sucedida de 30 neurônios, mas alteremos a velocidade de aprendizado para 100.0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100.0</span></span>, test_data=test_data)</code> </pre> <br>  No final, acontece que fomos longe demais e pegamos muita velocidade: <br><br> <code>Epoch 0: 1009 / 10000 <br> Epoch 1: 1009 / 10000 <br> Epoch 2: 1009 / 10000 <br> Epoch 3: 1009 / 10000 <br> ... <br> Epoch 27: 982 / 10000 <br> Epoch 28: 982 / 10000 <br> Epoch 29: 982 / 10000</code> <br> <br>  Agora imagine que estamos abordando essa tarefa pela primeira vez.  Obviamente, sabemos desde os primeiros experimentos que seria correto reduzir a velocidade do aprendizado.  Mas se abordássemos essa tarefa pela primeira vez, não teríamos resultados que pudessem nos levar à solução certa.  Poderíamos começar a pensar que talvez tenhamos escolhido os parâmetros iniciais errados para pesos e compensações, e é difícil para a rede aprender?  Ou talvez não tenhamos dados de treinamento suficientes para obter um resultado significativo?  Talvez não tenhamos esperado épocas suficientes?  Talvez uma rede neural com essa arquitetura simplesmente não consiga aprender a reconhecer números manuscritos?  Talvez a velocidade de aprendizado seja muito lenta?  Quando você aborda a tarefa pela primeira vez, nunca tem confiança. <br><br>  Com isso, vale a pena aprender uma lição de que depurar o NS não é uma tarefa trivial e isso, como a programação regular, faz parte da arte.  Você deve aprender esta arte de depuração para obter bons resultados do NS.  Em geral, precisamos desenvolver uma heurística para selecionar bons hiperparâmetros e boa arquitetura.  Discutiremos isso em detalhes no livro, incluindo como selecionei os hiperparâmetros acima. <br><br><h3>  Exercício </h3><br><ul><li>  Tente criar uma rede de apenas duas camadas - entrada e saída, sem ocultar - com 784 e 10 neurônios, respectivamente.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treine a rede com descida de gradiente estocástico. </font><font style="vertical-align: inherit;">Que precisão de classificação você obtém?</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anteriormente, pulei os detalhes do carregamento de dados MNIST. </font><font style="vertical-align: inherit;">Isso acontece simplesmente. </font><font style="vertical-align: inherit;">Aqui está o código para completar a imagem. </font><font style="vertical-align: inherit;">As estruturas de dados são descritas nos comentários - tudo é simples, tuplas e matrizes de objetos Numpy ndarray (se você não estiver familiarizado com esses objetos, pense neles como vetores).</font></font><br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" mnist_loader ~~~~~~~~~~~~      MNIST.       ``load_data``  ``load_data_wrapper``.  , ``load_data_wrapper`` -  ,     . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import cPickle import gzip #  import numpy as np def load_data(): """  MNIST   ,  ,    . ``training_data``      .    .  numpy ndarray  50 000 .   –     numpy ndarray  784 ,  28 * 28 = 784    MNIST.  –  numpy ndarray  50 000 .   –   0  9   ,    . ``validation_data``  ``test_data`` ,    10 000 .    ,           ``training_data``.    - ``load_data_wrapper()``. """ f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) def load_data_wrapper(): """ ,  ``(training_data, validation_data, test_data)``.   ``load_data``,         .  , ``training_data`` -    50 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   . ``y`` -  10- numpy.ndarray,   ,     ``x``. ``validation_data``  ``test_data`` -  ,   10 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   ,  ``y`` -   ,  ,   ( ),  ``x``. ,  ,           .         .""" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) def vectorized_result(j): """ 10-    1.0   j     .      (0..9)     .""" e = np.zeros((10, 1)) e[j] = 1.0 return e</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eu disse que nosso programa está obtendo bons resultados. O que isso significa? Bom em comparação com o que? É útil obter os resultados de alguns testes simples e básicos com os quais você pode fazer uma comparação para entender o que “bons resultados” significam. O nível básico mais simples, é claro, seria um palpite aleatório. Isso pode ser feito em cerca de 10% dos casos. E nós mostramos um resultado muito melhor! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Que tal um nível base menos trivial? Vamos ver como a imagem está escura. Por exemplo, a imagem 2 geralmente será mais escura que a imagem 1, simplesmente porque possui mais pixels escuros, como visto nos exemplos abaixo:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f59/0de/5b7/f590de5b7bc6581b9854b2013e5013de.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclui-se que podemos calcular a escuridão média para cada dígito de 0 a 9. Quando obtemos uma nova imagem, calculamos sua escuridão e achamos que ela mostra uma figura com a escuridão média mais próxima. Este é um procedimento simples e fácil de programar, por isso não escreverei código - se estiver interessado, ele </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fica no GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Mas essa é uma melhoria séria em comparação com suposições aleatórias - o código reconhece corretamente 2.225 de 10.000 imagens, ou seja, fornece uma precisão de 22,25%.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Não é difícil encontrar outras idéias que atinjam precisão entre 20 e 50%. Depois de trabalhar um pouco mais, você pode exceder 50%. Mas, para obter uma precisão muito maior, é melhor usar algoritmos MO autorizados. Vamos tentar um dos algoritmos mais famosos, o método do vetor de suporte ou SVM. Se você não conhece o SVM, não se preocupe, não precisamos entender esses detalhes. Apenas usamos uma biblioteca python chamada scikit-learn, que fornece uma interface simples para a biblioteca C rápida para SVM, conhecida como </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LIBSVM</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se executarmos o scikit-learn do classificador SVM nas configurações padrão, obteremos a classificação correta de 9.435 em 10.000 (o código </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">está disponível no link</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Isso já é uma grande melhoria em relação à abordagem ingênua de classificar imagens pela escuridão. Isso significa que o SVM funciona tão bem quanto o nosso NS, apenas um pouco pior. Nos capítulos seguintes, vamos nos familiarizar com novas técnicas que nos permitirão melhorar nosso NS para que eles superem em muito o SVM.</font></font><br><br>  Mas isso não é tudo.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O resultado 9 435 em 10 000 do scikit-learn é especificado para as configurações padrão. O SVM possui muitos parâmetros que podem ser ajustados e você pode procurar parâmetros que melhorem esse resultado. Não vou entrar em detalhes, eles podem ser lidos no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artigo de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andreas Muller. Ele mostrou que, trabalhando para otimizar os parâmetros, é possível obter uma precisão de pelo menos 98,5%. Em outras palavras, um SVM bem ajustado comete apenas um dígito dentre os 70 erros. Um bom resultado! NA pode conseguir mais? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acontece que eles podem. Hoje, um NS bem ajustado ultrapassa qualquer outra tecnologia conhecida na solução MNIST, incluindo o SVM. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Registro para 2013</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classificou corretamente 9.979 de 10.000 imagens. E veremos a maioria das tecnologias usadas para isso neste livro. Esse nível de precisão é próximo ao humano, e talvez até o exceda, pois várias imagens do MNIST são difíceis de decifrar, mesmo para humanos, por exemplo: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/def/4b3/b37/def4b3b37d8d510615d60a372a06ad47.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">acho que você concorda que é difícil classificá-las! Com essas imagens no conjunto de dados MNIST, é surpreendente que o NS possa reconhecer corretamente todas as 10.000 imagens, exceto 21. Geralmente, os programadores acreditam que resolver uma tarefa tão complexa exige um algoritmo complexo. Mas até o NS está </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabalhando</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o detentor do registro usa algoritmos bastante simples, que são pequenas variações daqueles que examinamos neste capítulo. </font><font style="vertical-align: inherit;">Toda a complexidade aparece automaticamente durante o treinamento com base em dados de treinamento. </font><font style="vertical-align: inherit;">Em certo sentido, a moral de nossos resultados e os contidos em trabalhos mais complexos é que, para algumas tarefas</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> algoritmo complexo ≤ algoritmo de treinamento simples + bons dados de treinamento </font></font></blockquote><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Para aprendizagem profunda </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Embora nossa rede esteja mostrando um desempenho impressionante, ela é alcançada de maneira misteriosa. </font><font style="vertical-align: inherit;">Pesos e redes de mistura são detectados automaticamente. </font><font style="vertical-align: inherit;">Portanto, não temos uma explicação pronta de como a rede faz o que faz. </font><font style="vertical-align: inherit;">Existe alguma maneira de entender os princípios básicos de classificação por uma rede de números manuscritos? </font><font style="vertical-align: inherit;">E é possível, dados a eles, melhorar o resultado?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reformulamos essas questões de maneira mais rigorosa: suponhamos que em algumas décadas o SN se transforme em inteligência artificial (IA). Vamos entender como essa IA funciona? Talvez as redes permaneçam incompreensíveis para nós, com seus pesos e compensações, uma vez que são atribuídas automaticamente. Nos primeiros anos da pesquisa em IA, as pessoas esperavam que tentar criar IA também nos ajudasse a entender os princípios subjacentes à inteligência, e talvez até o trabalho do cérebro humano. No entanto, no final, pode acontecer que não entendamos nem o cérebro nem como a IA funciona! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para lidar com essas questões, lembremos da interpretação dos neurônios artificiais que dei no começo do capítulo - que essa é uma maneira de pesar evidências. Suponha que desejamos determinar se o rosto de uma pessoa está em uma imagem:</font></font><br><br><img src="https://habrastorage.org/webt/up/2e/af/up2eafnwrrpph5xdai4oapmgw8m.jpeg"><br><br><img src="https://habrastorage.org/webt/tu/8i/an/tu8ianduufnfebbjnqaibtzskyy.jpeg"><br><br><img src="https://habrastorage.org/webt/wf/tj/4j/wftj4j86hzytgwyypbw_eqk0jte.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esse problema pode ser abordado da mesma maneira que o reconhecimento de escrita: usando pixels de imagem como entrada para o NS, e a saída do NS será um neurônio que dirá: "Sim, isso é um rosto" ou "Não, isso não é um rosto. " </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suponha que façamos isso, mas sem usar um algoritmo de aprendizado. Vamos tentar criar manualmente uma rede, escolhendo os pesos e compensações apropriados. Como podemos abordar isso? Por um momento, esquecendo a Assembléia Nacional, poderíamos dividir a tarefa em subtarefas: a imagem dos olhos tem no canto superior esquerdo? Existe um olho no canto superior direito? Existe um nariz do meio? Existe uma boca no meio? Existe cabelo no topo?</font></font> E assim por diante <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se as respostas para várias dessas perguntas forem positivas, ou mesmo "provavelmente sim", concluímos que a imagem pode ter um rosto. Por outro lado, se as respostas forem não, provavelmente não há pessoa.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">É claro que essa é uma heurística aproximada e possui muitas deficiências. Talvez este seja um homem careca, e ele não tem cabelo. Talvez possamos ver apenas parte do rosto, ou o rosto em ângulo, de modo que algumas partes do rosto estejam fechadas. No entanto, a heurística sugere que, se pudermos resolver subproblemas com a ajuda de redes neurais, talvez possamos criar NSs para reconhecimento de face combinando redes para subtarefas. A seguir, é apresentada uma arquitetura possível de uma rede na qual as sub-redes são indicadas por retângulos. Essa não é uma abordagem completamente realista para solucionar o problema de reconhecimento de face: é necessária para nos ajudar a entender intuitivamente o trabalho das redes neurais. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/61b/dd0/ef6/61bdd0ef651ebc30afff87ed56204bd0.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nos retângulos existem subtarefas: a imagem dos olhos tem no canto superior esquerdo? Existe um olho no canto superior direito? Existe um nariz do meio? Existe uma boca no meio? Existe cabelo no topo? E assim por diante</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">É possível que as sub-redes também possam ser desmontadas em componentes. Pegue a questão de ter um olho no canto superior esquerdo. Pode ser distinguido em perguntas como: "Existe uma sobrancelha?", "Existem cílios?", "Existe um aluno?" e assim por diante. Obviamente, as perguntas devem conter informações sobre o local - "A sobrancelha está localizada no canto superior esquerdo, acima da pupila?", E assim por diante - mas vamos simplificá-lo por enquanto. Portanto, a rede que responde à pergunta sobre a presença do olho pode ser desmontada nos componentes: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d87/871/6ff/d878716ff0ecad83cfa5b0e9c54a866e.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“Existe uma sobrancelha?”, “Existem cílios?”, “Existe uma pupila?”</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essas perguntas podem ser divididas em pequenas, em etapas através de várias camadas. Como resultado, trabalharemos com sub-redes que respondem a perguntas tão simples que podem ser facilmente desmontadas no nível de pixel. Essas perguntas podem dizer respeito, por exemplo, à presença ou ausência de formas simples em determinados locais da imagem. Neurônios individuais associados a pixels poderão responder a eles.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O resultado é uma rede que divide perguntas muito complexas - se uma pessoa está na imagem - em perguntas muito simples que podem ser respondidas no nível de pixel individual. Ela fará isso através de uma sequência de várias camadas, nas quais as primeiras respondem a perguntas muito simples e específicas sobre a imagem, e as últimas criam uma hierarquia de conceitos mais complexos e abstratos. As redes com uma estrutura multicamada - duas ou mais camadas ocultas - são chamadas de redes neurais profundas (GNS).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Claro, eu não falei sobre como fazer essa sub-rede recursiva. Definitivamente, será impraticável selecionar manualmente pesos e compensações. Gostaríamos de usar algoritmos de treinamento para que a rede aprenda automaticamente pesos e desvios - e através deles as hierarquias de conceitos - com base nos dados de treinamento. Pesquisadores nas décadas de 1980 e 1990 tentaram usar a descida do gradiente estocástico e a retropropagação para treinar o GNS. Infelizmente, com exceção de algumas arquiteturas especiais, elas não tiveram êxito. As redes treinaram, mas muito lentamente, e, na prática, era muito lento para ser usado de alguma maneira.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Desde 2006, várias tecnologias foram desenvolvidas para treinar STS. Eles são baseados na descida do gradiente estocástico e na propagação de retorno, mas também contêm novas idéias. Eles permitiram treinar redes muito mais profundas - hoje as pessoas treinam silenciosamente redes com 5 a 10 camadas. E acontece que eles resolvem muitos problemas muito melhor do que os NSs rasos, ou seja, redes com uma camada oculta. A razão, é claro, é que o STS pode criar uma hierarquia complexa de conceitos. É semelhante à maneira como as linguagens de programação usam esquemas modulares e idéias de abstração para que possam criar programas de computador complexos. Comparar um NS profundo com um NS raso é aproximadamente como comparar uma linguagem de programação que pode fazer chamadas de função com uma linguagem que não. A abstração no NS não se parece com linguagens de programação,mas tem a mesma importância.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456738/">https://habr.com/ru/post/pt456738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456722/index.html">Receitas do PostgreSQL: Agendador de tarefas assíncronas</a></li>
<li><a href="../pt456724/index.html">5 maneiras extremamente simples de acelerar significativamente seu aplicativo VueJS</a></li>
<li><a href="../pt456730/index.html">Livro "{Você não conhece JS} Tipos e construções gramaticais"</a></li>
<li><a href="../pt456732/index.html">Ser um mentor</a></li>
<li><a href="../pt456736/index.html">Receitas do PostgreSQL: cURL: get, post e ... email</a></li>
<li><a href="../pt456740/index.html">Imersão em redes neurais convolucionais. Parte 5/1 - 9</a></li>
<li><a href="../pt456744/index.html">10 problemas que resolvi com lembretes no meu smartphone</a></li>
<li><a href="../pt456746/index.html">Big data - grande responsabilidade, muito estresse e muito dinheiro</a></li>
<li><a href="../pt456748/index.html">Impressora térmica 2003 de um mercado de pulgas: o que pode fazer em 2019?</a></li>
<li><a href="../pt456754/index.html">GitOps: comparando métodos Pull e Push</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>