<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸŒ¸ ğŸ‘©ğŸ¿â€âš–ï¸ ğŸ‘©ğŸ½â€ğŸ¨ NLP. Dasar-dasarnya. Teknik Pengembangan diri. Bagian 2: NER ğŸ’ˆ ğŸ§ ğŸ’´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bagian pertama artikel tentang dasar-dasar NLP dapat dibaca di sini . Hari ini kita akan berbicara tentang salah satu tugas NLP paling populer - Named...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NLP. Dasar-dasarnya. Teknik Pengembangan diri. Bagian 2: NER</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/449514/">  Bagian pertama artikel tentang dasar-dasar NLP dapat dibaca di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Hari ini kita akan berbicara tentang salah satu tugas NLP paling populer - Named-Entity Recognition (NER) - dan menganalisis secara detail arsitektur solusi untuk masalah ini. <br><br><img src="https://habrastorage.org/webt/fu/n9/-j/fun9-jbc0m4wmnvzwfwb7m4duui.png" alt="gambar"><br><a name="habracut"></a><br>  Tugas NER adalah untuk menyoroti bentang entitas dalam teks (rentang adalah fragmen teks yang berkelanjutan).  Misalkan ada teks berita, dan kami ingin menyorot entitas di dalamnya (beberapa set pra-tetap - misalnya, orang, lokasi, organisasi, tanggal, dan sebagainya).  Tugas NER adalah untuk memahami bahwa bagian dari teks â€œ <i>1 Januari 1997</i> â€ adalah tanggal, â€œ <i>Kofi Annan</i> â€ adalah orangnya, dan â€œ <i>PBB</i> â€ adalah organisasi. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gk/ow/au/gkowauf-i0k8yz2y7m81y4yamiu.png"></div><br>  Apa yang disebut entitas?  Dalam pengaturan klasik pertama, yang dirumuskan pada konferensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">MUC-6</a> pada tahun 1995, ini adalah orang-orang, lokasi dan organisasi.  Sejak itu, beberapa paket yang tersedia telah muncul, masing-masing memiliki seperangkat entitas bernama sendiri.  Biasanya, tipe entitas baru ditambahkan ke orang, lokasi, dan organisasi.  Yang paling umum dari mereka adalah numerik (tanggal, jumlah moneter), serta entitas Misc (dari aneka - entitas bernama lainnya; contohnya adalah iPhone 6). <br><br><h2>  Mengapa Anda perlu memecahkan masalah NER </h2><br>  Mudah dipahami bahwa, bahkan jika kita belajar membedakan dengan baik dalam teks orang, lokasi, dan organisasi, ini tidak mungkin menimbulkan minat yang besar di antara para pelanggan.  Meskipun beberapa aplikasi praktis, tentu saja, memiliki masalah dalam pengaturan klasik. <br><br>  Salah satu skenario ketika solusi untuk masalah dalam perumusan klasik mungkin masih diperlukan adalah penataan data yang tidak terstruktur.  Misalkan Anda memiliki beberapa jenis teks (atau satu set teks), dan data darinya harus dimasukkan ke dalam basis data (tabel).  Entitas bernama klasik dapat sesuai dengan baris tabel tersebut atau berfungsi sebagai konten beberapa sel.  Oleh karena itu, untuk mengisi tabel dengan benar, Anda harus terlebih dahulu memilih dalam teks data yang akan Anda masukkan ke dalamnya (biasanya setelah ini ada langkah lain - mengidentifikasi entitas dalam teks, ketika kami memahami bahwa <i>PBB</i> dan <i>PBB</i> mencakup "Lihat organisasi yang sama; namun, tugas identifikasi atau menghubungkan entitas adalah tugas lain, dan kami tidak akan membicarakannya secara rinci dalam pos ini). <br><br>  Namun, ada beberapa alasan mengapa NER adalah salah satu tugas NLP paling populer. <br><br>  Pertama, mengekstraksi entitas bernama adalah langkah menuju "memahami" teks.  Keduanya dapat memiliki nilai independen, dan membantu menyelesaikan tugas NLP lainnya dengan lebih baik. <br><br>  Jadi, jika kita tahu di mana entitas disorot dalam teks, maka kita dapat menemukan fragmen teks yang penting untuk beberapa tugas.  Sebagai contoh, kita dapat memilih hanya paragraf-paragraf di mana entitas dari jenis tertentu ditemukan, dan kemudian bekerja hanya dengan mereka. <br><br>  Misalkan Anda menerima sepucuk surat, dan alangkah baiknya membuat cuplikan hanya dari bagian itu di mana ada sesuatu yang bermanfaat, dan bukan hanya " <i>Halo, Ivan Petrovich</i> ".  Jika Anda dapat membedakan entitas yang disebutkan, Anda dapat membuat cuplikan cerdas dengan menunjukkan bagian surat itu di mana entitas yang menarik bagi kami berada (dan tidak hanya menunjukkan kalimat pertama dari surat itu, seperti yang sering dilakukan).  Atau Anda dapat dengan mudah menyoroti dalam teks bagian-bagian surat yang diperlukan (atau, langsung, entitas yang penting bagi kami) untuk kenyamanan analis. <br><br>  Selain itu, entitas merupakan lokasi yang kaku dan andal, pemilihannya dapat menjadi penting untuk banyak tugas.  Misalkan Anda memiliki nama untuk entitas yang bernama dan, apa pun itu, kemungkinan besar itu adalah berkelanjutan, dan semua tindakan dengan itu perlu dilakukan seperti dengan satu blok.  Misalnya, terjemahkan nama entitas ke nama entitas.  Anda ingin menerjemahkan <i>"Toko Pyaterochka"</i> ke dalam bahasa Prancis dalam satu potong, dan tidak terpecah menjadi beberapa fragmen yang tidak terkait satu sama lain.  Kemampuan untuk mendeteksi collocations juga berguna untuk banyak tugas lain - misalnya, untuk parsing sintaksis. <br><br>  Tanpa menyelesaikan masalah NER, sulit membayangkan solusi bagi banyak masalah NLP, misalnya, menyelesaikan kata ganti anafora atau membangun sistem tanya jawab.  Anafora kata ganti memungkinkan kita untuk memahami elemen teks yang dirujuk oleh kata ganti tersebut.  Sebagai contoh, mari kita ingin menganalisis teks â€œ <i>Charming Galloped on a White Horse.</i>  <i>Sang putri berlari untuk menemuinya dan menciumnya</i> . "  Jika kita menyoroti esensi Persona pada kata "Tampan", maka mesin itu akan jauh lebih mudah untuk memahami bahwa sang putri kemungkinan besar tidak mencium kuda, tetapi sang pangeran Tampan. <br><br>  Sekarang kami memberikan contoh bagaimana alokasi entitas yang dinamai dapat membantu dalam pembangunan sistem tanya jawab.  Jika Anda mengajukan pertanyaan " <i>Siapa yang memainkan peran Darth Vader dalam film" The Empire Strikes Back "</i> " di mesin pencari favorit Anda, "maka dengan probabilitas tinggi Anda akan mendapatkan jawaban yang tepat.  Ini dilakukan hanya dengan mengisolasi entitas bernama: kita memilih entitas (film, peran, dll.), Memahami apa yang diminta, dan kemudian mencari jawabannya di database. <br><br>  Mungkin pertimbangan yang paling penting karena tugas NER sangat populer: pernyataan masalah sangat fleksibel.  Dengan kata lain, tidak ada yang memaksa kita untuk memilih lokasi, orang, dan organisasi.  Kita dapat memilih potongan teks berkelanjutan yang kita butuhkan yang agak berbeda dari teks lainnya.  Sebagai hasilnya, Anda dapat memilih set entitas Anda sendiri untuk tugas praktis spesifik yang datang dari pelanggan, menandai badan teks dengan set ini dan melatih model.  Skenario seperti itu ada di mana-mana, dan ini menjadikan NER salah satu tugas NLP yang paling sering dilakukan di industri. <br><br>  Saya akan memberikan beberapa contoh kasus seperti itu dari pelanggan tertentu, dalam solusi yang saya ikuti. <br><br>  Ini yang pertama: biarkan Anda memiliki satu set faktur (transfer uang).  Setiap faktur memiliki deskripsi teks, yang berisi informasi yang diperlukan tentang transfer (siapa, siapa, kapan, apa, dan untuk alasan apa dikirim).  Sebagai contoh, perusahaan X mentransfer $ 10 ke perusahaan Y pada tanggal ini dan itu untuk ini dan itu.  Teksnya cukup formal, tetapi ditulis dalam bahasa hidup.  Bank secara khusus melatih orang-orang yang membaca teks ini dan kemudian memasukkan informasi yang terkandung di dalamnya ke dalam basis data. <br><br>  Kami dapat memilih satu set entitas yang sesuai dengan kolom tabel dalam database (nama perusahaan, jumlah transfer, tanggalnya, jenis transfer, dll.) Dan belajar cara memilihnya secara otomatis.  Setelah ini, tetap hanya memasukkan entitas yang dipilih dalam tabel, dan orang-orang yang sebelumnya membaca teks dan memasukkan informasi ke dalam basis data akan dapat melakukan tugas yang lebih penting dan berguna. <br><br>  Kasus pengguna kedua adalah ini: Anda perlu menganalisis surat dengan pesanan dari toko online.  Untuk melakukan ini, Anda perlu mengetahui nomor pesanan (sehingga semua surat yang terkait dengan pesanan ini dapat ditandai atau dimasukkan ke dalam folder terpisah), serta informasi bermanfaat lainnya - nama toko, daftar barang yang dipesan, jumlah cek, dll. Semua ini - nomor pesanan, nama toko, dll. - dapat dianggap sebagai entitas, dan juga mudah untuk belajar membedakannya menggunakan metode yang sekarang akan kita analisis. <br><br><h2>  Jika NER sangat berguna, mengapa itu tidak digunakan di mana-mana? </h2><br>  Mengapa tugas APM tidak selalu terpecahkan dan pelanggan komersial masih bersedia membayar bukan uang terkecil untuk solusinya?  Tampaknya semuanya sederhana: untuk memahami bagian teks mana yang akan disorot, dan menyorotnya. <br><br>  Namun dalam hidup, semuanya tidak begitu mudah, berbagai kesulitan muncul. <br><br>  Kompleksitas klasik yang mencegah kita hidup dalam menyelesaikan berbagai masalah NLP adalah segala macam ambiguitas dalam bahasa.  Misalnya, kata-kata dan homonim polisemantik (lihat contoh di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">bagian 1</a> ).  Ada jenis homonim yang berbeda yang secara langsung terkait dengan tugas NER - entitas yang sama sekali berbeda dapat disebut kata yang sama.  Misalnya, mari kita beri kata " <i>Washington</i> ."  Apa ini  Orang, kota, negara bagian, nama toko, nama anjing, objek, sesuatu yang lain?  Untuk menyoroti bagian teks ini sebagai entitas tertentu, orang perlu mempertimbangkan banyak hal - konteks lokal (tentang apa teks sebelumnya), konteks global (pengetahuan tentang dunia).  Seseorang memperhitungkan semua ini, tetapi tidak mudah untuk mengajarkan mesin untuk melakukan ini. <br><br>  Kesulitan kedua adalah teknis, tetapi jangan meremehkannya.  Tidak peduli bagaimana Anda mendefinisikan esensi, kemungkinan besar akan ada beberapa batasan dan kasus-kasus sulit - ketika Anda perlu menyoroti esensi, ketika Anda tidak perlu apa yang harus dimasukkan dalam rentang entitas, dan apa yang tidak, dll. (Tentu saja, jika esensi kami adalah bukan sesuatu yang sedikit variabel, seperti email; namun, Anda biasanya dapat membedakan entitas sepele seperti itu dengan metode sepele - menulis ekspresi reguler dan tidak memikirkan pembelajaran mesin apa pun). <br><br>  Misalkan, misalnya, kami ingin menyorot nama toko. <br><br>  Dalam teks " <i>Toko Detektor Logam Profesional Menyambut Anda</i> ", kami hampir pasti ingin memasukkan kata "toko" dalam esensi kami - ini jelas bagian dari namanya. <br><br>  Contoh lain adalah " <i>Anda disambut oleh Volkhonka Prestige, toko merek favorit Anda dengan harga terjangkau</i> ."  Mungkin, kata "toko" tidak boleh dimasukkan dalam anotasi - ini jelas bukan bagian dari namanya, tetapi hanya deskripsinya.  Selain itu, jika Anda memasukkan kata ini dalam nama, Anda juga harus memasukkan kata "- favorit Anda," dan ini, mungkin, saya tidak ingin melakukannya sama sekali. <br><br>  Contoh ketiga: <i>"Toko hewan peliharaan Nemo menulis untuk Anda.</i> "  Tidak jelas apakah "toko hewan peliharaan" adalah bagian dari nama atau tidak.  Dalam contoh ini, tampaknya setiap pilihan akan memadai.  Namun, penting bahwa kita perlu membuat pilihan ini dan memperbaikinya dalam instruksi untuk spidol, sehingga dalam semua teks contoh-contoh tersebut ditandai secara sama (jika ini tidak dilakukan, pembelajaran mesin pasti akan mulai membuat kesalahan karena kontradiksi dalam markup). <br><br>  Ada banyak contoh garis batas seperti itu, dan jika kita ingin penandaannya konsisten, semuanya harus dimasukkan dalam instruksi untuk penanda tersebut.  Bahkan jika contoh itu sendiri sederhana, mereka perlu diperhitungkan dan dihitung, dan ini akan membuat instruksi lebih besar dan lebih rumit. <br><br>  Nah, semakin rumit instruksinya, di sana Anda membutuhkan spidol yang lebih berkualitas.  Ini adalah satu hal ketika juru tulis perlu menentukan apakah surat itu adalah teks dari urutan atau tidak (meskipun ada seluk-beluk dan kasus batas di sini), dan itu adalah hal lain ketika juru tulis perlu membaca instruksi 50 halaman, menemukan entitas tertentu, memahami apa yang harus dimasukkan dalam penjelasan dan apa yang tidak. <br><br>  Marker yang terampil mahal, dan biasanya tidak bekerja dengan sangat cepat.  Anda akan menghabiskan uang dengan pasti, tetapi sama sekali bukan fakta bahwa Anda mendapatkan markup sempurna, karena jika instruksinya kompleks, bahkan orang yang memenuhi syarat dapat membuat kesalahan dan memahami sesuatu yang salah.  Untuk mengatasi hal ini, banyak markup dari teks yang sama digunakan oleh orang yang berbeda, yang selanjutnya meningkatkan harga markup dan waktu pembuatannya.  Menghindari proses ini atau bahkan menguranginya secara serius tidak akan berhasil: untuk belajar, Anda perlu memiliki serangkaian pelatihan berkualitas tinggi dengan ukuran yang masuk akal. <br><br>  Ini adalah dua alasan utama mengapa NER belum menaklukkan dunia dan mengapa pohon apel masih belum tumbuh di Mars. <br><br><h2>  Bagaimana memahami apakah masalah NER telah dipecahkan secara berkualitas </h2><br>  Saya akan memberi tahu Anda sedikit tentang metrik yang digunakan orang untuk mengevaluasi kualitas solusi mereka untuk masalah NER, dan tentang kasus standar. <br><br>  Metrik utama untuk tugas kami adalah pengukuran-f yang ketat.  Jelaskan apa itu. <br><br>  Mari kita memiliki markup uji (hasil kerja sistem kami) dan standar (markup yang benar dari teks yang sama).  Lalu kita dapat menghitung dua metrik - akurasi dan kelengkapan.  Akurasi adalah fraksi dari entitas positif sejati (yaitu entitas yang dipilih oleh kami dalam teks, yang juga ada dalam standar), relatif terhadap semua entitas yang dipilih oleh sistem kami.  Dan kelengkapan adalah bagian dari entitas positif sejati sehubungan dengan semua entitas yang ada dalam standar.  Contoh dari classifier yang sangat akurat, tetapi tidak lengkap adalah classifier yang memilih satu objek yang benar dalam teks dan tidak ada yang lain.  Contoh dari pengklasifikasi yang sangat lengkap, tetapi umumnya tidak akurat adalah pengklasifikasi yang memilih entitas pada setiap segmen teks (dengan demikian, selain semua entitas standar, classifier kami mengalokasikan sejumlah besar sampah). <br><br>  Ukuran-F adalah rata-rata harmonis dari akurasi dan kelengkapan, metrik standar. <br><br>  Seperti yang kami jelaskan di bagian sebelumnya, membuat markup itu mahal.  Oleh karena itu, tidak banyak bangunan yang dapat diakses dengan markup. <br><br>  Ada beberapa variasi untuk bahasa Inggris - ada konferensi populer di mana orang berkompetisi dalam memecahkan masalah NER (dan markup dibuat untuk kompetisi).  Contoh konferensi semacam itu di mana tubuh mereka dengan entitas bernama dibuat adalah MUC, TAC, CoNLL.  Semua kasus ini hampir secara eksklusif berisi teks berita. <br><br>  Badan utama di mana kualitas penyelesaian masalah NER dievaluasi adalah kasus CoNLL 2003 (di sini adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tautan ke kasus itu sendiri</a> , di sini adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel tentang itu</a> ).  Ada sekitar 300 ribu token dan hingga 10 ribu entitas.  Sekarang sistem SOTA (canggih) - yaitu, hasil terbaik saat ini) menunjukkan pada kasus ini ukuran-f dari urutan 0,93. <br><br>  Untuk bahasa Rusia, semuanya jauh lebih buruk.  Ada satu badan publik ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">FactRuEval 2016</a> , ini adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel tentang itu</a> , ini adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel tentang HabrÃ©</a> ), dan itu sangat kecil - hanya ada 50 ribu token.  Dalam hal ini, kasusnya cukup spesifik.  Secara khusus, esensi LocOrg yang agak kontroversial (lokasi dalam konteks organisasi) menonjol dalam kasus ini, yang dikacaukan dengan organisasi dan lokasi, sehingga kualitas pemilihan yang terakhir lebih rendah daripada yang seharusnya. <br><br><h2>  Bagaimana mengatasi masalah NER </h2><br><h3>  Pengurangan masalah NER ke masalah klasifikasi </h3><br>  Terlepas dari kenyataan bahwa entitas sering bertele-tele, tugas NER biasanya turun ke masalah klasifikasi di tingkat token, yaitu, masing-masing token milik salah satu dari beberapa kelas yang mungkin.  Ada beberapa cara standar untuk melakukan ini, tetapi yang paling umum disebut skema BIOES.  Skema ini untuk menambahkan beberapa awalan ke label entitas (misalnya, PER untuk orang atau ORG untuk organisasi), yang menunjukkan posisi token dalam rentang entitas.  Lebih detail: <br><br>  B - dari kata awal - token pertama dalam rentang entitas, yang terdiri dari lebih dari 1 kata. <br>  Saya - dari kata-kata di dalam - inilah yang ada di tengah. <br>  E - dari kata ending, ini adalah token terakhir dari entitas, yang terdiri dari lebih dari 1 elemen. <br>  S itu tunggal.  Kami menambahkan awalan ini jika entitas terdiri dari satu kata. <br><br>  Jadi, kami menambahkan satu dari 4 awalan yang mungkin untuk setiap jenis entitas.  Jika token bukan milik entitas apa pun, itu ditandai dengan label khusus, biasanya diberi label OUT atau O. <br><br>  Kami memberi contoh.  Marilah kita memiliki teks " <i>Karl Friedrich Jerome von Munchausen lahir di Bodenwerder</i> ."  Di sini ada satu entitas verbose - orang "Karl Friedrich Jerome von MÃ¼nhausen" dan satu kata satu - lokasi "Bodenwerder". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/gp/2f/iagp2fausaarttfscowpqo8xpic.png"></div><br><br>  Dengan demikian, BIOES adalah cara untuk memetakan proyeksi bentang atau anotasi ke tingkat token. <br><br>  Jelas bahwa dengan markup ini kita dapat dengan jelas menetapkan batas-batas semua penjelasan entitas.  Memang, tentang masing-masing token, kita tahu apakah benar suatu entitas dimulai dengan token ini atau berakhir di atasnya, yang berarti apakah akan mengakhiri anotasi entitas pada token yang diberikan, atau memperluasnya ke token berikutnya. <br><br>  Sebagian besar peneliti menggunakan metode ini (atau variasinya dengan lebih sedikit label - BIOE atau BIO), tetapi memiliki beberapa kelemahan signifikan.  Yang utama adalah bahwa skema tidak memungkinkan bekerja dengan entitas bersarang atau berpotongan.  Misalnya, esensi " <i>Universitas Negeri Moskow dinamai M.V.</i>  <i>Lomonosov</i> â€adalah satu organisasi.  Tapi Lomonosov sendiri adalah seseorang, dan akan menyenangkan untuk bertanya di markup.  Dengan menggunakan metode markup yang dijelaskan di atas, kita tidak pernah dapat menyampaikan kedua fakta ini secara bersamaan (karena kita hanya dapat membuat satu tanda pada satu token).  Oleh karena itu, token "Lomonosov" dapat menjadi bagian dari anotasi organisasi, atau bagian dari anotasi orang tersebut, tetapi tidak pernah keduanya sekaligus pada saat yang bersamaan. <br><br>  Contoh lain dari entitas tertanam: " <i>Departemen Logika Matematika dan Teori Algoritma Fakultas Mekanika dan Matematika Universitas Negeri Moskow</i> ".  Di sini, idealnya, saya ingin membedakan 3 organisasi bersarang, tetapi metode markup di atas memungkinkan Anda untuk memilih 3 entitas yang terpisah, atau satu entitas yang menjelaskan seluruh fragmen. <br><br>  Selain cara standar untuk mengurangi tugas ke klasifikasi di tingkat token, ada juga format data standar yang nyaman untuk menyimpan markup untuk tugas NER (serta untuk banyak tugas NLP lainnya).  Format ini disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CoNLL-U</a> . <br><br>  Gagasan utama dari format ini adalah ini: kami menyimpan data dalam bentuk tabel, di mana satu baris sesuai dengan satu token, dan kolom sesuai dengan jenis atribut token tertentu (termasuk kata itu sendiri, bentuk kata).  Dalam arti yang sempit, format CoNLL-U mendefinisikan jenis fitur mana (yaitu kolom) yang termasuk dalam tabel - total 10 jenis fitur untuk setiap token.  Tetapi para peneliti biasanya mempertimbangkan format lebih luas dan memasukkan jenis-jenis fitur yang diperlukan untuk tugas dan metode penyelesaiannya. <br><br>  Di bawah ini adalah contoh data dalam format seperti CoNLL-U, di mana 6 jenis atribut dipertimbangkan: jumlah kalimat saat ini dalam teks, bentuk kata (yaitu kata itu sendiri), lemma (bentuk kata awal), tag POS (bagian dari ucapan), morfologis karakteristik kata dan, akhirnya, label entitas yang dialokasikan pada token ini. <br><br><img src="https://habrastorage.org/webt/yb/ue/th/ybuethtundoox6j79pt_yofveuq.png" alt="gambar"><br><br><h3>  Bagaimana Anda memecahkan masalah NER sebelumnya? </h3><br>  Sebenarnya, masalah dapat diselesaikan tanpa pembelajaran mesin - dengan bantuan sistem berbasis aturan (dalam versi paling sederhana - dengan bantuan ekspresi reguler).  Ini tampaknya ketinggalan jaman dan tidak efektif, namun, Anda perlu memahami jika area subjek Anda terbatas dan jelas dan jika entitas, dengan sendirinya, tidak memiliki banyak variabilitas, maka masalah NER diselesaikan menggunakan metode berbasis aturan dengan cepat dan efisien. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Misalnya, jika Anda perlu menyorot email atau entitas numerik (tanggal, uang atau nomor telepon), ekspresi reguler dapat membuat Anda lebih cepat sukses daripada mencoba menyelesaikan masalah menggunakan pembelajaran mesin. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Namun, begitu ambiguitas linguistik dari berbagai jenis berperan (kami menulis tentang beberapa di atas), metode sederhana seperti itu berhenti bekerja dengan baik. </font><font style="vertical-align: inherit;">Oleh karena itu, masuk akal untuk menggunakannya hanya untuk domain terbatas dan pada entitas yang sederhana dan jelas dapat dipisahkan dari teks lainnya. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Terlepas dari semua hal di atas, di gedung-gedung akademik hingga akhir tahun 2000-an, SOTA menunjukkan sistem yang didasarkan pada metode klasik pembelajaran mesin. </font><font style="vertical-align: inherit;">Mari kita lihat bagaimana mereka bekerja.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tanda </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sebelum embedding muncul, tanda utama token biasanya berupa bentuk kata - yaitu, indeks kata dalam kamus. Dengan demikian, setiap token diberi vektor Boolean dari dimensi besar (dimensi kamus), di mana di tempat kata indeks dalam kamus adalah 1, dan di tempat-tempat lain adalah 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selain bentuk kata, bagian-bagian ucapan (tag-POS) sering digunakan sebagai tanda-tanda token , karakter morfologis (untuk bahasa tanpa morfologi yang kaya - misalnya, bahasa Inggris, karakter morfologis praktis tidak berpengaruh), awalan (mis., beberapa karakter pertama kata), sufiks (sama, beberapa karakter terakhir dari token), keberadaan karakter khusus dalam token dan tampilan token. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dalam pengaturan klasik, tanda token yang sangat penting adalah jenis huruf kapitalnya, misalnya:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "Huruf pertama besar, sisanya kecil", </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "Semua huruf kecil", </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "Semua huruf besar", </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> atau umumnya "kapitalisasi non-standar" (diamati, khususnya, untuk token "iPhone"). </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Jika token memiliki kapitalisasi non-standar, sangat mungkin bahwa dapat disimpulkan bahwa token adalah semacam entitas, dan jenis entitas ini hampir tidak orang atau lokasi. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selain semua ini, surat kabar - kamus entitas secara aktif digunakan. Kita tahu bahwa Petya, Elena, Akaki adalah nama-nama, Ivanov, Rustaveli, von Goethe adalah nama-nama, dan Mytishchi, Barcelona, â€‹â€‹Sao Paulo adalah kota-kota. Penting untuk dicatat bahwa kamus entitas saja tidak menyelesaikan masalah ("Moskow" dapat menjadi bagian dari nama organisasi, dan "Elena" dapat menjadi bagian dari lokasi), tetapi mereka dapat meningkatkan solusinya. Namun, tentu saja, meskipun ambiguitasnya, token yang dimiliki oleh kamus entitas jenis tertentu adalah atribut yang sangat baik dan signifikan (begitu signifikan sehingga biasanya hasil penyelesaian masalah NER dibagi menjadi 2 kategori - dengan dan tanpa orang-orang koran). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jika Anda tertarik pada bagaimana orang memecahkan masalah NER ketika pohon-pohon besar, saya sarankan Anda untuk melihat artikel tersebut</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nadeau dan Sekine (2007), Sebuah survei Pengakuan dan Klasifikasi Entitas Bernama</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Metode yang dijelaskan ada, tentu saja, sudah ketinggalan zaman (bahkan jika Anda tidak dapat menggunakan jaringan saraf karena keterbatasan kinerja, Anda mungkin tidak akan menggunakan HMM, seperti yang dijelaskan dalam artikel, tetapi, katakanlah, peningkatan gradien), tetapi lihat deskripsi gejala mungkin masuk akal. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fitur menarik termasuk pola kapitalisasi (pola ringkasan dalam artikel di atas). </font><font style="vertical-align: inherit;">Mereka masih dapat membantu dengan beberapa tugas NLP. </font><font style="vertical-align: inherit;">Jadi, pada tahun 2018, ada </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">upaya yang berhasil untuk</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> menerapkan pola kapitalisasi (bentuk kata) ke metode jaringan saraf untuk menyelesaikan masalah.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bagaimana mengatasi masalah NER menggunakan jaringan saraf? </font></font></h2><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> NLP hampir dari awal </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Upaya sukses pertama untuk memecahkan masalah NER menggunakan jaringan saraf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dilakukan pada tahun 2011</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pada saat publikasi artikel ini, ia menunjukkan hasil SOTA pada paket CoNLL 2003. Tetapi Anda perlu memahami bahwa keunggulan model dibandingkan dengan sistem yang didasarkan pada algoritma pembelajaran mesin klasik cukup tidak signifikan. Dalam beberapa tahun ke depan, metode berdasarkan ML klasik menunjukkan hasil yang sebanding dengan metode jaringan saraf.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selain menggambarkan upaya sukses pertama untuk memecahkan masalah NER dengan bantuan jaringan saraf, artikel ini menjelaskan secara rinci banyak poin bahwa sebagian besar karya pada topik NLP tidak disertakan. </font><font style="vertical-align: inherit;">Oleh karena itu, terlepas dari kenyataan bahwa arsitektur jaringan saraf yang dijelaskan dalam artikel sudah usang, masuk akal untuk membaca artikel tersebut. </font><font style="vertical-align: inherit;">Ini akan membantu untuk memahami pendekatan dasar untuk jaringan saraf yang digunakan dalam menyelesaikan masalah NER (dan lebih luas, banyak tugas NLP lainnya). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami akan memberi tahu Anda lebih banyak tentang arsitektur jaringan saraf yang dijelaskan dalam artikel. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para penulis memperkenalkan dua jenis arsitektur yang sesuai dengan dua cara yang berbeda untuk memperhitungkan konteks token:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baik menggunakan "jendela" dari lebar yang diberikan (pendekatan berbasis jendela), </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> atau menganggap pendekatan berbasis kalimat sebagai konteks. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dalam kedua kasus, tanda yang digunakan adalah embeddings dari bentuk kata, serta beberapa tanda manual - huruf besar, bagian dari ucapan, dll. </font><font style="vertical-align: inherit;">Kami akan memberi tahu Anda lebih banyak tentang bagaimana mereka dihitung </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami mendapat daftar masukan kata-kata dari kalimat kami: misalnya, " </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kucing duduk di atas matras</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ".</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/n1/mo/pbn1mo7pfdzlc8aay2xm9lz3tse.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anggaplah ada tanda K yang berbeda untuk satu token (misalnya, bentuk kata, bagian ucapan, huruf besar, apakah token kita adalah yang pertama atau terakhir dalam kalimat, dll., Dapat bertindak sebagai tanda-tanda seperti itu). </font><font style="vertical-align: inherit;">Kita dapat mempertimbangkan semua tanda-tanda ini kategorikal (misalnya, bentuk kata sesuai dengan vektor Boolean panjang dimensi kamus, di mana 1 hanya pada koordinat indeks kata yang sesuai dalam kamus).</font></font> Biarkan <img src="https://habrastorage.org/webt/kq/0v/p-/kq0vp-1f7hnnnwfkk3ak6xk4lno.png" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apakah vektor Boolean sesuai dengan nilai atribut ke-i token j dalam kalimat. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Penting untuk dicatat bahwa dalam pendekatan berbasis kalimat, di samping fitur kategorikal yang ditentukan oleh kata-kata, fitur tersebut digunakan - pergeseran relatif terhadap token, label yang coba kita tentukan. Nilai atribut ini untuk nomor token i akan menjadi i-core, di mana core adalah jumlah token yang labelnya sedang kami coba tentukan saat ini (atribut ini juga dianggap kategorikal, dan vektor untuknya dihitung dengan cara yang sama seperti yang lain). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Langkah selanjutnya dalam menemukan tanda-tanda token adalah mengalikan masing-masing </font></font><img src="https://habrastorage.org/webt/kq/0v/p-/kq0vp-1f7hnnnwfkk3ak6xk4lno.png" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dengan matriks yang </font></font><img src="https://habrastorage.org/webt/wb/nv/h8/wbnvh8l37-5zigiw-vcap7nfmju.png" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">disebut Tabel Pencarian (dengan cara ini vektor Boolean "berubah" menjadi yang berkelanjutan). Ingat bahwa masing-masing</font></font><img src="https://habrastorage.org/webt/kq/0v/p-/kq0vp-1f7hnnnwfkk3ak6xk4lno.png" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adalah vektor Boolean di mana di satu tempat harganya 1, dan di tempat lain itu 0. Dengan demikian, ketika mengalikan</font></font><img src="https://habrastorage.org/webt/wb/nv/h8/wbnvh8l37-5zigiw-vcap7nfmju.png" alt="gambar">  pada <img src="https://habrastorage.org/webt/kq/0v/p-/kq0vp-1f7hnnnwfkk3ak6xk4lno.png" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, salah satu baris dalam matriks kami dipilih. Baris ini adalah penyematan fitur token yang sesuai. Matriks </font></font><img src="https://habrastorage.org/webt/wb/nv/h8/wbnvh8l37-5zigiw-vcap7nfmju.png" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(di mana saya dapat mengambil nilai dari 1 hingga K) adalah parameter dari jaringan kami, yang kami latih bersama dengan sisa lapisan jaringan saraf. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perbedaan antara metode bekerja dengan fitur-fitur kategorikal yang dijelaskan dalam artikel ini dan word2vec yang muncul kemudian (kami berbicara tentang bagaimana word2vec embeddings bentuk pra-dilatih di bagian sebelumnya dari posting kami) adalah bahwa matriks diinisialisasi secara acak di sini, dan dalam matriks word2vec yang pra-dilatih pada kasus besar pada tugas menentukan kata berdasarkan konteks (atau konteks dengan kata). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dengan demikian, untuk setiap token, vektor fitur kontinu diperoleh, yang merupakan gabungan dari hasil mengalikan semua jenis</font></font><img src="https://habrastorage.org/webt/kq/0v/p-/kq0vp-1f7hnnnwfkk3ak6xk4lno.png" alt="gambar">  pada <img src="https://habrastorage.org/webt/wb/nv/h8/wbnvh8l37-5zigiw-vcap7nfmju.png" alt="gambar">  . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sekarang kita akan mengerti bagaimana tanda-tanda ini digunakan dalam pendekatan berbasis kalimat (berbasis jendela secara ideologis lebih sederhana). Adalah penting bahwa kami akan meluncurkan arsitektur kami secara terpisah untuk setiap token (yaitu, untuk kalimat "Kucing duduk di atas tikar" kami akan meluncurkan jaringan kami 6 kali). Tanda-tanda di setiap run dikumpulkan sama, dengan pengecualian tanda yang bertanggung jawab untuk posisi token, label yang kami coba tentukan - token inti.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami mengambil vektor kontinu yang dihasilkan dari masing-masing token dan melewati mereka melalui konvolusi satu dimensi dengan filter dimensi tidak terlalu besar: 3-5. Dimensi filter sesuai dengan ukuran konteks yang diperhitungkan jaringan secara bersamaan, dan jumlah saluran sesuai dengan dimensi vektor kontinu sumber (jumlah dimensi embedding semua atribut). Setelah menerapkan konvolusi, kami mendapatkan matriks dimensi m oleh f, di mana m adalah sejumlah cara filter dapat diterapkan ke data kami (mis., Panjang kalimat dikurangi panjang filter ditambah satu), dan f adalah jumlah filter yang digunakan.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seperti hampir selalu ketika bekerja dengan konvolusi, setelah konvolusi kita menggunakan pooling - dalam hal ini max pooling (mis., Untuk setiap filter kita mengambil nilai maksimumnya pada seluruh kalimat), setelah itu kita mendapatkan vektor dimensi f. Dengan demikian, semua informasi yang terkandung dalam kalimat, yang mungkin kita perlukan ketika menentukan label token inti, dikompresi menjadi satu vektor (max pooling dipilih karena bukan rata-rata informasi pada kalimat yang penting bagi kita, tetapi nilai atribut di area terpentingnya) . "Konteks rata" ini memungkinkan kami untuk mengumpulkan tanda-tanda token kami di seluruh kalimat dan menggunakan informasi ini untuk menentukan label mana yang harus diterima oleh token inti.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selanjutnya, kita melewati vektor melalui perceptron multilayer dengan beberapa fungsi aktivasi (dalam artikel - HardTanh), dan sebagai lapisan terakhir kita menggunakan softmax yang sepenuhnya terhubung dari dimensi d, di mana d adalah jumlah label token yang mungkin. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dengan demikian, lapisan convolutional memungkinkan kita untuk mengumpulkan informasi yang terkandung dalam jendela dimensi filter, menyatukan - memilih informasi yang paling khas dalam kalimat (mengompresnya menjadi satu vektor), dan lapisan softmax - memungkinkan kita untuk menentukan label mana yang memiliki token nomor inti.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> CharCNN-BLSTM-CRF </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sekarang mari kita bicara tentang arsitektur CharCNN-BLSTM-CRF, yaitu, tentang apa yang SOTA pada periode 2016-2018 (pada tahun 2018 ada arsitektur berdasarkan embeddings pada model bahasa, setelah itu dunia NLP tidak akan pernah sama; tetapi saga ini bukan tentang ini). Sebagaimana diterapkan pada tugas NER, arsitektur pertama kali dijelaskan dalam artikel oleh </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lample et al (2016)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ma &amp; Hovy (2016)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lapisan pertama dari jaringan sama dengan di dalam pipa NLP yang diuraikan dalam bagian </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sebelumnya dari posting kami</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pertama, atribut konteks-independen dari setiap token dalam kalimat dihitung. Gejala biasanya dikumpulkan dari tiga sumber. Yang pertama adalah embedding bentuk kata dari token, yang kedua adalah tanda simbolik, yang ketiga adalah tanda-tanda tambahan: informasi tentang kapitalisasi, bagian dari pidato, dll. Penggabungan semua tanda-tanda ini membentuk tanda tanda yang bebas konteks. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami berbicara tentang embeddings bentuk kata secara rinci di bagian sebelumnya. Kami mencantumkan fitur tambahan, tetapi kami tidak mengatakan secara pasti bagaimana fitur tersebut tertanam di jaringan saraf. Jawabannya sederhana - untuk setiap kategori fitur tambahan, kita belajar dari awal tidak terlalu besar. Ini persis tabel pencarian dari paragraf sebelumnya, dan kami mengajari mereka persis seperti yang dijelaskan di sana. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sekarang kita akan tahu bagaimana tanda simbolik diatur.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pertama jawab pertanyaannya, apa itu. Sederhana - kami ingin setiap token mendapatkan vektor tanda ukuran konstan, yang hanya bergantung pada simbol yang membentuk token (dan tidak tergantung pada makna token dan atribut tambahan, seperti bagian dari ucapan). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kita sekarang beralih ke deskripsi arsitektur CharCNN (serta arsitektur CharRNN yang terkait dengannya). Kami diberi token, yang terdiri dari beberapa karakter. Untuk setiap simbol kita akan mengeluarkan vektor dari beberapa dimensi yang tidak terlalu besar (misalnya, 20) - simbol embedding. Embedded simbolik dapat dilakukan sebelum pelatihan, tetapi paling sering mereka belajar dari awal - ada banyak simbol bahkan dalam kasus yang tidak terlalu besar, dan emblem simbolik harus dilatih secara memadai.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/29/n7/t3/29n7t3o1ebdon6m7hfnmgqec3gu.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, kami memiliki embeddings dari semua simbol token kami, serta simbol tambahan yang menunjukkan batas token - paddings (biasanya embeddings paddings diinisialisasi dengan nol). Kami ingin mendapatkan dari vektor-vektor ini satu vektor dari beberapa dimensi konstan, yang merupakan tanda simbolis dari seluruh token dan mencerminkan interaksi antara simbol-simbol ini. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ada 2 metode standar. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yang sedikit lebih populer adalah menggunakan konvolusi satu dimensi (oleh karena itu bagian arsitektur ini disebut CharCNN). Kami melakukan ini dengan cara yang sama seperti yang kami lakukan dengan kata-kata dalam pendekatan berbasis kalimat dalam arsitektur sebelumnya.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, kami melewatkan hiasan semua karakter melalui konvolusi dengan filter dengan dimensi yang tidak terlalu besar (misalnya, 3), kami mendapatkan vektor dimensi dari jumlah filter. </font><font style="vertical-align: inherit;">Kami memproduksi penyatuan maksimum di atas vektor-vektor ini, kami mendapatkan 1 vektor dimensi dari jumlah filter. </font><font style="vertical-align: inherit;">Ini berisi informasi tentang simbol kata dan interaksinya dan akan menjadi vektor tanda simbolik token. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cara kedua untuk mengubah embeddings simbolis menjadi satu vektor adalah untuk memberi mereka menjadi jaringan saraf berulang bilateral (BLSTM atau BiGRU; apa itu, kami dijelaskan di bagian </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pertama dari posting kami</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Biasanya tanda simbolis dari token hanyalah gabungan dari kondisi terakhir maju dan mundur RNN.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, mari kita diberi vektor fitur token konteks-independen. </font><font style="vertical-align: inherit;">Menurutnya, kami ingin mendapatkan atribut konteks-sensitif.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nm/og/oi/nmogoisze82tw_mm6r-dvs52jh0.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ini dilakukan dengan menggunakan BLSTM atau BiGRU. Pada saat ini, layer menghasilkan vektor, yang merupakan gabungan dari output yang sesuai dari forward dan reverse RNN. Vektor ini berisi informasi baik tentang token sebelumnya dalam penawaran (itu ada di RNN langsung), dan tentang yang berikut (itu dalam RNN terbalik). Oleh karena itu, vektor ini adalah tanda token yang peka konteks. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arsitektur ini dapat digunakan dalam berbagai tugas NLP, dan karenanya dianggap sebagai bagian penting dari pipa NLP. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Namun, kami kembali ke masalah NER. Setelah menerima tanda-tanda konteks-sensitif dari semua token, kami ingin mendapatkan label yang benar untuk setiap token. Ini bisa dilakukan dengan banyak cara.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cara yang lebih sederhana dan lebih jelas adalah menggunakan, sebagai lapisan terakhir, dimensi d, sepenuhnya terhubung dengan softmax, di mana d adalah jumlah label token yang mungkin. Dengan demikian, kita mendapatkan probabilitas token untuk memiliki masing-masing label yang mungkin (dan kita dapat memilih yang paling memungkinkan dari mereka). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Metode ini berfungsi, namun, memiliki kelemahan yang signifikan - label token dihitung secara independen dari label token lain. Kami menganggap token tetangga sendiri karena BiRNN, tetapi label token tidak hanya bergantung pada token tetangga, tetapi juga pada label mereka. Misalnya, terlepas dari token, label I-PER terjadi hanya setelah B-PER atau I-PER. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cara standar untuk menjelaskan interaksi antara jenis label adalah dengan menggunakan CRF (bidang acak bersyarat). Kami tidak akan menjelaskan secara detail apa itu ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">di sini</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">deskripsi yang baik diberikan), tetapi kami menyebutkan bahwa CRF mengoptimalkan seluruh rantai label secara keseluruhan, dan tidak setiap elemen dalam rantai ini. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, kami menggambarkan arsitektur CharCNN-BLSTM-CRF, yang merupakan SOTA dalam tugas NER sampai munculnya embeddings pada model bahasa pada tahun 2018. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sebagai kesimpulan, mari kita bicara sedikit tentang pentingnya setiap elemen arsitektur. Untuk bahasa Inggris, CharCNN memberikan peningkatan ukuran-f sekitar 1%, CRF - sebesar 1-1,5%, dan tanda-tanda tambahan token tidak mengarah pada peningkatan kualitas (kecuali jika Anda menggunakan teknik yang lebih kompleks seperti pembelajaran multi-tugas, seperti dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wu et al. (2018)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). BiRNN adalah dasar arsitektur, yang, bagaimanapun, dapat digantikan oleh </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">transformator</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami berharap bahwa kami dapat memberi pembaca beberapa gagasan tentang masalah APM. </font><font style="vertical-align: inherit;">Meskipun ini adalah tugas yang penting, itu cukup sederhana, yang memungkinkan kami untuk menggambarkan solusinya dalam satu posting. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ivan Smurov, </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kepala Kelompok Penelitian Lanjutan NLP</font></font></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id449514/">https://habr.com/ru/post/id449514/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id449500/index.html">Bagaimana tidak panik jika banyak programmer datang berkunjung?</a></li>
<li><a href="../id449502/index.html">Barang Antik: Kaset Video Luar Biasa</a></li>
<li><a href="../id449506/index.html">Tinjauan Umum: enam cara untuk menggunakan proxy penduduk untuk menyelesaikan masalah perusahaan</a></li>
<li><a href="../id449508/index.html">10 fitur R berguna yang mungkin tidak Anda ketahui</a></li>
<li><a href="../id449510/index.html">.NET: Bagian Yang Baik - Dari CLR ke Komunitas</a></li>
<li><a href="../id449516/index.html">Bersiap-siap untuk hackathon: cara memeras diri sendiri dalam waktu maksimal 48 jam</a></li>
<li><a href="../id449518/index.html">Pilihan: 5 layanan bermanfaat untuk menulis artikel dalam bahasa Inggris</a></li>
<li><a href="../id449520/index.html">Bagaimana saya mengajar neuron dalam "dinosaurus" untuk bermain</a></li>
<li><a href="../id449522/index.html">Pikiran Pada Elixir: Pro Dan Kontra Dari Alat Yang Paling Populer Untuk Dev Beban Tinggi</a></li>
<li><a href="../id449524/index.html">Membedakan karakter dari sampah: bagaimana membangun model jaringan saraf yang kuat dalam tugas OCR</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>