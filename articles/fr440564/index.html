<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÖüèæ üë©üèø‚Äçüç≥ üê• R√©seau neuronal GPT-2 d'OpenAI. D√©marrage rapide üéø ‚òéÔ∏è üë®üèª‚Äçüè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sit√¥t le bruit sur le r√©seau neuronal BERT de Google, qui a montr√© des r√©sultats de pointe dans un certain nombre de t√¢ches conversationnelles (NLP) e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©seau neuronal GPT-2 d'OpenAI. D√©marrage rapide</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440564/"><p><img src="https://habrastorage.org/getpro/habr/post_images/1cf/d63/ae6/1cfd63ae6b68d59325ef90cc4ea93f35.png" alt="image"></p><br><p>  Sit√¥t le bruit sur le r√©seau neuronal BERT de Google, qui a montr√© des r√©sultats de pointe dans un certain nombre de t√¢ches conversationnelles (NLP) en apprentissage automatique, comme OpenAI a d√©ploy√© un nouveau d√©veloppement: GPT-2.  Ce r√©seau de neurones avec un nombre record de param√®tres √† l'heure actuelle (1,5 milliard, contre les 100 √† 300 millions couramment utilis√©s dans de tels cas) a pu g√©n√©rer des pages enti√®res de texte connect√©. </p><br><p>  Il est si bon de g√©n√©rer qu'OpenAI a refus√© de publier la version compl√®te, craignant qu'ils n'utilisent ce r√©seau de neurones pour cr√©er de fausses nouvelles, des commentaires et des critiques indiscernables des vrais. </p><br><p>  Cependant, dans OpenAI, une version r√©duite du r√©seau neuronal GPT-2 a √©t√© partag√©e avec 117 millions de param√®tres.  Nous allons le lancer via le service Google Colab et l'exp√©rimenter. </p><a name="habracut"></a><br><h1 id="nemnogo-predystorii">  Un peu de fond </h1><br><p>  Pour ceux qui n'ont pas suivi l'√©volution des progr√®s du traitement naturel de la parole (PNL). </p><br><p>  √Ä l'√©t√© 2018, OpenAI a pr√©-form√© sur un grand volume de texte un r√©seau neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GPT</a> bas√© sur l'architecture Transformer.  Il s'est av√©r√© que si vous remplacez quelques-unes des derni√®res couches et que vous la reconvertissez pour une t√¢che sp√©cifique (cette approche est appel√©e Fine Tuning et est largement utilis√©e dans l'apprentissage automatique), elle bat imm√©diatement les records pr√©c√©dents sur un large √©ventail de t√¢ches conversationnelles. </p><br><p>  Sur la base de ce d√©veloppement, Google a cr√©√© fin 2018 son propre r√©seau de neurones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BERT</a> .  Ils ont s√©rieusement am√©lior√© le r√©sultat en rendant le r√©seau neuronal bidirectionnel, contrairement au GPT. </p><br><p>  Ne voulant pas abandonner, en f√©vrier 2019, OpenAI a imm√©diatement augment√© son GPT de 10 fois et l'a form√© sur une quantit√© de texte encore plus importante - sur 8 millions de pages Web (soit un total de 40 Go de texte).  Le r√©seau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GPT-2</a> qui en r√©sulte est actuellement le plus grand r√©seau de neurones, avec un nombre sans pr√©c√©dent de param√®tres de 1,5 milliard (le BERT avait 340 millions dans le plus grand mod√®le et 110 millions dans le BERT standard). </p><br><p>  En cons√©quence, GPT-2 a pu g√©n√©rer des pages enti√®res de texte coh√©rent.  Avec des r√©f√©rences r√©p√©t√©es aux noms des personnages au cours du r√©cit, des citations, des r√©f√©rences √† des √©v√©nements connexes, etc.  Je ne donnerai pas d'exemples ici, mais r√©f√©rez ceux qui le souhaitent √† l'article original sur le blog OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Better Language Models and Its Implications</a> ou aux liens √† la fin de l'article. </p><br><p>  G√©n√©rer un texte coh√©rent de cette qualit√© est impressionnant en soi, mais la chose la plus int√©ressante est diff√©rente.  Le GPT-2 sans formation suppl√©mentaire a imm√©diatement montr√© des r√©sultats proches de l'√©tat de l'art sur un certain nombre de t√¢ches conversationnelles.  Je le r√©p√®te, qui a rat√© l'importance du moment - sans aucune formation suppl√©mentaire pour une t√¢che sp√©cifique! </p><br><p>  Comment ont-ils r√©ussi cela?  Il suffit de poser aux r√©seaux de neurones les bonnes questions. </p><br><h1 id="arhitektura-gpt-2">  Architecture GPT-2 </h1><br><p>  GPT-2 est form√© pour pr√©dire le mot suivant dans une phrase.  Il s'agit d'une approche classique pour g√©n√©rer du texte.  Dans un premier temps, les r√©seaux de r√©currence (RNN), en particulier le LSTM, ont tenu la primaut√© dans ce domaine.  Mais apr√®s l'invention de l'architecture <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Transformer</a> √† l'√©t√© 2017, elle a progressivement commenc√© √† s'imposer dans les t√¢ches conversationnelles.  Bien que le Transformer d'origine ait un probl√®me de stockage de longues s√©quences (les LSTM se souviennent des plus longues), la vitesse de formation et la profondeur du r√©seau ont plus que compens√© cela.  Soit dit en passant, un certain nombre de modifications du transformateur sont d√©j√† apparues - avec l'introduction de la r√©currence ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Universal Transformers</a> ), une modification pour les s√©quences plus longues ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Transformer-XL</a> ) et d'autres, mais jusqu'√† pr√©sent, seul un Transformer original l√©g√®rement r√©gl√© est utilis√© dans Google et OpenAI. </p><br><p>  Je me souviens que le BERT de Google a appris un peu diff√©remment: pr√©dire non pas le mot suivant dans une phrase, mais les mots (masqu√©s) manqu√©s dans une phrase.  Et aussi pour d√©terminer si deux phrases cons√©cutives sont une continuation logique l'une de l'autre, ou si elles ne sont en aucun cas li√©es par le sens.  Cela a permis au BERT d'√™tre un mod√®le de langage qui comprend la signification des mots en fonction de leur environnement (contexte).  Ce qui a d√©termin√© le succ√®s du BERT dans les t√¢ches NPL.  Mais seulement apr√®s un recyclage (r√©glage fin) pour une t√¢che sp√©cifique.  La simple pr√©diction de mots dans le mod√®le de base ne fonctionne pas tr√®s bien.  Vous pouvez jouer avec BERT dans votre navigateur (via Google Colab): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://habr.com/en/post/436878</a> . </p><br><p>  GPT-2 n'est pas n√©cessaire pour se recycler.  Ce n'est pas seulement un mod√®le de langage comme BERT, c'est un g√©n√©rateur de texte.  Donnez-lui simplement le d√©but de la phrase, puis elle compl√©tera son mot √† mot. </p><br><p>  Un d√©tail int√©ressant: la recherche OpenAI a montr√© que les tableaux de textes Wikip√©dia et de livres litt√©raires (que le BERT a √©tudi√©s en particulier) ont un style biais√©.  Par cons√©quent, form√©s uniquement sur eux, les r√©seaux de neurones ne g√©n√®rent pas tr√®s bien le texte.  Pour diversifier les donn√©es et les styles d'entr√©e, OpenAI a utilis√© GPT-2 pour la formation sur des pages Web r√©guli√®res collect√©es sur 8 millions de sites (un total de 40 Go de texte).  Et afin d'√©liminer les sites de publicit√© et de spammeurs, ils ont inclus dans les exemples de sites les liens vers lesquels dans le reddit ont une bonne note.  Autrement dit, les sites que les utilisateurs en direct ont trouv√© contiennent des informations utiles. </p><br><h1 id="pravilnyy-vopros-soderzhit-polovinu-otveta">  La bonne question contient la moiti√© de la r√©ponse. </h1><br><p>  Ainsi, GPT-2, gr√¢ce √† sa taille sans pr√©c√©dent, a pu g√©n√©rer des pages de texte coh√©rent.  Mais le plus √©tonnant, c'est qu'en lui posant la bonne question (c'est-√†-dire le d√©but correct d'une phrase), elle a pu r√©pondre √† diverses questions!  Tout simplement parce que la poursuite d'un tel d√©but est la plus naturelle. </p><br><p>  Par exemple, pour obtenir une r√©ponse √† la question ¬´Qu'est-ce que la Terre?¬ª, Vous pouvez appliquer √† l'entr√©e de ce r√©seau neuronal le d√©but de la phrase: ¬´La Terre est ...¬ª.  Et elle terminera cette phrase jusqu'au bout.  Parce que la r√©ponse sera une continuation naturelle de ce d√©but. </p><br><p>  De plus, en formant correctement le d√©but de la phrase, vous pouvez obtenir des explications pour diff√©rents publics cibles, en tenant compte de leur intelligence, de leur √¢ge et de leur √©ducation.  Imaginez des phrases continues: "Moi, en tant que scientifique, je crois que la Terre est ...".  Ou: "Moi, en tant que laboureur, je pr√©tends que la Terre est ...".  Ou: "Moi, en tant que professeur dans un jardin d'enfants, je vais maintenant vous expliquer, mes enfants, que la Terre est ...". </p><br><p>  Comme vous pouvez le voir, en formant les bonnes questions (le bon d√©but de la phrase), vous pouvez obtenir des r√©ponses de niveaux et de d√©tails compl√®tement diff√©rents.  D'une certaine mani√®re, une chose similaire se produit chez les gens.  Le m√©decin doit expliquer au patient l'√©volution de la maladie afin qu'il comprenne.  Au niveau du patient.  Si vous demandez √† un enfant de cinq ans pourquoi il a fait cela, alors il ne peut pas r√©pondre imm√©diatement (ce qui, naturellement, les enfants vivent avec des sentiments et des √©motions).  Mais pour donner la r√©ponse que l'on attend de lui, l'enfant commence √† l'inventer - √† g√©n√©rer du texte.  Bas√© sur le fait que la r√©ponse convient au parent et qu'elle correspond au moins en quelque sorte √† ce qui s'est pass√©.  Au d√©but, comme de nombreux parents le savent, ce seront des r√©ponses ridicules.  Mais en encourageant et en punissant ("dites-moi plus", "ne trouvez pas d'excuses"), l'enfant apprendra √† donner des r√©ponses d√©taill√©es et compl√®tes. </p><br><p>  Ce d√©veloppement d'OpenAI et la capacit√© du r√©seau GPT-2 √† fournir des r√©ponses aux t√¢ches conversationnelles sans formation suppl√©mentaire sp√©ciale pour une t√¢che sp√©cifique, ouvrent deux questions int√©ressantes: </p><br><p>  1) L'interpr√©tabilit√© des r√©seaux de neurones peut-elle √™tre obtenue par un tel g√©n√©rateur de texte √©l√©mentaire et le d√©but correct d'une phrase.  O√π la r√©ponse sera une extension naturelle.  Supposons, par exemple, qu'un r√©seau de neurones n'indique pas les sceaux dans une photographie par les nombres de coordonn√©es x-y, mais explique sa position en texte clair.  Puis, au cours de la clarification, en lui posant la bonne question, par exemple: "Je suis arriv√© √† cette conclusion parce que ...", vous pouvez en th√©orie avoir une explication sur la fa√ßon dont elle a trouv√© le chat sur la photo.  Et cette explication dans le cas extr√™me ne peut √™tre pire qu'humaine.  Ce qui r√©sout le probl√®me global d'interpr√©tabilit√© des r√©seaux de neurones. </p><br><p>  2) Un r√©seau neuronal pr√©-form√© sur de gros volumes de texte peut-il √™tre universel, avoir du bon sens et ne pas n√©cessiter de formation suppl√©mentaire pour des t√¢ches sp√©cifiques.  Cela signifie qu'en essayant d'imiter la parole humaine (r√©ponses humaines aux questions), le r√©seau neuronal doit in√©vitablement apprendre le bon sens afin de donner ces r√©ponses tr√®s similaires aux r√©ponses humaines.  Donner des r√©ponses fictives monosyllabiques, en g√©n√©ral, n'est pas typique des gens.  Pour la plupart, les gens donnent des r√©ponses ad√©quates et d√©taill√©es, ce qui signifie que le r√©seau doit apprendre √† faire de m√™me. </p><br><p>  Ces deux questions restent ouvertes, mais la premi√®re √©tape de leur approbation a d√©finitivement √©t√© franchie. </p><br><h1 id="a-tochnee">  Ou plut√¥t? </h1><br><p>  Si vous vous tenez maintenant, il vaut mieux vous asseoir.  Parce que c'est ainsi qu'OpenAI utilisant le r√©seau neuronal GPT-2 a obtenu ses r√©sultats dans des t√¢ches conversationnelles pour diff√©rents domaines: </p><br><p>  <strong>R√©ponses aux questions sur le texte</strong> </p><br><p>  Eh bien, c'est facile.  Ou a aliment√© le r√©seau quelques paragraphes avec une description qui comprenait quelque part au milieu, par exemple, "la pomme est sur la table", et √† la fin, il a √©t√© attribu√©: "la pomme est sur ..." et le r√©seau a √©t√© ajout√© √† la "table".  Parce qu'il est capable de se souvenir du contexte de plusieurs paragraphes. </p><br><p>  Ou aliment√© le r√©seau en tant que phrase initiale quelques exemples du type "Question: une question, R√©ponse: une r√©ponse", et √† la fin apr√®s la vraie question, ils ont ajout√©: "R√©ponse:".  Et le r√©seau neuronal a ajout√© la r√©ponse!  Puisqu'il a r√©v√©l√© la structure du document sur la question-r√©ponse pr√©c√©dente.  C'est incroyable. </p><br><p>  <strong>Version courte (r√©sum√©) du texte</strong> </p><br><p>  L'entr√©e est un long texte de plusieurs paragraphes ou m√™me des pages, et le r√©seau neuronal doit √©crire un court contenu.  Comment avez-vous obtenu ce comportement de GPT-2?  Juste apr√®s le texte, ils ont ajout√© "TL; DR".  Et c'est tout!  Cela s'est av√©r√© suffisant pour que le GPT-2 ajoute un r√©sum√© de l'article apr√®s ces caract√®res!  Parce que ces symboles sur Internet d√©signent souvent le r√©sum√© de la publication. </p><br><p>  <strong>Traduction de texte</strong> </p><br><p>  L'entr√©e GPT-2 a re√ßu le texte sous la forme: "bonjour = bonjour, chien = chien, vent = vent, chat = ...".  Et le r√©seau neuronal a ajout√© la traduction du dernier mot: "chat" (dans l'original en fran√ßais).  Parce qu'il r√©v√©lait la structure du document et le compl√©tait simplement par la suite la plus logique.  Si votre m√¢choire n'est toujours pas tomb√©e de tout cela, alors j'ai deux nouvelles pour vous, et les deux sont mauvaises =). </p><br><h1 id="zapusk-gpt-2-cherez-google-colab">  Lancement de GPT-2 via Google Colab </h1><br><p>  Malheureusement, la version compl√®te de GPT-2 dans OpenAI a √©t√© refus√©e √† √™tre partag√©e.  Motiver cela par le fait qu'en utilisant ce r√©seau de neurones, il sera trop facile de g√©n√©rer de fausses nouvelles et des critiques dans les magasins.  √Ä en juger par leur d√©claration, la discussion sur l'opportunit√© de pr√©senter ce mod√®le se poursuivra pendant les 6 prochains mois, √† la suite de l'OpenAI, ils d√©cideront de le t√©l√©charger ou non.  Cependant, pour une grande organisation, il n'est pas difficile de r√©p√©ter le mod√®le (il semble qu'ils l'ont form√© pour 256 TPU pendant plusieurs jours, et selon les estimations pr√©liminaires, cela leur a co√ªt√© environ 45 000 $) </p><br><p>  Cependant, ils ont publi√© une version r√©duite de GPT-2 avec 117 millions de param√®tres (au lieu de 1,5 milliard, comme dans le mod√®le complet): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/openai/gpt-2</a> .  Essayons de l'ex√©cuter et de jouer avec ce mod√®le. </p><br><p>  Mise √† jour du 9 novembre 2019: enfin, toute la gamme de mod√®les a √©t√© pr√©sent√©e, dont 1,5 milliard, les fichiers et les instructions de lancement ont √©t√© mis √† jour. </p><br><p>  La fa√ßon la plus simple de le faire est d'utiliser Google Colab: </p><br><ol><li>  Ouvrez le lien </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/Gpt-2.ipynb</a> </p><br><ol><li>  Dans le menu <strong>Runtime</strong> , s√©lectionnez <strong>Ex√©cuter tout</strong> , de sorte que pour la premi√®re fois toutes les cellules d√©marrent, les t√©l√©chargements de mod√®le et les biblioth√®ques n√©cessaires sont connect√©s.  Acceptez de r√©initialiser tout le Runtime si n√©cessaire.  Saisissez le texte apr√®s l'apparition de "Invite de mod√®le &gt;&gt;&gt;" et appuyez sur Entr√©e. </li></ol><br><p>  Faites attention √† la ligne au tout d√©but: </p><br><p>  model_name = '117M' </p><br><p>  Ici, vous pouvez sp√©cifier la taille du mod√®le GPT-2 √† utiliser.  Les mod√®les suivants sont disponibles (sous r√©serve de mise √† jour): </p><br><p>  117M <br>  124M <br>  355M <br>  774M <br>  1558M </p><br><p>  Ici, 117M est le plus petit mod√®le qui √©tait le seul disponible au moment d'√©crire ces lignes.  OpenAI a ensuite pr√©sent√© des mod√®les toujours croissants, jusqu'au 5 novembre 2019, a d√©fini le maximum de 1558 millions (avec 1,5 milliard de param√®tres). </p><br><div class="spoiler">  <b class="spoiler_title">En cas de probl√®me ...</b> <div class="spoiler_text"><p>  Assurez-vous que GPU et Python 3 sont s√©lectionn√©s dans le menu Runtime -&gt; Change runtime type </p><br><p>  Si le bouton de connexion n'est pas actif, cliquez dessus pour devenir connect√©. </p></div></div><br><p>  Ou cr√©ez tout le code manuellement: </p><br><ol><li>  Acc√©dez √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://colab.research.google.com</a> </li><li>  Appuyez sur le bouton bleu NEW PYTHON 3 NOTEBOOK </li><li>  Dans le menu Runtime -&gt; Change runtime type, s√©lectionnez Python 3 et le GPU (ce dernier pour ex√©cuter le r√©seau neuronal sur le GPU) </li><li>  Dans la premi√®re cellule, saisissez: </li></ol><br><pre><code class="python hljs">model_name = <span class="hljs-string"><span class="hljs-string">'117M'</span></span> !git clone https://github.com/openai/gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> %cd gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> !pip3 install -r requirements.txt !python3 download_model.py $model_name</code> </pre> <br><p>  Au lieu de 117M (le plus petit), vous pouvez sp√©cifier n'importe quel mod√®le interm√©diaire ou plus grand: 1558M. </p><br><p>  Et cliquez sur l'ic√¥ne de lecture noire √† gauche de la cellule.  Cela va t√©l√©charger le r√©seau neuronal GPT-2 s√©lectionn√© et installer les d√©pendances n√©cessaires. </p><br><p>  Dans la deuxi√®me cellule (vous pouvez l'ajouter via le menu Insertion -&gt; Cellule Code ou en passant la souris sous le centre de la cellule courante, les boutons d'ajout appara√Ætront): </p><br><pre> <code class="python hljs">!python3 src/interactive_conditional_samples.py --model_name=$model_name</code> </pre> <br><p>  Cela lancera le mode interactif.  Attendez que le r√©seau de neurones d√©marre et qu'une fen√™tre s'affiche pour saisir du texte avec l'inscription "" Mod√®le d'invite &gt;&gt;&gt; ". Entrez le d√©but de la phrase et appuyez sur Entr√©e. Apr√®s un certain temps, le texte g√©n√©r√© appara√Æt sous l'en-t√™te SAMPLE. </p><br><p>  Vous pouvez √©galement d√©marrer le mode de g√©n√©ration de texte compl√®tement al√©atoire.  Le texte sera g√©n√©r√© √† l'infini en petits morceaux de SAMPLE 1, SAMPLE 2, etc. jusqu'√† ce que vous cliquiez sur le bouton Arr√™ter de la cellule.  Pour ce faire, cr√©ez une nouvelle cellule avec le code: </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt</code> </pre> <br><p>  Le r√©sultat sera enregistr√© dans le fichier samples.txt.  Il peut √™tre t√©l√©charg√© avec les commandes suivantes (cr√©ez √† nouveau une nouvelle cellule et ex√©cutez-la apr√®s avoir g√©n√©r√© le texte): </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">'samples.txt'</span></span>)</code> </pre> <br><p>  Vous pouvez modifier les param√®tres de g√©n√©ration du texte (coefficient d'al√©atoire, etc., voir la description dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">oeuvre originale</a> ): </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k <span class="hljs-number"><span class="hljs-number">40</span></span> --temperature <span class="hljs-number"><span class="hljs-number">0.7</span></span> | tee samples.txt</code> </pre> <br><p>  √âtant donn√© que le 117M est un mod√®le <strong>consid√©rablement</strong> r√©duit, ne vous attendez pas √† des miracles (mise √† jour: au moment de la r√©daction de ce document, seul il √©tait disponible. Maintenant, tout est disponible, y compris le plus grand 1558M d'origine, voir ci-dessus).  La plupart des √©chantillons g√©n√©r√©s seront absurdes.  Mais il y a aussi des sections significatives.  Le texte doit √™tre en anglais, tandis que dans d'autres langues, GPT-2 n'est pas encore en mesure de fonctionner. </p><br><h2 id="primery-generiruemogo-teksta">  Exemples de texte g√©n√©r√© </h2><br><p>  √âchantillons du texte g√©n√©r√© par le mod√®le <strong>complet</strong> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://blog.openai.com/better-language-models/#sample1</a> (en haut de la barre pour 8 histoires). </p><br><p>  Il existe √©galement un √©norme fichier texte de 2,4 Mo avec des √©chantillons g√©n√©r√©s de mani√®re al√©atoire: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-samples.txt</a> </p><br><p>  Et un de plus, 2,27 Mo, avec d'autres param√®tres de caract√®re al√©atoire: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-topk40-samples.txt</a> </p><br><h1 id="ssylki">  Les r√©f√©rences </h1><br><ul><li>  Article original du blog OpenAI: de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">meilleurs mod√®les de langage et leurs implications</a> </li><li>  Github avec toutes les versions pr√©-form√©es de GPT-2: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/openai/gpt-2</a> </li><li>  Discussion sur les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">principales nouvelles de</a> reddit </li><li>  Discussion sur Reddit refusant de publier le mod√®le complet: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">il est temps pour OpenAI de renommer CloseAI</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bloc-notes Google Colab pour ex√©cuter GPT-2 (tous les mod√®les) dans un navigateur</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr440564/">https://habr.com/ru/post/fr440564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr440554/index.html">BEM pratique</a></li>
<li><a href="../fr440556/index.html">Apprentissage de la conception de diagrammes de relation d'entit√©</a></li>
<li><a href="../fr440558/index.html">Une technologie qui rapprochera les r√©seaux quantiques</a></li>
<li><a href="../fr440560/index.html">Alexander Belokrylov et Dmitry Chuyko √† propos de Liberica JDK sur jug.msk.ru</a></li>
<li><a href="../fr440562/index.html">Windows Phone - TOUT, c'est encore ou encore</a></li>
<li><a href="../fr440566/index.html">Acc√©l√©rer sans entrave ou d√©couvrir SIMD</a></li>
<li><a href="../fr440568/index.html">Nous √©crivons une application d'apprentissage en Go et Javascript pour √©valuer les rendements r√©els des actions. Partie 2 - Tester le backend</a></li>
<li><a href="../fr440570/index.html">Cartes d'ombres r√©fl√©chissantes: Partie 2 - Mise en ≈ìuvre</a></li>
<li><a href="../fr440574/index.html">Russian AI Cup 2018, histoire 9 places</a></li>
<li><a href="../fr440576/index.html">Modifications importantes apport√©es √† CTE dans PostgreSQL 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>