<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚞 👨🏿‍🌾 ℹ️ Analyse approfondie des forêts et des articles (apprentissage + aléatoire) ✍🏻 👁️ 👨‍🏭</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous continuons de parler de la conférence sur les statistiques et l'apprentissage automatique AISTATS 2019. Dans cet article, nous analyserons des ar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analyse approfondie des forêts et des articles (apprentissage + aléatoire)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/458388/"><p>  Nous continuons de parler de la conférence sur les statistiques et l'apprentissage automatique AISTATS 2019. Dans cet article, nous analyserons des articles sur les modèles profonds d'ensembles d'arbres, mélangerons la régularisation pour les données très rares et l'approximation efficace du temps de la validation croisée. </p><br><p><img src="https://habrastorage.org/webt/n-/uv/xj/n-uvxjud1se0puoearqtlott2de.jpeg"></p><a name="habracut"></a><br><h2 id="algoritm-glubokiy-les-an-exploration-to-non-nn-deep-models-based-on-non-differentiable-modules">  Algorithme de forêt profonde: une exploration des modèles profonds non-NN basés sur des modules non différenciables </h2><br><p>  Zhi-Hua Zhou (Université de Nanjing) <br>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Présentation</a> <br>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Article</a> <br>  Implémentations - ci-dessous </p><br><p>  Un professeur chinois a parlé de l'ensemble des arbres, que les auteurs appellent la première formation approfondie sur des modules non différenciables.  Cela peut sembler une déclaration trop forte, mais ce professeur et son H-index 95 sont des conférenciers invités, ce fait nous permet de prendre la déclaration plus au sérieux.  La théorie de base de Deep Forest a été développée depuis longtemps, l'article original est déjà 2017 (près de 200 citations), mais les auteurs écrivent des bibliothèques et chaque année ils améliorent l'algorithme en vitesse.  Et maintenant, semble-t-il, ils ont atteint le point où cette belle théorie peut enfin être mise en pratique. </p><br><p>  <em>Vue générale de l'architecture Deep Forest</em> <br><img src="https://habrastorage.org/webt/4k/7q/1q/4k7q1qlzs5rixw4itr5luctdpuq.jpeg"></p><br><p>  <strong>Contexte</strong> </p><br><p>  Les modèles profonds, qui sont maintenant compris comme des réseaux de neurones profonds, sont utilisés pour capturer des dépendances de données complexes.  De plus, il s'est avéré que l'augmentation du nombre de couches est plus efficace que l'augmentation du nombre d'unités sur chaque couche.  Mais les réseaux de neurones ont leurs inconvénients: </p><br><ul><li>  Il faut beaucoup de données pour ne pas se recycler, </li><li>  Il faut beaucoup de ressources informatiques pour apprendre dans un laps de temps raisonnable, </li><li>  Trop d'hyperparamètres difficiles à configurer de manière optimale </li></ul><br><p>  De plus, les éléments des réseaux de neurones profonds sont des modules différenciables qui ne sont pas nécessairement efficaces pour chaque tâche.  Malgré la complexité des réseaux de neurones, des algorithmes conceptuellement simples, comme une forêt aléatoire, fonctionnent souvent mieux ou pas moins bien.  Mais pour de tels algorithmes, vous devez concevoir manuellement des fonctionnalités, ce qui est également difficile à faire de manière optimale. </p><br><p>  Les chercheurs ont déjà remarqué que les ensembles sur Kaggle: sont «très parfaits», et inspirés par les mots de Scholl et Hinton que la différenciation est le côté le plus faible du Deep Learning, ils ont décidé de créer un ensemble d'arbres aux propriétés DL. </p><br><p>  <em>Slide "Comment faire un bon ensemble"</em> <br><img src="https://habrastorage.org/webt/8w/cb/z9/8wcbz9ml-7qidb5ii4-meqcinec.jpeg"></p><br><p>  L'architecture a été déduite des propriétés des ensembles: les éléments des ensembles ne devraient pas être de très mauvaise qualité et différer. </p><br><p>  GcForest se compose de deux phases: Cascade Forest et Multi-Grained Scanning.  De plus, pour que la cascade ne se recycle pas, elle se compose de 2 types d'arbres - dont l'un est des arbres absolument aléatoires qui peuvent être utilisés sur des données non allouées.  Le nombre de couches est déterminé à l'intérieur de l'algorithme de validation croisée. <br><img src="https://habrastorage.org/webt/qv/co/-b/qvco-br5vregwj-rrxn3bxnyfeq.jpeg"></p><br><p>  <em>Deux types d'arbres</em> <br><img src="https://habrastorage.org/webt/mc/kp/ia/mckpiaiavjyh9hawcxhvtbusego.jpeg"></p><br><p>  <strong>Résultats</strong> </p><br><p>  En plus des résultats sur les ensembles de données standard, les auteurs ont essayé d'utiliser gcForest sur les transactions du système de paiement chinois pour rechercher la fraude et ont obtenu F1 et AUC beaucoup plus élevés que ceux de LR et DNN.  Ces résultats ne sont que dans la présentation, mais le code à exécuter sur certains ensembles de données standard est sur Git. </p><br><p><img src="https://habrastorage.org/webt/y3/kf/gy/y3kfgytp_qawqyskwmvrrumzdna.jpeg"></p><br><p>  <em>Résultats de substitution d'algorithme.</em>  <em>mdDF est la distribution optimale des marges Deep Forest, une variante de gcForest</em> </p><br><p><img src="https://habrastorage.org/webt/e1/oh/wq/e1ohwqrilda60nmdnosaa_ye4yk.jpeg"></p><br><p>  Avantages: </p><br><ul><li>  Peu d'hyperparamètres, le nombre de couches est ajusté automatiquement à l'intérieur de l'algorithme </li><li>  Les paramètres par défaut sont choisis pour bien fonctionner sur de nombreuses tâches. </li><li>  Complexité adaptative du modèle, sur de petites données - un petit modèle </li><li>  Pas besoin de définir de fonctionnalités </li><li>  Il fonctionne de qualité comparable aux réseaux de neurones profonds, et parfois mieux </li></ul><br><p>  Inconvénients: </p><br><ul><li>  Pas accéléré sur GPU </li><li>  Dans les images perd des DNN </li></ul><br><p>  Les réseaux de neurones ont un problème d'atténuation du gradient, tandis que la forêt profonde a un problème de «disparition de la diversité».  Puisqu'il s'agit d'un ensemble, plus il y a d'éléments «différents» et «bons» à utiliser, meilleure est la qualité.  Le problème est que les auteurs ont déjà essayé presque toutes les approches classiques (échantillonnage, randomisation).  Tant qu'aucune nouvelle recherche fondamentale n'apparaîtra sur le thème des «différences», il sera difficile d'améliorer la qualité de la forêt profonde.  Mais maintenant, il est possible d'améliorer la vitesse de calcul. </p><br><p>  <strong>Reproductibilité des résultats</strong> </p><br><p>  J'ai été intrigué par XGBoost sur les données tabulaires, et je voulais reproduire le résultat.  J'ai pris le jeu de données Adultes et appliqué GcForestCS (une version légèrement accélérée de GcForest) avec les paramètres des auteurs de l'article et XGBoost avec les paramètres par défaut.  Dans l'exemple que les auteurs avaient, les caractéristiques catégorielles étaient déjà prétraitées d'une manière ou d'une autre, mais il n'était pas indiqué comment.  En conséquence, j'ai utilisé CatBoostEncoder et une autre métrique - ROC AUC.  Les résultats étaient statistiquement différents - XGBoost a gagné.  Le temps de fonctionnement de XGBoost est négligeable, tandis que gcForestCS dispose de 20 minutes de chaque validation croisée à 5 fois.  D'autre part, les auteurs ont testé l'algorithme sur différents ensembles de données et ajusté les paramètres de cet ensemble de données à leur prétraitement d'entités. </p><br><p>  Le code peut être trouvé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </p><br><p>  <strong>Implémentations</strong> </p><br><p>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le code officiel des auteurs de l'article</a> <br>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Modification officielle améliorée, plus rapide, mais sans documentation</a> <br>  → La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mise en œuvre est plus simple</a> </p><br><h2 id="pclasso-the-lasso-meets-principal-components-regression">  PcLasso: le lasso rencontre la régression des principaux composants </h2><br><p>  J. Kenneth Tay, Jerome Friedman, Robert Tibshirani (Université de Stanford) </p><br><p>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Article</a> <br>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Présentation</a> <br>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Exemple d'utilisation</a> </p><br><p>  Début 2019, J.Kenneth Tay, Jerome Friedman et Robert Tibshirani de l'Université de Stanford ont proposé une nouvelle méthode d'enseignement avec un enseignant, particulièrement adaptée aux données rares. </p><br><p>  Les auteurs de l'article ont résolu le problème de l'analyse des données sur les études d'expression génique, qui sont décrites dans Zeng &amp; Breesy (2016).  La cible est le statut mutationnel du gène p53, qui régule l'expression des gènes en réponse à divers signaux de stress cellulaire.  Le but de l'étude est d'identifier les prédicteurs qui sont en corrélation avec le statut mutationnel de p53.  Les données se composent de 50 lignes, dont 17 sont classées comme normales et les 33 autres portent des mutations dans le gène p53.  Selon l'analyse de Subramanian et al.  (2005) 308 ensembles de gènes compris entre 15 et 500 sont inclus dans cette analyse.  Ces kits de gènes contiennent un total de 4 301 gènes et sont disponibles dans le package grpregOverlap R.  Lors de l'expansion des données pour traiter les groupes qui se chevauchent, 13 237 colonnes sont sorties.  Les auteurs de l'article ont utilisé la méthode pcLasso, qui a permis d'améliorer les résultats du modèle. </p><br><p>  <em>Dans l'image, nous voyons une augmentation de l'ASC lors de l'utilisation de "pcLasso"</em> <br><img src="https://habrastorage.org/webt/ok/p6/mg/okp6mgex-l9p49vcz5gedg8xa5o.jpeg"></p><br><p>  <strong>L'essence de la méthode</strong> </p><br><p>  La méthode combine <img src="https://tex.s2cms.ru/svg/l_1" alt="l_1">  -régularisation avec <img src="https://tex.s2cms.ru/svg/l_2" alt="l_2">  , qui rétrécit le vecteur de coefficients aux principaux composants de la matrice d'entités.  Ils ont appelé la méthode proposée «composants principaux du lasso» («pcLasso» disponible sur R).  La méthode peut être particulièrement puissante si les variables sont préalablement groupées (l'utilisateur choisit quoi et comment grouper).  Dans ce cas, pcLasso compresse chaque groupe et obtient la solution en direction des principaux composants de ce groupe.  Dans le processus de résolution, la sélection des groupes significatifs parmi ceux disponibles est également effectuée. </p><br><p>  Nous présentons la matrice diagonale de la décomposition singulière d'une matrice centrée de traits <img src="https://tex.s2cms.ru/svg/X" alt="X">  comme suit: </p><br><p>  Nous représentons notre décomposition singulière de la matrice centrée X (SVD) comme <img src="https://tex.s2cms.ru/svg/X%3DUDV%5ET" alt="X = UDV ^ T">  où <img src="https://tex.s2cms.ru/svg/D" alt="D">  Est une matrice diagonale composée de valeurs singulières.  Sous cette forme <img src="https://tex.s2cms.ru/svg/l_2" alt="l_2">  - La régularisation peut être représentée: <br><img src="https://tex.s2cms.ru/svg/%5Cbeta%5ET%20VZV%5ET%20%5Cbeta" alt="\ beta ^ T VZV ^ T \ beta">  où <img src="https://tex.s2cms.ru/svg/Z" alt="Z">  - matrice diagonale contenant la fonction des carrés de valeurs singulières: <img src="https://tex.s2cms.ru/svg/Z_%7B11%7D%3Df_1%20(d_1%5E2%2Cd_2%5E2%2C%E2%80%A6%2Cd_m%5E2%20)%2C%E2%80%A6%2CZ_%7B22%7D%3Df_2%20(d_1%5E2%2Cd_2%5E2%2C%E2%80%A6%2Cd_m%5E2%20)" alt="Z_ {11} = f_1 (d_1 ^ 2, d_2 ^ 2, ..., d_m ^ 2), ..., Z_ {22} = f_2 (d_1 ^ 2, d_2 ^ 2, ..., d_m ^ 2)">  . </p><br><p>  En général, dans <img src="https://tex.s2cms.ru/svg/l_2" alt="l_2">  -régularisation <img src="https://tex.s2cms.ru/svg/Z_%7Bjj%7D%3D1" alt="Z_ {jj} = 1">  pour tous <img src="https://tex.s2cms.ru/svg/j" alt="j">  cela correspond <img src="https://tex.s2cms.ru/svg/%5Cbeta%5ET%20%5Cbeta" alt="\ beta ^ T \ beta">  .  Ils suggèrent de minimiser les fonctionnalités suivantes: </p><br><p><img src="https://habrastorage.org/webt/6l/fj/lv/6lfjlv9m-zy8qfhcvrcqcymuuxa.jpeg"></p><br><p>  Ici <img src="https://tex.s2cms.ru/svg/D" alt="D">  - matrice des différences d'éléments diagonaux <img src="https://tex.s2cms.ru/svg/d_1%5E2-d_1%5E2%2Cd_1%5E2-d_2%5E2%2C%E2%80%A6%2Cd_1%5E2-d_m%5E2" alt="d_1 ^ 2-d_1 ^ 2, d_1 ^ 2-d_2 ^ 2, ..., d_1 ^ 2-d_m ^ 2">  .  En d'autres termes, nous contrôlons le vecteur <img src="https://tex.s2cms.ru/svg/%5Cbeta%20" alt="\ beta">  en utilisant également l'hyperparamètre <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  . <br>  En transformant cette expression, nous obtenons la solution: </p><br><p><img src="https://habrastorage.org/webt/vf/qs/6b/vfqs6b8fnqo3bmlacyr5j4fpigs.jpeg"></p><br><p>  Mais la principale «caractéristique» de la méthode est, bien sûr, la capacité de regrouper les données et, sur la base de ces groupes, de mettre en évidence les principales composantes du groupe.  Ensuite, nous réécrivons notre solution sous la forme: </p><br><p><img src="https://habrastorage.org/webt/9l/ij/wc/9lijwc3_kvwtvxalszzyt4zq4l4.jpeg"></p><br><p>  Ici <img src="https://tex.s2cms.ru/svg/%5Cbeta_k" alt="\ beta_k">  - sous-vecteur <img src="https://tex.s2cms.ru/svg/%5Cbeta" alt="\ beta">  correspondant au groupe k, <img src="https://tex.s2cms.ru/svg/d_k%3D(d_%7Bk1%7D%2C%E2%80%A6%2Cd_%7Bkmk%7D)" alt="d_k = (d_ {k1}, ..., d_ {kmk})">  - valeurs singulières <img src="https://tex.s2cms.ru/svg/X_k" alt="X_k">  classés par ordre décroissant, et <img src="https://tex.s2cms.ru/svg/D_%7Bd_%7Bk1%7D%5E2-d_%7Bkj%7D%5E2%7D" alt="D_ {d_ {k1} ^ 2-d_ {kj} ^ 2}">  - matrice diagonale <img src="https://tex.s2cms.ru/svg/d_%7Bk1%7D%5E2-d_%7Bkj%7D%5E2%2C%20j%3D1%2C2%2C%E2%80%A6%2Cm_k" alt="d_ {k1} ^ 2-d_ {kj} ^ 2, j = 1,2, ..., m_k"></p><br><p>  Quelques notes sur la solution de la fonctionnelle cible: </p><br><ol><li><p>  La fonction objectif est convexe et la composante non lisse est séparable.  Par conséquent, il peut être efficacement optimisé en utilisant une descente de gradient. <br>  L'approche consiste à valider plusieurs valeurs <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  (y compris zéro, respectivement, obtention de la norme <img src="https://tex.s2cms.ru/svg/l_1" alt="l_1">  -régularisation), puis optimiser: <img src="https://habrastorage.org/webt/uz/bf/eo/uzbfeori9kupj8b05x46dcj6iri.jpeg">  ramasser <img src="https://tex.s2cms.ru/svg/%5Clambda" alt="\ lambda">  .  En conséquence, les paramètres <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  et <img src="https://tex.s2cms.ru/svg/%5Clambda" alt="\ lambda">  sont sélectionnés pour la validation croisée. </p><br></li><li><p>  Paramètre <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  difficile à interpréter.  Dans le logiciel (package pcLasso), l'utilisateur définit lui-même la valeur de ce paramètre, qui appartient à l'intervalle [0,1], où 1 correspond à <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  = 0 (lasso). </p><br></li></ol><br><p>  En pratique, faire varier les valeurs <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  = 0,25, 0,5, 0,75, 0,9, 0,95 et 1, vous pouvez couvrir une large gamme de modèles. </p><br><p>  <em>L'algorithme lui-même est le suivant</em> <br><img src="https://habrastorage.org/webt/l-/3x/si/l-3xsipork2hzeh7ws1bg1hz86o.jpeg"></p><br><p>  Cet algorithme est déjà écrit en R, si vous le souhaitez, vous pouvez déjà l'utiliser.  La bibliothèque s'appelle 'pcLasso'. </p><br><h2>  Un couteau suisse infinitésimal </h2><br><p>  Ryan Giordano (UC Berkeley);  William Stephenson (MIT);  Runjing Liu (UC Berkeley); <br>  Michael Jordan (UC Berkeley);  Tamara Broderick (MIT) </p><br><p>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Article</a> <br>  → <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> </p><br><p>  La qualité des algorithmes d'apprentissage automatique est souvent mesurée par plusieurs validations croisées (validation croisée ou bootstrap).  Ces méthodes sont puissantes, mais lentes sur les grands ensembles de données. </p><br><p>  Dans ce travail, les collègues utilisent une approximation linéaire des poids, produisant des résultats qui fonctionnent plus rapidement.  Cette approximation linéaire est connue dans la littérature statistique sous le nom de «jackknife infinitésimal».  Il est principalement utilisé comme outil théorique pour prouver des résultats asymptotiques.  Les résultats de l'article sont applicables, que les poids et les données soient stochastiques ou déterministes.  Par conséquent, cette approximation estime séquentiellement la véritable validation croisée pour tout k fixe. </p><br><p>  <em>Remise du Paper Award à l'auteur de l'article</em> <br><img src="https://habrastorage.org/webt/3n/1a/-k/3n1a-kygdmkbs0drfjdg38b9z5g.jpeg"></p><br><p>  <strong>L'essence de la méthode</strong> </p><br><p>  Considérez le problème de l'estimation d'un paramètre inconnu <img src="https://tex.s2cms.ru/svg/%5Ctheta%20%5Cin%20%5COmega_%7B%5Ctheta%7D%20%5Csubset%20R%5E%7BD%7D" alt="\ theta \ in \ Omega _ {\ theta} \ sous-ensemble R ^ {D}">  où <img src="https://tex.s2cms.ru/svg/%5COmega_%7B%5Ctheta%7D%20" alt="\ Omega _ {\ theta}">  Est compact et la taille de notre jeu de données est <img src="https://tex.s2cms.ru/svg/N" alt="N">  .  Notre analyse sera réalisée sur un ensemble de données fixes.  Définissez notre note <img src="https://tex.s2cms.ru/svg/%5Ctheta%20%5Cin%20%5COmega_%7B%5Ctheta%7D%20" alt="\ theta \ in \ Omega _ {\ theta}">  comme suit: </p><br><ol><li>  Pour chacun <img src="https://tex.s2cms.ru/svg/n%3D1%2C2%E2%80%A6%2CN" alt="n = 1,2 ..., N">  ensemble <img src="https://tex.s2cms.ru/svg/g_n" alt="g_n">  ( <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ thêta">  ) Est fonction de <img src="https://tex.s2cms.ru/svg/%5COmega_%7B%5Ctheta%7D%20%5Csubset%20R%5E%7BD%7D" alt="\ Omega _ {\ theta} \ sous-ensemble R ^ {D}"></li><li><img src="https://tex.s2cms.ru/svg/%5Comega_n%20" alt="\ omega_n">  Est un nombre réel, et <img src="https://tex.s2cms.ru/svg/%5Comega" alt="\ oméga">  Est un vecteur composé de <img src="https://tex.s2cms.ru/svg/%5Comega_n" alt="\ omega_n"></li></ol><br><p>  Alors <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D" alt="\ hat {\ theta}">  peut être représenté comme: </p><br><p><img src="https://habrastorage.org/webt/zh/ce/yi/zhceyifw80rl6neeoaedkd7x-20.jpeg"></p><br><p>  En résolvant ce problème d'optimisation par la méthode du gradient, nous supposons que les fonctions sont différenciables et nous pouvons calculer la Hesse.  Le principal problème que nous résolvons est le coût de calcul associé à l'évaluation <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D%20%CC%82(%5Comega)" alt="\ hat {\ theta} ̂ (\ omega)">  pour tous <img src="https://tex.s2cms.ru/svg/%5Comega%E2%88%88W" alt="\ omega∈W">  .  La principale contribution des auteurs de l'article consiste à calculer l'estimation <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_1%3D%5Chat%7B%5Ctheta%7D_1%20(1_%7B%5Comega%7D)" alt="\ hat {\ theta} _1 = \ hat {\ theta} _1 (1 _ {\ omega})">  où <img src="https://tex.s2cms.ru/svg/1_%5Comega%3D(1%2C1%2C%E2%80%A6%2C1)" alt="1_ \ omega = (1,1, ..., 1)">  .  En d'autres termes, notre optimisation ne dépendra que des dérivés <img src="https://tex.s2cms.ru/svg/g_n%20(%5Ctheta)" alt="g_n (\ thêta)">  que nous supposons exister et sont hessois: </p><br><p><img src="https://habrastorage.org/webt/tb/zb/t2/tbzbt2u2q_bdqidfjxfl9ywqesc.jpeg"></p><br><p>  Ensuite, nous définissons une équation avec un point fixe et sa dérivée: <br><img src="https://habrastorage.org/webt/hj/8f/x3/hj8fx3broftye-ssmq4mwpkvaui.jpeg"></p><br><p>  Ici, il convient de faire attention à ce que <img src="https://tex.s2cms.ru/svg/G(%5Ctheta%20%CC%82(%5Comega)%2Cw)%3D0" alt="G (\ thêta ̂ (\ omega), w) = 0">  depuis <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D%20(%5Comega)" alt="\ hat {\ theta} (\ omega)">  - solution pour <img src="https://tex.s2cms.ru/svg/%5Cfrac%7B%201%20%7D%7B%20N%20%7D%20%5Csum_%7Bn%3D1%7D%5E%7BN%7D%20%5Comega_n%20g_n%20(%5Ctheta)%3D0" alt="\ frac {1} {N} \ sum_ {n = 1} ^ {N} \ omega_n g_n (\ theta) = 0">  .  Nous définissons également: <img src="https://tex.s2cms.ru/svg/H_1%3DH(%5Chat%7B%5Ctheta%7D_1%2C1_%5Comega)" alt="H_1 = H (\ hat {\ theta} _1,1_ \ omega)">  , et la matrice de poids comme: <img src="https://tex.s2cms.ru/svg/%5CDelta%5Comega%3D%20%5Comega-1_%5Comega%20%5Cin%20R%5E%7Bn%7D" alt="\ Delta \ omega = \ omega-1_ \ omega \ dans R ^ {n}">  .  Dans le cas où <img src="https://tex.s2cms.ru/svg/H_1" alt="H_1">  a une matrice inverse, nous pouvons utiliser le théorème de fonction implicite et la «règle de chaîne»: </p><br><p><img src="https://habrastorage.org/webt/c7/iv/x2/c7ivx2dadeupxwlo2hzgmxslrxe.jpeg"></p><br><p>  Cette dérivée nous permet de former une approximation linéaire <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D%20%CC%82(%5Comega)" alt="\ hat {\ theta} ̂ (\ omega)">  à travers <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_1" alt="\ hat {\ theta} _1">  qui ressemble à ceci: </p><br><p><img src="https://habrastorage.org/webt/lc/rc/5h/lcrc5hirn1act9jl8lp2jakjpsk.jpeg"></p><br><p>  Depuis <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_%7BIJ%7D" alt="\ hat {\ theta} _ {IJ}">  ne dépend que de <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_1" alt="\ hat {\ theta} _1">  et <img src="https://tex.s2cms.ru/svg/%5CDelta%20%5Comega" alt="\ Delta \ oméga">  , et non à partir de solutions pour d'autres valeurs <img src="https://tex.s2cms.ru/svg/%5Comega" alt="\ oméga">  , en conséquence, il n'est pas nécessaire de recalculer et de trouver de nouvelles valeurs de ω.  Au lieu de cela, il faut résoudre le SLE (système d'équations linéaires). </p><br><p>  <strong>Résultats</strong> </p><br><p>  En pratique, cela réduit considérablement le temps par rapport à la validation croisée: <br><img src="https://habrastorage.org/webt/sw/dr/6-/swdr6-j8t7pqcdwf_96705qs1tg.jpeg"></p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr458388/">https://habr.com/ru/post/fr458388/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr458374/index.html">TJBOT comme illustration des services IBM Watson</a></li>
<li><a href="../fr458376/index.html">Pas un autre langage de programmation. Partie 1: Logique de domaine</a></li>
<li><a href="../fr458378/index.html">Utilisation d'Avocode pour la mise en page du site. Revue pour les débutants. Bonus - enregistrez une période d'essai de 30 jours</a></li>
<li><a href="../fr458382/index.html">Pourquoi enseignons-nous cela?</a></li>
<li><a href="../fr458384/index.html">Examen et test du scanner 3D à lumière structurée Pro S3 HP</a></li>
<li><a href="../fr458390/index.html">Ceph - de «à genoux» à «production» partie 2</a></li>
<li><a href="../fr458394/index.html">Sécurisation des protocoles sans fil en utilisant LoRaWAN comme exemple</a></li>
<li><a href="../fr458396/index.html">Comment j'ai rendu le développement sur Vue.js pratique avec le rendu côté serveur</a></li>
<li><a href="../fr458398/index.html">L'hygiène du travail à distance ou les avantages de la télépathie</a></li>
<li><a href="../fr458400/index.html">Architecture et implémentation des microservices Étape par étape, partie 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>