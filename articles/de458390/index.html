<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéã üö∂üèª üôÜüèΩ Ceph - von "auf dem Knie" bis "Produktion" Teil 2 üöú üë®‚Äç‚öïÔ∏è üë©üèæ‚Äçü§ù‚Äçüë®üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(erster Teil hier: https://habr.com/en/post/456446/ ) 
 Ceph 
 Einf√ºhrung 


 Da das Netzwerk eines der Schl√ºsselelemente von Ceph ist und in unserem ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - von "auf dem Knie" bis "Produktion" Teil 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (erster Teil hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  Einf√ºhrung </h3><br><p>  Da das Netzwerk eines der Schl√ºsselelemente von Ceph ist und in unserem Unternehmen ein wenig spezifisch ist, werden wir Ihnen zun√§chst ein wenig dar√ºber erz√§hlen. <br>  Es wird viel weniger Beschreibungen von Ceph selbst geben, haupts√§chlich eine Netzwerkinfrastruktur.  Es werden nur Ceph-Server und einige Funktionen von Proxmox-Virtualisierungsservern beschrieben. </p><a name="habracut"></a><br><p>  Also: Die Netzwerktopologie selbst ist als <strong>Leaf-Spine aufgebaut.</strong>  Die klassische dreistufige Architektur ist ein Netzwerk, in dem <strong>Core</strong> (Core-Router), <strong>Aggregation</strong> (Aggregation-Router) und direkt mit <strong>Access-</strong> Clients (Access-Router) verbunden sind: </p><br><p>  <strong>Drei-Ebenen-Schema</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  Die Leaf-Spine-Topologie besteht aus zwei Ebenen: <strong>Spine</strong> (grob gesagt der Hauptrouter) und <strong>Leaf</strong> (Zweige). </p><br><p>  <strong>Zwei-Ebenen-Schema</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Das gesamte interne und externe Routing basiert auf BGP.  Das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>Hauptsystem f√ºr</strong></a> Zugriffskontrolle, Ansagen und mehr ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>XCloud.</strong></a> <br>  Server f√ºr die Kanalreservierung (und auch f√ºr deren Erweiterung) sind mit zwei L3-Switches verbunden (die meisten Server sind mit Leaf-Switches verbunden, einige Server mit erh√∂hter Netzwerklast sind jedoch direkt mit dem Spine des Switch verbunden) und geben √ºber BGP ihre Unicast-Adresse bekannt. sowie eine Anycast-Adresse f√ºr den Dienst, wenn mehrere Server den Dienstverkehr bedienen und der ECMP-Ausgleich f√ºr sie ausreicht.  Eine separate Funktion dieses Schemas, mit der wir Adressen sparen konnten, aber auch Ingenieure mit der IPv6-Welt vertraut machen mussten, war die Verwendung des nicht nummerierten BGP-Standards basierend auf RFC 5549. F√ºr einige Zeit wurde Quagga f√ºr Server in BGP in diesem Schema f√ºr Server und regelm√§√üig verwendet Es gab Probleme mit dem Verlust von Festen und Konnektivit√§t.  Nach dem Wechsel zu FRRouting (dessen aktive Mitwirkende unsere Anbieter von Netzwerkger√§ten sind: Cumulus und XCloudNetworks) haben wir solche Probleme nicht mehr beobachtet. </p><br><p>  Der Einfachheit halber nennen wir dieses gesamte allgemeine Schema eine "Fabrik". </p><br><h2 id="poisk-puti">  Suche nach einem Weg </h2><br><p>  Konfigurationsoptionen f√ºr das Clusternetzwerk: </p><br><p>  1) Zweites Netzwerk auf BGP </p><br><p>  2) Das zweite Netzwerk auf zwei separaten gestapelten Switches mit LACP </p><br><p>  3) Zweites Netzwerk auf zwei separaten isolierten Switches mit OSPF </p><br><h3 id="testy">  Tests </h3><br><p>  Die Tests wurden in zwei Arten durchgef√ºhrt: </p><br><p>  a) Netzwerk mit den Dienstprogrammen iperf, qperf, nuttcp </p><br><p>  b) interne Tests Ceph Ceph-Gobench, Rados Bench, erstellt rbd und getestet auf ihnen mit dd in einem oder mehreren Threads, mit fio </p><br><p>  Alle Tests wurden auf Testmaschinen mit SAS-Festplatten durchgef√ºhrt.  Die Zahlen zur rbd-Leistung wurden nicht sehr genau betrachtet, sondern nur zum Vergleich herangezogen.  Interessiert an √Ñnderungen je nach Art der Verbindung. </p><br><h3 id="pervyy-variant">  Erste Option </h3><br><p>  <strong>Netzwerkkarten werden an das werkseitig konfigurierte BGP angeschlossen.</strong> </p><br><p>  Die Verwendung dieses Schemas f√ºr das interne Netzwerk wurde nicht als die beste Wahl angesehen: </p><br><p>  Erstens die √ºbersch√ºssige Anzahl von Zwischenelementen in Form von Schaltern, die eine zus√§tzliche Latenzzeit ergeben (dies war der Hauptgrund). <br>  Zweitens verwendeten sie zun√§chst, um die Statik √ºber s3 zu √ºbertragen, eine Anycast-Adresse, die auf mehreren Computern mit Radosgateway ausgel√∂st wurde.  Dies f√ºhrte dazu, dass der Verkehr von Front-End-Maschinen zu RGW nicht gleichm√§√üig verteilt wurde, sondern auf dem k√ºrzesten Weg weitergeleitet wurde - das hei√üt, Front-End-Nginx wandte sich immer an denselben Knoten mit RGW, der mit dem mit ihm geteilten Blatt verbunden war (dies war nat√ºrlich der Fall) nicht das Hauptargument - wir haben uns einfach sp√§ter von Anycast-Adressen geweigert, statisch zur√ºckzugeben).  Aus Gr√ºnden der Reinheit des Experiments beschlossen sie jedoch, Tests mit einem solchen Schema durchzuf√ºhren, um Daten zum Vergleich zu erhalten. </p><br><p>  Wir hatten Angst, Tests f√ºr die gesamte Bandbreite durchzuf√ºhren, da die Fabrik von Prod-Servern genutzt wird. Wenn wir die Verbindungen zwischen Blatt und Wirbels√§ule blockieren, w√ºrde dies einen Teil des Umsatzes beeintr√§chtigen. <br>  Tats√§chlich war dies ein weiterer Grund, ein solches System abzulehnen. <br>  Iperf-Tests mit einer BW-Grenze von 3 Gbit / s von 1, 10 und 100 Streams wurden zum Vergleich mit anderen Schemata verwendet. <br>  Tests zeigten die folgenden Ergebnisse: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  in <strong>1</strong> Stream ungef√§hr <strong>9,30 - 9,43 Gbit / s</strong> (in diesem Fall <strong>steigt</strong> die Anzahl der erneuten √úbertragungen stark auf <strong>39148</strong> ).  Die Zahl, die nahe am Maximum einer Schnittstelle liegt, legt nahe, dass eine der beiden verwendet wird.  Die Anzahl der erneuten √úbertragungen betr√§gt ungef√§hr <strong>500-600.</strong> <br>  <strong>10</strong> Streams mit <strong>9,63 Gbit / s</strong> pro Schnittstelle, w√§hrend die Anzahl der erneuten √úbertragungen auf durchschnittlich <strong>17045 anstieg.</strong> <br>  In <strong>100</strong> Threads war das Ergebnis schlechter als in <strong>10</strong> , w√§hrend die Anzahl der erneuten √úbertragungen geringer ist: Der Durchschnittswert betr√§gt <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Zweite Option </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Es gab zwei Juniper EX4500-Schalter.  Sie sammelten sie auf dem Stapel, verbanden den Server mit den ersten Links zu einem Switch, den zweiten zum zweiten. <br>  Der anf√§ngliche Verbindungsaufbau war wie folgt: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Die iperf- und qperf-Tests zeigten Bw bis zu <strong>16 Gbit / s.</strong>  Wir haben uns entschieden, verschiedene Mod-Typen zu vergleichen: <br>  <strong>rr, balance-xor und 802.3ad.</strong>  Wir haben auch verschiedene Arten von Hashing <strong>Layer2 + 3 und Layer3 + 4</strong> verglichen (in der Hoffnung, einen Vorteil beim Hash-Computing zu erzielen). <br>  Wir haben auch die Ergebnisse f√ºr verschiedene sysctl-Werte der Variablen <strong>net.ipv4.fib_multipath_hash_policy verglichen</strong> (nun, wir haben ein wenig mit <strong>net.ipv4.tcp_congestion_control gespielt</strong> , obwohl es nichts mit <strong>Bonding</strong> zu tun hat. Es gibt einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">guten ValdikSS-Artikel</a> zu dieser Variablen)). </p><br><p>  Bei allen Tests gelang es jedoch nicht, den Schwellenwert von <strong>18 Gbit / s zu</strong> √ºberschreiten (diese Zahl wurde mit <strong>balance-xor und 802.3ad erreicht</strong> , es gab keinen gro√üen Unterschied zwischen den Testergebnissen), und dieser Wert wurde "im Sprung" durch Bursts erreicht. </p><br><h3 id="tretiy-variant">  Dritte Option </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Um diese Option zu konfigurieren, wurde LACP von den Switches entfernt (das Stapeln wurde beibehalten, aber nur f√ºr die Verwaltung verwendet).  Auf jedem Switch haben wir ein separates VLAN f√ºr eine Gruppe von Ports gesammelt (mit Blick auf die Zukunft, dass sowohl QA- als auch PROD-Server in denselben Switches stecken bleiben). </p><br><p>  Konfigurierte zwei flache private Netzwerke f√ºr jedes VLAN (eine Schnittstelle pro Switch).  Zu diesen Adressen kommt die Ank√ºndigung einer weiteren Adresse aus dem dritten privaten Netzwerk, dem Clusternetzwerk f√ºr CEPH. </p><br><p>  Da das <em>√∂ffentliche Netzwerk</em> (√ºber das wir SSH verwenden) mit BGP funktioniert, haben wir frr verwendet, um OSPF zu konfigurieren, das sich bereits auf dem System befindet. </p><br><p>  <strong>10.10.10.0/24 und 20.20.20.0/24</strong> - zwei flache Netzwerke an den Switches </p><br><p>  <strong>172.16.1.0/24</strong> - Netzwerk zur Ank√ºndigung </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Maschineneinrichtung: <br>  Schnittstellen <strong>ens1f0 ens1f1</strong> betrachten ein privates Netzwerk <br>  Schnittstellen <strong>ens4f0 ens4f1</strong> betrachten das √∂ffentliche Netzwerk </p><br><p>  Die Netzwerkkonfiguration auf dem Computer sieht folgenderma√üen aus: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  Frr-Konfigurationen sehen folgenderma√üen aus: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  Bei diesen Einstellungen testen das Netzwerk iperf, qperf usw.  zeigten eine maximale Auslastung beider Kan√§le bei <strong>19,8 Gbit / s,</strong> w√§hrend die Latenz auf <strong>20 us abfiel</strong> </p><br><p>  <em><strong>Bgp-Router-ID-</strong> Feld <strong>:</strong> Wird verwendet, um den Knoten bei der Verarbeitung von Routing-Informationen und beim <strong>Erstellen von</strong> Routen zu identifizieren.</em>  <em>Wenn in der Konfiguration nicht angegeben, wird eine der Host-IP-Adressen ausgew√§hlt.</em>  <em>Verschiedene Hersteller von Hardware und Software haben m√∂glicherweise unterschiedliche Algorithmen. In unserem Fall hat FRR die gr√∂√üte Loopback-IP-Adresse verwendet.</em>  <em>Dies f√ºhrte zu zwei Problemen:</em> <em><br></em>  <em>1) Wenn wir versucht haben, eine andere Adresse (z. B. privat aus dem Netzwerk 172.16.0.0) als die aktuelle Adresse aufzuh√§ngen, f√ºhrte dies zu einer √Ñnderung der <strong>Router-ID</strong> und dementsprechend zu einer Neuinstallation der aktuellen Sitzungen.</em>  <em>Dies bedeutet eine kurze Pause und einen Verlust der Netzwerkkonnektivit√§t.</em> <em><br></em>  <em>2) Wenn wir versucht haben, eine von mehreren Computern gemeinsam genutzte Anycast-Adresse aufzulegen, die als <strong>Router-ID ausgew√§hlt wurde</strong> , wurden zwei Knoten mit derselben <strong>Router-ID</strong> im Netzwerk angezeigt <strong>.</strong></em> </p><br><h2 id="chast-2">  Teil 2 </h2><br><p>  Nach dem Testen der Qualit√§tssicherung haben wir begonnen, den Kampf gegen Ceph zu verbessern. </p><br><h3 id="network">  NETZWERK </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Wechsel von einem Netzwerk zu zwei </h3><br><p>  Der Cluster-Netzwerkparameter ist einer der Parameter, die nicht im <strong>laufenden Betrieb</strong> ge√§ndert werden k√∂nnen, indem das OSD √ºber <strong>ceph tell osd angegeben wird. * Injectargs.</strong>  Das √Ñndern in der Konfiguration und das Neustarten des gesamten Clusters ist eine tolerierbare L√∂sung, aber ich wollte wirklich nicht einmal eine kleine Ausfallzeit haben.  Es ist auch unm√∂glich, ein OSD mit einem neuen Netzwerkparameter neu zu starten - irgendwann h√§tten wir zwei Halbcluster gehabt - alte OSDs im alten Netzwerk, neue im neuen.  Gl√ºcklicherweise ist der Cluster-Netzwerkparameter (und √ºbrigens auch public_network) eine Liste, dh Sie k√∂nnen mehrere Werte angeben.  Wir haben uns entschlossen, schrittweise umzuziehen - f√ºgen Sie zuerst ein neues Netzwerk zu den Konfigurationen hinzu und entfernen Sie dann das alte.  Ceph geht die Liste der Netzwerke nacheinander durch - OSD beginnt zuerst mit dem zuerst aufgelisteten Netzwerk zu arbeiten. </p><br><p>  Die Schwierigkeit bestand darin, dass das erste Netzwerk √ºber bgp funktionierte und mit einem Switch verbunden war, und das zweite - mit ospf und mit anderen, die nicht physisch mit dem ersten verbunden waren.  Zum Zeitpunkt des √úbergangs war ein vor√ºbergehender Netzwerkzugriff zwischen den beiden Netzwerken erforderlich.  Die Besonderheit beim Einrichten unserer Factory war, dass ACLs nicht im Netzwerk konfiguriert werden k√∂nnen, wenn sie nicht in der Liste der angek√ºndigten ACLs enthalten sind (in diesem Fall handelt es sich um ‚Äûexterne‚Äú ACLs, und ACLs k√∂nnen nur extern erstellt werden. Sie wurden auf Spanien erstellt, sind jedoch nicht angekommen auf Bl√§ttern). </p><br><p>  Die L√∂sung war eine Kr√ºcke, kompliziert, aber es funktionierte: das interne Netzwerk √ºber bgp gleichzeitig mit ospf zu bewerben. </p><br><p>  Die √úbergangssequenz ist wie folgt: </p><br><p>  1) Konfigurieren Sie das Clusternetzwerk f√ºr ceph in zwei Netzwerken: √ºber bgp und √ºber ospf <br>  In frr configs war es nicht notwendig, irgendetwas zu √§ndern, eine Zeile </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  Es schr√§nkt uns nicht in den angek√ºndigten Adressen ein, die Adresse f√ºr das interne Netzwerk selbst wird auf der Loopback-Schnittstelle angehoben, es hat gereicht, den Empfang der Ansage dieser Adresse auf den Routern zu konfigurieren. </p><br><p>  2) F√ºgen Sie der <strong>ceph.conf-</strong> Konfiguration ein neues Netzwerk <strong>hinzu</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  Starten Sie das OSD nacheinander neu, bis alle <strong>Benutzer zum</strong> Netzwerk <strong>172.16.1.0/24 wechseln.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Dann entfernen wir das √ºbersch√ºssige Netzwerk aus der Konfiguration </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  und wiederholen Sie den Vorgang. </p><br><p>  Das ist alles, wir sind reibungslos in ein neues Netzwerk umgezogen. </p><br><p>  Referenzen: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458390/">https://habr.com/ru/post/de458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458376/index.html">Keine andere Programmiersprache. Teil 1: Dom√§nenlogik</a></li>
<li><a href="../de458378/index.html">Verwenden von Avocode f√ºr das Site-Layout. Bewertung f√ºr Anf√§nger. Bonus - Registrieren Sie eine 30-t√§gige Testphase</a></li>
<li><a href="../de458382/index.html">Warum unterrichten wir das?</a></li>
<li><a href="../de458384/index.html">HP 3D Strukturierter Lichtscanner Pro S3 √úberpr√ºfung und Test</a></li>
<li><a href="../de458388/index.html">Deep (Learning + Random) Wald- und Artikelanalyse</a></li>
<li><a href="../de458400/index.html">Microservices-Architektur und -Implementierung Schritt f√ºr Schritt Teil 1</a></li>
<li><a href="../de458404/index.html">√úbergang vom Monolithen zum Mikrodienst: Geschichte und Praxis</a></li>
<li><a href="../de458406/index.html">√úber 30 Fragen zu Dienstprogrammen und Nicht-Dienstprogrammen</a></li>
<li><a href="../de458408/index.html">Sicherheitswoche 27: Sicherheitsl√ºcken in der Insulinpumpe</a></li>
<li><a href="../de458410/index.html">Entwicklung einer Online-Shop-Struktur basierend auf Clustering und Lemmatisierung der Semantik</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>