<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎋 🚶🏻 🙆🏽 Ceph - von "auf dem Knie" bis "Produktion" Teil 2 🚜 👨‍⚕️ 👩🏾‍🤝‍👨🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(erster Teil hier: https://habr.com/en/post/456446/ ) 
 Ceph 
 Einführung 


 Da das Netzwerk eines der Schlüsselelemente von Ceph ist und in unserem ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - von "auf dem Knie" bis "Produktion" Teil 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (erster Teil hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  Einführung </h3><br><p>  Da das Netzwerk eines der Schlüsselelemente von Ceph ist und in unserem Unternehmen ein wenig spezifisch ist, werden wir Ihnen zunächst ein wenig darüber erzählen. <br>  Es wird viel weniger Beschreibungen von Ceph selbst geben, hauptsächlich eine Netzwerkinfrastruktur.  Es werden nur Ceph-Server und einige Funktionen von Proxmox-Virtualisierungsservern beschrieben. </p><a name="habracut"></a><br><p>  Also: Die Netzwerktopologie selbst ist als <strong>Leaf-Spine aufgebaut.</strong>  Die klassische dreistufige Architektur ist ein Netzwerk, in dem <strong>Core</strong> (Core-Router), <strong>Aggregation</strong> (Aggregation-Router) und direkt mit <strong>Access-</strong> Clients (Access-Router) verbunden sind: </p><br><p>  <strong>Drei-Ebenen-Schema</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  Die Leaf-Spine-Topologie besteht aus zwei Ebenen: <strong>Spine</strong> (grob gesagt der Hauptrouter) und <strong>Leaf</strong> (Zweige). </p><br><p>  <strong>Zwei-Ebenen-Schema</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Das gesamte interne und externe Routing basiert auf BGP.  Das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>Hauptsystem für</strong></a> Zugriffskontrolle, Ansagen und mehr ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>XCloud.</strong></a> <br>  Server für die Kanalreservierung (und auch für deren Erweiterung) sind mit zwei L3-Switches verbunden (die meisten Server sind mit Leaf-Switches verbunden, einige Server mit erhöhter Netzwerklast sind jedoch direkt mit dem Spine des Switch verbunden) und geben über BGP ihre Unicast-Adresse bekannt. sowie eine Anycast-Adresse für den Dienst, wenn mehrere Server den Dienstverkehr bedienen und der ECMP-Ausgleich für sie ausreicht.  Eine separate Funktion dieses Schemas, mit der wir Adressen sparen konnten, aber auch Ingenieure mit der IPv6-Welt vertraut machen mussten, war die Verwendung des nicht nummerierten BGP-Standards basierend auf RFC 5549. Für einige Zeit wurde Quagga für Server in BGP in diesem Schema für Server und regelmäßig verwendet Es gab Probleme mit dem Verlust von Festen und Konnektivität.  Nach dem Wechsel zu FRRouting (dessen aktive Mitwirkende unsere Anbieter von Netzwerkgeräten sind: Cumulus und XCloudNetworks) haben wir solche Probleme nicht mehr beobachtet. </p><br><p>  Der Einfachheit halber nennen wir dieses gesamte allgemeine Schema eine "Fabrik". </p><br><h2 id="poisk-puti">  Suche nach einem Weg </h2><br><p>  Konfigurationsoptionen für das Clusternetzwerk: </p><br><p>  1) Zweites Netzwerk auf BGP </p><br><p>  2) Das zweite Netzwerk auf zwei separaten gestapelten Switches mit LACP </p><br><p>  3) Zweites Netzwerk auf zwei separaten isolierten Switches mit OSPF </p><br><h3 id="testy">  Tests </h3><br><p>  Die Tests wurden in zwei Arten durchgeführt: </p><br><p>  a) Netzwerk mit den Dienstprogrammen iperf, qperf, nuttcp </p><br><p>  b) interne Tests Ceph Ceph-Gobench, Rados Bench, erstellt rbd und getestet auf ihnen mit dd in einem oder mehreren Threads, mit fio </p><br><p>  Alle Tests wurden auf Testmaschinen mit SAS-Festplatten durchgeführt.  Die Zahlen zur rbd-Leistung wurden nicht sehr genau betrachtet, sondern nur zum Vergleich herangezogen.  Interessiert an Änderungen je nach Art der Verbindung. </p><br><h3 id="pervyy-variant">  Erste Option </h3><br><p>  <strong>Netzwerkkarten werden an das werkseitig konfigurierte BGP angeschlossen.</strong> </p><br><p>  Die Verwendung dieses Schemas für das interne Netzwerk wurde nicht als die beste Wahl angesehen: </p><br><p>  Erstens die überschüssige Anzahl von Zwischenelementen in Form von Schaltern, die eine zusätzliche Latenzzeit ergeben (dies war der Hauptgrund). <br>  Zweitens verwendeten sie zunächst, um die Statik über s3 zu übertragen, eine Anycast-Adresse, die auf mehreren Computern mit Radosgateway ausgelöst wurde.  Dies führte dazu, dass der Verkehr von Front-End-Maschinen zu RGW nicht gleichmäßig verteilt wurde, sondern auf dem kürzesten Weg weitergeleitet wurde - das heißt, Front-End-Nginx wandte sich immer an denselben Knoten mit RGW, der mit dem mit ihm geteilten Blatt verbunden war (dies war natürlich der Fall) nicht das Hauptargument - wir haben uns einfach später von Anycast-Adressen geweigert, statisch zurückzugeben).  Aus Gründen der Reinheit des Experiments beschlossen sie jedoch, Tests mit einem solchen Schema durchzuführen, um Daten zum Vergleich zu erhalten. </p><br><p>  Wir hatten Angst, Tests für die gesamte Bandbreite durchzuführen, da die Fabrik von Prod-Servern genutzt wird. Wenn wir die Verbindungen zwischen Blatt und Wirbelsäule blockieren, würde dies einen Teil des Umsatzes beeinträchtigen. <br>  Tatsächlich war dies ein weiterer Grund, ein solches System abzulehnen. <br>  Iperf-Tests mit einer BW-Grenze von 3 Gbit / s von 1, 10 und 100 Streams wurden zum Vergleich mit anderen Schemata verwendet. <br>  Tests zeigten die folgenden Ergebnisse: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  in <strong>1</strong> Stream ungefähr <strong>9,30 - 9,43 Gbit / s</strong> (in diesem Fall <strong>steigt</strong> die Anzahl der erneuten Übertragungen stark auf <strong>39148</strong> ).  Die Zahl, die nahe am Maximum einer Schnittstelle liegt, legt nahe, dass eine der beiden verwendet wird.  Die Anzahl der erneuten Übertragungen beträgt ungefähr <strong>500-600.</strong> <br>  <strong>10</strong> Streams mit <strong>9,63 Gbit / s</strong> pro Schnittstelle, während die Anzahl der erneuten Übertragungen auf durchschnittlich <strong>17045 anstieg.</strong> <br>  In <strong>100</strong> Threads war das Ergebnis schlechter als in <strong>10</strong> , während die Anzahl der erneuten Übertragungen geringer ist: Der Durchschnittswert beträgt <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Zweite Option </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Es gab zwei Juniper EX4500-Schalter.  Sie sammelten sie auf dem Stapel, verbanden den Server mit den ersten Links zu einem Switch, den zweiten zum zweiten. <br>  Der anfängliche Verbindungsaufbau war wie folgt: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Die iperf- und qperf-Tests zeigten Bw bis zu <strong>16 Gbit / s.</strong>  Wir haben uns entschieden, verschiedene Mod-Typen zu vergleichen: <br>  <strong>rr, balance-xor und 802.3ad.</strong>  Wir haben auch verschiedene Arten von Hashing <strong>Layer2 + 3 und Layer3 + 4</strong> verglichen (in der Hoffnung, einen Vorteil beim Hash-Computing zu erzielen). <br>  Wir haben auch die Ergebnisse für verschiedene sysctl-Werte der Variablen <strong>net.ipv4.fib_multipath_hash_policy verglichen</strong> (nun, wir haben ein wenig mit <strong>net.ipv4.tcp_congestion_control gespielt</strong> , obwohl es nichts mit <strong>Bonding</strong> zu tun hat. Es gibt einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">guten ValdikSS-Artikel</a> zu dieser Variablen)). </p><br><p>  Bei allen Tests gelang es jedoch nicht, den Schwellenwert von <strong>18 Gbit / s zu</strong> überschreiten (diese Zahl wurde mit <strong>balance-xor und 802.3ad erreicht</strong> , es gab keinen großen Unterschied zwischen den Testergebnissen), und dieser Wert wurde "im Sprung" durch Bursts erreicht. </p><br><h3 id="tretiy-variant">  Dritte Option </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Um diese Option zu konfigurieren, wurde LACP von den Switches entfernt (das Stapeln wurde beibehalten, aber nur für die Verwaltung verwendet).  Auf jedem Switch haben wir ein separates VLAN für eine Gruppe von Ports gesammelt (mit Blick auf die Zukunft, dass sowohl QA- als auch PROD-Server in denselben Switches stecken bleiben). </p><br><p>  Konfigurierte zwei flache private Netzwerke für jedes VLAN (eine Schnittstelle pro Switch).  Zu diesen Adressen kommt die Ankündigung einer weiteren Adresse aus dem dritten privaten Netzwerk, dem Clusternetzwerk für CEPH. </p><br><p>  Da das <em>öffentliche Netzwerk</em> (über das wir SSH verwenden) mit BGP funktioniert, haben wir frr verwendet, um OSPF zu konfigurieren, das sich bereits auf dem System befindet. </p><br><p>  <strong>10.10.10.0/24 und 20.20.20.0/24</strong> - zwei flache Netzwerke an den Switches </p><br><p>  <strong>172.16.1.0/24</strong> - Netzwerk zur Ankündigung </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Maschineneinrichtung: <br>  Schnittstellen <strong>ens1f0 ens1f1</strong> betrachten ein privates Netzwerk <br>  Schnittstellen <strong>ens4f0 ens4f1</strong> betrachten das öffentliche Netzwerk </p><br><p>  Die Netzwerkkonfiguration auf dem Computer sieht folgendermaßen aus: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  Frr-Konfigurationen sehen folgendermaßen aus: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  Bei diesen Einstellungen testen das Netzwerk iperf, qperf usw.  zeigten eine maximale Auslastung beider Kanäle bei <strong>19,8 Gbit / s,</strong> während die Latenz auf <strong>20 us abfiel</strong> </p><br><p>  <em><strong>Bgp-Router-ID-</strong> Feld <strong>:</strong> Wird verwendet, um den Knoten bei der Verarbeitung von Routing-Informationen und beim <strong>Erstellen von</strong> Routen zu identifizieren.</em>  <em>Wenn in der Konfiguration nicht angegeben, wird eine der Host-IP-Adressen ausgewählt.</em>  <em>Verschiedene Hersteller von Hardware und Software haben möglicherweise unterschiedliche Algorithmen. In unserem Fall hat FRR die größte Loopback-IP-Adresse verwendet.</em>  <em>Dies führte zu zwei Problemen:</em> <em><br></em>  <em>1) Wenn wir versucht haben, eine andere Adresse (z. B. privat aus dem Netzwerk 172.16.0.0) als die aktuelle Adresse aufzuhängen, führte dies zu einer Änderung der <strong>Router-ID</strong> und dementsprechend zu einer Neuinstallation der aktuellen Sitzungen.</em>  <em>Dies bedeutet eine kurze Pause und einen Verlust der Netzwerkkonnektivität.</em> <em><br></em>  <em>2) Wenn wir versucht haben, eine von mehreren Computern gemeinsam genutzte Anycast-Adresse aufzulegen, die als <strong>Router-ID ausgewählt wurde</strong> , wurden zwei Knoten mit derselben <strong>Router-ID</strong> im Netzwerk angezeigt <strong>.</strong></em> </p><br><h2 id="chast-2">  Teil 2 </h2><br><p>  Nach dem Testen der Qualitätssicherung haben wir begonnen, den Kampf gegen Ceph zu verbessern. </p><br><h3 id="network">  NETZWERK </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Wechsel von einem Netzwerk zu zwei </h3><br><p>  Der Cluster-Netzwerkparameter ist einer der Parameter, die nicht im <strong>laufenden Betrieb</strong> geändert werden können, indem das OSD über <strong>ceph tell osd angegeben wird. * Injectargs.</strong>  Das Ändern in der Konfiguration und das Neustarten des gesamten Clusters ist eine tolerierbare Lösung, aber ich wollte wirklich nicht einmal eine kleine Ausfallzeit haben.  Es ist auch unmöglich, ein OSD mit einem neuen Netzwerkparameter neu zu starten - irgendwann hätten wir zwei Halbcluster gehabt - alte OSDs im alten Netzwerk, neue im neuen.  Glücklicherweise ist der Cluster-Netzwerkparameter (und übrigens auch public_network) eine Liste, dh Sie können mehrere Werte angeben.  Wir haben uns entschlossen, schrittweise umzuziehen - fügen Sie zuerst ein neues Netzwerk zu den Konfigurationen hinzu und entfernen Sie dann das alte.  Ceph geht die Liste der Netzwerke nacheinander durch - OSD beginnt zuerst mit dem zuerst aufgelisteten Netzwerk zu arbeiten. </p><br><p>  Die Schwierigkeit bestand darin, dass das erste Netzwerk über bgp funktionierte und mit einem Switch verbunden war, und das zweite - mit ospf und mit anderen, die nicht physisch mit dem ersten verbunden waren.  Zum Zeitpunkt des Übergangs war ein vorübergehender Netzwerkzugriff zwischen den beiden Netzwerken erforderlich.  Die Besonderheit beim Einrichten unserer Factory war, dass ACLs nicht im Netzwerk konfiguriert werden können, wenn sie nicht in der Liste der angekündigten ACLs enthalten sind (in diesem Fall handelt es sich um „externe“ ACLs, und ACLs können nur extern erstellt werden. Sie wurden auf Spanien erstellt, sind jedoch nicht angekommen auf Blättern). </p><br><p>  Die Lösung war eine Krücke, kompliziert, aber es funktionierte: das interne Netzwerk über bgp gleichzeitig mit ospf zu bewerben. </p><br><p>  Die Übergangssequenz ist wie folgt: </p><br><p>  1) Konfigurieren Sie das Clusternetzwerk für ceph in zwei Netzwerken: über bgp und über ospf <br>  In frr configs war es nicht notwendig, irgendetwas zu ändern, eine Zeile </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  Es schränkt uns nicht in den angekündigten Adressen ein, die Adresse für das interne Netzwerk selbst wird auf der Loopback-Schnittstelle angehoben, es hat gereicht, den Empfang der Ansage dieser Adresse auf den Routern zu konfigurieren. </p><br><p>  2) Fügen Sie der <strong>ceph.conf-</strong> Konfiguration ein neues Netzwerk <strong>hinzu</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  Starten Sie das OSD nacheinander neu, bis alle <strong>Benutzer zum</strong> Netzwerk <strong>172.16.1.0/24 wechseln.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Dann entfernen wir das überschüssige Netzwerk aus der Konfiguration </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  und wiederholen Sie den Vorgang. </p><br><p>  Das ist alles, wir sind reibungslos in ein neues Netzwerk umgezogen. </p><br><p>  Referenzen: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458390/">https://habr.com/ru/post/de458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458376/index.html">Keine andere Programmiersprache. Teil 1: Domänenlogik</a></li>
<li><a href="../de458378/index.html">Verwenden von Avocode für das Site-Layout. Bewertung für Anfänger. Bonus - Registrieren Sie eine 30-tägige Testphase</a></li>
<li><a href="../de458382/index.html">Warum unterrichten wir das?</a></li>
<li><a href="../de458384/index.html">HP 3D Strukturierter Lichtscanner Pro S3 Überprüfung und Test</a></li>
<li><a href="../de458388/index.html">Deep (Learning + Random) Wald- und Artikelanalyse</a></li>
<li><a href="../de458400/index.html">Microservices-Architektur und -Implementierung Schritt für Schritt Teil 1</a></li>
<li><a href="../de458404/index.html">Übergang vom Monolithen zum Mikrodienst: Geschichte und Praxis</a></li>
<li><a href="../de458406/index.html">Über 30 Fragen zu Dienstprogrammen und Nicht-Dienstprogrammen</a></li>
<li><a href="../de458408/index.html">Sicherheitswoche 27: Sicherheitslücken in der Insulinpumpe</a></li>
<li><a href="../de458410/index.html">Entwicklung einer Online-Shop-Struktur basierend auf Clustering und Lemmatisierung der Semantik</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>