<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧘🏻 🧝🏾 😙 La red neuronal lee el 46.8% de las palabras en los labios en la televisión, mientras que solo el 12.4% de las personas 😥 🎵 🙆</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Los marcos de los cuatro programas en los que se estudió el programa, así como la palabra "tarde", pronunciada por dos oradores diferentes
 
 Hace dos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La red neuronal lee el 46.8% de las palabras en los labios en la televisión, mientras que solo el 12.4% de las personas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/399429/"><img src="https://habrastorage.org/files/ec4/57a/c95/ec457ac95f72435cbb483bf431da7794.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los marcos de los cuatro programas en los que se estudió el programa, así como la palabra "tarde", pronunciada por dos oradores diferentes</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Hace dos semanas, hablaron sobre la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">red neuronal LipNet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que mostró una calidad de registro del 93,4% del reconocimiento del habla humana en los labios. Incluso entonces, se suponían muchas aplicaciones para tales sistemas informáticos: una nueva generación de audífonos médicos con reconocimiento de voz, sistemas para conferencias silenciosas en lugares públicos, identificación biométrica, sistemas para la transmisión secreta de información para espionaje, reconocimiento de voz por video desde cámaras de vigilancia, etc. Y ahora, expertos de la Universidad de Oxford junto con un empleado de Google DeepMind </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hablaron sobre sus propios desarrollos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en esta área.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La nueva red neuronal fue entrenada en </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">textos arbitrarios de</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> personas que actúan en el canal de televisión de la BBC. Curiosamente, el entrenamiento se realizó automáticamente, sin anotar primero el discurso manualmente. El sistema mismo reconoció el habla, anotó el video, encontró caras en el cuadro y luego aprendió a determinar la relación entre las palabras (sonidos) y el movimiento de los labios. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado, este sistema reconoce efectivamente </font><font style="vertical-align: inherit;">textos </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arbitrarios</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , en lugar de instancias del corpus especial de oraciones GRID, como lo hizo LipNet. El caso GRID tiene una estructura y vocabulario estrictamente limitados; por lo tanto, solo son posibles 33,000 oraciones. Por lo tanto, el número de opciones se reduce en órdenes de magnitud y se simplifica el reconocimiento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El caso especial GRID se compone de la siguiente manera:</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comando (4) + color (4) + preposición (4) + letra (25) + dígito (10) + adverbio (4),</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
donde el número corresponde al número de variantes de palabras para cada una de las seis categorías verbales. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A diferencia de LipNet, el desarrollo de DeepMind y especialistas de la Universidad de Oxford trabaja en flujos de voz arbitrarios en calidad de imagen de televisión. Es mucho más como un sistema real, listo para su uso práctico.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
AI entrenó 5,000 horas de video grabado de seis programas de televisión del canal de televisión británico de la BBC desde enero de 2010 hasta diciembre de 2015: estos son comunicados de prensa regulares (1584 horas), noticias de la mañana (1997 horas), emisiones de Newsnight (590 horas), World News (194 horas), turno de preguntas (323 horas) y Mundo de hoy (272 horas). </font><font style="vertical-align: inherit;">En total, los videos contienen 118,116 oraciones de habla humana continua. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Después de eso, el programa se verificó en las transmisiones que se emitieron entre marzo y septiembre de 2016.</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un ejemplo de lectura de labios desde una pantalla de televisión</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/files/c82/0d8/a76/c820d8a763214b4db9c28565a8ffaa25.gif"></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El programa mostró una calidad de lectura bastante alta. </font><font style="vertical-align: inherit;">Ella reconoció correctamente incluso oraciones muy complejas con construcciones gramaticales inusuales y el uso de nombres propios. </font><font style="vertical-align: inherit;">Ejemplos de oraciones perfectamente reconocidas:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MUCHAS MÁS PERSONAS QUE PARTICIPARON EN LOS ATAQUES</font></font></li>
<li>CLOSE TO THE EUROPEAN COMMISSION’S MAIN BUILDING</li>
<li>WEST WALES AND THE SOUTH WEST AS WELL AS WESTERN SCOTLAND</li>
<li>WE KNOW THERE WILL BE HUNDREDS OF JOURNALISTS HERE AS WELL</li>
<li>ACCORDING TO PROVISIONAL FIGURES FROM THE ELECTORAL COMMISSION</li>
<li>THAT’S THE LOWEST FIGURE FOR EIGHT YEARS</li>
<li>MANCHESTER FOOTBALL CORRESPONDENT FOR THE DAILY MIRROR</li>
<li>LAYING THE GROUNDS FOR A POSSIBLE SECOND REFERENDUM</li>
<li>ACCORDING TO THE LATEST FIGURES FROM THE OFFICE FOR NATIONAL STATISTICS</li>
<li>IT COMES AFTER A DAMNING REPORT BY THE HEALTH WATCHDOG</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
AI superó significativamente la efectividad del trabajo de una persona, un experto en lectura de labios, que intentó reconocer 200 videoclips aleatorios de un archivo de video de verificación grabado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El profesional pudo anotar sin un solo error solo el 12.4% de las palabras, mientras que la IA registró correctamente el 46.8%. Los investigadores señalan que muchos errores se pueden llamar menores. Por ejemplo, la "s" que falta al final de las palabras. Si abordamos el análisis de los resultados de manera menos estricta, en realidad el sistema reconoció mucho más de la mitad de las palabras en el aire.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Con este resultado, DeepMind es significativamente superior a todos los demás lectores de labios, incluido el mencionado LipNet, que también se desarrolla en la Universidad de Oxford. Sin embargo, es demasiado pronto para hablar de la superioridad máxima, porque LipNet no fue entrenado en un conjunto de datos tan grande. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Según los </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">expertos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , DeepMind es un gran paso hacia el desarrollo de un sistema de lectura de labios totalmente automático. </font></font><br>
<br>
<img src="https://habrastorage.org/files/ea1/f0c/ae3/ea1f0cae324341439194e33b61fd9f3e.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La arquitectura del módulo WLAS (Watch, Listen, Attend and Spell) y una red neuronal convolucional para leer los labios.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El gran mérito de los investigadores radica en el hecho de que compilaron un gigantesco conjunto de datos para entrenar y probar el sistema con 17,500 palabras únicas. Después de todo, no se trata solo de cinco años de grabación continua de programas de televisión en inglés, sino también de una sincronización clara de video y sonido (en la televisión a menudo hay una sincronización de hasta 1 segundo, incluso en la televisión profesional en inglés), así como el desarrollo de un módulo para reconocimiento de voz, que se superpone en video y se usa para enseñar el sistema de lectura de labios (módulo WLAS, ver diagrama arriba).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el caso del rassynchron más leve, entrenar el sistema se vuelve prácticamente inútil, ya que el programa no puede determinar la correspondencia correcta de los sonidos y los movimientos de los labios. Después de un exhaustivo trabajo preparatorio, la capacitación del programa fue completamente automática: procesó de forma independiente los 5000 videos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Anteriormente, tal conjunto simplemente no existía, por lo tanto, los mismos autores de LipNet se vieron obligados a limitarse a la base GRID. Para crédito de los desarrolladores de DeepMind, prometieron publicar un conjunto de datos en el dominio público para entrenar a otras IA. Los colegas del equipo de desarrollo de LipNet ya han dicho que lo esperan con ansias. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El trabajo científico se </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publica</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el dominio público en el sitio web arXiv (arXiv: 1611.05358v1).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si los sistemas comerciales de lectura de labios aparecen en el mercado, la vida de la gente común será mucho más simple. </font><font style="vertical-align: inherit;">Se puede suponer que tales sistemas se incorporarán de inmediato a televisores y otros electrodomésticos para mejorar el control por voz y el reconocimiento de voz casi sin errores.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es399429/">https://habr.com/ru/post/es399429/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es399417/index.html">Xiaomi nuevos productos con tales descuentos</a></li>
<li><a href="../es399419/index.html">Revisión del monitor de juegos curvo Lenovo Y27g Razer Edition</a></li>
<li><a href="../es399421/index.html">¿Por qué el MacBook Pro está limitado a 16 GB de almacenamiento?</a></li>
<li><a href="../es399425/index.html">Extraterrestres electrónicos: una mente alienígena puede resultar ser una máquina</a></li>
<li><a href="../es399427/index.html">"Madre Kuzkina" en estilo overclocking: cómo un entusiasta ruso rompió tres récords mundiales en un día</a></li>
<li><a href="../es399431/index.html">Noticias falsas y la era posterior a la verdad: es solo el comienzo</a></li>
<li><a href="../es399433/index.html">GFDM — как использовать радио-ресурсы еще эффективнее</a></li>
<li><a href="../es399435/index.html">Una vez más sobre las bombillas chinas: todo está muy mal</a></li>
<li><a href="../es399437/index.html">En 2018, China comenzará a transformar sus centrales térmicas en centrales nucleares.</a></li>
<li><a href="../es399439/index.html">Historia de las epidemias mundiales, parte 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>