<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßôüèº üë©üèø‚Äçü§ù‚Äçüë®üèæ ‚ö±Ô∏è Parsing: OOM en Kubernetes üë¨ ü¶ä üôá</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Los problemas en el entorno laboral son siempre un desastre. Ocurre cuando vas a casa, y la raz√≥n siempre parece est√∫pida. Recientemente, nos hemos qu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsing: OOM en Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/467823/"><p><img src="https://habrastorage.org/webt/m8/of/zv/m8ofzvtlhfh8mkwo-i04c5ren38.jpeg"></p><br><p>  Los problemas en el entorno laboral son siempre un desastre.  Ocurre cuando vas a casa, y la raz√≥n siempre parece est√∫pida.  Recientemente, nos hemos quedado sin memoria en los nodos del cl√∫ster de Kubernetes, aunque el nodo se recuper√≥ de inmediato, sin interrupciones visibles.  Hoy hablaremos sobre este caso, sobre qu√© da√±o sufrimos y c√≥mo pretendemos evitar un problema similar en el futuro. </p><br><h2 id="sluchay-pervyy">  Caso uno </h2><br><h3 id="subbota-15-iyunya-2019-g-1712">  S√°bado 15 de junio de 2019 5:12 p.m. </h3><a name="habracut"></a><br><p>  Blue Matador (s√≠, ¬°nos monitoreamos a nosotros mismos!) Genera una alerta: un evento en uno de los nodos del cl√∫ster de producci√≥n de Kubernetes: SystemOOM. </p><br><h3 id="1716">  17:16 </h3><br><p>  Blue Matador genera una advertencia: EBS Burst Balance es bajo en el volumen ra√≠z del nodo, el lugar donde tuvo lugar el evento SystemOOM.  Aunque se produjo una advertencia sobre el Saldo de r√°faga despu√©s de una notificaci√≥n sobre SystemOOM, los datos reales de CloudWatch muestran que el Saldo de r√°faga ha alcanzado un nivel m√≠nimo a las 17:02.  La raz√≥n de la demora es que las m√©tricas de EBS est√°n constantemente atrasadas entre 10 y 15 minutos, y nuestro sistema no detecta todos los eventos en tiempo real. </p><br><p><img src="https://habrastorage.org/webt/xw/gb/u3/xwgbu3jxukrpz_qmakfhvsb5bm4.png"></p><br><h3 id="1718">  17:18 </h3><br><p>  En este momento vi una alerta y una advertencia.  R√°pidamente ejecuto <strong>kubectl get pods</strong> para ver qu√© da√±o sufrimos, y me sorprende descubrir que los pods en la aplicaci√≥n murieron exactamente 0. <strong>Realizo nodos superiores de kubectl</strong> , pero esta comprobaci√≥n tambi√©n muestra que el nodo sospechoso tiene un problema de memoria;  Es cierto que ya se ha recuperado y utiliza aproximadamente el 60% de su memoria.  Son las 5 p.m., y la cerveza artesanal ya se est√° calentando.  Despu√©s de asegurarme de que el nodo funcionara y no se da√±ara una sola c√°psula, decid√≠ que hab√≠a ocurrido un accidente.  En todo caso, lo resolver√© el lunes. </p><br><p>  Aqu√≠ est√° nuestra correspondencia con la estaci√≥n de servicio en Slack esa noche: </p><br><p><img src="https://habrastorage.org/webt/ib/fp/f0/ibfpf05vnjd2uaukxr0rwjgcjj0.png"></p><br><h2 id="sluchay-vtoroy">  Caso dos </h2><br><h3 id="subbota-16-iyunya-2019-g-1802">  S√°bado 16 de junio de 2019 6:02 p.m. </h3><br><p>  Blue Matador genera una alerta: el evento ya est√° en otro nodo, tambi√©n SystemOOM.  Debe haber sido que la estaci√≥n de servicio en ese momento solo estaba mirando la pantalla del tel√©fono inteligente porque me escribi√≥ y me hizo tomar el evento de inmediato, yo mismo no puedo encender la computadora (¬øes hora de reinstalar Windows nuevamente?).  Y de nuevo, todo parece normal.  No se mata ni una sola c√°psula, y el nodo apenas consume el 70% de la memoria. </p><br><h3 id="1806">  18:06 </h3><br><p>  Blue Matador genera una advertencia nuevamente: EBS Burst Balance.  La segunda vez en un d√≠a, lo que significa que no puedo liberar este problema en los frenos.  Con CloudWatch sin cambios, Burst Balance se desvi√≥ de la norma 2 horas o m√°s antes de que se identificara el problema. </p><br><h3 id="1811">  18:11 </h3><br><p>  Voy al registro de datos y miro los datos sobre el consumo de memoria.  Veo que justo antes del evento SystemOOM, el nodo sospechoso realmente tom√≥ mucha memoria.  El sendero conduce a nuestras vainas de fluidos sumol√≥gicos. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/s-/k-/aes-k-fwfx5qtsm7ehvhkgnpofe.png"></a> </p><br><p>  Puede ver claramente una desviaci√≥n brusca en el consumo de memoria, aproximadamente al mismo tiempo que ocurrieron los eventos de SystemOOM.  Mi conclusi√≥n: fueron estos pods los que tomaron toda la memoria, y cuando sucedi√≥ SystemOOM, Kubernetes se dio cuenta de que estos pods pod√≠an ser eliminados y reiniciados para devolver la memoria necesaria sin afectar mis otros pods.  ¬°Bien hecho, Kubernetes! </p><br><p>  Entonces, ¬øpor qu√© no vi esto el s√°bado cuando descubr√≠ qu√© c√°psulas se reiniciaron?  El hecho es que ejecuto c√°psulas de suma fluida en un espacio de nombres separado y con prisa simplemente no pens√© en investigarlo. </p><br><blockquote>  Conclusi√≥n 1: cuando busque pods reiniciados, verifique todos los espacios de nombres. </blockquote><p>  Habiendo recibido estos datos, calcul√© que al d√≠a siguiente la memoria en otros nodos no terminar√≠a, sin embargo, segu√≠ adelante y reinici√© todos los pods sumol√≥gicos para que comenzaran a trabajar con un bajo consumo de memoria.  A la ma√±ana siguiente, planeo preparar c√≥mo integrar el trabajo sobre el problema en un plan para la semana y no cargar demasiado el domingo por la noche. </p><br><h3 id="2300">  23:00 </h3><br><p>  Vi la pr√≥xima serie de "Black Mirror" (por cierto, me gust√≥ Miley) y decid√≠ ver c√≥mo estaba el grupo.  El consumo de memoria es normal, as√≠ que si√©ntete libre de dejar todo como est√° por la noche. </p><br><h2 id="pochinka">  Arreglar </h2><br><p>  El lunes, hice tiempo para este problema.  No est√° de m√°s cazar con ella todas las noches.  Lo que s√© en este momento: </p><br><ul><li>  Los contenedores con suma fluidez engullen una tonelada de memoria; </li><li>  El evento SystemOOM est√° precedido por una alta actividad de disco, pero no s√© cu√°l. </li></ul><br><p>  Al principio, pens√© que los contenedores de sumolog√≠a fluida son aceptados para comer memoria ante una afluencia repentina de troncos.  Sin embargo, despu√©s de verificar Sumologic, vi que los registros se usaron de manera estable y, al mismo tiempo, cuando hubo problemas, no hubo un aumento en estos registros. </p><br><p>  Un poco en Google, encontr√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta tarea en github</a> , que sugiere ajustar algunas configuraciones de Ruby, para reducir el consumo de memoria.  Decid√≠ probarlo, agregar una variable de entorno a la especificaci√≥n del pod y ejecutarlo: </p><br><pre><code class="plaintext hljs">env: - name: RUBY_GC_HEAP_OLDOBJECT_LIMIT_FACTOR value: "0.9"</code> </pre> <br><p>  Mirando a trav√©s del manifiesto fluentd-sumologic, not√© que no defin√≠a las solicitudes y restricciones de recursos.  Estoy empezando a sospechar que la soluci√≥n RUBY_GCP_HEAP har√° alg√∫n tipo de milagro, por lo que ahora tiene sentido establecer l√≠mites de consumo de memoria.  Incluso si no soluciono el problema de memoria, al menos ser√° posible limitar su consumo a este conjunto de pods.  Usando <strong>kubectl top pods |</strong>  <strong>grep fluentd-sumologic</strong> , ya s√© cu√°ntos recursos solicitar: </p><br><pre> <code class="plaintext hljs">resources: requests: memory: "128Mi" cpu: "100m" limits: memory: "1024Mi" cpu: "250m"</code> </pre> <br><blockquote>  Conclusi√≥n 2: Establezca l√≠mites de recursos, especialmente para aplicaciones de terceros. </blockquote><br><h2 id="proverka-ispolneniya">  Verificaci√≥n de ejecuci√≥n </h2><br><p>  Despu√©s de unos d√≠as, confirmo que el m√©todo anterior funciona.  El consumo de memoria fue estable y no hubo problemas con ning√∫n componente de Kubernetes, EC2 y EBS.  Ahora est√° claro lo importante que es determinar las solicitudes y restricciones de recursos para todos los pods que ejecuto, y esto es lo que hay que hacer: aplicar una combinaci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">l√≠mites de recursos predeterminados y</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cuotas de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">recursos</a> . </p><br><p>  El √∫ltimo misterio sin resolver es EBS Burst Balance, que coincidi√≥ con el evento SystemOOM.  S√© que cuando hay poca memoria, el sistema operativo utiliza el espacio de intercambio para no quedarse completamente sin memoria.  Pero no nac√≠ ayer y soy consciente de que Kubernetes ni siquiera se iniciar√° en los servidores donde se activa el archivo de p√°gina.  Solo queriendo asegurarme, sub√≠ a mis nodos a trav√©s de SSH, para verificar si el archivo de p√°gina estaba activado;  Utilic√© tanto la memoria libre como la del √°rea de intercambio.  El archivo no fue activado. </p><br><p>  Y dado que el intercambio no funciona, tengo m√°s pistas sobre qu√© caus√≥ el crecimiento de los flujos de E / S, por lo que el nodo casi se qued√≥ sin memoria, no.  En realidad, tengo un presentimiento: el m√≥dulo de sumolog√≠a fluida estaba escribiendo una tonelada de mensajes de registro en este momento, posiblemente incluso un mensaje de registro relacionado con la configuraci√≥n de Ruby GC.  Tambi√©n es posible que haya otras fuentes de mensajes de Kubernetes o de diario que se vuelvan excesivamente productivos cuando se agote la memoria, y los elimin√© al configurar con fluidez.  Desafortunadamente, ya no tengo acceso a los archivos de registro grabados inmediatamente antes del mal funcionamiento, y ahora no puedo profundizar m√°s. </p><br><blockquote>  Conclusi√≥n 3: Si bien existe una oportunidad, profundice al analizar las causas ra√≠z, sea cual sea el problema. </blockquote><br><h2 id="zaklyuchenie">  Conclusi√≥n </h2><br><p>  Y aunque no llegu√© a la ra√≠z de las causas, estoy seguro de que no son necesarias para evitar el mismo mal funcionamiento en el futuro.  El tiempo es dinero, pero he estado ocupado durante demasiado tiempo, y despu√©s de eso tambi√©n escrib√≠ esta publicaci√≥n para ti.  Y como usamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Blue Matador</a> , tratamos con tales fallas con gran detalle, as√≠ que me permito liberar algo en los frenos sin distraerme del proyecto principal. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467823/">https://habr.com/ru/post/467823/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467811/index.html">Arquitectura y programaci√≥n Fairchild Channel F</a></li>
<li><a href="../467813/index.html">Revisi√≥n de los cambios en el orden 17 del FSTEC</a></li>
<li><a href="../467815/index.html">Los medios de comunicaci√≥n generaron p√°nico porque "las direcciones IP se est√°n agotando en Rusia". Como realmente</a></li>
<li><a href="../467817/index.html">Un poco sobre patrones de dise√±o generativo</a></li>
<li><a href="../467821/index.html">Simplifique y recorte lo necesario: Entrevista con John Romero, creador de Doom</a></li>
<li><a href="../467825/index.html">Algoritmos de aprendizaje autom√°tico imprescindibles</a></li>
<li><a href="../467827/index.html">C√≥mo hicimos nuestra peque√±a Unidad desde cero</a></li>
<li><a href="../467831/index.html">El camino espinoso a la programaci√≥n</a></li>
<li><a href="../467837/index.html">MCU de tres centavos "terrible": una breve descripci√≥n de los microcontroladores que cuestan menos de $ 0.1</a></li>
<li><a href="../467841/index.html">Facilite terminar: Entrevista con John Romero, desarrollador de Doom</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>