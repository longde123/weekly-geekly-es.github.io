<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔫 👩🏼‍✈️ 💯 Técnica de redução de rede de convolução Jedi - poda 👩🏽‍🤝‍👨🏿 🧕🏼 👇🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Antes de você novamente, a tarefa de detectar objetos. Prioridade - velocidade com precisão aceitável. Você pega a arquitetura do YOLOv3 e a treina. A...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Técnica de redução de rede de convolução Jedi - poda</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482050/"><p><img src="https://habrastorage.org/webt/tf/oa/br/tfoabr16w_dawnzb9hnjndyv_bg.png" alt="imagem"></p><br><p>  Antes de você novamente, a tarefa de detectar objetos.  Prioridade - velocidade com precisão aceitável.  Você pega a arquitetura do YOLOv3 e a treina.  A precisão (mAp75) é maior que 0,95.  Mas a velocidade de execução ainda é baixa.  Inferno </p><br><p>  Hoje vamos ignorar a quantização.  E, por baixo do corte, considere a <strong>poda de modelos</strong> - aparar peças de rede redundantes para acelerar a inferência sem perder a precisão.  Visualmente - onde, quanto e como cortar.  Vamos descobrir como fazer isso manualmente e onde você pode automatizar.  No final, há um repositório no keras. </p><a name="habracut"></a><br><h3 id="vvedenie">  1. Introdução </h3><br><p>  No último local de trabalho, Perm Macroscop, eu tenho um hábito - sempre monitorar o tempo de execução dos algoritmos.  E o tempo de execução da rede sempre deve ser verificado através do filtro de adequação.  Normalmente, o estado da arte no produto não passa nesse filtro, o que me levou à poda. </p><br><p>  A poda é um tema antigo sobre o qual falamos nas <a href="https://www.youtube.com/watch%3Fv%3DeZdOkDtYMoo" rel="nofollow">palestras de</a> 2017 em <a href="https://www.youtube.com/watch%3Fv%3DeZdOkDtYMoo" rel="nofollow">Stanford</a> .  A idéia principal é reduzir o tamanho da rede treinada sem perder a precisão, removendo vários nós.  Parece legal, mas eu raramente ouço sobre o seu uso.  Provavelmente, não há implementações suficientes, não há artigos em russo ou apenas todos consideram o poda e o silêncio. <br>  Mas vá desmontar </p><br><h3 id="vzglyad-v-biologiyu">  Um olhar sobre a biologia </h3><br><p>  Adoro quando, nas idéias do Deep Learning, vêm da biologia.  Eles, como a evolução, podem ser confiáveis ​​(você sabia que a ReLU é muito semelhante à <a href="http://www.gatsby.ucl.ac.uk/~lmate/biblio/dayanabbott.pdf" rel="nofollow">função de ativar neurônios no cérebro</a> ?) </p><br><p>  O processo de poda do modelo também está próximo da biologia.  A resposta da rede aqui pode ser comparada com a plasticidade do cérebro.  Alguns exemplos interessantes estão no livro de <a href="https://www.litres.ru/norman-doydzh/plastichnost-mozga/%3Futm_medium%3Dcpc%26utm_source%3Dgoogle%26utm_campaign%3DDSA%257C149839530%26utm_term%3D%26utm_content%3Dk50id%257Caud-499675211712%253Adsa-179513627318%257Ccid%257C149839530%257Caid%257C248455294996%257Cgid%257C6837176850%257Cpos%257C1t1%257Csrc%257Cg_%257Cdvc%257Cc%257Creg%257C1011993%257Crin%257C%257C%26k50id%3D6837176850%257Caud-499675211712%253Adsa-179513627318%26gclid%3DCj0KCQiA0ZHwBRCRARIsAK0Tr-oKPqkmL7_Oxg62JZO8Jlk9zO-9nYKIRFxHi_lgoCvsQQadvUGxUzkaApgpEALw_wcB" rel="nofollow">Norman Dodge</a> : </p><br><ol><li>  O cérebro de uma mulher que tinha apenas metade do nascimento reprogramava-se para desempenhar as funções da metade desaparecida </li><li>  O cara se matou na parte do cérebro responsável pela visão.  Com o tempo, outras partes do cérebro assumiram essas funções.  (não tente novamente) </li></ol><br><p>  Portanto, no seu modelo, você pode cortar alguns dos pacotes fracos.  Em casos extremos, os pacotes restantes ajudarão a substituir os pacotes cortados. </p><br><h3 id="lyubish-transfer-learning-ili-uchish-s-nulya">  Você gosta de Transfer Learning ou aprende do zero? </h3><br><p>  <strong>Opção número um.</strong>  Você está usando o Transfer Learning no Yolov3.  Retina, Mask-RCNN ou U-Net.  Mas, na maioria das vezes, não precisamos reconhecer 80 classes de objetos, como no COCO.  Na minha prática, tudo é limitado a 1-2 aulas.  Pode-se supor que a arquitetura para 80 classes seja redundante aqui.  Implora o pensamento de que a arquitetura precisa ser reduzida.  Além disso, eu gostaria de fazer isso sem perder os pesos pré-treinados existentes. </p><br><p>  <strong>Opção número dois.</strong>  Talvez você tenha muitos dados e recursos de computação ou apenas precise de uma arquitetura super personalizada.  Isso não importa.  Mas você aprende a rede do zero.  A ordem usual é olhar para a estrutura de dados, selecionar uma arquitetura que seja REDUZIDA em termos de energia e impedir que os desistentes sejam treinados novamente.  Vi desistências 0,6, Carl. </p><br><p>  Nos dois casos, a rede pode ser reduzida.  Promovido.  Agora vamos descobrir que tipo de <del>  circuncisão </del>  poda </p><br><h3 id="obschiy-algoritm">  Algoritmo geral </h3><br><p>  Decidimos que poderíamos remover a convolução.  Parece muito simples: </p><br><p><img src="https://habrastorage.org/webt/ey/yt/-g/eyyt-g-b6pfzjrbnim_ssosyqqk.png"></p><br><p>  A remoção de qualquer convolução é um estresse para a rede, o que geralmente leva a um aumento no erro.  Por um lado, esse crescimento de erro é um indicador de como removemos corretamente a convolução (por exemplo, um grande crescimento indica que estamos fazendo algo errado).  Mas um pequeno crescimento é bastante aceitável e é frequentemente eliminado por um treinamento posterior fácil e subsequente com uma pequena RL.  Adicionamos uma etapa de reciclagem: </p><br><p><img src="https://habrastorage.org/webt/kb/ui/5d/kbui5dm1k8sgflm5xzu0wggbbhs.png"></p><br><p>  Agora precisamos entender quando queremos interromper nosso ciclo de aprendizagem &lt;-&gt; Poda.  Pode haver opções exóticas quando precisamos reduzir a rede para um determinado tamanho e velocidade de execução (por exemplo, para dispositivos móveis).  No entanto, a opção mais comum é continuar o ciclo até que o erro se torne mais alto do que o permitido.  Adicionar condição: </p><br><p><img src="https://habrastorage.org/webt/1i/pi/52/1ipi52uqhkciw2ne-1zt2rbdmje.png"></p><br><p>  Então, o algoritmo fica claro.  Resta desmontar como determinar as convoluções excluídas. </p><br><h3 id="poisk-udalyaemyh-svertok">  Pesquisa por convolução a ser excluída </h3><br><p>  Precisamos remover algumas convoluções.  Ir adiante e "disparar" qualquer uma é uma má idéia, embora funcione.  Mas se você tem cabeça, pode pensar e tentar selecionar convoluções "fracas" para remoção.  Existem várias opções: </p><br><ol><li>  <a href="https://openreview.net/pdf%3Fid%3DrJqFGTslg" rel="nofollow">A menor medida L1 ou poda_de_magnitude_ baixa</a> .  A ideia de que convoluções com pesos pequenos contribuem pouco para a decisão final </li><li>  A menor medida L1, levando em consideração a média e o desvio padrão.  Complementamos a avaliação da natureza da distribuição. </li><li>  <a href="https://arxiv.org/abs/1512.08571" rel="nofollow">Mascarar as convoluções e eliminar o mínimo que afeta a precisão resultante</a> .  Uma definição mais precisa de convulsões insignificantes, mas que consomem muito tempo e consomem muitos recursos. </li><li>  Outros </li></ol><br><p>  Cada uma das opções tem direito à vida e seus próprios recursos de implementação.  Aqui consideramos a variante com a menor medida L1 </p><br><h3 id="ruchnoy-process-dlya-yolov3">  Processo manual para YOLOv3 </h3><br><p>  A arquitetura original contém blocos residuais.  Mas, por mais legais que sejam em redes profundas, eles nos atrapalham um pouco.  A dificuldade é que você não pode excluir reconciliações com diferentes índices nessas camadas: </p><br><p><img src="https://habrastorage.org/webt/mh/p-/-k/mhp--ksk3ifgurz5jx6exgcrm5c.png"></p><br><p>  Portanto, selecionamos as camadas das quais podemos remover livremente reconciliações: </p><br><p><img src="https://habrastorage.org/webt/qy/ek/zo/qyekzofcur-q0auqurg3egxnato.png"></p><br><p>  Agora vamos construir um ciclo de trabalho: </p><br><ol><li>  Descarregar a ativação </li><li>  Queremos saber quanto cortar </li><li>  Cortar </li><li>  Aprenda 10 eras com LR = 1e-4 </li><li>  Teste </li></ol><br><p>  A descarga de convoluções é útil para avaliar qual parte podemos remover em uma determinada etapa.  Exemplos de descarregamento: </p><br><p><img src="https://habrastorage.org/webt/rp/jo/pk/rpjopk6dzfrl6psoucr8tgj0log.png"></p><br><p>  Vemos que em quase todos os lugares 5% das convoluções têm uma norma L1 muito baixa e podemos removê-las.  Em cada etapa, esse descarregamento era repetido e uma avaliação era feita de quais camadas e quanto poderia ser cortado. </p><br><p>  Todo o processo foi concluído em 4 etapas (aqui e em toda parte os números do RTX 2060 Super): </p><br><div class="scrollable-table"><table><thead><tr><th>  Etapa </th><th>  mAp75 </th><th>  O número de parâmetros, milhões </th><th>  Tamanho da rede, mb </th><th>  Do original,% </th><th>  Tempo de execução, ms </th><th>  Condição da circuncisão </th></tr></thead><tbody><tr><td>  0 0 </td><td>  0,9656 </td><td>  60 </td><td>  241 </td><td>  100 </td><td>  180 </td><td>  - </td></tr><tr><td>  1 </td><td>  0,9622 </td><td>  55 </td><td>  218 </td><td>  91 </td><td>  175 </td><td>  5% de todos </td></tr><tr><td>  2 </td><td>  0,9625 </td><td>  50. </td><td>  197 </td><td>  83 </td><td>  168 </td><td>  5% de todos </td></tr><tr><td>  3 </td><td>  0,9633 </td><td>  39. </td><td>  155 </td><td>  64 </td><td>  155 </td><td>  15% para camadas com mais de 400 convoluções </td></tr><tr><td><del>  4 </del></td><td><del>  0,9555 </del></td><td><del>  31 </del></td><td><del>  124 </del></td><td><del>  51 </del></td><td><del>  146 </del></td><td><del>  10% para camadas com mais de 100 convoluções </del></td></tr></tbody></table></div><br><p>  Na etapa 2, um efeito positivo foi adicionado - o tamanho do patch 4 entrou na memória, o que acelerou bastante o processo de reciclagem. <br>  Na etapa 4, o processo foi interrompido porque  mesmo o ensino superior prolongado não elevou o mAp75 a valores antigos. <br>  Como resultado, conseguimos acelerar a inferência em <strong>15%</strong> , reduzir o tamanho em <strong>35%</strong> e não perder a precisão. </p><br><h3 id="avtomatizaciya-dlya-bolee-prostyh-arhitektur">  Automação para arquiteturas mais simples </h3><br><p>  Para arquiteturas de rede mais simples (sem adição condicional, blocos concatenados e residuais), é bem possível se concentrar no processamento de todas as camadas convolucionais e automatizar o processo de corte de convoluções. </p><br><p>  Eu implementei <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">essa</a> opção <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">aqui</a> . <br>  É simples: você só tem uma função de perda, um otimizador e geradores de lote: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pruning <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequence train_batch_generator = BatchGenerator... score_batch_generator = BatchGenerator... opt = Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>) pruner = pruning.Pruner(<span class="hljs-string"><span class="hljs-string">"config.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"categorical_crossentropy"</span></span>, opt) pruner.prune(train_batch, valid_batch)</code> </pre> <br><p>  Se necessário, você pode alterar os parâmetros de configuração: </p><br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"input_model_path"</span></span>: <span class="hljs-string"><span class="hljs-string">"model.h5"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"output_model_path"</span></span>: <span class="hljs-string"><span class="hljs-string">"model_pruned.h5"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"finetuning_epochs"</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>, # the number of epochs for train between pruning steps <span class="hljs-attr"><span class="hljs-attr">"stop_loss"</span></span>: <span class="hljs-number"><span class="hljs-number">0.1</span></span>, # loss for stopping process <span class="hljs-attr"><span class="hljs-attr">"pruning_percent_step"</span></span>: <span class="hljs-number"><span class="hljs-number">0.05</span></span>, # part of convs for delete on every pruning step <span class="hljs-attr"><span class="hljs-attr">"pruning_standart_deviation_part"</span></span>: <span class="hljs-number"><span class="hljs-number">0.2</span></span> # shift for limit pruning part }</code> </pre> <br><p>  Além disso, uma restrição baseada no desvio padrão é implementada.  O objetivo é limitar uma parte dos excluídos, excluindo convoluções com medidas L1 já "suficientes": </p><br><p><img src="https://habrastorage.org/webt/bh/_9/nq/bh_9nqasnp91xifixn7ilhco0mw.png"></p><br><p>  Assim, podemos remover apenas convoluções fracas de distribuições semelhantes à direita e não afetar a remoção de distribuições como a esquerda: </p><br><p><img src="https://habrastorage.org/webt/pr/r5/zp/prr5zpjrdvh1wow6slejnn6axya.png"></p><br><p>  Quando a distribuição se aproxima do normal, o coeficiente pruning_standart_deviation_part pode ser selecionado em: </p><br><p><img src="https://habrastorage.org/webt/dl/yl/7d/dlyl7dub216jsr67dcbnhez5fl8.png"><br>  Eu recomendo uma suposição de 2 sigma.  Ou você não pode se concentrar nesse recurso, deixando o valor &lt;1.0. </p><br><p>  A saída é um gráfico do tamanho da rede, perda e tempo de execução da rede para todo o teste, normalizado para 1,0.  Por exemplo, aqui o tamanho da rede foi reduzido quase duas vezes sem perda de qualidade (uma pequena rede de convolução para pesos de 100k): </p><br><p><img src="https://habrastorage.org/webt/ig/hu/x_/ighux_gyoaptm71iu2hk_txga_g.png"></p><br><p>  A velocidade de operação está sujeita a flutuações normais e não mudou muito.  Há uma explicação para isso: </p><br><ol><li>  O número de convoluções muda de conveniente (32, 64, 128) para não o mais conveniente para placas de vídeo - 27, 51 etc.  Aqui posso estar enganado, mas provavelmente isso afeta. </li><li>  A arquitetura não é ampla, mas consistente.  Reduzindo a largura, não tocamos na profundidade.  Assim, reduzimos a carga, mas não alteramos a velocidade. </li></ol><br><p>  Portanto, a melhoria foi expressa em uma redução na carga de CUDA durante a execução em 20 a 30%, mas não em uma diminuição no tempo de execução </p><br><h3 id="itogi">  Sumário </h3><br><p>  Reflita.  Consideramos duas opções de remoção - para o YOLOv3 (quando você precisa trabalhar com as mãos) e para redes com arquiteturas mais fáceis.  Pode-se observar que em ambos os casos é possível obter uma redução no tamanho e na aceleração da rede sem perda de precisão.  Resultados: </p><br><ul><li>  Downsizing </li><li>  Executar aceleração </li><li>  Redução de carga CUDA </li><li>  Como resultado, respeito pelo meio ambiente (otimizamos o uso futuro dos recursos de computação. Em algum lugar, <a href="https://meduza.io/feature/2019/12/12/kto-takaya-greta-tunberg-i-pochemu-ona-stala-chelovekom-goda-zhurnal-time" rel="nofollow">Greta Tunberg</a> se alegra sozinho) </li></ul><br><h3 id="appendix">  Apêndice </h3><br><ul><li>  Após a etapa de remoção, você também pode torcer a quantização (por exemplo, com TensorRT) </li><li>  O Tensorflow fornece recursos para <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="nofollow">poda</a> de <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="nofollow">baixa magnitude</a> .  Isso funciona. </li><li>  Quero desenvolver o <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">repositório</a> e terei prazer em ajudar </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt482050/">https://habr.com/ru/post/pt482050/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt482034/index.html">Yandex: há tudo ... sobre os usuários</a></li>
<li><a href="../pt482038/index.html">Estamos resumindo os resultados de 2019 na Haber Career</a></li>
<li><a href="../pt482040/index.html">Apresenta programas de criação de perfil em C ++</a></li>
<li><a href="../pt482042/index.html">Trabalhando com a biblioteca Newtonsoft.Json com um exemplo real. Parte 2</a></li>
<li><a href="../pt482044/index.html">10 práticas recomendadas para proteger imagens do Docker. Parte 2</a></li>
<li><a href="../pt482052/index.html">Conjunto de dados de ano novo 2019: dicionário tonal aberto do idioma russo</a></li>
<li><a href="../pt482054/index.html">3. Pilha elástica: análise de log de segurança. Dashboards</a></li>
<li><a href="../pt482058/index.html">Predador ou presa? Quem protegerá as autoridades de certificação</a></li>
<li><a href="../pt482060/index.html">Modelo de mandato de controle de acesso (MAC): visão geral e aplicativos</a></li>
<li><a href="../pt482064/index.html">Facilidade de desenvolvimento de sites multilíngues no CMS Umbraco 8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>