<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôã üëçüèø üë©üèΩ Rede neural GPT-2 da OpenAI. In√≠cio r√°pido üè£ üí± üëî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mal o barulho sobre a rede neural BERT do Google, que mostrou resultados avan√ßados em v√°rias tarefas de conversa√ß√£o (PNL) em aprendizado de m√°quina, c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Rede neural GPT-2 da OpenAI. In√≠cio r√°pido</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440564/"><p><img src="https://habrastorage.org/getpro/habr/post_images/1cf/d63/ae6/1cfd63ae6b68d59325ef90cc4ea93f35.png" alt="imagem"></p><br><p>  Mal o barulho sobre a rede neural BERT do Google, que mostrou resultados avan√ßados em v√°rias tarefas de conversa√ß√£o (PNL) em aprendizado de m√°quina, como a OpenAI lan√ßou um novo desenvolvimento: GPT-2.  Essa rede neural com um n√∫mero recorde de par√¢metros no momento (1,5 bilh√£o, contra os 100-300 milh√µes comumente usados ‚Äã‚Äãnesses casos) conseguiu gerar p√°ginas inteiras de texto conectado. </p><br><p>  √â t√£o bom gerar que a OpenAI se recusou a publicar a vers√£o completa, temendo que eles usassem essa rede neural para criar not√≠cias falsas, coment√°rios e cr√≠ticas indistingu√≠veis das reais. </p><br><p>  No entanto, no OpenAI, uma vers√£o reduzida da rede neural GPT-2 foi compartilhada com 117 milh√µes de par√¢metros.  Vamos lan√ß√°-lo atrav√©s do servi√ßo Google Colab e experimentar com ele. </p><a name="habracut"></a><br><h1 id="nemnogo-predystorii">  Um pouco de fundo </h1><br><p>  Para aqueles que n√£o acompanharam o desenvolvimento do progresso no processamento natural da fala (PNL). </p><br><p>  No ver√£o de 2018, a OpenAI pr√©-treinou em um grande volume de texto uma rede neural da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GPT</a> constru√≠da na arquitetura Transformer.  Descobriu-se que, se voc√™ substituir algumas das √∫ltimas camadas e trein√°-la novamente para uma tarefa espec√≠fica (essa abordagem √© chamada de Ajuste fino e √© amplamente usada no aprendizado de m√°quina), ela quebra imediatamente os registros anteriores em uma ampla variedade de tarefas de conversa√ß√£o. </p><br><p>  Com base nesse desenvolvimento, no final de 2018, o Google criou sua pr√≥pria rede neural <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BERT</a> .  Eles melhoraram seriamente o resultado, tornando a rede neural bidirecional, ao contr√°rio da GPT. </p><br><p>  N√£o querendo desistir, em fevereiro de 2019, o OpenAI imediatamente aumentou sua GPT em 10 vezes e a treinou em uma quantidade ainda maior de texto - em 8 milh√µes de p√°ginas da Web (um total de 40 GB de texto).  A rede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GPT-2</a> resultante √© atualmente a maior rede neural, com um n√∫mero sem precedentes de par√¢metros de 1,5 bilh√£o (o BERT tinha 340 milh√µes no maior modelo e 110 milh√µes no BERT padr√£o). </p><br><p>  Como resultado, o GPT-2 conseguiu gerar p√°ginas inteiras de texto coerente.  Com refer√™ncias repetidas aos nomes dos personagens no decorrer da narrativa, cita√ß√µes, refer√™ncias a eventos relacionados e assim por diante.  N√£o darei exemplos aqui, mas refiro aqueles que desejam o artigo original no blog da OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores modelos de linguagem e suas implica√ß√µes</a> ou os links no final do artigo. </p><br><p>  A gera√ß√£o de um texto coerente dessa qualidade √© impressionante por si s√≥, mas a coisa mais interessante √© diferente.  O GPT-2 sem nenhum treinamento adicional mostrou imediatamente resultados pr√≥ximos do estado da arte em v√°rias tarefas de conversa√ß√£o.  Repito, quem perdeu a import√¢ncia do momento - sem nenhum treinamento adicional para uma tarefa espec√≠fica! </p><br><p>  Como eles conseguiram isso?  Apenas perguntando √†s redes neurais as perguntas certas. </p><br><h1 id="arhitektura-gpt-2">  Arquitetura GPT-2 </h1><br><p>  O GPT-2 √© treinado para prever a pr√≥xima palavra em uma frase.  Essa √© uma abordagem cl√°ssica para gerar texto.  A princ√≠pio, as redes de recorr√™ncia (RNN), em particular a LSTM, mantinham o primado nessa √°rea.  Mas ap√≥s a inven√ß√£o da arquitetura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Transformer</a> no ver√£o de 2017, ela gradualmente come√ßou a prevalecer em tarefas de conversa√ß√£o.  Embora o Transformer original tenha um problema ao armazenar sequ√™ncias longas (os LSTMs lembram as mais longas), a velocidade do treinamento e a profundidade da rede mais do que compensaram isso.  A prop√≥sito, v√°rias modifica√ß√µes do transformador j√° apareceram - com a introdu√ß√£o de recorr√™ncia ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Universal Transformers</a> ), uma modifica√ß√£o para sequ√™ncias mais longas ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Transformer-XL</a> ) e outras, mas at√© agora apenas um Transformer original ligeiramente ajustado √© usado no Google e OpenAI. </p><br><p>  Lembro-me que o BERT do Google aprendeu um pouco diferente: prever n√£o a pr√≥xima palavra em uma frase, mas as palavras perdidas (mascaradas) em uma frase.  E tamb√©m para determinar se duas senten√ßas consecutivas s√£o uma continua√ß√£o l√≥gica uma da outra ou se n√£o est√£o de forma alguma conectadas por significado.  Isso permitiu que o BERT fosse um modelo de linguagem que entendesse o significado das palavras, dependendo do ambiente (contexto).  O que determinou o sucesso do BERT nas tarefas de NPL.  Mas somente ap√≥s a reciclagem (Ajuste fino) para uma tarefa espec√≠fica.  Apenas prever palavras no modelo base n√£o funciona muito bem nele.  Voc√™ pode jogar com o BERT no seu navegador (via Google Colab): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://habr.com/en/post/436878</a> . </p><br><p>  GPT-2 n√£o √© necess√°rio treinar novamente.  Este n√£o √© apenas um modelo de linguagem como o BERT, √© um gerador de texto.  Apenas d√™ a ela o in√≠cio da frase e ela complementar√° sua palavra por palavra. </p><br><p>  Um detalhe interessante: a pesquisa da OpenAI mostrou que matrizes de textos da Wikipedia e livros liter√°rios (que o BERT estudou em particular) t√™m um estilo tendencioso.  Portanto, treinados apenas nessas redes neurais n√£o geram texto muito bem.  Para diversificar os dados e estilos de entrada, a OpenAI usou o GPT-2 para treinamento em p√°ginas regulares da Web coletadas em 8 milh√µes de sites (um total de 40 GB de texto).  E, para descartar sites de publicidade e spammer, eles inclu√≠ram nos sites de amostra cujos links no reddit t√™m uma boa classifica√ß√£o.  Ou seja, sites que exibem usu√°rios que cont√™m informa√ß√µes √∫teis. </p><br><h1 id="pravilnyy-vopros-soderzhit-polovinu-otveta">  A pergunta correta cont√©m metade da resposta. </h1><br><p>  Assim, o GPT-2, gra√ßas ao seu tamanho sem precedentes, conseguiu gerar p√°ginas de texto coerente.  Mas o mais surpreendente √© que, ao fazer a pergunta certa (ou seja, o in√≠cio correto de uma frase), ela conseguiu responder a v√°rias perguntas!  S√≥ porque a continua√ß√£o desse come√ßo √© a mais natural. </p><br><p>  Por exemplo, para obter uma resposta para a pergunta "O que √© a Terra?", Voc√™ pode aplicar √† entrada dessa rede neural o in√≠cio da frase: "A Terra √© ...".  E ela completar√° esta frase at√© o fim.  Porque a resposta ser√° uma continua√ß√£o natural deste come√ßo. </p><br><p>  Al√©m disso, ao formar o in√≠cio da frase da maneira correta, voc√™ pode obter explica√ß√µes para diferentes p√∫blicos-alvo, levando em considera√ß√£o sua intelig√™ncia, idade e escolaridade.  Imagine frases cont√≠nuas: "Eu, como cientista, acredito que a Terra √© ...".  Ou: "Eu, como lavrador de terras, afirmo que a Terra √© ...".  Ou: "Eu, como professora de um jardim de inf√¢ncia, agora vou explicar para voc√™s, crian√ßas, que a Terra √© ...". </p><br><p>  Como voc√™ pode ver, formando as perguntas certas (o come√ßo correto da frase), voc√™ pode obter respostas de n√≠veis completamente diferentes e detalhes diferentes.  De certa forma, algo semelhante acontece nas pessoas.  O m√©dico deve explicar ao paciente o curso da doen√ßa para que ele entenda.  No n√≠vel do paciente.  Se voc√™ perguntar a uma crian√ßa de cinco anos por que ele fez isso, ela n√£o poder√° responder imediatamente (que, naturalmente, as crian√ßas convivem com sentimentos e emo√ß√µes).  Mas, para dar a resposta que se espera dele, a crian√ßa come√ßa a invent√°-la - a gerar texto.  Com base no fato de que a resposta combina com os pais e que pelo menos de alguma forma corresponde ao que aconteceu.  A princ√≠pio, como muitos pais sabem, essas ser√£o respostas rid√≠culas.  Mas, incentivando e punindo ("conte-me mais", "n√£o invente desculpas"), a crian√ßa aprender√° a dar respostas detalhadas e completas. </p><br><p>  Esse desenvolvimento do OpenAI e a capacidade da rede GPT-2 de fornecer respostas para tarefas de conversa√ß√£o sem treinamento adicional especial para uma tarefa espec√≠fica abrem duas quest√µes interessantes: </p><br><p>  1) A interpretabilidade das redes neurais pode ser alcan√ßada por um gerador de texto t√£o elementar e pelo in√≠cio correto de uma frase.  Onde a resposta ser√° uma extens√£o natural.  Suponha, por exemplo, que uma rede neural n√£o indique selos em uma fotografia pelos n√∫meros das coordenadas x-y, mas explique sua posi√ß√£o em texto simples.  Ent√£o, no decorrer do esclarecimento, fazendo a pergunta certa, por exemplo: "Cheguei a essa conclus√£o porque ...", em teoria, voc√™ pode obter uma explica√ß√£o de como ela encontrou o gato na foto.  E essa explica√ß√£o no caso extremo n√£o pode ser pior que a humana.  O que resolve o problema global de interpretabilidade das redes neurais. </p><br><p>  2) Uma rede neural pr√©-treinada em grandes volumes de texto pode ser universal, ter bom senso e n√£o exigir treinamento adicional para tarefas espec√≠ficas.  Isso significa que, ao tentar imitar a fala humana (respostas humanas a perguntas), a rede neural deve inevitavelmente aprender o bom senso, a fim de fornecer respostas muito semelhantes √†s humanas.  Dar respostas fict√≠cias monossil√°bicas, em geral, n√£o √© t√≠pico para as pessoas.  Na maioria das vezes, as pessoas fornecem respostas adequadas detalhadas, o que significa que a rede deve aprender a fazer o mesmo. </p><br><p>  Ambas as quest√µes permanecem em aberto, mas o primeiro passo em sua aprova√ß√£o foi definitivamente dado. </p><br><h1 id="a-tochnee">  Ou melhor? </h1><br><p>  Se voc√™ est√° de p√© agora, √© melhor sentar.  Porque √© assim que o OpenAI, usando a rede neural GPT-2, obteve seus resultados em tarefas de conversa√ß√£o para diferentes dom√≠nios: </p><br><p>  <strong>Respostas a perguntas no texto</strong> </p><br><p>  Bem, isso √© f√°cil.  Ou alimentou a rede com alguns par√°grafos com uma descri√ß√£o inclu√≠da em algum lugar no meio, por exemplo, ‚Äúa ma√ß√£ est√° sobre a mesa‚Äù e, no final, foi atribu√≠da: ‚Äúa ma√ß√£ est√° ligada ...‚Äù e a rede foi adicionada √† ‚Äúmesa‚Äù.  Porque √© capaz de lembrar o contexto de v√°rios par√°grafos. </p><br><p>  Ou alimentou a rede como uma frase inicial de alguns exemplos do tipo "Pergunta: alguma pergunta, resposta: alguma resposta" e, no final, ap√≥s a pergunta real, eles adicionaram: "Resposta:"  E a rede neural anexou a resposta!  Uma vez que revelou a estrutura do documento na pergunta-resposta anterior.  Isso √© incr√≠vel. </p><br><p>  <strong>Vers√£o curta (resumo) do texto</strong> </p><br><p>  A entrada √© um texto longo de v√°rios par√°grafos ou at√© p√°ginas, e a rede neural deve escrever um conte√∫do curto.  Como voc√™ conseguiu esse comportamento do GPT-2?  Logo ap√≥s o texto, eles adicionaram "TL; DR".  E isso √© tudo!  Isso acabou sendo suficiente para o GPT-2 adicionar um resumo do artigo ap√≥s esses caracteres!  Porque esses s√≠mbolos na Internet frequentemente denotam o resumo da publica√ß√£o. </p><br><p>  <strong>Tradu√ß√£o de texto</strong> </p><br><p>  A entrada GPT-2 recebeu o texto no formato: "ol√° = ol√°, cachorro = cachorro, vento = vento, gato = ...".  E a rede neural acrescentou a tradu√ß√£o da √∫ltima palavra: "gato" (no original em franc√™s).  Porque revelou a estrutura do documento e simplesmente a complementou com a continua√ß√£o mais l√≥gica.  Se sua mand√≠bula ainda n√£o caiu de tudo isso, tenho duas novidades para voc√™ e ambas s√£o ruins =). </p><br><h1 id="zapusk-gpt-2-cherez-google-colab">  Lan√ßamento do GPT-2 atrav√©s do Google Colab </h1><br><p>  Infelizmente, a vers√£o completa do GPT-2 no OpenAI foi recusada para ser compartilhada.  Motivando isso pelo fato de que, usando essa rede neural, ser√° muito f√°cil gerar not√≠cias e cr√≠ticas falsas nas lojas.  A julgar pela declara√ß√£o, a discuss√£o sobre a adequa√ß√£o do layout desse modelo continuar√° pelos pr√≥ximos 6 meses. Ap√≥s o OpenAI, eles decidir√£o se devem envi√°-lo ou n√£o.  No entanto, para uma grande organiza√ß√£o, n√£o √© dif√≠cil repetir o modelo (parece que eles o treinaram para 256 TPU por v√°rios dias e, de acordo com estimativas preliminares, isso lhes custou cerca de US $ 45 mil) </p><br><p>  No entanto, eles publicaram uma vers√£o reduzida do GPT-2 com 117 milh√µes de par√¢metros (em vez de 1,5 bilh√£o, como no modelo completo): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/openai/gpt-2</a> .  Vamos tentar execut√°-lo e brincar com este modelo. </p><br><p>  Atualiza√ß√£o em 9 de novembro de 2019: finalmente, foi apresentada toda a linha de modelos, incluindo 1,5 bilh√£o de arquivos e instru√ß√µes para o lan√ßamento atualizado. </p><br><p>  A maneira mais f√°cil de fazer isso √© atrav√©s do Google Colab: </p><br><ol><li>  Abra o link </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/Gpt-2.ipynb</a> </p><br><ol><li>  No menu <strong>Tempo de Execu√ß√£o</strong> , selecione <strong>Executar Tudo</strong> , para que pela primeira vez todas as c√©lulas sejam iniciadas, o modelo √© baixado e as bibliotecas necess√°rias est√£o conectadas.  Concorde em redefinir todo o tempo de execu√ß√£o, se necess√°rio.  Digite o texto ap√≥s o aparecimento de "Prompt do modelo &gt;&gt;&gt;" e pressione Enter. </li></ol><br><p>  Preste aten√ß√£o √† linha logo no in√≠cio: </p><br><p>  model_name = '117M' </p><br><p>  Aqui voc√™ pode especificar o tamanho do modelo GPT-2 a ser usado.  Os seguintes modelos est√£o dispon√≠veis (sujeitos a atualiza√ß√£o): </p><br><p>  117M <br>  124M <br>  355M <br>  774M <br>  1558M </p><br><p>  Aqui, 117M √© o menor modelo que era o √∫nico dispon√≠vel no momento em que este artigo foi escrito.  Mais tarde, a OpenAI estabeleceu modelos cada vez maiores, at√© 5 de novembro de 2019, estabeleceu o m√°ximo de 1558M (com 1,5 bilh√µes de par√¢metros). </p><br><div class="spoiler">  <b class="spoiler_title">Se algo desse errado ...</b> <div class="spoiler_text"><p>  Verifique se GPU e Python 3 est√£o selecionados no menu Tempo de execu√ß√£o -&gt; Alterar tipo de tempo de execu√ß√£o </p><br><p>  Se o bot√£o de conex√£o n√£o estiver ativo, clique nele para se conectar. </p></div></div><br><p>  Ou crie todo o c√≥digo manualmente: </p><br><ol><li>  Acesse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://colab.research.google.com</a> </li><li>  Pressione o bot√£o azul NEW PYTHON 3 NOTEBOOK </li><li>  No menu Tempo de execu√ß√£o -&gt; Alterar tipo de tempo de execu√ß√£o, selecione Python 3 e a GPU (a √∫ltima para executar a rede neural na GPU) </li><li>  Na primeira c√©lula, digite: </li></ol><br><pre><code class="python hljs">model_name = <span class="hljs-string"><span class="hljs-string">'117M'</span></span> !git clone https://github.com/openai/gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> %cd gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> !pip3 install -r requirements.txt !python3 download_model.py $model_name</code> </pre> <br><p>  Em vez de 117M (o menor), voc√™ pode especificar qualquer modelo intermedi√°rio ou maior: 1558M. </p><br><p>  E clique no √≠cone preto Play √† esquerda da c√©lula.  Isso far√° o download da rede neural GPT-2 selecionada e instalar√° as depend√™ncias necess√°rias. </p><br><p>  Na segunda c√©lula (voc√™ pode adicion√°-lo atrav√©s do menu Inserir -&gt; C√≥digo da c√©lula ou passando o mouse sob o centro da c√©lula atual, os bot√µes de adi√ß√£o ser√£o exibidos): </p><br><pre> <code class="python hljs">!python3 src/interactive_conditional_samples.py --model_name=$model_name</code> </pre> <br><p>  Isso iniciar√° o modo interativo.  Aguarde at√© que a rede neural seja inicializada e uma janela para inserir texto apare√ßa com a inscri√ß√£o ‚ÄúModel Model &gt;&gt;&gt;‚Äù. Digite o in√≠cio da frase e pressione Enter. Ap√≥s algum tempo, o texto gerado aparecer√° sob o cabe√ßalho SAMPLE. </p><br><p>  Voc√™ tamb√©m pode iniciar o modo de gerar texto completamente aleat√≥rio.  O texto ser√° gerado infinitamente em pequenos peda√ßos de AMOSTRA 1, AMOSTRA 2 e assim por diante, at√© voc√™ clicar no bot√£o Parar na c√©lula.  Para fazer isso, crie uma nova c√©lula com o c√≥digo: </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt</code> </pre> <br><p>  O resultado ser√° salvo no arquivo samples.txt.  Pode ser baixado com os seguintes comandos (crie uma nova c√©lula novamente e execute-a ap√≥s gerar o texto): </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">'samples.txt'</span></span>)</code> </pre> <br><p>  Voc√™ pode alterar os par√¢metros para gerar texto (coeficiente de aleatoriedade etc., veja a descri√ß√£o no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho original</a> ): </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k <span class="hljs-number"><span class="hljs-number">40</span></span> --temperature <span class="hljs-number"><span class="hljs-number">0.7</span></span> | tee samples.txt</code> </pre> <br><p>  Como o 117M √© um modelo <strong>bastante</strong> reduzido, n√£o espere milagres (atualiza√ß√£o: no momento da reda√ß√£o deste artigo, apenas ele estava dispon√≠vel. Agora tudo est√° dispon√≠vel, incluindo o maior 1558M original, veja acima).  A maioria das amostras geradas ser√° absurda.  Mas tamb√©m existem se√ß√µes significativas.  O texto deve estar em ingl√™s, enquanto em outros idiomas o GPT-2 ainda n√£o pode funcionar. </p><br><h2 id="primery-generiruemogo-teksta">  Exemplos de texto gerado </h2><br><p>  Amostras do texto gerado pelo modelo <strong>completo</strong> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://blog.openai.com/better-language-models/#sample1</a> (na parte superior da barra por 8 hist√≥rias). </p><br><p>  H√° tamb√©m um enorme arquivo de texto de 2,4 Mb com amostras geradas aleatoriamente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-samples.txt</a> </p><br><p>  E mais um, 2,27 Mb, com outras configura√ß√µes de aleatoriedade: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-topk40-samples.txt</a> </p><br><h1 id="ssylki">  Refer√™ncias </h1><br><ul><li>  Artigo original do blog da OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores modelos de linguagem e suas implica√ß√µes</a> </li><li>  Github com todas as vers√µes pr√©-treinadas do GPT-2: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/openai/gpt-2</a> </li><li>  Discuss√£o sobre as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">principais not√≠cias do</a> reddit </li><li>  Discuss√£o sobre o reddit que se recusa a publicar o modelo completo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Chegou a hora do OpenAI renomear CloseAI</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloco de anota√ß√µes do Google Colab para executar o GPT-2 (todos os modelos) em um navegador</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt440564/">https://habr.com/ru/post/pt440564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt440554/index.html">BEM conveniente</a></li>
<li><a href="../pt440556/index.html">Aprendendo o design de diagramas de relacionamento com entidades</a></li>
<li><a href="../pt440558/index.html">Tecnologia que aproxima redes qu√¢nticas</a></li>
<li><a href="../pt440560/index.html">Alexander Belokrylov e Dmitry Chuyko sobre o Liberica JDK em jug.msk.ru</a></li>
<li><a href="../pt440562/index.html">Windows Phone - TUDO, √© de novo ou de novo</a></li>
<li><a href="../pt440566/index.html">Acelerando sem obst√°culos ou conhecendo o SIMD</a></li>
<li><a href="../pt440568/index.html">Estamos escrevendo um aplicativo de aprendizado em Go e Javascript para avaliar o retorno real das a√ß√µes. Parte 2 - Testando o back-end</a></li>
<li><a href="../pt440570/index.html">Mapas de Sombra Reflexiva: Parte 2 - Implementa√ß√£o</a></li>
<li><a href="../pt440574/index.html">Russian AI Cup 2018, hist√≥ria 9 lugares</a></li>
<li><a href="../pt440576/index.html">Altera√ß√µes importantes no CTE no PostgreSQL 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>