<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙋 👍🏿 👩🏽 Rede neural GPT-2 da OpenAI. Início rápido 🏣 💱 👔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mal o barulho sobre a rede neural BERT do Google, que mostrou resultados avançados em várias tarefas de conversação (PNL) em aprendizado de máquina, c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Rede neural GPT-2 da OpenAI. Início rápido</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440564/"><p><img src="https://habrastorage.org/getpro/habr/post_images/1cf/d63/ae6/1cfd63ae6b68d59325ef90cc4ea93f35.png" alt="imagem"></p><br><p>  Mal o barulho sobre a rede neural BERT do Google, que mostrou resultados avançados em várias tarefas de conversação (PNL) em aprendizado de máquina, como a OpenAI lançou um novo desenvolvimento: GPT-2.  Essa rede neural com um número recorde de parâmetros no momento (1,5 bilhão, contra os 100-300 milhões comumente usados ​​nesses casos) conseguiu gerar páginas inteiras de texto conectado. </p><br><p>  É tão bom gerar que a OpenAI se recusou a publicar a versão completa, temendo que eles usassem essa rede neural para criar notícias falsas, comentários e críticas indistinguíveis das reais. </p><br><p>  No entanto, no OpenAI, uma versão reduzida da rede neural GPT-2 foi compartilhada com 117 milhões de parâmetros.  Vamos lançá-lo através do serviço Google Colab e experimentar com ele. </p><a name="habracut"></a><br><h1 id="nemnogo-predystorii">  Um pouco de fundo </h1><br><p>  Para aqueles que não acompanharam o desenvolvimento do progresso no processamento natural da fala (PNL). </p><br><p>  No verão de 2018, a OpenAI pré-treinou em um grande volume de texto uma rede neural da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GPT</a> construída na arquitetura Transformer.  Descobriu-se que, se você substituir algumas das últimas camadas e treiná-la novamente para uma tarefa específica (essa abordagem é chamada de Ajuste fino e é amplamente usada no aprendizado de máquina), ela quebra imediatamente os registros anteriores em uma ampla variedade de tarefas de conversação. </p><br><p>  Com base nesse desenvolvimento, no final de 2018, o Google criou sua própria rede neural <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BERT</a> .  Eles melhoraram seriamente o resultado, tornando a rede neural bidirecional, ao contrário da GPT. </p><br><p>  Não querendo desistir, em fevereiro de 2019, o OpenAI imediatamente aumentou sua GPT em 10 vezes e a treinou em uma quantidade ainda maior de texto - em 8 milhões de páginas da Web (um total de 40 GB de texto).  A rede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GPT-2</a> resultante é atualmente a maior rede neural, com um número sem precedentes de parâmetros de 1,5 bilhão (o BERT tinha 340 milhões no maior modelo e 110 milhões no BERT padrão). </p><br><p>  Como resultado, o GPT-2 conseguiu gerar páginas inteiras de texto coerente.  Com referências repetidas aos nomes dos personagens no decorrer da narrativa, citações, referências a eventos relacionados e assim por diante.  Não darei exemplos aqui, mas refiro aqueles que desejam o artigo original no blog da OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores modelos de linguagem e suas implicações</a> ou os links no final do artigo. </p><br><p>  A geração de um texto coerente dessa qualidade é impressionante por si só, mas a coisa mais interessante é diferente.  O GPT-2 sem nenhum treinamento adicional mostrou imediatamente resultados próximos do estado da arte em várias tarefas de conversação.  Repito, quem perdeu a importância do momento - sem nenhum treinamento adicional para uma tarefa específica! </p><br><p>  Como eles conseguiram isso?  Apenas perguntando às redes neurais as perguntas certas. </p><br><h1 id="arhitektura-gpt-2">  Arquitetura GPT-2 </h1><br><p>  O GPT-2 é treinado para prever a próxima palavra em uma frase.  Essa é uma abordagem clássica para gerar texto.  A princípio, as redes de recorrência (RNN), em particular a LSTM, mantinham o primado nessa área.  Mas após a invenção da arquitetura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Transformer</a> no verão de 2017, ela gradualmente começou a prevalecer em tarefas de conversação.  Embora o Transformer original tenha um problema ao armazenar sequências longas (os LSTMs lembram as mais longas), a velocidade do treinamento e a profundidade da rede mais do que compensaram isso.  A propósito, várias modificações do transformador já apareceram - com a introdução de recorrência ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Universal Transformers</a> ), uma modificação para sequências mais longas ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Transformer-XL</a> ) e outras, mas até agora apenas um Transformer original ligeiramente ajustado é usado no Google e OpenAI. </p><br><p>  Lembro-me que o BERT do Google aprendeu um pouco diferente: prever não a próxima palavra em uma frase, mas as palavras perdidas (mascaradas) em uma frase.  E também para determinar se duas sentenças consecutivas são uma continuação lógica uma da outra ou se não estão de forma alguma conectadas por significado.  Isso permitiu que o BERT fosse um modelo de linguagem que entendesse o significado das palavras, dependendo do ambiente (contexto).  O que determinou o sucesso do BERT nas tarefas de NPL.  Mas somente após a reciclagem (Ajuste fino) para uma tarefa específica.  Apenas prever palavras no modelo base não funciona muito bem nele.  Você pode jogar com o BERT no seu navegador (via Google Colab): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://habr.com/en/post/436878</a> . </p><br><p>  GPT-2 não é necessário treinar novamente.  Este não é apenas um modelo de linguagem como o BERT, é um gerador de texto.  Apenas dê a ela o início da frase e ela complementará sua palavra por palavra. </p><br><p>  Um detalhe interessante: a pesquisa da OpenAI mostrou que matrizes de textos da Wikipedia e livros literários (que o BERT estudou em particular) têm um estilo tendencioso.  Portanto, treinados apenas nessas redes neurais não geram texto muito bem.  Para diversificar os dados e estilos de entrada, a OpenAI usou o GPT-2 para treinamento em páginas regulares da Web coletadas em 8 milhões de sites (um total de 40 GB de texto).  E, para descartar sites de publicidade e spammer, eles incluíram nos sites de amostra cujos links no reddit têm uma boa classificação.  Ou seja, sites que exibem usuários que contêm informações úteis. </p><br><h1 id="pravilnyy-vopros-soderzhit-polovinu-otveta">  A pergunta correta contém metade da resposta. </h1><br><p>  Assim, o GPT-2, graças ao seu tamanho sem precedentes, conseguiu gerar páginas de texto coerente.  Mas o mais surpreendente é que, ao fazer a pergunta certa (ou seja, o início correto de uma frase), ela conseguiu responder a várias perguntas!  Só porque a continuação desse começo é a mais natural. </p><br><p>  Por exemplo, para obter uma resposta para a pergunta "O que é a Terra?", Você pode aplicar à entrada dessa rede neural o início da frase: "A Terra é ...".  E ela completará esta frase até o fim.  Porque a resposta será uma continuação natural deste começo. </p><br><p>  Além disso, ao formar o início da frase da maneira correta, você pode obter explicações para diferentes públicos-alvo, levando em consideração sua inteligência, idade e escolaridade.  Imagine frases contínuas: "Eu, como cientista, acredito que a Terra é ...".  Ou: "Eu, como lavrador de terras, afirmo que a Terra é ...".  Ou: "Eu, como professora de um jardim de infância, agora vou explicar para vocês, crianças, que a Terra é ...". </p><br><p>  Como você pode ver, formando as perguntas certas (o começo correto da frase), você pode obter respostas de níveis completamente diferentes e detalhes diferentes.  De certa forma, algo semelhante acontece nas pessoas.  O médico deve explicar ao paciente o curso da doença para que ele entenda.  No nível do paciente.  Se você perguntar a uma criança de cinco anos por que ele fez isso, ela não poderá responder imediatamente (que, naturalmente, as crianças convivem com sentimentos e emoções).  Mas, para dar a resposta que se espera dele, a criança começa a inventá-la - a gerar texto.  Com base no fato de que a resposta combina com os pais e que pelo menos de alguma forma corresponde ao que aconteceu.  A princípio, como muitos pais sabem, essas serão respostas ridículas.  Mas, incentivando e punindo ("conte-me mais", "não invente desculpas"), a criança aprenderá a dar respostas detalhadas e completas. </p><br><p>  Esse desenvolvimento do OpenAI e a capacidade da rede GPT-2 de fornecer respostas para tarefas de conversação sem treinamento adicional especial para uma tarefa específica abrem duas questões interessantes: </p><br><p>  1) A interpretabilidade das redes neurais pode ser alcançada por um gerador de texto tão elementar e pelo início correto de uma frase.  Onde a resposta será uma extensão natural.  Suponha, por exemplo, que uma rede neural não indique selos em uma fotografia pelos números das coordenadas x-y, mas explique sua posição em texto simples.  Então, no decorrer do esclarecimento, fazendo a pergunta certa, por exemplo: "Cheguei a essa conclusão porque ...", em teoria, você pode obter uma explicação de como ela encontrou o gato na foto.  E essa explicação no caso extremo não pode ser pior que a humana.  O que resolve o problema global de interpretabilidade das redes neurais. </p><br><p>  2) Uma rede neural pré-treinada em grandes volumes de texto pode ser universal, ter bom senso e não exigir treinamento adicional para tarefas específicas.  Isso significa que, ao tentar imitar a fala humana (respostas humanas a perguntas), a rede neural deve inevitavelmente aprender o bom senso, a fim de fornecer respostas muito semelhantes às humanas.  Dar respostas fictícias monossilábicas, em geral, não é típico para as pessoas.  Na maioria das vezes, as pessoas fornecem respostas adequadas detalhadas, o que significa que a rede deve aprender a fazer o mesmo. </p><br><p>  Ambas as questões permanecem em aberto, mas o primeiro passo em sua aprovação foi definitivamente dado. </p><br><h1 id="a-tochnee">  Ou melhor? </h1><br><p>  Se você está de pé agora, é melhor sentar.  Porque é assim que o OpenAI, usando a rede neural GPT-2, obteve seus resultados em tarefas de conversação para diferentes domínios: </p><br><p>  <strong>Respostas a perguntas no texto</strong> </p><br><p>  Bem, isso é fácil.  Ou alimentou a rede com alguns parágrafos com uma descrição incluída em algum lugar no meio, por exemplo, “a maçã está sobre a mesa” e, no final, foi atribuída: “a maçã está ligada ...” e a rede foi adicionada à “mesa”.  Porque é capaz de lembrar o contexto de vários parágrafos. </p><br><p>  Ou alimentou a rede como uma frase inicial de alguns exemplos do tipo "Pergunta: alguma pergunta, resposta: alguma resposta" e, no final, após a pergunta real, eles adicionaram: "Resposta:"  E a rede neural anexou a resposta!  Uma vez que revelou a estrutura do documento na pergunta-resposta anterior.  Isso é incrível. </p><br><p>  <strong>Versão curta (resumo) do texto</strong> </p><br><p>  A entrada é um texto longo de vários parágrafos ou até páginas, e a rede neural deve escrever um conteúdo curto.  Como você conseguiu esse comportamento do GPT-2?  Logo após o texto, eles adicionaram "TL; DR".  E isso é tudo!  Isso acabou sendo suficiente para o GPT-2 adicionar um resumo do artigo após esses caracteres!  Porque esses símbolos na Internet frequentemente denotam o resumo da publicação. </p><br><p>  <strong>Tradução de texto</strong> </p><br><p>  A entrada GPT-2 recebeu o texto no formato: "olá = olá, cachorro = cachorro, vento = vento, gato = ...".  E a rede neural acrescentou a tradução da última palavra: "gato" (no original em francês).  Porque revelou a estrutura do documento e simplesmente a complementou com a continuação mais lógica.  Se sua mandíbula ainda não caiu de tudo isso, tenho duas novidades para você e ambas são ruins =). </p><br><h1 id="zapusk-gpt-2-cherez-google-colab">  Lançamento do GPT-2 através do Google Colab </h1><br><p>  Infelizmente, a versão completa do GPT-2 no OpenAI foi recusada para ser compartilhada.  Motivando isso pelo fato de que, usando essa rede neural, será muito fácil gerar notícias e críticas falsas nas lojas.  A julgar pela declaração, a discussão sobre a adequação do layout desse modelo continuará pelos próximos 6 meses. Após o OpenAI, eles decidirão se devem enviá-lo ou não.  No entanto, para uma grande organização, não é difícil repetir o modelo (parece que eles o treinaram para 256 TPU por vários dias e, de acordo com estimativas preliminares, isso lhes custou cerca de US $ 45 mil) </p><br><p>  No entanto, eles publicaram uma versão reduzida do GPT-2 com 117 milhões de parâmetros (em vez de 1,5 bilhão, como no modelo completo): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/openai/gpt-2</a> .  Vamos tentar executá-lo e brincar com este modelo. </p><br><p>  Atualização em 9 de novembro de 2019: finalmente, foi apresentada toda a linha de modelos, incluindo 1,5 bilhão de arquivos e instruções para o lançamento atualizado. </p><br><p>  A maneira mais fácil de fazer isso é através do Google Colab: </p><br><ol><li>  Abra o link </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/Gpt-2.ipynb</a> </p><br><ol><li>  No menu <strong>Tempo de Execução</strong> , selecione <strong>Executar Tudo</strong> , para que pela primeira vez todas as células sejam iniciadas, o modelo é baixado e as bibliotecas necessárias estão conectadas.  Concorde em redefinir todo o tempo de execução, se necessário.  Digite o texto após o aparecimento de "Prompt do modelo &gt;&gt;&gt;" e pressione Enter. </li></ol><br><p>  Preste atenção à linha logo no início: </p><br><p>  model_name = '117M' </p><br><p>  Aqui você pode especificar o tamanho do modelo GPT-2 a ser usado.  Os seguintes modelos estão disponíveis (sujeitos a atualização): </p><br><p>  117M <br>  124M <br>  355M <br>  774M <br>  1558M </p><br><p>  Aqui, 117M é o menor modelo que era o único disponível no momento em que este artigo foi escrito.  Mais tarde, a OpenAI estabeleceu modelos cada vez maiores, até 5 de novembro de 2019, estabeleceu o máximo de 1558M (com 1,5 bilhões de parâmetros). </p><br><div class="spoiler">  <b class="spoiler_title">Se algo desse errado ...</b> <div class="spoiler_text"><p>  Verifique se GPU e Python 3 estão selecionados no menu Tempo de execução -&gt; Alterar tipo de tempo de execução </p><br><p>  Se o botão de conexão não estiver ativo, clique nele para se conectar. </p></div></div><br><p>  Ou crie todo o código manualmente: </p><br><ol><li>  Acesse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://colab.research.google.com</a> </li><li>  Pressione o botão azul NEW PYTHON 3 NOTEBOOK </li><li>  No menu Tempo de execução -&gt; Alterar tipo de tempo de execução, selecione Python 3 e a GPU (a última para executar a rede neural na GPU) </li><li>  Na primeira célula, digite: </li></ol><br><pre><code class="python hljs">model_name = <span class="hljs-string"><span class="hljs-string">'117M'</span></span> !git clone https://github.com/openai/gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> %cd gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> !pip3 install -r requirements.txt !python3 download_model.py $model_name</code> </pre> <br><p>  Em vez de 117M (o menor), você pode especificar qualquer modelo intermediário ou maior: 1558M. </p><br><p>  E clique no ícone preto Play à esquerda da célula.  Isso fará o download da rede neural GPT-2 selecionada e instalará as dependências necessárias. </p><br><p>  Na segunda célula (você pode adicioná-lo através do menu Inserir -&gt; Código da célula ou passando o mouse sob o centro da célula atual, os botões de adição serão exibidos): </p><br><pre> <code class="python hljs">!python3 src/interactive_conditional_samples.py --model_name=$model_name</code> </pre> <br><p>  Isso iniciará o modo interativo.  Aguarde até que a rede neural seja inicializada e uma janela para inserir texto apareça com a inscrição “Model Model &gt;&gt;&gt;”. Digite o início da frase e pressione Enter. Após algum tempo, o texto gerado aparecerá sob o cabeçalho SAMPLE. </p><br><p>  Você também pode iniciar o modo de gerar texto completamente aleatório.  O texto será gerado infinitamente em pequenos pedaços de AMOSTRA 1, AMOSTRA 2 e assim por diante, até você clicar no botão Parar na célula.  Para fazer isso, crie uma nova célula com o código: </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt</code> </pre> <br><p>  O resultado será salvo no arquivo samples.txt.  Pode ser baixado com os seguintes comandos (crie uma nova célula novamente e execute-a após gerar o texto): </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">'samples.txt'</span></span>)</code> </pre> <br><p>  Você pode alterar os parâmetros para gerar texto (coeficiente de aleatoriedade etc., veja a descrição no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho original</a> ): </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k <span class="hljs-number"><span class="hljs-number">40</span></span> --temperature <span class="hljs-number"><span class="hljs-number">0.7</span></span> | tee samples.txt</code> </pre> <br><p>  Como o 117M é um modelo <strong>bastante</strong> reduzido, não espere milagres (atualização: no momento da redação deste artigo, apenas ele estava disponível. Agora tudo está disponível, incluindo o maior 1558M original, veja acima).  A maioria das amostras geradas será absurda.  Mas também existem seções significativas.  O texto deve estar em inglês, enquanto em outros idiomas o GPT-2 ainda não pode funcionar. </p><br><h2 id="primery-generiruemogo-teksta">  Exemplos de texto gerado </h2><br><p>  Amostras do texto gerado pelo modelo <strong>completo</strong> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://blog.openai.com/better-language-models/#sample1</a> (na parte superior da barra por 8 histórias). </p><br><p>  Há também um enorme arquivo de texto de 2,4 Mb com amostras geradas aleatoriamente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-samples.txt</a> </p><br><p>  E mais um, 2,27 Mb, com outras configurações de aleatoriedade: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-topk40-samples.txt</a> </p><br><h1 id="ssylki">  Referências </h1><br><ul><li>  Artigo original do blog da OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores modelos de linguagem e suas implicações</a> </li><li>  Github com todas as versões pré-treinadas do GPT-2: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/openai/gpt-2</a> </li><li>  Discussão sobre as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">principais notícias do</a> reddit </li><li>  Discussão sobre o reddit que se recusa a publicar o modelo completo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Chegou a hora do OpenAI renomear CloseAI</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloco de anotações do Google Colab para executar o GPT-2 (todos os modelos) em um navegador</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt440564/">https://habr.com/ru/post/pt440564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt440554/index.html">BEM conveniente</a></li>
<li><a href="../pt440556/index.html">Aprendendo o design de diagramas de relacionamento com entidades</a></li>
<li><a href="../pt440558/index.html">Tecnologia que aproxima redes quânticas</a></li>
<li><a href="../pt440560/index.html">Alexander Belokrylov e Dmitry Chuyko sobre o Liberica JDK em jug.msk.ru</a></li>
<li><a href="../pt440562/index.html">Windows Phone - TUDO, é de novo ou de novo</a></li>
<li><a href="../pt440566/index.html">Acelerando sem obstáculos ou conhecendo o SIMD</a></li>
<li><a href="../pt440568/index.html">Estamos escrevendo um aplicativo de aprendizado em Go e Javascript para avaliar o retorno real das ações. Parte 2 - Testando o back-end</a></li>
<li><a href="../pt440570/index.html">Mapas de Sombra Reflexiva: Parte 2 - Implementação</a></li>
<li><a href="../pt440574/index.html">Russian AI Cup 2018, história 9 lugares</a></li>
<li><a href="../pt440576/index.html">Alterações importantes no CTE no PostgreSQL 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>