<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎅 🤾🏼 🧑🏻‍🤝‍🧑🏻 Deep Learning: Erkennen von Szenen und Orientierungspunkten in Bildern 👨‍🔧 🤴🏻 😔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zeit, das Sparschwein guter russischsprachiger Berichte über maschinelles Lernen aufzufüllen! Das Sparschwein selbst wird nicht aufgefüllt! 

 Dieses ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deep Learning: Erkennen von Szenen und Orientierungspunkten in Bildern</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jugru/blog/419501/">  Zeit, das Sparschwein guter russischsprachiger Berichte über maschinelles Lernen aufzufüllen!  Das Sparschwein selbst wird nicht aufgefüllt! <br><br>  Dieses Mal werden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Andrei Boyarovs</a> faszinierende Geschichte über die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Szenenerkennung</a> kennenlernen.  Andrey ist ein Computer Vision-Forscher, der sich bei der Mail.Ru Group mit Machine Vision beschäftigt. <br><br>  Die Szenenerkennung ist einer der weit verbreiteten Bereiche der Bildverarbeitung.  Diese Aufgabe ist komplizierter als das untersuchte Erkennen von Objekten: Die Szene ist ein komplexeres und weniger formalisiertes Konzept, es ist schwieriger, Merkmale zu unterscheiden.  Die Aufgabe, Sehenswürdigkeiten zu erkennen, ergibt sich aus der Szenenerkennung: Sie müssen bekannte Stellen auf dem Foto hervorheben, um eine geringe Anzahl von Fehlalarmen sicherzustellen. <br><br>  Dies sind <b>30 Minuten</b> Video von der Smart Data 2017-Konferenz. Das Video kann bequem zu Hause und unterwegs angesehen werden.  Für diejenigen, die nicht bereit sind, so viel auf dem Bildschirm zu sitzen, oder die Informationen lieber in Textform wahrnehmen möchten, wenden wir eine Volltextentschlüsselung an, die in Form von Habrosta gestaltet ist. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/dL1-OrjtMvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Ich mache Bildverarbeitung bei Mail.ru.  Heute werde ich darüber sprechen, wie wir durch tiefes Lernen Bilder von Szenen und Attraktionen erkennen. <br><br>  Das Unternehmen stellte die Notwendigkeit fest, Benutzerbilder zu markieren und zu suchen. Aus diesem Grund haben wir beschlossen, eine eigene Computer Vision-API zu erstellen, zu der auch ein Tool zum Markieren von Szenen gehören wird.  Als Ergebnis dieses Tools möchten wir etwas wie das im Bild unten gezeigte erhalten: Der Benutzer stellt eine Anfrage, zum Beispiel "Kathedrale", und erhält alle seine Fotos mit Kathedralen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d18/5f8/a3b/d185f8a3bd0c73280cdae408fe84cca1.png"><br><br>  In der Computer Vision-Community wurde das Thema Objekterkennung in Bildern recht gut untersucht.  Es gibt einen bekannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet-Wettbewerb</a> , der seit mehreren Jahren stattfindet und dessen Hauptteil die Objekterkennung ist. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/989/f18/b8a989f189970cabb0de00be7ff48afa.png"><br><br>  Grundsätzlich müssen wir ein Objekt lokalisieren und klassifizieren.  Bei Szenen ist die Aufgabe etwas komplizierter, da die Szene ein komplexeres Objekt ist, aus einer großen Anzahl anderer Objekte besteht und der Kontext sie vereint, sodass die Aufgaben unterschiedlich sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/be7/a6b/ba4be7a6bc9a4e0c2c7929644540799c.png"><br><br>  Im Internet stehen Dienste anderer Unternehmen zur Verfügung, die solche Funktionen implementieren.  Dies ist insbesondere die Google Vision-API oder die Microsoft Computer Vision-API, mit der Szenen in Bildern gefunden werden können. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/758/fd9/095/758fd909536fd930db97c025ca61ef38.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/eee/b28/6df/eeeb286df22a31809f507da7730b7d89.png"><br><br>  Wir haben dieses Problem mit Hilfe des maschinellen Lernens gelöst, daher benötigen wir Daten.  Es gibt jetzt zwei Hauptgrundlagen für die Szenenerkennung im Open Access.  Der erste von ihnen erschien 2013 - dies ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die SUN-Basis</a> der Princeton University.  Diese Basis besteht aus Hunderttausenden von Bildern und 397 Klassen. <br><br>  Die zweite Basis, auf der wir trainiert haben, ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Places2-Basis</a> vom MIT.  Sie erschien 2013 in zwei Versionen.  Das erste ist Places2-Standart, eine ausgewogenere Basis mit 1,8 Millionen Bildern und 365 Klassen.  Die zweite Option - Places2-Challenge - enthält acht Millionen Bilder und 365 Klassen, aber die Anzahl der Bilder zwischen den Klassen ist nicht ausgeglichen.  Beim ImageNet-Wettbewerb 2016 enthielt der Bereich Szenenerkennung die Places2-Challenge, und der Gewinner zeigte das beste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Top-5-Klassifizierungsfehlerergebnis</a> von etwa 9%. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a10/ea2/881/a10ea2881a5248449dce8202ceaa28ff.png"><br><br>  Wir haben auf Basis von Places2 trainiert.  Hier ist ein Beispielbild von dort: Es ist eine Schlucht, eine Landebahn, eine Küche, ein Fußballplatz.  Dies sind völlig unterschiedliche komplexe Objekte, an denen wir lernen müssen, sie zu erkennen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1cf/9b2/3ee/1cf9b23eedff87ae9bcbb2d8c4090551.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/602/617/1d1/6026171d19b38452ce0e4a071ba4e836.png"><br><br>  Vor dem Studium haben wir die Grundlagen an unsere Bedürfnisse angepasst.  Es gibt einen Trick für die Objekterkennung, wenn Sie mit Modellen auf kleinen CIFAR-10- und CIFAR-100-Basen anstelle von ImageNet experimentieren, und nur dann trainieren die besten auf ImageNet. <br><br>  Wir beschlossen, den gleichen Weg zu gehen, nahmen die SUN-Datenbank, reduzierten sie, erhielten 89 Klassen, 50.000 Bilder im Zug und 10.000 Bilder bei der Validierung.  Aus diesem Grund haben wir vor dem Training auf Places2 Experimente durchgeführt und unsere Modelle basierend auf SUN getestet.  Das Training dauert nur 6-10 Stunden, im Gegensatz zu mehreren Tagen auf Places2, wodurch viel mehr Experimente durchgeführt und die Effektivität gesteigert werden konnten. <br><br>  Wir haben uns auch die Places2-Datenbank selbst angesehen und festgestellt, dass wir einige Klassen nicht benötigen.  Entweder aus Produktionsgründen oder weil zu wenig Daten darüber vorliegen, schneiden wir Klassen aus, wie zum Beispiel ein Aquädukt, ein Baumhaus oder ein Scheunentor. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f8/31f/60f/4f831f60fd70ec7f74ea2bb0a29c69f5.png"><br><br>  Als Ergebnis haben wir nach all den Manipulationen die Places2-Datenbank erhalten, die 314 Klassen und eine halbe Million Bilder (in der Standardversion) enthält, in der Challenge-Version etwa 7,5 Millionen Bilder.  Auf diesen Grundlagen haben wir Schulungen aufgebaut. <br><br>  Außerdem haben wir beim Betrachten der verbleibenden Klassen festgestellt, dass es zu viele für die Produktion gibt, sie sind zu detailliert.  Aus diesem Grund haben wir den Szenenzuordnungsmechanismus angewendet, wenn einige Klassen zu einer gemeinsamen Klasse zusammengefasst wurden.  Zum Beispiel haben wir alles, was mit Wäldern zu tun hat, zu einem Wald verbunden, alles, was mit Krankenhäusern zu tun hat - zu einem Krankenhaus, zu Hotels - zu einem Hotel. <br><br>  Wir verwenden die Szenenzuordnung nur zum Testen und für den Endbenutzer, da dies bequemer ist.  Im Training verwenden wir alle Standardklassen 314.  Wir haben die resultierende Basis Places Sift genannt. <br><br><h2>  Ansätze, Lösungen </h2><br>  Betrachten Sie nun die Ansätze, mit denen wir dieses Problem gelöst haben.  Tatsächlich sind solche Aufgaben mit dem klassischen Ansatz verbunden - tiefen Faltungs-Neuronalen Netzen. <br><br>  Das Bild unten zeigt eines der ersten klassischen Netzwerke, enthält jedoch bereits die Hauptbausteine, die in modernen Netzwerken verwendet werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a4/61f/7f7/0a461f7f7ae8a9264f5fadb69271f29d.png"><br><br>  Dies sind Faltungsschichten, dies sind Zugschichten, vollständig verbundene Schichten.  Um die Architektur zu bestimmen, haben wir die Spitzen der ImageNet- und Places2-Wettbewerbe überprüft. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7dc/658/7d0/7dc6587d046a968988d4f52fa7bcbb3f.png"><br><br>  Wir können sagen, dass die wichtigsten führenden Architekturen in zwei Familien unterteilt werden können: Inception und die ResNet-Familie (Residual Network).  Im Verlauf der Experimente haben wir herausgefunden, dass die ResNet-Familie besser für unsere Aufgabe geeignet ist, und wir haben das nächste Experiment mit dieser Familie durchgeführt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a8/828/85b/1a882885b70533dd0341bd853d2818fd.png"><br><br>  ResNet ist ein tiefes Netzwerk, das aus einer großen Anzahl von Restblöcken besteht.  Dies ist der Hauptbaustein, der aus mehreren Schichten mit Gewichten und Verknüpfungsverbindung besteht.  Infolge dieses Entwurfs lernt diese Einheit, wie stark sich das Eingangssignal x vom Ausgang f (x) unterscheidet.  Infolgedessen können wir Netzwerke solcher Blöcke aufbauen, und während des Trainings kann das Netzwerk in den letzten Schichten Gewichte nahe Null erzeugen. <br><br>  Wir können also sagen, dass das Netzwerk selbst entscheidet, wie tief es sein muss, um einige der Aufgaben zu lösen.  Dank dieser Architektur können Netzwerke mit sehr großer Tiefe mit einer sehr großen Anzahl von Schichten aufgebaut werden.  Der Gewinner von ImageNet 2014 enthielt nur 22 Ebenen, ResNet übertraf dieses Ergebnis und enthielt bereits 152 Ebenen. <br><br>  Die Kernforschung von ResNet besteht darin, einen Restblock zu verbessern und ordnungsgemäß zu erstellen.  Das Bild unten zeigt eine empirisch und mathematisch fundierte Version, die das beste Ergebnis liefert.  Eine solche Konstruktion des Blocks ermöglicht es Ihnen, sich mit einem der grundlegenden Probleme des tiefen Lernens zu befassen - einem verblassenden Gradienten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/7a4/398/0207a439827163bc14895a47a1489ddc.png"><br><br>  Um unsere Netzwerke zu trainieren, haben wir das in Lua geschriebene Torch-Framework aufgrund seiner Flexibilität und Geschwindigkeit verwendet und für ResNet die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Implementierung von ResNet von Facebook vorangetrieben</a> .  Um die Qualität des Netzwerks zu überprüfen, haben wir drei Tests verwendet. <br><br>  Der erste Places-Val-Test ist die Validierung vieler Places-Sift-Sets.  Der zweite Test ist Places Sift mit Scene Mapping und der dritte ist der Cloud-Test, der der Kampfsituation am nächsten kommt.  Bilder von Mitarbeitern aus der Cloud, die manuell beschriftet wurden.  Im Bild unten gibt es zwei Beispiele für solche Bilder. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/796/6d0/066/7966d0066c9257617e81f290fbbc6283.png"><br><br>  Wir haben begonnen, Netzwerke zu messen, zu trainieren und miteinander zu vergleichen.  Der erste ist der ResNet-152-Benchmark, der mit Places2 geliefert wird, der zweite ist ResNet-50, den wir auf ImageNet trainiert und auf unserer Basis trainiert haben. Das Ergebnis war bereits besser.  Dann nahmen sie ResNet-200, das ebenfalls auf ImageNet trainiert wurde, und es zeigte am Ende das beste Ergebnis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d9/0de/320/6d90de3208968f8120c1f4be1e79f74e.png"><br><br>  Nachfolgend finden Sie Beispiele für Arbeiten.  Dies ist ein ResNet-152-Benchmark.  Vorausgesagt werden die Originaletiketten, die das Netzwerk ausgibt.  Zugeordnete Etiketten sind die Beschriftungen, die nach der Szenenzuordnung erstellt wurden.  Es ist ersichtlich, dass das Ergebnis nicht sehr gut ist.  Das heißt, sie scheint etwas zu dem Fall zu geben, aber nicht sehr gut. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/892/ceb/86e892ceb75b85dcf22bec203c0f4f3d.png"><br><br>  Das nächste Beispiel ist der Betrieb von ResNet-200.  Schon sehr ausreichend. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa6/621/d0d/fa6621d0dd29ac9d77da10378eec9454.png"><br><br><h2>  ResNet-Verbesserung </h2><br>  Wir haben uns entschlossen, unser Netzwerk zu verbessern, und zuerst haben wir nur versucht, die Tiefe des Netzwerks zu erhöhen, aber danach wurde es viel schwieriger zu trainieren.  Dies ist ein bekanntes Problem. Letztes Jahr wurden mehrere Artikel zu diesem Thema veröffentlicht, die besagen, dass ResNet tatsächlich ein Ensemble einer großen Anzahl gewöhnlicher Netzwerke unterschiedlicher Tiefe ist. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df2/0ad/023/df20ad0234f5eba5597939468f8afaf6.png"><br><br>  Res-Blöcke, die sich am Ende des Gitters befinden, tragen einen kleinen Beitrag zur Bildung des Endergebnisses bei.  Es erscheint vielversprechender, nicht die Netzwerktiefe, sondern die Breite, dh die Anzahl der Filter im Res-Block, zu erhöhen. <br><br>  Diese Idee wird vom Wide Residual Network umgesetzt, das 2016 erschien.  Am Ende haben wir WRN-50-2 verwendet, das übliche ResNet-50 mit der doppelten Anzahl von Filtern in der 3x3-Faltung des inneren Engpasses. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5be/f29/aed/5bef29aed5cfce3b0121ce69ee072c35.png"><br><br>  Das Netzwerk zeigt auf ImageNet ähnliche Ergebnisse mit dem ResNet-200, das wir bereits verwendet haben, aber vor allem ist es fast doppelt so schnell.  Hier sind zwei Implementierungen des Restblocks auf der Taschenlampe: Der Parameter, der verdoppelt wird, wird hell hervorgehoben.  Dies ist die Anzahl der Filter in der inneren Faltung. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1a/3fe/d24/f1a3fed24bf735ed30cfc407aac6ca79.png"><br><br>  Dies sind Messungen bei den ResNet-200 ImageNet-Tests.  Zuerst nahmen wir WRN-22-6, es zeigte ein schlechteres Ergebnis.  Dann nahmen sie WRN-50-2-ImageNet, trainierten ihn, nahmen WRN-50-2, trainierten auf ImageNet und trainierten ihn auf Places2-Challenge, und er zeigte das beste Ergebnis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/736/fd6/afe/736fd6afe39702160b87ad095ef0edcd.png"><br><br>  Hier ist ein Beispiel für den WRN-50-2 - ein völlig angemessenes Ergebnis in unseren Bildern, die Sie bereits gesehen haben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b0a/442/e1f/b0a442e1f87a4f2e2a6c43a96677ce3d.png"><br><br>  Und dies ist ein Beispiel für die erfolgreiche Arbeit mit Kampffotografien. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/924/715/6bf/9247156bf459bd5b005b4180d36a1da5.png"><br><br>  Es gibt natürlich nicht sehr erfolgreiche Arbeiten.  Die Brücke von Alexander III. In Paris wurde nicht als Brücke anerkannt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dd/c66/9ec/3ddc669ecf4aabe61b103dafbc4aabb2.png"><br><br><h2>  Modellverbesserung </h2><br>  Wir haben darüber nachgedacht, wie wir dieses Modell verbessern können.  Die ResNet-Familie verbessert sich weiter und es erscheinen neue Artikel.  Insbesondere wurde 2016 ein interessanter Artikel PyramidNet veröffentlicht, der vielversprechende Ergebnisse zu CIFAR-10/100 und ImageNet zeigte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdd/816/e1b/fdd816e1b970be4465ff8200b7f0cb56.png"><br><br>  Die Idee ist nicht, die Breite des Restblocks stark zu vergrößern, sondern dies schrittweise zu tun.  Wir haben verschiedene Optionen für dieses Netzwerk trainiert, aber leider zeigte es etwas schlechtere Ergebnisse als unser Kampfmodell. <br><br>  Im Frühjahr 2018 wurde das ResNext-Modell veröffentlicht, ebenfalls eine vielversprechende Idee: den Residual-Block in mehrere parallele Blöcke kleinerer Größe und kleinerer Breite zu unterteilen.  Dies ähnelt der Idee von Inception, wir haben auch damit experimentiert.  Leider zeigte sie schlechtere Ergebnisse als unser Modell. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/712/3c4/e0f/7123c4e0f7f826f9dab8a1791d4eb3cd.png"><br><br>  Wir haben auch mit verschiedenen „kreativen“ Ansätzen experimentiert, um unsere Modelle zu verbessern.  Insbesondere haben wir versucht, CAM (Class Activation Mapping) zu verwenden. Dies sind die Objekte, die das Netzwerk bei der Klassifizierung des Bildes betrachtet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1bf/639/ea4/1bf639ea430bd03bafeeacf8acb29a09.png"><br><br>  Unsere Idee war, dass Instanzen derselben Szene dieselben oder ähnliche Objekte wie eine CAM-Klasse haben sollten.  Wir haben versucht, diesen Ansatz zu verwenden.  Zuerst nahmen sie zwei Netzwerke.  Einer ist ImageNet-geschult, der zweite ist unser Modell, das wir verbessern wollen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/47d/ac5/538/47dac55383f098852c9a69a3d50f2cc1.png"><br><br>  Wir nehmen das Bild auf, führen es durch Netzwerk 2, fügen das CAM für die Schicht hinzu und führen es dann dem Eingang von Netzwerk 1 zu. Führen Sie es durch Netzwerk 1, fügen Sie die Ergebnisse zur Verlustfunktion von Netzwerk 2 hinzu und setzen Sie dies mit den neuen Verlustfunktionen fort. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/860/c3e/9c3/860c3e9c37f6d6ffd91d3cdd36d67682.png"><br><br>  Die zweite Möglichkeit besteht darin, dass wir das Image über Netzwerk 2 ausführen, das CAM nehmen, es dem Eingang von Netzwerk 1 zuführen und dann anhand dieser Daten einfach Netzwerk 1 trainieren und das Ensemble aus den Ergebnissen von Netzwerk 1 und Netzwerk 2 verwenden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fb/140/95b/5fb14095b117b05582a79a43ee0eadb6.png"><br><br>  Wir haben unser Modell auf WRN-50-2 umgeschult, da wir als Netzwerk 1 ResNet-50 ImageNet verwendet haben, aber es war nicht möglich, die Qualität unseres Modells irgendwie signifikant zu verbessern. <br><br>  Wir forschen jedoch weiter daran, wie wir unsere Ergebnisse verbessern können: Wir trainieren neue CNN-Architekturen, insbesondere die ResNet-Familie.  Wir versuchen, mit CAM zu experimentieren und betrachten verschiedene Ansätze mit einer intelligenteren Verarbeitung von Bildfeldern - es scheint uns, dass dieser Ansatz ziemlich vielversprechend ist. <br><br><h2>  Landmark Recognition </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/11b/9da/4c3/11b9da4c376bcee091afb70d3dbb0745.png"><br><br>  Wir haben ein gutes Modell zum Erkennen von Szenen, aber jetzt wollen wir einige ikonische Orte herausfinden, dh Sehenswürdigkeiten.  Darüber hinaus machen Benutzer häufig Fotos von ihnen oder machen Bilder vor ihrem Hintergrund. <br><br>  Wir möchten, dass das Ergebnis nicht nur die Kathedralen sind, wie auf dem Bild auf der Folie, sondern das System, das sagt: "Es gibt Notre Dame de Paris und die Kathedralen in Prag." <br><br>  Als wir dieses Problem lösten, stießen wir auf einige Schwierigkeiten. <br><br><ol><li>  Es gibt praktisch keine Studien zu diesem Thema und es gibt keine vorgefertigten Daten im öffentlichen Bereich. <br></li><li>  Eine kleine Anzahl von "sauberen" Bildern im öffentlichen Bereich für jede Attraktion. <br></li><li>  Es ist nicht ganz klar, was ein Wahrzeichen von Gebäuden ist.  Zum Beispiel ein Haus mit Türmen auf Sq.  Leo Tolstoi in Petersburg, TripAdvisor berücksichtigt keine Attraktionen, aber Google berücksichtigt. <br></li></ol><br>  Wir haben zunächst eine Datenbank gesammelt, eine Liste mit 100 Städten zusammengestellt und dann mithilfe der Google Places-API JSON-Daten für Sonderziele aus diesen Städten heruntergeladen. <br><br>  Die Daten wurden gefiltert und analysiert, und gemäß der Liste haben wir für jede Attraktion 20 Bilder von der Google-Suche heruntergeladen.  Die Zahl 20 stammt aus empirischen Überlegungen.  Als Ergebnis haben wir eine Basis von 2827 Attraktionen und ungefähr 56.000 Bildern.  Auf dieser Basis haben wir unser Modell trainiert.  Zur Validierung unseres Modells haben wir zwei Tests verwendet. <br><br>  Cloud-Test - Dies sind Bilder unserer Mitarbeiter, die manuell beschriftet wurden.  Es enthält 200 Bilder in 15 Städten und 10 Tausend Bilder ohne Attraktionen.  Der zweite ist der Suchtest.  Es wurde mit der Mail.ru-Suche erstellt, die 3 bis 10 Bilder für jede Attraktion enthält, aber leider ist dieser Test schmutzig. <br><br>  Wir haben die ersten Modelle trainiert, aber sie zeigten schlechte Ergebnisse beim Cloud-Test auf Kampffotos. <br><br>  Hier ist ein Beispiel für das Bild, auf dem wir trainiert wurden, und ein Beispiel für Kampffotografie.  Das Problem bei Menschen ist, dass sie oft vor dem Hintergrund der Sehenswürdigkeiten fotografiert werden.  In diesen Bildern, die wir von der Suche erhalten haben, waren keine Menschen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b1d/8a6/63b/b1d8a663bf52f5910cf35415a93c296a.png"><br><br>  Um dem entgegenzuwirken, haben wir während des Trainings eine „menschliche“ Erweiterung hinzugefügt.  Das heißt, wir haben Standardansätze verwendet: zufällige Rotationen, zufälliges Ausschneiden eines Teils des Bildes und so weiter.  Aber auch im Lernprozess haben wir zufällig Bilder zu einigen Bildern hinzugefügt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12c/6c7/b54/12c6c7b54ae4d64065ee7fefa18acb72.png"><br><br>  Dieser Ansatz hat uns geholfen, das Problem mit Menschen zu lösen und ein akzeptables Qualitätsmodell zu erhalten. <br><br><h2>  Feinabstimmung von Szenenmodellen </h2><br>  Wie wir das Modell trainiert haben: Es gibt eine Trainingsbasis, aber sie ist ziemlich klein.  Wir wissen jedoch, dass eine Touristenattraktion ein Sonderfall der Szene ist.  Und wir haben ein ziemlich gutes Szenenmodell.  Wir beschlossen, sie für die Sehenswürdigkeiten zu trainieren.  Zu diesem Zweck haben wir mehrere vollständig verbundene und BN-Schichten über dem Netzwerk hinzugefügt, sie und die drei obersten Restblöcke trainiert.  Der Rest des Netzwerks wurde eingefroren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5e/b4a/e80/a5eb4ae80781b95eec46821dd05d5cba.png"><br><br>  Darüber hinaus verwenden wir für das Training die nicht standardmäßige Center-Loss-Funktion.  Während des Trainings versucht Center Loss, Vertreter verschiedener Klassen in verschiedene Cluster zu „zerlegen“, wie in der Abbildung gezeigt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cc/2e8/bbc/8cc2e8bbc85f79dfb6301742d7c3d4ac.png"><br><br>  Im Training haben wir eine weitere Klasse hinzugefügt, „keine Touristenattraktion“.  Und der Mittelverlust wurde auf diese Klasse nicht angewendet.  Für eine solche gemischte Verlustfunktion wurde ein Training durchgeführt. <br><br>  Nachdem wir das Netzwerk trainiert haben, schneiden wir die letzte Klassifizierungsschicht davon ab. Wenn das Bild das Netzwerk durchläuft, wird es zu einem numerischen Vektor, der als Einbettung bezeichnet wird. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf2/ebc/0fd/cf2ebc0fd7dc3af7770e6dd2c616fc74.png"><br><br>  Um ein Landmark-Erkennungssystem weiter aufzubauen, haben wir Referenzvektoren für jede Klasse erstellt.  Wir haben jede Klasse von Attraktionen aus der Menge genommen und die Bilder durch das Netzwerk geleitet.  Sie bekamen Einbettungen und nahmen ihren mittleren Vektor, der als Klassenreferenzvektor bezeichnet wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5e9/8ef/f32/5e98eff32e669f6c3c0b7245c25bc4ea.png"><br><br>  Um die Sehenswürdigkeiten auf dem Foto zu bestimmen, führen wir das Eingabebild durch das Netzwerk und seine Einbettung wird mit dem Referenzvektor jeder Klasse verglichen.  Wenn das Ergebnis des Vergleichs unter dem Schwellenwert liegt, glauben wir, dass das Bild keine Attraktionen aufweist.  Ansonsten nehmen wir die Klasse mit dem höchsten Vergleichswert. <br><br><h2>  Testergebnisse </h2><br><ul><li>  Beim Wolkentest betrug die Genauigkeit der Visiere 0,616, nicht die der Visiere - 0,981 </li><li>  Die durchschnittliche Genauigkeit von 0,669 wurde beim Suchtest erhalten, und die durchschnittliche Vollständigkeit betrug 0,576. </li></ul><br>  Bei der Suche haben sie keine sehr guten Ergebnisse erzielt, aber dies erklärt sich aus der Tatsache, dass der erste ziemlich "schmutzig" ist und der zweite Merkmale aufweist - unter den Attraktionen gibt es verschiedene botanische Gärten, die in allen Städten ähnlich sind. <br><br>  Es gab eine Idee für die Szenenerkennung, um zuerst das Netzwerk zu trainieren, das die Szenenmaske bestimmt, dh Objekte aus dem Vordergrund entfernt und sie dann in das Modell selbst einspeist, das Bildszenen ohne diese Bereiche erkennt, in denen der Hintergrund blockiert ist.  Es ist jedoch nicht ganz klar, was genau von der vorderen Schicht entfernt werden muss, welche Maske benötigt wird. <br><br>  Es wird eine ziemlich komplizierte und kluge Sache sein, weil nicht jeder versteht, welche Objekte zur Szene gehören und welche überflüssig sind.  Beispielsweise können Personen in einem Restaurant benötigt werden.  Dies ist eine nicht triviale Entscheidung, wir haben versucht, etwas Ähnliches zu tun, aber es hat keine guten Ergebnisse gebracht. <br><br>  Hier ist ein Beispiel für die Arbeit mit Kampffotografien. <br><br>  Beispiele für erfolgreiche Arbeit: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c53/ca4/413/c53ca44130bc3b9a845a7f21a681a8e0.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/774/c01/642774c0174cb07eeb52c055cb92b8d4.png"><br><br>  Aber der erfolglose Job: Es wurden keine Sehenswürdigkeiten gefunden.  Das Hauptproblem unseres Modells besteht derzeit nicht darin, dass das Netzwerk die Sehenswürdigkeiten verwirrt, sondern dass es sie auf dem Foto nicht findet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c9/aca/7ab/6c9aca7ab564401b4c9b5cea126fc68a.png"><br><br>  In Zukunft planen wir, eine Basis für eine noch größere Anzahl von Städten zu sammeln, neue Methoden zu finden, um das Netzwerk für diese Aufgabe zu trainieren und die Möglichkeiten zu ermitteln, die Anzahl der Klassen zu erhöhen, ohne das Netzwerk neu auszubilden. <br><br><h2>  Schlussfolgerungen </h2><br>  Heute haben wir: <br><br><ul><li>  Wir haben uns angesehen, welche Datensätze für die Szenenerkennung verfügbar sind. <br></li><li>  Wir haben gesehen, dass das Wide Residual Network das beste Modell ist. <br></li><li>  Diskussion weiterer Möglichkeiten zur Qualitätssteigerung dieses Modells; <br></li><li>  Wir haben uns die Aufgabe angesehen, Sehenswürdigkeiten zu erkennen, welche Schwierigkeiten auftreten; <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir haben den Algorithmus zum Sammeln der Basis- und Lehrmethoden des Modells zum Erkennen von Attraktionen beschrieben. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich kann sagen, dass die Aufgaben interessant sind, aber in der Gemeinde wenig studiert werden. </font><font style="vertical-align: inherit;">Es ist interessant, mit ihnen umzugehen, da Sie nicht standardmäßige Ansätze anwenden können, die bei der üblichen Erkennung von Objekten nicht angewendet werden.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minute der Werbung. </font><font style="vertical-align: inherit;">Wenn Ihnen dieser Bericht von der SmartData-Konferenz gefallen hat, beachten Sie bitte, dass </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmartData 2018</font></font></b></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> am 15. Oktober in St. Petersburg stattfindet, einer </font><font style="vertical-align: inherit;">Konferenz für diejenigen, die in die Welt des maschinellen Lernens, der Analyse und der Datenverarbeitung eintauchen. </font><font style="vertical-align: inherit;">Das Programm wird viele interessante Dinge enthalten, die Seite hat bereits ihre ersten Redner und Berichte.</font></font></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de419501/">https://habr.com/ru/post/de419501/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de419485/index.html">Dafür gibt es eine App: Ankündigung von Mobius 2018 Moskau</a></li>
<li><a href="../de419491/index.html">Wie STP funktioniert</a></li>
<li><a href="../de419493/index.html">Warum brauchst du Splunk? Sicherheitsereignisanalyse</a></li>
<li><a href="../de419495/index.html">Wer hat die Knochenleitung "erfunden", warum wird sie verwendet und wie sicher ist sie für das Hören?</a></li>
<li><a href="../de419497/index.html">Hercules Strong 3D Großdrucker Bewertung</a></li>
<li><a href="../de419505/index.html">Sie können jetzt Pakete ohne Benachrichtigungen und Pässe per Post im ganzen Land erhalten.</a></li>
<li><a href="../de419507/index.html">Übersicht über den russischen 3D-Drucker PICASO 3D Designer X von 3Dtool</a></li>
<li><a href="../de419509/index.html">Photonisches künstliches neuronales Netzwerk</a></li>
<li><a href="../de419511/index.html">Typ (T) vs. TypeOf⟨T⟩</a></li>
<li><a href="../de419513/index.html">Konfigurieren Sie die Kennwortsicherheitsrichtlinie in Zimbra</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>