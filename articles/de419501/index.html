<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÖ ü§æüèº üßëüèª‚Äçü§ù‚Äçüßëüèª Deep Learning: Erkennen von Szenen und Orientierungspunkten in Bildern üë®‚Äçüîß ü§¥üèª üòî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zeit, das Sparschwein guter russischsprachiger Berichte √ºber maschinelles Lernen aufzuf√ºllen! Das Sparschwein selbst wird nicht aufgef√ºllt! 

 Dieses ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deep Learning: Erkennen von Szenen und Orientierungspunkten in Bildern</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jugru/blog/419501/">  Zeit, das Sparschwein guter russischsprachiger Berichte √ºber maschinelles Lernen aufzuf√ºllen!  Das Sparschwein selbst wird nicht aufgef√ºllt! <br><br>  Dieses Mal werden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Andrei Boyarovs</a> faszinierende Geschichte √ºber die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Szenenerkennung</a> kennenlernen.  Andrey ist ein Computer Vision-Forscher, der sich bei der Mail.Ru Group mit Machine Vision besch√§ftigt. <br><br>  Die Szenenerkennung ist einer der weit verbreiteten Bereiche der Bildverarbeitung.  Diese Aufgabe ist komplizierter als das untersuchte Erkennen von Objekten: Die Szene ist ein komplexeres und weniger formalisiertes Konzept, es ist schwieriger, Merkmale zu unterscheiden.  Die Aufgabe, Sehensw√ºrdigkeiten zu erkennen, ergibt sich aus der Szenenerkennung: Sie m√ºssen bekannte Stellen auf dem Foto hervorheben, um eine geringe Anzahl von Fehlalarmen sicherzustellen. <br><br>  Dies sind <b>30 Minuten</b> Video von der Smart Data 2017-Konferenz. Das Video kann bequem zu Hause und unterwegs angesehen werden.  F√ºr diejenigen, die nicht bereit sind, so viel auf dem Bildschirm zu sitzen, oder die Informationen lieber in Textform wahrnehmen m√∂chten, wenden wir eine Volltextentschl√ºsselung an, die in Form von Habrosta gestaltet ist. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/dL1-OrjtMvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Ich mache Bildverarbeitung bei Mail.ru.  Heute werde ich dar√ºber sprechen, wie wir durch tiefes Lernen Bilder von Szenen und Attraktionen erkennen. <br><br>  Das Unternehmen stellte die Notwendigkeit fest, Benutzerbilder zu markieren und zu suchen. Aus diesem Grund haben wir beschlossen, eine eigene Computer Vision-API zu erstellen, zu der auch ein Tool zum Markieren von Szenen geh√∂ren wird.  Als Ergebnis dieses Tools m√∂chten wir etwas wie das im Bild unten gezeigte erhalten: Der Benutzer stellt eine Anfrage, zum Beispiel "Kathedrale", und erh√§lt alle seine Fotos mit Kathedralen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d18/5f8/a3b/d185f8a3bd0c73280cdae408fe84cca1.png"><br><br>  In der Computer Vision-Community wurde das Thema Objekterkennung in Bildern recht gut untersucht.  Es gibt einen bekannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet-Wettbewerb</a> , der seit mehreren Jahren stattfindet und dessen Hauptteil die Objekterkennung ist. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/989/f18/b8a989f189970cabb0de00be7ff48afa.png"><br><br>  Grunds√§tzlich m√ºssen wir ein Objekt lokalisieren und klassifizieren.  Bei Szenen ist die Aufgabe etwas komplizierter, da die Szene ein komplexeres Objekt ist, aus einer gro√üen Anzahl anderer Objekte besteht und der Kontext sie vereint, sodass die Aufgaben unterschiedlich sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/be7/a6b/ba4be7a6bc9a4e0c2c7929644540799c.png"><br><br>  Im Internet stehen Dienste anderer Unternehmen zur Verf√ºgung, die solche Funktionen implementieren.  Dies ist insbesondere die Google Vision-API oder die Microsoft Computer Vision-API, mit der Szenen in Bildern gefunden werden k√∂nnen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/758/fd9/095/758fd909536fd930db97c025ca61ef38.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/eee/b28/6df/eeeb286df22a31809f507da7730b7d89.png"><br><br>  Wir haben dieses Problem mit Hilfe des maschinellen Lernens gel√∂st, daher ben√∂tigen wir Daten.  Es gibt jetzt zwei Hauptgrundlagen f√ºr die Szenenerkennung im Open Access.  Der erste von ihnen erschien 2013 - dies ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die SUN-Basis</a> der Princeton University.  Diese Basis besteht aus Hunderttausenden von Bildern und 397 Klassen. <br><br>  Die zweite Basis, auf der wir trainiert haben, ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Places2-Basis</a> vom MIT.  Sie erschien 2013 in zwei Versionen.  Das erste ist Places2-Standart, eine ausgewogenere Basis mit 1,8 Millionen Bildern und 365 Klassen.  Die zweite Option - Places2-Challenge - enth√§lt acht Millionen Bilder und 365 Klassen, aber die Anzahl der Bilder zwischen den Klassen ist nicht ausgeglichen.  Beim ImageNet-Wettbewerb 2016 enthielt der Bereich Szenenerkennung die Places2-Challenge, und der Gewinner zeigte das beste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Top-5-Klassifizierungsfehlerergebnis</a> von etwa 9%. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a10/ea2/881/a10ea2881a5248449dce8202ceaa28ff.png"><br><br>  Wir haben auf Basis von Places2 trainiert.  Hier ist ein Beispielbild von dort: Es ist eine Schlucht, eine Landebahn, eine K√ºche, ein Fu√üballplatz.  Dies sind v√∂llig unterschiedliche komplexe Objekte, an denen wir lernen m√ºssen, sie zu erkennen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1cf/9b2/3ee/1cf9b23eedff87ae9bcbb2d8c4090551.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/602/617/1d1/6026171d19b38452ce0e4a071ba4e836.png"><br><br>  Vor dem Studium haben wir die Grundlagen an unsere Bed√ºrfnisse angepasst.  Es gibt einen Trick f√ºr die Objekterkennung, wenn Sie mit Modellen auf kleinen CIFAR-10- und CIFAR-100-Basen anstelle von ImageNet experimentieren, und nur dann trainieren die besten auf ImageNet. <br><br>  Wir beschlossen, den gleichen Weg zu gehen, nahmen die SUN-Datenbank, reduzierten sie, erhielten 89 Klassen, 50.000 Bilder im Zug und 10.000 Bilder bei der Validierung.  Aus diesem Grund haben wir vor dem Training auf Places2 Experimente durchgef√ºhrt und unsere Modelle basierend auf SUN getestet.  Das Training dauert nur 6-10 Stunden, im Gegensatz zu mehreren Tagen auf Places2, wodurch viel mehr Experimente durchgef√ºhrt und die Effektivit√§t gesteigert werden konnten. <br><br>  Wir haben uns auch die Places2-Datenbank selbst angesehen und festgestellt, dass wir einige Klassen nicht ben√∂tigen.  Entweder aus Produktionsgr√ºnden oder weil zu wenig Daten dar√ºber vorliegen, schneiden wir Klassen aus, wie zum Beispiel ein Aqu√§dukt, ein Baumhaus oder ein Scheunentor. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f8/31f/60f/4f831f60fd70ec7f74ea2bb0a29c69f5.png"><br><br>  Als Ergebnis haben wir nach all den Manipulationen die Places2-Datenbank erhalten, die 314 Klassen und eine halbe Million Bilder (in der Standardversion) enth√§lt, in der Challenge-Version etwa 7,5 Millionen Bilder.  Auf diesen Grundlagen haben wir Schulungen aufgebaut. <br><br>  Au√üerdem haben wir beim Betrachten der verbleibenden Klassen festgestellt, dass es zu viele f√ºr die Produktion gibt, sie sind zu detailliert.  Aus diesem Grund haben wir den Szenenzuordnungsmechanismus angewendet, wenn einige Klassen zu einer gemeinsamen Klasse zusammengefasst wurden.  Zum Beispiel haben wir alles, was mit W√§ldern zu tun hat, zu einem Wald verbunden, alles, was mit Krankenh√§usern zu tun hat - zu einem Krankenhaus, zu Hotels - zu einem Hotel. <br><br>  Wir verwenden die Szenenzuordnung nur zum Testen und f√ºr den Endbenutzer, da dies bequemer ist.  Im Training verwenden wir alle Standardklassen 314.  Wir haben die resultierende Basis Places Sift genannt. <br><br><h2>  Ans√§tze, L√∂sungen </h2><br>  Betrachten Sie nun die Ans√§tze, mit denen wir dieses Problem gel√∂st haben.  Tats√§chlich sind solche Aufgaben mit dem klassischen Ansatz verbunden - tiefen Faltungs-Neuronalen Netzen. <br><br>  Das Bild unten zeigt eines der ersten klassischen Netzwerke, enth√§lt jedoch bereits die Hauptbausteine, die in modernen Netzwerken verwendet werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a4/61f/7f7/0a461f7f7ae8a9264f5fadb69271f29d.png"><br><br>  Dies sind Faltungsschichten, dies sind Zugschichten, vollst√§ndig verbundene Schichten.  Um die Architektur zu bestimmen, haben wir die Spitzen der ImageNet- und Places2-Wettbewerbe √ºberpr√ºft. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7dc/658/7d0/7dc6587d046a968988d4f52fa7bcbb3f.png"><br><br>  Wir k√∂nnen sagen, dass die wichtigsten f√ºhrenden Architekturen in zwei Familien unterteilt werden k√∂nnen: Inception und die ResNet-Familie (Residual Network).  Im Verlauf der Experimente haben wir herausgefunden, dass die ResNet-Familie besser f√ºr unsere Aufgabe geeignet ist, und wir haben das n√§chste Experiment mit dieser Familie durchgef√ºhrt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a8/828/85b/1a882885b70533dd0341bd853d2818fd.png"><br><br>  ResNet ist ein tiefes Netzwerk, das aus einer gro√üen Anzahl von Restbl√∂cken besteht.  Dies ist der Hauptbaustein, der aus mehreren Schichten mit Gewichten und Verkn√ºpfungsverbindung besteht.  Infolge dieses Entwurfs lernt diese Einheit, wie stark sich das Eingangssignal x vom Ausgang f (x) unterscheidet.  Infolgedessen k√∂nnen wir Netzwerke solcher Bl√∂cke aufbauen, und w√§hrend des Trainings kann das Netzwerk in den letzten Schichten Gewichte nahe Null erzeugen. <br><br>  Wir k√∂nnen also sagen, dass das Netzwerk selbst entscheidet, wie tief es sein muss, um einige der Aufgaben zu l√∂sen.  Dank dieser Architektur k√∂nnen Netzwerke mit sehr gro√üer Tiefe mit einer sehr gro√üen Anzahl von Schichten aufgebaut werden.  Der Gewinner von ImageNet 2014 enthielt nur 22 Ebenen, ResNet √ºbertraf dieses Ergebnis und enthielt bereits 152 Ebenen. <br><br>  Die Kernforschung von ResNet besteht darin, einen Restblock zu verbessern und ordnungsgem√§√ü zu erstellen.  Das Bild unten zeigt eine empirisch und mathematisch fundierte Version, die das beste Ergebnis liefert.  Eine solche Konstruktion des Blocks erm√∂glicht es Ihnen, sich mit einem der grundlegenden Probleme des tiefen Lernens zu befassen - einem verblassenden Gradienten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/7a4/398/0207a439827163bc14895a47a1489ddc.png"><br><br>  Um unsere Netzwerke zu trainieren, haben wir das in Lua geschriebene Torch-Framework aufgrund seiner Flexibilit√§t und Geschwindigkeit verwendet und f√ºr ResNet die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Implementierung von ResNet von Facebook vorangetrieben</a> .  Um die Qualit√§t des Netzwerks zu √ºberpr√ºfen, haben wir drei Tests verwendet. <br><br>  Der erste Places-Val-Test ist die Validierung vieler Places-Sift-Sets.  Der zweite Test ist Places Sift mit Scene Mapping und der dritte ist der Cloud-Test, der der Kampfsituation am n√§chsten kommt.  Bilder von Mitarbeitern aus der Cloud, die manuell beschriftet wurden.  Im Bild unten gibt es zwei Beispiele f√ºr solche Bilder. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/796/6d0/066/7966d0066c9257617e81f290fbbc6283.png"><br><br>  Wir haben begonnen, Netzwerke zu messen, zu trainieren und miteinander zu vergleichen.  Der erste ist der ResNet-152-Benchmark, der mit Places2 geliefert wird, der zweite ist ResNet-50, den wir auf ImageNet trainiert und auf unserer Basis trainiert haben. Das Ergebnis war bereits besser.  Dann nahmen sie ResNet-200, das ebenfalls auf ImageNet trainiert wurde, und es zeigte am Ende das beste Ergebnis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d9/0de/320/6d90de3208968f8120c1f4be1e79f74e.png"><br><br>  Nachfolgend finden Sie Beispiele f√ºr Arbeiten.  Dies ist ein ResNet-152-Benchmark.  Vorausgesagt werden die Originaletiketten, die das Netzwerk ausgibt.  Zugeordnete Etiketten sind die Beschriftungen, die nach der Szenenzuordnung erstellt wurden.  Es ist ersichtlich, dass das Ergebnis nicht sehr gut ist.  Das hei√üt, sie scheint etwas zu dem Fall zu geben, aber nicht sehr gut. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/892/ceb/86e892ceb75b85dcf22bec203c0f4f3d.png"><br><br>  Das n√§chste Beispiel ist der Betrieb von ResNet-200.  Schon sehr ausreichend. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa6/621/d0d/fa6621d0dd29ac9d77da10378eec9454.png"><br><br><h2>  ResNet-Verbesserung </h2><br>  Wir haben uns entschlossen, unser Netzwerk zu verbessern, und zuerst haben wir nur versucht, die Tiefe des Netzwerks zu erh√∂hen, aber danach wurde es viel schwieriger zu trainieren.  Dies ist ein bekanntes Problem. Letztes Jahr wurden mehrere Artikel zu diesem Thema ver√∂ffentlicht, die besagen, dass ResNet tats√§chlich ein Ensemble einer gro√üen Anzahl gew√∂hnlicher Netzwerke unterschiedlicher Tiefe ist. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df2/0ad/023/df20ad0234f5eba5597939468f8afaf6.png"><br><br>  Res-Bl√∂cke, die sich am Ende des Gitters befinden, tragen einen kleinen Beitrag zur Bildung des Endergebnisses bei.  Es erscheint vielversprechender, nicht die Netzwerktiefe, sondern die Breite, dh die Anzahl der Filter im Res-Block, zu erh√∂hen. <br><br>  Diese Idee wird vom Wide Residual Network umgesetzt, das 2016 erschien.  Am Ende haben wir WRN-50-2 verwendet, das √ºbliche ResNet-50 mit der doppelten Anzahl von Filtern in der 3x3-Faltung des inneren Engpasses. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5be/f29/aed/5bef29aed5cfce3b0121ce69ee072c35.png"><br><br>  Das Netzwerk zeigt auf ImageNet √§hnliche Ergebnisse mit dem ResNet-200, das wir bereits verwendet haben, aber vor allem ist es fast doppelt so schnell.  Hier sind zwei Implementierungen des Restblocks auf der Taschenlampe: Der Parameter, der verdoppelt wird, wird hell hervorgehoben.  Dies ist die Anzahl der Filter in der inneren Faltung. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1a/3fe/d24/f1a3fed24bf735ed30cfc407aac6ca79.png"><br><br>  Dies sind Messungen bei den ResNet-200 ImageNet-Tests.  Zuerst nahmen wir WRN-22-6, es zeigte ein schlechteres Ergebnis.  Dann nahmen sie WRN-50-2-ImageNet, trainierten ihn, nahmen WRN-50-2, trainierten auf ImageNet und trainierten ihn auf Places2-Challenge, und er zeigte das beste Ergebnis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/736/fd6/afe/736fd6afe39702160b87ad095ef0edcd.png"><br><br>  Hier ist ein Beispiel f√ºr den WRN-50-2 - ein v√∂llig angemessenes Ergebnis in unseren Bildern, die Sie bereits gesehen haben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b0a/442/e1f/b0a442e1f87a4f2e2a6c43a96677ce3d.png"><br><br>  Und dies ist ein Beispiel f√ºr die erfolgreiche Arbeit mit Kampffotografien. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/924/715/6bf/9247156bf459bd5b005b4180d36a1da5.png"><br><br>  Es gibt nat√ºrlich nicht sehr erfolgreiche Arbeiten.  Die Br√ºcke von Alexander III. In Paris wurde nicht als Br√ºcke anerkannt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dd/c66/9ec/3ddc669ecf4aabe61b103dafbc4aabb2.png"><br><br><h2>  Modellverbesserung </h2><br>  Wir haben dar√ºber nachgedacht, wie wir dieses Modell verbessern k√∂nnen.  Die ResNet-Familie verbessert sich weiter und es erscheinen neue Artikel.  Insbesondere wurde 2016 ein interessanter Artikel PyramidNet ver√∂ffentlicht, der vielversprechende Ergebnisse zu CIFAR-10/100 und ImageNet zeigte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdd/816/e1b/fdd816e1b970be4465ff8200b7f0cb56.png"><br><br>  Die Idee ist nicht, die Breite des Restblocks stark zu vergr√∂√üern, sondern dies schrittweise zu tun.  Wir haben verschiedene Optionen f√ºr dieses Netzwerk trainiert, aber leider zeigte es etwas schlechtere Ergebnisse als unser Kampfmodell. <br><br>  Im Fr√ºhjahr 2018 wurde das ResNext-Modell ver√∂ffentlicht, ebenfalls eine vielversprechende Idee: den Residual-Block in mehrere parallele Bl√∂cke kleinerer Gr√∂√üe und kleinerer Breite zu unterteilen.  Dies √§hnelt der Idee von Inception, wir haben auch damit experimentiert.  Leider zeigte sie schlechtere Ergebnisse als unser Modell. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/712/3c4/e0f/7123c4e0f7f826f9dab8a1791d4eb3cd.png"><br><br>  Wir haben auch mit verschiedenen ‚Äûkreativen‚Äú Ans√§tzen experimentiert, um unsere Modelle zu verbessern.  Insbesondere haben wir versucht, CAM (Class Activation Mapping) zu verwenden. Dies sind die Objekte, die das Netzwerk bei der Klassifizierung des Bildes betrachtet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1bf/639/ea4/1bf639ea430bd03bafeeacf8acb29a09.png"><br><br>  Unsere Idee war, dass Instanzen derselben Szene dieselben oder √§hnliche Objekte wie eine CAM-Klasse haben sollten.  Wir haben versucht, diesen Ansatz zu verwenden.  Zuerst nahmen sie zwei Netzwerke.  Einer ist ImageNet-geschult, der zweite ist unser Modell, das wir verbessern wollen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/47d/ac5/538/47dac55383f098852c9a69a3d50f2cc1.png"><br><br>  Wir nehmen das Bild auf, f√ºhren es durch Netzwerk 2, f√ºgen das CAM f√ºr die Schicht hinzu und f√ºhren es dann dem Eingang von Netzwerk 1 zu. F√ºhren Sie es durch Netzwerk 1, f√ºgen Sie die Ergebnisse zur Verlustfunktion von Netzwerk 2 hinzu und setzen Sie dies mit den neuen Verlustfunktionen fort. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/860/c3e/9c3/860c3e9c37f6d6ffd91d3cdd36d67682.png"><br><br>  Die zweite M√∂glichkeit besteht darin, dass wir das Image √ºber Netzwerk 2 ausf√ºhren, das CAM nehmen, es dem Eingang von Netzwerk 1 zuf√ºhren und dann anhand dieser Daten einfach Netzwerk 1 trainieren und das Ensemble aus den Ergebnissen von Netzwerk 1 und Netzwerk 2 verwenden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fb/140/95b/5fb14095b117b05582a79a43ee0eadb6.png"><br><br>  Wir haben unser Modell auf WRN-50-2 umgeschult, da wir als Netzwerk 1 ResNet-50 ImageNet verwendet haben, aber es war nicht m√∂glich, die Qualit√§t unseres Modells irgendwie signifikant zu verbessern. <br><br>  Wir forschen jedoch weiter daran, wie wir unsere Ergebnisse verbessern k√∂nnen: Wir trainieren neue CNN-Architekturen, insbesondere die ResNet-Familie.  Wir versuchen, mit CAM zu experimentieren und betrachten verschiedene Ans√§tze mit einer intelligenteren Verarbeitung von Bildfeldern - es scheint uns, dass dieser Ansatz ziemlich vielversprechend ist. <br><br><h2>  Landmark Recognition </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/11b/9da/4c3/11b9da4c376bcee091afb70d3dbb0745.png"><br><br>  Wir haben ein gutes Modell zum Erkennen von Szenen, aber jetzt wollen wir einige ikonische Orte herausfinden, dh Sehensw√ºrdigkeiten.  Dar√ºber hinaus machen Benutzer h√§ufig Fotos von ihnen oder machen Bilder vor ihrem Hintergrund. <br><br>  Wir m√∂chten, dass das Ergebnis nicht nur die Kathedralen sind, wie auf dem Bild auf der Folie, sondern das System, das sagt: "Es gibt Notre Dame de Paris und die Kathedralen in Prag." <br><br>  Als wir dieses Problem l√∂sten, stie√üen wir auf einige Schwierigkeiten. <br><br><ol><li>  Es gibt praktisch keine Studien zu diesem Thema und es gibt keine vorgefertigten Daten im √∂ffentlichen Bereich. <br></li><li>  Eine kleine Anzahl von "sauberen" Bildern im √∂ffentlichen Bereich f√ºr jede Attraktion. <br></li><li>  Es ist nicht ganz klar, was ein Wahrzeichen von Geb√§uden ist.  Zum Beispiel ein Haus mit T√ºrmen auf Sq.  Leo Tolstoi in Petersburg, TripAdvisor ber√ºcksichtigt keine Attraktionen, aber Google ber√ºcksichtigt. <br></li></ol><br>  Wir haben zun√§chst eine Datenbank gesammelt, eine Liste mit 100 St√§dten zusammengestellt und dann mithilfe der Google Places-API JSON-Daten f√ºr Sonderziele aus diesen St√§dten heruntergeladen. <br><br>  Die Daten wurden gefiltert und analysiert, und gem√§√ü der Liste haben wir f√ºr jede Attraktion 20 Bilder von der Google-Suche heruntergeladen.  Die Zahl 20 stammt aus empirischen √úberlegungen.  Als Ergebnis haben wir eine Basis von 2827 Attraktionen und ungef√§hr 56.000 Bildern.  Auf dieser Basis haben wir unser Modell trainiert.  Zur Validierung unseres Modells haben wir zwei Tests verwendet. <br><br>  Cloud-Test - Dies sind Bilder unserer Mitarbeiter, die manuell beschriftet wurden.  Es enth√§lt 200 Bilder in 15 St√§dten und 10 Tausend Bilder ohne Attraktionen.  Der zweite ist der Suchtest.  Es wurde mit der Mail.ru-Suche erstellt, die 3 bis 10 Bilder f√ºr jede Attraktion enth√§lt, aber leider ist dieser Test schmutzig. <br><br>  Wir haben die ersten Modelle trainiert, aber sie zeigten schlechte Ergebnisse beim Cloud-Test auf Kampffotos. <br><br>  Hier ist ein Beispiel f√ºr das Bild, auf dem wir trainiert wurden, und ein Beispiel f√ºr Kampffotografie.  Das Problem bei Menschen ist, dass sie oft vor dem Hintergrund der Sehensw√ºrdigkeiten fotografiert werden.  In diesen Bildern, die wir von der Suche erhalten haben, waren keine Menschen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b1d/8a6/63b/b1d8a663bf52f5910cf35415a93c296a.png"><br><br>  Um dem entgegenzuwirken, haben wir w√§hrend des Trainings eine ‚Äûmenschliche‚Äú Erweiterung hinzugef√ºgt.  Das hei√üt, wir haben Standardans√§tze verwendet: zuf√§llige Rotationen, zuf√§lliges Ausschneiden eines Teils des Bildes und so weiter.  Aber auch im Lernprozess haben wir zuf√§llig Bilder zu einigen Bildern hinzugef√ºgt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12c/6c7/b54/12c6c7b54ae4d64065ee7fefa18acb72.png"><br><br>  Dieser Ansatz hat uns geholfen, das Problem mit Menschen zu l√∂sen und ein akzeptables Qualit√§tsmodell zu erhalten. <br><br><h2>  Feinabstimmung von Szenenmodellen </h2><br>  Wie wir das Modell trainiert haben: Es gibt eine Trainingsbasis, aber sie ist ziemlich klein.  Wir wissen jedoch, dass eine Touristenattraktion ein Sonderfall der Szene ist.  Und wir haben ein ziemlich gutes Szenenmodell.  Wir beschlossen, sie f√ºr die Sehensw√ºrdigkeiten zu trainieren.  Zu diesem Zweck haben wir mehrere vollst√§ndig verbundene und BN-Schichten √ºber dem Netzwerk hinzugef√ºgt, sie und die drei obersten Restbl√∂cke trainiert.  Der Rest des Netzwerks wurde eingefroren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5e/b4a/e80/a5eb4ae80781b95eec46821dd05d5cba.png"><br><br>  Dar√ºber hinaus verwenden wir f√ºr das Training die nicht standardm√§√üige Center-Loss-Funktion.  W√§hrend des Trainings versucht Center Loss, Vertreter verschiedener Klassen in verschiedene Cluster zu ‚Äûzerlegen‚Äú, wie in der Abbildung gezeigt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cc/2e8/bbc/8cc2e8bbc85f79dfb6301742d7c3d4ac.png"><br><br>  Im Training haben wir eine weitere Klasse hinzugef√ºgt, ‚Äûkeine Touristenattraktion‚Äú.  Und der Mittelverlust wurde auf diese Klasse nicht angewendet.  F√ºr eine solche gemischte Verlustfunktion wurde ein Training durchgef√ºhrt. <br><br>  Nachdem wir das Netzwerk trainiert haben, schneiden wir die letzte Klassifizierungsschicht davon ab. Wenn das Bild das Netzwerk durchl√§uft, wird es zu einem numerischen Vektor, der als Einbettung bezeichnet wird. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf2/ebc/0fd/cf2ebc0fd7dc3af7770e6dd2c616fc74.png"><br><br>  Um ein Landmark-Erkennungssystem weiter aufzubauen, haben wir Referenzvektoren f√ºr jede Klasse erstellt.  Wir haben jede Klasse von Attraktionen aus der Menge genommen und die Bilder durch das Netzwerk geleitet.  Sie bekamen Einbettungen und nahmen ihren mittleren Vektor, der als Klassenreferenzvektor bezeichnet wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5e9/8ef/f32/5e98eff32e669f6c3c0b7245c25bc4ea.png"><br><br>  Um die Sehensw√ºrdigkeiten auf dem Foto zu bestimmen, f√ºhren wir das Eingabebild durch das Netzwerk und seine Einbettung wird mit dem Referenzvektor jeder Klasse verglichen.  Wenn das Ergebnis des Vergleichs unter dem Schwellenwert liegt, glauben wir, dass das Bild keine Attraktionen aufweist.  Ansonsten nehmen wir die Klasse mit dem h√∂chsten Vergleichswert. <br><br><h2>  Testergebnisse </h2><br><ul><li>  Beim Wolkentest betrug die Genauigkeit der Visiere 0,616, nicht die der Visiere - 0,981 </li><li>  Die durchschnittliche Genauigkeit von 0,669 wurde beim Suchtest erhalten, und die durchschnittliche Vollst√§ndigkeit betrug 0,576. </li></ul><br>  Bei der Suche haben sie keine sehr guten Ergebnisse erzielt, aber dies erkl√§rt sich aus der Tatsache, dass der erste ziemlich "schmutzig" ist und der zweite Merkmale aufweist - unter den Attraktionen gibt es verschiedene botanische G√§rten, die in allen St√§dten √§hnlich sind. <br><br>  Es gab eine Idee f√ºr die Szenenerkennung, um zuerst das Netzwerk zu trainieren, das die Szenenmaske bestimmt, dh Objekte aus dem Vordergrund entfernt und sie dann in das Modell selbst einspeist, das Bildszenen ohne diese Bereiche erkennt, in denen der Hintergrund blockiert ist.  Es ist jedoch nicht ganz klar, was genau von der vorderen Schicht entfernt werden muss, welche Maske ben√∂tigt wird. <br><br>  Es wird eine ziemlich komplizierte und kluge Sache sein, weil nicht jeder versteht, welche Objekte zur Szene geh√∂ren und welche √ºberfl√ºssig sind.  Beispielsweise k√∂nnen Personen in einem Restaurant ben√∂tigt werden.  Dies ist eine nicht triviale Entscheidung, wir haben versucht, etwas √Ñhnliches zu tun, aber es hat keine guten Ergebnisse gebracht. <br><br>  Hier ist ein Beispiel f√ºr die Arbeit mit Kampffotografien. <br><br>  Beispiele f√ºr erfolgreiche Arbeit: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c53/ca4/413/c53ca44130bc3b9a845a7f21a681a8e0.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/774/c01/642774c0174cb07eeb52c055cb92b8d4.png"><br><br>  Aber der erfolglose Job: Es wurden keine Sehensw√ºrdigkeiten gefunden.  Das Hauptproblem unseres Modells besteht derzeit nicht darin, dass das Netzwerk die Sehensw√ºrdigkeiten verwirrt, sondern dass es sie auf dem Foto nicht findet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c9/aca/7ab/6c9aca7ab564401b4c9b5cea126fc68a.png"><br><br>  In Zukunft planen wir, eine Basis f√ºr eine noch gr√∂√üere Anzahl von St√§dten zu sammeln, neue Methoden zu finden, um das Netzwerk f√ºr diese Aufgabe zu trainieren und die M√∂glichkeiten zu ermitteln, die Anzahl der Klassen zu erh√∂hen, ohne das Netzwerk neu auszubilden. <br><br><h2>  Schlussfolgerungen </h2><br>  Heute haben wir: <br><br><ul><li>  Wir haben uns angesehen, welche Datens√§tze f√ºr die Szenenerkennung verf√ºgbar sind. <br></li><li>  Wir haben gesehen, dass das Wide Residual Network das beste Modell ist. <br></li><li>  Diskussion weiterer M√∂glichkeiten zur Qualit√§tssteigerung dieses Modells; <br></li><li>  Wir haben uns die Aufgabe angesehen, Sehensw√ºrdigkeiten zu erkennen, welche Schwierigkeiten auftreten; <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir haben den Algorithmus zum Sammeln der Basis- und Lehrmethoden des Modells zum Erkennen von Attraktionen beschrieben. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich kann sagen, dass die Aufgaben interessant sind, aber in der Gemeinde wenig studiert werden. </font><font style="vertical-align: inherit;">Es ist interessant, mit ihnen umzugehen, da Sie nicht standardm√§√üige Ans√§tze anwenden k√∂nnen, die bei der √ºblichen Erkennung von Objekten nicht angewendet werden.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minute der Werbung. </font><font style="vertical-align: inherit;">Wenn Ihnen dieser Bericht von der SmartData-Konferenz gefallen hat, beachten Sie bitte, dass </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmartData 2018</font></font></b></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> am 15. Oktober in St. Petersburg stattfindet, einer </font><font style="vertical-align: inherit;">Konferenz f√ºr diejenigen, die in die Welt des maschinellen Lernens, der Analyse und der Datenverarbeitung eintauchen. </font><font style="vertical-align: inherit;">Das Programm wird viele interessante Dinge enthalten, die Seite hat bereits ihre ersten Redner und Berichte.</font></font></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de419501/">https://habr.com/ru/post/de419501/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de419485/index.html">Daf√ºr gibt es eine App: Ank√ºndigung von Mobius 2018 Moskau</a></li>
<li><a href="../de419491/index.html">Wie STP funktioniert</a></li>
<li><a href="../de419493/index.html">Warum brauchst du Splunk? Sicherheitsereignisanalyse</a></li>
<li><a href="../de419495/index.html">Wer hat die Knochenleitung "erfunden", warum wird sie verwendet und wie sicher ist sie f√ºr das H√∂ren?</a></li>
<li><a href="../de419497/index.html">Hercules Strong 3D Gro√üdrucker Bewertung</a></li>
<li><a href="../de419505/index.html">Sie k√∂nnen jetzt Pakete ohne Benachrichtigungen und P√§sse per Post im ganzen Land erhalten.</a></li>
<li><a href="../de419507/index.html">√úbersicht √ºber den russischen 3D-Drucker PICASO 3D Designer X von 3Dtool</a></li>
<li><a href="../de419509/index.html">Photonisches k√ºnstliches neuronales Netzwerk</a></li>
<li><a href="../de419511/index.html">Typ (T) vs. TypeOf‚ü®T‚ü©</a></li>
<li><a href="../de419513/index.html">Konfigurieren Sie die Kennwortsicherheitsrichtlinie in Zimbra</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>