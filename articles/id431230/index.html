<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â™‹ï¸ ğŸ‘©ğŸ¼â€ğŸŒ¾ ğŸ¹ Penyimpanan untuk Infrastruktur HPC, atau Cara Kami Mengumpulkan Penyimpanan 65 PB di Pusat Penelitian RIKEN Jepang ğŸ‘ âœ”ï¸ ğŸ§ğŸ¿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="datacenterknowledge.com 

 Tahun lalu, instalasi penyimpanan berbasis RAIDIX terbesar saat ini dilaksanakan. Sebuah sistem 11 kluster failover digunak...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Penyimpanan untuk Infrastruktur HPC, atau Cara Kami Mengumpulkan Penyimpanan 65 PB di Pusat Penelitian RIKEN Jepang</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raidix/blog/431230/"><img src="https://habrastorage.org/webt/gt/hj/ib/gthjibaqdw82ss1jxo2tmhpa97o.jpeg"><br>  <i><font color="#99999">datacenterknowledge.com</font></i> <br><br>  Tahun lalu, instalasi penyimpanan berbasis RAIDIX terbesar saat ini dilaksanakan.  Sebuah sistem 11 kluster failover digunakan di RIKEN Institute of Computing Sciences (Jepang).  Tujuan utama dari sistem ini adalah HPC Infrastructure Storage (HPCI), yang diimplementasikan sebagai bagian dari proyek pertukaran informasi akademik skala besar, Academic Cloud (berdasarkan jaringan SINET). <br><br>  Karakteristik yang signifikan dari proyek ini adalah volume totalnya 65 PB, di mana volume sistem yang dapat digunakan adalah 51,4 PB.  Untuk lebih memahami nilai ini, kami menambahkan bahwa ini adalah 6512 disk masing-masing 10 TB (paling modern pada saat instalasi).  Ini banyak sekali. <br><a name="habracut"></a><br>  Pekerjaan pada proyek berlangsung sepanjang tahun, setelah itu pemantauan stabilitas sistem berlanjut selama sekitar satu tahun.  Indikator yang diperoleh memenuhi persyaratan yang dinyatakan, dan sekarang kita dapat berbicara tentang keberhasilan catatan ini dan proyek yang signifikan bagi kita. <br><br><h2>  Superkomputer di RIKEN Institute Computing Center </h2><br>  Untuk industri TIK, RIKEN Institute dikenal terutama karena "K-computer" yang legendaris (dari "kei" Jepang, yang berarti 10 kuadriliun), yang pada saat diluncurkan (Juni 2011) dianggap sebagai superkomputer paling kuat di dunia. <br><br><div class="spoiler">  <b class="spoiler_title">Baca tentang K-komputer</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">www.nytimes.com/2011/06/20/technology/20computer.html</a> <br></div></div><br>  Superkomputer membantu Pusat Ilmu Komputasi dalam implementasi studi skala besar yang kompleks: memungkinkan pemodelan iklim, kondisi cuaca dan perilaku molekuler, menghitung dan menganalisis reaksi dalam fisika nuklir, prediksi gempa, dan banyak lagi.  Kapasitas superkomputer juga digunakan untuk penelitian yang lebih â€œsehari-hariâ€ dan terapan - untuk mencari ladang minyak dan memperkirakan tren di pasar saham. <br><br>  Perhitungan dan eksperimen semacam itu menghasilkan sejumlah besar data, nilai dan signifikansi yang tidak dapat ditaksir terlalu tinggi.  Untuk mendapatkan hasil maksimal dari ini, para ilmuwan Jepang telah mengembangkan konsep untuk ruang informasi tunggal di mana para profesional HPC dari berbagai pusat penelitian akan memiliki akses ke sumber daya HPC yang diterima. <br><br><h2>  Infrastruktur Komputasi Kinerja Tinggi (HPCI) </h2><br>  HPCI beroperasi berdasarkan SINET (The Science Information Network), jaringan tulang punggung untuk pertukaran data ilmiah antara universitas dan pusat penelitian Jepang.  Saat ini, SINET menyatukan sekitar 850 lembaga dan universitas, menciptakan peluang besar untuk pertukaran informasi dalam penelitian yang memengaruhi fisika nuklir, astronomi, geodesi, seismologi, dan ilmu komputer. <br><br>  HPCI adalah proyek infrastruktur unik yang membentuk sistem pertukaran informasi terpadu dalam bidang komputasi kinerja tinggi antara universitas dan pusat penelitian di Jepang. <br><br>  Dengan menggabungkan kemampuan superkomputer "K" dan pusat penelitian lainnya dalam bentuk yang dapat diakses, komunitas ilmiah menerima manfaat nyata untuk bekerja dengan data berharga yang dibuat oleh komputasi superkomputer. <br><br>  Untuk memberikan akses pengguna bersama yang efektif ke lingkungan HPCI, persyaratan tinggi diberlakukan pada penyimpanan untuk kecepatan akses.  Dan berkat "hyperproductivity" dari K-komputer, cluster penyimpanan di RIKEN Institute of Computing Sciences dihitung untuk dibuat dengan volume kerja minimal 50 PB. <br><br>  Infrastruktur proyek HPCI dibangun berdasarkan sistem file Gfarm, yang memungkinkan untuk memberikan kinerja tingkat tinggi dan menggabungkan cluster penyimpanan yang berbeda ke dalam ruang berbagi tunggal. <br><br><h2>  Sistem File Gfarm </h2><br>  Gfarm adalah sistem file terdistribusi open source yang dikembangkan oleh para insinyur Jepang.  Gfarm adalah buah dari pengembangan Institute for Advanced Industrial Science and Technology (AIST), dan nama sistem mengacu pada arsitektur yang digunakan oleh Grid Data Farm. <br><br>  Sistem file ini menggabungkan sejumlah properti yang tampaknya tidak kompatibel: <br><br><ul><li>  Skalabilitas tinggi dalam volume dan kinerja </li><li>  Distribusi jaringan jarak jauh dengan dukungan untuk namespace tunggal untuk beberapa pusat penelitian yang beragam </li><li>  Dukungan POSIX API </li><li>  Diperlukan kinerja tinggi untuk komputasi paralel </li><li>  Keamanan Penyimpanan Data </li></ul><br>  Gfarm menciptakan sistem file virtual menggunakan sumber daya penyimpanan dari beberapa server.  Data didistribusikan oleh server metadata, dan skema distribusi itu sendiri tersembunyi dari pengguna.  Saya harus mengatakan bahwa Gfarm tidak hanya terdiri dari cluster penyimpanan, tetapi juga grid komputasi yang menggunakan sumber daya dari server yang sama.  Prinsip operasi sistem menyerupai Hadoop: karya yang dikirimkan "diturunkan" ke node tempat data berada. <br><br>  Arsitektur sistem file asimetris.  Peran dialokasikan dengan jelas: Server Penyimpanan, Server Metadata, Klien.  Tetapi pada saat yang sama, ketiga peran dapat dilakukan oleh mesin yang sama.  Server penyimpanan menyimpan banyak salinan file, dan server metadata beroperasi dalam mode master-slave. <br><br><h2>  Pekerjaan proyek </h2><br>  Core Micro Systems, mitra strategis dan pemasok eksklusif RAIDIX di Jepang, mengimplementasikan implementasinya di RIKEN Institute of Computing Sciences Center.  Untuk melaksanakan proyek, dibutuhkan sekitar 12 bulan kerja yang melelahkan, di mana tidak hanya karyawan Core Micro Systems, tetapi juga spesialis teknis dari tim Reydix mengambil bagian aktif. <br><br>  Pada saat yang sama, transisi ke sistem penyimpanan lain tampaknya tidak mungkin: sistem yang ada memiliki banyak ikatan teknis, yang mempersulit transisi ke merek baru. <br><br>  Selama pengujian, pemeriksaan, dan peningkatan yang panjang, RAIDIX telah menunjukkan kinerja dan efisiensi tinggi secara konsisten saat bekerja dengan jumlah data yang mengesankan. <br><br>  Tentang perbaikan itu perlu diceritakan lebih banyak.  Itu diperlukan tidak hanya untuk menciptakan integrasi sistem penyimpanan dengan sistem file Gfarm, tetapi untuk memperluas beberapa karakteristik fungsional perangkat lunak.  Misalnya, untuk memenuhi persyaratan yang ditetapkan dari spesifikasi teknis, perlu untuk mengembangkan dan mengimplementasikan teknologi Automatic Write-Through sesegera mungkin. <br><br>  Penyebaran sistem itu sendiri sistematis.  Insinyur dari Core Micro Systems dengan cermat dan akurat melakukan setiap tahap pengujian, secara bertahap meningkatkan skala sistem. <br><br>  Pada Agustus 2017, fase penyebaran pertama selesai ketika volume sistem mencapai 18 PB.  Pada bulan Oktober tahun yang sama, fase kedua dilaksanakan, di mana volume naik ke rekor 51 PB. <br><br><h2>  Arsitektur Solusi </h2><br>  Solusi ini dibuat melalui integrasi sistem penyimpanan RAIDIX dan sistem file terdistribusi Gfarm.  Dalam hubungannya dengan Gfarm, kemampuan untuk membuat penyimpanan yang dapat diskalakan menggunakan 11 sistem RAIDIX dual-controller. <br><br>  Menghubungkan ke server Gfarm adalah melalui 8 x SAS 12G. <br><br><img src="https://habrastorage.org/webt/ii/3x/wk/ii3xwkawuyvwht0pczwonxbumnu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Gambar cluster dengan server data terpisah untuk setiap node</font></i> <br><br>  (1) 48Gbps Ã— 8 koneksi SAN Mesh;  bandwidth: 384Gbps <br>  (2) 48Gbps Ã— 40 KAIN koneksi FABRIC;  bandwidth: 1920Gbps <br><br><h3>  Konfigurasi platform pengontrol ganda </h3><br><table><tbody><tr><td>  CPU </td><td>  Intel Xeon E5-2637 - 4pcs </td></tr><tr><td>  Papan induk </td><td>  Kompatibel dengan model prosesor yang mendukung PCI Express 3.0 x8 / x16 </td></tr><tr><td>  Cache internal </td><td>  256 GB untuk setiap node </td></tr><tr><td>  Sasis </td><td>  2U </td></tr><tr><td>  Pengontrol SAS untuk menghubungkan rak disk, server dan menulis sinkronisasi cache </td><td>  Broadcom 9305 16e, 9300 8e </td></tr><tr><td>  HDD </td><td>  HGST Helium 10TB SAS HDD </td></tr><tr><td>  Sinkronisasi detak jantung </td><td>  Ethernet 1 GbE </td></tr><tr><td>  Sinkronisasi CacheSync </td><td>  6 x SAS 12G </td></tr></tbody></table><br>  Kedua node dari kluster failover terhubung ke 10 JBOD (masing-masing 60 disk dengan 10 TB) melalui 20 port SAS 12G untuk setiap node.  Pada rak disk ini, 58 10TB array RAID6 dibuat (8 disk data (D) + 2 disk paritas (P)) dan 12 disk dialokasikan untuk "hot swap". <br><br>  10 JBOD =&gt; 58 Ã— RAID6 (8 disk data (D) + 2 disk paritas (P)), LUN dari 580 HDD + 12 HDD untuk â€œhot swapâ€ (2,06% dari total volume) <br><br>  592 HDD (10TB SAS / 7.2k HDD) per cluster * HDD: HGST (MTBF: 2 500 000 jam) <br><br><img src="https://habrastorage.org/webt/qv/vc/0e/qvvc0egbrc1txvsztmx3o6yl918.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. Failover cluster dengan diagram koneksi 10 JBOD</font></i> <br><br><h3>  Sistem umum dan diagram koneksi </h3><br><img src="https://habrastorage.org/webt/6m/ju/xv/6mjuxvlhx5zlcnqb1eopx9nknfu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Gambar satu cluster di dalam sistem HPCI</font></i> <br><br><h2>  Indikator proyek utama </h2><br><blockquote>  Kapasitas yang dapat digunakan per kluster: <b>4,64 PB</b> ((RAID6 / 8D + 2P) LUN Ã— 58) <br><br>  Total kapasitas yang dapat digunakan dari keseluruhan sistem: <b>51,04 PB</b> (4,64 PB Ã— 11 cluster). <br><br>  Total kapasitas sistem: <b>65 PB</b> . <br><br>  Kinerja sistem adalah: <b>17 GB / s</b> untuk menulis, <b>22 GB / s</b> untuk membaca. <br><br>  Kinerja total subsistem disk kluster pada 11 sistem penyimpanan RAIDIX: <b>250 GB / s</b> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id431230/">https://habr.com/ru/post/id431230/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id431218/index.html">Bagaimana saya membuat game komik Lovecraft</a></li>
<li><a href="../id431220/index.html">Pandangan seorang ahli biologi tentang akar penuaan kita</a></li>
<li><a href="../id431222/index.html">Pengarsipan Situs Web</a></li>
<li><a href="../id431226/index.html">Gim Ular untuk FPGA Cyclone IV (dengan VGA & SPI joystick)</a></li>
<li><a href="../id431228/index.html">Rintangan Jalankan untuk Cahaya: Kristal Cair untuk Membantu</a></li>
<li><a href="../id431232/index.html">Kami menghasilkan placeholder SVG yang indah di Node.js</a></li>
<li><a href="../id431234/index.html">11 Desember, Moskow - Alfa JS MeetUp</a></li>
<li><a href="../id431236/index.html">Cara menulis di Objective-C pada tahun 2018. Bagian 1</a></li>
<li><a href="../id431238/index.html">Intisari acara untuk profesional SDM di bidang TI untuk Desember 2018</a></li>
<li><a href="../id431242/index.html">Sertifikat TLS dan Web</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>