<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê† üëèüèø üë®üèª‚Äçüéì C√≥mo funciona el almacenamiento S3 DataLine üëàüèæ ‚ôãÔ∏è üöª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! 

 No es ning√∫n secreto que grandes cantidades de datos est√°n involucrados en el trabajo de las aplicaciones modernas, y su flujo est√° en c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo funciona el almacenamiento S3 DataLine</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dataline/blog/423853/"><img src="https://habrastorage.org/webt/r2/ri/yr/r2riyrsimmtddysut6vvaatcqtw.png"><br><br>  Hola Habr! <br><br>  No es ning√∫n secreto que grandes cantidades de datos est√°n involucrados en el trabajo de las aplicaciones modernas, y su flujo est√° en constante crecimiento.  Estos datos deben almacenarse y procesarse, a menudo desde una gran cantidad de m√°quinas, y esta no es una tarea f√°cil.  Para resolverlo, hay tiendas de objetos en la nube.  Por lo general, son una implementaci√≥n de la tecnolog√≠a de almacenamiento definido por software. <br><br>  A principios de 2018, lanzamos (¬°y lanzamos!) Nuestro propio almacenamiento 100% compatible con S3 basado en Cloudian HyperStore.  Al final result√≥ que, la red tiene muy pocas publicaciones en ruso sobre Cloudian, y menos a√∫n sobre la aplicaci√≥n real de esta soluci√≥n. <br><br>  Hoy, con base en la experiencia de DataLine, le contar√© sobre la arquitectura y la estructura interna del software Cloudian, incluida la implementaci√≥n de SDS Cloudian basada en una serie de soluciones arquitect√≥nicas de Apache Cassandra.  Por separado, consideramos lo m√°s interesante en cualquier almacenamiento SDS: la l√≥gica de garantizar la tolerancia a fallos y la distribuci√≥n de objetos. <br><br>  Si est√° creando su almacenamiento S3 o est√° ocupado manteni√©ndolo, este art√≠culo le ser√° √∫til. <br><a name="habracut"></a><br>  En primer lugar, explicar√© por qu√© nuestra elecci√≥n recay√≥ en Cloudian.  Es simple: hay muy pocas opciones valiosas en este nicho.  Por ejemplo, hace un par de a√±os, cuando est√°bamos pensando en construir, solo hab√≠a tres opciones: <br><br><ul><li>  CEHP + RADOS Gateway; <br></li><li>  Minio <br></li><li>  Cloudian HyperStore. <br></li></ul><br>  Para nosotros, como proveedor de servicios, los factores decisivos fueron: un alto nivel de correspondencia entre la API de almacenamiento y el Amazon S3 original, la disponibilidad de facturaci√≥n incorporada, la escalabilidad con soporte multirregional y la presencia de una tercera l√≠nea de soporte de proveedores.  Cloudian tiene todo esto. <br><br>  Y s√≠, lo m√°s importante (¬°sin duda!) Lo m√°s importante es que DataLine y Cloudian tienen colores corporativos similares.  Debes admitir que no podr√≠amos resistirnos a tanta belleza. <br><br><img src="https://habrastorage.org/webt/cj/mh/_u/cjmh_uot3g8v4spn8vgm4fvvngo.png"><br><br>  Desafortunadamente, Cloudian no es el software m√°s com√∫n, y pr√°cticamente no hay informaci√≥n al respecto en RuNet.  Hoy corregiremos esta injusticia y hablaremos con usted sobre las caracter√≠sticas de la arquitectura HyperStore, examinaremos sus componentes m√°s importantes y trataremos los principales matices arquitect√≥nicos.  Comencemos con lo m√°s b√°sico, a saber: ¬øqu√© es Cloudian bajo el cap√≥? <br><br><h1>  C√≥mo funciona Cloudian HyperStore Storage </h1><br>  Echemos un vistazo al diagrama y veamos c√≥mo funciona la soluci√≥n Cloudian. <br><br><img src="https://habrastorage.org/webt/8m/_n/0x/8m_n0xtx0vtlx50himh-wxrkjfm.jpeg"><br>  <i>El esquema de almacenamiento del componente principal.</i> <br><br>  Como podemos ver, el sistema consta de varios componentes principales: <br><br><ul><li>  <b>Cloudian Management Control</b> - <i>consola de gesti√≥n</i> ; </li><li>  <b>Servicio de</b> <i>administraci√≥n: m√≥dulo de administraci√≥n interna</i> ; </li><li>  <b>Servicio S3</b> : el <i>m√≥dulo responsable de soportar el protocolo S3</i> ; </li><li>  <b>Servicio HyperStore</b> : el <i>servicio de almacenamiento real</i> ; </li><li>  <b>Apache Cassandra</b> - un <i>repositorio centralizado de datos de servicio</i> ; </li><li>  <b>Redis</b> : <i>para los datos le√≠dos con m√°s frecuencia</i> . </li></ul><br>  De gran inter√©s para nosotros ser√° el trabajo de los servicios principales, el Servicio S3 y el Servicio HyperStore, luego consideraremos cuidadosamente su trabajo.  Pero primero, tiene sentido averiguar c√≥mo se organiza la distribuci√≥n de servicios en el cl√∫ster y cu√°l es la tolerancia a fallas y la confiabilidad del almacenamiento de datos de esta soluci√≥n en su conjunto. <br><br><img src="https://habrastorage.org/webt/hr/vq/su/hrvqsuhmqgexmgetlc72lsfwhqu.jpeg"><br><br><br>  Por <i>servicios comunes</i> en el diagrama anterior nos referimos a los <b>servicios S3, HyperStore, CMC y Apache Cassandra</b> .  A primera vista, todo es hermoso y ordenado.  Pero despu√©s de un examen m√°s detallado, resulta que solo un fallo de un nodo se resuelve de manera efectiva.  Y la p√©rdida simult√°nea de dos nodos a la vez puede ser fatal para la disponibilidad del cl√∫ster: Redis QoS (en el nodo 2) tiene solo 1 esclavo (en el nodo 3).  La misma imagen con el riesgo de perder la administraci√≥n del cl√∫ster: Puppet Server solo est√° en dos nodos (1 y 2).  Sin embargo, la probabilidad de falla de dos nodos a la vez es muy baja, y puede vivir con ella. <br><br>  Sin embargo, para aumentar la confiabilidad del almacenamiento, utilizamos 4 nodos en la l√≠nea de datos en lugar de los tres m√≠nimos.  Se obtiene la siguiente distribuci√≥n de recursos: <br><br><img src="https://habrastorage.org/webt/x0/ue/f2/x0uef2dubkivpngycrxidcjvx1u.png"><br><br>  Un matiz m√°s llama la atenci√≥n de inmediato: las <b>credenciales de Redis</b> no se colocan en todos los nodos (como podr√≠a suponerse del esquema oficial anterior), sino solo en 3 de ellos.  En este caso, <b>Redis Credentials se</b> usa para cada solicitud entrante.  Resulta que debido a la necesidad de ir al Redis de otra persona, existe un desequilibrio en el rendimiento del cuarto nodo. <br><br>  Para nosotros, esto a√∫n no es significativo.  Durante las pruebas de estr√©s, no se notaron desviaciones significativas en la velocidad de respuesta de los nodos, pero en grandes grupos de docenas de nodos de trabajo, tiene sentido corregir este matiz. <br><br>  As√≠ es como se ve el esquema de migraci√≥n en 6 nodos: <br><br><img src="https://habrastorage.org/webt/yc/df/1l/ycdf1lkqic3oh2rb6iu-yhyoiyo.jpeg"><br><br>  <i>El diagrama muestra c√≥mo se implementa la migraci√≥n del servicio en caso de falla de un nodo.</i>  <i>Solo se tiene en cuenta la falla de un servidor de cada rol.</i>  <i>Si ambos servidores caen, se requerir√° intervenci√≥n manual.</i> <br><br>  Aqu√≠, tambi√©n, el negocio no estuvo exento de algunas sutilezas.  La migraci√≥n de roles utiliza Puppet.  Por lo tanto, si lo pierde o lo rompe accidentalmente, la conmutaci√≥n por error autom√°tica puede no funcionar.  Por el mismo motivo, no debe editar manualmente el manifiesto del Puppet incorporado.  Esto no es del todo seguro, los cambios pueden deshilacharse repentinamente, ya que los manifiestos se editan utilizando el panel de administraci√≥n del cl√∫ster. <br><br>  Desde el punto de vista de la seguridad de los datos, todo es mucho m√°s interesante.  Los metadatos del objeto se almacenan en Apache Cassandra, y cada registro se replica en 3 de los 4 nodos.  El factor de replicaci√≥n 3 tambi√©n se usa para almacenar datos, pero puede configurar uno m√°s grande.  Esto garantiza la seguridad de los datos incluso en caso de falla simult√°nea de 2 de 4 nodos.  Y si tiene tiempo para reequilibrar el cl√∫ster, no puede perder nada con un nodo restante.  Lo principal es tener suficiente espacio. <br><br><img src="https://habrastorage.org/webt/do/oo/ip/doooipch3gctjx0ruteyepc3n2w.jpeg"><br><br>  <i>Esto es lo que sucede cuando fallan dos nodos.</i>  <i>El diagrama muestra claramente que incluso en esta situaci√≥n, los datos permanecen seguros</i> <br><br>  Al mismo tiempo, la disponibilidad de datos y almacenamiento depender√° de la estrategia para garantizar la coherencia.  Para datos, metadatos, lectura y escritura, se configura por separado. <br><br>  Las opciones v√°lidas son al menos un nodo, qu√≥rum o todos los nodos. <br>  Esta configuraci√≥n determina cu√°ntos nodos deben confirmar escritura / lectura para que la solicitud se considere exitosa.  Usamos el qu√≥rum como un compromiso razonable entre el tiempo que lleva procesar una solicitud y la confiabilidad de la escritura / inconsistencia de lectura.  Es decir, de los tres nodos involucrados en la operaci√≥n, para una operaci√≥n libre de errores, es suficiente obtener una respuesta consistente de 2.  En consecuencia, para mantenerse a flote en caso de falla de m√°s de un nodo, deber√° cambiar a una sola estrategia de escritura / lectura. <br><br><h2>  Procesamiento de consultas en Cloudian </h2><br>  A continuaci√≥n consideraremos dos esquemas para procesar solicitudes entrantes en Cloudian HyperStore, PUT y GET.  Esta es la tarea principal para S3 Service e HyperStore. <br><br>  Comencemos con c√≥mo se procesa la solicitud de escritura: <br><br><img src="https://habrastorage.org/webt/ig/ae/g7/igaeg7v86nw9e1rgxt6xbdlhxb8.jpeg"><br><br>  Seguramente not√≥ que cada solicitud genera muchas comprobaciones y recuperaciones de datos, al menos 6 visitas de componente a componente.  Es a partir de aqu√≠ que aparecen retrasos en la grabaci√≥n y un alto consumo de tiempo de CPU al trabajar con archivos peque√±os. <br><br>  Los archivos grandes se transmiten por fragmentos.  Los fragmentos separados no se consideran solicitudes separadas y algunas comprobaciones no se llevan a cabo. <br><br>  El nodo que recibi√≥ la solicitud inicial adem√°s determina independientemente d√≥nde y qu√© escribir, incluso si no est√° escrito directamente en √©l.  Esto le permite ocultar la organizaci√≥n interna del cl√∫ster del cliente final y usar equilibradores de carga externos.  Todo esto afecta positivamente la facilidad de mantenimiento y la tolerancia a fallas del almacenamiento. <br><br><img src="https://habrastorage.org/webt/ss/tr/zj/sstrzjuve-nm6oyz2mj0yitmnts.jpeg"><br><br>  Como puede ver, la l√≥gica de lectura no es muy diferente de la escritura.  En √©l, se observa la misma alta sensibilidad de rendimiento al tama√±o de los objetos procesados.  Por lo tanto, debido a los ahorros significativos en el trabajo con metadatos, es mucho m√°s f√°cil extraer un objeto finamente cortado que muchos objetos separados del mismo volumen total. <br><br><h2>  Almacenamiento de datos y duplicaci√≥n </h2><br>  Como puede ver en los diagramas anteriores, Cloudian admite varios esquemas de almacenamiento y duplicaci√≥n de datos: <br><br>  <b>Replicaci√≥n</b> : mediante la replicaci√≥n, es posible mantener un n√∫mero personalizado de copias de cada objeto de datos en el sistema y almacenar cada copia en diferentes nodos.  Por ejemplo, usando la replicaci√≥n 3X, se crean 3 copias de cada objeto, y cada copia "se encuentra" en su propio nodo. <br><br>  <b>Codificaci√≥n de</b> borrado: con la codificaci√≥n de borrado, cada objeto se codifica en una cantidad personalizada (conocida como n√∫mero K) de fragmentos de datos m√°s una cantidad personalizada de c√≥digo de redundancia (n√∫mero M).  Cada fragmento K + M de un objeto es √∫nico y cada fragmento se almacena en su propio nodo.  Un objeto se puede decodificar usando cualquier fragmento de K.  En otras palabras, el objeto sigue siendo legible, incluso si M nodos son inaccesibles. <br><br>  Por ejemplo, en la codificaci√≥n de borrado, de acuerdo con la f√≥rmula 4 + 2 (4 fragmentos de datos m√°s 2 fragmentos de c√≥digo de redundancia), cada objeto se divide en 6 fragmentos √∫nicos almacenados en seis nodos diferentes, y este objeto puede restaurarse y leerse si hay 4 de 6 fragmentos disponibles. . <br><br>  La ventaja de Erasure Coding en comparaci√≥n con la replicaci√≥n es que ahorra espacio, aunque a costa de un aumento significativo en la carga del procesador, el empeoramiento de la velocidad de respuesta y la necesidad de procedimientos en segundo plano para controlar la consistencia de los objetos.  En cualquier caso, los metadatos se almacenan por separado de los datos (en Apache Cassandra), lo que aumenta la flexibilidad y la fiabilidad de la soluci√≥n. <br><br><h2>  Brevemente sobre otras funciones de HyperStore </h2><br>  Como escrib√≠ al principio de este art√≠culo, varias herramientas √∫tiles est√°n integradas en HyperStore.  Entre ellos est√°n: <br><br><ul><li>  Facturaci√≥n flexible con soporte para cambiar el precio de un recurso seg√∫n el volumen y el plan de tarifas; <br></li><li>  Monitoreo incorporado <br></li><li>  La capacidad de limitar el uso de recursos para usuarios y grupos de usuarios; <br></li><li>  La configuraci√≥n de QoS y los procedimientos integrados para equilibrar el uso de recursos entre nodos, as√≠ como los procedimientos regulares para reequilibrar entre nodos y discos en los nodos o al ingresar nuevos nodos en un cl√∫ster. <br></li></ul><br>  Sin embargo, Cloudian HyperStore todav√≠a no es perfecto.  Por ejemplo, por alguna raz√≥n, no puede transferir una cuenta existente a otro grupo o asignar m√∫ltiples grupos a un registro.  No es posible generar informes de facturaci√≥n provisionales: recibir√° todos los informes solo despu√©s de cerrar el per√≠odo de informe.  Por lo tanto, ni los clientes ni nosotros podemos averiguar cu√°nto ha crecido la cuenta en tiempo real. <br><br><h1>  Cloudian HyperStore Logic </h1><br>  Ahora profundizaremos a√∫n m√°s y veremos lo m√°s interesante en cualquier almacenamiento SDS: la l√≥gica de la distribuci√≥n de objetos por nodos.  En el caso del almacenamiento en Cloudian, los metadatos se almacenan por separado de los datos en s√≠.  Para los metadatos, Cassandra se utiliza, para los datos, la soluci√≥n patentada HyperStore. <br><br>  Desafortunadamente, hasta ahora no hay una traducci√≥n oficial de la documentaci√≥n de Cloudian al ruso en Internet, as√≠ que a continuaci√≥n publicar√© mi traducci√≥n de las partes m√°s interesantes de esta documentaci√≥n. <br><br><h2>  El papel de Apache Cassandra en HyperStore </h2><br>  En HyperStore, Cassandra se usa para almacenar metadatos de objetos, informaci√≥n de cuenta de usuario y datos de uso del servicio.  En una implementaci√≥n t√≠pica en cada HyperStore, los datos de Cassandra se almacenan en la misma unidad que el sistema operativo.  El sistema tambi√©n admite datos Cassandra en una unidad dedicada en cada nodo.  Los datos de Cassandra no se almacenan en discos de datos de HyperStore.  Cuando se asignan vNodes al host, se distribuyen solo a los nodos de almacenamiento HyperStore.  Los vNodos no est√°n asignados a la unidad donde se almacenan los datos de Cassandra. <br>  Dentro del cl√∫ster, los metadatos en Cassandra se replican de acuerdo con la pol√≠tica (estrategia) de su repositorio.  Cassandra Data Replication usa vNodes de esta manera: <br><br><ul><li>  Cuando se crea un nuevo objeto Cassandra (clave primaria y sus valores correspondientes), se convierte en hash y el hash se usa para asociar el objeto con un vNode espec√≠fico.  El sistema verifica a qu√© host est√° asignado este vNode, y luego la primera r√©plica del objeto Cassandra se almacena en la unidad Cassandra en ese host. <br></li><li>  Por ejemplo, suponga que a un host se le asignan 96 vNodos distribuidos en varios discos de datos de HyperStore.  Los objetos Cassandra cuyos valores hash se encuentran dentro de los rangos de token de cualquiera de estos 96 vNodes se escribir√°n en la unidad Cassandra en este host. <br></li><li>  Las r√©plicas adicionales del objeto Cassandra (el n√∫mero de r√©plicas depende de su configuraci√≥n) se asocian con vNodes con el siguiente n√∫mero de secuencia y se almacenan en el nodo al que se asignan estos vNodes, siempre que se omitan vNodes si es necesario, de modo que cada r√©plica del objeto Cassandra se almacene en otro m√°quina host <br></li></ul><br><h2>  C√≥mo funciona el almacenamiento HyperStore </h2><br>  La ubicaci√≥n y la replicaci√≥n de los objetos S3 en un cl√∫ster HyperStore se basa en un esquema de almacenamiento en cach√© consistente que utiliza espacio de token entero en el rango de 0 a 2 <sup>127</sup> -1.  Los tokens enteros se asignan a los nodos HyperStore.  Para cada objeto S3, se calcula un hash a medida que se carga en el almacenamiento.  El objeto se almacena en el nodo al que se le asign√≥ el valor m√°s bajo del token, mayor o igual que el valor hash del objeto.  La replicaci√≥n tambi√©n se implementa almacenando el objeto en los nodos a los que se han asignado tokens, que tienen un valor m√≠nimo. <br><br>  En un almacenamiento basado en hash coherente "cl√°sico", se asigna un token a un nodo f√≠sico.  El sistema Cloudian HyperStore usa y ampl√≠a la funcionalidad del "nodo virtual" (vNode) introducido en Cassandra en la versi√≥n 1.2: se asigna una gran cantidad de tokens a cada host f√≠sico (m√°ximo 256).  De hecho, el cl√∫ster de almacenamiento consta de una gran cantidad de "nodos virtuales" con una gran cantidad de nodos virtuales (tokens) en cada host f√≠sico. <br><br>  El sistema HyperStore asigna un conjunto separado de tokens (nodos virtuales) a cada disco en cada host f√≠sico.  Cada disco en el host es responsable de su propio conjunto de r√©plicas de objetos.  Un fallo de disco solo afecta a las r√©plicas de los objetos que est√°n en √©l.  Otras unidades en el host continuar√°n funcionando y llevando a cabo sus responsabilidades de almacenamiento de datos. <br><br>  Damos un ejemplo y consideramos un cl√∫ster de 6 hosts HyperStore, cada uno de los cuales tiene 4 discos de almacenamiento S3.  Suponga que se asignan 32 tokens a cada host f√≠sico y que hay un espacio de tokens simplificado de 0 a 960, y el valor de 192 tokens en este sistema (6 hosts de 32 tokens) es 0, 5, 10, 15, 20, y as√≠ sucesivamente hasta 955. <br><br>  El siguiente diagrama muestra una posible distribuci√≥n de tokens en todo el cl√∫ster.  32 tokens de cada host se distribuyen uniformemente en 4 discos (8 tokens por disco), y los tokens se distribuyen aleatoriamente en todo el cl√∫ster. <br><br><img src="https://habrastorage.org/webt/wa/w2/9c/waw29ckv34avc3fdmqvfq4-a40a.jpeg"><br><br>  Ahora suponga que configur√≥ HyperStore para replicar 3X objetos S3.  Acordemos que el objeto S3 se carga en el sistema, y ‚Äã‚Äãel algoritmo hash aplicado a su nombre nos da el valor hash 322 (en este espacio hash simplificado).  El siguiente diagrama muestra c√≥mo se almacenar√°n tres instancias o r√©plicas de un objeto en un cl√∫ster: <br><br><ul><li>  Con su valor hash de nombre 322, la primera r√©plica del objeto se almacena en el token 325, porque  Este es el valor de token m√°s peque√±o que es mayor o igual que el valor hash del objeto.  Se asignan 325 tokens (resaltados en rojo en el diagrama) a hyperstore2: Disk2.  En consecuencia, la primera r√©plica del objeto se almacena all√≠. <br></li></ul><br><ul><li>  La segunda r√©plica se almacena en el disco al que se le asigna el siguiente token (330, resaltado en naranja), es decir, en hyperstore4: Disk2. <br></li><li>  La tercera r√©plica se guarda en el disco, al que se le asigna el siguiente token despu√©s de 330 - 335 (amarillo), en hyperstore3: Disk3. <br></li></ul><br><img src="https://habrastorage.org/webt/xy/0w/k-/xy0wk-hrqt3lgppbyeohdpllsnq.jpeg"><br><blockquote>  <b>Agregar√© un comentario:</b> desde un punto de vista pr√°ctico, esta optimizaci√≥n (la distribuci√≥n de tokens no solo entre nodos f√≠sicos, sino tambi√©n entre discos individuales) es necesaria no solo para garantizar la accesibilidad, sino tambi√©n para garantizar una distribuci√≥n uniforme de los datos entre los discos.  En este caso, la matriz RAID no se utiliza, toda la l√≥gica de asignaci√≥n de datos en los discos est√° controlada por el propio HyperStore.  Por un lado, es conveniente y controlado; si se pierde un disco, todo se reequilibrar√° por s√≠ solo.  Por otro lado, personalmente conf√≠o en m√°s controladores RAID buenos, despu√©s de todo, su l√≥gica se ha optimizado durante muchos a√±os.  Pero estas son todas mis preferencias personales, en jambas y problemas reales, nunca descubrimos HyperStore, si seguimos las recomendaciones del proveedor al instalar el software en servidores f√≠sicos.  Pero el intento de usar la virtualizaci√≥n y los discos virtuales en la parte superior de la misma luna en el sistema de almacenamiento fall√≥, cuando sobrecarg√≥ el sistema de almacenamiento durante las pruebas de carga, HyperStore se volvi√≥ loco y dispers√≥ los datos de manera completamente desigual, obstruyendo algunos discos y sin tocar otros. </blockquote><h2>  Dispositivo de accionamiento dentro de un cl√∫ster </h2><br>  Recuerde que cada host tiene 32 tokens, y los tokens de cada host se distribuyen uniformemente entre sus discos.  Echemos un vistazo m√°s de cerca a hyperstore2: Disk2 (en el diagrama a continuaci√≥n).  Vemos que los tokens 325, 425, 370 y dem√°s est√°n asignados a este disco. <br><br>  Dado que el cl√∫ster est√° configurado para la replicaci√≥n 3X, lo siguiente se almacenar√° en hyperstore2: Disco2: <br><br>  De acuerdo con el token de disco 325: <br><ul><li>  Las primeras r√©plicas de objetos con un valor hash de 320 (exclusivamente) a 325 (inclusive); </li><li>  Segundas r√©plicas de objetos con un valor hash de 315 (exclusivamente) a 320 (inclusive); </li><li>  Terceras r√©plicas de objetos con un valor hash de 310 (exclusivamente) a 315 (inclusive). </li></ul><br>  Seg√∫n el token de disco 425: <br><ul><li>  Las primeras r√©plicas de objetos con un valor hash de 420 (exclusivamente) a 425 (inclusive); </li><li>  Segundas r√©plicas de objetos con un valor hash de 415 (exclusivamente) a 420 (inclusive); </li><li>  Terceras r√©plicas de objetos con un valor hash de 410 (exclusivamente) a 415 (inclusive). </li></ul><br>  Y as√≠ sucesivamente. <br><br>    ,       HyperStore      ,           .    hyperstore2:disk2            . <br><br><img src="https://habrastorage.org/webt/hl/rf/dq/hlrfdqwtqhiuwvdrrplv0mxt8dk.jpeg"><br><br>    2   1, 3  4   ,     2   , ..     . <br><blockquote> <b>:</b>  ,   /     HyperStore         Cassandra.  ,    ,       ,     ,   ¬´¬ª  .          .                     .   ,      :        ,    ,     . </blockquote><h2>      </h2><br>   ,   HyperStore       .    -         .      .           ()     ,    . <br>  ,   ,     ,  <b>¬´Multi-Data Center Deployments¬ª.</b> <br><br>   HyperStore   -.   DC1  DC2.   -   3  .      ,      ,    32  (vNodes),        0  960.      -,     192  ‚Äî  32     6  .      . <br><br>  ,    S3          -. <br><br>  ,    S3    942    2 -: <br><br><ul><li>     vNode 945 (     ),    DC2,  hyperstore5:Disk3. <br></li><li>     vNode 950 (  ) DC2,  hyperstore6:Disk4. <br></li><li>  vNode 955   DC2,      ,   vNode . <br></li><li>     vNode 0 () ‚Äî  DC1, hyperstore2:Disk3.  ,       (955)       (0). <br></li><li>  vNode (5)   DC2,      ,   vNode . <br></li><li>       vNode 10 () ‚Äî  DC1, hyperstore3:Disk3. <br></li></ul><br><img src="https://habrastorage.org/webt/uu/vl/ak/uuvlakjhabsho_u_swahrrf8cz4.png"><br><blockquote> <b>:</b>        ,      ,   ,  ,           .     ,      . </blockquote>           Cloudian.   ,      ,            . ,    ,   ,            ,       . <br>       S3   DataLine,         ,          ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es423853/">https://habr.com/ru/post/es423853/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es423839/index.html">Prueba sha256 por toothOK para red neuronal</a></li>
<li><a href="../es423843/index.html">Si est√°s en Kazan o Novosibirsk y quieres dise√±ar microchips, como en Cupertino</a></li>
<li><a href="../es423845/index.html">Cond√≥n corporativo</a></li>
<li><a href="../es423847/index.html">Reconocimiento de color y luz con APDS-9960</a></li>
<li><a href="../es423851/index.html">Presentamos el nuevo complemento de Grafana: panel de mapa de estado</a></li>
<li><a href="../es423855/index.html">Nebulosa Zyxel: facilidad de administraci√≥n como base para el ahorro</a></li>
<li><a href="../es423857/index.html">6 desaf√≠os que encontrar√°s al aprender a programar</a></li>
<li><a href="../es423861/index.html">Linternas solares: necesitamos m√°s luz</a></li>
<li><a href="../es423863/index.html">Confrontaci√≥n en PHDays 8 - Vista SOC</a></li>
<li><a href="../es423865/index.html">Roskomnadzor inform√≥ p√∫blicamente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>