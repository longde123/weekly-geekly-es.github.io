<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÄ ‚úèÔ∏è üë©üèæ‚Äçüéì Unsere H√§nde sind nicht f√ºr Langeweile: die Wiederherstellung des Turm-Clusters in K8s ü¶Ä üíáüèø üëåüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir haben bereits dar√ºber gesprochen, wie / warum wir Rook m√∂gen: Es vereinfacht in erheblichem Ma√üe die Arbeit mit Speichern in Kubernetes-Clustern. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Unsere H√§nde sind nicht f√ºr Langeweile: die Wiederherstellung des Turm-Clusters in K8s</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/477680/"><img src="https://habrastorage.org/webt/x_/rb/jm/x_rbjmnxk6egjidxf_fvk1otkqe.png"><br><br>  Wir haben <a href="https://habr.com/ru/company/flant/blog/451818/">bereits dar√ºber gesprochen,</a> wie / warum wir Rook m√∂gen: Es vereinfacht in erheblichem Ma√üe die Arbeit mit Speichern in Kubernetes-Clustern.  Mit dieser Einfachheit sind jedoch gewisse Schwierigkeiten verbunden.  Wir hoffen, dass das neue Material hilft, solche Schwierigkeiten besser zu verstehen, noch bevor sie sich manifestieren. <br><br>  Und um es interessanter zu lesen, beginnen wir mit den <i>Konsequenzen eines</i> hypothetischen Problems im Cluster. <a name="habracut"></a><br><br><h2>  "Alles ist weg!" </h2><br>  Stellen Sie sich vor, Sie haben Rook einmal in Ihrem K8-Cluster konfiguriert und gestartet. Er war mit seiner Arbeit zufrieden, aber in einem ‚Äûwunderbaren‚Äú Moment passiert Folgendes: <br><br><ul><li>  Neue Pods k√∂nnen keine Ceph-RBDs mounten. </li><li>  Befehle wie <code>lsblk</code> und <code>df</code> funktionieren nicht auf Kubernetes-Hosts.  Dies bedeutet automatisch, dass mit den knotenmontierten RBD-Images etwas nicht stimmt.  Ich kann sie nicht lesen, was auf die Unzug√§nglichkeit von Monitoren hinweist ... </li><li>  Ja, der Cluster enth√§lt keine Betriebsmonitore.  Dar√ºber hinaus gibt es weder Pods mit OSD noch Pods mit MGR. </li></ul><br>  Wann wurde der Pod <code>rook-ceph-operator</code> den <code>rook-ceph-operator</code> ?  Vor nicht allzu langer Zeit wurde er eingesetzt.  Warum?  Rook-Operator hat beschlossen, einen neuen Cluster zu erstellen ... Wie k√∂nnen wir nun den Cluster und die darin enthaltenen Daten wiederherstellen? <br><br>  Lassen Sie uns zun√§chst einen <s>l√§ngeren,</s> interessanten Weg beschreiten, nachdem wir die ‚ÄûInterna‚Äú von Rook gr√ºndlich untersucht und seine Komponenten schrittweise restauriert haben.  Nat√ºrlich gibt es einen <s>k√ºrzeren</s> richtigen Weg: das Verwenden von Backups.  Wie Sie wissen, gibt es zwei Arten von Administratoren: diejenigen, die keine Backups durchf√ºhren, und diejenigen, die sie bereits durchf√ºhren ... Aber mehr dazu nach den Ermittlungen. <br><br><h2>  Ein bisschen √úbung oder ein langer Weg </h2><br><h3>  Schauen Sie sich die Monitore an und stellen Sie sie wieder her </h3><br>  Schauen wir uns also die Liste der ConfigMaps an: F√ºr die Sicherung sind <code>rook-ceph-config</code> und <code>rook-config-override</code> erforderlich.  Sie werden nach erfolgreicher Bereitstellung des Clusters angezeigt. <br><br>  <i><b>Hinweis</b> : In neuen Versionen sind ConfigMaps nach der Einf√ºhrung <a href="https://github.com/rook/rook/pull/3573">dieses PR</a> kein Indikator f√ºr den Erfolg einer Clusterbereitstellung mehr.</i> <br><br>  Um weitere Aktionen ausf√ºhren zu k√∂nnen, m√ºssen alle Server, auf denen gemountete RBD-Images vorhanden sind, neu <code>ls /dev/rbd*</code> werden ( <code>ls /dev/rbd*</code> ).  Dies muss √ºber sysrq (oder "zu Fu√ü" zum Rechenzentrum) erfolgen.  Diese Anforderung wird durch die Aufgabe verursacht, gemountete RBDs zu trennen, f√ºr die ein regul√§rer Neustart nicht funktioniert (es wird erfolglos versucht, sie normal zu mounten). <br><br>  Das Theater beginnt mit einem Kleiderb√ºgel und der Ceph-Cluster beginnt mit Monitoren.  Schauen wir sie uns an. <br><br>  Rook stellt die folgenden Entit√§ten im Monitor-Pod bereit: <br><br><pre> <code class="plaintext hljs">Volumes: rook-ceph-config: Type: ConfigMap (a volume populated by a ConfigMap) Name: rook-ceph-config rook-ceph-mons-keyring: Type: Secret (a volume populated by a Secret) SecretName: rook-ceph-mons-keyring rook-ceph-log: Type: HostPath (bare host directory volume) Path: /var/lib/rook/kube-rook/log ceph-daemon-data: Type: HostPath (bare host directory volume) Path: /var/lib/rook/mon-a/data Mounts: /etc/ceph from rook-ceph-config (ro) /etc/ceph/keyring-store/ from rook-ceph-mons-keyring (ro) /var/lib/ceph/mon/ceph-a from ceph-daemon-data (rw) /var/log/ceph from rook-ceph-log (rw)</code> </pre> <br>  Mal sehen, was das Geheimnis von <code>rook-ceph-mons-keyring</code> : <br><br><pre> <code class="plaintext hljs">kind: Secret data: keyring: LongBase64EncodedString=</code> </pre> <br>  Wir entschl√ºsseln und erhalten den √ºblichen Schl√ºsselring mit Rechten f√ºr den Administrator und die Monitore: <br><br><pre> <code class="plaintext hljs">[mon.] key = AQAhT19dlUz0LhBBINv5M5G4YyBswyU43RsLxA== caps mon = "allow *" [client.admin] key = AQAhT19d9MMEMRGG+wxIwDqWO1aZiZGcGlSMKp== caps mds = "allow *" caps mon = "allow *" caps osd = "allow *" caps mgr = "allow *"</code> </pre> <br>  Erinnere dich.  Schauen Sie sich nun den Schl√ºsselring im geheimen <code>rook-ceph-admin-keyring</code> : <br><br><pre> <code class="plaintext hljs">kind: Secret data: keyring: anotherBase64EncodedString=</code> </pre> <br>  Was ist drin? <br><br><pre> <code class="plaintext hljs">[client.admin] key = AQAhT19d9MMEMRGG+wxIwDqWO1aZiZGcGlSMKp== caps mds = "allow *" caps mon = "allow *" caps osd = "allow *" caps mgr = "allow *"</code> </pre> <br>  Derselbe.  Mal sehen, mehr ... Hier ist zum Beispiel das Geheimnis von <code>rook-ceph-mgr-a-keyring</code> : <br><br><pre> <code class="plaintext hljs">[mgr.a] key = AQBZR19dbVeaIhBBXFYyxGyusGf8x1bNQunuew== caps mon = "allow *" caps mds = "allow *" caps osd = "allow *"</code> </pre> <br>  Am Ende finden wir ein paar weitere Geheimnisse in ConfigMap rook <code>rook-ceph-mon</code> : <br><br><pre> <code class="plaintext hljs">kind: Secret data: admin-secret: AQAhT19d9MMEMRGG+wxIwDqWO1aZiZGcGlSMKp== cluster-name: a3ViZS1yb29r fsid: ZmZiYjliZDMtODRkOS00ZDk1LTczNTItYWY4MzZhOGJkNDJhCg== mon-secret: AQAhT19dlUz0LhBBINv5M5G4YyBswyU43RsLxA==</code> </pre> <br>  Und dies ist die erste Liste mit Schl√ºsselbund, aus der alle oben beschriebenen Geheimnisse stammen. <br><br>  Wie Sie wissen (siehe <code>dataDirHostPath</code> in der <a href="https://rook.github.io/docs/rook/master/ceph-cluster-crd.html">Dokumentation</a> ), speichert Rook diese Daten an zwei Stellen.  Gehen wir daher zu den Knoten, um den Schl√ºsselbund zu betrachten, der in den Verzeichnissen liegt, die in Pods mit Monitoren und OSD eingebunden sind.  Suchen Sie dazu die Knoten <code>/var/lib/rook/mon-a/data/keyring</code> und sehen Sie: <br><br><pre> <code class="plaintext hljs"># cat /var/lib/rook/mon-a/data/keyring [mon.] key = AXAbS19d8NNUXOBB+XyYwXqXI1asIzGcGlzMGg== caps mon = "allow *"</code> </pre> <br>  <b>Pl√∂tzlich stellte</b> sich heraus, dass <b>das</b> Geheimnis anders war - nicht wie in ConfigMap. <br><br>  Was ist mit dem Admin-Schl√ºsselbund?  Wir haben es auch: <br><br><pre> <code class="plaintext hljs"># cat /var/lib/rook/kube-rook/client.admin.keyring [client.admin] key = AXAbR19d8GGSMUBN+FyYwEqGI1aZizGcJlHMLgx= caps mds = "allow *" caps mon = "allow *" caps osd = "allow *" caps mgr = "allow *"</code> </pre> <br>  Hier liegt das Problem.  Es ist ein Fehler aufgetreten: Der Cluster wurde neu erstellt ... aber in Wirklichkeit nicht. <br><br>  Es wird deutlich, dass der neu generierte Schl√ºsselring in Geheimnissen gespeichert ist und <b>nicht</b> aus unserem alten Cluster stammt.  Deshalb: <br><br><ul><li>  wir nehmen den Schl√ºsselring vom Monitor aus der Datei <code>/var/lib/rook/mon-a/data/keyring</code> (oder aus dem Backup); </li><li>  √§ndere den Schl√ºsselring im geheimen <code>rook-ceph-mons-keyring</code> . </li><li>  Registrieren Sie den Schl√ºsselbund vom Admin und Monitor in ConfigMap'e rook <code>rook-ceph-mon</code> . </li><li>  Entfernen Sie Pod-Controller mit Monitoren. </li></ul><br>  Das Wunder wird nicht lange dauern: Monitore erscheinen und starten.  Hurra, ein Anfang! <br><br><h3>  OSD wiederherstellen </h3><br>  Wir gehen in den Pod- <code>rook-operator</code> : Der Aufruf von <code>ceph mon dump</code> zeigt an, dass alle Monitore vorhanden sind, und <code>ceph -s</code> dass sie sich in einem Quorum befinden.  Wenn Sie sich jedoch den OSD-Baum ( <code>ceph osd tree</code> ) <code>ceph osd tree</code> , werden Sie etwas Seltsames darin sehen: <code>ceph osd tree</code> , aber sie sind leer.  Es stellt sich heraus, dass sie auch irgendwie restauriert werden m√ºssen.  Aber wie? <br><br>  In der Zwischenzeit wurden in <code>rook-ceph-config</code> und <code>rook-ceph-config</code> <code>rook-config-override</code> sowie viele andere ConfigMaps mit Namen der Form <code>rook-ceph-osd-$nodename-config</code> angezeigt.  Schauen wir sie uns an: <br><br><pre> <code class="plaintext hljs">kind: ConfigMap data: osd-dirs: '{"/mnt/osd1":16,"/mnt/osd2":18}'</code> </pre> <br>  Alles ist falsch, alles ist durcheinander! <br><br>  Skalieren Sie den Operator-Pod auf Null, l√∂schen Sie die generierten Bereitstellungs-Pods aus dem OSD und korrigieren Sie diese ConfigMaps.  Aber wo bekommt man die <b>richtige</b> OSD-Karte nach Knoten? <br><br><ul><li>  Versuchen wir noch einmal, die <code>/mnt/osd[1-2]</code> auf den Knoten zu <code>/mnt/osd[1-2]</code> - in der Hoffnung, dass wir dort etwas fassen k√∂nnen. </li><li>  Es gibt 2 Unterverzeichnisse im Verzeichnis <code>/mnt/osd1</code> : <code>osd0</code> und <code>osd16</code> .  Das letzte ist nur die ID, die in ConfigMap (16) angegeben ist? </li><li>  Lassen Sie uns <code>osd0</code> Gr√∂√üe <code>osd0</code> und <code>osd0</code> dass <code>osd0</code> viel gr√∂√üer als <code>osd16</code> . </li></ul><br>  Wir schlie√üen daraus, dass <code>osd0</code> das erforderliche OSD ist, das in ConfigMap als <code>/mnt/osd1</code> angegeben wurde (da wir verzeichnisbasiertes <a href="https://github.com/rook/rook/issues/3379">osd verwenden</a> ). <br><br>  Schritt f√ºr Schritt √ºberpr√ºfen wir alle Knoten und bearbeiten ConfigMaps.  Nach all den Anweisungen k√∂nnen Sie den Pod des Rook-Bedieners ausf√ºhren und dessen Protokolle lesen.  Und in ihnen ist alles wunderbar: <br><br><ul><li>  Ich bin ein Clusterbetreiber. </li><li>  Ich habe Laufwerke auf Knoten gefunden. </li><li>  Ich habe Monitore gefunden; </li><li>  Monitore wurden Freunde, d.h.  ein Kollegium gebildet; </li><li>  Ausf√ºhren von OSD-Bereitstellungen ... </li></ul><br>  Kehren wir zum Pod des Rook-Operators zur√ºck und √ºberpr√ºfen die Clusterlebensdauer ... Ja, wir haben einen kleinen Fehler bei den Schlussfolgerungen zu den OSD-Namen auf einigen Knoten gemacht!  Es spielt keine Rolle: Sie haben ConfigMaps erneut korrigiert, die zus√§tzlichen Verzeichnisse aus den neuen OSDs gel√∂scht und sind in den lang ersehnten Zustand von <code>HEALTH_OK</code> ! <br><br>  √úberpr√ºfen Sie die Bilder im Pool: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># rbd ls -p kube pvc-9cfa2a98-b878-437e-8d57-acb26c7118fb pvc-9fcc4308-0343-434c-a65f-9fd181ab103e pvc-a6466fea-bded-4ac7-8935-7c347cff0d43 pvc-b284d098-f0fc-420c-8ef1-7d60e330af67 pvc-b6d02124-143d-4ce3-810f-3326cfa180ae pvc-c0800871-0749-40ab-8545-b900b83eeee9 pvc-c274dbe9-1566-4a33-bada-aabeb4c76c32 ‚Ä¶</span></span></code> </pre> <br>  Alles ist an Ort und Stelle - der Cluster wird gespeichert! <br><br><h2>  Ich bin <s>faul</s> Backups zu machen, oder der schnelle Weg </h2><br>  Wenn f√ºr Rook Sicherungen durchgef√ºhrt wurden, wird das Wiederherstellungsverfahren viel einfacher und l√§uft auf Folgendes hinaus: <br><br><ol><li>  Bereitstellung des Rook-Operators auf Null skalieren; </li><li>  Wir entfernen alle Bereitstellungen mit Ausnahme des Rook-Operators. </li><li>  Wir stellen alle Geheimnisse und ConfigMaps aus dem Backup wieder her. </li><li>  Stellen Sie den Inhalt der <code>/var/lib/rook/mon-*</code> auf den Knoten wieder her. </li><li>  Wiederherstellen (falls pl√∂tzlich verloren) CRD <code>CephCluster</code> , <code>CephFilesystem</code> , <code>CephBlockPool</code> , <code>CephNFS</code> , <code>CephObjectStore</code> ; </li><li>  Skalieren Sie die Bereitstellung des Rook-Operators auf 1 zur√ºck. </li></ol><br><h2>  Hilfreiche Ratschl√§ge </h2><br>  Machen Sie Backups! <br><br>  Und um Situationen zu vermeiden, in denen Sie sich davon erholen m√ºssen: <br><br><ol><li>  Skalieren Sie den Rook-Operator auf Null, damit er nicht zu viel bewirkt, bevor Sie in gro√üem Umfang mit dem Cluster arbeiten, der aus Serverneustarts besteht. </li><li>  <a href="">F√ºgen</a> Sie auf Monitoren <a href="">vorher nodeAffinity hinzu</a> . </li><li>  <code>ROOK_MON_HEALTHCHECK_INTERVAL</code> <code>ROOK_MON_OUT_TIMEOUT</code> <a href="">der Timeouts</a> <code>ROOK_MON_HEALTHCHECK_INTERVAL</code> und <code>ROOK_MON_OUT_TIMEOUT</code> . </li></ol><br><h2>  Anstelle einer Schlussfolgerung </h2><br>  Es macht keinen Sinn zu behaupten, dass Rook als zus√§tzliche "Schicht" (im allgemeinen Schema der Speicherorganisation in Kubernetes) sowohl vereinfacht als auch neue Schwierigkeiten und potenzielle Probleme in der Infrastruktur hinzuf√ºgt.  Die Sache bleibt ‚Äûklein‚Äú: Eine ausgewogene und fundierte Wahl zwischen diesen Risiken einerseits und den Vorteilen, die die L√∂sung in Ihrem speziellen Fall bringt, andererseits zu treffen. <br><br>  √úbrigens wurde k√ºrzlich der Abschnitt ‚ÄûEinen vorhandenen Rook Ceph-Cluster in einen neuen Kubernetes-Cluster √ºbernehmen‚Äú <a href="https://github.com/rook/rook/commit/b651239d3f9a793c95b5c06668b7f28771254082">in die</a> Rook-Dokumentation aufgenommen.  Es wird detaillierter beschrieben, was zu tun ist, um vorhandene Daten in einen neuen Kubernetes-Cluster zu verschieben oder einen Cluster wiederherzustellen, der aus dem einen oder anderen Grund zusammengebrochen ist. <br><br><h2>  PS </h2><br>  Lesen Sie auch in unserem Blog: <br><br><ul><li>  " <a href="https://habr.com/ru/company/flant/blog/451818/">Turm oder nicht Turm - das ist die Frage</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/348044/">Rook ist ein" Self-Service "-Datenlager f√ºr Kubernetes</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/474208/">Longhorn, das verteilte Lager von Rancher f√ºr K8s, wurde an CNCF √ºbergeben</a> ." </li><li>  " <a href="https://habr.com/ru/company/flant/blog/329666/">Erstellen von persistentem Speicher mit der Bereitstellung in Kubernetes basierend auf Ceph</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de477680/">https://habr.com/ru/post/de477680/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de477668/index.html">29. November, 18 Uhr - devleads-mitap</a></li>
<li><a href="../de477670/index.html">Was gibt Testautomatisierung</a></li>
<li><a href="../de477672/index.html">Rechte und Pflichten der Teammitglieder: rechtliche und kulturelle Aspekte</a></li>
<li><a href="../de477674/index.html">Bedeutet KI Liebe?</a></li>
<li><a href="../de477678/index.html">Perspektiven f√ºr das digitale Fernsehen in Russland</a></li>
<li><a href="../de477682/index.html">Legacy-Services in Ihrer Infrastruktur</a></li>
<li><a href="../de477684/index.html">Angular: Der beste Begleiter f√ºr interaktive Apps</a></li>
<li><a href="../de477686/index.html">Unsere auf der AI Journey Konferenz</a></li>
<li><a href="../de477688/index.html">Dezember IT Events Digest</a></li>
<li><a href="../de477692/index.html">Erfahrung mit ZGC und Shenandoah GC in der Produktion</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>