<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîµ üòï üåì Nomad: Probleme und L√∂sungen üë©‚Äçüè≠ üê´ ü•ö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Der erste Dienst in Nomad I wurde im September 2016 gestartet. Im Moment benutze ich es als Programmierer und unterst√ºtze als Administrator von zwei N...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nomad: Probleme und L√∂sungen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435132/"><p>  Der erste Dienst in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nomad</a> I wurde im September 2016 gestartet.  Im Moment benutze ich es als Programmierer und unterst√ºtze als Administrator von zwei Nomad-Clustern - ein "Zuhause" f√ºr meine pers√∂nlichen Projekte (6 mikro-virtuelle Maschinen in Hetzner Cloud und ArubaCloud in 5 verschiedenen Rechenzentren in Europa) und das zweite funktionierende (ungef√§hr 40 private virtuelle und physische Server) in zwei Rechenzentren). </p><br><p>  In der letzten Zeit wurde viel Erfahrung mit der Nomad-Umgebung gesammelt. In dem Artikel werde ich die Probleme beschreiben, auf die Nomad st√∂√üt, und wie man damit umgeht. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Yamal Nomad erstellt eine Continuous Delivery-Instanz Ihrer Software ¬© National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. Die Anzahl der Serverknoten pro Rechenzentrum </h2><br><p>  <strong>L√∂sung: Ein Serverknoten reicht f√ºr ein Rechenzentrum.</strong> </p><br><p>  In der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> wird nicht explizit angegeben, wie viele Serverknoten in einem Rechenzentrum erforderlich sind.  Es wird nur angegeben, dass 3-5 Knoten pro Region ben√∂tigt werden, was f√ºr den Raft-Protokoll-Konsens logisch ist. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  Am Anfang plante ich 2-3 Serverknoten in jedem Rechenzentrum, um Redundanz zu gew√§hrleisten. </p><br><p>  Bei Gebrauch stellte sich heraus: </p><br><ol><li>  Dies ist einfach nicht erforderlich, da bei einem Knotenausfall im Rechenzentrum die Rolle des Serverknotens f√ºr die Agenten in diesem Rechenzentrum von anderen Serverknoten in der Region gespielt wird. </li><li>  Es wird noch schlimmer, wenn Problem 8 nicht gel√∂st ist.  Wenn der Assistent wiedergew√§hlt wird, k√∂nnen Inkonsistenzen auftreten und Nomad startet einen Teil der Dienste neu. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Serverressourcen f√ºr den Serverknoten </h2><br><p>  <strong>L√∂sung: Eine kleine virtuelle Maschine reicht f√ºr den Serverknoten.</strong>  <strong>Auf demselben Server d√ºrfen andere nicht ressourcenintensive Dienste ausgef√ºhrt werden.</strong> </p><br><p>  Der Speicherverbrauch des Nomad-Daemons h√§ngt von der Anzahl der ausgef√ºhrten Aufgaben ab.  CPU-Verbrauch - basierend auf der Anzahl der Aufgaben und der Anzahl der Server / Agenten in der Region (nicht linear). </p><br><p>  In unserem Fall: F√ºr 300 ausgef√ºhrte Aufgaben betr√§gt der Speicherverbrauch f√ºr den aktuellen Masterknoten ca. 500 MB. </p><br><p>  In einem funktionierenden Cluster eine virtuelle Maschine f√ºr einen Serverknoten: 4 CPU, 6 GB RAM. <br>  Zus√§tzlich gestartet: Consul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Konsens √ºber den Mangel an Rechenzentren </h2><br><p>  <strong>L√∂sung: Wir erstellen drei virtuelle Rechenzentren und drei Serverknoten f√ºr zwei physische Rechenzentren.</strong> </p><br><p>  Die Arbeit von Nomad in der Region basiert auf dem Flo√üprotokoll.  F√ºr einen korrekten Betrieb ben√∂tigen Sie mindestens 3 Serverknoten in verschiedenen Rechenzentren.  Dies erm√∂glicht einen korrekten Betrieb mit einem vollst√§ndigen Verlust der Netzwerkkonnektivit√§t mit einem der Rechenzentren. </p><br><p>  Wir haben aber nur zwei Rechenzentren.  Wir machen einen Kompromiss: Wir w√§hlen ein Rechenzentrum aus, dem wir mehr vertrauen, und machen darin einen zus√§tzlichen Serverknoten.  Dazu f√ºhren wir ein zus√§tzliches virtuelles Rechenzentrum ein, das sich physisch im selben Rechenzentrum befindet (siehe Unterabsatz 2 von Problem 1). </p><br><p>  <strong>Alternative L√∂sung: Wir unterteilen Rechenzentren in separate Regionen.</strong> </p><br><p>  Infolgedessen funktionieren Rechenzentren unabh√§ngig voneinander, und ein Konsens ist nur innerhalb eines Rechenzentrums erforderlich.  In einem Rechenzentrum ist es in diesem Fall besser, drei Serverknoten zu erstellen, indem drei virtuelle Rechenzentren in einem physischen implementiert werden. </p><br><p>  Diese Option ist f√ºr die Aufgabenverteilung weniger praktisch, bietet jedoch eine 100% ige Garantie f√ºr die Unabh√§ngigkeit der Dienste bei Netzwerkproblemen zwischen Rechenzentren. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Server" und "Agent" auf demselben Server </h2><br><p>  <strong>L√∂sung: G√ºltig, wenn Sie eine begrenzte Anzahl von Servern haben.</strong> </p><br><p>  Laut Nomadendokumentation ist dies unerw√ºnscht.  Wenn Sie jedoch nicht die M√∂glichkeit haben, separate virtuelle Maschinen f√ºr Serverknoten zuzuweisen, k√∂nnen Sie die Server- und Agentenknoten auf demselben Server platzieren. </p><br><p>  Gleichzeitiges Ausf√ºhren bedeutet, dass der Nomad-Daemon sowohl im Client- als auch im Servermodus gestartet wird. </p><br><p>  Was bedroht das?  Bei einer hohen Belastung der CPU dieses Servers funktioniert der Nomad-Serverknoten instabil, es kann zu einem Verlust des Konsenses und des Herzschlags sowie zum erneuten Laden von Diensten kommen. <br>  Um dies zu vermeiden, erh√∂hen wir die Grenzen aus der Beschreibung von Problem Nr. 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Implementierung von Namespaces </h2><br><p>  <strong>L√∂sung: M√∂glicherweise durch die Organisation eines virtuellen Rechenzentrums.</strong> </p><br><p>  Manchmal m√ºssen Sie einen Teil der Dienste auf separaten Servern ausf√ºhren. </p><br><p>  Die L√∂sung ist die erste, einfache, aber anspruchsvollere.  Wir unterteilen alle Dienste nach ihrem Zweck in Gruppen: Frontend, Backend, ... F√ºgen Sie den Servern Metaattribute hinzu und schreiben Sie die Attribute vor, die f√ºr alle Dienste ausgef√ºhrt werden sollen. </p><br><p>  Die zweite L√∂sung ist einfach.  Wir f√ºgen neue Server hinzu, schreiben ihnen Metaattribute vor, schreiben diese Startattribute den erforderlichen Diensten vor, alle anderen Dienste schreiben ein Startverbot f√ºr Server mit diesem Attribut vor. </p><br><p>  Die dritte L√∂sung ist kompliziert.  Wir erstellen ein virtuelles Rechenzentrum: Starten Sie Consul f√ºr ein neues Rechenzentrum, starten Sie den Nomad-Serverknoten f√ºr dieses Rechenzentrum, ohne die Anzahl der Serverknoten f√ºr diese Region zu vergessen.  Jetzt k√∂nnen Sie einzelne Dienste in diesem dedizierten virtuellen Rechenzentrum ausf√ºhren. </p><br><h2 id="6-integraciya-s-vault">  6. Integration mit Vault </h2><br><p>  <strong>L√∂sung: Vermeiden Sie zirkul√§re Abh√§ngigkeiten von Nomad &lt;-&gt; Vault.</strong> </p><br><p>  Der gestartete Tresor sollte keine Abh√§ngigkeiten von Nomad haben.  Die in Nomad registrierte Vault-Adresse sollte vorzugsweise direkt auf den Vault verweisen, ohne Schichten von Balancern (aber g√ºltig).  Die Tresorreservierung kann in diesem Fall √ºber DNS - Consul DNS oder extern erfolgen. </p><br><p>  Wenn Vault-Daten in die Nomad-Konfigurationsdateien geschrieben sind, versucht Nomad beim Start, auf Vault zuzugreifen.  Wenn der Zugriff nicht erfolgreich ist, weigert sich Nomad zu starten. </p><br><p>  Ich habe vor langer Zeit einen Fehler mit einer zyklischen Abh√§ngigkeit gemacht, der den Nomad-Cluster einmal kurzzeitig fast vollst√§ndig zerst√∂rt hat.  Vault wurde unabh√§ngig von Nomad korrekt gestartet, aber Nomad √ºberpr√ºfte die Vault-Adresse √ºber die Balancer, die in Nomad selbst ausgef√ºhrt wurden.  Die Neukonfiguration und der Neustart der Nomad-Serverknoten f√ºhrten zu einem Neustart der Balancer-Dienste, was dazu f√ºhrte, dass die Serverknoten selbst nicht gestartet werden konnten. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Starten wichtiger staatlicher Dienste </h2><br><p>  <strong>L√∂sung: g√ºltig, aber ich nicht.</strong> </p><br><p>  Ist es m√∂glich, PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB √ºber Nomad auszuf√ºhren? </p><br><p>  Stellen Sie sich vor, Sie haben eine Reihe wichtiger Dienste, deren Arbeit mit den meisten anderen Diensten verbunden ist.  Zum Beispiel eine Datenbank in PostgreSQL / ClickHouse.  Oder allgemeine Kurzzeitspeicherung in Redis Cluster / MongoDB.  Oder ein Datenbus in Redis Cluster / RabbitMQ. </p><br><p>  Alle diese Dienste implementieren in irgendeiner Form ein fehlertolerantes Schema: Stolon / Patroni f√ºr PostgreSQL, eine eigene Raft-Implementierung in Redis Cluster, eine eigene Cluster-Implementierung in RabbitMQ, MongoDB, ClickHouse. </p><br><p>  Ja, alle diese Dienste k√∂nnen √ºber Nomad unter Bezugnahme auf bestimmte Server gestartet werden. Aber warum? </p><br><p>  Plus - einfacher Start, ein einziges Skriptformat, wie bei anderen Diensten.  Sie m√ºssen sich keine Sorgen um ansible Skripte / irgendetwas anderes machen. </p><br><p>  Minus ist ein zus√§tzlicher Fehlerpunkt, der keine Vorteile bringt.  Pers√∂nlich habe ich den Nomad-Cluster aus verschiedenen Gr√ºnden zweimal vollst√§ndig gel√∂scht: einmal "zu Hause", einmal arbeiten.  Dies war in den fr√ºhen Stadien der Einf√ºhrung von Nomad und aufgrund von Schlamperei. <br>  Au√üerdem verh√§lt sich Nomad aufgrund von Problem Nummer 8 schlecht und startet die Dienste neu.  Aber selbst wenn dieses Problem gel√∂st ist, bleibt die Gefahr bestehen. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. Die Stabilisierung von Arbeit und Service wird in einem instabilen Netzwerk neu gestartet </h2><br><p>  <strong>L√∂sung: Verwenden Sie die Optionen zur Optimierung des Herzschlags.</strong> </p><br><p>  Standardm√§√üig ist Nomad so konfiguriert, dass kurzfristige Netzwerkprobleme oder CPU-Auslastungen zu einem Konsensverlust und einer Wiederwahl des Assistenten f√ºhren oder den Agentenknoten als unzug√§nglich markieren.  Dies f√ºhrt zu spontanen Neustarts von Diensten und deren √úbertragung auf andere Knoten. </p><br><p>  Statistik des "Home" -Clusters vor Behebung des Problems: Die maximale Lebensdauer des Containers vor dem Neustart betr√§gt ca. 10 Tage.  Hier wird es immer noch belastet, indem der Agent und der Server auf einem Server ausgef√ºhrt und in 5 verschiedenen Rechenzentren in Europa platziert werden, was eine gro√üe Belastung der CPU und ein weniger stabiles Netzwerk impliziert. </p><br><p>  Statistik des Arbeitsclusters vor Behebung des Problems: Die maximale Lebensdauer des Containers vor dem Neustart betr√§gt mehr als 2 Monate.  Aufgrund der separaten Server f√ºr die Nomad-Serverknoten und des hervorragenden Netzwerks zwischen den Rechenzentren ist hier alles relativ gut. </p><br><p>  Standardwerte </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Nach dem Code zu urteilen: In dieser Konfiguration werden alle 10 Sekunden Herzschl√§ge ausgef√ºhrt.  Mit dem Verlust von zwei Herzschl√§gen beginnt die Wiederwahl des Masters oder die √úbertragung von Diensten vom Agentenknoten.  Meiner Meinung nach umstrittene Einstellungen.  Wir bearbeiten sie je nach Anwendung. </p><br><p>  Wenn alle Dienste in mehreren Instanzen ausgef√ºhrt werden und von Rechenzentren verteilt werden, spielt es f√ºr Sie h√∂chstwahrscheinlich keine Rolle, wie lange der Zugriff auf den Server bestimmt wird (im folgenden Beispiel etwa 5 Minuten). Wir machen das Heartbeat-Intervall weniger h√§ufig und den Zeitraum f√ºr die Ermittlung der Unzug√§nglichkeit l√§nger.  Dies ist ein Beispiel f√ºr die Einrichtung meines Heimclusters: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  Wenn Sie √ºber eine gute Netzwerkkonnektivit√§t, separate Server f√ºr Serverknoten und die Zeitspanne f√ºr die Feststellung der Unzug√§nglichkeit von Servern verf√ºgen (in einer Instanz wird ein Dienst ausgef√ºhrt und es ist wichtig, ihn schnell zu √ºbertragen), verl√§ngern Sie die Zeitspanne f√ºr die Feststellung der Unzug√§nglichkeit (heartbeat_grace).  Optional k√∂nnen Sie mehr Herzschl√§ge ausf√ºhren (indem Sie min_heartbeat_ttl verringern) - dies erh√∂ht die CPU-Belastung geringf√ºgig.  Beispiel f√ºr eine funktionierende Clusterkonfiguration: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Diese Einstellungen beheben das Problem vollst√§ndig. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. Starten Sie regelm√§√üige Aufgaben </h2><br><p>  <strong>L√∂sung: Periodische Nomadendienste k√∂nnen verwendet werden, aber cron ist f√ºr den Support bequemer.</strong> </p><br><p>  Nomad kann den Dienst regelm√§√üig starten. </p><br><p>  Das einzige Plus ist die Einfachheit dieser Konfiguration. </p><br><p>  Das erste Minus ist, dass wenn der Dienst h√§ufig gestartet wird, die Liste der Aufgaben verschmutzt wird.  Beispielsweise werden beim Start alle 5 Minuten st√ºndlich 12 zus√§tzliche Aufgaben zur Liste hinzugef√ºgt, bis der GC Nomad ausgel√∂st wird, wodurch alte Aufgaben gel√∂scht werden. </p><br><p>  Das zweite Minus - es ist nicht klar, wie die √úberwachung eines solchen Dienstes richtig konfiguriert werden kann.  Wie kann man verstehen, dass ein Dienst beginnt, sich erf√ºllt und seine Arbeit bis zum Ende erledigt? </p><br><p>  Infolgedessen kam ich f√ºr mich zur "Cron" -Implementierung periodischer Aufgaben: </p><br><ol><li>  Es kann ein normaler Cron in einem st√§ndig laufenden Container sein.  Cron f√ºhrt regelm√§√üig ein bestimmtes Skript aus.  Ein Skript-Healthcheck kann einfach zu einem solchen Container hinzugef√ºgt werden, der jedes Flag √ºberpr√ºft, das ein laufendes Skript erstellt. </li><li>  Es kann sich um einen st√§ndig laufenden Container mit einem st√§ndig laufenden Dienst handeln.  Innerhalb des Dienstes wurde bereits ein periodischer Start implementiert.  Zu einem solchen Dienst, der den Status sofort anhand seiner "Innenseiten" √ºberpr√ºft, kann problemlos ein √§hnlicher Script-Healthcheck oder ein http-Healthcheck hinzugef√ºgt werden. </li></ol><br><p>  Momentan schreibe ich die meiste Zeit in Go. Ich bevorzuge die zweite Option mit http healthcheck - on Go und periodischem Start, und http healthcheck'i wird mit ein paar Codezeilen hinzugef√ºgt. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Bereitstellung redundanter Dienste </h2><br><p>  <strong>L√∂sung: Es gibt keine einfache L√∂sung.</strong>  <strong>Es gibt zwei schwierigere Optionen.</strong> </p><br><p>  Das von Nomad-Entwicklern bereitgestellte Bereitstellungsschema soll die Anzahl der ausgef√ºhrten Dienste unterst√ºtzen.  Sie sagen, der Nomade "starte mir 5 Instanzen des Dienstes" und er startet sie irgendwo dort.  Es gibt keine Kontrolle √ºber die Verteilung.  Instanzen k√∂nnen auf demselben Server ausgef√ºhrt werden. </p><br><p>  Wenn der Server abst√ºrzt, werden die Instanzen auf andere Server √ºbertragen.  W√§hrend die Instanzen √ºbertragen werden, funktioniert der Dienst nicht.  Dies ist eine schlechte R√ºckstellungsoption. </p><br><p>  Wir machen es richtig: </p><br><ol><li>  Wir verteilen Instanzen auf Servern √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">different_hosts</a> . </li><li>  Wir verteilen Instanzen auf Rechenzentren.  Leider nur durch Erstellen einer Kopie des Skripts des Formulars service1, service2 mit demselben Inhalt, unterschiedlichen Namen und einer Angabe des Starts in verschiedenen Rechenzentren. </li></ol><br><p>  In Nomad 0.9 wird eine Funktionalit√§t angezeigt, die dieses Problem behebt: Es ist m√∂glich, Dienste in einem prozentualen Verh√§ltnis zwischen Servern und Rechenzentren zu verteilen. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>L√∂sung: Die eingebaute Benutzeroberfl√§che ist schrecklich, Hashi-UI ist wundersch√∂n.</strong> </p><br><p>  Der Konsolenclient f√ºhrt die meisten erforderlichen Funktionen aus, aber manchmal m√∂chten Sie die Grafiken sehen, die Tasten dr√ºcken ... </p><br><p>  Nomad verf√ºgt √ºber eine integrierte Benutzeroberfl√§che.  Es ist nicht sehr praktisch (noch schlimmer als die Konsole). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  Die einzige Alternative, die ich kenne, ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hashi-UI</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  Tats√§chlich brauche ich den Konsolen-Client jetzt pers√∂nlich nur f√ºr "Nomad Run".  Und selbst das plant, auf CI zu √ºbertragen. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Unterst√ºtzung f√ºr √úberzeichnung aus dem Speicher </h2><br><p>  <strong>L√∂sung: nein.</strong> </p><br><p>  In der aktuellen Version von Nomad m√ºssen Sie ein striktes Speicherlimit f√ºr den Dienst angeben.  Wenn das Limit √ºberschritten wird, wird der Dienst von OOM Killer beendet. </p><br><p>  √úberzeichnung ist, wenn Grenzwerte f√ºr einen Dienst "von und bis" angegeben werden k√∂nnen.  Einige Dienste ben√∂tigen beim Start mehr Speicher als im normalen Betrieb.  Einige Dienste verbrauchen m√∂glicherweise f√ºr kurze Zeit mehr Speicher als gew√∂hnlich. </p><br><p>  Die Wahl einer strengen Einschr√§nkung oder einer weichen Einschr√§nkung ist ein Diskussionsthema, aber beispielsweise erm√∂glicht Kubernetes dem Programmierer, eine Wahl zu treffen.  Leider gibt es in aktuellen Versionen von Nomad keine solche M√∂glichkeit.  Ich gebe zu, dass dies in zuk√ºnftigen Versionen erscheinen wird. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Reinigen des Servers von Nomad-Diensten </h2><br><p>  <strong>L√∂sung:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  Manchmal "geht etwas schief."  Auf dem Server wird der Agentenknoten beendet und der Start verweigert.  Oder der Agentenknoten reagiert nicht mehr.  Oder der Agentenknoten "verliert" Dienste auf diesem Server. <br>  Dies geschah manchmal mit √§lteren Versionen von Nomad, jetzt passiert dies entweder nicht oder sehr selten. </p><br><p>  Was ist in diesem Fall am einfachsten, da der Drain-Server nicht das gew√ºnschte Ergebnis liefert?  Wir reinigen den Server manuell: </p><br><ol><li>  Stoppen Sie den Nomadenagenten. </li><li>  Machen Sie umount auf der Halterung, die es erstellt. </li><li>  L√∂schen Sie alle Agentendaten. </li><li>  Wir entfernen alle Container durch Filtern von Service-Containern (falls vorhanden). </li><li>  Wir starten den Agenten. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. Wie kann Nomad am besten bereitgestellt werden? </h2><br><p>  <strong>L√∂sung: nat√ºrlich durch Konsul.</strong> </p><br><p>  Consul ist in diesem Fall keineswegs eine zus√§tzliche Schicht, sondern ein Dienst, der organisch in die Infrastruktur passt und mehr Pluspunkte als Minuspunkte bietet: DNS, KV-Speicher, Suche nach Diensten, √úberwachung der Verf√ºgbarkeit des Dienstes, F√§higkeit zum sicheren Informationsaustausch. </p><br><p>  Au√üerdem entfaltet es sich so leicht wie Nomad selbst. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. Was ist besser - Nomad oder Kubernetes? </h2><br><p>  <strong>L√∂sung: h√§ngt ab von ...</strong> </p><br><p>  Fr√ºher hatte ich manchmal den Gedanken, eine Migration zu Kubernetes zu starten - ich war so ver√§rgert √ºber den regelm√§√üigen spontanen Neustart von Diensten (siehe Problem Nummer 8).  Aber nach einer vollst√§ndigen L√∂sung des Problems kann ich sagen: Nomad passt im Moment zu mir. </p><br><p>  Auf der anderen Seite: Kubernetes hat auch ein halb-spontanes Neuladen von Diensten - wenn der Kubernetes-Scheduler Instanzen abh√§ngig von der Last neu verteilt.  Das ist nicht sehr cool, aber dort ist es h√∂chstwahrscheinlich konfiguriert. </p><br><p>  Vorteile von Nomad: Die Infrastruktur ist sehr einfach bereitzustellen, einfache Skripte, gute Dokumentation, integrierte Unterst√ºtzung f√ºr Consul / Vault, was wiederum Folgendes bietet: eine einfache L√∂sung f√ºr das Problem der Kennwortspeicherung, integriertes DNS und einfach zu konfigurierende Helchecks. </p><br><p>  Vorteile von Kubernetes: Jetzt ist es ein "De-facto-Standard".  Gute Dokumentation, viele vorgefertigte L√∂sungen mit einer guten Beschreibung und Standardisierung des Starts. </p><br><p>  Leider habe ich nicht das gleiche gro√üe Fachwissen in Kubernetes, um die Frage eindeutig zu beantworten - was f√ºr den neuen Cluster zu verwenden ist.  Kommt auf die geplanten Bed√ºrfnisse an. <br>  Wenn Sie viele Namespaces geplant haben (Problem Nr. 5) oder Ihre spezifischen Dienste zu Beginn viel Speicher verbrauchen, dann geben Sie ihn frei (Problem Nr. 12) - definitiv Kubernetes, weil  Diese beiden Probleme in Nomad sind nicht vollst√§ndig gel√∂st oder unpraktisch. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de435132/">https://habr.com/ru/post/de435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de435120/index.html">Lambda funktioniert in SQL ... denken wir mal</a></li>
<li><a href="../de435122/index.html">Wie die Flamme in Doom auf der Playstation implementiert wurde</a></li>
<li><a href="../de435124/index.html">Meisterwerke der Welts√§ulenkonstruktion: ein Studiomonitor-Transformator mit variabler Anzahl von B√§ndern</a></li>
<li><a href="../de435126/index.html">Erfahrung in der Organisation und Durchf√ºhrung von Unternehmenskonferenzen f√ºr Analysten</a></li>
<li><a href="../de435128/index.html">Pi-Sonos: ein au√üer Kontrolle geratenes Hobby</a></li>
<li><a href="../de435134/index.html">Vereinfachen Sie die Arbeit mit Datenbanken in Qt mit QSqlRelationalTableModel</a></li>
<li><a href="../de435136/index.html">Sergey und die wissenschaftliche Methode</a></li>
<li><a href="../de435138/index.html">So √ºbernehmen Sie die Kontrolle √ºber Ihre Netzwerkinfrastruktur. Kapitel Drei Netzwerksicherheit. Teil eins</a></li>
<li><a href="../de435142/index.html">Trace mit eBPF lernen: Ein Leitfaden und Beispiele</a></li>
<li><a href="../de435144/index.html">Einf√ºhrung in Spring Boot: Erstellen einer einfachen REST-API in Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>