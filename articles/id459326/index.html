<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ ğŸ¥  ğŸ•´ğŸ¿ Penskalaan otomatis dan manajemen sumber daya di Kubernetes (tinjau dan laporkan video) â—»ï¸ â˜¦ï¸ ğŸ’‡ğŸ¿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pada tanggal 27 April, di konferensi Strike-2019 , dalam kerangka bagian DevOps, sebuah laporan dibuat berjudul "Penskalaan otomatis dan manajemen sum...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Penskalaan otomatis dan manajemen sumber daya di Kubernetes (tinjau dan laporkan video)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/459326/">  Pada tanggal 27 April, di konferensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Strike-2019</a> , dalam kerangka bagian DevOps, sebuah laporan dibuat berjudul "Penskalaan otomatis dan manajemen sumber daya di Kubernetes".  Ini berbicara tentang bagaimana menggunakan K8 untuk memastikan ketersediaan aplikasi yang tinggi dan memastikan kinerja maksimum mereka. <br><br><img src="https://habrastorage.org/webt/ol/sv/vf/olsvvfwmfrorzavctm_ipdufuxo.jpeg"><br><br>  Secara tradisi, kami senang menyajikan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><b>video dengan laporan</b></a> (44 menit, jauh lebih informatif daripada artikel) dan penekanan utama dalam bentuk teks.  Ayo pergi! <a name="habracut"></a><br><br>  Kami akan menganalisis topik laporan sesuai dengan kata-kata dan mulai dari akhir. <br><br><h2>  Kubernetes </h2><br>  Biarkan kami memiliki wadah Docker di host.  Mengapa  Untuk memastikan pengulangan dan isolasi, yang pada gilirannya memungkinkan untuk penyebaran sederhana dan baik, CI / CD.  Kami memiliki banyak mesin dengan kontainer. <br><br>  Apa yang dalam hal ini memberi Kubernetes? <br><br><ol><li>  Kami berhenti memikirkan mesin ini dan mulai bekerja dengan "cloud", <b>sekelompok wadah</b> atau pod (kelompok wadah). </li><li>  Selain itu, kami bahkan tidak memikirkan pod individual, tetapi mengelola lebih banyak grup besar.  <b>Primitif tingkat tinggi</b> semacam itu memungkinkan kita untuk mengatakan bahwa ada template untuk meluncurkan beban kerja tertentu, tetapi jumlah instance yang diperlukan untuk peluncurannya.  Jika kami kemudian mengubah template, semua instance juga akan berubah. </li><li>  Menggunakan <b>API deklaratif,</b> alih-alih mengeksekusi urutan perintah tertentu, kami menggambarkan "perangkat dunia" (dalam YAML) yang dibuat Kubernetes.  Dan lagi: ketika deskripsi berubah, tampilan aslinya juga akan berubah. </li></ol><br><h2>  Manajemen sumber daya </h2><br><h3>  CPU </h3><br>  Mari kita jalankan nginx, php-fpm dan mysql di server.  Layanan ini sebenarnya akan memiliki lebih banyak proses yang berjalan, yang masing-masing membutuhkan sumber daya komputasi: <br><br><img src="https://habrastorage.org/webt/yu/v6/t8/yuv6t8a5q6txbhi25fe5mwv8f4k.png"><br>  <i>(angka-angka pada slide adalah "burung beo," kebutuhan abstrak dari setiap proses untuk daya komputasi)</i> <br><br>  Untuk membuatnya nyaman untuk bekerja dengan ini, adalah logis untuk menggabungkan proses menjadi grup (misalnya, semua proses nginx menjadi satu grup "nginx").  Cara sederhana dan jelas untuk melakukan ini adalah menempatkan masing-masing kelompok dalam wadah: <br><br><img src="https://habrastorage.org/webt/yu/en/fr/yuenfrw6vzvexx1xrkvy3dfgw3o.png"><br><br>  Untuk melanjutkan, Anda perlu mengingat apa itu wadah (di Linux).  Penampilan mereka dimungkinkan berkat tiga fitur utama di kernel, yang diimplementasikan untuk waktu yang lama: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kemampuan</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ruang nama</a> , dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">grup</a> .  Dan teknologi lainnya (termasuk "cangkang" yang mudah digunakan seperti Docker) berkontribusi pada pengembangan lebih lanjut: <br><br><img src="https://habrastorage.org/webt/f-/nf/ua/f-nfuaos1_9xdwyblt2em4yp-gs.png"><br><br>  Dalam konteks laporan, kami hanya tertarik pada <b>cgroup</b> , karena kelompok kontrol adalah bagian dari fungsi kontainer (Docker, dll.) Yang menerapkan manajemen sumber daya.  Proses, yang disatukan dalam kelompok, seperti yang kita inginkan, adalah kelompok kontrol. <br><br>  Mari kita kembali ke persyaratan CPU untuk proses ini, dan sekarang ke grup proses: <br><br><img src="https://habrastorage.org/webt/_s/cl/7v/_scl7v6nsak1ieo-dipaz9sgb_a.png"><br>  <i>(Saya ulangi bahwa semua angka adalah ekspresi abstrak dari persyaratan sumber daya)</i> <br><br>  Pada saat yang sama, CPU itu sendiri memiliki sumber daya final tertentu <i>(dalam contoh itu adalah 1000)</i> , yang mungkin tidak cukup untuk semua orang (jumlah kebutuhan semua kelompok adalah 150 + 850 + 460 = 1460).  Apa yang akan terjadi dalam kasus ini? <br><br>  Kernel mulai mendistribusikan sumber daya dan melakukannya dengan â€œjujurâ€, memberikan jumlah sumber daya yang sama untuk setiap kelompok.  Tetapi dalam kasus pertama ada lebih banyak dari yang diperlukan (333&gt; 150), sehingga kelebihan (333-150 = 183) tetap dalam cadangan, yang juga terdistribusi secara merata antara dua wadah lainnya: <br><br><img src="https://habrastorage.org/webt/nm/qv/te/nmqvteopou65lsc_ku2b0-qmbco.gif"><br><br>  Akibatnya: wadah pertama memiliki sumber daya yang cukup, yang kedua - tidak cukup, yang ketiga - tidak cukup.  Ini adalah hasil dari <b>penjadwal "jujur" di Linux</b> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CFS</a> .  Pekerjaannya dapat diatur dengan menetapkan <b>berat untuk</b> masing-masing wadah.  Misalnya, seperti ini: <br><br><img src="https://habrastorage.org/webt/z1/c_/_b/z1c__bumb8hy1k5zdrkcwo_aak0.gif"><br><br>  Mari kita lihat kasus kekurangan sumber daya di wadah kedua (php-fpm).  Semua sumber daya kontainer didistribusikan di antara proses-proses tersebut secara merata.  Hasilnya, proses master bekerja dengan baik, dan semua pekerja melambat, menerima kurang dari setengah dari yang dibutuhkan: <br><br><img src="https://habrastorage.org/webt/9z/wi/d2/9zwid2g1znmdoslkl9bf9cogrnq.gif"><br><br>  Ini adalah cara kerja penjadwal CFS.  Bobot yang kami tetapkan untuk kontainer akan disebut <b>permintaan</b> di masa mendatang.  Kenapa begitu - lihat di bawah. <br><br>  Mari kita lihat keseluruhan situasi dari sisi lain.  Seperti yang Anda tahu, semua jalan mengarah ke Roma, dan dalam kasus komputer ke CPU.  Satu CPU, banyak tugas - Anda perlu lampu lalu lintas.  Cara termudah untuk mengelola sumber daya adalah "lampu lalu lintas": mereka memberi satu proses waktu akses tetap ke CPU, lalu yang berikutnya, dll. <br><br><img src="https://habrastorage.org/webt/vf/af/qe/vfafqespiii6mnts87i4amfdyku.gif"><br><br>  Pendekatan ini disebut <i>pembatasan keras</i> .  Ingat itu hanya sebagai <b>batas</b> .  Namun, jika Anda mendistribusikan batas ke semua kontainer, muncul masalah: mysql sedang bepergian di sepanjang jalan dan pada titik tertentu kebutuhannya untuk CPU berakhir, tetapi semua proses lainnya terpaksa menunggu saat CPU <b>diam</b> . <br><br><img src="https://habrastorage.org/webt/7o/1m/ps/7o1mps5khnoqkfywizrusxtug1q.png"><br><br>  Mari kita kembali ke kernel Linux dan interaksinya dengan CPU - gambaran keseluruhannya adalah sebagai berikut: <br><br><img src="https://habrastorage.org/webt/m7/xz/x8/m7xzx8brcdgbihu8qchb63rwjwc.png"><br><br>  Cgroup memiliki dua pengaturan - pada kenyataannya, ini adalah dua "tikungan" sederhana yang memungkinkan Anda untuk menentukan: <br><br><ol><li>  berat untuk wadah (permintaan) <b>dibagi</b> ; </li><li>  persentase dari total waktu CPU untuk mengerjakan tugas kontainer (batas) adalah <b>kuota</b> . </li></ol><br><h3>  Bagaimana cara mengukur CPU? </h3><br>  Ada berbagai cara: <br><br><ol><li>  Apa itu <i>burung beo</i> , tidak ada yang tahu - setiap kali Anda harus setuju. </li><li>  <i>Bunga</i> lebih jelas, tetapi relatif: 50% dari server dengan 4 core dan 20 core adalah hal yang sama sekali berbeda. </li><li>  Anda dapat menggunakan <i>bobot</i> yang telah disebutkan di Linux, tetapi mereka juga relatif. </li><li>  Opsi yang paling memadai adalah mengukur sumber daya komputasi dalam <i>hitungan detik</i> .  Yaitu  dalam detik waktu prosesor relatif terhadap detik waktu nyata: mereka mengeluarkan 1 detik waktu prosesor dalam 1 detik nyata - ini adalah satu inti CPU keseluruhan. </li></ol><br>  Untuk membuatnya lebih mudah untuk dikatakan, mereka mulai mengukur langsung di <i>inti</i> , yang berarti waktu CPU relatif terhadap yang asli.  Karena Linux memahami bobot daripada waktu prosesor / inti, diperlukan mekanisme penerjemahan dari satu ke yang lain. <br><br>  Pertimbangkan contoh sederhana dengan server dengan 3 core CPU, di mana tiga pod akan memilih bobot (500, 1000 dan 1500) yang mudah dikonversi ke bagian terkait dari core yang dialokasikan untuk mereka (0,5, 1 dan 1,5). <br><br><img src="https://habrastorage.org/webt/mz/vl/1x/mzvl1xmzbtvlwgsqtf1-_n_rhns.png"><br><br>  Jika Anda mengambil server kedua, di mana akan ada dua kali lebih banyak core (6), dan menempatkan pod yang sama di sana, distribusi core dapat dengan mudah dihitung dengan hanya mengalikannya dengan 2 (masing-masing 1, 2 dan 3).  Tetapi poin penting terjadi ketika pod keempat muncul di server ini, yang beratnya adalah 3.000 untuk kenyamanan. Ini menghilangkan beberapa sumber daya CPU (setengah inti), dan sisa pod menceritakannya kembali (membagi dua): <br><br><img src="https://habrastorage.org/webt/p3/1t/nd/p31tndrejrchgwgbnpk53q5qnig.gif"><br><br><h3>  Kubernet dan Sumber Daya CPU </h3><br>  Di Kubernetes, sumber daya CPU biasanya diukur dalam <b>mili-core</b> , yaitu  Kernel 0,001 diambil sebagai berat dasar.  <i>(Hal yang sama dalam terminologi Linux / cgroup disebut CPU share, meskipun, lebih tepatnya, 1000 CPU = 1024 share CPU.)</i> K8s memastikan untuk tidak menempatkan lebih banyak pod pada server daripada sumber daya CPU untuk jumlah bobot semua polong. <br><br>  Bagaimana kabarnya?  Ketika server ditambahkan ke kluster Kubernetes, server melaporkan berapa banyak core CPU yang tersedia.  Dan ketika membuat pod baru, scheduler Kubernetes tahu berapa core yang dibutuhkan pod ini.  Dengan demikian, pod akan ditentukan di server, di mana ada cukup inti. <br><br>  Apa yang akan terjadi jika permintaan <b>tidak</b> ditentukan (mis., Pod tidak menentukan jumlah kernel yang dibutuhkan)?  Mari kita lihat bagaimana Kubernet umumnya menghitung sumber daya. <br><br>  Pod dapat menentukan permintaan (penjadwal CFS) dan batasan (ingat lampu lalu lintas?): <br><br><ul><li>  Jika mereka sama, maka kelas QoS dijamin ditugaskan ke pod.  Jumlah kernel yang selalu tersedia untuknya dijamin. </li><li>  Jika permintaan kurang dari batas, kelas QoS bisa <b>meledak</b> .  Yaitu  kami berharap bahwa pod, misalnya, selalu menggunakan 1 inti, tetapi nilai ini bukan batasan untuk itu: <i>kadang-kadang</i> pod dapat menggunakan lebih banyak (ketika ada sumber daya gratis di server untuk ini). </li><li>  Ada juga kelas QoS <b>upaya terbaik</b> - pod yang tidak ditentukan permintaannya.  Sumber daya diberikan terakhir kepada mereka. </li></ul><br><h3>  Memori </h3><br>  Situasinya mirip dengan memori, tetapi sedikit berbeda - setelah semua, sifat sumber daya ini berbeda.  Secara umum, analoginya adalah sebagai berikut: <br><br><img src="https://habrastorage.org/webt/hw/zu/ja/hwzuja_vf0ojiz8uai-hhtn23ys.png"><br><br>  Mari kita lihat bagaimana permintaan diimplementasikan dalam memori.  Biarkan pod hidup di server, ubah memori yang dikonsumsi, hingga salah satunya menjadi sangat besar sehingga memori habis.  Dalam hal ini, pembunuh OOM muncul dan membunuh proses terbesar: <br><br><img src="https://habrastorage.org/webt/mg/i0/at/mgi0atkc5o5m0xo-crxt3augbnm.gif"><br><br>  Ini tidak selalu cocok untuk kita, oleh karena itu, dimungkinkan untuk mengatur proses mana yang penting bagi kita dan tidak boleh dibunuh.  Untuk melakukan ini, gunakan parameter <b>oom_score_adj</b> . <br><br>  Mari kita kembali ke kelas QoS dari CPU dan menggambar analogi dengan nilai-nilai oom_score_adj, yang menentukan prioritas untuk pod pada konsumsi memori: <br><br><ul><li>  Nilai oom_score_adj terendah dari sebuah pod adalah -998, yang berarti bahwa pod tersebut harus dibunuh di tempat terakhir, ini <b>dijamin</b> . </li><li>  Yang tertinggi - 1000 - adalah <b>upaya terbaik</b> , polong seperti itu terbunuh sebelum orang lain. </li><li>  Untuk menghitung sisa nilai ( <b>burstable</b> ), ada formula yang esensinya bermuara pada fakta bahwa semakin banyak pod meminta sumber daya, semakin sedikit peluang itu akan dibunuh. </li></ul><br><img src="https://habrastorage.org/webt/sc/yo/rm/scyorm9zwn_lltxminknyv-cbhy.png"><br><br>  " <b>Putar</b> " kedua - <b>limit_in_bytes</b> - untuk batas.  Semuanya lebih sederhana dengan itu: kami hanya menetapkan jumlah maksimum memori yang akan dikeluarkan, dan di sini (tidak seperti CPU) tidak ada pertanyaan apa itu (memori) diukur. <br><br><h3>  Total </h3><br>  Permintaan dan <code>limits</code> ditetapkan untuk setiap pod di Kubernetes - baik parameter untuk CPU dan untuk memori: <br><br><ol><li>  berdasarkan permintaan, penjadwal Kubernet berfungsi, yang mendistribusikan pod di seluruh server; </li><li>  berdasarkan semua parameter, kelas QodS pod ditentukan; </li><li>  Bobot relatif dihitung berdasarkan permintaan CPU; </li><li>  Berdasarkan permintaan CPU, penjadwal CFS dikonfigurasikan; </li><li>  Berdasarkan permintaan memori, pembunuh OOM dikonfigurasi; </li><li>  Berdasarkan batas CPU, "lampu lalu lintas" dikonfigurasi; </li><li>  berdasarkan batas memori, batas ditetapkan pada cgroup. </li></ol><br><img src="https://habrastorage.org/webt/dr/1i/0_/dr1i0_troiqlcg4q_ki2fytefs0.png"><br><br>  Secara umum, gambar ini menjawab semua pertanyaan tentang bagaimana bagian utama dari manajemen sumber daya di Kubernetes terjadi. <br><br><h2>  Autoscaling </h2><br><h3>  K8s cluster-autoscaler </h3><br>  Bayangkan bahwa seluruh cluster sudah terisi dan pod baru harus dibuat.  Meskipun pod tidak dapat muncul, ia menggantung dalam status <i>Ditunda</i> .  Agar muncul, kita dapat menghubungkan server baru ke cluster atau ... menempatkan cluster-autoscaler, yang akan melakukannya untuk kita: memesan mesin virtual dari penyedia cloud (dengan permintaan API) dan menghubungkannya ke cluster, setelah itu pod akan ditambahkan . <br><br><img src="https://habrastorage.org/webt/zu/va/dq/zuvadqhlpycxkqaw5q35bmr08_e.gif"><br><br>  Ini adalah penskalaan otomatis kluster Kubernetes, yang sangat bagus (menurut pengalaman kami).  Namun, seperti di tempat lain, ada beberapa nuansa di sini ... <br><br>  Ketika kami meningkatkan ukuran cluster, semuanya baik-baik saja, tetapi apa yang terjadi ketika cluster <b>mulai bebas</b> ?  Masalahnya adalah bermigrasi pod (ke host gratis) secara teknis sangat sulit dan mahal dalam hal sumber daya.  Kubernetes memiliki pendekatan yang sangat berbeda. <br><br>  Pertimbangkan sekelompok 3 server di mana ada Penyebaran.  Ia memiliki 6 pod: sekarang 2 untuk setiap server.  Untuk beberapa alasan, kami ingin mematikan salah satu server.  Untuk melakukan ini, gunakan perintah <code>kubectl drain</code> , yang: <br><br><ul><li>  melarang pengiriman pod baru ke server ini; </li><li>  hapus pod yang ada di server. </li></ul><br>  Karena Kubernetes memantau pemeliharaan jumlah polong (6), ia hanya akan membuatnya <b>kembali</b> di node lain, tetapi tidak pada yang terputus, karena sudah ditandai sebagai tidak dapat diakses untuk menempatkan polong baru.  Ini adalah mekanisme dasar untuk Kubernetes. <br><br><img src="https://habrastorage.org/webt/l8/dw/jf/l8dwjfv1yyszva-knkk6p0hls_s.gif"><br><br>  Namun, ada nuansa di sini.  Dalam situasi yang sama untuk tindakan StatefulSet (bukan Penempatan) akan berbeda.  Sekarang kita sudah memiliki aplikasi stateful - misalnya, tiga pod dengan MongoDB, salah satunya memiliki beberapa masalah (data rusak atau kesalahan lain yang mencegah pod memulai dengan benar).  Dan lagi kami memutuskan untuk memutuskan satu server.  Apa yang akan terjadi <br><br><img src="https://habrastorage.org/webt/a1/dx/ck/a1dxckkad2wpckrygftdzcjbuyq.gif"><br><br>  MongoDB <i>bisa</i> mati karena membutuhkan kuorum: untuk sekelompok tiga instalasi, setidaknya dua harus berfungsi.  Namun, ini <i>tidak terjadi</i> - terima kasih kepada <b>PodDisruptionBudget</b> .  Parameter ini menentukan jumlah minimum yang diperlukan untuk pod yang berfungsi.  Mengetahui bahwa salah satu pod dengan MongoDB tidak lagi berfungsi, dan melihat bahwa minAvailable diatur untuk MongoDB di <code>minAvailable: 2</code> , Kubernetes tidak akan membiarkan Anda menghapus pod. <br><br>  Intinya: untuk memindahkan (dan benar-benar membuat ulang) pod dengan benar ketika cluster dilepaskan, Anda perlu mengonfigurasi PodDisruptBudget. <br><br><h3>  Penskalaan horisontal </h3><br>  Pertimbangkan situasi yang berbeda.  Ada aplikasi yang berjalan sebagai Penerapan di Kubernetes.  Lalu lintas pengguna datang ke podnya (misalnya, ada tiga di antaranya), dan kami mengukur indikator tertentu di dalamnya (katakanlah, beban CPU).  Ketika beban meningkat, kami memperbaikinya sesuai jadwal dan menambah jumlah polong untuk mendistribusikan permintaan. <br><br>  Hari ini di Kubernetes Anda tidak perlu melakukan ini secara manual: Anda dapat secara otomatis menambah / mengurangi jumlah polong tergantung pada nilai indikator beban yang diukur. <br><br><img src="https://habrastorage.org/webt/kj/fm/_t/kjfm_tu0u83c4mthfjayisabme0.gif"><br><br>  Pertanyaan utama di sini adalah <b>apa sebenarnya yang diukur</b> dan <b>bagaimana menafsirkan nilai yang</b> diperoleh (untuk membuat keputusan tentang mengubah jumlah polong).  Anda bisa mengukur banyak: <br><br><img src="https://habrastorage.org/webt/h-/tw/a8/h-twa8kqe49av8gwxwxeoadyalc.png"><br><br>  Cara melakukannya secara teknis - kumpulkan metrik, dll.  - Saya berbicara secara rinci dalam laporan tentang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pemantauan dan Kubernet</a> .  Dan saran utama untuk memilih parameter optimal adalah <b>bereksperimen</b> ! <br><br>  Ada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">metode USE</a> <i>(Utilization Saturation and Errors</i> ), artinya adalah sebagai berikut.  Atas dasar apa masuk akal untuk mengukur, misalnya, php-fpm?  Berdasarkan kenyataan bahwa pekerja berakhir, itu adalah <i>pemanfaatan</i> .  Dan jika pekerja sudah selesai dan koneksi baru tidak diterima - ini adalah <i>kejenuhan</i> .  Kedua parameter ini perlu diukur, dan tergantung pada nilainya, penskalaan harus dilakukan. <br><br><h2>  Alih-alih sebuah kesimpulan </h2><br>  Laporan ini memiliki kelanjutan: tentang penskalaan vertikal dan tentang bagaimana memilih sumber daya yang tepat.  Saya akan membicarakan ini di video mendatang di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">YouTube kami</a> - berlangganan, agar tidak ketinggalan! <br><br><h2>  Video dan slide </h2><br>  Video dari kinerja (44 menit): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/10ZR-fbyuSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Penyajian laporan: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Laporan Kubernet lainnya di blog kami: <br><br><ul><li>  â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Memperluas dan melengkapi Kubernetes</a> â€ <i>(Andrey Polov; 8 April 2019 di Saint HighLoad ++)</i> ; </li><li>  â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Databases and Kubernetes</a> â€ <i>(Dmitry Stolyarov; 8 November 2018 tentang HighLoad ++)</i> ; </li><li>  â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Monitoring and Kubernetes</a> â€ <i>(Dmitry Stolyarov; 28 Mei 2018 di RootConf)</i> ; </li><li>  â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Praktek CI / CD terbaik dengan Kubernetes dan GitLab</a> â€ <i>(Dmitry Stolyarov; 7 November 2017 di HighLoad ++)</i> ; </li><li>  â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pengalaman kami dengan Kubernetes dalam proyek-proyek kecil</a> â€ <i>(Dmitry Stolyarov; 6 Juni 2017 di RootConf)</i> . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id459326/">https://habr.com/ru/post/id459326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id459314/index.html">Visualisasikan dan hadapi Hash Match Join</a></li>
<li><a href="../id459316/index.html">Hydra 2019: siaran gratis aula pertama dan sedikit tentang apa yang akan ada di konferensi</a></li>
<li><a href="../id459318/index.html">TypeScript dan sprint pendek. Bagaimana kami membuat alat variasi wawancara ujung depan</a></li>
<li><a href="../id459320/index.html">Operator Kubernetes di Python tanpa kerangka kerja dan SDK</a></li>
<li><a href="../id459322/index.html">Penerbit Peter. Penjualan musim panas</a></li>
<li><a href="../id459328/index.html">Nilai Uang Terbaik di Kelasnya - Mpow A5 (059)</a></li>
<li><a href="../id459330/index.html">Bitrix untuk programmer dan manajer: cinta dan benci</a></li>
<li><a href="../id459334/index.html">YouTrack 2019.2: spanduk seluruh sistem, peningkatan ke halaman daftar tugas, opsi pencarian baru dan banyak lagi</a></li>
<li><a href="../id459336/index.html">Hidup dan belajar. Bagian 1. Sekolah dan bimbingan karier</a></li>
<li><a href="../id459338/index.html">Menggunakan verifier sebagai alat pemodelan cepat proyek RTL. Pengantar UVM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>