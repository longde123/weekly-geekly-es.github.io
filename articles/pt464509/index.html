<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👉🏼 🕡 ✖️ O livro "Grok deep learning" ⚪️ 🍪 📢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Oi, habrozhiteli! O livro estabelece as bases para um maior domínio da tecnologia de aprendizado profundo. Começa com uma descrição dos conceitos bási...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O livro "Grok deep learning"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/464509/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/go/gm/1s/gogm1solwetphsozljuyzuizcbs.jpeg" align="left" alt="imagem"></a>  Oi, habrozhiteli!  O livro estabelece as bases para um maior domínio da tecnologia de aprendizado profundo.  Começa com uma descrição dos conceitos básicos de redes neurais e, em seguida, examina em detalhes as camadas adicionais da arquitetura. <br><br>  O livro foi especialmente escrito com a intenção de fornecer o menor limite de entrada possível.  Você não precisa de conhecimento de álgebra linear, métodos numéricos, otimizações convexas e até aprendizado de máquina.  Tudo o que é necessário para entender o aprendizado profundo será esclarecido à medida que você avança. <br><br>  Oferecemos a você que se familiarize com a passagem "O que é uma estrutura de aprendizado profundo?" <br><a name="habracut"></a><br>  <b>Boas ferramentas reduzem erros, aceleram o desenvolvimento e aumentam a velocidade de execução.</b> <br><br>  Se você lê muito sobre aprendizado profundo, provavelmente encontrou estruturas conhecidas como PyTorch, TensorFlow, Theano (recentemente declarado obsoleto), Keras, Lasagne e DyNet.  Nos últimos anos, as estruturas evoluíram muito rapidamente e, apesar de todas elas serem distribuídas gratuitamente e de código aberto, cada uma delas tem um espírito de competição e camaradagem. <br><br>  Até agora, evitei discutir estruturas, porque, em primeiro lugar, era extremamente importante para você entender o que estava acontecendo nos bastidores, implementando os algoritmos manualmente (usando apenas a biblioteca NumPy).  Mas agora começaremos a usar essas estruturas, porque as redes que vamos treinar, as redes com memória de longo prazo (LSTM) são muito complexas e o código que as implementa usando o NumPy é difícil de ler, usar e depurar (gradientes neste código são encontrados em todos os lugares). <br><br>  É essa complexidade que as estruturas de aprendizado profundo são projetadas para abordar.  A estrutura de aprendizado profundo pode reduzir significativamente a complexidade do código (além de reduzir o número de erros e aumentar a velocidade do desenvolvimento) e aumentar a velocidade de sua execução, especialmente se você usar um processador gráfico (GPU) para treinar a rede neural, o que pode acelerar o processo de 10 a 100 vezes.  Por esses motivos, as estruturas são usadas em quase todos os lugares da comunidade de pesquisa, e a compreensão dos recursos de seu trabalho será útil para você em sua carreira como usuário e pesquisador de aprendizado profundo. <br><br>  Mas não nos limitaremos à estrutura de nenhuma estrutura específica, porque isso impedirá que você aprenda como todos esses modelos complexos (como o LSTM) funcionam.  Em vez disso, criaremos nossa própria estrutura leve, seguindo as últimas tendências no desenvolvimento de estruturas.  Seguindo esse caminho, você saberá exatamente o que as estruturas fazem quando arquiteturas complexas são criadas com a ajuda delas.  Além disso, uma tentativa de criar sua própria estrutura pequena por conta própria ajudará você a mudar suavemente para o uso de estruturas reais de aprendizado profundo, porque você já conhecerá os princípios de organização de uma interface de programa (API) e sua funcionalidade.  Esse exercício foi muito útil para mim, e o conhecimento adquirido ao criar minha própria estrutura acabou sendo muito útil na depuração de modelos de problemas. <br><br>  Como a estrutura simplifica o código?  Falando abstratamente, elimina a necessidade de escrever o mesmo código repetidamente.  Especificamente, o recurso mais conveniente da estrutura de aprendizado profundo é o suporte à retropropagação automática e otimização automática.  Isso permite que você escreva apenas o código de distribuição direta, e a estrutura cuidará automaticamente da distribuição reversa e correção dos pesos.  A maioria das estruturas modernas simplifica o código que implementa a distribuição direta, oferecendo interfaces de alto nível para definir camadas típicas e funções de perda. <br><br><h3>  Introdução aos tensores </h3><br>  <b>Os tensores são uma forma abstrata de vetores e matrizes</b> <br><br>  Até o momento, utilizamos vetores e matrizes como principais estruturas.  Deixe-me lembrá-lo de que uma matriz é uma lista de vetores e um vetor é uma lista de escalares (números únicos).  Um tensor é uma forma abstrata para representar listas de números aninhadas.  O vetor é um tensor unidimensional.  A matriz é um tensor bidimensional e estruturas com um grande número de dimensões são chamadas tensores n-dimensionais.  Então, vamos começar a criar uma nova estrutura de aprendizado profundo definindo um tipo de base, que chamaremos de Tensor: <br><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class"> </span></span>= np.array(data) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) print(x) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span>] y = x + x print(y) [<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>]</code> </pre> <br>  Esta é a primeira versão da nossa estrutura de dados básica.  Observe que ele armazena todas as informações numéricas na matriz NumPy (self.data) e suporta uma operação de tensor único (adição).  Adicionar operações adicionais não é nada difícil, basta adicionar funções adicionais com a funcionalidade correspondente à classe Tensor. <br><br><h3>  Introdução ao cálculo automático de gradiente (autograd) </h3><br>  <b>Anteriormente, realizamos a propagação manual traseira.</b>  <b>Agora vamos torná-lo automático!</b> <br><br>  No capítulo 4, introduzimos derivativos.  Desde então, calculamos manualmente essas derivadas em cada nova rede neural.  Deixe-me lembrá-lo de que isso é alcançado pelo movimento reverso através da rede neural: primeiro, o gradiente na saída da rede é calculado; depois, esse resultado é usado para calcular a derivada no componente anterior, e assim por diante, até que os gradientes corretos sejam determinados para todos os pesos na arquitetura.  Essa lógica para calcular gradientes também pode ser adicionada à classe de tensores.  O seguinte mostra o que eu tinha em mente. <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">creators</span></span></span></span>=None, creation_op=None): self.data = np.array(data) self.creation_op = creation_op self.creators = creators self.grad = None def backward(self, grad): self.grad = grad <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(self.creation_op == <span class="hljs-string"><span class="hljs-string">"add"</span></span>): self.creators[<span class="hljs-number"><span class="hljs-number">0</span></span>].backward(grad) self.creators[<span class="hljs-number"><span class="hljs-number">1</span></span>].backward(grad) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data, creators=[self,other], creation_op=<span class="hljs-string"><span class="hljs-string">"add"</span></span>) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) y = Tensor([<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>]) z = x + y z.backward(Tensor(np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>])))</code> </pre> <br>  Este método apresenta duas inovações.  Primeiro, cada tensor recebe dois novos atributos.  criadores é uma lista de todos os tensores usados ​​para criar o tensor atual (o padrão é Nenhum).  Ou seja, se o tensor z for obtido pela adição dos outros dois tensores, x e y, o atributo criador do tensor z conterá os tensores x e y.  creation_op é um atributo complementar que armazena as operações usadas no processo de criação desse tensor.  Ou seja, a instrução z = x + y criará um gráfico computacional com três nós (x, ye z) e duas arestas (z -&gt; x e z -&gt; y).  Cada borda é assinada pela operação a partir de creation_op, ou seja, adicione.  Este gráfico ajudará a organizar a propagação recursiva de gradientes para trás. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/4d/qz/fx4dqzrh6y62rtttfy2vrn1jley.png" alt="imagem"></div><br>  A primeira inovação nesta implementação é a criação automática de um gráfico durante cada operação matemática.  Se pegarmos z e executar outra operação, o gráfico continuará em uma nova variável referente a z. <br><br>  A segunda inovação nesta versão da classe Tensor é a capacidade de usar um gráfico para calcular gradientes.  Se você chamar o método z.backward (), ele passará o gradiente para x e y, levando em consideração a função com a qual o tensor z (add) foi criado.  Como mostrado no exemplo acima, passamos o vetor gradiente (np.array ([1,1,1,1,1]]]) para z, e esse aplica-o a seus pais.  Como você provavelmente se lembra do Capítulo 4, propagação para trás por meio de adição significa aplicar propagação para trás.  Nesse caso, temos apenas um gradiente para adicionar a x e y, portanto, copiamos de z para x e y: <br><br><pre> <code class="javascript hljs">print(x.grad) print(y.grad) print(z.creators) print(z.creation_op) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>]), array([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])] add</code> </pre> <br>  A característica mais notável dessa forma de cálculo automático de gradiente é que ele funciona recursivamente - cada vetor chama o método .backward () de todos os seus pais da lista self.creators: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bt/ye/if/btyeiflumrhlbprlzqtzjxwipcs.png" alt="imagem"></div><br>  »Mais informações sobre o livro podem ser encontradas no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site do editor</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Conteúdo</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trecho</a> <br><br>  Cupom de 25% de desconto para vendedores ambulantes - <b>Deep Learning</b> <br>  Após o pagamento da versão impressa do livro, um livro eletrônico é enviado por e-mail. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt464509/">https://habr.com/ru/post/pt464509/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt464491/index.html">Vivaldi 2.7 - Vida Intensa em Silêncio</a></li>
<li><a href="../pt464495/index.html">Desenvolvimento e reflexão de equipes como comunicação gerencial do líder de equipe</a></li>
<li><a href="../pt464497/index.html">JIRA como remédio para insônia e colapsos nervosos</a></li>
<li><a href="../pt464499/index.html">"Mat. Modelo de Wall Street "ou uma tentativa de otimizar o custo da infraestrutura de TI na nuvem</a></li>
<li><a href="../pt464503/index.html">Correspondência de senha Wi-Fi com aircrack-ng</a></li>
<li><a href="../pt464511/index.html">Como coletar coortes de usuários na forma de gráficos no Grafana [+ imagem do docker com um exemplo]</a></li>
<li><a href="../pt464513/index.html">Duffle: Transformador de XD Design</a></li>
<li><a href="../pt464515/index.html">Como criar e-mails e não bagunçar: dicas práticas</a></li>
<li><a href="../pt464517/index.html">Novos cartões CUBA</a></li>
<li><a href="../pt464523/index.html">Sistemas de Pagamento (PSP) para empresas de TI: jogamos muito</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>