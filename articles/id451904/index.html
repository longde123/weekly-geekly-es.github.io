<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤷🏿 💣 🧔🏻 Terkadang lebih banyak lebih sedikit. Ketika penurunan beban menyebabkan peningkatan penundaan 🐋 🌉 🚴🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Seperti di sebagian besar posting , ada masalah dengan layanan terdistribusi, sebut saja layanan ini Alvin. Kali ini saya tidak menemukan masalah send...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Terkadang lebih banyak lebih sedikit. Ketika penurunan beban menyebabkan peningkatan penundaan</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451904/"> Seperti di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sebagian besar posting</a> , ada masalah dengan layanan terdistribusi, sebut saja layanan ini Alvin.  Kali ini saya tidak menemukan masalah sendiri, orang-orang dari bagian klien memberi tahu saya. <br><br>  Suatu kali saya terbangun dari surat yang tidak puas karena keterlambatan besar Alvin, yang kami rencanakan untuk diluncurkan dalam waktu dekat.  Secara khusus, klien mengalami keterlambatan persentil ke-99 di sekitar 50 ms, jauh di atas anggaran keterlambatan kami.  Ini mengejutkan, karena saya benar-benar menguji layanan, terutama untuk keterlambatan, karena ini adalah masalah yang sering dikeluhkan. <br><br>  Sebelum memberikan Alvin untuk pengujian, saya melakukan banyak percobaan dengan 40 ribu permintaan per detik (QPS), semua menunjukkan penundaan kurang dari 10 ms.  Saya siap menyatakan bahwa saya tidak setuju dengan hasil mereka.  Tetapi sekali lagi melihat surat itu, saya menarik perhatian pada sesuatu yang baru: Saya pasti tidak menguji kondisi yang mereka sebutkan, QPS mereka jauh lebih rendah daripada milik saya.  Saya menguji pada 40k QPS, dan mereka hanya pada 1k.  Saya menjalankan percobaan lain, kali ini dengan QPS lebih rendah, hanya untuk menyenangkan mereka. <br><a name="habracut"></a><br>  Karena saya menulis tentang ini di blog saya, Anda mungkin sudah mengerti: jumlahnya ternyata benar.  Saya menguji klien virtual saya lagi dan lagi, semua dengan hasil yang sama: sejumlah kecil permintaan tidak hanya meningkatkan penundaan, tetapi juga meningkatkan jumlah permintaan dengan penundaan lebih dari 10 ms.  Dengan kata lain, jika pada 40k QPS sekitar 50 permintaan per detik melebihi 50 ms, maka pada 1k QPS setiap detik ada 100 permintaan di atas 50 ms.  Paradoks! <br><br><img src="https://habrastorage.org/webt/xd/86/x-/xd86x-er30ek6tg4hkv4qdevvgq.png"><br><br><h1>  Persempit pencarian Anda </h1><br>  Menghadapi masalah keterlambatan dalam sistem terdistribusi dengan banyak komponen, hal pertama yang perlu Anda lakukan adalah membuat daftar tersangka.  Kami menggali sedikit lebih dalam tentang arsitektur Alvin: <br><br><img src="https://habrastorage.org/webt/lh/is/s5/lhiss5mqv9dq2h9moyhlss_chlk.png"><br><br>  Titik awal yang baik adalah daftar transisi I / O yang telah selesai (panggilan jaringan / pencarian disk, dll.).  Mari kita cari tahu di mana penundaannya.  Selain I / O yang jelas dengan klien, Alvin mengambil langkah tambahan: dia mengakses data warehouse.  Namun, penyimpanan ini berfungsi di kluster yang sama dengan Alvin, jadi seharusnya ada sedikit keterlambatan dibandingkan dengan klien.  Jadi, daftar tersangka: <br><br><ol><li>  Panggilan jaringan dari klien ke Alvin. <br></li><li>  Panggilan jaringan dari Alvin ke gudang data. <br></li><li>  Cari di disk di gudang data. <br></li><li>  Panggilan jaringan dari gudang data ke Alvin. <br></li><li>  Panggilan jaringan dari Alvin ke klien. </li></ol><br>  Mari kita coba mencoret beberapa poin. <br><br><h3>  Gudang Data </h3><br>  Hal pertama yang saya lakukan adalah mengonversi Alvin ke server ping-ping yang tidak menangani permintaan.  Setelah menerima permintaan, itu mengembalikan respons kosong.  Jika penundaan berkurang, maka kesalahan dalam implementasi Alvin atau data warehouse tidak pernah terdengar sebelumnya.  Dalam percobaan pertama, kami mendapatkan grafik berikut: <br><br><img src="https://habrastorage.org/webt/i4/zs/9s/i4zs9saymr5ra4tmry3diqbgdyy.png"><br><br>  Seperti yang Anda lihat, saat menggunakan server ping-ping tidak ada peningkatan.  Ini berarti bahwa gudang data tidak meningkatkan penundaan, dan daftar tersangka dibelah dua: <br><br><ol><li>  Panggilan jaringan dari klien ke Alvin. <br></li><li>  Panggilan jaringan dari Alvin ke klien. </li></ol><br>  Wow!  Daftar ini menyusut dengan cepat.  Saya pikir saya hampir menemukan alasannya. <br><br><h3>  gRPC </h3><br>  Sekarang saatnya memperkenalkan Anda kepada pemain baru: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">gRPC</a> .  Ini adalah pustaka sumber terbuka dari Google untuk komunikasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">RPC</a> dalam proses.  Meskipun <code>gRPC</code> dioptimalkan dengan baik dan banyak digunakan, pertama kali saya menggunakannya pada sistem skala ini, dan saya berharap bahwa implementasi saya akan kurang optimal - untuk sedikitnya. <br><br>  Kehadiran <code>gRPC</code> di stack memunculkan pertanyaan baru: mungkin ini implementasi saya atau apakah <code>gRPC</code> sendiri menyebabkan masalah keterlambatan?  Tambahkan ke daftar tersangka baru: <br><br><ol><li>  Klien memanggil pustaka <code>gRPC</code> <br></li><li>  Pustaka <code>gRPC</code> pada klien membuat panggilan jaringan ke pustaka <code>gRPC</code> di server <br></li><li>  Pustaka <code>gRPC</code> mengakses Alvin (tidak ada operasi dalam kasus server ping-pong) </li></ol><br>  Untuk membuat Anda memahami seperti apa kode itu, implementasi klien / Alvin saya tidak jauh berbeda dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">contoh</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">contoh</a> server klien <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">async</a> . <br><br><blockquote>  <i>Catatan: daftar di atas sedikit disederhanakan, karena <code>gRPC</code> memungkinkan Anda untuk menggunakan model aliran (templat?) Anda sendiri, di mana <code>gRPC</code> eksekusi <code>gRPC</code> dan implementasi pengguna saling terkait.</i>  <i>Demi kesederhanaan, kami akan tetap berpegang pada model ini.</i> </blockquote><br><h3>  Profiling akan memperbaiki semuanya </h3><br>  Mencoret gudang data, saya pikir saya hampir selesai: “Sekarang mudah!  Kami akan menerapkan profil dan mencari tahu di mana penundaan terjadi. "  Saya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">penggemar berat profil yang akurat</a> karena CPU sangat cepat dan paling sering mereka tidak mengalami hambatan.  Kebanyakan penundaan terjadi ketika prosesor harus berhenti memproses untuk melakukan sesuatu yang lain.  Penentuan profil CPU yang tepat dilakukan hanya untuk ini: ia secara akurat mencatat semua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sakelar konteks</a> dan memperjelas di mana terjadi keterlambatan. <br><br>  Saya mengambil empat profil: di bawah QPS tinggi (latensi rendah) dan dengan server ping-pong pada QPS rendah (latensi tinggi), baik di sisi klien maupun di sisi server.  Dan untuk berjaga-jaga, saya juga mengambil profil prosesor sampel.  Saat membandingkan profil, saya biasanya mencari tumpukan panggilan yang tidak normal.  Misalnya, di sisi buruk dengan penundaan tinggi, ada lebih banyak saklar konteks (10 kali atau lebih).  Tetapi dalam kasus saya, jumlah konteks beralih hampir bersamaan.  Yang membuatku ngeri, tidak ada yang signifikan di sana. <br><br><h1>  Debugging tambahan </h1><br>  Saya putus asa.  Saya tidak tahu alat apa yang bisa digunakan, dan rencana saya selanjutnya adalah mengulang percobaan dengan variasi yang berbeda, dan tidak mendiagnosis masalah dengan jelas. <br><br><h3>  Bagaimana jika </h3><br>  Sejak awal, saya khawatir tentang waktu tunda spesifik 50 ms.  Ini waktu yang sangat besar.  Saya memutuskan bahwa saya akan memotong bagian-bagian dari kode sampai saya bisa mengetahui bagian mana yang menyebabkan kesalahan ini.  Kemudian diikuti percobaan yang berhasil. <br><br>  Seperti biasa, dengan pikiran belakang sepertinya semuanya sudah jelas.  Saya menempatkan klien pada mesin yang sama dengan Alvin - dan mengirim permintaan ke <code>localhost</code> .  Dan peningkatan penundaan telah menghilang! <br><br><img src="https://habrastorage.org/webt/kl/bk/fv/klbkfv7ajppr9uqp_tywvxwgjre.png"><br><br>  Ada yang salah dengan jaringan. <br><br><h3>  Mempelajari keterampilan seorang insinyur jaringan </h3><br>  Saya harus mengakui: pengetahuan saya tentang teknologi jaringan sangat buruk, terutama mengingat fakta bahwa saya bekerja dengan mereka setiap hari.  Tetapi jaringan itu adalah tersangka utama, dan saya perlu belajar cara men-debug-nya. <br><br>  Untungnya, Internet menyukai mereka yang ingin belajar.  Kombinasi ping dan tracert tampaknya merupakan awal yang cukup baik untuk men-debug masalah transportasi jaringan. <br><br>  Pertama, saya menjalankan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PsPing</a> pada port TCP <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Alvin</a> .  Saya menggunakan opsi default - tidak ada yang istimewa.  Dari lebih dari seribu ping, tidak ada yang melebihi 10 ms, dengan pengecualian yang pertama untuk pemanasan.  Ini bertentangan dengan peningkatan yang diamati dalam keterlambatan 50 ms dalam persentil ke-99: di sana, untuk setiap 100 permintaan, kita harus melihat tentang satu permintaan dengan penundaan 50 ms. <br><br>  Lalu saya mencoba <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tracert</a> : mungkin masalahnya ada di salah satu node di sepanjang rute antara Alvin dan klien.  Tapi pelacak itu kembali dengan tangan kosong. <br><br>  Jadi, alasan penundaan itu bukan karena kode saya, bukan implementasi gRPC, dan bukan jaringan.  Saya sudah mulai khawatir bahwa saya tidak akan pernah mengerti ini. <br><br><h3>  Sekarang apa OS kita </h3><br>  <code>gRPC</code> banyak digunakan di Linux, tetapi eksotis untuk Windows.  Saya memutuskan untuk melakukan percobaan yang berhasil: Saya menciptakan mesin virtual Linux, mengkompilasi Alvin untuk Linux, dan menggunakannya. <br><br><img src="https://habrastorage.org/webt/z1/t8/tk/z1t8tkyhrobvurzcdqlmpdtetzc.png"><br><br>  Dan inilah yang terjadi: server Linux ping-pong tidak memiliki penundaan seperti simpul Windows yang serupa, meskipun sumber data tidak berbeda.  Ternyata masalahnya adalah dalam mengimplementasikan gRPC untuk Windows. <br><br><h3>  Algoritma Nagle </h3><br>  Selama ini, saya pikir saya kehilangan bendera <code>gRPC</code> .  Sekarang saya menyadari bahwa ini <code>gRPC</code> tidak memiliki bendera Windows di <code>gRPC</code> .  Saya menemukan perpustakaan RPC internal, di mana saya yakin itu berfungsi dengan baik untuk semua flag <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Winsock yang</a> diinstal.  Kemudian dia menambahkan semua flag ini ke gRPC dan menyebarkan Alvin ke Windows, di server ping-pong tetap untuk Windows! <br><br><img src="https://habrastorage.org/webt/7o/it/sf/7oitsfp2rzxotix0cf1xhktbrri.png"><br><br>  <i>Hampir</i> selesai: Saya mulai menghapus bendera yang ditambahkan satu per satu sampai regresi kembali, sehingga saya dapat menentukan penyebabnya.  Itu adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">TCP_NODELAY yang</a> terkenal, sebuah saklar dari algoritma Nagle. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Algoritme Neigl</a> mencoba untuk mengurangi jumlah paket yang dikirim melalui jaringan dengan menunda pengiriman pesan hingga ukuran paket melebihi jumlah byte tertentu.  Meskipun ini mungkin menyenangkan bagi pengguna rata-rata, ini merusak untuk server waktu-nyata, karena OS akan menunda beberapa pesan, menyebabkan keterlambatan pada QPS rendah.  <code>gRPC</code> menetapkan flag ini dalam implementasi Linux untuk soket TCP, tetapi tidak untuk Windows.  Saya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">memperbaikinya</a> . <br><br><h1>  Kesimpulan </h1><br>  Penundaan besar dalam QPS rendah disebabkan oleh optimasi OS.  Melihat ke belakang, pembuatan profil tidak mendeteksi penundaan karena dilakukan dalam mode kernel dan tidak dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mode pengguna</a> .  Saya tidak tahu apakah mungkin untuk mengamati algoritma Nagle melalui tangkapan ETW, tapi itu akan menarik. <br><br>  Adapun percobaan localhost, itu mungkin tidak menyentuh kode jaringan yang sebenarnya, dan algoritma Neigl tidak memulai, sehingga masalah penundaan menghilang ketika klien menghubungi Alvin melalui localhost. <br><br>  Lain kali Anda melihat peningkatan latensi sambil mengurangi jumlah permintaan per detik, algoritma Neigl harus ada dalam daftar tersangka Anda! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id451904/">https://habr.com/ru/post/id451904/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id451894/index.html">Tinjauan singkat dan segar arsitektur kompiler</a></li>
<li><a href="../id451896/index.html">EyeDisk "yang tidak dapat dipecahkan" dilindungi oleh pemindaian iris, tetapi mengirimkan kata sandi dalam teks yang jelas</a></li>
<li><a href="../id451898/index.html">Inovasi dalam bahasa Rusia</a></li>
<li><a href="../id451900/index.html">Kontribusi pertama ke API browser dari Facebook</a></li>
<li><a href="../id451902/index.html">Kamp Pengembang Microsoft Azure Rusia</a></li>
<li><a href="../id451906/index.html">Exchange Vulnerability: Cara Mendeteksi Peningkatan Privilege ke Administrator Domain</a></li>
<li><a href="../id451908/index.html">Sejarah komputer: malam di Yandex Museum</a></li>
<li><a href="../id451912/index.html">Jaringan saraf mendalam MuseNet menulis musik</a></li>
<li><a href="../id451916/index.html">PHP asinkron dan kisah satu sepeda</a></li>
<li><a href="../id451918/index.html">Untuk pertanyaan TI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>