<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüéì ü§† ü§¶üèø C√≥mo Yandex ense√±√≥ inteligencia artificial para encontrar errores en las noticias üÜö üíâ üåç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A menudo hablamos de tecnolog√≠as y bibliotecas que se originaron y formaron en Yandex. De hecho, al menos aplicamos y desarrollamos soluciones de terc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo Yandex ense√±√≥ inteligencia artificial para encontrar errores en las noticias</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/479662/">  A menudo hablamos de tecnolog√≠as y bibliotecas que se originaron y formaron en Yandex.  De hecho, al menos aplicamos y desarrollamos soluciones de terceros. <br><br>  Hoy contar√© a la comunidad Habr sobre uno de esos ejemplos.  Aprender√° por qu√© ense√±amos a la red neuronal BERT a buscar errores tipogr√°ficos en los titulares de las noticias, y no us√≥ el modelo ya hecho, por qu√© no puede obtener y ejecutar BERT en varias tarjetas de video y c√≥mo usamos la caracter√≠stica clave de esta tecnolog√≠a: el mecanismo de atenci√≥n. <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br><a name="habracut"></a><h2>  Desaf√≠o </h2><br>  Yandex.News es un servicio que recopila noticias de publicaciones relacionadas con nosotros.  Estas no son solo las noticias de los medios m√°s le√≠das y citadas en la p√°gina principal, sino tambi√©n secciones <a href="https://yandex.ru/sport">tem√°ticas</a> o incluso selecciones personales de todas las publicaciones.  En cualquier caso, estos son miles de sitios y millones de encabezados, de los cuales la m√°quina debe formar una selecci√≥n cada pocos minutos. <br><br>  Es la m√°quina, porque nunca intervenimos en la imagen del d√≠a: no agregamos noticias all√≠ manualmente, no las eliminamos de all√≠ (no importa cu√°nto nos gustar√≠a), no editamos los titulares.  Alrededor de esto ya se han roto muchas copias.  Un enfoque totalmente algor√≠tmico tiene ventajas y desventajas.  Algo que podemos mejorar con la tecnolog√≠a, algo que no.  Incluso si hay errores ortogr√°ficos o errores tipogr√°ficos en los encabezados, no los corregimos.  Hemos agregado los favicons de publicaciones a los titulares para que quede claro de d√≥nde provienen las noticias.  Esto ayud√≥ en parte, pero no llegamos a un acuerdo con los errores y comenzamos a buscar una forma de deshacernos de ellos sin hacer cambios en el texto. <br><br>  Si es imposible corregir el error, puede entrenar la m√°quina para encontrar encabezados que, debido a errores, no sean adecuados para la parte superior.  Adem√°s, Yandex se ha especializado en la morfolog√≠a rusa desde el momento en que el nombre a√∫n no se hab√≠a inventado.  Parece que tomamos una red neuronal, y el punto est√° en el sombrero. <br><br><h2>  Las herramientas </h2><br>  Yandex tiene tecnolog√≠a <a href="https://yandex.ru/dev/speller/">Speller</a> para encontrar y corregir errores.  Gracias a la <a href="https://habr.com/ru/company/yandex/blog/333522/">biblioteca de</a> aprendizaje autom√°tico <a href="https://habr.com/ru/company/yandex/blog/333522/">CatBoost,</a> Speller puede descifrar palabras irreconocibles ("adjetivos" ‚Üí "compa√±eros de clase") y tener en cuenta el contexto al buscar errores tipogr√°ficos ("m√∫sica perdida" ‚Üí "descargar m√∫sica").  Puede parecer que Speller es ideal para nuestra tarea, pero no. <br><br>  El deletreador (conocido internamente como el guardi√°n de b√∫squeda) ya est√° afilado a nivel de arquitectura para resolver una tarea completamente diferente: ayudar a los usuarios a restaurar el formulario de solicitud correcto.  En la b√∫squeda, no es tan importante si el caso se seleccion√≥ correctamente, si se coloca una letra may√∫scula o una coma.  All√≠, es m√°s importante para la consulta de b√∫squeda "Haminguel" adivinar que la persona ten√≠a en mente a Hemingway. <br><br>  Los errores en los titulares son cometidos por personas relativamente alfabetizadas que es poco probable que escriban Haminguel.  Pero la aprobaci√≥n incorrecta ("el vuelo se retras√≥"), las palabras faltantes ("el joven intent√≥ el autom√≥vil") y las letras may√∫sculas adicionales ("Presidente del Banco") son comunes.  Finalmente, hay una oraci√≥n formalmente correcta "Reparar√© la calle Gorki en Pskov", a la que un tutor normal no se aferrar√° (bueno, ¬øy si esto es una promesa del autor?), Pero este es obviamente un titular de noticias estropeado.  Adem√°s, en las Noticias la tarea no era la misma que en la B√∫squeda: no corregir errores tipogr√°ficos y errores, sino detectarlos. <br><br>  Ten√≠amos otras opciones, por ejemplo, modelos basados ‚Äã‚Äãen DSSM (si es interesante, hablamos brevemente sobre este enfoque en una publicaci√≥n sobre <a href="https://habr.com/ru/company/yandex/blog/314222/">el algoritmo Palekh</a> ), pero tambi√©n ten√≠an limitaciones.  Por ejemplo, el orden de las palabras no se tuvo en cuenta perfectamente. <br><br>  En general, las herramientas preparadas no eran adecuadas para nuestra tarea o eran limitadas.  Por lo tanto, debe crear el suyo propio para entrenar a su modelo.  Y esta fue una buena raz√≥n para trabajar con la tecnolog√≠a BERT, que estuvo disponible para los desarrolladores en 2018 y mostr√≥ resultados impresionantes. <br><br><h2>  Introduciendo BERT </h2><br>  El principal problema de los problemas modernos del procesamiento del lenguaje natural (PNL) es encontrar suficientes ejemplos marcados por personas para entrenar una red neuronal.  Si necesita un crecimiento de calidad, la muestra de capacitaci√≥n debe ser muy grande: millones y miles de millones de ejemplos.  Al mismo tiempo, hay muchas tareas en PNL y todas son diferentes.  Recopilar datos en vol√∫menes similares para cada tarea es largo, costoso y, a menudo, imposible.  Incluso para las empresas m√°s grandes del mundo. <br><br>  Pero hay una opci√≥n para solucionar este problema, con la ayuda del entrenamiento en dos etapas.  Al principio, a la red neuronal se le ense√±a una estructura de lenguaje durante un tiempo largo y costoso en un gran cuerpo de miles de millones de palabras (esto es entrenamiento previo).  Luego, la red se retuerce r√°pida y econ√≥micamente para una tarea espec√≠fica, por ejemplo, dividir las revisiones en buenas y malas (esto es un ajuste fino).  Bastantes de unos 10 mil ejemplos marcados en <a href="https://habr.com/ru/company/yandex/blog/305956/">Tolok</a> . <br><br>  La tecnolog√≠a BERT (Representaciones de codificador bidireccional de transformadores) se basa en esta idea.  La idea en s√≠ no es nueva y se ha aplicado antes, pero hay una diferencia significativa.  Transformer es una arquitectura de red neuronal que le permite tener en cuenta todo el contexto a la vez, incluido el otro extremo de la oraci√≥n y la rotaci√≥n de participios en alg√∫n punto intermedio.  Y esta es su diferencia con las arquitecturas de moda anteriores, que tuvieron en cuenta el contexto.  Por ejemplo, una red neuronal LSTM tiene una longitud de contexto de decenas de palabras en el mejor de los casos, y aqu√≠ son 200. <br><br>  En <a href="https://github.com/google-research/bert">GitHub</a> , el c√≥digo fuente de TensorFlow e incluso un modelo universal pre-entrenado en 102 idiomas est√°n disponibles, desde ruso hasta volapyuk.  Tome, al parecer, la soluci√≥n lista para usar, y obtenga el resultado de inmediato.  Pero no <br><br>  Result√≥ que el modelo universal en textos en ruso mostr√≥ una calidad significativamente menor que el modelo en ingl√©s, rompiendo r√©cords en textos en ingl√©s (lo cual, como ve, es l√≥gico).  En textos rusos, perdi√≥ con nuestros modelos internos en DSSM. <br><br>  Bien, puedes pre-educarte a ti mismo, afortunadamente, Yandex tiene suficientes textos rusos y experiencia en aprendizaje autom√°tico.  Pero hay un matiz.  ¬°Toma un a√±o aprender! <br><br>  El hecho es que BERT est√° dise√±ado para procesadores de tensor de Google (TPU), por lo tanto, de f√°brica puede funcionar con una sola tarjeta de video (GPU).  Y es imposible paralelizar la frente de uno con cualquier <a href="https://github.com/horovod">horovod</a> : transferir 400 megabytes de datos de una tarjeta a otra en cada paso es muy costoso, la paralelizaci√≥n ser√° in√∫til.  Que hacer <br><br><h2>  Optimizaci√≥n </h2><br>  Comenzaron a buscar ideas y soluciones que pudieran acelerar significativamente el asunto.  En primer lugar, notamos que cada n√∫mero en nuestro modelo ocupaba 32 bits de memoria (el flotante est√°ndar para los n√∫meros en la computadora).  Parece ser peque√±o, pero cuando tienes 100 millones de pesos, esto es cr√≠tico.  No necesit√°bamos tal precisi√≥n en todas partes, por lo que decidimos convertir parcialmente los n√∫meros al formato de 16 bits (esto es lo que se llama entrenamiento de precisi√≥n mixta). <br><br>  En el camino, con la ayuda de muchos archivos y muletas, atornillamos la compilaci√≥n de XLA, confiando en la <a href="https://github.com/google-research/bert/pull/255">confirmaci√≥n de</a> NVIDIA que a√∫n estaba sin procesar.  Gracias a esto, nuestras tarjetas NVIDIA Tesla V100 (un peque√±o servidor de ellas se encuentra como un apartamento en un √°rea econ√≥mica de Mosc√∫) pudieron revelar completamente su potencial debido a la aritm√©tica de 16 bits en los n√∫cleos tensores. <br><br>  Est√°bamos interesados ‚Äã‚Äãsolo en los titulares en ruso, pero el modelo multiling√ºe, que tomamos como base, fue entrenado en cientos de idiomas, incluido incluso un volapuk artificial.  Las palabras de todos los idiomas traducidos al espacio vectorial se almacenaron en el modelo.  Adem√°s, no puede tomarlos y simplemente eliminarlos de all√≠: tuve que sudar para reducir el tama√±o del diccionario. <br><br>  Y una cosa m√°s.  Si usted es un cient√≠fico y su computadora est√° debajo de la mesa, puede reconfigurar todo para cada tarea espec√≠fica.  Pero en una nube inform√°tica real, donde miles de m√°quinas est√°n configuradas de la misma manera, es bastante problem√°tico, por ejemplo, reconstruir el n√∫cleo para cada nueva caracter√≠stica de TensorFlow.  Por lo tanto, dedicamos mucho esfuerzo a recopilar esas versiones de paquetes que pueden hacer todos los chips nuevos y no requieren una actualizaci√≥n radical y la reconfiguraci√≥n de las tarjetas de video en la nube. <br><br>  En general, exprimieron todos los jugos donde pudieron.  Y lo hicimos  El a√±o se convirti√≥ en una semana. <br><br><h2>  Entrenamiento </h2><br>  Construir el conjunto de datos correcto suele ser la parte m√°s dif√≠cil del trabajo.  Primero, aprendimos el clasificador en tres millones de encabezados marcados con tolokers.  Parece ser mucho, pero solo 30 mil de ellos, con errores tipogr√°ficos.  ¬øD√≥nde obtener m√°s ejemplos? <br><br>  Decidimos ver qu√© t√≠tulos corrigieron los propios medios.  Hay m√°s de 2 millones de tales en la historia de Yandex.News.  Bingo!  Aunque era demasiado temprano para alegrarse. <br><br>  Result√≥ que muy a menudo los medios rehacen los titulares no por errores.  Nuevos detalles salieron a la luz, y el editor reemplaz√≥ una redacci√≥n correcta por otra.  Por lo tanto, nos limitamos a correcciones con una diferencia entre versiones de hasta tres letras (aunque todav√≠a hab√≠a algo de ruido aqu√≠: se "encontr√≥ dos" - se convirti√≥ en "encontr√≥ tres").  As√≠ que anotamos un mill√≥n de errores tipogr√°ficos.  Primero estudiamos en esta gran selecci√≥n con ruido, y luego en un peque√±o marcador sin ruido. <br><br><h2>  Calidad </h2><br>  En tales tareas, es costumbre medir la precisi√≥n y la integridad.  En nuestro caso, la precisi√≥n es la proporci√≥n de veredictos correctos entre todos los veredictos sobre un error en el encabezado.  Integridad: la proporci√≥n de encabezados de error que detectamos entre todos los encabezados de error.  Tanto eso como otro en el mundo ideal deber√≠an aspirar al 100%.  Pero en las tareas de aprendizaje autom√°tico, estos indicadores tienden a entrar en conflicto.  Es decir, cuanto m√°s giramos la precisi√≥n, m√°s cae la integridad.  Y viceversa. <br><br>  En nuestro enfoque anterior basado en DSSM, ya hemos logrado un 95% de precisi√≥n (es decir, un 5% de veredictos de falsos positivos).  Esto ya es un indicador bastante alto.  Por lo tanto, decidimos mantener el mismo nivel de precisi√≥n y ver c√≥mo cambia la integridad con el nuevo modelo.  Y ella salt√≥ del 21 al 78%.  Y definitivamente es un √©xito. <br><br>  Aqu√≠ ser√≠a posible ponerle fin, pero recuerdo la promesa de hablar de atenci√≥n. <br><br><h2>  Red neuronal con rotulador </h2><br>  En general, se acepta que una red neuronal es una caja negra.  Alimentamos algo a la entrada y obtenemos algo a la salida.  Por qu√© y c√≥mo es un misterio. <br><br>  Esta limitaci√≥n est√° destinada a eludir las redes neuronales interpretadas.  BERT es uno de ellos.  Su interpretabilidad radica en el mecanismo de atenci√≥n.  Hablando en t√©rminos generales, en cada capa de la red neuronal repetimos la misma t√©cnica: miramos las palabras vecinas con diferente "atenci√≥n" y tomamos en cuenta la interacci√≥n con ellas.  Por ejemplo, cuando una red neuronal procesa el pronombre "√©l", "mira cuidadosamente" el sustantivo al que se refiere "√©l". <br><br>  La siguiente imagen muestra en diferentes tonos de rojo qu√© palabras "mira" el token, que acumula informaci√≥n sobre el t√≠tulo completo de la capa de clasificaci√≥n final.  Si un error tipogr√°fico en la palabra, la atenci√≥n lo resalta, si las palabras son inconsistentes, entonces ambas (y, posiblemente, dependen de ellas). <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br>  En este lugar, por cierto, uno puede discernir el potencial completo de las redes neuronales.  En ninguna etapa de la capacitaci√≥n, nuestro modelo sabe exactamente d√≥nde se encuentra el error tipogr√°fico en el ejemplo: solo sabe que todo el t√≠tulo es incorrecto.  Y a√∫n as√≠, ella aprende que "una escuela para 1224 lugares" es incorrecta de escribir debido a un n√∫mero inconsistente, y destaca espec√≠ficamente el n√∫mero 4. <br><br>  No nos detuvimos en los errores tipogr√°ficos y comenzamos a aplicar un nuevo enfoque no solo para buscar errores, sino tambi√©n para identificar encabezados obsoletos.  Pero esta es una historia completamente diferente con la que esperamos volver a Habr en el futuro cercano. <br><br><h2>  Enlaces √∫tiles para aquellos que quieran profundizar en el tema. </h2><br><ul><li>  <a href="https://github.com/google-research/bert">C√≥digo TensorFlow y modelos pre-entrenados para BERT</a> </li><li>  <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">Open Sourcing BERT: pre-entrenamiento de vanguardia para el procesamiento del lenguaje natural</a> </li><li>  <a href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPU vs GPU para transformadores (BERT)</a> </li><li>  <a href="http://jalammar.github.io/illustrated-transformer/">El transformador ilustrado.</a> </li><li>  <a href="https://news.developer.nvidia.com/nvidia-achieves-4x-speedup-on-bert-neural-network/">NVIDIA logra una aceleraci√≥n 4X en la red neuronal BERT</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/479662/">https://habr.com/ru/post/479662/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../479646/index.html">Malos consejos o razones para seguir aprendiendo ingl√©s despu√©s de intermedio</a></li>
<li><a href="../479650/index.html">Las 12 infograf√≠as din√°micas din√°micas de TI m√°s interesantes</a></li>
<li><a href="../479654/index.html">Django vue generador</a></li>
<li><a href="../479656/index.html">PostgreSQL Antipatterns: estad√≠sticas alrededor de la cabeza</a></li>
<li><a href="../479660/index.html">3. An√°lisis de malware mediante el an√°lisis forense de Check Point. Chorro de arena m√≥vil</a></li>
<li><a href="../479664/index.html">C√≥mo funcionan los kubernetes gestionados y OpenShift gestionados en IBM Cloud. Parte 1 - Arquitectura y seguridad</a></li>
<li><a href="../479666/index.html">Golang: ¬øEn qu√© conf√≠a un especialista en Go en un mar de especialidades de TI?</a></li>
<li><a href="../479668/index.html">QA para principiantes: ¬øc√≥mo probar un cohete o avi√≥n?</a></li>
<li><a href="../479672/index.html">Puede rastrear</a></li>
<li><a href="../479676/index.html">ExtJS 7 y Spring Boot 2. ¬øC√≥mo construir un SPA que interact√∫e con su API y complementos externos de ReactJS?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>