<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëåüèø üê∞ üì§ Ya veo, significa que existo: una revisi√≥n de Deep Learning in Computer Vision (parte 1) üõ£Ô∏è üßñüèæ ‚ôæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Visi√≥n por computadora. Ahora hablan mucho sobre eso, donde se aplica e implementa mucho. Y de alguna manera hace bastante tiempo no hab√≠a art√≠culos d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ya veo, significa que existo: una revisi√≥n de Deep Learning in Computer Vision (parte 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Visi√≥n por computadora.  Ahora hablan mucho sobre eso, donde se aplica e implementa mucho.  Y de alguna manera hace bastante tiempo no hab√≠a art√≠culos de revisi√≥n sobre Habr√© en CV, con ejemplos de arquitecturas y tareas modernas.  Pero hay muchos, ¬°y son realmente geniales!  Si est√° interesado en lo que est√° sucediendo en Computer Vision ahora, no solo desde el punto de vista de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">investigaci√≥n y los art√≠culos</a> , sino tambi√©n desde el punto de vista de los problemas aplicados, entonces es bienvenido a cat.  Adem√°s, el art√≠culo puede ser una buena introducci√≥n para aquellos que siempre han querido comenzar a entender todo esto, pero algo estaba en el camino;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="imagen"><br><a name="habracut"></a><br>  Hoy en el PhysTech hay una colaboraci√≥n activa de la "Academia" y socios industriales.  En particular, hay muchos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">laboratorios interesantes</a> de compa√±√≠as como Sberbank, Biocad, 1C, Tinkoff, MTS, Huawei en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Escuela de Matem√°ticas Aplicadas y Ciencias de la Computaci√≥n de PhysTech</a> . <br><br>  Me inspir√≥ para escribir este art√≠culo trabajando en el Laboratorio de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sistemas Inteligentes H√≠bridos</a> , inaugurado por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VkusVill</a> .  El laboratorio tiene una tarea ambiciosa: construir una tienda que funcione sin cajas, principalmente con la ayuda de la visi√≥n por computadora.  Durante casi un a√±o de trabajo, tuve la oportunidad de trabajar en muchas tareas de visi√≥n, que se discutir√°n en estas dos partes. <br><br><div class="spoiler">  <b class="spoiler_title">¬øComprar sin mostradores de caja?</b>  <b class="spoiler_title">En alg√∫n lugar ya lo escuch√© ...</b> <div class="spoiler_text">  Probablemente, querido lector, pensaste en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Amazon Go</a> .  En cierto sentido, la tarea es repetir su √©xito, pero nuestra decisi√≥n tiene m√°s que ver con la implementaci√≥n que con construir una tienda desde cero por un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mont√≥n de dinero</a> . <br></div></div><br>  Nos moveremos de acuerdo al plan: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Motivaci√≥n y lo que est√° pasando.</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Clasificaci√≥n como estilo de vida</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Arquitecturas de redes neuronales convolucionales: 1000 formas de lograr un objetivo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Visualizaci√≥n de redes neuronales convolucionales: mu√©strame pasi√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Yo mismo soy una especie de cirujano: extraemos caracter√≠sticas de las redes neuronales</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mantente cerca: aprendizaje de representaci√≥n para personas y personas</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: <s>detectar, evaluar posturas y reconocer acciones</s> sin spoilers</a> </li></ol><br><a name="1"></a><h2>  Motivaci√≥n y lo que est√° pasando. </h2><br><div class="spoiler">  <b class="spoiler_title">¬øPara qui√©n es el art√≠culo?</b> <div class="spoiler_text">  El art√≠culo se centra m√°s en personas que ya est√°n familiarizadas con el aprendizaje autom√°tico y las redes neuronales.  Sin embargo, le aconsejo que lea al menos las dos primeras secciones, de repente todo estar√° claro :) <br></div></div><br>  En 2019, todos hablan de inteligencia artificial, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cuarta revoluci√≥n industrial</a> y el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque de la humanidad hacia una singularidad</a> .  Genial, genial, pero quiero detalles.  Despu√©s de todo, somos t√©cnicos curiosos que no creen en los cuentos de hadas sobre IA, creemos en el establecimiento de tareas formales, las matem√°ticas y la programaci√≥n.  En este art√≠culo, hablaremos sobre casos espec√≠ficos del uso de la IA muy moderna: el uso del aprendizaje profundo (es decir, redes neuronales convolucionales) en una variedad de tareas de visi√≥n por computadora. <br><br>  S√≠, hablaremos espec√≠ficamente sobre las redes, a veces mencionaremos algunas ideas desde una visi√≥n "cl√°sica" (llamaremos al conjunto de m√©todos en visi√≥n que se usaron antes de las redes neuronales, pero esto de ninguna manera significa que no se usen ahora). <br><br><div class="spoiler">  <b class="spoiler_title">Quiero aprender la visi√≥n por computadora desde cero</b> <div class="spoiler_text">  Recomiendo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el curso de Anton Konushin "Introducci√≥n a la visi√≥n por computadora"</a> .  Personalmente, revis√© su contraparte en SHAD, que sent√≥ una base s√≥lida para comprender el procesamiento de im√°genes y videos. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="imagen" width="300"></div><br>  En mi opini√≥n, la primera aplicaci√≥n realmente interesante de redes neuronales en visi√≥n, que fue cubierta en los medios en 1993, es el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">reconocimiento de escritura a mano por</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Jan LeCun</a> .  Ahora es uno de los principales AI en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Facebook AI Research</a> , su equipo ya ha lanzado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muchas cosas √∫tiles de c√≥digo abierto</a> . <br><br>  Hoy, la visi√≥n se usa en muchas √°reas.  Dar√© solo algunos ejemplos sorprendentes: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="imagen" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="imagen" width="400" height="300"></div><br><br>  <i>Veh√≠culos no tripulados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tesla</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="imagen" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">An√°lisis de im√°genes m√©dicas</a> y <a href="">predicci√≥n del c√°ncer.</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="imagen" width="500"></div><br><br>  <i>Consolas de juegos: Kinect 2.0 (aunque tambi√©n usa informaci√≥n de profundidad, es decir, im√°genes RGB-D)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="imagen" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Reconocimiento facial: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Apple FaceID</a> (usando m√∫ltiples sensores)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="imagen" width="400"></div><br><br>  <i>Clasificaci√≥n de puntos faciales: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">m√°scaras de Snapchat</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="imagen" width="400"></div><br><br>  <i>Biometr√≠a de los movimientos de la cara y los ojos (un ejemplo del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">proyecto de FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="imagen" width="400"></div><br><br>  <i>B√∫squeda por imagen: Yandex y Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="imagen" width="500"></div><br><br>  <i>Reconocimiento del texto en la imagen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Reconocimiento √≥ptico de caracteres</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="imagen" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="imagen" width="400"></div><br><br>  <i>Drones y robots: recibir y procesar informaci√≥n a trav√©s de la visi√≥n</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="imagen" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Odometr√≠a</a> : construcci√≥n de un mapa y planificaci√≥n al mover robots</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="imagen" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mejora de gr√°ficos y texturas en videojuegos</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="imagen" width="200" height="300"></div><br><br>  <i>Traducci√≥n de im√°genes: Yandex y Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="imagen" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="imagen" width="500"></div><br><br>  <i>Realidad aumentada: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Leap Motion (Project North Star)</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="imagen" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Transferencia de estilo y textura: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PicsArt</a></i> <br><br>  Sin mencionar las numerosas aplicaciones en diversas tareas internas de las empresas.  Facebook, por ejemplo, tambi√©n usa la visi√≥n para filtrar contenido multimedia.  Los m√©todos de visi√≥n por computadora tambi√©n se utilizan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en pruebas de calidad / da√±os en la industria</a> . <br><br>  La realidad aumentada aqu√≠ necesita, de hecho, recibir una atenci√≥n especial, ya <s>que no funciona</s> en el futuro cercano, esta puede convertirse en una de las principales √°reas de aplicaci√≥n de la visi√≥n. <br><br>  Motivado  Cargado  Vamos: <br><br><a name="2"></a><h2>  Clasificaci√≥n como estilo de vida </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="imagen"><br><br>  Como dije, en los a√±os 90, las redes fueron disparadas a la vista.  Y filmaron en una tarea espec√≠fica: la tarea de clasificar im√°genes de n√∫meros escritos a mano (el famoso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conjunto de datos MNIST</a> ).  Hist√≥ricamente, fue la tarea de clasificar im√°genes lo que se convirti√≥ en la base para resolver casi todas las tareas posteriores en visi√≥n.  Considere un ejemplo espec√≠fico: <br><br>  <b>Tarea</b> : se da una carpeta con fotos en la entrada, cada foto tiene un objeto particular: ya sea un gato, un perro o una persona (incluso si no hay fotos de "basura", es una tarea s√∫per no vital, pero debe comenzar en alg√∫n lugar).  <s><code>/leather_bags</code></s> descomponer las im√°genes en tres carpetas: <code>/cats</code> , <code>/dogs</code> y <s><code>/leather_bags</code></s> <code>/humans</code> , colocando solo fotos con los objetos correspondientes en cada carpeta. <br><br><div class="spoiler">  <b class="spoiler_title">¬øQu√© es una foto / foto?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="imagen"><br>  Casi en todas partes en visi√≥n es habitual trabajar con im√°genes en formato RGB.  Cada imagen tiene una altura (H), un ancho (W) y una profundidad de 3 (colores).  Por lo tanto, una imagen se puede representar como un tensor de dimensi√≥n HxWx3 (cada p√≠xel es un conjunto de tres n√∫meros: valores de intensidad en los canales). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Imagine que todav√≠a no estamos familiarizados con la visi√≥n por computadora, pero conocemos el aprendizaje autom√°tico.  Las im√°genes son simplemente tensores num√©ricos en la memoria de la computadora.  Formalizamos el problema en t√©rminos de aprendizaje autom√°tico: los objetos son im√°genes, sus signos son valores en p√≠xeles, la respuesta para cada uno de los objetos es una etiqueta de clase (gato, perro o persona).  Esta es una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tarea de clasificaci√≥n</a> pura. <br><br><div class="spoiler">  <b class="spoiler_title">Si ahora se ha vuelto dif√≠cil ...</b> <div class="spoiler_text">  ... es mejor leer primero los primeros 4 art√≠culos del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">curso abierto OpenDataScience ML</a> y familiarizarse con un art√≠culo m√°s introductorio sobre visi√≥n, por ejemplo, una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buena conferencia en Small ShAD</a> . <br></div></div><br>  Puede tomar algunos m√©todos desde la vista "cl√°sica" o el aprendizaje autom√°tico "cl√°sico", es decir, no una red neuronal.  B√°sicamente, estos m√©todos consisten en resaltar las im√°genes de ciertas caracter√≠sticas (puntos especiales) o regiones locales que caracterizar√°n la imagen (" <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bolsa de palabras visuales</a> ").  Por lo general, todo se reduce a algo como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SVM</a> sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SIFT</a> . <br><br>  Pero nos reunimos aqu√≠ para hablar sobre redes neuronales, por lo que no queremos usar los signos que inventamos, pero queremos que la red haga todo por nosotros.  Nuestro clasificador tomar√° los signos de un objeto como entrada y devolver√° una predicci√≥n (etiqueta de clase).  Aqu√≠, los valores de intensidad en p√≠xeles act√∫an como signos (vea el modelo de imagen en <br>  spoiler arriba).  Recuerde que una imagen es un tensor de tama√±o (Altura, Ancho, 3) (si es color).  Cuando se aprende a ingresar a la cuadr√≠cula, todo esto generalmente no es servido por una imagen y no por un conjunto de datos completo, sino por lotes, es decir,  en peque√±as porciones de objetos (por ejemplo, 64 im√°genes en el lote). <br><br>  Por lo tanto, la red recibe un tensor de entrada de tama√±o (BATCH_SIZE, H, W, 3).  Puede "expandir" cada imagen en una l√≠nea vectorial de n√∫meros H * W * 3 y trabajar con los valores en p√≠xeles al igual que con los signos en el aprendizaje autom√°tico, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Perceptr√≥n Multicapa (MLP)</a> normal har√≠a exactamente eso, pero honestamente, es as√≠ l√≠nea de base, ya que trabajar con p√≠xeles como una fila de vectores no tiene en cuenta, por ejemplo, la invariancia traslacional de los objetos en la imagen.  El mismo gato puede estar en el medio de la foto, y en la esquina, MLP no aprender√° este patr√≥n. <br><br>  Entonces necesita algo m√°s inteligente, por ejemplo, una operaci√≥n de convoluci√≥n.  Y se trata de la visi√≥n moderna, de <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las redes neuronales convolucionales</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">El c√≥digo de entrenamiento de la red de convoluci√≥n puede verse m√°s o menos as√≠ (en el marco PyTorch)</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Como ahora estamos hablando de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrenar con un maestro</a> , necesitamos varios componentes para entrenar una red neuronal: <br><br><ul><li>  Datos (ya existe) </li><li>  Arquitectura de red (resaltado) </li><li>  Una funci√≥n de p√©rdida que dir√° c√≥mo debe aprender la red neuronal (aqu√≠ ser√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrop√≠a cruzada</a> ) </li><li>  M√©todo de optimizaci√≥n (cambiar√° el peso de la red en la direcci√≥n correcta) </li><li>  Defina la arquitectura y los hiperpar√°metros del optimizador (por ejemplo, tama√±o del paso del optimizador, n√∫mero de neuronas en capas, coeficientes de regularizaci√≥n) </li></ul><br>  Esto es exactamente lo que se implementa en el c√≥digo; la propia red neuronal convolucional se describe en la clase Net (). <br><br>  Si desea aprender lentamente y desde el principio sobre paquetes y redes de convoluci√≥n, le recomiendo una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conferencia en la Escuela de Aprendizaje Profundo (MIPT MIPT) (en ruso)</a> sobre este tema y, por supuesto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el curso cs231n de Stanford (en ingl√©s)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Escuela de aprendizaje profundo: ¬øqu√© es?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deep Learning School</a> en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Laboratorio de Innovaci√≥n FPMI MIPT</a> es una organizaci√≥n que participa activamente en el desarrollo de un curso abierto de idioma ruso en redes neuronales.  En el art√≠culo me referir√© a estos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">videos tutoriales</a> varias veces. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="imagen" width="650"></div><br>  En resumen, la operaci√≥n de convoluci√≥n le permite encontrar patrones en las im√°genes en funci√≥n de su variabilidad.  Cuando entrenamos redes neuronales convolucionales (eng: Redes neuronales convolucionales), de hecho, encontramos filtros de convoluci√≥n (pesos de neuronas) que describen bien las im√°genes, y tan bien que puede determinar con precisi√≥n la clase a partir de ellas.  Se han inventado muchas formas para construir dicha red.  M√°s de lo que piensas ... <br><br><a name="3"></a><h3>  Arquitecturas de redes neuronales convolucionales: 1000 formas de lograr un objetivo </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="imagen" width="500"></div><br><br>  S√≠, s√≠, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otra revisi√≥n arquitect√≥nica</a> .  ¬°Pero aqu√≠ tratar√© de hacerlo lo m√°s relevante posible! <br><br>  Primero fue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LeNet</a> , que ayud√≥ a Jan LeCun a reconocer los n√∫meros en 1998.  Esta fue la primera red neuronal convolucional para la clasificaci√≥n.  Su caracter√≠stica principal era que b√°sicamente comenz√≥ a usar operaciones de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">convoluci√≥n y agrupaci√≥n</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="imagen"><br><br>  Luego hubo una pausa en el desarrollo de las redes, pero el hardware no se detuvo; se desarrollaron c√°lculos efectivos en GPU y <abbr title="√Ålgebra Lineal Acelerada">XLA</abbr> .  En 2012, apareci√≥ AlexNet, particip√≥ en el concurso ILSVRC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ImageNet Large-Scale Visual Recognition Challenge Challenge</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Una peque√±a digresi√≥n sobre ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ImageNet</a> se ensambl√≥ en 2012, y se utiliz√≥ un subconjunto de miles de im√°genes y 1000 clases para la competencia ILSVRC.  ImageNet actualmente tiene ~ 14 millones de im√°genes y 21.841 clases (tomadas del sitio oficial), pero para la competencia, por lo general, solo seleccionan un subconjunto.  ILSVRC se convirti√≥ en la competencia anual de clasificaci√≥n de im√°genes m√°s grande.  Por cierto, recientemente descubrimos c√≥mo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrenar en ImageNet en cuesti√≥n de minutos</a> . <br><br>  Fue en ImageNet (en ILSVRC) de 2010 a 2018 que recibieron redes <abbr title="Estado del arte">SOTA</abbr> en la clasificaci√≥n de im√°genes.  Es cierto que desde 2016, las competencias en localizaci√≥n, detecci√≥n y comprensi√≥n de la escena, en lugar de la clasificaci√≥n, son m√°s relevantes. <br></div></div><br>  Por lo general, varias <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">revisiones arquitect√≥nicas</a> arrojan luz sobre las que fueron las primeras en el ILSVRC de 2010 a 2016, y en algunas redes individuales.  Para no saturar la historia, los coloqu√© debajo del spoiler a continuaci√≥n, tratando de enfatizar las ideas principales: <br><br><div class="spoiler">  <b class="spoiler_title">Arquitectura de 2012 a 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  Peso </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Alexnet</a> </td><td>  usa dos paquetes seguidos;  dividir el entrenamiento de la red en dos ramas paralelas </td><td>  240 MB </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Zfnet</a> </td><td>  tama√±o del filtro, n√∫mero de filtros en capas </td><td>  - </td></tr><tr><td>  2013 </td><td>  <a href="">Sobrepeso</a> </td><td>  uno de los primeros detectores de redes neuronales </td><td>  - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  profundidad de red (13-19 capas), el uso de varios bloques Conv-Conv-Pool con un tama√±o de convoluci√≥n menor (3x3) </td><td>  549MB (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">Inception (v1) (tambi√©n conocido como GoogLeNet)</a> </td><td>  1x1-convoluci√≥n (idea de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red en red</a> ), p√©rdidas auxiliares (o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">supervisi√≥n profunda</a> ), apilamiento de las salidas de varias convoluciones (bloque de inicio) </td><td>  - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conexiones residuales</a> , muy profundas (152 capas ..) </td><td>  98 MB (ResNet-50), 232 MB (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  Las ideas de todas estas arquitecturas (a excepci√≥n de ZFNet, generalmente se menciona poco) en un momento fueron una nueva palabra en las redes neuronales para la visi√≥n.  Sin embargo, despu√©s de 2015 hubo muchas mejoras m√°s importantes, por ejemplo, Inception-ResNet, Xception, DenseNet, SENet.  A continuaci√≥n intent√© recogerlos en un solo lugar. <br><br><div class="spoiler">  <b class="spoiler_title">Arquitectura de 2015 a 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  Peso </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Inicio v2 y v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">descomposici√≥n de paquetes en paquetes 1xN y Nx1</a> </td><td>  92 MB </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Inception v4 e Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">combinaci√≥n de inicio y ResNet</a> </td><td>  215 MB </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Resnext</a> </td><td>  2do lugar ILSVRC, el uso de muchas ramas (bloque de inicio "generalizado") </td><td>  - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">convoluci√≥n separable en profundidad</a> , pesa menos con una precisi√≥n comparable a Inception </td><td>  88 MB </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bloque denso</a>  ligero pero preciso </td><td>  33 MB (DenseNet-121), 80 MB (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bloqueo de exprimir y excitar</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">46 MB (SENet-Inception), 440 MB (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  La mayor√≠a de estos modelos para PyTorch se pueden encontrar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> , y hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">algo genial</a> . <br><br>  Es posible que haya notado que todo pesa bastante (me gustar√≠a un m√°ximo de 20 MB, o incluso menos), mientras que hoy en d√≠a usan dispositivos m√≥viles en todas partes y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">IoT est√°</a> ganando popularidad, lo que significa que tambi√©n desea usar redes all√≠. <br><br><div class="spoiler">  <b class="spoiler_title">Relaci√≥n entre peso y velocidad del modelo.</b> <div class="spoiler_text">  Dado que las redes neuronales dentro de s√≠ mismas solo multiplican los tensores, el n√∫mero de operaciones de multiplicaci√≥n (l√©ase: el n√∫mero de pesos) afecta directamente la velocidad del trabajo (si no se usa el procesamiento o preprocesamiento intensivo en mano de obra).  La velocidad de la red en s√≠ depende de la implementaci√≥n (marco), el hardware en el que se ejecuta y el tama√±o de la imagen de entrada. <br></div></div><br>  Los autores de muchos art√≠culos tomaron el camino de inventar arquitecturas r√°pidas, reun√≠ sus m√©todos en el siguiente spoiler: <br><br><div class="spoiler">  <b class="spoiler_title">Arquitectura ligera de CNN</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  Peso </th><th>  Ejemplo de implementaci√≥n </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Compresi√≥n FireModule</a> </td><td>  0,5 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">obtenido por una b√∫squeda neuronal de arquitecturas, esta es una red de la categor√≠a de AutoML</a> </td><td>  23 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">grupo puntiagudo conv, canal aleatorio</a> </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">circunvoluciones separables en profundidad y muchos otros trucos</a> </td><td>  16 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Recomiendo este art√≠culo sobre Habr√©</a> </td><td>  14 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Squeezenext</a> </td><td>  ver fotos en el repositorio original </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">b√∫squeda de arquitectura neuronal espec√≠ficamente para dispositivos m√≥viles que utilizan RL</a> </td><td>  ~ 2 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MobileNet (v3)</a> </td><td>  ella sali√≥ mientras escrib√≠a un art√≠culo :) </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Los n√∫meros en todas las tablas <s>se toman del techo</s> de los repositorios, de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la tabla de aplicaciones de Keras</a> y de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> . <br><br>  Usted pregunta: "¬øPor qu√© escribiste sobre todo este" zool√≥gico "de modelos?  ¬øY por qu√© es la tarea de clasificaci√≥n?  Pero queremos ense√±ar a las m√°quinas a ver, y la clasificaci√≥n es solo una especie de tarea estrecha ... ".  El hecho es que las redes neuronales para detectar objetos, evaluar posturas / puntos, volver a identificar y buscar en una imagen usan modelos precisos para la clasificaci√≥n como <b><abbr title="base, literalmente - la columna vertebral">columna vertebral</abbr></b> , y el 80% del √©xito depende de ellos. <br><br>  Pero de alguna manera quiero confiar m√°s en CNN, o pensaron en cajas negras, pero lo que est√° "adentro" no es obvio.  Para comprender mejor el mecanismo de funcionamiento de las redes convolucionales, a los investigadores se les ocurri√≥ el uso de la visualizaci√≥n. <br><br><a name="4"></a><h3>  Visualizaci√≥n de redes neuronales convolucionales: mu√©strame pasi√≥n </h3><br>  Un paso importante para comprender lo que est√° sucediendo dentro de las redes convolucionales es el art√≠culo <a href="">"Visualizar y comprender las redes convolucionales"</a> .  En √©l, los autores propusieron varias formas de visualizar exactamente a qu√© (en qu√© partes de la imagen) reaccionan las neuronas en diferentes capas de CNN (tambi√©n recomiendo ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una conferencia de Stanford sobre este tema</a> ).  Los resultados fueron muy impresionantes: los autores mostraron que las primeras capas de la red convolucional responden a algunas "cosas de bajo nivel" por el tipo de bordes / √°ngulos / l√≠neas, y las √∫ltimas capas ya responden a partes enteras de las im√°genes (ver la imagen a continuaci√≥n), es decir, ya llevan en s√≠ misma sem√°ntica. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="imagen"></div><br><br>  Adem√°s, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">proyecto de visualizaci√≥n profunda de la Universidad de Cornell y la compa√±√≠a</a> avanzaron a√∫n m√°s la visualizaci√≥n, mientras que el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">famoso DeepDream</a> aprendi√≥ a distorsionar en un estilo interesante y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">adictivo</a> (a continuaci√≥n se muestra una imagen de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="imagen" width="500"></div><br><br>  En 2017, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se public√≥</a> un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muy buen art√≠culo en Distill</a> , en el que realizaron un an√°lisis detallado de lo que "ve" cada capa, y m√°s recientemente (en marzo de 2019) Google invent√≥ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">atlas de activaci√≥n</a> : mapas √∫nicos que se pueden construir para cada capa de red, lo que nos acerca a comprender la imagen general del trabajo de CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Si quieres jugar con la visualizaci√≥n t√∫ mismo, recomendar√≠a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lucid</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorSpace</a> . <br><br>  De acuerdo, CNN parece ser cierto hasta cierto punto.  Necesitamos aprender a usar esto en otras tareas, y no solo en la clasificaci√≥n.  Esto nos ayudar√° a extraer im√°genes de Embedding'ov y Transfer Learning. <br><br><a name="5"></a><h2>  Yo mismo soy una especie de cirujano: extraemos caracter√≠sticas de las redes neuronales </h2><br>  Imagine que hay una imagen y queremos encontrar una que se vea visualmente (esto es, por ejemplo, la b√∫squeda en una imagen en Yandex.Pictures).  Anteriormente (antes de las redes neuronales), los ingenieros sol√≠an extraer caracter√≠sticas manualmente para esto, por ejemplo, inventando algo que describe bien la imagen y permite compararla con otras.  B√°sicamente, estos m√©todos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SIFT</a> ) funcionan con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">gradientes de imagen</a> , por lo general, estos elementos se denominan descriptores de imagen "cl√°sicos".  De particular inter√©s, me refiero al <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> y al <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">curso de Anton Konushin</a> (esto no es publicidad, solo un buen curso :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="imagen" width="500"></div><br><br>  Usando redes neuronales, no podemos inventar estas caracter√≠sticas y heur√≠sticas nosotros mismos, sino entrenar adecuadamente el modelo y luego <b>tomar la salida de una o m√°s capas de la red como signos de la imagen</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="imagen" width="650"></div><br>  Una mirada m√°s cercana a todas las arquitecturas deja en claro que hay dos pasos para la clasificaci√≥n en CNN: <br>  1)  Capas de <b>extractor de caracter√≠sticas</b> para extraer caracter√≠sticas informativas de im√°genes usando capas convolucionales <br>  2)  Aprendiendo sobre estas caracter√≠sticas capas de clasificador <b>totalmente conectadas (FC)</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="imagen" width="500"></div><br><br>  <b>La incrustaci√≥n de im√°genes (caracter√≠sticas) se</b> trata solo del hecho de que puede tomar sus signos despu√©s del extractor de caracter√≠sticas de una red neuronal convolucional (aunque se pueden agregar de diferentes maneras) como una descripci√≥n informativa de las im√°genes.  Es decir, capacitamos a la red para la clasificaci√≥n, y luego tomamos la salida frente a las capas de clasificaci√≥n.  Estos signos se denominan <i>caracter√≠sticas</i> , <i>descriptores de redes neuronales</i> o <i>incrustaciones de</i> im√°genes (aunque las incrustaciones generalmente se aceptan en la PNL, ya que esto es visi√≥n, a menudo hablar√© <i>caracter√≠sticas</i> ).  Por lo general, este es un tipo de vector num√©rico, por ejemplo, 128 n√∫meros, con el que ya puede trabajar. <br><br><div class="spoiler">  <b class="spoiler_title">¬øPero qu√© pasa con los codificadores autom√°ticos?</b> <div class="spoiler_text">  S√≠, de hecho, las caracter√≠sticas se pueden obtener mediante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">codificadores autom√°ticos</a> .  En mi pr√°ctica, lo hicieron de diferentes maneras, pero, por ejemplo, en art√≠culos sobre reidentificaci√≥n (que se discutir√°n m√°s adelante), con mayor frecuencia a√∫n toman caracter√≠sticas despu√©s del extractor, en lugar de entrenar el codificador autom√°tico para esto.  Me parece que vale la pena realizar experimentos en ambas direcciones, si la pregunta es qu√© funciona mejor. <br></div></div><br>  Por lo tanto, la tuber√≠a para resolver <b>el problema de b√∫squeda en una imagen</b> se puede organizar simplemente: ejecutamos las im√°genes a trav√©s de CNN, tomamos se√±ales de las capas deseadas y comparamos estas caracter√≠sticas entre s√≠ a partir de diferentes im√°genes.  Por ejemplo, simplemente consideramos la distancia euclidiana de estos vectores. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="imagen" width="500"></div><br><br>  <b>Transfer Learning</b> es una t√©cnica bien conocida para el entrenamiento efectivo de redes neuronales que ya est√°n capacitadas en un conjunto de datos espec√≠fico para su tarea.  A menudo tambi√©n dicen Ajuste fino en lugar de Aprendizaje de transferencia, en las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">notas del curso de Stanford cs231n</a> se comparten estos conceptos, dicen, Aprendizaje de transferencia es una idea general, y Ajuste fino es una de las implementaciones de la t√©cnica.  Esto no es tan importante para nosotros en el futuro, lo principal es entender que podemos entrenar a la red para predecir bien en el nuevo conjunto de datos, comenzando no por pesos aleatorios, sino por aquellos entrenados en alg√∫n tipo grande de ImageNet.  Esto es especialmente cierto cuando hay pocos datos y desea resolver el problema cualitativamente. <br><br><div class="spoiler">  <b class="spoiler_title">Aprenda m√°s sobre Transfer Learning</b> <div class="spoiler_text">  <a href="">Art√≠culo original</a> , pero <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øpor qu√© leer mucho texto si puedes ver el video?</a> <br></div></div><br>  Sin embargo, simplemente tomar las caracter√≠sticas necesarias y realizar capacitaci√≥n adicional del conjunto de datos al conjunto de datos puede no ser suficiente, por ejemplo, para tareas de b√∫squeda de personas / personas similares / algo espec√≠fico.  Las fotos de la misma persona visualmente a veces pueden ser a√∫n m√°s diferentes que las fotograf√≠as de diferentes personas.  Es necesario hacer que la red resalte exactamente los signos inherentes a una persona / objeto, incluso si es dif√≠cil para nosotros hacer esto con nuestros ojos.  Bienvenido al mundo del <b>aprendizaje</b> de <b>representaci√≥n</b> . <br><br><a name="6"></a><h2>  Mantente cerca: aprendizaje de representaci√≥n para personas y personas </h2><br><div class="spoiler">  <b class="spoiler_title">Nota de terminolog√≠a</b> <div class="spoiler_text">  Si lee art√≠culos cient√≠ficos, a veces parece que algunos autores entienden la frase <b>aprendizaje m√©trico de manera</b> diferente, y no hay consenso sobre qu√© m√©todos llamar aprendizaje m√©trico y cu√°les no.  Es por eso que en este art√≠culo decid√≠ evitar esta frase en particular y utilic√© un <b>aprendizaje de representaci√≥n</b> m√°s l√≥gico, algunos lectores pueden no estar de acuerdo con esto; estar√© encantado de discutirlo en los comentarios. <br></div></div><br>  Establecemos las tareas: <br><br><ul><li>  <b>Tarea 1</b> : hay una galer√≠a (conjunto) de fotograf√≠as de los rostros de las personas, queremos que la red pueda responder de acuerdo con una nueva foto, ya sea con el nombre de una persona de la galer√≠a (supuestamente esta es), o dijo que no hay tal persona en la galer√≠a (y, tal vez, le agregamos nueva persona) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>Tarea 2</b> : lo mismo, pero no estamos trabajando con fotograf√≠as de rostros, sino con personas de cuerpo entero. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  La primera tarea generalmente se llama <b>reconocimiento facial</b> , la segunda, <b>reidentificaci√≥n</b> (abreviada como <i>Reid</i> ).  Los combin√© en un bloque, porque sus soluciones usan ideas similares hoy: para aprender incrustaciones de im√°genes efectivas que pueden hacer frente a situaciones bastante dif√≠ciles, hoy usan diferentes tipos de p√©rdidas, como, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">p√©rdida de triplete</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">p√©rdida de cu√°druple</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">p√©rdida de centro contrastante, p√©rdida de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">coseno</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Todav√≠a hay maravillosas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">redes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">siamesas</a> , pero sinceramente no las us√© yo mismo.  Por cierto, no solo la p√©rdida en s√≠ misma "decide", sino c√≥mo muestrear pares de aspectos positivos y negativos para ello, enfatizan los autores del art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Las cuestiones de muestreo en el aprendizaje de inserci√≥n profunda</a> . <br><br>  La esencia de todas estas p√©rdidas y redes siamesas es simple: queremos que las im√°genes de una clase (persona) en el espacio latente de caracter√≠sticas (incrustaciones) est√©n "cercanas", y de diferentes clases (personas) est√©n "lejos".  La proximidad generalmente se mide de la siguiente manera: se toman incrustaciones de im√°genes de una red neuronal (por ejemplo, un vector de 128 n√∫meros) y consideramos la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">distancia euclidiana</a> habitual entre estos vectores o la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">proximidad del coseno.</a>  C√≥mo medirlo es mejor elegir en su conjunto de datos / tarea. <br><br>  Una representaci√≥n esquem√°tica de una tuber√≠a de resoluci√≥n de problemas en el aprendizaje de representaci√≥n se parece a esto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Pero para ser m√°s precisos, as√≠</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        ‚Äî   .   ,     ‚Äî     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> <br></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hay varios buenos art√≠culos </font><font style="vertical-align: inherit;">espec√≠ficamente sobre </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reconocimiento facial</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art√≠culo de revisi√≥n (¬° </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DEBE LEER!</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FaceNet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ArcFace</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CosFace</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="imagen" width="700"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambi√©n hay muchas implementaciones: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dlib</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenFace</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FaceNet repo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en Habr√© ya se habl√≥ de ello durante mucho tiempo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Parece que solo se han agregado recientemente ArcFace y CosFace (escriba en los comentarios, si me perd√≠ algo aqu√≠, estar√© encantado de saber algo m√°s). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, ahora est√° m√°s de moda no reconocer caras, sino </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">generarlas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ¬øverdad?</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="detecci√≥n humana cortada de una fotograf√≠a grande"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> Reid ‚Äî    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 2016 </a> . ,     ,    ‚Äî   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="imagen" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   ‚Äî    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TorchReid</a> .      ‚Äî   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  ,   </a> ).   ?  ‚Ä¶ <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="imagen" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/450732/">https://habr.com/ru/post/450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../450720/index.html">C√≥mo desarrollar una aplicaci√≥n f√°cil de usar</a></li>
<li><a href="../450724/index.html">Presentamos Python para camaradas que superan el "lenguaje A vs. V" lenguaje B "y otros prejuicios</a></li>
<li><a href="../450726/index.html">Creaci√≥n de una herramienta para escribir de forma r√°pida y eficiente autotests en Selenium</a></li>
<li><a href="../450728/index.html">NLog: reglas y filtros</a></li>
<li><a href="../450730/index.html">ok.tech: encuentro frontend</a></li>
<li><a href="../450734/index.html">Fuzzing es un paso importante en el desarrollo seguro</a></li>
<li><a href="../450736/index.html">"Aislar Internet es mucho m√°s f√°cil y m√°s barato que proporcionarle un bloqueo externo".</a></li>
<li><a href="../450738/index.html">Robots en el centro de datos: ¬øc√≥mo puede ser √∫til la inteligencia artificial?</a></li>
<li><a href="../450740/index.html">Base de l√°mpara inteligente REDMOND - agregue a la casa inteligente</a></li>
<li><a href="../450744/index.html">Infraestructura de bicicletas de Minsk para un expatriado de TI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>