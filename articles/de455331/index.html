<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äç‚úàÔ∏è üçÖ üëé Wie Computer erstaunlich gut gelernt haben, Bilder zu erkennen üë©‚Äçüöí üö™ üë©üèº‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bedeutende wissenschaftliche Arbeiten aus dem Jahr 2012 haben das Gebiet der Bilderkennungssoftware ver√§ndert 


 Heute kann ich beispielsweise Google...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie Computer erstaunlich gut gelernt haben, Bilder zu erkennen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455331/"><h3>  Bedeutende wissenschaftliche Arbeiten aus dem Jahr 2012 haben das Gebiet der Bilderkennungssoftware ver√§ndert </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/0d6/b8d/67e/0d6b8d67e771c94acab7f0ed50a54ba4.jpg"><br><br>  Heute kann ich beispielsweise Google Fotos √∂ffnen, "Strand" schreiben und einige meiner Fotos von verschiedenen Str√§nden sehen, die ich im letzten Jahrzehnt besucht habe.  Und ich habe meine Fotos nie signiert - Google erkennt Str√§nde an ihnen anhand ihres Inhalts.  Diese scheinbar langweilige Funktion basiert auf einer Technologie namens ‚ÄûDeep Convolutional Neural Network‚Äú, mit der Programme Bilder mit einer komplexen Methode verstehen k√∂nnen, auf die Technologien fr√ºherer Generationen nicht zugreifen k√∂nnen. <br><br>  In den letzten Jahren haben Forscher festgestellt, dass die Softwaregenauigkeit besser wird, wenn sie tiefere neuronale Netze (NS) aufbauen und diese auf immer gr√∂√üeren Datenmengen trainieren.  Dies f√ºhrte zu einem unstillbaren Bedarf an Rechenleistung und bereicherte GPU-Hersteller wie Nvidia und AMD.  Vor einigen Jahren hat Google eigene Spezialchips f√ºr die Nationalversammlung entwickelt, w√§hrend andere Unternehmen versuchen, mitzuhalten. <br><a name="habracut"></a><br>  Bei Tesla beispielsweise wurde Andrei Karpati, ein Experte f√ºr tiefes Lernen, zum Leiter des Autopilot-Projekts ernannt.  Jetzt entwickelt der Autohersteller einen eigenen Chip, um die Arbeit des NS in zuk√ºnftigen Versionen des Autopiloten zu beschleunigen.  Oder nehmen Sie Apple: Die A11- und A12-Chips, die f√ºr die neuesten iPhones von zentraler Bedeutung sind, verf√ºgen √ºber einen ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuronalen Prozessor</a> ‚Äú der Neural Engine, der den NS beschleunigt und es Bild- und Spracherkennungsanwendungen erm√∂glicht, besser zu funktionieren. <br><br>  Die Experten, die ich f√ºr diesen Artikel interviewt habe, verfolgen den Beginn des Deep-Learning-Booms f√ºr einen bestimmten Job: AlexNet, benannt nach dem Hauptautor Alex Krizhevsky.  "Ich glaube, dass 2012 ein Meilenstein war, als AlexNets Arbeit herauskam", sagte Sean Gerrish, Experte f√ºr Verteidigung und Autor des Buches " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">How Smart Cars Think</a> ". <br><br>  Bis 2012 waren tiefe neuronale Netze (GNS) in der Welt der Region Moskau ein R√ºckstau.  Aber dann nahmen Krizhevsky und seine Kollegen von der Universit√§t von Toronto an dem prestigetr√§chtigen Wettbewerb um Bilderkennung teil, und ihr Programm √ºbertraf in seiner Genauigkeit alles, was zuvor entwickelt wurde, dramatisch.  Fast augenblicklich wurde STS zur f√ºhrenden Technologie bei der Bilderkennung.  Andere Forscher, die diese Technologie verwendeten, zeigten bald weitere Verbesserungen der Erkennungsgenauigkeit. <br><br>  In diesem Artikel werden wir uns mit tiefem Lernen befassen.  Ich werde erkl√§ren, was NS ist, wie sie geschult werden und warum sie solche Computerressourcen ben√∂tigen.  Und dann werde ich erkl√§ren, warum eine bestimmte Art von NS - Deep Convolution Networks - Bilder so gut versteht.  Keine Sorge, es wird viele Bilder geben. <br><br><h2>  Ein einfaches Beispiel mit einem Neuron </h2><br>  Das Konzept eines ‚Äûneuronalen Netzwerks‚Äú mag Ihnen vage erscheinen. Beginnen wir also mit einem einfachen Beispiel.  Angenommen, Sie m√∂chten, dass die Nationalversammlung anhand der gr√ºnen, gelben und roten Verkehrssignale entscheidet, ob Sie ein Auto fahren.  NS kann dieses Problem mit einem einzelnen Neuron l√∂sen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c00/2d0/dda/c002d0dda45d5fef70ab7a156ad8e7cd.png"><br><br>  Ein Neuron empf√§ngt Eingabedaten (1 - Ein, 0 - Aus), multipliziert mit dem entsprechenden Gewicht und addiert alle Werte der Gewichte.  Dann f√ºgt das Neuron einen Offset hinzu, der den Schwellenwert f√ºr die "Aktivierung" des Neurons definiert.  In diesem Fall glauben wir, dass das Neuron aktiviert wurde, wenn die Ausgabe positiv ist - und umgekehrt.  Das Neuron entspricht der Ungleichung "gr√ºn - rot - 0,5&gt; 0".  Wenn sich herausstellt, dass es wahr ist - das hei√üt, Gr√ºn ist an und Rot ist nicht an -, sollte das Auto fahren. <br><br>  In der realen NS machen k√ºnstliche Neuronen einen weiteren Schritt.  Durch Addieren einer gewichteten Eingabe und Hinzuf√ºgen eines Offsets verwendet das Neuron eine nichtlineare Aktivierungsfunktion.  Oft wird ein Sigmoid verwendet, eine S-f√∂rmige Funktion, die immer einen Wert von 0 bis 1 ergibt. <br><br>  Die Verwendung der Aktivierungsfunktion √§ndert nichts am Ergebnis unseres einfachen Ampelmodells (wir m√ºssen nur einen Schwellenwert von 0,5 verwenden, nicht 0).  Die Nichtlinearit√§t von Aktivierungsfunktionen ist jedoch erforderlich, damit NS komplexere Funktionen modellieren k√∂nnen.  Ohne die Aktivierungsfunktion wird jeder beliebig komplexe NS auf eine lineare Kombination von Eingabedaten reduziert.  Eine lineare Funktion kann komplexe Ph√§nomene in der realen Welt nicht simulieren.  Die nichtlineare Aktivierungsfunktion erm√∂glicht es dem NS, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">jede mathematische Funktion</a> zu approximieren. <br><br><h2>  Netzwerkbeispiel </h2><br>  Nat√ºrlich gibt es viele M√∂glichkeiten, eine Funktion zu approximieren.  NS zeichnet sich dadurch aus, dass wir wissen, wie man sie mit ein wenig Algebra, einer Menge Daten und einem Meer an Rechenleistung ‚Äûtrainiert‚Äú.  Anstatt den Programmierer anzuweisen, die NS f√ºr eine bestimmte Aufgabe zu entwickeln, k√∂nnen wir Software erstellen, die mit einer ziemlich allgemeinen NS beginnt, eine Reihe von markierten Beispielen untersucht und dann die NS so √§ndert, dass sie f√ºr so viele Beispiele wie m√∂glich die richtige Bezeichnung gibt.  Es wird erwartet, dass der endg√ºltige NS die Daten zusammenfasst und die richtigen Bezeichnungen f√ºr Beispiele erstellt, die zuvor nicht in der Datenbank enthalten waren. <br><br>  Der Prozess, der zu diesem Ziel f√ºhrte, begann lange vor AlexNet.  1986 ver√∂ffentlichte ein Trio von Forschern eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wegweisende Arbeit</a> zur Backpropagation, eine Technologie, die dazu beitrug, das mathematische Lernen komplexer NS zu verwirklichen. <br><br>  Um sich vorzustellen, wie Backpropagation funktioniert, schauen wir uns einen einfachen NS an, den Michael Nielsen in seinem hervorragenden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Online-GO-Lehrbuch beschrieben hat</a> .  Der Zweck des Netzwerks besteht darin, das Bild einer handgeschriebenen Nummer in einer Aufl√∂sung von 28 x 28 Pixel zu verarbeiten und korrekt zu bestimmen, ob die Nummer 0, 1, 2 usw. geschrieben ist. <br><br>  Jedes Bild hat 28 * 28 = 784 Eingangsgr√∂√üen, von denen jede eine reelle Zahl von 0 bis 1 ist, die angibt, wie hell oder dunkel das Pixel ist.  Nielsen schuf die NA dieser Art: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbf/85e/fe1/fbf85efe11ae6dc62ccd0257e2325229.png"><br><br>  Jeder Kreis in der Mitte und in der rechten Spalte ist ein Neuron √§hnlich dem, das wir im vorherigen Abschnitt untersucht haben.  Jedes Neuron nimmt einen gewichteten Durchschnitt der Eingabe, f√ºgt einen Versatz hinzu und wendet eine Aktivierungsfunktion an.  Die Kreise links sind keine Neuronen, sondern repr√§sentieren die Eingabedaten des Netzwerks.  Und obwohl das Bild nur 8 Eingabekreise zeigt, gibt es tats√§chlich 784 davon - einen f√ºr jedes Pixel. <br><br>  Jedes der 10 Neuronen auf der rechten Seite sollte seine eigene Nummer "ausl√∂sen": Das oberste sollte sich einschalten, wenn eine handgeschriebene 0 eingegeben wird (und nur in diesem Fall), das zweite, wenn das Netzwerk eine handgeschriebene 1 sieht (und nur diese) und so weiter. <br><br>  Jedes Neuron nimmt Eingaben von jedem Neuron der vorherigen Schicht wahr.  Jedes der 15 Neuronen in der Mitte erh√§lt also 784 Eingabewerte.  Jedes dieser 15 Neuronen hat einen Gewichtungsparameter f√ºr jeden der 784 Eingabewerte.  Dies bedeutet, dass nur diese Schicht 15 * 784 = 11 760 Gewichtsparameter hat.  In √§hnlicher Weise enth√§lt die Ausgabeschicht 10 Neuronen, von denen jede Eingabe von allen 15 Neuronen der mittleren Schicht empf√§ngt, wodurch weitere 15 √ó 10 = 150 Gewichtsparameter hinzugef√ºgt werden.  Dar√ºber hinaus verf√ºgt das Netzwerk √ºber 25 Verschiebungsvariablen - eine f√ºr jedes der 25 Neuronen. <br><br><h2>  Neuronales Netzwerktraining </h2><br>  Ziel des Trainings ist es, diese 11.935 Parameter zu optimieren, um die Wahrscheinlichkeit zu maximieren, dass das gew√ºnschte Ausgangsneuron - und nur dieses - aktiviert wird, wenn die Netzwerke ein Bild einer handgeschriebenen Ziffer liefern.  Wir k√∂nnen dies mit dem bekannten Satz von Bildern MNIST tun, bei dem 60.000 markierte Bilder mit einer Aufl√∂sung von 28 x 28 Pixel vorhanden sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16d/dab/2ee/16ddab2ee3bcd9e22d96f267e473a2f4.png"><br>  <i>160 von 60.000 Bildern aus dem MNIST-Set</i> <br><br>  Nielsen zeigt, wie man ein Netzwerk mit 74 Zeilen regul√§ren Python-Codes trainiert - ohne Bibliotheken f√ºr MO.  Das Lernen beginnt mit der Auswahl von Zufallswerten f√ºr jeden dieser 11.935 Parameter, Gewichte und Offsets.  Anschlie√üend durchl√§uft das Programm Beispiele f√ºr Bilder und durchl√§uft jeweils zwei Phasen: <br><ol><li>  Der Vorw√§rtsausbreitungsschritt berechnet die Netzwerkausgabe basierend auf dem Eingabebild und den aktuellen Parametern. </li><li>  Der Backpropagation-Schritt berechnet die Abweichung des Ergebnisses von den korrekten Ausgabedaten und √§ndert die Netzwerkparameter, um die Effizienz in diesem Bild geringf√ºgig zu verbessern. </li></ol><br><br>  Ein Beispiel.  Angenommen, das Netzwerk hat das folgende Bild erhalten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e22/129/af7/e22129af76f6376aaa204ae02164a790.png"><br><br>  Wenn es gut kalibriert ist, sollte Pin ‚Äû7‚Äú auf 1 und die anderen neun Schlussfolgerungen auf 0 gehen. Nehmen wir jedoch an, dass das Netzwerk am Ausgang ‚Äû0‚Äú stattdessen einen Wert von 0,8 ergibt.  Das ist zu viel!  Der Trainingsalgorithmus √§ndert die Eingabegewichte des f√ºr ‚Äû0‚Äú verantwortlichen Neurons so, dass es bei der n√§chsten Verarbeitung dieses Bildes n√§her an 0 kommt. <br><br>  Hierzu berechnet der Backpropagation-Algorithmus f√ºr jedes Eingabegewicht einen Fehlergradienten.  Dies ist ein Ma√ü daf√ºr, wie sich der Ausgabefehler bei einer bestimmten √Ñnderung des Eingabegewichts √§ndert.  Dann verwendet der Algorithmus den Gradienten, um zu entscheiden, um wie viel jedes Eingabegewicht ge√§ndert werden soll - je gr√∂√üer der Gradient, desto st√§rker die √Ñnderung. <br><br>  Mit anderen Worten, der Trainingsprozess ‚Äûtrainiert‚Äú die Neuronen der Ausgabeschicht, um den Eingaben (Neuronen in der mittleren Schicht), die sie zur falschen Antwort f√ºhren, weniger Aufmerksamkeit zu schenken, als den Eingaben, die in die richtige Richtung dr√ºcken. <br><br>  Der Algorithmus wiederholt diesen Schritt f√ºr alle anderen Ausgangsneuronen.  Es reduziert die Eingabegewichte f√ºr die Neuronen "1", "2", "3", "4", "5", "6", "8" und "9" (aber nicht "7"), um deren Wert zu senken Ausgangsneuronen.  Je h√∂her der Ausgabewert ist, desto gr√∂√üer ist der Gradient des Ausgabefehlers in Bezug auf das Eingabegewicht - und desto st√§rker nimmt sein Gewicht ab. <br><br>  Und umgekehrt erh√∂ht der Algorithmus das Gewicht der Eingabedaten f√ºr die Ausgabe "7", wodurch das Neuron beim n√§chsten Erhalt dieses Bildes einen h√∂heren Wert erzeugt.  Wiederum erh√∂hen Eingaben mit gr√∂√üeren Werten die Gewichte st√§rker, wodurch das Ausgangsneuron ‚Äû7‚Äú diesen Eingaben beim n√§chsten Mal mehr Aufmerksamkeit schenkt. <br><br>  Dann sollte der Algorithmus die gleichen Berechnungen f√ºr die mittlere Schicht durchf√ºhren: √Ñndern Sie jedes Eingabegewicht in eine Richtung, die Netzwerkfehler reduziert - und bringen Sie die Ausgabe ‚Äû7‚Äú n√§her an 1 und den Rest an 0. Aber jedes mittlere Neuron hat eine Verbindung mit allen 10 freien Tagen, was die Sache in zweierlei Hinsicht kompliziert. <br><br>  Erstens h√§ngt der Fehlergradient f√ºr jedes durchschnittliche Neuron nicht nur vom Eingabewert ab, sondern auch von den Fehlergradienten in der n√§chsten Schicht.  Der Algorithmus wird als Backpropagation bezeichnet, da sich die Fehlergradienten der sp√§teren Schichten des Netzwerks in die entgegengesetzte Richtung ausbreiten und zur Berechnung der Gradienten in den fr√ºheren Schichten verwendet werden. <br><br>  Au√üerdem ist jedes mittlere Neuron eine Eingabe f√ºr alle zehn freien Tage.  Daher muss der Trainingsalgorithmus den Fehlergradienten berechnen, der widerspiegelt, wie sich eine √Ñnderung eines bestimmten Eingabegewichts auf den durchschnittlichen Fehler f√ºr alle Ausgaben auswirkt. <br><br>  Backpropagation ist ein Algorithmus zum Besteigen eines H√ºgels: Jeder Durchgang bringt die Ausgabewerte n√§her an die korrekten Werte f√ºr ein bestimmtes Bild, jedoch nur geringf√ºgig.  Je mehr Beispiele der Algorithmus betrachtet, desto h√∂her steigt er den H√ºgel hinauf in Richtung des optimalen Parametersatzes, der die maximale Anzahl von Trainingsbeispielen korrekt klassifiziert.  Um eine hohe Genauigkeit zu erreichen, sind Tausende von Beispielen erforderlich, und der Algorithmus muss m√∂glicherweise jedes Bild in diesem Satz Dutzende Male durchlaufen, bevor seine Wirksamkeit nicht mehr zunimmt. <br><br>  Nielsen zeigt, wie diese 74 Zeilen in Python implementiert werden.  √úberraschenderweise kann ein mit einem so einfachen Programm trainiertes Netzwerk mehr als 95% der handgeschriebenen Zahlen aus der MNIST-Datenbank erkennen.  Mit zus√§tzlichen Verbesserungen kann ein einfaches zweischichtiges Netzwerk mehr als 98% der Zahlen erkennen. <br><br><h2>  Durchbruch AlexNet </h2><br>  Sie k√∂nnten denken, dass die Entwicklung des Themas Backpropagation in den 1980er Jahren stattfinden und zu raschen Fortschritten in der MO auf der Grundlage der Nationalversammlung f√ºhren sollte - aber dies geschah nicht.  In den 1990er und fr√ºhen 2000er Jahren arbeiteten einige Leute an dieser Technologie, aber das Interesse an der Nationalversammlung gewann erst Anfang der 2010er Jahre an Dynamik. <br><br>  Dies l√§sst sich auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den ImageNet-Wettbewerb</a> zur√ºckf√ºhren, einen j√§hrlichen MO-Wettbewerb, der von Stanford Fay Fay Lee, einem IT-Spezialisten, organisiert wird.  Jedes Jahr erhalten die Rivalen den gleichen Satz von mehr als einer Million Bildern f√ºr das Training, von denen jedes manuell in Kategorien von mehr als 1000 gekennzeichnet ist - von ‚ÄûFeuerwehrauto‚Äú und ‚ÄûPilz‚Äú bis ‚ÄûGepard‚Äú.  Die Software der Teilnehmer wird nach der M√∂glichkeit beurteilt, andere Bilder zu klassifizieren, die nicht im Set enthalten waren.  Ein Programm kann einige Vermutungen anstellen, und seine Arbeit wird als erfolgreich angesehen, wenn mindestens eine der ersten f√ºnf Vermutungen mit der von einer Person festgelegten Note √ºbereinstimmt. <br><br>  Der Wettbewerb begann im Jahr 2010 und tiefe NS spielten in den ersten zwei Jahren keine gro√üe Rolle.  Die besten Teams verwendeten verschiedene MO-Techniken und erzielten ziemlich durchschnittliche Ergebnisse.  Im Jahr 2010 gewann das Team mit einem Fehleranteil von 28. Im Jahr 2011 - mit einem Fehler von 25%. <br><br>  Und dann kam 2012.  Ein Team von der University of Toronto machte ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Angebot</a> - sp√§ter AlexNet zu Ehren des Hauptautors Alex Krizhevsky genannt - und lie√ü die Rivalen weit hinter sich.  Mit Deep NS erreichte das Team eine Fehlerrate von 16%.  F√ºr den engsten Konkurrenten betrug diese Zahl 26. <br><br>  Die im Artikel zur Handschrifterkennung beschriebene NS hat zwei Schichten, 25 Neuronen und fast 12.000 Parameter.  AlexNet war viel gr√∂√üer und komplexer: acht trainierte Schichten, 650.000 Neuronen und 60 Millionen Parameter. <br><br>  Zum Trainieren von NS dieser Gr√∂√üe ist eine enorme Verarbeitungsleistung erforderlich. AlexNet wurde entwickelt, um die massive Parallelisierung moderner GPUs zu nutzen.  Die Forscher fanden heraus, wie die Arbeit des Trainings des Netzwerks in zwei GPUs aufgeteilt werden kann, wodurch sich die Leistung verdoppelt.  Trotz der strengen Optimierung dauerte das Netzwerktraining auf der 2012 verf√ºgbaren Hardware (auf einem Paar Nvidia GTX 580 mit 3 GB Speicher) 5 bis 6 Tage. <br><br>  Es ist n√ºtzlich, Beispiele f√ºr die Ergebnisse von AlexNet zu studieren, um zu verstehen, wie ernst dieser Durchbruch war.  Hier ist ein Bild aus einer wissenschaftlichen Arbeit, das Beispiele von Bildern und die ersten f√ºnf Vermutungen des Netzwerks nach ihrer Klassifizierung zeigt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d36/1ee/3b6/d361ee3b61cc19de787edda63d6e1e65.png"><br><br>  AlexNet konnte das H√§kchen im ersten Bild erkennen, obwohl es in der Ecke nur eine kleine Form gibt.  Die Software identifizierte nicht nur den Leoparden korrekt, sondern gab auch andere nahe Optionen - einen Jaguar, einen Geparden, einen Schneeleoparden und einen √§gyptischen Mau.  AlexNet hat das Hainbuchenfoto als "Agaric" markiert.  Nur "Pilz" war die zweite Version des Netzwerks. <br><br>  "Fehler" AlexNet sind ebenfalls beeindruckend.  Sie markierte das Foto mit einem Dalmatiner, der hinter einem B√ºndel Kirschen stand, als ‚ÄûDalmatiner‚Äú, obwohl das offizielle Etikett ‚ÄûKirsche‚Äú war.  AlexNet erkannte, dass auf dem Foto eine Art Beere zu sehen war - unter den ersten f√ºnf Optionen waren ‚ÄûTrauben‚Äú und ‚ÄûHolunder‚Äú -, erkannte die Kirsche einfach nicht.  Auf einem Foto eines Madagaskar-Makis, der auf einem Baum sitzt, gab AlexNet eine Liste kleiner S√§ugetiere, die auf B√§umen leben.  Ich denke, dass viele Leute (einschlie√ülich ich) hier die falsche Unterschrift gesetzt h√§tten. <br><br>  Die Qualit√§t der Arbeit war beeindruckend und zeigte, dass die Software gew√∂hnliche Objekte in einer Vielzahl ihrer Ausrichtungen und Umgebungen erkennen kann.  GNS wurde schnell zur beliebtesten Technik f√ºr die Bilderkennung, und seitdem hat die Welt von MO sie nicht aufgegeben. <br><br>  ‚ÄûNach dem Erfolg der auf GO basierenden Methode im Jahr 2012 haben die meisten Teilnehmer des Wettbewerbs 2013 auf tief gefaltete neuronale Netze umgestellt‚Äú, schreiben die Sponsoren von ImageNet.  In den folgenden Jahren setzte sich dieser Trend fort, und anschlie√üend arbeiteten die Gewinner auf der Grundlage von Basistechnologien, die zuerst vom AlexNet-Team angewendet wurden.  Bis 2017 reduzierten Rivalen, die tiefere NS verwendeten, die Fehlerrate ernsthaft auf weniger als drei.  Angesichts der Komplexit√§t der Aufgabe haben Computer bis zu einem gewissen Grad gelernt, sie besser zu l√∂sen als viele Menschen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/433/872/5e1/4338725e10f14ecf679878fb267394c9.png"><br>  <i>Der Prozentsatz der Fehler bei der Klassifizierung von Bildern in verschiedenen Jahren</i> <br><br><h2>  Faltungsnetzwerke: Ein Konzept </h2><br>  Technisch gesehen war AlexNet ein Faltungs-NS.  In diesem Abschnitt werde ich erkl√§ren, was das Convolutional Neural Network (SNA) tut und warum diese Technologie f√ºr moderne Mustererkennungsalgorithmen von entscheidender Bedeutung geworden ist. <br><br>  Das zuvor diskutierte einfache Netzwerk zur Handschrifterkennung war vollst√§ndig verbunden: Jedes Neuron der ersten Schicht war eine Eingabe f√ºr jedes Neuron der zweiten Schicht.  Eine solche Struktur eignet sich sehr gut f√ºr einfache Aufgaben mit Erkennung von Zahlen in Bildern mit 28 x 28 Pixeln.  Aber es skaliert nicht gut. <br><br>  In der handgeschriebenen MNIST-Zifferndatenbank sind alle Zeichen zentriert.  Dies vereinfacht das Lernen erheblich, da beispielsweise die sieben oben und rechts immer mehrere dunkle Pixel haben und die untere linke Ecke immer wei√ü ist.  Null hat fast immer einen wei√üen Fleck in der Mitte und dunkle Pixel an den R√§ndern.  Ein einfaches und vollst√§ndig verbundenes Netzwerk kann solche Muster recht leicht erkennen. <br><br>  Angenommen, Sie m√∂chten einen NS erstellen, der Zahlen erkennt, die sich an einer beliebigen Stelle auf einem gr√∂√üeren Bild befinden k√∂nnen.  Ein vollst√§ndig verbundenes Netzwerk funktioniert bei dieser Aufgabe nicht so gut, da √§hnliche Funktionen in Formularen, die sich in verschiedenen Teilen des Bildes befinden, nicht effektiv erkannt werden k√∂nnen.  Wenn sich in Ihrem Trainingsdatensatz die meisten Siebenen in der oberen linken Ecke befinden, kann Ihr Netzwerk die Siebenen in der oberen linken Ecke besser erkennen als in jedem anderen Teil des Bildes. <br><br>  Theoretisch kann dieses Problem gel√∂st werden, indem sichergestellt wird, dass Ihr Satz viele Beispiele f√ºr jede Ziffer an jeder der m√∂glichen Positionen enth√§lt.  In der Praxis wird dies jedoch eine enorme Verschwendung von Ressourcen sein.  Mit zunehmender Bildgr√∂√üe und Netzwerktiefe steigt die Anzahl der Links - und die Anzahl der Gewichtungsparameter - explosionsartig an.  Sie ben√∂tigen viel mehr Trainingsbilder (und Rechenleistung), um eine angemessene Genauigkeit zu erzielen. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn ein neuronales Netzwerk lernt, eine Form zu erkennen, die sich an einer Stelle eines Bildes befindet, muss es in der Lage sein, dieses Wissen anzuwenden, um dieselbe Form in anderen Teilen des Bildes zu erkennen. SNA bietet eine elegante L√∂sung f√ºr dieses Problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Es ist, als w√ºrde man eine Schablone nehmen und an allen Stellen im Bild anbringen", sagte der KI-Forscher Jai Teng. - Sie haben eine Schablone mit einem Bild eines Hundes und befestigen sie zuerst an der oberen rechten Ecke des Bildes, um zu sehen, ob sich dort ein Hund befindet? Wenn nicht, verschieben Sie die Schablone etwas. Und so f√ºr das ganze Bild. Es ist egal, wo das Bild des Hundes ist. Die Schablone passt zu ihr. Sie brauchen nicht jeden Teil des Netzwerks, um seine eigene Klassifizierung von Hunden zu lernen. ‚Äú</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stellen Sie sich vor, wir h√§tten ein gro√ües Bild aufgenommen und es in Quadrate von 28 x 28 Pixel unterteilt. </font><font style="vertical-align: inherit;">Dann k√∂nnen wir jedes Quadrat eines vollst√§ndig verbundenen Netzwerks speisen, das die zuvor untersuchte Handschrift erkennt. </font><font style="vertical-align: inherit;">Wenn die Ausgabe ‚Äû7‚Äú in mindestens einem der Quadrate ausgel√∂st wird, ist dies ein Zeichen daf√ºr, dass das gesamte Bild eine Sieben enth√§lt. </font><font style="vertical-align: inherit;">Genau das tun Faltungsnetzwerke.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wie Faltungsnetzwerke in AlexNet funktionierten </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In Faltungsnetzwerken sind solche "Schablonen" als Merkmalsdetektoren bekannt, und der Bereich, den sie untersuchen, ist als Empfangsfeld bekannt. Echte Merkmalsdetektoren arbeiten mit viel kleineren Feldern als ein Quadrat mit einer Seite von 28 Pixeln. In AlexNet arbeiteten Feature-Detektoren in der ersten Faltungsschicht mit einem Empfangsfeld von 11 x 11 Pixel. In nachfolgenden Schichten waren die Empfangsfelder 3 bis 5 Einheiten breit. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">W√§hrend des Durchlaufs erzeugt der Detektor der Zeichen des Eingabebildes eine Karte der Zeichen: ein zweidimensionales Gitter, auf dem vermerkt ist, wie stark der Detektor in verschiedenen Teilen des Bildes aktiviert wurde. Faltungsschichten haben normalerweise mehr als einen Detektor, und jeder von ihnen scannt das Bild auf der Suche nach verschiedenen Mustern. AlexNet hatte 96 Feature-Detektoren auf der ersten Ebene und gab 96 Feature-Karten aus.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/94f/1a0/c3d/94f1a0c3deacf7903606f4ecaf040b20.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um dies besser zu verstehen, sollten Sie eine visuelle Darstellung der Muster betrachten, die von jedem der 96 AlexNet-Detektoren der ersten Schicht nach dem Training des Netzwerks untersucht wurden. Es gibt Detektoren, die nach horizontalen oder vertikalen Linien, √úberg√§ngen von hell nach dunkel, Schachmustern und vielen anderen Formen suchen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein Farbbild wird normalerweise als Pixelkarte mit drei Zahlen f√ºr jedes Pixel dargestellt: dem Wert von Rot, Gr√ºn und Blau. Die erste Ebene von AlexNet nimmt diese Ansicht und verwandelt sie in eine Ansicht mit 96 Zahlen. Jedes ‚ÄûPixel‚Äú in diesem Bild hat 96 Werte, einen f√ºr jeden Merkmaldetektor. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Beispiel gibt der erste von 96 Werten an, ob ein Punkt im Bild diesem Muster entspricht:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/260/3fa/933/2603fa9332cbf509d9afd9dc1870e53a.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der zweite Wert gibt an, ob ein Bildpunkt mit einem solchen Muster √ºbereinstimmt: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a0b/736/b01/a0b736b01ab683e1dfbd229e29904500.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der dritte Wert gibt an, ob ein Bildpunkt mit einem solchen Muster √ºbereinstimmt: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd9/a72/09c/bd9a7209cb808f24e18d54327974a0e7.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">usw. f√ºr 93 Merkmaldetektoren in der ersten AlexNet-Schicht. Die erste Ebene erzeugt eine neue Darstellung des Bildes, wobei jedes Pixel ein Vektor in 96 Dimensionen ist (ich werde sp√§ter erkl√§ren, dass diese Darstellung um das Vierfache reduziert wird). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist die erste Schicht von AlexNet. Dann gibt es vier weitere Faltungsschichten, von denen jede die Ausgabe der vorherigen als Eingabe verwendet.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie wir gesehen haben, zeigt die erste Ebene grundlegende Muster wie horizontale und vertikale Linien, √úberg√§nge von hell nach dunkel und Kurven. Die zweite Ebene verwendet sie als Baustein zum Erkennen etwas komplexerer Formen. Beispielsweise k√∂nnte die zweite Schicht einen Merkmalsdetektor haben, der Kreise unter Verwendung einer Kombination der Ausgaben von Merkmalsdetektoren der ersten Schicht findet, die Kurven finden. Die dritte Ebene findet noch komplexere Formen, indem Merkmale aus der zweiten Ebene kombiniert werden. Der vierte und f√ºnfte finden noch komplexere Muster. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Forscher Matthew Zeiler und Rob Fergus haben 2014 eine </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hervorragende Arbeit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ver√∂ffentlicht </font><font style="vertical-align: inherit;">, die sehr n√ºtzliche M√∂glichkeiten zur Visualisierung von Mustern bietet, die von einem f√ºnfschichtigen neuronalen Netzwerk √§hnlich wie ImageNet erkannt werden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der n√§chsten Diashow aus ihrer Arbeit hat jedes Bild au√üer dem ersten zwei H√§lften. Rechts sehen Sie Beispiele f√ºr Miniaturansichten, die einen bestimmten Feature-Detektor stark aktiviert haben. Sie werden in neun gesammelt - und jede Gruppe entspricht einem eigenen Detektor. Auf der linken Seite befindet sich eine Karte, die genau zeigt, welche Pixel in dieser Miniaturansicht am meisten f√ºr die √úbereinstimmung verantwortlich sind. Dies zeigt sich insbesondere in der f√ºnften Schicht, da es Feature-Detektoren gibt, die stark auf Hunde, Logos, R√§der usw. reagieren. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2c/beb/74e/a2cbeb74e2f71a9b1b84fbea587294b2.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die erste Schicht - einfache Muster und Formen. Die </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/701/ebd/501/701ebd5013d881b6892c66c3fb63d77f.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zweite Schicht - kleine Strukturen erscheinen. Feature- </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/34d/283/91a/34d28391a4bb2d0804e15d12992208ba.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detektoren auf der dritten Schicht k√∂nnen komplexere Formen wie Autor√§der, Waben und sogar die Silhouetten von Menschen erkennen</font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/41a/ee2/cb6/41aee2cb61324edc33313a0b01c874b5.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die vierte Schicht kann komplexe Formen wie die Gesichter von Hunden oder die F√º√üe von V√∂geln unterscheiden. </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/04b/96a/baa/04b96abaa00cf8ed2dcb994d56a5af3f.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die f√ºnfte Schicht kann sehr komplexe Formen erkennen.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wenn Sie sich die Bilder ansehen, k√∂nnen Sie sehen, wie jede nachfolgende Schicht immer komplexere Muster erkennen kann. Die erste Ebene erkennt einfache Muster, die nichts sind. Der zweite erkennt Texturen und einfache Formen. Durch die dritte Schicht werden erkennbare Formen wie R√§der und rot-orangefarbene Kugeln (Tomaten, Marienk√§fer, etwas anderes) sichtbar.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der ersten Schicht betr√§gt die Seite des Empfangsfeldes 11 und in den sp√§teren drei bis f√ºnf. Denken Sie jedoch daran, dass sp√§tere Ebenen Feature-Maps erkennen, die von fr√ºheren Ebenen generiert wurden, sodass jedes ihrer ‚ÄûPixel‚Äú mehrere Pixel des Originalbilds bezeichnet. Daher enth√§lt das Empfangsfeld jeder Schicht einen gr√∂√üeren Teil des ersten Bildes als die vorherigen Schichten. Dies ist Teil des Grundes daf√ºr, dass Miniaturansichten in sp√§teren Ebenen komplexer aussehen als in fr√ºheren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die f√ºnfte, letzte Schicht des Netzwerks kann eine beeindruckend gro√üe Anzahl von Elementen erkennen. Schauen Sie sich zum Beispiel dieses Bild an, das ich aus der oberen rechten Ecke des Bildes ausgew√§hlt habe, das der f√ºnften Ebene entspricht:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e82/3fb/895/e823fb8956c1b12d92b32607c41837ec.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die neun Bilder rechts sind m√∂glicherweise nicht gleich. Wenn Sie sich jedoch die neun Heatmaps auf der linken Seite ansehen, werden Sie feststellen, dass dieser Funktionsdetektor nicht auf Objekte im Vordergrund der Fotos fokussiert. Stattdessen konzentriert er sich auf das Gras im Hintergrund eines jeden von ihnen! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich ist ein Grasdetektor n√ºtzlich, wenn eine der Kategorien, die Sie identifizieren m√∂chten, ‚ÄûGras‚Äú ist, aber er kann f√ºr viele andere Kategorien n√ºtzlich sein. Nach f√ºnf Faltungsschichten hat AlexNet drei Schichten vollst√§ndig verbunden, wie unser Netzwerk f√ºr die Handschrifterkennung. Diese Ebenen untersuchen jede der von f√ºnf Faltungsebenen ausgegebenen Feature-Maps und versuchen, das Bild in eine der 1000 m√∂glichen Kategorien einzuteilen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn sich also Gras im Hintergrund befindet, ist mit hoher Wahrscheinlichkeit ein wildes Tier im Bild zu sehen. </font><font style="vertical-align: inherit;">Wenn sich im Hintergrund Gras befindet, ist es weniger wahrscheinlich, dass M√∂bel im Haus abgebildet sind. </font><font style="vertical-align: inherit;">Diese und andere Feature-Detektoren der f√ºnften Schicht liefern eine Menge Informationen √ºber den wahrscheinlichen Inhalt des Fotos. </font><font style="vertical-align: inherit;">Die letzten Schichten des Netzwerks synthetisieren diese Informationen, um eine faktengest√ºtzte Vermutung dar√ºber zu liefern, was im Allgemeinen im Bild dargestellt ist.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Was Faltungsschichten anders macht: gemeinsame Eingabegewichte </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben gesehen, dass Feature-Detektoren auf Faltungsschichten eine beeindruckende Mustererkennung zeigen, aber bisher habe ich nicht erkl√§rt, wie Faltungsnetzwerke tats√§chlich funktionieren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Faltungsschicht (SS) besteht aus Neuronen. Sie nehmen wie alle Neuronen einen gewichteten Durchschnitt am Eingang und verwenden die Aktivierungsfunktion. Die Parameter werden unter Verwendung von R√ºckausbreitungstechniken trainiert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Gegensatz zu fr√ºheren NS ist die SS jedoch nicht vollst√§ndig verbunden. Jedes Neuron erh√§lt Eingaben von einem kleinen Teil der Neuronen aus der vorherigen Schicht. Und vor allem haben Faltungsnetzwerkneuronen gemeinsame Eingabegewichte.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schauen wir uns das erste Neuron der ersten AlexNet SS genauer an. Das Empfangsfeld dieser Schicht hat eine Gr√∂√üe von 11 x 11 Pixel, sodass das erste Neuron ein Quadrat von 11 x 11 Pixel in einer Ecke des Bildes untersucht. Dieses Neuron empf√§ngt Eingaben von diesen 121 Pixeln, und jedes Pixel hat drei Werte - Rot, Gr√ºn und Blau. Daher hat das Neuron im Allgemeinen 363 Eingabeparameter. Wie jedes Neuron nimmt dieses einen gewichteten Durchschnitt von 363 Parametern und wendet eine Aktivierungsfunktion auf diese an. Und da die Eingabeparameter 363 sind, ben√∂tigen die Gewichtsparameter auch 363.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das zweite Neuron der ersten Schicht √§hnelt dem ersten. Er untersucht auch die Quadrate von 11 x 11 Pixel, aber sein Empfangsfeld ist gegen√ºber dem ersten um vier Pixel verschoben. Die beiden Felder haben eine √úberlappung von 7 Pixeln, sodass das Netzwerk die interessanten Muster nicht aus den Augen verliert, die in die Verbindung zweier Quadrate gefallen sind. Das zweite Neuron nimmt auch 363 Parameter, die das 11x11-Quadrat beschreiben, multipliziert jeden von ihnen mit dem Gewicht, addiert die Aktivierungsfunktion und wendet sie an. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anstatt einen separaten Satz von 363 Gewichten zu verwenden, verwendet das zweite Neuron dieselben Gewichte wie das erste. Das obere linke Pixel des ersten Neurons verwendet die gleichen Gewichte wie das obere linke Pixel des zweiten. Daher suchen beide Neuronen nach dem gleichen Muster; Ihre Empfangsfelder werden einfach um 4 Pixel relativ zueinander verschoben.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich gibt es mehr als zwei Neuronen: Im 55x55-Gitter befinden sich 3025 Neuronen. Jeder von ihnen verwendet den gleichen Satz von 363 Gewichten wie die ersten beiden. Zusammen bilden alle Neuronen einen Merkmalsdetektor, der das Bild nach dem gew√ºnschten Muster "scannt", das sich √ºberall befinden kann. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Denken Sie daran, dass die erste AlexNet-Schicht 96 Feature-Detektoren hat. Die 3025 Neuronen, die ich gerade erw√§hnte, bilden einen dieser 96 Detektoren. Jede der verbleibenden 95 ist eine separate Gruppe von 3025 Neuronen. Jede Gruppe von 3025 Neuronen verwendet einen gemeinsamen Satz von 363 Gewichten - f√ºr jede der 95 Gruppen hat sie jedoch ihre eigenen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HFs werden mit derselben Backpropagation trainiert, die f√ºr vollst√§ndig verbundene Netzwerke verwendet wird, aber die Faltungsstruktur macht den Lernprozess effizienter und effektiver. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Die Verwendung von Faltung hilft wirklich - die Parameter k√∂nnen wiederverwendet werden", sagte Sean Gerrish, Experte f√ºr Verteidigung und Autorisierung. </font><font style="vertical-align: inherit;">Dies reduziert die Anzahl der Eingabegewichte, die das Netzwerk lernen muss, drastisch, wodurch es mit weniger Trainingsbeispielen bessere Ergebnisse erzielen kann. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Lernen an einem Teil des Bildes f√ºhrt zu einer verbesserten Erkennung des gleichen Musters in anderen Teilen des Bildes. </font><font style="vertical-align: inherit;">Dies erm√∂glicht es dem Netzwerk, bei einer viel geringeren Anzahl von Trainingsbeispielen eine hohe Leistung zu erzielen.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Menschen erkannten schnell die Kraft tiefer Faltungsnetzwerke. </font></font></h2><br>  Die Arbeit von AlexNet wurde in der akademischen Gemeinschaft der Region Moskau zu einer Sensation, aber ihre Bedeutung wurde in der IT-Branche schnell verstanden.  Google war besonders an ihr interessiert. <br><br>  Im Jahr 2013 erwarb Google ein von den Autoren AlexNet gegr√ºndetes Startup.  Das Unternehmen verwendete diese Technologie, um Google Fotos eine neue Funktion zur Fotosuche hinzuzuf√ºgen.  "Wir haben die fortgeschrittene Forschung etwas mehr als sechs Monate sp√§ter in Betrieb genommen", schrieb Chuck Rosenberg von Google. <br><br>  In der Zwischenzeit wurde 2013 beschrieben, wie Google mithilfe von GSS Adressen aus Fotos von Google Street View erkennt.  "Unser System hat uns geholfen, fast 100 Millionen physische Adressen aus diesen Bildern zu extrahieren", schrieben die Autoren. <br><br>  Die Forscher fanden heraus, dass die Wirksamkeit von NS mit zunehmender Tiefe zunimmt.  "Wir haben festgestellt, dass die Effektivit√§t dieses Ansatzes mit der Tiefe des SNA zunimmt und die tiefste der von uns trainierten Architekturen die besten Ergebnisse zeigt", schrieb das Google Street View-Team.  "Unsere Experimente legen nahe, dass tiefere Architekturen eine h√∂here Genauigkeit erzielen k√∂nnen, jedoch mit einer Verlangsamung der Effizienz." <br><br>  Nach AlexNet wurden die Netzwerke immer tiefer.  Das Google-Team hat sich 2014 f√ºr den Wettbewerb beworben - nur zwei Jahre nach dem Gewinn von AlexNet im Jahr 2012. Es basierte ebenfalls auf einer tiefen SNA, aber Goolge verwendete ein viel tieferes Netzwerk von 22 Schichten, um eine Fehlerrate von zu erreichen 6,7% - dies war eine wesentliche Verbesserung gegen√ºber 16% bei AlexNet. <br><br>  Gleichzeitig funktionierten tiefere Netzwerke nur mit gr√∂√üeren Trainingsdatens√§tzen besser.  Daher sagte Gerrish, dass der ImageNet-Datensatz und der Wettbewerb eine wichtige Rolle f√ºr den Erfolg der SNA gespielt haben.  Denken Sie daran, dass die Teilnehmer beim ImageNet-Wettbewerb eine Million Bilder erhalten und gebeten werden, diese in 1.000 Kategorien zu sortieren. <br><br>  "Wenn Sie eine Million Bilder f√ºr das Training haben, enth√§lt jede Klasse 1.000 Bilder", sagte Gerrish.  Ohne einen so gro√üen Datensatz sagte er: "Sie h√§tten zu viele Optionen, um das Netzwerk zu trainieren." <br><br>  In den letzten Jahren konzentrieren sich Experten zunehmend darauf, eine gro√üe Datenmenge zu sammeln, um tiefere und genauere Netzwerke zu trainieren.  Aus diesem Grund konzentrieren sich Unternehmen, die Roboterautos entwickeln, auf das Fahren auf √∂ffentlichen Stra√üen. Bilder und Videos dieser Fahrten werden an die Zentrale gesendet und f√ºr die Schulung von NS-Unternehmen verwendet. <br><br><h2>  Computing Deep Learning Boom </h2><br>  Die Entdeckung der Tatsache, dass tiefere Netzwerke und gr√∂√üere Datens√§tze die NS-Leistung verbessern k√∂nnen, hat einen unstillbaren Durst nach immer gr√∂√üerer Rechenleistung erzeugt.  Eine der Hauptkomponenten f√ºr den Erfolg von AlexNet war die Idee, dass Matrixtraining im NS-Training verwendet wird, das auf gut parallelisierbaren GPUs effizient durchgef√ºhrt werden kann. <br><br>  "Die NS sind gut parallelisiert", sagte Jai Ten, ein MO-Forscher.  Grafikkarten, die eine enorme Parallelverarbeitungsleistung f√ºr Videospiele bieten, haben sich f√ºr NS als n√ºtzlich erwiesen. <br><br>  "Der zentrale Teil der Arbeit der GPU, die sehr schnelle Matrixmultiplikation, erwies sich als zentraler Teil der Arbeit der Nationalversammlung", sagte Ten. <br><br>  All dies war f√ºr f√ºhrende Hersteller von GPU, Nvidia und AMD erfolgreich.  Beide Unternehmen haben neue Chips entwickelt, die speziell auf die Anforderungen der MO-Anwendung zugeschnitten sind. Jetzt sind AI-Anwendungen f√ºr einen erheblichen Teil des GPU-Umsatzes dieser Unternehmen verantwortlich. <br><br>  Im Jahr 2016 k√ºndigte Google die Schaffung eines speziellen Chips an, der Tensor Processing Unit (TPU), die f√ºr den Betrieb in der Nationalversammlung entwickelt wurde.  "Obwohl Google bereits 2006 die M√∂glichkeit in Betracht gezogen hat, spezielle integrierte Schaltkreise (ASICs) zu schaffen, wurde diese Situation 2013 dringend", <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schrieb</a> ein Unternehmensvertreter im vergangenen Jahr.  "Damals wurde uns klar, dass wir aufgrund der schnell wachsenden Anforderungen der Nationalversammlung an die Rechenleistung m√∂glicherweise die Anzahl unserer Rechenzentren verdoppeln m√ºssen." <br><br>  Anfangs hatten nur die eigenen Dienste von Google Zugriff auf TPUs. Sp√§ter erlaubte das Unternehmen jedem, diese Technologie √ºber eine Cloud-Computing-Plattform zu nutzen. <br><br>  Nat√ºrlich ist Google nicht das einzige Unternehmen, das an KI-Chips arbeitet.  Nur ein paar Beispiele: In den neuesten Versionen der iPhone-Chips <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gibt es einen</a> ‚Äûneuronalen Kern‚Äú, der f√ºr den Betrieb mit dem NS optimiert ist.  Intel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">entwickelt</a> eine eigene Reihe von Chips, die f√ºr GO optimiert sind.  Tesla <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">k√ºndigte</a> k√ºrzlich die Ablehnung von Chips von Nvidia zugunsten seiner eigenen NS-Chips an.  Es wird auch gemunkelt, dass Amazon an seinen KI-Chips arbeitet. <br><br><h2>  Warum tiefe neuronale Netze schwer zu verstehen sind </h2><br>  Ich habe erkl√§rt, wie neuronale Netze funktionieren, aber ich habe nicht erkl√§rt, warum sie so gut funktionieren.  Es ist nicht klar, wie genau die immense Menge an Matrixberechnungen es einem Computersystem erm√∂glicht, einen Jaguar von einem Geparden und Holunder von Johannisbeeren zu unterscheiden. <br><br>  Die vielleicht bemerkenswerteste Eigenschaft der Nationalversammlung ist, dass dies nicht der Fall ist.  Durch die Faltung kann der NS die Silbentrennung verstehen - er kann erkennen, ob das Bild in der oberen rechten Ecke des Bildes dem Bild in der oberen linken Ecke eines anderen Bildes √§hnlich ist. <br><br>  Gleichzeitig hat der SNA keine Ahnung von der Geometrie.  Sie k√∂nnen die √Ñhnlichkeit der beiden Bilder nicht erkennen, wenn sie um 45 Grad gedreht oder verdoppelt werden.  SNA versucht nicht, die dreidimensionale Struktur von Objekten zu verstehen und kann unterschiedliche Lichtverh√§ltnisse nicht ber√ºcksichtigen. <br><br>  Gleichzeitig k√∂nnen NS Fotos von Hunden erkennen, die sowohl von vorne als auch von der Seite aufgenommen wurden, und es spielt keine Rolle, ob der Hund einen kleinen oder einen gro√üen Teil des Bildes einnimmt.  Wie machen sie das?  Es stellt sich heraus, dass ein statistischer Ansatz mit direkter Aufz√§hlung die Aufgabe bew√§ltigen kann, wenn gen√ºgend Daten vorhanden sind.  Der SNA ist nicht so konzipiert, dass er sich vorstellen kann, wie ein bestimmtes Bild aus einem anderen Blickwinkel oder unter anderen Bedingungen aussehen w√ºrde, aber mit einer ausreichenden Anzahl von beschrifteten Beispielen kann er durch einfache Wiederholung alle m√∂glichen Variationen des Bildes lernen. <br><br>  Es gibt Hinweise darauf, dass das visuelle System von Menschen auf √§hnliche Weise funktioniert.  Schauen Sie sich ein paar Bilder an - studieren Sie zuerst das erste sorgf√§ltig und √∂ffnen Sie dann das zweite. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d43/861/b2d/d43861b2ddeebcec572cab31c9ddb81a.png"><br>  <i>Erstes Foto</i> <br><br><div class="spoiler">  <b class="spoiler_title">Zweites Foto</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/529/451/bde/529451bde2016793fd9cfe4ec092b46a.png"><br></div></div><br><br>  Der Sch√∂pfer des Bildes machte ein Foto von jemandem und stellte seine Augen und seinen Mund auf den Kopf.  Das Bild erscheint relativ normal, wenn man es verkehrt herum betrachtet, da das menschliche visuelle System es gewohnt ist, Augen und M√ºnder in dieser Position zu sehen.  Wenn Sie das Bild jedoch in der richtigen Ausrichtung betrachten, k√∂nnen Sie sofort feststellen, dass das Gesicht merkw√ºrdig verzerrt ist. <br><br>  Dies legt nahe, dass das menschliche visuelle System auf denselben groben Mustererkennungstechniken wie das NS basiert.  Wenn wir etwas betrachten, das fast immer in einer Ausrichtung sichtbar ist - das menschliche Auge -, k√∂nnen wir es in seiner normalen Ausrichtung viel besser erkennen. <br><br>  NS erkennen Bilder gut anhand des gesamten auf ihnen verf√ºgbaren Kontexts.  Zum Beispiel fahren Autos normalerweise auf Stra√üen.  Kleider werden normalerweise am K√∂rper einer Frau getragen oder h√§ngen in einem Schrank.  Flugzeuge werden normalerweise gegen den Himmel geschossen oder sie regieren auf der Landebahn.  Niemand lehrt die NS speziell diese Korrelationen, aber mit einer ausreichenden Anzahl von beschrifteten Beispielen kann das Netzwerk selbst sie lernen. <br><br>  Im Jahr 2015 versuchten Forscher von Google, die NS besser zu verstehen, "indem sie sie r√ºckw√§rts laufen lie√üen".  Anstatt Bilder zum Trainieren von NS zu verwenden, verwendeten sie trainiertes NS, um Bilder zu √§ndern.  Zum Beispiel begannen sie mit einem Bild, das zuf√§lliges Rauschen enthielt, und √§nderten es dann allm√§hlich, so dass es eines der Ausgangsneuronen des NS stark aktivierte. Tats√§chlich baten sie das NS, eine der Kategorien zu ‚Äûzeichnen‚Äú, deren Erkennung ihm beigebracht wurde.  In einem interessanten Fall zwangen sie die NS, Bilder zu erzeugen, die die NS aktivieren und darauf trainiert sind, Hanteln zu erkennen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ece/817/a18/ece817a18e8c2fc63281cb0a1773ac91.png"><br><br>  "Nat√ºrlich gibt es hier Hanteln, aber kein einziges Bild von Hanteln scheint vollst√§ndig zu sein, ohne dass eine muskul√∂se Muskelrolle sie anhebt", schrieben Google-Forscher. <br><br>  Auf den ersten Blick sieht es seltsam aus, aber in Wirklichkeit unterscheidet es sich nicht so sehr von dem, was Menschen tun.  Wenn wir ein kleines oder verschwommenes Objekt auf dem Bild sehen, suchen wir nach einem Hinweis in seiner Umgebung, um zu verstehen, was dort passieren kann.  Menschen sprechen offensichtlich anders √ºber Bilder und verwenden dabei ein komplexes konzeptionelles Verst√§ndnis der Welt um sie herum.  Aber am Ende erkennt das STS Bilder gut, weil sie den gesamten auf ihnen dargestellten Kontext voll ausnutzen, und dies unterscheidet sich nicht wesentlich von der Art und Weise, wie Menschen dies tun. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455331/">https://habr.com/ru/post/de455331/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455312/index.html">Ein-Bit-Volladdierer auf ungew√∂hnlichen Chips</a></li>
<li><a href="../de455316/index.html">Wir modifizieren den Bluetooth-Stack, um den Klang von Kopfh√∂rern ohne Codecs AAC, aptX und LDAC zu verbessern</a></li>
<li><a href="../de455319/index.html">Workflow 3D-K√ºnstler. Wie man nicht in einer Menge Informationen ertrinkt. Teil 1</a></li>
<li><a href="../de455325/index.html">Portieren von Desktop-Apps nach .NET Core</a></li>
<li><a href="../de455329/index.html">Ank√ºndigung der Azure IoT Edge Tools-Erweiterung (Vorschau)</a></li>
<li><a href="../de455333/index.html">Wer hat Python in das Windows-Update vom 10. Mai 2019 aufgenommen?</a></li>
<li><a href="../de455335/index.html">Petty Petty Joy # 3: Poesie</a></li>
<li><a href="../de455337/index.html">Wer hat Python zum neuesten Windows-Update hinzugef√ºgt?</a></li>
<li><a href="../de455339/index.html">Gr√§ber graben, SQL Server, jahrelanges Outsourcing und Ihr erstes Projekt</a></li>
<li><a href="../de455341/index.html">Was ist √ºber die ITIL 4-Zertifizierung bekannt?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>