<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïü üë®üèº‚Äçüé® üàÅ Keberuntungan menceritakan pada jaringan saraf: apakah penulis sendiri mencatat dalam komentar di posting üíè üåã üö¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Saya akan berbagi cerita tentang proyek kecil: cara menemukan jawaban penulis di komentar tanpa mengetahui siapa penulis pos tersebut. 

 Saya memulai...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Keberuntungan menceritakan pada jaringan saraf: apakah penulis sendiri mencatat dalam komentar di posting</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441850/"><img src="https://habrastorage.org/webt/dg/5l/lw/dg5llwwkakl3ndxtcjjwlwudupk.jpeg"><br><br>  Saya akan berbagi cerita tentang proyek kecil: cara menemukan jawaban penulis di komentar tanpa mengetahui siapa penulis pos tersebut. <br><br>  Saya memulai proyek saya dengan sedikit pengetahuan tentang pembelajaran mesin dan saya pikir tidak akan ada yang baru untuk spesialis di sini.  Materi ini, dalam arti tertentu, adalah kompilasi artikel yang berbeda, di dalamnya saya akan memberi tahu Anda bagaimana mendekati tugas, dalam kode Anda dapat menemukan hal-hal kecil yang berguna dan trik dengan pemrosesan bahasa alami. <br><a name="habracut"></a><br>  Data awal saya adalah sebagai berikut: database berisi materi media 2,5 juta dan komentar 39,5 juta.  Untuk posting 1M, dengan satu atau lain cara, penulis materi diketahui (informasi ini ada dalam database, atau diperoleh dengan menganalisis data dengan alasan tidak langsung).  Atas dasar ini, dataset dibuat dari catatan 215 ribu yang ditandai. <br><br>  Awalnya, saya menggunakan pendekatan berbasis heuristik yang dipancarkan oleh kecerdasan alami dan diterjemahkan ke dalam query sql dengan pencarian teks lengkap atau ekspresi reguler.  Contoh teks paling sederhana untuk diuraikan: "terima kasih atas komentarnya" atau "terima kasih atas peringkat yang baik" ini adalah penulis dalam 99,99% kasus, dan "terima kasih atas pekerjaannya" atau "Terima kasih!"  Kirim materi melalui surat.  Terima kasih! "  - ulasan biasa.  Dengan pendekatan seperti itu, hanya kebetulan yang jelas yang dapat disaring, kecuali untuk kasus kesalahan ketik dangkal atau ketika penulis sedang berdialog dengan komentator.  Oleh karena itu, diputuskan untuk menggunakan jaringan saraf, ide ini datang bukan tanpa bantuan seorang teman. <br><br>  Urutan komentar yang khas, yang manakah di antara penulisnya? <br><br><img src="https://habrastorage.org/webt/vv/sy/st/vvsystg0cv8nbrkntqqcrkbt45g.png"><br><br><div class="spoiler">  <b class="spoiler_title">Jawabannya</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/nx/jd/dq/nxjddq9-3dckcrg_26r81hvufgy.png"><br></div></div><br>  Metode untuk menentukan nada suara teks diambil sebagai dasar.Tugas ini sederhana bagi kita dalam dua kelas: penulis dan bukan penulis.  Untuk melatih model, saya menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">layanan</a> dari Google yang menyediakan mesin virtual dengan GPU dan antarmuka notebook Jupiter. <br><br>  Contoh jaringan yang ditemukan di Internet: <br><br><pre><code class="python hljs">embed_dim = <span class="hljs-number"><span class="hljs-number">128</span></span> model = Sequential() model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>])) model.add(SpatialDropout1D(<span class="hljs-number"><span class="hljs-number">0.2</span></span>)) model.add(LSTM(<span class="hljs-number"><span class="hljs-number">196</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, recurrent_dropout=<span class="hljs-number"><span class="hljs-number">0.2</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(loss = <span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>,metrics = [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  pada baris yang dihapus dari tag html dan karakter khusus, mereka memberikan akurasi sekitar 65-74%, yang tidak jauh berbeda dengan melempar koin. <br><br>  Hal yang menarik adalah bahwa penyelarasan urutan input melalui <code>pad_sequences(x_train, maxlen=max_len, padding='pre')</code> memberikan perbedaan yang signifikan dalam hasil.  Dalam kasus saya, hasil terbaik adalah dengan padding = 'post'. <br><br>  Langkah selanjutnya adalah penggunaan lemmatization, yang segera memberi peningkatan keakuratan hingga 80% dan ini bisa dikerjakan lebih lanjut.  Sekarang masalah utama adalah pembersihan teks yang benar.  Misalnya, kesalahan ketik pada kata "terima kasih" dikonversi (kesalahan ketik dipilih berdasarkan frekuensi penggunaan) menjadi ekspresi reguler (ekspresi seperti itu telah terakumulasi setengah hingga dua lusin). <br><br><pre> <code class="python hljs">re16 = re.compile(<span class="hljs-string"><span class="hljs-string">ur"(?:\b:(?:1|c(?:|)|(?:|)|(?:(?:|(?:(?:(?:|(?:)?|))?|(?:)?))|)|(?:(?:(?:|)|)||||(?:(?:||(?:|)|(?:|(?:(?:(?:||(?:(?:||(?:[]|)|[]))?|[—ñ]))?|||1)||)|)|||[]|(?:|)|(?:(?:(?:[]|)|?|(?:(?:(?:|(?:)?))?|)|(?:|)))?)||)|(?:|x))\b)"</span></span>, re.UNICODE)</code> </pre> <br>  Di sini, saya ingin mengucapkan terima kasih khusus kepada orang-orang yang terlalu sopan yang menganggap perlu menambahkan kata ini ke setiap kalimat mereka. <br><br>  Mengurangi proporsi kesalahan ketik itu perlu, karena  di pintu keluar dari lemmatizer mereka memberikan kata-kata aneh dan kami kehilangan informasi yang bermanfaat. <br><br>  Tapi ada hikmahnya, kami sudah bosan berurusan dengan kesalahan ketik, berurusan dengan pembersihan teks yang kompleks, saya menggunakan representasi vektor kata - word2vec.  Metode ini memungkinkan untuk menerjemahkan semua kesalahan ketik, kesalahan ketik, dan sinonim ke dalam vektor yang berjarak dekat. <br><br><img src="https://habrastorage.org/webt/54/r5/r9/54r5r9mqktawmbwxstrjaeyckvo.png"><br><br>  Kata-kata dan hubungannya dalam ruang vektor. <br><br>  Aturan pembersihan secara signifikan disederhanakan (aha, pendongeng), semua pesan, nama pengguna, dibagi menjadi kalimat dan diunggah ke file.  Poin penting: karena singkatnya komentator kami, untuk membangun vektor berkualitas tinggi, kata-kata memerlukan informasi kontekstual tambahan, misalnya, dari forum dan Wikipedia.  Tiga model dilatih pada file yang dihasilkan: classic word2vec, Glove dan FastText.  Setelah banyak percobaan, dia akhirnya memilih FastText, sebagai kelompok kata yang paling membedakan secara kualitatif dalam kasus saya. <br><br><img src="https://habrastorage.org/webt/t6/lu/t6/t6lut6wyvpf8b2lba8l2bgjjbd8.png"><br><br>  Semua perubahan ini menghasilkan akurasi 84-85 persen yang stabil. <br><br><div class="spoiler">  <b class="spoiler_title">Contoh Model</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_conv_core</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_input, embd_size = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">128</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> num_filters = <span class="hljs-number"><span class="hljs-number">128</span></span> X = Embedding(total_unique_words, DIM, input_length=max_words, weights=[embedding_matrix], trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, name=<span class="hljs-string"><span class="hljs-string">'Word2Vec'</span></span>)(model_input) X = Conv1D(num_filters, <span class="hljs-number"><span class="hljs-number">3</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(X) X = Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)(X) X = MaxPooling1D(<span class="hljs-number"><span class="hljs-number">2</span></span>)(X) X = Conv1D(num_filters, <span class="hljs-number"><span class="hljs-number">5</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_conv1d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_input, embd_size = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">128</span></span></span></span><span class="hljs-function"><span class="hljs-params">, num_filters = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">64</span></span></span></span><span class="hljs-function"><span class="hljs-params">, kernel_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> X = Embedding(total_unique_words, DIM, input_length=max_words, weights=[embedding_matrix], trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, name=<span class="hljs-string"><span class="hljs-string">'Word2Vec'</span></span>)(model_input) X = Conv1D(num_filters, kernel_size, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, strides=<span class="hljs-number"><span class="hljs-number">1</span></span>)(X) <span class="hljs-comment"><span class="hljs-comment"># X = Dropout(0.1)(X) X = MaxPooling1D(pool_size=2)(X) X = LSTM(256, kernel_regularizer=regularizers.l2(0.004))(X) X = Dropout(0.3)(X) X = Dense(128, kernel_regularizer=regularizers.l2(0.0004))(X) X = LeakyReLU()(X) X = BatchNormalization()(X) X = Dense(1, activation="sigmoid")(X) model = Model(model_input, X, name='w2v_conv1d') return model def model_gru(model_input, embd_size = 128): X = model_conv_core(model_input, embd_size) X = MaxPooling1D(2)(X) X = Dropout(0.2)(X) X = GRU(256, activation='relu', return_sequences=True, kernel_regularizer=regularizers.l2(0.004))(X) X = Dropout(0.5)(X) X = GRU(128, activation='relu', kernel_regularizer=regularizers.l2(0.0004))(X) X = Dropout(0.5)(X) X = BatchNormalization()(X) X = Dense(1, activation="sigmoid")(X) model = Model(model_input, X, name='w2v_gru') return model def model_conv2d(model_input, embd_size = 128): from keras.layers import MaxPool2D, Conv2D, Reshape num_filters = 256 filter_sizes = [3, 5, 7] X = Embedding(total_unique_words, DIM, input_length=max_words, weights=[embedding_matrix], trainable=False, name='Word2Vec')(model_input) reshape = Reshape((maxSequenceLength, embd_size, 1))(X) conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embd_size), padding='valid', kernel_initializer='normal', activation='relu')(reshape) conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embd_size), padding='valid', kernel_initializer='normal', activation='relu')(reshape) conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embd_size), padding='valid', kernel_initializer='normal', activation='relu')(reshape) maxpool_0 = MaxPool2D(pool_size=(maxSequenceLength - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0) maxpool_1 = MaxPool2D(pool_size=(maxSequenceLength - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1) maxpool_2 = MaxPool2D(pool_size=(maxSequenceLength - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2) X = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1) X = Dropout(0.2)(X) X = Flatten()(X) X = Dense(int(embd_size / 2.0), activation='relu', kernel_regularizer=regularizers.l2(0.004))(X) X = Dropout(0.5)(X) X = BatchNormalization()(X) X = Dense(1, activation="sigmoid")(X) model = Model(model_input, X, name='w2v_conv2d') return model</span></span></code> </pre><br></div></div><br>  dan 6 model lainnya dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kode</a> .  Beberapa model diambil dari jaringan, beberapa diciptakan secara independen. <br><br>  Telah diperhatikan bahwa komentar yang berbeda menonjol pada model yang berbeda, ini mendorong gagasan untuk menggunakan ansambel model.  Pertama, saya memasang ansambel secara manual, memilih pasangan model terbaik, lalu saya membuat generator.  Untuk mengoptimalkan pencarian lengkap, saya mengambil kode abu-abu sebagai dasar. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gray_code</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gray_code_recurse</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(g,n)</span></span></span><span class="hljs-function">:</span></span> k = len(g) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n &lt;= <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range (k<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>): char=<span class="hljs-string"><span class="hljs-string">'1'</span></span> + g[i] g.append(char) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range (k<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>): g[i]=<span class="hljs-string"><span class="hljs-string">'0'</span></span> + g[i] gray_code_recurse (g, n<span class="hljs-number"><span class="hljs-number">-1</span></span>) g = [<span class="hljs-string"><span class="hljs-string">'0'</span></span>,<span class="hljs-string"><span class="hljs-string">'1'</span></span>] gray_code_recurse(g, n<span class="hljs-number"><span class="hljs-number">-1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> g <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen_list</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(m)</span></span></span><span class="hljs-function">:</span></span> out = [] g = gray_code(len(m)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range (len(g)): mask_str = g[i] idx = <span class="hljs-number"><span class="hljs-number">0</span></span> v = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> c <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(mask_str): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> c == <span class="hljs-string"><span class="hljs-string">'1'</span></span>: v.append(m[idx]) idx += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(v) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>: out.append(v) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out</code> </pre> <br>  Dengan ansambel "hidup telah menjadi lebih menyenangkan" dan persentase ketepatan model saat ini berada pada level 86-87%, yang terutama terkait dengan klasifikasi berkualitas buruk dari beberapa penulis dalam dataset. <br><br><img src="https://habrastorage.org/webt/q8/8s/zt/q88sztzfnzspm90oanbhll4bz8c.png"><br><br>  Masalah yang saya temui: <br><br><ol><li>  Dataset tidak seimbang.  Jumlah komentar dari penulis secara signifikan lebih sedikit daripada komentator lain. <br></li><li>  Kelas dalam sampel berjalan dalam urutan yang ketat.  Intinya adalah bahwa awal, tengah dan akhir berbeda secara signifikan dalam kualitas klasifikasi.  Ini terlihat jelas dalam proses pembelajaran pada jadwal ukuran f1. <img src="https://habrastorage.org/webt/-f/mx/5e/-fmx5evtj0tsfch0y35sedqqezu.png"><br></li></ol><br>  Untuk solusinya, sepeda dibuat untuk pemisahan menjadi sampel pelatihan dan validasi.  Meskipun dalam prakteknya dalam banyak kasus prosedur train_test_split dari pustaka sklearn sudah cukup. <br><br>  Grafik model kerja saat ini: <br><br><img src="https://habrastorage.org/webt/cc/8q/zl/cc8qzlyein_kblslui7tgnq5qpg.png"><br><br>  Akibatnya, saya mendapatkan model dengan definisi percaya diri dari penulis dari komentar singkat.  Perbaikan lebih lanjut akan dikaitkan dengan pembersihan dan transfer hasil klasifikasi data nyata ke dalam dataset pelatihan. <br><br>  Semua kode dengan penjelasan tambahan ada di dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori</a> . <br><br>  Sebagai catatan tambahan: jika Anda perlu mengklasifikasikan teks dalam jumlah besar, lihat model <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">VDCNN</a> ‚ÄúVery Deep Convolutional Neural Network‚Äù ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">implementasi</a> pada keras), ini adalah analog dari ResNet untuk teks. <br><br>  Bahan yang digunakan: <br><br>  ‚Ä¢ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Ikhtisar kursus pembelajaran mesin</a> <br>  ‚Ä¢ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Analisis konvolusi menggunakan konvolusi</a> <br>  ‚Ä¢ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Jaringan konvolusional di NLP</a> <br>  ‚Ä¢ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Metrik dalam pembelajaran mesin</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://ld86.github.io/ml-slides/unbalanced.html</a> <br>  ‚Ä¢ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tampilan di dalam model</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id441850/">https://habr.com/ru/post/id441850/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id441834/index.html">Jangan mulai bekerja. Tetapi bagaimana jika masalahnya ada pada Anda?</a></li>
<li><a href="../id441836/index.html">One Cloud Story: Huawei + 3data = Cloud</a></li>
<li><a href="../id441842/index.html">Go Praktis: Kiat untuk Menulis Program yang Didukung di Dunia Nyata</a></li>
<li><a href="../id441844/index.html">iRobot Scooba: pengalaman dan solusi untuk masalah umum pembersih robot pencuci</a></li>
<li><a href="../id441848/index.html">Magang untuk pengembang di Avito: misi tempur dan bekerja dengan mentor berpengalaman</a></li>
<li><a href="../id441852/index.html">42 Silicon Valley: bagaimana cara dipilih</a></li>
<li><a href="../id441854/index.html">SISA? Ambil JSON-RPC bodoh</a></li>
<li><a href="../id441858/index.html">Akses data sederhana FAT12 internal untuk STM32</a></li>
<li><a href="../id441862/index.html">Sedikit tentang Phong shading</a></li>
<li><a href="../id441864/index.html">Tinjauan Pasar Monowheels 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>