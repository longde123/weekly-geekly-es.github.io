<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë¥üèæ üíÑ üß† C√≥mo usar la capacidad de almacenamiento disponible correctamente üç£ üßöüèª üèôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hemos estado utilizando servicios en la nube durante mucho tiempo: correo, almacenamiento, redes sociales, mensajer√≠a instant√°nea. Todos funcionan de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo usar la capacidad de almacenamiento disponible correctamente</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/480622/">  Hemos estado utilizando servicios en la nube durante mucho tiempo: correo, almacenamiento, redes sociales, mensajer√≠a instant√°nea.  Todos funcionan de forma remota: enviamos mensajes y archivos, y se almacenan y procesan en servidores remotos.  Los juegos en la nube tambi√©n funcionan: el usuario se conecta al servicio, selecciona el juego y lo inicia.  Esto es conveniente para el jugador, porque los juegos comienzan casi instant√°neamente, no ocupan memoria y no necesitan una computadora de juego potente. <br><br><img src="https://habrastorage.org/webt/ej/k4/oy/ejk4oyjjh1r3riqgzzd239qu_va.jpeg"><br><br>  Para un servicio en la nube, todo es diferente: tiene problemas de almacenamiento de datos.  Cada juego puede pesar decenas o cientos de gigabytes, por ejemplo, "The Witcher 3" toma 50 GB y "Call of Duty: Black Ops III" - 113. Al mismo tiempo, los jugadores no usar√°n el servicio con 2-3 juegos, se necesitan al menos varias docenas. .  Adem√°s de almacenar cientos de juegos, el servicio debe decidir cu√°nto almacenamiento asignar por jugador y escalar cuando hay miles de ellos. <br><br>  ¬øDeber√≠a almacenarse todo esto en sus servidores: cu√°ntos necesitan, d√≥nde ubicar los centros de datos, c√≥mo "sincronizar" los datos entre varios centros de datos sobre la marcha?  Comprar "nubes"?  ¬øUsar m√°quinas virtuales?  ¬øEs posible almacenar datos de usuario con compresi√≥n 5 veces y proporcionarlos en tiempo real?  ¬øC√≥mo excluir cualquier influencia de los usuarios entre s√≠ durante el uso constante de la misma m√°quina virtual? <br><br>  Todas estas tareas se resolvieron con √©xito en Playkey.net, una plataforma de juegos basada en la nube.  <strong>Vladimir Ryabov</strong> ( <a href="https://habr.com/ru/users/graymansama/" class="user_link">Graymansama</a> ), jefe del departamento de administraci√≥n de sistemas, hablar√° en detalle sobre la tecnolog√≠a ZFS para FreeBSD, que ayud√≥ en esto, y su nueva bifurcaci√≥n de ZOL (ZFS en Linux). <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SssLwMbMrQ4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Mil servidores de la compa√±√≠a est√°n ubicados en centros de datos remotos en Mosc√∫, Londres y Frankfurt.  Hay m√°s de 250 juegos en el servicio, que juegan 100 mil jugadores al mes. <br><br><img src="https://habrastorage.org/webt/s6/mv/nw/s6mvnwi5z_b5ltjl_vico2x7ro4.jpeg"><br><br>  El servicio funciona as√≠: el juego se ejecuta en los servidores de la compa√±√≠a, el usuario recibe una secuencia de controles desde el teclado, el mouse o el gamepad, y se env√≠a una secuencia de video en respuesta.  Esto le permite jugar juegos modernos de alta gama en computadoras con hardware d√©bil, computadoras port√°tiles con video integrado o en Mac para las que estos juegos no se lanzan en absoluto. <br><br><h2>  Los juegos deben ser almacenados y actualizados </h2><br>  Los datos principales para el servicio de juegos en la nube son las distribuciones de juegos, que pueden superar los cientos de GB, y el ahorro del usuario. <br><br>  Cuando √©ramos peque√±os, ten√≠amos solo una docena de servidores y un modesto cat√°logo de 50 juegos.  Almacenamos todos los datos localmente en los servidores, los actualizamos manualmente, todo estaba bien.  Pero ha llegado el momento de crecer y partimos <strong>hacia las nubes de AWS</strong> . <br><br>  Con AWS, obtuvimos varios cientos de servidores, pero la arquitectura no ha cambiado.  Tambi√©n eran servidores, pero ahora virtuales, con discos locales en los que se basaban las distribuciones de juegos.  Sin embargo, la actualizaci√≥n manual en un centenar de servidores fallar√°. <br><br>  Comenzamos a buscar una soluci√≥n.  Al principio intentamos actualizar a trav√©s de <strong>rsync</strong> .  Pero result√≥ que esto es extremadamente lento, y la carga en el nodo principal es demasiado.  Pero esto ni siquiera es lo peor: cuando ten√≠amos un nivel bajo en l√≠nea, apagamos algunas de las m√°quinas virtuales para no pagarlas, y al actualizar, los datos no se vierten en los servidores apagados.  Todos quedaron sin actualizaciones. <br><br>  La soluci√≥n fue torrentes: el programa <strong>BTSync</strong> .  Le permite sincronizar una carpeta en una gran cantidad de nodos sin especificar expl√≠citamente un nodo central. <br><br><h2>  Problemas de crecimiento </h2><br>  Por un tiempo, todo esto funcion√≥ maravillosamente.  Pero el servicio se estaba desarrollando, hab√≠a m√°s juegos y servidores.  El n√∫mero de almacenamientos locales tambi√©n aument√≥, tuvimos que pagar m√°s y m√°s.  En las nubes es costoso, especialmente para los SSD.  En un momento, incluso la indexaci√≥n habitual de una carpeta para comenzar su sincronizaci√≥n comenz√≥ a tomar m√°s de una hora, y todos los servidores pod√≠an actualizarse durante varios d√≠as. <br><br>  BTSync ha creado otro problema con tr√°fico de red excesivo.  En ese momento, en Amazon se pagaba incluso entre virtuales internos.  Si el cl√°sico iniciador de juegos realiza peque√±os cambios en archivos grandes, BTSync cree inmediatamente que todo el archivo ha cambiado y comienza a transferirlo completamente a todos los nodos.  Como resultado, incluso una actualizaci√≥n de 15 MB podr√≠a generar decenas de GB de tr√°fico de sincronizaci√≥n. <br><br>  La situaci√≥n se volvi√≥ cr√≠tica cuando el almacenamiento aument√≥ a 1 TB.  Acabo de lanzar un nuevo juego World of Warships.  Su distribuci√≥n ten√≠a varios cientos de miles de archivos peque√±os.  BTSync no pudo digerirlo y distribuirlo a todos los dem√°s servidores; esto ralentiz√≥ la distribuci√≥n de otros juegos. <br><br>  Todos estos factores crearon dos problemas: <br><br><ul><li>  producir almacenamiento local es costoso, inconveniente y dif√≠cil de actualizar; </li><li>  Las nubes eran muy caras. </li></ul><br>  Decidimos volver al concepto de nuestros servidores f√≠sicos. <br><br><h2>  Sistema de almacenamiento propio </h2><br>  Antes de pasar a los servidores f√≠sicos, debemos deshacernos del almacenamiento local.  Esto requiere su propio <strong>sistema de almacenamiento: almacenamiento</strong> .  Este es un sistema que almacena todas las distribuciones y las distribuye centralmente a todos los servidores. <br><br>  Parece que la tarea es simple: ya se ha resuelto repetidamente.  Pero con los juegos hay matices.  Por ejemplo, la mayor√≠a de los juegos simplemente se niegan a funcionar si se les da acceso de solo lectura.  Incluso con el inicio habitual habitual, les gusta escribir algo en sus archivos, y sin eso se niegan a trabajar.  Por el contrario, si un gran n√∫mero de usuarios tiene acceso a un conjunto de distribuciones, comienzan a vencer a los archivos del otro con acceso competitivo. <br><br>  Pensamos en el problema, verificamos varias soluciones posibles y llegamos a <strong>ZFS - Zettabyte File System en FreeBSD</strong> . <br><br><h2>  ZFS en FreeBSD </h2><br>  Este no es un sistema de archivos ordinario.  Los sistemas cl√°sicos se instalan inicialmente en un dispositivo, y para trabajar con varios discos ya requieren un administrador de vol√∫menes. <br><blockquote>  ZFS se cre√≥ originalmente en grupos virtuales. </blockquote>  Se llaman <strong>zpool</strong> y consisten en grupos de discos o matrices RAID.  El volumen completo de estos discos est√° disponible para cualquier sistema de archivos dentro de zpool.  Esto se debe a que ZFS se desarroll√≥ originalmente como un sistema que funcionar√° con grandes cantidades de datos. <br><br><h3>  C√≥mo ZFS ayud√≥ a resolver nuestros problemas </h3><br>  Este sistema tiene un <strong>mecanismo</strong> maravilloso <strong>para crear instant√°neas y clones</strong> .  Se crean al <strong>instante</strong> y pesan solo unos pocos KB.  Cuando realizamos cambios en uno de los clones, aumenta el volumen de estos cambios.  Al mismo tiempo, los datos en los clones restantes no cambian y siguen siendo √∫nicos.  Esto le permite distribuir un disco de <strong>10 TB</strong> con acceso exclusivo para el usuario final, gastando solo unos pocos KB. <br><br>  Si los clones crecen en el proceso de realizar cambios en una sesi√≥n de juego, ¬øno ocupar√°n tanto espacio como todos los juegos?  No, descubrimos que incluso en sesiones de juego bastante largas, el conjunto de cambios rara vez supera los 100-200 MB, esto no es cr√≠tico.  Por lo tanto, podemos dar acceso completo a un disco duro de alta capacidad completo a varios cientos de usuarios al mismo tiempo, gastando solo 10 TB con una cola. <br><br><h3>  C√≥mo funciona ZFS </h3><br>  La descripci√≥n parece complicada, pero ZFS funciona de manera bastante simple.  Analicemos su trabajo con un ejemplo simple: cree <code>zpool data</code> partir de los discos de <code>zpool create data /dev/da /dev/db /dev/dc</code> disponibles <code>zpool create data /dev/da /dev/db /dev/dc</code> . <br><br>  <em>Nota</em>  <em>Esto no es necesario para la producci√≥n, porque si al menos un disco muere, todo el grupo quedar√° en el olvido con √©l.</em>  <em>Mejor usar grupos RAID.</em> <br><br>  Creamos el sistema de archivos <code>zfs create data/games</code> , y en √©l un dispositivo de bloque con el nombre <code>data/games/disk</code> de 10 TB.  El dispositivo est√° disponible en <code>/dev/zvol/data/games/disk</code> como un disco normal; puede realizar las mismas manipulaciones con √©l. <br><br>  Entonces comienza la diversi√≥n.  Entregamos este disco a trav√©s de <strong>iSCSI a</strong> nuestro asistente de actualizaci√≥n, una m√°quina virtual normal que ejecuta Windows.  Conectamos el disco y colocamos los juegos simplemente desde Steam, como en una computadora dom√©stica normal. <br><br>  Llena el disco con juegos.  Ahora queda por distribuir estos datos a <strong>200 servidores</strong> para usuarios finales. <br><br><ul><li>  Cree una instant√°nea de este disco y <code>zfs snapshot data/games/disk@ver1</code> la primera versi√≥n: <code>zfs snapshot data/games/disk@ver1</code> .  <strong>Cree su clon</strong> <code>zfs clone data/games/disk@ver1 data/games/disk-vm1</code> , que ir√° a la primera m√°quina virtual. </li><li>  Entregamos el clon a trav√©s de iSCSI y <strong>KVM lanza una</strong> m√°quina virtual <strong>con este disco</strong> .  Se carga, entra en un grupo de servidores accesibles para los usuarios y espera un jugador. </li><li>  Cuando se completa la sesi√≥n del usuario, tomamos todos los archivos guardados de esta m√°quina virtual y los colocamos <strong>en un servidor separado</strong> .  <code>zfs destroy data/games/disk-vm1</code> <strong>la m√°quina</strong> virtual <strong>y destruimos el clon</strong> : <code>zfs destroy data/games/disk-vm1</code> . </li><li>  Volvemos al primer paso, nuevamente creamos un clon e iniciamos la m√°quina virtual. </li></ul><br>  Esto nos permite proporcionar a cada pr√≥ximo usuario una <strong>m√°quina siempre limpia</strong> , en la que no hay cambios con respecto al jugador anterior.  El disco despu√©s de cada sesi√≥n de usuario se elimina y se libera el espacio que ocupaba en el sistema de almacenamiento.  Tambi√©n realizamos operaciones similares con el disco del sistema y con todas nuestras m√°quinas virtuales. <br><br>  Recientemente, me encontr√© con un video en YouTube, donde un usuario satisfecho durante una sesi√≥n de juego formate√≥ nuestros discos duros en los servidores, y estaba muy contento de haber roto todo.  S√≠, por favor, solo para pagar: puede jugar y disfrutar.  En cualquier caso, el siguiente usuario siempre obtendr√° una m√°quina virtual funcional y limpia, sin importar lo que haga la anterior. <br><br>  Seg√∫n este esquema, los juegos se distribuyen a solo 200 servidores.  Calculamos el n√∫mero 200 experimentalmente: este es el n√∫mero de servidores en los que no se producen cargas cr√≠ticas en las unidades de almacenamiento.  Esto se debe a que los <strong>juegos tienen un perfil de carga bastante espec√≠fico</strong> : leen mucho en la etapa de lanzamiento o en la etapa de carga de nivel, y durante el juego, por el contrario, pr√°cticamente no usan un disco.  Si su perfil de carga es diferente, entonces la figura ser√° diferente. <br><br>  En el esquema anterior, para el servicio simult√°neo de 200 usuarios, necesitar√≠amos 2,000 TB de almacenamiento local.  Ahora podemos gastar un poco m√°s de 10 TB para el conjunto de datos principal, y todav√≠a hay 0,5 TB en stock para cambios de usuarios.  Aunque ZFS ama cuando tiene al menos el 15% de espacio libre en su grupo, me parece que hemos ahorrado significativamente. <br><br><h3>  ¬øQu√© pasa si tenemos varios centros de datos? </h3><br>  Este mecanismo funcionar√° solo dentro de un centro de datos, donde los servidores con un sistema de almacenamiento est√°n conectados por al menos 10 interfaces de gigabit.  ¬øQu√© hacer si hay varios DC?  ¬øC√≥mo actualizar el disco principal con juegos (conjunto de datos) entre ellos? <br><br>  Para esto, ZFS tiene su propia soluci√≥n: <strong>el mecanismo de env√≠o / recepci√≥n</strong> .  El comando de ejecuci√≥n es muy simple: <br><pre> <code class="bash hljs">zfs send -v data/games/disk@ver1 | ssh myzfsuser@myserverip zfs receive data/games/disk</code> </pre> <br>  El mecanismo le permite transferir de un sistema de almacenamiento a otro una instant√°nea del sistema principal.  Por primera vez, deber√° enviar los 10 terabytes de datos escritos al nodo maestro a un sistema de almacenamiento vac√≠o.  Pero con las pr√≥ximas actualizaciones, solo enviaremos cambios desde el momento en que creamos la instant√°nea anterior. <br><br>  Como resultado, obtenemos: <br><br><ul><li>  <strong>Todos los cambios se realizan centralmente en un sistema de almacenamiento</strong> .  Luego se dispersan a todos los dem√°s centros de datos en cualquier cantidad, y los datos en todos los nodos son siempre id√©nticos. </li><li>  <strong>El mecanismo de env√≠o / recepci√≥n no teme una desconexi√≥n</strong> .  Los datos no se aplican al conjunto de datos principal hasta que se transmiten completamente al nodo esclavo.  Si se pierde la conexi√≥n, es imposible da√±ar los datos y simplemente repita el procedimiento de env√≠o. </li><li>  <strong>Cualquier nodo puede convertirse f√°cilmente en un nodo maestro</strong> durante un accidente en solo unos minutos, ya que los datos en todos los nodos son siempre id√©nticos. </li></ul><br><h3>  Deduplicaci√≥n y copias de seguridad </h3><br>  ZFS tiene otra caracter√≠stica √∫til: la <strong>deduplicaci√≥n</strong> .  Esta funci√≥n ayuda a <strong>no almacenar dos bloques de datos id√©nticos</strong> .  En cambio, solo se almacena el primer bloque y, en lugar del segundo, se almacena un enlace al primero.  Dos archivos id√©nticos ocupar√°n espacio como uno, y si coinciden en un 90%, llenar√°n el 110% del volumen original. <br><br>  La funci√≥n nos ayud√≥ mucho en el almacenamiento de guardar el usuario.  En un juego, diferentes usuarios tienen un guardado similar, muchos archivos son iguales.  Mediante el uso de la deduplicaci√≥n, podemos almacenar cinco veces m√°s datos.  Nuestro √≠ndice de deduplicaci√≥n es 5.22.  F√≠sicamente, tenemos 4,43 terabytes, multiplicamos por un factor y obtenemos casi 23 terabytes de datos reales.  Esto ahorra espacio al evitar el almacenamiento duplicado. <br><div class="scrollable-table"><table><tbody><tr><td>  Nombre </td><td>  Tama√±o </td><td>  ALLOC </td><td>  GRATIS </td><td>  DEDUP </td></tr><tr><td>  datos </td><td>  7.16 TB </td><td>  4,43 TB </td><td>  2,73 TB </td><td>  5.22x </td></tr></tbody></table></div>  <strong>Las instant√°neas son buenas para las copias de seguridad</strong> .  Utilizamos esta tecnolog√≠a en nuestros almacenes de archivos.  Por ejemplo, si guarda una imagen todos los d√≠as durante un mes, puede implementar un clon en cualquier momento en cualquier d√≠a de ese mes y extraer archivos perdidos o da√±ados.  Esto elimina la necesidad de revertir todo el almacenamiento o implementar una copia completa del mismo. <br><br>  <strong>Usamos clones para ayudar a nuestros desarrolladores</strong> .  Por ejemplo, quieren experimentar una migraci√≥n potencialmente peligrosa en una base de combate.  No es r√°pido implementar una copia de seguridad cl√°sica de una base de datos que se acerca a 1 TB.  Por lo tanto, simplemente eliminamos el clon del disco base y lo agregamos instant√°neamente a la nueva instancia.  Ahora los desarrolladores pueden probar todo de forma segura all√≠. <br><br><h3>  API ZFS </h3><br>  Por supuesto, todo esto debe ser automatizado.  ¬øPor qu√© subir a los servidores, trabajar con las manos, escribir scripts, si esto se puede dar a los programadores?  Por lo tanto, escribimos nuestra <a href="https://github.com/drook/zfsapi">API web</a> simple. <br><br>  Envolvimos todas las funciones est√°ndar de ZFS, cortamos el acceso a aquellas que son potencialmente peligrosas y podr√≠an romper todo el sistema de almacenamiento, y dimos todo esto a los programadores.  Ahora <strong>todas las operaciones de disco est√°n estrictamente centralizadas</strong> y realizadas por c√≥digo, y <strong>siempre conocemos el estado de cada disco</strong> .  Todo funciona muy bien. <br><br><h2>  ZoL - ZFS en Linux </h2><br>  Centralizamos el sistema y pensamos, ¬øes tan bueno?  De hecho, ahora para cualquier extensi√≥n, inmediatamente necesitamos comprar varios racks de servidores: est√°n vinculados a los sistemas de almacenamiento y es irracional dividir el sistema.  ¬øQu√© hacer cuando decidimos implementar un peque√±o stand de demostraci√≥n para mostrar tecnolog√≠a a socios en otros pa√≠ses? <br><br>  Pensando, llegamos a la vieja idea: <strong>usar unidades locales</strong> , pero solo con toda la experiencia y el conocimiento que recibimos.  Si expande la idea de manera m√°s global, ¬øpor qu√© no dar a nuestros usuarios la oportunidad no solo de usar nuestros servidores, sino tambi√©n de alquilar sus computadoras? <br><br>  La bifurcaci√≥n relativamente reciente de <strong>ZFS en Linux: ZoL</strong> nos ayud√≥ mucho en esto. <br><blockquote>  Ahora cada servidor tiene su propio almacenamiento. </blockquote>  Solo que no almacena 10 terabytes de datos, como en el caso de una instalaci√≥n centralizada, sino solo 1-2 distribuciones de los juegos que sirve.  Un SSD es suficiente para esto.  Todo esto funciona bien: cada pr√≥ximo usuario siempre obtiene una m√°quina virtual limpia, as√≠ como en una instalaci√≥n de combate. <br><br>  Sin embargo, aqu√≠ encontramos dos problemas. <br><br><h3>  ¬øC√≥mo actualizar? </h3><br>  <strong>Actualice centralmente a trav√©s de SSH, como lo hacemos en los centros de datos no funcionar√°</strong> .  Los usuarios pueden conectarse a la red local o simplemente apagarse, a diferencia de los sistemas de almacenamiento, y no desea generar tantas conexiones SSH. <br><br>  Encontramos los mismos problemas que cuando usamos rsync.  Sin embargo, ya no se pueden obtener torrentes sobre ZFS.  Pensamos cuidadosamente c√≥mo funciona el mecanismo de env√≠o: env√≠a todos los bloques de datos modificados al almacenamiento final, donde Recibir los aplica al conjunto de datos actual.  ¬øPor qu√© no escribir los datos en un archivo, en lugar de enviarlos al usuario final? <br><br>  El resultado es lo que llamamos <strong>diff</strong> .  Este es un archivo en el que todos los bloques modificados entre las dos √∫ltimas instant√°neas se escriben secuencialmente.  Ponemos este diff en un CDN y lo enviamos a todos nuestros usuarios a trav√©s de HTTP: encendi√≥ la m√°quina, vio que hab√≠a actualizaciones, desinfl√≥ y lo aplic√≥ al conjunto de datos local usando Recibir. <br><br><h3>  ¬øQu√© hacer con los conductores? </h3><br>  Los servidores centralizados tienen la misma configuraci√≥n, y los <strong>usuarios finales siempre tienen diferentes computadoras y tarjetas de video</strong> .  Incluso si llenamos la distribuci√≥n del sistema operativo con todos los controladores posibles tanto como sea posible, la primera vez que se inicie, a√∫n querr√° instalar estos controladores, luego se reiniciar√° y luego, posiblemente, nuevamente.  Dado que cada vez que proporcionamos un clon limpio, todo este carrusel ocurrir√° despu√©s de cada sesi√≥n de usuario, esto es malo. <br><br>  Quer√≠amos hacer una ejecuci√≥n de inicializaci√≥n: espere hasta que Windows se inicie, instale todos los controladores, haga todo lo que quiera y solo luego opere en esta unidad.  Pero el problema es que si realiza cambios en el conjunto de datos principal, las actualizaciones se interrumpir√°n, porque los datos en la fuente y en el receptor ser√°n diferentes, y diff simplemente no se aplicar√°. <br><br>  Sin embargo, ZFS es un sistema flexible y nos permiti√≥ hacer una peque√±a muleta. <br><br><ul><li>  Como de costumbre, cree una instant√°nea: <code>zfs snapshot data/games/os@init</code> . </li><li>  Cree su clon ( <code>zfs clone data/games/os@init data/games/os-init</code> ) y ejec√∫telo en modo de inicializaci√≥n. </li><li>  Estamos esperando que todos los controladores se instalen y todo se reiniciar√°. </li><li>  Apague la m√°quina virtual y tome una instant√°nea nuevamente.  Pero esta vez, no del conjunto de datos original, sino del clon de inicializaci√≥n: <code>zfs snapshot data/games/os-init@ver1</code> . </li><li>  Creamos un clon de la instant√°nea con todos los controladores instalados.  Ya no se reiniciar√°: <code>zfs clone data/games/os-init@ver1 data/games/os-vm1</code> . </li><li>  Luego trabajamos en el grupo cl√°sico. </li></ul><br>  Ahora este sistema est√° en la etapa de prueba alfa.  Lo probamos en usuarios reales sin conocimiento de Linux, pero logran implementarlo todo en casa.  Nuestro objetivo final es que cualquier usuario simplemente conecte una unidad flash USB de arranque a su computadora, conecte una unidad SSD adicional y la alquile en nuestra plataforma en la nube. <br><br>  Discutimos solo una peque√±a parte de la funcionalidad de ZFS.  Este sistema puede hacer cosas mucho m√°s interesantes y diferentes, pero pocas personas saben acerca de ZFS; los usuarios no quieren hablar de ello.  Espero que despu√©s de este art√≠culo aparezcan nuevos usuarios en la comunidad ZFS. <br><br><blockquote>  Suscr√≠base a un <a href="https://t.me/DevOpsConfChannel">canal de telegramas</a> o <a href="http://eepurl.com/bN_0E1">bolet√≠n informativo</a> para obtener informaci√≥n sobre nuevos art√≠culos y videos de la conferencia <a href="https://devopsconf.io/">DevOpsConf</a> .  Adem√°s del bolet√≠n, recopilamos noticias de las pr√≥ximas conferencias y contamos, por ejemplo, qu√© ser√° interesante para los fan√°ticos de DevOps en <a href="https://www.highload.ru/spb/2020">Saint HighLoad ++</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/480622/">https://habr.com/ru/post/480622/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../480610/index.html">C ++ vtables. Parte 2 (herencia virtual + c√≥digo generado por compilador)</a></li>
<li><a href="../480612/index.html">Realice estos cambios para cumplir con los est√°ndares de accesibilidad del dise√±o web.</a></li>
<li><a href="../480614/index.html">ENUM r√°pido</a></li>
<li><a href="../480618/index.html">Juego electr√≥nico Tic Tac Toe. ¬øA qu√© he venido?</a></li>
<li><a href="../480620/index.html">SD-WAN y DNA para ayudar al administrador: caracter√≠sticas de arquitecturas y pr√°ctica</a></li>
<li><a href="../480626/index.html">Herencia de sistemas y procesos heredados o Los primeros 90 d√≠as en el rol de CTO</a></li>
<li><a href="../480642/index.html">Introducci√≥n a los ELF de Linux: comprensi√≥n y an√°lisis</a></li>
<li><a href="../480646/index.html">Habr - mejores art√≠culos, autores y estad√≠sticas 2019</a></li>
<li><a href="../480650/index.html">Cuyo cabello es m√°s fuerte: morfolog√≠a capilar</a></li>
<li><a href="../480652/index.html">C√°mara PoE de 250 metros sobre cable: es posible</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>