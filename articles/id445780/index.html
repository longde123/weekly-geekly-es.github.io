<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘¨ğŸ¾â€ğŸ­ ğŸ”« ğŸ’• Metode ansambel. Kutipan dari buku ğŸ‘¼ğŸ¾ ğŸ™ğŸ¼ ğŸ”º</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hai, Khabrozhiteli, kami telah menyerahkan kepada sebuah percetakan sebuah buku baru â€œPembelajaran Mesin: Algoritma untuk Bisnisâ€ . Berikut adalah kut...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Metode ansambel. Kutipan dari buku</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/445780/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/webt/vk/fr/zn/vkfrzn9ctkjsd2wjx8puqifp980.jpeg" alt="gambar"></a> <br><br>  Hai, Khabrozhiteli, kami telah menyerahkan kepada sebuah percetakan sebuah buku baru <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">â€œPembelajaran Mesin: Algoritma untuk Bisnisâ€</a> .  Berikut adalah kutipan tentang metode ensemble, tujuannya adalah untuk menjelaskan apa yang membuatnya efektif, dan bagaimana menghindari kesalahan umum yang mengarah pada penyalahgunaan mereka dalam keuangan. <br><a name="habracut"></a><br><h3>  6.2.  Tiga sumber kesalahan </h3><br>  Model MO biasanya mengalami tiga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kesalahan</a> . <br><br>  1. Bias: kesalahan ini disebabkan oleh asumsi yang tidak realistis.  Ketika bias tinggi, ini berarti bahwa algoritma MO tidak bisa mengenali hubungan penting antara sifat dan hasil.  Dalam situasi ini, dikatakan bahwa algoritme "tidak disetujui". <br><br>  2. Dispersi: kesalahan ini disebabkan oleh sensitivitas terhadap perubahan kecil dalam subset pelatihan.  Ketika variansnya tinggi, ini berarti bahwa algoritme itu terlalu pas untuk subset pelatihan, dan karenanya perubahan minimal dalam subset pelatihan dapat menghasilkan prediksi yang sangat berbeda.  Alih-alih memodelkan pola umum dalam himpunan bagian pelatihan, algoritma secara keliru mengambil noise untuk sinyal. <br><br>  3. Noise: Kesalahan ini disebabkan oleh dispersi dari nilai yang diamati, seperti perubahan yang tidak dapat diprediksi atau kesalahan pengukuran.  Ini adalah kesalahan fatal yang tidak bisa dijelaskan oleh model apa pun. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bc/8g/ag/bc8gaguh6e1w07vd3aoqcqcz4m0.png" alt="gambar"></div><br>  Metode ansambel adalah metode yang menggabungkan banyak siswa yang lemah, yang didasarkan pada algoritma pembelajaran yang sama, dengan tujuan menciptakan siswa (yang lebih kuat) yang kinerjanya lebih baik daripada setiap siswa.  Teknik ensemble membantu mengurangi bias dan / atau dispersi. <br><br><h3>  6.3.  Agregasi Bootstrap </h3><br>  Bagging (agregasi), atau agregasi sampel bootstrap, adalah cara yang efektif untuk mengurangi varians dalam perkiraan.  Ini berfungsi sebagai berikut: pertama, perlu untuk menghasilkan subset pelatihan N data menggunakan sampling acak dengan pengembalian.  Kedua, cocokkan evaluator N, satu untuk setiap himpunan pelatihan.  Evaluator ini disesuaikan secara independen satu sama lain, oleh karena itu, model dapat disesuaikan secara paralel.  Ketiga, perkiraan ensemble adalah rata-rata aritmatika sederhana dari perkiraan individual dari model N.  Dalam kasus variabel kategorikal, probabilitas bahwa pengamatan termasuk dalam kelas ditentukan oleh bagian evaluator yang mengklasifikasikan pengamatan ini sebagai anggota kelas ini (dengan suara terbanyak, yaitu, dengan suara terbanyak).  Ketika penilai dasar dapat membuat prediksi dengan probabilitas prediksi, pengklasifikasi kantong dapat mendapatkan nilai rata-rata probabilitas. <br><br>  Jika Anda menggunakan kelas baggingClassifier dari perpustakaan sklearn untuk menghitung akurasi non-paket, maka Anda harus tahu tentang cacat ini: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://github.com/scikit-learn/scikitlearn/issues/8933</a> .  Salah satu solusinya adalah mengubah nama label dalam urutan berurutan integer. <br><br><h3>  6.3.1.  Pengurangan dispersi </h3><br>  Keuntungan utama mengantongi adalah mengurangi variasi perkiraan, sehingga membantu menyelesaikan masalah overfitting.  Variasi dalam prediksi kantong (Ï†i [c]) adalah fungsi dari jumlah penilai kantong (N), varian rata-rata prediksi yang dilakukan oleh satu penilai (ÏƒÌ„), dan korelasi rata-rata antara prediksi mereka (ÏÌ„): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pq/ly/9i/pqly9iyqdqu5m8ty8zahuoeomve.png" alt="gambar"></div><br>  bootstraping berurutan (bab 4) adalah membuat pengambilan sampel sebebas mungkin, sehingga mengurangi ÏÌ„, yang seharusnya mengurangi dispersi pengklasifikasi kantong.  Dalam gbr.  6.1, kami merencanakan diagram deviasi standar prediksi kantong sebagai fungsi dari N âˆˆ [5, 30], ÏÌ„ âˆˆ [0, 1], dan ÏƒÌ„ = 1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/na/bm/8z/nabm8zbq6st62mg82lmhac45y0i.png" alt="gambar"></div><br><h3>  6.3.2.  Akurasi yang ditingkatkan </h3><br>  Pertimbangkan pengelompokan kantong, yang membuat prediksi pada kelas k dengan suara terbanyak di antara N pengklasifikasi independen.  Kita dapat menetapkan prediksi sebagai {0,1}, di mana 1 berarti prediksi yang benar.  Keakuratan classifier adalah probabilitas p untuk menandai prediksi sebagai 1. Rata-rata, kami mendapatkan prediksi Np yang ditandai sebagai 1 dengan varian Np (1 - p).  Suara terbanyak membuat prediksi yang benar ketika kelas yang paling dapat diprediksi diamati.  Misalnya, untuk N = 10 dan k = 3, classifier kantong membuat prediksi yang benar ketika diamati <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g-/oq/oi/g-oqoilmsmjgpaukoouor90ndbo.png" alt="gambar"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/wk/cl/wxwkcl97h4cnn1n-yx8ljmdj14g.png" alt="gambar"></div><br>  Daftar 6.1.  Kebenaran dari classifier kantong <br><br><pre><code class="plaintext hljs">from scipy.misc import comb N,p,k=100,1./3,3. p_=0 for i in xrange(0,int(N/k)+1): p_+=comb(N,i)*p**i*(1-p)**(Ni) print p,1-p_</code> </pre> <br>  Ini adalah argumen yang kuat untuk mengantongi classifier apa pun dalam kasus umum, ketika kemampuan komputasi memungkinkannya.  Namun, tidak seperti meningkatkan, mengantongi tidak dapat meningkatkan keakuratan pengklasifikasi lemah: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/fz/i8/9nfzi86m4gvaqru13iicjf1q0bg.png" alt="gambar"></div><br>  Untuk analisis rinci tentang topik ini, pembaca disarankan untuk beralih ke teorema juri Condorcet.  Meskipun teorema ini diperoleh untuk tujuan pemungutan suara mayoritas dalam ilmu politik, masalah yang dibahas oleh teorema ini memiliki kesamaan dengan yang dijelaskan di atas. <br><br><h3>  6.3.3.  Redundansi pengamatan </h3><br>  Dalam Bab 4, kami memeriksa salah satu alasan mengapa pengamatan keuangan tidak dapat dianggap merata dan saling independen.  Pengamatan berlebihan memiliki dua efek yang merugikan pada mengantongi.  Pertama, sampel yang diambil dengan pengembalian lebih cenderung hampir identik, bahkan jika mereka tidak memiliki pengamatan umum.  Itu benar <img src="https://habrastorage.org/webt/sx/i4/u0/sxi4u0afpswnvbe5nzxexnp_k80.png" alt="gambar">  dan mengantongi tidak akan mengurangi varians, terlepas dari N. Misalnya, jika setiap kasus di t ditandai sesuai dengan pengembalian keuangan antara t dan t + 100, maka kita harus memilih 1% dari kasus per penilai kantong, tetapi tidak lebih.  Dalam bab 4, bagian 4.5, tiga solusi alternatif direkomendasikan, salah satunya adalah pengaturan max_samples = out ['tW']. Mean () dalam implementasi kelas classifier kantong di perpustakaan sklearn.  Solusi (yang lebih baik) lainnya adalah penerapan metode pemilihan bootstrap berurutan. <br><br>  Efek merugikan kedua dari redundansi pengamatan adalah bahwa akurasi paket ekstra akan meningkat.  Hal ini disebabkan oleh fakta bahwa pengambilan sampel acak dengan pengembalian sampel ke sampel subset pelatihan yang sangat mirip dengan yang di luar paket.  Dalam hal ini, validasi silang k-blok bertingkat yang benar tanpa pengocokan sebelum pemisahan akan menunjukkan akurasi yang jauh lebih sedikit pada subset pengujian daripada yang dievaluasi di luar paket.  Karena alasan ini, saat menggunakan kelas pustaka sklearn ini, disarankan untuk menetapkan stratifiedKFold (n_splits = k, shuffle = False), periksa silang classifier kantong, dan abaikan hasil akurasi non-paket.  K yang rendah lebih disukai daripada k yang tinggi, karena pemisahan yang berlebihan akan sekali lagi menempatkan pola dalam subset tes yang terlalu mirip dengan yang digunakan dalam subset pelatihan. <br><br><h3>  6.4.  Hutan acak </h3><br>  Pohon keputusan terkenal karena cenderung over-fit, yang meningkatkan varian perkiraan.  Untuk mengatasi masalah ini, metode hutan acak (RF) dikembangkan untuk menghasilkan prakiraan ensemble dengan varian yang lebih rendah. <br><br>  Hutan acak memiliki beberapa kesamaan umum dengan mengantongi dalam arti melatih evaluator individu secara mandiri pada subset data yang di-bootstrap.  Perbedaan utama dari mengantongi adalah bahwa tingkat keacakan kedua dibangun ke dalam hutan acak: selama optimisasi setiap fragmentasi nodal, hanya subsampel acak (tanpa pengembalian) atribut yang akan dievaluasi untuk menghias lebih lanjut terkait evaluator. <br><br>  Seperti mengantongi, hutan acak mengurangi ragam perkiraan tanpa overfitting (ingat itu sampai).  Keuntungan kedua adalah bahwa hutan acak mengevaluasi pentingnya atribut, yang akan kita bahas secara rinci dalam Bab 8. Keuntungan ketiga adalah bahwa hutan acak memberikan perkiraan keakuratan yang tidak sesuai dengan paket, namun dalam aplikasi keuangan kemungkinan besar akan meningkat (seperti dijelaskan dalam Bagian 6.3.3).  Tetapi seperti mengantongi, hutan acak tidak selalu menunjukkan bias yang lebih rendah daripada pohon keputusan individu. <br><br>  Jika sejumlah besar sampel berlebihan (tidak terdistribusi secara merata dan saling independen), masih akan ada penyesuaian ulang: pengambilan sampel acak dengan pengembalian akan membangun sejumlah besar pohon yang hampir identik (), di mana setiap pohon keputusan dilengkapi secara berlebihan (kelemahan karena pohon keputusan mana yang terkenal) .  Tidak seperti bagging, forest acak selalu menetapkan ukuran sampel bootstrap sesuai dengan ukuran subset pelatihan data.  Mari kita lihat bagaimana kita bisa menyelesaikan masalah ini dengan memasang kembali hutan acak di perpustakaan sklearn.  Untuk tujuan ilustrasi, saya akan merujuk ke kelas perpustakaan sklearn;  namun, solusi ini dapat diterapkan untuk implementasi apa pun: <br><br>  1. Atur parameter max_features ke nilai yang lebih rendah untuk mencapai perbedaan di antara pohon-pohon. <br><br>  2. Pemberhentian awal: atur parameter regularisasi min_weight_fraction_leaf ke nilai yang cukup besar (misalnya, 5%) sehingga keakuratan paket-paket menyatu dengan ketepatan out-of-sample (k-block). <br><br>  3. Gunakan evaluator BaggingClassifier di dasar DecisionTreeClassifier evaluator, di mana max_samples diatur ke rata-rata keunikan (avgU) antara sampel. <br><br><ul><li>  clf = DecisionTreeClassifier (kriteria = 'entropi', max_features = 'otomatis', class_weight = 'seimbang') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  4. Gunakan evaluator BaggingClassifier di dasar evaluator RandomForestClassifier, di mana max_samples diatur ke rata-rata keunikan (avgU) antara sampel. <br><br><ul><li>  clf = RandomForestClassifier (n_estimators = 1, kriteria = 'entropi', bootstrap = Salah, class_weight = 'balanced_subsample') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  5. Ubah kelas hutan acak untuk mengganti bootstraps standar dengan bootstraps berurutan. <br><br>  Untuk meringkas, Listing 6.2 menunjukkan tiga cara alternatif untuk mengkonfigurasi hutan acak menggunakan kelas yang berbeda. <br><br>  Daftar 6.2.  Tiga cara untuk mengatur hutan acak <br><br><pre> <code class="plaintext hljs">clf0=RandomForestClassifier(n_estimators=1000, class_weight='balanced_ subsample', criterion='entropy') clf1=DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced') clf1=BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU) clf2=RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample') clf2=BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)</code> </pre> <br>  Saat memasang pohon keputusan, rotasi ruang fitur ke arah yang bertepatan dengan sumbu, sebagai aturan, mengurangi jumlah level yang diperlukan untuk pohon.  Untuk alasan ini, saya sarankan Anda memasukkan pohon acak pada atribut PCA, karena ini dapat mempercepat perhitungan dan sedikit mengurangi re-fit (lebih lanjut tentang ini di Bab 8).  Selain itu, sebagaimana dijelaskan dalam Bab 4, Bagian 4.8, argumen class_weight = 'balanced_subsample' akan membantu mencegah pohon dari kesalahan klasifikasi kelas minoritas. <br><br><h3>  6.5.  Tingkatkan </h3><br>  Kearns dan Valiant [1989] adalah yang pertama bertanya apakah penilai yang lemah dapat digabungkan untuk mencapai realisasi penilai yang sangat akurat.  Tak lama setelah itu, Schapire [1990] menunjukkan jawaban positif untuk pertanyaan ini menggunakan prosedur yang kami sebut meningkatkan hari ini (meningkatkan, meningkatkan, memperkuat).  Secara umum, ini berfungsi sebagai berikut: pertama, buat satu himpunan bagian pelatihan dengan seleksi acak dengan pengembalian sesuai dengan bobot sampel tertentu (diinisialisasi oleh bobot seragam).  Kedua, paskan satu evaluator menggunakan subset pelatihan ini.  Ketiga, jika seorang penilai tunggal mencapai akurasi yang melebihi ambang batas penerimaan (misalnya, dalam pengklasifikasi biner adalah 50% sehingga pengklasifikasi bekerja lebih baik daripada peramalan acak), maka penilai tetap, jika tidak dibuang.  Keempat, beri bobot lebih untuk observasi yang salah klasifikasi dan kurangi bobot untuk observasi dengan klasifikasi benar.  Kelima, ulangi langkah-langkah sebelumnya sampai N penilai diterima.  Keenam, prakiraan ensembel adalah rata-rata tertimbang dari prakiraan individu dari model N, di mana bobot ditentukan oleh keakuratan masing-masing pengevaluasi.  Ada sejumlah algoritma yang dikuatkan, di mana AdaBoost meningkatkan adaptif adalah salah satu yang paling populer (Geron [2017]).  Gambar 6.3 merangkum aliran keputusan dalam implementasi standar dari algoritma AdaBoost. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1d/u-/cr/1du-crm0gpkjgvy7lk45jj9f9ig.png" alt="gambar"></div><br><h3>  6.6.  Bagging vs meningkatkan keuangan </h3><br>  Dari uraian di atas, beberapa aspek membuat peningkatan sama sekali berbeda dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengantongi</a> : <br><br><ul><li>  Penyesuaian pengklasifikasi individual dilakukan secara berurutan. </li><li>  Klasifikasi yang buruk ditolak. </li><li>  Pada setiap iterasi, pengamatan tertimbang secara berbeda. </li></ul><br>  Perkiraan ensemble adalah rata-rata tertimbang dari masing-masing siswa. <br><br>  Keuntungan utama meningkatkan adalah mengurangi varians dan bias dalam perkiraan.  Meskipun demikian, koreksi bias terjadi karena risiko over-fitting yang lebih besar.  Dapat dikatakan bahwa dalam aplikasi keuangan, mengantongi biasanya lebih baik daripada meningkatkan.  Mengantongi memecahkan masalah overfitting, sementara meningkatkan memecahkan masalah overfitting.  Overfitting seringkali merupakan masalah yang lebih serius daripada underfitting, karena menyesuaikan algoritma MO terlalu erat dengan data keuangan sama sekali tidak sulit karena rasio sinyal terhadap noise yang rendah.  Selain itu, mengantongi dapat diparalelkan, sementara meningkatkan biasanya membutuhkan eksekusi berurutan. <br><br><h3>  6.7.  Mengantongi untuk skalabilitas </h3><br>  Seperti yang Anda ketahui, beberapa algoritme MO populer tidak skala dengan sangat baik tergantung pada ukuran sampel.  Metode mesin dukungan vektor (SVM) adalah contoh utama.  Jika Anda mencoba untuk menyesuaikan evaluator SVM lebih dari satu juta pengamatan, mungkin butuh waktu lama sampai algoritma menyatu.  Dan bahkan setelah konvergen, tidak ada jaminan bahwa solusinya adalah global optimal atau tidak akan diluruskan kembali. <br><br>  Salah satu pendekatan praktis adalah membangun algoritma kantong di mana evaluator dasar milik kelas yang tidak berskala baik dengan ukuran sampel, seperti SVM.  Dalam mendefinisikan penilai dasar ini, kami memperkenalkan kondisi yang ketat untuk pemberhentian awal.  Misalnya, dalam penerapan mesin vektor dukungan (SVM) di pustaka sklearn, Anda bisa menetapkan nilai rendah untuk parameter max_iter, misalnya, iterasi 1E5.  Nilai default adalah max_iter = -1, yang memberi tahu evaluator untuk melanjutkan iterasi sampai kesalahan jatuh di bawah level toleransi.  Di sisi lain, Anda dapat meningkatkan level toleransi dengan parameter tol, yang defaultnya tol = iE-3.  Salah satu dari dua opsi ini akan menyebabkan pemberhentian awal.  Anda dapat menghentikan algoritma lain lebih awal menggunakan parameter yang setara, seperti jumlah level dalam hutan acak (max_depth) atau fraksi tertimbang minimum dari jumlah total bobot (semua sampel input) yang diperlukan berada pada simpul daun (min_weight_fraction_leaf). <br><br>  Mengingat bahwa algoritma kantong dapat diparalelkan, kami mengubah tugas berurutan besar menjadi serangkaian yang lebih kecil yang dijalankan secara bersamaan.  Tentu saja, pemberhentian awal akan meningkatkan variasi hasil dari evaluator basis individu;  Namun, peningkatan ini bisa lebih dari diimbangi oleh penurunan varian yang terkait dengan algoritma kantong.  Anda dapat mengontrol pengurangan ini dengan menambahkan penilai basis independen baru.  Digunakan dengan cara ini, mengantongi memungkinkan Anda untuk mendapatkan taksiran cepat dan kuat pada kumpulan data yang sangat besar. <br><br>  Â»Informasi lebih lanjut tentang buku ini dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web penerbit</a> <br>  Â» <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Isi</a> <br>  Â» <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kutipan</a> <br><br>  Diskon 25% untuk buku pre-order Khabrozhiteley dengan kupon - <b>Pembelajaran Mesin</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id445780/">https://habr.com/ru/post/id445780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id445762/index.html">Apakah matematika itu logis atau mengapa teori aksiomatik bersifat paradoks</a></li>
<li><a href="../id445764/index.html">Cara saya membuat komponen master dalam Gambar</a></li>
<li><a href="../id445766/index.html">Tentang pusat data dalam semua kejujuran: bagaimana kami memecahkan masalah debu di ruang server pusat data</a></li>
<li><a href="../id445772/index.html">Sistem pembayaran cepat atau yang tidak mungkin adalah mungkin</a></li>
<li><a href="../id445778/index.html">10 kursus gratis baru tentang layanan kognitif dan Azure</a></li>
<li><a href="../id445782/index.html">Pilihan obeng geek dan multitool yang tidak biasa dari Leatherman hingga Xiaomi</a></li>
<li><a href="../id445784/index.html">Pertumbuhan karyawan yang profesional - apa dan mengapa perlu: kami berkomunikasi dengan Dodo Pizza, Icons8 dan Evil Martians</a></li>
<li><a href="../id445786/index.html">Kriptografi di Jawa. Kelas KeyStore</a></li>
<li><a href="../id445788/index.html">Surveilans video cloud mandiri: fitur-fitur baru dari Ivideon Web SDK</a></li>
<li><a href="../id445792/index.html">Bagaimana kami mengembangkan dokumentasi dalam proyek Embox yang terbuka</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>