<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚ÄçüöÄ üß¶ üïê Inicie o LDA no mundo real. Guia detalhado üë∏üèø üç¨ üà∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pref√°cio 


 Existem muitos tutoriais na Internet que explicam como o LDA funciona (aloca√ß√£o de diret√≥rios latentes) e como coloc√°-lo em pr√°tica. Exem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Inicie o LDA no mundo real. Guia detalhado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417167/"><h2 id="predislovie">  Pref√°cio </h2><br><p> Existem muitos tutoriais na Internet que explicam como o LDA funciona (aloca√ß√£o de diret√≥rios latentes) e como coloc√°-lo em pr√°tica.  Exemplos de treinamento de LDA s√£o frequentemente demonstrados em conjuntos de dados "exemplares", como o "conjunto de dados de 20 grupos de not√≠cias", dispon√≠vel no sklearn. </p><br><p>  Uma caracter√≠stica do treinamento no exemplo de conjuntos de dados "exemplares" √© que os dados sempre est√£o em ordem e convenientemente empilhados em um s√≥ lugar.  Ao treinar modelos de produ√ß√£o, os dados obtidos diretamente de fontes reais s√£o geralmente o oposto: </p><br><ul><li>  Muitas emiss√µes. </li><li>  Marca√ß√£o incorreta (se houver). </li><li>  Desequil√≠brios de classe muito fortes e distribui√ß√µes feias de quaisquer par√¢metros do conjunto de dados. </li><li>  Para textos, s√£o eles: erros gramaticais, um grande n√∫mero de palavras raras e √∫nicas, multilinguismo. </li><li>  Uma maneira inconveniente de armazenar dados (formatos diferentes ou raros, necessidade de an√°lise) </li></ul><br><p>  Historicamente, tento aprender com exemplos o mais pr√≥ximo poss√≠vel das realidades da realidade da produ√ß√£o, porque √© dessa maneira que se pode sentir mais plenamente as √°reas problem√°ticas de um determinado tipo de tarefa.  O mesmo aconteceu com o LDA e, neste artigo, quero compartilhar minha experi√™ncia - como executar o LDA do zero, com dados completamente brutos.  Alguma parte do artigo ser√° dedicada √† obten√ß√£o desses mesmos dados, para que o exemplo se torne um 'caso de engenharia' completo. </p><a name="habracut"></a><br><h2 id="topic-modeling-i-lda">  Modelagem de t√≥picos e LDA. </h2><br><p>  Para come√ßar, considere o que o LDA faz em geral e quais tarefas ele usa. <br>  Na maioria das vezes, o LDA √© usado para tarefas de modelagem de t√≥picos.  Tais tarefas significam as tarefas de agrupar ou classificar textos - de maneira que cada classe ou agrupamento contenha textos com t√≥picos semelhantes. </p><br><p>  Para aplicar a LDA ao conjunto de dados de textos (doravante denominado corpo do texto), √© necess√°rio transformar o corpo em uma matriz termo-documento. </p><br><p>  Um termo matriz de documentos √© uma matriz que tem um tamanho <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>e</mi><mi>z</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.553ex" height="2.057ex" viewBox="0 -780.1 4543.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-76" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-65" x="1624" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-7A" x="2090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-65" x="2559" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-73" x="3025" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-57" x="3495" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>v</mi><mi>e</mi><mi>z</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> N \ vezes W </script>  onde <br>  N √© o n√∫mero de documentos no caso e W √© o tamanho do dicion√°rio do caso, ou seja,  o n√∫mero de palavras (√∫nicas) encontradas em nosso corpus.  Na i-√©sima linha, a j-√©sima coluna da matriz √© um n√∫mero - quantas vezes no i-√©sima texto a j-√©sima palavra foi encontrada. </p><br><p>  A LDA constr√≥i, para um determinado termo, matriz de documentos e T de um n√∫mero predeterminado de t√≥picos, duas distribui√ß√µes: </p><br><ol><li>  A distribui√ß√£o dos t√≥picos nos textos (na pr√°tica, dada pela matriz de tamanhos <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>e</mi><mi>z</mi><mi>e</mi><mi>s</mi><mi>T</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.754ex" height="2.057ex" viewBox="0 -780.1 4199.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-76" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-65" x="1624" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-7A" x="2090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-65" x="2559" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-73" x="3025" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-54" x="3495" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>v</mi><mi>e</mi><mi>z</mi><mi>e</mi><mi>s</mi><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> N \ vezes T </script>  ) </li><li>  A distribui√ß√£o das palavras por t√≥pico (matriz de tamanho <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>e</mi><mi>z</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.125ex" height="2.057ex" viewBox="0 -780.1 4359.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-54" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-76" x="954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-65" x="1440" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-7A" x="1906" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-65" x="2375" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-73" x="2841" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-57" x="3311" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mtext>&nbsp;</mtext><mi>v</mi><mi>e</mi><mi>z</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> T \ vezes W </script>  ) </li></ol><br><p>  Os valores das c√©lulas dessas matrizes s√£o, respectivamente, as probabilidades de que este t√≥pico esteja contido neste documento (ou a propor√ß√£o do t√≥pico no documento, se considerarmos o documento como uma mistura de t√≥picos diferentes) para a matriz 'Distribui√ß√£o de t√≥picos em textos'. </p><br><p>  Para a matriz 'Distribui√ß√£o de palavras por temas', os valores s√£o a probabilidade de encontrar a palavra j no texto com o t√≥pico i, qualitativamente, podemos considerar esses n√∫meros como coeficientes que caracterizam como essa palavra √© t√≠pica para este t√≥pico. </p><br><p>  Deve-se dizer que a palavra t√≥pico n√£o √© uma defini√ß√£o "cotidiana" dessa palavra.  O LDA aloca T a esses, mas que tipo de t√≥picos s√£o esses e se correspondem a algum t√≥pico conhecido dos textos, como: 'Esporte', 'Ci√™ncia', 'Pol√≠tica' - √© desconhecido.  Nesse caso, √© mais apropriado falar sobre o t√≥pico como um tipo de entidade abstrata, que √© definida por uma linha na matriz de distribui√ß√£o de palavras por t√≥pico e com alguma probabilidade corresponde a este texto, se voc√™ pode imagin√°-lo como uma fam√≠lia de conjuntos de palavras caracter√≠sticos reunidos com probabilidades correspondentes (da tabela) em um determinado conjunto de textos. </p><br><p>  Se voc√™ estiver interessado em estudar mais detalhadamente e 'em f√≥rmulas' como o LDA √© treinado e funciona, aqui est√£o alguns materiais (que foram usados ‚Äã‚Äãpelo autor): </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Artigo original</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Em ingl√™s, com exemplos ilustrativos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Detalhes em russo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sobre a implementa√ß√£o do Python</a> </li></ul><br><h2 id="dobyvaem-dikie-dannye">  Obtemos dados selvagens </h2><br><p>  Para nosso 'trabalho de laborat√≥rio', precisamos de um conjunto de dados personalizado com suas pr√≥prias falhas e recursos.  Voc√™ pode obt√™-lo em diferentes lugares: fa√ßa o download de an√°lises do Kinopoisk, artigos da Wikipedia, not√≠cias de algum portal de not√≠cias, teremos uma op√ß√£o um pouco mais extrema - publica√ß√µes das comunidades VKontakte. </p><br><p>  Faremos isso assim: </p><br><ol><li>  N√≥s selecionamos algum usu√°rio VK. </li><li>  Temos uma lista de todos os amigos dele. </li><li>  Para cada amigo, levamos toda a sua comunidade. </li><li>  Para cada comunidade de cada amigo, distribu√≠mos as primeiras n (n = 100) postagens da comunidade e as combinamos em um conte√∫do de texto da comunidade. </li></ol><br><h4 id="instrumenty-i-stati">  Ferramentas e artigos </h4><br><p>  Para baixar posts, usaremos o m√≥dulo vk para trabalhar com a API VKontakte, para Python.  Um dos momentos mais complicados ao escrever um aplicativo usando a API do VKontakte √© a autoriza√ß√£o, felizmente, o c√≥digo que executa esse trabalho j√° est√° escrito e est√° em dom√≠nio p√∫blico, exceto pelo vk, usei um pequeno m√≥dulo de autoriza√ß√£o - vkauth. </p><br><p>  Links para os m√≥dulos e artigos usados ‚Äã‚Äãpara estudar a API do VKontakte: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vkauth</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tutorial vkauth</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tutorial vk</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vk tutorial number 2</a> </li><li>  Documenta√ß√£o oficial da API do Vkontakte </li></ul><br><h4 id="pishem-kod">  Escrevendo um c√≥digo </h4><br><p>  E assim, usando vkauth, efetue login: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#authorization of app using modules imported. app_id = '6203169' perms = ['photos','friends','groups'] API_ver = '5.68' Auth = VKAuth(perms, app_id, API_ver) Auth.auth() token = Auth.get_token() user_id = Auth.get_user_id() #starting session session = vk.Session(access_token=token) api = vk.API(session)</span></span></code> </pre> <br><p>  No processo, um pequeno m√≥dulo foi escrito contendo todas as fun√ß√µes necess√°rias para o download de conte√∫do no formato apropriado; elas est√£o listadas abaixo, vamos passar por elas: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_friends_ids</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API object and user_id returns a list of all his friends ids. '''</span></span> friends = api.friends.get(user_id=user_id, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) friends_ids = friends[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> friends_ids <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_user_groups</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id, moder=True, only_open=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API user_id returns list of all groups he subscribed to. Flag model to get only those groups where user is a moderator or an admin) Flag only_open to get only public(open) groups. '''</span></span> kwargs = {<span class="hljs-string"><span class="hljs-string">'user_id'</span></span> : user_id, <span class="hljs-string"><span class="hljs-string">'v'</span></span> : <span class="hljs-string"><span class="hljs-string">'5.68'</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> moder == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'filter'</span></span>] = <span class="hljs-string"><span class="hljs-string">'moder'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> only_open == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'extended'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> kwargs[<span class="hljs-string"><span class="hljs-string">'fields'</span></span>] = [<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] groups = api.groups.get(**kwargs) groups_refined = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> groups[<span class="hljs-string"><span class="hljs-string">'items'</span></span>]: cond_check = (only_open <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> group[<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> only_open <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cond_check: refined = {} refined[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] * (<span class="hljs-number"><span class="hljs-number">-1</span></span>) refined[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] groups_refined.append(refined) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> groups_refined <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_n_posts_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, group_id, n_posts=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">50</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given api and group_id returns first n_posts concatenated as one text. '''</span></span> wall_contents = api.wall.get(owner_id = group_id, count=n_posts, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) wall_contents = wall_contents[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] text = <span class="hljs-string"><span class="hljs-string">''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> post <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wall_contents: text += post[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] + <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text</code> </pre> <br><p>  O pipeline final √© o seguinte: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#id of user whose friends you gonna get, like: https://vk.com/id111111111 user_id = 111111111 friends_ids = vt.get_friends_ids(api, user_id) #collecting all groups groups = [] for i,friend in tqdm(enumerate(friends_ids)): if i % 3 == 0: sleep(1) friend_groups = vt.get_user_groups(api, friend, moder=False) groups += friend_groups #converting groups to dataFrame groups_df = pd.DataFrame(groups) groups_df.drop_duplicates(inplace=True) #reading content(content == first 100 posts) for i,group in tqdm(groups_df.iterrows()): name = group['name'] group_id = group['id'] #Different kinds of fails occures during scrapping #For examples there are names of groups with slashes #Like: 'The Kaaats / Indie-rock' try: content = vt.get_n_posts_text(api, group_id, n_posts=100) dst_path = join(data_path, name + '.txt') with open(dst_path, 'w+t') as f: f.write(content) except Exception as e: print('Error occured on group:', name) print(e) continue #need it because of requests limitaion in VK API. if i % 3 == 0: sleep(1)</span></span></code> </pre> <br><h4 id="fails">  Falha </h4><br><p>  Em geral, o processo de download de dados n√£o √© dif√≠cil por si s√≥; voc√™ deve prestar aten√ß√£o apenas a dois pontos: </p><br><ol><li>  √Äs vezes, devido √† privacidade de algumas comunidades, voc√™ receber√° erros de acesso; outras vezes, outros erros ser√£o resolvidos com a instala√ß√£o do try, exceto no lugar certo. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VK tem um limite</a> no n√∫mero de solicita√ß√µes por segundo. </li></ol><br><p>  Ao fazer um grande n√∫mero de solicita√ß√µes, por exemplo em um loop, tamb√©m capturaremos erros.  Esse problema pode ser resolvido de v√°rias maneiras: </p><br><ol><li>  Estupidamente e sem rodeios: durma (alguns) a cada 3 solicita√ß√µes.  Isso √© feito em uma linha e diminui bastante a descarga, em situa√ß√µes em que os volumes de dados n√£o s√£o grandes e n√£o h√° tempo para m√©todos mais sofisticados - isso √© bastante aceit√°vel (implementado neste artigo). </li><li>  Compreenda o trabalho dos pedidos de pesquisa longa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://vk.com/dev/using_longpoll</a> </li></ol><br><p>  Neste artigo, um m√©todo simples e lento foi escolhido; no futuro, provavelmente escreverei um micro artigo sobre maneiras de contornar ou facilitar as restri√ß√µes no n√∫mero de solicita√ß√µes por segundo. </p><br><h4 id="itog">  Sum√°rio </h4><br><p>  Com o "alguns" usu√°rios iniciais com cerca de 150 amigos, eles conseguiram 4.679 textos - cada um caracteriza uma determinada comunidade VK.  Os textos variam muito em tamanho e s√£o escritos em v√°rios idiomas - alguns deles n√£o s√£o adequados para nossos prop√≥sitos, mas falaremos um pouco mais sobre isso. </p><br><h3 id="osnovnaya-chast">  Corpo principal </h3><br><p><img src="https://habrastorage.org/webt/bj/to/hm/bjtohmsrvsxlcbs78u0thlawxky.png" alt="imagem"></p><br><p>  Vamos analisar todos os blocos do nosso pipeline - primeiro, no obrigat√≥rio (Ideal), depois no resto - eles s√£o do maior interesse. </p><br><h4 id="countvectorizer">  Countvectorizer </h4><br><p>  Antes de ensinar a LDA, precisamos apresentar nossos documentos na forma de uma matriz de documentos de termos.  Isso geralmente inclui opera√ß√µes como: </p><br><ul><li>  Remo√ß√£o de puttuctions / n√∫meros / tokens desnecess√°rios. </li><li>  Tokeniza√ß√£o (apresenta√ß√£o como uma lista de palavras) </li><li>  Contando palavras, compilando uma matriz t√©rmica de documentos. </li></ul><br><p>  Todas essas a√ß√µes no sklearn s√£o convenientemente implementadas na estrutura de uma entidade do programa - sklearn.feature_extraction.text.CountVectorizer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link da documenta√ß√£o</a> </p><br><p>  Tudo que voc√™ precisa fazer √©: </p><br><pre> <code class="python hljs">count_vect = CountVectorizer(input=<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, stop_words=stopwords, vocabulary=voc) dataset = count_vect.fit_transform(train_names)</code> </pre> <br><h4 id="lda">  Lda </h4><br><p>  Da mesma forma que com o CountVectorizer, o LDA √© perfeitamente implementado no Sklearn e em outras estruturas; portanto, n√£o faz muito sentido dedicar muito espa√ßo diretamente √†s suas implementa√ß√µes, em nosso artigo puramente pr√°tico. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link da documenta√ß√£o</a> </p><br><p>  Tudo o que voc√™ precisa para iniciar o LDA √©: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#training LDA lda = LDA(n_components = 60, max_iter=30, n_jobs=6, learning_method='batch', verbose=1) lda.fit(dataset)</span></span></code> </pre> <br><h4 id="preprocessing">  Pr√©-processamento </h4><br><p>  Se apenas pegarmos nossos textos imediatamente ap√≥s baix√°-los e convert√™-los em uma matriz Term-document usando o CountVectorizer, com o tokenizer padr√£o embutido, obteremos uma matriz de tamanho 4679x769801 (nos dados que eu uso). </p><br><p>  O tamanho do nosso dicion√°rio ser√° 769801. Mesmo se assumirmos que a maioria das palavras √© informativa, ainda n√£o √© poss√≠vel obter um bom LDA, algo como "Maldi√ß√µes das Dimens√µes" nos espera, sem mencionar que, para quase qualquer computador, vamos entupir toda a RAM.  De fato, a maioria dessas palavras √© completamente pouco informativa.  A grande maioria deles s√£o: </p><br><ul><li>  Emoticons, personagens, n√∫meros. </li><li>  Palavras √∫nicas ou muito raras (por exemplo, palavras polonesas de um grupo com memes poloneses, palavras escritas incorretamente ou em 'alban√™s'). </li><li>  Partes do discurso muito frequentes (por exemplo, preposi√ß√µes e pronomes). </li></ul><br><p>  Al√©m disso, muitos grupos no VK se especializam exclusivamente em imagens - quase n√£o h√° postagens de texto - os textos correspondentes a eles s√£o degenerados; na matriz de documentos t√©rmicos, eles nos fornecer√£o quase zero linhas. </p><br><p>  E ent√£o, vamos resolver tudo! <br>  N√≥s tokenizamos todos os textos, removemos pontua√ß√£o e n√∫meros deles, observamos o histograma da distribui√ß√£o dos textos pelo n√∫mero de palavras: <br><img src="https://habrastorage.org/webt/v4/qh/w0/v4qhw0mrgpizranmnptbz5lnivk.png" alt="imagem"></p><br><p>  Removemos todos os textos com menos de 100 palavras (existem 525) </p><br><p>  Agora o dicion√°rio: <br>  Remover todos os tokens (palavras) que n√£o s√£o letras, na estrutura de nossa tarefa - isso √© bastante aceit√°vel.  O CountVectorizer faz isso por conta pr√≥pria, mesmo que n√£o, ent√£o acho que n√£o h√° necessidade de dar exemplos (eles est√£o na vers√£o completa do c√≥digo do artigo). </p><br><p>  Um dos procedimentos mais comuns para reduzir o tamanho de um dicion√°rio √© remover as chamadas palavras de parada (palavras de parada) - palavras que n√£o carregam uma carga sem√¢ntica e / ou n√£o t√™m cores tem√°ticas (no nosso caso, Modelagem de T√≥picos).  Tais palavras em nosso caso s√£o, por exemplo: </p><br><ul><li>  Pronomes e preposi√ß√µes. </li><li>  Artigos - a, a. </li><li>  Palavras comuns: 'ser', 'bom', 'provavelmente' etc. </li></ul><br><p>  O m√≥dulo nltk formou listas de palavras irrelevantes em russo e ingl√™s, mas elas s√£o bastante fracas.  Na Internet, voc√™ tamb√©m pode encontrar listas de palavras irrelevantes para qualquer idioma e adicion√°-las √†s do nltk.  Ent√£o vamos fazer.  Tome palavras-chave adicionais a partir daqui: </p><br><ul><li>  <a href="">https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.json</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://gist.github.com/menzenski/7047705</a> </li></ul><br><p>  Na pr√°tica, ao resolver problemas espec√≠ficos, as listas de palavras irrelevantes s√£o gradualmente ajustadas e suplementadas √† medida que os modelos s√£o treinados, pois para cada conjunto de dados e problema espec√≠ficos existem palavras "inconsistentes" espec√≠ficas.  Tamb√©m pegaremos palavras de ordem personalizadas ap√≥s o treinamento de nossa primeira gera√ß√£o de LDA. </p><br><p>  Por si s√≥, o procedimento para remover palavras irrelevantes √© incorporado ao CountVectorizer - precisamos apenas de uma lista delas. </p><br><p>  O que fizemos foi suficiente? </p><br><p><img src="https://habrastorage.org/webt/ja/xd/6l/jaxd6lnbbnd_dmmmk6nog9a2ntm.png" alt="imagem"></p><br><p>  A maioria das palavras que est√£o em nosso dicion√°rio ainda n√£o √© muito informativa para aprender LDA sobre elas e n√£o est√° na lista de palavras irrelevantes.  Portanto, aplicamos outro m√©todo de filtragem aos nossos dados. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mspace linebreak=&quot;newline&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak=&quot;newline&quot; /></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="37.218ex" height="8.202ex" viewBox="0 -832 16024.3 3531.4" role="img" focusable="false" style="vertical-align: -6.27ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-69" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-64" x="345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-66" x="869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-28" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-74" x="1809" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-2C" x="2170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-44" x="2615" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-29" x="3444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-3D" x="4111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-6C" x="5417" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-6F" x="5716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-67" x="6201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-66" x="6932" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-72" x="7482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-61" x="7934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-63" x="8463" y="0"></use><g transform="translate(8897,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-7C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-44" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-7C" x="1107" y="0"></use></g><g transform="translate(10282,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-7C" x="0" y="0"></use><g transform="translate(0,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-69" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-6E" x="1119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-44" x="1719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-3A" x="2825" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-74" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-69" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-6E" x="4339" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMATHI-64" x="4939" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhibgXqZl_jai9tCFiLwBlwxsQBJ-w#MJMAIN-7C" x="5463" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak="newline"></mspace></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> idf (t, D) = \ log \ frac {| D |} {| \\ {d \ in D: t \ in d \\} |} </script></p><br><p>  onde <br>  t √© uma palavra do dicion√°rio. <br>  D - caso (muitos textos) <br>  d √© um dos textos do corpo. <br>  N√≥s calculamos o IDF de todas as nossas palavras e cortamos as palavras com o maior IDF (muito raro) e com o menor (palavras comuns). </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#'training' (tf-)idf vectorizer. tf_idf = TfidfVectorizer(input='filename', stop_words=stopwords, smooth_idf=False ) tf_idf.fit(train_names) #getting idfs idfs = tf_idf.idf_ #sorting out too rare and too common words lower_thresh = 3. upper_thresh = 6. not_often = idfs &gt; lower_thresh not_rare = idfs &lt; upper_thresh mask = not_often * not_rare good_words = np.array(tf_idf.get_feature_names())[mask] #deleting punctuation as well. cleaned = [] for word in good_words: word = re.sub("^(\d+\w*$|_+)", "", word) if len(word) == 0: continue cleaned.append(word)</span></span></code> </pre> <br><p>  Obtidos ap√≥s os procedimentos acima j√° s√£o bastante adequados para o treinamento de LDA, mas faremos mais esfor√ßos - as mesmas palavras s√£o frequentemente encontradas em nosso conjunto de dados, mas em casos diferentes.  Para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">stemming, foi utilizado o pymystem3</a> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Stemming m = Mystem() stemmed = set() voc_len = len(cleaned) for i in tqdm(range(voc_len)): word = cleaned.pop() stemmed_word = m.lemmatize(word)[0] stemmed.add(stemmed_word) stemmed = list(stemmed) print('After stemming: %d'%(len(stemmed)))</span></span></code> </pre> <br><p>  Depois de aplicar a filtragem acima, o tamanho do dicion√°rio diminuiu de 769801 para <br>  13611 e j√° com esses dados, voc√™ pode obter um modelo de LDA de qualidade aceit√°vel. </p><br><h3 id="testirovanie-primenenie-i-tyuning-lda">  Testando, aplicando e ajustando LDA </h3><br><p>  Agora que temos o conjunto de dados, o pr√©-processamento e os modelos que treinamos no conjunto de dados processado, seria bom verificar a adequa√ß√£o de nossos modelos e criar alguns aplicativos para eles. </p><br><p>  Como um aplicativo, para iniciantes, considere a tarefa de gerar palavras-chave para um determinado texto.  Voc√™ pode fazer isso de uma maneira bastante simples da seguinte maneira: </p><br><ol><li>  Obtemos da LDA a distribui√ß√£o de t√≥picos para este texto. </li><li>  Escolha n (por exemplo, n = 2) dos t√≥picos mais pronunciados. </li><li>  Para cada t√≥pico, escolha m (por exemplo m = 3) as palavras mais caracter√≠sticas. </li><li>  Temos um conjunto de n * m palavras que caracterizam um determinado texto. </li></ol><br><p>  Escreveremos uma classe de interface simples que implementar√° este m√©todo de gera√ß√£o de palavras-chave: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Let\`s do simple interface class class TopicModeler(object): ''' Inteface object for CountVectorizer + LDA simple usage. ''' def __init__(self, count_vect, lda): ''' Args: count_vect - CountVectorizer object from sklearn. lda - LDA object from sklearn. ''' self.lda = lda self.count_vect = count_vect self.count_vect.input = 'content' def __call__(self, text): ''' Gives topics distribution for a given text Args: text - raw text via python string. returns: numpy array - topics distribution for a given text. ''' vectorized = self.count_vect.transform([text]) lda_topics = self.lda.transform(vectorized) return lda_topics def get_keywords(self, text, n_topics=3, n_keywords=5): ''' For a given text gives n top keywords for each of m top texts topics. Args: text - raw text via python string. n_topics - int how many top topics to use. n_keywords - how many top words of each topic to return. returns: list - of m*n keywords for a given text. ''' lda_topics = self(text) lda_topics = np.squeeze(lda_topics, axis=0) n_topics_indices = lda_topics.argsort()[-n_topics:][::-1] top_topics_words_dists = [] for i in n_topics_indices: top_topics_words_dists.append(self.lda.components_[i]) shape=(n_keywords*n_topics, self.lda.components_.shape[1]) keywords = np.zeros(shape=shape) for i,topic in enumerate(top_topics_words_dists): n_keywords_indices = topic.argsort()[-n_keywords:][::-1] for k,j in enumerate(n_keywords_indices): keywords[i * n_keywords + k, j] = 1 keywords = self.count_vect.inverse_transform(keywords) keywords = [keyword[0] for keyword in keywords] return keywords</span></span></code> </pre> <br><p>  Aplicamos nosso m√©todo a v√°rios textos e vemos o que acontece: <br>  Comunidade <strong>:</strong> Ag√™ncia de viagens "Colors of the World" <br>  <strong>Palavras-chave:</strong> ['foto', 'social', 'viagem', 'comunidade', 'viagem', 'euro', 'acomoda√ß√£o', 'pre√ßo', 'Pol√¥nia', 'partida'] <br>  <strong>Comunidade:</strong> Food Gifs <br>  <strong>Palavras-chave:</strong> ['manteiga', 'st', 'sal', 'pc', 'massa', 'cozinhar', 'cebola', 'pimenta', 'a√ß√∫car', 'gr'] </p><br><p>  Os resultados acima n√£o s√£o 'cherry pick' e parecem bastante adequados.  De fato, esses s√£o os resultados de um modelo j√° configurado.  Os primeiros LDAs treinados como parte deste artigo produziram resultados significativamente piores, entre as palavras-chave que voc√™ costumava ver, por exemplo: </p><br><ol><li>  Componentes compostos de endere√ßos da web: www, http, ru, com ... </li><li>  Palavras comuns. </li><li>  unidades: cm, metro, km ... </li></ol><br><p>  O ajuste (ajuste) do modelo foi realizado da seguinte forma: </p><br><ol><li>  Para cada t√≥pico, selecione n (n = 5) palavras mais caracter√≠sticas. </li><li>  N√≥s os consideramos idf, de acordo com o caso de treinamento. </li><li>  Trazemos palavras-chave de 5 a 10% das mais difundidas. </li></ol><br><p>  Essa "limpeza" deve ser realizada com cuidado, pr√©-visualizando aquelas 10% das palavras.  Em vez disso, os candidatos √† exclus√£o devem ser escolhidos dessa maneira e, em seguida, as palavras que devem ser exclu√≠das devem ser selecionadas manualmente. </p><br><p>  Em algum momento da gera√ß√£o 2-3 dos modelos, com uma maneira semelhante de selecionar palavras irrelevantes, para os 5% principais das distribui√ß√µes generalizadas de palavras principais, obtemos: <br>  ['qualquer', 'completamente', 'certo', 'f√°cil', 'pr√≥ximo', 'internet', 'pequeno', 'caminho', 'dif√≠cil', 'humor', 'tanto', 'conjunto', ' op√ß√£o ',' nome ',' discurso ',' programa ',' competi√ß√£o ',' m√∫sica ',' alvo ',' filme ',' pre√ßo ',' jogo ',' sistema ',' jogar ',' empresa ' , 'legal'] </p><br><h3 id="esche-prilozheniya">  Mais aplica√ß√µes </h3><br><p>  A primeira coisa que me vem √† mente especificamente √© usar a distribui√ß√£o de t√≥picos no texto como 'incorpora√ß√£o' de textos; nessa interpreta√ß√£o, voc√™ pode aplicar algoritmos de visualiza√ß√£o ou clustering a eles e procurar os agrupamentos tem√°ticos 'efetivos' finais dessa maneira. </p><br><p>  Vamos fazer o seguinte: </p><br><pre> <code class="python hljs">term_doc_matrix = count_vect.transform(names) embeddings = lda.transform(term_doc_matrix) kmeans = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">30</span></span>) clust_labels = kmeans.fit_predict(embeddings) clust_centers = kmeans.cluster_centers_ embeddings_to_tsne = np.concatenate((embeddings,clust_centers), axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) tSNE = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>) tsne_embeddings = tSNE.fit_transform(embeddings_to_tsne) tsne_embeddings, centroids_embeddings = np.split(tsne_embeddings, [len(clust_labels)], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  Na sa√≠da, temos a seguinte imagem: <br><img src="https://habrastorage.org/webt/7j/2r/qv/7j2rqvpp2-blr9vfq44qjs75upc.png" alt="imagem"></p><br><p>  Cruzes s√£o os centros de gravidade (cenroids) dos aglomerados. </p><br><p>  Na imagem tSNE de incorpora√ß√µes, pode-se ver que os clusters selecionados usando o KMeans formam conjuntos bastante conectados e, na maioria das vezes, separ√°veis ‚Äã‚Äãespacialmente. </p><br><p>  Tudo o resto, depende de voc√™. </p><br><p>  Link para todo o c√≥digo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://gitlab.com/Mozes/VK_LDA</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417167/">https://habr.com/ru/post/pt417167/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt417155/index.html">Linux kernel 4.18: o que est√° se preparando para o pr√≥ximo lan√ßamento</a></li>
<li><a href="../pt417157/index.html">A singularidade est√° se aproximando: a IA come√ßa a controlar rob√¥s</a></li>
<li><a href="../pt417161/index.html">Burger King: vigil√¢ncia secreta, mentiras, roubo de cart√µes banc√°rios. Continua√ß√£o</a></li>
<li><a href="../pt417163/index.html">Commits bem</a></li>
<li><a href="../pt417165/index.html">O que amea√ßa o Burger King</a></li>
<li><a href="../pt417171/index.html">Estudo: Fundos de hedge administrados por mulheres mostram melhores resultados</a></li>
<li><a href="../pt417173/index.html">‚ÄúOld New Vinyl‚Äù: 20 materiais sobre a hist√≥ria e a produ√ß√£o de toca-discos e discos</a></li>
<li><a href="../pt417175/index.html">Restaura√ß√£o do sem√°foro de estrada Acme da primeira metade do s√©culo XX</a></li>
<li><a href="../pt417177/index.html">Servidor web local no Linux, com aumento autom√°tico de host e altern√¢ncia de vers√£o PHP</a></li>
<li><a href="../pt417179/index.html">Configurando um ambiente de desenvolvimento dom√©stico (janela de encaixe + gitlab + DNS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>