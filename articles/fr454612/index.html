<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö∫ üéå üòï Agents d'apprentissage automatique chez Unity ü§ó üë®üèæ‚Äçüîß üíï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cet article sur les agents d'apprentissage automatique chez Unity a √©t√© √©crit par Michael Lanham, un innovateur technique, d√©veloppeur actif pour Unit...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Agents d'apprentissage automatique chez Unity</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454612/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/6fe/055/9aa6fe055c20cde3642bb0f0782f62d3.jpg" alt="image"></div><br>  <em>Cet article sur les agents d'apprentissage automatique chez Unity a √©t√© √©crit par Michael Lanham, un innovateur technique, d√©veloppeur actif pour Unity, consultant, gestionnaire et auteur de nombreux jeux, projets graphiques et livres Unity.</em> <br><br>  Les d√©veloppeurs Unity ont impl√©ment√© la prise en charge de l'apprentissage automatique, et en particulier de l'apprentissage par renforcement pour la cr√©ation de SDK d'apprentissage par renforcement profond (DRL) pour les d√©veloppeurs de jeux et de simulation.  Heureusement, l'√©quipe Unity, dirig√©e par Danny Lange, a r√©ussi √† mettre en ≈ìuvre un moteur DRL fiable et moderne capable de fournir des r√©sultats impressionnants.  Unity utilise le mod√®le d'optimisation de politique proximale (PPO) comme base du moteur DRL;  ce mod√®le est beaucoup plus complexe et peut diff√©rer √† certains √©gards. <br><br>  Dans cet article, je vais vous pr√©senter les outils et les SDK pour cr√©er des agents DRL dans les jeux et les simulations.  Malgr√© la nouveaut√© et la puissance de cet outil, il est facile √† utiliser et il dispose d'outils auxiliaires qui vous permettent d'apprendre des concepts d'apprentissage automatique en d√©placement.  Pour travailler avec le didacticiel, vous devez installer le moteur Unity. <br><a name="habracut"></a><br><h2>  Installer des agents ML </h2><br>  Dans cette section, je parlerai bri√®vement des √©tapes √† suivre pour installer le SDK ML-Agents.  Ce mat√©riel est toujours en version b√™ta et peut varier d'une version √† l'autre.  Suivez ces √©tapes: <br><br><ol><li>  Installez Git sur l'ordinateur;  Cela fonctionne √† partir de la ligne de commande.  Git est un syst√®me de gestion de code source tr√®s populaire, et il existe de nombreuses ressources sur Internet concernant l'installation et l'utilisation de Git sur toutes les plateformes.  Apr√®s avoir install√© Git, assurez-vous que cela fonctionne en cr√©ant un clone de n'importe quel r√©f√©rentiel. </li><li>  Ouvrez une invite de commande ou un shell standard.  Les utilisateurs de Windows peuvent ouvrir la fen√™tre Anaconda. </li><li>  Acc√©dez au dossier de travail dans lequel vous souhaitez placer votre nouveau code et entrez la commande suivante (les utilisateurs Windows peuvent s√©lectionner C: \ ML-Agents): <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents </pre></li><li>  Vous clonez donc le r√©f√©rentiel ml-agents sur votre ordinateur et cr√©ez un nouveau dossier du m√™me nom.  Vous pouvez √©galement ajouter un num√©ro de version au nom du dossier.  L'unit√©, comme presque tout le monde de l'intelligence artificielle, est en constante √©volution, du moins pour l'instant.  Cela signifie que de nouveaux changements apparaissent constamment.  Au moment de l'√©criture, nous clonons le r√©f√©rentiel dans le dossier ml-agents.6: <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents ml-agents.6 </pre></li><li>  Cr√©ez un nouvel environnement virtuel pour ml-agents et sp√©cifiez la version 3.6, comme ceci: <br><br><pre>  #Windows 
 conda create -n ml-agents python = 3,6
 
 #Mac
 Utilisez la documentation de votre environnement pr√©f√©r√© </pre></li><li>  R√©activez votre environnement avec Anaconda: <br><br><pre>  activer les agents ml </pre></li><li>  Installez TensorFlow.  Dans Anaconda, cela peut √™tre fait avec la commande suivante: <br><br><pre>  pip install tensorflow == 1.7.1 </pre></li><li>  Installez les packages Python.  Dans Anaconda, entrez les informations suivantes: <br><br><pre><code class="plaintext hljs">cd ML-Agents #from root folder cd ml-agents or cd ml-agents.6 #for example cd ml-agents pip install -e . or pip3 install -e .</code> </pre> </li><li>  Vous installez donc tous les packages SDK d'agents n√©cessaires;  cela peut prendre plusieurs minutes.  Ne fermez pas la fen√™tre, elle vous sera bient√¥t utile. </li></ol><br>  Nous avons donc install√© et configur√© le SDK Unity Python pour ML-Agents.  Dans la section suivante, nous apprendrons comment configurer et former l'un des nombreux environnements fournis par Unity. <br><br><h2>  Formation d'agent </h2><br>  Nous pouvons maintenant passer imm√©diatement aux choses s√©rieuses et explorer des exemples qui utilisent l'apprentissage par renforcement profond (DRL).  Heureusement, il existe plusieurs exemples dans la bo√Æte √† outils du nouvel agent pour d√©montrer la puissance du moteur.  Ouvrez Unity ou Unity Hub et proc√©dez comme suit: <br><br><ol><li>  Cliquez sur le bouton Ouvrir un projet en haut de la bo√Æte de dialogue Projet. </li><li>  Localisez et ouvrez le dossier du projet UnitySDK, comme indiqu√© dans la capture d'√©cran: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/371/706/016/3717060168f78fccd271c064f0e055ce.png"></div><br>  <i>Ouvrez le projet Unity SDK</i> </li><li>  Attendez que le projet se charge, puis ouvrez la fen√™tre Projet en bas de l'√©diteur.  Si une fen√™tre s'ouvre vous demandant de mettre √† jour le projet, s√©lectionnez oui ou continuez.  Actuellement, tout le code d'agent est r√©trocompatible. </li><li>  Localisez et ouvrez la sc√®ne GridWorld comme indiqu√© dans la capture d'√©cran: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0aa/6bd/ab5/0aa6bdab5b8d07cdf414e59255862c05.png"></div><br>  <em>Ouverture d'un exemple de sc√®ne GridWorld</em> </li><li>  S√©lectionnez l'objet GridAcademy dans la fen√™tre Hi√©rarchie. </li><li>  Acc√©dez √† la fen√™tre Inspecteur et √† c√¥t√© du champ Cerveau, cliquez sur l'ic√¥ne pour ouvrir la bo√Æte de dialogue de s√©lection du cerveau: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08c/cda/126/08ccda1263b74d131c70cff42edae92c.png"></div></li><li>  S√©lectionnez le cerveau du GridWorldPlayer.  Ce cerveau appartient au joueur, c'est-√†-dire que le joueur (vous) peut contr√¥ler le jeu. </li><li>  Cliquez sur le bouton Lecture en haut de l'√©diteur et observez l'environnement.  Le jeu √©tant d√©sormais configur√© pour contr√¥ler le joueur, vous pouvez utiliser les touches WASD pour d√©placer le cube.  La t√¢che consiste √† d√©placer le cube bleu vers le symbole vert +, tout en √©vitant le X rouge. </li></ol><br>  Mettez-vous √† l'aise dans le jeu.  Notez que le jeu ne fonctionne que pendant une certaine p√©riode de temps et n'est pas au tour par tour.  Dans la section suivante, nous apprendrons comment ex√©cuter cet exemple avec l'agent DRL. <br><br><h2>  Qu'y a-t-il dans le cerveau? </h2><br>  L'un des aspects √©tonnants de la plate-forme ML-Agents est la possibilit√© de passer rapidement et facilement de la gestion des joueurs √† la gestion de l'IA / des agents.  Pour cela, Unity utilise le concept de ¬´cerveau¬ª.  Le cerveau peut √™tre contr√¥l√© soit par le joueur, soit par l'agent (cerveau apprenant).  La chose la plus √©tonnante est que vous pouvez assembler le jeu et le tester en tant que joueur, puis le donner sous le contr√¥le d'un agent RL.  Gr√¢ce √† cela, tout jeu √©crit avec un peu d'effort peut √™tre fait pour √™tre contr√¥l√© √† l'aide de l'IA. <br><br>  Le processus de configuration et de d√©marrage de la formation d'agent RL dans Unity est assez simple.  Unity utilise Python externe pour construire un mod√®le du cerveau apprenant.  L'utilisation de Python a beaucoup de sens car il existe d√©j√† plusieurs biblioth√®ques d'apprentissage profond (DL) construites autour de lui.  Pour former l'agent dans GridWorld, proc√©dez comme suit: <br><br><ol><li>  S√©lectionnez √† nouveau GridAcademy et s√©lectionnez GridWorldLearning dans le champ Brains au lieu de GridWorldPlayer: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/869/fc8/b42/869fc8b42b64d8b3ccc785eb9a6e765e.png"></div><br>  <em>Passer √† l'utilisation de GridWorldLearning Brain</em> </li><li>  Cochez la case Contr√¥le √† droite.  Ce param√®tre simple indique que le cerveau peut √™tre contr√¥l√© de l'ext√©rieur.  Cette option doit √™tre activ√©e. </li><li>  S√©lectionnez l'objet trueAgent dans la fen√™tre Hi√©rarchie, puis dans la fen√™tre Inspecteur, changez la propri√©t√© Brain dans le composant Grid Agent en GridWorldLearning brain: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/ff3/ed2/832ff3ed2bc17c16cc5ed823d10a7156.png"></div><br>  <em>GridWorldLearning emploi de cerveau pour agent</em> </li><li>  Dans cet exemple, nous avons besoin √† la fois de l'Acad√©mie et de l'Agent pour utiliser le m√™me cerveau GridWorldLearning.  Basculez vers la fen√™tre Anaconda ou Python et s√©lectionnez le dossier ML-Agents / ml-agents. </li><li>  Ex√©cutez la commande suivante dans une fen√™tre Anaconda ou Python √† l'aide de l'environnement virtuel ml-agents: <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = firstRun --train </pre></li><li>  Cela lancera le mod√®le de formation Unity PPO et un exemple d'agent avec la configuration sp√©cifi√©e.  √Ä un certain moment, la fen√™tre d'invite de commandes vous demandera de d√©marrer l'√©diteur Unity avec l'environnement charg√©. </li><li>  Cliquez sur Lecture dans l'√©diteur Unity pour lancer l'environnement GridWorld.  Peu de temps apr√®s, vous devriez voir la formation de l'agent et la sortie vers la fen√™tre de script Python: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/583/cc2/fd9/583cc2fd94c49e4a1039d9af64312f68.png"></div><br>  <em>Ex√©cuter GridWorld en mode d'apprentissage</em> </li><li>  Notez que le script mlagents-learn est un code Python qui construit un mod√®le RL pour ex√©cuter un agent.  Comme vous pouvez le voir sur la sortie du script, plusieurs param√®tres (hyperparam√®tres) doivent √™tre configur√©s. </li><li>  Laissez l'agent apprendre quelques milliers d'it√©rations et notez √† quelle vitesse il apprend.  Le mod√®le interne utilis√© ici appel√© PPO s'est av√©r√© √™tre un mod√®le d'apprentissage tr√®s efficace pour de nombreuses t√¢ches diff√©rentes, et il est tr√®s bien adapt√© au d√©veloppement de jeux.  Avec un √©quipement suffisamment puissant, un agent peut id√©alement apprendre en moins d'une heure. </li></ol><br>  Laissez l‚Äôagent apprendre davantage et explorez d‚Äôautres moyens de suivre le processus d‚Äôapprentissage de l‚Äôagent, comme indiqu√© dans la section suivante. <br><br><h2>  Surveillance de l'apprentissage avec TensorBoard </h2><br>  La formation d'un agent √† l'aide du mod√®le RL ou de tout mod√®le DL est souvent une t√¢che ardue et n√©cessite une attention aux d√©tails.  Heureusement, TensorFlow dispose d'un ensemble d'outils graphiques appel√©s TensorBoard que vous pouvez utiliser pour surveiller votre processus d'apprentissage.  Suivez ces √©tapes pour d√©marrer TensorBoard: <br><br><ol><li>  Ouvrez une fen√™tre Anaconda ou Python.  Activez l'environnement virtuel ml-agents.  Ne fermez pas la fen√™tre dans laquelle le mod√®le de formation s'ex√©cute;  nous en avons besoin pour continuer. </li><li>  Acc√©dez au dossier ML-Agents / ml-agents et ex√©cutez la commande suivante: <br><br><pre>  tensorboard --logdir = r√©sum√©s </pre></li><li>  Nous lan√ßons donc TensorBoard sur notre propre serveur Web int√©gr√©.  Vous pouvez charger la page en utilisant l'URL indiqu√©e apr√®s la commande pr√©c√©dente. </li><li>  Saisissez l'URL du TensorBoard comme indiqu√© dans la fen√™tre ou saisissez localhost: 6006 ou machinename: 6006 dans le navigateur.  Apr√®s environ une heure, vous devriez voir quelque chose comme ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/619/b62/964/619b62964a88d9ee4b7635073799caf8.png"></div><br>  <em>Fen√™tre de graphique TensorBoard</em> </li><li>  La capture d'√©cran pr√©c√©dente montre des graphiques, chacun affichant un aspect distinct de la formation.  Pour comprendre comment notre agent est form√©, vous devez traiter chacun de ces graphiques, nous analyserons donc les r√©sultats de chaque section: </li></ol><br><ul><li>  Environnement: cette section montre comment l'agent se manifeste dans l'environnement dans son ensemble.  Voici une vue plus d√©taill√©e des graphiques avec la tendance pr√©f√©r√©e: </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/596/956/bc15969569f3d959bc550c2ee629ac3d.png"></div><br>  <em>Une image d√©taill√©e des graphiques de la section Environnement</em> <br><br><ul><li>  R√©compense cumulative: il s'agit de la r√©compense totale qui maximise l'agent.  Habituellement, il est n√©cessaire qu'il augmente, mais pour une raison quelconque, il peut diminuer.  Il est toujours pr√©f√©rable de maximiser les r√©compenses entre 1 et -1.  Si les r√©compenses de calendrier d√©passent cette plage, cela doit √©galement √™tre corrig√©. </li><li>  Dur√©e de l'√©pisode: si cette valeur diminue, c'est g√©n√©ralement un bon signe.  En fin de compte, plus les √©pisodes sont courts, plus l'entra√Ænement est important.  Cependant, gardez √† l'esprit que si n√©cessaire, la dur√©e des √©pisodes peut augmenter, de sorte que l'image peut √™tre diff√©rente. </li><li>  Le√ßon: ce tableau indique clairement dans quelle le√ßon l'agent se trouve;  Il est destin√© √† l'apprentissage du curriculum. </li><li>  Pertes: cette section pr√©sente des graphiques repr√©sentant les pertes ou les co√ªts calcul√©s pour la police et la valeur.  Voici une capture d'√©cran de cette section avec des fl√®ches pointant vers les param√®tres optimaux: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aaa/8c5/3f6/aaa8c53f67533a8d349b62d216c15a1b.png"></div><br>  <em>Pertes et formation pr√©f√©r√©e</em> </li></ul><br><ul><li>  Perte de police: ce graphique d√©termine la quantit√© de changement de police au fil du temps.  La politique est un √©l√©ment qui d√©finit les actions, et dans le cas g√©n√©ral, ce calendrier devrait tendre √† la baisse, montrant que la politique prend de meilleures d√©cisions. </li><li>  Perte de valeur: il s'agit de la perte moyenne de la fonction de valeur.  En substance, il mod√©lise la fa√ßon dont l'agent pr√©dit la valeur de son prochain √©tat.  Initialement, cette valeur devrait augmenter et apr√®s stabilisation de la r√©mun√©ration, elle devrait diminuer. </li><li>  Politique: pour √©valuer la qualit√© des actions en PPO, le concept de politique est utilis√©, pas un mod√®le.  La capture d'√©cran ci-dessous montre les graphiques de politique et la tendance pr√©f√©r√©e: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/0ce/2c0/8450ce2c0e0a95bb39e92fe698dbee7c.png"></div><br>  <i>Graphiques des politiques et tendances privil√©gi√©es</i> </li><li>  Entropie: ce graphique montre l'ampleur de l'agent de recherche.  Cette valeur doit √™tre r√©duite, car l'agent en apprend plus sur l'environnement et a besoin de moins de recherches. </li><li>  Taux d'apprentissage: dans ce cas, cette valeur devrait diminuer progressivement de fa√ßon lin√©aire. </li><li>  Estimation de la valeur: il s'agit de la valeur moyenne visit√©e par tous les √âtats agents.  Pour refl√©ter les connaissances accrues d‚Äôun agent, cette valeur doit cro√Ætre puis se stabiliser. </li></ul><br>  6. Laissez l'agent en cours d'ex√©cution jusqu'√† la fin et ne fermez pas le TensorBoard. <br>  7. Revenez √† la fen√™tre Anaconda / Python qui a entra√Æn√© le cerveau et ex√©cutez cette commande: <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = secondRun --train </pre><br>  8. Il vous sera √† nouveau demand√© de cliquer sur Lecture dans l'√©diteur;  faites-le.  Laissez l'agent commencer la formation et effectuez plusieurs s√©ances.  Dans le processus, regardez la fen√™tre TensorBoard et notez comment secondRun est affich√© sur les graphiques.  Vous pouvez laisser cet agent s'ex√©cuter jusqu'√† son terme, mais vous pouvez l'arr√™ter si vous le souhaitez. <br><br>  Dans les versions pr√©c√©dentes de ML-Agents, vous deviez d'abord cr√©er l'ex√©cutable Unity comme environnement d'apprentissage pour le jeu, puis l'ex√©cuter.  Le cerveau ext√©rieur de Python aurait d√ª fonctionner de la m√™me mani√®re.  Cette m√©thode a rendu tr√®s difficile le d√©bogage des probl√®mes dans le code ou dans le jeu.  Dans la nouvelle technique, toutes ces difficult√©s ont √©t√© √©limin√©es. <br><br>  Maintenant que nous avons vu √† quel point il est facile de configurer et de former l'agent, nous allons passer √† la section suivante, dans laquelle nous allons apprendre √† ex√©cuter l'agent sans le cerveau externe Python et √† l'ex√©cuter directement dans Unity. <br><br><h2>  Lancement d'agent </h2><br>  La formation en Python est g√©niale, mais vous ne pouvez pas l'utiliser dans un vrai jeu.  Id√©alement, nous aimerions cr√©er un graphique TensorFlow et l'utiliser dans Unity.  Heureusement, la biblioth√®que TensorFlowSharp a √©t√© cr√©√©e qui permet √† .NET d'utiliser les graphiques TensorFlow.  Cela nous permet de cr√©er des mod√®les TFModels hors ligne, puis de les injecter dans le jeu.  Malheureusement, nous ne pouvons utiliser que des mod√®les form√©s, mais pas les former de cette fa√ßon, du moins pas encore. <br><br>  Voyons comment cela fonctionne, en utilisant l'exemple du graphique que nous venons de former pour l'environnement GridWorld;  utilisez-le comme un cerveau int√©rieur dans Unity.  Suivez les √©tapes de la section suivante pour configurer et utiliser votre cerveau int√©rieur: <br><br><ol><li>  T√©l√©chargez le plugin TFSharp <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="external nofollow">ici</a> </li><li>  Dans le menu de l'√©diteur, s√©lectionnez Actifs |  Importer un package |  Forfait personnalis√© ... </li><li>  Recherchez le package d'actifs que vous venez de t√©l√©charger et utilisez les bo√Ætes de dialogue d'importation pour charger le plugin dans le projet. </li><li>  Dans le menu, s√©lectionnez Modifier |  Param√®tres du projet.  La fen√™tre Param√®tres s'ouvre (apparue dans la version 2018.3) </li><li>  Recherchez les caract√®res Scripting Define Symbols dans les options du lecteur et modifiez le texte en ENABLE_TENSORFLOW, et activez √©galement le code Allow Unsafe, comme indiqu√© dans la capture d'√©cran: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/ecf/c31/79decfc310a7f6cf38b33d1998c14c1f.png"></div><br>  <em>D√©finition de l'indicateur ENABLE_TENSORFLOW</em> </li><li>  Recherchez l'objet GridWorldAcademy dans la fen√™tre Hi√©rarchie et assurez-vous qu'il utilise Brains |  GridWorldLearning.  D√©sactivez l'option Control dans la section Brains du script Grid Academy. </li><li>  Recherchez le cerveau GridWorldLearning dans le dossier Assets / Examples / GridWorld / Brains et assurez-vous que le param√®tre Model dans la fen√™tre Inspector est d√©fini, comme indiqu√© dans la capture d'√©cran: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/7c8/fef/fb47c8fefd97d4e855e8a2c029e4da5d.png"></div><br>  <em>T√¢che mod√®le pour le cerveau</em> </li><li>  GridWorldLearning doit d√©j√† √™tre d√©fini comme mod√®le.  Dans cet exemple, nous utilisons le TFModel fourni avec l'exemple GridWorld. </li><li>  Cliquez sur Lecture pour d√©marrer l'√©diteur et voir comment l'agent g√®re le cube. </li></ol><br>  Nous lan√ßons maintenant l'environnement pr√©-form√© Unity.  Dans la section suivante, nous apprendrons √† utiliser le cerveau que nous avons form√© dans la section pr√©c√©dente. <br><br><h2>  Chargement du cerveau form√© </h2><br>  Tous les exemples Unity ont des cerveaux pr√©-form√©s qui peuvent √™tre utilis√©s pour √©tudier des exemples.  Bien s√ªr, nous voulons pouvoir charger nos propres graphiques TF dans Unity et les ex√©cuter.  Pour charger un graphique entra√Æn√©, proc√©dez comme suit: <br><br><ol><li>  Acc√©dez au dossier ML-Agents / ml-agents / models / firstRun-0.  √Ä l'int√©rieur de ce dossier se trouve le fichier GridWorldLearning.bytes.  Faites glisser ce fichier dans le dossier Project / Assets / ML-Agents / Examples / GridWorld / TFModels √† l'int√©rieur de l'√©diteur Unity: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f51/955/ef5/f51955ef56aae53e7946378048b9e13e.png"></div><br>  <em>Faire glisser un graphique d'octets dans Unity</em> </li><li>  Nous importons donc le graphique dans le projet Unity en tant que ressource et le renommons en GridWorldLearning 1. Le moteur le fait car le mod√®le par d√©faut porte d√©j√† le m√™me nom. </li><li>  Recherchez GridWorldLearning dans le dossier cerveau, s√©lectionnez-le dans la fen√™tre Inspecteur et faites glisser le nouveau mod√®le GridWorldLearning 1 dans le champ Mod√®le des param√®tres du cerveau: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/784/8a3/9b7/7848a39b79b02ffb11d968f835054295.png"></div><br>  <em>Chargement du cerveau dans le champ Graph Model</em> </li><li>  √Ä ce stade, nous n'avons pas besoin de modifier d'autres param√®tres, mais portons une attention particuli√®re √† la configuration du cerveau.  Pour l'instant, les param√®tres standard feront l'affaire. </li><li>  Cliquez sur Jouer dans l'√©diteur Unity et voyez comment l'agent se d√©place avec succ√®s dans le jeu. </li><li>  Le succ√®s de l'agent dans le jeu d√©pend du temps de sa formation.  Si vous lui permettez de suivre la formation, l'agent sera similaire √† un agent Unity parfaitement form√©. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr454612/">https://habr.com/ru/post/fr454612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr454600/index.html">Comment nous avons conclu un accord s√ªr sur Freelansim: donner un choix, r√©duire les fonctionnalit√©s, comparer les commissions</a></li>
<li><a href="../fr454604/index.html">G√©n√©rer une application React avec un backend GraphQL en quelques minutes</a></li>
<li><a href="../fr454606/index.html">Caract√©ristiques de l'attribut inputmode pour le syst√®me d'exploitation mobile et les navigateurs</a></li>
<li><a href="../fr454608/index.html">Contrat de niveau de service: nous r√©digeons des SLA pour ... d'autres, ou la conclusion d'un SLA avec un op√©rateur de t√©l√©communications</a></li>
<li><a href="../fr454610/index.html">Marketing de contenu, r√©f√©rencement, tests et sondages: 9 outils pour promouvoir une startup √† l'√©tranger</a></li>
<li><a href="../fr454614/index.html">XXE: entit√© externe XML</a></li>
<li><a href="../fr454616/index.html">L'utilisation de l'IA pour augmenter l'efficacit√© des travailleurs mentaux</a></li>
<li><a href="../fr454618/index.html">Fosse de productivit√©: comment Slack blesse notre flux de travail</a></li>
<li><a href="../fr454620/index.html">#NoDeployFriday: aide ou nuit?</a></li>
<li><a href="../fr454622/index.html">Kreisel EVEX 910e: mod√®le historique - nouvelle vie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>