<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé¨ üë®‚Äçüé® ‚ùóÔ∏è ¬øQu√© tiene de malo el aprendizaje por refuerzo? üë®üèæ‚Äçüç≥ üìî üè¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A principios de 2018, se public√≥ un art√≠culo El aprendizaje de refuerzo profundo a√∫n no funciona ("Aprender con refuerzo a√∫n no funciona"). La princip...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øQu√© tiene de malo el aprendizaje por refuerzo?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/437020/"><p><img src="https://habrastorage.org/webt/hv/1l/vs/hv1lvsyszoctmnrbxex7valfo8a.jpeg"></p><br><p>  A principios de 2018, se public√≥ un art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El aprendizaje de refuerzo profundo a√∫n no funciona</a> ("Aprender con refuerzo a√∫n no funciona").  La principal queja fue que los algoritmos de aprendizaje modernos con refuerzo requieren aproximadamente la misma cantidad de tiempo para resolver un problema que una b√∫squeda aleatoria regular. </p><br><p>  ¬øHa cambiado algo desde entonces?  No </p><br><p>  El aprendizaje reforzado se considera uno de los tres caminos principales para construir una IA fuerte.  Pero las dificultades a las que se enfrenta esta √°rea de aprendizaje autom√°tico y los m√©todos que los cient√≠ficos est√°n tratando de enfrentar estas dificultades sugieren que puede haber problemas fundamentales con este enfoque en s√≠. </p><a name="habracut"></a><br><h2 id="postoyte-chto-znachit-odin-iz-treh-a-ostalnye-dva-kakie">  Espera, ¬øqu√© significa uno de los tres?  ¬øCu√°les son los otros dos? </h2><br><p>  Dado el √©xito de las redes neuronales en los √∫ltimos a√±os y el an√°lisis de c√≥mo funcionan con habilidades cognitivas de alto nivel, que anteriormente se consideraban caracter√≠sticas solo de humanos y animales superiores, hoy en la comunidad cient√≠fica existe la opini√≥n de que existen tres enfoques principales para crear IA fuerte en La base de las redes neuronales, que pueden considerarse m√°s o menos realistas: </p><br><h2 id="1-obrabotka-tekstov">  1. Procesamiento de textos </h2><br><p>  El mundo ha acumulado una gran cantidad de libros y textos en Internet, incluidos libros de texto y libros de referencia.  El texto es conveniente y r√°pido para procesar en una computadora.  Te√≥ricamente, este conjunto de textos deber√≠a ser suficiente para entrenar una IA de conversaci√≥n fuerte. </p><br><p>  Est√° impl√≠cito que en estas matrices textuales se refleja la estructura completa del mundo (al menos, se describe en libros de texto y libros de referencia).  Pero esto no es un hecho en absoluto.  Los textos como una forma de presentaci√≥n de informaci√≥n est√°n fuertemente divorciados del mundo tridimensional real y el curso del tiempo en que vivimos. </p><br><p>  Buenos ejemplos de IA entrenada en matrices de texto son los bots de chat y los traductores autom√°ticos.  Como para traducir el texto, debe comprender el significado de la frase y volver a contarla en palabras nuevas (en otro idioma).  Existe una idea err√≥nea com√∫n de que las reglas de gram√°tica y sintaxis, incluida una descripci√≥n de todas las posibles excepciones, describen completamente un lenguaje en particular.  Esto no es asi.  El lenguaje es solo una herramienta auxiliar en la vida, cambia f√°cilmente y se adapta a nuevas situaciones. </p><br><p>  El problema con el procesamiento de texto (incluso por sistemas expertos, incluso redes neuronales) es que <strong>no hay un</strong> conjunto de reglas, qu√© frases deben aplicarse en qu√© situaciones.  Tenga en cuenta: no las reglas para construir las frases en s√≠ mismas (qu√© hace la gram√°tica y la sintaxis), sino qu√© frases en qu√© situaciones.  En la misma situaci√≥n, las personas pronuncian frases en diferentes idiomas que generalmente no est√°n relacionadas entre s√≠ en t√©rminos de la estructura del idioma.  Compare frases con extrema sorpresa: "¬°oh Dios!"  y "¬°oh santa mierda!".  Bueno, y ¬øc√≥mo hacer una correspondencia entre ellos, conociendo el modelo de lenguaje?  De ninguna manera  Sucedi√≥ por casualidad hist√≥ricamente.  Necesita saber la situaci√≥n y lo que suelen hablar en un idioma en particular.  Es por esto que los traductores autom√°ticos son tan imperfectos. </p><br><p>  Se desconoce si este conocimiento puede distinguirse √∫nicamente de una serie de textos.  Pero si los traductores autom√°ticos traducen perfectamente sin cometer errores tontos y rid√≠culos, esto ser√° una prueba de que es posible crear una IA fuerte solo basada en texto. </p><br><h2 id="2-raspoznavanie-izobrazheniy">  2. Reconocimiento de imagen </h2><br><p>  Mira esta imagen </p><br><p><img src="https://habrastorage.org/webt/pa/od/nd/paodndrl6p5dkuhig3rwo68cu-q.jpeg"></p><br><p>  Mirando esta foto, entendemos que el rodaje se realiz√≥ por la noche.  A juzgar por las banderas, el viento sopla de derecha a izquierda.  Y a juzgar por el tr√°fico de la derecha, el caso no est√° sucediendo en Inglaterra o Australia.  Ninguna de esta informaci√≥n se indica expl√≠citamente en los p√≠xeles de la imagen, esto es conocimiento externo.  En la foto solo hay signos por los cuales podemos usar el conocimiento obtenido de otras fuentes. </p><br><div class="spoiler">  <b class="spoiler_title">¬øSabes algo m√°s mirando esta foto?</b> <div class="spoiler_text"><p>  Sobre eso y el discurso ... Y encuentrate una chica, finalmente </p></div></div><br><p>  Por lo tanto, se cree que si entrena una red neuronal para reconocer objetos en una imagen, tendr√° una idea interna de c√≥mo funciona el mundo real.  Y esta vista, obtenida de las fotograf√≠as, ciertamente corresponder√° a nuestro mundo real y real.  A diferencia de las matrices de textos donde esto no est√° garantizado. </p><br><p>  El valor de las redes neuronales entrenadas en una serie de fotograf√≠as ImageNet (y ahora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenImages V4</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">COCO</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">KITTI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BDD100K</a> y otras) no es en absoluto el reconocimiento de un gato en una foto.  Y eso se almacena en la pen√∫ltima capa.  Aqu√≠ es donde se encuentra un conjunto de caracter√≠sticas de alto nivel que describen nuestro mundo.  Un vector de 1024 n√∫meros es suficiente para obtener una descripci√≥n de 1000 categor√≠as diferentes de objetos con un 80% de precisi√≥n (y en el 95% de los casos la respuesta correcta estar√° en las 5 opciones m√°s cercanas).  Solo pi√©nsalo. </p><br><p>  Es por eso que estas caracter√≠sticas de la pen√∫ltima capa se usan con tanto √©xito en tareas completamente diferentes en la visi√≥n por computadora.  Mediante transferencia de aprendizaje y puesta a punto.  De este vector en 1024 n√∫meros puede obtener, por ejemplo, un mapa de profundidad de la imagen </p><br><p><img src="https://habrastorage.org/webt/vs/k6/lm/vsk6lmod2grqjzous7knxl5ekaq.jpeg"></p><br><p>  (un ejemplo del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo</a> donde se utiliza una red Densenet-169 pre-entrenada pr√°cticamente sin cambios) </p><br><p>  O determinar la pose de una persona.  Hay muchas aplicaciones </p><br><p><img src="https://habrastorage.org/webt/id/rs/sp/idrsspge5oaq0dae1-li5pghf3s.jpeg"></p><br><p>  Como resultado, el reconocimiento de im√°genes puede usarse potencialmente para crear una IA fuerte, ya que realmente refleja el modelo de nuestro mundo real.  Un paso de la fotograf√≠a al video, y el video es nuestra vida, ya que obtenemos aproximadamente el 99% de la informaci√≥n visualmente. </p><br><p>  Pero de la fotograf√≠a es completamente incomprensible c√≥mo motivar a la red neuronal a pensar y sacar conclusiones.  Ella puede ser entrenada para responder preguntas como "¬øcu√°ntos l√°pices hay en la mesa?"  (esta clase de tareas se llama Visual Question Answering, un ejemplo de dicho conjunto de datos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://visualqa.org</a> ).  O d√© una descripci√≥n textual de lo que est√° sucediendo en la foto.  Esta es la clase de tarea <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Subt√≠tulos de im√°genes</a> . </p><br><p><img src="https://habrastorage.org/webt/mp/lz/0y/mplz0y9uleukwz68u-lyc35wlqk.jpeg"></p><br><p>  ¬øPero es esta inteligencia?  Habiendo desarrollado este enfoque, en un futuro cercano, las redes neuronales podr√°n responder preguntas en video como "Dos gorriones estaban sentados en los cables, uno de ellos se fue volando, ¬øcu√°ntos gorriones quedaron?"  Esto es matem√°tica real, en casos un poco m√°s complicados, inaccesibles para los animales y al nivel de la educaci√≥n escolar humana.  Especialmente si, a excepci√≥n de los gorriones, habr√° pechos sentados junto a ellos, pero no es necesario tenerlos en cuenta, ya que la pregunta era solo sobre gorriones.  S√≠, definitivamente ser√° inteligencia. </p><br><h2 id="3-obuchenie-s-podkrepleniem-reinforcement-learning">  3. Aprendizaje de refuerzo </h2><br><p>  La idea es muy simple: alentar acciones que conduzcan a la recompensa y evitar el fracaso.  Esta es una forma universal de aprendizaje y, obviamente, definitivamente puede conducir a la creaci√≥n de una IA fuerte.  Por lo tanto, ha habido mucho inter√©s en el aprendizaje por refuerzo en los √∫ltimos a√±os. </p><br><div class="spoiler">  <b class="spoiler_title">Mezclar pero no agitar</b> <div class="spoiler_text"><p>  Por supuesto, es mejor crear una IA fuerte combinando los tres enfoques.  En im√°genes y con entrenamiento de refuerzo, puedes obtener IA de nivel animal.  Y agregando nombres textuales de objetos a las im√°genes (una broma, por supuesto, obligando a la IA a mirar videos donde las personas interact√∫an y hablan, como cuando ense√±a a un beb√©), y volver a entrenar en una matriz de texto para obtener conocimiento (un an√°logo de nuestra escuela y universidad), en teor√≠a puede obtener Nivel humano AI.  Capaz de hablar. </p></div></div><br><p>  El aprendizaje reforzado tiene una gran ventaja.  En el simulador, puede crear un modelo simplificado del mundo.  Entonces, para una figura humana, solo 17 grados de libertad son suficientes, en lugar de 700 en una persona viva (n√∫mero aproximado de m√∫sculos).  Por lo tanto, en el simulador puede resolver el problema en una dimensi√≥n muy peque√±a. </p><br><p>  Mirando hacia el futuro, los algoritmos modernos de aprendizaje por refuerzo no pueden controlar arbitrariamente el modelo de una persona, incluso con 17 grados de libertad.  Es decir, no pueden resolver el problema de optimizaci√≥n, donde hay 44 n√∫meros en la entrada y 17. En la entrada, es posible hacerlo solo en casos muy simples, con un ajuste fino de las condiciones iniciales y los hiperpar√°metros.  E incluso en este caso, por ejemplo, para ense√±ar un modelo humanoide con 17 grados de libertad para correr y comenzar desde una posici√≥n de pie (que es mucho m√°s simple), necesita varios d√≠as de c√°lculos en una GPU potente.  Y los casos un poco m√°s complicados, por ejemplo, aprender a levantarse de una pose arbitraria, tal vez nunca aprendan en absoluto.  Esto es un fracaso </p><br><p>  Adem√°s, todos los algoritmos de aprendizaje por refuerzo funcionan con redes neuronales deprimentemente peque√±as, pero no pueden hacer frente al aprendizaje de redes grandes.  Las redes de convoluci√≥n grandes se usan solo para reducir la dimensi√≥n de la imagen a varias caracter√≠sticas, que se alimentan a los algoritmos de aprendizaje con refuerzo.  El mismo humanoide en funcionamiento est√° controlado por una red Feed Forward con dos o tres capas de 128 neuronas.  Enserio?  Y en base a esto, ¬øestamos tratando de construir una IA fuerte? </p><br><p>  Para tratar de entender por qu√© sucede esto y qu√© tiene de malo el aprendizaje por refuerzo, primero debe familiarizarse con las arquitecturas b√°sicas del aprendizaje por refuerzo moderno. </p><br><p>  La estructura f√≠sica del cerebro y el sistema nervioso est√° ajustada por la evoluci√≥n al tipo espec√≠fico de animal y sus condiciones de vida.  Entonces, en el curso de la evoluci√≥n, una mosca desarroll√≥ un sistema nervioso y un trabajo de neurotransmisores en los ganglios (un an√°logo del cerebro en los insectos) para esquivar r√°pidamente a un matamoscas.  Bueno, no de un matamoscas, sino de aves que han pescado durante 400 millones de a√±os (es broma, las mismas aves aparecieron hace 150 millones de a√±os, muy probablemente de ranas 360 millones de a√±os).  Un rinoceronte lo suficiente como un sistema nervioso y cerebro para girar lentamente hacia el objetivo y comenzar a correr.  Y all√≠, como dicen, el rinoceronte tiene mala vista, pero este no es su problema. </p><br><p>  Pero adem√°s de la evoluci√≥n, cada individuo espec√≠fico, desde el nacimiento y durante toda la vida, trabaja precisamente con el mecanismo de aprendizaje habitual con refuerzo.  En el caso de los mam√≠feros, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">y tambi√©n de los insectos</a> , el sistema de dopamina hace este trabajo.  Su trabajo est√° lleno de secretos y matices, pero todo se reduce al hecho de que, en caso de un premio, el sistema de dopamina, a trav√©s de mecanismos de memoria, de alguna manera corrige las conexiones entre las neuronas que estaban activas inmediatamente antes.  As√≠ es como se forma la memoria asociativa. </p><br><p>  Que, debido a su asociatividad, se utiliza en la toma de decisiones.  En pocas palabras, si la situaci√≥n actual (neuronas activas actuales en esta situaci√≥n) a trav√©s de la memoria asociativa activa las neuronas de placer, entonces el individuo selecciona las acciones que realiz√≥ en una situaci√≥n similar y que record√≥.  "Elige acciones" es una definici√≥n pobre.  No hay elecci√≥n  Las neuronas de memoria de placer simplemente activadas, fijadas por el sistema de dopamina para una situaci√≥n dada, activan autom√°ticamente las neuronas motoras, lo que conduce a la contracci√≥n muscular.  Esto es si se necesita una acci√≥n inmediata. </p><br><p>  El aprendizaje artificial con refuerzo, como campo de conocimiento, es necesario para resolver estos dos problemas: </p><br><h3 id="1-podobrat-arhitekturu-neyroseti-chto-dlya-nas-uzhe-sdelala-evolyuciya">  1. Elija la arquitectura de la red neuronal (lo que la evoluci√≥n ya ha hecho por nosotros) </h3><br><p>  La buena noticia es que las funciones cognitivas superiores que se realizan en la neocorteza en los mam√≠feros (y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el cuerpo estriado de los c√≥rvidos</a> ) se realizan en una estructura aproximadamente uniforme.  Aparentemente, esto no necesita alguna "arquitectura" r√≠gidamente prescrita. </p><br><p>  La diversidad de las regiones del cerebro probablemente se deba a razones puramente hist√≥ricas.  Cuando, a medida que evolucionaron, nuevas partes del cerebro crecieron sobre las bases que quedaron de los primeros animales.  Por el principio funciona, no tocar.  Por otro lado, en diferentes personas, las mismas partes del cerebro reaccionan a las mismas situaciones.  Esto puede explicarse tanto por la asociatividad (caracter√≠sticas y "neuronas de la abuela" formadas naturalmente en estos lugares durante el proceso de aprendizaje) como por la fisiolog√≠a.  Que las v√≠as de se√±alizaci√≥n codificadas en los genes conducen precisamente a estas √°reas.  No hay consenso, pero puede leer, por ejemplo, este art√≠culo reciente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Inteligencia biol√≥gica y artificial"</a> . </p><br><h3 id="2-nauchitsya-obuchat-neyronnye-seti-po-principam-obucheniya-s-podkrepleniem">  2. Aprenda a entrenar redes neuronales de acuerdo con los principios de aprendizaje con refuerzo </h3><br><p>  Esto es lo que el Aprendizaje de refuerzo moderno est√° haciendo principalmente.  ¬øY cu√°les son los √©xitos?  En realidad no </p><br><h1 id="naivnyy-podhod">  Enfoque ingenuo </h1><br><p>  Parece que es muy simple entrenar una red neuronal con refuerzo: realizamos acciones aleatorias, y si obtenemos una recompensa, consideramos las acciones tomadas como "referencia".  Los colocamos en la salida de la red neuronal como etiquetas est√°ndar y entrenamos la red neuronal mediante el m√©todo de propagaci√≥n inversa del error, para que produzca exactamente esa salida.  Bueno, el entrenamiento de red neuronal m√°s com√∫n.  Y si las acciones condujeron al fracaso, entonces ignore este caso o suprima estas acciones (establecemos algunas otras como salida, por ejemplo, cualquier otra acci√≥n aleatoria).  En general, esta idea repite el sistema de dopamina. </p><br><p>  Pero si intentas entrenar cualquier red neuronal de esta manera, no importa cu√°n compleja sea la arquitectura, recursiva, convolucional u ordinaria, entonces ... ¬°No funcionar√°! </p><br><p>  Por qu√©  Desconocido </p><br><p>  Se cree que la se√±al √∫til es tan peque√±a que se pierde en el contexto del ruido.  Por lo tanto, la red no aprende el m√©todo est√°ndar de propagaci√≥n inversa del error.  Una recompensa ocurre muy raramente, tal vez una vez en cientos o incluso miles de pasos.  E incluso LSTM recuerda un m√°ximo de 100-500 puntos en la historia, y luego solo en tareas muy simples.  Pero en los m√°s complejos, si hay 10-20 puntos en la historia, entonces ya es bueno. </p><br><p>  Pero la ra√≠z del problema est√° precisamente en recompensas muy raras (al menos en tareas de valor pr√°ctico).  Por el momento, no sabemos c√≥mo entrenar redes neuronales que recuerden casos aislados.  Lo que el cerebro enfrenta con brillantez.  Puedes recordar algo que sucedi√≥ solo una vez en la vida.  Y, por cierto, la mayor parte de la capacitaci√≥n y el trabajo del intelecto se basa en esos casos. </p><br><p>  Esto es algo as√≠ como un terrible desequilibrio de clases en el campo del reconocimiento de im√°genes.  Simplemente no hay formas de lidiar con esto.  Lo mejor que han podido encontrar hasta ahora es simplemente enviar a la entrada de la red, junto con nuevas situaciones, situaciones exitosas del pasado almacenadas en un b√∫fer especial artificial.  Es decir, ense√±ar constantemente no solo casos nuevos, sino tambi√©n casos antiguos exitosos.  Naturalmente, dicho b√∫fer no se puede aumentar infinitamente, y no est√° claro qu√© almacenar exactamente en √©l.  Todav√≠a estoy tratando de arreglar temporalmente las rutas dentro de la red neuronal, que estuvieron activas durante un caso exitoso, para que el entrenamiento posterior no las sobrescriba.  Una analog√≠a bastante cercana a lo que est√° sucediendo en el cerebro, en mi opini√≥n, aunque tampoco han logrado mucho √©xito en esta direcci√≥n.  Como las nuevas tareas entrenadas en su c√°lculo utilizan los resultados de las neuronas que salen de las rutas congeladas, como resultado, la se√±al solo interfiere con las nuevas congeladas, y las viejas tareas dejan de funcionar.  Hay otro enfoque curioso: entrenar la red con nuevos ejemplos / tareas solo en la direcci√≥n ortogonal a las tareas anteriores ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://arxiv.org/abs/1810.01256</a> ).  Esto no sobrescribe la experiencia previa, pero limita dr√°sticamente la capacidad de la red. </p><br><p>  En Meta-Learning se est√° desarrollando una clase separada de algoritmos dise√±ados para lidiar con este desastre (y al mismo tiempo dar esperanza para lograr una IA fuerte).  Estos son intentos de ense√±arle a una red neuronal varias tareas a la vez.  No en el sentido de que reconoce diferentes im√°genes en una tarea, es decir, diferentes tareas en diferentes dominios (cada uno con su propia distribuci√≥n y panorama de soluciones).  Diga, reconozca fotos y ande en bicicleta al mismo tiempo.  Hasta ahora, el √©xito tampoco es muy bueno, ya que generalmente todo se reduce a preparar una red neuronal por adelantado con pesas universales generales, y luego r√°pidamente, en solo unos pocos pasos de descenso de gradiente, para adaptarlos a una tarea espec√≠fica.  Ejemplos de algoritmos de meta-aprendizaje son MAML y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Reptile</a> . </p><br><p>  En general, solo este problema (la incapacidad de aprender de ejemplos √∫nicos exitosos) pone fin al entrenamiento moderno con refuerzo.  Todo el poder de las redes neuronales ante este triste hecho es hasta ahora impotente. </p><br><p>  Este hecho, que la forma m√°s simple y obvia no funciona, oblig√≥ a los investigadores a volver al cl√°sico Aprendizaje por refuerzo basado en tablas.  Lo cual, como ciencia, apareci√≥ en la antigua antig√ºedad, cuando las redes neuronales ni siquiera estaban en el proyecto.  Pero ahora, en lugar de calcular manualmente los valores en tablas y f√≥rmulas, ¬°usemos un aproximador tan poderoso como redes neuronales como las funciones objetivas!  Esta es la esencia del aprendizaje de refuerzo moderno.  Y su principal diferencia con el entrenamiento habitual de las redes neuronales. </p><br><h1 id="q-learning-i-dqn">  Q-learning y DQN </h1><br><p>  El aprendizaje por refuerzo (incluso antes de las redes neuronales) naci√≥ como una idea bastante simple y original: hagamos acciones aleatorias, y luego para cada celda en la tabla y cada direcci√≥n de movimiento, calculamos de acuerdo con una f√≥rmula especial (llamada la ecuaci√≥n de Bellman, esta palabra ser√° para conocer en casi todos los trabajos con entrenamiento de refuerzo) qu√© tan buena es esta celda y la direcci√≥n elegida.  Cuanto mayor sea este n√∫mero, m√°s probable es que este camino conduzca a la victoria. </p><br><p><img src="https://habrastorage.org/webt/nx/zm/-7/nxzm-7q1_oc-igaim3j0mrr7vki.png"></p><br><p>  No importa en qu√© celda aparezcas, ¬°mu√©vete por el verde en crecimiento!  (hacia el n√∫mero m√°ximo a los lados de la celda actual). </p><br><p>  Este n√∫mero se llama Q (de la palabra calidad - calidad de elecci√≥n, obviamente), y el m√©todo es Q-learning.  Reemplazando la f√≥rmula para calcular este n√∫mero con una red neuronal, o m√°s bien ense√±ando la red neuronal usando esta f√≥rmula (m√°s un par de trucos m√°s relacionados puramente con las matem√°ticas de las redes neuronales de entrenamiento), Deepmind obtuvo el m√©todo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DQN</a> .  Este es quien en 2015 gan√≥ el mont√≥n de juegos de Atari y marc√≥ el comienzo de una revoluci√≥n en Deep Reinforcement Learning. </p><br><p>  Desafortunadamente, este m√©todo en su arquitectura funciona solo con acciones discretas discretas.  En la DQN, el estado actual (la situaci√≥n actual) se alimenta a la entrada de la red neuronal, y en la salida la red neuronal predice el n√∫mero Q. Y dado que la salida de la red enumera todas las acciones posibles a la vez (cada una con su propia Q predicha), resulta que la red neuronal en DQN implementa la funci√≥n cl√°sica Q (s, a) de Q-learning.  Q  state  action (  Q(s,a)    s  a).     argmax          Q   ,     . </p><br><p>        Q,      .        ,    Q- (..    Q   ,   ).    .      ,        (Exploration),       ,     ,        .         ,        . </p><br><p>   ,    ?    5     Atari,  continuous    ? ,    -1..1      0.1,          ,     Atari.         . ,        .       10    .  -      ,    10       .     .   DQN     ,      17     .  ,    ,  . </p><br><p>             DQN, ,   ,   continuous  (      ): DDQN, DuDQN, BDQN, CDQN, NAF, Rainbow. ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Direct Future Prediction (DFP)</a> ,    DQN     .    Q   , DFP          ,    .          .                     ,     . ,   ,       ,     . </p><br><p>    ,          Reinforcement Learning. </p><br><p><img src="https://habrastorage.org/webt/f3/lc/3t/f3lc3tno4mpvwren4rfocva9iv8.png"></p><br><h1 id="policy-gradient"> Policy Gradient </h1><br><p>       state,       (  ,        ).   ,  actions,  .   ,   R   .        (   ),   (  ).        .     . </p><br><p> ,    R   ,   ,        .       !       .   ""       labels (       ),      .     ,   ,      R. </p><br><p>   Policy Gradient.      ‚Äî    ,     R,        .    ‚Äî     ,       ,      .     ,    . </p><br><h1 id="actor-critic-ddpg"> Actor-critic, DDPG </h1><br><p>   ,       ‚Äî      ,       .  ,  Q-   ,    DQN.      state,    action(s).       state,     action,   ,      Q     : Q(s,a). </p><br><p> ,   Q(s,a),    (  critic, ),       ,      (  , actor),       R.       ,    .     actor-critic.       Policy Gradient,        ,    .   . </p><br><p>      DDPG.       actions,     continuous . DDPG   continuous  DQN    . </p><br><p><img src="https://habrastorage.org/webt/9b/th/fk/9bthfkh7cfpymc6_f6xrt7sica0.png"></p><br><h1 id="advantage-actor-critic-a3ca2c"> Advantage Actor Critic (A3C/A2C) </h1><br><p>             critic  Q(s,a) ‚Äî   ,   actor,     DDPG.         ,   . </p><br><p>     ,     .   ,           ,    <strong></strong> ,    . ,    ,    ,      (     ,   ). </p><br><p>          Q(s,a),    Advantage: A(s,a) = Q(s,a) ‚Äî V(s).  A(s,a)     Q(s,a)  ,    ‚Äî      ,    V(s).  A(s,a) &gt; 0,      ,    .  A(s,a) &lt; 0,      ,     , ..   . </p><br><p>    V(s)     state   ,     (    s,  a).         ‚Äî     state,   V(s).       ,      state,   V(s). </p><br><p>  ,    Q(s,a)     r,     ,         A = r ‚Äî V(s). </p><br><p>   ,    V(s) (          ),    ‚Äî actor  critic,    !     state,        head:    actions,    V(s).     c , ..       state. ,      . </p><br><p><img src="https://habrastorage.org/webt/eo/ph/5y/eoph5ypzawg11tachwn-nt_7nyg.png"></p><br><p>        V(s)      .     V(s),          action (     ),      .    Dueling Q-Network (DuDQN),  Q(s,a)      Q(s,a) = V(s) + A(a),    . </p><br><p> Asynchronous Advantage Actor Critic (A3C)   ,   ,     actor.        batch  .  ,     actor.     ,     ,   .   ,   A2C ‚Äî   A3C,         actor       ( ). A2C    ,    ,     . </p><br><h1 id="trpo-ppo-sac"> TRPO, PPO, SAC </h1><br><p> ,    . </p><br><p>        ,     .   Reinforcement Learning     ,      ,   ,          ‚Äî      ,  .   . </p><br><p>   ‚Äî TRPO  PPO,   state-of-the-art,   Actor-Critic.  PPO         RL.  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI Five</a>    Dota 2. </p><br><p>   ,       TRPO  PPO ‚Äî        ,     .   ,   A3C/A2C   ,    .  ,   policy     ,     . -  gradient clipping        ,     .   ,         (       ,      ),      ,   ,    -  . </p><br><p> Recientemente, el algoritmo Soft-Actor-Critic (SAC) ha ido ganando popularidad.  No es muy diferente de PPO, solo se ha agregado un objetivo al aprender a aumentar la entrop√≠a en la pol√≠tica.  Hacer que el comportamiento del agente sea m√°s aleatorio.  No, no asi.  Que el agente pudo actuar en situaciones m√°s aleatorias.  Esto aumenta autom√°ticamente la confiabilidad de la pol√≠tica, una vez que el agente est√° listo para cualquier situaci√≥n aleatoria.  Adem√°s, el SAC requiere un poco menos de ejemplos de entrenamiento que PPO, y es menos sensible a la configuraci√≥n de hiperpar√°metros, lo que tambi√©n es una ventaja.  Sin embargo, incluso con SAC, para entrenar a un humanoide a correr con 17 grados de libertad, comenzando desde una posici√≥n de pie, necesita aproximadamente 20 millones de fotogramas y aproximadamente un d√≠a de c√°lculo en una GPU.  Las condiciones iniciales m√°s dif√≠ciles, por ejemplo, para ense√±ar a un humanoide a levantarse de una pose arbitraria, pueden no ense√±arse en absoluto. </p><br><p>  Total, la recomendaci√≥n general en el aprendizaje de refuerzo moderno: use SAC, PPO, DDPG, DQN (en ese orden, descendente). </p><br><h1 id="model-based">  Basado en modelos </h1><br><p>  Hay otro enfoque interesante, indirectamente relacionado con el aprendizaje por refuerzo.  Esto es para construir un modelo del entorno y usarlo para predecir lo que suceder√° si tomamos alguna medida. </p><br><p>  Su desventaja es que no dice de ninguna manera qu√© acciones deben tomarse.  Solo sobre su resultado.  Pero una red neuronal de este tipo es f√°cil de entrenar, solo entrena con cualquier estad√≠stica.  Resulta algo as√≠ como un simulador mundial basado en una red neuronal. </p><br><p>  Despu√©s de eso, generamos una gran cantidad de acciones aleatorias, y cada una se maneja a trav√©s de este simulador (a trav√©s de una red neuronal).  Y miramos cu√°l traer√° la m√°xima recompensa.  Hay una peque√±a optimizaci√≥n: generar no solo acciones aleatorias, sino tambi√©n desviarse de acuerdo con la ley normal de la trayectoria actual.  Y, de hecho, si levantamos la mano, entonces con una alta probabilidad necesitamos continuar levant√°ndola.  Por lo tanto, antes que nada, debe verificar las desviaciones m√≠nimas de la trayectoria actual. </p><br><p>  El truco aqu√≠ es que incluso un simulador f√≠sico primitivo como MuJoCo o pyBullet produce alrededor de 200 FPS.  Y si entrena una red neuronal para predecir hacia adelante al menos unos pocos pasos, entonces, para entornos simples, puede obtener f√°cilmente lotes de 2000-5000 predicciones a la vez.  Dependiendo de la potencia de la GPU, puede obtener un pron√≥stico de decenas de miles de acciones aleatorias por segundo debido a la paralelizaci√≥n en la GPU y la velocidad de c√°lculo en la red neuronal.  La red neuronal aqu√≠ simplemente act√∫a como un simulador muy r√°pido de la realidad. </p><br><p>  Adem√°s, dado que la red neuronal puede predecir el mundo real (este es un enfoque basado en modelos, en el sentido general), el entrenamiento puede llevarse a cabo completamente en la imaginaci√≥n, por as√≠ decirlo.  Este concepto en Reinforcement Learning se llama Dream Worlds o World Models.  Esto funciona bien, una buena descripci√≥n est√° aqu√≠: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://worldmodels.github.io</a> .  Adem√°s, tiene una contraparte natural: los sue√±os ordinarios.  Y desplazamiento m√∫ltiple de eventos recientes o planeados en la cabeza. </p><br><h1 id="imitation-learning">  Imitaci√≥n de aprendizaje </h1><br><p>  Debido a la impotencia de que los algoritmos de aprendizaje por refuerzo no funcionan en grandes dimensiones y tareas complejas, las personas se propusieron al menos repetir las acciones de los expertos en forma de personas.  Aqu√≠, se lograron buenos resultados (inalcanzables mediante el aprendizaje de refuerzo convencional).  Entonces, OpenAI result√≥ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pasar el juego La venganza de Montezuma</a> .  El truco result√≥ ser simple: colocar al agente inmediatamente al final del juego (al final de la trayectoria mostrada por la persona).  All√≠, con la ayuda de PPO, gracias a la proximidad de la recompensa final, el agente aprende r√°pidamente a caminar a lo largo de la trayectoria.  Despu√©s de eso lo ponemos un poco atr√°s, donde r√°pidamente aprende a llegar al lugar que ya ha estudiado.  Y as√≠, cambiando gradualmente el punto de "reaparici√≥n" a lo largo de la trayectoria hasta el comienzo del juego, el agente aprende a pasar / simular la trayectoria experta a lo largo del juego. </p><br><p>  Otro resultado impresionante es la repetici√≥n de movimientos para personas filmadas en Motion Capture: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepMimic</a> .  La receta es similar al m√©todo OpenAI: cada episodio no comienza desde el principio de la ruta, sino desde un punto aleatorio a lo largo de la ruta.  Entonces PPO estudia con √©xito los alrededores de este punto. </p><br><p>  Debo decir que el sensacional algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Go-Explore</a> de Uber, que super√≥ la venganza de Montezuma con puntos r√©cord, no es un algoritmo de refuerzo de aprendizaje.  Esta es una b√∫squeda aleatoria regular, pero comienza con una celda celular visitada aleatoriamente (una celda gruesa en la que caen varios estados).  Y solo cuando la b√∫squeda hasta el final del juego se encuentra mediante una b√∫squeda tan aleatoria, la red neuronal se entrena utilizando el aprendizaje de imitaci√≥n.  De manera similar a OpenAI, es decir  comenzando al final de la trayectoria. </p><br><h1 id="curiosity-lyubopytstvo">  Curiosidad (Curiosidad) </h1><br><p>  Un concepto muy importante en el aprendizaje por refuerzo es la curiosidad.  En la naturaleza, es un motor para la investigaci√≥n ambiental. </p><br><p>  El problema es que, como medida de curiosidad, no puede usar un simple error de predicci√≥n de red, lo que suceder√° despu√©s.  De lo contrario, dicha red se colgar√° frente al primer √°rbol con follaje ondulante.  O frente a un televisor con cambio de canal aleatorio.  Dado que el resultado debido a la complejidad ser√° imposible de predecir y el error siempre ser√° grande.  Sin embargo, esta es precisamente la raz√≥n por la que a nosotros (las personas) nos encanta mirar el follaje, el agua y el fuego.  Y c√≥mo trabajan otras personas =).  Pero tenemos mecanismos de protecci√≥n para no colgar para siempre. </p><br><p>  Uno de esos mecanismos fue inventado como el Modelo Inverso en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Exploraci√≥n de Curiosidad por</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><br></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Predicci√≥n auto supervisada</a> .  En resumen, un agente (red neuronal), adem√°s de predecir qu√© acciones se realizan mejor en una situaci√≥n dada, tambi√©n trata de predecir qu√© suceder√° con el mundo despu√©s de las acciones tomadas.  Y usa esta predicci√≥n del mundo para el siguiente paso, para que √©l y el paso actual puedan predecir sus acciones tomadas antes (s√≠, es dif√≠cil, no puedes resolverlo sin una pinta). </p><br><p>  Esto lleva a un efecto curioso: el agente se vuelve curioso solo sobre lo que puede influir con sus acciones.  No puede influir en las ramas que se balancean de un √°rbol, por lo que no le interesan.  Pero √©l puede caminar por el distrito, por lo que siente curiosidad por caminar y explorar el mundo. </p><br><p>  Sin embargo, si el agente tiene un control remoto de TV que cambia canales aleatorios, ¬°entonces puede afectarlo!  Y tendr√° curiosidad por hacer clic en los canales hasta el infinito (ya que no puede predecir cu√°l ser√° el pr√≥ximo canal, porque es aleatorio).  Google intent√≥ evadir este problema en el trabajo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Curiosidad Epis√≥dica a trav√©s de Accesibilidad</a> . </p><br><p>  Pero quiz√°s el mejor resultado de vanguardia se deba a la curiosidad, OpenAI actualmente posee la idea de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Destilaci√≥n de red aleatoria (RND)</a> .  Su esencia es que toma una segunda red, completamente aleatoriamente inicializada, y el estado actual se alimenta a ella.  Y nuestra red neuronal de trabajo principal est√° tratando de adivinar la salida de esta red neuronal.  La segunda red no est√° entrenada, permanece fija todo el tiempo tal como se inicializ√≥. </p><br><p>  Cual es el punto?  El punto es que si nuestra red de trabajo ya ha visitado y estudiado alg√∫n estado, entonces podr√° predecir con mayor o menor √©xito la salida de esa segunda red.  Y si este es un estado nuevo, donde nunca hemos estado, entonces nuestra red neuronal no podr√° predecir la salida de esa red RND.  Este error al predecir la salida de esa red inicializada aleatoriamente se utiliza como un indicador de curiosidad (da grandes recompensas si no podemos predecir su salida en esta situaci√≥n). </p><br><p>  Por qu√© esto funciona no est√° del todo claro.  Pero escriben que esto elimina el problema cuando el objetivo de predicci√≥n es estoc√°stico y cuando no hay suficientes datos para hacer una predicci√≥n de lo que suceder√° a continuaci√≥n (lo que da un gran error de predicci√≥n en los algoritmos de curiosidad ordinarios).  De una forma u otra, pero RND realmente mostr√≥ excelentes resultados de investigaci√≥n basados ‚Äã‚Äãen la curiosidad en los juegos.  Y hace frente al problema de la televisi√≥n aleatoria. </p><br><p>  Con RND, la curiosidad en OpenAI por primera vez honestamente (y no a trav√©s de una b√∫squeda aleatoria preliminar, como en Uber) pas√≥ el primer nivel de la venganza de Montezuma.  No siempre y de manera poco confiable, pero de vez en cuando resulta. </p><br><p><img src="https://habrastorage.org/webt/iw/jm/kb/iwjmkbze4r8efybc5-01pbqzjf8.png"></p><br><h1 id="chto-v-itoge">  Cual es el resultado? </h1><br><p>  Como puede ver, en solo unos a√±os, el aprendizaje por refuerzo ha recorrido un largo camino.  No solo unas pocas soluciones exitosas, como en las redes convolucionales, donde las conexiones resudales y omitidas permitieron entrenar redes de cientos de capas de profundidad, en lugar de una docena de capas con la funci√≥n de activaci√≥n Relu sola, que super√≥ el problema de la desaparici√≥n de los gradientes en sigmoide y tanh.  En el aprendizaje con refuerzo, ha habido progreso en los conceptos y en la comprensi√≥n de las razones por las cuales esta o aquella versi√≥n ingenua de la implementaci√≥n no funcion√≥.  La palabra clave "no funcion√≥". </p><br><p>  Pero desde el punto de vista t√©cnico, todo a√∫n se basa en las predicciones de los mismos valores Q, V o A.  No hay dependencias de tiempo a diferentes escalas, como en el cerebro (el aprendizaje jer√°rquico por refuerzo no cuenta, la jerarqu√≠a es demasiado primitiva en comparaci√≥n con la asociatividad en el cerebro vivo).  No hay intentos de llegar a una arquitectura de red dise√±ada espec√≠ficamente para el aprendizaje de refuerzo, como sucedi√≥ con LSTM y otras redes recurrentes para secuencias de tiempo.  El aprendizaje por refuerzo pisa fuerte en el acto, se regocija en peque√±os √©xitos o se mueve en una direcci√≥n completamente equivocada. </p><br><p>  Me gustar√≠a creer que una vez en el aprendizaje por refuerzo habr√° un avance en la arquitectura de las redes neuronales, similar a lo que sucedi√≥ en las redes convolucionales.  Y veremos un aprendizaje de refuerzo realmente funcional.  Aprendiendo en ejemplos aislados, trabajando con memoria asociativa y trabajando en diferentes escalas de tiempo. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/437020/">https://habr.com/ru/post/437020/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../437006/index.html">Rese√±a de la impresora 3D Wanhao Duplicator 10</a></li>
<li><a href="../437008/index.html">PNL. Lo basico. T√©cnicas Autodesarrollo. Parte 1</a></li>
<li><a href="../437010/index.html">Ecos del pasado: la experiencia de Young en la base del nuevo m√©todo de espectroscop√≠a de rayos X</a></li>
<li><a href="../437014/index.html">La tarea de N cuerpos o c√≥mo volar una galaxia sin salir de la cocina</a></li>
<li><a href="../437018/index.html">Algunas trampas de tipeo est√°tico en Python</a></li>
<li><a href="../437022/index.html">Noise Security Bit 0x22 (Ataques de inyecci√≥n de falla, 35C3 y Wallet.fail)</a></li>
<li><a href="../437026/index.html">Google en Francia mult√≥ 50 millones de euros por GDPR por mal uso de datos personales</a></li>
<li><a href="../437030/index.html">Automatizaci√≥n de la infraestructura de una oficina de lujo: c√≥mo se ve</a></li>
<li><a href="../437032/index.html">Instrucciones de instalaci√≥n de NGINX ModSecurity</a></li>
<li><a href="../437034/index.html">Silbatos universales: revisi√≥n de dongle USB Snom A230 y A210</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>