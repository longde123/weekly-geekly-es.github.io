<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏾‍🤝‍🧑🏾 📙 🕺🏿 Python é um assistente para encontrar voos baratos para quem gosta de viajar 👩🏾‍🤝‍👨🏻 😻 ⚽️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A autora do artigo, cuja tradução estamos publicando hoje, diz que seu objetivo é falar sobre o desenvolvimento de um raspador de web em Python usando...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python é um assistente para encontrar voos baratos para quem gosta de viajar</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/451872/">  A autora do artigo, cuja tradução estamos publicando hoje, diz que seu objetivo é falar sobre o desenvolvimento de um raspador de web em Python usando Selenium, que busca preços de passagem aérea.  Ao procurar por ingressos, são usadas datas flexíveis (+ - 3 dias em relação às datas especificadas).  O raspador salva os resultados da pesquisa em um arquivo do Excel e envia para a pessoa que o lançou um email com informações gerais sobre o que ele conseguiu encontrar.  O objetivo deste projeto é ajudar os viajantes a encontrar as melhores ofertas. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/xl/jo/rr/xljorr2xue-q63wegfrfyt5uxu4.jpeg"></a> <br><br>  Se você, ao lidar com o material, sentir que está perdido - dê uma olhada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste</a> artigo. <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">O que estamos procurando?</font> </h2><br>  Você é livre para usar o sistema descrito aqui da maneira que desejar.  Por exemplo, usei-o para procurar passeios e ingressos de fim de semana para minha cidade natal.  Se você é sério em encontrar tickets lucrativos, pode executar o script no servidor (um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">servidor</a> simples, por 130 rublos por mês, é bastante adequado para isso) e executá-lo uma ou duas vezes por dia.  Os resultados da pesquisa serão enviados por e-mail para você.  Além disso, recomendo que você configure tudo para que o script salve o arquivo do Excel com os resultados da pesquisa na pasta Dropbox, que permite exibir esses arquivos de qualquer lugar e a qualquer momento. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d39/b7b/f3b/d39b7bf3b4f8c11fa9617ae308f01247.png"></div><br>  <i><font color="#999999">Ainda não encontrei tarifas com erros, mas acredito que isso seja possível</font></i> <br><br>  Ao pesquisar, como já foi dito, uma "data flexível" é usada, o script encontra ofertas dentro de três dias a partir das datas especificadas.  Embora, ao iniciar o script, ele procure ofertas em apenas uma direção, é fácil refiná-lo para que ele possa coletar dados em várias direções dos voos.  Com sua ajuda, você pode até procurar tarifas erradas, tais achados podem ser muito interessantes. <br><br><h2>  <font color="#3AC1EF">Por que preciso de outro raspador da web?</font> </h2><br>  Quando comecei a raspagem na web, para ser sincero, não era particularmente interessante.  Eu queria fazer mais projetos no campo da modelagem preditiva, análise financeira e, possivelmente, no campo de análise da coloração emocional dos textos.  Mas acabou sendo muito interessante - descobrir como criar um programa que coleta dados de sites.  Ao me aprofundar neste tópico, percebi que a raspagem da Web é o "mecanismo" da Internet. <br><br>  Você pode decidir que esta é uma declaração muito ousada.  Mas pense em como o Google começou com um raspador da Web criado por Larry Page usando Java e Python.  O Googlebots está pesquisando e explorando a Internet, tentando fornecer a seus usuários as melhores respostas possíveis para suas perguntas.  A raspagem da Web possui um número infinito de aplicativos e, mesmo se você, no campo da Ciência de Dados, estiver interessado em outra coisa, para obter dados para análise, você precisará de algumas habilidades de raspagem. <br><br>  Alguns dos truques usados ​​aqui foram encontrados em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">livro</a> maravilhoso sobre raspagem na web, que adquiri recentemente.  Nele você pode encontrar muitos exemplos e idéias simples sobre a aplicação prática dos estudados.  Além disso, há um capítulo muito interessante sobre o desvio de teste do reCaptcha.  Para mim, isso era novidade, pois eu não sabia que havia ferramentas especiais e serviços completos para resolver esses problemas. <br><br><h2>  <font color="#3AC1EF">Você gosta de viajar ?!</font> </h2><br>  À pergunta simples e bastante inofensiva colocada no cabeçalho desta seção, muitas vezes é possível ouvir uma resposta positiva, fornecida com algumas histórias de viagem da pessoa a quem ela foi solicitada.  Muitos de nós concordam que viajar é uma ótima maneira de mergulhar em novos ambientes culturais e expandir nossos horizontes.  No entanto, se você perguntar a alguém se ele gosta de procurar passagens aéreas, tenho certeza de que a resposta estará longe de ser tão positiva.  De fato, aqui o Python vem em socorro. <br><br>  A primeira tarefa que precisamos resolver no caminho de criar um sistema de busca de informações sobre passagens aéreas é selecionar uma plataforma adequada com a qual obteremos informações.  A solução para esse problema não foi fácil para mim, mas, no final, escolhi o serviço Kayak.  Tentei os serviços de Momondo, Skyscanner, Expedia e um pouco mais, mas os mecanismos de proteção contra robôs nesses recursos eram impenetráveis.  Depois de várias tentativas, durante as quais, ao tentar convencer os sistemas de que eu era humano, tive que lidar com semáforos, passagens para pedestres e bicicletas, decidi que o caiaque me convinha melhor, mesmo aqui também, se carregar muitas páginas em pouco tempo, as verificações também começam.  Consegui fazer o bot enviar solicitações para o site em intervalos de 4 a 6 horas, e tudo funcionou bem.  As dificuldades também surgem periodicamente ao trabalhar com o Kayak, mas se você começar a ser incomodado por verificações, precisará lidar com elas manualmente, iniciar o bot ou esperar algumas horas, e as verificações devem parar.  Se necessário, você pode adaptar o código para outra plataforma e, se fizer isso, poderá denunciá-lo nos comentários. <br><br>  Se você está começando o scraping na Web e não sabe por que alguns sites estão tendo problemas, antes de iniciar seu primeiro projeto nessa área, faça um favor a si mesmo e pesquise no Google por palavras "Etiqueta de raspagem na web".  Seus experimentos podem terminar mais cedo do que você imagina se estiver envolvido de maneira irracional na raspagem da Web. <br><br><h2>  <font color="#3AC1EF">Introdução</font> </h2><br>  Aqui está uma visão geral do que acontecerá no código do nosso raspador da Web: <br><br><ul><li>  Importe as bibliotecas necessárias. </li><li>  Abra a guia Google Chrome. </li><li>  Chamando a função que lança o bot, passando a cidade e a data, que serão usadas na procura de ingressos. </li><li>  Esta função recebe os primeiros resultados da pesquisa, classificados pelos critérios dos mais atraentes (melhores), e pressiona o botão para carregar resultados adicionais. </li><li>  Outra função coleta dados da página inteira e retorna um quadro de dados. </li><li>  As duas etapas anteriores são executadas usando tipos de classificação pelo preço do bilhete (barato) e pela velocidade do vôo (mais rápida). </li><li>  Um email é enviado ao usuário do script contendo um breve resumo dos preços dos ingressos (os mais baratos e o preço médio), e o quadro de dados com informações classificadas pelos três indicadores mencionados acima é salvo como um arquivo do Excel. </li><li>  Todas as ações acima são executadas em um ciclo após um período especificado. </li></ul><br>  Note-se que todo projeto Selenium começa com um driver da web.  Uso o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Chromedriver</a> , trabalho com o Google Chrome, mas existem outras opções.  Também populares são o PhantomJS e o Firefox.  Depois de carregar o driver, você precisa colocá-lo na pasta apropriada, isso completa a preparação para o seu uso.  As primeiras linhas do nosso script abrem uma nova guia Chrome. <br><br>  Lembre-se de que, na minha história, não estou tentando abrir novos horizontes para encontrar ofertas lucrativas em passagens aéreas.  Existem técnicas muito mais avançadas para encontrar essas ofertas.  Eu só quero oferecer aos leitores deste material uma maneira simples, mas prática, de resolver esse problema. <br><br>  Aqui está o código que falamos acima. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep, strftime <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> randint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> webdriver <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium.webdriver.common.keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> smtplib <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> email.mime.multipart <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MIMEMultipart <span class="hljs-comment"><span class="hljs-comment">#      chromedriver! chromedriver_path = 'C:/{YOUR PATH HERE}/chromedriver_win32/chromedriver.exe' driver = webdriver.Chrome(executable_path=chromedriver_path) #     Chrome sleep(2)</span></span></code> </pre> <br>  No início do código, você pode ver os comandos de importação de pacotes usados ​​em todo o nosso projeto.  Portanto, o <code>randint</code> é usado para que o bot "adormeça" por um número aleatório de segundos antes de iniciar uma nova operação de pesquisa.  Normalmente, nenhum bot pode ficar sem ele.  Se você executar o código acima, uma janela do Chrome será aberta, a qual o bot usará para trabalhar com sites. <br><br>  Vamos fazer um pequeno experimento e abrir o site kayak.com em uma janela separada.  Escolha a cidade a partir da qual vamos voar, a cidade para a qual queremos chegar e as datas dos voos.  Ao escolher as datas, verificaremos se o intervalo é de + -3 dias.  Eu escrevi o código levando em consideração o que o site produz em resposta a esses pedidos.  Se, por exemplo, você precisar procurar tickets apenas para determinadas datas, é altamente provável que você precise modificar o código bot.  Falando sobre o código, faço explicações apropriadas, mas se você sentir que está confuso, me avise. <br><br>  Agora clique no botão iniciar da pesquisa e veja o link na barra de endereço.  Ele deve se parecer com o link que eu uso no exemplo abaixo, onde a variável <code>kayak</code> que armazena a URL é declarada e o método <code>get</code> do driver da web é usado.  Depois de clicar no botão de pesquisa, os resultados devem aparecer na página. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae6/ac7/1f7/ae6ac71f71c92c6ff76d5dd6a8fcfa25.png"></div><br>  Quando usei o comando <code>get</code> mais de duas a três vezes em alguns minutos, fui solicitado a passar em um teste usando o reCaptcha.  Você pode passar por essa verificação manualmente e continuar as experiências até que o sistema decida organizar uma nova verificação.  Quando testei o script, tive a sensação de que a primeira sessão de pesquisa sempre fica sem problemas; portanto, se você quiser experimentar o código, basta checá-lo manualmente periodicamente e deixar que o código seja executado usando longos intervalos entre as sessões de pesquisa.  Sim, e se você pensar bem, é improvável que uma pessoa precise de informações sobre os preços dos ingressos recebidos em intervalos de 10 minutos entre as operações de pesquisa. <br><br><h2>  <font color="#3AC1EF">Trabalhando com uma Página Usando XPath</font> </h2><br>  Então, abrimos a janela e carregamos o site.  Para obter preços e outras informações, precisamos usar a tecnologia XPath ou seletores CSS.  Eu decidi me debruçar sobre o XPath e não senti a necessidade de usar seletores CSS, mas é bem possível trabalhar assim.  Mover uma página usando o XPath pode ser uma tarefa assustadora e, mesmo que você use os métodos que descrevi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste</a> artigo, que usavam a cópia dos identificadores correspondentes do código da página, percebi que essa não é a melhor maneira de acessar elementos necessários.  A propósito, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste</a> livro, você pode encontrar uma excelente descrição dos conceitos básicos sobre como trabalhar com páginas usando os seletores XPath e CSS.  Aqui está a aparência do método correspondente do driver da web. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fa6/5c0/5c0/fa65c05c0385c658a4eee0c08a6274ff.png"></div><br>  Então, continuamos a trabalhar no bot.  Aproveite o programa para selecionar os bilhetes mais baratos.  Na imagem a seguir, o código do seletor XPath é destacado em vermelho.  Para visualizar o código, você precisa clicar com o botão direito do mouse no elemento da página em que está interessado e selecionar o comando Inspecionar no menu exibido.  Este comando pode ser chamado para diferentes elementos da página, cujo código será exibido e destacado na janela de visualização de código. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c4/eee/9dc/2c4eee9dc28a8ef0ff540e1c01d3eda5.png"></div><br>  <i><font color="#999999">Exibir código da página</font></i> <br><br>  Para encontrar confirmação do meu raciocínio sobre as desvantagens de copiar seletores do código, preste atenção aos seguintes recursos. <br><br>  Aqui está o que você obtém ao copiar código: <br><br><pre> <code class="python hljs">//*[@id=<span class="hljs-string"><span class="hljs-string">"wtKI-price_aTab"</span></span>]/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/div/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/span/span</code> </pre> <br>  Para copiar algo semelhante, é necessário clicar com o botão direito do mouse na parte do código que lhe interessa e selecionar Copiar&gt; Copiar XPath no menu exibido. <br><br>  Aqui está o que eu usei para definir o botão Mais Barato: <br><br><pre> <code class="python hljs">cheap_results = <span class="hljs-string"><span class="hljs-string">'//a[@data-code = "price"]'</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9c9/4e3/8a4/9c94e38a436fa588ade8a2f92d97aa2d.png"></div><br>  <i><font color="#999999">Copiar&gt; Copiar Comando XPath</font></i> <br><br>  É bastante óbvio que a segunda opção parece muito mais simples.  Ao usá-lo, ele procura o elemento a, que possui o atributo de <code>data-code</code> igual ao <code>price</code> .  Utilizando a primeira opção, é pesquisado um elemento <code>id</code> que é <code>wtKI-price_aTab</code> , e o caminho XPath para o elemento se parece com <code>/div[1]/div/div/div[1]/div/span/span</code> .  Uma solicitação XPath semelhante a uma página fará o truque, mas apenas uma vez.  Posso dizer agora que o <code>id</code> será alterado na próxima vez que a página for carregada.  A <code>wtKI</code> caracteres <code>wtKI</code> muda dinamicamente toda vez que a página é carregada; como resultado, o código no qual ela é usada será inútil após a próxima página ser recarregada.  Portanto, dedique algum tempo para descobrir o XPath.  Este conhecimento irá atendê-lo bem. <br><br>  No entanto, deve-se observar que a cópia de seletores XPath pode ser útil ao trabalhar com sites bastante simples e, se isso lhe convém, não há nada de errado nisso. <br><br>  Agora, vamos pensar no que fazer se precisar obter todos os resultados da pesquisa em várias linhas, dentro da lista.  Muito simples  Cada resultado está dentro de um objeto com a classe <code>resultWrapper</code> .  O download de todos os resultados pode ser feito em um loop semelhante ao mostrado abaixo. <br><br>  Deve-se notar que, se você entende o que foi dito acima, deve entender facilmente a maior parte do código que analisaremos.  No decorrer do trabalho desse código, nos voltamos para o que precisamos (de fato, este é o elemento no qual o resultado é agrupado) usando algum mecanismo para indicar o caminho (XPath).  Isso é feito para obter o texto do elemento e colocá-lo em um objeto a partir do qual os dados podem ser lidos (primeiro use <code>flight_containers</code> , em seguida, <code>flights_list</code> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b2/3fc/b7f/0b23fcb7f32738fd6012541c40f68b97.png"></div><br>  As três primeiras linhas são exibidas e podemos ver claramente tudo o que precisamos.  No entanto, temos maneiras mais interessantes de obter informações.  Precisamos coletar dados de cada elemento separadamente. <br><br><h2>  <font color="#3AC1EF">Trabalhar!</font> </h2><br>  É mais fácil escrever uma função para carregar resultados adicionais, então vamos começar com ela.  Gostaria de maximizar o número de vôos sobre os quais o programa recebe informações e, ao mesmo tempo, não causar suspeitas no serviço que levam à verificação, por isso clico no botão Carregar mais resultados uma vez toda vez que a página é exibida.  Neste código, você deve prestar atenção ao bloco <code>try</code> , que eu adicionei devido ao fato de que às vezes o botão não carrega normalmente.  Se você também encontrar isso, comente as chamadas para essa função no código da função <code>start_kayak</code> , que discutiremos abaixo. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,      def load_more():   try:       more_results = '//a[@class = "moreButton"]'       driver.find_element_by_xpath(more_results).click()       #            ,          print('sleeping.....')       sleep(randint(45,60))   except:       pass</span></span></code> </pre> <br>  Agora, depois de uma longa análise dessa função (às vezes eu posso me deixar levar), estamos prontos para declarar uma função que lidará com a raspagem de página. <br><br>  Eu já coletei a maior parte do necessário na próxima função chamada <code>page_scrape</code> .  Às vezes, os dados retornados sobre os estágios do caminho acabam sendo combinados; para a separação deles, uso um método simples.  Por exemplo, na primeira vez que utilizo as variáveis <code>section_a_list</code> e <code>section_b_list</code> .  Nossa função retorna o quadro de dados <code>flights_df</code> , isso nos permite separar os resultados obtidos usando diferentes métodos de classificação de dados e depois combiná-los. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">page_scrape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""This function takes care of the scraping part"""</span></span>     xp_sections = <span class="hljs-string"><span class="hljs-string">'//*[@class="section duration"]'</span></span>   sections = driver.find_elements_by_xpath(xp_sections)   sections_list = [value.text <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sections]   section_a_list = sections_list[::<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-comment"><span class="hljs-comment">#          section_b_list = sections_list[1::2]     #     reCaptcha,    - .   #  ,  -   ,     ,       #   if        -   #    ,           #    SystemExit           if section_a_list == []:       raise SystemExit     #     A     B     a_duration = []   a_section_names = []   for n in section_a_list:       #         a_section_names.append(''.join(n.split()[2:5]))       a_duration.append(''.join(n.split()[0:2]))   b_duration = []   b_section_names = []   for n in section_b_list:       #         b_section_names.append(''.join(n.split()[2:5]))       b_duration.append(''.join(n.split()[0:2]))   xp_dates = '//div[@class="section date"]'   dates = driver.find_elements_by_xpath(xp_dates)   dates_list = [value.text for value in dates]   a_date_list = dates_list[::2]   b_date_list = dates_list[1::2]   #      a_day = [value.split()[0] for value in a_date_list]   a_weekday = [value.split()[1] for value in a_date_list]   b_day = [value.split()[0] for value in b_date_list]   b_weekday = [value.split()[1] for value in b_date_list]     #     xp_prices = '//a[@class="booking-link"]/span[@class="price option-text"]'   prices = driver.find_elements_by_xpath(xp_prices)   prices_list = [price.text.replace('$','') for price in prices if price.text != '']   prices_list = list(map(int, prices_list))   # stops -   ,         ,   -     xp_stops = '//div[@class="section stops"]/div[1]'   stops = driver.find_elements_by_xpath(xp_stops)   stops_list = [stop.text[0].replace('n','0') for stop in stops]   a_stop_list = stops_list[::2]   b_stop_list = stops_list[1::2]   xp_stops_cities = '//div[@class="section stops"]/div[2]'   stops_cities = driver.find_elements_by_xpath(xp_stops_cities)   stops_cities_list = [stop.text for stop in stops_cities]   a_stop_name_list = stops_cities_list[::2]   b_stop_name_list = stops_cities_list[1::2]     #   -,          xp_schedule = '//div[@class="section times"]'   schedules = driver.find_elements_by_xpath(xp_schedule)   hours_list = []   carrier_list = []   for schedule in schedules:       hours_list.append(schedule.text.split('\n')[0])       carrier_list.append(schedule.text.split('\n')[1])   #          a  b   a_hours = hours_list[::2]   a_carrier = carrier_list[1::2]   b_hours = hours_list[::2]   b_carrier = carrier_list[1::2]     cols = (['Out Day', 'Out Time', 'Out Weekday', 'Out Airline', 'Out Cities', 'Out Duration', 'Out Stops', 'Out Stop Cities',           'Return Day', 'Return Time', 'Return Weekday', 'Return Airline', 'Return Cities', 'Return Duration', 'Return Stops', 'Return Stop Cities',           'Price'])   flights_df = pd.DataFrame({'Out Day': a_day,                              'Out Weekday': a_weekday,                              'Out Duration': a_duration,                              'Out Cities': a_section_names,                              'Return Day': b_day,                              'Return Weekday': b_weekday,                              'Return Duration': b_duration,                              'Return Cities': b_section_names,                              'Out Stops': a_stop_list,                              'Out Stop Cities': a_stop_name_list,                              'Return Stops': b_stop_list,                              'Return Stop Cities': b_stop_name_list,                              'Out Time': a_hours,                              'Out Airline': a_carrier,                              'Return Time': b_hours,                              'Return Airline': b_carrier,                                                     'Price': prices_list})[cols]     flights_df['timestamp'] = strftime("%Y%m%d-%H%M") #      return flights_df</span></span></code> </pre> <br>  Tentei nomear as variáveis ​​para que o código ficasse claro.  Lembre-se de que as variáveis ​​que começam com <code>a</code> referem-se ao primeiro passo do caminho <code>b</code> ao segundo.  Vá para a próxima função. <br><br><h2>  <font color="#3AC1EF">Mecanismos auxiliares</font> </h2><br>  Agora, temos uma função que permite carregar resultados de pesquisa adicionais e uma função para processar esses resultados.  Este artigo pode ser concluído, pois essas duas funções fornecem tudo o necessário para raspar páginas que podem ser abertas independentemente.  Mas ainda não consideramos alguns dos mecanismos auxiliares discutidos acima.  Por exemplo, este é um código para enviar e-mails e outras coisas.  Tudo isso pode ser encontrado na função <code>start_kayak</code> , que agora consideramos. <br><br>  Para usar esta função, você precisa de informações sobre cidades e datas.  Usando essas informações, ela forma um link na variável <code>kayak</code> , que é usada para ir para uma página que conterá os resultados da pesquisa classificados pela melhor correspondência.  Após a primeira sessão de raspagem, trabalharemos com os preços na tabela na parte superior da página.  Ou seja, encontramos o preço mínimo do ingresso e o preço médio.  Tudo isso, juntamente com a previsão emitida pelo site, serão enviados por e-mail.  Na página, a tabela correspondente deve estar no canto superior esquerdo.  Trabalhar com esta tabela, a propósito, pode causar um erro ao pesquisar usando datas exatas, pois nesse caso a tabela não é exibida na página. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">start_kayak</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(city_from, city_to, date_start, date_end)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""City codes - it's the IATA codes!   Date format -  YYYY-MM-DD"""</span></span>     kayak = (<span class="hljs-string"><span class="hljs-string">'https://www.kayak.com/flights/'</span></span> + city_from + <span class="hljs-string"><span class="hljs-string">'-'</span></span> + city_to +            <span class="hljs-string"><span class="hljs-string">'/'</span></span> + date_start + <span class="hljs-string"><span class="hljs-string">'-flexible/'</span></span> + date_end + <span class="hljs-string"><span class="hljs-string">'-flexible?sort=bestflight_a'</span></span>)   driver.get(kayak)   sleep(randint(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))     <span class="hljs-comment"><span class="hljs-comment">#    ,           try   try:       xp_popup_close = '//button[contains(@id,"dialog-close") and contains(@class,"Button-No-Standard-Style close ")]'       driver.find_elements_by_xpath(xp_popup_close)[5].click()   except Exception as e:       pass   sleep(randint(60,95))   print('loading more.....')  #     load_more()     print('starting first scrape.....')   df_flights_best = page_scrape()   df_flights_best['sort'] = 'best'   sleep(randint(60,80))     #      ,        matrix = driver.find_elements_by_xpath('//*[contains(@id,"FlexMatrixCell")]')   matrix_prices = [price.text.replace('$','') for price in matrix]   matrix_prices = list(map(int, matrix_prices))   matrix_min = min(matrix_prices)   matrix_avg = sum(matrix_prices)/len(matrix_prices)     print('switching to cheapest results.....')   cheap_results = '//a[@data-code = "price"]'   driver.find_element_by_xpath(cheap_results).click()   sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting second scrape.....')   df_flights_cheap = page_scrape()   df_flights_cheap['sort'] = 'cheap'   sleep(randint(60,80))     print('switching to quickest results.....')   quick_results = '//a[@data-code = "duration"]'   driver.find_element_by_xpath(quick_results).click()    sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting third scrape.....')   df_flights_fast = page_scrape()   df_flights_fast['sort'] = 'fast'   sleep(randint(60,80))     #     Excel-,         final_df = df_flights_cheap.append(df_flights_best).append(df_flights_fast)   final_df.to_excel('search_backups//{}_flights_{}-{}_from_{}_to_{}.xlsx'.format(strftime("%Y%m%d-%H%M"),                                                                                  city_from, city_to,                                                                                  date_start, date_end), index=False)   print('saved df.....')     #    ,  ,  ,      xp_loading = '//div[contains(@id,"advice")]'   loading = driver.find_element_by_xpath(xp_loading).text   xp_prediction = '//span[@class="info-text"]'   prediction = driver.find_element_by_xpath(xp_prediction).text   print(loading+'\n'+prediction)     #    loading   , , ,        #    -    "Not Sure"   weird = '¯\\_(ツ)_/¯'   if loading == weird:       loading = 'Not sure'     username = 'YOUREMAIL@hotmail.com'   password = 'YOUR PASSWORD'   server = smtplib.SMTP('smtp.outlook.com', 587)   server.ehlo()   server.starttls()   server.login(username, password)   msg = ('Subject: Flight Scraper\n\n\ Cheapest Flight: {}\nAverage Price: {}\n\nRecommendation: {}\n\nEnd of message'.format(matrix_min, matrix_avg, (loading+'\n'+prediction)))   message = MIMEMultipart()   message['From'] = 'YOUREMAIL@hotmail.com'   message['to'] = 'YOUROTHEREMAIL@domain.com'   server.sendmail('YOUREMAIL@hotmail.com', 'YOUROTHEREMAIL@domain.com', msg)   print('sent email.....')</span></span></code> </pre> <br>  Testei esse script usando uma conta do Outlook (hotmail.com).  Não verifiquei o funcionamento correto da conta do Gmail, esse sistema de e-mail é muito popular, mas existem muitas opções possíveis.  Se você usa uma conta do Hotmail, para que tudo funcione, basta inserir seus dados no código. <br><br>  Se você deseja entender exatamente o que é executado em seções separadas do código dessa função, você pode copiá-las e experimentar com elas.  As experiências com código são a única maneira de entendê-lo. <br><br><h2>  <font color="#3AC1EF">Sistema pronto</font> </h2><br>  Agora que tudo o que falamos está pronto, podemos criar um loop simples no qual nossas funções são chamadas.  O script solicita ao usuário dados sobre cidades e datas.  Ao testar com uma reinicialização constante do script, é improvável que você insira esses dados manualmente a cada vez, para que as linhas correspondentes, durante a duração do teste, possam ser comentadas descomentando aquelas abaixo delas nas quais os dados necessários para o script são codificados. <br><br><pre> <code class="python hljs">city_from = input(<span class="hljs-string"><span class="hljs-string">'From which city? '</span></span>) city_to = input(<span class="hljs-string"><span class="hljs-string">'Where to? '</span></span>) date_start = input(<span class="hljs-string"><span class="hljs-string">'Search around which departure date? Please use YYYY-MM-DD format only '</span></span>) date_end = input(<span class="hljs-string"><span class="hljs-string">'Return when? Please use YYYY-MM-DD format only '</span></span>) <span class="hljs-comment"><span class="hljs-comment"># city_from = 'LIS' # city_to = 'SIN' # date_start = '2019-08-21' # date_end = '2019-09-07' for n in range(0,5):   start_kayak(city_from, city_to, date_start, date_end)   print('iteration {} was complete @ {}'.format(n, strftime("%Y%m%d-%H%M")))     #  4    sleep(60*60*4)   print('sleep finished.....')</span></span></code> </pre> <br>  Aqui está a execução de teste do script. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44c/305/023/44c305023996a98a3dec745493dce7a7.png"></div><br>  <i><font color="#999999">Script de execução de teste</font></i> <br><br><h2>  <font color="#3AC1EF">Sumário</font> </h2><br>  Se você chegar a esse ponto - parabéns!  Agora você tem um raspador de web em funcionamento, embora eu já veja muitas maneiras de melhorá-lo.  Por exemplo, ele pode ser integrado ao Twilio para que, em vez de emails, envie mensagens de texto.  Você pode usar uma VPN ou outra coisa para receber simultaneamente resultados de vários servidores.  Há também um problema recorrente ao verificar se o usuário do site é uma pessoa, mas esse problema também pode ser resolvido.  De qualquer forma, agora você tem uma base que pode ser expandida, se desejar.  Por exemplo, para garantir que o arquivo do Excel seja enviado ao usuário como um anexo de um email. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt451872/">https://habr.com/ru/post/pt451872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt451860/index.html">Matemáticos descobriram a maneira perfeita de multiplicar números</a></li>
<li><a href="../pt451862/index.html">Musical Lightning, de Joe Diprim: um engenheiro autodidata faz bobinas de Tesla para entretenimento e ganhos</a></li>
<li><a href="../pt451864/index.html">Vulnerabilidade crítica ao nível de EternalBlue no RCE detectada no sistema operacional Windows</a></li>
<li><a href="../pt451866/index.html">Escolha os nós mais próximos na rede</a></li>
<li><a href="../pt451870/index.html">Recursos modernos em C ++ que todos os programadores precisam conhecer</a></li>
<li><a href="../pt451874/index.html">Principais tendências de SEO no Google</a></li>
<li><a href="../pt451876/index.html">Data center de Frankfurt: data center de Telehouse</a></li>
<li><a href="../pt451878/index.html">Transmissão ao vivo de vídeo estéreo para óculos VR (Oculus Go)</a></li>
<li><a href="../pt451880/index.html">DevPRO'19: vista do estande da Wrike</a></li>
<li><a href="../pt451884/index.html">Sete anos trabalhando como desenvolvedor: que lições aprendi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>