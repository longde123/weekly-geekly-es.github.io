<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ†Ô∏è ‚è±Ô∏è üë®üèø‚Äçüè´ Trends in der Bildverarbeitung. H√∂hepunkte ICCV 2019 üëº üë¥üèª üõ¢Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neuronale Netze in der Bildverarbeitung entwickeln sich aktiv weiter, viele Aufgaben sind noch lange nicht gel√∂st. Um auf Ihrem Gebiet im Trend zu sei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Trends in der Bildverarbeitung. H√∂hepunkte ICCV 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/474902/"><img src="https://habrastorage.org/webt/in/lj/qf/inljqfmjnklszyujlyua8n_w0bo.jpeg"><br><br>  Neuronale Netze in der Bildverarbeitung entwickeln sich aktiv weiter, viele Aufgaben sind noch lange nicht gel√∂st.  Um auf Ihrem Gebiet im Trend zu sein, folgen Sie einfach den Influencern auf Twitter und lesen Sie die relevanten Artikel auf arXiv.org.  Wir hatten jedoch die Gelegenheit, an der Internationalen Konferenz f√ºr Computer Vision (ICCV) 2019 teilzunehmen. Dieses Jahr findet sie in S√ºdkorea statt.  Jetzt m√∂chten wir mit den Lesern von Habr teilen, dass wir gesehen und gelernt haben. <br><a name="habracut"></a><br>  Es waren viele von uns von Yandex: unbemannte Fahrzeugentwickler, Forscher und Mitarbeiter, die mit CV-Aufgaben im Servicebereich befasst waren, sind eingetroffen.  Jetzt wollen wir jedoch eine etwas subjektive Sichtweise unseres Teams vorstellen - das Machine Intelligence Labor (Yandex MILAB).  Andere Leute haben die Konferenz wahrscheinlich aus ihrem Blickwinkel betrachtet. <br><br><div class="spoiler">  <b class="spoiler_title">Was macht das Labor?</b> <div class="spoiler_text">  Wir machen experimentelle Projekte zur Erzeugung von Bildern und Musik f√ºr Unterhaltungszwecke.  Wir sind besonders an neuronalen Netzen interessiert, mit denen Sie den Inhalt des Benutzers √§ndern k√∂nnen (f√ºr ein Foto wird diese Aufgabe als Bildbearbeitung bezeichnet).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Beispiel f√ºr das</a> Ergebnis unserer Arbeit von der YaC-Konferenz 2019. </div></div><br>  Es gibt viele wissenschaftliche Konferenzen, von denen sich jedoch die wichtigsten, sogenannten A * -Konferenzen abheben, auf denen in der Regel Artikel √ºber die interessantesten und wichtigsten Technologien ver√∂ffentlicht werden.  Es gibt keine genaue Liste der A * -Konferenzen, hier ein Beispiel und unvollst√§ndig: NeurIPS (fr√ºher NIPS), ICML, SIGIR, WWW, WSDM, KDD, ACL, CVPR, ICCV, ECCV.  Die letzten drei sind auf das Thema Lebenslauf spezialisiert. <br><br><h2>  ICCV auf einen Blick: Poster, Tutorials, Workshops, St√§nde </h2><br>  1075 Vortr√§ge wurden auf der Konferenz angenommen, die Teilnehmerzahl betrug 7.500. 103 Personen kamen aus Russland, es gab Artikel von Mitarbeitern von Yandex, Skoltech, dem Samsung AI Center Moskau und der Samara University.  In diesem Jahr besuchten nicht viele Spitzenforscher das ICCV, aber hier zum Beispiel Alexey (Alyosha) Efros, der immer viele Menschen versammelt: <br><br><img src="https://habrastorage.org/webt/4g/ie/3w/4gie3wyqaablh0wmnxbq4ucdwbs.jpeg"><br><br><div class="spoiler">  <b class="spoiler_title">Statistiken</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/1e/lz/am/1elzamxsr2xf9k_gqrwvvvrwey0.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/vb/yr/1i/vbyr1im56rz6ib-6sj_0fjiokjc.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/l6/wc/iy/l6wciy-qakwq9wwe65hz9-pqjl8.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/jb/dr/rq/jbdrrqkeo6mw26wx3zi5taa_zog.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/2g/rb/rt/2grbrtsd1xwbzwzxfstgdck6jis.jpeg" width="500"><br></div></div><br>  Bei all diesen Konferenzen werden Artikel in Form von Postern ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehr</a> zum Format) und die besten auch in Form von Kurzberichten pr√§sentiert. <br><br><div class="spoiler">  <b class="spoiler_title">Hier ist ein Teil der Arbeit aus Russland</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/iu/sc/po/iuscpof__g3ikvdguuy94javuvo.jpeg"><br><br><img src="https://habrastorage.org/webt/ae/e0/xj/aee0xjbluiby-b_xxoednb7yfws.jpeg"><br><br><img src="https://habrastorage.org/webt/1s/qt/la/1sqtla2h9xgccuhu2dr1iyvxuek.jpeg"><br></div></div><br>  In den Tutorials k√∂nnen Sie sich in ein Fachgebiet eintauchen, es √§hnelt einer Vorlesung an einer Universit√§t.  Es wird von einer Person gelesen, normalerweise ohne √ºber bestimmte Werke zu sprechen.  Beispiel f√ºr ein cooles Tutorial ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Michael Brown, Grundlegendes zu Farben und der kamerainternen Bildverarbeitungs-Pipeline f√ºr Computer Vision</a> ): <br><br><img src="https://habrastorage.org/webt/2z/sq/e6/2zsqe6wgv1rtrl0tpvwl-eu5enw.jpeg"><br><br>  In Workshops sprechen sie dagegen √ºber Artikel.  In der Regel handelt es sich dabei um Arbeiten zu einem engen Thema, Geschichten von Laborleitern √ºber die neuesten Arbeiten der Studenten oder Artikel, die auf der Hauptkonferenz nicht akzeptiert wurden. <br><br>  Sponsoring-Unternehmen kommen mit St√§nden auf die ICCV.  In diesem Jahr kamen Google, Facebook, Amazon und viele andere internationale Unternehmen sowie eine gro√üe Anzahl von Start-ups - Koreaner und Chinesen.  Es gab besonders viele Startups, die sich auf Datenmarkierungen spezialisiert haben.  An den St√§nden finden Vorstellungen statt, man kann Waren mitnehmen und Fragen stellen.  Sponsoring-Unternehmen haben Parteien f√ºr die Jagd.  Sie schaffen es, Personalvermittler davon zu √ºberzeugen, dass Sie interessiert sind und m√∂glicherweise interviewt werden k√∂nnen.  Wenn Sie einen Artikel ver√∂ffentlicht (oder dar√ºber hinaus eine Pr√§sentation damit erstellt), die Promotion begonnen oder beendet haben, ist dies ein Plus, aber manchmal k√∂nnen Sie sich auf einen Stand einigen und den Ingenieuren des Unternehmens interessante Fragen stellen. <br><br><h2>  Trends </h2><br>  Bei der Konferenz k√∂nnen Sie einen Blick auf den gesamten Lebenslauf werfen.  Anhand der Anzahl der Poster zu einem bestimmten Thema k√∂nnen Sie bewerten, wie aktuell das Thema ist.  Einige Schlussfolgerungen bitten um die Schl√ºsselw√∂rter: <br><br><img src="https://habrastorage.org/webt/7u/td/1v/7utd1vf3hcbbhtrgl3xvldtjnjc.jpeg"><br><br><h4>  Zero-Shot, One-Shot, Wenig-Shot, Selbst- und Halb-Supervised: Neue Ans√§tze f√ºr lang untersuchte Probleme </h4><br>  Die Menschen lernen, Daten effizienter zu nutzen.  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FUNIT k√∂nnen</a> Sie beispielsweise Gesichtsausdr√ºcke von Tieren generieren, die nicht im Trainingssatz enthalten waren (Anwenden mehrerer Referenzbilder in der Anwendung).  Die Ideen von Deep Image Prior wurden entwickelt, und jetzt k√∂nnen <abbr title="Generative gegnerische Netzwerke, generative gegnerische Netzwerke.">GAN-</abbr> Netzwerke in einem Bild trainiert werden - dar√ºber werden wir sp√§ter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in den H√∂hepunkten</a> sprechen.  Sie k√∂nnen die Selbst√ºberwachung vor dem Training verwenden (um ein Problem zu l√∂sen, bei dem Sie ausgerichtete Daten synthetisieren k√∂nnen, um beispielsweise den Drehwinkel eines Bildes vorherzusagen) oder gleichzeitig aus markierten und unmarkierten Daten lernen.  In diesem Sinne kann die Krone der Sch√∂pfung als ein Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">S4L betrachtet werden: Selbst√ºberwachtes halb√ºberwachtes Lernen</a> .  Das Pre-Training in ImageNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hilft jedoch nicht immer</a> . <br><br><img src="https://habrastorage.org/webt/gj/yy/n4/gjyyn40ktbwpjaslfp0v7c-avhi.jpeg"><br><br><img src="https://habrastorage.org/webt/x5/6-/l8/x56-l8y26lyq9unxyt0soa8koay.jpeg"><br><br><h4>  3D und 360 ¬∞ </h4><br>  Aufgaben, die haupts√§chlich f√ºr Fotos (Segmentierung, Erkennung) gel√∂st werden, erfordern zus√§tzliche Recherchen f√ºr 3D-Modelle und Panorama-Videos.  Wir haben viele Artikel √ºber die Konvertierung von RGB und <abbr title="RGB-Tiefenbild. F√ºr jeden Punkt ist nicht nur seine Farbe bekannt, sondern auch seine ‚ÄûTiefe‚Äú - die Entfernung vom Standpunkt / Aufnahmepunkt.">RGB-D</abbr> in 3D gesehen.  Einige Aufgaben, wie das Bestimmen der Pose einer Person (Posensch√§tzung), werden nat√ºrlicher gel√∂st, wenn wir dreidimensionale Modelle verwenden.  Bisher besteht jedoch kein Konsens dar√ºber, wie 3D-Modelle genau dargestellt werden sollen - in Form eines Rasters, einer Punktewolke, von <abbr title="Analoga von Pixeln in 3D.">Voxeln</abbr> oder <abbr title="Vorzeichenbehaftete Distanzfelder - Vorzeichenbehaftete Distanzfelder.">SDF</abbr> .  Hier ist eine andere Option: <br><br><img src="https://habrastorage.org/webt/n7/i-/1x/n7i-1xtwmc5xxvs4cfsf4vt4srw.jpeg"><br><br>  In den Panoramen entwickeln sich aktiv Windungen auf der Kugel (siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Orientierungsbewusste semantische Segmentierung auf Ikosaederkugeln</a> ) und die Suche nach Schl√ºsselobjekten im Rahmen. <br><br><img src="https://habrastorage.org/webt/bk/4l/gw/bk4lgwc3dzrh_uliw83x21hskyy.png"><br><br><h4>  Definition der K√∂rperhaltung und Vorhersage menschlicher Bewegungen </h4><br>  Um die Pose in 2D zu bestimmen, gibt es bereits Erfolge - jetzt hat sich der Fokus auf die Arbeit mit mehreren Kameras und in 3D verlagert.  Sie k√∂nnen beispielsweise das Skelett durch die Wand bestimmen und √Ñnderungen im WLAN-Signal verfolgen, w√§hrend es durch den menschlichen K√∂rper flie√üt. <br><br>  Auf dem Gebiet der Hand-Schl√ºsselpunkterkennung wurde viel Arbeit geleistet.  Es wurden neue Datens√§tze angezeigt, einschlie√ülich derer, die auf Videos mit den Dialogen von zwei Personen basieren. Jetzt k√∂nnen Sie Handbewegungen anhand von Audio oder Text einer Unterhaltung vorhersagen.  Die gleichen Fortschritte wurden bei den Aufgaben zur Beurteilung des Blicks erzielt. <br><br><img src="https://habrastorage.org/webt/j0/-j/kv/j0-jkvftadbawqem0ccmm7qmdpa.jpeg"><br><br><img src="https://habrastorage.org/webt/u_/gj/j6/u_gjj6f2d-icebbun428-1e0ztc.jpeg"><br><br>  Sie k√∂nnen auch eine gro√üe Sammlung von Werken zur Vorhersage menschlicher Bewegungen hervorheben (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Human Motion Prediction √ºber r√§umlich-zeitliches Inpainting</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Structured Prediction Helps 3D Human Motion Modeling</a> ).  Die Aufgabe ist wichtig und wird auf der Grundlage von Gespr√§chen mit den Autoren am h√§ufigsten zur Analyse des Verhaltens von Fu√üg√§ngern beim autonomen Fahren verwendet. <br><br><h4>  Manipulieren von Personen in Fotos und Videos, virtuelle Umkleidekabinen </h4><br>  Der Haupttrend besteht darin, die Gesichtsbilder in Bezug auf die interpretierten Parameter zu √§ndern.  Ideen: <abbr title="Vertretung von Fremden im Video.">Deepfake</abbr> auf einem Bild, Ausdrucks√§nderung durch Gesichtsrendering ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PuppetGAN</a> ), Feedforward-√Ñnderung von Parametern (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alter</a> ).  Stil√ºbertragungen wurden vom Titel des Themas in die Anwendung der Arbeit verschoben.  Eine andere Geschichte - virtuelle Umkleidekabinen, die fast immer schlecht funktionieren, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier ein Beispiel f√ºr eine</a> Demo. <br><br><img src="https://habrastorage.org/webt/u-/q9/rw/u-q9rwdkgxxor2snnsrkstmykbe.jpeg"><br><br><img src="https://habrastorage.org/webt/qw/f4/ys/qwf4ys-wamesjxss4gs30v9wbpq.jpeg"><br><br><h4>  Sketch / Graph Generation </h4><br>  Die Entwicklung der Idee ‚ÄûLass das Gitter etwas basierend auf fr√ºheren Erfahrungen erzeugen‚Äú ist anders geworden: ‚ÄûZeigen wir dem Gitter, welche Option uns interessiert.‚Äú <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SC-FEGAN</a> erm√∂glicht Ihnen gef√ºhrtes Malen: Der Benutzer kann einen Teil des Gesichts im gel√∂schten Bereich des Bildes zeichnen und das wiederhergestellte Bild abh√§ngig vom Rendering erhalten. <br><br><img src="https://habrastorage.org/webt/kn/pv/0d/knpv0dzfajvu2hhcbqdgiw-sbek.gif"><br><br>  In einem der 25 Adobe-Artikel f√ºr ICCV werden zwei GANs kombiniert: Einer zeichnet eine Skizze f√ºr den Benutzer, der andere generiert aus der Skizze ein fotorealistisches Bild ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> ). <br><br><img src="https://habrastorage.org/webt/ua/ba/ap/uabaap4mv5jwm9tdc0qgsxty3ho.gif"><br><br>  Fr√ºher bei der Erstellung von Bildern wurden keine Grafiken ben√∂tigt, jetzt wurden sie zu einem Wissensbeh√§lter √ºber die Szene.  Der ICCV Best Paper Honourable Mentions Award wurde auch f√ºr den Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Specifying Object Attributes and Relations in Interactive Scene Generation verliehen</a> .  Im Allgemeinen k√∂nnen Sie sie auf verschiedene Arten verwenden: Generieren Sie Grafiken aus Bildern oder Bilder und Texte aus Grafiken. <br><br><img src="https://habrastorage.org/webt/5h/qf/zw/5hqfzwxfjjt-1bnyqopp7ozoqi4.png"><br><br><h4>  Neuidentifizierung von Personen und Maschinen, Z√§hlung der Menschenmenge (!) </h4><br>  Viele Artikel widmen sich der Verfolgung von Personen und der <abbr title="Neuidentifizierung - kann frei als &quot;Deanonymisierung&quot; √ºbersetzt werden.">erneuten Identifizierung von</abbr> Personen und Maschinen.  Was uns jedoch √ºberraschte, war eine Reihe von Artikeln √ºber das Z√§hlen von Menschen in einer Menschenmenge und allesamt aus China. <br><br><div class="spoiler">  <b class="spoiler_title">Plakate</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/fh/cn/ma/fhcnma3kitjamuo8yty7hjp1loa.jpeg"><br><br><img src="https://habrastorage.org/webt/ej/_p/66/ej_p66wxpnx52yzlv97osbkys1u.jpeg"><br><br><img src="https://habrastorage.org/webt/j7/7z/bv/j77zbvjegwrfp6emzv-ddcylgmm.jpeg"><br><br><img src="https://habrastorage.org/webt/q9/lw/kr/q9lwkrpkzozcvfa609k6t5krmnw.jpeg"><br><br><img src="https://habrastorage.org/webt/3x/rv/1i/3xrv1ibiocsa5cgbmdvecwxbkyu.jpeg"></div></div><br>  Facebook hingegen anonymisiert das Foto.  Au√üerdem macht es das auf interessante Weise: Es lehrt das neuronale Netzwerk, ein Gesicht ohne eindeutige Details zu erzeugen - √§hnlich, aber nicht so sehr, dass es von Gesichtserkennungssystemen korrekt erkannt wird. <br><br><img src="https://habrastorage.org/webt/jg/az/oe/jgazoe4ptlfckqpaxdgri2kdlwc.jpeg"><br><br><h4>  Schutz vor gegnerischen Angriffen </h4><br>  Mit der Entwicklung von Computer-Vision-Anwendungen in der realen Welt (in unbemannten Fahrzeugen, in der Gesichtserkennung) stellt sich immer h√§ufiger die Frage nach der Zuverl√§ssigkeit solcher Systeme.  Um CV vollst√§ndig nutzen zu k√∂nnen, m√ºssen Sie sicherstellen, dass das System widerstandsf√§hig gegen feindliche Angriffe ist. Daher gab es nicht weniger Artikel zum Schutz vor solchen Angriffen als zu den Angriffen selbst.  Viel Arbeit galt der Erkl√§rung von Netzwerkvorhersagen (Saliency Map) und der Messung des Vertrauens in das Ergebnis. <br><br><h4>  Kombinierte Aufgaben </h4><br>  Bei den meisten Aufgaben mit einem Ziel sind die M√∂glichkeiten zur Qualit√§tsverbesserung nahezu ausgesch√∂pft: Einer der neuen Bereiche f√ºr weiteres Qualit√§tswachstum besteht darin, neuronale Netze zu lehren, mehrere √§hnliche Probleme gleichzeitig zu l√∂sen.  Beispiele: <br>  - Vorhersage von Handlungen + Vorhersage des optischen Flusses, <br>  - Videopr√§sentation + <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachdarstellung</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VideoBERT</a> ), <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Superaufl√∂sung + HDR</a> . <br><br>  Und es gab Artikel √ºber Segmentierung, die die Haltung und die erneute Identifizierung von Tieren festlegten! <br><br><img src="https://habrastorage.org/webt/qe/gk/fi/qegkfif0smvsnit1kjrpdbkajco.jpeg"><br><br><img src="https://habrastorage.org/webt/tz/hk/fo/tzhkfogbi5sxyvzbhu5bjaoii54.jpeg"><br><br><a name="highlights"></a><h2>  H√∂hepunkte </h2><br>  Fast alle Artikel waren im Voraus bekannt, der Text war auf arXiv.org verf√ºgbar.  Daher erscheint die Pr√§sentation von Werken wie Everybody Dance Now, FUNIT, Image2StyleGAN eher seltsam - dies sind sehr n√ºtzliche, aber keineswegs neue Werke.  Hier scheint der klassische Prozess der wissenschaftlichen Publikation zu scheitern - die Wissenschaft entwickelt sich zu schnell. <br><br>  Es ist sehr schwierig, die besten Werke zu bestimmen - es gibt viele davon, die Themen sind unterschiedlich.  Mehrere Artikel haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszeichnungen und Referenzen erhalten</a> . <br><br>  Wir wollen Arbeiten hervorheben, die im Hinblick auf die Bildmanipulation interessant sind, da dies unser Thema ist.  Sie erwiesen sich f√ºr uns als recht frisch und interessant (wir geben nicht vor, objektiv zu sein). <br><br><h4>  SinGAN (Best Paper Award) und InGAN </h4>  SinGAN: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> . <br>  InGAN: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> . <br><br>  Die Entwicklung der Idee von Deep Image Prior durch Dmitry Ulyanov, Andrea Vedaldi und Victor Lempitsky.  Anstatt GAN auf einem Datensatz zu trainieren, lernen Netzwerke aus Fragmenten desselben Bildes, um Statistiken darin zu speichern.  Das geschulte Netzwerk erm√∂glicht es Ihnen, Fotos (SinGAN) zu bearbeiten und zu animieren oder neue Bilder jeder Gr√∂√üe aus den Texturen des Originalbilds zu generieren, wobei die lokale Struktur (InGAN) beibehalten wird. <br><br>  SinGAN: <br><br><img src="https://habrastorage.org/webt/oc/ba/pt/ocbaptuxkshaswnhnfhloplqmrm.png"><br><br>  InGAN: <br><br><img src="https://habrastorage.org/webt/xn/cc/i0/xncci0dgonpmajjiv0fisak0twa.gif"><br><br><h4>  Sehen, was ein GAN nicht generieren kann </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> . <br><br>  Bilderzeugende neuronale Netze empfangen h√§ufig einen zuf√§lligen Rauschvektor als Eingabe.  In einem trainierten Netzwerk bilden viele Eingabevektoren einen Raum, kleine Bewegungen, die zu kleinen √Ñnderungen im Bild f√ºhren.  Mithilfe der Optimierung k√∂nnen Sie das umgekehrte Problem l√∂sen: Finden Sie einen geeigneten Eingabevektor f√ºr ein Bild aus der realen Welt.  Der Autor zeigt, dass es fast nie m√∂glich ist, ein vollst√§ndig passendes Bild in einem neuronalen Netzwerk zu finden.  Einige Objekte im Bild werden nicht generiert (anscheinend aufgrund der gro√üen Variabilit√§t dieser Objekte). <br><br><img src="https://habrastorage.org/webt/pv/pa/f2/pvpaf2havdxksu-mhmbus-naina.png"><br><br>  Der Autor geht davon aus, dass die GAN nicht den gesamten Bildraum abdeckt, sondern nur eine Teilmenge, die mit L√∂chern wie K√§se gef√ºllt ist.  Wenn wir versuchen, Fotos aus der realen Welt zu finden, werden wir immer scheitern, weil die GAN immer noch nicht ganz reale Fotos erzeugt.  Sie k√∂nnen die Unterschiede zwischen realen und generierten Bildern nur √ºberwinden, indem Sie das Gewicht des Netzwerks √§ndern, dh es f√ºr ein bestimmtes Foto neu trainieren. <br><br><img src="https://habrastorage.org/webt/ci/vg/bp/civgbpxixfgs_76svkwewjksn3a.jpeg"><br><br>  Wenn das Netzwerk f√ºr ein bestimmtes Foto erneut trainiert wird, k√∂nnen Sie versuchen, verschiedene Manipulationen mit diesem Bild durchzuf√ºhren.  Im folgenden Beispiel wurde dem Foto ein Fenster hinzugef√ºgt, und das Netzwerk erzeugte zus√§tzlich Reflexionen am K√ºchenset.  Dies bedeutet, dass das Netzwerk nach der Umschulung f√ºr die Fotografie nicht die F√§higkeit verlor, die Verbindung zwischen den Objekten der Szene zu erkennen. <br><br><img src="https://habrastorage.org/webt/ov/gp/qd/ovgpqdyldwsdptpav699a_vl2qo.jpeg"><br><br><h4>  GANalyze: Visuelle Definitionen kognitiver Bildeigenschaften </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv</a> . <br><br>  Mit dem Ansatz aus dieser Arbeit k√∂nnen Sie visualisieren und analysieren, was das neuronale Netzwerk gelernt hat.  Die Autoren schlagen das Training GAN vor, um Bilder zu erstellen, f√ºr die das Netzwerk gegebene Vorhersagen generiert.  In dem Artikel wurden mehrere Netzwerke als Beispiele verwendet, einschlie√ülich MemNet, das die Erinnerbarkeit von Fotos vorhersagt.  Es stellte sich heraus, dass das Objekt auf dem Foto zur besseren Einpr√§gsamkeit: <br><br><ul><li>  n√§her an der Mitte sein </li><li>  eine runde oder quadratische Form und eine einfache Struktur haben, </li><li>  auf einem einheitlichen hintergrund sein, </li><li>  ausdrucksstarke Augen enthalten (zumindest f√ºr Fotos von Hunden), </li><li>  sei heller, reicher, in manchen F√§llen - r√∂ter. </li></ul><br><img src="https://habrastorage.org/webt/9y/b2/wd/9yb2wdgndc2qvmugfdk5yctxfbg.png"><br><br><h4>  Liquid Warping GAN: Ein einheitliches Framework f√ºr die Nachahmung menschlicher Bewegungen, die √úbertragung von Erscheinungsbildern und die Synthese neuartiger Ansichten </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> . <br><br>  Pipeline zum Erzeugen von Fotos von Personen aus einem Foto.  Die Autoren zeigen erfolgreiche Beispiele f√ºr die √úbertragung der Bewegung einer Person auf eine andere, die √úbertragung von Kleidung zwischen Menschen und die Generierung neuer Perspektiven einer Person - alles auf einem Foto.  Im Gegensatz zu fr√ºheren Arbeiten werden hier zum Erstellen von Bedingungen nicht Schl√ºsselpunkte in 2D (Pose) verwendet, sondern ein 3D-Netz des K√∂rpers (Pose + Form).  Die Autoren haben auch herausgefunden, wie Informationen vom Originalbild auf das generierte √ºbertragen werden k√∂nnen (Liquid Warping Block).  Die Ergebnisse sehen anst√§ndig aus, aber die Aufl√∂sung des resultierenden Bildes betr√§gt nur 256x256.  Zum Vergleich: vid2vid, das vor einem Jahr auf den Markt kam, kann mit einer Aufl√∂sung von 2048 x 1024 Pixel erstellt werden, ben√∂tigt jedoch bis zu 10 Minuten Videoaufnahme als Datensatz. <br><br><img src="https://habrastorage.org/webt/el/m1/eh/elm1ehlgjasetl9leqejn9elvbu.png"><br><br><h4>  FSGAN: Agnostisches Face Swapping und Reenactment </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektseite</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv</a> . <br><br>  Auf den ersten Blick scheint das nichts Ungew√∂hnliches zu sein: Deepfake mit mehr oder weniger normaler Qualit√§t.  Die Hauptleistung der Arbeit ist jedoch die Substitution von Gesichtern in einem Bild.  Im Gegensatz zu fr√ºheren Arbeiten war eine Ausbildung auf einer Vielzahl von Fotografien einer bestimmten Person erforderlich.  Die Pipeline erwies sich als umst√§ndlich (Nachstellung und Segmentierung, Ansichtsinterpolation, Inpainting, Blending) und mit vielen technischen Hacks, aber das Ergebnis ist es wert. <br><br><img src="https://habrastorage.org/webt/43/33/zn/4333zncbuoqflhf2srkk-05e6m0.png"><br><br><h4>  Erkennen des Unerwarteten durch Bildsynthese </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv</a> . <br><br>  Wie kann eine Drohne verstehen, dass pl√∂tzlich ein Objekt davor auftauchte, das keiner Klasse semantischer Segmentierung zuzuordnen ist?  Es gibt mehrere Methoden, aber die Autoren bieten einen neuen, intuitiven Algorithmus an, der besser als seine Vorg√§nger funktioniert.  Die semantische Segmentierung wird aus dem Eingabebild der Stra√üe vorhergesagt.  Es wird in die GAN (pix2pixHD) eingespeist, die versucht, das Originalbild nur von der semantischen Karte wiederherzustellen.  Anomalien, die nicht in eines der Segmente fallen, unterscheiden sich erheblich in der Quelle und im generierten Bild.  Dann werden drei Bilder (anf√§nglich, segmentiert und rekonstruiert) an ein anderes Netzwerk gesendet, das Anomalien vorhersagt.  Das Dataset hierf√ºr wurde aus dem bekannten Cityscapes-Dataset generiert, wobei versehentlich die Klassen f√ºr die semantische Segmentierung ge√§ndert wurden.  Interessanterweise ist in dieser Umgebung ein Hund, der mitten auf der Stra√üe steht, aber korrekt segmentiert ist (was bedeutet, dass es eine Klasse daf√ºr gibt), keine Anomalie, da das System dies erkennen konnte. <br><br><img src="https://habrastorage.org/webt/fc/1_/ug/fc1_ugp2xbh5qxttjgskyxprji4.png"><br><br><h2>  Fazit </h2><br>  Vor der Konferenz ist es wichtig zu wissen, welche wissenschaftlichen Interessen Sie haben, welche Reden ich gerne halten w√ºrde und mit wem ich sprechen m√∂chte.  Dann wird alles viel produktiver. <br><br>  ICCV vernetzt sich haupts√§chlich.  Sie verstehen, dass es Spitzeninstitutionen und Spitzenwissenschaftler gibt, Sie beginnen dies zu verstehen, Menschen kennenzulernen.  Und Sie k√∂nnen Artikel √ºber arXiv lesen - und es ist √ºbrigens sehr cool, dass Sie nicht √ºberall nach Wissen suchen k√∂nnen. <br><br>  Dar√ºber hinaus k√∂nnen Sie auf der Konferenz tief in Themen eintauchen, die nicht in Ihrer N√§he sind, und Trends erkennen.  Schreiben Sie eine Liste der Artikel, die Sie lesen m√∂chten.  Wenn Sie Student sind, ist dies eine Gelegenheit f√ºr Sie, einen potenziellen Wissenschaftler kennenzulernen, wenn Sie aus der Branche stammen, einen neuen Arbeitgeber zu finden und wenn das Unternehmen, dann zeigen Sie sich. <br><br>  Abonnieren Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@loss_function_porn</a> !  Dies ist ein pers√∂nliches Projekt: Wir sind zusammen mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">karfly</a> .  Die ganze Arbeit, die uns w√§hrend der Konferenz gefallen hat, haben wir hier gepostet: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@loss_function_live</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de474902/">https://habr.com/ru/post/de474902/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de474890/index.html">Vergleichstest von Kameras alter Handys und ein bisschen Geschichte</a></li>
<li><a href="../de474892/index.html">Programmierung f√ºr Kinder. F√ºnf der coolsten HTML- und JavaScript-Spiele</a></li>
<li><a href="../de474894/index.html">Zusammenfassung durch die Augen eines Interviewers</a></li>
<li><a href="../de474896/index.html">Wissenschaftler haben einen neuen Faktor f√ºr die wirksame Abgabe von Arzneimitteln im Tumor entdeckt</a></li>
<li><a href="../de474900/index.html">OpenTitan Open-Chip-Chip ersetzt die propriet√§ren Vertrauenswurzeln von Intel und ARM</a></li>
<li><a href="../de474906/index.html">Xamarin.Forms - Dekorative QRCode-Zuordnung mit SkiaSharp</a></li>
<li><a href="../de474910/index.html">Was mit Kindern vor der Schule zu spielen</a></li>
<li><a href="../de474912/index.html">Nachrichten und Warnungen auf Android √ºber JSON</a></li>
<li><a href="../de474916/index.html">Wenden Sie die Nix-Shell-Umgebung in Visual Studio-Code an</a></li>
<li><a href="../de474918/index.html">Verbesserung des Gelenkdesigns von elektromechanischen Bauteilen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>