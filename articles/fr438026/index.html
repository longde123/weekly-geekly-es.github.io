<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⏬ 🤽🏾 🌀 RedisPipe - Plus de plaisir ensemble 🙇🏾 👨🏼‍🌾 👨🏽‍🎨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quand je pense au fonctionnement des clients RPC naïfs, je me souviens d'une blague: 
 La cour. 
 "Accusé, pourquoi avez-vous tué une femme?" 
 - Je s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>RedisPipe - Plus de plaisir ensemble</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/joom/blog/438026/"><p>  Quand je pense au fonctionnement des clients RPC naïfs, je me souviens d'une blague: </p><br><blockquote>  La cour. <br>  "Accusé, pourquoi avez-vous tué une femme?" <br>  - Je suis dans le bus, le chef d'orchestre s'approche de la femme, lui demandant d'acheter un billet.  La femme a ouvert son sac à main, a sorti son sac à main, a fermé son sac à main, a sorti son sac à main, a fermé son sac à main, a ouvert son sac à main, a mis son sac à main là-dedans, a fermé son sac à main, a ouvert son sac à main, a sorti son sac à main, a ouvert son sac à main, a sorti son sac à main, a fermé son sac à main, a ouvert son sac à main, a mis son sac à l'intérieur. , ferma son sac, ouvrit son sac, y mit son sac. <br>  - Et quoi? <br>  - Le contrôleur lui a donné un ticket.  Une femme a ouvert son sac à main, a sorti son sac à main, a fermé son sac à main, a ouvert son sac à main, a fermé son sac à main, a ouvert son sac à main, a mis son sac à main là, a fermé son sac à main, a ouvert son sac à main, a mis son ticket là, a fermé son sac à main, a ouvert son sac à main, a sorti son sac à main, a fermé son sac à main, a ouvert son sac à main , mettez le sac dedans, fermez le sac, ouvrez le sac, mettez le sac dedans, fermez le sac. <br>  "Prenez le changement", vint la voix du contrôleur.  La femme ... a ouvert son sac à main ... <br>  "Oui, il ne suffit pas de la tuer", le procureur ne résiste pas. <br>  "Alors je l'ai fait." <br>  © S. Altov </blockquote><p><img src="https://habrastorage.org/webt/qx/ne/-b/qxne-bocftxvmk99ouizmn3iqyo.jpeg"></p><a name="habracut"></a><br><p>  À peu près la même chose se produit dans le processus de demande-réponse, si vous l'abordez frivolement: </p><br><ul><li>  le processus utilisateur écrit une requête sérialisée dans le socket, la copiant réellement dans le tampon de socket au niveau du système d'exploitation; <br>  C'est une opération assez difficile, car  il est nécessaire de faire un changement de contexte (même si cela peut être «facile»); </li><li>  lorsqu'il apparaît à l'OS que quelque chose peut être écrit sur le réseau, un paquet est formé (la demande est à nouveau copiée) et envoyé à la carte réseau; </li><li>  la carte réseau écrit le paquet sur le réseau (éventuellement après la mise en mémoire tampon); </li><li>  (en cours de route, un paquet peut être mis en mémoire tampon plusieurs fois dans les routeurs); </li><li>  enfin, le paquet arrive à l'hôte de destination et est mis en mémoire tampon sur la carte réseau; </li><li>  la carte réseau envoie une notification au système d'exploitation, et lorsque le système d'exploitation trouve l'heure, elle copie le paquet dans sa mémoire tampon et définit l'indicateur prêt sur le descripteur de fichier; </li><li>  (vous devez toujours vous rappeler d'envoyer l'ACK en réponse); </li><li>  après un certain temps, l'application serveur se rend compte que le descripteur est prêt (à l'aide d'epoll) et copie un jour la demande dans le tampon d'application; </li><li>  et enfin, l'application serveur traite la demande. </li></ul><br><p>  Comme vous le savez, la réponse est transmise exactement de la même manière que dans la direction opposée.  Ainsi, chaque demande passe du temps notable sur le système d'exploitation pour sa maintenance, et chaque réponse passe à nouveau le même temps. </p><br><p>  Cela est devenu particulièrement visible après Meltdown / Spectre, car les correctifs publiés ont entraîné une augmentation significative du coût des appels système.  Début janvier 2018, notre cluster Redis a soudainement commencé à consommer un an et demi à deux fois plus de CPU, car  Amazon a appliqué les correctifs de noyau appropriés pour couvrir ces vulnérabilités.  (Certes, Amazon a appliqué une nouvelle version du correctif un peu plus tard, et la consommation du processeur a chuté presque aux niveaux précédents. Mais le connecteur a déjà commencé à naître.) </p><br><p> Malheureusement, tous les connecteurs Go largement connus de Redis et Memcached fonctionnent exactement comme ceci: le connecteur crée un pool de connexions, et lorsqu'il doit envoyer une demande, il extrait une connexion du pool, y écrit une demande, puis attend une réponse.  (Il est particulièrement triste que le connecteur vers Memcached ait été écrit par Brad Fitzpatrick lui-même.) Et certains connecteurs ont une implémentation si infructueuse de ce pool que le processus de suppression de la connexion du pool devient un botnet en soi. </p><br><p>  Il existe deux façons de faciliter ce travail difficile de transfert de demande / réponse: </p><br><ol><li>  Utilisez un accès direct à la carte réseau: DPDK, netmap, PF_RING, etc. </li><li>  N'envoyez pas chaque demande / réponse dans un package distinct, mais combinez-les, si possible, dans des packages plus volumineux, c'est-à-dire répartissez la charge de travail avec le réseau pour plusieurs requêtes.  Ensemble plus de plaisir! </li></ol><br><p>  Bien entendu, la première option est possible.  Mais, tout d'abord, c'est pour les courageux, car vous devez écrire l'implémentation TCP / IP vous-même (par exemple, comme dans ScyllaDB).  Et deuxièmement, de cette façon, nous facilitons la situation d'un seul côté - de celui que nous écrivons nous-mêmes.  Je ne veux pas (encore) réécrire Redis, donc les serveurs consommeront la même quantité, même si le client utilise le cool DPDK. </p><br><p>  La deuxième option est beaucoup plus simple et, surtout, facilite la situation immédiatement sur le client et sur le serveur.  Par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une base de données en mémoire se</a> targue de pouvoir desservir des millions de RPS, tandis que Redis ne peut pas en desservir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">quelques centaines de milliers</a> .  Cependant, ce succès n'est pas tant la mise en œuvre de cette base en mémoire que la décision une fois acceptée que le protocole sera complètement asynchrone, et les clients devraient utiliser cette asynchronie chaque fois que possible.  Ce que de nombreux clients (en particulier ceux utilisés dans les tests de performances) mettent en œuvre avec succès en envoyant des demandes via une connexion TCP et, si possible, en les envoyant ensemble au réseau. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un article bien connu</a> montre que Redis peut également fournir un million de réponses par seconde si un pipeline est utilisé.  L'expérience personnelle dans le développement d'histoires en mémoire indique également que le pipelage réduit considérablement la consommation de CPU SYS et vous permet d'utiliser le processeur et le réseau beaucoup plus efficacement. </p><br><p>  La seule question est de savoir comment utiliser le pipelining, si dans les demandes d'application dans Redis arrivent souvent une à la fois?  Et si un serveur est manquant et que Redis Cluster est utilisé avec un grand nombre de fragments, alors même lorsqu'un paquet de demandes est rencontré, il se divise en demandes uniques pour chaque fragment. </p><br><p>  La réponse, bien sûr, est «évidente»: faire un pipeline implicite, collecter les demandes de tous les goroutines fonctionnant en parallèle sur un serveur Redis et les envoyer via une connexion. </p><br><p>  Soit dit en passant, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pose</a> implicite de tuyaux n'est pas si rare dans les connecteurs dans d'autres langages: nodejs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">node_redis</a> , C # <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RedisBoost</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">aioredis de</a> python et bien d'autres.  Beaucoup de ces connecteurs sont écrits au-dessus des boucles d'événements, et la collecte de requêtes à partir de "flux de calcul" parallèles semble donc naturelle.  Dans Go, l'utilisation des interfaces synchrones est encouragée et, apparemment, parce que peu de gens décident d'organiser leur propre «boucle». </p><br><p>  Nous voulions utiliser Redis le plus efficacement possible et avons donc décidé d'écrire un nouveau "meilleur" connecteur (tm): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RedisPipe</a> . </p><br><h2 id="kak-sdelat-neyavnyy-payplayning">  Comment faire la pose implicite de tuyaux? </h2><br><p>  Le schéma de base: </p><br><ul><li>  Les goroutines de la logique d'application n'écrivent pas les demandes directement au réseau, mais les transmettent au collecteur de goroutines; </li><li>  si possible, le collectionneur recueille un tas de demandes, les écrit sur le réseau et les transmet au lecteur de goroutine; </li><li>  Goroutine-reader lit les réponses du réseau, les compare avec les requêtes correspondantes, et informe les goroutines de la logique de la réponse arrivée. </li></ul><br><p> Quelque chose doit être notifié de la réponse.  Un programmeur astucieux de Go dira certainement: "Par le canal!" <br>  Mais ce n'est pas la seule primitive de synchronisation possible et pas la plus efficace même dans l'environnement Go.  Et puisque différentes personnes ont des besoins différents, nous rendrons le mécanisme extensible, permettant à l'utilisateur d'implémenter l'interface (appelons-le <code>Future</code> ): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Future <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> { Resolve(val <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span>{}) }</code> </pre> <br><p>  Et puis le schéma de base ressemblera à ceci: </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> future <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { req Request fut Future } <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Conn <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c net.Conn futmtx sync.Mutex wfutures []future futtimer *time.Timer rfutures <span class="hljs-keyword"><span class="hljs-keyword">chan</span></span> []future } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Send</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r Request, f Future)</span></span></span></span> { c.futmtx.Lock() <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> c.futmtx.Unlock() c.wfutures = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(c.wfutures, future{req: r, fut: f}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(c.wfutures) == <span class="hljs-number"><span class="hljs-number">1</span></span> { futtimer.Reset(<span class="hljs-number"><span class="hljs-number">100</span></span>*time.Microsecond) } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">writer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> c.futtimer.C { c.futmtx.Lock() futures, c.wfutures = c.wfutures, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> c.futmtx.Unlock() <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> b []<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, ft := <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> futures { b = AppendRequest(b, ft.req) } _, _err := ccWrite(b) c.rfutures &lt;- futures } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reader</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { rd := bufio.NewReader(cc) <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> futures []future <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> { response, _err := ParseResponse(rd) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(futures) == <span class="hljs-number"><span class="hljs-number">0</span></span> { futures = &lt;- c.rfutures } futures[<span class="hljs-number"><span class="hljs-number">0</span></span>].fut.Resolve(response) futures = futures[<span class="hljs-number"><span class="hljs-number">1</span></span>:] } }</code> </pre> <br><p>  Bien sûr, c'est un code très simplifié.  Omis: </p><br><ul><li>  établissement de connexion; </li><li>  Délais d'E / S </li><li>  gestion des erreurs de lecture / écriture; </li><li>  rétablir la connexion; </li><li>  la possibilité d'annuler la demande avant de l'envoyer au réseau; </li><li>  optimisation de l'allocation de mémoire (réutilisation de la mémoire tampon et des tableaux à terme). </li></ul><br><p>  Toute erreur d'E / S (y compris un délai d'attente) dans le code réel conduit à une résolution de toutes les erreurs futures correspondant aux demandes envoyées et en attente d'être envoyées. <br>  La couche de connexion n'est pas impliquée dans la nouvelle tentative de demande, et s'il est nécessaire (et possible) de relancer la demande, cela peut être fait à un niveau d'abstraction plus élevé (par exemple, dans la mise en œuvre de la prise en charge de Redis Cluster décrite ci-dessous). </p><br><p>  Remarque.  Au départ, le circuit semblait un peu plus compliqué.  Mais dans le processus d'expériences simplifiées à une telle option. </p><br><p>  Remarque 2. Des exigences très strictes sont imposées à la méthode Future.Resolve: elle doit être aussi rapide que possible, pratiquement non bloquante et en aucun cas panique.  Cela est dû au fait qu'il est appelé de manière synchrone dans le cycle du lecteur, et tout "frein" entraînera inévitablement une dégradation.  La mise en œuvre de Future.Resolve doit faire le minimum nécessaire d'actions linéaires: réveiller l'expectatif;  peut-être gérer l'erreur et envoyer une nouvelle tentative asynchrone (utilisée dans l'implémentation de la prise en charge du cluster). </p><br><h2 id="effekt">  Effet </h2><br><p>  Une bonne référence est la moitié de l'article! </p><br><p>  Une bonne référence est celle qui est la plus proche possible de la lutte contre l'utilisation en termes d'effets observés.  Et ce n'est pas facile à faire. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'option de référence</a> , qui, il me semble, ressemble assez à la vraie: </p><br><ul><li>  le "script" principal émule 5 clients parallèles, </li><li>  dans chaque "client", pour chaque 300-1000 rps "souhaités", la goroutine est déclenchée (3 gorutins sont déclenchés pour 1000 rps, 124 gorutins sont exécutés pour 128000 rps), </li><li>  gorutin utilise une instance distincte du limiteur de débit et envoie des requêtes en série aléatoire - de 5 à 15 requêtes. </li></ul><br><p>  Le caractère aléatoire de la série de requêtes nous permet d'obtenir une distribution aléatoire de la série dans la chronologie, qui reflète plus correctement la charge réelle. </p><br><div class="spoiler">  <b class="spoiler_title">Texte masqué</b> <div class="spoiler_text"><p>  Les mauvaises options étaient: <br>  a) utiliser un limiteur de débit pour tous les gorutins du «client» et y recourir pour chaque demande - cela entraîne une consommation excessive du processeur par le limiteur de débit lui-même, ainsi qu'une augmentation de la rotation dans le temps du goroutin, ce qui dégrade les performances de RedisPipe à des vitesses moyennes (mais inexplicablement s'améliore à haut); <br>  b) utiliser un limiteur de débit pour tous les gorutins du «client» et envoyer des requêtes en série - le limiteur de débit ne mange pas déjà tant le CPU, mais l'alternance des goroutines dans le temps ne fait qu'augmenter; <br>  c) utiliser un limiteur de débit pour chaque goroutine, mais envoyer la même série de 10 demandes - dans ce scénario, les goroutines se réveillent trop simultanément, ce qui améliore injustement les résultats de RedisPipe. </p></div></div><br><p>  Les tests ont eu lieu sur une instance AWS c5-2xlarge à quatre cœurs.  La version de Redis est 5.0. </p><br><p>  Le rapport de l'intensité de requête souhaitée, l'intensité totale résultante et consommée par le radis CPU: </p><br><table><thead><tr><th>  rps prévus </th><th>  redigo <br>  rps /% cpu </th><th>  redispipe pas d'attente <br>  rps /% cpu </th><th>  redispipe 50µs <br>  rps /% cpu </th><th>  redispipe 150µs <br>  rps /% cpu </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  5015/7% </td><td>  5015/6% </td><td>  5015/6% </td><td>  5015/6% </td></tr><tr><td>  2000 * 5 </td><td>  10022/11% </td><td>  10022/10% </td><td>  10022/10% </td><td>  10022/10% </td></tr><tr><td>  4000 * 5 </td><td>  20036/21% </td><td>  20036/18% </td><td>  20035/17% </td><td>  20035/15% </td></tr><tr><td>  8000 * 5 </td><td>  40020/45% </td><td>  40062/37% </td><td>  40060/26% </td><td>  40056/19% </td></tr><tr><td>  16000 * 5 </td><td>  79994/71% </td><td>  80102/58% </td><td>  80096/33% </td><td>  80090/23% </td></tr><tr><td>  32000 * 5 </td><td>  159590/96% </td><td>  160 180/80% </td><td>  160.167 / 39% </td><td>  160 150/29% </td></tr><tr><td>  64000 * 5 </td><td>  187774/99% </td><td>  320 313/98% </td><td>  320 283/47% </td><td>  320 258/37% </td></tr><tr><td>  92000 * 5 </td><td>  183206/99% </td><td>  480 443/97% </td><td>  480 407/52% </td><td>  480 366/42% </td></tr><tr><td>  128000 * 5 </td><td>  179744/99% </td><td>  640,484 / 97% </td><td>  640,488 / 55% </td><td>  640 428/46% </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/g-/d7/7k/g-d77kznsmwmgejiyplpipwv7t4.png" alt="Taux de demande"><img src="https://habrastorage.org/webt/cs/xf/lz/csxflztfjbnmzc0kuhpnqddeokq.png" alt="Redis cpu"></p><br><p>  Vous pouvez remarquer qu'avec un connecteur fonctionnant selon le schéma classique (demande / réponse + pool de connexions), Redis mange assez rapidement le cœur du processeur, après quoi il devient impossible d'obtenir plus de 190 krps. </p><br><p>  RedisPipe vous permet d'extraire toute la puissance requise de Redis.  Et plus nous nous arrêtons pour collecter des requêtes parallèles, moins Redis consomme de CPU.  Un avantage tangible est déjà obtenu à 4 krps du client (20 krps au total) si une pause de 150 microsecondes est utilisée. </p><br><p>  Même si la pause n'est pas explicitement utilisée lorsque Redis repose sur le CPU, le retard apparaît de lui-même.  De plus, les demandes commencent à être mises en mémoire tampon par le système d'exploitation.  Cela permet à RedisPipe d'augmenter le nombre de requêtes exécutées avec succès lorsque le connecteur classique baisse déjà ses pattes. </p><br><p>  C'est le résultat principal, pour lequel il fallait créer un nouveau connecteur. </p><br><p>  Que se passe-t-il alors avec la consommation du processeur sur le client et avec les demandes retardées? </p><br><table><thead><tr><th>  rps prévus </th><th>  redigo <br>  % cpu / ms </th><th>  redispipe nowait <br>  % cpu / ms </th><th>  redispipe 50ms <br>  % cpu / ms </th><th>  redispipe 150ms <br>  % cpu / ms </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  13 / 0,03 </td><td>  20 / 0,04 </td><td>  46 / 0,16 </td><td>  44 / 0,26 </td></tr><tr><td>  2000 * 5 </td><td>  25 / 0,03 </td><td>  33 / 0,04 </td><td>  77 / 0,16 </td><td>  71 / 0,26 </td></tr><tr><td>  4000 * 5 </td><td>  48 / 0,03 </td><td>  60 / 0,04 </td><td>  124 / 0,16 </td><td>  107 / 0,26 </td></tr><tr><td>  8000 * 5 </td><td>  94 / 0,03 </td><td>  119 / 0,04 </td><td>  178 / 0,15 </td><td>  141 / 0,26 </td></tr><tr><td>  16000 * 5 </td><td>  184 / 0,04 </td><td>  206 / 0,04 </td><td>  228 / 0,15 </td><td>  177 / 0,25 </td></tr><tr><td>  32000 * 5 </td><td>  341 / 0,08 </td><td>  322 / 0,05 </td><td>  280 / 0,15 </td><td>  226 / 0,26 </td></tr><tr><td>  64000 * 5 </td><td>  316 / 1,88 </td><td>  469 / 0,08 </td><td>  345 / 0,16 </td><td>  307 / 0,26 </td></tr><tr><td>  92000 * 5 </td><td>  313 / 2,88 </td><td>  511 / 0,16 </td><td>  398 / 0,17 </td><td>  366 / 0,27 </td></tr><tr><td>  128000 * 5 </td><td>  312 / 3,54 </td><td>  509 / 0,37 </td><td>  441 / 0,19 </td><td>  418 / 0,29 </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/fp/ws/ra/fpwsra-pgtexl6ua7crpc4vt5fa.png" alt="CPU client"><img src="https://habrastorage.org/webt/kw/kl/lk/kwkllkjyf-vfpv77miu4jnnlrqm.png" alt="latence"></p><br><p>  Vous pouvez remarquer que sur les petits rps, RedisPipe lui-même consomme plus de CPU que le "concurrent", surtout si une petite pause est utilisée.  Cela est principalement dû à l'implémentation de minuteries dans Go et à l'implémentation des appels système utilisés dans le système d'exploitation (sous Linux, c'est futexsleep), car en mode «sans pause», la différence est nettement moins importante. </p><br><p>  Avec l'augmentation de la vitesse par seconde, la surcharge des minuteries est compensée par une baisse de la surcharge pour la mise en réseau et après 16 krps par client, l'utilisation de RedisPipe avec une pause de 150 microsecondes commence à apporter des avantages tangibles. </p><br><p>  Bien sûr, après que Redis se soit reposé sur le CPU, la latence des requêtes utilisant le connecteur classique commence à augmenter.  Je ne suis pas sûr, cependant, qu'en pratique, vous atteignez souvent 180 krps à partir d'une instance Redis.  Mais si c'est le cas, gardez à l'esprit que vous pourriez avoir des problèmes. </p><br><p>  Tant que Redis ne s'exécute pas dans le CPU, la latence de la demande souffre bien sûr de l'utilisation d'une pause.  Ce compromis est intentionnellement posé dans le connecteur.  Cependant, ce compromis n'est perceptible que si Redis et le client sont sur le même hôte physique.  Selon la topologie du réseau, un aller-retour vers un hôte voisin peut aller de cent microsecondes à une milliseconde.  En conséquence, la différence de retard déjà au lieu de neuf fois (0,26 / 0,03) devient trois fois (0,36 / 0,13) ou n'est mesurée que par quelques dizaines de pour cent (1,26 / 1,03). </p><br><p>  Dans notre charge de travail, lorsque Redis est utilisé comme cache, l'attente totale des réponses de la base de données avec un échec de cache est supérieure à l'attente totale des réponses de Redis, car on pense que l'augmentation du délai n'est pas significative. </p><br><p>  Le principal résultat positif est une tolérance à la croissance de la charge: si soudainement la charge sur le service augmente N fois, Redis ne consommera pas le même CPU N fois plus.  Pour résister au quadruplement de la charge de 160 krps à 640 krps, Redis n'a dépensé que 1,6 fois plus de CPU, augmentant la consommation de 29 à 46%.  Cela nous permet de ne pas avoir peur que Redis se plie soudainement.  L'évolutivité de l'application ne sera pas non plus déterminée par le fonctionnement du connecteur et le coût de la connectivité réseau (lire: Coûts du processeur SYS). </p><br><p>  Remarque.  Le code de référence fonctionne avec de petites valeurs.  Pour effacer ma conscience, j'ai répété le test avec des valeurs de taille 768 octets.  La consommation de CPU par le radis a augmenté de façon marquée (jusqu'à 66% sur une pause de 150 µs), et le plafond pour un connecteur classique tombe à 170 krps.  Mais toutes les proportions observées sont restées les mêmes, et donc les conclusions. </p><br><h2 id="klaster">  Cluster </h2><br><p>  Pour la mise à l'échelle, nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Redis Cluster</a> .  Cela nous permet d'utiliser Redis non seulement comme cache, mais aussi comme stockage volatile et en même temps de ne pas perdre de données lors de l'expansion / compression d'un cluster. </p><br><p>  Redis Cluster utilise le principe du client intelligent, c'est-à-dire  le client doit surveiller l'état du cluster lui-même et également répondre aux erreurs auxiliaires renvoyées par le "radis" lorsque le "bouquet" se déplace d'instance en instance. </p><br><p>  Par conséquent, le client doit conserver les connexions à toutes les instances Redis du cluster et établir une connexion à celle requise pour chaque demande.  Et c'est à cet endroit que le client utilisait auparavant (on ne va pas pointer du doigt) a été bien foiré.  L'auteur, qui a surestimé la commercialisation de Go (CSP, canaux, goroutines), a implémenté la synchronisation du travail avec l'état du cluster en envoyant des rappels au goroutine central.  C'est devenu un sérieux botnek pour nous.  En tant que correctif temporaire, nous avons dû lancer quatre clients sur un cluster, chacun, à son tour, augmentant jusqu'à des centaines de connexions dans le pool pour chaque instance de Redis. </p><br><p>  Par conséquent, le nouveau connecteur a été chargé d'empêcher cette erreur.  Toutes les interactions avec l'état du cluster sur le chemin d'exécution de la requête se font aussi sans verrouillage que possible: </p><br><ul><li>  l'état de cluster est rendu pratiquement immuable, et pas de nombreuses mutations aromatisées par des atomes </li><li>  l'accès à l'état se produit à l'aide de atomic.StorePointer / atomic.LoadPointer, et peut donc être obtenu sans blocage. </li></ul><br><p>  Ainsi, même pendant la mise à jour de l'état du cluster, les requêtes peuvent utiliser l'état précédent sans craindre d'attendre un verrou. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// storeConfig atomically stores config func (c *Cluster) storeConfig(cfg *clusterConfig) { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) atomic.StorePointer(p, unsafe.Pointer(cfg)) } // getConfig loads config atomically func (c *Cluster) getConfig() *clusterConfig { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) return (*clusterConfig)(atomic.LoadPointer(p)) } func (cfg *clusterConfig) slot2shardno(slot uint16) uint16 { return uint16(atomic.LoadUint32(&amp;cfg.slots[slot])) } func (cfg *clusterConfig) slotSetShard(slot, shard uint16) { atomic.StoreUint32(&amp;cfg.slots[slot], shard) }</span></span></code> </pre> <br><p>  Le statut du cluster est mis à jour toutes les 5 secondes.  Mais en cas de suspicion d'instabilité du cluster, la mise à jour est forcée: </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">control</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { t := time.NewTicker(c.opts.CheckInterval) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> t.Stop() <span class="hljs-comment"><span class="hljs-comment">// main control loop for { select { case &lt;-c.ctx.Done(): // cluster closed, exit control loop c.report(LogContextClosed{Error: c.ctx.Err()}) return case cmd := &lt;-c.commands: // execute some asynchronous "cluster-wide" actions c.execCommand(cmd) continue case &lt;-forceReload: // forced mapping reload c.reloadMapping() case &lt;-tC: // regular mapping reload c.reloadMapping() } } } func (c *Cluster) ForceReloading() { select { case c.forceReload &lt;- struct{}{}: default: } }</span></span></code> </pre> <br><p>  Si la réponse MOVED ou ASK reçue du radis contient une adresse inconnue, son ajout asynchrone à la configuration est lancé.  (Je suis désolé, je n'ai pas compris comment simplifier le code, car <a href="">voici le lien</a> .) Ce n'était pas sans utiliser de verrous, mais ils sont pris pour une courte période de temps.  La principale attente est réalisée en enregistrant le rappel dans le tableau - le même avenir, vue latérale. </p><br><p>  Des connexions sont établies avec toutes les instances Redis, ainsi qu'avec les maîtres et les esclaves.  Selon la politique préférée et le type de demande (lecture ou écriture), la demande peut être envoyée à la fois au maître et à l'esclave.  Cela prend en compte la "vivacité" de l'instance, qui se compose à la fois des informations obtenues lors de la mise à jour de l'état du cluster et de l'état actuel de la connexion. </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">connForSlot</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(slot </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint16</span></span></span></span><span class="hljs-function"><span class="hljs-params">, policy ReplicaPolicyEnum)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*redisconn.Connection, *errorx.Error)</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conn *redisconn.Connection cfg := c.getConfig() shard := cfg.slot2shard(slot) nodes := cfg.nodes <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> addr <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> policy { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> MasterOnly: addr = shard.addr[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment">// master is always first node := nodes[addr] if conn = node.getConn(c.opts.ConnHostPolicy, needConnected); conn == nil { conn = node.getConn(c.opts.ConnHostPolicy, mayBeConnected) } case MasterAndSlaves: n := uint32(len(shard.addr)) off := c.opts.RoundRobinSeed.Current() for _, needState := range []int{needConnected, mayBeConnected} { mask := atomic.LoadUint32(&amp;shard.good) // load health information for ; mask != 0; off++ { bit := 1 &lt;&lt; (off % n) if mask&amp;bit == 0 { // replica isn't healthy, or already viewed continue } mask &amp;^= bit addr = shard.addr[k] if conn = nodes[addr].getConn(c.opts.ConnHostPolicy, needState); conn != nil { return conn, nil } } } } if conn == nil { c.ForceReloading() return nil, c.err(ErrNoAliveConnection) } return conn, nil } func (n *node) getConn(policy ConnHostPolicyEnum, liveness int) *redisconn.Connection { for _, conn := range n.conns { switch liveness { case needConnected: if c.ConnectedNow() { return conn } case mayBeConnected: if c.MayBeConnected() { return conn } } } return nil }</span></span></code> </pre> <br><p>  Il y a un <code>RoundRobinSeed.Current()</code> cryptique <code>RoundRobinSeed.Current()</code> .  Ceci, d'une part, est une source de hasard, et d'autre part, un hasard qui ne change pas fréquemment.  Si vous sélectionnez une nouvelle connexion pour chaque demande, cela dégrade l'efficacité du pipeline.  C'est pourquoi l'implémentation par défaut modifie la valeur de Current toutes les quelques dizaines de millisecondes.  Afin d'avoir moins de superpositions dans le temps, chaque hôte sélectionne son propre intervalle. </p><br><p>  Comme vous vous en souvenez, la connexion utilise le concept de Future pour les requêtes asynchrones.  Le cluster utilise le même concept: un Future personnalisé s'enroule dans un cluster et celui-ci est alimenté à la connexion. </p><br><p>  Pourquoi envelopper un avenir personnalisé?  Tout d'abord, en mode Cluster, "radish" renvoie de merveilleuses "erreurs" MOVED et ASK avec des informations où aller pour la clé dont vous avez besoin, et, après avoir reçu une telle erreur, vous devez répéter la demande à un autre hôte.  Deuxièmement, étant donné que nous devons toujours implémenter la logique de redirection, alors pourquoi ne pas intégrer la nouvelle tentative de demande avec une erreur d'E / S (bien sûr, uniquement si la demande de lecture): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> request <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c *Cluster req Request cb Future slot <span class="hljs-keyword"><span class="hljs-keyword">uint16</span></span> policy ReplicaPolicyEnum mayRetry <span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SendWithPolicy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy ReplicaPolicyEnum, req Request, cb Future)</span></span></span></span> { slot := redisclusterutil.ReqSlot(req) policy = c.fixPolicy(slot, req, policy) conn, err := c.connForSlot(slot, policy, <span class="hljs-literal"><span class="hljs-literal">nil</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { cb.Resolve(err) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } r := &amp;request{ c: c, req: req, cb: cb, slot: slot, policy: policy, mayRetry: policy != MasterOnly || redis.ReplicaSafe(req.Cmd), } conn.Send(req, r, <span class="hljs-number"><span class="hljs-number">0</span></span>) } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r *request)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Resolve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(res </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">interface</span></span></span></span><span class="hljs-function"><span class="hljs-params">{}, _ </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span> { err := redis.AsErrorx(res) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err == <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { r.resolve(res) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> err.IsOfType(redis.ErrIO): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> !r.mayRetry { <span class="hljs-comment"><span class="hljs-comment">// It is not safe to retry read-write operation r.resolve(err) return } fallthrough case err.HasTrait(redis.ErrTraitNotSent): // It is request were not sent at all, it is safe to retry both readonly and write requests. conn, err := rcconnForSlot(r.slot, r.policy, r.seen) if err != nil { r.resolve(err) return } conn.Send(r.req, r) return case err.HasTrait(redis.ErrTraitClusterMove): addr := movedTo(err) ask := err.IsOfType(redis.ErrAsk) rcensureConnForAddress(addr, func(conn *redisconn.Connection, cerr error) { if cerr != nil { r.resolve(cerr) } else { r.lastconn = conn conn.SendAsk(r.req, r, ask) } }) return default: // All other errors: just resolve. r.resolve(err) } }</span></span></code> </pre> <br><p>  Il s'agit également d'un code simplifié.  La restriction sur le nombre de tentatives, la mémorisation des connexions problématiques, etc., est omise. </p><br><h2 id="komfort">  Le confort </h2><br><p>  Demandes asynchrones, Future est un superkule!  Mais terriblement inconfortable. </p><br><p>  L'interface est la chose la plus importante.  Vous pouvez vendre n'importe quoi s'il a une belle interface.  C'est pourquoi Redis et MongoDB ont gagné en popularité. </p><br><p>  Il est donc nécessaire de transformer nos requêtes asynchrones en synchrones. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Sync provides convenient synchronous interface over asynchronous Sender. type Sync struct { S Sender } // Do is convenient method to construct and send request. // Returns value that could be either result or error. func (s Sync) Do(cmd string, args ...interface{}) interface{} { return s.Send(Request{cmd, args}) } // Send sends request to redis. // Returns value that could be either result or error. func (s Sync) Send(r Request) interface{} { var res syncRes res.Add(1) sSSend(r, &amp;res) res.Wait() return res.r } type syncRes struct { r interface{} sync.WaitGroup } // Resolve implements Future.Resolve func (s *syncRes) Resolve(res interface{}) { sr = res s.Done() } // Usage func get(s redis.Sender, key string) (interface{}, error) { res := redis.Sync{s}.Do("GET", key) if err := redis.AsError(res); err != nil { return nil, err } return res, nil }</span></span></code> </pre> <br><p>  <code>AsError</code> ne ressemble pas à un Go-way natif pour obtenir une erreur.  Mais j'aime ça, car  à mon avis, le résultat est <code>Result&lt;T,Error&gt;</code> et <code>AsError</code> est un modèle de correspondance ersatz. </p><br><h2 id="nedostatki">  Inconvénients </h2><br><p>  Mais, malheureusement, il y a une mouche dans la pommade dans ce bien-être. </p><br><p>  Le protocole Redis n'implique pas de réorganisation des demandes.  Et en même temps, il a des demandes de blocage telles que BLPOP, BRPOP. </p><br><p>  C'est un échec. </p><br><p>  Comme vous le savez, si une telle demande est bloquée, elle bloquera toutes les demandes qui la suivront.  Et il n'y a rien à faire à ce sujet. </p><br><p>  Après une longue discussion, il a été décidé d'interdire l'utilisation de ces requêtes dans RedisPipe. </p><br><p>  Bien sûr, si vous en avez vraiment besoin, vous pouvez: exposer le paramètre <code>ScriptMode: true</code> , et tout cela est dans votre conscience. </p><br><h2 id="alternativy">  Alternatives </h2><br><p>  En fait, il existe encore une alternative que je n'ai pas mentionnée, mais à laquelle les lecteurs avertis ont pensé, est le roi des caches de cluster twemproxy. </p><br><p>  Il fait pour Redis ce que fait notre connecteur: il transforme une "demande / réponse" brute et sans âme en une "pose de tuyaux" douce. </p><br><p>  Mais twemproxy lui-même souffrira du fait qu'il devra travailler sur un système de "demande / réponse".  Cette fois.  Et deuxièmement, nous utilisons le «radis» ainsi que le «stockage non fiable» et parfois nous redimensionnons le cluster.  Et twemproxy ne facilite en aucun cas la tâche de rééquilibrage et, en outre, nécessite un redémarrage lors du changement de la configuration du cluster. </p><br><h2 id="vliyanie">  Influence </h2><br><p>  Je n'ai pas eu le temps d'écrire un article et les vagues de RedisPipe ont déjà disparu.  Un correctif a été adopté dans Radix.v3 qui ajoute un pipeline à leur pool: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Découvrez RedisPipe et découvrez si sa stratégie de pipelining / batching implicite peut être intégrée</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pipelining automatique pour les commandes dans le pool</a> </p><br><p>  Ils sont légèrement inférieurs en vitesse (à en juger par leurs repères; mais je ne le dirai pas avec certitude).  Mais leur avantage est qu'ils peuvent envoyer des commandes de blocage à d'autres connexions à partir du pool. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  Cela fait déjà un an que RedisPipe contribue à l'efficacité de notre service. <br>  Et en prévision de toute "journée chaude", l'une des ressources, dont la capacité ne pose pas de problème, est le CPU sur les serveurs Redis. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dépôt</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Benchmark</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr438026/">https://habr.com/ru/post/fr438026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr438012/index.html">En Allemagne, en 2018, les sources d'énergie renouvelables tirées par le vent ont fourni plus d'énergie que le charbon</a></li>
<li><a href="../fr438016/index.html">Le Nasdaq et Citi investissent des millions de dollars dans une startup pour introduire la blockchain sur les marchés financiers</a></li>
<li><a href="../fr438018/index.html">CNC dans un atelier de bricolage (partie 2)</a></li>
<li><a href="../fr438020/index.html">Caterpillar présente une pelle électrique de 26 tonnes avec une batterie géante de 300 kWh</a></li>
<li><a href="../fr438022/index.html">La façon dont la taille du code dépend du minificateur, du collecteur et de la langue. Mise à jour inattendue du webpack</a></li>
<li><a href="../fr438028/index.html">Vous n'avez pas besoin de blockchain: 8 cas d'utilisateurs populaires et pourquoi ils ne fonctionnent pas</a></li>
<li><a href="../fr438032/index.html">Open source populaire - deuxième partie: 5 outils de gestion du cloud</a></li>
<li><a href="../fr438034/index.html">Android, Rx et Kotlin, ou comment faire rétrécir une griffe Lego. Partie 1</a></li>
<li><a href="../fr438036/index.html">3blue1brown et MIT en russe</a></li>
<li><a href="../fr438038/index.html">Stéroïdes de carrière. Histoires vraies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>