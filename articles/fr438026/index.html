<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è¨ ü§Ωüèæ üåÄ RedisPipe - Plus de plaisir ensemble üôáüèæ üë®üèº‚Äçüåæ üë®üèΩ‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quand je pense au fonctionnement des clients RPC na√Øfs, je me souviens d'une blague: 
 La cour. 
 "Accus√©, pourquoi avez-vous tu√© une femme?" 
 - Je s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>RedisPipe - Plus de plaisir ensemble</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/joom/blog/438026/"><p>  Quand je pense au fonctionnement des clients RPC na√Øfs, je me souviens d'une blague: </p><br><blockquote>  La cour. <br>  "Accus√©, pourquoi avez-vous tu√© une femme?" <br>  - Je suis dans le bus, le chef d'orchestre s'approche de la femme, lui demandant d'acheter un billet.  La femme a ouvert son sac √† main, a sorti son sac √† main, a ferm√© son sac √† main, a sorti son sac √† main, a ferm√© son sac √† main, a ouvert son sac √† main, a mis son sac √† main l√†-dedans, a ferm√© son sac √† main, a ouvert son sac √† main, a sorti son sac √† main, a ouvert son sac √† main, a sorti son sac √† main, a ferm√© son sac √† main, a ouvert son sac √† main, a mis son sac √† l'int√©rieur. , ferma son sac, ouvrit son sac, y mit son sac. <br>  - Et quoi? <br>  - Le contr√¥leur lui a donn√© un ticket.  Une femme a ouvert son sac √† main, a sorti son sac √† main, a ferm√© son sac √† main, a ouvert son sac √† main, a ferm√© son sac √† main, a ouvert son sac √† main, a mis son sac √† main l√†, a ferm√© son sac √† main, a ouvert son sac √† main, a mis son ticket l√†, a ferm√© son sac √† main, a ouvert son sac √† main, a sorti son sac √† main, a ferm√© son sac √† main, a ouvert son sac √† main , mettez le sac dedans, fermez le sac, ouvrez le sac, mettez le sac dedans, fermez le sac. <br>  "Prenez le changement", vint la voix du contr√¥leur.  La femme ... a ouvert son sac √† main ... <br>  "Oui, il ne suffit pas de la tuer", le procureur ne r√©siste pas. <br>  "Alors je l'ai fait." <br>  ¬© S. Altov </blockquote><p><img src="https://habrastorage.org/webt/qx/ne/-b/qxne-bocftxvmk99ouizmn3iqyo.jpeg"></p><a name="habracut"></a><br><p>  √Ä peu pr√®s la m√™me chose se produit dans le processus de demande-r√©ponse, si vous l'abordez frivolement: </p><br><ul><li>  le processus utilisateur √©crit une requ√™te s√©rialis√©e dans le socket, la copiant r√©ellement dans le tampon de socket au niveau du syst√®me d'exploitation; <br>  C'est une op√©ration assez difficile, car  il est n√©cessaire de faire un changement de contexte (m√™me si cela peut √™tre ¬´facile¬ª); </li><li>  lorsqu'il appara√Æt √† l'OS que quelque chose peut √™tre √©crit sur le r√©seau, un paquet est form√© (la demande est √† nouveau copi√©e) et envoy√© √† la carte r√©seau; </li><li>  la carte r√©seau √©crit le paquet sur le r√©seau (√©ventuellement apr√®s la mise en m√©moire tampon); </li><li>  (en cours de route, un paquet peut √™tre mis en m√©moire tampon plusieurs fois dans les routeurs); </li><li>  enfin, le paquet arrive √† l'h√¥te de destination et est mis en m√©moire tampon sur la carte r√©seau; </li><li>  la carte r√©seau envoie une notification au syst√®me d'exploitation, et lorsque le syst√®me d'exploitation trouve l'heure, elle copie le paquet dans sa m√©moire tampon et d√©finit l'indicateur pr√™t sur le descripteur de fichier; </li><li>  (vous devez toujours vous rappeler d'envoyer l'ACK en r√©ponse); </li><li>  apr√®s un certain temps, l'application serveur se rend compte que le descripteur est pr√™t (√† l'aide d'epoll) et copie un jour la demande dans le tampon d'application; </li><li>  et enfin, l'application serveur traite la demande. </li></ul><br><p>  Comme vous le savez, la r√©ponse est transmise exactement de la m√™me mani√®re que dans la direction oppos√©e.  Ainsi, chaque demande passe du temps notable sur le syst√®me d'exploitation pour sa maintenance, et chaque r√©ponse passe √† nouveau le m√™me temps. </p><br><p>  Cela est devenu particuli√®rement visible apr√®s Meltdown / Spectre, car les correctifs publi√©s ont entra√Æn√© une augmentation significative du co√ªt des appels syst√®me.  D√©but janvier 2018, notre cluster Redis a soudainement commenc√© √† consommer un an et demi √† deux fois plus de CPU, car  Amazon a appliqu√© les correctifs de noyau appropri√©s pour couvrir ces vuln√©rabilit√©s.  (Certes, Amazon a appliqu√© une nouvelle version du correctif un peu plus tard, et la consommation du processeur a chut√© presque aux niveaux pr√©c√©dents. Mais le connecteur a d√©j√† commenc√© √† na√Ætre.) </p><br><p> Malheureusement, tous les connecteurs Go largement connus de Redis et Memcached fonctionnent exactement comme ceci: le connecteur cr√©e un pool de connexions, et lorsqu'il doit envoyer une demande, il extrait une connexion du pool, y √©crit une demande, puis attend une r√©ponse.  (Il est particuli√®rement triste que le connecteur vers Memcached ait √©t√© √©crit par Brad Fitzpatrick lui-m√™me.) Et certains connecteurs ont une impl√©mentation si infructueuse de ce pool que le processus de suppression de la connexion du pool devient un botnet en soi. </p><br><p>  Il existe deux fa√ßons de faciliter ce travail difficile de transfert de demande / r√©ponse: </p><br><ol><li>  Utilisez un acc√®s direct √† la carte r√©seau: DPDK, netmap, PF_RING, etc. </li><li>  N'envoyez pas chaque demande / r√©ponse dans un package distinct, mais combinez-les, si possible, dans des packages plus volumineux, c'est-√†-dire r√©partissez la charge de travail avec le r√©seau pour plusieurs requ√™tes.  Ensemble plus de plaisir! </li></ol><br><p>  Bien entendu, la premi√®re option est possible.  Mais, tout d'abord, c'est pour les courageux, car vous devez √©crire l'impl√©mentation TCP / IP vous-m√™me (par exemple, comme dans ScyllaDB).  Et deuxi√®mement, de cette fa√ßon, nous facilitons la situation d'un seul c√¥t√© - de celui que nous √©crivons nous-m√™mes.  Je ne veux pas (encore) r√©√©crire Redis, donc les serveurs consommeront la m√™me quantit√©, m√™me si le client utilise le cool DPDK. </p><br><p>  La deuxi√®me option est beaucoup plus simple et, surtout, facilite la situation imm√©diatement sur le client et sur le serveur.  Par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une base de donn√©es en m√©moire se</a> targue de pouvoir desservir des millions de RPS, tandis que Redis ne peut pas en desservir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">quelques centaines de milliers</a> .  Cependant, ce succ√®s n'est pas tant la mise en ≈ìuvre de cette base en m√©moire que la d√©cision une fois accept√©e que le protocole sera compl√®tement asynchrone, et les clients devraient utiliser cette asynchronie chaque fois que possible.  Ce que de nombreux clients (en particulier ceux utilis√©s dans les tests de performances) mettent en ≈ìuvre avec succ√®s en envoyant des demandes via une connexion TCP et, si possible, en les envoyant ensemble au r√©seau. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un article bien connu</a> montre que Redis peut √©galement fournir un million de r√©ponses par seconde si un pipeline est utilis√©.  L'exp√©rience personnelle dans le d√©veloppement d'histoires en m√©moire indique √©galement que le pipelage r√©duit consid√©rablement la consommation de CPU SYS et vous permet d'utiliser le processeur et le r√©seau beaucoup plus efficacement. </p><br><p>  La seule question est de savoir comment utiliser le pipelining, si dans les demandes d'application dans Redis arrivent souvent une √† la fois?  Et si un serveur est manquant et que Redis Cluster est utilis√© avec un grand nombre de fragments, alors m√™me lorsqu'un paquet de demandes est rencontr√©, il se divise en demandes uniques pour chaque fragment. </p><br><p>  La r√©ponse, bien s√ªr, est ¬´√©vidente¬ª: faire un pipeline implicite, collecter les demandes de tous les goroutines fonctionnant en parall√®le sur un serveur Redis et les envoyer via une connexion. </p><br><p>  Soit dit en passant, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pose</a> implicite de tuyaux n'est pas si rare dans les connecteurs dans d'autres langages: nodejs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">node_redis</a> , C # <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RedisBoost</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">aioredis de</a> python et bien d'autres.  Beaucoup de ces connecteurs sont √©crits au-dessus des boucles d'√©v√©nements, et la collecte de requ√™tes √† partir de "flux de calcul" parall√®les semble donc naturelle.  Dans Go, l'utilisation des interfaces synchrones est encourag√©e et, apparemment, parce que peu de gens d√©cident d'organiser leur propre ¬´boucle¬ª. </p><br><p>  Nous voulions utiliser Redis le plus efficacement possible et avons donc d√©cid√© d'√©crire un nouveau "meilleur" connecteur (tm): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RedisPipe</a> . </p><br><h2 id="kak-sdelat-neyavnyy-payplayning">  Comment faire la pose implicite de tuyaux? </h2><br><p>  Le sch√©ma de base: </p><br><ul><li>  Les goroutines de la logique d'application n'√©crivent pas les demandes directement au r√©seau, mais les transmettent au collecteur de goroutines; </li><li>  si possible, le collectionneur recueille un tas de demandes, les √©crit sur le r√©seau et les transmet au lecteur de goroutine; </li><li>  Goroutine-reader lit les r√©ponses du r√©seau, les compare avec les requ√™tes correspondantes, et informe les goroutines de la logique de la r√©ponse arriv√©e. </li></ul><br><p> Quelque chose doit √™tre notifi√© de la r√©ponse.  Un programmeur astucieux de Go dira certainement: "Par le canal!" <br>  Mais ce n'est pas la seule primitive de synchronisation possible et pas la plus efficace m√™me dans l'environnement Go.  Et puisque diff√©rentes personnes ont des besoins diff√©rents, nous rendrons le m√©canisme extensible, permettant √† l'utilisateur d'impl√©menter l'interface (appelons-le <code>Future</code> ): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Future <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> { Resolve(val <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span>{}) }</code> </pre> <br><p>  Et puis le sch√©ma de base ressemblera √† ceci: </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> future <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { req Request fut Future } <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Conn <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c net.Conn futmtx sync.Mutex wfutures []future futtimer *time.Timer rfutures <span class="hljs-keyword"><span class="hljs-keyword">chan</span></span> []future } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Send</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r Request, f Future)</span></span></span></span> { c.futmtx.Lock() <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> c.futmtx.Unlock() c.wfutures = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(c.wfutures, future{req: r, fut: f}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(c.wfutures) == <span class="hljs-number"><span class="hljs-number">1</span></span> { futtimer.Reset(<span class="hljs-number"><span class="hljs-number">100</span></span>*time.Microsecond) } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">writer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> c.futtimer.C { c.futmtx.Lock() futures, c.wfutures = c.wfutures, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> c.futmtx.Unlock() <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> b []<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, ft := <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> futures { b = AppendRequest(b, ft.req) } _, _err := ccWrite(b) c.rfutures &lt;- futures } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reader</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { rd := bufio.NewReader(cc) <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> futures []future <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> { response, _err := ParseResponse(rd) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(futures) == <span class="hljs-number"><span class="hljs-number">0</span></span> { futures = &lt;- c.rfutures } futures[<span class="hljs-number"><span class="hljs-number">0</span></span>].fut.Resolve(response) futures = futures[<span class="hljs-number"><span class="hljs-number">1</span></span>:] } }</code> </pre> <br><p>  Bien s√ªr, c'est un code tr√®s simplifi√©.  Omis: </p><br><ul><li>  √©tablissement de connexion; </li><li>  D√©lais d'E / S </li><li>  gestion des erreurs de lecture / √©criture; </li><li>  r√©tablir la connexion; </li><li>  la possibilit√© d'annuler la demande avant de l'envoyer au r√©seau; </li><li>  optimisation de l'allocation de m√©moire (r√©utilisation de la m√©moire tampon et des tableaux √† terme). </li></ul><br><p>  Toute erreur d'E / S (y compris un d√©lai d'attente) dans le code r√©el conduit √† une r√©solution de toutes les erreurs futures correspondant aux demandes envoy√©es et en attente d'√™tre envoy√©es. <br>  La couche de connexion n'est pas impliqu√©e dans la nouvelle tentative de demande, et s'il est n√©cessaire (et possible) de relancer la demande, cela peut √™tre fait √† un niveau d'abstraction plus √©lev√© (par exemple, dans la mise en ≈ìuvre de la prise en charge de Redis Cluster d√©crite ci-dessous). </p><br><p>  Remarque.  Au d√©part, le circuit semblait un peu plus compliqu√©.  Mais dans le processus d'exp√©riences simplifi√©es √† une telle option. </p><br><p>  Remarque 2. Des exigences tr√®s strictes sont impos√©es √† la m√©thode Future.Resolve: elle doit √™tre aussi rapide que possible, pratiquement non bloquante et en aucun cas panique.  Cela est d√ª au fait qu'il est appel√© de mani√®re synchrone dans le cycle du lecteur, et tout "frein" entra√Ænera in√©vitablement une d√©gradation.  La mise en ≈ìuvre de Future.Resolve doit faire le minimum n√©cessaire d'actions lin√©aires: r√©veiller l'expectatif;  peut-√™tre g√©rer l'erreur et envoyer une nouvelle tentative asynchrone (utilis√©e dans l'impl√©mentation de la prise en charge du cluster). </p><br><h2 id="effekt">  Effet </h2><br><p>  Une bonne r√©f√©rence est la moiti√© de l'article! </p><br><p>  Une bonne r√©f√©rence est celle qui est la plus proche possible de la lutte contre l'utilisation en termes d'effets observ√©s.  Et ce n'est pas facile √† faire. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'option de r√©f√©rence</a> , qui, il me semble, ressemble assez √† la vraie: </p><br><ul><li>  le "script" principal √©mule 5 clients parall√®les, </li><li>  dans chaque "client", pour chaque 300-1000 rps "souhait√©s", la goroutine est d√©clench√©e (3 gorutins sont d√©clench√©s pour 1000 rps, 124 gorutins sont ex√©cut√©s pour 128000 rps), </li><li>  gorutin utilise une instance distincte du limiteur de d√©bit et envoie des requ√™tes en s√©rie al√©atoire - de 5 √† 15 requ√™tes. </li></ul><br><p>  Le caract√®re al√©atoire de la s√©rie de requ√™tes nous permet d'obtenir une distribution al√©atoire de la s√©rie dans la chronologie, qui refl√®te plus correctement la charge r√©elle. </p><br><div class="spoiler">  <b class="spoiler_title">Texte masqu√©</b> <div class="spoiler_text"><p>  Les mauvaises options √©taient: <br>  a) utiliser un limiteur de d√©bit pour tous les gorutins du ¬´client¬ª et y recourir pour chaque demande - cela entra√Æne une consommation excessive du processeur par le limiteur de d√©bit lui-m√™me, ainsi qu'une augmentation de la rotation dans le temps du goroutin, ce qui d√©grade les performances de RedisPipe √† des vitesses moyennes (mais inexplicablement s'am√©liore √† haut); <br>  b) utiliser un limiteur de d√©bit pour tous les gorutins du ¬´client¬ª et envoyer des requ√™tes en s√©rie - le limiteur de d√©bit ne mange pas d√©j√† tant le CPU, mais l'alternance des goroutines dans le temps ne fait qu'augmenter; <br>  c) utiliser un limiteur de d√©bit pour chaque goroutine, mais envoyer la m√™me s√©rie de 10 demandes - dans ce sc√©nario, les goroutines se r√©veillent trop simultan√©ment, ce qui am√©liore injustement les r√©sultats de RedisPipe. </p></div></div><br><p>  Les tests ont eu lieu sur une instance AWS c5-2xlarge √† quatre c≈ìurs.  La version de Redis est 5.0. </p><br><p>  Le rapport de l'intensit√© de requ√™te souhait√©e, l'intensit√© totale r√©sultante et consomm√©e par le radis CPU: </p><br><table><thead><tr><th>  rps pr√©vus </th><th>  redigo <br>  rps /% cpu </th><th>  redispipe pas d'attente <br>  rps /% cpu </th><th>  redispipe 50¬µs <br>  rps /% cpu </th><th>  redispipe 150¬µs <br>  rps /% cpu </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  5015/7% </td><td>  5015/6% </td><td>  5015/6% </td><td>  5015/6% </td></tr><tr><td>  2000 * 5 </td><td>  10022/11% </td><td>  10022/10% </td><td>  10022/10% </td><td>  10022/10% </td></tr><tr><td>  4000 * 5 </td><td>  20036/21% </td><td>  20036/18% </td><td>  20035/17% </td><td>  20035/15% </td></tr><tr><td>  8000 * 5 </td><td>  40020/45% </td><td>  40062/37% </td><td>  40060/26% </td><td>  40056/19% </td></tr><tr><td>  16000 * 5 </td><td>  79994/71% </td><td>  80102/58% </td><td>  80096/33% </td><td>  80090/23% </td></tr><tr><td>  32000 * 5 </td><td>  159590/96% </td><td>  160 180/80% </td><td>  160.167 / 39% </td><td>  160 150/29% </td></tr><tr><td>  64000 * 5 </td><td>  187774/99% </td><td>  320 313/98% </td><td>  320 283/47% </td><td>  320 258/37% </td></tr><tr><td>  92000 * 5 </td><td>  183206/99% </td><td>  480 443/97% </td><td>  480 407/52% </td><td>  480 366/42% </td></tr><tr><td>  128000 * 5 </td><td>  179744/99% </td><td>  640,484 / 97% </td><td>  640,488 / 55% </td><td>  640 428/46% </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/g-/d7/7k/g-d77kznsmwmgejiyplpipwv7t4.png" alt="Taux de demande"><img src="https://habrastorage.org/webt/cs/xf/lz/csxflztfjbnmzc0kuhpnqddeokq.png" alt="Redis cpu"></p><br><p>  Vous pouvez remarquer qu'avec un connecteur fonctionnant selon le sch√©ma classique (demande / r√©ponse + pool de connexions), Redis mange assez rapidement le c≈ìur du processeur, apr√®s quoi il devient impossible d'obtenir plus de 190 krps. </p><br><p>  RedisPipe vous permet d'extraire toute la puissance requise de Redis.  Et plus nous nous arr√™tons pour collecter des requ√™tes parall√®les, moins Redis consomme de CPU.  Un avantage tangible est d√©j√† obtenu √† 4 krps du client (20 krps au total) si une pause de 150 microsecondes est utilis√©e. </p><br><p>  M√™me si la pause n'est pas explicitement utilis√©e lorsque Redis repose sur le CPU, le retard appara√Æt de lui-m√™me.  De plus, les demandes commencent √† √™tre mises en m√©moire tampon par le syst√®me d'exploitation.  Cela permet √† RedisPipe d'augmenter le nombre de requ√™tes ex√©cut√©es avec succ√®s lorsque le connecteur classique baisse d√©j√† ses pattes. </p><br><p>  C'est le r√©sultat principal, pour lequel il fallait cr√©er un nouveau connecteur. </p><br><p>  Que se passe-t-il alors avec la consommation du processeur sur le client et avec les demandes retard√©es? </p><br><table><thead><tr><th>  rps pr√©vus </th><th>  redigo <br>  % cpu / ms </th><th>  redispipe nowait <br>  % cpu / ms </th><th>  redispipe 50ms <br>  % cpu / ms </th><th>  redispipe 150ms <br>  % cpu / ms </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  13 / 0,03 </td><td>  20 / 0,04 </td><td>  46 / 0,16 </td><td>  44 / 0,26 </td></tr><tr><td>  2000 * 5 </td><td>  25 / 0,03 </td><td>  33 / 0,04 </td><td>  77 / 0,16 </td><td>  71 / 0,26 </td></tr><tr><td>  4000 * 5 </td><td>  48 / 0,03 </td><td>  60 / 0,04 </td><td>  124 / 0,16 </td><td>  107 / 0,26 </td></tr><tr><td>  8000 * 5 </td><td>  94 / 0,03 </td><td>  119 / 0,04 </td><td>  178 / 0,15 </td><td>  141 / 0,26 </td></tr><tr><td>  16000 * 5 </td><td>  184 / 0,04 </td><td>  206 / 0,04 </td><td>  228 / 0,15 </td><td>  177 / 0,25 </td></tr><tr><td>  32000 * 5 </td><td>  341 / 0,08 </td><td>  322 / 0,05 </td><td>  280 / 0,15 </td><td>  226 / 0,26 </td></tr><tr><td>  64000 * 5 </td><td>  316 / 1,88 </td><td>  469 / 0,08 </td><td>  345 / 0,16 </td><td>  307 / 0,26 </td></tr><tr><td>  92000 * 5 </td><td>  313 / 2,88 </td><td>  511 / 0,16 </td><td>  398 / 0,17 </td><td>  366 / 0,27 </td></tr><tr><td>  128000 * 5 </td><td>  312 / 3,54 </td><td>  509 / 0,37 </td><td>  441 / 0,19 </td><td>  418 / 0,29 </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/fp/ws/ra/fpwsra-pgtexl6ua7crpc4vt5fa.png" alt="CPU client"><img src="https://habrastorage.org/webt/kw/kl/lk/kwkllkjyf-vfpv77miu4jnnlrqm.png" alt="latence"></p><br><p>  Vous pouvez remarquer que sur les petits rps, RedisPipe lui-m√™me consomme plus de CPU que le "concurrent", surtout si une petite pause est utilis√©e.  Cela est principalement d√ª √† l'impl√©mentation de minuteries dans Go et √† l'impl√©mentation des appels syst√®me utilis√©s dans le syst√®me d'exploitation (sous Linux, c'est futexsleep), car en mode ¬´sans pause¬ª, la diff√©rence est nettement moins importante. </p><br><p>  Avec l'augmentation de la vitesse par seconde, la surcharge des minuteries est compens√©e par une baisse de la surcharge pour la mise en r√©seau et apr√®s 16 krps par client, l'utilisation de RedisPipe avec une pause de 150 microsecondes commence √† apporter des avantages tangibles. </p><br><p>  Bien s√ªr, apr√®s que Redis se soit repos√© sur le CPU, la latence des requ√™tes utilisant le connecteur classique commence √† augmenter.  Je ne suis pas s√ªr, cependant, qu'en pratique, vous atteignez souvent 180 krps √† partir d'une instance Redis.  Mais si c'est le cas, gardez √† l'esprit que vous pourriez avoir des probl√®mes. </p><br><p>  Tant que Redis ne s'ex√©cute pas dans le CPU, la latence de la demande souffre bien s√ªr de l'utilisation d'une pause.  Ce compromis est intentionnellement pos√© dans le connecteur.  Cependant, ce compromis n'est perceptible que si Redis et le client sont sur le m√™me h√¥te physique.  Selon la topologie du r√©seau, un aller-retour vers un h√¥te voisin peut aller de cent microsecondes √† une milliseconde.  En cons√©quence, la diff√©rence de retard d√©j√† au lieu de neuf fois (0,26 / 0,03) devient trois fois (0,36 / 0,13) ou n'est mesur√©e que par quelques dizaines de pour cent (1,26 / 1,03). </p><br><p>  Dans notre charge de travail, lorsque Redis est utilis√© comme cache, l'attente totale des r√©ponses de la base de donn√©es avec un √©chec de cache est sup√©rieure √† l'attente totale des r√©ponses de Redis, car on pense que l'augmentation du d√©lai n'est pas significative. </p><br><p>  Le principal r√©sultat positif est une tol√©rance √† la croissance de la charge: si soudainement la charge sur le service augmente N fois, Redis ne consommera pas le m√™me CPU N fois plus.  Pour r√©sister au quadruplement de la charge de 160 krps √† 640 krps, Redis n'a d√©pens√© que 1,6 fois plus de CPU, augmentant la consommation de 29 √† 46%.  Cela nous permet de ne pas avoir peur que Redis se plie soudainement.  L'√©volutivit√© de l'application ne sera pas non plus d√©termin√©e par le fonctionnement du connecteur et le co√ªt de la connectivit√© r√©seau (lire: Co√ªts du processeur SYS). </p><br><p>  Remarque.  Le code de r√©f√©rence fonctionne avec de petites valeurs.  Pour effacer ma conscience, j'ai r√©p√©t√© le test avec des valeurs de taille 768 octets.  La consommation de CPU par le radis a augment√© de fa√ßon marqu√©e (jusqu'√† 66% sur une pause de 150 ¬µs), et le plafond pour un connecteur classique tombe √† 170 krps.  Mais toutes les proportions observ√©es sont rest√©es les m√™mes, et donc les conclusions. </p><br><h2 id="klaster">  Cluster </h2><br><p>  Pour la mise √† l'√©chelle, nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Redis Cluster</a> .  Cela nous permet d'utiliser Redis non seulement comme cache, mais aussi comme stockage volatile et en m√™me temps de ne pas perdre de donn√©es lors de l'expansion / compression d'un cluster. </p><br><p>  Redis Cluster utilise le principe du client intelligent, c'est-√†-dire  le client doit surveiller l'√©tat du cluster lui-m√™me et √©galement r√©pondre aux erreurs auxiliaires renvoy√©es par le "radis" lorsque le "bouquet" se d√©place d'instance en instance. </p><br><p>  Par cons√©quent, le client doit conserver les connexions √† toutes les instances Redis du cluster et √©tablir une connexion √† celle requise pour chaque demande.  Et c'est √† cet endroit que le client utilisait auparavant (on ne va pas pointer du doigt) a √©t√© bien foir√©.  L'auteur, qui a surestim√© la commercialisation de Go (CSP, canaux, goroutines), a impl√©ment√© la synchronisation du travail avec l'√©tat du cluster en envoyant des rappels au goroutine central.  C'est devenu un s√©rieux botnek pour nous.  En tant que correctif temporaire, nous avons d√ª lancer quatre clients sur un cluster, chacun, √† son tour, augmentant jusqu'√† des centaines de connexions dans le pool pour chaque instance de Redis. </p><br><p>  Par cons√©quent, le nouveau connecteur a √©t√© charg√© d'emp√™cher cette erreur.  Toutes les interactions avec l'√©tat du cluster sur le chemin d'ex√©cution de la requ√™te se font aussi sans verrouillage que possible: </p><br><ul><li>  l'√©tat de cluster est rendu pratiquement immuable, et pas de nombreuses mutations aromatis√©es par des atomes </li><li>  l'acc√®s √† l'√©tat se produit √† l'aide de atomic.StorePointer / atomic.LoadPointer, et peut donc √™tre obtenu sans blocage. </li></ul><br><p>  Ainsi, m√™me pendant la mise √† jour de l'√©tat du cluster, les requ√™tes peuvent utiliser l'√©tat pr√©c√©dent sans craindre d'attendre un verrou. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// storeConfig atomically stores config func (c *Cluster) storeConfig(cfg *clusterConfig) { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) atomic.StorePointer(p, unsafe.Pointer(cfg)) } // getConfig loads config atomically func (c *Cluster) getConfig() *clusterConfig { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) return (*clusterConfig)(atomic.LoadPointer(p)) } func (cfg *clusterConfig) slot2shardno(slot uint16) uint16 { return uint16(atomic.LoadUint32(&amp;cfg.slots[slot])) } func (cfg *clusterConfig) slotSetShard(slot, shard uint16) { atomic.StoreUint32(&amp;cfg.slots[slot], shard) }</span></span></code> </pre> <br><p>  Le statut du cluster est mis √† jour toutes les 5 secondes.  Mais en cas de suspicion d'instabilit√© du cluster, la mise √† jour est forc√©e: </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">control</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { t := time.NewTicker(c.opts.CheckInterval) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> t.Stop() <span class="hljs-comment"><span class="hljs-comment">// main control loop for { select { case &lt;-c.ctx.Done(): // cluster closed, exit control loop c.report(LogContextClosed{Error: c.ctx.Err()}) return case cmd := &lt;-c.commands: // execute some asynchronous "cluster-wide" actions c.execCommand(cmd) continue case &lt;-forceReload: // forced mapping reload c.reloadMapping() case &lt;-tC: // regular mapping reload c.reloadMapping() } } } func (c *Cluster) ForceReloading() { select { case c.forceReload &lt;- struct{}{}: default: } }</span></span></code> </pre> <br><p>  Si la r√©ponse MOVED ou ASK re√ßue du radis contient une adresse inconnue, son ajout asynchrone √† la configuration est lanc√©.  (Je suis d√©sol√©, je n'ai pas compris comment simplifier le code, car <a href="">voici le lien</a> .) Ce n'√©tait pas sans utiliser de verrous, mais ils sont pris pour une courte p√©riode de temps.  La principale attente est r√©alis√©e en enregistrant le rappel dans le tableau - le m√™me avenir, vue lat√©rale. </p><br><p>  Des connexions sont √©tablies avec toutes les instances Redis, ainsi qu'avec les ma√Ætres et les esclaves.  Selon la politique pr√©f√©r√©e et le type de demande (lecture ou √©criture), la demande peut √™tre envoy√©e √† la fois au ma√Ætre et √† l'esclave.  Cela prend en compte la "vivacit√©" de l'instance, qui se compose √† la fois des informations obtenues lors de la mise √† jour de l'√©tat du cluster et de l'√©tat actuel de la connexion. </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">connForSlot</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(slot </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint16</span></span></span></span><span class="hljs-function"><span class="hljs-params">, policy ReplicaPolicyEnum)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*redisconn.Connection, *errorx.Error)</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conn *redisconn.Connection cfg := c.getConfig() shard := cfg.slot2shard(slot) nodes := cfg.nodes <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> addr <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> policy { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> MasterOnly: addr = shard.addr[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment">// master is always first node := nodes[addr] if conn = node.getConn(c.opts.ConnHostPolicy, needConnected); conn == nil { conn = node.getConn(c.opts.ConnHostPolicy, mayBeConnected) } case MasterAndSlaves: n := uint32(len(shard.addr)) off := c.opts.RoundRobinSeed.Current() for _, needState := range []int{needConnected, mayBeConnected} { mask := atomic.LoadUint32(&amp;shard.good) // load health information for ; mask != 0; off++ { bit := 1 &lt;&lt; (off % n) if mask&amp;bit == 0 { // replica isn't healthy, or already viewed continue } mask &amp;^= bit addr = shard.addr[k] if conn = nodes[addr].getConn(c.opts.ConnHostPolicy, needState); conn != nil { return conn, nil } } } } if conn == nil { c.ForceReloading() return nil, c.err(ErrNoAliveConnection) } return conn, nil } func (n *node) getConn(policy ConnHostPolicyEnum, liveness int) *redisconn.Connection { for _, conn := range n.conns { switch liveness { case needConnected: if c.ConnectedNow() { return conn } case mayBeConnected: if c.MayBeConnected() { return conn } } } return nil }</span></span></code> </pre> <br><p>  Il y a un <code>RoundRobinSeed.Current()</code> cryptique <code>RoundRobinSeed.Current()</code> .  Ceci, d'une part, est une source de hasard, et d'autre part, un hasard qui ne change pas fr√©quemment.  Si vous s√©lectionnez une nouvelle connexion pour chaque demande, cela d√©grade l'efficacit√© du pipeline.  C'est pourquoi l'impl√©mentation par d√©faut modifie la valeur de Current toutes les quelques dizaines de millisecondes.  Afin d'avoir moins de superpositions dans le temps, chaque h√¥te s√©lectionne son propre intervalle. </p><br><p>  Comme vous vous en souvenez, la connexion utilise le concept de Future pour les requ√™tes asynchrones.  Le cluster utilise le m√™me concept: un Future personnalis√© s'enroule dans un cluster et celui-ci est aliment√© √† la connexion. </p><br><p>  Pourquoi envelopper un avenir personnalis√©?  Tout d'abord, en mode Cluster, "radish" renvoie de merveilleuses "erreurs" MOVED et ASK avec des informations o√π aller pour la cl√© dont vous avez besoin, et, apr√®s avoir re√ßu une telle erreur, vous devez r√©p√©ter la demande √† un autre h√¥te.  Deuxi√®mement, √©tant donn√© que nous devons toujours impl√©menter la logique de redirection, alors pourquoi ne pas int√©grer la nouvelle tentative de demande avec une erreur d'E / S (bien s√ªr, uniquement si la demande de lecture): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> request <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c *Cluster req Request cb Future slot <span class="hljs-keyword"><span class="hljs-keyword">uint16</span></span> policy ReplicaPolicyEnum mayRetry <span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SendWithPolicy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy ReplicaPolicyEnum, req Request, cb Future)</span></span></span></span> { slot := redisclusterutil.ReqSlot(req) policy = c.fixPolicy(slot, req, policy) conn, err := c.connForSlot(slot, policy, <span class="hljs-literal"><span class="hljs-literal">nil</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { cb.Resolve(err) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } r := &amp;request{ c: c, req: req, cb: cb, slot: slot, policy: policy, mayRetry: policy != MasterOnly || redis.ReplicaSafe(req.Cmd), } conn.Send(req, r, <span class="hljs-number"><span class="hljs-number">0</span></span>) } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r *request)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Resolve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(res </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">interface</span></span></span></span><span class="hljs-function"><span class="hljs-params">{}, _ </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span> { err := redis.AsErrorx(res) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err == <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { r.resolve(res) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> err.IsOfType(redis.ErrIO): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> !r.mayRetry { <span class="hljs-comment"><span class="hljs-comment">// It is not safe to retry read-write operation r.resolve(err) return } fallthrough case err.HasTrait(redis.ErrTraitNotSent): // It is request were not sent at all, it is safe to retry both readonly and write requests. conn, err := rcconnForSlot(r.slot, r.policy, r.seen) if err != nil { r.resolve(err) return } conn.Send(r.req, r) return case err.HasTrait(redis.ErrTraitClusterMove): addr := movedTo(err) ask := err.IsOfType(redis.ErrAsk) rcensureConnForAddress(addr, func(conn *redisconn.Connection, cerr error) { if cerr != nil { r.resolve(cerr) } else { r.lastconn = conn conn.SendAsk(r.req, r, ask) } }) return default: // All other errors: just resolve. r.resolve(err) } }</span></span></code> </pre> <br><p>  Il s'agit √©galement d'un code simplifi√©.  La restriction sur le nombre de tentatives, la m√©morisation des connexions probl√©matiques, etc., est omise. </p><br><h2 id="komfort">  Le confort </h2><br><p>  Demandes asynchrones, Future est un superkule!  Mais terriblement inconfortable. </p><br><p>  L'interface est la chose la plus importante.  Vous pouvez vendre n'importe quoi s'il a une belle interface.  C'est pourquoi Redis et MongoDB ont gagn√© en popularit√©. </p><br><p>  Il est donc n√©cessaire de transformer nos requ√™tes asynchrones en synchrones. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Sync provides convenient synchronous interface over asynchronous Sender. type Sync struct { S Sender } // Do is convenient method to construct and send request. // Returns value that could be either result or error. func (s Sync) Do(cmd string, args ...interface{}) interface{} { return s.Send(Request{cmd, args}) } // Send sends request to redis. // Returns value that could be either result or error. func (s Sync) Send(r Request) interface{} { var res syncRes res.Add(1) sSSend(r, &amp;res) res.Wait() return res.r } type syncRes struct { r interface{} sync.WaitGroup } // Resolve implements Future.Resolve func (s *syncRes) Resolve(res interface{}) { sr = res s.Done() } // Usage func get(s redis.Sender, key string) (interface{}, error) { res := redis.Sync{s}.Do("GET", key) if err := redis.AsError(res); err != nil { return nil, err } return res, nil }</span></span></code> </pre> <br><p>  <code>AsError</code> ne ressemble pas √† un Go-way natif pour obtenir une erreur.  Mais j'aime √ßa, car  √† mon avis, le r√©sultat est <code>Result&lt;T,Error&gt;</code> et <code>AsError</code> est un mod√®le de correspondance ersatz. </p><br><h2 id="nedostatki">  Inconv√©nients </h2><br><p>  Mais, malheureusement, il y a une mouche dans la pommade dans ce bien-√™tre. </p><br><p>  Le protocole Redis n'implique pas de r√©organisation des demandes.  Et en m√™me temps, il a des demandes de blocage telles que BLPOP, BRPOP. </p><br><p>  C'est un √©chec. </p><br><p>  Comme vous le savez, si une telle demande est bloqu√©e, elle bloquera toutes les demandes qui la suivront.  Et il n'y a rien √† faire √† ce sujet. </p><br><p>  Apr√®s une longue discussion, il a √©t√© d√©cid√© d'interdire l'utilisation de ces requ√™tes dans RedisPipe. </p><br><p>  Bien s√ªr, si vous en avez vraiment besoin, vous pouvez: exposer le param√®tre <code>ScriptMode: true</code> , et tout cela est dans votre conscience. </p><br><h2 id="alternativy">  Alternatives </h2><br><p>  En fait, il existe encore une alternative que je n'ai pas mentionn√©e, mais √† laquelle les lecteurs avertis ont pens√©, est le roi des caches de cluster twemproxy. </p><br><p>  Il fait pour Redis ce que fait notre connecteur: il transforme une "demande / r√©ponse" brute et sans √¢me en une "pose de tuyaux" douce. </p><br><p>  Mais twemproxy lui-m√™me souffrira du fait qu'il devra travailler sur un syst√®me de "demande / r√©ponse".  Cette fois.  Et deuxi√®mement, nous utilisons le ¬´radis¬ª ainsi que le ¬´stockage non fiable¬ª et parfois nous redimensionnons le cluster.  Et twemproxy ne facilite en aucun cas la t√¢che de r√©√©quilibrage et, en outre, n√©cessite un red√©marrage lors du changement de la configuration du cluster. </p><br><h2 id="vliyanie">  Influence </h2><br><p>  Je n'ai pas eu le temps d'√©crire un article et les vagues de RedisPipe ont d√©j√† disparu.  Un correctif a √©t√© adopt√© dans Radix.v3 qui ajoute un pipeline √† leur pool: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©couvrez RedisPipe et d√©couvrez si sa strat√©gie de pipelining / batching implicite peut √™tre int√©gr√©e</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pipelining automatique pour les commandes dans le pool</a> </p><br><p>  Ils sont l√©g√®rement inf√©rieurs en vitesse (√† en juger par leurs rep√®res; mais je ne le dirai pas avec certitude).  Mais leur avantage est qu'ils peuvent envoyer des commandes de blocage √† d'autres connexions √† partir du pool. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  Cela fait d√©j√† un an que RedisPipe contribue √† l'efficacit√© de notre service. <br>  Et en pr√©vision de toute "journ√©e chaude", l'une des ressources, dont la capacit√© ne pose pas de probl√®me, est le CPU sur les serveurs Redis. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©p√¥t</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Benchmark</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr438026/">https://habr.com/ru/post/fr438026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr438012/index.html">En Allemagne, en 2018, les sources d'√©nergie renouvelables tir√©es par le vent ont fourni plus d'√©nergie que le charbon</a></li>
<li><a href="../fr438016/index.html">Le Nasdaq et Citi investissent des millions de dollars dans une startup pour introduire la blockchain sur les march√©s financiers</a></li>
<li><a href="../fr438018/index.html">CNC dans un atelier de bricolage (partie 2)</a></li>
<li><a href="../fr438020/index.html">Caterpillar pr√©sente une pelle √©lectrique de 26 tonnes avec une batterie g√©ante de 300 kWh</a></li>
<li><a href="../fr438022/index.html">La fa√ßon dont la taille du code d√©pend du minificateur, du collecteur et de la langue. Mise √† jour inattendue du webpack</a></li>
<li><a href="../fr438028/index.html">Vous n'avez pas besoin de blockchain: 8 cas d'utilisateurs populaires et pourquoi ils ne fonctionnent pas</a></li>
<li><a href="../fr438032/index.html">Open source populaire - deuxi√®me partie: 5 outils de gestion du cloud</a></li>
<li><a href="../fr438034/index.html">Android, Rx et Kotlin, ou comment faire r√©tr√©cir une griffe Lego. Partie 1</a></li>
<li><a href="../fr438036/index.html">3blue1brown et MIT en russe</a></li>
<li><a href="../fr438038/index.html">St√©ro√Ødes de carri√®re. Histoires vraies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>