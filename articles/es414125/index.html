<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üç® üéÉ üßîüèæ C√≥mo probamos VMware vSAN ‚Ñ¢: por qu√© funciona en la pr√°ctica üëπ üîº ü§µüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hace un a√±o, ensambl√© un centro de datos a partir de una pila de Intel NUC . Hubo un sistema de almacenamiento de software que la se√±ora de la limpiez...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo probamos VMware vSAN ‚Ñ¢: por qu√© funciona en la pr√°ctica</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croc/blog/414125/">  Hace un a√±o, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ensambl√© un centro de datos a partir de una pila de Intel NUC</a> .  Hubo un sistema de almacenamiento de software que la se√±ora de la limpieza no pudo destruir, un par de veces el cl√∫ster colaps√≥. <br><br>  Y ahora decidimos ejecutar vSAN en varios servidores en una configuraci√≥n muy buena para evaluar completamente el rendimiento y la tolerancia a fallas de la soluci√≥n.  Por supuesto, tuvimos una serie de implementaciones exitosas en producci√≥n para nuestros clientes, donde vSAN resuelve con √©xito las tareas, pero no fue posible realizar pruebas exhaustivas.  En realidad, quiero compartir los resultados de la prueba hoy. <br><br>  Tormentaremos el almacenamiento con una carga, lo soltaremos y probaremos la tolerancia a fallas de todas las formas posibles.  Para m√°s detalles, invito a todos a gato. <br><a name="habracut"></a><br><h3>  ¬øQu√© es VMware vSAN en general y por qu√© nos metimos en √©l? </h3><br>  Hay un cl√∫ster de servidores regular para m√°quinas virtuales.  Tiene un mont√≥n de componentes independientes, el hipervisor se ejecuta directamente en el hardware y el almacenamiento se configura por separado seg√∫n DAS, NAS o SAN.  Datos lentos en HDD, datos calientes en SSD.  Todo es familiar  Pero aqu√≠ surge el problema del despliegue y administraci√≥n de este zool√≥gico.  Se vuelve especialmente divertido en situaciones donde los elementos individuales del sistema provienen de diferentes proveedores.  En caso de problemas, el atraque de boletos para soporte t√©cnico de diferentes fabricantes tiene su propia atm√≥sfera especial. <br><br>  Hay piezas de hierro separadas, que desde el punto de vista del servidor parecen discos para grabar. <br><br>  Y hay sistemas hiperconvergentes.  En ellos, se le da una unidad universal que asume todo el dolor de cabeza de la interacci√≥n de la red, los discos, los procesadores, la memoria y las m√°quinas virtuales que giran sobre ellos.  Todos los datos fluyen en un panel de control y, si es necesario, simplemente puede agregar un par de unidades m√°s para compensar el aumento de carga.  La administraci√≥n est√° muy simplificada y estandarizada. <br><br><img src="https://habrastorage.org/webt/xi/4m/uw/xi4muweeppbjng1b8mrpc-bzi3m.png"><br>  VMware vSAN solo se refiere a las soluciones en base a las cuales se implementa <br>  Infraestructura hiperconvergente.  La caracter√≠stica clave del producto es una estrecha integraci√≥n con la plataforma de virtualizaci√≥n VMware vSphere, que es l√≠der entre las soluciones de virtualizaci√≥n que le permite implementar el almacenamiento de software para m√°quinas virtuales en servidores de virtualizaci√≥n en minutos.  vSAN toma directamente el control de las operaciones de E / S a un nivel bajo, distribuyendo de manera √≥ptima la carga, almacenando en cach√© las operaciones de lectura / escritura y haciendo mucho m√°s con una carga m√≠nima en la memoria y el procesador.  La transparencia del sistema se reduce ligeramente, pero como resultado, todo funciona, como dicen, vSAN se puede configurar autom√°ticamente como un almacenamiento h√≠brido y en forma de una versi√≥n todo flash.  Se escala horizontalmente agregando nuevos nodos al cl√∫ster y verticalmente, aumentando el n√∫mero de discos en nodos individuales.  Administrar con su propio cliente web vSphere es muy conveniente debido a la estrecha integraci√≥n con otros productos. <br><br>  Nos decidimos por una configuraci√≥n limpia todo flash, que deber√≠a ser √≥ptima en t√©rminos de precio y rendimiento.  Est√° claro que la capacidad total es ligeramente menor en comparaci√≥n con la configuraci√≥n h√≠brida que utiliza discos magn√©ticos, pero aqu√≠ decidimos comprobar c√≥mo se puede evitar esto parcialmente mediante la codificaci√≥n de borrado, as√≠ como la deduplicaci√≥n y la compresi√≥n sobre la marcha.  Como resultado, la eficiencia del almacenamiento se acerca a las soluciones h√≠bridas, pero significativamente m√°s r√°pido con una sobrecarga m√≠nima. <br><br><h3>  C√≥mo probar </h3><br>  Para probar el rendimiento, utilizamos el software <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HCIBench v1.6.6</a> , que automatiza el proceso de creaci√≥n de muchas m√°quinas virtuales y luego compila los resultados.  La prueba de rendimiento en s√≠ se lleva a cabo utilizando el software Vdbench, uno de los programas de prueba de carga sint√©tica m√°s populares.  Iron estaba en las siguientes opciones de configuraci√≥n: <br><br><ol><li>  All-flash - 2 grupos de discos: SSD 1xNVMe Samsung PM1725 800 GB + 3xSATA </li><li>  SSD Toshiba HK4E 1.6 TB. </li><li>  Todo flash: 1 grupo de discos: SSD 1xNVMe Samsung PM1725 800 GB + SSD 6xSATA Toshiba HK4E 1.6 TB. </li><li>  All-flash - 1 grupo de discos: SSD 1xNVMe Samsung PM1725 800GB + 6xSATA SSD Toshiba HK4E 1.6 TB + Eficiencia espacial (deduplicaci√≥n y compresi√≥n). </li><li>  All-flash - 1 grupo de discos: SSD 1xNVMe Samsung PM1725 800GB + 6xSATA SSD Toshiba HK4E 1.6 TB + Codificaci√≥n de borrado (RAID 5/6). </li><li>  All-flash: 1 grupo de discos: SSD 1xNVMe Samsung PM1725 800GB + SSD 6xSATA Toshiba HK4E 1.6 TB + Codificaci√≥n de borrado (RAID 5/6) + Eficiencia espacial (deduplicaci√≥n y compresi√≥n). </li></ol><br>  Durante las pruebas, emulamos tres vol√∫menes diferentes de datos activos utilizados por las aplicaciones: 1 TB (250 GB por servidor), 2 TB (500 GB por servidor) y 4 TB (1 TB cada uno). <br><br>  Para cada configuraci√≥n, se realiz√≥ el mismo conjunto de pruebas con los siguientes perfiles de carga: <br><br><ol><li>  0 lectura / 100 escritura, aleatoriamente 50%, tama√±o de bloque - 4k. </li><li>  30 lecturas / 70 escrituras, 50% al azar, tama√±o de bloque - 4k. </li><li>  70 lecturas / 30 escrituras, 50% al azar, tama√±o de bloque - 4k. </li><li>  100 lectura / 0 escritura, aleatoriamente 50%, tama√±o de bloque - 4k. </li></ol><br>  Las opciones primera y cuarta eran necesarias para comprender c√≥mo se comportar√° el sistema bajo cargas m√°ximas y m√≠nimas.  Pero el segundo y el tercero est√°n lo m√°s cerca posible del caso de uso t√≠pico real: por ejemplo, 30 lectura / 70 escritura - VDI.  Esas cargas que encontr√© en la producci√≥n estaban muy cerca de ellos.  En el proceso, probamos la efectividad del mecanismo de gesti√≥n de datos vSAN. <br><br>  En general, el sistema demostr√≥ ser muy bueno.  Seg√∫n los resultados de la prueba, nos dimos cuenta de que podemos contar con un rendimiento en la regi√≥n de 20 mil IOPS por nodo.  Para tareas ordinarias y muy cargadas, estos son buenos indicadores que tienen en cuenta retrasos de 5 ms.  A continuaci√≥n di gr√°ficos con los resultados: <br><br><img src="https://habrastorage.org/webt/2w/l5/q3/2wl5q38sg-f3f0t5a9smo2nz9m0.png"><br><img src="https://habrastorage.org/webt/fn/zm/bf/fnzmbff0apti4eqq8dk9o3emrym.png"><br><img src="https://habrastorage.org/webt/lc/_c/yv/lc_cyvqfd1hwe15iggyo8lxxagg.png"><br><img src="https://habrastorage.org/webt/i6/at/yu/i6atyu48yarwd5d31qgcstxx2so.png"><br><img src="https://habrastorage.org/webt/ih/y3/8o/ihy38oa_emlvkdokq9ryzt3hhum.png"><br><br>  Tabla resumen con resultados de la prueba: <br><img src="https://habrastorage.org/webt/1v/xg/tm/1vxgtm0tpyiwqzujesrkzjcjlj8.jpeg"><br>  El color verde indica datos activos que est√°n completamente en cach√©. <br><br><h3>  Tolerancia a fallos </h3><br>  Cort√© un nodo tras otro, a pesar de la indignaci√≥n de la m√°quina, y observ√© la reacci√≥n del sistema en su conjunto.  Despu√©s de desconectar el primer nodo, no pas√≥ nada, excepto por una peque√±a ca√≠da en el rendimiento, en aproximadamente un 10-15%.  Apagu√© el segundo nodo: algunas de las m√°quinas virtuales se apagaron, pero el resto continu√≥ funcionando con una ligera degradaci√≥n en el rendimiento.  Supervivencia general satisfecha.  Reinici√© todos los nodos: el sistema lo pens√≥ un poco y se sincroniz√≥ nuevamente sin ning√∫n problema, todas las m√°quinas virtuales se iniciaron sin ning√∫n problema.  Como alguna vez en NUC.  La integridad de los datos tampoco se ve afectada, lo cual est√° muy satisfecho. <br><br><h3>  Impresiones generales </h3><br>  Software Defined Storage Systems (SDS) ya es una tecnolog√≠a madura. <br><br>  Hoy, uno de los principales factores de detenci√≥n que se interponen en el camino de la implementaci√≥n de vSAN es el costo bastante alto de una licencia en rublos.  Si crea la infraestructura desde cero, puede resultar que un sistema de almacenamiento tradicional en una configuraci√≥n similar costar√° aproximadamente lo mismo.  Pero ser√° menos flexible tanto en t√©rminos de administraci√≥n como de escala.  As√≠ que hoy, al elegir una soluci√≥n para almacenar datos de m√°quinas virtuales en la plataforma de virtualizaci√≥n vSphere, es muy bueno sopesar todos los pros y los contras de usar soluciones tradicionales e implementar tecnolog√≠a de almacenamiento definida por software. <br><br>  Puede crear una soluci√≥n en el mismo Ceph o GlusterFS, pero cuando trabaja con la infraestructura de VMware, la integraci√≥n de vSAN con componentes individuales es cautivadora, as√≠ como la facilidad de administraci√≥n, implementaci√≥n y un rendimiento significativamente mayor, especialmente en un peque√±o n√∫mero de nodos.  Por lo tanto, si ya est√° trabajando en la infraestructura de VMware, ser√° mucho m√°s f√°cil implementarlo.  Realmente haces una docena de clics y obtienes una SDS funcional de la caja. <br><br>  Otra motivaci√≥n para implementar vSAN es usarlo para sucursales, lo que le permite duplicar nodos en unidades remotas con un host testigo en el centro de datos.  Esta configuraci√≥n proporciona almacenamiento tolerante a fallas para m√°quinas virtuales con todas las tecnolog√≠as y rendimiento de vSAN en solo dos nodos.  Por cierto, para usar vSAN hay un esquema de licencia por separado para la cantidad de m√°quinas virtuales, que permite reducir los costos en comparaci√≥n con el esquema de licencia de vSAN tradicional para procesadores. <br><br>  Arquitect√≥nicamente, la soluci√≥n requiere 10 Gb Ethernet con dos enlaces por nodo para una distribuci√≥n de tr√°fico adecuada cuando se utiliza una soluci√≥n todo flash.  En comparaci√≥n con los sistemas tradicionales, ahorra espacio en el rack y ahorra en redes SAN al eliminar Fibre Channel en favor del est√°ndar Ethernet m√°s universal.  Para garantizar la tolerancia a fallas, se requieren al menos tres nodos; en dos r√©plicas de objetos con datos se almacenar√°n, y en el tercero, objetos testigos de estos datos, se resuelve el problema del cerebro dividido. <br><br>  Ahora algunas preguntas para ti: <br><br><ol><li>  Cuando decide un sistema de almacenamiento, ¬øqu√© criterios son m√°s importantes para usted? </li><li>  ¬øQu√© factores de detenci√≥n ve en el camino hacia la implementaci√≥n de sistemas de almacenamiento definidos por software? </li><li>  ¬øQu√© sistemas de almacenamiento definidos por software considera b√°sicamente como una opci√≥n para la implementaci√≥n? </li></ol><br><br>  UPD: olvid√© por completo escribir la configuraci√≥n del soporte y cargar los par√°metros: <br><br>  1. Descripci√≥n del hierro.  Por ejemplo: <br>  Servidores - 4xDellR630, cada uno: <br>  ‚Ä¢ 2xE5-2680v4 <br>  ‚Ä¢ 128 GB de RAM <br>  ‚Ä¢ 2x10GbE <br>  ‚Ä¢ 2x1GbE para gesti√≥n / red VM <br>  ‚Ä¢ Dell HBA330 <br>  Configuraci√≥n de almacenamiento n. ¬∞ 1: <br>  2xPM1725 800GB <br>  6xToshiba HK4E 1.6TB <br>  Configuraci√≥n de almacenamiento n. ¬∞ 2: <br>  1xPM1725 800GB <br>  6xToshiba HK4E 1.6TB <br><br>  2. Descripci√≥n de las versiones de software: <br>  vSphere 6.5U1 (7967591) (vSAN 6.6.1), es decir  parches despu√©s de Meltdown / Spectre <br>  vCenter 6.5U1g <br>  Los √∫ltimos controladores compatibles y FW de vSAN y ESXi para todos los componentes <br>  LACP para tr√°fico vSAN y vMotion (con recursos compartidos / l√≠mites / reserva de NIOC habilitados) <br>  Todas las dem√°s configuraciones son predeterminadas <br><br>  3. Par√°metros de carga: <br>  ‚Ä¢ HCIBench 1.6.6 <br>  ‚Ä¢ Oracle Vdbench - 05/04/06 <br>  ‚Ä¢ 40VM por cluster (10 por nodo) <br>  ‚Ä¢ 10 vmdk por VM <br>  ‚Ä¢ Tama√±o de 10 GB de vmdk y porcentaje de conjunto de carga de trabajo del 100/50/25% <br>  ‚Ä¢ Tiempo de calentamiento: 1800 segundos (0,5 horas), tiempo de prueba 3600 (1 hora) <br>  ‚Ä¢ 1 subprocesos por vmdk </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es414125/">https://habr.com/ru/post/es414125/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es414115/index.html">Qu√© es Lazy FP State Restore: una nueva vulnerabilidad descubierta en los procesadores Intel</a></li>
<li><a href="../es414117/index.html">Formato binario decimal mixto vs IEEE754</a></li>
<li><a href="../es414119/index.html">La oportunidad se durmi√≥ debido a una tormenta de arena en Marte. No est√° claro si el rover podr√° volver a funcionar</a></li>
<li><a href="../es414121/index.html">Dron aut√≥nomo de bricolaje con control de internet</a></li>
<li><a href="../es414123/index.html">Actualizamos los protocolos de texto a binario y luchamos contra el c√≥digo heredado en una reuni√≥n del Grupo de usuarios de C ++</a></li>
<li><a href="../es414127/index.html">Instale 3CX en un hosting por 2,99 euros / mes. en 10 minutos</a></li>
<li><a href="../es414129/index.html">M√°ster en Inform√°tica Te√≥rica en la Universidad Estatal de San Petersburgo</a></li>
<li><a href="../es414131/index.html">El efecto de la frecuencia de la se√±al en la energ√≠a de los enlaces de radio en el espacio libre</a></li>
<li><a href="../es414133/index.html">Dise√±o de juego de rompecabezas con el ejemplo de In The Shadows</a></li>
<li><a href="../es414135/index.html">C√≥mo portar un juego a PSVita mejor√≥ el rendimiento general</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>