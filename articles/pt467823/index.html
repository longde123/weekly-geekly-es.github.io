<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíõ üë©üèΩ‚Äçü§ù‚Äçüë®üèø üë≥üèø An√°lise: OOM no Kubernetes üëã üåÇ ü§ôüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Problemas no ambiente de trabalho s√£o sempre um desastre. Isso acontece quando voc√™ vai para casa, e o motivo sempre parece est√∫pido. Recentemente, fi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>An√°lise: OOM no Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/467823/"><p><img src="https://habrastorage.org/webt/m8/of/zv/m8ofzvtlhfh8mkwo-i04c5ren38.jpeg"></p><br><p>  Problemas no ambiente de trabalho s√£o sempre um desastre.  Isso acontece quando voc√™ vai para casa, e o motivo sempre parece est√∫pido.  Recentemente, ficamos sem mem√≥ria nos n√≥s do cluster Kubernetes, embora o n√≥ tenha se recuperado imediatamente, sem interrup√ß√µes vis√≠veis.  Hoje falaremos sobre este caso, sobre quais danos sofremos e como pretendemos evitar um problema semelhante no futuro. </p><br><h2 id="sluchay-pervyy">  Caso um </h2><br><h3 id="subbota-15-iyunya-2019-g-1712">  S√°bado, 15 de junho de 2019 17:12 </h3><a name="habracut"></a><br><p>  Blue Matador (sim, n√≥s nos monitoramos!) Gera um alerta: um evento em um dos n√≥s no cluster de produ√ß√£o Kubernetes - SystemOOM. </p><br><h3 id="1716">  17:16 </h3><br><p>  O Blue Matador gera um aviso: O EBS Burst Balance est√° baixo no volume raiz do n√≥ - aquele em que o evento SystemOOM ocorreu.  Embora um aviso sobre o Burst Balance tenha surgido ap√≥s uma notifica√ß√£o sobre o SystemOOM, dados reais do CloudWatch mostram que o Burst Balance atingiu um n√≠vel m√≠nimo √†s 17:02.  O motivo do atraso √© que as m√©tricas do EBS est√£o constantemente 10 a 15 minutos atrasadas e nosso sistema n√£o captura todos os eventos em tempo real. </p><br><p><img src="https://habrastorage.org/webt/xw/gb/u3/xwgbu3jxukrpz_qmakfhvsb5bm4.png"></p><br><h3 id="1718">  17:18 </h3><br><p>  Agora eu vi um alerta e um aviso.  Eu corro rapidamente os <strong>pods do kubectl get</strong> para ver o dano que sofremos e fico surpreso ao descobrir que os pods do aplicativo morreram exatamente 0. Eu <strong>executo os n√≥s principais do kubectl</strong> , mas essa verifica√ß√£o tamb√©m mostra que o n√≥ suspeito tem um problema de mem√≥ria;  √â verdade que ele j√° se recuperou e usa aproximadamente 60% de sua mem√≥ria.  S√£o 17h e a cerveja artesanal j√° est√° esquentando.  Depois de me certificar de que o n√≥ estava operacional e que nenhum pod estava danificado, decidi que ocorreu um acidente.  Se alguma coisa, eu vou descobrir na segunda-feira. </p><br><p>  Aqui est√° nossa correspond√™ncia com a esta√ß√£o de servi√ßo em Slack naquela noite: </p><br><p><img src="https://habrastorage.org/webt/ib/fp/f0/ibfpf05vnjd2uaukxr0rwjgcjj0.png"></p><br><h2 id="sluchay-vtoroy">  Caso dois </h2><br><h3 id="subbota-16-iyunya-2019-g-1802">  S√°bado, 16 de junho de 2019 18:02 </h3><br><p>  O Blue Matador gera um alerta: o evento j√° est√° em outro n√≥, tamb√©m SystemOOM.  Deve ter sido que a esta√ß√£o de servi√ßo naquele momento estava apenas olhando para a tela do smartphone, porque me escreveu e me levou a aceitar o evento imediatamente, eu mesmo n√£o consigo ligar o computador (√© hora de reinstalar o Windows novamente?).  E, novamente, tudo parece normal.  Nem um √∫nico pod √© eliminado e o n√≥ dificilmente consome 70% da mem√≥ria. </p><br><h3 id="1806">  18:06 </h3><br><p>  O Blue Matador gera um aviso novamente: EBS Burst Balance.  A segunda vez em um dia, o que significa que n√£o posso liberar esse problema nos freios.  Com o CloudWatch inalterado, o Burst Balance desviou-se da norma 2 horas ou mais antes do problema ser identificado. </p><br><h3 id="1811">  18:11 </h3><br><p>  Vou ao Datalog e olho os dados sobre o consumo de mem√≥ria.  Vejo isso logo antes do evento SystemOOM, o n√≥ suspeito realmente consumiu muita mem√≥ria.  A trilha leva √†s nossas vagens fluentes e sumol√≥gicas. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/s-/k-/aes-k-fwfx5qtsm7ehvhkgnpofe.png"></a> </p><br><p>  Voc√™ pode ver claramente um desvio acentuado no consumo de mem√≥ria, aproximadamente ao mesmo tempo em que os eventos do SystemOOM ocorreram.  Minha conclus√£o: foram esses pods que consumiram toda a mem√≥ria e, quando o SystemOOM aconteceu, o Kubernetes percebeu que esses pods poderiam ser eliminados e reiniciados para retornar a mem√≥ria necess√°ria sem afetar meus outros pods.  Muito bem, Kubernetes! </p><br><p>  Ent√£o, por que n√£o vi isso no s√°bado quando descobri quais pods foram reiniciados?  O fato √© que eu corro vagens fluentes e sumol√≥gicas em um espa√ßo para nome separado e com pressa n√£o pensei em investig√°-lo. </p><br><blockquote>  Conclus√£o 1: Ao procurar pods reiniciados, verifique todos os namespaces. </blockquote><p>  Depois de receber esses dados, calculei que no dia seguinte a mem√≥ria em outros n√≥s n√£o terminaria. No entanto, fui em frente e reiniciei todos os pods sumol√≥gicos para que eles come√ßassem a trabalhar com baixo consumo de mem√≥ria.  Na manh√£ seguinte, planejo explicar como integrar o trabalho sobre o problema em um plano para a semana e n√£o carregar muito no domingo √† noite. </p><br><h3 id="2300">  23:00 </h3><br><p>  Eu assisti a pr√≥xima s√©rie de "Black Mirror" (a prop√≥sito, gostei de Miley) e decidi ver como o cluster estava indo.  O consumo de mem√≥ria √© normal, portanto, fique √† vontade para deixar tudo como est√° para a noite. </p><br><h2 id="pochinka">  Fix </h2><br><p>  Na segunda-feira, arrumei tempo para esse problema.  N√£o d√≥i ca√ßar com ela todas as noites.  O que eu sei no momento: </p><br><ul><li>  Recipientes fluidos-sumol√≥gicos devoravam uma tonelada de mem√≥ria; </li><li>  O evento SystemOOM √© precedido por alta atividade de disco, mas n√£o sei qual. </li></ul><br><p>  A princ√≠pio, pensei que os recipientes fluol√≥gicos-sumol√≥gicos s√£o aceitos para comer mem√≥ria com um repentino fluxo de toras.  No entanto, ap√≥s verificar o Sumologic, vi que os logs eram usados ‚Äã‚Äãde maneira est√°vel e, ao mesmo tempo em que havia problemas, n√£o havia aumento nesses logs. </p><br><p>  Um pouco pesquisando, encontrei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esta tarefa no github</a> , que sugere ajustar algumas configura√ß√µes do Ruby - para reduzir o consumo de mem√≥ria.  Decidi tentar, adicionar uma vari√°vel de ambiente √† especifica√ß√£o do pod e execut√°-la: </p><br><pre><code class="plaintext hljs">env: - name: RUBY_GC_HEAP_OLDOBJECT_LIMIT_FACTOR value: "0.9"</code> </pre> <br><p>  Examinando o manifesto fluente-sumol√≥gico, notei que n√£o definia solicita√ß√µes e restri√ß√µes de recursos.  Estou come√ßando a suspeitar que a corre√ß√£o RUBY_GCP_HEAP far√° algum tipo de milagre, ent√£o agora faz sentido definir limites de consumo de mem√≥ria.  Mesmo que eu n√£o corrija o problema de mem√≥ria, ser√° pelo menos poss√≠vel limitar seu consumo a esse conjunto de pods.  Usando os <strong>principais pods do kubectl |</strong>  <strong>grep fluentd-sumologic</strong> , j√° sei quantos recursos solicitar: </p><br><pre> <code class="plaintext hljs">resources: requests: memory: "128Mi" cpu: "100m" limits: memory: "1024Mi" cpu: "250m"</code> </pre> <br><blockquote>  Conclus√£o 2: defina limites de recursos, especialmente para aplicativos de terceiros. </blockquote><br><h2 id="proverka-ispolneniya">  Verifica√ß√£o de execu√ß√£o </h2><br><p>  Ap√≥s alguns dias, confirmo que o m√©todo acima funciona.  O consumo de mem√≥ria foi est√°vel e - sem problemas com qualquer componente do Kubernetes, EC2 e EBS.  Agora est√° claro o qu√£o importante √© determinar solicita√ß√µes e restri√ß√µes de recursos para todos os pods que eu corro, e aqui est√° o que precisa ser feito: aplique uma combina√ß√£o de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">limites e</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cotas de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">recursos padr√£o</a> . </p><br><p>  O √∫ltimo mist√©rio n√£o resolvido √© o EBS Burst Balance, que coincidiu com o evento SystemOOM.  Eu sei que quando h√° pouca mem√≥ria, o sistema operacional usa o espa√ßo de troca para n√£o ficar completamente sem mem√≥ria.  Mas eu n√£o nasci ontem e sei que o Kubernetes nem ser√° iniciado nos servidores em que o arquivo de pagina√ß√£o est√° ativado.  Apenas querendo ter certeza, entrei nos meus n√≥s via SSH - para verificar se o arquivo de pagina√ß√£o estava ativado;  Usei tanto a mem√≥ria livre quanto a da √°rea de troca.  O arquivo n√£o foi ativado. </p><br><p>  E como a troca n√£o est√° funcionando, tenho mais pistas sobre o que causou o crescimento dos fluxos de E / S, e √© por isso que o n√≥ quase ficou sem mem√≥ria, n√£o.  Na verdade, tenho um palpite: o pr√≥prio pod fluente-sumol√≥gico estava escrevendo uma tonelada de mensagens de log nesse momento, possivelmente at√© uma mensagem de log relacionada √† configura√ß√£o do Ruby GC.  Tamb√©m √© poss√≠vel que existam outras fontes de Kubernetes ou mensagens de di√°rio que se tornem excessivamente produtivas quando a mem√≥ria se esgote, e eu as classifiquei enquanto configurava fluentemente.  Infelizmente, n√£o tenho mais acesso aos arquivos de log gravados imediatamente antes do mau funcionamento e agora n√£o posso me aprofundar mais. </p><br><blockquote>  Conclus√£o 3: Enquanto houver uma oportunidade, v√° mais fundo ao analisar as causas principais, qualquer que seja o problema. </blockquote><br><h2 id="zaklyuchenie">  Conclus√£o </h2><br><p>  E embora eu n√£o tenha chegado √† raiz das causas, tenho certeza de que elas n√£o s√£o necess√°rias para evitar as mesmas falhas no futuro.  Tempo √© dinheiro, mas estou ocupado h√° muito tempo e depois disso tamb√©m escrevi este post para voc√™.  E como usamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Blue Matador</a> , essas avarias s√£o tratadas com grande detalhe, por isso me permito liberar algo no freio, sem me distrair do projeto principal. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt467823/">https://habr.com/ru/post/pt467823/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt467811/index.html">Arquitetura e programa√ß√£o Fairchild Channel F</a></li>
<li><a href="../pt467813/index.html">Revis√£o das mudan√ßas na 17¬™ ordem do FSTEC</a></li>
<li><a href="../pt467815/index.html">A m√≠dia levantou p√¢nico de que "os endere√ßos IP est√£o acabando na R√∫ssia". Como realmente?</a></li>
<li><a href="../pt467817/index.html">Um pouco sobre os padr√µes de design generativo</a></li>
<li><a href="../pt467821/index.html">Simplifique e elimine o necess√°rio: Entrevista com John Romero, criador de Doom</a></li>
<li><a href="../pt467825/index.html">Algoritmos obrigat√≥rios de aprendizado de m√°quina</a></li>
<li><a href="../pt467827/index.html">Como fizemos nossa pequena Unidade a partir do zero</a></li>
<li><a href="../pt467831/index.html">O caminho espinhoso para a programa√ß√£o</a></li>
<li><a href="../pt467837/index.html">MCU ‚ÄúTerr√≠vel‚Äù de tr√™s centavos - uma breve vis√£o geral dos microcontroladores que custam menos de US $ 0,1</a></li>
<li><a href="../pt467841/index.html">Facilite a conclus√£o: Entrevista com John Romero, desenvolvedor do Doom</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>