<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òÉÔ∏è üìà üëéüèº El libro "Grok Deep Learning" üë©üèø‚Äçü§ù‚Äçüë©üèº üßòüèª üí•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola habrozhiteli! El libro sienta las bases para un mayor dominio de la tecnolog√≠a de aprendizaje profundo. Comienza con una descripci√≥n de los conce...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>El libro "Grok Deep Learning"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/464509/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/go/gm/1s/gogm1solwetphsozljuyzuizcbs.jpeg" align="left" alt="imagen"></a>  Hola habrozhiteli!  El libro sienta las bases para un mayor dominio de la tecnolog√≠a de aprendizaje profundo.  Comienza con una descripci√≥n de los conceptos b√°sicos de las redes neuronales y luego examina en detalle las capas adicionales de la arquitectura. <br><br>  El libro est√° especialmente escrito con la intenci√≥n de proporcionar el umbral de entrada m√°s bajo posible.  No necesita conocimientos de √°lgebra lineal, m√©todos num√©ricos, optimizaciones convexas e incluso aprendizaje autom√°tico.  Todo lo que se requiere para comprender el aprendizaje profundo se aclarar√° a medida que avanza. <br><br>  Le ofrecemos que se familiarice con el pasaje "¬øQu√© es un marco de aprendizaje profundo?" <br><a name="habracut"></a><br>  <b>Las buenas herramientas reducen los errores, aceleran el desarrollo y aumentan la velocidad de ejecuci√≥n.</b> <br><br>  Si lee mucho sobre el aprendizaje profundo, es probable que se encuentre con marcos tan conocidos como PyTorch, TensorFlow, Theano (recientemente declarado obsoleto), Keras, Lasagne y DyNet.  En los √∫ltimos a√±os, los marcos han evolucionado muy r√°pidamente, y a pesar de que todos estos marcos se distribuyen de forma gratuita y de c√≥digo abierto, cada uno de ellos tiene un esp√≠ritu de competencia y camarader√≠a. <br><br>  Hasta ahora, he evitado discutir marcos, porque, antes que nada, era extremadamente importante que entendiera lo que estaba sucediendo detr√°s de escena, implementando los algoritmos manualmente (usando solo la biblioteca NumPy).  Pero ahora comenzaremos a usar dichos marcos, porque las redes que vamos a entrenar, las redes con memoria a corto plazo a largo plazo (LSTM), son muy complejas, y el c√≥digo que las implementa usando NumPy es dif√≠cil de leer, usar y depurar (gradientes en este c√≥digo se encuentran en todas partes). <br><br>  Es esta complejidad que los marcos de aprendizaje profundo est√°n dise√±ados para abordar.  El marco de aprendizaje profundo puede reducir significativamente la complejidad del c√≥digo (as√≠ como reducir la cantidad de errores y aumentar la velocidad de desarrollo) y aumentar la velocidad de su ejecuci√≥n, especialmente si usa un procesador de gr√°ficos (GPU) para entrenar la red neuronal, que puede acelerar el proceso de 10 a 100 veces.  Por estas razones, los marcos se usan en casi todas partes en la comunidad de investigaci√≥n, y comprender las caracter√≠sticas de su trabajo ser√° √∫til en su carrera como usuario e investigador de aprendizaje profundo. <br><br>  Pero no nos limitaremos al marco de ning√∫n marco en particular, ya que esto le impedir√° aprender c√≥mo funcionan todos estos modelos complejos (como el LSTM).  En su lugar, crearemos nuestro propio marco ligero, siguiendo las √∫ltimas tendencias en el desarrollo de marcos.  Siguiendo este camino, sabr√° exactamente qu√© hacen los marcos cuando se crean arquitecturas complejas con su ayuda.  Adem√°s, un intento de crear su propio marco peque√±o lo ayudar√° a cambiar sin problemas al uso de marcos de aprendizaje profundo reales, porque ya conocer√° los principios de organizar una interfaz de programa (API) y su funcionalidad.  Este ejercicio fue muy √∫til para m√≠, y el conocimiento adquirido al crear mi propio marco result√≥ ser muy √∫til para depurar modelos problem√°ticos. <br><br>  ¬øC√≥mo simplifica el marco el c√≥digo?  Hablando de manera abstracta, elimina la necesidad de escribir el mismo c√≥digo una y otra vez.  Espec√≠ficamente, la caracter√≠stica m√°s conveniente del marco de aprendizaje profundo es el soporte para la retropropagaci√≥n autom√°tica y la optimizaci√≥n autom√°tica.  Esto le permite escribir solo c√≥digo de distribuci√≥n directa, y el marco se encargar√° autom√°ticamente de la distribuci√≥n posterior y la correcci√≥n de los pesos.  La mayor√≠a de los marcos modernos incluso simplifican el c√≥digo que implementa la distribuci√≥n directa al ofrecer interfaces de alto nivel para definir capas t√≠picas y funciones de p√©rdida. <br><br><h3>  Introducci√≥n a los tensores </h3><br>  <b>Los tensores son una forma abstracta de vectores y matrices.</b> <br><br>  Hasta este momento, usamos vectores y matrices como las estructuras principales.  Perm√≠tame recordarle que una matriz es una lista de vectores, y un vector es una lista de escalares (n√∫meros individuales).  Un tensor es una forma abstracta para representar listas anidadas de n√∫meros.  El vector es un tensor unidimensional.  La matriz es un tensor bidimensional, y las estructuras con una gran cantidad de dimensiones se denominan tensores n-dimensionales.  Entonces, comencemos a crear un nuevo marco de aprendizaje profundo definiendo un tipo base, que llamaremos Tensor: <br><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class"> </span></span>= np.array(data) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) print(x) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span>] y = x + x print(y) [<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>]</code> </pre> <br>  Esta es la primera versi√≥n de nuestra estructura de datos b√°sica.  Tenga en cuenta que almacena toda la informaci√≥n num√©rica en la matriz NumPy (self.data) y admite una sola operaci√≥n de tensor (adici√≥n).  Agregar operaciones adicionales no es nada dif√≠cil, solo agregue funciones adicionales con la funcionalidad correspondiente a la clase Tensor. <br><br><h3>  Introducci√≥n al c√°lculo autom√°tico de gradiente (autogrado) </h3><br>  <b>Anteriormente, realizamos la propagaci√≥n manual hacia atr√°s.</b>  <b>¬°Ahora hag√°moslo autom√°tico!</b> <br><br>  En el cap√≠tulo 4, presentamos derivados.  Desde entonces, hemos calculado manualmente estas derivadas en cada nueva red neuronal.  Perm√≠tame recordarle que esto se logra mediante el movimiento inverso a trav√©s de la red neuronal: primero, se calcula el gradiente en la salida de la red, luego este resultado se usa para calcular la derivada en el componente anterior, y as√≠ sucesivamente, hasta que se determinen los gradientes correctos para todos los pesos en la arquitectura.  Esta l√≥gica para calcular gradientes tambi√©n se puede agregar a la clase tensorial.  Lo siguiente muestra lo que ten√≠a en mente. <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">creators</span></span></span></span>=None, creation_op=None): self.data = np.array(data) self.creation_op = creation_op self.creators = creators self.grad = None def backward(self, grad): self.grad = grad <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(self.creation_op == <span class="hljs-string"><span class="hljs-string">"add"</span></span>): self.creators[<span class="hljs-number"><span class="hljs-number">0</span></span>].backward(grad) self.creators[<span class="hljs-number"><span class="hljs-number">1</span></span>].backward(grad) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data, creators=[self,other], creation_op=<span class="hljs-string"><span class="hljs-string">"add"</span></span>) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) y = Tensor([<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>]) z = x + y z.backward(Tensor(np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>])))</code> </pre> <br>  Este m√©todo presenta dos innovaciones.  Primero, cada tensor recibe dos nuevos atributos.  creadores es una lista de los tensores utilizados para crear el tensor actual (el valor predeterminado es Ninguno).  Es decir, si el tensor z se obtiene sumando los otros dos tensores, x e y, el atributo creador del tensor z contendr√° los tensores x e y.  creation_op es un atributo complementario que almacena las operaciones utilizadas en el proceso de creaci√≥n de este tensor.  Es decir, la instrucci√≥n z = x + y crear√° un gr√°fico computacional con tres nodos (x, y y z) y dos aristas (z -&gt; x y z -&gt; y).  Cada borde est√° firmado por la operaci√≥n desde creation_op, es decir, agregar.  Este gr√°fico ayudar√° a organizar la propagaci√≥n recursiva hacia atr√°s de gradientes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/4d/qz/fx4dqzrh6y62rtttfy2vrn1jley.png" alt="imagen"></div><br>  La primera innovaci√≥n en esta implementaci√≥n es la creaci√≥n autom√°tica de un gr√°fico durante cada operaci√≥n matem√°tica.  Si tomamos z y realizamos otra operaci√≥n, el gr√°fico continuar√° en una nueva variable que hace referencia a z. <br><br>  La segunda innovaci√≥n en esta versi√≥n de la clase Tensor es la capacidad de usar un gr√°fico para calcular gradientes.  Si llama al m√©todo z.backward (), pasar√° el gradiente para x e y, teniendo en cuenta la funci√≥n con la que se cre√≥ el tensor z (add).  Como se muestra en el ejemplo anterior, pasamos el vector de gradiente (np.array ([1,1,1,1,1]]) a z, y ese se aplica a sus padres.  Como probablemente recuerde del Cap√≠tulo 4, la propagaci√≥n hacia atr√°s a trav√©s de la suma significa aplicar la propagaci√≥n hacia atr√°s.  En este caso, solo tenemos que agregar un degradado a x e y, por lo que lo copiamos de z a x e y: <br><br><pre> <code class="javascript hljs">print(x.grad) print(y.grad) print(z.creators) print(z.creation_op) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>]), array([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])] add</code> </pre> <br>  La caracter√≠stica m√°s notable de esta forma de c√°lculo autom√°tico de gradiente es que funciona de forma recursiva: cada vector llama al m√©todo .backward () de todos sus padres de la lista self.creators: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bt/ye/if/btyeiflumrhlbprlzqtzjxwipcs.png" alt="imagen"></div><br>  ¬ªSe puede encontrar m√°s informaci√≥n sobre el libro en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web del editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Contenidos</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Extracto</a> <br><br>  Cup√≥n de 25% de descuento para vendedores ambulantes - <b>Deep Learning</b> <br>  Tras el pago de la versi√≥n en papel del libro, se env√≠a un libro electr√≥nico por correo electr√≥nico. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/464509/">https://habr.com/ru/post/464509/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../464491/index.html">Vivaldi 2.7 - Vida intensa en silencio</a></li>
<li><a href="../464495/index.html">Desarrollo del equipo y reflexi√≥n como comunicaci√≥n gerencial del l√≠der del equipo.</a></li>
<li><a href="../464497/index.html">JIRA como remedio para el insomnio y las crisis nerviosas</a></li>
<li><a href="../464499/index.html">"Mat. Modelo de Wall Street "o un intento de optimizar el costo de la infraestructura de TI en la nube</a></li>
<li><a href="../464503/index.html">Contrase√±a de Wi-Fi que coincida con aircrack-ng</a></li>
<li><a href="../464511/index.html">C√≥mo recopilar cohortes de usuarios en forma de gr√°ficos en Grafana [+ imagen acoplable con un ejemplo]</a></li>
<li><a href="../464513/index.html">Duffle: transformador de XD Design</a></li>
<li><a href="../464515/index.html">C√≥mo hacer correos electr√≥nicos y no desordenar: consejos pr√°cticos</a></li>
<li><a href="../464517/index.html">Nuevas tarjetas CUBA</a></li>
<li><a href="../464523/index.html">Sistemas de pago (PSP) para negocios de TI: jugamos a lo grande</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>