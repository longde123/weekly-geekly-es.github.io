<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛹🏼 ✊🏼 🥃 Kami sedang mengembangkan lingkungan untuk bekerja dengan layanan microser. Bagian 1 menginstal Kubernetes HA pada bare metal (Debian) 🌼 🧑 🐈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo para pembaca Habr! 


 Dengan publikasi ini, saya ingin memulai serangkaian artikel tentang penggelaran lingkungan orkestrasi penuh dengan wadah ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kami sedang mengembangkan lingkungan untuk bekerja dengan layanan microser. Bagian 1 menginstal Kubernetes HA pada bare metal (Debian)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462473/"><p><img src="https://habrastorage.org/webt/aq/dx/l3/aqdxl3a5akxfl3rh9b7bzc_kqji.jpeg"></p><br><h4>  Halo para pembaca Habr! </h4><br><p>  Dengan publikasi ini, saya ingin memulai serangkaian artikel tentang penggelaran lingkungan orkestrasi penuh dengan wadah Kubernetes, yang akan siap untuk operasi dan meluncurkan aplikasi. <br>  Saya ingin memberi tahu tidak hanya tentang cara menggunakan kluster Kubernetes, tetapi juga tentang cara mengkonfigurasi kluster setelah instalasi, cara menambahkan alat yang mudah digunakan dan add-on untuk menggunakan arsitektur microservice. </p><br><h2>  Siklus ini terdiri dari minimal empat artikel: </h2><br><ol><li>  Pada yang pertama dari mereka saya akan memberitahu Anda cara menginstal cluster kubernet gagal-aman pada besi kosong, cara menginstal dasbor standar dan mengkonfigurasi akses ke sana, cara menginstal pengendali masuknya. </li><li>  Pada artikel kedua, saya akan menunjukkan kepada Anda cara menggunakan cluster failover Ceph dan cara mulai menggunakan volume RBD di cluster Kubernetes kami.  Saya juga akan sedikit menyentuh jenis penyimpanan lain (penyimpanan) dan mempertimbangkan penyimpanan lokal secara lebih rinci.  Selain itu, saya akan memberi tahu Anda bagaimana mengatur penyimpanan S3 yang toleran terhadap kesalahan berdasarkan pada cluster CEPH yang dibuat </li><li>  Pada artikel ketiga, saya akan menjelaskan cara menggunakan MySql cluster failover di cluster Kubernetes kami, yaitu, Percona XtraDB Cluster di Kubernetes.  Dan juga saya akan menjelaskan semua masalah yang kami temui ketika kami memutuskan untuk mentransfer database ke kubernetes. </li><li>  Pada artikel keempat, saya akan mencoba menyatukan semuanya dan memberi tahu cara menggunakan dan menjalankan aplikasi yang akan menggunakan basis data dan volume ceph.  Saya akan memberi tahu Anda cara mengkonfigurasi pengendali masuknya untuk mengakses aplikasi kami dari luar dan layanan pemesanan sertifikat otomatis dari Let's Encrypt.  Cara lain adalah cara mempertahankan sertifikat ini secara otomatis.  Kami juga akan menyentuh RBAC dalam konteks akses ke panel kontrol.  Saya akan memberi tahu Anda secara singkat tentang Helm dan pemasangannya. <br>  Jika Anda tertarik dengan informasi dalam publikasi ini, selamat datang! <a name="habracut"></a></li></ol><br><h2>  Entri: </h2><br><p>  Untuk siapa artikel ini?  Pertama-tama, bagi mereka yang baru memulai perjalanan mereka dalam mempelajari Kubernet.  Juga, siklus ini akan bermanfaat bagi insinyur yang berpikir untuk pindah dari monolit ke layanan mikro.  Semuanya dijelaskan adalah pengalaman saya, termasuk yang diperoleh ketika menerjemahkan beberapa proyek dari monolit ke Kubernetes.  Ada kemungkinan bahwa beberapa bagian dari publikasi akan menarik bagi insinyur yang berpengalaman. </p><br><h4>  Yang tidak akan saya pertimbangkan secara rinci dalam seri publikasi ini: </h4><br><ul><li>  menjelaskan secara terperinci apa primitif kubernetes, seperti: pod, penyebaran, layanan, masuknya barang, dll. </li><li>  Saya akan menganggap CNI (Container Networking Interface) sangat dangkal, kami menggunakan callico karena itu solusi lain, saya hanya akan mendaftar. </li><li>  proses pembuatan gambar buruh pelabuhan. </li><li>  Proses CI \ CD.  (Mungkin publikasi terpisah, tetapi setelah seluruh siklus) </li><li>  helm;  cukup banyak yang telah ditulis tentang dia, saya hanya akan menyentuh pada proses menginstalnya di cluster dan mengatur klien. </li></ul><br><h4>  Apa yang ingin saya pertimbangkan secara rinci: </h4><br><ul><li>  Proses penyebaran cluster Kubernetes langkah demi langkah.  Saya akan menggunakan kubeadm.  Tetapi pada saat yang sama, saya akan mengambil langkah-demi-langkah rinci pada proses menginstal cluster pada logam kosong, berbagai jenis instalasi ETCD, dan mengkonfigurasi file untuk kube admina.  Saya akan mencoba mengklarifikasi semua opsi penyeimbang untuk pengontrol Ingress dan perbedaan dalam berbagai skema akses worknodes ke server api. <br>  Saya tahu bahwa hari ini ada banyak alat hebat untuk menyebarkan kubernet, misalnya kubespray atau peternak yang sama.  Mungkin akan lebih nyaman bagi seseorang untuk menggunakannya.  Tapi, saya pikir, ada banyak insinyur yang ingin mempertimbangkan masalah ini secara lebih rinci. </li><li>  Terminologi CEPH dan pemasangan langkah-demi-langkah klaster CEPH, serta petunjuk langkah-demi-langkah tentang menghubungkan penyimpanan ceph ke kluster yang dibuat oleh Kubernetes. </li><li>  penyimpanan lokal, koneksi ke cluster kubernetes, serta perbedaan dari koneksi seperti hostpath, dll. </li><li>  kubernetes operator dan penyebaran Percona XtraDB Cluster dengan bantuan operator, serta mencoba untuk berbicara tentang pro dan kontra dari solusi seperti itu setelah enam bulan pengalaman dalam produksi.  Dan juga saya akan membagikan sedikit rencana untuk menyelesaikan operator dari percona. </li></ul><br><h2>  Daftar isi: </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Daftar host, sumber daya host, OS dan versi perangkat lunak</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes cluster diagram HA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sebelum Anda mulai atau sebelum Anda mulai</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Isi file create-config.sh</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pembaruan kernel OS</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mempersiapkan Node Memasang Kubelet, Kubectl, Kubeadm, dan Docker</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Menginstal ETCD (berbagai opsi)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Meluncurkan wisaya kubernet pertama</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Instalasi Callico CNI</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Peluncuran penyihir kubernetes kedua dan ketiga</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tambahkan node pekerja ke cluster</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Instal haproxy pada node pekerja untuk HA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Menginstal Ingress Controller</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pasang UI Web (Dasbor)</a> </li></ol><br><a name="vm"></a><br><h2>  Daftar dan tujuan host </h2><br><p>  Semua node dari kluster saya akan ditempatkan pada mesin virtual dengan sistem peregangan Debian 9 yang sudah diinstal dengan kernel 4.19.0-0.bpo.5-amd64.  Untuk virtualisasi, saya menggunakan Proxmox VE. </p><br><h4>  Tabel VM dan karakteristik kinerjanya: </h4><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nama</b> </th><th>  <b>Alamat IP</b> </th><th>  <b>Komentar</b> </th><th>  <b>CPU</b> </th><th>  <b>Nona</b> </th><th>  <b>DISK1</b> </th><th>  <b>DISK2</b> </th></tr><tr><td>  master01 </td><td>  10.73.71.25 </td><td>  master node </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  --- </td></tr><tr><td>  master02 </td><td>  10.73.71.26 </td><td>  master node </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  --- </td></tr><tr><td>  master03 </td><td>  10.73.71.27 </td><td>  master node </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  --- </td></tr><tr><td>  worknode01 </td><td>  10.73.75.241 </td><td>  simpul kerja </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  SSD </td></tr><tr><td>  worknode02 </td><td>  10.73.75.242 </td><td>  simpul kerja </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  SSD </td></tr><tr><td>  worknode03 </td><td>  10.73.75.243 </td><td>  simpul kerja </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  SSD </td></tr></tbody></table></div><br><p>  Tidak perlu bahwa Anda harus memiliki konfigurasi mesin seperti itu, tetapi tetap saya menyarankan Anda untuk mematuhi rekomendasi dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi</a> , dan untuk master, tingkatkan jumlah RAM minimal 4GB.  Ke depan, saya akan mengatakan bahwa dengan jumlah yang lebih kecil, saya menangkap gangguan dalam karya CNI Callico <br>  Ceph juga cukup lincah dalam hal kinerja memori dan disk. <br>  Instalasi produksi kami berfungsi tanpa virtualisasi bare-metal, tapi saya tahu banyak contoh di mana mesin virtual dengan sumber daya yang cukup sudah cukup.  Itu semua tergantung pada kebutuhan dan beban kerja Anda. </p><br><h2>  Daftar dan versi perangkat lunak </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nama</b> </th><th>  <b>Versi</b> </th></tr><tr><td>  Kubernetes </td><td>  1.15.1 </td></tr><tr><td>  Docker </td><td>  19.3.1 </td></tr></tbody></table></div><br><p>  Mulai dari versi 1.14, Kubeadm berhenti mendukung versi API v1alpha3 dan sepenuhnya beralih ke versi API v1beta1, yang akan didukungnya dalam waktu dekat, jadi dalam artikel ini saya hanya akan berbicara tentang v1beta1. <br>  Jadi, kami yakin Anda telah menyiapkan mesin untuk cluster kubernetes.  Mereka semua dapat diakses satu sama lain melalui jaringan, memiliki "koneksi Internet" ke Internet, dan sistem operasi "bersih" diinstal pada mereka. <br>  Untuk setiap langkah instalasi, saya akan mengklarifikasi pada mesin mana perintah atau blok perintah dijalankan.  Jalankan semua perintah sebagai root, kecuali ditentukan lain. <br>  Semua file konfigurasi, serta skrip untuk persiapannya, tersedia untuk diunduh di <a href="">github</a> saya <br>  Jadi mari kita mulai. </p><br><a name="ha-image"></a><br><h2>  Kubernetes cluster diagram HA </h2><br><p><img src="https://habrastorage.org/webt/ch/mx/pg/chmxpgzdyk0bvxwx7-6lawqfmvo.jpeg"><br>  Diagram perkiraan cluster HA.  Seniman dari saya ini begitu-begitu, jujur, tetapi saya akan mencoba untuk menjelaskan secara singkat dan cukup sederhana, tanpa secara khusus menggali teori. <br>  Jadi, cluster kami akan terdiri dari tiga node master dan tiga node pekerja.  Pada setiap simpul utama kubernetes, dlld (panah hijau pada diagram) dan bagian layanan kubernetes akan bekerja untuk kita;  sebut saja mereka secara umum - kubeapi. <br>  Melalui cluster master etcd, node bertukar status cluster kubernet.  Saya akan menunjukkan alamat yang sama dengan titik masuk pengendali masuk untuk lalu lintas eksternal (panah merah pada diagram) <br>  Pada node pekerja, kubelet bekerja untuk kita, yang berkomunikasi dengan server api kubernetes melalui haproxy yang diinstal secara lokal pada setiap node pekerja.  Sebagai alamat server api untuk kubelet, saya akan menggunakan localhost 127.0.0.1:6443, dan haproxy pada roundrobin akan menyebarkan permintaan pada tiga node master, juga akan memeriksa ketersediaan node master.  Skema ini akan memungkinkan kita untuk membuat HA, dan jika terjadi kegagalan salah satu node master, node pekerja akan diam-diam mengirim permintaan ke dua node master yang tersisa. </p><br><a name="begin"></a><br><h2>  Sebelum Anda mulai </h2><br><p>  Sebelum mulai bekerja pada setiap node cluster, kami akan menyediakan paket-paket yang kami perlukan untuk bekerja: </p><br><pre><code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y curl apt-transport-https git</code> </pre> <br><p>  Pada node master, salin repositori dengan templat config </p><br><pre> <code class="plaintext hljs">sudo -i git clone https://github.com/rjeka/kubernetes-ceph-percona.git</code> </pre> <br><p>  Periksa bahwa alamat ip host pada penyihir cocok dengan yang di mana server kubernetes akan mendengarkan </p><br><pre> <code class="plaintext hljs">hostname &amp;&amp; hostname -i master01 10.73.71.25</code> </pre> <br><p>  dan untuk semua node master. </p><br><p>  Pastikan untuk menonaktifkan SWAP, jika tidak kubeadm akan menimbulkan kesalahan </p><br><pre> <code class="plaintext hljs">[ERROR Swap]: running with swap on is not supported. Please disable swap</code> </pre> <br><p>  Anda dapat menonaktifkan perintah </p><br><pre> <code class="plaintext hljs">swapoff -a</code> </pre> <br><p>  Ingatlah untuk berkomentar di / etc / fstab </p><br><a name="create-config"></a><br><h2>  Isi file create-config.sh </h2><br><p>  Untuk secara otomatis mengisi konfigurasi yang diperlukan untuk menginstal cluster kubernetes, saya mengunggah skrip kecil create-config.sh.  Anda harus mengisinya secara harfiah 8 baris.  Tunjukkan alamat IP dan nama host tuan Anda.  Dan juga tentukan etcd tocken, Anda tidak dapat mengubahnya.  Saya akan memberikan di bawah bagian skrip di mana Anda perlu melakukan perubahan. </p><br><pre> <code class="plaintext hljs">#!/bin/bash ####################################### # all masters settings below must be same ####################################### # master01 ip address export K8SHA_IP1=10.73.71.25 # master02 ip address export K8SHA_IP2=10.73.71.26 # master03 ip address export K8SHA_IP3=10.73.71.27 # master01 hostname export K8SHA_HOSTNAME1=master01 # master02 hostname export K8SHA_HOSTNAME2=master02 # master03 hostname export K8SHA_HOSTNAME3=master03 #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd #etcd version export ETCD_VERSION="v3.3.10"</code> </pre><br><a name="kernel"></a><br><h2>  Pembaruan kernel OS </h2><br><p>  Langkah ini opsional, karena kernel perlu diperbarui dari port belakang, dan Anda melakukan ini atas risiko dan risiko Anda sendiri.  Mungkin Anda tidak akan pernah menghadapi masalah ini, dan jika Anda melakukannya, Anda dapat memperbarui kernel bahkan setelah menggunakan kubernet.  Secara umum, Anda memutuskan. <br>  Pembaruan kernel diperlukan untuk memperbaiki bug buruh pelabuhan yang lama, yang diperbaiki hanya di kernel linux versi 4.18.  Anda dapat membaca lebih lanjut tentang bug ini di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Sebuah bug diekspresikan dalam antarmuka jaringan periodik pada node kubernetes dengan kesalahan: </p><br><pre> <code class="plaintext hljs">waiting for eth0 to become free. Usage count = 1</code> </pre> <br><p>  Setelah menginstal OS, saya memiliki versi kernel 4.9 </p><br><pre> <code class="bash hljs">uname -a Linux master01 4.9.0-7-amd64 <span class="hljs-comment"><span class="hljs-comment">#1 SMP Debian 4.9.110-3+deb9u2 (2018-08-13) x86_64 GNU/Linux</span></span></code> </pre> <br><p>  Pada setiap mesin untuk kubernet kita jalankan <br>  Langkah # 1 <br>  Tambahkan port kembali ke daftar sumber </p><br><pre> <code class="plaintext hljs">echo deb http://ftp.debian.org/debian stretch-backports main &gt; /etc/apt/sources.list apt-get update apt-cache policy linux-compiler-gcc-6-x86</code> </pre> <br><p>  Langkah nomor 2 <br>  Instalasi Paket </p><br><pre> <code class="plaintext hljs">apt install -y -t stretch-backports linux-image-amd64 linux-headers-amd64</code> </pre> <br><p>  Langkah nomor 3 <br>  Mulai ulang </p><br><pre> <code class="plaintext hljs">reboot</code> </pre> <br><p>  Periksa apakah semuanya baik-baik saja </p><br><pre> <code class="plaintext hljs">uname -a Linux master01 4.19.0-0.bpo.5-amd64 #1 SMP Debian 4.19.37-4~bpo9+1 (2019-06-19) x86_64 GNU/Linux</code> </pre><br><a name="kubelet"></a><br><h2>  Mempersiapkan Node Memasang Kubelet, Kubectl, Kubeadm, dan Docker </h2><br><h4>  Instal Kubelet, Kubectl, Kubeadm </h4><br><p>  Kami memakai semua node cluster, sesuai dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi kubernetes</a> </p><br><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Instal Docker </h4><br><p>  Instal buruh pelabuhan sesuai dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">instruksi dari dokumentasi</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Instalasi Menginstal Kubelet, Kubectl, Kubeadm dan buruh pelabuhan menggunakan mungkin</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Di grup master, daftarkan master ip. <br>  Dalam kelompok pekerja, tuliskan ip dari simpul yang berfungsi. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div><br><p>  Jika karena alasan tertentu Anda tidak ingin menggunakan buruh pelabuhan, Anda dapat menggunakan CRI apa pun.  Anda dapat membacanya, misalnya di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , tetapi topik ini berada di luar cakupan artikel ini. </p><br><a name="etcd"></a><br><h2>  Instalasi ETCD </h2><br><p>  Saya tidak akan masuk jauh ke dalam teori, singkatnya: etcd adalah penyimpanan nilai kunci didistribusikan open source.  etcd ditulis dalam GO dan digunakan di kubernetes pada kenyataannya, sebagai basis data untuk menyimpan status cluster.  Untuk ulasan yang lebih rinci, lihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi kubernetes</a> . <br>  etcd dapat diinstal dengan berbagai cara.  Anda dapat menginstalnya secara lokal dan menjalankannya sebagai daemon, Anda dapat menjalankannya dalam wadah buruh pelabuhan, Anda dapat menginstalnya bahkan sebagai bagian bawah kubernetes.  Anda dapat menginstalnya dengan tangan, atau Anda dapat menginstalnya menggunakan kubeadm (saya belum mencoba metode ini).  Dapat diinstal pada mesin cluster atau server individual. <br>  Saya akan menginstal etcd secara lokal pada node master dan dijalankan sebagai daemon melalui systemd, serta mempertimbangkan untuk menginstal di docker.  Saya menggunakan etcd tanpa TLS, jika Anda memerlukan TLS lihat dokumentasi etcd atau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kubernetes itu sendiri</a> <br>  Juga di github saya akan dimungkinkan-playbook untuk menginstal dll dengan peluncuran melalui systemd. </p><br><h4>  Opsi nomor 1 <br>  Instal secara lokal, jalankan melalui systemd </h4><br><p>  Pada semua master: (pada node cluster yang berfungsi, langkah ini tidak perlu) <br>  Langkah # 1 <br>  Unduh dan buka arsipnya dengan etcd: </p><br><pre> <code class="plaintext hljs">mkdir archives cd archives export etcdVersion=v3.3.10 wget https://github.com/coreos/etcd/releases/download/$etcdVersion/etcd-$etcdVersion-linux-amd64.tar.gz tar -xvf etcd-$etcdVersion-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1</code> </pre> <br><p>  Langkah nomor 2 <br>  Buat file konfigurasi untuk ETCD </p><br><pre> <code class="plaintext hljs">cd .. ./create-config.sh etcd</code> </pre> <br><p>  Script menerima nilai etcd sebagai input, dan menghasilkan file config di direktori etcd.  Setelah skrip berjalan, file konfigurasi yang sudah selesai akan ditemukan di direktori etcd. <br>  Untuk semua konfigurasi lainnya, skrip bekerja dengan prinsip yang sama.  Dibutuhkan beberapa input dan membuat konfigurasi di direktori tertentu. </p><br><p>  Langkah nomor 3 <br>  Kami memulai gugus etcd dan memeriksa kinerjanya </p><br><pre> <code class="plaintext hljs">systemctl start etcd</code> </pre> <br><p>  Memeriksa kinerja daemon </p><br><pre> <code class="plaintext hljs">systemctl status etcd ● etcd.service - etcd Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled) Active: active (running) since Sun 2019-07-07 02:34:28 MSK; 4min 46s ago Docs: https://github.com/coreos/etcd Main PID: 7471 (etcd) Tasks: 14 (limit: 4915) CGroup: /system.slice/etcd.service └─7471 /usr/local/bin/etcd --name master01 --data-dir /var/lib/etcd --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --advertise-client-urls http://10.73.71.25:2379,http://10.73.71. Jul 07 02:34:28 master01 etcd[7471]: b11e73358a31b109 [logterm: 1, index: 3, vote: 0] cast MsgVote for f67dd9aaa8a44ab9 [logterm: 2, index: 5] at term 554 Jul 07 02:34:28 master01 etcd[7471]: raft.node: b11e73358a31b109 elected leader f67dd9aaa8a44ab9 at term 554 Jul 07 02:34:28 master01 etcd[7471]: published {Name:master01 ClientURLs:[http://10.73.71.25:2379 http://10.73.71.25:4001]} to cluster d0979b2e7159c1e6 Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:4001, this is strongly discouraged! Jul 07 02:34:28 master01 systemd[1]: Started etcd. Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:2379, this is strongly discouraged! Jul 07 02:34:28 master01 etcd[7471]: set the initial cluster version to 3.3 Jul 07 02:34:28 master01 etcd[7471]: enabled capabilities for version 3.3 lines 1-19</code> </pre> <br><p>  Dan kesehatan cluster itu sendiri: </p><br><pre> <code class="plaintext hljs">etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy etcdctl member list 61db137992290fc: name=master03 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=master01 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=false f67dd9aaa8a44ab9: name=master02 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=true</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Instal etcd secara lokal dengan ansible, jalankan melalui systemd</b> <div class="spoiler_text"><p>  Dengan github, kami akan mengkloning repositori dengan kode ke mesin tempat Anda menjalankan playbook.  Mesin ini harus memiliki akses ssh pada kunci master cluster masa depan kita. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Di grup master, daftarkan master ip. <br>  etcd_version adalah versi etcd.  Anda dapat melihatnya di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">halaman etcd di github</a> .  Pada saat penulisan, ada versi v3.3.13 Saya menggunakan v3.3.10. <br>  etcdToken - Anda dapat meninggalkan yang sama, atau menghasilkan milik Anda sendiri. <br>  Jalankan tim playbook </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sudo c  ansible-playbook -i hosts.ini -l masters etcd.yaml -K BECOME password: &lt;sudo &gt; # sudo  ansible-playbook -i hosts.ini -l masters etcd.yaml</span></span></code> </pre> </div></div><br><p>  Jika Anda ingin menjalankan etcd di buruh pelabuhan, maka ada instruksi di bawah spoiler. </p><br><div class="spoiler">  <b class="spoiler_title">Instal dll dengan komposisi buruh pelabuhan, luncurkan di buruh pelabuhan</b> <div class="spoiler_text"><p>  Perintah-perintah ini harus dijalankan pada semua node master cluster. <br>  Dengan github, kami mengkloning repositori dengan kode </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona</code> </pre> <br><p>  etcd_version adalah versi etcd.  Anda dapat melihatnya di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">halaman etcd di github</a> .  Pada saat penulisan, ada versi v3.3.13 Saya menggunakan v3.3.10. <br>  etcdToken - Anda dapat meninggalkan yang sama, atau menghasilkan milik Anda sendiri. </p><br><p>  Kami membuat komposisi buruh pelabuhan </p><br><pre> <code class="plaintext hljs">apt-get install -y docker-compose</code> </pre> <br><p>  Kami menghasilkan konfigurasi </p><br><pre> <code class="plaintext hljs">./create-config.sh docker</code> </pre> <br><p>  Jalankan instalasi cluster etcd di buruh pelabuhan </p><br><pre> <code class="plaintext hljs">docker-compose --file etcd-docker/docker-compose.yaml up -d</code> </pre> <br><p>  Periksa apakah wadah sudah habis </p><br><pre> <code class="plaintext hljs">docker ps</code> </pre> <br><p>  Dan status cluster, dll </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl member list 61db137992290fc: name=etcd3 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=etcd1 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=true f67dd9aaa8a44ab9: name=etcd2 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=false</code> </pre> <br><p>  Jika ada yang salah </p><br><pre> <code class="plaintext hljs">docker logs etcd</code> </pre> </div></div><br><a name="master-one"></a><br><h2>  Meluncurkan wisaya kubernet pertama </h2><br><p>  Pertama-tama, kita perlu membuat konfigurasi untuk kubeadmin </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Kami membongkar konfigurasi untuk kubeadm</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.73.71.25 #    API- --- apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable #      apiServer: #    kubeadm   certSANs: - 127.0.0.1 - 10.73.71.25 - 10.73.71.26 - 10.73.71.27 controlPlaneEndpoint: 10.73.71.25 #     etcd: #  etc external: endpoints: - http://10.73.71.25:2379 - http://10.73.71.26:2379 - http://10.73.71.27:2379 networking: podSubnet: 192.168.0.0/16 #   ,   CNI  .</code> </pre> <br><p>  Anda dapat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">membaca</a> tentang subnet CNI dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi kubernetes.</a> <br>  Ini adalah konfigurasi minimum yang berfungsi.  Untuk cluster dengan tiga penyihir, Anda bisa mengubahnya ke konfigurasi cluster Anda.  Misalnya, jika Anda ingin menggunakan 2 penyihir, maka cukup tentukan dua alamat di certSANs. <br>  Semua parameter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">konfigurasi</a> dapat ditemukan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">deskripsi API kubeadm</a> . </p></div></div><br><p>  Kami memulai tuan pertama </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><p>  Jika kubeadm bekerja tanpa kesalahan, maka pada output kita mendapatkan kira-kira output berikut: </p><br><pre> <code class="plaintext hljs">You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3 \ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><a name="callica"></a><br><h2>  Instalasi Calico CNI </h2><br><p>  Waktunya telah tiba untuk membangun jaringan di mana pod kami akan bekerja.  Saya menggunakan belacu, dan kami akan menaruhnya. <br>  Dan sebagai permulaan, konfigurasikan akses untuk kubelet.  Kami menjalankan semua perintah pada master01 <br>  Jika Anda menjalankan sebagai root </p><br><pre> <code class="plaintext hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code> </pre> <br><p>  Jika dari bawah pengguna yang sederhana </p><br><pre> <code class="plaintext hljs">mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code> </pre> <br><p>  Anda juga dapat mengelola cluster dari laptop Anda atau mesin lokal apa pun.  Untuk melakukan ini, salin file /etc/kubernetes/admin.conf ke laptop Anda atau komputer lain di $ HOME / .kube / config </p><br><p>  Kami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menempatkan</a> CNI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sesuai dengan dokumentasi Kubernetes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code> </pre> <br><p>  Kami menunggu sampai semua polong naik </p><br><pre> <code class="plaintext hljs">watch -n1 kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-59f54d6bbc-psr2z 1/1 Running 0 96s kube-system calico-node-hm49z 1/1 Running 0 96s kube-system coredns-5c98db65d4-svcx9 1/1 Running 0 77m kube-system coredns-5c98db65d4-zdlb8 1/1 Running 0 77m kube-system kube-apiserver-master01 1/1 Running 0 76m kube-system kube-controller-manager-master01 1/1 Running 0 77m kube-system kube-proxy-nkdqn 1/1 Running 0 77m kube-system kube-scheduler-master01 1/1 Running 0 77m</code> </pre> <br><a name="mastes-other"></a><br><h2>  Peluncuran penyihir kubernetes kedua dan ketiga </h2><br><p>  Sebelum memulai master02 dan master03, Anda perlu menyalin sertifikat dengan master01 yang dihasilkan kubus saat membuat cluster.  Saya akan menyalin melalui scp <br>  Pada master01 </p><br><pre> <code class="plaintext hljs">export master02=10.73.71.26 export master03=10.73.71.27 scp -r /etc/kubernetes/pki $master02:/etc/kubernetes/ scp -r /etc/kubernetes/pki $master03:/etc/kubernetes/</code> </pre> <br><p>  Pada master02 dan master03 <br>  Buat config untuk kubeadm </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><p>  Dan tambahkan master02 dan master03 ke cluster </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Gangguan di beberapa antarmuka jaringan !!!!</b> <div class="spoiler_text"><p>  Dalam produksi, saya menggunakan kubernetes v1.13.5 dan calico v3.3.  Dan saya tidak punya gangguan seperti itu. <br>  Tetapi ketika menyiapkan artikel dan menggunakan versi stabil (pada saat penulisan ini adalah v1.15.1 kubernetes dan versi 3.8 callico) saya mengalami masalah yang diekspresikan dalam kesalahan awal CNI </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# kubectl get pods -A -w NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-658558ddf8-t6gfs 0/1 ContainerCreating 0 11s kube-system calico-node-7td8g 1/1 Running 0 11s kube-system calico-node-dthg5 0/1 CrashLoopBackOff 1 11s kube-system calico-node-tvhkq 0/1 CrashLoopBackOff 1 11s</code> </pre> <br><p>  Ini adalah kesalahan set calico daemon ketika server memiliki beberapa antarmuka jaringan <br>  Di githab ada masalah pada kesalahan ini <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://github.com/projectcalico/calico/issues/2720</a> <br>  Itu diselesaikan dengan mengedit daemon set calico-node dan menambahkan parameter IP_AUTODETECTION_METHOD ke env </p><br><pre> <code class="plaintext hljs">kubectl edit -n kube-system ds calico-node</code> </pre> <br><p>  tambahkan parameter IP_AUTODETECTION_METHOD dengan nama antarmuka Anda tempat wizard bekerja;  dalam kasus saya itu EN19 </p><br><pre> <code class="plaintext hljs">- name: IP_AUTODETECTION_METHOD value: ens19</code> </pre> <br><p><img src="https://habrastorage.org/webt/xe/pq/7h/xepq7hbpjwhgowfk0hkc6-ryw8a.png"><br>  Periksa bahwa semua node di cluster sudah aktif dan berjalan </p><br><pre> <code class="plaintext hljs"># kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 28m v1.15.1 master02 Ready master 26m v1.15.1 master03 Ready master 18m v1.15.1</code> </pre> <br><p>  Dan apa yang disebut calica hidup </p><br><pre> <code class="plaintext hljs"># kubectl get pods -A -o wide | grep calico kube-system calico-kube-controllers-59f54d6bbc-5lxgn 1/1 Running 0 27m kube-system calico-node-fngpz 1/1 Running 1 24m kube-system calico-node-gk7rh 1/1 Running 0 8m55s kube-system calico-node-w4xtt 1/1 Running 0 25m</code> </pre> </div></div><br><a name="worknodes"></a><br><h2>  Tambahkan node pekerja ke cluster </h2><br><p>  Saat ini, kami memiliki cluster di mana tiga node master berjalan.  Tetapi master nodes adalah mesin yang menjalankan api, scheduler, dan layanan lain dari cluster kubernet.  Agar kami dapat menjalankan pod kami, kami membutuhkan apa yang disebut node pekerja. <br>  Jika sumber daya Anda terbatas, Anda dapat menjalankan pod pada node master, tetapi saya pribadi tidak menyarankan untuk melakukan ini. </p><br><div class="spoiler">  <b class="spoiler_title">Menjalankan perapian di masternode</b> <div class="spoiler_text"><p>  Untuk memungkinkan peluncuran perapian pada node master, jalankan perintah berikut pada salah satu penyihir </p><br><pre> <code class="plaintext hljs">kubectl taint nodes --all node-role.kubernetes.io/master-</code> </pre> </div></div><br><p>  Instal node kubelet, kubeadm, kubectl dan docker pada pekerja seperti pada node master </p><br><div class="spoiler">  <b class="spoiler_title">Instal kubelet, kubeadm, kubectl dan docker</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Instal Docker </h4><br><p>  Instal buruh pelabuhan sesuai dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">instruksi dari dokumentasi</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Instalasi Menginstal Kubelet, Kubectl, Kubeadm dan buruh pelabuhan menggunakan mungkin</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Di grup master, daftarkan master ip. <br>  Dalam kelompok pekerja, tuliskan ip dari simpul yang berfungsi. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div></div></div><br><p>  Sekarang saatnya untuk kembali ke baris yang dihasilkan kubeadm ketika kita menginstal master node. <br>  Dia terlihat seperti ini untukku. </p><br><pre> <code class="plaintext hljs">kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><p>  Diperlukan untuk mengeksekusi perintah ini pada setiap node pekerja. <br>  Jika Anda belum menulis token, maka Anda dapat membuat yang baru </p><br><pre> <code class="plaintext hljs">kubeadm token create --print-join-command --ttl=0</code> </pre> <br><p>  Setelah kubeadm bekerja, node baru Anda dimasukkan ke dalam cluster dan siap untuk bekerja </p><br><pre> <code class="plaintext hljs">This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code> </pre> <br><p>  Sekarang mari kita lihat hasilnya </p><br><pre> <code class="plaintext hljs">root@master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 10d v1.15.1 master02 Ready master 10d v1.15.1 master03 Ready master 10d v1.15.1 worknode01 Ready &lt;none&gt; 5m44s v1.15.1 worknode02 Ready &lt;none&gt; 59s v1.15.1 worknode03 Ready &lt;none&gt; 51s v1.15.1</code> </pre> <br><a name="haproxy"></a><br><h2>  Instal haproxy di worknodes </h2><br><p>  Sekarang kami memiliki cluster yang berfungsi dengan tiga node master dan tiga node pekerja. <br>  Masalahnya adalah bahwa sekarang node pekerja kami tidak memiliki mode HA. <br>  Jika Anda melihat file konfigurasi kubelet, kita akan melihat bahwa node pekerja kami hanya mengakses satu dari tiga node master. </p><br><pre> <code class="plaintext hljs">root@worknode01:~# cat /etc/kubernetes/kubelet.conf | grep server: server: https://10.73.71.27:6443</code> </pre> <br><p>  Dalam kasus saya, ini master03.  Dengan konfigurasi ini, jika master03 lumpuh, simpul pekerja akan kehilangan komunikasi dengan server API cluster.  Untuk membuat cluster kami sepenuhnya HA, kami akan menginstal Load Balancer (Haproxy) pada masing-masing pekerja, yang, menurut round robin, akan menyebarkan permintaan untuk tiga node master, dan dalam konfigurasi kubelet pada node pekerja kami akan mengubah alamat server menjadi 127.0.0.1:6443 <br>  Pertama, instal HAProxy pada setiap node pekerja. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Ada lembar contekan yang bagus untuk instalasi</a> </p><br><pre> <code class="plaintext hljs">curl https://haproxy.debian.net/bernat.debian.org.gpg | \ apt-key add - echo deb http://haproxy.debian.net stretch-backports-2.0 main | \ tee /etc/apt/sources.list.d/haproxy.list apt-get update apt-get install haproxy=2.0.\*</code> </pre> <br><p>  Setelah HAproxy diinstal, kita perlu membuat konfigurasi untuk itu. <br>  Jika pada node pekerja tidak ada direktori dengan file konfigurasi, maka kami mengkloningnya </p><br><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/</code> </pre> <br><p>  Dan jalankan skrip config dengan flag haproxy </p><br><pre> <code class="plaintext hljs">./create-config.sh haproxy</code> </pre> <br><p>  Script akan mengkonfigurasi dan me-restart haproxy. <br>  Periksa apakah haproxy mulai mendengarkan port 6443. </p><br><pre> <code class="plaintext hljs">root@worknode01:~/kubernetes-ceph-percona# netstat -alpn | grep 6443 tcp 0 0 127.0.0.1:6443 0.0.0.0:* LISTEN 30675/haproxy tcp 0 0 10.73.75.241:6443 0.0.0.0:* LISTEN 30675/haproxy</code> </pre> <br><p>  Sekarang kita perlu memberitahu kubelet untuk mengakses localhost alih-alih node master.  Untuk melakukan ini, edit nilai server di file /etc/kubernetes/kubelet.conf dan /etc/kubernetes/bootstrap-kubelet.conf di semua node pekerja. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/kubelet.conf vim nano /etc/kubernetes/bootstrap-kubelet.conf</code> </pre> <br><p>  Nilai server harus berupa: </p><br><pre> <code class="plaintext hljs">server: https://127.0.0.1:6443</code> </pre> <br><p>  Setelah melakukan perubahan, restart layanan kubelet dan buruh pelabuhan </p><br><pre> <code class="plaintext hljs">systemctl restart kubelet &amp;&amp; systemctl restart docker</code> </pre> <br><p>  Periksa apakah semua node berfungsi dengan benar. </p><br><pre> <code class="plaintext hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 29m v1.15.1 master02 Ready master 27m v1.15.1 master03 Ready master 26m v1.15.1 worknode01 Ready &lt;none&gt; 25m v1.15.1 worknode02 Ready &lt;none&gt; 3m15s v1.15.1 worknode03 Ready &lt;none&gt; 3m16s v1.15.1</code> </pre> <br><p>  Sejauh ini, kami tidak memiliki aplikasi di cluster untuk menguji HA.  Tapi kita bisa menghentikan operasi kubelet pada master node pertama dan memastikan bahwa cluster kita tetap beroperasi. </p><br><pre> <code class="plaintext hljs">systemctl stop kubelet &amp;&amp; systemctl stop docker</code> </pre> <br><p>  Periksa dari master node kedua </p><br><pre> <code class="plaintext hljs">root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 NotReady master 15h v1.15.1 master02 Ready master 15h v1.15.1 master03 Ready master 15h v1.15.1 worknode01 Ready &lt;none&gt; 15h v1.15.1 worknode02 Ready &lt;none&gt; 15h v1.15.1 worknode03 Ready &lt;none&gt; 15h v1.15.1</code> </pre> <br><p>  Semua node berfungsi secara normal kecuali yang kita hentikan layanan. <br>  Jangan lupa untuk mengembalikan layanan kubernetes pada master node pertama </p><br><pre> <code class="plaintext hljs">systemctl start kubelet &amp;&amp; systemctl start docker</code> </pre> <br><a name="ingress"></a><br><h2>  Menginstal Ingress Controller </h2><br><p>  Pengontrol ingress adalah add-on Kubernetes, yang dengannya kita dapat mengakses aplikasi kita dari luar.  Penjelasan terperinci ada di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi Kuberbnetes</a> .  Ada cukup banyak pengendali masuknya, saya menggunakan controller dari Nginx.  Saya akan berbicara tentang pemasangannya.  Dokumentasi tentang operasi, konfigurasi dan pemasangan pengontrol Ingress dari Nginx dapat dibaca di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web resmi</a> </p><br><p>  Mari kita mulai instalasi, semua perintah dapat dijalankan dengan master01. <br>  Instal pengontrol itu sendiri </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</code> </pre> <br><p>  Dan sekarang - layanan melalui mana masuknya akan tersedia <br>  Untuk melakukan ini, siapkan konfigurasi </p><br><pre> <code class="plaintext hljs">./create-config.sh ingress</code> </pre> <br><p>  Dan kirimkan ke cluster kami </p><br><pre> <code class="plaintext hljs">kubectl apply -f ingress/service-nodeport.yaml</code> </pre> <br><p>  Periksa apakah Ingress kami berfungsi pada alamat yang benar dan mendengarkan pada port yang benar. </p><br><pre> <code class="plaintext hljs"># kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.99.35.95 10.73.71.25,10.73.71.26,10.73.71.27 80:31669/TCP,443:31604/TCP 10m</code> </pre> <br><pre> <code class="plaintext hljs"> kubectl describe svc -n ingress-nginx ingress-nginx Name: ingress-nginx Namespace: ingress-nginx Labels: app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx Annotations: kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/par... Selector: app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx Type: NodePort IP: 10.99.35.95 External IPs: 10.73.71.25,10.73.71.26,10.73.71.27 Port: http 80/TCP TargetPort: 80/TCP NodePort: http 31669/TCP Endpoints: 192.168.142.129:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31604/TCP Endpoints: 192.168.142.129:443 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt;</code> </pre> <br><a name="dashboard"></a><br><h2>  Pasang UI Web (Dasbor) </h2><br><p>  Kubernetes memiliki UI Web standar, yang terkadang nyaman untuk dengan cepat melihat status gugus atau bagian-bagian individualnya.  Dalam pekerjaan saya, saya sering menggunakan dasbor untuk diagnosis awal penyebaran atau keadaan bagian dari sebuah cluster. <br>  Tautan ke dokumentasi ada di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs kubernetes</a> <br>  Instalasi  Saya menggunakan versi stabil, saya belum mencoba 2.0. </p><br><pre> <code class="plaintext hljs">#  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml # 2.0 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml</code> </pre> <br><p>  Setelah kami memasang panel di cluster kami, panel menjadi tersedia di </p><br><pre> <code class="plaintext hljs">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.</code> </pre> <br><p>  Tetapi untuk sampai ke sana, kita perlu meneruskan porta dari mesin lokal menggunakan proksi kubectl.  Bagi saya, skema ini sangat tidak nyaman.  Oleh karena itu, saya akan mengubah layanan panel kontrol sehingga dasbor menjadi tersedia pada alamat simpul gugus pada port 30443. Masih ada cara lain untuk mengakses dasbor, misalnya, melalui masuknya.  Mungkin saya akan mempertimbangkan metode ini dalam publikasi berikut. <br>  Untuk mengubah layanan, jalankan penyebaran layanan yang diubah </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/service-nodeport.yaml</code> </pre> <br><p>  Tetap membuat pengguna admin dan token untuk mengakses cluster melalui dasbor </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/rbac.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')</code> </pre> <br><p>  Setelah itu, Anda dapat masuk ke panel kontrol di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://10.73.71.25:30443</a> <br><img src="https://habrastorage.org/webt/p7/zu/8q/p7zu8qv47mwdsmdydtvyo7gs_y4.png"><br>  Layar beranda dasbor <br><img src="https://habrastorage.org/webt/h2/ks/jq/h2ksjq_7egqatf4ulnl0zkwqvqk.png"></p><br><p>  Selamat!  Jika Anda telah mencapai langkah ini, maka Anda memiliki kluster HA kubernet yang berfungsi, yang siap untuk penyebaran aplikasi Anda. <br> Kubernetes    ,      .          . <br>       ,  GitHub,    ,    . <br> C ,  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id462473/">https://habr.com/ru/post/id462473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id462461/index.html">GOES-17 Hasil Investigasi Kecelakaan</a></li>
<li><a href="../id462465/index.html">Menggunakan Tempat Asli Apple</a></li>
<li><a href="../id462467/index.html">Frontend Weekly Digest (29 Juli - 4 Agustus 2019)</a></li>
<li><a href="../id462469/index.html">Beberapa Pertimbangan untuk Komputasi Bersamaan dalam R untuk Tugas "Perusahaan"</a></li>
<li><a href="../id462471/index.html">Pemecahan masalah dengan pwnable.kr 16 - uaf. Gunakan setelah kerentanan gratis</a></li>
<li><a href="../id462475/index.html">Alexey Savvateev: Bagaimana cara memerangi korupsi dengan bantuan matematika (Hadiah Nobel Ekonomi untuk 2016)</a></li>
<li><a href="../id462477/index.html">Para ilmuwan mengklaim AI sebagai penulis paten baru dan sedang berusaha mengubah hukum paten</a></li>
<li><a href="../id462479/index.html">Uap Eskalasi Privilege Lokal Klien Windows 0 hari</a></li>
<li><a href="../id462481/index.html">Ketik FAQ Sistem</a></li>
<li><a href="../id462483/index.html">Pemrograman fungsional: mainan aneh yang membunuh produktivitas tenaga kerja. Bagian 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>