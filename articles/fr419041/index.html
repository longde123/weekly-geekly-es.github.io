<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ôüèº ü¶ë üëåüèæ Les neuf r√¢teaux Elasticsearch sur lesquels j'ai march√© ‚òòÔ∏è üë©üèø‚Äçü§ù‚Äçüë©üèæ üò¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬´Une personne form√©e marche √©galement sur un r√¢teau. 
 Mais d'un autre c√¥t√©, o√π se trouve le stylo. ¬ª 

 Elasticsearch est un excellent outil, mais ch...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les neuf r√¢teaux Elasticsearch sur lesquels j'ai march√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/419041/"><img src="https://habrastorage.org/webt/ap/2k/jc/ap2kjcsehhaliahrmgg6a3r27xw.jpeg" alt="Illustration d'Anton Gudim"><br><br><br>  <i>¬´Une personne form√©e marche √©galement sur un r√¢teau.</i> <i><br></i>  <i>Mais d'un autre c√¥t√©, o√π se trouve le stylo. ¬ª</i> <br><br>  Elasticsearch est un excellent outil, mais chaque outil n√©cessite non seulement un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©glage</a> et une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">maintenance</a> , mais √©galement une attention aux d√©tails.  Certains sont insignifiants et reposent √† la surface, tandis que d'autres sont cach√©s si profond√©ment qu'il faudra plus d'une journ√©e pour fouiller, pas une douzaine de tasses de caf√© et pas un kilom√®tre de nerfs.  Dans cet article, je vais vous parler de neuf merveilleux r√¢teaux dans les r√©glages √©lastiques sur lesquels j'ai march√©. <br><a name="habracut"></a><br>  Je vais organiser le r√¢teau par ordre d√©croissant de preuve.  De ceux qui peuvent √™tre pr√©vus et contourn√©s au stade de la cr√©ation et de l'entr√©e d'un cluster √† l'√©tat de production, √† ceux tr√®s √©tranges qui apportent le plus d'exp√©rience (et d'√©toiles dans les yeux). <br><br><h2>  Les n≈ìuds de donn√©es doivent √™tre identiques </h2><br>  ¬´Le cluster fonctionne √† la vitesse du n≈ìud de donn√©es le plus lent¬ª - un axiome agonis√©.  Mais il y a un autre point √©vident qui n'est pas li√© aux performances: l'√©lastique ne pense pas dans l'espace disque, mais dans les fragments, et essaie de les r√©partir uniform√©ment entre les n≈ìuds de donn√©es.  Si certains des n≈ìuds de donn√©es ont plus d'espace que d'autres, il sera inutile de rester inactif. <br><br><h2>  Deprecation.log </h2><br>  Il peut arriver que quelqu'un n'utilise pas les moyens les plus modernes pour envoyer des donn√©es √† l'√©lastique, qui ne peut pas d√©finir le type de contenu lors de l'ex√©cution des requ√™tes.  Dans cette liste, par exemple, heka, ou lorsque les journaux quittent les appareils par leurs moyens int√©gr√©s).  Dans ce cas, d√©pr√©ciation.  journal commence √† cro√Ætre √† un rythme alarmant, et pour chaque demande, les lignes suivantes y apparaissent: <br><br><pre><code class="hljs markdown">[<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,659</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,670</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,671</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,673</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,677</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController </span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [Content-Type] header.</code> </pre> <br>  Les demandes arrivent, en moyenne, toutes les 5 √† 10 ms - et chaque fois qu'une nouvelle ligne est ajout√©e au journal.  Cela affecte n√©gativement les performances du sous-syst√®me de disque et augmente iowait.  Deprecation.log peut √™tre d√©sactiv√©, mais ce n'est pas trop raisonnable.  Pour y collecter des journaux √©lastiques, mais pas pour les d√©tritus, je d√©sactive uniquement les journaux de la classe oedrRestController. <br><br>  Pour ce faire, ajoutez la construction suivante √† logs4j2.properties: <br><br><pre> <code class="hljs pgsql">logger.restcontroller.name = org.elasticsearch.deprecation.rest.RestController logger.restcontroller.<span class="hljs-keyword"><span class="hljs-keyword">level</span></span> = error</code> </pre><br>  Cela augmentera les journaux de cette classe au niveau d'erreur et ils ne tomberont plus dans deprecation.log. <br><br><h2>  .kibana </h2><br>  √Ä quoi ressemble un processus d'installation de cluster typique?  Nous mettons les n≈ìuds, les combinons dans un cluster, mettons le x-pack (qui en a besoin), et bien s√ªr, Kibana.  Nous commen√ßons, v√©rifions que tout fonctionne et Kibana voit le cluster, et continuons de configurer.  Le probl√®me est que sur un cluster fra√Æchement install√©, le mod√®le par d√©faut ressemble √† ceci: <br><br><pre> <code class="hljs json">{ <span class="hljs-attr"><span class="hljs-attr">"default"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"order"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"template"</span></span>: <span class="hljs-string"><span class="hljs-string">"*"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"settings"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"number_of_shards"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"0"</span></span> } }, <span class="hljs-attr"><span class="hljs-attr">"mappings"</span></span>: {}, <span class="hljs-attr"><span class="hljs-attr">"aliases"</span></span>: {} }</code> </pre> <br>  Et l'index .kibana, o√π tous les param√®tres sont stock√©s, est cr√©√© en une seule copie. <br><br>  Il y a eu un cas o√π, en raison d'une d√©faillance mat√©rielle, l'un des n≈ìuds de donn√©es du cluster a √©t√© tu√©.  Il a rapidement atteint un √©tat coh√©rent, g√©n√©rant des r√©pliques du fragment √† partir des n≈ìuds de donn√©es voisins, mais, heureusement, c'est sur ce n≈ìud de donn√©es que le seul fragment avec l'index .kibana a √©t√© localis√©.  La situation est dans l'impasse - le cluster est vivant, en √©tat de marche et Kibana est en statut rouge, et mon t√©l√©phone est d√©chir√© par les appels des employ√©s qui ont un besoin urgent de leurs journaux. <br><br>  Tout cela est r√©solu simplement.  Jusqu'√† pr√©sent, rien n'est tomb√©: <br><br><pre> <code class="hljs objectivec">XPUT .kibana/_settings { <span class="hljs-string"><span class="hljs-string">"index"</span></span>: { <span class="hljs-string"><span class="hljs-string">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;__&gt;"</span></span> } }</code> </pre> <br><h2>  XMX / XMS </h2><br>  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> dit ¬´pas plus de 32 Go¬ª, et √† juste titre.  Mais il est √©galement correct que vous n'ayez pas besoin d'installer dans les param√®tres de service <br><pre> <code class="hljs powershell"><span class="hljs-literal"><span class="hljs-literal">-Xms32g</span></span> <span class="hljs-literal"><span class="hljs-literal">-Xmx32g</span></span></code> </pre> <br>  Parce que c'est d√©j√† plus de 32 gigaoctets, et ici nous rencontrons une nuance int√©ressante de Java travaillant avec la m√©moire.  Au-del√† d'une certaine limite, Java cesse d'utiliser des pointeurs compress√©s et commence √† consommer beaucoup trop de m√©moire.  V√©rifier si les pointeurs compress√©s utilisent une machine Java ex√©cutant Elasticsearch est tr√®s simple.  Nous regardons dans le journal de service: <br><br><pre> <code class="hljs powershell">[<span class="hljs-number"><span class="hljs-number">2018</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">29</span></span><span class="hljs-type"><span class="hljs-type">T15</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span>:<span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">041</span></span>][<span class="hljs-type"><span class="hljs-type">INFO</span></span>][<span class="hljs-type"><span class="hljs-type">oeeNodeEnvironment</span></span>][<span class="hljs-type"><span class="hljs-type">log</span></span>-<span class="hljs-type"><span class="hljs-type">elastic</span></span>-<span class="hljs-type"><span class="hljs-type">hot3</span></span>] heap size [<span class="hljs-number"><span class="hljs-number">31.6</span></span><span class="hljs-type"><span class="hljs-type">gb</span></span>], compressed ordinary object pointers [<span class="hljs-type"><span class="hljs-type">true</span></span>]</code> </pre> <br>  La quantit√© de m√©moire √† ne pas d√©passer d√©pend, entre autres, de la version de Java utilis√©e.  Pour calculer le volume exact dans votre cas, consultez la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> . <br><br>  Maintenant, j'ai install√© sur tous les n≈ìuds de donn√©es de l'√©lastique: <br><br><pre> <code class="hljs powershell"><span class="hljs-literal"><span class="hljs-literal">-Xms32766m</span></span> <span class="hljs-literal"><span class="hljs-literal">-Xmx32766m</span></span></code> </pre> <br>  Cela semble √™tre un fait banal, et la documentation est bien d√©crite, mais je rencontre r√©guli√®rement des installations Elasticsearch o√π j'ai rat√© ce point, et Xms / Xmx sont d√©finis sur 32g. <br><br><h2>  / var / lib / elasticsearch </h2><br>  Il s'agit du chemin par d√©faut pour le stockage des donn√©es dans elasticsearch.  yml: <br><br><pre> <code class="hljs kotlin">path.<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>: /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch</code> </pre> <br>  L√†, je monte g√©n√©ralement une grande matrice RAID, et voici pourquoi: nous sp√©cifions ES de plusieurs fa√ßons pour stocker des donn√©es, par exemple, comme ceci: <br><br><pre> <code class="hljs kotlin">path.<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>: /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch/data1, /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch/data2</code> </pre> <br>  Diff√©rents disques ou tableaux RAID sont mont√©s dans data1 et data2.  Mais l'√©lastique ne s'√©quilibre pas et ne r√©partit pas la charge entre ces chemins.  Tout d'abord, il remplit une section, puis commence √† √©crire dans une autre, de sorte que la charge sur le stockage sera in√©gale.  Sachant cela, j'ai pris une d√©cision sans ambigu√Øt√© - j'ai combin√© tous les disques dans RAID0 / 1 et l'ai mont√© dans le chemin sp√©cifi√© dans path.data. <br><br><h2>  processeurs_disponibles </h2><br>  Et non, je ne parle pas des processeurs sur les n≈ìuds d'ingestion maintenant.  Si vous regardez les propri√©t√©s d'un n≈ìud en cours d'ex√©cution (via l'API _nodes), vous pouvez voir quelque chose comme ceci: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"os"</span></span>. { <span class="hljs-string"><span class="hljs-string">"refresh_interval_in_millis"</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"arch"</span></span>: <span class="hljs-string"><span class="hljs-string">"amd64"</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"4.4.0-87-generic"</span></span>, <span class="hljs-string"><span class="hljs-string">"available_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"allocated_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span> }</code> </pre> <br>  On peut voir que le n≈ìud fonctionne sur un h√¥te avec 28 c≈ìurs, et l'√©lastique a correctement d√©termin√© leur nombre et a commenc√© sur tous.  Mais s'il y a plus de 32 c≈ìurs, cela arrive parfois comme ceci: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"os"</span></span>: { <span class="hljs-string"><span class="hljs-string">"refresh_interval_in_millis"</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"arch"</span></span>: <span class="hljs-string"><span class="hljs-string">"amd64"</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"4.4.0-116-generic"</span></span>, <span class="hljs-string"><span class="hljs-string">"available_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">72</span></span>, <span class="hljs-string"><span class="hljs-string">"allocated_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">32</span></span> }</code> </pre> <br>  Vous devez forcer le nombre de processeurs disponibles pour le service - cela a un bon effet sur les performances du n≈ìud. <br><br><pre> <code class="hljs">processors: 72</code> </pre> <br><h2>  thread_pool.bulk.queue_size </h2><br>  Dans la section thread_pool.bulk.rejected du dernier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article, il y</a> avait une telle mesure - le nombre du nombre d'√©checs pour les demandes d'ajout de donn√©es. <br><br>  J'ai √©crit que la croissance de cet indicateur est un tr√®s mauvais signe, et les d√©veloppeurs recommandent de ne pas configurer de pools de threads, mais d'ajouter de nouveaux n≈ìuds au cluster - soi-disant, cela r√©sout les probl√®mes de performances.  Mais les r√®gles sont n√©cessaires pour les briser parfois.  Et il n'est pas toujours possible de "jeter le probl√®me avec du fer", donc l'une des mesures pour lutter contre les √©checs dans les demandes group√©es est d'augmenter la taille de cette file d'attente. <br><br>  Par d√©faut, les param√®tres de file d'attente ressemblent √† ceci: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"thread_pool"</span></span>: { <span class="hljs-string"><span class="hljs-string">"bulk"</span></span>: { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"fixed"</span></span>, <span class="hljs-string"><span class="hljs-string">"min"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"max"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"queue_size"</span></span>: <span class="hljs-number"><span class="hljs-number">200</span></span> } }</code> </pre> <br>  L'algorithme est le suivant: <br><br><ol><li>  Nous collectons des statistiques sur la taille moyenne de la file d'attente pendant la journ√©e (la valeur instantan√©e est stock√©e dans thread_pool.bulk.queue); </li><li>  Augmentez soigneusement queue_size √† des tailles l√©g√®rement sup√©rieures √† la taille moyenne de la file d'attente active - car une d√©faillance se produit lorsqu'elle est d√©pass√©e; </li><li>  Nous augmentons la taille de la piscine - ce n'est pas n√©cessaire, mais acceptable. </li></ol><br>  Pour ce faire, ajoutez quelque chose comme √ßa aux param√®tres de l'h√¥te (vous aurez, bien s√ªr, vos propres valeurs): <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">thread_pool</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.bulk</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.size</span></span>: 32 <span class="hljs-selector-tag"><span class="hljs-selector-tag">thread_pool</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.bulk</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.queue_size</span></span>: 500</code> </pre> <br>  Et apr√®s avoir red√©marr√© le n≈ìud, nous surveillerons certainement la charge, les E / S et la consommation de m√©moire.  et tout ce qui est possible pour restaurer les param√®tres si n√©cessaire. <br><br>  <i>Important: ces param√®tres n'ont de sens que sur les n≈ìuds travaillant sur la r√©ception de nouvelles donn√©es.</i> <br><br><h2>  Cr√©ation d'un index pr√©liminaire </h2><br>  Comme je l'ai dit dans le premier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de la</a> s√©rie, nous utilisons Elasticsearch pour stocker les journaux de tous les microservices.  L'essentiel est simple: un index stocke les journaux d'un composant en une journ√©e. <br><br>  Il en r√©sulte que chaque jour de nouveaux index sont cr√©√©s par le nombre de microservices - par cons√©quent, plus t√¥t chaque nuit, l'√©lastique est tomb√© dans le corps √† corps pendant environ 8 minutes, tandis qu'une centaine de nouveaux index ont √©t√© cr√©√©s, plusieurs centaines de nouveaux fragments, le programme de chargement du disque est sorti du plateau, les files d'attente ont augment√© d'envoyer des journaux √† l'√©lastique sur les h√¥tes, et Zabbix a fleuri avec des alertes comme un arbre de No√´l. <br><br>  Pour √©viter cela, il √©tait logique d'√©crire un script Python pour pr√©-cr√©er des index.  Le script fonctionne comme ceci: il trouve les indices pour aujourd'hui, extrait leurs mappages et cr√©e de nouveaux index avec les m√™mes mappages, mais pour la journ√©e √† venir.  Il fonctionne sur cron, fonctionne pendant les heures o√π l'√©lastique est le moins charg√©.  Le script utilise la biblioth√®que elasticsearch et est disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> . <br><br><h2>  Pages √©normes parentales transparentes </h2><br>  Une fois que nous avons constat√© que les n≈ìuds √©lastiques qui op√®rent la r√©ception des donn√©es ont commenc√© √† se bloquer sous charge pendant les heures de pointe.  Et avec des sympt√¥mes tr√®s √©tranges: l'utilisation de tous les c≈ìurs de processeur tombe √† z√©ro, mais n√©anmoins, le service se bloque en m√©moire, √©coute correctement le port, ne fait rien, ne r√©pond pas aux demandes et tombe du cluster apr√®s un certain temps.  Le service ne r√©pond pas au red√©marrage de systemctl.  Seul le bon vieux kill ‚àí9 aide. <br><br>  Ce n'est pas pris en compte par les outils de surveillance standard, sur les graphiques jusqu'au moment m√™me de l'automne, l'image r√©guli√®re, dans les journaux de service - est vide.  Le vidage de la m√©moire de la machine java √† ce stade n'√©tait √©galement pas possible. <br><br>  Mais, comme on dit, "nous sommes des professionnels, donc apr√®s un certain temps, nous avons recherch√© la solution sur Google".  Un probl√®me similaire a √©t√© couvert dans le fil de discussion sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">discuter.elastic.co</a> et s'est av√©r√© √™tre un bogue du noyau li√© aux √©normes pages transparentes.  Tout a √©t√© r√©solu en d√©sactivant thp dans le noyau en utilisant le paquet sysfsutils. <br><br>  V√©rifier si vous avez activ√© des pages √©normes transparentes est simple: <br><br><pre> <code class="hljs powershell">cat /sys/kernel/mm/transparent_hugepage/enabled always madvise [<span class="hljs-type"><span class="hljs-type">never</span></span>]</code> </pre> <br>  Si [toujours] est l√†, vous √™tes potentiellement √† risque. <br><br><h2>  Conclusion </h2><br>  C'est le principal r√¢teau (en fait, il y en avait, bien s√ªr, plus), que j'ai eu l'occasion de suivre pendant un an et demi en tant qu'administrateur du cluster Elasticsearch.  J'esp√®re que ces informations vous seront utiles lors du voyage difficile et myst√©rieux vers le cluster Elasticsearch id√©al. <br><br>  Merci pour l'illustration, Anton Gudim - il y a encore beaucoup de bien dans son <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">instagram</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr419041/">https://habr.com/ru/post/fr419041/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr419027/index.html">S√©curit√© de l'information des paiements bancaires sans esp√®ces. Partie 6 - Analyse de la criminalit√© bancaire</a></li>
<li><a href="../fr419029/index.html">Fortnite est devenu un ph√©nom√®ne social. Les parents recrutent de plus en plus des entra√Æneurs pour leurs enfants et jouent avec eux</a></li>
<li><a href="../fr419033/index.html">Une petite note sur le sujet de l'ex√©cution de vue.js dans le cluster kubernetes</a></li>
<li><a href="../fr419035/index.html">Livre ¬´Head First Agile. Gestion de projet flexible ‚Äù</a></li>
<li><a href="../fr419037/index.html">Impl√©mentation PPPOS sur stm32f4-discovery</a></li>
<li><a href="../fr419043/index.html">Le probl√®me insaisissable de synchronisation de trame</a></li>
<li><a href="../fr419047/index.html">Reddit pirat√©, fuite de base de donn√©es avec mots de passe et e-mail pour 2005-2007</a></li>
<li><a href="../fr419049/index.html">GeekBrains lance le marathon √©ducatif en ligne gratuit ¬´Find Yourself in Digital¬ª</a></li>
<li><a href="../fr419051/index.html">Comment Flant aide les d√©butants</a></li>
<li><a href="../fr419053/index.html">Test de la technologie de cache RAID Adaptec</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>