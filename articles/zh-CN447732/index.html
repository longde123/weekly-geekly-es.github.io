<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🤝‍👨🏼 🤚 🌅 狗的品种标识符：从Keras程序到Android App的全周期开发。 在市场上 👋🏻 🕶️ 🚷</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="随着神经网络（特别是图像识别）领域的最新进展，似乎创建基于NN的图像识别应用程序是一项简单的常规操作。 好吧，在某种程度上说是正确的：如果您可以想象图像识别的应用，那么很可能有人已经做了类似的事情。 您需要做的就是将它添加到Google并重复进行。 

 但是，仍然有无数小细节……它们不是无法解决的...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>狗的品种标识符：从Keras程序到Android App的全周期开发。 在市场上</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/447732/"> 随着神经网络（特别是图像识别）领域的最新进展，似乎创建基于NN的图像识别应用程序是一项简单的常规操作。 好吧，在某种程度上说是正确的：如果您可以想象图像识别的应用，那么很可能有人已经做了类似的事情。 您需要做的就是将它添加到Google并重复进行。 <br><br> 但是，仍然有无数小细节……它们不是无法解决的，不是。 它们只会占用您太多时间，尤其是对于初学者而言。 这将是一个逐步的项目，这将对您有所帮助，然后从头开始。 一个不包含“这部分很明显，所以我们跳过它”语句的项目。 好吧，几乎:) <br><br> 在本教程中，我们将逐步介绍一个“狗品种标识符”：我们将创建并教授一个神经网络，然后将其移植到Java for Android并在Google Play上发布。 <br><br> 对于那些想要查看最终结果的人，这里是Google Play上<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">NeuroDog App</a>的链接。 <br><br> 使用我的机器人<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">技术的</a>网站： <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">robotics.snowcron.com</a> 。 <br> 网站： <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">NeuroDog用户指南</a> 。 <br><br> 这是该程序的屏幕截图： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/186/b91/457/186b914572170b01446ed1d722bce200.png" alt="图片"><br><br><a name="habracut"></a><br><br><h3> 概述 </h3><br><br> 我们将使用Keras：Google的与神经网络合作的库。 它是高级的，这意味着学习曲线将是陡峭的，绝对比我所知道的其他库快。 使自己熟悉它：在线上有许多高质量的教程。 <br><br> 我们将使用CNN-卷积神经网络。  CNN（以及基于它们的更高级的网络）在图像识别中已成为事实上的标准。 但是，正确地教人可能会成为一项艰巨的任务：网络的结构，学习参数（所有这些学习率，动量，L1和L2等）都应谨慎调整，并且由于该任务需要大量的计算资源，因此我们不能简单地尝试所有可能的组合。 <br><br> 这是为什么在大多数情况下我们更喜欢使用“转移知识”而不是所谓的“香草”方法的少数原因之一。  Transfer Knowlege使用由其他人（例如Google）训练的神经网络来执行其他一些任务。 然后，我们删除它的最后几层，添加我们自己的层……这行之有效。 <br><br> 听起来很奇怪：我们让Google的网络接受了识别猫，花和家具的培训，现在它可以识别狗的品种！ 为了了解其工作原理，让我们看一下深度神经网络（包括用于图像识别的神经网络）的工作方式。 <br><br> 我们将图像作为输入。 网络的第一层分析图像中的简单图案，例如“短水平线”，“拱形”等。 下一层采用这些图案（以及它们在图像上的位置），并产生更高级别的图案，例如“毛发”，“眼角”等。 最后，我们有一个谜题，可以组合成对狗的描述：毛皮，两只眼睛，嘴里的人的腿等等。 <br><br> 现在，这一切都是由我们（从Google或其他一些大型公司）获得的一组经过预训练的图层完成的。 最后，我们在其上添加自己的图层，并教会其使用这些模式来识别狗的品种。 听起来合乎逻辑。 <br><br> 总而言之，在本教程中，我们将创建“香草” CNN和几个不同类型的“转移学习”网络。 至于“香草”：我将仅以它为例进行说明，但由于“预训练”网络更易于使用，因此我不会对其进行微调。  Keras带有一些预先训练的网络，我将选择几种配置并进行比较。 <br><br> 因为我们希望我们的神经网络能够识别狗的品种，所以我们需要“显示”它的不同品种的样本图像。 幸运的是，为类似的任务创建了一个<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">大型数据集</a> （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">原始</a> ）。 在本文中，我将使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kaggle</a>的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">版本</a> <br><br> 然后，我要将“优胜者”移植到Android。 将Keras NN移植到Android相对容易，我们将逐步完成所有必需的步骤。 <br><br> 然后，我们将其发布在Google Play上。 正如人们所期望的那样，Google不会合作，因此几乎不需要其他技巧。 例如，我们的神经网络超出了Android APK的允许大小：我们将不得不使用bundle。 此外，除非我们做某些不可思议的事情，否则Google不会在搜索结果中显示我们的应用程序。 <br><br> 最后，我们将提供一个功能全面的“商业”（报价，因为它是免费的，尽管已经投入市场）是Android NN支持的应用程序。 <br><br><h3> 开发环境 </h3><br><br>  Keras编程的方法很少，这取决于所使用的操作系统（建议使用Ubuntu），所拥有（或没有）视频卡等。 在本地计算机上配置开发环境并安装所有必需的库等没有错。 除了...还有一种更简单的方法。 <br><br> 首先，安装和配置多个开发工具会花费一些时间，并且当新版本可用时，您将不得不再次花费时间。 其次，训练神经网络需要大量的计算能力。 您可以通过使用GPU来加速计算机的工作……在撰写本文时，用于NN相关计算的顶级GPU的价格为2000-7000美元。 配置它也需要时间。 <br><br> 因此，我们将使用另一种方法。 可以看到，Google允许人们免费使用其GPU进行与NN相关的计算，它还创建了一个完全配置的环境； 统称为Google Colab。 该服务使您可以访问带有Python，Keras和大量已安装的其他库的Jupiter Notebook。 您所需要做的就是获得一个Google帐户（获得一个Gmail帐户，您将可以访问其他所有内容），仅此而已。 <br><br> 在撰写本文时，可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">通过此链接</a>访问Colab，但是它可以更改。 只需用谷歌搜索“ Google Colab”。 <br><br>  Colab的一个明显问题是它是一个Web服务。 您将如何从中访问您的文件？ 培训结束后保存神经网络，加载特定于您任务的数据等吗？ <br><br> 几乎没有（在撰写本文之时-三种）不同的方法。 我们将使用我认为最好的方法：使用Google云端硬盘。 <br><br>  Google云端硬盘是一种云存储，可以像硬盘一样工作，并且可以映射到Google Colab（请参见下面的代码）。 然后，您就可以像处理本地硬盘一样使用它。 因此，例如，如果您想从在Colab中创建的神经网络访问狗的照片，则必须将这些照片上传到Google云端硬盘。 <br><br><h2> 创建和训练NN </h2><br><br> 下面，我将逐步介绍Python代码，这是Jupiter Notebook中的另一段代码。 您可以将代码复制到笔记本中并运行它，因为可以相互独立地执行块。 <br><br><h3> 初始化 </h3><br><br> 首先，让我们安装Google云端硬盘。 只需两行代码。 该代码在每个Colab会话中仅需要执行一次（例如，每六个小时的工作一次）。 如果您第二次运行它，则由于已安装驱动器，因此它将被跳过。 <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> drive drive.mount(<span class="hljs-string"><span class="hljs-string">'/content/drive/'</span></span>)</code> </pre> <br><br> 首次要求您确认安装-这里没有什么复杂的。 看起来像这样： <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Go to this URL <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a browser: ... &gt;&gt;&gt; Enter your authorization code: &gt;&gt;&gt; ·········· &gt;&gt;&gt; Mounted at /content/drive/</code> </pre><br><br> 一个相当标准的<i>include</i>部分； 最有可能不需要其中的一些。 另外，当我要测试不同的NN配置时，您将必须针对特定类型的NN注释/取消注释其中的一些配置：例如，使用InceptionV3类型的NN，取消注释InceptionV3并注释（例如，ResNet50）。 还是不行：您可以不注释那些包含项，它将使用更多的内存，仅此而已。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> regularizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Conv2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, GlobalAveragePooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback, EarlyStopping <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReduceLROnPlateau <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.vgg16 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> inception_v3_preprocessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.mobilenetv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MobileNetV2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.nasnet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NASNetMobile</code> </pre><br><br> 在Google云端硬盘上，我们将为文件创建一个文件夹。 第二行显示其内容： <br><br><pre> <code class="python hljs">working_path = <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data/"</span></span> !ls <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data"</span></span> &gt;&gt;&gt; all_images labels.csv models test train valid</code> </pre><br><br> 如您所见，狗的照片（从斯坦福数据集（见上文）复制到Google云端硬盘的照片）最初存储在<i>all_images</i>文件夹中。稍后，我们将它们复制到<i>训练，有效</i>和<i>测试</i>文件夹中。我们将保存在<i>models</i>文件夹中训练有素的模型，对于labels.csv文件，它是数据集的一部分，它将图像文件映射到犬种。 <br><br> 您可以运行许多测试来弄清楚您拥有什么，让我们仅运行一个： <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Is GPU Working? import tensorflow as tf tf.test.gpu_device_name() &gt;&gt;&gt; '/device:GPU:0'</span></span></code> </pre><br><br> 好的，GPU已连接。 如果没有，请在Jupiter Notebook设置中找到它并将其打开。 <br><br> 现在，我们需要声明一些将要使用的常量，例如神经网络应该期望的图像大小等等。 请注意，我们使用256x256的图片，因为该图片的一侧足够大，而另一侧可以容纳在内存中。 但是，我们将要使用的某些类型的神经网络期望224x224的图像。 要处理此问题，必要时，注释旧的图像大小，然后取消注释新的图像大小。 <br><br> 相同的方法（一种注释-另一种注释）适用于我们保存的模型名称，这仅仅是因为我们在尝试新的配置时不想覆盖先前测试的结果。 <br><pre> <code class="python hljs">warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">"ignore"</span></span>) os.environ[<span class="hljs-string"><span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span></span>] = <span class="hljs-string"><span class="hljs-string">'2'</span></span> np.random.seed(<span class="hljs-number"><span class="hljs-number">7</span></span>) start = dt.datetime.now() BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">16</span></span> EPOCHS = <span class="hljs-number"><span class="hljs-number">15</span></span> TESTING_SPLIT=<span class="hljs-number"><span class="hljs-number">0.3</span></span> <span class="hljs-comment"><span class="hljs-comment"># 70/30 % NUM_CLASSES = 120 IMAGE_SIZE = 256 #strModelFileName = "models/ResNet50.h5" # strModelFileName = "models/InceptionV3.h5" strModelFileName = "models/InceptionV3_Sgd.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/MobileNetV2.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/NASNetMobileSgd.h5"</span></span></code> </pre><br><br><h3> 载入资料 </h3><br><br> 首先，让我们加载<i>labels.csv</i>文件，并将其内容拆分为训练和验证部分。 请注意，目前还没有测试部分，我将作弊，以获取更多训练数据。 <br><br><pre> <code class="python hljs">labels = pd.read_csv(working_path + <span class="hljs-string"><span class="hljs-string">'labels.csv'</span></span>) print(labels.head()) train_ids, valid_ids = train_test_split(labels, test_size = TESTING_SPLIT) print(len(train_ids), <span class="hljs-string"><span class="hljs-string">'train ids'</span></span>, len(valid_ids), <span class="hljs-string"><span class="hljs-string">'validation ids'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'Total'</span></span>, len(labels), <span class="hljs-string"><span class="hljs-string">'testing images'</span></span>) &gt;&gt;&gt; id breed &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">000</span></span>bec180eb18c7604dcecc8fe0dba07 boston_bull &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">001513</span></span>dfcb2ffafc82cccf4d8bbaba97 dingo &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">001</span></span>cdf01b096e06d78e9e5112d419397 pekinese &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">00214</span></span>f311d5d2247d5dfe4fe24b2303d bluetick &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">0021</span></span>f9ceb3235effd7fcde7f7538ed62 golden_retriever &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">7155</span></span> train ids <span class="hljs-number"><span class="hljs-number">3067</span></span> validation ids &gt;&gt;&gt; Total <span class="hljs-number"><span class="hljs-number">10222</span></span> testing images</code> </pre><br><br> 接下来，我们需要根据传递的文件名数组将实际的图像文件复制到training / validation / testing文件夹中。 以下功能将具有提供名称的文件复制到指定的文件夹。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copyFileSet</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(strDirFrom, strDirTo, arrFileNames)</span></span></span><span class="hljs-function">:</span></span> arrBreeds = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) arrFileNames = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'id'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo): os.makedirs(strDirTo) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(range(len(arrFileNames))): strFileNameFrom = strDirFrom + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> strFileNameTo = strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>): os.makedirs(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># As a new breed dir is created, copy 1st file # to "test" under name of that breed if not os.path.exists(working_path + "test/"): os.makedirs(working_path + "test/") strFileNameTo = working_path + "test/" + arrBreeds[i] + ".jpg" shutil.copy(strFileNameFrom, strFileNameTo) shutil.copy(strFileNameFrom, strFileNameTo)</span></span></code> </pre><br><br> 如您所见，我们仅将每种犬只的一个文件复制到一个<i>测试</i>文件夹中。 复制文件时，我们还会创建子文件夹-每个品种的狗一个子文件夹。 每个特定品种的图像都会复制到其子文件夹中。 <br><br> 原因是Keras可以使用以这种方式组织的目录结构，根据需要加载图像文件，从而节省了内存。 同时将所有15,000张图像加载到内存中是一个非常糟糕的主意。 <br><br> 每次我们运行代码时都调用此函数将是一个过大的杀手：图像已经被复制，为什么我们要再次复制它们。 因此，请先删除注释后再使用： <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Move the data in subfolders so we can # use the Keras ImageDataGenerator. # This way we can also later use Keras # Data augmentation features. # --- Uncomment once, to copy files --- #copyFileSet(working_path + "all_images/", # working_path + "train/", train_ids) #copyFileSet(working_path + "all_images/", # working_path + "valid/", valid_ids)</span></span></code> </pre><br><br> 此外，我们需要狗的品种清单： <br><br><pre> <code class="python hljs">breeds = np.unique(labels[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) map_characters = {} <span class="hljs-comment"><span class="hljs-comment">#{0:'none'} for i in range(len(breeds)): map_characters[i] = breeds[i] print("&lt;item&gt;" + breeds[i] + "&lt;/item&gt;") &gt;&gt;&gt; &lt;item&gt;affenpinscher&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;afghan_hound&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;african_hunting_dog&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;airedale&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;american_staffordshire_terrier&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;appenzeller&lt;/item&gt;</span></span></code> </pre><br><br><h3> 处理图像 </h3><br><br> 我们将使用Keras的功能ImageDataGenerators。  ImageDataGenerator可以处理图像，调整图像大小，旋转等。 它还可以采用执行自定义图像处理的<i>处理</i>功能。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) <span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255... img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. #img = cv2.blur(img,(5,5)) return img_1[0]</span></span></code> </pre><br><br> 请注意以下行： <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255...</span></span></code> </pre><br><br> 我们可以在ImageDataGenerator本身中执行归一化（将图像通道的0-255范围调整为0-1）。 那么为什么我们需要预处理器？ 例如，我提供了（注释掉） <i>模糊</i>功能：这是一种自定义图像处理。 您可以在此处使用从锐化到HDR的任何内容。 <br><br> 我们将使用两种不同的ImageDataGenerator，一种用于训练，另一种用于验证。 不同之处在于，我们需要旋转和缩放来进行训练，以使图像更加“多样化”，但我们不需要它来进行验证（不在此任务中）。 <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( preprocessing_function=preprocess, <span class="hljs-comment"><span class="hljs-comment">#rescale=1./255, # done in preprocess() # randomly rotate images (degrees, 0 to 30) rotation_range=30, # randomly shift images horizontally # (fraction of total width) width_shift_range=0.3, height_shift_range=0.3, # randomly flip images horizontal_flip=True, ,vertical_flip=False, zoom_range=0.3) val_datagen = ImageDataGenerator( preprocessing_function=preprocess) train_gen = train_datagen.flow_from_directory( working_path + "train/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical") val_gen = val_datagen.flow_from_directory( working_path + "valid/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical")</span></span></code> </pre><br><br><h3> 创建神经网络 </h3><br><br> 如上所述，我们将创建几种类型的神经网络。 每次我们使用不同的功能时，都会使用不同的包含库，在某些情况下还会使用不同的图像尺寸。 因此，要从一种类型的神经网络切换到另一种类型，您需要注释/取消注释相应的代码。 <br><br> 首先，让我们创建“香草” CNN。 由于我尚未对其进行优化，因此它的性能很差，但至少它提供了可用于创建自己的网络的框架（通常，这是一个坏主意，因为有可用的预先训练的网络）。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelVanilla</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() <span class="hljs-comment"><span class="hljs-comment"># Note the (7, 7) here. This is one of technics # used to reduce memory use by the NN: we scan # the image in a larger steps. # Also note regularizers.l2: this technic is # used to prevent overfitting. The "0.001" here # is an empirical value and can be optimized. model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.001))) # Note the use of a standard CNN building blocks: # Conv2D - BatchNormalization - Activation # MaxPooling2D - Dropout # The last two are used to avoid overfitting, also, # MaxPooling2D reduces memory use. model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(16, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) # This is the end on "convolutional" part of CNN. # Now we need to transform multidementional # data into one-dim. array for a fully-connected # classifier: model.add(Flatten()) # And two layers of classifier itself (plus an # Activation layer in between): model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) model.add(Activation("relu")) model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) # We need to compile the resulting network. # Note that there are few parameters we can # try here: the best performing one is uncommented, # the rest is commented out for your reference. #model.compile(optimizer='rmsprop', # loss='categorical_crossentropy', # metrics=['accuracy']) #model.compile( # optimizer=keras.optimizers.RMSprop(lr=0.0005), # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #model.compile(optimizer='adadelta', # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.Adadelta(lr=1.0, # rho=0.95, epsilon=0.01, decay=0.01) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.RMSprop(lr=0.0005, # rho=0.9, epsilon=None, decay=0.0001) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) # model.summary() return(model)</span></span></code> </pre><br><br> 当我们使用<i>转移学习</i>创建神经网络时，过程将更改： <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelMobileNetV2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, create the NN and load pre-trained # weights for it ('imagenet') # Note that we are not loading last layers of # the network (include_top=False), as we are # going to add layers of our own: base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Then attach our layers at the end. These are # to build "classifier" that makes sense of # the patterns previous layers provide: x = base_model.output x = Dense(512)(x) x = Activation('relu')(x) x = Dropout(0.5)(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Create a model model = Model(inputs=base_model.input, outputs=predictions) # We need to make sure that pre-trained # layers are not changed when we train # our classifier: # Either this: #model.layers[0].trainable = False # or that: for layer in base_model.layers: layer.trainable = False # As always, there are different possible # settings, I tried few and chose the best: # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br> 创建其他类型的预训练NN非常相似： <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelResNet50</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> base_model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>, input_shape=(IMAGE_SIZE, IMAGE_SIZE, <span class="hljs-number"><span class="hljs-number">3</span></span>)) x = base_model.output x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>)(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) predictions = Dense(NUM_CLASSES, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=base_model.input, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#model.layers[0].trainable = False # model.compile(loss='categorical_crossentropy', # optimizer='adam', metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Attn：获胜者！ 该NN展示了最佳结果： <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelInceptionV3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br> 一： <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelNASNetMobile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = NASNetMobile(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br> 在不同情况下使用不同类型的NN。 除了精度问题外，大小问题（移动NN比Inception规模小5倍）和速度（如果我们需要对视频流进行实时分析，则可能不得不牺牲精度）。 <br><br><h3> 训练神经网络 </h3><br><br> 首先，我们正在<i>试验</i> ，因此我们需要能够删除之前保存的NN，但不再需要了。 如果文件存在，以下函数将删除NN： <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make sure that previous "best network" is deleted. def deleteSavedNet(best_weights_filepath): if(os.path.isfile(best_weights_filepath)): os.remove(best_weights_filepath) print("deleteSavedNet():File removed") else: print("deleteSavedNet():No file to remove")</span></span></code> </pre><br><br> 我们创建和删除NN的方法很简单。 首先，我们删除。 现在，如果您不希望调用<i>delete</i> ，只需记住Jupiter Notebook具有“运行选择”功能-仅选择所需的内容，然后运行它。 <br><br> 然后，如果文件不存在，则创建NN；如果文件存在，则将其<i>加载</i> ：当然，我们不能调用“ delete”，然后期望NN存在，因此要使用以前保存的网络，请不要调用<i>delete</i> 。 <br><br> 换句话说，我们可以创建新的NN或使用现有的NN，具体取决于我们目前正在尝试的内容。 一个简单的场景：我们已经训练了神经网络，然后去度假。  Google已将我们注销，因此我们需要重新加载NN：注释掉“删除”部分，然后取消注释“加载”部分。 <br><br><pre> <code class="python hljs">deleteSavedNet(working_path + strModelFileName) <span class="hljs-comment"><span class="hljs-comment">#if not os.path.exists(working_path + "models"): # os.makedirs(working_path + "models") # #if not os.path.exists(working_path + # strModelFileName): # model = createModelResNet50() model = createModelInceptionV3() # model = createModelMobileNetV2() # model = createModelNASNetMobile() #else: # model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br> 在教授神经网络时， <b>检查点</b>非常重要。 您可以创建一个函数数组，在每个训练时期的末尾调用该函数，例如，如果显示的结果比上一个保存的结果更好，则可以保存NN。 <br><br><pre> <code class="python hljs">checkpoint = ModelCheckpoint(working_path + strModelFileName, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, save_weights_only=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) callbacks_list = [ checkpoint ]</code> </pre><br><br> 最后，我们将使用训练集教授NN： <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Calculate sizes of training and validation sets STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size STEP_SIZE_VALID=val_gen.n//val_gen.batch_size # Set to False if we are experimenting with # some other part of code, use history that # was calculated before (and is still in # memory bDoTraining = True if bDoTraining == True: # model.fit_generator does the actual training # Note the use of generators and callbacks # that were defined earlier history = model.fit_generator(generator=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=val_gen, validation_steps=STEP_SIZE_VALID, epochs=EPOCHS, callbacks=callbacks_list) # --- After fitting, load the best model # This is important as otherwise we'll # have the LAST model loaded, not necessarily # the best one. model.load_weights(working_path + strModelFileName) # --- Presentation part # summarize history for accuracy plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['acc', 'val_acc'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['loss', 'val_loss'], loc='upper left') plt.show() # As grid optimization of NN would take too long, # I did just few tests with different parameters. # Below I keep results, commented out, in the same # code. As you can see, Inception shows the best # results: # Inception: # adam: val_acc 0.79393 # sgd: val_acc 0.80892 # Mobile: # adam: val_acc 0.65290 # sgd: Epoch 00015: val_acc improved from 0.67584 to 0.68469 # sgd-30 epochs: 0.68 # NASNetMobile, adam: val_acc did not improve from 0.78335 # NASNetMobile, sgd: 0.8</span></span></code> </pre><br><br> 以下是获胜者NN的准确性和损失图： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/97d/9cc/f0e97d9ccdc8f8ed9e44ddba02cf1f8d.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/612/e09/8b0/612e098b088979768d1cc66c2f6972bc.png"><br><br> 如您所见，网络学习得很好。 <br><br><h3> 测试神经网络 </h3><br><br> 培训阶段完成后，我们需要进行测试； 为此，向NN展示了它从未见过的图像。 您可能还记得，我们为每种狗留了一张图片。 <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># --- Test j = 0 # Final cycle performs testing on the entire # testing set. for file_name in os.listdir( working_path + "test/"): img = image.load_img(working_path + "test/" + file_name); img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict_on_batch(img_1) # get 5 best predictions y_pred_ids = y_pred[0].argsort()[-5:][::-1] print(file_name) for i in range(len(y_pred_ids)): print("\n\t" + map_characters[y_pred_ids[i]] + " (" + str(y_pred[0][y_pred_ids[i]]) + ")") print("--------------------\n") j = j + 1</span></span></code> </pre><br><br><h3> 将NN导出到Java </h3><br><br> 首先，我们需要加载NN。 原因是，导出是一个单独的代码块，因此我们很可能单独运行它，而无需重新训练NN。 使用我的代码时，您并不在乎，但是如果您进行自己的开发，则应避免一次又一次地重新训练<i>同一个</i>网络。 <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Test: load and run model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br> 出于相同的原因-这是以某种方式单独的代码块-我们在这里使用其他包含。 当然，没有什么可以阻止我们向上移动它们： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre><br><br> 进行一些测试，以确保我们正确加载了所有内容： <br><br><pre> <code class="python hljs">img = image.load_img(working_path + <span class="hljs-string"><span class="hljs-string">"test/affenpinscher.jpg"</span></span>) <span class="hljs-comment"><span class="hljs-comment">#basset.jpg") img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict(img_1) Y_pred_classes = np.argmax(y_pred,axis = 1) # print(y_pred) fig, ax = plt.subplots() ax.imshow(img) ax.axis('off') ax.set_title(map_characters[Y_pred_classes[0]]) plt.show()</span></span></code> </pre><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/032/846/05c03284674e4337a2e5a3ba617634dd.png" alt="图片"><br><br> 接下来，我们需要获取网络输入和输出层的名称（除非在创建网络时使用了“ name”参数，否则就没有）。 <br><br><pre> <code class="python hljs">model.summary() &gt;&gt;&gt; Layer (type) &gt;&gt;&gt; ====================== &gt;&gt;&gt; input_7 (InputLayer) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; conv2d_283 (Conv2D) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; ... &gt;&gt;&gt; dense_14 (Dense) &gt;&gt;&gt; ====================== &gt;&gt;&gt; Total params: <span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">913</span></span>,<span class="hljs-number"><span class="hljs-number">432</span></span> &gt;&gt;&gt; Trainable params: <span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">110</span></span>,<span class="hljs-number"><span class="hljs-number">648</span></span> &gt;&gt;&gt; Non-trainable params: <span class="hljs-number"><span class="hljs-number">21</span></span>,<span class="hljs-number"><span class="hljs-number">802</span></span>,<span class="hljs-number"><span class="hljs-number">784</span></span></code> </pre><br><br> 稍后，当在Android Java应用程序中导入NN时，我们将使用输入和输出层的名称。 <br><br> 我们还可以使用以下代码获取此信息： <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">print_graph_nodes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> g = tf.GraphDef() g.ParseFromString(open(filename, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>).read()) print() print(filename) print(<span class="hljs-string"><span class="hljs-string">"=======================INPUT==================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'input'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"=======================OUTPUT=================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'output'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"===================KERAS_LEARNING=============="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'keras_learning_phase'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"==============================================="</span></span>) print() <span class="hljs-comment"><span class="hljs-comment">#def get_script_path(): # return os.path.dirname(os.path.realpath(sys.argv[0]))</span></span></code> </pre><br><br> 但是，首选方法。 <br><br> 以下函数将Keras神经网络导出为<i>pb</i>格式，这是我们将在Android中使用的格式。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">keras_to_tensorflow</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keras_model, output_dir, model_name,out_prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"output_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, log_tensorboard=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> os.path.exists(output_dir) == <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: os.mkdir(output_dir) out_nodes = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(keras_model.outputs)): out_nodes.append(out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) tf.identity(keras_model.output[i], out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) sess = K.get_session() <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> graph_util <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework graph_io init_graph = sess.graph.as_graph_def() main_graph = graph_util.convert_variables_to_constants( sess, init_graph, out_nodes) graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> log_tensorboard: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> import_pb_to_tensorboard import_pb_to_tensorboard.import_to_tensorboard( os.path.join(output_dir, model_name), output_dir)</code> </pre><br><br><p> 让我们使用这些函数来创建导出NN： <br><br></p><pre> <code class="python hljs">model = load_model(working_path + strModelFileName) keras_to_tensorflow(model, output_dir=working_path + strModelFileName, model_name=working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>) print_graph_nodes(working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>)</code> </pre><br><br> 最后一行显示了我们的神经网络的结构。 <br><br><h2> 创建具有NN功能的Android应用 </h2><br><br> 将NN导出到Android应用。 已经正规化，不应造成任何困难。 和往常一样，有多种方法可以做到这一点。 我们将使用最受欢迎（至少目前）。 <br><br> 首先，使用Android Studio创建一个新项目。 我们将偷工减料，因此它仅包含一个活动。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b3/76e/997/6b376e997b34f45359c46923f6613d60.png" alt="图片"><br><br> 如您所见，我们添加了“ assets”文件夹，并在此处复制了神经网络文件。 <br><br><h3> 摇篮文件 </h3><br><br> 我们需要对gradle文件进行一些更改。 首先，我们必须导入<i>tensorflow-android</i>库。 它用于从Java处理Tensorflow（以及相应的Keras）： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a16/091/fab/a16091fab2166f834827812611142d26.png" alt="图片"><br><br> 作为其他“很难找到”的细节，请注意版本： <i>versionCode</i>和<i>versionName</i> 。 在处理应用程序时，您需要将新版本上传到Google Play。 如果不更新版本（例如1-&gt; 2-&gt; 3 ...），您将无法执行更新。 <br><br><h3> 清单 </h3><br><br> 首先，我们的应用程序。 将会变得“沉重”-一个100 Mb的神经网络很容易容纳在现代手机的内存中，但是每次用户从Facebook“共享”图像时都打开一个单独的实例绝对不是一个好主意。 <br><br> 因此，我们将确保应用程序只有一个实例： <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">activity</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">".MainActivity"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:launchMode</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"singleTask"</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br> 通过向MainActivity添加<i>android：launchMode =“ singleTask”</i> ，我们告诉Android打开现有应用程序，而不是启动另一个实例。 <br><br> 然后，确保我们的应用程序。 出现在能够处理<i>共享</i>图像的应用程序列表中： <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Send action required to display activity in share list --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">action</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.action.SEND"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Make activity default to launch --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">category</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.category.DEFAULT"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Mime type ie what can be shared with this activity only image and text --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">data</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:mimeType</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"image/*"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br> 最后，我们需要请求功能和权限，以便该应用可以访问所需的系统功能： <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-feature</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.hardware.camera"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:required</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"true"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">= </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.WRITE_EXTERNAL_STORAGE"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.READ_PHONE_STATE"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tools:node</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"remove"</span></span></span><span class="hljs-tag"> /&gt;</span></span></code> </pre><br><br> 如果您熟悉Android编程，则本部分不会提出任何问题。 <br><br><h3> 应用的布局。 </h3><br><br> 我们将创建两种布局，一种用于纵向，另一种用于横向模式。 这是<a href="">纵向布局</a> 。 <br><br> 我们在这里拥有的功能：显示图像的大视图，相当烦人的广告列表（按下“骨头”按钮时显示），“帮助”按钮，用于从文件/库和相机中加载图像的按钮，以及最后，一个（最初隐藏的）按钮“ Process”。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f71/882/81f/f7188281ff581965c20c7e818cb0fd77.png" alt="图片"><br><br> 在活动本身中，我们将根据应用程序的状态来实现一些显示/隐藏以及启用/禁用按钮的逻辑。 <br><br><h3> 主要活动 </h3><br><br> 该活动扩展了标准的Android活动： <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MainActivity</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Activity</span></span></span></span></code> </pre><br><br> 让我们看一下负责NN操作的代码。 <br><br> 首先，NN接受位图。 最初，它是来自文件或相机的大位图（m_bitmap），然后将其转换为标准的256x256位图（m_bitmapForNn）。 我们还将图像尺寸（256）保持恒定： <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmap = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmapForNn = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> m_nImageSize = <span class="hljs-number"><span class="hljs-number">256</span></span>;</code> </pre><br><br> 我们需要告诉NN输入和输出层的名称是什么； 如果您查阅上面的清单，您会发现名称是（在我们的情况下！您的情况可能有所不同！）： <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String INPUT_NAME = <span class="hljs-string"><span class="hljs-string">"input_7_1"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String OUTPUT_NAME = <span class="hljs-string"><span class="hljs-string">"output_1"</span></span>;</code> </pre><br><br> 然后，我们声明该变量以保存TensofFlow对象。 另外，我们在资产中存储NN文件的路径： <br><br><p></p><pre>私有TensorFlowInferenceInterface tf;
私有字符串MODEL_PATH = 
	 “文件：////android_asset/dogs.pb”；
</pre><br><br> 品种的狗，向用户展示有意义的信息，而不是数组中的索引： <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String[] m_arrBreedsArray;</code> </pre><br><br> 最初，我们加载位图。 但是，NN本身需要一个RGB值数组，并且它的输出是所呈现图像是特定品种的概率数组。 因此，我们需要再添加两个数组（请注意，训练数据集中的品种数为120）： <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrPrediction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">120</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrInput = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>;</code> </pre><br><br> 加载tensorflow推理库 <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> { System.loadLibrary(<span class="hljs-string"><span class="hljs-string">"tensorflow_inference"</span></span>); }</code> </pre><br><br> 由于NN的操作很长，我们需要在单独的线程中执行它，否则很有可能会碰到系统“ app”。  “不响应”警告，更不用说破坏用户体验了。 <br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PredictionTask</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AsyncTask</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onPreExecute</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onPreExecute(); } <span class="hljs-comment"><span class="hljs-comment">// --- @Override protected Void doInBackground(Void... params) { try { # We get RGB values packed in integers # from the Bitmap, then break those # integers into individual triplets m_arrInput = new float[ m_nImageSize * m_nImageSize * 3]; int[] intValues = new int[ m_nImageSize * m_nImageSize]; m_bitmapForNn.getPixels(intValues, 0, m_nImageSize, 0, 0, m_nImageSize, m_nImageSize); for (int i = 0; i &lt; intValues.length; i++) { int val = intValues[i]; m_arrInput[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 2] = (val &amp; 0xFF) / 255f; } // --- tf = new TensorFlowInferenceInterface( getAssets(), MODEL_PATH); //Pass input into the tensorflow tf.feed(INPUT_NAME, m_arrInput, 1, m_nImageSize, m_nImageSize, 3); //compute predictions tf.run(new String[]{OUTPUT_NAME}, false); //copy output into PREDICTIONS array tf.fetch(OUTPUT_NAME, m_arrPrediction); } catch (Exception e) { e.getMessage(); } return null; } // --- @Override protected void onPostExecute(Void result) { super.onPostExecute(result); // --- enableControls(true); // --- tf = null; m_arrInput = null; # strResult contains 5 lines of text # with most probable dog breeds and # their probabilities m_strResult = ""; # What we do below is sorting the array # by probabilities (using map) # and getting in reverse order) the # first five entries TreeMap&lt;Float, Integer&gt; map = new TreeMap&lt;Float, Integer&gt;( Collections.reverseOrder()); for(int i = 0; i &lt; m_arrPrediction.length; i++) map.put(m_arrPrediction[i], i); int i = 0; for (TreeMap.Entry&lt;Float, Integer&gt; pair : map.entrySet()) { float key = pair.getKey(); int idx = pair.getValue(); String strBreed = m_arrBreedsArray[idx]; m_strResult += strBreed + ": " + String.format("%.6f", key) + "\n"; i++; if (i &gt; 5) break; } m_txtViewBreed.setVisibility(View.VISIBLE); m_txtViewBreed.setText(m_strResult); } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 在MainActivity的onCreate（）中，我们需要为“ Process”按钮添加onClickListener： </font></font><br><br><pre> <code class="java hljs">m_btn_process.setOnClickListener(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> View.OnClickListener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onClick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(View v)</span></span></span><span class="hljs-function"> </span></span>{ processImage(); } });</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> processImage（）所做的只是简单地调用我们在上面看到的线程： </font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">processImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { enableControls(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); <span class="hljs-comment"><span class="hljs-comment">// --- PredictionTask prediction_task = new PredictionTask(); prediction_task.execute(); } catch (Exception e) { e.printStackTrace(); } }</span></span></code> </pre><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 其他细节 </font></font></h3><br><br> We are not going to discuss UI-related code in this tutorial, as it is trivial and definitely is not part of «porting NN» task. However, there are few thing that should be clarified. <br><br> When we prevemted our app. from launching multiple instances, we have prevented, in the same time, a normal flow on control: if you share an image from Facebook, and then share another one, the application will not be restarted. It means that the «traditional» way of handling shared data by catching it in onCreate is not sufficient in our case, as onCreate is not called in a scenario we just created. <br><br> Here is a way to handle the situation: <br><br> 1. In onCreate of MainActivity, call the onSharedIntent function: <br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onCreate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( Bundle savedInstanceState)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onCreate(savedInstanceState); .... onSharedIntent(); ....</code> </pre><br><br> Also, add a handler for onNewIntent: <br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onNewIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Intent intent)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onNewIntent(intent); setIntent(intent); onSharedIntent(); }</code> </pre><br><br> The onSharedIntent function itself: <br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onSharedIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ Intent receivedIntent = getIntent(); String receivedAction = receivedIntent.getAction(); String receivedType = receivedIntent.getType(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (receivedAction.equals(Intent.ACTION_SEND)) { <span class="hljs-comment"><span class="hljs-comment">// If mime type is equal to image if (receivedType.startsWith("image/")) { m_txtViewBreed.setText(""); m_strResult = ""; Uri receivedUri = receivedIntent.getParcelableExtra( Intent.EXTRA_STREAM); if (receivedUri != null) { try { Bitmap bitmap = MediaStore.Images.Media.getBitmap( this.getContentResolver(), receivedUri); if(bitmap != null) { m_bitmap = bitmap; m_picView.setImageBitmap(m_bitmap); storeBitmap(); enableControls(true); } } catch (Exception e) { e.printStackTrace(); } } } } }</span></span></code> </pre><br><br> Now we either handle the shared image from onCreate (if the app was just started) or from onNewIntent if an instance was found in memory. <br><br><br><br><br> Good luck! If you like this article, please «like» it in social networks, also there are social buttons on a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">site</a> itself. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN447732/">https://habr.com/ru/post/zh-CN447732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN447718/index.html">一切都会按计划进行</a></li>
<li><a href="../zh-CN447720/index.html">物联网安全。 问题2。智能家居</a></li>
<li><a href="../zh-CN447724/index.html">智慧城市如何来临</a></li>
<li><a href="../zh-CN447728/index.html">我们计算CubeSat格式卫星的无线电线路的能量预算</a></li>
<li><a href="../zh-CN447730/index.html">电子邮件营销的演变：从QWERTYUIOP到GDPR</a></li>
<li><a href="../zh-CN447734/index.html">为什么前端应该理解UI的原理</a></li>
<li><a href="../zh-CN447736/index.html">无人机视频-社交网络的新趋势</a></li>
<li><a href="../zh-CN447738/index.html">朱利安·阿桑奇（Julian Assange）被英国警察逮捕</a></li>
<li><a href="../zh-CN447742/index.html">什么是DevOps方法，谁需要它</a></li>
<li><a href="../zh-CN447744/index.html">爬Elbrus-侦察战。 技术部分2。中断，异常，系统计时器</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>