<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👋 👨🏾‍🏫 🍇 Breve reseña del artículo "DeViSE: un modelo de inclusión visual-semántica profunda" 🚴🏽 👃🏽 🧙🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El artículo en cuestión. 
 Introduccion 


 Los sistemas de reconocimiento modernos se limitan a clasificar en un número relativamente pequeño de clas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Breve reseña del artículo "DeViSE: un modelo de inclusión visual-semántica profunda"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451738/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El artículo en cuestión.</a> </p><br><h3 id="vvedenie">  Introduccion </h3><br><p>  Los sistemas de reconocimiento modernos se limitan a clasificar en un número relativamente pequeño de clases semánticamente no relacionadas.  La atracción de información textual, incluso sin relación con las imágenes, permite enriquecer el modelo y, en cierta medida, resolver los siguientes problemas: </p><br><ol><li>  si el modelo de reconocimiento comete un error, a menudo este error no está semánticamente cerca de la clase correcta; </li><li>  no hay forma de predecir un objeto que pertenezca a una nueva clase que no se haya representado en el conjunto de datos de entrenamiento. </li></ol><br><p>  El enfoque propuesto sugiere mostrar imágenes en un rico espacio semántico en el que las etiquetas de clases más similares están más cercanas entre sí que las etiquetas de clases menos similares.  Como resultado, el modelo da menos semánticamente distante de la verdadera clase de predicciones.  Además, el modelo, teniendo en cuenta la proximidad visual y semántica, puede clasificar correctamente las imágenes relacionadas con una clase que no estaba representada en el conjunto de datos de entrenamiento. </p><a name="habracut"></a><br><h3 id="algoritm-arhitektura">  Algoritmo  Arquitectura </h3><br><ol><li>  Preparamos previamente el modelo de lenguaje, que proporciona buenas incorporaciones semánticamente significativas.  La dimensión del espacio es n.  A continuación, n se tomará igual a 500 o 1000. </li><li>  Preparamos previamente el modelo visual, que clasifica los objetos en 1000 clases. </li><li>  Cortamos la última capa softmax del modelo visual pre-entrenado y agregamos una capa completamente conectada de 4096 a n neuronas.  Entrenamos el modelo resultante para cada imagen para predecir la incrustación correspondiente a la etiqueta de la imagen. </li></ol><br><p>  Vamos a explicar con la ayuda de los mapeos.  Deje que LM sea un modelo de lenguaje, VM sea un modelo visual con softmax cortado y una capa completamente conectada agregada, imagen I, etiqueta L de imagen, incrustación de etiqueta LM (L) en el espacio semántico.  Luego, en el tercer paso, entrenamos la VM para que: </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">V</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3">M</span><span class="MJXp-mo" id="MJXp-Span-4" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-5">I</span><span class="MJXp-mo" id="MJXp-Span-6" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-7" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">L</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9">M</span><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-11">L</span><span class="MJXp-mo" id="MJXp-Span-12" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.726ex" height="2.66ex" viewBox="0 -832 7632.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMATHI-56" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMATHI-4D" x="769" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMAIN-28" x="1821" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMATHI-49" x="2210" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMAIN-29" x="2715" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMAIN-3D" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMATHI-4C" x="4438" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMATHI-4D" x="5120" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMAIN-28" x="6171" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMATHI-4C" x="6561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/451738/&amp;usg=ALkJrhjivnc4I_QwcuRVUsoe8z1CO2uybw#MJMAIN-29" x="7242" y="0"></use></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> VM (I) = LM (L) </script></p><br><p>  Arquitectura: </p><br><img src="https://habrastorage.org/webt/59/ki/-t/59ki-torfbo8ufarpy8vlwdalze.png"><br><h3 id="yazykovaya-model">  Modelo de idioma </h3><br><p>  Para aprender el modelo de lenguaje, se utilizó el modelo skip-gram, un corpus de 5.4 billones de palabras tomadas de wikipedia.org.  El modelo usó una capa jerárquica softmax para predecir conceptos relacionados, una ventana - 20 palabras, el número de pasadas a través del cuerpo - 1. Se estableció experimentalmente que el tamaño de incrustación es mejor para tomar 500-1000. </p><br><p>  La imagen de la disposición de las clases en el espacio muestra que el modelo ha aprendido una estructura semántica rica y cualitativa.  Por ejemplo, para una determinada especie de tiburones en el espacio semántico resultante, 9 vecinos más cercanos son los otros 9 tipos de tiburones. </p><br><img src="https://habrastorage.org/webt/wd/wu/-b/wdwu-bbzhgw2jwdcyn_bnxa9h_o.png"><br><h3 id="vizualnaya-model">  Modelo visual </h3><br><p>  La arquitectura que ganó el concurso ILSVRC 2012 se tomó como modelo visual.  Se eliminó Softmax y se agregó una capa completamente conectada para obtener el tamaño de incrustación deseado en la salida. </p><br><h3 id="funkciya-poter">  Función de pérdida </h3><br><p>  Resultó que la elección de la función de pérdida es importante.  Se usó una combinación de similitud de coseno y pérdida de rango de bisagra.  La función de pérdida alentó un producto escalar más grande entre el vector resultante de la red visual y la incrustación de la etiqueta correspondiente, y multado por un producto escalar grande entre el resultado de la red visual y las incrustaciones de posibles etiquetas de imagen al azar.  El número de etiquetas aleatorias arbitrarias no fue fijo, pero estaba limitado por la condición bajo la cual la suma de productos escalares con etiquetas falsas se convirtió en más que un producto escalar con una etiqueta válida menos un margen fijo (constante igual a 0.1).  Por supuesto, todos los vectores fueron pre-normalizados. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-13"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">p</span><span class="MJXp-mrow" id="MJXp-Span-15"><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0em;">é</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-21">a</span><span class="MJXp-mo" id="MJXp-Span-22" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23">I</span><span class="MJXp-mo" id="MJXp-Span-24" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25">L</span><span class="MJXp-mo" id="MJXp-Span-26" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-27" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-28">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30">u</span><span class="MJXp-msubsup" id="MJXp-Span-31"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32" style="margin-right: 0.05em;">m</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-33" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-34">j</span></span></span><span class="MJXp-mrow" id="MJXp-Span-35"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38">x</span><span class="MJXp-mo" id="MJXp-Span-39" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-40">0</span><span class="MJXp-mo" id="MJXp-Span-41" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-42">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-47">n</span><span class="MJXp-mo" id="MJXp-Span-48" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mo" id="MJXp-Span-49" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-50">L</span><span class="MJXp-mo" id="MJXp-Span-51" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52">V</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53">M</span><span class="MJXp-mo" id="MJXp-Span-54" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55">I</span><span class="MJXp-mo" id="MJXp-Span-56" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-57" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-58" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mo" id="MJXp-Span-59" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-63">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-64">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-65">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-66">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69">o</span><span class="MJXp-msubsup" id="MJXp-Span-70"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-71" style="margin-right: 0.05em;">L</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-72" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-73" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-74">V</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-75">M</span><span class="MJXp-mo" id="MJXp-Span-76" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-77">I</span><span class="MJXp-mo" id="MJXp-Span-78" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-79" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-80" style="margin-left: 0em; margin-right: 0em;">]</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> pérdida (I, L) = \ sum_ {j} {max [0, margen - (L, VM (I)) + (incorrecto L_j, VM (I))]} </script></p><br><h3 id="process-trenirovki">  Proceso de entrenamiento </h3><br><p>  Al principio, solo se entrenó la última capa totalmente conectada agregada, el resto de la red no actualizó el peso.  En este caso, se utilizó el método de optimización SGD.  Luego, toda la red visual se descongeló y se entrenó utilizando el optimizador Adagrad para que durante la propagación hacia atrás en diferentes capas de la red los gradientes se escalen correctamente. </p><br><h3 id="predskazanie">  Predicción </h3><br><p>  Durante la predicción, a partir de la imagen que usa la red visual, obtenemos algunos vectores en nuestro espacio semántico.  A continuación, encontramos los vecinos más cercanos, es decir, algunas etiquetas posibles y, de manera especial, las mostramos de nuevo en los sistemas de imagen de ImageNet para su puntuación.  El procedimiento para la última visualización no es tan simple, ya que las etiquetas en ImageNet son un conjunto de sinónimos, no solo una etiqueta.  Si el lector está interesado en conocer los detalles, recomiendo el artículo original (apéndice 2). </p><br><h2 id="rezultaty">  Resultados </h2><br><p>  El resultado del modelo DEVISE se comparó con dos modelos: </p><br><ol><li>  Modelo de referencia de Softmax: un modelo de visión de vanguardia (SOTA, en el momento de la publicación) </li><li>  El modelo de incrustación aleatorio es una versión del modelo DEVISE descrito, donde las incrustaciones no se aprenden mediante el modelo de lenguaje, sino que se inicializan arbitrariamente. </li></ol><br><p>  Para evaluar la calidad, se utilizaron métricas hit @ k planas y métrica @ k de precisión jerárquica.  La métrica "plana" hit @ k es el porcentaje de imágenes de prueba para las cuales la etiqueta correcta está presente entre las primeras k opciones predichas.  La precisión jerárquica @ k métrica se utilizó para evaluar la calidad de la correspondencia semántica.  Esta métrica se basó en la jerarquía de etiquetas en ImageNet.  Para cada etiqueta verdadera y k fija, el conjunto <br>  etiquetas semánticamente correctas: lista de verdad básica.  Obtener la predicción (vecinos más cercanos) fue el porcentaje de intersección con la lista de verdad del terreno. </p><br><img src="https://habrastorage.org/webt/p_/i7/dm/p_i7dmm6gvhc8h9ndzd20hrryye.png"><br><p>  Los autores esperaban que el modelo softmax mostrara los mejores resultados en la métrica plana debido al hecho de que minimiza la pérdida de entropía cruzada, lo cual es muy adecuado para las métricas hit @ k "planas".  Los autores se sorprendieron de lo cerca que está el modelo DEVISE del modelo softmax, alcanza la paridad en k grande e incluso supera en k = 20. </p><br><p>  En la métrica jerárquica, el modelo DEVISE se muestra en todo su esplendor y supera al béisbol softmax en un 3% para k = 5 y en un 7% para k = 20. </p><br><h3 id="zero-shot-learning">  Aprendizaje de tiro cero </h3><br><p>  Una ventaja particular del modelo DEVISE es la capacidad de proporcionar una predicción adecuada para las imágenes cuyas etiquetas la red nunca ha visto durante el entrenamiento.  Por ejemplo, durante el entrenamiento, la red vio imágenes etiquetadas de tiburón tigre, tiburón toro y tiburón azul y nunca alcanzó la marca de tiburón.  Dado que el modelo de lenguaje tiene una representación de tiburón en el espacio semántico y está cerca de incrustaciones de diferentes tipos de tiburón, es muy probable que el modelo dé una predicción adecuada.  Esto se llama la capacidad de generalizar: generalización. </p><br><p>  Demostremos algunos ejemplos de predicciones Zero-Shot: </p><br><img src="https://habrastorage.org/webt/50/w6/bc/50w6bceiaq61y6ohmehsmqzrrhc.png"><br><p>  Tenga en cuenta que el modelo DEVISE, incluso en sus supuestos erróneos, está más cerca de la respuesta correcta que los supuestos erróneos del modelo softmax. </p><br><p>  Entonces, el modelo presentado pierde un poco a softmax en la línea de base en métricas planas, pero gana significativamente en precisión jerárquica @ k métrica.  El modelo tiene la capacidad de generalizar, produciendo predicciones adecuadas para imágenes cuyas etiquetas no ha cumplido la red (aprendizaje de disparo cero). </p><br><p>  El enfoque descrito se puede implementar fácilmente, ya que se basa en dos modelos previamente entrenados: el lenguaje y el visual. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/451738/">https://habr.com/ru/post/451738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451720/index.html">FIAS cargando en la base de datos en MSSQLSERVER por medios improvisados ​​(SQLXMLBULKLOAD). Cómo (probablemente) no necesita hacerse</a></li>
<li><a href="../451722/index.html">Biblioteca de widgets asíncronos Qt-async</a></li>
<li><a href="../451724/index.html">Skyrmion a skyrmion discord: skyrmions polares tridimensionales en ferroelastia</a></li>
<li><a href="../451726/index.html">Buscando trabajo en el extranjero: 7 consejos simples para profesionales de TI</a></li>
<li><a href="../451728/index.html">RESTinio es un servidor HTTP asíncrono. Asincrónico</a></li>
<li><a href="../451742/index.html">Un día antes de DotNext 2019 Piter. Anuncio de transmisión gratuita</a></li>
<li><a href="../451746/index.html">MegaSlerm para ingenieros y arquitectos Kubernetes</a></li>
<li><a href="../451748/index.html">Supervisión del estado de SSD en matrices Qsan</a></li>
<li><a href="../451750/index.html">Libro "Elasticsearch, Kibana, Logstash y los motores de búsqueda de la próxima generación"</a></li>
<li><a href="../451752/index.html">30 aniversario de la inseguridad desenfrenada</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>