<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚Ü©Ô∏è üõ≥Ô∏è ‚òÇÔ∏è Teoria Geral e Arqueologia da Virtualiza√ß√£o x86 üßóüèæ üë∞üèæ üëºüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="1. Introdu√ß√£o 
 Equipe de autores 
 Postado por Anton Zhbankov ( AntonVirtual , cloudarchitect.cc ) 
 Co-autores: Grigory Pryalukhin , Evgeny Parfenov...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teoria Geral e Arqueologia da Virtualiza√ß√£o x86</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/474776/"><h2>  1. Introdu√ß√£o </h2><br><h4>  Equipe de autores </h4><br>  Postado por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Anton Zhbankov</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">AntonVirtual</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cloudarchitect.cc</a> ) <br>  Co-autores: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Grigory Pryalukhin</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Evgeny Parfenov</a> <br><br><h3>  Conceitos gerais de virtualiza√ß√£o </h3><br>  Eu tive que ver muitas interpreta√ß√µes do que <i>√© a virtualiza√ß√£o</i> e ouvir muita controv√©rsia, nem um pouco mais perto de discutir o resultado pr√°tico.  E como voc√™ sabe, o argumento de duas pessoas inteligentes se resume a um debate sobre defini√ß√µes.  Vamos definir o que √© virtualiza√ß√£o e o que vem dela. <br><br>  Provavelmente, a defini√ß√£o mais pr√≥xima de virtualiza√ß√£o ser√° "abstra√ß√£o" da programa√ß√£o orientada a objetos.  Ou, se traduzido para o russo normal, isso est√° ocultando a implementa√ß√£o por tr√°s de uma interface abstrata.  O que, √© claro, explicava tudo de uma vez.  Vamos tentar novamente, mas para aqueles que n√£o estudaram programa√ß√£o. <br><blockquote>  Virtualiza√ß√£o - oculta uma implementa√ß√£o espec√≠fica por tr√°s de um m√©todo universal padronizado de acesso a recursos / dados. </blockquote><br>  Se voc√™ tentar colocar essa defini√ß√£o em pr√°tica, ela funcionar√° em assuntos completamente inesperados.  Vamos dizer o rel√≥gio.  Assim, um rel√≥gio de sol foi inventado h√° v√°rios milhares de anos e, na Idade M√©dia, um mec√¢nico foi inventado.  O que h√° em comum?  O sol e algumas engrenagens?  Algum tipo de bobagem.  E ent√£o osciladores de quartzo e tudo mais. <br>  A linha inferior √© que temos uma interface padr√£o - um ponteiro ou ponteiro digital, que em uma forma padr√£o universal indica a hora atual.  Mas importa para n√≥s como especificamente esse mecanismo √© implementado dentro da caixa, se o tempo √© indicado com precis√£o suficiente para n√≥s? <br>  ‚ÄúDeixe-me‚Äù, voc√™ pode dizer, ‚Äúmas eu pensei que a virtualiza√ß√£o fosse sobre m√°quinas, processadores e assim por diante! <br>  Sim, trata-se de carros e processadores, mas este √© apenas um caso especial.  Vejamos de maneira mais ampla, uma vez que o artigo afirma corajosamente uma teoria geral. <br><a name="habracut"></a><br><h2>  POZOR! </h2><br><h3>  Uwaga!  Achtung!  Pozor! </h3><br>  Este artigo tem um objetivo <b>educacional geral</b> de vincular um monte de tecnologias e palavras assustadoras, juntamente com a hist√≥ria, a uma determinada estrutura e, devido a essa circunst√¢ncia, cont√©m uma quantidade significativa de simplifica√ß√µes <b>intencionais</b> .  Obviamente, ele tamb√©m cont√©m um grande n√∫mero de omiss√µes irritantes e at√© mesmo erros brutos com erros de digita√ß√£o.  A cr√≠tica construtiva s√≥ √© bem-vinda, especialmente na forma de "Deixe-me lhe lembrar esta parte". <br><br><h2>  Tipos de virtualiza√ß√£o </h2><br>  Voltemos dos conceitos completamente abstratos aos mais familiares aos nossos computadores amados. <br><br><h3>  Virtualiza√ß√£o de armazenamento </h3><br>  O primeiro, provavelmente, √© o tipo de virtualiza√ß√£o que um nerd iniciante encontra - virtualiza√ß√£o de um sistema de armazenamento de dados.  Nesse caso, o sistema de armazenamento √© usado n√£o no sentido de uma grande matriz com discos conectados via canal de fibra, mas como um subsistema l√≥gico respons√°vel pelo armazenamento de dados a longo prazo. <br><br><h4>  FS -&gt; LBA -&gt; CHS </h4><br>  Veja o caso mais simples de um sistema de armazenamento em um √∫nico disco magn√©tico r√≠gido.  O formato usual para trabalhar com dados s√£o os arquivos que est√£o na unidade l√≥gica.  O arquivo pode ser aberto, lido e fechado.  Mas um objeto como um arquivo simplesmente n√£o existe fisicamente - existe apenas uma maneira de acessar determinados blocos de dados usando o m√©todo de endere√ßamento no formato ‚Äúdrive: \ folder1 \ folder2 \ file‚Äù.  I.e.  conhecemos a primeira camada de virtualiza√ß√£o - de mnem√¥nico e compreens√≠vel a humanos, traduzimos tudo em endere√ßos compreens√≠veis do sistema.  Nas tabelas de metadados, o driver do sistema de arquivos procura por quais tipos de blocos de dados existem e obtemos o endere√ßo no sistema LBA (Logical Block Addressing).  No sistema LBA, os blocos t√™m um tamanho fixo e se seguem linearmente, ou seja,  de alguma forma, pode ter a ver com o armazenamento de dados em fita magn√©tica, mas o disco r√≠gido √© de alguma forma completamente diferente!  E aqui vamos para a segunda camada de virtualiza√ß√£o - tradu√ß√£o do endere√ßamento LBA para CHS (cilindro / cabe√ßote / setor). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e95/940/bd4/e95940bd4389a0187c2bf3d82406e118.png" alt="imagem"><br><br>  O CHS, por sua vez, j√° no controlador de disco r√≠gido come√ßa a se traduzir em par√¢metros f√≠sicos para leitura, mas essa √© uma hist√≥ria completamente diferente. <br>  Mesmo em um acesso simples ao arquivo para, digamos, visualizar um vidosik com memasics, encontramos tr√™s camadas de virtualiza√ß√£o imediatamente. <br>  Tudo seria muito simples se as camadas n√£o come√ßassem a se sobrepor em ordem aleat√≥ria e de v√°rias maneiras. <br><br><h4>  RAID </h4><br>  A pr√≥xima camada de virtualiza√ß√£o, que muitas pessoas erradamente consideram n√£o ser virtualiza√ß√£o, √© o RAID (matriz redundante de discos independentes / baratos). <br><br>  O principal recurso do RAID no contexto dos conceitos discutidos n√£o √© sua capacidade de proteger dados contra a falha de um disco f√≠sico espec√≠fico.  O RAID fornece um segundo n√≠vel de endere√ßamento LBA sobre v√°rios endere√ßos LBA independentes (√†s vezes muitos).  Como podemos acessar o RAID, independentemente do n√≠vel RAID, exatamente da mesma maneira que um √∫nico disco sem RAID, podemos dizer com confian√ßa: <blockquote>  RAID √© virtualiza√ß√£o de disco. </blockquote><br>  Al√©m disso, o controlador RAID n√£o cria apenas um grande disco virtual a partir de v√°rios discos f√≠sicos, mas pode criar um n√∫mero arbitr√°rio deles adicionando outra camada de virtualiza√ß√£o. <br><br><h3>  Ver virtualiza√ß√£o </h3><br>  O pr√≥ximo tipo de virtualiza√ß√£o, que muitos de n√≥s usamos quase todos os dias, mas n√£o consideramos virtualiza√ß√£o, √© uma conex√£o remota √† √°rea de trabalho. <br><br>  Servidores de terminal, VDI e at√© apenas RDP via VPN para o servidor s√£o todos virtualiza√ß√£o de sess√£o.  Usando uma interface padr√£o (monitor, teclado, mouse), trabalhamos com uma m√°quina real ou com um design incompreens√≠vel de uma √°rea de trabalho virtual em um clone vinculado a um aplicativo em cont√™iner, do qual transferimos dados atrav√©s de um buffer para um aplicativo com entrega de streaming.  Ou n√£o, quem descobrir√°, al√©m de quem o projetou? <br><br><h2>  Introdu√ß√£o √† virtualiza√ß√£o x86 </h2><br><h3>  Hist√≥rico e vis√£o geral dos processadores </h3><br><h4>  Execu√ß√£o do programa </h4><br>  Na primeira li√ß√£o de um curso especial de programa√ß√£o, Vladimir Denisovich Lelyukh (descanse em paz para ele) disse aos alunos: o computador, apesar do nome, n√£o pode contar, pode fingir que pode contar.  Mas se algo parece um pato, anda como um pato e grasna como um pato, de um ponto de vista pr√°tico, √© um pato. <br><br>  Vamos tentar lembrar disso para uso pr√°tico adicional. <br><br>  O computador, e especificamente o processador, na verdade n√£o faz nada - apenas espera alguns par√¢metros de entrada em determinados lugares e, em seguida, atrav√©s de uma terr√≠vel magia negra, fornece alguns resultados em determinados locais. <br><br>  Um programa nesse caso √© um certo fluxo de comandos executados estritamente em sequ√™ncia, como resultado do qual esperamos ver um determinado resultado. <br>  Mas se o programa estiver em execu√ß√£o, como os dados podem ser inseridos?  E, em geral, de alguma forma, interagir em um computador? <br><br>  Para isso, as interrup√ß√µes de hardware foram inventadas.  O usu√°rio pressiona uma tecla - o controlador do teclado sinaliza isso e h√° uma interrup√ß√£o na execu√ß√£o do encadeamento de c√≥digo atual.  Os endere√ßos dos manipuladores de interrup√ß√£o s√£o registrados em uma √°rea de mem√≥ria espec√≠fica e, ap√≥s salvar o estado atual, o controle √© transferido para o manipulador de interrup√ß√£o.  Por sua vez, o manipulador deve, em teoria, processar tudo rapidamente, depois ele e o manipulador, anotam a tecla pressionada no buffer desejado e retornam o controle novamente.  Assim, o aplicativo parece estar em execu√ß√£o e podemos interagir com o sistema. <br><br>  Manipuladores de interrup√ß√£o (e o principal tipo de manipuladores s√£o drivers de dispositivo) t√™m a oportunidade de entrar em um modo de processador especial, quando outras interrup√ß√µes n√£o podem ser implementadas antes de sair desse modo.  O que no final muitas vezes levava a um problema de suspens√£o - um erro no driver n√£o permitia sair da interrup√ß√£o. <br><br><h4>  Multitarefa </h4><br>  O que fazer em uma situa√ß√£o se for necess√°rio executar v√°rios programas (fluxos de c√≥digo com suas estruturas de dados e mem√≥ria) ao mesmo tempo?  Obviamente, se houver mais fluxos de c√≥digo do que dispositivos capazes de execut√°-los, isso √© um problema. <br><br>  A pseudo-multitarefa aparece quando uma tarefa √© executada ao alternar diretamente para ela. <br><br>  No futuro, uma cooperativa (multitarefa n√£o-preemptiva) aparece - a pr√≥pria tarefa execut√°vel entende que n√£o precisa mais de recursos do processador e d√° controle a outra pessoa.  Mas tudo isso n√£o √© suficiente. <br><br>  E aqui novamente interrup√ß√µes + capacidade de fingir vir em nosso socorro.  Realmente n√£o importa para o usu√°rio que eles sejam executados estritamente simultaneamente, basta parecer assim. <br>  Portanto, um manipulador √© simplesmente desligado para interromper o cron√¥metro, que come√ßa a controlar qual fluxo de c√≥digo deve ser executado a seguir.  Se o timer for acionado com bastante frequ√™ncia (digamos 15ms), ent√£o para o usu√°rio tudo parecer√° uma opera√ß√£o paralela.  E, portanto, h√° uma multitarefa moderna de exclus√£o. <br><br><h4>  Modo real </h4><br>  O modo de processador real neste artigo pode ser descrito de maneira simples - toda a mem√≥ria est√° dispon√≠vel para todos.  Qualquer aplicativo, incluindo malware (malware, software malicioso), pode acessar em qualquer lugar, tanto para leitura quanto para grava√ß√£o. <br><br>  Esse √© o modo inicial de opera√ß√£o da fam√≠lia de processadores Intel x86. <br><br><h4>  Modo protegido </h4><br>  Em 1982, surgiu uma inova√ß√£o no processador Intel 80286 (a seguir simplesmente 286) - um modo de opera√ß√£o protegido, que trouxe inova√ß√µes na organiza√ß√£o do trabalho com mem√≥ria (por exemplo, aloca√ß√£o de tipos de segmentos de mem√≥ria - c√≥digo, dados, pilha).  Mas a coisa mais importante que o processador 286 trouxe para o mundo x86 √© o conceito de an√©is de prote√ß√£o, que ainda usamos. <br><br>  O conceito de an√©is de prote√ß√£o apareceu originalmente no Multics OS para o mainframe GE645 (1967) com uma implementa√ß√£o parcialmente de software e com hardware completo j√° em 1970 no sistema Honeywell 6180. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a9/249/522/2a9249522d8bdb211dbb4eb0aec70b75.png" alt="imagem"><br><br>  A id√©ia b√°sica dos an√©is de defesa se assemelha a fortalezas medievais de v√°rios n√≠veis; a mais valiosa fica no centro atr√°s das m√∫ltiplas muralhas.  Nesse caso, o mais valioso √© o acesso direto ilimitado a qualquer √°rea da RAM e o controle sobre todos os processos.  Eles s√£o possu√≠dos por processos que operam no anel zero de prote√ß√£o.  Atr√°s da parede, no primeiro toque, processos menos importantes funcionam, como drivers de dispositivo e, finalmente, aplicativos de usu√°rio.  O princ√≠pio √© simples - de dentro voc√™ pode sair, mas de fora para dentro √© proibido.  I.e.  nenhum processo do usu√°rio pode acessar a mem√≥ria do kernel do SO, como era poss√≠vel no modo real anteriormente. <br><br>  Na primeira implementa√ß√£o completa do Honeywell 6180, 8 an√©is de prote√ß√£o foram implementados, mas a Intel decidiu simplificar o circuito para 4, dos quais, na pr√°tica, os fabricantes de sistemas operacionais come√ßaram a usar apenas dois - zero e terceiro. <br><br><h4>  32bit </h4><br>  Em 1985, foi lan√ßado outro processador extremamente importante de arquitetura na linha x86 - 80386 (doravante 386), que implementava o endere√ßamento de mem√≥ria de 32 bits e usava instru√ß√µes de 32 bits.  E, claro, virtualiza√ß√£o de mem√≥ria.  Como j√° mencionado, a virtualiza√ß√£o √© a oculta√ß√£o da implementa√ß√£o real por meio do fornecimento de recursos "virtuais" artificiais.  Nesse caso, estamos falando sobre endere√ßamento de mem√≥ria.  O segmento de mem√≥ria tem seu pr√≥prio endere√ßamento, o que n√£o tem nada a ver com a localiza√ß√£o real das c√©lulas de mem√≥ria. <br>  O processador acabou sendo t√£o requisitado que foi produzido antes de 2007. <br>  A arquitetura em termos da Intel √© chamada IA32. <br><br><h4>  64bit </h4><br>  Obviamente, mesmo sem virtualiza√ß√£o em meados dos anos 2000, o setor j√° estava atingindo os limites de 32 bits.  Havia solu√ß√µes alternativas parciais na forma de PAE (Extens√£o de Endere√ßo F√≠sico), mas complicaram e desaceleraram o c√≥digo.  A transi√ß√£o para 64 bits foi uma conclus√£o precipitada. <br><br>  A AMD apresentou sua vers√£o da arquitetura, chamada AMD64.  Na Intel, eles esperavam a plataforma IA64 (Intel Architecture 64), que tamb√©m conhecemos com o nome Itanium.  No entanto, o mercado atendeu a essa arquitetura sem muito entusiasmo e, como resultado, a Intel foi for√ßada a implementar seu pr√≥prio suporte √†s instru√ß√µes AMD64, primeiro denominadas EM64T e depois apenas Intel 64. <br><br>  Por fim, todos conhecemos essa arquitetura como AMD64, x86-64, x86_64 ou, √†s vezes, x64. <br><br>  Como o principal uso de servidores da √©poca era f√≠sico, sem virtualiza√ß√£o, uma coisa t√©cnica engra√ßada aconteceu com os primeiros processadores de 64 bits na virtualiza√ß√£o.  Os hipervisores aninhados eram frequentemente usados ‚Äã‚Äãcomo servidores de laborat√≥rio; nem todos podiam pagar v√°rios agrupamentos de servidores f√≠sicos.  E, no final, descobriu-se que a VM de carga no hipervisor incorporado s√≥ poderia funcionar no modo de 32 bits. <br><br>  Nos primeiros processadores x86-64, os desenvolvedores, mantendo total compatibilidade com o modo operacional de 32 bits, descartaram uma parte significativa da funcionalidade no modo de 64 bits.  Nesse caso, o problema era simplificar bastante a segmenta√ß√£o da mem√≥ria.  A capacidade de garantir a inviolabilidade de um pequeno peda√ßo de mem√≥ria na VM em que o manipulador de exce√ß√£o do hypervisor funcionava foi removida.  Consequentemente, o sistema operacional convidado conseguiu modific√°-lo. <br>  Posteriormente, a AMD retornou a possibilidade de limitar segmentos, e a Intel simplesmente esperou pela introdu√ß√£o da virtualiza√ß√£o de hardware. <br><br><h4>  UMA </h4><br>  Os sistemas multiprocessadores X86 come√ßaram a trabalhar no modo UMA (Acesso uniforme √† mem√≥ria), no qual a dist√¢ncia de qualquer processador (atraso no acesso a uma c√©lula de mem√≥ria) a qualquer barra de mem√≥ria √© a mesma.  Nos processadores Intel, esse esquema de trabalho foi preservado mesmo ap√≥s o surgimento de processadores com v√°rios n√∫cleos at√© a gera√ß√£o 54xx (Harpertown).  Come√ßando com a gera√ß√£o 55xx (Nehalem), os processadores mudaram para a arquitetura NUMA. <br><br>  Do ponto de vista da l√≥gica de execu√ß√£o, essa √© a apar√™ncia de encadeamentos de hardware adicionais, nos quais voc√™ pode atribuir fluxos de c√≥digo para execu√ß√£o em paralelo. <br><br><h4>  NUMA </h4><br>  NUMA (acesso n√£o uniforme √† mem√≥ria) - arquitetura com acesso desigual √† mem√≥ria.  Dentro dessa arquitetura, cada processador possui sua pr√≥pria mem√≥ria local, que √© acessada diretamente com baixa lat√™ncia.  A mem√≥ria de outros processadores √© acessada indiretamente com atrasos mais altos, o que resulta em desempenho reduzido. <br><br>  Para os processadores Intel Xeon Scalable v2 de 2019, a arquitetura interna ainda permanece UMA no soquete, transformando-se em NUMA para outros soquetes (embora n√£o seja realmente, e apenas finge ser).  Os processadores Opteron da AMD tinham arquitetura NUMA mesmo durante o UMA Xeon mais antigo e, em seguida, o NUMA ficou dentro do soquete at√© a √∫ltima gera√ß√£o de Roma, na qual retornaram ao NUMA = soquete. <br><br><h3>  M√°quina virtual </h3><br>  M√°quina virtual , a plataforma host) ou virtualizar alguma plataforma e criar ambientes que isolem programas e at√© sistemas operacionais uns dos outros.  Wikipedia <br>  Neste artigo, vamos dizer "m√°quina virtual", significando "m√°quinas virtuais do sistema", permitindo simular completamente todos os recursos e hardware na forma de constru√ß√µes de software. <br>  Existem dois tipos principais de software para criar m√°quinas virtuais - com full e resp.  virtualiza√ß√£o incompleta. <br><br>  <b>A virtualiza√ß√£o completa</b> √© uma abordagem na qual todo o hardware, incluindo o processador, √© emulado.  Permite criar ambientes independentes de hardware e executar, por exemplo, o SO e o software aplicativo para a plataforma x86 nos sistemas SPARC ou os conhecidos emuladores Spectrum com o processador Z80 no familiar x86.  O outro lado da total independ√™ncia √© a alta sobrecarga para virtualizar o processador e o baixo desempenho geral. <br><br>  <b>A virtualiza√ß√£o incompleta</b> √© uma abordagem na qual nem 100% do hardware √© virtualizado.  Como a virtualiza√ß√£o incompleta √© a mais comum no setor, falaremos sobre isso.  Sobre plataformas e tecnologias de m√°quinas virtuais do sistema com virtualiza√ß√£o incompleta para a arquitetura x86.  Nesse caso, h√° virtualiza√ß√£o incompleta do processador, ou seja,  com exce√ß√£o da substitui√ß√£o parcial ou oculta√ß√£o de determinadas chamadas do sistema, o c√≥digo bin√°rio da m√°quina virtual √© executado diretamente pelo processador. <br><br><h4>  Virtualiza√ß√£o de software </h4><br>  A conseq√º√™ncia √≥bvia da arquitetura do processador e os h√°bitos dos sistemas operacionais de trabalhar no anel zero foi o problema - o kernel do sistema operacional convidado n√£o pode funcionar no local habitual.  O anel zero √© ocupado pelo hipervisor, e voc√™ s√≥ precisa deixar o sistema operacional convidado chegar l√° - por um lado, retornamos ao modo real com todas as consequ√™ncias e, por outro lado, o sistema operacional convidado n√£o espera ningu√©m l√°, destruindo instantaneamente todas as estruturas de dados e largando o carro. <br><br>  Mas tudo foi decidido com simplicidade: como para o hypervisor, o SO convidado √© apenas um conjunto de p√°ginas de mem√≥ria com acesso direto total e o processador virtual √© apenas uma fila de comandos, por que n√£o reescrev√™-los?  Imediatamente, o hipervisor lan√ßa fora da fila de instru√ß√µes para execu√ß√£o no processador virtual todas as instru√ß√µes que requerem privil√©gios de toque zero, substituindo-as por menos privilegiadas.  Mas o resultado dessas instru√ß√µes √© apresentado exatamente da mesma maneira como se o sistema operacional convidado estivesse no anel zero.  Assim, voc√™ pode virtualizar qualquer coisa, at√© a completa aus√™ncia de um sistema operacional convidado. <br>  Essa abordagem foi implementada pela equipe de desenvolvimento em 1999 no produto VMware Workstation e, em 2001, nos hipervisores do servidor GSX (o segundo tipo, como Workstation) e ESX (o primeiro tipo). <br><br><h4>  Paravirtualiza√ß√£o </h4><br>  <b>A paravirtualiza√ß√£o</b> √© um conceito muito simples, que pressup√µe que o SO convidado saiba que est√° em uma m√°quina virtual e saiba como acessar o SO host para determinadas fun√ß√µes do sistema.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Isso elimina o problema de emula√ß√£o do anel zero - o sistema operacional convidado sabe que n√£o est√° no zero e se comporta de acordo. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A para-virtualiza√ß√£o em x86 apareceu em 2003 com o projeto Linux Xen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Certas fun√ß√µes paravirtualizadas tamb√©m s√£o implementadas em hipervisores com virtualiza√ß√£o completa por meio de drivers virtuais especiais em sistemas operacionais convidados que se comunicam com o hipervisor para reduzir a sobrecarga da virtualiza√ß√£o. </font><font style="vertical-align: inherit;">Por exemplo, o VMware ESXi for VMs possui um adaptador SCSI paravirtual PVSCSI, que melhora o desempenho geral de VMs com opera√ß√µes intensivas de disco, como DBMSs carregados. </font><font style="vertical-align: inherit;">Os drivers para dispositivos paravirtuais v√™m em pacotes adicionais (por exemplo, VMware Tools) ou j√° est√£o inclu√≠dos nas distribui√ß√µes Linux (open-vm-tools).</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Virtualiza√ß√£o de hardware </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com o desenvolvimento e o crescimento da popularidade da virtualiza√ß√£o, surgiu o desejo de ambos os fabricantes de plataformas reduzirem os custos de suporte e, do ponto de vista da seguran√ßa, garantir a prote√ß√£o no hardware. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O problema foi resolvido de uma maneira muito simples - as tecnologias de virtualiza√ß√£o de hardware propriet√°rias Intel VT-x e AMD-V foram adicionadas, se descartarmos detalhes t√©cnicos profundos, menos o primeiro anel de prote√ß√£o para o hipervisor. </font><font style="vertical-align: inherit;">Assim, a situa√ß√£o do trabalho no anel zero familiar ao SO foi finalmente estabelecida.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tipos de hipervisores </font></font></h3><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tipo 2 (hospedado) </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os hipervisores do segundo tipo s√£o aplicativos executados sobre o sistema operacional host. Todas as chamadas de m√°quinas virtuais s√£o tratadas pelo sistema operacional host upstream. Os hipervisores do segundo tipo s√£o severamente limitados em desempenho, uma vez que a aplica√ß√£o do hipervisor, n√£o tendo o direito de aloca√ß√£o exclusiva de recursos de computa√ß√£o, √© for√ßada a competir por eles com outros aplicativos do usu√°rio. Em termos de seguran√ßa, os hipervisores do tipo 2 dependem diretamente das pol√≠ticas de seguran√ßa do sistema operacional do usu√°rio e de sua vulnerabilidade a ataques. Hoje, existe uma opini√£o un√¢nime no setor de que essas plataformas de virtualiza√ß√£o para o n√≠vel corporativo n√£o s√£o adequadas. No entanto, eles s√£o adequados para o desenvolvimento entre plataformas e a implanta√ß√£o de stands diretamente nas m√°quinas dos desenvolvedores de software, pois s√£o f√°ceis de gerenciar e implantar.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exemplos do segundo tipo de hypervisor: VMware Workstation / Fusion, Oracle VM VirtualBox, Parallels Desktop, VMware Server (ex-GSX), Microsoft Virtual Server 2005 </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tipo 1 (bare-metal) </font></font></h4><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os hipervisores do primeiro tipo n√£o requerem um sistema operacional de uso geral, diferentemente dos anteriores. O hipervisor em si √© um mon√≥lito que controla a aloca√ß√£o de recursos de computa√ß√£o e E / S. No anel de seguran√ßa zero, existe um micro-n√∫cleo, sobre o qual todas as estruturas de controle funcionam. Nesta arquitetura, o hypervisor controla a distribui√ß√£o dos recursos de computa√ß√£o e ele pr√≥prio controla todas as chamadas de m√°quinas virtuais para dispositivos. O VMware ESX foi considerado o primeiro hypervisor do primeiro tipo para x86 por um longo tempo, embora agora o atribu√≠ssemos a 1+. O √∫nico representante "honesto" desse tipo hoje √© o VMware ESXi - o sucessor do ESX, depois de ter desviado a se√ß√£o pai do RHEL.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por exemplo, considere a arquitetura ESXi. </font><font style="vertical-align: inherit;">Os comandos de gerenciamento do hipervisor s√£o executados por meio da API do agente, que √© executada sobre o VMkernel. </font><font style="vertical-align: inherit;">Isso pode parecer uma conex√£o direta com o hipervisor, mas n√£o √©. </font><font style="vertical-align: inherit;">N√£o h√° acesso direto ao hypervisor, que distingue esse tipo de hypervisor do segundo tipo de hypervisor em termos de seguran√ßa. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/361/f83/77d/361f8377d29345aa2498b31b9af66030.jpg" alt="imagem"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A desvantagem aqui s√£o os drivers de dispositivo: para garantir a "espessura" da plataforma e eliminar complica√ß√µes desnecess√°rias de vers√£o para vers√£o, os drivers de dispositivo s√£o alternados, o que torna a infraestrutura f√≠sica dependente da HCL (lista de compatibilidade de hardware).</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tipo 1+ (hipervisor h√≠brido) </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os hipervisores do tipo h√≠brido (tamb√©m s√£o do tipo 1+, 1a, 1.5) s√£o caracterizados pelo isolamento do SO b√°sico em uma entidade especial chamada parti√ß√£o pai (parti√ß√£o pai na terminologia Microsoft Hyper-V) ou em um dom√≠nio pai (dom√≠nio dom0 na terminologia Xen). Portanto, depois de instalar a fun√ß√£o do hypervisor, o kernel entra no modo de suporte √† virtualiza√ß√£o e o hypervisor √© respons√°vel pela aloca√ß√£o de recursos no host. Mas a se√ß√£o pai assume a fun√ß√£o de processar chamadas para drivers de dispositivo e opera√ß√µes de E / S.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De fato, a se√ß√£o pai se torna um tipo de provedor entre todas as entidades da pilha de virtualiza√ß√£o. Essa abordagem √© conveniente do ponto de vista da compatibilidade com o equipamento: n√£o √© necess√°rio incorporar drivers de dispositivo no hipervisor, como √© o caso do ESXi, o que significa que a lista de dispositivos √© muito expandida e menos dependente da HCL. As vantagens incluem o descarregamento do hypervisor da tarefa de processar chamadas para drivers de dispositivo, pois todas as chamadas s√£o tratadas pela se√ß√£o pai. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A arquitetura de n√≠vel superior dos hipervisores do tipo 1+ √© assim:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/89c/b74/eb3/89cb74eb3f97b6a8af063fbd447b1c87.jpg" alt="imagem"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os hipervisores desse tipo incluem: os hipervisores VMware ESX, Microsoft Hyper-V e Xen falecidos (implementa√ß√µes Citrix XenServer e Xen em v√°rias distribui√ß√µes Linux). </font><font style="vertical-align: inherit;">Lembre-se de que o Citrix XenServer √© um SO baseado em RHEL ligeiramente truncado, e sua vers√£o e funcionalidade dependiam diretamente da vers√£o atual do Red-Hat Enterprise Linux. </font><font style="vertical-align: inherit;">No caso de outras implementa√ß√µes do Xen, a situa√ß√£o n√£o √© muito diferente: √© o mesmo kernel Linux no modo hipervisor Xen e o SO base no dom√≠nio dom0. </font><font style="vertical-align: inherit;">Isso leva √† conclus√£o inequ√≠voca de que os hipervisores baseados em Xen s√£o do tipo h√≠brido e n√£o s√£o hipervisores honestos do tipo 1.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Principais tecnologias de plataformas industriais </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A base ser√° a terminologia do VMware, como a plataforma de virtualiza√ß√£o mais avan√ßada tecnologicamente. </font><font style="vertical-align: inherit;">Neste artigo, nos restringimos √†s tecnologias dos pr√≥prios hipervisores e ao sistema de controle b√°sico. </font><font style="vertical-align: inherit;">Todas as funcionalidades avan√ßadas implementadas por produtos adicionais por dinheiro adicional ser√£o deixadas nos bastidores. </font><font style="vertical-align: inherit;">As tecnologias s√£o agrupadas em grupos condicionais para o objetivo principal, como pareceu ao autor, com quem voc√™ tem o direito de discordar.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> SLA </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Essa √© uma cole√ß√£o de tecnologias que afetam principalmente o desempenho dos SLAs para acessibilidade (RPO / RTO). </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alta disponibilidade - uma tecnologia para garantir alta disponibilidade de VMs em um cluster por um hipervisor. </font><font style="vertical-align: inherit;">Em caso de morte do host, a VM √© reiniciada automaticamente nos hosts sobreviventes. </font><font style="vertical-align: inherit;">Efeito: minimizar o RTO antes do tempo limite do HA + reiniciar OS / servi√ßos.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> FT </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Toler√¢ncia a falhas - tecnologia para garantir a opera√ß√£o cont√≠nua de VMs, mesmo em caso de morte do host. </font><font style="vertical-align: inherit;">Uma VM sombra √© criada no segundo host, que √© completamente id√™ntico ao principal e repete as instru√ß√µes por tr√°s dele. </font><font style="vertical-align: inherit;">Assim, a diferen√ßa nos estados da VM √© medida em dezenas ou centenas de milissegundos, o que √© bastante aceit√°vel para muitos servi√ßos. </font><font style="vertical-align: inherit;">Quando o host morre, a execu√ß√£o muda automaticamente para a sombra da VM. </font><font style="vertical-align: inherit;">Efeito: minimizando o RTO para zero.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tco </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Esta √© uma cole√ß√£o de tecnologias que influenciam principalmente o custo total de propriedade. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vMotion </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O vMotion √© uma tecnologia para migra√ß√£o ao vivo de um ponto de execu√ß√£o de VM de um host totalmente funcional para outro. </font><font style="vertical-align: inherit;">Ao mesmo tempo, o ponto de comuta√ß√£o do ponto de execu√ß√£o √© menor que o tempo limite da conex√£o de rede, o que nos permite considerar a migra√ß√£o como ativa, ou seja, </font><font style="vertical-align: inherit;">sem interrup√ß√£o no trabalho dos servi√ßos produtivos. </font><font style="vertical-align: inherit;">Efeito: reduzir o RTO para zero para interrup√ß√µes planejadas para manuten√ß√£o do servidor e, como resultado, elimina√ß√£o parcial das pr√≥prias interrup√ß√µes.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Storage vMotion </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O Storage vMotion √© uma tecnologia para migra√ß√£o ao vivo de um ponto de armazenamento da VM de um armazenamento totalmente funcional para outro. </font><font style="vertical-align: inherit;">Ao mesmo tempo, o trabalho com o sistema de disco n√£o para, o que permite que a migra√ß√£o seja considerada ativa. </font><font style="vertical-align: inherit;">Efeito: reduzir o RTO para zero para interrup√ß√µes planejadas para manuten√ß√£o de armazenamento e, como resultado, elimina√ß√£o parcial das pr√≥prias interrup√ß√µes.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DPM </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distributed Power Management - tecnologia para controlar o n√≠vel de carga do host e ligar / desligar os hosts √† medida que a carga no cluster muda. </font><font style="vertical-align: inherit;">Requer DRS para sua opera√ß√£o. </font><font style="vertical-align: inherit;">Efeito: redu√ß√£o geral no consumo de energia.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VSwitch distribu√≠do </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O vSwitch distribu√≠do √© uma tecnologia para gerenciamento centralizado das configura√ß√µes de rede de switches de host virtual. </font><font style="vertical-align: inherit;">Efeito: reduzindo o volume e a complexidade do trabalho de reconfigura√ß√£o do subsistema de rede, reduzindo os riscos de erros.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> EVC </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A compatibilidade aprimorada do vMotion √© uma tecnologia que permite mascarar as instru√ß√µes dispon√≠veis do processador para VMs no modo autom√°tico. </font><font style="vertical-align: inherit;">√â usado para alinhar o trabalho das VMs em um cluster desigual com a fam√≠lia de processadores mais antiga, fornecendo a capacidade de migrar VMs para qualquer host. </font><font style="vertical-align: inherit;">Efeito: economizando na complexidade da infraestrutura e aumentando gradualmente a capacidade / atualizando parcialmente os clusters.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> QoS </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Essa √© uma cole√ß√£o de tecnologias que influenciam principalmente o desempenho do SLA em termos de qualidade de servi√ßo. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vNUMA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O vNUMA √© uma tecnologia que permite que o sistema operacional convidado se comunique com a topologia NUMA virtual da VM para m√°quinas amplas (n√≥ vCPU ou vRAM&gt; NUMA). </font><font style="vertical-align: inherit;">Efeito: A falta de penalidade no desempenho do software aplicativo que suporta NUMA.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Conjunto de recursos </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pools de recursos - a tecnologia de combinar v√°rias VMs em um √∫nico pool de recursos para controlar o consumo ou garantir a aloca√ß√£o de recursos. </font><font style="vertical-align: inherit;">Efeito: simplifique a administra√ß√£o, forne√ßa um n√≠vel de servi√ßo.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Limite / reserva </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O processador / mem√≥ria limitador e redundante permite limitar a aloca√ß√£o de recursos, ou vice-versa, para garantir sua aloca√ß√£o em uma situa√ß√£o de escassez e concorr√™ncia para garantir a manuten√ß√£o de VMs / pools de alta prioridade. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DRS </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dynamic Resource Scheduler - balanceamento autom√°tico de VMs por hosts, dependendo da carga, para reduzir a fragmenta√ß√£o de recursos no cluster e fornecer um n√≠vel de servi√ßo para VMs. </font><font style="vertical-align: inherit;">Requer suporte ao vMotion.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Controle de E / S de armazenamento </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O controle de E / S de armazenamento √© uma tecnologia que limita o ‚Äúvizinho barulhento‚Äù, uma m√°quina de baixa prioridade com alta carga de disco para manter o desempenho de um sistema de armazenamento caro dispon√≠vel para cargas de trabalho produtivas. </font><font style="vertical-align: inherit;">Como exemplo, um sistema de indexa√ß√£o / mecanismo de pesquisa interno e um DBMS produtivo.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Controle de E / S de rede </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O Network IO Control √© uma tecnologia para limitar o ‚Äúvizinho barulhento‚Äù, uma m√°quina de baixa prioridade com alta carga de rede. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Integra√ß√£o de armazenamento (VAAI etc) </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Duas categorias de tecnologias se enquadram na se√ß√£o de integra√ß√£o: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A integra√ß√£o do sistema de gerenciamento de virtualiza√ß√£o ao sistema de gerenciamento de armazenamento pode simplificar bastante a sele√ß√£o e a apresenta√ß√£o de volumes / bal√µes de armazenamento para hipervisores, reduzindo o risco de erros e a complexidade do trabalho. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Integra√ß√£o em n√≠vel de protocolo - VAAI, ODX. </font><font style="vertical-align: inherit;">Essas tecnologias permitem descarregar o subsistema de disco, transferindo parte da carga padr√£o para o descarte de armazenamento inteligente. </font><font style="vertical-align: inherit;">Por exemplo, essa categoria inclui opera√ß√µes como zerar blocos, clonar VMs etc. </font><font style="vertical-align: inherit;">Devido a isso, o canal para o sistema de armazenamento √© significativamente descarregado, e o pr√≥prio sistema de armazenamento realiza opera√ß√µes de disco de maneira mais otimizada.</font></font></li></ul><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seguran√ßa </font></font></h3><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Microssegmenta√ß√£o </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A microssegmenta√ß√£o de uma rede virtual em uso pr√°tico √© a capacidade de criar um firewall distribu√≠do virtual que controla as redes virtuais dentro do host. </font><font style="vertical-align: inherit;">Melhora extremamente a seguran√ßa da rede virtual.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AV sem agente </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suporte de tecnologia antiv√≠rus sem agente. </font><font style="vertical-align: inherit;">Em vez de ser verificado pelos agentes no SO convidado, o tr√°fego das opera√ß√µes do disco da VM √© direcionado pelo hipervisor √† VM do servi√ßo selecionado. </font><font style="vertical-align: inherit;">Reduz significativamente a carga nos processadores e no sistema de disco, eliminando efetivamente as ‚Äútempestades antiv√≠rus‚Äù.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sistemas hiperconvergentes </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sistemas convergentes, como o nome sugere, s√£o sistemas com uma combina√ß√£o de fun√ß√µes. E, neste caso, queremos dizer a combina√ß√£o do armazenamento e execu√ß√£o da VM. Parece simples, mas o marketing de repente entra em cena. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pela primeira vez com o termo sistemas convergentes, os profissionais de marketing entram no mercado. Os sistemas convergentes venderam servidores cl√°ssicos comuns + armazenamento + comutadores. Pouco menos de um n√∫mero de parceiro. Ou eles nem estavam vendendo, mas um documento chamado "arquitetura de refer√™ncia" foi produzido. Sinceramente, condenamos essa abordagem e passamos √† considera√ß√£o da arquitetura.</font></font><br><br><h3>  Arquitetura </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mantendo a converg√™ncia como um princ√≠pio arquitetural, obtemos uma combina√ß√£o do ponto de armazenamento e do ponto de execu√ß√£o da VM em um √∫nico sistema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A arquitetura convergente, em outras palavras, implica o uso dos mesmos servi√ßos de hardware para executar VMs e armazen√°-los em discos locais. </font><font style="vertical-align: inherit;">Bem, como deve haver toler√¢ncia a falhas - em uma arquitetura convergida, h√° uma camada de SDS distribu√≠do.</font></font><br><br>  Temos: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O sistema cl√°ssico - software, armazenamento, comuta√ß√£o e servidores v√™m de diferentes locais, combinados pelas m√£os do cliente / integrador. </font><font style="vertical-align: inherit;">Contratos de suporte separados.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sistema convergente - tudo de uma fonte, um suporte, um n√∫mero de parceiro. </font><font style="vertical-align: inherit;">N√£o deve ser confundido com a montagem autom√°tica de um fornecedor.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E acontece que o termo para nossa arquitetura convergente j√° foi adotado. </font><font style="vertical-align: inherit;">Exatamente a mesma situa√ß√£o que com o supervisor. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sistema hiperconvergente</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Um sistema convergente com arquitetura convergente. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Claro, n√£o foi sem a segunda vinda dos profissionais de marketing. </font><font style="vertical-align: inherit;">Apareceram sistemas convergentes nos quais n√£o havia combina√ß√£o de armazenamento, mas existem n√≥s de armazenamento dedicados sob o controle do SDS distribu√≠do. </font><font style="vertical-align: inherit;">No √¢mbito das guerras de marketing, at√© o termo especial desagregou HCI (infraestrutura hiperverg√™nica desagregada). </font><font style="vertical-align: inherit;">Em particular, por exemplo, a NetApp com um sistema semelhante inicialmente lutou intensamente pelo direito de chamar seu sistema de hiper convergente, mas acabou se rendendo. </font><font style="vertical-align: inherit;">NetApp HCI hoje (final de 2019) - infraestrutura de nuvem h√≠brida.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Op√ß√µes de implementa√ß√£o </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Devido ao fato de os sistemas hiperconvergentes funcionarem com a virtualiza√ß√£o, na verdade existem duas op√ß√µes e meia para implementa√ß√£o. </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. O m√≥dulo do kernel. </font><font style="vertical-align: inherit;">O SDS funciona como um mon√≥lito no n√∫cleo do hipervisor, por exemplo vSAN + ESXi</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1.5 M√≥dulo da se√ß√£o pai. </font><font style="vertical-align: inherit;">O SDS funciona como um servi√ßo como parte da se√ß√£o pai do hypervisor, por exemplo S2D + Hyper-V</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. A m√°quina virtual. </font><font style="vertical-align: inherit;">O SDS √© implementado como uma m√°quina virtual dedicada em cada host. </font><font style="vertical-align: inherit;">Nutanix, Cisco Hyperflex, HPE Simplivity.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obviamente, al√©m das quest√µes discutidas com o efeito da incorpora√ß√£o no desempenho, h√° uma quest√£o muito importante de isolamento e suporte de hipervisores de terceiros. </font><font style="vertical-align: inherit;">No caso 1, √© √≥bvio que esse pode ser apenas um sistema do provedor do hipervisor, enquanto 2 pode potencialmente funcionar em qualquer hipervisor.</font></font><br><br><h2>  Contentores </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A virtualiza√ß√£o em cont√™iner, embora tecnicamente muito diferente da virtualiza√ß√£o completa, parece bastante simples em sua estrutura. </font><font style="vertical-align: inherit;">Como no modelo de rede OSI, a quest√£o √© nivelada. </font><font style="vertical-align: inherit;">A virtualiza√ß√£o de cont√™iner √© um n√≠vel mais alto - no n√≠vel do ambiente de aplicativos, e n√£o na f√≠sica. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A principal tarefa da virtualiza√ß√£o de cont√™iner √© dividir o SO em partes independentes, das quais aplicativos isolados n√£o podem interferir entre si. </font><font style="vertical-align: inherit;">A virtualiza√ß√£o completa √© compartilhada n√£o pelo sistema operacional, mas por um servidor f√≠sico.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VM vs Container </font></font></h3><br>  Os pr√≥s e contras de ambas as abordagens s√£o bastante simples e diretamente opostos. <br><br>  A virtualiza√ß√£o total (VM) d√° total independ√™ncia ao n√≠vel de ferro, incluindo SOs totalmente independentes, pilhas de disco e rede.  Por outro lado, cada aplicativo, porque aderimos ao esquema 1 aplicativo = 1 servidor, requer seu pr√≥prio sistema operacional, seu pr√≥prio disco e pilha de rede.  isto √©  h√° uma despesa m√∫ltipla de recursos. <br><br>  Os cont√™ineres t√™m pilhas comuns de disco e rede com o sistema operacional host e, juntos, eles usam um n√∫cleo em todo o servidor f√≠sico (bem ou virtual, nos √∫ltimos tempos), o que, como um todo, permite economizar recursos significativamente em paisagens homog√™neas. <br><br>  Historicamente, o x86 inicialmente tinha cont√™ineres para tudo, al√©m de servidores f√≠sicos.  Ap√≥s o advento da virtualiza√ß√£o completa, a import√¢ncia dos cont√™ineres caiu drasticamente em quase 15 anos e as VMs grossas reinaram no mundo corporativo.  Naquela √©poca, os cont√™ineres encontravam-se em hosters que forneciam centenas do mesmo tipo de servidores da Web, onde havia pouca demanda.  Mas, nos √∫ltimos anos, desde cerca de 2015, os cont√™ineres voltaram √† realidade corporativa na forma de aplicativos nativos da nuvem. <br><br><h3>  Contentores 0.1 </h3><br><h4>  chroot </h4><br>  O prot√≥tipo de cont√™ineres em 1979 foi chroot. <br><br>  ‚ÄúChroot √© a opera√ß√£o de alterar o diret√≥rio raiz em sistemas operacionais semelhantes ao Unix.  Um programa iniciado com um diret√≥rio raiz modificado ter√° acesso apenas aos arquivos contidos nesse diret√≥rio. ‚Äù <br><br>  I.e.  de fato, o isolamento √© apenas no n√≠vel do sistema de arquivos; caso contr√°rio, √© apenas um processo normal no sistema operacional. <br><br><h4>  Cadeia Freebsd </h4><br>  Significativamente mais avan√ßado foi a pris√£o do BSD livre, que apareceu em 1999.  O Jail permitiu criar inst√¢ncias completas de SO virtual com seus pr√≥prios conjuntos de aplicativos e arquivos de configura√ß√£o com base no FreeBSD base.  Certamente h√° quem diga - e o que a cadeia faz em cont√™ineres, porque isso √© paravirtualiza√ß√£o!  E eles estar√£o parcialmente certos. <br><br>  No entanto, antes da virtualiza√ß√£o completa (e sua variante na forma de paravirtualiza√ß√£o), a cadeia carece da capacidade de executar o kernel de uma vers√£o diferente na VM convidada e de agrupar com a migra√ß√£o da VM para outro sistema host. <br><br><h4>  Zonas Solaris </h4><br>  O Solaris Zones √© uma tecnologia de virtualiza√ß√£o de sistema operacional (virtualiza√ß√£o de cont√™iner), introduzida em 2004 no Sun Solaris.  O princ√≠pio b√°sico √© a baixa sobrecarga de virtualiza√ß√£o. <br><br>  N√£o ganhando muita popularidade, migrou para o OpenSolaris e distribui√ß√µes baseadas nele, dispon√≠veis em 2019. <br><br><h3>  Containers 1.0 </h3><br>  Na era dos cont√™ineres 1.0, surgiram duas dire√ß√µes principais de cont√™ineres - produtos comerciais para provedores de hospedagem e cont√™ineres de aplicativos. <br><br><h4>  Virtuozzo / OpenVZ </h4><br>  A SWsoft da R√∫ssia lan√ßou em 2001 sua primeira vers√£o da virtualiza√ß√£o de cont√™ineres Virtuozzo, voltada para o mercado de provedores de hospedagem.  Devido √† determina√ß√£o e ao p√∫blico-alvo comercial espec√≠fico, o produto mostrou-se bastante bem-sucedido e ganhou popularidade.  Tecnologicamente, em 2002, foi demonstrada a opera√ß√£o simult√¢nea de 2500 cont√™ineres em um servidor com 8 processadores. <br><br>  Em 2005, uma vers√£o aberta dos cont√™ineres Virtuozzo para Linux, chamada OpenVZ, apareceu.  E quase se tornou o padr√£o ouro para hospedagem de VPS. <br><br><h4>  Lxc </h4><br>  O LinuX Containers (LXC) √© outra virtualiza√ß√£o de cont√™iner bem conhecida, baseada em namespaces e cgroups, que apareceu em 2008. Ela √© subjacente √†s janelas de encaixe atualmente populares, etc. <br><br><h3>  Containers 1.1 (Application Virtualization) </h3><br>  Se os cont√™ineres restantes foram projetados para dividir o SO b√°sico em segmentos, por que n√£o separar essa camada do sistema e embal√°-la em uma √∫nica caixa com o aplicativo e todos os seus arredores.  E esse pacote pronto pode ser lan√ßado como um aplicativo comum no n√≠vel do usu√°rio. <br><br><h4>  App-v </h4><br>  Microsoft Application Virtualization (App-V), anteriormente Softricity SoftGrid - tecnologia para cont√™iner de aplicativos espec√≠ficos (o cont√™iner √© o contr√°rio) em uma caixa de prote√ß√£o isolada e, em seguida, na Microsoft.  Em 2006, a Microsoft adquiriu a startup Softricity, que realmente mudou o cont√™iner. <br><br><h4>  Thinapp </h4><br>  O VMware ThinApp (anteriormente Thinstall) √© um produto de conteineriza√ß√£o de aplicativos da Jilt adquirido pela VMware em 2008.  A VMware estima que 90-95% de todos os aplicativos empacotados no mundo usam essa tecnologia. <br><br><h3>  Containers 2.0 </h3><br>  O hist√≥rico do surgimento de containers 2.0 est√° muito associado a uma mudan√ßa no processo de desenvolvimento de software.  O desejo da empresa de reduzir um par√¢metro t√£o importante quanto o tempo de coloca√ß√£o no mercado obrigou os desenvolvedores a reconsiderar abordagens para a cria√ß√£o de produtos de software.  A metodologia de desenvolvimento Waterfall (ciclos de libera√ß√£o longos, todo o aplicativo √© atualizado) √© substitu√≠da pelo Agile (ciclos de libera√ß√£o curtos e com tempo determinado, os componentes do aplicativo s√£o atualizados independentemente) e for√ßa os desenvolvedores a separar aplicativos monol√≠ticos em componentes.  Embora os componentes de aplicativos monol√≠ticos ainda sejam bastante grandes e n√£o haja muitos deles que possam ser colocados em m√°quinas virtuais, mas quando um aplicativo consiste em dezenas ou centenas de componentes, as m√°quinas virtuais n√£o s√£o mais muito adequadas.  Al√©m disso, tamb√©m surge o problema de vers√µes auxiliares de software, bibliotecas e depend√™ncias; geralmente h√° uma situa√ß√£o em que componentes diferentes requerem vers√µes diferentes ou vari√°veis ‚Äã‚Äãde ambiente configuradas de maneira diferente.  Esses componentes precisam ser distribu√≠dos para diferentes m√°quinas virtuais, porque  √© quase imposs√≠vel executar simultaneamente v√°rias vers√µes de software no mesmo sistema operacional.  O n√∫mero de VM come√ßa a crescer como uma avalanche.  Aqui, os cont√™ineres aparecem no palco, permitindo que, dentro da estrutura de um SO convidado, crie v√°rios ambientes isolados para o lan√ßamento de componentes de aplicativos.  A conteineriza√ß√£o de aplicativos permite continuar a segmenta√ß√£o de um aplicativo monol√≠tico em componentes ainda menores e passar para o paradigma de uma tarefa = um componente - um cont√™iner, isso √© chamado de abordagem de microsservi√ßo, e cada componente √© um microsservi√ßo. <br><br><h4>  Recipiente sob o cap√¥ </h4><br>  Se voc√™ olhar para o cont√™iner com uma olhada do administrador do sistema, esses s√£o apenas processos Linux que t√™m seus pr√≥prios pids, etc.  O que torna poss√≠vel isolar os processos em execu√ß√£o nos cont√™ineres e consumir os recursos do SO convidado juntos?  Dois mecanismos padr√£o presentes no kernel de qualquer distribui√ß√£o moderna do Linux.  O primeiro, Namespaces do Linux, que garante que cada processo veja sua pr√≥pria representa√ß√£o do sistema operacional (sistema de arquivos, interfaces de rede, nome do host etc.) e o segundo, Linux Control Groups (cgroups), restringindo o processo ao consumo de recursos do SO convidado (CPU, mem√≥ria largura de banda da rede etc.). <br><br><h4>  Namespaces do Linux </h4><br>  Por padr√£o, todo sistema Linux cont√©m um √∫nico espa√ßo para nome.  Todos os recursos do sistema, como sistemas de arquivos, identificadores de processo (IDs de processo), identificadores de usu√°rio (IDs de usu√°rio) e interfaces de rede pertencem a esse espa√ßo para nome.  Mas ningu√©m est√° nos impedindo de criar espa√ßos para nome adicionais e redistribuir recursos do sistema entre eles. <br><br>  Quando um novo processo √© iniciado, ele inicia em um espa√ßo para nome, padr√£o do sistema ou um dos criados.  E esse processo ver√° apenas os recursos dispon√≠veis no espa√ßo para nome usado para execut√°-lo. <br><br>  Mas nem tudo √© t√£o simples, cada processo n√£o pertence a um √∫nico espa√ßo para nome, mas a um espa√ßo para nome em cada uma das categorias: <br><br><ul><li>  Montagem (mnt) </li><li>  ID do processo (pid) </li><li>  Rede (l√≠quida) </li><li>  Comunica√ß√£o entre processos (ipc) </li><li>  UTS </li><li>  ID do usu√°rio (usu√°rio) </li></ul><br>  Cada tipo de espa√ßo para nome isola um grupo de recursos correspondente.  Por exemplo, o espa√ßo UTS define o nome do host e o nome do dom√≠nio vis√≠veis aos processos.  Portanto, dois processos no sistema operacional convidado podem assumir que eles est√£o sendo executados em servidores diferentes. <br><br>  O espa√ßo para nome da rede determina a visibilidade das interfaces de rede; o processo interno ver√° apenas as interfaces pertencentes a esse espa√ßo para nome. <br><br><h4>  Grupos de controle do Linux (cgroups) </h4><br>  O Linux Control Groups (cgroups) √© o mecanismo do sistema do kernel (Kernel) dos sistemas Linux que limita o consumo de recursos do sistema pelos processos.  Cada processo ou grupo de processos n√£o poder√° obter mais recursos (CPU, mem√≥ria, largura de banda da rede etc.) do que o que est√° alocado e n√£o poder√° capturar os "outros" recursos - os recursos dos processos vizinhos. <br><br><h3>  Docker </h3><br>  Como mencionado acima, o Docker n√£o inventou cont√™ineres como tal.  Os cont√™ineres existem h√° muitos anos (incluindo aqueles baseados no LXC), mas o Docker os tornou muito populares ao criar o primeiro sistema que tornou f√°cil e simples a transfer√™ncia de cont√™ineres entre m√°quinas diferentes.  O Docker criou uma ferramenta para criar cont√™ineres - empacotando o aplicativo e suas depend√™ncias e executando cont√™ineres em qualquer sistema Linux com o Docker instalado. <br><br>  Um recurso importante do Docker √© a portabilidade n√£o apenas do pr√≥prio aplicativo e suas depend√™ncias entre distribui√ß√µes Linux completamente diferentes, mas tamb√©m a portabilidade do ambiente e do sistema de arquivos.  Por exemplo, um cont√™iner criado no CentOS pode ser executado em um sistema Ubuntu.  Nesse caso, dentro do cont√™iner lan√ßado, o sistema de arquivos ser√° herdado do CentOS e o aplicativo considerar√° que ele √© executado sobre o CentOS.  Isso √© um pouco semelhante a uma imagem OVF de uma m√°quina virtual, mas o conceito de uma imagem do Docker usa camadas.  Isso significa que, ao atualizar apenas parte da imagem, n√£o √© necess√°rio fazer o download da imagem inteira novamente, basta fazer o download apenas da camada alterada, como se na imagem OVF fosse poss√≠vel atualizar o sistema operacional sem atualizar a imagem inteira. <br><br>  O Docker criou um ecossistema para criar, armazenar, transferir e iniciar cont√™ineres.  Existem tr√™s componentes principais no mundo do Docker: <br><br><ul><li>  Imagens - uma imagem, √© a entidade que cont√©m seu aplicativo, o ambiente necess√°rio e outros metadados necess√°rios para iniciar o cont√™iner; </li><li>  Registradores - reposit√≥rio, local de armazenamento para imagens do Docker.  Existem v√°rios reposit√≥rios, desde o oficial - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hub.docker.com</a> at√© os privados implantados na infraestrutura da empresa; </li><li>  Cont√™ineres - um cont√™iner, cont√™iner Linux criado a partir de uma imagem do Docker.  Como mencionado acima, esse √© um processo Linux em execu√ß√£o em um sistema Linux com o Docker instalado, isolado de outros processos e do pr√≥prio sistema operacional. </li></ul><br>  Considere o ciclo de vida do cont√™iner.  Inicialmente, um desenvolvedor cria uma imagem do Docker com seu aplicativo (o comando docker build), completamente do zero ou usando imagens j√° criadas como base (lembre-se das camadas).  Al√©m disso, essa imagem pode ser lan√ßada pelo desenvolvedor diretamente em sua pr√≥pria m√°quina ou pode ser transferida para outra m√°quina - o servidor.  Para portabilidade, os reposit√≥rios s√£o frequentemente usados ‚Äã‚Äã(o comando docker push) - eles carregam a imagem no reposit√≥rio.  Depois disso, a imagem pode ser baixada para qualquer outra m√°quina ou servidor (docker pull).  Por fim, crie um container de trabalho (docker run) a partir desta imagem. <br><br><h3>  Kubernetes </h3><br>  Como j√° dissemos, o conceito de microsservi√ßos significa dividir um aplicativo monol√≠tico em muitos servi√ßos pequenos, geralmente executando uma √∫nica fun√ß√£o.  Bem, quando existem dezenas desses servi√ßos, eles ainda podem ser gerenciados manualmente atrav√©s, por exemplo, do Docker.  Mas o que fazer quando existem centenas e milhares desses servi√ßos?  Al√©m do ambiente industrial, voc√™ precisa de um ambiente de teste e ambientes adicionais para diferentes vers√µes do produto, ou seja,  multiplique por 2, por 3 ou mais.  O Google tamb√©m enfrentou os mesmos problemas, seus engenheiros foram um dos primeiros a usar cont√™ineres em escala industrial.  Assim nasceu o Kubernetes (K8s), criado sob o nome de Borg nas paredes do produto Google, mais tarde dado ao p√∫blico em geral e renomeado. <br><br>  O K8s √© um sistema que facilita a implanta√ß√£o, o gerenciamento e o monitoramento de aplicativos em cont√™iner (microsservi√ßos).  Como j√° sabemos, qualquer m√°quina Linux √© adequada para o lan√ßamento de cont√™ineres e os cont√™ineres s√£o isolados um do outro, respectivamente, e os K8s podem gerenciar servidores diferentes com hardware diferente e sob o controle de diferentes distribui√ß√µes Linux.  Tudo isso nos ajuda a usar o hardware dispon√≠vel de forma eficaz.  Como a virtualiza√ß√£o, o K8s fornece um conjunto comum de recursos para iniciar, gerenciar e monitorar nossos microsservi√ßos. <br><br>  Como este artigo √© voltado principalmente para engenheiros de virtualiza√ß√£o, para uma compreens√£o geral dos princ√≠pios de opera√ß√£o e dos principais componentes do K8s, recomendamos que voc√™ leia o artigo que tra√ßa o paralelo entre o K8s e o VMware vSphere: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://medium.com/@pryalukhin/kubernetes-introduction-for-vmware- users-232cc2f69c58</a> <br><br><h2>  Hist√≥rico de virtualiza√ß√£o industrial X86 </h2><br><h3>  VMware </h3><br>  A VMware apareceu em 1998, come√ßando com o desenvolvimento de um segundo tipo de hypervisor, que mais tarde ficou conhecido como VMware Workstation. <br><br>  A empresa entrou no mercado de servidores em 2001 com dois hipervisores - GSX (Ground Storm X, segundo tipo) e ESX (Elastic Sky X, primeiro tipo).  Com o tempo, as perspectivas do segundo tipo em aplicativos de servidor tornaram-se √≥bvias, ou seja,  Nenhuma.  E o GSX pago foi primeiro transformado em um servidor VMware gratuito e depois completamente parado e enterrado. <br><br>  Em 2003, o sistema de gerenciamento central do Virtual Center, a tecnologia vSMP e a migra√ß√£o ao vivo de m√°quinas virtuais apareceram. <br><br>  Em 2004, a VMware foi adquirida pela EMC, uma gigante do armazenamento, mas deixou operacional independente. <br><br>  Em 2008, tornando-se o padr√£o de fato do setor, a VMware estimulou o r√°pido crescimento de ofertas competitivas - Citrix, Microsoft etc. Torna-se clara a necessidade de obter uma vers√£o gratuita do hipervisor, o que era imposs√≠vel - como uma se√ß√£o pai no ESX, um RHEL completamente comercial foi usado.  O projeto de substituir o RHEL por algo mais f√°cil e gratuito teve sua implementa√ß√£o em 2008 com o sistema busybox.  O resultado √© o ESXi, conhecido por todos hoje. <br><br>  Paralelamente, a empresa est√° desenvolvendo projetos internos e aquisi√ß√µes de startups.  Alguns anos atr√°s, uma lista de produtos VMware ocupava algumas p√°ginas A4, ent√£o vamos dizer.  O VMware para 2019 ainda √© o padr√£o de fato no mercado de virtualiza√ß√£o corporativa corporativa no local, com uma participa√ß√£o de mercado de mais de 70% e um l√≠der absoluto em tecnologia, e uma revis√£o detalhada da hist√≥ria merece um artigo muito amplo. <br><br><h3>  Connectix </h3><br>  Fundada em 1988, a Connectix trabalhou em uma variedade de utilit√°rios de sistema at√© ocupar a virtualiza√ß√£o.  Em 1997, foi criado o primeiro produto VirtualPC para Apple Macintosh, permitindo que o Windows fosse executado em uma m√°quina virtual.  A primeira vers√£o do VirtualPC para Windows apareceu em 2001. <br><br>  Em 2003, a Microsoft comprou o VirtualPC e, de acordo com o Connectix, os desenvolvedores mudaram para a Microsoft.  Depois disso, o Connectix fechou. <br><br>  O formato VHD (disco r√≠gido virtual) foi desenvolvido pelo Connectix para VirtualPC e, como lembrete, os discos virtuais das m√°quinas Hyper-V cont√™m "conectix" em sua assinatura. <br>  Virtual PC, como voc√™ pode imaginar, √© um hypervisor de desktop cl√°ssico do segundo tipo. <br><br><h3>  Microsoft </h3><br>  A jornada da Microsoft para a virtualiza√ß√£o industrial come√ßou com a compra do Connectix e a renomea√ß√£o do Connectix Virtual PC no Microsoft Virtual PC 2004. O PC virtual desenvolvido por um tempo foi inclu√≠do com o nome Windows Virtual PC no Windows 7. No Windows 8 e posterior, o Virtual PC foi substitu√≠do por vers√£o desktop do Hyper-V. <br><br>  Baseado no Virtual PC, foi criado o hipervisor do servidor Virtual Server, que existia at√© o in√≠cio de 2008.  Devido √† √≥bvia perda tecnol√≥gica antes do VMware ESX, foi decidido restringir o desenvolvimento do segundo tipo de hypervisor em favor de seu pr√≥prio primeiro tipo de hypervisor, que se tornou o Hyper-V.  H√° uma opini√£o n√£o oficial no setor de que o Hyper-V √© surpreendentemente semelhante ao Xen na arquitetura.  Aproximadamente o mesmo que .Net em Java. <br><blockquote>  "Claro, voc√™ pode pensar que a Microsoft roubou a id√©ia de Java."  Mas isso n√£o √© verdade, a Microsoft a inspirou!  - (de um discurso de um representante da Microsoft na apresenta√ß√£o do Windows 2003 Server) </blockquote><br>  Dos momentos curiosos, pode-se notar que, dentro da Microsoft, o uso de produtos de virtualiza√ß√£o propriet√°rios nos anos zero foi, para dizer o m√≠nimo, opcional.  Existem capturas de tela do Technet de artigos sobre virtualiza√ß√£o, nas quais o logotipo do VMware Tools est√° claramente presente na bandeja.  Al√©m disso, Mark Russinovich na Plataforma de 2009 em Moscou realizou uma demonstra√ß√£o com a VMware Workstation. <br><br>  Em um esfor√ßo para entrar em novos mercados, a Microsoft criou sua pr√≥pria nuvem p√∫blica, o Azure, usando um Nano Server altamente modificado com suporte para Hyper-V, S2D e SDN como plataforma.  Vale ressaltar que, inicialmente, o Azure, em alguns pontos, estava muito atr√°s dos sistemas locais.  Por exemplo, o suporte para m√°quinas virtuais de segunda gera√ß√£o (com suporte para Inicializa√ß√£o segura, inicializa√ß√£o a partir de parti√ß√µes GPT, inicializa√ß√£o PXE etc.) apareceu no Azure somente em 2018.  Enquanto no local, as VMs de segunda gera√ß√£o s√£o conhecidas desde o Windows Server 2012R2.  O mesmo vale para solu√ß√µes de portal: at√© 2017, o Azure e o Windows Azure Pack (solu√ß√£o de nuvem multiloca√ß√£o com suporte a SDN e VM protegida, que substituiu o System Center App Controller em 2013) usavam o mesmo design de portal.  Depois que a Microsoft anunciou um curso sobre nuvens p√∫blicas, o Azure avan√ßou no desenvolvimento e implementa√ß√£o de v√°rios conhecimentos.  Por volta do ano de 2016, voc√™ pode observar uma imagem completamente l√≥gica: agora todas as inova√ß√µes no Windows Server v√™m do Azure, mas n√£o na dire√ß√£o oposta.  O fato de copiar partes da documenta√ß√£o do Azure para o local "como est√°" indica isso (consulte a documenta√ß√£o no Azure SDN e Network Controller), que por um lado indica a atitude em rela√ß√£o √†s solu√ß√µes no local e, por outro, indica o relacionamento das solu√ß√µes em termos de entidades e arquitetura.  Quem copiou de quem e como realmente √© - uma quest√£o discut√≠vel. <br><br>  Em mar√ßo de 2018, Satya Nadela (CEO da Microsoft) anunciou oficialmente que a nuvem p√∫blica estava se tornando a prioridade da empresa.  O que, obviamente, simboliza o desbotamento gradual da linha de servidores para produtos no local (no entanto, a estagna√ß√£o foi observada em 2016, mas foi confirmada com a primeira vers√£o beta do Windows Server e outras linhas de produtos no local), com exce√ß√£o do Azure Edge - o servidor m√≠nimo necess√°rio Infraestrutura no escrit√≥rio do cliente para servi√ßos que n√£o podem ser levados para a nuvem. <br><br><h3>  Ferro virtual </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fundada em 2003, a Virtual Iron ofereceu uma vers√£o comercial do Xen e foi uma das primeiras a oferecer ao mercado suporte completo √† virtualiza√ß√£o de hardware. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 2009, a Oracle foi contratada para desenvolver sua pr√≥pria linha de virtualiza√ß√£o Oracle VM e expandi-la no x86. </font><font style="vertical-align: inherit;">Antes disso, o Oracle VM era oferecido apenas na plataforma SPARC.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Innotek </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No in√≠cio de 2007, a Innotek GmbH lan√ßou o segundo tipo de hipervisor propriet√°rio, o VirtualBox, que √© gratuito para uso n√£o comercial. No mesmo ano, uma vers√£o de c√≥digo aberto foi lan√ßada. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 2008, foi adquirida pela Sun, que por sua vez foi adquirida pela Oracle. A Oracle manteve o uso gratuito do produto para fins n√£o comerciais. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O VirtualBox suporta tr√™s formatos de discos virtuais - VDI (nativo), VMDK (VMware), VHD (Microsoft). Como SO host, Windows, macOS, Linux, Solaris e OpenSolaris s√£o suportados. O fork do VirtualBox for FreeBSD √© conhecido.</font></font><br><br><h3>  Ibm </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O mainframe √© o computador principal do datacenter com uma grande quantidade de mem√≥ria interna e externa (para refer√™ncia: nos anos 60, 1 MB de mem√≥ria era considerado irrealisticamente grande). Na verdade, o mainframe era um centro de computa√ß√£o: os primeiros computadores ocupavam salas de m√°quinas inteiras e consistiam em enormes racks. Hoje em dia √© chamado de data centers. Mas nos datacenters da mesma sala de m√°quinas pode haver milhares de computadores e, no in√≠cio da tecnologia da computa√ß√£o, um computador ocupava uma sala inteira. Cada rack vendeu um (!) Dispositivo de computador (racks separados com mem√≥ria, racks separados com dispositivos de armazenamento e dispositivos perif√©ricos separadamente). O n√∫cleo desta enorme m√°quina era um rack com um processador - era chamado de principal ou mainframe.Depois de mudar para os circuitos integrados de transistor, o tamanho desse milagre do pensamento cient√≠fico e de engenharia diminuiu significativamente, e o mainframe da IBM e seus an√°logos come√ßaram a ser entendidos como o mainframe.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nos anos 60 do s√©culo XX, o aluguel do poder de computa√ß√£o de todo o mainframe, para n√£o mencionar sua compra, custou muito dinheiro. Pouqu√≠ssimas empresas e institui√ß√µes poderiam ter esse luxo. O poder de computa√ß√£o do leasing era de hora em hora (o prot√≥tipo do modelo moderno de Pay as you go em nuvens p√∫blicas, n√£o √©?). O acesso aos inquilinos para a computa√ß√£o foi concedido sequencialmente. A solu√ß√£o l√≥gica foi paralelizar a carga computacional e isolar os c√°lculos dos inquilinos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pela primeira vez, a id√©ia de isolar v√°rias inst√¢ncias de sistemas operacionais em um mainframe foi proposta pelo IBM Cambridge Science Center com base no mainframe IBM System / 360-67. O desenvolvimento foi chamado de CP / CMS e, de fato, foi o primeiro hipervisor e forneceu paravirtualiza√ß√£o. CP (Control Program) - o pr√≥prio hypervisor, que criou v√°rias "m√°quinas virtuais" (VM) independentes. O CMS (originalmente o Cambridge Monitor System, mais tarde renomeado como Conversational Monitor System) era um sistema operacional leve e de usu√°rio √∫nico. Curiosamente, o CMS ainda est√° ativo e ainda √© usado na √∫ltima gera√ß√£o de mainframes do z / VM. Vale ressaltar que, na √©poca e at√© os anos 90, uma m√°quina virtual significava uma separa√ß√£o l√≥gica de discos f√≠sicos (discos ou dispositivos de armazenamento eram compartilhados,o hipervisor n√£o forneceu armazenamento para suas pr√≥prias necessidades) com uma parte dedicada de mem√≥ria virtual e tempo do processador usando a tecnologia Time-Sharing. As VMs n√£o previam intera√ß√£o de rede, porque as VMs da √©poca eram sobre computa√ß√£o e armazenamento de dados, e n√£o sobre transfer√™ncia. Nesse sentido, as VMs da √©poca eram mais como cont√™ineres do que VMs no sentido moderno.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O primeiro hypervisor comercial baseado no CP / CMS, chamado VM / 370, apareceu nos mainframes da s√©rie System / 370 em 2 de agosto de 1972. O nome geral dessa fam√≠lia de sistemas operacionais √© VM e, como parte desta se√ß√£o, VM ser√° entendida como o hypervisor IBM. A capacidade de executar v√°rios sistemas operacionais ao mesmo tempo, garantindo a estabilidade do sistema e isolando os usu√°rios um do outro (um erro no SO de um usu√°rio n√£o poderia afetar os c√°lculos de outro usu√°rio) - foi revolucion√°ria e se tornou um fator essencial no sucesso comercial da VM / 370. Um fato curioso: naquela √©poca na URSS, os esfor√ßos do Instituto de Pesquisa Cient√≠fica da Ci√™ncia da Computa√ß√£o (Minsk) clonaram com muito sucesso a arquitetura System / 370 e criaram sua pr√≥pria VM / 370 anal√≥gica sob o nome de computador da UE (com suporte para virtualiza√ß√£o incorporada! - para a possibilidade de desenvolver o sistema operacional mais b√°sico).Tais mainframes foram usados ‚Äã‚Äãpor institutos de pesquisa e empresas de defesa do campo socialista.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os anos 80 podem ser chamados com seguran√ßa de "era do mainframe". A VM foi um sucesso com os desenvolvedores de sistemas operacionais, os aplicativos foram escritos para ela e os c√°lculos foram feitos. Esta foi a d√©cada em que o compartilhamento de bancos de dados dominados pelo VM OS come√ßou a prevalecer nos mainframes. Uma das mudan√ßas mais importantes foi a LPAR (Logical Partition Access Resources), que realmente forneceu dois n√≠veis de virtualiza√ß√£o. Agora, os clientes podiam usar o mesmo conjunto de processadores, dispositivos de E / S e modems em sistemas VM executando em diferentes LPARs e permitindo que os recursos fossem migrados de um sistema VM para outro. Isso permitiu √†s organiza√ß√µes de TI oferecer desempenho consistente ao processar picos de carga de trabalho. Para otimizar a crescente base de clientes, a VM foi dividida em tr√™s produtos separados,dispon√≠vel no final dos anos 80:</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VM / SP - o sistema operacional de virtualiza√ß√£o multiuso usual para servidores System z </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HPO (High Performance Option) - VM / SP de alto desempenho para modelos de servidor System z mais antigos </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VM / XA (arquitetura ampliada) - Variante VM com suporte para a arquitetura S / S estendida 370</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No in√≠cio dos anos 90, a simplicidade e a conveni√™ncia da arquitetura x86 se tornaram mais atraentes para os clientes, e os mainframes estavam perdendo rapidamente a relev√¢ncia. </font><font style="vertical-align: inherit;">Os mainframes foram substitu√≠dos por sistemas de cluster, como o grunge, que substituiu o glam metal ao mesmo tempo. </font><font style="vertical-align: inherit;">No entanto, para uma certa classe de tarefas, por exemplo, ao construir um data warehouse centralizado, os mainframes se justificam tanto em termos de produtividade quanto do ponto de vista econ√¥mico. </font><font style="vertical-align: inherit;">Portanto, algumas empresas ainda usam mainframes em suas infraestruturas e a IBM projeta, libera e suporta novas gera√ß√µes.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Linux Xen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Xen (pronunciado zen) √© um hypervisor desenvolvido no Laborat√≥rio de Inform√°tica da Universidade de Cambridge, sob a dire√ß√£o de Ian Pratt e distribu√≠do sob a GPL. </font><font style="vertical-align: inherit;">A primeira vers√£o p√∫blica apareceu em 2003. </font><font style="vertical-align: inherit;">Posteriormente, Ian continuou trabalhando no hipervisor em sua vers√£o comercial, estabelecendo a empresa XenSource. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 2013, o Xen ficou sob o controle da Linux Foundation.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> XenSource </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tendo existido por v√°rios anos no mercado com os produtos XenServer e XenEnterprise, no final de 2007 foi adquirido pela Citrix. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Citrix XenServer </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tendo absorvido o XenSource por US $ 500 milh√µes, a Citrix n√£o conseguiu comercializar o problema. </font><font style="vertical-align: inherit;">Mais precisamente, eu realmente n√£o tentei fazer isso, n√£o considerando o XenServer como o principal produto e confiando no pre√ßo baixo das licen√ßas permanentes. </font><font style="vertical-align: inherit;">Ap√≥s vendas francamente malsucedidas em meio ao altamente bem-sucedido VMware ESX, foi decidido lan√ßar o XenServer no mundo de gra√ßa e com c√≥digo-fonte aberto completo em 2009. </font><font style="vertical-align: inherit;">No entanto, o c√≥digo do sistema de gerenciamento propriet√°rio do XenCenter n√£o foi aberto. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deve-se notar uma interessante coincid√™ncia cronol√≥gica das iniciativas da Citrix e da Microsoft no campo da virtualiza√ß√£o industrial, apesar de as empresas estarem sempre muito pr√≥ximas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apesar do nome de marketing comum, o Citrix XenApp e o XenDesktop n√£o t√™m nada a ver com o hipervisor Xen.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Amaz√¥nia </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A Amazon lan√ßou sua oferta p√∫blica de nuvem IaaS chamada EC2 (Elastic Compute) em 2006. </font><font style="vertical-align: inherit;">Inicialmente, a plataforma EC2 usava o hipervisor Xen e, posteriormente, a Amazon dividiu a plataforma em tr√™s partes, cada uma das quais usava uma ramifica√ß√£o e uma vers√£o separadas do hipervisor para minimizar o impacto de erros no c√≥digo na disponibilidade do servi√ßo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 2017, o KVM para cargas pesadas apareceu como um hipervisor adicional no EC2. </font><font style="vertical-align: inherit;">H√° opini√µes de que isso indica a transfer√™ncia gradual do EC2 para a KVM inteiramente no futuro.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> QEMU / KVM do Linux </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O QEMU (Quick EMUlator) √© um software universal para emular hardware de v√°rias plataformas, distribu√≠do sob a licen√ßa GPL v2. </font><font style="vertical-align: inherit;">Al√©m do x86, tamb√©m s√£o suportados ARM, MIPS, RISC-V, PowerPC, SPARC e SPARC64. </font><font style="vertical-align: inherit;">Com a versatilidade de uma plataforma com virtualiza√ß√£o completa, o QEMU n√£o teve desempenho compar√°vel a um sistema n√£o virtualizado. </font><font style="vertical-align: inherit;">Para acelerar o trabalho do QEMU no x86, foram oferecidas duas op√ß√µes principais, que foram finalmente rejeitadas em favor do desenvolvimento de KVM (m√°quina virtual baseada em kernel) da Qumranet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dizemos KVM - queremos dizer QEMU KVM e, portanto, obtemos o formato de disco virtual qcow2 (QEMU copy-on-write 2) para todas as plataformas baseadas no hypervisor KVM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Embora o QEMU funcione inicialmente como um segundo tipo de hypervisor, o QEMU / KVM √© o primeiro tipo de hypervisor.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Qumranet </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma empresa israelense, ex-desenvolvedora e principal patrocinadora do hypervisor KVM e do protocolo SPICE. Fundada em 2005, ganhou fama ap√≥s incorporar o KVM no kernel do Linux. 4 de setembro de 2008, adquirido pela Red Hat.</font></font><br><br><h3>  Chap√©u vermelho </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como todos os fabricantes de distribui√ß√£o GNU / Linux, at√© 2010, a Red Hat tinha suporte interno para o hipervisor Xen em suas distribui√ß√µes. </font><font style="vertical-align: inherit;">No entanto, como um dos principais players do mercado e uma marca s√©ria, pensei na minha pr√≥pria implementa√ß√£o do hypervisor. </font><font style="vertical-align: inherit;">A base foi tomada pelo hipervisor KVM normal, mas promissor. </font><font style="vertical-align: inherit;">A primeira vers√£o do Red Hat Enterprise Virtualization 2.2 (RHEV) foi lan√ßada em 2010 com a pretens√£o de competir por uma parte do mercado de solu√ß√µes VDI com a Citrix e a VMware devido ao desenvolvimento do Qumranet, que foi adquirido dois anos antes. </font><font style="vertical-align: inherit;">Pronto para uso, estavam dispon√≠veis clusters de alta disponibilidade, Live Migration e ferramentas de migra√ß√£o M2M (somente RHEL). </font><font style="vertical-align: inherit;">Vale ressaltar que, a julgar pela documenta√ß√£o da √©poca, a Red Hat manteve a nota√ß√£o Xen ao descrever a arquitetura da solu√ß√£o.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Em 28 de outubro de 2018, a IBM anunciou a compra da Red Hat. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Openstack </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Historicamente, o projeto OpenStack surgiu como uma iniciativa para contrastar algo com o monop√≥lio real da VMware no campo da virtualiza√ß√£o de servidores pesados ‚Äã‚Äãx86. </font><font style="vertical-align: inherit;">O projeto surgiu em 2010, gra√ßas aos esfor√ßos conjuntos da Rackspace Hosting (um provedor de nuvem) e da NASA (que abriu o c√≥digo para sua pr√≥pria plataforma Nebula). </font><font style="vertical-align: inherit;">O ponto alto da situa√ß√£o foi dado pelo fato de que em 2012 a VMware ingressou no gerenciamento de projetos do OpenStack e causou uma onda de indigna√ß√£o entre os ativistas fundadores. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com o tempo, Canonical (Ubuntu Linux), Debian, SUSE, Red Hat, HP e Oracle se juntaram ao projeto. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, nem tudo foi tranquilo. </font><font style="vertical-align: inherit;">Em 2012, a NASA deixou o projeto, optando pela AWS. </font><font style="vertical-align: inherit;">No in√≠cio de 2016, a HPE encerrou completamente seu projeto Helion com base no OpenStack.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como parte do projeto OpenStack, o KVM foi adotado como o hypervisor padr√£o. No entanto, devido √† modularidade da abordagem, um sistema baseado no OpenStack pode ser implementado usando outros hipervisores, deixando, por exemplo, apenas um sistema de controle do OpenStack. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">H√° uma ampla gama de opini√µes sobre o projeto OpenStack, desde adora√ß√£o entusiasta a ceticismo s√©rio e cr√≠ticas duras. As cr√≠ticas n√£o s√£o sem raz√£o - um n√∫mero significativo de problemas e perdas de dados foram registrados ao usar o OpenStack. O que, no entanto, n√£o impede que os f√£s neguem tudo e se refiram √† curvatura na implementa√ß√£o e opera√ß√£o dos sistemas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O projeto OpenStack n√£o se limita apenas √† virtualiza√ß√£o, mas com o tempo se tornou um n√∫mero significativo de v√°rios subprojetos e componentes para expans√£o na √°rea da pilha de servi√ßos de nuvem p√∫blica. </font><font style="vertical-align: inherit;">Al√©m disso, a import√¢ncia do OpenStack provavelmente deve ser avaliada precisamente nesta parte - esses componentes se tornaram essenciais em muitos produtos e sistemas comerciais, tanto no campo da virtualiza√ß√£o quanto al√©m. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Na R√∫ssia, o OpenStack fora das nuvens p√∫blicas √© amplamente conhecido principalmente por seu papel na substitui√ß√£o de importa√ß√µes. </font><font style="vertical-align: inherit;">A grande maioria das solu√ß√µes e produtos de virtualiza√ß√£o, incluindo sistemas hiperconvergentes, s√£o empacotados pelo OpenStack com v√°rios graus de refinamento.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nutanix AHV </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Desde a sua cria√ß√£o, a Nutanix √© um produto e plataforma exclusivamente para o VMware vSphere. </font><font style="vertical-align: inherit;">No entanto, em parte devido ao desejo de expandir a oferta de outros hipervisores, em parte devido √† crise pol√≠tica nas rela√ß√µes com a VMware, foi decidido desenvolver seu pr√≥prio hipervisor que completaria a plataforma in a box e permitiria abandonar produtos de terceiros. </font><font style="vertical-align: inherit;">A KVM foi escolhida como seu pr√≥prio hypervisor, que no √¢mbito da plataforma foi chamado de AHV (Acropolis HyperVisor).</font></font><br><br><h4>  Parallels </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Na vers√£o 7 do Virtuozzo, a empresa mudou de seu pr√≥prio hypervisor para a KVM. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Proxmox </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O Proxmox VE (Ambiente Virtual) √© um projeto de c√≥digo aberto da empresa austr√≠aca Proxmox Server Solutions GmbH, baseado no Debian Linux. </font><font style="vertical-align: inherit;">O primeiro lan√ßamento foi em 2008. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O produto suporta virtualiza√ß√£o de cont√™iner LXC (anteriormente OpenVZ) e virtualiza√ß√£o completa com o hipervisor KVM.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Paralelos / Virtuozzo / Rosplatform </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fundada em 1999 por Sergey Belousov, a SWsoft adotou o software de gerenciamento de hospedagem. Em 2003, a empresa rival de Novosibirsk, a Plesk, foi adquirida. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 2004, a SWsoft adquiriu a empresa russa Parallels Nikolai Dobrovolsky com seu produto Parallels Workstation (hypervisor de desktop do segundo tipo no Windows). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A empresa combinada mant√©m o nome de Parallels e em breve explodir√° o mercado com o Parallels Desktop for Mac (hypervisor de desktop do segundo tipo para MacOS).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como parte da virtualiza√ß√£o de servidores, o foco continua em provedores de hospedagem e data centers, em vez de uso corporativo. Devido √†s especificidades desse mercado espec√≠fico, os cont√™ineres Virtuozzo e OpenVZ, em vez das m√°quinas virtuais do sistema, tornaram-se o principal produto. Posteriormente, a Parallels, sem muito sucesso, est√° tentando entrar no mercado de virtualiza√ß√£o de servidores corporativos com o produto Parallels Bare Metal Server (posteriormente Parallels Hypervisor e Cloud Server e Virtuozzo), acrescenta hiperconverg√™ncia com seu Cloud Storage. O trabalho continua na automa√ß√£o e orquestra√ß√£o de provedores de hospedagem.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 2015, com base nos produtos de virtualiza√ß√£o de servidores, √© criado o projeto da plataforma Rosplatform - tecnicamente (omitindo aspectos legais e organizacionais) o mesmo Virtuozzo, apenas com pap√©is de parede modificados e no registro de software russo. </font><font style="vertical-align: inherit;">Baseado no software da plataforma Rosplatform e no equipamento Depo, o IBS cria uma oferta hiperconvergente do pacote Scala-R. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Antes da vers√£o 7, o Virtuozzo usava um hipervisor de seu pr√≥prio design; na vers√£o 7, era feita uma transi√ß√£o para o KVM. </font><font style="vertical-align: inherit;">Por conseguinte, a Rosplatform tamb√©m se baseia na KVM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ap√≥s v√°rias fus√µes, aquisi√ß√µes e rebrandings, a pr√≥xima imagem √© formada em 2019. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parallels Desktop √© uma subsidi√°ria da Parallels e vendida para a Corel. </font><font style="vertical-align: inherit;">Toda a automa√ß√£o foi para Odin e vendida para a IngramMicro. </font><font style="vertical-align: inherit;">A virtualiza√ß√£o de servidores permaneceu sob a marca da plataforma Virtuozzo / Rosplatform.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt474776/">https://habr.com/ru/post/pt474776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt474760/index.html">Prendemos o ngx-translate no aplicativo Angular. Passo a passo pr√°tico</a></li>
<li><a href="../pt474762/index.html">Semin√°rio: Solu√ß√µes de TI h√≠bridas para neg√≥cios. 14 de novembro, Moscou</a></li>
<li><a href="../pt474768/index.html">Transmiss√£o aberta do Main Hall HighLoad ++ 2019</a></li>
<li><a href="../pt474770/index.html">Como conduzimos o teste de regress√£o da folha de pagamento no SAP HCM</a></li>
<li><a href="../pt474772/index.html">Uma startup que usou a IA para desenvolver uma cura em 21 dias</a></li>
<li><a href="../pt474782/index.html">Vis√£o geral da tecnologia de s√≠ntese de fala</a></li>
<li><a href="../pt474784/index.html">Arcade Stick Story</a></li>
<li><a href="../pt474788/index.html">Organiza√ß√£o de rotas no Laravel</a></li>
<li><a href="../pt474790/index.html">Contos do negociador</a></li>
<li><a href="../pt474792/index.html">6-8 de dezembro - Rosbank Tech.Madness Hackathon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>