<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛩️ 🤑 👩🏻‍🤝‍👨🏿 Tupperware: Facebook killer Kubernetes? 🏞️ 🐊 🗽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Manajemen klaster yang efisien dan andal pada skala apa pun dengan Tupperware 





 Hari ini di konferensi Systems @Scale, kami memperkenalkan Tupper...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tupperware: Facebook killer Kubernetes?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/455579/"><p>  Manajemen klaster yang efisien dan andal pada skala apa pun dengan Tupperware </p><br><p><img src="https://habrastorage.org/webt/gm/mv/jn/gmmvjn5ev7lk2kyhaiejighzrf0.jpeg"></p><br><p>  Hari ini di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">konferensi Systems @Scale,</a> kami memperkenalkan Tupperware, sistem manajemen kluster kami yang mengatur wadah di jutaan server, tempat hampir semua layanan kami bekerja.  Kami pertama kali meluncurkan Tupperware pada tahun 2011, dan sejak itu infrastruktur kami telah berkembang dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">1 pusat data</a> menjadi sebanyak <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">15 pusat data yang didistribusikan secara geografis</a> .  Selama ini Tupperware tidak berdiri diam dan berkembang bersama kami.  Kami akan memberi tahu Anda dalam situasi apa Tupperware menyediakan manajemen klaster kelas satu, termasuk dukungan yang mudah untuk layanan stateful, panel kontrol tunggal untuk semua pusat data dan kemampuan untuk mendistribusikan daya antar layanan secara real time.  Dan kami akan membagikan pelajaran yang kami pelajari saat infrastruktur kami berkembang. </p><br><p> Tupperware melakukan berbagai tugas.  Pengembang aplikasi menggunakannya untuk mengirim dan mengelola aplikasi.  Ini mengemas kode dan dependensi aplikasi ke dalam gambar dan mengirimkannya ke server dalam bentuk wadah.  Kontainer menyediakan isolasi antara aplikasi pada server yang sama sehingga pengembang sibuk dengan logika aplikasi dan tidak memikirkan cara menemukan server atau mengontrol pembaruan.  Tupperware juga memonitor kinerja server, dan jika menemukan kegagalan, ia mentransfer kontainer dari server yang bermasalah. </p><a name="habracut"></a><br><p>  Insinyur Perencanaan Kapasitas menggunakan Tupperware untuk mendistribusikan kapasitas server ke dalam tim sesuai anggaran dan kendala.  Mereka juga menggunakannya untuk meningkatkan pemanfaatan server.  Operator pusat data beralih ke Tupperware untuk mendistribusikan kontainer dengan benar di antara pusat data dan menghentikan atau memindahkan kontainer selama perawatan.  Karena itu, pemeliharaan server, jaringan, dan peralatan membutuhkan keterlibatan manusia yang minimal. </p><br><h3 id="arhitektura-tupperware">  Arsitektur Tupperware </h3><br><p> <a href=""><img src="https://habrastorage.org/webt/e7/q1/oz/e7q1ozhv85xlsvgczzofpgwoikg.jpeg"></a> </p><br><p>  <em>Arsitektur Tupperware PRN adalah salah satu wilayah pusat data kami.</em>  <em>Wilayah ini terdiri dari beberapa bangunan pusat data (PRN1 dan PRN2) yang terletak di dekatnya.</em>  <em>Kami berencana untuk membuat satu panel kontrol yang akan mengelola semua server di satu wilayah.</em> </p><br><p>  Pengembang aplikasi memberikan layanan dalam bentuk pekerjaan Tupperware.  Tugas terdiri dari beberapa kontainer, dan semuanya biasanya menjalankan kode aplikasi yang sama. </p><br><p>  Tupperware bertanggung jawab atas penyediaan wadah dan manajemen siklus hidup.  Ini terdiri dari beberapa komponen: </p><br><ul><li>  Frontend Tupperware menyediakan API untuk antarmuka pengguna, CLI, dan alat otomatisasi lainnya di mana Anda dapat berinteraksi dengan Tupperware.  Mereka menyembunyikan seluruh struktur internal dari pemilik pekerjaan Tupperware. </li><li>  Penjadwal Tupperware adalah panel kontrol yang bertanggung jawab untuk mengelola wadah dan siklus kehidupan pekerjaan.  Ini dikerahkan di tingkat regional dan global, di mana penjadwal regional mengelola server di satu wilayah, dan penjadwal global mengelola server dari berbagai wilayah.  Penjadwal dibagi menjadi pecahan, dan setiap pecahan mengontrol satu set tugas. </li><li>  Proxy scheduler di Tupperware menyembunyikan sharding internal dan menyediakan panel kontrol terpadu yang nyaman bagi pengguna Tupperware. </li><li>  Distributor Tupperware memberikan kontainer ke server.  Penjadwal bertanggung jawab untuk menghentikan, memulai, memperbarui, dan gagal wadah.  Saat ini, satu distributor dapat mengelola seluruh wilayah tanpa membaginya menjadi pecahan.  (Perhatikan perbedaan dalam terminologi. Misalnya, scheduler di Tupperware sesuai dengan panel kontrol di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes</a> , dan distributor Tupperware disebut scheduler di Kubernetes.) </li><li>  Broker sumber daya menyimpan sumber kebenaran untuk acara server dan layanan.  Kami menjalankan satu broker sumber daya untuk setiap pusat data, dan menyimpan semua informasi server di pusat data ini.  Broker sumber daya dan sistem manajemen kapasitas, atau sistem alokasi sumber daya, secara dinamis memutuskan persediaan penjadwal mana yang mengendalikan server mana.  Layanan pemeriksaan kesehatan memonitor server dan menyimpan data tentang kesehatan mereka di broker sumber daya.  Jika server memiliki masalah atau perlu pemeliharaan, broker sumber daya memberi tahu distributor dan penjadwal untuk menghentikan kontainer atau mentransfernya ke server lain. </li><li>  Tupperware Agent adalah daemon yang berjalan di setiap server yang menyiapkan dan menghapus kontainer.  Aplikasi bekerja di dalam wadah, yang memberi mereka lebih banyak isolasi dan reproduktifitas.  Pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">konferensi Systems @Scale tahun lalu,</a> kami telah menjelaskan bagaimana masing-masing wadah Tupperware dibuat menggunakan gambar, btrfs, cgroupv2, dan systemd. </li></ul><br><h3 id="otlichitelnye-osobennosti-tupperware">  Fitur khas Tupperware </h3><br><p>  Tupperware sangat mirip dengan sistem manajemen cluster lainnya, seperti Kubernetes dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mesos</a> , tetapi ada beberapa perbedaan: </p><br><ul><li>  Dukungan asli untuk layanan stateful. </li><li>  Panel kontrol tunggal untuk server di berbagai pusat data untuk mengotomatiskan pengiriman kontainer berdasarkan maksud, dekomisioning kluster, dan pemeliharaan. </li><li>  Hapus pemisahan panel kontrol untuk zoom. </li><li>  Perhitungan fleksibel memungkinkan Anda untuk mendistribusikan daya antar layanan secara real time. </li></ul><br><p>  Kami merancang fitur-fitur keren ini untuk mendukung berbagai aplikasi tanpa status dan stateful di taman server bersama global yang besar. </p><br><h3 id="vstroennaya-podderzhka-stateful-sevisov">  Dukungan asli untuk layanan stateful. </h3><br><p>  Tupperware mengelola banyak layanan penting yang menyimpan data produk persisten untuk Facebook, Instagram, Messenger dan WhatsApp.  Ini bisa berupa pasangan nilai kunci yang besar (misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ZippyDB</a> ) dan memantau penyimpanan data (misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ODS Gorilla</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Scuba</a> ).  Mempertahankan layanan stateful tidak mudah, karena sistem harus memastikan bahwa pengiriman kontainer dapat menahan kegagalan skala besar, termasuk pemadaman listrik atau pemadaman listrik.  Meskipun metode konvensional, seperti mendistribusikan kontainer di seluruh domain kegagalan, sangat cocok untuk layanan stateless, layanan stateful membutuhkan dukungan tambahan. </p><br><p>  Sebagai contoh, jika sebagai akibat dari kegagalan server satu replika dari database menjadi tidak tersedia, apakah perlu untuk memungkinkan pemeliharaan otomatis yang akan memperbarui kernel pada 50 server dari kumpulan 10 ribu?  Itu tergantung situasi.  Jika pada salah satu dari 50 server ini terdapat replika lain dari database yang sama, lebih baik menunggu dan tidak kehilangan 2 replika sekaligus.  Untuk membuat keputusan dinamis tentang pemeliharaan dan kesehatan sistem, Anda memerlukan informasi tentang replikasi data internal dan logika lokasi setiap layanan stateful. </p><br><p>  Antarmuka TaskControl memungkinkan layanan stateful untuk mempengaruhi keputusan yang mempengaruhi ketersediaan data.  Menggunakan antarmuka ini, penjadwal memberi tahu aplikasi eksternal dari operasi wadah (restart, perbarui, migrasi, pemeliharaan).  Layanan Stateful mengimplementasikan pengontrol yang memberi tahu Tupperware ketika setiap operasi dapat dilakukan dengan aman, dan operasi ini dapat ditukar atau ditunda sementara.  Dalam contoh di atas, pengontrol basis data dapat memerintahkan Tupperware untuk memutakhirkan 49 dari 50 server, tetapi sejauh ini tidak menyentuh server tertentu (X).  Akibatnya, jika periode pembaruan kernel berlalu, dan database masih tidak dapat mengembalikan replika masalah, Tupperware masih akan meningkatkan server X. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/xu/xi/5q/xuxi5qpox1v3gpc6khbipxgna0i.jpeg"></a> </p><br><p>  Banyak layanan stateful di Tupperware tidak menggunakan TaskControl secara langsung, tetapi melalui ShardManager, platform umum untuk membuat layanan stateful di Facebook.  Dengan Tupperware, pengembang dapat menunjukkan niat mereka tentang bagaimana kontainer harus didistribusikan di seluruh pusat data.  Dengan ShardManager, pengembang menunjukkan niat mereka tentang bagaimana seharusnya pecahan data didistribusikan di seluruh wadah.  ShardManager menyadari hosting data dan replikasi aplikasi dan berinteraksi dengan Tupperware melalui antarmuka TaskControl untuk merencanakan operasi kontainer tanpa keterlibatan aplikasi langsung.  Integrasi ini sangat menyederhanakan pengelolaan layanan stateful, tetapi TaskControl mampu melakukan lebih.  Misalnya, tingkat web kami yang luas tidak memiliki kewarganegaraan dan menggunakan TaskControl untuk secara dinamis menyesuaikan kecepatan pembaruan dalam wadah.  Akibatnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tier web dapat dengan cepat menyelesaikan beberapa rilis perangkat lunak</a> per hari tanpa mengurangi ketersediaan. </p><br><h3 id="upravlenie-serverami-v-datacentrah">  Manajemen server di pusat data </h3><br><p>  Ketika Tupperware pertama kali muncul pada 2011, scheduler terpisah mengendalikan setiap server cluster.  Kemudian cluster Facebook adalah sekelompok rak server yang terhubung ke satu switch jaringan, dan pusat data berisi beberapa cluster.  Penjadwal dapat mengelola server hanya dalam satu cluster, yaitu tugas tidak dapat diperluas ke beberapa cluster.  Infrastruktur kami tumbuh, kami semakin menghapus cluster.  Karena Tupperware tidak dapat mentransfer tugas dari cluster yang dinonaktifkan ke kluster lain tanpa perubahan, diperlukan banyak upaya dan koordinasi yang cermat antara pengembang aplikasi dan operator pusat data.  Proses ini menyebabkan pemborosan sumber daya ketika server menganggur selama berbulan-bulan karena prosedur dekomisioning. </p><br><p>  Kami menciptakan broker sumber daya untuk menyelesaikan masalah dekomisioning cluster dan mengoordinasikan jenis tugas pemeliharaan lainnya.  Broker sumber daya memantau semua informasi fisik yang terkait dengan server, dan secara dinamis memutuskan penjadwal mana yang mengelola setiap server.  Pengikatan dinamis server ke penjadwal memungkinkan penjadwal untuk mengelola server di pusat data yang berbeda.  Karena pekerjaan Tupperware tidak lagi terbatas pada satu cluster, pengguna Tupperware dapat menentukan bagaimana kontainer harus didistribusikan di seluruh domain kegagalan.  Misalnya, pengembang dapat menyatakan niatnya (misalnya: "jalankan tugas saya di 2 domain gagal di wilayah PRN") tanpa menentukan zona ketersediaan khusus.  Tupperware sendiri akan menemukan server yang tepat untuk mewujudkan maksud ini bahkan dalam kasus menonaktifkan cluster atau layanan. </p><br><h3 id="masshtabirovanie-dlya-podderzhki-vsey-globalnoy-sistemy">  Penskalaan untuk mendukung seluruh sistem global </h3><br><p>  Secara historis, infrastruktur kami telah dibagi menjadi ratusan kumpulan server khusus untuk masing-masing tim.  Karena fragmentasi dan kurangnya standar, kami memiliki biaya transaksi yang tinggi, dan server idle lebih sulit untuk digunakan lagi.  Pada konferensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Systems @Scale</a> tahun lalu, kami memperkenalkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Infrastruktur sebagai Layanan (IaaS)</a> , yang harus mengintegrasikan infrastruktur kami ke dalam armada server besar dan terpadu.  Tetapi armada server tunggal memiliki kesulitan sendiri.  Itu harus memenuhi persyaratan tertentu: </p><br><ul><li>  <strong>Skalabilitas.</strong>  Infrastruktur kami tumbuh dengan penambahan pusat data di setiap wilayah.  Server menjadi lebih kecil dan lebih hemat energi, sehingga di setiap wilayah ada lebih banyak.  Akibatnya, satu penjadwal untuk suatu wilayah tidak dapat mengatasi jumlah kontainer yang dapat dijalankan pada ratusan ribu server di setiap wilayah. </li><li>  <strong>Keandalan</strong>  Bahkan jika skala penjadwal dapat ditingkatkan, karena cakupan yang besar dari penjadwal, risiko kesalahan akan lebih tinggi, dan seluruh wilayah kontainer mungkin menjadi tidak terkelola. </li><li>  <strong>Toleransi kesalahan.</strong>  Jika terjadi kegagalan infrastruktur yang besar (misalnya, karena gangguan jaringan atau pemadaman listrik, server tempat penjadwal berjalan akan gagal), hanya sebagian dari server wilayah akan memiliki konsekuensi negatif. </li><li>  <strong>Kemudahan penggunaan.</strong>  Tampaknya Anda perlu menjalankan beberapa penjadwal independen di satu wilayah.  Namun dalam hal kenyamanan, satu titik masuk ke kolam bersama di wilayah ini menyederhanakan kapasitas dan manajemen pekerjaan. </li></ul><br><p>  Kami membagi penjadwal menjadi pecahan untuk memecahkan masalah dengan mendukung kumpulan bersama yang besar.  Setiap penjadwal scheduler mengelola serangkaian tugasnya di wilayah tersebut, dan ini mengurangi risiko yang terkait dengan penjadwal.  Saat jumlah total bertambah, kita dapat menambahkan lebih banyak pecahan penjadwal.  Untuk pengguna Tupperware, pecahan dan penjadwal proksi terlihat seperti satu panel kontrol.  Mereka tidak harus bekerja dengan sekelompok pecahan yang mengatur tugas.  Pecahan scheduler pada dasarnya berbeda dari penjadwal cluster yang kami gunakan sebelumnya, ketika panel kontrol dibagi tanpa pemisahan statis dari kumpulan server umum sesuai dengan topologi jaringan. </p><br><h3 id="povyshenie-effektivnosti-ispolzovaniya-s-pomoschyu-elastichnyh-vychisleniy">  Meningkatkan pemanfaatan dengan komputasi elastis </h3><br><p>  Semakin besar infrastruktur kami, semakin penting untuk menggunakan server kami secara efisien untuk mengoptimalkan biaya infrastruktur dan mengurangi beban.  Ada dua cara untuk meningkatkan pemanfaatan server: </p><br><ul><li>  Komputasi yang fleksibel - mengurangi skala layanan online selama jam-jam tenang dan menggunakan server yang dibebaskan untuk beban offline, misalnya, untuk pembelajaran mesin dan tugas-tugas MapReduce. </li><li>  Beban berlebihan - layanan online host dan beban kerja batch pada server yang sama sehingga beban batch dieksekusi dengan prioritas rendah. </li></ul><br><p>  Hambatan di pusat data kami adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">konsumsi energi</a> .  Oleh karena itu, kami lebih memilih server kecil yang hemat energi yang bersama-sama memberikan daya pemrosesan yang lebih besar.  Sayangnya, pada server kecil dengan sejumlah kecil sumber daya prosesor dan memori, pemuatan berlebihan kurang efisien.  Tentu saja, kita dapat menempatkan beberapa wadah layanan kecil di satu server kecil hemat energi yang menggunakan sedikit sumber daya prosesor dan memori, tetapi layanan besar akan memiliki kinerja rendah dalam situasi ini.  Oleh karena itu, kami menyarankan pengembang layanan besar kami untuk mengoptimalkannya sehingga mereka menggunakan seluruh server. </p><br><p>  Pada dasarnya, kami meningkatkan pemanfaatan dengan komputasi elastis.  Intensitas penggunaan banyak layanan besar kami, misalnya, umpan berita, fitur pesan, dan level web front-end, tergantung pada waktu hari.  Kami sengaja mengurangi skala layanan online selama jam-jam tenang dan menggunakan server yang dibebaskan untuk beban offline, misalnya, untuk pembelajaran mesin dan tugas MapReduce. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/6w/zu/dp/6wzudppzm9tobgoryvtrssaxlra.jpeg"></a> </p><br><p>  Dari pengalaman, kami tahu bahwa yang terbaik adalah menyediakan seluruh server sebagai unit daya elastis, karena layanan besar adalah donor utama dan konsumen utama daya elastis, dan mereka dioptimalkan untuk penggunaan seluruh server.  Ketika server dibebaskan dari layanan online dalam jam-jam tenang, broker sumber daya memberikan server ke penjadwal untuk penggunaan sementara sehingga menjalankan beban offline di atasnya.  Jika puncak beban terjadi dalam layanan online, broker sumber daya dengan cepat memanggil kembali server yang dipinjamkan dan, bersama dengan penjadwal, mengembalikannya ke layanan online. </p><br><h3 id="usvoennye-uroki-i-plany-na-buduschee">  Pelajaran yang Dipetik dan Rencana Mendatang </h3><br><p>  Selama 8 tahun terakhir, kami telah mengembangkan Tupperware untuk mengimbangi perkembangan pesat Facebook.  Kami berbicara tentang apa yang telah kami pelajari dan berharap itu membantu orang lain mengelola infrastruktur yang berkembang pesat: </p><br><ul><li>  Atur komunikasi fleksibel antara panel kontrol dan server yang dikelolanya.  Fleksibilitas ini memungkinkan panel kontrol untuk mengelola server di berbagai pusat data, membantu mengotomatiskan dekomisioning dan pemeliharaan cluster, dan menyediakan distribusi daya dinamis menggunakan komputasi fleksibel. </li><li>  Dengan panel kontrol tunggal di wilayah ini, menjadi lebih mudah untuk bekerja dengan tugas dan lebih mudah untuk mengelola armada server besar yang umum.  Harap dicatat bahwa panel kontrol mendukung satu titik masuk, bahkan jika struktur internalnya dibagi karena alasan skala atau toleransi kesalahan. </li><li>  Menggunakan model plug-in, panel kontrol dapat memberi tahu aplikasi eksternal dari operasi kontainer yang akan datang.  Selain itu, layanan stateful dapat menggunakan antarmuka plugin untuk mengkonfigurasi manajemen kontainer.  Menggunakan model plug-in ini, panel kontrol memberikan kesederhanaan dan secara efektif melayani berbagai layanan stateful. </li><li>  Kami percaya bahwa komputasi elastis, di mana kami mengambil seluruh server untuk pekerjaan batch, pembelajaran mesin dan layanan tidak mendesak lainnya dari layanan donor, adalah cara terbaik untuk meningkatkan efisiensi penggunaan server kecil dan hemat energi. </li></ul><br><p>  Kami baru mulai menerapkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">taman server global tunggal yang umum</a> .  Sekarang sekitar 20% dari server kami berada di kolam renang umum.  Untuk mencapai 100%, Anda perlu menyelesaikan banyak masalah, termasuk mendukung kumpulan umum untuk sistem penyimpanan, mengotomatisasi pemeliharaan, mengelola persyaratan klien yang berbeda, meningkatkan pemanfaatan server dan meningkatkan dukungan untuk beban kerja pembelajaran mesin.  Kami tidak sabar untuk menangani tugas-tugas ini dan berbagi kesuksesan kami. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id455579/">https://habr.com/ru/post/id455579/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id455565/index.html">Bisakah pikiran memalsukan alam semesta?</a></li>
<li><a href="../id455569/index.html">Kami mengundang Anda ke Konferensi Tarantool pada 17 Juni</a></li>
<li><a href="../id455571/index.html">DB Cursors dalam Doctrine</a></li>
<li><a href="../id455575/index.html">Pencocokan Saraf Tiruan: Cara menyesuaikan konten dengan realitas Google</a></li>
<li><a href="../id455577/index.html">SDL 2 Pelajaran: Pelajaran 3 - Acara</a></li>
<li><a href="../id455580/index.html">Animasi Aplikasi Mobile yang Harus Dimiliki</a></li>
<li><a href="../id455582/index.html">Navigasi di toko: melalui augmented reality ke rak yang diinginkan</a></li>
<li><a href="../id455584/index.html">Wawancara khusus dengan kekuatan internal perusahaan: melalui kesalahan penemuan</a></li>
<li><a href="../id455586/index.html">Seri Kuliah tentang Robotika oleh Profesor Gregor Schöner, Direktur Institute of Neuroinformatics (INI) Bochum, Jerman</a></li>
<li><a href="../id455588/index.html">Cara mendidik komunitas Anda agar tidak menari dengan rebana</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>