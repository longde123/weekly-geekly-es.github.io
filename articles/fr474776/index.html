<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ∞Ô∏è ü§∏ üèÇüèø Th√©orie g√©n√©rale et arch√©ologie de la virtualisation x86 üëçüèø üç§ ‚úèÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pr√©sentation 
 √âquipe d'auteurs 
 Publi√© par Anton Zhbankov ( AntonVirtual , cloudarchitect.cc ) 
 Co-auteurs: Grigory Pryalukhin , Evgeny Parfenov 

...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Th√©orie g√©n√©rale et arch√©ologie de la virtualisation x86</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/474776/"><h2>  Pr√©sentation </h2><br><h4>  √âquipe d'auteurs </h4><br>  Publi√© par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Anton Zhbankov</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">AntonVirtual</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cloudarchitect.cc</a> ) <br>  Co-auteurs: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Grigory Pryalukhin</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Evgeny Parfenov</a> <br><br><h3>  Concepts g√©n√©raux de virtualisation </h3><br>  J'ai d√ª voir beaucoup d'interpr√©tations de ce qu'est la <i>virtualisation</i> et √©couter beaucoup de controverses, pas un peu plus pr√®s de discuter du r√©sultat pratique.  Et comme vous le savez, l'argument de deux personnes intelligentes se r√©sume √† un d√©bat sur les d√©finitions.  D√©finissons ce qu'est la virtualisation et ce qui en d√©coule. <br><br>  La d√©finition la plus proche de la virtualisation sera probablement ¬´l'abstraction¬ª de la programmation orient√©e objet.  Ou, si traduit en russe normal, cela cache l'impl√©mentation derri√®re une interface abstraite.  Ce qui, bien s√ªr, expliquait tout √† la fois.  R√©essayons, mais pour ceux qui n'ont pas √©tudi√© la programmation. <br><blockquote>  Virtualisation - cacher une impl√©mentation sp√©cifique derri√®re une m√©thode standardis√©e universelle d'acc√®s aux ressources / donn√©es. </blockquote><br>  Si vous essayez de mettre cette d√©finition en pratique, il s'av√®re qu'elle fonctionne sur des sujets compl√®tement inattendus.  Disons l'horloge.  Ainsi, un cadran solaire a √©t√© invent√© il y a plusieurs milliers d'ann√©es, et au Moyen √Çge, un cadran m√©canique a √©t√© invent√©.  Qu'y a-t-il en commun?  Le soleil et quelques engrenages?  Une sorte de non-sens.  Et puis les oscillateurs √† quartz et tout le reste. <br>  L'essentiel est que nous avons une interface standard - un pointeur ou un pointeur num√©rique, qui sous une forme standard universelle indique l'heure actuelle.  Mais est-ce important pour nous de savoir comment ce m√©canisme est sp√©cifiquement impl√©ment√© √† l'int√©rieur de la bo√Æte, si l'heure est indiqu√©e avec une pr√©cision suffisante pour nous? <br>  ¬´Laissez-moi¬ª, vous pouvez dire, ¬´mais je pensais que la virtualisation concernait les machines, les processeurs l√†-bas, etc.! <br>  Oui, il s'agit de voitures et de processeurs, mais ce n'est qu'un cas sp√©cial.  Regardons plus largement, puisque l'article revendique hardiment une th√©orie g√©n√©rale. <br><a name="habracut"></a><br><h2>  POZOR! </h2><br><h3>  Uwaga!  Achtung!  Pozor! </h3><br>  Cet article a un objectif <b>p√©dagogique g√©n√©ral</b> pour relier un tas de technologies et de mots effrayants avec l'histoire dans une certaine structure, et en raison de cette circonstance contient une quantit√© importante de simplifications <b>intentionnelles</b> .  Bien s√ªr, il contient √©galement un grand nombre d'omissions ennuyeuses, et m√™me des erreurs ringardes avec des fautes de frappe.  Les critiques constructives ne sont que les bienvenues, en particulier sous la forme de "Permettez-moi de vous rappeler cette partie." <br><br><h2>  Types de virtualisation </h2><br>  Revenons de concepts compl√®tement abstraits aux plus familiers √† nos ordinateurs bien-aim√©s. <br><br><h3>  Virtualisation du stockage </h3><br>  Le premier, probablement, est le type de virtualisation qu'un geek novice rencontre - la virtualisation d'un syst√®me de stockage de donn√©es.  Dans ce cas, le syst√®me de stockage n'est pas utilis√© dans le sens d'une grande baie avec des disques connect√©s via Fibre Channel, mais comme un sous-syst√®me logique responsable du stockage de donn√©es √† long terme. <br><br><h4>  FS -&gt; LBA -&gt; CHS </h4><br>  Prenez le cas le plus simple d'un syst√®me de stockage sur un seul disque magn√©tique dur.  Le format habituel pour travailler avec des donn√©es est les fichiers qui se trouvent sur le lecteur logique.  Le fichier peut √™tre ouvert, lu, ferm√©.  Mais un objet tel qu'un fichier n'existe tout simplement pas physiquement - il n'y a qu'un moyen d'acc√©der √† certains blocs de donn√©es en utilisant la m√©thode d'adressage du formulaire ¬´lecteur: \ dossier1 \ dossier2 \ fichier¬ª.  C'est-√†-dire  nous rencontrons la premi√®re couche de virtualisation - du mn√©monique et compr√©hensible aux humains, nous traduisons tout en adresses compr√©hensibles par le syst√®me.  Dans les tableaux de m√©tadonn√©es, le pilote du syst√®me de fichiers recherche le type de blocs de donn√©es et nous obtenons l'adresse dans le syst√®me d'adressage de bloc logique (LBA).  Dans le syst√®me LBA, les blocs ont une taille fixe et se suivent lin√©airement, c'est-√†-dire  d'une mani√®re ou d'une autre, cela peut avoir √† voir avec le stockage de donn√©es sur bande magn√©tique, mais le disque dur est en quelque sorte compl√®tement diff√©rent!  Et nous passons ici √† la deuxi√®me couche de virtualisation - traduction de l'adressage LBA en CHS (cylindre / culasse / secteur). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e95/940/bd4/e95940bd4389a0187c2bf3d82406e118.png" alt="image"><br><br>  CHS, √† son tour, d√©j√† dans le contr√¥leur de disque dur commence √† se traduire en param√®tres physiques pour la lecture, mais c'est une histoire compl√®tement diff√©rente. <br>  M√™me dans un acc√®s simple au fichier pour, par exemple, visualiser un vidosik avec des m√©mos, nous avons rencontr√© imm√©diatement trois couches de virtualisation. <br>  Tout serait trop simple si les couches ne commen√ßaient pas √† se chevaucher dans un ordre al√©atoire et de diverses mani√®res. <br><br><h4>  RAID </h4><br>  La prochaine couche de virtualisation, que beaucoup de gens ne consid√®rent pas √† tort comme de la virtualisation, est le RAID (matrice redondante de disques peu co√ªteux / ind√©pendants). <br><br>  La caract√©ristique cl√© du RAID dans le contexte des concepts discut√©s n'est pas sa capacit√© √† prot√©ger les donn√©es contre la d√©faillance d'un disque physique particulier.  RAID fournit un deuxi√®me niveau d'adressage LBA en plus de plusieurs (parfois tr√®s nombreuses) adresses LBA ind√©pendantes.  √âtant donn√© que nous pouvons acc√©der au RAID, quel que soit le niveau RAID, exactement de la m√™me mani√®re qu'un seul disque sans RAID, nous pouvons dire avec confiance: <blockquote>  RAID est la virtualisation de disque. </blockquote><br>  De plus, le contr√¥leur RAID ne cr√©e pas seulement un grand disque virtuel √† partir de plusieurs disques physiques, mais peut en cr√©er un nombre arbitraire en ajoutant une autre couche de virtualisation. <br><br><h3>  Voir la virtualisation </h3><br>  Le prochain type de virtualisation, que beaucoup d'entre nous utilisent presque tous les jours, mais ne consid√®rent pas comme une virtualisation, est une connexion √† distance au bureau. <br><br>  Les serveurs de terminaux, VDI et m√™me juste RDP via VPN vers le serveur sont tous des virtualisations de session.  En utilisant une interface standard (moniteur, clavier, souris), nous travaillons soit avec une vraie machine, soit avec une conception incompr√©hensible √† partir d'un bureau virtuel sur un clone li√© avec une application conteneuris√©e, √† partir de laquelle nous transf√©rons des donn√©es via un tampon vers une application avec livraison en streaming.  Ou non, qui le d√©couvrira, √† part celui qui l'a con√ßu? <br><br><h2>  Introduction √† la virtualisation x86 </h2><br><h3>  Histoire et vue d'ensemble des processeurs </h3><br><h4>  Ex√©cution du programme </h4><br>  Lors de la premi√®re le√ßon d'un cours de programmation sp√©cial, Vladimir Denisovich Lelyukh (repose en paix pour lui) a dit aux √©l√®ves: l'ordinateur, malgr√© son nom, ne peut pas compter, il peut pr√©tendre qu'il peut compter.  Mais si quelque chose ressemble √† un canard, marche comme un canard et charlatan comme un canard, d'un point de vue pratique, c'est un canard. <br><br>  Essayons de nous en souvenir pour une utilisation pratique suppl√©mentaire. <br><br>  L'ordinateur, et en particulier le processeur, ne fait rien - il attend juste certains param√®tres d'entr√©e √† certains endroits, puis, √† travers une terrible magie noire, donne des r√©sultats √† certains endroits. <br><br>  Un programme dans ce cas est un certain flux de commandes ex√©cut√©es strictement s√©quentiellement, √† la suite de quoi nous nous attendons √† voir un certain r√©sultat. <br>  Mais si le programme est en cours d'ex√©cution, comment les donn√©es peuvent-elles √™tre saisies?  Et en g√©n√©ral, interagissent en quelque sorte sur un ordinateur? <br><br>  Pour cela, des interruptions mat√©rielles ont √©t√© invent√©es.  L'utilisateur appuie sur une touche - le contr√¥leur du clavier le signale et il y a une interruption dans l'ex√©cution du thread de code actuel.  Les adresses des gestionnaires d'interruption sont enregistr√©es dans une zone de m√©moire sp√©cifique, et apr√®s avoir enregistr√© l'√©tat actuel, le contr√¥le est transf√©r√© au gestionnaire d'interruption.  √Ä son tour, le gestionnaire devrait, en th√©orie, tout traiter rapidement, puis lui et le gestionnaire, √©crire la touche enfonc√©e dans le tampon souhait√© et retourner le contr√¥le.  Ainsi, l'application semble fonctionner et nous pouvons interagir avec le syst√®me. <br><br>  Les gestionnaires d'interruptions (et les principaux types de gestionnaires sont des pilotes de p√©riph√©riques) ont la possibilit√© d'entrer dans un mode processeur sp√©cial, lorsque d'autres interruptions ne peuvent pas √™tre impl√©ment√©es avant de quitter ce mode.  Ce qui au final a souvent conduit √† un probl√®me de blocage - une erreur dans le pilote n'a pas permis de sortir de l'interruption. <br><br><h4>  Multit√¢che </h4><br>  Que faire dans une situation s'il est n√©cessaire d'ex√©cuter plusieurs programmes (flux de code avec leurs structures de donn√©es et de m√©moire) en m√™me temps?  √âvidemment, s'il y a plus de flux de code que de p√©riph√©riques capables de les ex√©cuter, c'est un probl√®me. <br><br>  Le pseudo-multit√¢che appara√Æt lorsqu'une t√¢che est ex√©cut√©e lors du passage direct √† celle-ci. <br><br>  √Ä l'avenir, une coop√©rative (multit√¢che non pr√©emptive) appara√Æt - la t√¢che ex√©cutable elle-m√™me comprend qu'elle n'a plus besoin de ressources processeur et donne le contr√¥le √† quelqu'un d'autre.  Mais tout cela ne suffit pas. <br><br>  Et l√† encore, les interruptions + la capacit√© de faire semblant viennent √† notre secours.  Peu importe √† l'utilisateur qu'ils soient ex√©cut√©s strictement simultan√©ment, il suffit de ressembler √† √ßa. <br>  Par cons√©quent, un gestionnaire est simplement raccroch√© pour interrompre le temporisateur, qui commence √† contr√¥ler le flux de code qui doit √™tre ex√©cut√© ensuite.  Si le temporisateur est d√©clench√© assez souvent (disons 15 ms), alors pour l'utilisateur, tout ressemble √† une op√©ration parall√®le.  Et donc il y a un multit√¢che moderne d'√©viction. <br><br><h4>  Mode r√©el </h4><br>  Le v√©ritable mode processeur dans cet article peut √™tre d√©crit simplement - toute la m√©moire est disponible pour tout le monde.  Toute application, y compris les logiciels malveillants (logiciels malveillants, logiciels malveillants), peut acc√©der n'importe o√π, √† la fois pour la lecture et l'√©criture. <br><br>  Il s'agit du mode de fonctionnement initial de la famille de processeurs Intel x86. <br><br><h4>  Mode prot√©g√© </h4><br>  En 1982, une innovation est apparue dans le processeur Intel 80286 (ci-apr√®s simplement 286) - un mode de fonctionnement prot√©g√©, qui a apport√© des innovations dans l'organisation du travail avec la m√©moire (par exemple, l'allocation des types de segments de m√©moire - code, donn√©es, pile).  Mais la chose la plus importante que le processeur 286 a apport√©e au monde x86 est le concept d'anneaux de protection, que nous utilisons toujours. <br><br>  Le concept d'anneaux de protection est apparu √† l'origine dans le syst√®me d'exploitation Multics pour le mainframe GE645 (1967) avec une impl√©mentation partiellement logicielle et enti√®rement mat√©rielle d√©j√† en 1970 dans le syst√®me Honeywell 6180. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a9/249/522/2a9249522d8bdb211dbb4eb0aec70b75.png" alt="image"><br><br>  L'id√©e de base des anneaux de d√©fense ressemble √† des forteresses m√©di√©vales √† plusieurs niveaux; la plus pr√©cieuse r√©side au centre m√™me derri√®re les multiples murs.  Dans ce cas, la chose la plus pr√©cieuse est un acc√®s direct illimit√© √† n'importe quelle zone de RAM et un contr√¥le sur tous les processus.  Ils sont poss√©d√©s par des processus op√©rant dans l'anneau de protection z√©ro.  Derri√®re le mur, dans le premier anneau, des processus moins importants fonctionnent, tels que les pilotes de p√©riph√©rique et, dans le dernier, les applications utilisateur.  Le principe est simple - de l'int√©rieur, vous pouvez sortir, mais de l'ext√©rieur vers l'int√©rieur est interdit.  C'est-√†-dire  aucun processus utilisateur ne peut acc√©der √† la m√©moire du noyau du syst√®me d'exploitation, comme cela √©tait possible auparavant en mode r√©el. <br><br>  Dans la toute premi√®re mise en ≈ìuvre compl√®te du Honeywell 6180, 8 anneaux de protection ont √©t√© mis en ≈ìuvre, mais Intel a d√©cid√© de simplifier le circuit √† 4, dont les fabricants de syst√®mes d'exploitation ont commenc√© √† n'utiliser que deux, z√©ro et troisi√®me. <br><br><h4>  32bit </h4><br>  En 1985, un autre processeur extr√™mement important sur le plan architectural de la gamme x86 a √©t√© lanc√© - 80386 (ci-apr√®s 386), qui impl√©mentait un adressage m√©moire 32 bits et utilisait des instructions 32 bits.  Et bien s√ªr, la virtualisation de la m√©moire.  Comme d√©j√† mentionn√©, la virtualisation est la dissimulation de la mise en ≈ìuvre r√©elle par la fourniture de ressources artificielles ¬´virtuelles¬ª.  Dans ce cas, nous parlons d'adressage m√©moire.  Le segment de m√©moire a son propre adressage, qui n'a rien √† voir avec l'emplacement r√©el des cellules de m√©moire. <br>  Le processeur s'est av√©r√© si demand√© qu'il a √©t√© produit avant 2007. <br>  L'architecture en termes d'Intel s'appelle IA32. <br><br><h4>  64bit </h4><br>  Bien s√ªr, m√™me sans virtualisation au milieu des ann√©es 2000, l'industrie se heurtait d√©j√† aux limites de 32 bits.  Il existe des solutions de contournement partielles sous la forme de PAE (Physical Address Extension), mais elles compliquent et ralentissent le code.  La transition vers 64 bits √©tait une fatalit√©. <br><br>  AMD a pr√©sent√© sa version de l'architecture, appel√©e AMD64.  Chez Intel, ils esp√©raient la plate-forme IA64 (Intel Architecture 64), que nous connaissons √©galement sous le nom d'Itanium.  Cependant, le march√© a rencontr√© cette architecture sans grand enthousiasme et, par cons√©quent, Intel a √©t√© contraint d'impl√©menter son propre support pour les instructions AMD64, qui s'appelait d'abord EM64T, puis juste Intel 64. <br><br>  En fin de compte, nous connaissons tous cette architecture comme AMD64, x86-64, x86_64 ou parfois x64. <br><br>  √âtant donn√© que l'utilisation principale des serveurs √† l'√©poque √©tait cens√©e √™tre physique, sans virtualisation, une dr√¥le de technique s'est produite avec les premiers processeurs 64 bits de la virtualisation.  Les hyperviseurs imbriqu√©s √©taient souvent utilis√©s comme serveurs de laboratoire; tout le monde ne pouvait pas se permettre plusieurs clusters de serveurs physiques.  Et √† la fin, il s'est av√©r√© que la machine virtuelle de charge dans l'hyperviseur int√©gr√© ne pouvait fonctionner qu'en mode 32 bits. <br><br>  Dans les premiers processeurs x86-64, les d√©veloppeurs, tout en conservant une compatibilit√© totale avec le mode de fonctionnement 32 bits, ont supprim√© une partie importante des fonctionnalit√©s en mode 64 bits.  Dans ce cas, le probl√®me √©tait de simplifier consid√©rablement la segmentation de la m√©moire.  La possibilit√© de garantir l'inviolabilit√© d'un petit morceau de m√©moire dans la machine virtuelle o√π fonctionnait le gestionnaire d'exceptions de l'hyperviseur a √©t√© supprim√©e.  En cons√©quence, le syst√®me d'exploitation invit√© a pu le modifier. <br>  Par la suite, AMD a renvoy√© la possibilit√© de limiter les segments, et Intel a simplement attendu l'introduction de la virtualisation mat√©rielle. <br><br><h4>  UMA </h4><br>  Les syst√®mes multiprocesseurs X86 ont commenc√© √† fonctionner avec le mode UMA (Uniform Memory Access), dans lequel la distance entre tout processeur (retard dans l'acc√®s √† une cellule de m√©moire) et n'importe quelle barre de m√©moire est la m√™me.  Dans les processeurs Intel, ce sch√©ma de travail a √©t√© conserv√© m√™me apr√®s l'apparition des processeurs multic≈ìurs jusqu'√† la g√©n√©ration 54xx (Harpertown).  √Ä partir de la g√©n√©ration 55xx (Nehalem), les processeurs sont pass√©s √† l'architecture NUMA. <br><br>  Du point de vue de la logique d'ex√©cution, c'est l'apparition de threads mat√©riels suppl√©mentaires, sur lesquels vous pouvez affecter des flux de code pour une ex√©cution en parall√®le. <br><br><h4>  NUMA </h4><br>  NUMA (Non Uniform Memory Access) - architecture avec un acc√®s in√©gal √† la m√©moire.  Dans cette architecture, chaque processeur poss√®de sa propre m√©moire locale, accessible directement avec une faible latence.  La m√©moire des autres processeurs est accessible indirectement avec des retards plus √©lev√©s, ce qui entra√Æne une baisse des performances. <br><br>  Pour les processeurs Intel Xeon Scalable v2 pour 2019, l'architecture interne reste toujours UMA dans le socket, se transformant en NUMA pour les autres sockets (bien que pas vraiment, et cela ne fait que pr√©tendre l'√™tre).  Les processeurs Opteron d'AMD avaient une architecture NUMA m√™me √† l'√©poque du plus ancien UMA Xeon, puis NUMA est devenu m√™me √† l'int√©rieur du socket jusqu'√† la derni√®re g√©n√©ration de Rome, dans laquelle ils sont revenus au socket NUMA =. <br><br><h3>  Machine virtuelle </h3><br>  Machine virtuelle , la plate-forme h√¥te), ou virtualiser une plate-forme et cr√©er des environnements qui isolent les programmes et m√™me les syst√®mes d'exploitation les uns des autres.  Wikip√©dia <br>  Dans cet article, nous dirons ¬´machine virtuelle¬ª, ce qui signifie ¬´machines virtuelles syst√®me¬ª, permettant de simuler compl√®tement toutes les ressources et le mat√©riel sous la forme de constructions logicielles. <br>  Il existe deux principaux types de logiciels pour cr√©er des machines virtuelles - avec full et resp.  virtualisation incompl√®te. <br><br>  <b>La virtualisation compl√®te</b> est une approche dans laquelle tout le mat√©riel, y compris le processeur, est √©mul√©.  Vous permet de cr√©er des environnements ind√©pendants du mat√©riel et d'ex√©cuter par exemple le syst√®me d'exploitation et les logiciels d'application pour la plate-forme x86 sur les syst√®mes SPARC, ou les √©mulateurs Spectrum bien connus avec le processeur Z80 sur le x86 familier.  Le revers de la m√©daille de l'ind√©pendance totale est le surco√ªt √©lev√© de virtualisation du processeur et les faibles performances globales. <br><br>  <b>La virtualisation incompl√®te</b> est une approche dans laquelle pas 100% du mat√©riel est virtualis√©.  √âtant donn√© que la virtualisation incompl√®te est la plus courante dans l'industrie, nous en parlerons.  A propos des plateformes et technologies des machines virtuelles syst√®me √† virtualisation incompl√®te pour l'architecture x86.  Dans ce cas, la virtualisation du processeur est incompl√®te, c'est-√†-dire  √† l'exception de la substitution partielle ou du masquage de certains appels syst√®me, le code binaire de la machine virtuelle est ex√©cut√© directement par le processeur. <br><br><h4>  Virtualisation logicielle </h4><br>  La cons√©quence √©vidente de l'architecture du processeur et des habitudes des syst√®mes d'exploitation pour fonctionner dans le ring z√©ro √©tait le probl√®me - le noyau de l'OS invit√© ne peut pas fonctionner √† l'endroit habituel.  L'anneau z√©ro est occup√© par l'hyperviseur, et il suffit de laisser le syst√®me d'exploitation invit√© y arriver √©galement - d'une part, nous sommes revenus en mode r√©el avec toutes les cons√©quences, et d'autre part, le syst√®me d'exploitation invit√© n'attend personne l√†-bas et d√©truira instantan√©ment toutes les structures de donn√©es et abandonnera la voiture. <br><br>  Mais tout a √©t√© d√©cid√© tout simplement: puisque pour l'hyperviseur, le syst√®me d'exploitation invit√© n'est qu'un ensemble de pages m√©moire avec acc√®s direct complet, et le processeur virtuel n'est qu'une file d'attente de commandes, pourquoi ne pas les r√©√©crire?  √Ä la vol√©e, l'hyperviseur rejette de la file d'attente d'instructions √† ex√©cuter sur le processeur virtuel toutes les instructions qui n√©cessitent des privil√®ges de sonnerie nulle, en les rempla√ßant par des privil√®ges moins privil√©gi√©s.  Mais le r√©sultat de ces instructions est pr√©sent√© exactement de la m√™me mani√®re que si l'OS invit√© √©tait dans l'anneau z√©ro.  Ainsi, vous pouvez virtualiser n'importe quoi, jusqu'√† l'absence totale d'un OS invit√©. <br>  Cette approche a √©t√© mise en ≈ìuvre par l'√©quipe de d√©veloppement en 1999 dans le produit VMware Workstation, puis en 2001 dans les hyperviseurs de serveur GSX (le deuxi√®me type, comme Workstation) et ESX (le premier type). <br><br><h4>  Paravirtualisation </h4><br>  <b>La paravirtualisation</b> est un concept tr√®s simple, qui suppose que le syst√®me d'exploitation invit√© sait qu'il se trouve dans une machine virtuelle et sait comment acc√©der au syst√®me d'exploitation h√¥te pour certaines fonctions du syst√®me.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cela √©limine le probl√®me d'√©mulation de l'anneau z√©ro - le syst√®me d'exploitation invit√© sait qu'il n'est pas dans le z√©ro et se comporte en cons√©quence. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La paravirtualisation en x86 est apparue en 2003 avec le projet Linux Xen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Certaines fonctions paravirtualis√©es sont √©galement impl√©ment√©es dans des hyperviseurs avec virtualisation compl√®te via des pilotes virtuels sp√©ciaux dans les OS invit√©s qui communiquent avec l'hyperviseur pour r√©duire la surcharge de virtualisation. </font><font style="vertical-align: inherit;">Par exemple, VMware ESXi pour machines virtuelles dispose d'un adaptateur SCSI paravirtuel PVSCSI, qui am√©liore les performances globales des machines virtuelles avec des op√©rations de disque intensives, telles que les SGBD charg√©s. </font><font style="vertical-align: inherit;">Les pilotes pour les p√©riph√©riques paravirtuels sont fournis dans des packages suppl√©mentaires (par exemple VMware Tools), ou sont d√©j√† inclus dans les distributions Linux (open-vm-tools).</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Virtualisation mat√©rielle </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Avec le d√©veloppement et la croissance de la popularit√© de la virtualisation, les deux fabricants de plates-formes ont souhait√© r√©duire leurs co√ªts de support et, d'un point de vue de la s√©curit√©, garantir la protection du mat√©riel. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le probl√®me a √©t√© r√©solu de mani√®re tr√®s simple - les technologies de virtualisation mat√©rielle propri√©taires Intel VT-x et AMD-V ont √©t√© ajout√©es, si nous ignorons les d√©tails techniques approfondis, moins le premier anneau de protection pour l'hyperviseur. </font><font style="vertical-align: inherit;">Ainsi, la situation de travail dans l'anneau z√©ro familier √† l'OS a finalement √©t√© √©tablie.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Types d'hyperviseurs </font></font></h3><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Type 2 (h√©berg√©) </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les hyperviseurs du deuxi√®me type sont des applications qui s'ex√©cutent sur le syst√®me d'exploitation h√¥te. Tous les appels de machine virtuelle sont g√©r√©s par le syst√®me d'exploitation h√¥te en amont. Les hyperviseurs du deuxi√®me type ont des performances tr√®s limit√©es, car l'application de l'hyperviseur, n'ayant pas le droit √† l'allocation exclusive de ressources informatiques, est oblig√©e de rivaliser pour eux avec d'autres applications utilisateur. En termes de s√©curit√©, les hyperviseurs de type 2 d√©pendent directement des politiques de s√©curit√© de l'OS utilisateur et de sa vuln√©rabilit√© aux attaques. Aujourd'hui, il existe une opinion unanime dans l'industrie selon laquelle de telles plates-formes de virtualisation au niveau de l'entreprise ne conviennent pas. Cependant, ils sont bien adapt√©s au d√©veloppement multiplateforme et au d√©ploiement de stands directement sur les machines des d√©veloppeurs de logiciels, car ils sont faciles √† g√©rer et √† d√©ployer.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exemples du deuxi√®me type d'hyperviseur: VMware Workstation / Fusion, Oracle VM VirtualBox, Parallels Desktop, VMware Server (ex-GSX), Microsoft Virtual Server 2005 </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Type 1 (m√©tal nu) </font></font></h4><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les hyperviseurs du premier type ne n√©cessitent pas de syst√®me d'exploitation √† usage g√©n√©ral, contrairement aux pr√©c√©dents. L'hyperviseur lui-m√™me est un monolithe qui contr√¥le √† la fois l'allocation des ressources informatiques et les E / S. Dans l'anneau de s√©curit√© z√©ro, il y a un microc≈ìur, au-dessus duquel toutes les structures de contr√¥le fonctionnent. Dans cette architecture, l'hyperviseur contr√¥le la distribution des ressources informatiques et contr√¥le lui-m√™me tous les appels des machines virtuelles aux appareils. VMware ESX √©tait consid√©r√© comme le premier hyperviseur du premier type pour x86 depuis longtemps, bien que maintenant nous l'attribuions √† 1+. Le seul repr√©sentant ¬´honn√™te¬ª de ce type aujourd'hui est VMware ESXi - le successeur d'ESX, apr√®s avoir √©t√© mordu de la section parent avec RHEL.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Par exemple, consid√©rez l'architecture ESXi. </font><font style="vertical-align: inherit;">Les commandes de gestion d'hyperviseur sont ex√©cut√©es via l'API d'agent, qui s'ex√©cute au-dessus de VMkernel. </font><font style="vertical-align: inherit;">Cela peut sembler √™tre une connexion directe √† l'hyperviseur, mais ce n'est pas le cas. </font><font style="vertical-align: inherit;">Il n'y a pas d'acc√®s direct √† l'hyperviseur, ce qui distingue ce type d'hyperviseur du deuxi√®me type d'hyperviseur en termes de s√©curit√©. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/361/f83/77d/361f8377d29345aa2498b31b9af66030.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'inconv√©nient est ici les pilotes de p√©riph√©riques: pour assurer la ¬´finesse¬ª de la plateforme et √©liminer les complications inutiles de version en version, les pilotes de p√©riph√©riques sont tourn√©s, ce qui rend l'infrastructure physique d√©pendante de HCL (liste de compatibilit√© mat√©rielle).</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Type 1+ (hyperviseur hybride) </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les hyperviseurs de type hybride (alias 1+, 1a, 1.5) sont caract√©ris√©s par l'isolement du syst√®me d'exploitation de base dans une entit√© sp√©ciale appel√©e la partition parent (partition parent dans la terminologie Microsoft Hyper-V) ou le domaine parent (domaine dom0 dans la terminologie Xen). Ainsi, apr√®s avoir install√© le r√¥le de l'hyperviseur, le noyau passe en mode de prise en charge de la virtualisation et l'hyperviseur est responsable de l'allocation des ressources sur l'h√¥te. Mais la section parent prend en charge la fonction de traitement des appels aux pilotes de p√©riph√©rique et des op√©rations d'E / S.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En fait, la section parent devient une sorte de fournisseur entre toutes les entit√©s de la pile de virtualisation. Cette approche est pratique du point de vue de la compatibilit√© avec les √©quipements: vous n'avez pas besoin d'incorporer des pilotes de p√©riph√©riques dans l'hyperviseur, comme c'est le cas avec ESXi, ce qui signifie que la liste des p√©riph√©riques est beaucoup plus √©tendue et d√©pend moins de HCL. Les avantages incluent le d√©chargement de l'hyperviseur de la t√¢che de traitement des appels aux pilotes de p√©riph√©rique, car tous les appels sont trait√©s par la section parent. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'architecture de niveau sup√©rieur des hyperviseurs de type 1+ ressemble √† ceci:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/89c/b74/eb3/89cb74eb3f97b6a8af063fbd447b1c87.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les hyperviseurs de ce type incluent: les hyperviseurs VMware ESX, Microsoft Hyper-V, Xen d√©c√©d√©s (impl√©mentations Citrix XenServer et Xen dans diverses distributions Linux). </font><font style="vertical-align: inherit;">Rappelons que Citrix XenServer est un syst√®me d'exploitation RHEL l√©g√®rement tronqu√©, et que sa version et ses fonctionnalit√©s d√©pendent directement de la version actuelle de Red-Hat Enterprise Linux. </font><font style="vertical-align: inherit;">Dans le cas d'autres impl√©mentations Xen, la situation n'est pas tr√®s diff√©rente: il s'agit du m√™me noyau Linux en mode hyperviseur Xen et du syst√®me d'exploitation de base dans le domaine dom0. </font><font style="vertical-align: inherit;">Cela conduit √† la conclusion sans ambigu√Øt√© que les hyperviseurs √† base de Xen sont de type hybride et ne sont pas des hyperviseurs honn√™tes de type 1.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Principales technologies des plateformes industrielles </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La base sera prise par la terminologie de VMware, en tant que plate-forme de virtualisation la plus avanc√©e technologiquement. </font><font style="vertical-align: inherit;">Dans cet article, nous nous limitons aux technologies des hyperviseurs eux-m√™mes et du syst√®me de contr√¥le de base. </font><font style="vertical-align: inherit;">Toutes les fonctionnalit√©s avanc√©es mises en ≈ìuvre par des produits suppl√©mentaires pour de l'argent suppl√©mentaire seront laiss√©es en coulisses. </font><font style="vertical-align: inherit;">Les technologies sont regroup√©es en groupes conditionnels dans le but principal, comme il semble √† l'auteur, avec qui vous avez le droit d'√™tre en d√©saccord.</font></font><br><br><h3>  SLA </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Il s'agit d'un ensemble de technologies qui affectent principalement les performances des SLA pour l'accessibilit√© (RPO / RTO). </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Haute disponibilit√© - une technologie pour assurer la haute disponibilit√© des machines virtuelles dans un cluster par un hyperviseur. </font><font style="vertical-align: inherit;">En cas de d√©c√®s d'un h√¥te, la machine virtuelle red√©marre automatiquement sur les h√¥tes survivants. </font><font style="vertical-align: inherit;">Effet: minimisation du RTO avant l'expiration de HA + red√©marrage OS / services.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> FT </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fault Tolerance - technologie pour assurer un fonctionnement continu des machines virtuelles m√™me en cas de d√©c√®s de l'h√¥te. </font><font style="vertical-align: inherit;">Une machine virtuelle fant√¥me est cr√©√©e sur le deuxi√®me h√¥te, qui est compl√®tement identique √† la principale et r√©p√®te les instructions derri√®re. </font><font style="vertical-align: inherit;">Ainsi, la diff√©rence dans les √©tats de VM est mesur√©e en dizaines ou centaines de millisecondes, ce qui est tout √† fait acceptable pour de nombreux services. </font><font style="vertical-align: inherit;">Lorsque l'h√¥te meurt, l'ex√©cution passe automatiquement √† la machine virtuelle fant√¥me. </font><font style="vertical-align: inherit;">Effet: minimiser le RTO √† z√©ro.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tco </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Il s'agit d'un ensemble de technologies qui influencent principalement le TCO. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vMotion </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vMotion est une technologie de migration en direct d'un point d'ex√©cution de machine virtuelle d'un h√¥te enti√®rement fonctionnel √† un autre. </font><font style="vertical-align: inherit;">Dans le m√™me temps, le point de commutation du point d'ex√©cution est inf√©rieur aux d√©lais d'expiration de la connexion r√©seau, ce qui nous permet de consid√©rer la migration comme live, c'est-√†-dire </font><font style="vertical-align: inherit;">sans interruption dans le travail des services productifs. </font><font style="vertical-align: inherit;">Effet: r√©duction du RTO √† z√©ro pour les pannes planifi√©es pour la maintenance du serveur et, par cons√©quent, √©limination partielle des pannes elles-m√™mes.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Storage vMotion </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Storage vMotion est une technologie pour la migration en direct d'un point de stockage VM d'un stockage enti√®rement fonctionnel √† un autre. </font><font style="vertical-align: inherit;">Dans le m√™me temps, le travail avec le syst√®me de disques ne s'arr√™te pas, ce qui permet de consid√©rer la migration comme active. </font><font style="vertical-align: inherit;">Effet: r√©duction du RTO √† z√©ro pour les pannes planifi√©es pour la maintenance du stockage et, par cons√©quent, √©limination partielle des pannes elles-m√™mes.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DPM </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distributed Power Management - technologie pour contr√¥ler le niveau de charge de l'h√¥te et la mise sous / hors tension des h√¥tes √† mesure que la charge du cluster change. </font><font style="vertical-align: inherit;">N√©cessite DRS pour son fonctionnement. </font><font style="vertical-align: inherit;">Effet: r√©duction globale de la consommation d'√©nergie.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VSwitch distribu√© </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VSwitch distribu√© est une technologie de gestion centralis√©e des param√®tres r√©seau des commutateurs d'h√¥te virtuel. </font><font style="vertical-align: inherit;">Effet: r√©duction du volume et de la complexit√© du travail de reconfiguration du sous-syst√®me r√©seau, r√©duction des risques d'erreurs.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> EVC </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La compatibilit√© vMotion am√©lior√©e est une technologie qui permet de masquer les instructions de processeur disponibles pour les machines virtuelles en mode automatique. </font><font style="vertical-align: inherit;">Il est utilis√© pour aligner le travail des machines virtuelles dans un cluster in√©gal avec la plus ancienne famille de processeurs, offrant la possibilit√© de migrer des machines virtuelles vers n'importe quel h√¥te. </font><font style="vertical-align: inherit;">Effet: √©conomie sur la complexit√© de l'infrastructure tout en augmentant progressivement la capacit√© / mise √† niveau partielle des clusters.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> QoS </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Il s'agit d'un ensemble de technologies qui influencent principalement les performances du SLA en termes de qualit√© de service. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vNUMA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vNUMA est une technologie qui permet au syst√®me d'exploitation invit√© de communiquer avec la topologie NUMA virtuelle de la machine virtuelle pour les machines larges (vCPU ou vRAM&gt; n≈ìud NUMA). </font><font style="vertical-align: inherit;">Effet: L'absence de p√©nalit√© sur les performances des logiciels d'application prenant en charge NUMA.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pool de ressources </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pools de ressources - la technologie consistant √† combiner plusieurs machines virtuelles en un seul pool de ressources pour contr√¥ler la consommation ou garantir l'allocation des ressources. </font><font style="vertical-align: inherit;">Effet: simplifier l'administration, fournir un niveau de service.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Limite / r√©serve </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La limitation / redondance du processeur / m√©moire vous permet de limiter l'allocation des ressources, ou inversement, pour garantir leur allocation en situation de raret√© et de concurrence pour assurer la maintenance des VM / pools prioritaires. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DRS </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dynamic Resource Scheduler - √©quilibrage automatique des machines virtuelles par les h√¥tes en fonction de la charge pour r√©duire la fragmentation des ressources dans le cluster et fournir un niveau de service pour les machines virtuelles. </font><font style="vertical-align: inherit;">N√©cessite le support de vMotion.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Contr√¥le d'E / S de stockage </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le contr√¥le d'E / S de stockage est une technologie qui limite le ¬´voisin bruyant¬ª, une machine √† faible priorit√© avec une charge de disque √©lev√©e pour maintenir les performances d'un syst√®me de stockage co√ªteux disponibles pour des charges de travail productives. </font><font style="vertical-align: inherit;">A titre d'exemple, un syst√®me d'indexation / moteur de recherche interne et un SGBD productif.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Contr√¥le d'E / S r√©seau </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Network IO Control est une technologie pour limiter le ¬´voisin bruyant¬ª, une machine √† faible priorit√© avec une charge r√©seau √©lev√©e. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Int√©gration de stockage (VAAI, etc.) </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Deux cat√©gories de technologies entrent dans la section d'int√©gration: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> L'int√©gration du syst√®me de gestion de la virtualisation avec le syst√®me de gestion du stockage peut grandement simplifier la s√©lection et la pr√©sentation des volumes / ballons de stockage aux hyperviseurs, r√©duisant le risque d'erreurs et la complexit√© du travail. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Int√©gration au niveau du protocole - VAAI, ODX. </font><font style="vertical-align: inherit;">Ces technologies vous permettent de d√©charger le sous-syst√®me de disque, transf√©rant une partie de la charge standard √† l'√©limination du stockage intelligent. </font><font style="vertical-align: inherit;">Par exemple, cette cat√©gorie comprend des op√©rations telles que la mise √† z√©ro des blocs, le clonage de machines virtuelles, etc. </font><font style="vertical-align: inherit;">De ce fait, le canal vers le syst√®me de stockage est consid√©rablement d√©charg√© et le syst√®me de stockage lui-m√™me effectue les op√©rations sur disque de mani√®re plus optimale.</font></font></li></ul><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La s√©curit√© </font></font></h3><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Microsegmentation </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La microsegmentation d'un r√©seau virtuel en pratique est la possibilit√© de construire un pare-feu distribu√© virtuel qui contr√¥le les r√©seaux virtuels √† l'int√©rieur de l'h√¥te. </font><font style="vertical-align: inherit;">Am√©liore consid√©rablement la s√©curit√© du r√©seau virtuel.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AV sans agent </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La technologie prend en charge les antivirus sans agent. </font><font style="vertical-align: inherit;">Au lieu d'√™tre v√©rifi√© par des agents dans le syst√®me d'exploitation invit√©, le trafic des op√©rations de disque de la machine virtuelle est dirig√© par l'hyperviseur vers la machine virtuelle de service s√©lectionn√©e. </font><font style="vertical-align: inherit;">R√©duit consid√©rablement la charge sur les processeurs et le syst√®me de disques, tuant ainsi efficacement les ¬´temp√™tes antivirus¬ª.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Syst√®mes hyper converg√©s </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les syst√®mes convergents, comme leur nom l'indique, sont des syst√®mes avec une combinaison de fonctions. Et dans ce cas, nous entendons la combinaison du stockage et de l'ex√©cution de la VM. Cela semble simple, mais le marketing fait soudainement irruption. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour la premi√®re fois avec le terme syst√®mes convergents, les sp√©cialistes du marketing font irruption sur le march√©. Les syst√®mes convergents vendaient des serveurs classiques classiques + stockage + commutateurs. Un peu moins d'un num√©ro de partenaire. Ou ils ne vendaient m√™me pas, mais un document intitul√© ¬´architecture de r√©f√©rence¬ª a √©t√© produit. Nous condamnons sinc√®rement cette approche et passons √† la consid√©ration architecturale.</font></font><br><br><h3>  L'architecture </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En gardant la convergence comme principe architectural, nous obtenons une combinaison du point de stockage et du point d'ex√©cution de la VM dans un seul syst√®me. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'architecture convergente, en d'autres termes, implique l'utilisation des m√™mes services mat√©riels √† la fois pour ex√©cuter des machines virtuelles et pour les stocker sur des disques locaux. </font><font style="vertical-align: inherit;">Eh bien, puisqu'il devrait y avoir une tol√©rance aux pannes - dans une architecture convergente, il y a une couche de SDS distribu√©s.</font></font><br><br>  Nous obtenons: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le syst√®me classique - les logiciels, le stockage, la commutation et les serveurs proviennent de diff√©rents endroits, combin√©s par les mains du client / int√©grateur. </font><font style="vertical-align: inherit;">Contrats de support s√©par√©s.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Syst√®me converg√© - le tout √† partir d'une seule source, d'un seul support, d'un seul num√©ro de partenaire. </font><font style="vertical-align: inherit;">√Ä ne pas confondre avec l'auto-assemblage d'un fournisseur.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et il s'av√®re que le terme de notre architecture convergente est d√©j√† pris. </font><font style="vertical-align: inherit;">Exactement la m√™me situation qu'avec le superviseur. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Syst√®me hyperconverg√©</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Un syst√®me converg√© avec une architecture convergente. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien s√ªr, ce ne fut pas sans la seconde venue des sp√©cialistes du marketing. </font><font style="vertical-align: inherit;">Des syst√®mes convergents sont apparus dans lesquels il n'y avait pas de combinaison de stockage, mais il existe des n≈ìuds de stockage d√©di√©s sous le contr√¥le de SDS distribu√©s. </font><font style="vertical-align: inherit;">Dans le cadre des guerres de commercialisation, m√™me le terme sp√©cial HCI d√©sagr√©g√© (infrastructure hyperverg√©nique d√©sagr√©g√©e) est apparu. </font><font style="vertical-align: inherit;">En particulier, par exemple, NetApp avec un syst√®me similaire s'est d'abord battu assez intens√©ment pour le droit d'appeler son syst√®me hyper-convergent, mais s'est finalement rendu. </font><font style="vertical-align: inherit;">NetApp HCI aujourd'hui (fin 2019) - infrastructure cloud hybride.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Options d'impl√©mentation </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √âtant donn√© que les syst√®mes hyperconverg√©s fonctionnent avec la virtualisation, il existe en fait deux options et demie pour la mise en ≈ìuvre. </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Le module du noyau. </font><font style="vertical-align: inherit;">SDS fonctionne comme un monolithe au c≈ìur de l'hyperviseur, par exemple vSAN + ESXi</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1.5 Module de section parent. </font><font style="vertical-align: inherit;">SDS fonctionne comme un service dans le cadre de la section parent de l'hyperviseur, par exemple S2D + Hyper-V</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. La machine virtuelle. </font><font style="vertical-align: inherit;">SDS est impl√©ment√© en tant que machine virtuelle d√©di√©e sur chaque h√¥te. </font><font style="vertical-align: inherit;">Nutanix, Cisco Hyperflex, HPE Simplivity.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De toute √©vidence, en plus des probl√®mes abord√©s concernant l'effet de l'int√©gration sur les performances, il existe un probl√®me tr√®s important d'isolement et de prise en charge des hyperviseurs tiers. </font><font style="vertical-align: inherit;">Dans le cas 1, il est √©vident qu'il ne peut s'agir que d'un seul syst√®me du fournisseur de l'hyperviseur, tandis que 2 peut potentiellement fonctionner dans n'importe quel hyperviseur.</font></font><br><br><h2>  Conteneurs </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La virtualisation conteneuris√©e, bien que techniquement tr√®s diff√©rente de la virtualisation compl√®te, a une structure assez simple. </font><font style="vertical-align: inherit;">Comme pour le mod√®le de r√©seau OSI, la question est de niveau. </font><font style="vertical-align: inherit;">La virtualisation des conteneurs est un niveau sup√©rieur - au niveau de l'environnement d'application, et non au niveau physique. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La t√¢che principale de la virtualisation de conteneurs est de diviser le syst√®me d'exploitation en √©l√©ments ind√©pendants, √† partir desquels les applications isol√©es ne peuvent pas interf√©rer les unes avec les autres. </font><font style="vertical-align: inherit;">La virtualisation compl√®te n'est pas partag√©e par le syst√®me d'exploitation, mais par un serveur physique.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VM vs Container </font></font></h3><br>  Les avantages et les inconv√©nients des deux approches sont assez simples et directement oppos√©s. <br><br>  La virtualisation compl√®te (VM) donne une ind√©pendance totale au niveau du fer, y compris des piles de syst√®me d'exploitation, de disque et de r√©seau enti√®rement ind√©pendantes.  D'un autre c√¥t√©, chaque application, parce que nous adh√©rons au sch√©ma 1 application = 1 serveur, n√©cessite son propre syst√®me d'exploitation, son propre disque et sa propre pile r√©seau.  c'est-√†-dire  il y a une d√©pense multiple de ressources. <br><br>  Les conteneurs ont des piles de disque et de r√©seau communes avec le syst√®me d'exploitation h√¥te, et tous ensemble, ils utilisent un c≈ìur sur l'ensemble du serveur physique (enfin, ou virtuel, r√©cemment), ce qui dans son ensemble vous permet d'√©conomiser de mani√®re assez significative les ressources sur des paysages homog√®nes. <br><br>  Historiquement, x86 avait initialement des conteneurs pour tout, ainsi que des serveurs physiques.  Apr√®s l'av√®nement de la virtualisation compl√®te, l'importance des conteneurs a chut√© de pr√®s de 15 ans, et les machines virtuelles √©paisses ont r√©gn√© dans le monde de l'entreprise.  √Ä cette √©poque, les conteneurs se sont retrouv√©s chez des h√©bergeurs qui fournissaient des centaines de serveurs Web du m√™me type, o√π leur l√©g√®ret√© √©tait recherch√©e.  Mais ces derni√®res ann√©es, depuis environ 2015, les conteneurs sont revenus √† la r√©alit√© d'entreprise sous la forme d'applications natives du cloud. <br><br><h3>  Conteneurs 0,1 </h3><br><h4>  chroot </h4><br>  Le prototype de conteneurs en 1979 √©tait chroot. <br><br>  ¬´Chroot consiste √† changer le r√©pertoire racine sur les syst√®mes d'exploitation de type Unix.  Un programme lanc√© avec un r√©pertoire racine modifi√© n'aura acc√®s qu'aux fichiers contenus dans ce r√©pertoire. ¬ª <br><br>  C'est-√†-dire  en fait, l'isolement se fait uniquement au niveau du syst√®me de fichiers, sinon ce n'est qu'un processus normal dans le syst√®me d'exploitation. <br><br><h4>  La prison de Freebsd </h4><br>  La prison de BSD gratuite, apparue en 1999, √©tait beaucoup plus avanc√©e.  La prison vous a permis de cr√©er des instances de syst√®me d'exploitation virtuel √† part enti√®re avec leurs propres ensembles d'applications et de fichiers de configuration bas√©s sur FreeBSD de base.  Il y a s√ªrement ceux qui disent - et que fait la prison dans des conteneurs, parce que c'est la paravirtualisation!  Et ils auront partiellement raison. <br><br>  Cependant, avant la virtualisation compl√®te (et sa variante sous forme de paravirtualisation), la prison n'a pas la possibilit√© d'ex√©cuter le noyau d'une version diff√©rente dans la machine virtuelle invit√©e et le clustering avec la migration de la machine virtuelle vers un autre syst√®me h√¥te. <br><br><h4>  Zones Solaris </h4><br>  Solaris Zones est une technologie de virtualisation de syst√®me d'exploitation (virtualisation de conteneurs), introduite en 2004 dans Sun Solaris.  Le principe de base est une faible surcharge de virtualisation. <br><br>  Ne gagne pas beaucoup de popularit√©, migr√© vers OpenSolaris et les distributions bas√©es sur celui-ci, disponible en 2019. <br><br><h3>  Containers 1.0 </h3><br>  √Ä l'√®re des conteneurs 1.0, deux directions principales de la conteneurisation sont apparues - ce sont les produits commerciaux pour les h√©bergeurs et la conteneurisation des applications. <br><br><h4>  Virtuozzo / OpenVZ </h4><br>  Russian SWsoft a pr√©sent√© en 2001 sa premi√®re version de virtualisation de conteneurs Virtuozzo, destin√©e au march√© des h√©bergeurs.  En raison de la d√©termination et du public cible sp√©cifique, le produit s'est av√©r√© √™tre un succ√®s et a gagn√© en popularit√©.  Technologiquement, en 2002, le fonctionnement simultan√© de 2500 conteneurs sur un serveur √† 8 processeurs a √©t√© d√©montr√©. <br><br>  En 2005, une version ouverte des conteneurs Virtuozzo pour Linux appel√©e OpenVZ est apparue.  Et est presque devenu l'√©talon-or pour l'h√©bergement de VPS. <br><br><h4>  Lxc </h4><br>  LinuX Containers (LXC) est une autre virtualisation de conteneurs bien connue bas√©e sur des espaces de noms et des groupes de contr√¥le, qui est apparue en 2008. Elle sous-tend les dockers actuellement populaires, etc. <br><br><h3>  Containers 1.1 (Application Virtualization) </h3><br>  Si le reste des conteneurs est con√ßu pour diviser l'OS de base en segments, alors pourquoi ne pas arracher cette couche du syst√®me et l'emballer dans une seule bo√Æte avec l'application et avec tout son environnement.  Et puis, ce package pr√™t √† l'emploi peut √™tre lanc√© en tant qu'application de niveau utilisateur standard. <br><br><h4>  App-v </h4><br>  Microsoft Application Virtualization (App-V), anciennement Softricity SoftGrid - technologie de conteneurisation d'applications sp√©cifiques (le conteneur est l'inverse) dans un bac √† sable isol√©, puis Microsoft.  En 2006, Microsoft a acquis la startup Softricity, qui a en fait transform√© le conteneur. <br><br><h4>  Thinapp </h4><br>  VMware ThinApp (anciennement Thinstall) est un produit de conteneurisation d'applications de Jilt acquis par VMware en 2008.  VMware estime que 90 √† 95% de toutes les applications packag√©es dans le monde utilisent cette technologie. <br><br><h3>  Conteneurs 2.0 </h3><br>  L'histoire de l'√©mergence des conteneurs 2.0 est tr√®s associ√©e √† un changement dans le processus de d√©veloppement logiciel.  Le d√©sir de l'entreprise de r√©duire un param√®tre aussi important que le d√©lai de mise sur le march√© a contraint les d√©veloppeurs √† reconsid√©rer leurs approches de cr√©ation de produits logiciels.  La m√©thodologie de d√©veloppement Waterfall (longs cycles de publication, toute l'application est mise √† jour) est remplac√©e par Agile (courts cycles de publication √† dur√©e fixe, les composants de l'application sont mis √† jour ind√©pendamment) et oblige les d√©veloppeurs √† s√©parer les applications monolithiques en composants.  Alors que les composants des applications monolithiques sont encore assez volumineux et peu nombreux peuvent √™tre plac√©s dans des machines virtuelles, mais lorsqu'une application est constitu√©e de dizaines ou de centaines de composants, les machines virtuelles ne sont plus tr√®s adapt√©es.  En outre, le probl√®me des versions de logiciels auxiliaires, des biblioth√®ques et des d√©pendances se pose √©galement, il arrive souvent que diff√©rents composants n√©cessitent des versions diff√©rentes ou des variables d'environnement configur√©es diff√©remment.  Ces composants doivent √™tre distribu√©s sur diff√©rentes machines virtuelles, car  il est presque impossible d'ex√©cuter simultan√©ment plusieurs versions de logiciels dans le m√™me syst√®me d'exploitation.  Le nombre de VM commence √† cro√Ætre comme une avalanche.  Ici, des conteneurs apparaissent sur la sc√®ne, permettant dans le cadre d'un OS invit√© de cr√©er plusieurs environnements isol√©s pour lancer des composants d'application.  La conteneurisation des applications vous permet de continuer √† segmenter une application monolithique en composants encore plus petits et de passer au paradigme d'une t√¢che = un composant - un conteneur, c'est ce qu'on appelle une approche de microservice, et chacun de ces composants est un microservice. <br><br><h4>  Conteneur sous le capot </h4><br>  Si vous regardez le conteneur avec un coup d'≈ìil de l'administrateur syst√®me, ce ne sont que des processus Linux qui ont leurs propres pids, etc.  Qu'est-ce qui permet d'isoler les processus s'ex√©cutant dans des conteneurs les uns des autres et de consommer ensemble les ressources de l'OS invit√©?  Deux m√©canismes standard pr√©sents dans le noyau de toute distribution Linux moderne.  Le premier, Linux Namespaces, qui garantit que chaque processus voit sa propre repr√©sentation du syst√®me d'exploitation (syst√®me de fichiers, interfaces r√©seau, nom d'h√¥te, etc.) et le second, Linux Control Groups (cgroups), limitant le processus √† la consommation des ressources du syst√®me d'exploitation invit√© (CPU, m√©moire bande passante r√©seau, etc.). <br><br><h4>  Espaces de noms Linux </h4><br>  Par d√©faut, chaque syst√®me Linux contient un seul espace de noms.  Toutes les ressources syst√®me, telles que les syst√®mes de fichiers, les identifiants de processus (ID de processus), les identifiants d'utilisateur (ID d'utilisateur), les interfaces r√©seau appartiennent √† cet espace de noms.  Mais personne ne nous emp√™che de cr√©er des espaces de noms suppl√©mentaires et de redistribuer les ressources syst√®me entre eux. <br><br>  Lorsqu'un nouveau processus d√©marre, il d√©marre dans un espace de noms, standard du syst√®me ou l'un des cr√©√©s.  Et ce processus ne verra que les ressources disponibles dans l'espace de noms utilis√© pour l'ex√©cuter. <br><br>  Mais tout n'est pas si simple, chaque processus n'appartient pas √† un seul espace de noms, mais √† un espace de noms dans chacune des cat√©gories: <br><br><ul><li>  Mont (mnt) </li><li>  ID de processus (pid) </li><li>  R√©seau (net) </li><li>  Communication inter-processus (ipc) </li><li>  UTS </li><li>  ID utilisateur (utilisateur) </li></ul><br>  Chaque type d'espace de noms isole un groupe de ressources correspondant.  Par exemple, l'espace UTS d√©finit le nom d'h√¥te et le nom de domaine visibles par les processus.  Ainsi, deux processus au sein du syst√®me d'exploitation invit√© peuvent supposer qu'ils s'ex√©cutent sur des serveurs diff√©rents. <br><br>  L'espace de noms r√©seau d√©termine la visibilit√© des interfaces r√©seau, le processus √† l'int√©rieur ne verra que les interfaces appartenant √† cet espace de noms. <br><br><h4>  Groupes de contr√¥le Linux (cgroups) </h4><br>  Les groupes de contr√¥le Linux (cgroups) sont le m√©canisme du syst√®me du noyau (noyau) des syst√®mes Linux qui limite la consommation des ressources syst√®me par les processus.  Chaque processus ou groupe de processus ne pourra pas obtenir plus de ressources (CPU, m√©moire, bande passante r√©seau, etc.) qu'il n'est allou√© et ne pourra pas capturer les "autres" ressources - les ressources des processus voisins. <br><br><h3>  Docker </h3><br>  Comme indiqu√© ci-dessus, Docker n'a pas invent√© les conteneurs en tant que tels.  Les conteneurs existent depuis de nombreuses ann√©es (y compris ceux bas√©s sur LXC), mais Docker les a rendus tr√®s populaires en cr√©ant le premier syst√®me qui facilitait et simplifiait le transfert de conteneurs entre diff√©rentes machines.  Docker a cr√©√© un outil pour cr√©er des conteneurs - empaqueter l'application et ses d√©pendances, et ex√©cuter des conteneurs sur n'importe quel syst√®me Linux avec Docker install√©. <br><br>  Une caract√©ristique importante de Docker est la portabilit√© non seulement de l'application elle-m√™me et de ses d√©pendances entre des distributions Linux compl√®tement diff√©rentes, mais √©galement la portabilit√© de l'environnement et du syst√®me de fichiers.  Par exemple, un conteneur cr√©√© sur CentOS peut √™tre ex√©cut√© sur un syst√®me Ubuntu.  Dans ce cas, √† l'int√©rieur du conteneur lanc√©, le syst√®me de fichiers sera h√©rit√© de CentOS et l'application consid√©rera qu'il s'ex√©cute au-dessus de CentOS.  Ceci est quelque peu similaire √† une image OVF d'une machine virtuelle, mais le concept d'une image Docker utilise des couches.  Cela signifie que lors de la mise √† jour d'une partie seulement de l'image, il n'est pas n√©cessaire de t√©l√©charger √† nouveau l'image enti√®re, il suffit de t√©l√©charger uniquement la couche modifi√©e, comme si l'image OVF pouvait mettre √† jour le syst√®me d'exploitation sans mettre √† jour l'image enti√®re. <br><br>  Docker a cr√©√© un √©cosyst√®me pour cr√©er, stocker, transf√©rer et lancer des conteneurs.  Le monde Docker comprend trois composants cl√©s: <br><br><ul><li>  Images - une image, c'est l'entit√© qui contient votre application, l'environnement n√©cessaire et d'autres m√©tadonn√©es n√©cessaires pour lancer le conteneur; </li><li>  Registres - r√©f√©rentiel, lieu de stockage pour les images Docker.  Il existe une vari√©t√© de r√©f√©rentiels, allant du site officiel - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">hub.docker.com</a> et se terminant par des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©pertoires</a> priv√©s d√©ploy√©s dans l'infrastructure de l'entreprise; </li><li>  Conteneurs - un conteneur, un conteneur Linux cr√©√© √† partir d'une image Docker.  Comme mentionn√© ci-dessus, il s'agit d'un processus Linux ex√©cut√© sur un syst√®me Linux avec Docker install√©, isol√© des autres processus et du syst√®me d'exploitation lui-m√™me. </li></ul><br>  Tenez compte du cycle de vie des conteneurs.  Initialement, un d√©veloppeur cr√©e une image Docker avec son application (la commande de construction Docker), compl√®tement √† partir de z√©ro ou en utilisant des images d√©j√† cr√©√©es comme base (rappelez-vous des calques).  De plus, cette image peut √™tre lanc√©e par le d√©veloppeur directement sur sa propre machine ou peut √™tre transf√©r√©e sur une autre machine - le serveur.  Pour la portabilit√©, les r√©f√©rentiels sont souvent utilis√©s (la commande push du docker) - ils chargent l'image dans le r√©f√©rentiel.  Apr√®s cela, l'image peut √™tre t√©l√©charg√©e sur n'importe quelle autre machine ou serveur (docker pull).  Enfin, cr√©ez un conteneur de travail (docker run) √† partir de cette image. <br><br><h3>  Kubernetes </h3><br>  Comme nous l'avons d√©j√† dit, le concept de microservices signifie diviser une application monolithique en de nombreux petits services, ex√©cutant g√©n√©ralement une seule fonction.  Eh bien, lorsqu'il existe des dizaines de ces services, ils peuvent toujours √™tre g√©r√©s manuellement via, par exemple, Docker.  Mais que faire quand il existe des centaines et des milliers de tels services?  En plus de l'environnement industriel, vous avez besoin d'un environnement de test et d'environnements suppl√©mentaires pour diff√©rentes versions du produit, c'est-√†-dire  multipliez par 2, par 3 ou m√™me plus.  Google a √©galement √©t√© confront√© aux m√™mes probl√®mes, ses ing√©nieurs ont √©t√© l'un des premiers √† utiliser des conteneurs √† l'√©chelle industrielle.  C'est ainsi que Kubernetes (K8s) est n√©, cr√©√© sous le nom de Borg dans les murs du produit Google, plus tard donn√© au grand public et renomm√©. <br><br>  K8s est un syst√®me qui facilite le d√©ploiement, la gestion et la surveillance des applications conteneuris√©es (microservices).  Comme nous le savons d√©j√†, n'importe quelle machine Linux convient pour lancer des conteneurs et les conteneurs sont isol√©s les uns des autres, respectivement, et les K8 peuvent g√©rer diff√©rents serveurs avec un mat√©riel diff√©rent et sous le contr√¥le de diff√©rentes distributions Linux.  Tout cela nous aide √† utiliser efficacement le mat√©riel disponible.  Comme la virtualisation, K8s nous fournit un pool commun de ressources pour le lancement, la gestion et la surveillance de nos microservices. <br><br>  √âtant donn√© que cet article est principalement destin√© aux ing√©nieurs de virtualisation, pour une compr√©hension g√©n√©rale des principes de fonctionnement et des principaux composants des K8, nous vous recommandons de lire l'article qui √©tablit le parall√®le entre K8 et VMware vSphere: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://medium.com/@pryalukhin/kubernetes-introduction-for-vmware- users-232cc2f69c58</a> <br><br><h2>  Historique de la virtualisation industrielle X86 </h2><br><h3>  VMware </h3><br>  VMware est apparu en 1998, en commen√ßant par le d√©veloppement d'un deuxi√®me type d'hyperviseur, qui est devenu plus tard VMware Workstation. <br><br>  La soci√©t√© est entr√©e sur le march√© des serveurs en 2001 avec deux hyperviseurs - GSX (Ground Storm X, deuxi√®me type) et ESX (Elastic Sky X, premier type).  Au fil du temps, les perspectives du deuxi√®me type dans les applications serveur sont devenues √©videntes, √† savoir  Aucun.  Et le GSX payant a d'abord √©t√© transform√© en serveur VMware gratuit, puis compl√®tement arr√™t√© et enterr√©. <br><br>  En 2003, le syst√®me de gestion central de Virtual Center, la technologie vSMP et la migration en direct des machines virtuelles sont apparus. <br><br>  En 2004, VMware a √©t√© acquis par EMC, un g√©ant du stockage, mais est rest√© op√©rationnel ind√©pendant. <br><br>  En 2008, devenant la norme de facto de l'industrie, VMware a stimul√© la croissance rapide des offres concurrentielles - Citrix, Microsoft, etc. Il devient clair qu'il √©tait n√©cessaire d'obtenir une version gratuite de l'hyperviseur, ce qui √©tait impossible - car une section parent d'ESX utilisait un RHEL enti√®rement commercial.  Le projet de remplacement de RHEL par quelque chose de plus simple et gratuit a √©t√© mis en ≈ìuvre en 2008 avec le syst√®me busybox.  Le r√©sultat est ESXi, connu de tous aujourd'hui. <br><br>  En parall√®le, l'entreprise se d√©veloppe √† travers des projets internes et des acquisitions de startups.  Il y a quelques ann√©es, la liste des produits VMware occupait quelques pages A4, alors disons simplement.  VMware pour 2019 est toujours la norme de facto sur le march√© de la virtualisation compl√®te d'entreprise sur site avec une part de march√© de plus de 70% et un leader technologique absolu, et un examen d√©taill√© de l'histoire m√©rite un article tr√®s volumineux. <br><br><h3>  Connectix </h3><br>  Fond√©e en 1988, Connectix a travaill√© sur une vari√©t√© d'utilitaires syst√®me jusqu'√† la virtualisation.  En 1997, le premier produit VirtualPC pour Apple Macintosh a √©t√© cr√©√©, permettant √† Windows de s'ex√©cuter sur une machine virtuelle.  La premi√®re version de VirtualPC pour Windows est apparue en 2001. <br><br>  En 2003, Microsoft a achet√© VirtualPC et, en accord avec Connectix, les d√©veloppeurs sont pass√©s √† Microsoft.  Apr√®s cela, Connectix a ferm√©. <br><br>  Le format VHD (disque dur virtuel) a √©t√© d√©velopp√© par Connectix pour VirtualPC, et pour rappel, les disques virtuels des machines Hyper-V contiennent ¬´conectix¬ª dans leur signature. <br>  VIrtual PC, comme vous pouvez le deviner, est un hyperviseur de bureau classique du deuxi√®me type. <br><br><h3>  Microsoft </h3><br>  L'aventure de Microsoft dans la virtualisation industrielle a commenc√© avec l'achat de Connectix et le changement de marque de Connectix Virtual PC dans Microsoft Virtual PC 2004. Virtual PC d√©velopp√© pendant un certain temps, a √©t√© inclus sous le nom Windows Virtual PC dans Windows 7. Dans Windows 8 et versions ult√©rieures, Virtual PC a √©t√© remplac√© par version de bureau d'Hyper-V. <br><br>  Bas√© sur Virtual PC, l'hyperviseur du serveur Virtual Server a √©t√© cr√©√©, qui existait jusqu'au d√©but de 2008.  En raison de la perte technologique √©vidente avant VMware ESX, il a √©t√© d√©cid√© de limiter le d√©veloppement du deuxi√®me type d'hyperviseur au profit de son propre premier type d'hyperviseur, qui est devenu Hyper-V.  Il existe une opinion non officielle dans l'industrie selon laquelle Hyper-V est √©tonnamment similaire √† Xen en architecture.  Identique √† .Net en Java. <br><blockquote>  "Bien s√ªr, vous pourriez penser que Microsoft a vol√© l'id√©e de Java."  Mais ce n'est pas vrai, Microsoft l'a inspir√©e!  - (extrait d'un discours prononc√© par un repr√©sentant de Microsoft lors de la pr√©sentation de Windows 2003 Server) </blockquote><br>  √Ä partir des moments curieux, on peut noter qu'√† l'int√©rieur de Microsoft, l'utilisation de produits de virtualisation propri√©taires au cours des ann√©es z√©ro √©tait, pour le moins, facultative.  Il y a des captures d'√©cran de Technet √† partir d'articles sur la virtualisation, o√π le logo VMware Tools est clairement pr√©sent dans la barre d'√©tat.  En outre, Mark Russinovich sur la plateforme 2009 √† Moscou a effectu√© une d√©monstration avec VMware Workstation. <br><br>  Afin de p√©n√©trer de nouveaux march√©s, Microsoft a cr√©√© son propre cloud public, Azure, en utilisant un serveur Nano hautement modifi√© avec prise en charge Hyper-V, S2D et SDN comme plate-forme.  Il convient de noter qu'au d√©part, Azure √©tait √† certains moments loin derri√®re les syst√®mes sur site.  Par exemple, la prise en charge des machines virtuelles de deuxi√®me g√©n√©ration (avec prise en charge du d√©marrage s√©curis√©, du d√©marrage √† partir des partitions GPT, du d√©marrage PXE, etc.) n'est apparue dans Azure qu'en 2018.  En local, les machines virtuelles de deuxi√®me g√©n√©ration sont connues depuis Windows Server 2012R2.  Il en va de m√™me pour les solutions de portail: jusqu'en 2017, Azure et le Windows Azure Pack (solution cloud multi-tenancy avec prise en charge SDN et VM blind√©e, qui a remplac√© System Center App Controller en 2013) ont utilis√© la m√™me conception de portail.  Apr√®s que Microsoft a annonc√© un cours sur les clouds publics, Azure s'est avanc√© pour d√©velopper et impl√©menter divers savoir-faire.  Autour de l'ann√©e 2016, vous pouvez observer une image tout √† fait logique: maintenant toutes les innovations de Windows Server viennent d'Azure, mais pas dans la direction oppos√©e.  Le fait de copier des parties de la documentation d'Azure vers sur site "en l'√©tat" l'indique (voir la documentation sur Azure SDN et le contr√¥leur de r√©seau), qui d'une part fait allusion √† l'attitude vis-√†-vis des solutions sur site, et d'autre part, indique la relation des solutions en termes d'entit√©s et d'architecture.  Qui a copi√© de qui et comment c'est vraiment - une question discutable. <br><br>  En mars 2018, Satya Nadela (PDG de Microsoft) a officiellement annonc√© que le cloud public devenait la priorit√© de l'entreprise.  Ce qui, √©videmment, symbolise le pliage et la d√©coloration graduels de la gamme de serveurs pour les produits sur site (cependant, une stagnation a √©t√© observ√©e en 2016, mais a √©t√© confirm√©e avec la premi√®re version b√™ta de Windows Server et d'autres gammes de produits sur site), √† ‚Äã‚Äãl'exception d'Azure Edge - le serveur minimum requis Infrastructure dans le bureau du client pour des services qui ne peuvent pas √™tre transf√©r√©s vers le cloud. <br><br><h3>  Fer virtuel </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fond√© en 2003, Virtual Iron a offert une version commerciale de Xen et a √©t√© l'un des premiers √† offrir au march√© une prise en charge compl√®te de la virtualisation mat√©rielle. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2009, Oracle a √©t√© repris pour d√©velopper sa propre ligne de virtualisation Oracle VM et l'√©tendre sur x86. </font><font style="vertical-align: inherit;">Auparavant, Oracle VM n'√©tait propos√© que sur la plate-forme SPARC.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Innotek </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">D√©but 2007, Innotek GmbH a commercialis√© l'hyperviseur de bureau propri√©taire de deuxi√®me type, VirtualBox, qui est gratuit pour une utilisation non commerciale. La m√™me ann√©e, une version open source est sortie. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2008, il a √©t√© acquis par Sun, qui √† son tour a √©t√© acquis par Oracle. Oracle a maintenu l'utilisation gratuite du produit √† des fins non commerciales. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VirtualBox prend en charge trois formats de disques virtuels - VDI (natif), VMDK (VMware), VHD (Microsoft). En tant qu'OS h√¥te, Windows, macOS, Linux, Solaris et OpenSolaris sont pris en charge. Le fork de VirtualBox pour FreeBSD est connu.</font></font><br><br><h3>  Ibm </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le mainframe est l'ordinateur principal du centre de donn√©es avec une grande quantit√© de m√©moire interne et externe (pour r√©f√©rence: dans les ann√©es 60, 1 Mo de m√©moire √©tait consid√©r√© comme irr√©aliste). En fait, l'ordinateur central √©tait un centre de calcul: les premiers ordinateurs occupaient des salles de machines enti√®res et consistaient en d'√©normes racks. De nos jours, cela s'appelle les centres de donn√©es. Mais dans les centres de donn√©es d'une m√™me salle des machines, il peut y avoir des milliers d'ordinateurs et, √† l'aube de la technologie informatique, un ordinateur occupait une salle enti√®re. Chaque rack a vendu un (!) P√©riph√©rique informatique (racks s√©par√©s avec m√©moire, racks s√©par√©s avec p√©riph√©riques de stockage et p√©riph√©riques s√©par√©ment). Le c≈ìur de cette √©norme machine √©tait un rack avec un processeur - on l'appelait le principal, ou mainframe.Apr√®s le passage aux circuits int√©gr√©s √† transistors, la taille de ce miracle de la pens√©e scientifique et technique a consid√©rablement diminu√©, et le mainframe d'IBM et de leurs analogues a commenc√© √† √™tre compris comme le mainframe.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans les ann√©es 60 du XXe si√®cle, la location de la puissance de calcul de l'ensemble du mainframe, sans parler de son achat, a co√ªt√© cher. Tr√®s peu d'entreprises et d'institutions pouvaient se permettre un tel luxe. La location de la puissance de calcul √©tait horaire (le prototype du mod√®le moderne Pay as you go dans les clouds publics, n'est-ce pas?). L'acc√®s aux locataires pour l'informatique a √©t√© accord√© s√©quentiellement. La solution logique √©tait de parall√©liser la charge de calcul et d‚Äôisoler les calculs des locataires les uns des autres.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour la premi√®re fois, l'id√©e d'isoler plusieurs instances de syst√®mes d'exploitation sur un seul ordinateur central a √©t√© propos√©e par IBM Cambridge Science Center sur la base de l'ordinateur central IBM System / 360-67. Le d√©veloppement a √©t√© appel√© CP / CMS et, en fait, a √©t√© le premier hyperviseur et a fourni la paravirtualisation. CP (Control Program) - l'hyperviseur lui-m√™me, qui a cr√©√© plusieurs "machines virtuelles" (VM) ind√©pendantes. CMS (√† l'origine le Cambridge Monitor System, renomm√© plus tard Conversational Monitor System) √©tait un syst√®me d'exploitation l√©ger √† utilisateur unique. Curieusement, CMS est toujours en vie et est toujours utilis√© dans la derni√®re g√©n√©ration de mainframes z / VM. Il convient de noter qu'√† cette √©poque et jusqu'aux ann√©es 90, une machine virtuelle signifiait une s√©paration logique des disques physiques (les disques ou les p√©riph√©riques de stockage √©taient partag√©s,l'hyperviseur n'a pas fourni de stockage pour ses propres besoins) avec un morceau d√©di√© de m√©moire virtuelle et de temps processeur utilisant la technologie Time-Sharing. Les machines virtuelles ne pr√©voyaient pas d'interaction avec le r√©seau, car les machines virtuelles de l'√©poque concernaient le calcul et le stockage de donn√©es, et non leur transfert. En ce sens, les VM de cette √©poque ressemblaient plus √† des conteneurs que des VM au sens moderne.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le premier hyperviseur commercial bas√© sur CP / CMS, appel√© VM / 370, est apparu sur les ordinateurs centraux de la s√©rie System / 370 le 2 ao√ªt 1972. Le nom g√©n√©ral de cette famille de syst√®mes d'exploitation est VM, et dans le cadre de cette section, VM sera consid√©r√©e comme l'hyperviseur IBM. La possibilit√© d'ex√©cuter plusieurs syst√®mes d'exploitation en m√™me temps, garantissant la stabilit√© du syst√®me et isolant les utilisateurs les uns des autres (une erreur dans le syst√®me d'exploitation d'un utilisateur ne pouvait pas affecter les calculs d'un autre utilisateur) - √©tait r√©volutionnaire et est devenue un facteur cl√© du succ√®s commercial de VM / 370. Un fait curieux: √† cette √©poque en URSS, les efforts de l'Institut de recherche scientifique en informatique (Minsk) ont tr√®s bien clon√© l'architecture System / 370 et cr√©√© sa propre VM / 370 analogique sous le nom de l'ordinateur de l'UE (avec le soutien de la virtualisation int√©gr√©e! - pour la possibilit√© de d√©velopper le syst√®me d'exploitation le plus √©l√©mentaire).Ces unit√©s centrales √©taient utilis√©es par les instituts de recherche et les entreprises de d√©fense du camp socialiste.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les ann√©es 80 peuvent √™tre appel√©es en toute s√©curit√© ¬´l'√®re du mainframe¬ª. VM a √©t√© un succ√®s aupr√®s des d√©veloppeurs de syst√®mes d'exploitation, des applications ont √©t√© √©crites pour cela et des calculs ont √©t√© effectu√©s. C'√©tait la d√©cennie o√π la part des bases de donn√©es domin√©es par VM OS a commenc√© √† pr√©valoir dans les mainframes. L'un des changements les plus importants a √©t√© les ressources d'acc√®s aux partitions logiques (LPAR), qui offraient en fait deux niveaux de virtualisation. Les clients peuvent d√©sormais utiliser le m√™me ensemble de processeurs, de p√©riph√©riques d'E / S et de modems dans les syst√®mes de VM s'ex√©cutant dans diff√©rentes partitions logiques et permettant la migration des ressources d'un syst√®me de VM vers un autre. Cela a permis aux organisations informatiques de fournir des performances coh√©rentes tout en traitant les pics de charge de travail. Pour rationaliser la client√®le croissante, VM a √©t√© divis√©e en trois produits distincts,disponible √† la fin des ann√©es 80:</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VM / SP - le syst√®me d'exploitation de virtualisation polyvalent habituel pour les serveurs System z </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HPO (option haute performance) - VM / SP haute performance pour les anciens mod√®les de serveurs System z </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VM / XA (architecture √©tendue) - Variante VM avec prise en charge de l'architecture S / S √©tendue 370</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Au d√©but des ann√©es 90, la simplicit√© et la commodit√© de l'architecture x86 sont devenues plus attrayantes pour les clients, et les mainframes ont rapidement perdu de leur pertinence. </font><font style="vertical-align: inherit;">Les mainframes ont √©t√© remplac√©s par des syst√®mes de cluster, comme le grunge, qui a remplac√© le glam metal en m√™me temps. </font><font style="vertical-align: inherit;">Cependant, pour une certaine classe de t√¢ches, par exemple, lors de la construction d'un entrep√¥t de donn√©es centralis√©, les mainframes se justifient √† la fois en termes de productivit√© et d'un point de vue √©conomique. </font><font style="vertical-align: inherit;">Par cons√©quent, certaines entreprises utilisent encore des mainframes dans leurs infrastructures et IBM con√ßoit, publie et prend en charge les nouvelles g√©n√©rations.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Linux Xen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Xen (prononc√© zen) est un hyperviseur d√©velopp√© √† l'Universit√© de Cambridge Computer Lab sous la direction d'Ian Pratt et distribu√© sous GPL. </font><font style="vertical-align: inherit;">La premi√®re version publique est apparue en 2003. </font><font style="vertical-align: inherit;">Par la suite, Ian a continu√© √† travailler sur l'hyperviseur dans sa version commerciale, cr√©ant la soci√©t√© XenSource. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2013, Xen est pass√© sous le contr√¥le de la Linux Foundation.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> XenSource </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Existant depuis plusieurs ann√©es sur le march√© avec les produits XenServer et XenEnterprise, il a √©t√© rachet√© fin 2007 par Citrix. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Citrix XenServer </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ayant absorb√© XenSource pour 500 millions de dollars, Citrix n'a pas pu commercialiser le probl√®me. </font><font style="vertical-align: inherit;">Plus pr√©cis√©ment, je n'ai pas vraiment essay√© de le faire, ne consid√©rant pas XenServer comme le produit principal et comptant sur le bon march√© des licences permanentes. </font><font style="vertical-align: inherit;">Apr√®s des ventes franchement infructueuses au milieu du tr√®s r√©ussi VMware ESX, il a √©t√© d√©cid√© de lancer XenServer dans le monde gratuitement et en open source en 2009. </font><font style="vertical-align: inherit;">Cependant, le code du syst√®me de gestion propri√©taire XenCenter ne s'est pas ouvert. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il convient de noter une co√Øncidence chronologique int√©ressante des initiatives Citrix et Microsoft dans le domaine de la virtualisation industrielle, malgr√© le fait que les entreprises √©taient toujours tr√®s proches. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Malgr√© leur nom marketing commun, Citrix XenApp et XenDesktop n'ont rien √† voir avec l'hyperviseur Xen.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Amazon </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Amazon a pr√©sent√© son offre publique de cloud IaaS appel√©e EC2 (Elastic Compute) en 2006. </font><font style="vertical-align: inherit;">Initialement, la plate-forme EC2 a utilis√© l'hyperviseur Xen, puis Amazon a divis√© la plate-forme en trois parties, chacune utilisant une branche et une version distinctes de l'hyperviseur pour minimiser l'effet des erreurs du code sur la disponibilit√© du service. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2017, KVM pour les charges lourdes est apparu comme un hyperviseur suppl√©mentaire dans EC2. </font><font style="vertical-align: inherit;">Selon certains, cela indique le transfert progressif d'EC2 vers KVM enti√®rement √† l'avenir.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Linux QEMU / KVM </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">QEMU (Quick EMUlator) est un logiciel universel pour √©muler le mat√©riel de diverses plates-formes, distribu√© sous licence GPL v2. </font><font style="vertical-align: inherit;">En plus de x86, ARM, MIPS, RISC-V, PowerPC, SPARC, SPARC64 sont √©galement pris en charge. </font><font style="vertical-align: inherit;">Avec la polyvalence d'une plate-forme avec virtualisation compl√®te, QEMU manquait de performances comparables √† un syst√®me non virtualis√©. </font><font style="vertical-align: inherit;">Pour acc√©l√©rer le travail de QEMU sur x86, deux options principales ont √©t√© propos√©es, qui ont finalement √©t√© rejet√©es en faveur du d√©veloppement KVM (Kernel-based Virtual Machine) de Qumranet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous disons KVM - nous voulons dire QEMU KVM, et en cons√©quence nous obtenons le format de disque virtuel qcow2 (QEMU copy-on-write 2) pour toutes les plates-formes bas√©es sur l'hyperviseur KVM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien que QEMU fonctionne initialement comme un deuxi√®me type d'hyperviseur, QEMU / KVM est un premier type d'hyperviseur.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Qumranet </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Une soci√©t√© isra√©lienne, ancien d√©veloppeur et sponsor principal de l'hyperviseur KVM et du protocole SPICE. Fond√©e en 2005, a acquis une renomm√©e apr√®s avoir incorpor√© KVM dans le noyau Linux. 4 septembre 2008, acquis par Red Hat.</font></font><br><br><h3>  Chapeau rouge </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme tous les fabricants de distribution GNU / Linux, jusqu'en 2010, Red Hat avait une prise en charge int√©gr√©e de l'hyperviseur Xen dans leurs distributions. </font><font style="vertical-align: inherit;">Cependant, √©tant un acteur majeur du march√© et une marque s√©rieuse, j'ai pens√© √† ma propre impl√©mentation de l'hyperviseur. </font><font style="vertical-align: inherit;">L'hyperviseur KVM, peu remarquable mais prometteur, a ensuite pris la base. </font><font style="vertical-align: inherit;">La premi√®re version de Red Hat Enterprise Virtualization 2.2 (RHEV) a √©t√© introduite en 2010 avec la pr√©tention de concurrencer une partie du march√© des solutions VDI avec Citrix et VMware en raison du d√©veloppement de Qumranet, acquis deux ans plus t√¥t. </font><font style="vertical-align: inherit;">Des clusters haute disponibilit√©, Live Migration et des outils de migration M2M (RHEL uniquement) √©taient disponibles. </font><font style="vertical-align: inherit;">Il est √† noter que, √† en juger par la documentation de l'√©poque, Red Hat a conserv√© la notation Xen lors de la description de l'architecture de la solution.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Le 28 octobre 2018, IBM a annonc√© l'achat de Red Hat. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Openstack </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Historiquement, le projet OpenStack a √©merg√© comme une initiative pour contraster quelque chose avec le monopole r√©el de VMware dans le domaine de la virtualisation de serveurs lourds x86. </font><font style="vertical-align: inherit;">Le projet est apparu en 2010 gr√¢ce aux efforts conjoints de Rackspace Hosting (un fournisseur de cloud) et de la NASA (qui a ouvert le code de sa propre plate-forme Nebula). </font><font style="vertical-align: inherit;">Le piquant de la situation a √©t√© donn√© par le fait qu'en 2012, VMware a rejoint la direction du projet OpenStack et a provoqu√© une vague d'indignation parmi les militants fondateurs. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Au fil du temps, Canonical (Ubuntu Linux), Debian, SUSE, Red Hat, HP, Oracle ont rejoint le projet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, tout n'a pas √©t√© fluide. </font><font style="vertical-align: inherit;">En 2012, la NASA a quitt√© le projet, optant pour AWS. </font><font style="vertical-align: inherit;">D√©but 2016, HPE a compl√®tement cl√¥tur√© son projet Helion bas√© sur OpenStack.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans le cadre du projet OpenStack, KVM a √©t√© adopt√© comme hyperviseur standard. Cependant, en raison de la modularit√© de l'approche, un syst√®me bas√© sur OpenStack peut √™tre impl√©ment√© en utilisant d'autres hyperviseurs, ne laissant, par exemple, qu'un syst√®me de contr√¥le d'OpenStack. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il existe un large √©ventail d'opinions concernant le projet OpenStack, du culte enthousiaste au scepticisme s√©rieux et aux critiques s√©v√®res. La critique n'est pas sans raison - un nombre important de probl√®mes et de pertes de donn√©es ont √©t√© enregistr√©s lors de l'utilisation d'OpenStack. Ce qui n'emp√™che cependant pas les fans de tout nier et de se r√©f√©rer √† la courbure dans la mise en ≈ìuvre et le fonctionnement des syst√®mes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le projet OpenStack ne se limite pas uniquement √† la virtualisation, mais avec le temps, il est devenu un nombre important de divers sous-projets et composants √† √©tendre dans le domaine de la pile de services de cloud public. </font><font style="vertical-align: inherit;">De plus, l'importance d'OpenStack devrait probablement √™tre √©valu√©e pr√©cis√©ment dans cette partie - ces composants sont devenus essentiels dans de nombreux produits et syst√®mes commerciaux, dans le domaine de la virtualisation et au-del√†. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En Russie, OpenStack en dehors des clouds publics est largement connu principalement pour son r√¥le dans la substitution des importations. </font><font style="vertical-align: inherit;">La grande majorit√© des solutions et produits de virtualisation, y compris les syst√®mes hyperconverg√©s, sont conditionn√©s par OpenStack avec diff√©rents degr√©s de raffinement.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nutanix AHV </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depuis sa cr√©ation, Nutanix est un produit et une plate-forme exclusivement pour VMware vSphere. </font><font style="vertical-align: inherit;">Cependant, en partie en raison de la volont√© d'√©largir l'offre pour d'autres hyperviseurs, en partie en raison de la crise politique dans les relations avec VMware, il a √©t√© d√©cid√© de d√©velopper leur propre hyperviseur, qui compl√©terait la plateforme en bo√Æte et permettrait d'abandonner les produits tiers. </font><font style="vertical-align: inherit;">KVM a √©t√© choisi comme son propre hyperviseur, qui dans le cadre de la plate-forme s'appelait AHV (Acropolis HyperVisor).</font></font><br><br><h4>  Parallels </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dans la version 7 de Virtuozzo, la soci√©t√© est pass√©e de son propre hyperviseur √† KVM. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Proxmox </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Proxmox VE (Virtual Environment) est un projet open source de la soci√©t√© autrichienne Proxmox Server Solutions GmbH bas√© sur Debian Linux. </font><font style="vertical-align: inherit;">La premi√®re version date de 2008. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le produit prend en charge la virtualisation de conteneurs LXC (anciennement OpenVZ) et la virtualisation compl√®te avec l'hyperviseur KVM.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Parallels / Virtuozzo / Rosplatform </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fond√© en 1999 par Sergey Belousov, SWsoft a adopt√© un logiciel de gestion d'h√©bergement. En 2003, la soci√©t√© rivale Plosk de Novossibirsk a √©t√© acquise. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2004, SWsoft a acquis la soci√©t√© russe Parallels Nikolai Dobrovolsky avec son produit Parallels Workstation (hyperviseur de bureau du deuxi√®me type sous Windows). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La nouvelle soci√©t√© conserve son nom Parallels et va bient√¥t faire exploser le march√© avec Parallels Desktop pour Mac (hyperviseur de bureau du deuxi√®me type pour MacOS).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans le cadre de la virtualisation des serveurs, l'accent continue d'√™tre mis sur les fournisseurs d'h√©bergement et les centres de donn√©es, plut√¥t que sur l'utilisation en entreprise. En raison des sp√©cificit√©s de ce march√© particulier, les conteneurs Virtuozzo et OpenVZ, plut√¥t que les machines virtuelles syst√®me, sont devenus le produit cl√©. Par la suite, Parallels, sans grand succ√®s, tente de p√©n√©trer le march√© de la virtualisation des serveurs d'entreprise avec le produit Parallels Bare Metal Server (par la suite Parallels Hypervisor et Cloud Server, puis Virtuozzo), ajoute une hyper-convergence avec son Cloud Storage. Les travaux se poursuivent sur l'automatisation et l'orchestration des h√©bergeurs.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2015, sur la base des produits de virtualisation de serveurs, le projet de plateforme Rosplatform est cr√©√© - techniquement (en omettant les probl√®mes juridiques et organisationnels) le m√™me Virtuozzo, uniquement avec des fonds d'√©cran modifi√©s et dans le registre russe des logiciels. </font><font style="vertical-align: inherit;">Bas√© sur le logiciel de la plateforme Rosplatform et les √©quipements Depo, IBS cr√©e une offre hyperconverg√©e de package Scala-R. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Avant la version 7, Virtuozzo utilisait un hyperviseur de sa propre conception; dans la version 7, une transition vers KVM a √©t√© effectu√©e. </font><font style="vertical-align: inherit;">En cons√©quence, Rosplatform est √©galement bas√© sur KVM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s plusieurs fusions, acquisitions et rebrandings, l'image suivante se forme d'ici 2019. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parallels Desktop est une filiale de Parallels et vendue √† Corel. </font><font style="vertical-align: inherit;">Toute l'automatisation a √©t√© confi√©e √† Odin et revendue √† IngramMicro. </font><font style="vertical-align: inherit;">La virtualisation des serveurs est rest√©e sous la marque de plateforme Virtuozzo / Rosplatform.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr474776/">https://habr.com/ru/post/fr474776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr474760/index.html">Nous fixons ngx-translate dans une application angulaire. Proc√©dure pratique</a></li>
<li><a href="../fr474762/index.html">S√©minaire: Solutions informatiques hybrides pour les entreprises. 14 novembre, Moscou</a></li>
<li><a href="../fr474768/index.html">Diffusion ouverte du Main Hall HighLoad ++ 2019</a></li>
<li><a href="../fr474770/index.html">Comment nous effectuons les tests de r√©gression de la paie dans SAP HCM</a></li>
<li><a href="../fr474772/index.html">Une startup qui a utilis√© l'IA pour d√©velopper un rem√®de en 21 jours</a></li>
<li><a href="../fr474782/index.html">Pr√©sentation de la technologie de synth√®se vocale</a></li>
<li><a href="../fr474784/index.html">Arcade Stick Story</a></li>
<li><a href="../fr474788/index.html">Organisation d'itin√©raires √† Laravel</a></li>
<li><a href="../fr474790/index.html">Contes de n√©gociateur</a></li>
<li><a href="../fr474792/index.html">6-8 d√©cembre - Rosbank Tech.Madness Hackathon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>