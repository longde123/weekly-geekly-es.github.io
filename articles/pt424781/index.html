<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçü§ù‚Äçüë®üèº üôÜüèΩ üôéüèΩ Ensinando e testando redes neurais no PyTorch usando o Ignite üßò üë≤üèæ üëÇüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√°, Habr, neste artigo, falarei sobre a biblioteca de igni√ß√£o , com a qual voc√™ pode treinar e testar facilmente redes neurais usando a estrutura PyT...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ensinando e testando redes neurais no PyTorch usando o Ignite</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/424781/"><p>  <em>Ol√°, Habr, neste artigo, falarei sobre a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> , com a qual voc√™ pode treinar e testar facilmente redes neurais usando a estrutura PyTorch.</em> </p><br><p> Com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite,</a> voc√™ pode escrever ciclos para treinar a rede em apenas algumas linhas, adicionar c√°lculos de m√©tricas padr√£o da caixa, salvar o modelo etc.  Bem, para aqueles que mudaram do TF para o PyTorch, podemos dizer que a biblioteca de <em>igni√ß√£o</em> √© o Keras for PyTorch. </p><br><p>  O artigo <em>examinar√°</em> em detalhes um exemplo de treinamento de uma rede neural para uma tarefa de classifica√ß√£o usando <em>igni√ß√£o.</em> </p><br><p><img src="https://habrastorage.org/webt/35/ar/af/35arafc8y9aicrbpz5unazs-y-a.png"></p><a name="habracut"></a><br><h2 id="dobavim-esche-bolshe-ognya-v-pytorch">  Adicione mais fogo ao PyTorch </h2><br><p>  N√£o vou perder tempo falando sobre o qu√£o <em>legal √© a</em> estrutura <em>do</em> PyTorch.  Qualquer um que j√° o tenha usado entende o que estou escrevendo.  Mas, com todas as suas vantagens, ainda √© de baixo n√≠vel em termos de ciclos de escrita para treinamento, teste e teste de redes neurais. </p><br><p>  Se olharmos para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">exemplos oficiais de</a> uso da estrutura PyTorch, veremos pelo menos dois ciclos de itera√ß√µes por √©poca e por lotes do treinamento definido no c√≥digo de treinamento da grade: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, epochs + <span class="hljs-number"><span class="hljs-number">1</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): <span class="hljs-comment"><span class="hljs-comment"># ...</span></span></code> </pre> <br><p>  A id√©ia principal da biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> √© fatorar esses loops em uma √∫nica classe, enquanto permite ao usu√°rio interagir com esses loops usando manipuladores de eventos. </p><br><p>  Como resultado, no caso de tarefas padr√£o de aprendizado profundo, podemos economizar muito no n√∫mero de linhas de c√≥digo.  Menos linhas - menos erros! </p><br><p>  Por exemplo, para compara√ß√£o, √† esquerda est√° o c√≥digo para treinamento e valida√ß√£o de modelo usando <em>ignite</em> , e √† direita est√° o PyTorch puro: <br><img src="https://habrastorage.org/getpro/habr/post_images/914/408/b07/914408b07fa093c696e66cb15ae36bfc.png" alt="imagem"></p><br><p>  Ent√£o, novamente, para que serve a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> ? </p><br><ul><li>  n√£o √© mais necess√°rio gravar em cada loop de tarefa <code>for epoch in range(n_epochs)</code> e o <code>for batch in data_loader</code> . </li><li>  permite que voc√™ fatore melhor o c√≥digo </li><li>  permite calcular m√©tricas b√°sicas prontas para uso </li><li>  fornece "p√£ezinhos" do tipo <br><ul><li>  salvar os melhores e mais recentes modelos (tamb√©m otimizador e programador de taxas de aprendizado) durante o treinamento, </li><li>  parar de aprender cedo </li><li>  etc. </li></ul></li><li>  integra-se facilmente √†s ferramentas de visualiza√ß√£o: tensorboardX, visdom, ... </li></ul><br><p>  De certa forma, como j√° mencionado, a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> pode ser comparada com todas as famosas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Keras</a> e sua API para treinamento e teste de redes.  Al√©m disso, √† primeira vista, a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> √© muito semelhante √† biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tnt</a> , pois inicialmente as duas bibliotecas tinham objetivos comuns e id√©ias semelhantes para sua implementa√ß√£o. </p><br><p>  Ent√£o, acenda: </p><br><pre> <code class="hljs sql">pip <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> pytorch-ignite</code> </pre> <br><p>  ou </p><br><pre> <code class="hljs swift">conda install ignite -<span class="hljs-built_in"><span class="hljs-built_in">c</span></span> pytorch</code> </pre> <br><p>  A seguir, com um exemplo concreto, vamos nos familiarizar com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">API da</a> biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> . </p><br><h2 id="zadacha-klassifikacii-s-ignitehttpspytorchorgignite">  Tarefa de classifica√ß√£o com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> </h2><br><p>  Nesta parte do artigo, consideraremos um exemplo <em>escolar</em> de treinamento de uma rede neural para o problema de classifica√ß√£o usando a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> . </p><br><p>  Ent√£o, vamos tirar um conjunto de dados simples com fotos de frutas com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kaggle</a> .  A tarefa √© associar uma classe correspondente a cada figura de fruta. </p><br><p>  Antes de usar o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> , vamos definir os principais componentes: </p><br><p>  Fluxo de dados </p><br><ul><li>  carregador de amostras de treinamento, <code>train_loader</code> </li><li>  <code>val_loader</code> lote de checkout, <code>val_loader</code> </li></ul><br><p>  Modelo: </p><br><ul><li>  pegue a pequena grade <code>torchvision</code> da <code>torchvision</code> da <code>torchvision</code> </li></ul><br><p>  Algoritmo de otimiza√ß√£o: </p><br><ul><li>  tomar sgd </li></ul><br><p>  Fun√ß√£o de perda: </p><br><ul><li>  Entropia cruzada </li></ul><br><div class="spoiler">  <b class="spoiler_title">C√≥digo</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dataset, DataLoader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data.dataset <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Subset <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageFolder <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.transforms <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Compose, RandomResizedCrop, RandomVerticalFlip, RandomHorizontalFlip <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.transforms <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ColorJitter, ToTensor, Normalize FRUIT360_PATH = Path(<span class="hljs-string"><span class="hljs-string">"."</span></span>).resolve().parent / <span class="hljs-string"><span class="hljs-string">"input"</span></span> / <span class="hljs-string"><span class="hljs-string">"fruits-360_dataset"</span></span> / <span class="hljs-string"><span class="hljs-string">"fruits-360"</span></span> device = <span class="hljs-string"><span class="hljs-string">"cuda"</span></span> train_transform = Compose([ RandomHorizontalFlip(), RandomResizedCrop(size=<span class="hljs-number"><span class="hljs-number">32</span></span>), ColorJitter(brightness=<span class="hljs-number"><span class="hljs-number">0.12</span></span>), ToTensor(), Normalize(mean=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>], std=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) ]) val_transform = Compose([ RandomResizedCrop(size=<span class="hljs-number"><span class="hljs-number">32</span></span>), ToTensor(), Normalize(mean=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>], std=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) ]) batch_size = <span class="hljs-number"><span class="hljs-number">128</span></span> num_workers = <span class="hljs-number"><span class="hljs-number">8</span></span> train_dataset = ImageFolder((FRUIT360_PATH /<span class="hljs-string"><span class="hljs-string">"Training"</span></span>).as_posix(), transform=train_transform, target_transform=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) val_dataset = ImageFolder((FRUIT360_PATH /<span class="hljs-string"><span class="hljs-string">"Test"</span></span>).as_posix(), transform=val_transform, target_transform=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device) val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.models.squeezenet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> squeezenet1_1 model = squeezenet1_1(pretrained=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">81</span></span>) model.classifier[<span class="hljs-number"><span class="hljs-number">-1</span></span>] = nn.AdaptiveAvgPool2d(<span class="hljs-number"><span class="hljs-number">1</span></span>) model = model.to(device)</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.optim <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD optimizer = SGD(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) criterion = nn.CrossEntropyLoss()</code> </pre> </div></div><br><p>  Ent√£o agora √© hora de executar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Engine, _prepare_batch <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">process_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.train() optimizer.zero_grad() x, y = _prepare_batch(batch, device=device) y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() trainer = Engine(process_function)</code> </pre> <br><p>  Vamos ver o que esse c√≥digo significa. </p><br><h3 id="dvizhok-engine">  Motor <code>Engine</code> </h3><br><p>  A classe <code>ignite.engine.Engine</code> √© a estrutura da biblioteca e o objeto dessa classe √© o <code>trainer</code> : </p><br><pre> <code class="python hljs">trainer = Engine(process_function)</code> </pre> <br><p>  √â definido com a fun√ß√£o de entrada <code>process_function</code> para processar um lote e serve para implementar passes para a amostra de treinamento.  Dentro da classe <code>ignite.engine.Engine</code> , acontece o seguinte: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epoch &lt; max_epochs: <span class="hljs-comment"><span class="hljs-comment"># run once on data for batch in data: output = process_function(batch)</span></span></code> </pre> <br><p>  De volta √† fun√ß√£o <code>process_function</code> : </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">process_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.train() optimizer.zero_grad() x, y = _prepare_batch(batch, device=device) y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item()</code> </pre> <br><p>  Vemos que, dentro da fun√ß√£o, como de costume no caso do treinamento em modelo, calculamos as previs√µes <code>y_pred</code> , calculamos a fun√ß√£o de <code>loss</code> , <code>loss</code> e gradientes.  Este √∫ltimo permite atualizar o peso do modelo: <code>optimizer.step()</code> . </p><br><p>  Em geral, n√£o h√° restri√ß√µes no c√≥digo da fun√ß√£o <code>process_function</code> .  Observamos apenas que s√£o necess√°rios dois argumentos como entrada: o objeto <code>Engine</code> (no nosso caso, <code>trainer</code> ) e o lote do carregador de dados.  Portanto, por exemplo, para testar uma rede neural, podemos definir outro objeto da classe <code>ignite.engine.Engine</code> , na qual a fun√ß√£o de entrada simplesmente calcula as previs√µes e implementa uma passagem na amostra de teste uma vez.  Leia mais tarde. </p><br><p>  Portanto, o c√≥digo acima define apenas os objetos necess√°rios sem iniciar o treinamento.  Basicamente, em um exemplo m√≠nimo, voc√™ pode chamar o m√©todo: </p><br><pre> <code class="python hljs">trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><p>  e esse c√≥digo √© suficiente para "silenciosamente" (sem nenhuma deriva√ß√£o de resultados intermedi√°rios) treinar o modelo. </p><br><div class="spoiler">  <b class="spoiler_title">Uma nota</b> <div class="spoiler_text"><p>  Observe tamb√©m que, para tarefas desse tipo, a biblioteca possui um m√©todo conveniente para criar o objeto de <code>trainer</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_supervised_trainer trainer = create_supervised_trainer(model, optimizer, criterion, device)</code> </pre> </div></div><br><p>  Obviamente, na pr√°tica, o exemplo acima √© de pouco interesse, ent√£o vamos adicionar as seguintes op√ß√µes para o "coach": </p><br><ul><li>  exibi√ß√£o do valor da fun√ß√£o de perda a cada 50 itera√ß√µes </li><li>  in√≠cio do c√°lculo das m√©tricas no conjunto de treinamento com um modelo fixo </li><li>  in√≠cio do c√°lculo das m√©tricas na amostra de teste ap√≥s cada √©poca </li><li>  salvando os par√¢metros do modelo ap√≥s cada √©poca </li><li>  preserva√ß√£o dos tr√™s melhores modelos </li><li>  mudan√ßa na velocidade de aprendizado, dependendo da √©poca (programa√ß√£o da taxa de aprendizado) </li><li>  treinamento de parada precoce (parada precoce) </li></ul><br><h3 id="sobytiya-i-obrabotchiki-sobytiy">  Eventos e manipuladores de eventos </h3><br><p>  Para adicionar as op√ß√µes acima para o "treinador", a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">igni√ß√£o</a> fornece um sistema de eventos e o lan√ßamento de manipuladores de eventos personalizados.  Assim, o usu√°rio pode controlar um objeto da classe <code>Engine</code> em cada est√°gio: </p><br><ul><li>  motor iniciado / lan√ßamento conclu√≠do </li><li>  era come√ßou / terminou </li><li>  a itera√ß√£o em lote iniciada / finalizada </li></ul><br><p>  e execute seu c√≥digo em todos os eventos. </p><br><h4 id="vyvod-na-ekran-znacheniya-funkcii-poter">  Exibe valores da fun√ß√£o de perda </h4><br><p>  Para fazer isso, basta determinar a fun√ß√£o na qual a sa√≠da ser√° exibida na tela e adicion√°-la ao "treinador": </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Events log_interval = <span class="hljs-number"><span class="hljs-number">50</span></span> @trainer.on(Events.ITERATION_COMPLETED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_training_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> iteration = (engine.state.iteration - <span class="hljs-number"><span class="hljs-number">1</span></span>) % len(train_loader) + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> iteration % log_interval == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">"Epoch[{}] Iteration[{}/{}] Loss: {:.4f}"</span></span> .format(engine.state.epoch, iteration, len(train_loader), engine.state.output))</code> </pre> <br><p>  Na verdade, existem duas maneiras de adicionar um manipulador de eventos: atrav√©s de <code>add_event_handler</code> ou atrav√©s do decorador <code>on</code> .  O mesmo que acima pode ser feito assim: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Events log_interval = <span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_training_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># ... trainer.add_event_handler(Events.ITERATION_COMPLETED, log_training_loss)</span></span></code> </pre> <br><p>  Observe que qualquer argumento pode ser passado para a fun√ß√£o de manipula√ß√£o de eventos.  Em geral, essa fun√ß√£o ter√° a seguinte apar√™ncia: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">custom_handler</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, *args, **kwargs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> trainer.add_event_handler(Events.ITERATION_COMPLETED, custom_handler, *args, **kwargs) <span class="hljs-comment"><span class="hljs-comment">#  @trainer.on(Events.ITERATION_COMPLETED, *args, **kwargs) def custom_handler(engine, *args, **kwargs): pass</span></span></code> </pre> <br><p>  Ent√£o, vamos come√ßar o treinamento em uma √©poca e ver o que acontece: </p><br><pre> <code class="python hljs">output = trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[50/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.3459</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[100/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.2801</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[150/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.2294</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[200/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.1467</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[250/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 3<span class="hljs-selector-class"><span class="hljs-selector-class">.8607</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[300/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 3<span class="hljs-selector-class"><span class="hljs-selector-class">.6688</span></span></code> </pre> <br><p>  Nada mal!  Vamos mais longe. </p><br><h4 id="zapusk-rascheta-metrik-na-obuchayuschey-i-testovoy-vyborkah">  Iniciando o c√°lculo de m√©tricas em amostras de treinamento e teste </h4><br><p>  Vamos calcular as seguintes m√©tricas: precis√£o m√©dia, completude m√©dia ap√≥s cada √©poca da parte do treinamento e de toda a amostra de teste.  Observe que calcularemos as m√©tricas da parte da amostra de treinamento ap√≥s cada era do treinamento, e n√£o durante o treinamento.  Assim, a medi√ß√£o da efici√™ncia ser√° mais precisa, pois o modelo n√£o muda durante o c√°lculo. </p><br><p>  Ent√£o, definimos as m√©tricas: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Loss, CategoricalAccuracy, Precision, Recall metrics = { <span class="hljs-string"><span class="hljs-string">'avg_loss'</span></span>: Loss(criterion), <span class="hljs-string"><span class="hljs-string">'avg_accuracy'</span></span>: CategoricalAccuracy(), <span class="hljs-string"><span class="hljs-string">'avg_precision'</span></span>: Precision(average=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), <span class="hljs-string"><span class="hljs-string">'avg_recall'</span></span>: Recall(average=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) }</code> </pre> <br><p>  Em seguida, criaremos dois mecanismos para avaliar o modelo usando <code>ignite.engine.create_supervised_evaluator</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_supervised_evaluator <span class="hljs-comment"><span class="hljs-comment"># ,  device = ‚Äúcuda‚Äù    train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device) val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)</span></span></code> </pre> <br><p>  Criamos dois mecanismos para anexar ainda mais manipuladores de eventos adicionais a um deles ( <code>val_evaluator</code> ) para salvar o modelo e parar de aprender mais cedo (sobre tudo isso abaixo). </p><br><p>  Vamos tamb√©m dar uma olhada em como o mecanismo para avaliar o modelo √© definido, como a fun√ß√£o de entrada <code>process_function</code> definida para processar um lote: </p><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_supervised_evaluator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, metrics={}, device=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> device: model.to(device) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_inference</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.eval() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): x, y = _prepare_batch(batch, device=device) y_pred = model(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> y_pred, y engine = Engine(_inference) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> name, metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> metrics.items(): metric.attach(engine, name) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> engine</code> </pre> <br><p>  Continuamos mais.  Vamos selecionar aleatoriamente a parte da amostra de treinamento na qual calcularemos as m√©tricas: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data.dataset <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Subset indices = np.arange(len(train_dataset)) random_indices = np.random.permutation(indices)[:len(val_dataset)] train_subset = Subset(train_dataset, indices=random_indices) train_eval_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre> <br><p>  Em seguida, vamos determinar em que momento do treinamento iniciaremos o c√°lculo das m√©tricas e produziremos na tela: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@trainer.on(Events.EPOCH_COMPLETED) def compute_and_display_offline_train_metrics(engine): epoch = engine.state.epoch print("Compute train metrics...") metrics = train_evaluator.run(train_eval_loader).metrics print("Training Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}" .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall'])) @trainer.on(Events.EPOCH_COMPLETED) def compute_and_display_val_metrics(engine): epoch = engine.state.epoch print("Compute validation metrics...") metrics = val_evaluator.run(val_loader).metrics print("Validation Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}" .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall']))</span></span></code> </pre><br><p>  Voc√™ pode correr! </p><br><pre> <code class="python hljs">output = trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  Chegamos na tela </p><br><pre> <code class="hljs powershell">Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">3.5112</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.9840</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.8807</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.9285</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.5026</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1944</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">2.1018</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.3699</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.3981</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.3686</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">2.0519</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.3850</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.3578</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.3845</span></span></code> </pre><br><p>  J√° est√° melhor! </p><br><p>  <strong>Alguns detalhes</strong> <br>  Vejamos um pouco o c√≥digo anterior.  O leitor pode ter notado a seguinte linha de c√≥digo: </p><br><pre> <code class="python hljs">metrics = train_evaluator.run(train_eval_loader).metrics</code> </pre> <br><p>  e provavelmente houve uma pergunta sobre o tipo de objeto obtido de <code>train_evaluator.run(train_eval_loader)</code> , que possui o atributo de <code>metrics</code> . </p><br><p>  De fato, a classe <code>Engine</code> cont√©m uma estrutura chamada <code>state</code> (type <code>State</code> ) para poder transferir dados entre manipuladores de eventos.  Este atributo <code>state</code> cont√©m informa√ß√µes b√°sicas sobre a era atual, itera√ß√£o, n√∫mero de eras etc.  Tamb√©m pode ser usado para transferir quaisquer dados do usu√°rio, incluindo os resultados do c√°lculo das m√©tricas. </p><br><pre> <code class="python hljs">state = train_evaluator.run(train_eval_loader) metrics = state.metrics <span class="hljs-comment"><span class="hljs-comment">#   train_evaluator.run(train_eval_loader) metrics = train_evaluator.state.metrics</span></span></code> </pre> <br><h5 id="raschet-metrik-vo-vremya-obucheniya">  C√°lculo de m√©tricas durante o treinamento </h5><br><p>  Se a tarefa tiver um grande conjunto de treinamentos e calcular m√©tricas ap√≥s cada √©poca de treinamento, ser√° caro, mas ainda <code>RunningAverage</code> algumas m√©tricas sejam alteradas durante o treinamento, voc√™ poder√° usar o seguinte manipulador de eventos <code>RunningAverage</code> na caixa.  Por exemplo, queremos calcular e exibir a precis√£o do classificador: </p><br><pre> <code class="python hljs">acc_metric = RunningAverage(CategoryAccuracy(...), alpha=<span class="hljs-number"><span class="hljs-number">0.98</span></span>) acc_metric.attach(trainer, <span class="hljs-string"><span class="hljs-string">'running_avg_accuracy'</span></span>) @trainer.on(Events.ITERATION_COMPLETED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_running_avg_metrics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> print(<span class="hljs-string"><span class="hljs-string">"running avg accuracy:"</span></span>, engine.state.metrics[<span class="hljs-string"><span class="hljs-string">'running_avg_accuracy'</span></span>])</code> </pre> <br><p>  Para usar a funcionalidade <code>RunningAverage</code> , √© necess√°rio instalar o <em>ignite a</em> partir das fontes: </p><br><pre> <code class="hljs objectivec">pip install git+https:<span class="hljs-comment"><span class="hljs-comment">//github.com/pytorch/ignite</span></span></code> </pre> <br><h4 id="izmenenie-skorosti-obuchenie-learning-rate-scheduling">  Programa√ß√£o da taxa de aprendizado </h4><br><p>  Existem v√°rias maneiras de alterar a velocidade de aprendizado usando <em>igni√ß√£o</em> .  Em seguida, considere o m√©todo mais simples chamando a fun√ß√£o <code>lr_scheduler.step()</code> no in√≠cio de cada era. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.optim.lr_scheduler <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ExponentialLR lr_scheduler = ExponentialLR(optimizer, gamma=<span class="hljs-number"><span class="hljs-number">0.8</span></span>) @trainer.on(Events.EPOCH_STARTED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_lr_scheduler</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> lr_scheduler.step() <span class="hljs-comment"><span class="hljs-comment">#    : if len(optimizer.param_groups) == 1: lr = float(optimizer.param_groups[0]['lr']) print("Learning rate: {}".format(lr)) else: for i, param_group in enumerate(optimizer.param_groups): lr = float(param_group['lr']) print("Learning rate (group {}): {}".format(i, lr))</span></span></code> </pre> <br><h4 id="sohranenie-luchshih-modeley-i-drugih-parametrov-vo-vremya-obucheniya">  Salvando os melhores modelos e outros par√¢metros durante o treinamento </h4><br><p>  Durante o treinamento, seria √≥timo gravar os pesos do melhor modelo no disco, al√©m de salvar periodicamente os pesos do modelo, os par√¢metros do otimizador e os par√¢metros para alterar a velocidade de aprendizado.  O √∫ltimo pode ser √∫til para retomar a aprendizagem do √∫ltimo estado salvo. </p><br><p>  <em>O Ignite</em> possui uma classe <code>ModelCheckpoint</code> especial para isso.  Portanto, vamos criar um <code>ModelCheckpoint</code> eventos <code>ModelCheckpoint</code> e salvar o melhor modelo em termos de precis√£o no conjunto de testes.  Nesse caso, definimos uma fun√ß√£o <code>score_function</code> que fornece o valor de precis√£o ao manipulador de eventos e decide se o modelo deve ser salvo ou n√£o: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.handlers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">score_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> val_avg_accuracy = engine.state.metrics[<span class="hljs-string"><span class="hljs-string">'avg_accuracy'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> val_avg_accuracy best_model_saver = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"best_models"</span></span>, filename_prefix=<span class="hljs-string"><span class="hljs-string">"model"</span></span>, score_name=<span class="hljs-string"><span class="hljs-string">"val_accuracy"</span></span>, score_function=score_function, n_saved=<span class="hljs-number"><span class="hljs-number">3</span></span>, save_as_state_dict=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, create_dir=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-comment"><span class="hljs-comment"># "best_models" -    1     #   -&gt; {filename_prefix}_{name}_{step_number}_{score_name}={abs(score_function_result)}.pth # save_as_state_dict=True, #   `state_dict` val_evaluator.add_event_handler(Events.COMPLETED, best_model_saver, {"best_model": model})</span></span></code> </pre> <br><p>  Agora crie outro <code>ModelCheckpoint</code> eventos <code>ModelCheckpoint</code> para manter o estado de aprendizado a cada 1000 itera√ß√µes: </p><br><pre> <code class="python hljs">training_saver = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"checkpoint"</span></span>, filename_prefix=<span class="hljs-string"><span class="hljs-string">"checkpoint"</span></span>, save_interval=<span class="hljs-number"><span class="hljs-number">1000</span></span>, n_saved=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_as_state_dict=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, create_dir=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) to_save = {<span class="hljs-string"><span class="hljs-string">"model"</span></span>: model, <span class="hljs-string"><span class="hljs-string">"optimizer"</span></span>: optimizer, <span class="hljs-string"><span class="hljs-string">"lr_scheduler"</span></span>: lr_scheduler} trainer.add_event_handler(Events.ITERATION_COMPLETED, training_saver, to_save)</code> </pre> <br><p>  Ent√£o, quase tudo est√° pronto, adicione o √∫ltimo elemento: </p><br><h4 id="rannyaya-ostanovka-obucheniya-early-stopping">  Treinamento de parada precoce (parada antecipada) </h4><br><p>  Vamos adicionar outro manipulador de eventos que interromper√° o aprendizado se n√£o houver melhoria na qualidade do modelo em mais de 10 √©pocas.  Vamos avaliar a qualidade do modelo novamente usando a <code>score_function</code> score_function. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.handlers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EarlyStopping early_stopping = EarlyStopping(patience=<span class="hljs-number"><span class="hljs-number">10</span></span>, score_function=score_function, trainer=trainer) val_evaluator.add_event_handler(Events.EPOCH_COMPLETED, early_stopping)</code> </pre> <br><h3 id="zapusk-obucheniya">  Iniciar treinamento </h3><br><p>  Para iniciar o treinamento, basta chamar o m√©todo <code>run()</code> .  Treinaremos o modelo por 10 √©pocas: </p><br><pre> <code class="python hljs">max_epochs = <span class="hljs-number"><span class="hljs-number">10</span></span> output = trainer.run(train_loader, max_epochs=max_epochs)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Sa√≠da de tela</b> <div class="spoiler_text"><pre> <code class="hljs powershell">Learning rate: <span class="hljs-number"><span class="hljs-number">0.01</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.7984</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.9736</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">4.3419</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.0261</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1724</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1599</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">1.5363</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.5177</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.5477</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.5178</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">1.5116</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.5139</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.5400</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.5140</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.008</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4076</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4892</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.2485</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.6511</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">3.3376</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.3299</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">2</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">3.2686</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.1977</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.1792</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.1942</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">2</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">3.2772</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.1962</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.1628</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.1918</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.006400000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9016</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.2006</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8892</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8141</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4005</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8888</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">3</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7368</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7554</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7818</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7554</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">3</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7177</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7623</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7863</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7611</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.005120000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8490</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8493</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8100</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9165</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9370</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6548</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">4</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7047</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7713</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8040</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7728</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">4</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.6737</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7778</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7955</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7806</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.004096000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6965</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6196</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6194</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3986</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6032</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.7152</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">5</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.5049</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8282</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8393</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8314</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">5</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.5084</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8304</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8386</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8328</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.0032768000000000007</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4433</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4764</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5578</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3684</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4847</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3811</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">6</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4383</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8474</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8618</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8495</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">6</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4419</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8446</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8532</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8442</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.002621440000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4447</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4602</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5345</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3973</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5023</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5303</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">7</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4305</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8579</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8691</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8596</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">7</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4262</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8590</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8685</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8606</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.002097152000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4867</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3090</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3721</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4559</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3958</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4222</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">8</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3432</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8818</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8895</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8817</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">8</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3644</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8713</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8784</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8707</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.001677721600000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3557</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3692</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3510</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3446</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3966</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3451</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">9</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3315</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8954</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.9001</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8982</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">9</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3559</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8818</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8876</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8847</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.0013421772800000006</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3340</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3370</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3694</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3409</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4420</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.2770</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">10</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3246</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8921</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8988</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8925</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">10</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3536</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8731</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8785</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8722</span></span></code> </pre> </div></div><br><p>  Agora verifique os modelos e par√¢metros salvos no disco: </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">ls</span></span> best_models/ model_best_model_10_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8730994</span></span>.pth model_best_model_8_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8712978</span></span>.pth model_best_model_9_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8818188</span></span>.pth</code> </pre> <br><p>  e </p><br><pre> <code class="hljs pgsql">ls <span class="hljs-keyword"><span class="hljs-keyword">checkpoint</span></span>/ checkpoint_lr_scheduler_3000.pth checkpoint_optimizer_3000.pth checkpoint_model_3000.pth</code> </pre> <br><h3 id="predskazaniya-obuchennoy-modelyu">  Previs√µes de um modelo treinado </h3><br><p>  Primeiro, crie um carregador de dados de teste (por exemplo, tire uma amostra de valida√ß√£o) para que o lote de dados consista em imagens e seus √≠ndices: </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestDataset</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Dataset)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, ds)</span></span></span><span class="hljs-function">:</span></span> self.ds = ds <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(self.ds) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__getitem__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, index)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.ds[index][<span class="hljs-number"><span class="hljs-number">0</span></span>], index test_dataset = TestDataset(val_dataset) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre> <br><p>  Usando <em>ignite,</em> criaremos um novo mecanismo de previs√£o para dados de teste.  Para isso, definimos a fun√ß√£o <code>inference_update</code> , que retorna o resultado da previs√£o e o √≠ndice da imagem.  Para aumentar a precis√£o, tamb√©m usaremos o truque conhecido "aumento do tempo de teste" (TTA). </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite._utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> convert_tensor <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_prepare_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch)</span></span></span><span class="hljs-function">:</span></span> x, index = batch x = convert_tensor(x, device=device) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x, index <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">inference_update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> x, indices = _prepare_batch(batch) y_pred = model(x) y_pred = F.softmax(y_pred, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> {<span class="hljs-string"><span class="hljs-string">"y_pred"</span></span>: convert_tensor(y_pred, device=<span class="hljs-string"><span class="hljs-string">'cpu'</span></span>), <span class="hljs-string"><span class="hljs-string">"indices"</span></span>: indices} model.eval() inferencer = Engine(inference_update)</code> </pre> <br><p>  Em seguida, crie manipuladores de eventos que notificar√£o sobre o est√°gio das previs√µes e salvem as previs√µes em uma matriz dedicada: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@inferencer.on(Events.EPOCH_COMPLETED) def log_tta(engine): print("TTA {} / {}".format(engine.state.epoch, n_tta)) n_tta = 3 num_classes = 81 n_samples = len(val_dataset) #     y_probas_tta = np.zeros((n_samples, num_classes, n_tta), dtype=np.float32) @inferencer.on(Events.ITERATION_COMPLETED) def save_results(engine): output = engine.state.output tta_index = engine.state.epoch - 1 start_index = ((engine.state.iteration - 1) % len(test_loader)) * batch_size end_index = min(start_index + batch_size, n_samples) batch_y_probas = output['y_pred'].detach().numpy() y_probas_tta[start_index:end_index, :, tta_index] = batch_y_probas</span></span></code> </pre> <br><p>  Antes de iniciar o processo, vamos baixar o melhor modelo: </p><br><pre> <code class="python hljs">model = squeezenet1_1(pretrained=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">64</span></span>) model.classifier[<span class="hljs-number"><span class="hljs-number">-1</span></span>] = nn.AdaptiveAvgPool2d(<span class="hljs-number"><span class="hljs-number">1</span></span>) model = model.to(device) model_state_dict = torch.load(<span class="hljs-string"><span class="hljs-string">"best_models/model_best_model_10_val_accuracy=0.8730994.pth"</span></span>) model.load_state_dict(model_state_dict)</code> </pre> <br><p>  Lan√ßamos: </p><br><pre> <code class="python hljs">inferencer.run(test_loader, max_epochs=n_tta) &gt; TTA <span class="hljs-number"><span class="hljs-number">1</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; TTA <span class="hljs-number"><span class="hljs-number">2</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; TTA <span class="hljs-number"><span class="hljs-number">3</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  Em seguida, de maneira padr√£o, tomamos a m√©dia das previs√µes de TTA e calculamos o √≠ndice de classe com a maior probabilidade: </p><br><pre> <code class="python hljs">y_probas = np.mean(y_probas_tta, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) y_preds = np.argmax(y_probas, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  E agora podemos mais uma vez calcular a precis√£o do modelo de acordo com as previs√µes: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score y_test_true = [y <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> val_dataset] accuracy_score(y_test_true, y_preds) &gt; <span class="hljs-number"><span class="hljs-number">0.9310369676443035</span></span></code> </pre> <br><p> ,     ,          .   ,   ,      ,    <em>ignite</em>      . </p><br><h2 id="drugie-primery-s-ignitehttpspytorchorgignite">    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> </h2><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> . </p><br><p>  github      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>       </p><br><ul><li> fast neural transfer </li><li> reinforcement learning </li><li> dcgan </li></ul><br><h2 id="zaklyuchenie">  Conclus√£o </h2><br><p>    ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a>      Facebook           (.   ).        0.1.0,   API (Engine, State, Events, Metric, ...)           .       ,      ,     ,     pull request-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> github</a> . </p><br><p>  Obrigado pela aten√ß√£o! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt424781/">https://habr.com/ru/post/pt424781/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt424767/index.html">N√≥s fazemos um bolo de Habr. Novamente</a></li>
<li><a href="../pt424771/index.html">Experi√™ncia pessoal: de uma ideia e uma folha em branco a uma vers√£o de rascunho de um site</a></li>
<li><a href="../pt424773/index.html">Biofarma e modelagem num√©rica: experi√™ncia e pr√°tica da Amgen</a></li>
<li><a href="../pt424777/index.html">Usando o c√¥nsul para dimensionar servi√ßos estatais</a></li>
<li><a href="../pt424779/index.html">SPA de v√°rias p√°ginas em Python</a></li>
<li><a href="../pt424787/index.html">Entrevista com Aaron Patterson, presidente da Confer√™ncia RubyRussia 2018</a></li>
<li><a href="../pt424789/index.html">Como implantar um aplicativo Ruby on Rails com o HAProxy Ingress, unicorn / puma e soquetes da web</a></li>
<li><a href="../pt424791/index.html">Expandindo os recursos de rede de um rel√© program√°vel usando WI-FI</a></li>
<li><a href="../pt424793/index.html">Como imprimir um motor el√©trico</a></li>
<li><a href="../pt424795/index.html">Qual o tamanho de um drone movido a energia solar?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>