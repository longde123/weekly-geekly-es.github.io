<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏻‍🤝‍👨🏼 🙆🏽 🙎🏽 Ensinando e testando redes neurais no PyTorch usando o Ignite 🧘 👲🏾 👂🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá, Habr, neste artigo, falarei sobre a biblioteca de ignição , com a qual você pode treinar e testar facilmente redes neurais usando a estrutura PyT...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ensinando e testando redes neurais no PyTorch usando o Ignite</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/424781/"><p>  <em>Olá, Habr, neste artigo, falarei sobre a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> , com a qual você pode treinar e testar facilmente redes neurais usando a estrutura PyTorch.</em> </p><br><p> Com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite,</a> você pode escrever ciclos para treinar a rede em apenas algumas linhas, adicionar cálculos de métricas padrão da caixa, salvar o modelo etc.  Bem, para aqueles que mudaram do TF para o PyTorch, podemos dizer que a biblioteca de <em>ignição</em> é o Keras for PyTorch. </p><br><p>  O artigo <em>examinará</em> em detalhes um exemplo de treinamento de uma rede neural para uma tarefa de classificação usando <em>ignição.</em> </p><br><p><img src="https://habrastorage.org/webt/35/ar/af/35arafc8y9aicrbpz5unazs-y-a.png"></p><a name="habracut"></a><br><h2 id="dobavim-esche-bolshe-ognya-v-pytorch">  Adicione mais fogo ao PyTorch </h2><br><p>  Não vou perder tempo falando sobre o quão <em>legal é a</em> estrutura <em>do</em> PyTorch.  Qualquer um que já o tenha usado entende o que estou escrevendo.  Mas, com todas as suas vantagens, ainda é de baixo nível em termos de ciclos de escrita para treinamento, teste e teste de redes neurais. </p><br><p>  Se olharmos para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">exemplos oficiais de</a> uso da estrutura PyTorch, veremos pelo menos dois ciclos de iterações por época e por lotes do treinamento definido no código de treinamento da grade: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, epochs + <span class="hljs-number"><span class="hljs-number">1</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): <span class="hljs-comment"><span class="hljs-comment"># ...</span></span></code> </pre> <br><p>  A idéia principal da biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> é fatorar esses loops em uma única classe, enquanto permite ao usuário interagir com esses loops usando manipuladores de eventos. </p><br><p>  Como resultado, no caso de tarefas padrão de aprendizado profundo, podemos economizar muito no número de linhas de código.  Menos linhas - menos erros! </p><br><p>  Por exemplo, para comparação, à esquerda está o código para treinamento e validação de modelo usando <em>ignite</em> , e à direita está o PyTorch puro: <br><img src="https://habrastorage.org/getpro/habr/post_images/914/408/b07/914408b07fa093c696e66cb15ae36bfc.png" alt="imagem"></p><br><p>  Então, novamente, para que serve a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> ? </p><br><ul><li>  não é mais necessário gravar em cada loop de tarefa <code>for epoch in range(n_epochs)</code> e o <code>for batch in data_loader</code> . </li><li>  permite que você fatore melhor o código </li><li>  permite calcular métricas básicas prontas para uso </li><li>  fornece "pãezinhos" do tipo <br><ul><li>  salvar os melhores e mais recentes modelos (também otimizador e programador de taxas de aprendizado) durante o treinamento, </li><li>  parar de aprender cedo </li><li>  etc. </li></ul></li><li>  integra-se facilmente às ferramentas de visualização: tensorboardX, visdom, ... </li></ul><br><p>  De certa forma, como já mencionado, a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> pode ser comparada com todas as famosas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Keras</a> e sua API para treinamento e teste de redes.  Além disso, à primeira vista, a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> é muito semelhante à biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tnt</a> , pois inicialmente as duas bibliotecas tinham objetivos comuns e idéias semelhantes para sua implementação. </p><br><p>  Então, acenda: </p><br><pre> <code class="hljs sql">pip <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> pytorch-ignite</code> </pre> <br><p>  ou </p><br><pre> <code class="hljs swift">conda install ignite -<span class="hljs-built_in"><span class="hljs-built_in">c</span></span> pytorch</code> </pre> <br><p>  A seguir, com um exemplo concreto, vamos nos familiarizar com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">API da</a> biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> . </p><br><h2 id="zadacha-klassifikacii-s-ignitehttpspytorchorgignite">  Tarefa de classificação com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> </h2><br><p>  Nesta parte do artigo, consideraremos um exemplo <em>escolar</em> de treinamento de uma rede neural para o problema de classificação usando a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> . </p><br><p>  Então, vamos tirar um conjunto de dados simples com fotos de frutas com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kaggle</a> .  A tarefa é associar uma classe correspondente a cada figura de fruta. </p><br><p>  Antes de usar o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> , vamos definir os principais componentes: </p><br><p>  Fluxo de dados </p><br><ul><li>  carregador de amostras de treinamento, <code>train_loader</code> </li><li>  <code>val_loader</code> lote de checkout, <code>val_loader</code> </li></ul><br><p>  Modelo: </p><br><ul><li>  pegue a pequena grade <code>torchvision</code> da <code>torchvision</code> da <code>torchvision</code> </li></ul><br><p>  Algoritmo de otimização: </p><br><ul><li>  tomar sgd </li></ul><br><p>  Função de perda: </p><br><ul><li>  Entropia cruzada </li></ul><br><div class="spoiler">  <b class="spoiler_title">Código</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dataset, DataLoader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data.dataset <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Subset <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageFolder <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.transforms <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Compose, RandomResizedCrop, RandomVerticalFlip, RandomHorizontalFlip <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.transforms <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ColorJitter, ToTensor, Normalize FRUIT360_PATH = Path(<span class="hljs-string"><span class="hljs-string">"."</span></span>).resolve().parent / <span class="hljs-string"><span class="hljs-string">"input"</span></span> / <span class="hljs-string"><span class="hljs-string">"fruits-360_dataset"</span></span> / <span class="hljs-string"><span class="hljs-string">"fruits-360"</span></span> device = <span class="hljs-string"><span class="hljs-string">"cuda"</span></span> train_transform = Compose([ RandomHorizontalFlip(), RandomResizedCrop(size=<span class="hljs-number"><span class="hljs-number">32</span></span>), ColorJitter(brightness=<span class="hljs-number"><span class="hljs-number">0.12</span></span>), ToTensor(), Normalize(mean=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>], std=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) ]) val_transform = Compose([ RandomResizedCrop(size=<span class="hljs-number"><span class="hljs-number">32</span></span>), ToTensor(), Normalize(mean=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>], std=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) ]) batch_size = <span class="hljs-number"><span class="hljs-number">128</span></span> num_workers = <span class="hljs-number"><span class="hljs-number">8</span></span> train_dataset = ImageFolder((FRUIT360_PATH /<span class="hljs-string"><span class="hljs-string">"Training"</span></span>).as_posix(), transform=train_transform, target_transform=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) val_dataset = ImageFolder((FRUIT360_PATH /<span class="hljs-string"><span class="hljs-string">"Test"</span></span>).as_posix(), transform=val_transform, target_transform=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device) val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.models.squeezenet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> squeezenet1_1 model = squeezenet1_1(pretrained=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">81</span></span>) model.classifier[<span class="hljs-number"><span class="hljs-number">-1</span></span>] = nn.AdaptiveAvgPool2d(<span class="hljs-number"><span class="hljs-number">1</span></span>) model = model.to(device)</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.optim <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD optimizer = SGD(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) criterion = nn.CrossEntropyLoss()</code> </pre> </div></div><br><p>  Então agora é hora de executar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Engine, _prepare_batch <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">process_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.train() optimizer.zero_grad() x, y = _prepare_batch(batch, device=device) y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() trainer = Engine(process_function)</code> </pre> <br><p>  Vamos ver o que esse código significa. </p><br><h3 id="dvizhok-engine">  Motor <code>Engine</code> </h3><br><p>  A classe <code>ignite.engine.Engine</code> é a estrutura da biblioteca e o objeto dessa classe é o <code>trainer</code> : </p><br><pre> <code class="python hljs">trainer = Engine(process_function)</code> </pre> <br><p>  É definido com a função de entrada <code>process_function</code> para processar um lote e serve para implementar passes para a amostra de treinamento.  Dentro da classe <code>ignite.engine.Engine</code> , acontece o seguinte: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epoch &lt; max_epochs: <span class="hljs-comment"><span class="hljs-comment"># run once on data for batch in data: output = process_function(batch)</span></span></code> </pre> <br><p>  De volta à função <code>process_function</code> : </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">process_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.train() optimizer.zero_grad() x, y = _prepare_batch(batch, device=device) y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item()</code> </pre> <br><p>  Vemos que, dentro da função, como de costume no caso do treinamento em modelo, calculamos as previsões <code>y_pred</code> , calculamos a função de <code>loss</code> , <code>loss</code> e gradientes.  Este último permite atualizar o peso do modelo: <code>optimizer.step()</code> . </p><br><p>  Em geral, não há restrições no código da função <code>process_function</code> .  Observamos apenas que são necessários dois argumentos como entrada: o objeto <code>Engine</code> (no nosso caso, <code>trainer</code> ) e o lote do carregador de dados.  Portanto, por exemplo, para testar uma rede neural, podemos definir outro objeto da classe <code>ignite.engine.Engine</code> , na qual a função de entrada simplesmente calcula as previsões e implementa uma passagem na amostra de teste uma vez.  Leia mais tarde. </p><br><p>  Portanto, o código acima define apenas os objetos necessários sem iniciar o treinamento.  Basicamente, em um exemplo mínimo, você pode chamar o método: </p><br><pre> <code class="python hljs">trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><p>  e esse código é suficiente para "silenciosamente" (sem nenhuma derivação de resultados intermediários) treinar o modelo. </p><br><div class="spoiler">  <b class="spoiler_title">Uma nota</b> <div class="spoiler_text"><p>  Observe também que, para tarefas desse tipo, a biblioteca possui um método conveniente para criar o objeto de <code>trainer</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_supervised_trainer trainer = create_supervised_trainer(model, optimizer, criterion, device)</code> </pre> </div></div><br><p>  Obviamente, na prática, o exemplo acima é de pouco interesse, então vamos adicionar as seguintes opções para o "coach": </p><br><ul><li>  exibição do valor da função de perda a cada 50 iterações </li><li>  início do cálculo das métricas no conjunto de treinamento com um modelo fixo </li><li>  início do cálculo das métricas na amostra de teste após cada época </li><li>  salvando os parâmetros do modelo após cada época </li><li>  preservação dos três melhores modelos </li><li>  mudança na velocidade de aprendizado, dependendo da época (programação da taxa de aprendizado) </li><li>  treinamento de parada precoce (parada precoce) </li></ul><br><h3 id="sobytiya-i-obrabotchiki-sobytiy">  Eventos e manipuladores de eventos </h3><br><p>  Para adicionar as opções acima para o "treinador", a biblioteca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignição</a> fornece um sistema de eventos e o lançamento de manipuladores de eventos personalizados.  Assim, o usuário pode controlar um objeto da classe <code>Engine</code> em cada estágio: </p><br><ul><li>  motor iniciado / lançamento concluído </li><li>  era começou / terminou </li><li>  a iteração em lote iniciada / finalizada </li></ul><br><p>  e execute seu código em todos os eventos. </p><br><h4 id="vyvod-na-ekran-znacheniya-funkcii-poter">  Exibe valores da função de perda </h4><br><p>  Para fazer isso, basta determinar a função na qual a saída será exibida na tela e adicioná-la ao "treinador": </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Events log_interval = <span class="hljs-number"><span class="hljs-number">50</span></span> @trainer.on(Events.ITERATION_COMPLETED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_training_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> iteration = (engine.state.iteration - <span class="hljs-number"><span class="hljs-number">1</span></span>) % len(train_loader) + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> iteration % log_interval == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">"Epoch[{}] Iteration[{}/{}] Loss: {:.4f}"</span></span> .format(engine.state.epoch, iteration, len(train_loader), engine.state.output))</code> </pre> <br><p>  Na verdade, existem duas maneiras de adicionar um manipulador de eventos: através de <code>add_event_handler</code> ou através do decorador <code>on</code> .  O mesmo que acima pode ser feito assim: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Events log_interval = <span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_training_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># ... trainer.add_event_handler(Events.ITERATION_COMPLETED, log_training_loss)</span></span></code> </pre> <br><p>  Observe que qualquer argumento pode ser passado para a função de manipulação de eventos.  Em geral, essa função terá a seguinte aparência: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">custom_handler</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, *args, **kwargs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> trainer.add_event_handler(Events.ITERATION_COMPLETED, custom_handler, *args, **kwargs) <span class="hljs-comment"><span class="hljs-comment">#  @trainer.on(Events.ITERATION_COMPLETED, *args, **kwargs) def custom_handler(engine, *args, **kwargs): pass</span></span></code> </pre> <br><p>  Então, vamos começar o treinamento em uma época e ver o que acontece: </p><br><pre> <code class="python hljs">output = trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[50/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.3459</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[100/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.2801</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[150/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.2294</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[200/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.1467</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[250/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 3<span class="hljs-selector-class"><span class="hljs-selector-class">.8607</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[300/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 3<span class="hljs-selector-class"><span class="hljs-selector-class">.6688</span></span></code> </pre> <br><p>  Nada mal!  Vamos mais longe. </p><br><h4 id="zapusk-rascheta-metrik-na-obuchayuschey-i-testovoy-vyborkah">  Iniciando o cálculo de métricas em amostras de treinamento e teste </h4><br><p>  Vamos calcular as seguintes métricas: precisão média, completude média após cada época da parte do treinamento e de toda a amostra de teste.  Observe que calcularemos as métricas da parte da amostra de treinamento após cada era do treinamento, e não durante o treinamento.  Assim, a medição da eficiência será mais precisa, pois o modelo não muda durante o cálculo. </p><br><p>  Então, definimos as métricas: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Loss, CategoricalAccuracy, Precision, Recall metrics = { <span class="hljs-string"><span class="hljs-string">'avg_loss'</span></span>: Loss(criterion), <span class="hljs-string"><span class="hljs-string">'avg_accuracy'</span></span>: CategoricalAccuracy(), <span class="hljs-string"><span class="hljs-string">'avg_precision'</span></span>: Precision(average=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), <span class="hljs-string"><span class="hljs-string">'avg_recall'</span></span>: Recall(average=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) }</code> </pre> <br><p>  Em seguida, criaremos dois mecanismos para avaliar o modelo usando <code>ignite.engine.create_supervised_evaluator</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_supervised_evaluator <span class="hljs-comment"><span class="hljs-comment"># ,  device = “cuda”    train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device) val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)</span></span></code> </pre> <br><p>  Criamos dois mecanismos para anexar ainda mais manipuladores de eventos adicionais a um deles ( <code>val_evaluator</code> ) para salvar o modelo e parar de aprender mais cedo (sobre tudo isso abaixo). </p><br><p>  Vamos também dar uma olhada em como o mecanismo para avaliar o modelo é definido, como a função de entrada <code>process_function</code> definida para processar um lote: </p><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_supervised_evaluator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, metrics={}, device=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> device: model.to(device) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_inference</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.eval() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): x, y = _prepare_batch(batch, device=device) y_pred = model(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> y_pred, y engine = Engine(_inference) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> name, metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> metrics.items(): metric.attach(engine, name) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> engine</code> </pre> <br><p>  Continuamos mais.  Vamos selecionar aleatoriamente a parte da amostra de treinamento na qual calcularemos as métricas: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data.dataset <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Subset indices = np.arange(len(train_dataset)) random_indices = np.random.permutation(indices)[:len(val_dataset)] train_subset = Subset(train_dataset, indices=random_indices) train_eval_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre> <br><p>  Em seguida, vamos determinar em que momento do treinamento iniciaremos o cálculo das métricas e produziremos na tela: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@trainer.on(Events.EPOCH_COMPLETED) def compute_and_display_offline_train_metrics(engine): epoch = engine.state.epoch print("Compute train metrics...") metrics = train_evaluator.run(train_eval_loader).metrics print("Training Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}" .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall'])) @trainer.on(Events.EPOCH_COMPLETED) def compute_and_display_val_metrics(engine): epoch = engine.state.epoch print("Compute validation metrics...") metrics = val_evaluator.run(val_loader).metrics print("Validation Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}" .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall']))</span></span></code> </pre><br><p>  Você pode correr! </p><br><pre> <code class="python hljs">output = trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  Chegamos na tela </p><br><pre> <code class="hljs powershell">Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">3.5112</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.9840</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.8807</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.9285</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.5026</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1944</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">2.1018</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.3699</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.3981</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.3686</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">2.0519</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.3850</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.3578</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.3845</span></span></code> </pre><br><p>  Já está melhor! </p><br><p>  <strong>Alguns detalhes</strong> <br>  Vejamos um pouco o código anterior.  O leitor pode ter notado a seguinte linha de código: </p><br><pre> <code class="python hljs">metrics = train_evaluator.run(train_eval_loader).metrics</code> </pre> <br><p>  e provavelmente houve uma pergunta sobre o tipo de objeto obtido de <code>train_evaluator.run(train_eval_loader)</code> , que possui o atributo de <code>metrics</code> . </p><br><p>  De fato, a classe <code>Engine</code> contém uma estrutura chamada <code>state</code> (type <code>State</code> ) para poder transferir dados entre manipuladores de eventos.  Este atributo <code>state</code> contém informações básicas sobre a era atual, iteração, número de eras etc.  Também pode ser usado para transferir quaisquer dados do usuário, incluindo os resultados do cálculo das métricas. </p><br><pre> <code class="python hljs">state = train_evaluator.run(train_eval_loader) metrics = state.metrics <span class="hljs-comment"><span class="hljs-comment">#   train_evaluator.run(train_eval_loader) metrics = train_evaluator.state.metrics</span></span></code> </pre> <br><h5 id="raschet-metrik-vo-vremya-obucheniya">  Cálculo de métricas durante o treinamento </h5><br><p>  Se a tarefa tiver um grande conjunto de treinamentos e calcular métricas após cada época de treinamento, será caro, mas ainda <code>RunningAverage</code> algumas métricas sejam alteradas durante o treinamento, você poderá usar o seguinte manipulador de eventos <code>RunningAverage</code> na caixa.  Por exemplo, queremos calcular e exibir a precisão do classificador: </p><br><pre> <code class="python hljs">acc_metric = RunningAverage(CategoryAccuracy(...), alpha=<span class="hljs-number"><span class="hljs-number">0.98</span></span>) acc_metric.attach(trainer, <span class="hljs-string"><span class="hljs-string">'running_avg_accuracy'</span></span>) @trainer.on(Events.ITERATION_COMPLETED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_running_avg_metrics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> print(<span class="hljs-string"><span class="hljs-string">"running avg accuracy:"</span></span>, engine.state.metrics[<span class="hljs-string"><span class="hljs-string">'running_avg_accuracy'</span></span>])</code> </pre> <br><p>  Para usar a funcionalidade <code>RunningAverage</code> , é necessário instalar o <em>ignite a</em> partir das fontes: </p><br><pre> <code class="hljs objectivec">pip install git+https:<span class="hljs-comment"><span class="hljs-comment">//github.com/pytorch/ignite</span></span></code> </pre> <br><h4 id="izmenenie-skorosti-obuchenie-learning-rate-scheduling">  Programação da taxa de aprendizado </h4><br><p>  Existem várias maneiras de alterar a velocidade de aprendizado usando <em>ignição</em> .  Em seguida, considere o método mais simples chamando a função <code>lr_scheduler.step()</code> no início de cada era. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.optim.lr_scheduler <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ExponentialLR lr_scheduler = ExponentialLR(optimizer, gamma=<span class="hljs-number"><span class="hljs-number">0.8</span></span>) @trainer.on(Events.EPOCH_STARTED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_lr_scheduler</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> lr_scheduler.step() <span class="hljs-comment"><span class="hljs-comment">#    : if len(optimizer.param_groups) == 1: lr = float(optimizer.param_groups[0]['lr']) print("Learning rate: {}".format(lr)) else: for i, param_group in enumerate(optimizer.param_groups): lr = float(param_group['lr']) print("Learning rate (group {}): {}".format(i, lr))</span></span></code> </pre> <br><h4 id="sohranenie-luchshih-modeley-i-drugih-parametrov-vo-vremya-obucheniya">  Salvando os melhores modelos e outros parâmetros durante o treinamento </h4><br><p>  Durante o treinamento, seria ótimo gravar os pesos do melhor modelo no disco, além de salvar periodicamente os pesos do modelo, os parâmetros do otimizador e os parâmetros para alterar a velocidade de aprendizado.  O último pode ser útil para retomar a aprendizagem do último estado salvo. </p><br><p>  <em>O Ignite</em> possui uma classe <code>ModelCheckpoint</code> especial para isso.  Portanto, vamos criar um <code>ModelCheckpoint</code> eventos <code>ModelCheckpoint</code> e salvar o melhor modelo em termos de precisão no conjunto de testes.  Nesse caso, definimos uma função <code>score_function</code> que fornece o valor de precisão ao manipulador de eventos e decide se o modelo deve ser salvo ou não: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.handlers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">score_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> val_avg_accuracy = engine.state.metrics[<span class="hljs-string"><span class="hljs-string">'avg_accuracy'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> val_avg_accuracy best_model_saver = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"best_models"</span></span>, filename_prefix=<span class="hljs-string"><span class="hljs-string">"model"</span></span>, score_name=<span class="hljs-string"><span class="hljs-string">"val_accuracy"</span></span>, score_function=score_function, n_saved=<span class="hljs-number"><span class="hljs-number">3</span></span>, save_as_state_dict=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, create_dir=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-comment"><span class="hljs-comment"># "best_models" -    1     #   -&gt; {filename_prefix}_{name}_{step_number}_{score_name}={abs(score_function_result)}.pth # save_as_state_dict=True, #   `state_dict` val_evaluator.add_event_handler(Events.COMPLETED, best_model_saver, {"best_model": model})</span></span></code> </pre> <br><p>  Agora crie outro <code>ModelCheckpoint</code> eventos <code>ModelCheckpoint</code> para manter o estado de aprendizado a cada 1000 iterações: </p><br><pre> <code class="python hljs">training_saver = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"checkpoint"</span></span>, filename_prefix=<span class="hljs-string"><span class="hljs-string">"checkpoint"</span></span>, save_interval=<span class="hljs-number"><span class="hljs-number">1000</span></span>, n_saved=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_as_state_dict=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, create_dir=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) to_save = {<span class="hljs-string"><span class="hljs-string">"model"</span></span>: model, <span class="hljs-string"><span class="hljs-string">"optimizer"</span></span>: optimizer, <span class="hljs-string"><span class="hljs-string">"lr_scheduler"</span></span>: lr_scheduler} trainer.add_event_handler(Events.ITERATION_COMPLETED, training_saver, to_save)</code> </pre> <br><p>  Então, quase tudo está pronto, adicione o último elemento: </p><br><h4 id="rannyaya-ostanovka-obucheniya-early-stopping">  Treinamento de parada precoce (parada antecipada) </h4><br><p>  Vamos adicionar outro manipulador de eventos que interromperá o aprendizado se não houver melhoria na qualidade do modelo em mais de 10 épocas.  Vamos avaliar a qualidade do modelo novamente usando a <code>score_function</code> score_function. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.handlers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EarlyStopping early_stopping = EarlyStopping(patience=<span class="hljs-number"><span class="hljs-number">10</span></span>, score_function=score_function, trainer=trainer) val_evaluator.add_event_handler(Events.EPOCH_COMPLETED, early_stopping)</code> </pre> <br><h3 id="zapusk-obucheniya">  Iniciar treinamento </h3><br><p>  Para iniciar o treinamento, basta chamar o método <code>run()</code> .  Treinaremos o modelo por 10 épocas: </p><br><pre> <code class="python hljs">max_epochs = <span class="hljs-number"><span class="hljs-number">10</span></span> output = trainer.run(train_loader, max_epochs=max_epochs)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Saída de tela</b> <div class="spoiler_text"><pre> <code class="hljs powershell">Learning rate: <span class="hljs-number"><span class="hljs-number">0.01</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.7984</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.9736</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">4.3419</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.0261</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1724</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1599</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">1.5363</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.5177</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.5477</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.5178</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">1.5116</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.5139</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.5400</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.5140</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.008</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4076</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4892</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.2485</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.6511</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">3.3376</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.3299</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">2</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">3.2686</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.1977</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.1792</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.1942</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">2</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">3.2772</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.1962</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.1628</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.1918</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.006400000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9016</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.2006</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8892</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8141</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4005</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8888</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">3</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7368</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7554</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7818</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7554</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">3</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7177</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7623</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7863</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7611</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.005120000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8490</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8493</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8100</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9165</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9370</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6548</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">4</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7047</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7713</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8040</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7728</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">4</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.6737</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7778</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7955</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7806</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.004096000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6965</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6196</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6194</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3986</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6032</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.7152</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">5</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.5049</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8282</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8393</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8314</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">5</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.5084</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8304</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8386</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8328</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.0032768000000000007</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4433</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4764</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5578</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3684</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4847</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3811</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">6</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4383</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8474</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8618</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8495</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">6</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4419</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8446</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8532</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8442</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.002621440000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4447</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4602</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5345</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3973</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5023</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5303</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">7</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4305</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8579</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8691</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8596</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">7</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4262</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8590</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8685</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8606</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.002097152000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4867</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3090</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3721</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4559</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3958</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4222</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">8</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3432</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8818</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8895</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8817</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">8</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3644</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8713</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8784</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8707</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.001677721600000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3557</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3692</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3510</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3446</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3966</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3451</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">9</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3315</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8954</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.9001</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8982</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">9</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3559</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8818</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8876</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8847</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.0013421772800000006</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3340</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3370</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3694</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3409</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4420</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.2770</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">10</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3246</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8921</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8988</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8925</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">10</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3536</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8731</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8785</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8722</span></span></code> </pre> </div></div><br><p>  Agora verifique os modelos e parâmetros salvos no disco: </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">ls</span></span> best_models/ model_best_model_10_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8730994</span></span>.pth model_best_model_8_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8712978</span></span>.pth model_best_model_9_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8818188</span></span>.pth</code> </pre> <br><p>  e </p><br><pre> <code class="hljs pgsql">ls <span class="hljs-keyword"><span class="hljs-keyword">checkpoint</span></span>/ checkpoint_lr_scheduler_3000.pth checkpoint_optimizer_3000.pth checkpoint_model_3000.pth</code> </pre> <br><h3 id="predskazaniya-obuchennoy-modelyu">  Previsões de um modelo treinado </h3><br><p>  Primeiro, crie um carregador de dados de teste (por exemplo, tire uma amostra de validação) para que o lote de dados consista em imagens e seus índices: </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestDataset</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Dataset)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, ds)</span></span></span><span class="hljs-function">:</span></span> self.ds = ds <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(self.ds) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__getitem__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, index)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.ds[index][<span class="hljs-number"><span class="hljs-number">0</span></span>], index test_dataset = TestDataset(val_dataset) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre> <br><p>  Usando <em>ignite,</em> criaremos um novo mecanismo de previsão para dados de teste.  Para isso, definimos a função <code>inference_update</code> , que retorna o resultado da previsão e o índice da imagem.  Para aumentar a precisão, também usaremos o truque conhecido "aumento do tempo de teste" (TTA). </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite._utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> convert_tensor <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_prepare_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch)</span></span></span><span class="hljs-function">:</span></span> x, index = batch x = convert_tensor(x, device=device) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x, index <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">inference_update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> x, indices = _prepare_batch(batch) y_pred = model(x) y_pred = F.softmax(y_pred, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> {<span class="hljs-string"><span class="hljs-string">"y_pred"</span></span>: convert_tensor(y_pred, device=<span class="hljs-string"><span class="hljs-string">'cpu'</span></span>), <span class="hljs-string"><span class="hljs-string">"indices"</span></span>: indices} model.eval() inferencer = Engine(inference_update)</code> </pre> <br><p>  Em seguida, crie manipuladores de eventos que notificarão sobre o estágio das previsões e salvem as previsões em uma matriz dedicada: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@inferencer.on(Events.EPOCH_COMPLETED) def log_tta(engine): print("TTA {} / {}".format(engine.state.epoch, n_tta)) n_tta = 3 num_classes = 81 n_samples = len(val_dataset) #     y_probas_tta = np.zeros((n_samples, num_classes, n_tta), dtype=np.float32) @inferencer.on(Events.ITERATION_COMPLETED) def save_results(engine): output = engine.state.output tta_index = engine.state.epoch - 1 start_index = ((engine.state.iteration - 1) % len(test_loader)) * batch_size end_index = min(start_index + batch_size, n_samples) batch_y_probas = output['y_pred'].detach().numpy() y_probas_tta[start_index:end_index, :, tta_index] = batch_y_probas</span></span></code> </pre> <br><p>  Antes de iniciar o processo, vamos baixar o melhor modelo: </p><br><pre> <code class="python hljs">model = squeezenet1_1(pretrained=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">64</span></span>) model.classifier[<span class="hljs-number"><span class="hljs-number">-1</span></span>] = nn.AdaptiveAvgPool2d(<span class="hljs-number"><span class="hljs-number">1</span></span>) model = model.to(device) model_state_dict = torch.load(<span class="hljs-string"><span class="hljs-string">"best_models/model_best_model_10_val_accuracy=0.8730994.pth"</span></span>) model.load_state_dict(model_state_dict)</code> </pre> <br><p>  Lançamos: </p><br><pre> <code class="python hljs">inferencer.run(test_loader, max_epochs=n_tta) &gt; TTA <span class="hljs-number"><span class="hljs-number">1</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; TTA <span class="hljs-number"><span class="hljs-number">2</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; TTA <span class="hljs-number"><span class="hljs-number">3</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  Em seguida, de maneira padrão, tomamos a média das previsões de TTA e calculamos o índice de classe com a maior probabilidade: </p><br><pre> <code class="python hljs">y_probas = np.mean(y_probas_tta, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) y_preds = np.argmax(y_probas, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  E agora podemos mais uma vez calcular a precisão do modelo de acordo com as previsões: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score y_test_true = [y <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> val_dataset] accuracy_score(y_test_true, y_preds) &gt; <span class="hljs-number"><span class="hljs-number">0.9310369676443035</span></span></code> </pre> <br><p> ,     ,          .   ,   ,      ,    <em>ignite</em>      . </p><br><h2 id="drugie-primery-s-ignitehttpspytorchorgignite">    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a> </h2><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> . </p><br><p>  github      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>       </p><br><ul><li> fast neural transfer </li><li> reinforcement learning </li><li> dcgan </li></ul><br><h2 id="zaklyuchenie">  Conclusão </h2><br><p>    ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ignite</a>      Facebook           (.   ).        0.1.0,   API (Engine, State, Events, Metric, ...)           .       ,      ,     ,     pull request-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> github</a> . </p><br><p>  Obrigado pela atenção! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt424781/">https://habr.com/ru/post/pt424781/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt424767/index.html">Nós fazemos um bolo de Habr. Novamente</a></li>
<li><a href="../pt424771/index.html">Experiência pessoal: de uma ideia e uma folha em branco a uma versão de rascunho de um site</a></li>
<li><a href="../pt424773/index.html">Biofarma e modelagem numérica: experiência e prática da Amgen</a></li>
<li><a href="../pt424777/index.html">Usando o cônsul para dimensionar serviços estatais</a></li>
<li><a href="../pt424779/index.html">SPA de várias páginas em Python</a></li>
<li><a href="../pt424787/index.html">Entrevista com Aaron Patterson, presidente da Conferência RubyRussia 2018</a></li>
<li><a href="../pt424789/index.html">Como implantar um aplicativo Ruby on Rails com o HAProxy Ingress, unicorn / puma e soquetes da web</a></li>
<li><a href="../pt424791/index.html">Expandindo os recursos de rede de um relé programável usando WI-FI</a></li>
<li><a href="../pt424793/index.html">Como imprimir um motor elétrico</a></li>
<li><a href="../pt424795/index.html">Qual o tamanho de um drone movido a energia solar?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>