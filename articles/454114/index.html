<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìÄ ü§≠ üîé Experiencia con Starwind VSAN y EMC ScaleIO (VxFlexOS) + hoja de trucos para almacenamiento mini Enterprise (1 parte) üòÇ üèΩ üëó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A veces se hace necesario organizar un almacenamiento tolerante a fallas de peque√±os vol√∫menes de almacenamiento de hasta 20Tb, pero con funcionalidad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Experiencia con Starwind VSAN y EMC ScaleIO (VxFlexOS) + hoja de trucos para almacenamiento mini Enterprise (1 parte)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454114/">  A veces se hace necesario organizar un almacenamiento tolerante a fallas de peque√±os vol√∫menes de almacenamiento de hasta 20Tb, pero con funcionalidad Enterprise: All-Flash, cach√© SSD, MPIO, HA (Activ-Activ) y todo esto con un precio econ√≥mico.  Las soluciones de hardware listas para usar con estas funciones comienzan con cientos de terabytes y precios de 8 o m√°s letreros en rublos.  Tener un peque√±o presupuesto de 6-7 personajes en el r√≠o.  y la necesidad de un almacenamiento peque√±o y r√°pido (pero confiable), desde 2009 se han probado y puesto en funcionamiento comercial dos versiones de sistemas de almacenamiento (lo com√∫n con estos sistemas es que son sistemas altamente confiables sin un solo punto de falla + puedes tocarlos antes de comprarlos o "prescindir de ellos" (GRATIS)). <br><br>  A qui√©n le interesa esta experiencia, a continuaci√≥n se describir√° lo siguiente: <br><br><ol><li>  Experiencia de software <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">StarWind Virtual SAN (VSAN)</a> . </li><li>  C√≥mo hacer un peque√±o almacenamiento empresarial. </li><li>  Historial de overclocking de IOPS (pr√°ctica). </li><li>  Hoja de trucos para la implementaci√≥n y operaci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sistemas</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">almacenamiento EMC ScaleIO (VxFlexOS)</a> (en ausencia de soporte t√©cnico por parte de los especialistas de "NOT Linux-guru") 1 parte. </li></ol><a name="habracut"></a><br><h2>  1. Experiencia operativa StarWind Virtual SAN (VSAN) Software </h2><br>  <b>StarWind Virtual SAN (VSAN)</b> : en la soluci√≥n Activ-Activ (replicaci√≥n sincr√≥nica en 3 servidores), en funcionamiento desde 2009-2016 en diferentes ediciones (Starwind ISCSI SAN HA-3) basada en servidores con matrices RAID de hardware. <br><br>  <i>Pros</i> : <br><br><ul><li>  F√°cil y r√°pido, ni siquiera instalado por un profesional; </li><li>  MPIO sobre iSCSI Ethernet; </li><li>  HA (Activ-Activ); </li><li>  En los servidores nuevos (de garant√≠a) (con discos nuevos), puede olvidarse de mantener el almacenamiento durante varios a√±os (los usuarios ni siquiera notar√°n la falla de dos de los tres servidores); </li><li>  Vol√∫menes de cach√© de RAM y SSD; </li><li>  R√°pido Sincronizaci√≥n r√°pida para interrupciones menores de la red. </li></ul><br>  <i>Contras</i> : <br><br><ul><li>  Anteriormente, solo hab√≠a una versi√≥n para la plataforma Windows; </li><li>  Con operaci√≥n a largo plazo (m√°s de 3 a√±os): es dif√≠cil encontrar una unidad para reemplazar una fallida (fuera de producci√≥n) para reparar una matriz RAID (con discos heterog√©neos, pueden ocurrir fallas de la matriz); </li><li>  Un aumento en el n√∫mero de interfaces de red y las ranuras PCI ocupadas por ellas (adicionalmente para sincronizaci√≥n, tarjetas de red, conmutadores); </li><li>  Cuando se utiliza el "sistema de archivos de registro en diario" LSFS, el apagado prolongado del sistema, que puede ser perjudicial cuando el UPS se activa cuando se apaga la alimentaci√≥n; </li><li>  Un tiempo muy largo de sincronizaci√≥n completa con un gran volumen. </li></ul><br>  <i>Quiz√°s problemas ya resueltos</i> (ocurridos anteriormente durante la operaci√≥n en nuestro centro de datos): <br><br><ul><li>  Cuando la matriz RAID colapsa, el servidor permanece visible a trav√©s de la sincronizaci√≥n y el canal de datos, pero el disco en el servidor de Windows est√° desconectado, el registro de Starwind se infla y la memoria del servidor se consume, como resultado de la congelaci√≥n del servidor.  Posible tratamiento: asignaci√≥n de un archivo de control y eliminaci√≥n de mensajes no cr√≠ticos de la configuraci√≥n del registro. </li><li>  Si el conmutador o las interfaces de red fallan, una elecci√≥n ambigua del servidor host (a veces sucedi√≥, el sistema no pudo entender con qui√©n sincronizar). </li></ul><br>  <i>Noticias √∫tiles</i> (a√∫n no probadas): <br>  StarWind Virtual SAN para vSphere (soluci√≥n hiperconvergente), le permite incrustar la virtualizaci√≥n de Vmware en un cl√∫ster sin vincular a los servidores de Windows (basados ‚Äã‚Äãen m√°quinas virtuales Linux). <br><br>  <i><b>Resumen</b></i> : Una soluci√≥n tolerante a fallas si hay un programa de reemplazo de servidor de hardware normal al final de la garant√≠a y el soporte t√©cnico de StarWindSoftWare est√° disponible. <br><br><h2>  2. C√≥mo hacer un peque√±o almacenamiento empresarial </h2><br>  <b>Declaraci√≥n del problema:</b> <br><br>  Cree una red de almacenamiento de datos de peque√±o volumen a prueba de fallas con un total de 4 TB-20TB, con operaci√≥n garantizada a mediano plazo sin costos financieros adicionales significativos. <br><br><ul><li>  El sistema debe ser tolerante a fallas (transfiera con calma la falla de al menos un conmutador, un servidor, discos y tarjetas de red en el servidor). </li><li>  Aproveche al m√°ximo todos los recursos de la flota de servidores de hardware disponible (servidores y conmutadores de 3 a 10 a√±os). </li><li>  Asegure el funcionamiento de vol√∫menes de diferentes niveles: All-Flash y HDD + cach√© SSD. </li></ul><br>  <b>Datos de origen:</b> <br><br><ul><li>  presupuesto limitado; </li><li>  equipo de generaci√≥n hace 3-10 a√±os; </li><li>  Especialistas: no Linux-Guru. </li></ul><br>  <b>C√°lculo de caracter√≠sticas.</b> <br><br>  Para evitar cuellos de botella en el rendimiento cuando se usan discos SSD, que ser√°n cortados por algo de la cadena del equipo: tarjetas de red, controlador RAID (HBA), expansor (cesta), discos. <br><br>  Es necesario en el momento de la creaci√≥n proporcionar, en funci√≥n de sus caracter√≠sticas requeridas, una determinada configuraci√≥n del equipo. <br><br>  Por supuesto, puede ejecutar una configuraci√≥n con SSD de almacenamiento en cach√© de disco duro SAS en redes de 1 Gb / sy controladores 3G, pero el resultado ser√° 3-7 veces peor que en redes RAID de 6 Gb y 10 Gb / s (verificado por pruebas). <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Las instrucciones de ajuste de VxFlexOS</a> describen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">instrucciones</a> simples para calcular el ancho de banda necesario, basado en las estimaciones SSD -450 MB / C y HDD -100 MB / C, para la grabaci√≥n secuencial (por ejemplo, cuando el servidor se reequilibra y se reconstruye). <br><br><img src="https://habrastorage.org/webt/ia/-h/kl/ia-hklgcn0f0i_t6126vqpondoi.jpeg"><br>  Por ejemplo: <br><br><ul><li>  (SSD cach√© + 3 HDD), obtenemos ((450 * 1) + (3 * 100)) * 8/1000 = 6GB </li><li>  (TODO FLASH SSD) + (cach√© SSD + 3 HDD) ((450 * 2) + (3 * 100)) * 8/1000 = 9.6 GB </li></ul><br>  Para determinar el ancho de banda de la red por IOPS (carga est√°ndar en servidores de bases de datos y servidores virtuales cargados), hay una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tabla indicativa de StariWindSoftware</a> <br><br><img src="https://habrastorage.org/webt/uc/xm/qf/ucxmqfcjkdfvg2qyfzyuofsso6e.jpeg"><br>  <b>Configuraci√≥n final</b> : <br><br><ul><li>  Software de almacenamiento, que puede no combinar discos en matrices RAID, sino transferirlos al almacenamiento en forma de discos separados (para que no haya problemas al reemplazar discos despu√©s de un cierto per√≠odo de tiempo cuando fallan, simplemente selecci√≥nelos por capacidad); </li><li>  Servidores de generaci√≥n de procesadores e55xx-x56xx y superior, buses pci-express v 2.0 y superior, controladores Raid (HBA) 6G-12G con memoria, cestas de expansi√≥n para 6-16 discos; </li><li>  Interruptores SMB 10G Layer 2 (MARCO JUMBO, LACP). </li></ul><br>  <b>M√©todo de soluci√≥n</b> <br><br>  Por el momento, no se han encontrado opciones de presupuesto para un "almacenamiento de peque√±a empresa empresarial" de un peque√±o volumen con los requisitos anteriores. <br><br>  Nos detuvimos en soluciones de software que le permiten aprovechar Enterprise Storage, con la opci√≥n de usar servidores existentes, que en este caso tienen derecho a morir de vejez sin comprometer el almacenamiento. <br><br><ul><li>  Ceph: no hay suficientes especialistas en Linux; </li><li>  EMC ScaleIO, por un par de a√±os de soporte t√©cnico, puede sobrevivir con el personal existente. </li><li>  (como result√≥, el conocimiento en Linux puede ser m√≠nimo, m√°s sobre eso m√°s adelante en la hoja de trucos). </li></ul><br><h2>  3. Historia del overclocking de IOPS (pr√°ctica presupuestaria) </h2><br>  Para acelerar las operaciones de lectura y escritura en los sistemas de almacenamiento, se utilizaron los siguientes dispositivos SSD: <br><br>  3.1.  Controladores con caracter√≠sticas de almacenamiento en cach√© SSD. <br><br>  En 2010, aparecieron los controladores RAID con funciones de almacenamiento en cach√© Adaptec 5445 SSD con un disco <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MaxIQ</a> (para un resultado tangible, ten√≠a que tener al menos el 10% del disco MaxIQ del volumen del volumen almacenado en cach√©), el resultado es insignificante * probado en uno mismo; <br>  M√°s tarde, hubo controladores que pueden usar un disco SSD arbitrario para el almacenamiento en cach√©, tanto la serie Q de Adaptec como la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CacheCade LSI</a> (pero las licencias est√°n separadas all√≠); <br><br>  3.2.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Almacenamiento en</a> cach√© de software utilizando discos, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Intel DC S3700</a> , que son vistos por el controlador y el expansor de los servidores de servidores HP, IBM, FUJI de marca (la mayor√≠a de los servidores los reconocen con √©xito, costosos para All-Flash, pero para el 10% en cach√© SSD es tolerable no liberarlos bajo socios de IBM, HP, FUJI y solo Intel).  * Pero ahora hay opciones compatibles m√°s baratas (ver p√°rrafo 3.5.); <br><br>  3.3.  Se verifica el almacenamiento en cach√© del software utilizando el adaptador PCIe-M.2, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Synology M.2 M2D18 SSD</a> , funciona en servidores normales (no solo en Synology), es √∫til cuando el controlador RAID y la cesta se niegan a ver los SSD que el fabricante no indic√≥ en los compatibles (n HP D2700)?  *; <br><br>  3.4.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Unidades</a> h√≠bridas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Seagate EXOS</a>  600Gb Seagate Exos 10E2400 (ST600MM0099) {SAS 12Gb / s, 10000rpm, 256Mb, 2.5 "}, * verificado reconocido por los servidores HP, IBM, FUJI (alternativa a las versiones 3.1.-3.3.); <br><br>  3.5.  Unidades SSD con un gran recurso y precio comparable con SAS de clase empresarial, <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Crucial Micron 5200 MAX</a> MTFDDAK480TDN-1AT1ZABYY, * verificado reconocido por los servidores HP, IBM, FUJI <br>  (una alternativa para reemplazar discos HDD con aquellos compatibles con la cl√°usula 3.4 y con servidores SAS antiguos: SAS2.5 "600GB AL14SEB060N TOSHIBA * disco duro *, <br>  C10K1800 0B31229 HGST, ST600MM0099 SEAGATE).  Permite que un presupuesto cambie de HDD + SSD a vol√∫menes All-Flash. <br><br><h2>  4. Cheat sheet para la implementaci√≥n y operaci√≥n del almacenamiento EMC ScaleIO (VxFlexOS) 1 parte </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Almacenamiento EMC ScaleIO (VxFlexOS)</a> <br><br>  Despu√©s de probar la soluci√≥n antes de la compra, llegu√© a la conclusi√≥n de que para el funcionamiento normal del sistema se necesitan m√°s de 3 nodos (la conmutaci√≥n por error es inestable en 3), por ejemplo, tome una configuraci√≥n de 8 servidores (sobrevivir√° a la falla secuencial de 4 servidores sin perder vol√∫menes). <br><br>  <i>Pieza de hardware</i> : <br><br>  FUJI CX2550M1 (E5-2xxx) - 3 piezas  (VmWare VSphere + ScaleIO servidor de virtualizaci√≥n del n√∫cleo del cl√∫ster cliente SDC y servidor SDS); <br>  +5 servidores de generaci√≥n HP G6 (G7) o IBM M3 (e55xx-x56xx) - servidores ScaleIO SDS; <br>  + 2 conmutadores NetGear XS712T-100NES <br><br>  Al ejecutar el almacenamiento en modo RFCache, pude overclockear a 44KIops usando Iometer <br><br><img src="https://habrastorage.org/webt/ro/vg/19/rovg1982umabe29n5vqa6_9erpi.jpeg"><br><br>  Configuraci√≥n de almacenamiento: <br><br>  Capacidad bruta de 12 TB (licencia m√≠nima en el momento en que todav√≠a se vend√≠a como software) <br><br><img src="https://habrastorage.org/webt/vw/os/4e/vwos4eljcmjyrnk8drmccc9vsoa.jpeg"><br><br>  8 servidores SDS 28 unidades <br><br><img src="https://habrastorage.org/webt/gd/-y/5j/gd-y5j3wve2zmum8vexmkbwhmu8.jpeg"><br><br>  Leer RAM cach√© 14 Gb <br><br><img src="https://habrastorage.org/webt/uo/-j/ck/uo-jck7jbbvfdbjvap9emzkzjvu.jpeg"><br><br>  Leer Flash Cashe 1.27 TB (RFCashe) <br><br><img src="https://habrastorage.org/webt/2x/y2/km/2xy2kmosvvldbniqfv6sr_6hykw.jpeg"><br><br>  En la versi√≥n intermedia, donde solo 3 servidores de 2x10Gb tienen tarjetas de red, en los 2 x1Gb restantes. <br><br><img src="https://habrastorage.org/webt/vh/k8/xc/vhk8xcff6fqdhzp2347gmgtksuu.jpeg"><br>  Se ve claramente que incluso con el almacenamiento en cach√© SSD a 1 Gb en lugar de 10 Gb, hay una p√©rdida de ancho de banda SDS tres veces o m√°s, con medios id√©nticos. <br><br>  Sin almacenamiento en cach√©, si considera de acuerdo con estos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"est√°ndares",</a> entonces con 28 discos duros, <br>  obtenemos 28X140 = 3920 IOPS, es decir  para obtener 44,000 IOPS, necesitar√≠as 11 veces m√°s discos.  Es econ√≥micamente m√°s rentable para requisitos de peque√±o volumen, no para aumentar el n√∫mero de discos, sino para el cach√© SSD. <br><br>  A la pregunta de por qu√© tales velocidades con un volumen peque√±o, ¬°responder√© de inmediato! <br><br>  Existen organizaciones tan peque√±as (como la nuestra) en las que hay una gran cantidad de documentos electr√≥nicos que se procesan en el software durante mucho tiempo (cada registro controla el env√≠o del software hasta 1 hora, incluso en este almacenamiento overclockeado).  Todas las dem√°s opciones ya se han aplicado anteriormente (aumento en RM-RAM, CPU i5, SSD, 1Gb-NET).  Incluso el uso de solo paquetes SSD + SAS en el almacenamiento (sin ALL-Flash hasta ahora) permiti√≥ utilizar la mayor√≠a de los recursos de los servidores de virtualizaci√≥n, transfiriendo m√°quinas virtuales cargadas a ScaleIO, duplicando la carga en los procesadores FUJI CX400M1 (anteriormente reten√≠a el almacenamiento). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/454114/">https://habr.com/ru/post/454114/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../454100/index.html">Problemas comunes de c√≥digo en microservicios</a></li>
<li><a href="../454102/index.html">Usando un or√°culo al azar en el ejemplo de una loter√≠a</a></li>
<li><a href="../454104/index.html">Servicios en la nube para jugar en PC d√©biles, relevantes en 2019</a></li>
<li><a href="../454110/index.html">Desarrollo de una tienda en l√≠nea para preservar la naturaleza de Kamchatka</a></li>
<li><a href="../454112/index.html">Historia de dise√±o de nivel de Duke Nukem (con bocetos de Levelord)</a></li>
<li><a href="../454124/index.html">El libro "Aprendiendo a codificar en JavaScript"</a></li>
<li><a href="../454126/index.html">Desde accidentes diarios hasta estabilidad: Informatica con 10 ojos administrativos</a></li>
<li><a href="../454128/index.html">C√≥mo hacer dos aplicaciones de una. Tinkoff Junior Experience</a></li>
<li><a href="../454130/index.html">C-V2X con soporte para redes 5G NR: un nuevo paradigma para el intercambio de datos entre veh√≠culos</a></li>
<li><a href="../454132/index.html">Video vigilancia en orange pi zero - barato y nada enojado</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>