<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äç‚ù§Ô∏è‚Äçüë© ‚å®Ô∏è ‚èØÔ∏è Comment nous avons appris √† exploiter Java dans Docker üë®üèª‚Äçüè´ üßîüèΩ üë®üèΩ‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sous le capot, hh.ru contient un grand nombre de services Java ex√©cut√©s dans des conteneurs Docker. Pendant leur fonctionnement, nous avons rencontr√© ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons appris √† exploiter Java dans Docker</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/hh/blog/450954/">  Sous le capot, hh.ru contient un grand nombre de services Java ex√©cut√©s dans des conteneurs Docker.  Pendant leur fonctionnement, nous avons rencontr√© beaucoup de probl√®mes non triviaux.  Dans de nombreux cas, pour aller au fond de la solution, il a fallu longtemps sur Google, lire les sources OpenJDK et m√™me profiler les services en production.  Dans cet article, je vais essayer de transmettre la quintessence des connaissances acquises au cours du processus. <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Limites du processeur</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Docker et machine de classe serveur</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Limites du processeur (oui, encore une fois) et fragmentation de la m√©moire</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nous traitons Java-OOM</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Optimiser la consommation de m√©moire</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Limiter la consommation de m√©moire: tas, non-tas, m√©moire directe</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Limiter la consommation de m√©moire: suivi de la m√©moire native</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Java et lecteurs</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment garder une trace de tout?</a> </li></ul><br><a name="habracut"></a><a name="cpu"></a><h4>  Limites du processeur </h4><br>  Nous vivions dans des machines virtuelles kvm avec des limitations de CPU et de m√©moire et, en passant √† Docker, nous avons d√©fini des restrictions similaires dans les groupes de contr√¥le.  Et le premier probl√®me que nous avons rencontr√© √©tait pr√©cis√©ment les limites du processeur.  Je dois dire tout de suite que ce probl√®me n'est plus pertinent pour les versions r√©centes de Java 8 et Java ‚â• 10. Si vous suivez les temps, vous pouvez sauter cette section en toute s√©curit√©. <br><br>  Donc, nous d√©marrons un petit service dans le conteneur et voyons qu'il produit un grand nombre de threads.  Ou le CPU consomme beaucoup plus que pr√©vu, timeout combien en vain.  Ou voici une autre situation r√©elle: sur une machine, le service d√©marre normalement, et sur une autre, avec les m√™mes param√®tres, il se bloque, clou√© par un tueur OOM. <br><br>  La solution s'av√®re tr√®s simple - juste Java ne voit pas les limitations de <code>--cpus</code> d√©finies dans le docker et pense que tous les noyaux de la machine h√¥te lui sont accessibles.  Et il peut y en avoir beaucoup (dans notre configuration standard - 80). <br>  Les biblioth√®ques ajustent la taille des pools de threads au nombre de processeurs disponibles - d'o√π le grand nombre de threads. <br>  Java lui-m√™me fait √©voluer le nombre de threads GC de la m√™me mani√®re, d'o√π la consommation du processeur et les d√©lais d'expiration - le service commence √† d√©penser une grande quantit√© de ressources pour la collecte des ordures, en utilisant la part du lion du quota qui lui est allou√©. <br>  De plus, les biblioth√®ques (Netty en particulier) peuvent, dans certains cas, ajuster la taille de la m√©moire off-hip au nombre de CPU, ce qui conduit √† une forte probabilit√© de d√©passer les limites d√©finies pour le conteneur lors de l'ex√©cution sur un mat√©riel plus puissant. <br><br>  Au d√©but, comme ce probl√®me s'est manifest√©, nous avons essay√© d'utiliser les cycles de travail suivants: <br>  - essay√© d'utiliser quelques services <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">libnumcpus</a> - une biblioth√®que qui vous permet de "tromper" Java en d√©finissant un nombre diff√©rent de processeurs disponibles; <br>  - a indiqu√© explicitement le nombre de threads GC, <br>  - fixer explicitement des limites √† l'utilisation de tampons d'octets directs. <br><br>  Mais, bien s√ªr, se d√©placer avec de telles b√©quilles n'est pas tr√®s pratique, et le passage √† Java 10 (puis √† Java 11), dans lequel tous ces probl√®mes sont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">absents</a> , √©tait une vraie solution.  En toute honn√™tet√©, il convient de dire que dans les huit aussi, tout allait bien avec la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mise √† jour 191</a> , publi√©e en octobre 2018.  √Ä ce moment-l√†, c'√©tait d√©j√† hors de propos pour nous, ce que je vous souhaite √©galement. <br><br>  C'est un exemple o√π la mise √† jour de la version Java apporte non seulement une satisfaction morale, mais aussi un r√©el b√©n√©fice tangible sous la forme d'un fonctionnement simplifi√© et de performances de service accrues. <br><br><a name="server-class"></a><h4>  Docker et machine de classe serveur </h4><br>  Ainsi, dans Java 10, les <code>-XX:ActiveProcessorCount</code> et <code>-XX:+UseContainerSupport</code> sont apparues (et ont √©t√© r√©troport√©es vers Java 8), en tenant compte des limites par d√©faut des <code>-XX:+UseContainerSupport</code> .  Maintenant, tout √©tait merveilleux.  Ou pas? <br><br>  Quelque temps apr√®s notre passage √† Java 10/11, nous avons commenc√© √† remarquer quelques bizarreries.  Pour une raison quelconque, dans certains services, les graphiques GC semblaient ne pas utiliser G1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gc/69/bo/gc69boghbrkf2wmxiuioszq5qgg.png"></div><br><br>  C'√©tait, pour le moins, un peu inattendu, car nous savions avec certitude que G1 est le collecteur par d√©faut, √† commencer par Java 9. En m√™me temps, il n'y a pas un tel probl√®me dans certains services - G1 est activ√©, comme pr√©vu. <br><br>  Nous commen√ßons √† comprendre et √† tomber sur une <a href="">chose int√©ressante</a> .  Il s'av√®re que si Java fonctionne sur moins de 3 processeurs et avec une limite de m√©moire inf√©rieure √† 2 Go, il se consid√®re comme client et ne permet pas d'utiliser autre chose que SerialGC. <br><br>  Soit dit en passant, cela n'affecte que le <a href="">choix de GC</a> et n'a rien √† voir avec les options de compilation -client / -server et JIT. <br><br>  √âvidemment, lorsque nous avons utilis√© Java 8, il ne tenait pas compte des limites des dockers et pensait qu'il avait beaucoup de processeurs et de m√©moire.  Apr√®s la mise √† niveau vers Java 10, de nombreux services avec des limites plus basses ont soudainement commenc√© √† utiliser SerialGC.  Heureusement, cela est trait√© tr√®s simplement - en d√©finissant explicitement l' <code>-XX:+AlwaysActAsServerClassMachine</code> . <br><br><a name="malloc"></a><h4>  Limites du processeur (oui, encore une fois) et fragmentation de la m√©moire </h4><br>  En regardant les graphiques de surveillance, nous avons en quelque sorte remarqu√© que la taille de l'ensemble r√©sident du conteneur est trop grande - jusqu'√† trois fois plus que la taille maximale de la hanche.  Cela pourrait-il √™tre le cas dans un autre m√©canisme d√©licat qui √©volue en fonction du nombre de processeurs dans le syst√®me et ne conna√Æt pas les limites du docker? <br><br>  Il s'av√®re que le m√©canisme n'est pas du tout d√©licat - c'est le malloc bien connu de la glibc.  En bref, la glibc utilise les soi-disant ar√®nes pour allouer de la m√©moire.  Lors de la cr√©ation, chaque thread est affect√© √† l'une des ar√®nes.  Lorsqu'un thread utilisant glibc souhaite allouer une certaine quantit√© de m√©moire dans le tas natif √† ses besoins et appelle malloc, la m√©moire est allou√©e dans l'ar√®ne qui lui est affect√©e.  Si l'ar√®ne sert plusieurs threads, ces threads rivaliseront pour cela.  Plus il y a d'ar√®nes, moins il y a de comp√©tition, mais plus il y a de fragmentation, car chaque ar√®ne a sa propre liste de zones libres. <br><br>  Sur les syst√®mes 64 bits, le nombre d'ar√®nes par d√©faut est d√©fini sur 8 * le nombre de CPU.  De toute √©vidence, cela repr√©sente une √©norme surcharge pour nous, car tous les processeurs ne sont pas disponibles pour le conteneur.  De plus, pour les applications bas√©es sur Java, la concurrence pour les ar√®nes n'est pas aussi pertinente, car la plupart des allocations se font en Java-tas, dont la m√©moire peut √™tre enti√®rement allou√©e au d√©marrage. <br><br>  Cette fonctionnalit√© de malloc est connue depuis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tr√®s</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">longtemps</a> , ainsi que sa solution - utiliser la variable d'environnement <code>MALLOC_ARENA_MAX</code> pour indiquer explicitement le nombre d'ar√®nes.  C'est tr√®s facile √† faire pour n'importe quel conteneur.  Voici l'effet de la sp√©cification <code>MALLOC_ARENA_MAX = 4</code> pour notre backend principal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jk/zq/lo/jkzqlo_pqiu4xppzbe-itkjvviy.png"></div><br><br>  Il y a deux exemples sur le graphique RSS: dans l'un (bleu) nous <code>MALLOC_ARENA_MAX</code> , dans l'autre (rouge) nous venons de red√©marrer.  La diff√©rence est √©vidente. <br><br>  Mais apr√®s cela, il y a un d√©sir raisonnable de comprendre sur quoi Java d√©pense g√©n√©ralement la m√©moire.  Est-il possible d'ex√©cuter un microservice sur Java avec une limite de m√©moire de 300 √† 400 m√©gaoctets et de ne pas avoir peur qu'il tombe de Java-OOM ou ne soit pas tu√© par un tueur OOM syst√®me? <br><br><a name="oom"></a><h4>  Nous traitons Java-OOM </h4><br>  Tout d'abord, vous devez vous pr√©parer au fait que les MOO sont in√©vitables et vous devez les g√©rer correctement - au moins √©conomiser les d√©charges de la hanche.  Curieusement, m√™me cette simple entreprise a ses propres nuances.  Par exemple, les vidages de hanche ne sont pas remplac√©s - si un vidage de hanche avec le m√™me nom est d√©j√† enregistr√©, alors un nouveau ne sera tout simplement pas cr√©√©. <br><br>  Java peut <a href="">ajouter automatiquement le</a> num√©ro de s√©rie de vidage et l'ID de processus au nom de fichier, mais cela ne nous aidera pas.  Le num√©ro de s√©rie n'est pas utile, car il s'agit de MOO, et non du vidage de hanche r√©guli√®rement demand√© - l'application red√©marre apr√®s, r√©initialisant le compteur.  Et l'ID de processus ne convient pas, car dans Docker, il est toujours le m√™me (le plus souvent 1). <br><br>  Par cons√©quent, nous sommes arriv√©s √† cette option: <br><br> <code>-XX:+HeapDumpOnOutOfMemoryError <br> -XX:+ExitOnOutOfMemoryError <br> -XX:HeapDumpPath=/var/crash/java.hprof <br> -XX:OnOutOfMemoryError="mv /var/crash/java.hprof /var/crash/heapdump.hprof"</code> <br> <br>  C'est assez simple et avec quelques am√©liorations, vous pouvez m√™me apprendre √† le stocker non seulement la derni√®re version de la hanche, mais pour nos besoins, c'est plus que suffisant. <br><br>  Java OOM n'est pas la seule chose √† laquelle nous devons faire face.  Chaque conteneur a une limite sur la m√©moire qu'il occupe et elle peut √™tre d√©pass√©e.  Si cela se produit, le conteneur est tu√© par le tueur OOM du syst√®me et red√©marre (nous utilisons <code>restart_policy: always</code> ).  Naturellement, cela n'est pas souhaitable et nous voulons apprendre √† d√©finir correctement les limites des ressources utilis√©es par la JVM. <br><br><a name="opt-mem"></a><h4>  Optimiser la consommation de m√©moire </h4><br>  Mais avant de fixer des limites, vous devez vous assurer que la JVM ne gaspille pas les ressources.  Nous avons d√©j√† r√©ussi √† r√©duire la consommation de m√©moire en utilisant une limite sur le nombre de CPU et la variable <code>MALLOC_ARENA_MAX</code> .  Existe-t-il d'autres moyens ¬´presque gratuits¬ª de proc√©der? <br><br>  Il s'av√®re qu'il y a quelques astuces suppl√©mentaires qui permettront d'√©conomiser un peu de m√©moire. <br><br>  Le premier est l'utilisation de l' <code>-Xss</code> (ou <code>-XX:ThreadStackSize</code> ), qui contr√¥le la taille de la pile pour les threads.  La valeur par d√©faut pour une JVM 64 bits est de 1 Mo.  Nous avons d√©couvert que 512 Ko nous suffisaient.  Pour cette raison, une StackOverflowException n'a jamais √©t√© d√©tect√©e auparavant, mais j'avoue que cela ne convient pas √† tout le monde.  Et le b√©n√©fice de cela est tr√®s faible. <br><br>  Le second est l' <code>-XX:+UseStringDeduplication</code> (avec G1 GC activ√©).  Il vous permet d'√©conomiser de la m√©moire en r√©duisant les lignes en double en raison de la charge suppl√©mentaire du processeur.  Le compromis entre la m√©moire et le processeur d√©pend uniquement de l'application sp√©cifique et des param√®tres du m√©canisme de d√©duplication lui-m√™me.  Lisez le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dock</a> et testez dans vos services, nous avons cette option n'a pas encore trouv√© son application. <br><br>  Et enfin, une m√©thode qui ne convient pas √† tout le monde (mais qui nous convient) consiste √† utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">jemalloc</a> au lieu du malloc natif.  Cette impl√©mentation vise √† r√©duire la fragmentation de la m√©moire et un meilleur support multithreading par rapport √† malloc de glibc.  Pour nos services, jemalloc a donn√© un peu plus de gain de m√©moire que malloc avec <code>MALLOC_ARENA_MAX=4</code> , sans affecter significativement les performances. <br><br>  D'autres options, notamment celles d√©crites par Alexei Shipilev dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">JVM Anatomy Quark # 12: Native Memory Tracking</a> , semblaient plut√¥t dangereuses ou entra√Ænaient une d√©gradation notable des performances.  Cependant, √† des fins √©ducatives, je recommande de lire cet article. <br><br>  En attendant, passons au sujet suivant et, enfin, essayons d'apprendre √† limiter la consommation de m√©moire et √† s√©lectionner les limites correctes. <br><br><a name="lim-mem-1"></a><h4>  Limiter la consommation de m√©moire: tas, non-tas, m√©moire directe </h4><br>  Pour tout faire correctement, vous devez vous rappeler en quoi consiste la m√©moire en g√©n√©ral en Java.  Examinons d'abord les pools dont l'√©tat peut √™tre surveill√© via JMX. <br><br>  Le premier, bien s√ªr, est la <b>hanche</b> .  C'est simple: nous <code>-Xmx</code> , mais comment le faire correctement?  Malheureusement, il n'y a pas de recette universelle ici, tout d√©pend de l'application et du profil de charge.  Pour les nouveaux services, nous partons d'une taille de segment de m√©moire relativement raisonnable (128 Mo) et, si n√©cessaire, l'augmentons ou la diminuons.  Pour prendre en charge ceux existants, il existe une surveillance avec des graphiques de consommation de m√©moire et des m√©triques GC. <br><br>  En m√™me temps que <code>-Xmx</code> nous d√©finissons <code>-Xms == -Xmx</code> .  Nous n'avons pas de survente de m√©moire, il est donc dans notre int√©r√™t que le service utilise au maximum les ressources que nous lui avons donn√©es.  De plus, dans les services ordinaires, nous incluons <code>-XX:+AlwaysPreTouch</code> et le m√©canisme Transparent Huge Pages: <code>-XX:+UseTransparentHugePages -XX:+UseLargePagesInMetaspace</code> .  Cependant, avant d'activer THP, lisez attentivement la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> et testez le comportement des services avec cette option pendant une longue p√©riode.  Les surprises ne sont pas exclues sur les machines avec une RAM insuffisante (par exemple, nous avons d√ª d√©sactiver le THP sur les bancs de test). <br><br>  Vient ensuite le <b>non-tas</b> .  La m√©moire non segment√©e comprend: <br>  - Metaspace et espace de classe compress√©, <br>  - Cache de code. <br><br>  Consid√©rez ces pools dans l'ordre. <br><br>  Bien s√ªr, tout le monde a entendu parler de <b>Metaspace</b> , je n'en parlerai pas en d√©tail.  Il stocke les m√©tadonn√©es de classe, le bytecode de m√©thode, etc.  En fait, l'utilisation de Metaspace d√©pend directement du nombre et de la taille des classes charg√©es, et vous ne pouvez le d√©terminer, comme hip, qu'en lan√ßant l'application et en supprimant les m√©triques via JMX.  Par d√©faut, Metaspace n'est limit√© par rien, mais il est assez facile de le faire avec l' <code>-XX:MaxMetaspaceSize</code> . <br><br>  <b>L'espace de classe compress√©</b> fait partie de Metaspace et appara√Æt lorsque l'option <code>-XX:+UseCompressedClassPointers</code> est activ√©e (activ√©e par d√©faut pour les tas de moins de 32 Go, c'est-√†-dire lorsqu'elle peut donner un gain de m√©moire r√©el).  La taille de ce pool peut √™tre limit√©e par l'option <code>-XX:CompressedClassSpaceSize</code> , mais cela n'a pas beaucoup de sens, car l'espace de classe compress√© est inclus dans Metaspace et la quantit√© totale de m√©moire verrouill√©e pour Metaspace et l'espace de classe compress√© est finalement limit√©e √† une <code>-XX:MaxMetaspaceSize</code> . <br><br>  Par ailleurs, si vous regardez les lectures JMX, la quantit√© de m√©moire non-tas est toujours calcul√©e comme la <a href="">somme de</a> Metaspace, de l'espace de classe compress√© et du cache de code.  En fait, il vous suffit de r√©sumer Metaspace et CodeCache. <br><br>  Ainsi, dans le non-tas, seul le <b>cache de code</b> est rest√© - le r√©f√©rentiel de code compil√© par le compilateur JIT.  Par d√©faut, sa taille maximale est fix√©e √† 240 Mo et, pour les petits services, elle est plusieurs fois plus importante que n√©cessaire.  La taille du cache de code peut √™tre d√©finie avec l'option <code>-XX:ReservedCodeCacheSize</code> .  La taille correcte ne peut √™tre d√©termin√©e qu'en ex√©cutant l'application et en la suivant sous un profil de charge typique. <br><br>  Il est important de ne pas se tromper ici, car un cache de code insuffisant supprime le code froid et ancien du cache (l' <code>-XX:+UseCodeCacheFlushing</code> activ√©e par d√©faut), ce qui, √† son tour, peut entra√Æner une consommation CPU plus √©lev√©e et une d√©gradation des performances .  Ce serait formidable si vous pouviez lancer un MOO lorsque le cache de code d√©borde, pour cela, il y a m√™me l' <code>-XX:+ExitOnFullCodeCache</code> , mais, malheureusement, il n'est disponible que dans la <a href="">version de d√©veloppement de la</a> JVM. <br><br>  Le dernier pool sur lequel il y a des informations dans JMX est <b>la m√©moire directe</b> .  Par d√©faut, sa taille n'est pas limit√©e, il est donc important de lui fixer une sorte de limite - au moins des biblioth√®ques comme Netty, qui utilisent activement des tampons d'octets directs, seront guid√©es par elle.  Il n'est pas difficile de d√©finir une limite √† l'aide de l' <code>-XX:MaxDirectMemorySize</code> et, encore une fois, seule la surveillance nous aidera √† d√©terminer la valeur correcte. <br><br>  Alors qu'obtenons-nous jusqu'√† pr√©sent? <br><br><pre>  M√©moire de processus Java = 
     Heap + Metaspace + Code Cache + Direct Memory =
         -Xmx +
         -XX: MaxMetaspaceSize +
         -XX: ReservedCodeCacheSize +
         -XX: MaxDirectMemorySize </pre><br><br>  Essayons de tout dessiner sur le graphique et de le comparer avec le conteneur Docker RSS. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pg/ue/fx/pguefx_0kisoyxg8mna7dxlmimo.png"></div><br><br>  La ligne ci-dessus est le RSS du conteneur et elle est une fois et demie sup√©rieure √† la consommation de m√©moire de la JVM, que nous pouvons surveiller via JMX. <br><br>  Creuser plus loin! <br><br><a name="lim-mem-2"></a><h4>  Limiter la consommation de m√©moire: suivi de la m√©moire native </h4><br>  Bien s√ªr, en plus de la m√©moire tas, non tas et directe, la JVM utilise tout un tas d'autres pools de m√©moire.  L'indicateur <code>-XX:NativeMemoryTracking=summary</code> nous aidera √† les <code>-XX:NativeMemoryTracking=summary</code> .  En activant cette option, nous pourrons obtenir des informations sur les pools connus de la JVM, mais non disponibles dans JMX.  Vous pouvez en savoir plus sur l'utilisation de cette option dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> . <br><br>  Commen√ßons par le plus √©vident - la m√©moire occup√©e par les <b>piles de threads</b> .  NMT produit quelque chose comme ce qui suit pour notre service: <br><br><pre>  Thread (r√©serv√© = 32166 Ko, engag√© = 5358 Ko)
     (fil # 52)
     (pile: r√©serv√© = 31920 Ko, engag√© = 5112 Ko)
     (malloc = 185 Ko # 270) 
     (ar√®ne = 61 Ko # 102) </pre><br>  Soit dit en passant, sa taille peut √©galement √™tre trouv√©e sans suivi de la m√©moire native, en utilisant jstack et en creusant un peu dans <code>/proc/&lt;pid&gt;/smaps</code> .  Andrey Pangin a pr√©sent√© une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">utilit√© sp√©ciale</a> pour cela. <br><br>  La taille de l' <b>espace de classe partag√©</b> est encore plus facile √† √©valuer: <br><br><pre>  Espace de classe partag√© (r√©serv√© = 17084 Ko, engag√© = 17084 Ko)
     (mmap: r√©serv√© = 17084 Ko, engag√© = 17084 Ko) </pre><br>  Il s'agit du m√©canisme de partage de donn√©es de classe, <code>-Xshare</code> et <code>-XX:+UseAppCDS</code> .  Dans Java 11, l'option <code>-Xshare</code> est d√©finie sur auto par d√©faut, ce qui signifie que si vous avez l' <code>$JAVA_HOME/lib/server/classes.jsa</code> (elle se trouve dans l'image de docker officielle d'OpenJDK), elle chargera la carte m√©moire- Ohm au d√©marrage de la JVM, acc√©l√©rant le temps de d√©marrage.  Par cons√©quent, la taille de l'espace de classe partag√© est facile √† d√©terminer si vous connaissez la taille des archives jsa. <br><br>  Voici les structures natives du <b>garbage collector</b> : <br><br><pre>  GC (r√©serv√© = 42137 Ko, engag√© = 41801 Ko)
     (malloc = 5705 Ko # 9460) 
     (mmap: r√©serv√© = 36432 Ko, engag√© = 36096 Ko) </pre><br>  Alexey Shipilev dans le manuel d√©j√† mentionn√© sur le suivi de la m√©moire native <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dit</a> qu'ils occupent environ 4 √† 5% de la taille du tas, mais dans notre configuration pour les petits tas (jusqu'√† plusieurs centaines de m√©gaoctets), les frais g√©n√©raux ont atteint 50% de la taille du tas. <br><br>  Beaucoup d'espace peut √™tre occup√© par les <b>tables de symboles</b> : <br><br><pre>  Symbole (r√©serv√© = 16421 Ko, engag√© = 16421 Ko)
     (malloc = 15261 Ko # 203089) 
     (ar√®ne = 1159 Ko # 1) </pre><br>  Ils stockent les noms des m√©thodes, des signatures, ainsi que des liens vers des cha√Ænes internes.  Malheureusement, il semble possible d'estimer la taille de la table des symboles uniquement apr√®s factum √† l'aide du suivi de la m√©moire native. <br><br>  Que reste-t-il?  Selon Native Memory Tracking, beaucoup de choses: <br><br><pre>  Compilateur (r√©serv√© = 509 Ko, valid√© = 509 Ko)
 Interne (r√©serv√© = 1647 Ko, engag√© = 1647 Ko)
 Autre (r√©serv√© = 2110 Ko, engag√© = 2110 Ko)
 Morceau d'ar√®ne (r√©serv√© = 1712 Ko, engag√© = 1712 Ko)
 Journalisation (r√©serv√© = 6 Ko, engag√© = 6 Ko)
 Arguments (r√©serv√©s = 19 Ko, engag√©s = 19 Ko)
 Module (r√©serv√© = 227 Ko, engag√© = 227 Ko)
 Inconnu (r√©serv√© = 32 Ko, engag√© = 32 Ko) </pre><br>  Mais tout cela prend beaucoup de place. <br><br>  Malheureusement, la plupart des zones de m√©moire mentionn√©es ne peuvent √™tre ni limit√©es ni contr√¥l√©es, et si cela √©tait possible, la configuration se transformerait en enfer.  M√™me la surveillance de leur √©tat n'est pas une t√¢che triviale, car l'inclusion du suivi de la m√©moire native draine l√©g√®rement les performances de l'application et l'activer en production dans un service critique n'est pas une bonne id√©e. <br><br>  N√©anmoins, par int√©r√™t, essayons de refl√©ter sur le graphique tout ce que rapporte le suivi de la m√©moire native: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t_/5d/kn/t_5dkncjh0wrn9qmftrp3wgwg2w.png"></div><br><br>  Pas mal!  La diff√©rence restante est une surcharge pour la fragmentation / allocation de m√©moire (elle est tr√®s petite, car nous utilisons jemalloc) ou la m√©moire allou√©e par les biblioth√®ques natives.  Nous utilisons simplement l'un d'entre eux pour un stockage efficace de l'arborescence des pr√©fixes. <br><br>  Donc, pour nos besoins, il suffit de limiter ce que nous pouvons: Heap, Metaspace, Code Cache, Direct Memory.  Pour tout le reste, nous laissons des bases raisonnables, d√©termin√©es par les r√©sultats des mesures pratiques. <br><br>  Apr√®s avoir trait√© le CPU et la m√©moire, nous passons √† la prochaine ressource pour laquelle les applications peuvent rivaliser - les disques. <br><br><a name="disks"></a><h4>  Java et lecteurs </h4><br>  Et avec eux, tout est tr√®s mauvais: ils sont lents et peuvent entra√Æner une matit√© tangible de l'application.  Par cons√©quent, nous d√©lions autant que possible Java des disques: <br><br><ul><li>  Nous √©crivons tous les journaux d'application dans le syslog local via UDP.  Cela laisse une certaine chance que les journaux n√©cessaires seront perdus quelque part en cours de route, mais, comme la pratique l'a montr√©, de tels cas sont tr√®s rares. </li><li>  Nous allons √©crire les journaux JVM dans tmpfs, pour cela nous avons juste besoin de monter le docker √† l'emplacement souhait√© avec le <code>/dev/shm</code> . </li></ul><br><br>  Si nous √©crivons des journaux dans syslog ou dans tmpfs, et que l'application elle-m√™me n'√©crit rien sur le disque, √† l'exception des vidages de la hanche, alors il s'av√®re que l'histoire avec les disques peut √™tre consid√©r√©e comme close √† ce sujet? <br><br>  Bien s√ªr que non. <br><br>  Nous pr√™tons attention au calendrier de la dur√©e des pauses d'arr√™t du monde et nous voyons une image triste - Les pauses d'arr√™t du monde sur les h√¥tes sont des centaines de millisecondes, et sur un h√¥te, elles peuvent m√™me atteindre une seconde: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tf/nd/pb/tfndpbg7mtpaylny7-cflvrukzg.png"></div><br><br>  Inutile de dire que cela affecte n√©gativement l'application?  Voici, par exemple, un graphique refl√©tant le temps de r√©ponse du service selon les clients: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nc/d0/_n/ncd0_ndoybiyy42wzrh68ppj-_0.png"></div><br><br>  Il s'agit d'un service tr√®s simple, pour la plupart donnant des r√©ponses mises en cache, alors d'o√π viennent ces horaires prohibitifs, en commen√ßant par le 95e centile?  D'autres services ont une image similaire, en outre, les d√©lais d'attente pleuvent avec une constance enviable lors de la connexion du pool de connexions √† la base de donn√©es, lors de l'ex√©cution des demandes, etc. <br><br>  Qu'est-ce que le lecteur a √† voir avec √ßa?  - demandez-vous.  Il s‚Äôav√®re beaucoup √† voir avec cela. <br>  Une analyse d√©taill√©e du probl√®me a montr√© que de longues pauses STW se produisent du fait que les threads vont au point de s√©curit√© pendant une longue p√©riode.  Apr√®s avoir lu le code JVM, nous avons r√©alis√© que lors de la synchronisation des threads sur le safepoint, la JVM peut √©crire le fichier <code>/tmp/hsperfdata*</code> via la carte m√©moire, dans laquelle elle exporte des statistiques.  Des utilitaires comme <code>jstat</code> et <code>jps</code> utilisent <code>jstat</code> <code>jps</code> . <br><br>  D√©sactivez-le sur la m√™me machine avec l'option <code>-XX:+PerfDisableSharedMem</code> et ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1r/aw/q7/1rawq7kjvmrjznko2781or7kzdm.png"></div><br><br>  Les mesures de Jetty Treadpool se stabilisent: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qz/bs/pj/qzbspjvpdjfhjjtbwrn6et55wns.png"></div><br><br>         (,         ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ni/ig/ji/niigjizzguoke8dcfdz2ssqnqa8.png"></div><br><br>  ,         ,  ,        . <br><br><a name="monitor"></a><h4>    ? </h4><br>     Java-  , ,  ,    . <br><br>         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nuts and Bolts</a> ,          .              ,     .     ,      ,  JMX. <br><br>      ,          .          . <br><br>     statsd    JVM,    (heap,   non-heap   ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n9/iw/vs/n9iwvsjan7hxeo-xsksaggthrqy.png"></div><br><br>  ,    ,       . <br><br>    ‚Äî       ,    ,  ,    ,    ?        .     ()  -,     ,   RPS   . <br><br>     :   ,              .         .        ammo-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">.</a> .    . : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qr/ry/wh/qrrywh-id3u5lbk7ldms8n-n_ck.png"></div><br><br>        . <br><br>               ,     .  ,      ,     - ,   ,   . <br><br><h4>  En conclusion </h4><br>   ,  Java  Docker ‚Äî    ,      .     . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr450954/">https://habr.com/ru/post/fr450954/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr450942/index.html">Sidecar pour un fractionnement de code</a></li>
<li><a href="../fr450946/index.html">Disque cellulaire sur LPC810</a></li>
<li><a href="../fr450948/index.html">MU-MIMO: l'un des algorithmes d'impl√©mentation</a></li>
<li><a href="../fr450950/index.html">Dart Streams Basics</a></li>
<li><a href="../fr450952/index.html">Indice moyen et antibank</a></li>
<li><a href="../fr450956/index.html">Comparaison des COB industriels: ISIM vs. Kics</a></li>
<li><a href="../fr450958/index.html">AnyStub, biblioth√®que de stub de connexion Java</a></li>
<li><a href="../fr450962/index.html">Pompes √† insuline, micropuces inviolables et radio d√©finie par logiciel</a></li>
<li><a href="../fr450964/index.html">Nouvelle biblioth√®que intrins√®que SIMD x86 - D√©bogage immintrin</a></li>
<li><a href="../fr450966/index.html">Enregistrement de vid√©o √† partir d'un vieil ordinateur - m√©thodes de LGR</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>