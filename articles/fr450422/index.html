<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏺 🍋 ⛵️ Les limites des algorithmes de reconnaissance d'image 🤛🏼 👨🏿‍🤝‍👨🏾 🚄</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Non, il ne s'agit pas d'algorithmes de reconnaissance d'image - il s'agit des limites de leur utilisation, en particulier lors de la création d'IA. 

...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les limites des algorithmes de reconnaissance d'image</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/450422/"><img src="https://habrastorage.org/webt/wv/6j/k6/wv6jk6e24vdagqflmeorpmhjg-0.jpeg"><br><br>  Non, il ne s'agit pas d'algorithmes de reconnaissance d'image - il s'agit des limites de leur utilisation, en particulier lors de la création d'IA. <br><br>  À mon avis, la reconnaissance des images visuelles par une personne et un système informatique est très différente - à tel point qu'elle a peu en commun.  Lorsqu'une personne dit «je vois», elle pense en fait plus qu'elle ne voit, ce qui ne peut pas être dit d'un système informatique équipé d'un équipement de reconnaissance d'image. <br><br>  Je sais que l'idée n'est pas nouvelle, mais je propose encore une fois de s'assurer de sa validité par l'exemple d'un robot prétendant posséder de l'intelligence.  La question test est: quel type de robot le monde environnant devrait-il voir pour devenir pleinement comme une personne? <br><a name="habracut"></a><br>  Bien sûr, le robot doit reconnaître les objets.  Oh oui, les algorithmes y font face - grâce à une formation sur les échantillons originaux, si je comprends bien.  Mais c'est catastrophiquement petit! <br><br>  <b>I.</b> <br>  Premièrement, chaque objet du monde environnant se compose de nombreux objets et, à son tour, est un sous-ensemble d'autres objets.  J'appelle cette propriété imbriquée.  Mais que se passe-t-il si un sujet n'a tout simplement pas de nom, il n'est donc pas dans la base des échantillons originaux utilisés pour apprendre l'algorithme - que devrait reconnaître le robot dans ce cas? <br><br>  Le nuage que j'observe actuellement dans la fenêtre n'a pas de parties nommées, bien qu'il se compose évidemment de bords et d'un milieu.  Cependant, il n'y a pas de termes spéciaux pour les bords et le milieu du nuage, non inventés.  Pour indiquer un objet sans nom, j'ai utilisé une formulation verbale («cloud» - type d'objet, «cloud edge» - formulation verbale), qui n'est pas incluse dans les capacités de l'algorithme de reconnaissance d'image. <br><br>  Il s'avère qu'un algorithme sans bloc logique est de peu d'utilité.  Si l'algorithme détecte une partie de l'objet entier, il ne sera pas toujours en mesure de comprendre - en conséquence, le robot ne pourra pas dire - de quoi il s'agit. <br><br>  <b>II.</b> <br>  Deuxièmement, la liste des objets qui composent le monde n'est pas close: elle est constamment mise à jour. <br><br>  Une personne a la capacité de construire des objets de réalité, en attribuant des noms à de nouveaux objets découverts, par exemple, des espèces de faune.  Il appellera un cheval avec une tête et un torse humains un centaure, mais pour cela, il sera d'abord compris que la créature a une tête et un torse humains, et que tout le reste est équin, reconnaissant ainsi l'objet vu comme nouveau.  C'est ce que fait le cerveau humain.  Et l'algorithme en l'absence de données d'entrée déterminera une telle créature soit en tant que personne, soit en tant que cheval: sans opérer avec les caractéristiques des types, il ne pourra pas établir leur combinaison. <br><br>  Pour qu'un robot devienne comme un être humain, il doit pouvoir lui définir de nouveaux types d'objets et lui attribuer des noms.  Dans les descriptions du nouveau type, les caractéristiques des types connus doivent apparaître.  Et si le robot ne sait pas comment, pourquoi diable en avons-nous besoin, si beau? <br><br>  Disons que nous envoyons un robot de reconnaissance sur Mars.  Un robot voit quelque chose d'inhabituel, mais est capable d'identifier un objet exclusivement en termes terrestres qui lui sont connus.  Qu'est-ce que cela donnera aux gens qui écoutent les messages verbaux provenant du robot?  Parfois, cela donnera quelque chose, bien sûr (si des objets terrestres sont trouvés sur Mars), et dans d'autres cas, rien (si les objets martiens ne sont pas similaires aux objets terrestres). <br><br>  L'image est une autre affaire: une personne elle-même pourra tout voir, l'évaluer correctement et la nommer.  Seulement grâce à un algorithme de reconnaissance d'image non pré-formé, mais à votre cerveau humain plus habilement construit. <br><br>  <b>III.</b> <br>  Troisièmement, il y a un problème avec l'individualisation des objets. <br><br>  Le monde autour se compose d'objets spécifiques.  En fait, vous ne pouvez voir que des objets spécifiques.  Mais dans certains cas, ils doivent être individualisés verbalement, pour lesquels soit des noms personnels sont utilisés («Vasya Petrov»), soit une simple indication d'un objet spécifique, prononcé ou implicite («ce tableau»).  Ce que j'appelle des types d'objets («personnes», «tables») ne sont que des noms collectifs d'objets qui ont certaines caractéristiques communes. <br><br>  Les algorithmes de reconnaissance d'image, s'ils sont formés sur les échantillons originaux, seront capables de reconnaître à la fois les objets individualisés et non individualisés - c'est bien.  Reconnaissance faciale dans des endroits bondés et tout ça.  La mauvaise chose est que de tels algorithmes ne comprendront pas quels objets devraient être reconnus comme possédant une individualité et lesquels n'en valent absolument pas la peine. <br><br>  Le robot, en tant que propriétaire de l'IA, devrait occasionnellement éclater en messages comme: <br>  <i>- Oh, et j'ai vu cette vieille femme il y a une semaine!</i> <br><br>  Mais il ne vaut pas la peine d'abuser de telles répliques sur les brins d'herbe, d'autant plus qu'il existe des craintes fondées sur la suffisance de la puissance de calcul pour effectuer une telle tâche. <br><br>  Il n'est pas clair pour moi où la ligne fine est tracée entre une vieille femme individualisée et d'innombrables brins d'herbe des champs, qui sont individualisés par au moins une vieille femme, mais qui ne présentent aucun intérêt pour une personne du point de vue de l'individualisation.  Quelle est l'image reconnue dans ce sens?  Presque rien - le début d'une perception difficile à douloureuse de la réalité environnante. <br><br>  <b>IV.</b> <br>  Quatrièmement, la dynamique des objets, déterminée par leur disposition spatiale mutuelle.  C'est, je vous le dis, quelque chose! <br><br>  Je suis assis devant la cheminée dans un fauteuil profond et j'essaye maintenant de me lever. <br>  <i>"Que voyez-vous, robot?"</i> <br><br>  De notre point de vue quotidien, le robot me voit me lever d'une chaise.  Que doit-il répondre?  La réponse pertinente serait probablement: <br>  <i>"Je te vois te lever de ta chaise."</i> <br><br>  Pour ce faire, le robot doit savoir qui je suis, ce qu'est une chaise et ce que signifie se lever ... <br><br>  L'algorithme de reconnaissance d'image, après les réglages appropriés, pourra me reconnaître moi et le fauteuil, puis en comparant les cadres, nous pourrons déterminer le fait que je me retire mutuellement du fauteuil, mais que signifie «se lever»?  Comment le «soulèvement» se produit-il dans la réalité physique? <br><br>  Si je me suis déjà levé et suis parti, tout est assez simple.  Après m'être éloigné de la chaise, tous les objets dans le bureau n'ont pas changé la position spatiale les uns par rapport aux autres, à l'exception de moi, qui était à l'origine dans la chaise, et après un certain temps était loin de la chaise.  Il est permis de conclure que j'ai quitté le fauteuil. <br><br>  Si je suis encore en train de me lever de la chaise, tout est un peu plus compliqué.  Je suis toujours à côté de la chaise, cependant, la position spatiale relative des parties de mon corps a changé: <br><br><ul><li>  au départ le tibia et le tronc étaient en position verticale, et la cuisse était en position horizontale (j'étais assis), </li><li>  l'instant d'après, toutes les parties du corps étaient en position verticale (je me suis levé). </li></ul><br>  Observez mon comportement en tant que personne, il conclura instantanément que je me lève d'une chaise.  Pour une personne, ce ne sera pas tant une conclusion logique qu'une perception visuelle: il me verra littéralement se lever de ma chaise, même s'il verra en fait un changement dans la position relative de certaines parties de mon corps.  Cependant, en réalité, ce sera une conclusion logique que quelqu'un doit expliquer au robot, ou le robot doit élaborer cette conclusion logique par lui-même. <br><br>  Les deux sont tout aussi difficiles: <br><br><ul><li>  entrer dans la base de connaissances initiale des informations selon lesquelles se lever est un changement séquentiel dans la position spatiale mutuelle de certaines parties du corps n'est en quelque sorte pas inspirant; </li><li>  il n'est pas moins stupide d'espérer que le robot, en tant que créature à pensée artificielle, devinera lui-même rapidement que le changement de position spatiale mutuelle de certaines parties du corps décrit ci-dessus est appelé debout.  Chez l'homme, ce processus prend des années, combien cela prendra-t-il pour un robot? </li></ul><br>  Et qu'est-ce que les algorithmes de reconnaissance d'image ont à voir avec cela?  Ils ne pourront jamais déterminer que je me lève d'une chaise. <br><br>  <b>V.</b> <br>  «Debout» est un concept abstrait, déterminé par un changement dans les caractéristiques des objets matériels, dans ce cas, un changement dans leur position spatiale mutuelle.  Dans le cas général, cela est vrai pour tous les concepts abstraits, car les concepts abstraits eux-mêmes n'existent pas dans le monde matériel, mais dépendent complètement des objets matériels.  Bien que souvent, nous les percevons comme observés personnellement. <br><br>  Pour déplacer la mâchoire vers la droite ou vers la gauche, sans ouvrir la bouche - comment s'appelle cette action?  Mais pas question.  Sans aucun doute, pour la raison qu'un tel mouvement est généralement inhabituel pour une personne.  En utilisant les algorithmes discutés, le robot verra quelque chose, mais à quoi ça sert?  Dans la base des échantillons initiaux, le nom souhaité sera absent, et il sera difficile de nommer l'action enregistrée du robot.  Et pour donner des formulations verbales détaillées à des actions sans nom, ainsi qu'à d'autres concepts abstraits, les algorithmes de reconnaissance d'image ne sont pas formés. <br><br>  En fait, nous avons un double du premier paragraphe, non seulement en ce qui concerne les objets, mais aussi les concepts abstraits.  Cependant, le reste des paragraphes, précédent et suivant, peut également être lié à des concepts abstraits - je fais juste attention à augmenter le niveau de complexité lorsque vous travaillez avec des abstractions. <br><br>  <b>VI.</b> <br>  Sixièmement, une relation causale. <br><br>  Imaginez que vous regardez une camionnette décoller de la route et abattre une clôture.  La démolition de la clôture est due au mouvement de ramassage et, à son tour, le mouvement de ramassage entraîne la démolition de la clôture. <br><br>  <i>- Je l'ai vu de mes propres yeux!</i> <br>  C'est la réponse à la question de savoir si vous avez vu ce qui s'est passé ou si vous y avez pensé.  Et qu'avez-vous réellement vu? <br><br>  Quelques éléments dans une telle dynamique: <br><br><ul><li>  une camionnette a quitté la route </li><li>  le ramassage est venu près de la clôture, </li><li>  la clôture a changé de forme et d'emplacement. </li></ul><br>  Sur la base de la perception visuelle, le robot doit réaliser que dans le cas habituel, les clôtures ne changent pas de forme et d'emplacement: ici, cela s'est produit à la suite d'un contact avec le pick-up.  Le sujet-cause et le sujet-effet doivent être en contact l'un avec l'autre, sinon la causalité est absente dans leur relation. <br><br>  Bien que nous tombions ici dans un piège logique, parce que d'autres objets peuvent entrer en contact avec le sujet-conséquence, pas seulement avec le sujet-raison. <br><br>  Supposons qu'au moment du ramassage, frappez le choucas sur la clôture.  Une camionnette et un choucas sont entrés en contact avec la clôture en même temps: comment déterminer le résultat de quel contact la clôture a été démolie? <br><br>  Utilisant probablement la répétabilité: <br><br><ul><li>  si dans chaque cas, lorsqu'un choucas est assis sur la clôture, la clôture est démolie, le choucas est à blâmer; </li><li>  si dans chaque cas, quand un pick-up s'écrase dans la clôture, le pick-up est à blâmer. </li></ul><br>  Ainsi, la conclusion que la clôture a été démolie par un pick-up n'est pas exactement une observation, mais le résultat d'une analyse basée sur l'observation d'objets en contact. <br><br>  D'autre part, l'action peut être réalisée à distance, par exemple l'action d'un aimant sur un objet en fer.  Comment le robot suppose-t-il que le fait de rapprocher un aimant d'un clou fait que l'ongle se précipite vers l'aimant?  L'image visuelle n'est pas comme ça: <br><br><ul><li>  l'aimant s'approche mais n'est pas en contact avec l'ongle, </li><li>  au même instant, l'ongle se précipite sur l'aimant de sa propre initiative et entre en contact avec lui. </li></ul><br>  Comme vous pouvez le voir, il est très difficile de suivre les relations de cause à effet, même dans les cas où le témoin déclare avec une conviction ironique qu’il l’a vu de ses propres yeux.  Les algorithmes de reconnaissance d'image sont impuissants ici. <br><br>  <b>VII.</b> <br>  Septième et dernier, c'est le choix des objectifs de perception visuelle. <br><br>  L'image visuelle environnante peut être constituée de centaines et de milliers d'objets imbriqués les uns dans les autres, dont beaucoup changent constamment de position spatiale et d'autres caractéristiques.  De toute évidence, le robot n'a pas besoin de percevoir chaque brin d'herbe dans le champ, cependant, comme chaque visage dans une rue de la ville: il vous suffit de percevoir l'important, en fonction des tâches effectuées. <br><br>  De toute évidence, l'ajustement de l'algorithme de reconnaissance d'image à la perception de certains objets et l'ignorance d'autres ne fonctionneront pas, car on ne sait peut-être pas à l'avance à quoi faire attention et quoi ignorer, d'autant plus que les objectifs actuels peuvent changer en cours de route.  Une situation peut survenir lorsque vous devez d'abord percevoir plusieurs milliers d'objets imbriqués les uns dans les autres - littéralement chacun d'eux - pour analyser et ensuite rendre un verdict quels objets sont essentiels pour résoudre le problème actuel et qui ne vous intéressent pas.  C'est ainsi que la personne perçoit le monde qui l'entoure: elle ne voit que l'important, sans prêter attention aux événements de fond sans intérêt.  Comment il réussit est un secret. <br><br>  Et le robot, même équipé des algorithmes de reconnaissance d'image les plus modernes et les plus ingénieux? ... Si, lors d'une attaque par des extraterrestres martiens, il commence un rapport avec des bulletins météo et continue avec une description du nouveau paysage étalé devant lui, il n'a peut-être pas le temps de rapporter l'attaque elle-même. <br><br>  <b>Conclusions</b> <br><br><ol><li>  La simple reconnaissance d'images visuelles ne remplacera pas les yeux humains. </li><li>  Les algorithmes de reconnaissance d'image sont un outil auxiliaire avec une portée très étroite. </li><li>  Pour qu'un robot commence non seulement à penser, mais au moins à le voir humainement, des algorithmes sont nécessaires non seulement pour la reconnaissance des formes, mais aussi pour la même pensée humaine à part entière et pourtant inaccessible. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr450422/">https://habr.com/ru/post/fr450422/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr450398/index.html">Les poisons les plus intrépides</a></li>
<li><a href="../fr450410/index.html">Terraformer - Infrastructure à coder</a></li>
<li><a href="../fr450416/index.html">Comment les fournisseurs VPN de shareware vendent vos données</a></li>
<li><a href="../fr450418/index.html">L'art de créer des modèles 3D organiques: les ombrages sous-cutanés</a></li>
<li><a href="../fr450420/index.html">Pourquoi les équipes de science des données ont besoin d'universel, pas de spécialistes</a></li>
<li><a href="../fr450426/index.html">2011 vs AM4. Dinosaures vs mammifères</a></li>
<li><a href="../fr450428/index.html">Indexeurs en C # sous le capot: mieux indexer que Dow Jones</a></li>
<li><a href="../fr450430/index.html">Qu'est-ce qu'une attaque à la poussière?</a></li>
<li><a href="../fr450432/index.html">Eh bien, où est-elle?</a></li>
<li><a href="../fr450436/index.html">Qu'est-ce qu'un bootcamp de codage?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>