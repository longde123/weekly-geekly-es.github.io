<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸº ğŸ‹ â›µï¸ Les limites des algorithmes de reconnaissance d'image ğŸ¤›ğŸ¼ ğŸ‘¨ğŸ¿â€ğŸ¤â€ğŸ‘¨ğŸ¾ ğŸš„</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Non, il ne s'agit pas d'algorithmes de reconnaissance d'image - il s'agit des limites de leur utilisation, en particulier lors de la crÃ©ation d'IA. 

...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les limites des algorithmes de reconnaissance d'image</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/450422/"><img src="https://habrastorage.org/webt/wv/6j/k6/wv6jk6e24vdagqflmeorpmhjg-0.jpeg"><br><br>  Non, il ne s'agit pas d'algorithmes de reconnaissance d'image - il s'agit des limites de leur utilisation, en particulier lors de la crÃ©ation d'IA. <br><br>  Ã€ mon avis, la reconnaissance des images visuelles par une personne et un systÃ¨me informatique est trÃ¨s diffÃ©rente - Ã  tel point qu'elle a peu en commun.  Lorsqu'une personne dit Â«je voisÂ», elle pense en fait plus qu'elle ne voit, ce qui ne peut pas Ãªtre dit d'un systÃ¨me informatique Ã©quipÃ© d'un Ã©quipement de reconnaissance d'image. <br><br>  Je sais que l'idÃ©e n'est pas nouvelle, mais je propose encore une fois de s'assurer de sa validitÃ© par l'exemple d'un robot prÃ©tendant possÃ©der de l'intelligence.  La question test est: quel type de robot le monde environnant devrait-il voir pour devenir pleinement comme une personne? <br><a name="habracut"></a><br>  Bien sÃ»r, le robot doit reconnaÃ®tre les objets.  Oh oui, les algorithmes y font face - grÃ¢ce Ã  une formation sur les Ã©chantillons originaux, si je comprends bien.  Mais c'est catastrophiquement petit! <br><br>  <b>I.</b> <br>  PremiÃ¨rement, chaque objet du monde environnant se compose de nombreux objets et, Ã  son tour, est un sous-ensemble d'autres objets.  J'appelle cette propriÃ©tÃ© imbriquÃ©e.  Mais que se passe-t-il si un sujet n'a tout simplement pas de nom, il n'est donc pas dans la base des Ã©chantillons originaux utilisÃ©s pour apprendre l'algorithme - que devrait reconnaÃ®tre le robot dans ce cas? <br><br>  Le nuage que j'observe actuellement dans la fenÃªtre n'a pas de parties nommÃ©es, bien qu'il se compose Ã©videmment de bords et d'un milieu.  Cependant, il n'y a pas de termes spÃ©ciaux pour les bords et le milieu du nuage, non inventÃ©s.  Pour indiquer un objet sans nom, j'ai utilisÃ© une formulation verbale (Â«cloudÂ» - type d'objet, Â«cloud edgeÂ» - formulation verbale), qui n'est pas incluse dans les capacitÃ©s de l'algorithme de reconnaissance d'image. <br><br>  Il s'avÃ¨re qu'un algorithme sans bloc logique est de peu d'utilitÃ©.  Si l'algorithme dÃ©tecte une partie de l'objet entier, il ne sera pas toujours en mesure de comprendre - en consÃ©quence, le robot ne pourra pas dire - de quoi il s'agit. <br><br>  <b>II.</b> <br>  DeuxiÃ¨mement, la liste des objets qui composent le monde n'est pas close: elle est constamment mise Ã  jour. <br><br>  Une personne a la capacitÃ© de construire des objets de rÃ©alitÃ©, en attribuant des noms Ã  de nouveaux objets dÃ©couverts, par exemple, des espÃ¨ces de faune.  Il appellera un cheval avec une tÃªte et un torse humains un centaure, mais pour cela, il sera d'abord compris que la crÃ©ature a une tÃªte et un torse humains, et que tout le reste est Ã©quin, reconnaissant ainsi l'objet vu comme nouveau.  C'est ce que fait le cerveau humain.  Et l'algorithme en l'absence de donnÃ©es d'entrÃ©e dÃ©terminera une telle crÃ©ature soit en tant que personne, soit en tant que cheval: sans opÃ©rer avec les caractÃ©ristiques des types, il ne pourra pas Ã©tablir leur combinaison. <br><br>  Pour qu'un robot devienne comme un Ãªtre humain, il doit pouvoir lui dÃ©finir de nouveaux types d'objets et lui attribuer des noms.  Dans les descriptions du nouveau type, les caractÃ©ristiques des types connus doivent apparaÃ®tre.  Et si le robot ne sait pas comment, pourquoi diable en avons-nous besoin, si beau? <br><br>  Disons que nous envoyons un robot de reconnaissance sur Mars.  Un robot voit quelque chose d'inhabituel, mais est capable d'identifier un objet exclusivement en termes terrestres qui lui sont connus.  Qu'est-ce que cela donnera aux gens qui Ã©coutent les messages verbaux provenant du robot?  Parfois, cela donnera quelque chose, bien sÃ»r (si des objets terrestres sont trouvÃ©s sur Mars), et dans d'autres cas, rien (si les objets martiens ne sont pas similaires aux objets terrestres). <br><br>  L'image est une autre affaire: une personne elle-mÃªme pourra tout voir, l'Ã©valuer correctement et la nommer.  Seulement grÃ¢ce Ã  un algorithme de reconnaissance d'image non prÃ©-formÃ©, mais Ã  votre cerveau humain plus habilement construit. <br><br>  <b>III.</b> <br>  TroisiÃ¨mement, il y a un problÃ¨me avec l'individualisation des objets. <br><br>  Le monde autour se compose d'objets spÃ©cifiques.  En fait, vous ne pouvez voir que des objets spÃ©cifiques.  Mais dans certains cas, ils doivent Ãªtre individualisÃ©s verbalement, pour lesquels soit des noms personnels sont utilisÃ©s (Â«Vasya PetrovÂ»), soit une simple indication d'un objet spÃ©cifique, prononcÃ© ou implicite (Â«ce tableauÂ»).  Ce que j'appelle des types d'objets (Â«personnesÂ», Â«tablesÂ») ne sont que des noms collectifs d'objets qui ont certaines caractÃ©ristiques communes. <br><br>  Les algorithmes de reconnaissance d'image, s'ils sont formÃ©s sur les Ã©chantillons originaux, seront capables de reconnaÃ®tre Ã  la fois les objets individualisÃ©s et non individualisÃ©s - c'est bien.  Reconnaissance faciale dans des endroits bondÃ©s et tout Ã§a.  La mauvaise chose est que de tels algorithmes ne comprendront pas quels objets devraient Ãªtre reconnus comme possÃ©dant une individualitÃ© et lesquels n'en valent absolument pas la peine. <br><br>  Le robot, en tant que propriÃ©taire de l'IA, devrait occasionnellement Ã©clater en messages comme: <br>  <i>- Oh, et j'ai vu cette vieille femme il y a une semaine!</i> <br><br>  Mais il ne vaut pas la peine d'abuser de telles rÃ©pliques sur les brins d'herbe, d'autant plus qu'il existe des craintes fondÃ©es sur la suffisance de la puissance de calcul pour effectuer une telle tÃ¢che. <br><br>  Il n'est pas clair pour moi oÃ¹ la ligne fine est tracÃ©e entre une vieille femme individualisÃ©e et d'innombrables brins d'herbe des champs, qui sont individualisÃ©s par au moins une vieille femme, mais qui ne prÃ©sentent aucun intÃ©rÃªt pour une personne du point de vue de l'individualisation.  Quelle est l'image reconnue dans ce sens?  Presque rien - le dÃ©but d'une perception difficile Ã  douloureuse de la rÃ©alitÃ© environnante. <br><br>  <b>IV.</b> <br>  QuatriÃ¨mement, la dynamique des objets, dÃ©terminÃ©e par leur disposition spatiale mutuelle.  C'est, je vous le dis, quelque chose! <br><br>  Je suis assis devant la cheminÃ©e dans un fauteuil profond et j'essaye maintenant de me lever. <br>  <i>"Que voyez-vous, robot?"</i> <br><br>  De notre point de vue quotidien, le robot me voit me lever d'une chaise.  Que doit-il rÃ©pondre?  La rÃ©ponse pertinente serait probablement: <br>  <i>"Je te vois te lever de ta chaise."</i> <br><br>  Pour ce faire, le robot doit savoir qui je suis, ce qu'est une chaise et ce que signifie se lever ... <br><br>  L'algorithme de reconnaissance d'image, aprÃ¨s les rÃ©glages appropriÃ©s, pourra me reconnaÃ®tre moi et le fauteuil, puis en comparant les cadres, nous pourrons dÃ©terminer le fait que je me retire mutuellement du fauteuil, mais que signifie Â«se leverÂ»?  Comment le Â«soulÃ¨vementÂ» se produit-il dans la rÃ©alitÃ© physique? <br><br>  Si je me suis dÃ©jÃ  levÃ© et suis parti, tout est assez simple.  AprÃ¨s m'Ãªtre Ã©loignÃ© de la chaise, tous les objets dans le bureau n'ont pas changÃ© la position spatiale les uns par rapport aux autres, Ã  l'exception de moi, qui Ã©tait Ã  l'origine dans la chaise, et aprÃ¨s un certain temps Ã©tait loin de la chaise.  Il est permis de conclure que j'ai quittÃ© le fauteuil. <br><br>  Si je suis encore en train de me lever de la chaise, tout est un peu plus compliquÃ©.  Je suis toujours Ã  cÃ´tÃ© de la chaise, cependant, la position spatiale relative des parties de mon corps a changÃ©: <br><br><ul><li>  au dÃ©part le tibia et le tronc Ã©taient en position verticale, et la cuisse Ã©tait en position horizontale (j'Ã©tais assis), </li><li>  l'instant d'aprÃ¨s, toutes les parties du corps Ã©taient en position verticale (je me suis levÃ©). </li></ul><br>  Observez mon comportement en tant que personne, il conclura instantanÃ©ment que je me lÃ¨ve d'une chaise.  Pour une personne, ce ne sera pas tant une conclusion logique qu'une perception visuelle: il me verra littÃ©ralement se lever de ma chaise, mÃªme s'il verra en fait un changement dans la position relative de certaines parties de mon corps.  Cependant, en rÃ©alitÃ©, ce sera une conclusion logique que quelqu'un doit expliquer au robot, ou le robot doit Ã©laborer cette conclusion logique par lui-mÃªme. <br><br>  Les deux sont tout aussi difficiles: <br><br><ul><li>  entrer dans la base de connaissances initiale des informations selon lesquelles se lever est un changement sÃ©quentiel dans la position spatiale mutuelle de certaines parties du corps n'est en quelque sorte pas inspirant; </li><li>  il n'est pas moins stupide d'espÃ©rer que le robot, en tant que crÃ©ature Ã  pensÃ©e artificielle, devinera lui-mÃªme rapidement que le changement de position spatiale mutuelle de certaines parties du corps dÃ©crit ci-dessus est appelÃ© debout.  Chez l'homme, ce processus prend des annÃ©es, combien cela prendra-t-il pour un robot? </li></ul><br>  Et qu'est-ce que les algorithmes de reconnaissance d'image ont Ã  voir avec cela?  Ils ne pourront jamais dÃ©terminer que je me lÃ¨ve d'une chaise. <br><br>  <b>V.</b> <br>  Â«DeboutÂ» est un concept abstrait, dÃ©terminÃ© par un changement dans les caractÃ©ristiques des objets matÃ©riels, dans ce cas, un changement dans leur position spatiale mutuelle.  Dans le cas gÃ©nÃ©ral, cela est vrai pour tous les concepts abstraits, car les concepts abstraits eux-mÃªmes n'existent pas dans le monde matÃ©riel, mais dÃ©pendent complÃ¨tement des objets matÃ©riels.  Bien que souvent, nous les percevons comme observÃ©s personnellement. <br><br>  Pour dÃ©placer la mÃ¢choire vers la droite ou vers la gauche, sans ouvrir la bouche - comment s'appelle cette action?  Mais pas question.  Sans aucun doute, pour la raison qu'un tel mouvement est gÃ©nÃ©ralement inhabituel pour une personne.  En utilisant les algorithmes discutÃ©s, le robot verra quelque chose, mais Ã  quoi Ã§a sert?  Dans la base des Ã©chantillons initiaux, le nom souhaitÃ© sera absent, et il sera difficile de nommer l'action enregistrÃ©e du robot.  Et pour donner des formulations verbales dÃ©taillÃ©es Ã  des actions sans nom, ainsi qu'Ã  d'autres concepts abstraits, les algorithmes de reconnaissance d'image ne sont pas formÃ©s. <br><br>  En fait, nous avons un double du premier paragraphe, non seulement en ce qui concerne les objets, mais aussi les concepts abstraits.  Cependant, le reste des paragraphes, prÃ©cÃ©dent et suivant, peut Ã©galement Ãªtre liÃ© Ã  des concepts abstraits - je fais juste attention Ã  augmenter le niveau de complexitÃ© lorsque vous travaillez avec des abstractions. <br><br>  <b>VI.</b> <br>  SixiÃ¨mement, une relation causale. <br><br>  Imaginez que vous regardez une camionnette dÃ©coller de la route et abattre une clÃ´ture.  La dÃ©molition de la clÃ´ture est due au mouvement de ramassage et, Ã  son tour, le mouvement de ramassage entraÃ®ne la dÃ©molition de la clÃ´ture. <br><br>  <i>- Je l'ai vu de mes propres yeux!</i> <br>  C'est la rÃ©ponse Ã  la question de savoir si vous avez vu ce qui s'est passÃ© ou si vous y avez pensÃ©.  Et qu'avez-vous rÃ©ellement vu? <br><br>  Quelques Ã©lÃ©ments dans une telle dynamique: <br><br><ul><li>  une camionnette a quittÃ© la route </li><li>  le ramassage est venu prÃ¨s de la clÃ´ture, </li><li>  la clÃ´ture a changÃ© de forme et d'emplacement. </li></ul><br>  Sur la base de la perception visuelle, le robot doit rÃ©aliser que dans le cas habituel, les clÃ´tures ne changent pas de forme et d'emplacement: ici, cela s'est produit Ã  la suite d'un contact avec le pick-up.  Le sujet-cause et le sujet-effet doivent Ãªtre en contact l'un avec l'autre, sinon la causalitÃ© est absente dans leur relation. <br><br>  Bien que nous tombions ici dans un piÃ¨ge logique, parce que d'autres objets peuvent entrer en contact avec le sujet-consÃ©quence, pas seulement avec le sujet-raison. <br><br>  Supposons qu'au moment du ramassage, frappez le choucas sur la clÃ´ture.  Une camionnette et un choucas sont entrÃ©s en contact avec la clÃ´ture en mÃªme temps: comment dÃ©terminer le rÃ©sultat de quel contact la clÃ´ture a Ã©tÃ© dÃ©molie? <br><br>  Utilisant probablement la rÃ©pÃ©tabilitÃ©: <br><br><ul><li>  si dans chaque cas, lorsqu'un choucas est assis sur la clÃ´ture, la clÃ´ture est dÃ©molie, le choucas est Ã  blÃ¢mer; </li><li>  si dans chaque cas, quand un pick-up s'Ã©crase dans la clÃ´ture, le pick-up est Ã  blÃ¢mer. </li></ul><br>  Ainsi, la conclusion que la clÃ´ture a Ã©tÃ© dÃ©molie par un pick-up n'est pas exactement une observation, mais le rÃ©sultat d'une analyse basÃ©e sur l'observation d'objets en contact. <br><br>  D'autre part, l'action peut Ãªtre rÃ©alisÃ©e Ã  distance, par exemple l'action d'un aimant sur un objet en fer.  Comment le robot suppose-t-il que le fait de rapprocher un aimant d'un clou fait que l'ongle se prÃ©cipite vers l'aimant?  L'image visuelle n'est pas comme Ã§a: <br><br><ul><li>  l'aimant s'approche mais n'est pas en contact avec l'ongle, </li><li>  au mÃªme instant, l'ongle se prÃ©cipite sur l'aimant de sa propre initiative et entre en contact avec lui. </li></ul><br>  Comme vous pouvez le voir, il est trÃ¨s difficile de suivre les relations de cause Ã  effet, mÃªme dans les cas oÃ¹ le tÃ©moin dÃ©clare avec une conviction ironique quâ€™il lâ€™a vu de ses propres yeux.  Les algorithmes de reconnaissance d'image sont impuissants ici. <br><br>  <b>VII.</b> <br>  SeptiÃ¨me et dernier, c'est le choix des objectifs de perception visuelle. <br><br>  L'image visuelle environnante peut Ãªtre constituÃ©e de centaines et de milliers d'objets imbriquÃ©s les uns dans les autres, dont beaucoup changent constamment de position spatiale et d'autres caractÃ©ristiques.  De toute Ã©vidence, le robot n'a pas besoin de percevoir chaque brin d'herbe dans le champ, cependant, comme chaque visage dans une rue de la ville: il vous suffit de percevoir l'important, en fonction des tÃ¢ches effectuÃ©es. <br><br>  De toute Ã©vidence, l'ajustement de l'algorithme de reconnaissance d'image Ã  la perception de certains objets et l'ignorance d'autres ne fonctionneront pas, car on ne sait peut-Ãªtre pas Ã  l'avance Ã  quoi faire attention et quoi ignorer, d'autant plus que les objectifs actuels peuvent changer en cours de route.  Une situation peut survenir lorsque vous devez d'abord percevoir plusieurs milliers d'objets imbriquÃ©s les uns dans les autres - littÃ©ralement chacun d'eux - pour analyser et ensuite rendre un verdict quels objets sont essentiels pour rÃ©soudre le problÃ¨me actuel et qui ne vous intÃ©ressent pas.  C'est ainsi que la personne perÃ§oit le monde qui l'entoure: elle ne voit que l'important, sans prÃªter attention aux Ã©vÃ©nements de fond sans intÃ©rÃªt.  Comment il rÃ©ussit est un secret. <br><br>  Et le robot, mÃªme Ã©quipÃ© des algorithmes de reconnaissance d'image les plus modernes et les plus ingÃ©nieux? ... Si, lors d'une attaque par des extraterrestres martiens, il commence un rapport avec des bulletins mÃ©tÃ©o et continue avec une description du nouveau paysage Ã©talÃ© devant lui, il n'a peut-Ãªtre pas le temps de rapporter l'attaque elle-mÃªme. <br><br>  <b>Conclusions</b> <br><br><ol><li>  La simple reconnaissance d'images visuelles ne remplacera pas les yeux humains. </li><li>  Les algorithmes de reconnaissance d'image sont un outil auxiliaire avec une portÃ©e trÃ¨s Ã©troite. </li><li>  Pour qu'un robot commence non seulement Ã  penser, mais au moins Ã  le voir humainement, des algorithmes sont nÃ©cessaires non seulement pour la reconnaissance des formes, mais aussi pour la mÃªme pensÃ©e humaine Ã  part entiÃ¨re et pourtant inaccessible. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr450422/">https://habr.com/ru/post/fr450422/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr450398/index.html">Les poisons les plus intrÃ©pides</a></li>
<li><a href="../fr450410/index.html">Terraformer - Infrastructure Ã  coder</a></li>
<li><a href="../fr450416/index.html">Comment les fournisseurs VPN de shareware vendent vos donnÃ©es</a></li>
<li><a href="../fr450418/index.html">L'art de crÃ©er des modÃ¨les 3D organiques: les ombrages sous-cutanÃ©s</a></li>
<li><a href="../fr450420/index.html">Pourquoi les Ã©quipes de science des donnÃ©es ont besoin d'universel, pas de spÃ©cialistes</a></li>
<li><a href="../fr450426/index.html">2011 vs AM4. Dinosaures vs mammifÃ¨res</a></li>
<li><a href="../fr450428/index.html">Indexeurs en C # sous le capot: mieux indexer que Dow Jones</a></li>
<li><a href="../fr450430/index.html">Qu'est-ce qu'une attaque Ã  la poussiÃ¨re?</a></li>
<li><a href="../fr450432/index.html">Eh bien, oÃ¹ est-elle?</a></li>
<li><a href="../fr450436/index.html">Qu'est-ce qu'un bootcamp de codage?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>