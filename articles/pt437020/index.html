<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëºüèæ üë®üèæ‚Äçüéì üßíüèª O que h√° de errado com o aprendizado por refor√ßo? üë©üèΩ‚Äçüé§ üë©üèº‚Äçüíº üßî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No in√≠cio de 2018, foi publicado um artigo O aprendizado profundo por refor√ßo ainda n√£o funciona ("O aprendizado com refor√ßo ainda n√£o funciona"). A p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O que h√° de errado com o aprendizado por refor√ßo?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/437020/"><p><img src="https://habrastorage.org/webt/hv/1l/vs/hv1lvsyszoctmnrbxex7valfo8a.jpeg"></p><br><p>  No in√≠cio de 2018, foi publicado um artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O aprendizado profundo por refor√ßo ainda n√£o funciona</a> ("O aprendizado com refor√ßo ainda n√£o funciona").  A principal reclama√ß√£o era que algoritmos modernos de aprendizado com refor√ßo exigem aproximadamente a mesma quantidade de tempo para resolver um problema que uma pesquisa aleat√≥ria regular. </p><br><p>  Alguma coisa mudou desde ent√£o?  N√£o. </p><br><p>  O aprendizado refor√ßado √© considerado um dos tr√™s principais caminhos para a constru√ß√£o de IA forte.  Mas as dificuldades enfrentadas por essa √°rea de aprendizado de m√°quina e os m√©todos que os cientistas est√£o tentando lidar com essas dificuldades sugerem que pode haver problemas fundamentais com essa abordagem em si. </p><a name="habracut"></a><br><h2 id="postoyte-chto-znachit-odin-iz-treh-a-ostalnye-dva-kakie">  Espere, o que significa um de tr√™s?  Quais s√£o os outros dois? </h2><br><p>  Dado o sucesso das redes neurais nos √∫ltimos anos e a an√°lise de como elas funcionam com habilidades cognitivas de alto n√≠vel, que antes eram consideradas caracter√≠sticas apenas de seres humanos e animais superiores, hoje na comunidade cient√≠fica existe uma opini√£o de que existem tr√™s abordagens principais para criar IA forte em a base de redes neurais, que podem ser consideradas mais ou menos realistas: </p><br><h2 id="1-obrabotka-tekstov">  1. Processamento de texto </h2><br><p>  O mundo acumulou um grande n√∫mero de livros e textos na Internet, incluindo livros e livros de refer√™ncia.  O texto √© conveniente e r√°pido para processamento em um computador.  Teoricamente, esse conjunto de textos deve ser suficiente para treinar uma forte IA de conversa√ß√£o. </p><br><p>  Est√° impl√≠cito que nessas matrizes textuais a estrutura completa do mundo √© refletida (pelo menos, √© descrita em livros did√°ticos e livros de refer√™ncia).  Mas isso n√£o √© fato.  Os textos como forma de apresenta√ß√£o de informa√ß√µes s√£o fortemente divorciados do mundo tridimensional real e do curso do tempo em que vivemos. </p><br><p>  Bons exemplos de IA treinados em matrizes de texto s√£o bots de bate-papo e tradutores autom√°ticos.  Como para traduzir o texto, voc√™ precisa entender o significado da frase e recont√°-lo em novas palavras (em outro idioma).  Existe um equ√≠voco comum de que regras de gram√°tica e sintaxe, incluindo uma descri√ß√£o de todas as exce√ß√µes poss√≠veis, descrevem completamente um idioma espec√≠fico.  Isto n√£o √© verdade.  A linguagem √© apenas uma ferramenta auxiliar na vida, muda e se adapta facilmente a novas situa√ß√µes. </p><br><p>  O problema com o processamento de texto (mesmo por sistemas especialistas, at√© redes neurais) √© que <strong>n√£o h√° um</strong> conjunto de regras, cujas frases devem ser aplicadas em quais situa√ß√µes.  Observe - n√£o as regras para a constru√ß√£o das pr√≥prias frases (o que a gram√°tica e a sintaxe fazem), mas as frases em que situa√ß√µes.  Na mesma situa√ß√£o, as pessoas pronunciam frases em idiomas diferentes que geralmente n√£o s√£o relacionados entre si em termos de estrutura do idioma.  Compare frases com extrema surpresa: "oh deus!"  e "oh, merda!".  Bem, e como fazer uma correspond√™ncia entre eles, conhecendo o modelo de linguagem?  De jeito nenhum.  Isso aconteceu por acaso historicamente.  Voc√™ precisa conhecer a situa√ß√£o e o que eles normalmente falam em um idioma espec√≠fico.  √â por isso que os tradutores autom√°ticos s√£o t√£o imperfeitos. </p><br><p>  N√£o se sabe se esse conhecimento pode ser distinguido apenas de uma s√©rie de textos.  Mas se os tradutores autom√°ticos traduzirem perfeitamente sem cometer erros tolos e rid√≠culos, isso ser√° uma prova de que √© poss√≠vel criar uma IA forte apenas com base no texto. </p><br><h2 id="2-raspoznavanie-izobrazheniy">  2. Reconhecimento de imagem </h2><br><p>  Olhe para esta imagem </p><br><p><img src="https://habrastorage.org/webt/pa/od/nd/paodndrl6p5dkuhig3rwo68cu-q.jpeg"></p><br><p>  Olhando para esta foto, entendemos que as filmagens foram realizadas √† noite.  A julgar pelas bandeiras, o vento sopra da direita para a esquerda.  E, a julgar pelo tr√°fego da direita, o caso n√£o est√° acontecendo na Inglaterra ou na Austr√°lia.  Nenhuma dessas informa√ß√µes √© indicada explicitamente nos pixels da imagem, isto √© conhecimento externo.  Na foto, existem apenas sinais pelos quais podemos usar o conhecimento obtido de outras fontes. </p><br><div class="spoiler">  <b class="spoiler_title">Voc√™ sabe mais alguma coisa olhando para esta foto?</b> <div class="spoiler_text"><p>  Sobre isso e o discurso ... E encontre uma garota, finalmente </p></div></div><br><p>  Portanto, acredita-se que, se voc√™ treinar uma rede neural para reconhecer objetos em uma imagem, ela ter√° uma ideia interna de como o mundo real funciona.  E essa vis√£o, obtida das fotografias, certamente corresponder√° ao nosso mundo real e real.  Ao contr√°rio de matrizes de textos onde isso n√£o √© garantido. </p><br><p>  O valor das redes neurais treinadas em um conjunto de fotografias ImageNet (e agora o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenImages V4</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">COCO</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">KITTI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BDD100K</a> e outros) n√£o √© de todo o fato do reconhecimento de um gato em uma foto.  E isso √© armazenado na pen√∫ltima camada.  √â aqui que um conjunto de recursos de alto n√≠vel que descrevem nosso mundo est√° localizado.  Um vetor de 1024 n√∫meros √© suficiente para obter uma descri√ß√£o de 1000 categorias diferentes de objetos, com 80% de precis√£o (e em 95% dos casos, a resposta correta estar√° nas 5 op√ß√µes mais pr√≥ximas).  Apenas pense sobre isso. </p><br><p>  √â por isso que esses recursos da pen√∫ltima camada s√£o usados ‚Äã‚Äãcom tanto √™xito em tarefas completamente diferentes na vis√£o computacional.  Atrav√©s do aprendizado de transfer√™ncia e do ajuste fino.  A partir deste vetor em 1024 n√∫meros, √© poss√≠vel obter, por exemplo, um mapa de profundidade da imagem </p><br><p><img src="https://habrastorage.org/webt/vs/k6/lm/vsk6lmod2grqjzous7knxl5ekaq.jpeg"></p><br><p>  (um exemplo do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho em</a> que uma rede Densenet-169 pr√©-treinada praticamente inalterada √© usada) </p><br><p>  Ou determine a pose de uma pessoa.  Existem muitas aplica√ß√µes. </p><br><p><img src="https://habrastorage.org/webt/id/rs/sp/idrsspge5oaq0dae1-li5pghf3s.jpeg"></p><br><p>  Como resultado, o reconhecimento de imagem pode ser potencialmente usado para criar uma IA forte, pois reflete realmente o modelo do nosso mundo real.  Um passo da fotografia para o v√≠deo, e o v√≠deo √© a nossa vida, pois obtemos cerca de 99% das informa√ß√µes visualmente. </p><br><p>  Mas, a partir da fotografia, √© completamente incompreens√≠vel como motivar a rede neural a pensar e tirar conclus√µes.  Ela pode ser treinada para responder perguntas como "quantos l√°pis est√£o sobre a mesa?"  (essa classe de tarefas √© chamada Resposta visual √†s perguntas, um exemplo desse conjunto de dados: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://visualqa.org</a> ).  Ou d√™ uma descri√ß√£o textual do que est√° acontecendo na foto.  Esta √© a classe de tarefa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Legenda</a> da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">imagem</a> . </p><br><p><img src="https://habrastorage.org/webt/mp/lz/0y/mplz0y9uleukwz68u-lyc35wlqk.jpeg"></p><br><p>  Mas isso √© intelig√™ncia?  Tendo desenvolvido essa abordagem, em um futuro pr√≥ximo, as redes neurais poder√£o responder a perguntas em v√≠deo como "Dois pardais sentados nos fios, um deles voou para longe, quantos pardais restaram?".  Isso √© matem√°tica real, em casos um pouco mais complicados, inacess√≠veis aos animais e no n√≠vel da educa√ß√£o escolar humana.  Especialmente se, exceto os pardais, houver mamas sentadas ao lado deles, mas eles n√£o precisam ser levados em considera√ß√£o, pois a pergunta era apenas sobre pardais.  Sim, definitivamente ser√° intelig√™ncia. </p><br><h2 id="3-obuchenie-s-podkrepleniem-reinforcement-learning">  3. Aprendizado por Refor√ßo </h2><br><p>  A id√©ia √© muito simples: incentivar a√ß√µes que levem √† recompensa e evitar levar ao fracasso.  Essa √© uma maneira universal de aprender e, obviamente, pode definitivamente levar √† cria√ß√£o de uma IA forte.  Portanto, tem havido tanto interesse no aprendizado por refor√ßo nos √∫ltimos anos. </p><br><div class="spoiler">  <b class="spoiler_title">Misture, mas n√£o agite</b> <div class="spoiler_text"><p>  Obviamente, √© melhor criar uma IA forte combinando as tr√™s abordagens.  Nas fotos e com o treinamento de refor√ßo, voc√™ pode obter IA de n√≠vel animal.  E adicionando nomes textuais de objetos √†s imagens (uma piada, √© claro - for√ßando a IA a assistir v√≠deos onde as pessoas interagem e conversam, como quando ensinam um beb√™), e treinando novamente em uma matriz de texto para obter conhecimento (um an√°logo de nossa escola e universidade), em teoria, voc√™ pode obter AI de n√≠vel humano.  Capaz de falar. </p></div></div><br><p>  O aprendizado refor√ßado tem uma grande vantagem.  No simulador, voc√™ pode criar um modelo simplificado do mundo.  Portanto, para uma figura humana, basta 17 graus de liberdade, em vez de 700 em uma pessoa viva (n√∫mero aproximado de m√∫sculos).  Portanto, no simulador, voc√™ pode resolver o problema em uma dimens√£o muito pequena. </p><br><p>  Olhando para o futuro, os algoritmos modernos de Aprendizado por Refor√ßo n√£o s√£o capazes de controlar arbitrariamente o modelo de uma pessoa, mesmo com 17 graus de liberdade.  Ou seja, eles n√£o podem resolver o problema de otimiza√ß√£o, onde existem 44 n√∫meros na entrada e 17. Na entrada, √© poss√≠vel fazer isso apenas em casos muito simples, com o ajuste fino das condi√ß√µes iniciais e dos hiperpar√¢metros.  E mesmo neste caso, por exemplo, para ensinar um modelo human√≥ide com 17 graus de liberdade para correr, e come√ßando de uma posi√ß√£o em p√© (que √© muito mais simples), voc√™ precisa de v√°rios dias de c√°lculos em uma poderosa GPU.  E casos um pouco mais complicados, por exemplo, aprender a levantar-se de uma pose arbitr√°ria, podem nunca aprender nada.  Isso √© um fracasso. </p><br><p>  Al√©m disso, todos os algoritmos de Aprendizagem por Refor√ßo funcionam com redes neurais deprimente pequenas, mas n√£o conseguem lidar com a aprendizagem de grandes.  As grandes redes de convolu√ß√£o s√£o usadas apenas para reduzir a dimens√£o da imagem a v√°rios recursos, que s√£o alimentados com algoritmos de aprendizado com refor√ßo.  O mesmo human√≥ide em execu√ß√£o √© controlado por uma rede Feed Forward com duas ou tr√™s camadas de 128 neur√¥nios.  S√©rio?  E com base nisso, estamos tentando construir uma IA forte? </p><br><p>  Para tentar entender por que isso acontece e o que h√° de errado com o aprendizado por refor√ßo, voc√™ primeiro precisa se familiarizar com as arquiteturas b√°sicas do aprendizado por refor√ßo moderno. </p><br><p>  A estrutura f√≠sica do c√©rebro e do sistema nervoso √© sintonizada pela evolu√ß√£o para o tipo espec√≠fico de animal e suas condi√ß√µes de vida.  Assim, no curso da evolu√ß√£o, uma mosca desenvolveu um sistema nervoso e esse trabalho de neurotransmissores nos g√¢nglios (um an√°logo do c√©rebro em insetos) para evitar rapidamente um mata-moscas.  Bem, n√£o de um mata-moscas, mas de p√°ssaros que pescam h√° 400 milh√µes de anos (brincando, os pr√≥prios p√°ssaros apareceram 150 milh√µes de anos atr√°s, provavelmente de sapos 360 milh√µes de anos).  Um rinoceronte suficiente para o sistema nervoso e o c√©rebro girarem lentamente em dire√ß√£o ao alvo e come√ßarem a correr.  E l√°, como eles dizem, o rinoceronte tem uma vis√£o ruim, mas esse n√£o √© o problema dele. </p><br><p>  Mas, al√©m da evolu√ß√£o, cada indiv√≠duo espec√≠fico, a partir do nascimento e ao longo da vida, trabalha precisamente o mecanismo de aprendizado usual com refor√ßo.  No caso de mam√≠feros <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">e insetos tamb√©m</a> , o sistema de dopamina faz esse trabalho.  Seu trabalho √© cheio de segredos e nuances, mas tudo se resume ao fato de que, no caso de um pr√™mio, o sistema de dopamina, atrav√©s de mecanismos de mem√≥ria, de alguma forma corrige as conex√µes entre os neur√¥nios que estavam ativos imediatamente antes.  √â assim que a mem√≥ria associativa √© formada. </p><br><p>  Que, devido √† sua associatividade, √© ent√£o usado na tomada de decis√£o.  Simplificando, se a situa√ß√£o atual (neur√¥nios ativos atuais nessa situa√ß√£o) atrav√©s da mem√≥ria associativa ativar neur√¥nios do prazer, o indiv√≠duo seleciona as a√ß√µes que realizou em uma situa√ß√£o semelhante e das quais se lembrou.  "Escolhe a√ß√µes" √© uma defini√ß√£o ruim.  N√£o h√° escolha.  Os neur√¥nios da mem√≥ria de prazer simplesmente ativados, fixados pelo sistema de dopamina para uma determinada situa√ß√£o, ativam automaticamente os neur√¥nios motores, levando √† contra√ß√£o muscular.  Isso ocorre se uma a√ß√£o imediata for necess√°ria. </p><br><p>  Aprendizado artificial com refor√ßo, como um campo de conhecimento, √© necess√°rio resolver esses dois problemas: </p><br><h3 id="1-podobrat-arhitekturu-neyroseti-chto-dlya-nas-uzhe-sdelala-evolyuciya">  1. Escolha a arquitetura da rede neural (o que a evolu√ß√£o j√° fez por n√≥s) </h3><br><p>  A boa not√≠cia √© que fun√ß√µes cognitivas mais altas desempenhadas no neoc√≥rtex em mam√≠feros (e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no estriado em corv√≠deos</a> ) s√£o executadas em uma estrutura aproximadamente uniforme.  Aparentemente, isso n√£o precisa de alguma "arquitetura" rigidamente prescrita. </p><br><p>  A diversidade das regi√µes do c√©rebro deve-se provavelmente a raz√µes puramente hist√≥ricas.  Quando, √† medida que evolu√≠am, novas partes do c√©rebro cresceram acima das b√°sicas que sobraram dos primeiros animais.  Pelo princ√≠pio que funciona - n√£o toque.  Por outro lado, em pessoas diferentes, as mesmas partes do c√©rebro reagem √†s mesmas situa√ß√µes.  Isso pode ser explicado tanto pela associatividade (caracter√≠sticas e "neur√¥nios da av√≥" naturalmente formados nesses locais durante o processo de aprendizado) quanto pela fisiologia.  Que as vias de sinaliza√ß√£o codificadas nos genes levam exatamente a essas √°reas.  N√£o h√° consenso, mas voc√™ pode ler, por exemplo, este artigo recente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Intelig√™ncia biol√≥gica e artificial"</a> . </p><br><h3 id="2-nauchitsya-obuchat-neyronnye-seti-po-principam-obucheniya-s-podkrepleniem">  2. Aprenda a treinar redes neurais de acordo com os princ√≠pios de aprendizado com refor√ßo </h3><br><p>  Isso √© o que o moderno aprendizado por refor√ßo est√° fazendo principalmente.  E quais s√£o os sucessos?  Na verdade n√£o. </p><br><h1 id="naivnyy-podhod">  Abordagem ing√™nua </h1><br><p>  Parece que √© muito simples treinar uma rede neural com refor√ßo: realizamos a√ß√µes aleat√≥rias e, se recebermos uma recompensa, consideramos as a√ß√µes tomadas como ‚Äúrefer√™ncia‚Äù.  N√≥s os colocamos na sa√≠da da rede neural como etiquetas padr√£o e treinamos a rede neural pelo m√©todo de propaga√ß√£o reversa do erro, para que ele produz exatamente essa sa√≠da.  Bem, o treinamento de rede neural mais comum.  E se as a√ß√µes levaram ao fracasso, ignore esse caso ou suprima-o (definimos outros como sa√≠da, por exemplo, qualquer outra a√ß√£o aleat√≥ria).  Em geral, essa id√©ia repete o sistema de dopamina. </p><br><p>  Mas se voc√™ tentar treinar qualquer rede neural dessa maneira, n√£o importa qu√£o complexa seja a arquitetura, a distribui√ß√£o direta recursiva, convolucional ou comum, ent√£o ... N√£o funcionar√°! </p><br><p>  Porque  Desconhecido </p><br><p>  Acredita-se que o sinal √∫til seja t√£o pequeno que se perca no contexto do ru√≠do.  Portanto, a rede n√£o aprende o m√©todo padr√£o de propaga√ß√£o traseira do erro.  Uma recompensa acontece muito raramente, talvez uma vez em centenas ou mesmo milhares de etapas.  E at√© o LSTM se lembra de um m√°ximo de 100-500 pontos na hist√≥ria e, em seguida, apenas em tarefas muito simples.  Mas nos mais complexos, se houver 10 a 20 pontos na hist√≥ria, j√° √© bom. </p><br><p>  Mas a raiz do problema est√° justamente em recompensas muito raras (pelo menos em tarefas de valor pr√°tico).  No momento, n√£o sabemos como treinar redes neurais que lembrariam casos isolados.  O que o c√©rebro lida com brilho.  Voc√™ pode se lembrar de algo que aconteceu apenas uma vez na vida.  E, a prop√≥sito, a maior parte do treinamento e do trabalho do intelecto √© constru√≠da justamente nesses casos. </p><br><p>  Isso √© algo como um terr√≠vel desequil√≠brio de classes no campo do reconhecimento de imagens.  Simplesmente n√£o h√° maneiras de lidar com isso.  O melhor que eles conseguiram criar at√© agora √© simplesmente enviar √† entrada da rede, juntamente com novas situa√ß√µes, situa√ß√µes bem-sucedidas do passado armazenadas em um buffer especial artificial.  Ou seja, ensinar constantemente n√£o apenas casos novos, mas tamb√©m casos antigos bem-sucedidos.  Naturalmente, esse buffer n√£o pode ser aumentado infinitamente, e n√£o est√° claro o que exatamente armazenar nele.  Ainda tentando consertar temporariamente os caminhos dentro da rede neural, ativos durante um caso de sucesso, para que o treinamento subsequente n√£o os substitua.  Uma analogia bastante pr√≥xima do que est√° acontecendo no c√©rebro, na minha opini√£o, embora eles tamb√©m n√£o tenham conseguido muito sucesso nessa dire√ß√£o.  Como as novas tarefas treinadas em seus c√°lculos usam os resultados dos neur√¥nios que saem dos caminhos congelados, o sinal interfere apenas nos novos congelados, e as tarefas antigas param de funcionar.  H√° outra abordagem curiosa: treinar a rede com novos exemplos / tarefas apenas na dire√ß√£o ortogonal √†s tarefas anteriores ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://arxiv.org/abs/1810.01256</a> ).  Isso n√£o substitui a experi√™ncia anterior, mas limita drasticamente a capacidade da rede. </p><br><p>  Uma classe separada de algoritmos projetados para lidar com esse desastre (e ao mesmo tempo dando esperan√ßa de obter uma IA forte) est√° sendo desenvolvida no Meta-Learning.  Essas s√£o tentativas de ensinar uma rede neural v√°rias tarefas ao mesmo tempo.  N√£o no sentido de reconhecer imagens diferentes em uma tarefa, ou seja, tarefas diferentes em dom√≠nios diferentes (cada uma com seu pr√≥prio cen√°rio de distribui√ß√£o e solu√ß√£o).  Diga, reconhe√ßa fotos e ande de bicicleta ao mesmo tempo.  At√© agora, o sucesso tamb√©m n√£o √© muito bom, j√° que geralmente tudo se resume a preparar uma rede neural antecipadamente com pesos universais gerais e, em seguida, rapidamente, em apenas alguns passos de descida de gradiente, para adapt√°-los a uma tarefa espec√≠fica.  Exemplos de algoritmos de meta-aprendizado s√£o MAML e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Reptile</a> . </p><br><p>  Em geral, apenas esse problema (a incapacidade de aprender com exemplos √∫nicos bem-sucedidos) p√µe fim ao treinamento moderno com refor√ßo.  Todo o poder das redes neurais antes desse triste fato √© at√© agora impotente. </p><br><p>  Esse fato, de que a maneira mais simples e √≥bvia n√£o funciona, for√ßou os pesquisadores a voltar ao cl√°ssico Aprendizado por Refor√ßo baseado em tabela.  O qual, como ci√™ncia, apareceu at√© na antiga antiguidade, quando as redes neurais n√£o estavam no projeto.  Mas agora, em vez de calcular manualmente os valores em tabelas e f√≥rmulas, vamos usar um aproximador t√£o poderoso como redes neurais como o objetivo funciona!  Essa √© a ess√™ncia do aprendizado moderno por refor√ßo.  E sua principal diferen√ßa do treinamento usual de redes neurais. </p><br><h1 id="q-learning-i-dqn">  Q-learning e DQN </h1><br><p>  O aprendizado por refor√ßo (mesmo antes das redes neurais) nasceu como uma id√©ia bastante simples e original: vamos fazer a√ß√µes aleat√≥rias e, em seguida, para cada c√©lula da tabela e cada dire√ß√£o do movimento, calculamos de acordo com uma f√≥rmula especial (chamada equa√ß√£o de Bellman, esta palavra voc√™ em quase todos os trabalhos com treinamento de refor√ßo) qu√£o boa √© essa c√©lula e a dire√ß√£o escolhida.  Quanto maior esse n√∫mero, maior a probabilidade de esse caminho levar √† vit√≥ria. </p><br><p><img src="https://habrastorage.org/webt/nx/zm/-7/nxzm-7q1_oc-igaim3j0mrr7vki.png"></p><br><p>  N√£o importa em que c√©lula voc√™ aparece, mova-se ao longo do verde crescente!  (em dire√ß√£o ao n√∫mero m√°ximo nas laterais da c√©lula atual). </p><br><p>  Esse n√∫mero √© chamado Q (da palavra qualidade - qualidade de escolha, obviamente), e o m√©todo √© Q-learning.  Substituindo a f√≥rmula para calcular esse n√∫mero por uma rede neural, ou melhor, ensinando a rede neural usando essa f√≥rmula (mais alguns truques conectados puramente √† matem√°tica do treinamento de redes neurais), o Deepmind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">adotou o</a> m√©todo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DQN</a> .  Foi ele quem, em 2015, ganhou a pilha de jogos da Atari e marcou o in√≠cio da revolu√ß√£o no Deep Reinforcement Learning. </p><br><p>  Infelizmente, esse m√©todo em sua arquitetura funciona apenas com a√ß√µes discretas e discretas.  No DQN, o estado atual (a situa√ß√£o atual) √© alimentado √† entrada da rede neural e, na sa√≠da, a rede neural prediz o n√∫mero Q. E como a sa√≠da da rede lista todas as a√ß√µes poss√≠veis de uma s√≥ vez (cada uma com seu pr√≥prio Q previsto), verifica-se que a rede neural no DQN implementa a fun√ß√£o cl√°ssica Q (s, a) do Q-learning.  Q  state  action (  Q(s,a)    s  a).     argmax          Q   ,     . </p><br><p>        Q,      .        ,    Q- (..    Q   ,   ).    .      ,        (Exploration),       ,     ,        .         ,        . </p><br><p>   ,    ?    5     Atari,  continuous    ? ,    -1..1      0.1,          ,     Atari.         . ,        .       10    .  -      ,    10       .     .   DQN     ,      17     .  ,    ,  . </p><br><p>             DQN, ,   ,   continuous  (      ): DDQN, DuDQN, BDQN, CDQN, NAF, Rainbow. ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Direct Future Prediction (DFP)</a> ,    DQN     .    Q   , DFP          ,    .          .                     ,     . ,   ,       ,     . </p><br><p>    ,          Reinforcement Learning. </p><br><p><img src="https://habrastorage.org/webt/f3/lc/3t/f3lc3tno4mpvwren4rfocva9iv8.png"></p><br><h1 id="policy-gradient"> Policy Gradient </h1><br><p>       state,       (  ,        ).   ,  actions,  .   ,   R   .        (   ),   (  ).        .     . </p><br><p> ,    R   ,   ,        .       !       .   ""       labels (       ),      .     ,   ,      R. </p><br><p>   Policy Gradient.      ‚Äî    ,     R,        .    ‚Äî     ,       ,      .     ,    . </p><br><h1 id="actor-critic-ddpg"> Actor-critic, DDPG </h1><br><p>   ,       ‚Äî      ,       .  ,  Q-   ,    DQN.      state,    action(s).       state,     action,   ,      Q     : Q(s,a). </p><br><p> ,   Q(s,a),    (  critic, ),       ,      (  , actor),       R.       ,    .     actor-critic.       Policy Gradient,        ,    .   . </p><br><p>      DDPG.       actions,     continuous . DDPG   continuous  DQN    . </p><br><p><img src="https://habrastorage.org/webt/9b/th/fk/9bthfkh7cfpymc6_f6xrt7sica0.png"></p><br><h1 id="advantage-actor-critic-a3ca2c"> Advantage Actor Critic (A3C/A2C) </h1><br><p>             critic  Q(s,a) ‚Äî   ,   actor,     DDPG.         ,   . </p><br><p>     ,     .   ,           ,    <strong></strong> ,    . ,    ,    ,      (     ,   ). </p><br><p>          Q(s,a),    Advantage: A(s,a) = Q(s,a) ‚Äî V(s).  A(s,a)     Q(s,a)  ,    ‚Äî      ,    V(s).  A(s,a) &gt; 0,      ,    .  A(s,a) &lt; 0,      ,     , ..   . </p><br><p>    V(s)     state   ,     (    s,  a).         ‚Äî     state,   V(s).       ,      state,   V(s). </p><br><p>  ,    Q(s,a)     r,     ,         A = r ‚Äî V(s). </p><br><p>   ,    V(s) (          ),    ‚Äî actor  critic,    !     state,        head:    actions,    V(s).     c , ..       state. ,      . </p><br><p><img src="https://habrastorage.org/webt/eo/ph/5y/eoph5ypzawg11tachwn-nt_7nyg.png"></p><br><p>        V(s)      .     V(s),          action (     ),      .    Dueling Q-Network (DuDQN),  Q(s,a)      Q(s,a) = V(s) + A(a),    . </p><br><p> Asynchronous Advantage Actor Critic (A3C)   ,   ,     actor.        batch  .  ,     actor.     ,     ,   .   ,   A2C ‚Äî   A3C,         actor       ( ). A2C    ,    ,     . </p><br><h1 id="trpo-ppo-sac"> TRPO, PPO, SAC </h1><br><p> ,    . </p><br><p>        ,     .   Reinforcement Learning     ,      ,   ,          ‚Äî      ,  .   . </p><br><p>   ‚Äî TRPO  PPO,   state-of-the-art,   Actor-Critic.  PPO         RL.  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI Five</a>    Dota 2. </p><br><p>   ,       TRPO  PPO ‚Äî        ,     .   ,   A3C/A2C   ,    .  ,   policy     ,     . -  gradient clipping        ,     .   ,         (       ,      ),      ,   ,    -  . </p><br><p> Recentemente, o algoritmo Soft-Actor-Critic (SAC) vem ganhando popularidade.  N√£o √© muito diferente do PPO, apenas um objetivo foi adicionado ao aprender a aumentar a entropia na pol√≠tica.  Torne o comportamento do agente mais aleat√≥rio.  N√£o, n√£o √© assim.  Que o agente foi capaz de agir em situa√ß√µes mais aleat√≥rias.  Isso aumenta automaticamente a confiabilidade da pol√≠tica, assim que o agente estiver pronto para qualquer situa√ß√£o aleat√≥ria.  Al√©m disso, o SAC exige um pouco menos de exemplos de treinamento que o PPO e √© menos sens√≠vel √†s configura√ß√µes de hiperpar√¢metro, o que tamb√©m √© uma vantagem.  No entanto, mesmo com o SAC, para treinar um human√≥ide para correr com 17 graus de liberdade, a partir de uma posi√ß√£o ereta, voc√™ precisa de cerca de 20 milh√µes de quadros e cerca de um dia de c√°lculo em uma GPU.  Condi√ß√µes iniciais mais dif√≠ceis, digamos, para ensinar um human√≥ide a se levantar de uma pose arbitr√°ria, podem n√£o ser ensinadas. </p><br><p>  Total, a recomenda√ß√£o geral no Aprendizado por Refor√ßo moderno: use SAC, PPO, DDPG, DQN (nessa ordem, decrescente). </p><br><h1 id="model-based">  Baseado em modelo </h1><br><p>  Existe outra abordagem interessante, indiretamente relacionada ao aprendizado por refor√ßo.  Isso √© para construir um modelo do ambiente e us√°-lo para prever o que acontecer√° se tomarmos alguma a√ß√£o. </p><br><p>  Sua desvantagem √© que n√£o diz de forma alguma quais a√ß√µes devem ser tomadas.  Apenas sobre o resultado deles.  Mas essa rede neural √© f√°cil de treinar - basta treinar em qualquer estat√≠stica.  Acontece algo como um simulador de mundo baseado em uma rede neural. </p><br><p>  Depois disso, geramos um grande n√∫mero de a√ß√µes aleat√≥rias, e cada uma √© conduzida por esse simulador (por uma rede neural).  E olhamos qual deles trar√° a recompensa m√°xima.  H√° uma pequena otimiza√ß√£o - para gerar n√£o apenas a√ß√µes aleat√≥rias, mas desviar-se de acordo com a lei normal da trajet√≥ria atual.  E, de fato, se levantarmos a m√£o, com alta probabilidade, precisamos continuar a aument√°-la.  Portanto, primeiro voc√™ precisa verificar os desvios m√≠nimos da trajet√≥ria atual. </p><br><p>  O truque aqui √© que mesmo um simulador f√≠sico primitivo como MuJoCo ou pyBullet produz cerca de 200 FPS.  E se voc√™ treina uma rede neural para prever o avan√ßo de pelo menos algumas etapas, em ambientes simples, √© poss√≠vel obter facilmente lotes de previs√µes de 2000 a 5000 por vez.  Dependendo do poder da GPU, √© poss√≠vel obter uma previs√£o para dezenas de milhares de a√ß√µes aleat√≥rias por segundo devido √† paraleliza√ß√£o na GPU e √† velocidade computacional na rede neural.  A rede neural aqui simplesmente atua como um simulador muito r√°pido da realidade. </p><br><p>  Al√©m disso, como a rede neural pode prever o mundo real (essa √© uma abordagem baseada em modelos, no sentido geral), o treinamento pode ser realizado inteiramente na imagina√ß√£o, por assim dizer.  Esse conceito no Aprendizado por Refor√ßo √© chamado Mundos dos Sonhos, ou Modelos do Mundo.  Isso funciona bem, uma boa descri√ß√£o est√° aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://worldmodels.github.io</a> .  Al√©m disso, possui uma contrapartida natural - sonhos comuns.  E rolagem m√∫ltipla de eventos recentes ou planejados na cabe√ßa. </p><br><h1 id="imitation-learning">  Imita√ß√£o de aprendizagem </h1><br><p>  Devido √† impot√™ncia de os algoritmos de Aprendizagem por Refor√ßo n√£o funcionarem em grandes dimens√µes e tarefas complexas, as pessoas decidiram pelo menos repetir as a√ß√µes de especialistas na forma de pessoas.  Aqui, bons resultados foram alcan√ßados (inating√≠veis pelo aprendizado convencional por refor√ßo).  Ent√£o, OpenAI acabou por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">passar o jogo Montezuma's Revenge</a> .  O truque acabou sendo simples - colocar o agente imediatamente no final do jogo (no final da trajet√≥ria mostrada pela pessoa).  L√°, com a ajuda do PPO, gra√ßas √† proximidade da recompensa final, o agente aprende rapidamente a caminhar ao longo da trajet√≥ria.  Depois disso, colocamos-lhe um pouco de volta, onde ele rapidamente aprende a chegar ao lugar que j√° estudou.  E assim, mudando gradualmente o ponto de "renascimento" ao longo da trajet√≥ria at√© o in√≠cio do jogo, o agente aprende a passar / simular a trajet√≥ria do especialista ao longo do jogo. </p><br><p>  Outro resultado impressionante √© a repeti√ß√£o de movimentos para pessoas filmadas no Motion Capture: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMimic</a> .  A receita √© semelhante ao m√©todo OpenAI: cada epis√≥dio n√£o inicia do in√≠cio do caminho, mas de um ponto aleat√≥rio ao longo do caminho.  Em seguida, o PPO estuda com √™xito os arredores deste ponto. </p><br><p>  Devo dizer que o sensacional algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Go-Explore</a> do Uber, que passou pela Vingan√ßa de Montezuma com pontos de registro, n√£o √© um algoritmo de Aprendizado por Refor√ßo.  Esta √© uma pesquisa aleat√≥ria regular, mas come√ßando com uma c√©lula celular visitada aleatoriamente (uma c√©lula grossa na qual v√°rios estados se enquadram).  E somente quando a trajet√≥ria at√© o final do jogo √© encontrada por uma pesquisa t√£o aleat√≥ria, a rede neural √© treinada usando o Imitation Learning.  De maneira semelhante √† OpenAI, ou seja,  come√ßando no final da trajet√≥ria. </p><br><h1 id="curiosity-lyubopytstvo">  Curiosidade (Curiosidade) </h1><br><p>  Um conceito muito importante no aprendizado por refor√ßo √© a curiosidade.  Na natureza, √© um motor de pesquisa ambiental. </p><br><p>  O problema √© que, como medida de curiosidade, voc√™ n√£o pode usar um simples erro de previs√£o de rede, o que acontecer√° a seguir.  Caso contr√°rio, essa rede ficar√° pendurada na frente da primeira √°rvore com folhagem ondulada.  Ou na frente de uma TV com comuta√ß√£o aleat√≥ria de canal.  Como o resultado devido √† complexidade ser√° imposs√≠vel de prever e o erro sempre ser√° grande.  No entanto, esta √© precisamente a raz√£o pela qual n√≥s (pessoas) adoramos olhar para a folhagem, a √°gua e o fogo.  E como as outras pessoas trabalham =).  Mas temos mecanismos de prote√ß√£o para n√£o ficarmos para sempre. </p><br><p>  Um desses mecanismos foi inventado como o Modelo Inverso na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Explora√ß√£o Conduzida pela Curiosidade por</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><br></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Previs√£o auto-supervisionada</a> .  Em resumo, um agente (rede neural), al√©m de prever quais a√ß√µes s√£o melhores executadas em uma determinada situa√ß√£o, tenta adicionalmente prever o que acontecer√° com o mundo ap√≥s as a√ß√µes tomadas.  E ele usa essa previs√£o do mundo para o pr√≥ximo passo, para que ele e o passo atual possam prever suas a√ß√µes tomadas anteriormente (sim, √© dif√≠cil, voc√™ n√£o consegue descobrir sem um litro). </p><br><p>  Isso leva a um efeito curioso: o agente fica curioso apenas para o que ele pode influenciar com suas a√ß√µes.  Ele n√£o pode influenciar os galhos de uma √°rvore, de modo que eles se tornam desinteressantes para ele.  Mas ele pode andar pelo distrito, por isso est√° curioso para andar e explorar o mundo. </p><br><p>  No entanto, se o agente tiver um controle remoto de TV que alterna canais aleat√≥rios, ele poder√° afet√°-lo!  E ele ficar√° curioso em clicar nos canais ad infinitum (j√° que ele n√£o pode prever qual ser√° o pr√≥ximo canal, porque √© aleat√≥rio).  Uma tentativa de contornar esse problema foi feita pelo Google no trabalho de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Curiosidade epis√≥dica por meio da acessibilidade</a> . </p><br><p>  Mas talvez o melhor resultado de ponta seja devido √† curiosidade, atualmente a OpenAI √© propriet√°ria da id√©ia de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Random Network Distillation (RND)</a> .  Sua ess√™ncia √© que √© necess√°ria uma segunda rede completamente inicializada aleatoriamente e o estado atual √© alimentado a ela.  E nossa principal rede neural em funcionamento est√° tentando adivinhar o resultado dessa rede neural.  A segunda rede n√£o √© treinada, permanece fixa o tempo todo como foi inicializada. </p><br><p>  Qual √© o objetivo?  O ponto √© que, se algum estado j√° foi visitado e estudado por nossa rede de trabalho, ele poder√° prever com mais ou menos sucesso a sa√≠da dessa segunda rede.  E se esse √© um estado novo, onde nunca estivemos, nossa rede neural n√£o ser√° capaz de prever a sa√≠da dessa rede RND.  Esse erro ao prever a sa√≠da daquela rede inicializada aleatoriamente √© usado como um indicador de curiosidade (ele oferece grandes recompensas se n√£o pudermos prever sua sa√≠da nessa situa√ß√£o). </p><br><p>  Por que isso funciona n√£o √© totalmente claro.  Mas eles escrevem que isso elimina o problema quando o destino da previs√£o √© estoc√°stico e quando n√£o h√° dados suficientes para prever o que acontecer√° a seguir (o que gera um grande erro de previs√£o nos algoritmos comuns de curiosidade).  De um jeito ou de outro, mas o RND realmente mostrou excelentes resultados de pesquisa com base na curiosidade nos jogos.  E lida com o problema da TV aleat√≥ria. </p><br><p>  Com o RND, a curiosidade no OpenAI pela primeira vez honestamente (e n√£o atrav√©s de uma pesquisa aleat√≥ria preliminar, como no Uber) passou o primeiro n√≠vel da Vingan√ßa de Montezuma.  Nem sempre e de maneira n√£o confi√°vel, mas de vez em quando acontece. </p><br><p><img src="https://habrastorage.org/webt/iw/jm/kb/iwjmkbze4r8efybc5-01pbqzjf8.png"></p><br><h1 id="chto-v-itoge">  Qual √© o resultado? </h1><br><p>  Como voc√™ pode ver, em apenas alguns anos, o Aprendizado por Refor√ßo j√° percorreu um longo caminho.  N√£o apenas algumas solu√ß√µes bem-sucedidas, como em redes convolucionais, em que conex√µes ressudais e pulam tornaram poss√≠vel treinar redes com centenas de camadas de profundidade, em vez de uma d√∫zia de camadas apenas com a fun√ß√£o de ativa√ß√£o Relu, que superou o problema de gradientes de fuga no sigm√≥ide e no tanh.  No aprendizado com refor√ßo, houve progresso nos conceitos e no entendimento das raz√µes pelas quais esta ou aquela vers√£o ing√™nua da implementa√ß√£o n√£o funcionou.  A palavra-chave "n√£o funcionou". </p><br><p>  Mas, do ponto de vista t√©cnico, tudo ainda se baseia nas previs√µes dos mesmos valores Q, V ou A.  N√£o h√° depend√™ncias de tempo em escalas diferentes, como no c√©rebro (o aprendizado de refor√ßo hier√°rquico n√£o conta, a hierarquia √© muito primitiva em compara√ß√£o com a associatividade no c√©rebro vivo).  Nenhuma tentativa de criar uma arquitetura de rede adaptada especificamente para o aprendizado por refor√ßo, como aconteceu com o LSTM e outras redes recorrentes para seq√º√™ncias de tempo.  O Aprendizado por Refor√ßo, ou pisa no local, regozijando-se em pequenos sucessos, ou se move em uma dire√ß√£o completamente errada. </p><br><p>  Eu gostaria de acreditar que, uma vez no aprendizado por refor√ßo, haver√° um avan√ßo na arquitetura das redes neurais, semelhante ao que aconteceu nas redes convolucionais.  E veremos um aprendizado de refor√ßo realmente funcionando.  Aprendendo exemplos isolados, trabalhando com mem√≥ria associativa e trabalhando em diferentes escalas de tempo. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt437020/">https://habr.com/ru/post/pt437020/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt437006/index.html">Revis√£o da impressora 3D Wanhao Duplicator 10</a></li>
<li><a href="../pt437008/index.html">PNL. O b√°sico. T√©cnicas. Autodesenvolvimento. Parte 1</a></li>
<li><a href="../pt437010/index.html">Ecos do passado: a experi√™ncia de Young na base do novo m√©todo de espectroscopia de raios-X</a></li>
<li><a href="../pt437014/index.html">A tarefa de N corpos ou como explodir uma gal√°xia sem sair da cozinha</a></li>
<li><a href="../pt437018/index.html">Algumas armadilhas da digita√ß√£o est√°tica em Python</a></li>
<li><a href="../pt437022/index.html">Bit de seguran√ßa de ru√≠do 0x22 (ataques de inje√ß√£o de falha, 35C3 e Wallet.fail)</a></li>
<li><a href="../pt437026/index.html">Google na Fran√ßa multado em 50 milh√µes de euros por GDPR por uso indevido de dados pessoais</a></li>
<li><a href="../pt437030/index.html">Automa√ß√£o da infraestrutura de um escrit√≥rio de luxo: como fica</a></li>
<li><a href="../pt437032/index.html">Instru√ß√µes de instala√ß√£o do NGINX ModSecurity</a></li>
<li><a href="../pt437034/index.html">Assobios universais: Revis√£o do Dongle USB Snom A230 e A210</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>