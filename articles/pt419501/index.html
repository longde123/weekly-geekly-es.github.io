<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéº üçü üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Aprendizado profundo: reconhecendo cenas e pontos de refer√™ncia em imagens üë©üèΩ‚Äçü§ù‚Äçüë®üèª üà≥ ‚ôæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hora de reabastecer o cofrinho de bons relat√≥rios em russo sobre Machine Learning! O pr√≥prio mealheiro n√£o ser√° reabastecido! 

 Desta vez, vamos nos ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizado profundo: reconhecendo cenas e pontos de refer√™ncia em imagens</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jugru/blog/419501/">  Hora de reabastecer o cofrinho de bons relat√≥rios em russo sobre Machine Learning!  O pr√≥prio mealheiro n√£o ser√° reabastecido! <br><br>  Desta vez, vamos nos familiarizar com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a</a> fascinante hist√≥ria de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Andrei Boyarov</a> sobre reconhecimento de cenas.  Andrey √© um pesquisador de vis√£o computacional envolvido em vis√£o de m√°quina no Mail.Ru Group. <br><br>  O reconhecimento de cena √© uma das √°reas amplamente usadas da vis√£o de m√°quina.  Essa tarefa √© mais complicada do que o reconhecimento estudado de objetos: a cena √© um conceito mais complexo e menos formalizado, √© mais dif√≠cil distinguir caracter√≠sticas.  A tarefa de reconhecer pontos tur√≠sticos segue do reconhecimento de cena: voc√™ precisa destacar lugares conhecidos na foto, garantindo um baixo n√≠vel de falsos positivos. <br><br>  S√£o <b>30 minutos de</b> v√≠deo da confer√™ncia Smart Data 2017. O v√≠deo √© conveniente para assistir em casa e em qualquer lugar.  Para aqueles que n√£o est√£o prontos para sentar-se tanto na tela ou que preferem perceber as informa√ß√µes em forma de texto, aplicamos uma descriptografia de texto completo, projetada na forma de habrosta. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/dL1-OrjtMvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Eu fa√ßo vis√£o de m√°quina no Mail.ru.  Hoje vou falar sobre como usamos o aprendizado profundo para reconhecer imagens de cenas e atra√ß√µes. <br><br>  A empresa surgiu com a necessidade de marcar e pesquisar por imagens do usu√°rio e, por isso, decidimos criar nossa pr√≥pria API do Computer Vision, parte da qual ser√° uma ferramenta de marca√ß√£o de cenas.  Como resultado dessa ferramenta, queremos obter algo como o mostrado na figura abaixo: o usu√°rio faz uma solicita√ß√£o, por exemplo, ‚Äúcatedral‚Äù e recebe todas as suas fotos com catedrais. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d18/5f8/a3b/d185f8a3bd0c73280cdae408fe84cca1.png"><br><br>  Na comunidade Computer Vision, o t√≥pico de reconhecimento de objetos em imagens foi estudado muito bem.  Existe um conhecido <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">concurso ImageNet</a> realizado h√° v√°rios anos e a parte principal √© o reconhecimento de objetos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/989/f18/b8a989f189970cabb0de00be7ff48afa.png"><br><br>  Basicamente, precisamos localizar algum objeto e classific√°-lo.  Nas cenas, a tarefa √© um pouco mais complicada, porque a cena √© um objeto mais complexo, consiste em um grande n√∫mero de outros objetos e no contexto que os une, portanto as tarefas s√£o diferentes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/be7/a6b/ba4be7a6bc9a4e0c2c7929644540799c.png"><br><br>  Na Internet, existem servi√ßos dispon√≠veis de outras empresas que implementam essa funcionalidade.  Em particular, esta √© a API do Google Vision ou da API do Microsoft Computer Vision, que pode encontrar cenas nas imagens. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/758/fd9/095/758fd909536fd930db97c025ca61ef38.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/eee/b28/6df/eeeb286df22a31809f507da7730b7d89.png"><br><br>  Resolvemos esse problema com a ajuda do aprendizado de m√°quina, portanto, para isso, precisamos de dados.  Existem duas bases principais para o reconhecimento de cenas em acesso aberto agora.  O primeiro deles apareceu em 2013 - esta √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a base</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SUN</a> da Universidade de Princeton.  Essa base consiste em centenas de milhares de imagens e 397 classes. <br><br>  A segunda base na qual treinamos √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a base Places2</a> do MIT.  Ela apareceu em 2013 em duas vers√µes.  O primeiro √© o Places2-Standart, uma base mais equilibrada com 1,8 milh√£o de imagens e 365 classes.  A segunda op√ß√£o - Places2-Challenge, cont√©m oito milh√µes de imagens e 365 classes, mas o n√∫mero de imagens entre as classes n√£o √© equilibrado.  No concurso ImageNet de 2016, a se√ß√£o Reconhecimento de cena incluiu o desafio Places2, e o vencedor mostrou o melhor resultado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">erro de classifica√ß√£o entre os 5</a> melhores, de cerca de 9%. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a10/ea2/881/a10ea2881a5248449dce8202ceaa28ff.png"><br><br>  Treinamos com base no Places2.  Aqui est√° um exemplo de imagem de l√°: √© um canyon, pista, cozinha, campo de futebol.  Esses s√£o objetos complexos completamente diferentes sobre os quais precisamos aprender a reconhecer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1cf/9b2/3ee/1cf9b23eedff87ae9bcbb2d8c4090551.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/602/617/1d1/6026171d19b38452ce0e4a071ba4e836.png"><br><br>  Antes de estudar, adaptamos as bases que temos para atender √†s nossas necessidades.  Existe um truque para o reconhecimento de objetos ao experimentar modelos em pequenas bases CIFAR-10 e CIFAR-100 em vez do ImageNet, e somente ent√£o os melhores treinam no ImageNet. <br><br>  Decidimos seguir o mesmo caminho, pegamos o banco de dados SUN, reduzimos, obtivemos 89 aulas, 50 mil imagens no trem e 10 mil imagens na valida√ß√£o.  Como resultado, antes do treinamento no Places2, montamos experimentos e testamos nossos modelos com base no SUN.  O treinamento leva apenas 6 a 10 horas, ao contr√°rio de v√°rios dias no Places2, o que permitiu realizar muito mais experimentos e torn√°-lo mais eficaz. <br><br>  Tamb√©m analisamos o pr√≥prio banco de dados do Places2 e percebemos que n√£o precis√°vamos de algumas classes.  Devido a considera√ß√µes de produ√ß√£o ou porque h√° poucos dados sobre elas, cortamos classes como, por exemplo, um aqueduto, uma casa na √°rvore, uma porta do celeiro. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f8/31f/60f/4f831f60fd70ec7f74ea2bb0a29c69f5.png"><br><br>  Como resultado, ap√≥s todas as manipula√ß√µes, obtivemos o banco de dados do Places2, que cont√©m 314 classes e meio milh√£o de imagens (em sua vers√£o padr√£o), na vers√£o Challenge, cerca de 7,5 milh√µes de imagens.  Constru√≠mos treinamento nessas bases. <br><br>  Al√©m disso, ao visualizar as classes restantes, descobrimos que h√° muitas delas para produ√ß√£o, elas s√£o muito detalhadas.  E, para isso, aplicamos o mecanismo de mapeamento de cena quando algumas classes s√£o combinadas em uma comum.  Por exemplo, conectamos tudo o que estava conectado √†s florestas em uma floresta, tudo conectado aos hospitais - em um hospital, com hot√©is - em um hotel. <br><br>  Usamos o mapeamento de cena apenas para teste e para o usu√°rio final, porque √© mais conveniente.  No treinamento, usamos todas as 314 classes padr√£o.  Chamamos a base resultante de Peneirar Locais. <br><br><h2>  Abordagens, Solu√ß√µes </h2><br>  Agora considere as abordagens que usamos para resolver esse problema.  Na verdade, essas tarefas est√£o relacionadas √† abordagem cl√°ssica - redes neurais convolucionais profundas. <br><br>  A imagem abaixo mostra uma das primeiras redes cl√°ssicas, mas j√° cont√©m os principais blocos de constru√ß√£o usados ‚Äã‚Äãnas redes modernas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a4/61f/7f7/0a461f7f7ae8a9264f5fadb69271f29d.png"><br><br>  S√£o camadas convolucionais, s√£o camadas de tra√ß√£o, camadas totalmente conectadas.  Para determinar a arquitetura, verificamos os topos das competi√ß√µes ImageNet e Places2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7dc/658/7d0/7dc6587d046a968988d4f52fa7bcbb3f.png"><br><br>  Podemos dizer que as principais arquiteturas l√≠deres podem ser divididas em duas fam√≠lias: Inception e a fam√≠lia ResNet (rede residual).  No decorrer dos experimentos, descobrimos que a fam√≠lia ResNet √© mais adequada para nossa tarefa e realizamos o pr√≥ximo experimento nessa fam√≠lia. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a8/828/85b/1a882885b70533dd0341bd853d2818fd.png"><br><br>  O ResNet √© uma rede profunda que consiste em um grande n√∫mero de blocos residuais.  Esse √© o seu principal componente b√°sico, que consiste em v√°rias camadas com pesos e conex√£o de atalho.  Como resultado desse projeto, esta unidade aprende quanto o sinal de entrada x difere da sa√≠da f (x).  Como resultado, podemos construir redes desses blocos e, durante o treinamento, a rede nas √∫ltimas camadas pode tornar os pesos pr√≥ximos a zero. <br><br>  Assim, podemos dizer que a pr√≥pria rede decide a profundidade necess√°ria para resolver algumas das tarefas.  Gra√ßas a essa arquitetura, √© poss√≠vel construir redes de grande profundidade com um n√∫mero muito grande de camadas.  O vencedor do ImageNet 2014 continha apenas 22 camadas, o ResNet excedeu esse resultado e j√° continha 152 camadas. <br><br>  A principal pesquisa da ResNet √© melhorar e construir adequadamente um bloco residual.  A imagem abaixo mostra uma vers√£o empiricamente e matematicamente s√≥lida que oferece o melhor resultado.  Essa constru√ß√£o do bloco permite que voc√™ lide com um dos problemas fundamentais da aprendizagem profunda - um gradiente gradual. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/7a4/398/0207a439827163bc14895a47a1489ddc.png"><br><br>  Para treinar nossas redes, usamos o framework Torch escrito em Lua por causa de sua flexibilidade e velocidade e, para o ResNet, criamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">implementa√ß√£o do ResNet pelo Facebook</a> .  Para validar a qualidade da rede, usamos tr√™s testes. <br><br>  O primeiro teste de valor do Places √© a valida√ß√£o de muitos conjuntos do Sift do Places.  O segundo teste √© Peneirar Locais usando o Mapeamento de Cena e o terceiro √© o teste em Nuvem mais pr√≥ximo da situa√ß√£o de combate.  Imagens de funcion√°rios tiradas da nuvem e rotuladas manualmente.  Na imagem abaixo, existem dois exemplos de tais imagens. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/796/6d0/066/7966d0066c9257617e81f290fbbc6283.png"><br><br>  Come√ßamos a medir e treinar redes, compar√°-las entre si.  O primeiro √© o benchmark ResNet-152, que vem com o Places2, o segundo √© o ResNet-50, que treinamos no ImageNet e treinamos em nossa base, o resultado j√° foi melhor.  Em seguida, eles fizeram o ResNet-200, tamb√©m treinado no ImageNet, e mostrou o melhor resultado no final. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d9/0de/320/6d90de3208968f8120c1f4be1e79f74e.png"><br><br>  Abaixo est√£o exemplos de trabalho.  Este √© um benchmark ResNet-152.  Previstos s√£o os r√≥tulos originais que a rede distribui.  Os r√≥tulos mapeados s√£o os r√≥tulos que vieram ap√≥s o Mapeamento de cena.  Pode-se ver que o resultado n√£o √© muito bom.  Ou seja, ela parece estar dando algo sobre o caso, mas n√£o muito bem. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/892/ceb/86e892ceb75b85dcf22bec203c0f4f3d.png"><br><br>  O pr√≥ximo exemplo √© a opera√ß√£o do ResNet-200.  J√° √© muito adequado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa6/621/d0d/fa6621d0dd29ac9d77da10378eec9454.png"><br><br><h2>  Aprimoramento ResNet </h2><br>  Decidimos tentar melhorar nossa rede e, a princ√≠pio, apenas tentamos aumentar a profundidade da rede, mas depois disso ficou muito mais dif√≠cil treinar.  Esse √© um problema conhecido. No ano passado, v√°rios artigos foram publicados sobre o assunto, que afirmam que a ResNet √©, de fato, um conjunto de um grande n√∫mero de redes comuns de v√°rias profundidades. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df2/0ad/023/df20ad0234f5eba5597939468f8afaf6.png"><br><br>  Blocos res, que est√£o no final da grade, d√£o uma pequena contribui√ß√£o para a forma√ß√£o do resultado final.  Parece mais promissor aumentar n√£o a profundidade da rede, mas sua largura, ou seja, o n√∫mero de filtros dentro do bloco Res. <br><br>  Essa ideia √© implementada pela Wide Residual Network, que surgiu em 2016.  Acabamos usando o WRN-50-2, que √© o ResNet-50 usual, com o dobro do n√∫mero de filtros na convolu√ß√£o do gargalo interno 3x3. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5be/f29/aed/5bef29aed5cfce3b0121ce69ee072c35.png"><br><br>  A rede mostra no ImageNet resultados semelhantes ao ResNet-200, que j√° usamos, mas, o mais importante, √© quase o dobro da velocidade.  Aqui est√£o duas implementa√ß√µes do bloco Residual na tocha; o par√¢metro que √© dobrado √© destacado com intensidade.  Este √© o n√∫mero de filtros na convolu√ß√£o interna. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1a/3fe/d24/f1a3fed24bf735ed30cfc407aac6ca79.png"><br><br>  Essas s√£o medidas nos testes do ResNet-200 ImageNet.  No come√ßo, pegamos o WRN-22-6, ele mostrou um resultado pior.  Ent√£o eles pegaram o WRN-50-2-ImageNet, treinaram-no, pegaram o WRN-50-2, treinaram no ImageNet e o treinaram no desafio Places2, e ele mostrou o melhor resultado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/736/fd6/afe/736fd6afe39702160b87ad095ef0edcd.png"><br><br>  Aqui est√° um exemplo do WRN-50-2 - um resultado bastante adequado em nossas fotos que voc√™ j√° viu. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b0a/442/e1f/b0a442e1f87a4f2e2a6c43a96677ce3d.png"><br><br>  E este √© um exemplo de trabalho em fotografias de combate, tamb√©m com sucesso. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/924/715/6bf/9247156bf459bd5b005b4180d36a1da5.png"><br><br>  Obviamente, n√£o existem obras de muito sucesso.  A ponte de Alexandre III em Paris n√£o foi reconhecida como ponte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dd/c66/9ec/3ddc669ecf4aabe61b103dafbc4aabb2.png"><br><br><h2>  Melhoria do modelo </h2><br>  Pensamos em como melhorar esse modelo.  A fam√≠lia ResNet continua melhorando, com novos artigos sendo publicados.  Em particular, em 2016 foi publicado um artigo interessante PyramidNet, que mostrou resultados promissores no CIFAR-10/100 e ImageNet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdd/816/e1b/fdd816e1b970be4465ff8200b7f0cb56.png"><br><br>  A id√©ia n√£o √© aumentar drasticamente a largura do bloco Residual, mas faz√™-lo gradualmente.  Treinamos v√°rias op√ß√µes para essa rede, mas, infelizmente, ela mostrou resultados um pouco piores que o nosso modelo de combate. <br><br>  Na primavera de 2018, o modelo ResNext foi lan√ßado, tamb√©m uma id√©ia promissora: dividir o bloco Residual em v√°rios blocos paralelos de tamanho menor e largura menor.  Isso √© semelhante √† id√©ia de Inicia√ß√£o, tamb√©m experimentamos.  Mas, infelizmente, ela mostrou resultados piores que o nosso modelo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/712/3c4/e0f/7123c4e0f7f826f9dab8a1791d4eb3cd.png"><br><br>  Tamb√©m experimentamos v√°rias abordagens "criativas" para melhorar nossos modelos.  Em particular, tentamos usar o mapeamento de ativa√ß√£o de classe (CAM), ou seja, esses s√£o os objetos que a rede observa quando classifica a imagem. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1bf/639/ea4/1bf639ea430bd03bafeeacf8acb29a09.png"><br><br>  Nossa id√©ia era que inst√¢ncias da mesma cena deveriam ter objetos iguais ou semelhantes a uma classe CAM.  Tentamos usar essa abordagem.  No come√ßo eles pegaram duas redes.  Um √© treinado pela ImageNet, o segundo √© o nosso modelo, que queremos melhorar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/47d/ac5/538/47dac55383f098852c9a69a3d50f2cc1.png"><br><br>  Pegamos a imagem, percorremos a rede 2, adicionamos o CAM para a camada e alimentamos a entrada da rede 1. Percorremos a rede 1, adicionamos os resultados √† fun√ß√£o de perda da rede 2, continuamos com as novas fun√ß√µes de perda. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/860/c3e/9c3/860c3e9c37f6d6ffd91d3cdd36d67682.png"><br><br>  A segunda op√ß√£o √© executar a imagem pela rede 2, pegar o CAM, aliment√°-la com a entrada da rede 1 e, nesses dados, simplesmente treinamos a rede 1 e usamos o conjunto a partir dos resultados da rede 1 e da rede 2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fb/140/95b/5fb14095b117b05582a79a43ee0eadb6.png"><br><br>  N√≥s treinamos novamente o nosso modelo no WRN-50-2, pois na rede 1 usamos o ResNet-50 ImageNet, mas n√£o foi poss√≠vel aumentar de alguma forma significativamente a qualidade do nosso modelo. <br><br>  Mas continuamos a pesquisar sobre como melhorar nossos resultados: estamos treinando novas arquiteturas da CNN, em particular a fam√≠lia ResNet.  Tentamos experimentar o CAM e consideramos v√°rias abordagens com um processamento mais inteligente de patches de imagem - parece-nos que essa abordagem √© bastante promissora. <br><br><h2>  Reconhecimento de Marcos </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/11b/9da/4c3/11b9da4c376bcee091afb70d3dbb0745.png"><br><br>  Temos um bom modelo para reconhecer cenas, mas agora queremos descobrir alguns lugares ic√¥nicos, ou seja, pontos tur√≠sticos.  Al√©m disso, os usu√°rios costumam tirar fotos deles ou tirar fotos contra o fundo. <br><br>  Queremos que o resultado n√£o seja apenas as catedrais, como na imagem do slide, mas o sistema para dizer: "H√° Notre Dame de Paris e as catedrais em Praga". <br><br>  Quando resolvemos esse problema, encontramos algumas dificuldades. <br><br><ol><li>  Praticamente n√£o h√° estudos sobre esse t√≥pico e n√£o h√° dados prontos no dom√≠nio p√∫blico. <br></li><li>  Um pequeno n√∫mero de imagens "limpas" em dom√≠nio p√∫blico para cada atra√ß√£o. <br></li><li>  N√£o est√° totalmente claro o que √© um marco nos edif√≠cios.  Por exemplo, uma casa com torres na pra√ßa.  Leo Tolstoy, em Petersburgo, o TripAdvisor n√£o considera atra√ß√µes, mas o Google considera. <br></li></ol><br>  Come√ßamos coletando um banco de dados, compilamos uma lista de 100 cidades e usamos a API do Google Places para fazer o download de dados JSON para pontos de interesse dessas cidades. <br><br>  Os dados foram filtrados e analisados ‚Äã‚Äãe, de acordo com a lista, baixamos 20 imagens da Pesquisa Google para cada atra√ß√£o.  O n√∫mero 20 √© retirado de considera√ß√µes emp√≠ricas.  Como resultado, obtivemos uma base de 2827 atra√ß√µes e cerca de 56 mil imagens.  Essa √© a base sobre a qual treinamos nosso modelo.  Para validar nosso modelo, usamos dois testes. <br><br>  Teste em nuvem - s√£o imagens de nossos funcion√°rios, rotuladas manualmente.  Ele cont√©m 200 fotos em 15 cidades e 10 mil imagens sem atra√ß√µes.  O segundo √© o teste de pesquisa.  Foi constru√≠do usando a pesquisa Mail.ru, que cont√©m de 3 a 10 imagens para cada atra√ß√£o, mas, infelizmente, esse teste est√° sujo. <br><br>  N√≥s treinamos os primeiros modelos, mas eles mostraram resultados ruins no teste Cloud em fotos de combate. <br><br>  Aqui est√° um exemplo da imagem em que fomos treinados e um exemplo de fotografia de combate.  O problema das pessoas √© que elas s√£o frequentemente fotografadas contra o pano de fundo das vistas.  Nas imagens que obtivemos da pesquisa, n√£o havia pessoas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b1d/8a6/63b/b1d8a663bf52f5910cf35415a93c296a.png"><br><br>  Para combater isso, adicionamos aumento "humano" durante o treinamento.  Ou seja, usamos abordagens padr√£o: rota√ß√µes aleat√≥rias, corte aleat√≥rio de parte da imagem e assim por diante.  Mas tamb√©m no processo de aprendizado, adicionamos pessoas a algumas imagens aleatoriamente. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12c/6c7/b54/12c6c7b54ae4d64065ee7fefa18acb72.png"><br><br>  Essa abordagem nos ajudou a resolver o problema com as pessoas e a obter um modelo de qualidade aceit√°vel. <br><br><h2>  Modelos de cenas finas </h2><br>  Como treinamos o modelo: existe alguma base de treinamento, mas √© bem pequena.  Mas sabemos que uma atra√ß√£o tur√≠stica √© um caso especial da cena.  E n√≥s temos um bom modelo de cena.  Decidimos trein√°-la para os pontos tur√≠sticos.  Para fazer isso, adicionamos v√°rias camadas BN e totalmente conectadas na parte superior da rede, treinamos elas e os tr√™s principais blocos residenciais.  O restante da rede estava congelado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5e/b4a/e80/a5eb4ae80781b95eec46821dd05d5cba.png"><br><br>  Al√©m disso, para treinamento, usamos a fun√ß√£o de perda central n√£o padronizada.  Durante o treinamento, a perda do Center tenta "separar" representantes de diferentes classes em diferentes grupos, como mostra a figura. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cc/2e8/bbc/8cc2e8bbc85f79dfb6301742d7c3d4ac.png"><br><br>  No treinamento, adicionamos outra classe "n√£o √© uma atra√ß√£o tur√≠stica".  E a perda de centro n√£o foi aplicada a essa classe.  Em uma fun√ß√£o de perda mista, foi realizado treinamento. <br><br>  Depois de treinarmos a rede, cortamos a √∫ltima camada de classifica√ß√£o e, quando a imagem passa pela rede, ela se transforma em um vetor num√©rico chamado incorpora√ß√£o. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf2/ebc/0fd/cf2ebc0fd7dc3af7770e6dd2c616fc74.png"><br><br>  Para construir ainda mais um sistema de reconhecimento de pontos de refer√™ncia, criamos vetores de refer√™ncia para cada classe.  Pegamos cada classe de atra√ß√µes da multid√£o e exibimos as imagens pela rede.  Eles se casaram e pegaram o vetor do meio, chamado de vetor de refer√™ncia da classe. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5e9/8ef/f32/5e98eff32e669f6c3c0b7245c25bc4ea.png"><br><br>  Para determinar as vistas na foto, executamos a imagem de entrada atrav√©s da rede e sua incorpora√ß√£o √© comparada com o vetor de refer√™ncia de cada classe.  Se o resultado da compara√ß√£o for menor que o limite, acreditamos que n√£o h√° ponto de interesse na imagem.  Caso contr√°rio, teremos a classe com o maior valor de compara√ß√£o. <br><br><h2>  Resultados do teste </h2><br><ul><li>  No teste da nuvem, a precis√£o das imagens foi de 0,616, n√£o das imagens - 0,981 </li><li>  A precis√£o m√©dia de 0,669 foi obtida no teste de busca e a completude m√©dia foi de 0,576. </li></ul><br>  Na Pesquisa, eles n√£o obtiveram resultados muito bons, mas isso √© explicado pelo fato de o primeiro ser bastante "sujo" e o segundo ter caracter√≠sticas - entre as atra√ß√µes, existem diferentes jardins bot√¢nicos semelhantes em todas as cidades. <br><br>  Havia uma id√©ia para o reconhecimento de cena para treinar primeiro a rede, o que determinar√° a m√°scara de cena, ou seja, remover√° os objetos do primeiro plano e depois o alimentar√° no pr√≥prio modelo, que reconhece cenas de imagem sem essas √°reas, onde o fundo √© obstru√≠do.  Mas n√£o est√° muito claro o que exatamente precisa ser removido da camada frontal, qual m√°scara √© necess√°ria. <br><br>  Ser√° algo bastante complicado e inteligente, porque nem todo mundo entende quais objetos pertencem √† cena e quais s√£o sup√©rfluos.  Por exemplo, pessoas em um restaurante podem ser necess√°rias.  Esta √© uma decis√£o n√£o trivial, tentamos fazer algo semelhante, mas n√£o deu bons resultados. <br><br>  Aqui est√° um exemplo de trabalho em fotografias de combate. <br><br>  Exemplos de trabalhos bem-sucedidos: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c53/ca4/413/c53ca44130bc3b9a845a7f21a681a8e0.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/774/c01/642774c0174cb07eeb52c055cb92b8d4.png"><br><br>  Mas o trabalho malsucedido: nenhuma mira foi encontrada.  O principal problema do nosso modelo no momento n√£o √© que a rede confunda os pontos tur√≠sticos, mas que n√£o os encontra na foto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c9/aca/7ab/6c9aca7ab564401b4c9b5cea126fc68a.png"><br><br>  No futuro, planejamos coletar uma base para um n√∫mero ainda maior de cidades, encontrar novos m√©todos para treinar a rede para esta tarefa e determinar as possibilidades de aumentar o n√∫mero de classes sem treinar novamente a rede. <br><br><h2>  Conclus√µes </h2><br>  Hoje n√≥s: <br><br><ul><li>  Vimos quais conjuntos de dados est√£o dispon√≠veis para reconhecimento de cena; <br></li><li>  Vimos que a Wide Residual Network √© o melhor modelo; <br></li><li>  Discutimos outras possibilidades para aumentar a qualidade desse modelo; <br></li><li>  Examinamos a tarefa de reconhecer pontos tur√≠sticos, que dificuldades surgem; <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> N√≥s descrevemos o algoritmo para coletar a base e os m√©todos de ensino do modelo para reconhecer atra√ß√µes. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Posso dizer que as tarefas s√£o interessantes, mas pouco estudadas na comunidade. </font><font style="vertical-align: inherit;">√â interessante lidar com eles, porque voc√™ pode aplicar abordagens n√£o padronizadas que n√£o s√£o aplicadas no reconhecimento usual de objetos.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minuto de publicidade. </font><font style="vertical-align: inherit;">Se voc√™ gostou deste relat√≥rio da confer√™ncia SmartData, observe que o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmartData 2018</font></font></b></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser√° realizado em S√£o Petersburgo em 15 de outubro, uma </font><font style="vertical-align: inherit;">confer√™ncia para aqueles que est√£o imersos no mundo do aprendizado de m√°quina, an√°lise e processamento de dados. </font><font style="vertical-align: inherit;">O programa ter√° muitas coisas interessantes, o site j√° tem seus primeiros oradores e relat√≥rios.</font></font></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt419501/">https://habr.com/ru/post/pt419501/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt419485/index.html">H√° um aplicativo para isso: an√∫ncio do Mobius 2018 Moscow</a></li>
<li><a href="../pt419491/index.html">Como o STP funciona</a></li>
<li><a href="../pt419493/index.html">Por que voc√™ precisa do Splunk? An√°lise de eventos de seguran√ßa</a></li>
<li><a href="../pt419495/index.html">Quem "inventou" a condu√ß√£o √≥ssea, por que √© usada e qu√£o segura √© para a audi√ß√£o</a></li>
<li><a href="../pt419497/index.html">Teste da Impressora 3D Grande Hercules Strong</a></li>
<li><a href="../pt419503/index.html">O livro ‚ÄúAlgoritmos e estruturas de dados. Recupera√ß√£o de informa√ß√£o Java ¬ª</a></li>
<li><a href="../pt419505/index.html">Agora voc√™ pode receber pacotes sem avisos e passaportes por correio em todo o pa√≠s.</a></li>
<li><a href="../pt419507/index.html">Vis√£o geral da impressora 3D russa PICASO 3D Designer X da 3Dtool</a></li>
<li><a href="../pt419509/index.html">Rede neural artificial fot√¥nica</a></li>
<li><a href="../pt419511/index.html">typeof (T) vs. TypeOf‚ü®T‚ü©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>