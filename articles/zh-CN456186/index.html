<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏛️ 🧟 🌦️ CS231n：用于模式识别的卷积神经网络 🈷️ 🔀 👨🏾‍⚖️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="欢迎来到CS231n中的讲座之一：用于视觉识别的卷积神经网络 。 



 目录内容 


- 架构概述 
- 卷积神经网络中的层 -卷积层 -图层二次采样 -归一化层 -全连接层 -将完全连接的图层转换为卷积图层 
- 卷积神经网络架构 -图层模板 -层大小模式 -案例研究（LeNet，AlexN...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CS231n：用于模式识别的卷积神经网络</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456186/"><p> 欢迎来到<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">CS231n</a>中的讲座之一<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">：用于视觉识别的卷积神经网络</a> 。 </p><br><p><img src="https://habrastorage.org/webt/b0/yc/wm/b0ycwm3fl6uveqvlr-usz5w9iqa.png"></p><a name="habracut"></a><br><h1> 目录内容 </h1><br><ul><li> 架构概述 </li><li> 卷积神经网络中的层 <br>  -卷积层 <br>  -图层二次采样 <br>  -归一化层 <br>  -全连接层 <br>  -将完全连接的图层转换为卷积图层 </li><li> 卷积神经网络架构 <br>  -图层模板 <br>  -层大小模式 <br>  -案例研究（LeNet，AlexNet，ZFNet，GoogLeNet，VGGNet） <br>  -计算方面 </li><li> 进一步阅读 </li></ul><br><h1> 卷积神经网络（CNN / ConvNets） </h1><br><p> 卷积神经网络与上一章（参考CS231n课程的最后一章）研究的常规神经网络非常相似：它们由神经元组成，而神经元又包含可变的权重和位移。 每个神经元接收一些输入数据，计算标量积，并可以选择使用非线性激活函数。 像以前一样，整个网络是唯一可区分的评估功能：从一端的初始像素（图像）到另一端属于特定类别的概率分布。 这些网络在最后一个（完全连接的）层上仍然具有损耗功能（例如SVM / Softmax），并且上一章中有关普通神经网络的所有提示和建议也与卷积神经网络相关。 </p><br><p> 那么，发生了什么变化？ 卷积神经网络的体系结构明确涉及在输入端获取图像，这使我们能够考虑网络体系结构本身中输入数据的某些属性。 这些属性使您可以更有效地实现直接分配功能，并大大减少网络中的参数总数。 </p><br><h1> 架构概述 </h1><br><p> 我们回想起普通的神经网络。 正如我们在上一章中所看到的，神经网络接收输入数据（单个向量）并通过“推”经过一系列<em>隐藏层来对其进行转换</em> 。 每个隐藏层由一定数量的神经元组成，每个神经元都连接到上一层的所有神经元，并且每一层的神经元完全独立于同一级别的其他神经元。 最后一个完全连接的层称为“输出层”，在分类问题中是按等级分配等级。 </p><br><p>  <em>常规的神经网络无法很好地缩放较大的图像</em> 。 在CIFAR-10数据集中，图像尺寸为32x32x3（高32像素，宽32像素，3个颜色通道）。 为了处理此类图像，正常神经网络的第一个隐藏层中的完全连接的神经元将具有32x32x3 = 3072的权重。 这个量仍然可以接受，但是很明显，这种结构不适用于较大的图像。 例如，较大的图像-200x200x3，将导致权重数变为200x200x3 = 120,000。此外，我们将需要多个神经元，因此权重总数将迅速开始增长。 很明显，连通性过大，大量参数将迅速导致网络重新训练。 </p><br><p>  <em>神经元的3D表示</em> 。 卷积神经网络利用输入数据是图像这一事实，因此它们为这种类型的数据形成了更为敏感的体系结构。 特别是，与传统的神经网络不同，卷积神经网络中的层将神经元排列为3个维度-宽度，高度，深度（ <em>请注意</em> ：“深度”一词是指激活神经元的第3维，而不是指神经网络本身在其中测量的深度。层数）。 例如，来自CIFAR-10数据集的输入图像是3D表示形式的输入数据，其尺寸为32x32x3（宽度，高度，深度）。 稍后我们将看到，一层中的神经元将与上一层中的少量神经元相关联，而不是连接到该层中所有先前的神经元。 此外，CIFAR-10数据集的图像输出层的尺寸为1×1×10，因为在接近神经网络的末端时，我们会将图像尺寸减小为沿着深度（第3维）定位的类估计向量。 </p><br><p> 可视化： </p><br><div class="scrollable-table"><table><thead><tr><th> 标准神经网络 </th><th> 卷积神经网络 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/2da/120/014/2da120014faf76c47fa4294c7206e291.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/d45/f30/a26/d45f30a26e57d437828f90567867c96f.jpg"></td></tr></tbody></table></div><br><hr><br><p>  <em>左侧：</em>标准的3层神经网络。 <br>  <em>在右侧：</em>卷积神经网络的神经元具有3个维度（宽度，高度，深度），如其中一层所示。 每个卷积神经网络层将输入的3D表示转换为作为激活神经元的输出的3D表示。 在此示例中，红色输入层包含图像，因此其大小将等于图像的大小，并且深度将为3（三个通道-红色，绿色，蓝色）。 </p><br><blockquote> 卷积神经网络由层组成。 每一层都是一个简单的API：将输入3D表示转换为可微函数的输出3D表示，该函数可能包含也可能不包含参数。 </blockquote><br><h1> 用于构建卷积神经网络的层 </h1><br><p> 正如我们上面已经描述的，简单的卷积神经网络是一组层，其中每个层使用可微函数将一个表示转换为另一种表示。 我们使用三种主要类型的层来构建卷积神经网络： <em>卷积层</em> ， <em>子采样</em> <em>层</em>和<em>完全连接层</em> （与在普通神经网络中使用的<em>层</em>相同）。 我们按顺序排列这些层以获得SNA体系结构。 </p><br><p> <em>体系结构示例：概述。</em> 下面我们将详细介绍，但是对于CIFAR-10数据集而言，我们的卷积神经网络的架构可能是<code>[INPUT -&gt; CONV -&gt; RELU -&gt; POOL -&gt; FC]</code> 。 现在更详细： </p><br><ul><li>  <code>INPUT</code> [32x32x3]将包含图像像素的原始值，在我们的示例中，图像为32px宽，32px高和3个颜色通道R，G，B。 </li><li>  <code>CONV</code>层将产生一组输出神经元，这些神经元将与输入源图像的局部区域关联； 每个这样的神经元将计算其权重与与之关联的原始图像的一小部分之间的标量积。 例如，如果我们决定使用12个滤镜，则输出值可以是<code>323212</code>的3D表示形式。 </li><li>  <code>RELU</code>层将应用元素激活函数<code>max(0, x)</code> 。 此转换不会更改数据尺寸- <code>[32x32x12]</code> 。 </li><li>  <code>POOL</code>层将执行在两个维度（高度和宽度）中对图像进行采样的操作，结果将为我们提供新的3D表示形式<code>[161612]</code> 。 </li><li>  <code>FC</code>层（完全连接的层）将按类别计算等级，结果尺寸为<code>[1x1x10]</code> ，其中10个值中的每一个将对应于CIFAR-10的10类图像中特定类别的等级。 与传统的神经网络一样，该层的每个神经元将与前一层的所有神经元（3D表示）相关联。 </li></ul><br><p> 这就是卷积神经网络如何将原始图像从初始像素值逐层转换为最终类别估计的方式。 请注意，有些图层包含选项，有些则不包含。 特别是， <code>CONV/FC</code>层执行转换，这不仅取决于输入数据的功能，还取决于神经元本身的权重和位移的内部值。 另一方面， <code>RELU/POOL</code>层使用非参数化功能。  <code>CONV/FC</code>层中的参数将通过梯度下降进行训练，以便输入接收相应的正确输出标签。 </p><br><p> 总结一下： </p><br><ul><li> 卷积神经网络的架构以其最简单的表示形式是一组有序的层，这些层将图像的表示形式转换为另一种表示形式，例如类成员估计。 </li><li> 有几种不同类型的层（CONV-卷积层，FC-完全连接，RELU-激活功能，POOL-子样本层-最受欢迎）。 </li><li> 每个输入层接收3D表示，然后使用可微函数将其转换为输出3D表示。 </li><li> 每层可能有也可能没有参数（CONV / FC-有参数，RELU / POOL-否）。 </li><li> 每层可能有也可能没有超级参数（CONV / FC / POOL-有，RELU-否） </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d92/59b/e82/d9259be829b1cdb3d98a399ebc56defa.jpg"><br>  <em>初始表示包含图像的像素值（在左侧），并估计图像中的对象所属的类（在右侧）。</em>  <em>每个视图转换都标记为一列。</em> </p><br><h1> 卷积层 </h1><br><p>  <em>卷积层</em>是卷积神经网络构建的主要层。 </p><br><p>  <em>概述，无需深入研究大脑的功能。</em> 让我们首先尝试找出CONV层仍在计算的内容，而不用浸入和触摸大脑和神经元的对象。 卷积层参数由一组经过训练的滤波器组成。 每个过滤器是沿着宽度和高度的小网格，但在输入表示的整个深度上延伸。 </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/497/c0f/e18/497c0fe1851288e36fad00c004d4f9cf.png"></p><br><p> 例如，卷积神经网络第一层上的标准过滤器可以具有5x5x3的尺寸（5px-宽度和高度，3-颜色通道的数量）。 在直接传递过程中，我们沿着输入表示的宽度和高度移动（准确地说，我们折叠了）滤镜，并在任意点上计算了滤镜值和输入表示的相应值之间的标量积。 在沿输入表示形式的宽度和高度移动滤镜的过程中，我们形成了一个二维激活图，其中包含将此滤镜应用于输入表示的每个区域的值。 直观地，很明显，网络将告诉过滤器在看到某个视觉符号（例如，某个角度的直线或更高级别的轮状表示）时激活。 现在，我们已将所有滤镜应用于原始图像，例如，有12个。由于应用了12个滤镜，我们收到了12个尺寸为2的激活卡。为产生输出表示，我们将这些卡组合在一起（依次在第3维中）并得到一个表示尺寸[WxHx12]。 </p><br><p>  <em>连接大脑和神经元的概述。</em> 如果您是大脑和神经元的粉丝，您可以想象每个神经元“注视”输入表示的很大一部分并将该部分的信息传输到相邻的神经元。 下面我们将讨论神经元连通性，它们在空间中的位置以及共享参数的机制的详细信息。 </p><br><p>  <em>本地连接。</em> 例如，当我们处理具有大量维度的输入数据时，例如在图像的情况下，那么，正如我们已经看到的，绝对不需要将神经元与上一层中的所有神经元连接起来。 相反，我们只会将神经元连接到输入表示的局部区域。 连接的空间程度是超参数之一，被称为<em>感受野</em> （神经元的感受野是同一滤波器/卷积核的大小）。 沿第3维的连接度（深度）始终等于原始表示的深度。 再次专注于这一点非常重要，注意我们如何定义空间尺寸（宽度和高度）和深度：神经元连接在宽度和高度上是局部的，但<em>始终</em>延伸到输入表示的整个深度。 </p><br><p>  <em>示例1.</em>假设输入表示的大小为32x32x3（RGB，CIFAR-10）。 如果过滤器大小（神经元的接收场）为5×5，则卷积层中的每个神经元将在原始表示的5×5×3区域中具有权重，最终将导致建立5×5×3 = 75键（权重）+ 1个偏移参数。 请注意，深度的连接度应等于3，因为这是原始表示的尺寸。 </p><br><p>  <em>示例2.</em>假设输入表示的大小为16x16x20。 以大小为3x3的神经元的接受场为例，每个卷积层神经元将具有3x3x320 = 180个连接（权重）+1个位移参数。 请注意，连通性在宽度和高度上是局部的，但在深度上是完整的（20）。 </p><br><div class="scrollable-table"><table><thead><tr><th>  ＃1 </th><th>  ＃2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/490/db9/7a0/490db97a0f3fa98eb2f44e84764f8991.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/61f/e81/589/61fe81589ab491d1d3ba612b3bdf5b51.jpg"></td></tr></tbody></table></div><br><p>  <em>从左侧开始：</em>输入表示形式以红色显示（例如，一张32x332 CIFAR-10大小的图像），并显示了第一卷积层中神经元的表示形式的示例。 卷积层中的每个神经元仅与输入表示的局部区域相关联，但与深度（在示例中，沿所有颜色通道）完全相关。 请注意，图像中有很多神经元（在示例中-5），它们沿第3维（深度）定位-下面将给出有关这种排列的说明。 <br>  <em>在右侧：</em>来自神经网络的神经元仍然保持不变：它们仍在计算权重和输入数据之间的标量积，应用激活函数，但它们的连接性现在受到空间局部区域的限制。 </p><br><p>  <em>空间位置。</em> 我们已经弄清楚了卷积层中每个神经元与输入表示的连通性，但是尚未讨论这些神经元中有多少个或如何定位。 三个超级参数会影响输出视图的大小： <em>depth</em> ， <em>step</em>和<em>alignment</em> 。 </p><br><ol><li> 输出表示<em>的深度</em>是一个超级参数：它对应于我们要应用的过滤器的数量，每个过滤器都会学习原始表示中的其他内容。 例如，如果第一卷积层接收图像作为输入，则可以在特定区域或特定颜色的簇中存在不同方向的线的情况下激活沿第三维度（深度）的不同神经元。 在输入表示的相同区域“看”的一组神经元，我们将其称为<em>深列</em> （或“纤维”-纤维）。 </li><li> 我们需要确定过滤器移动的<em>步长</em> （偏移量，以像素为单位）。 如果步长为1，则我们将滤波器一次迭代移动1个像素。 如果步长为2（或什至更不常用，则为3或更大），则在一次迭代中每两个像素发生一次偏移。 较大的步长将导致较小的输出表示。 </li><li> 正如我们将很快看到的那样，有时有必要在边缘处用零来补充输入表示。 对齐大小（填充的零行数/行数）也是一个超参数。 使用对齐的一个不错的功能是对齐将使我们能够控制输出表示的尺寸（大多数情况下，我们将保留视图的原始尺寸-保留输入表示的宽度和高度以及输出表示的宽度和高度）。 </li></ol><br><p> 我们可以通过将输出表示形式表示为输入表示形式的大小（ <strong>W</strong> ），卷积层神经元的感受野的大小（ <strong>F</strong> ），阶跃（ <strong>S</strong> ）和边界处的对齐大小（ <strong>P</strong> ）的函数来计算最终表示形式。 您可以自己看到，用于计算输出表示形式中神经元数量的正确公式如下<strong>（W-F + 2P）/ S +1</strong> 。 例如，对于大小为7x7的输入表示和3x3的过滤器大小，步骤1和对齐方式0，我们得到大小为5x5的输出表示。 在第2步中，我们将获得3x3的输出表示。 让我们来看另一个示例，这次以图形方式说明： </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/90a/f0b/d67/90af0bd67ba498239688c81fd61bbc66.jpg"><br>  <em>空间安排的例证。.</em>  <em>在此示例中，只有一个空间维度（x轴），一个神经元的接收场<strong>F = 3</strong> ，输入表示大小<strong>W = 5</strong>且对齐方式<strong>P = 1</strong> 。</em>  <em><strong>左侧</strong> ：神经元的感受野以步长<strong>S = 1</strong>移动，结果是输出表示的大小为（5-3 + 2）/ 1 + 1 =5。 <strong>右侧</strong> ：神经元使用大小为<strong>S = 2</strong>的感受野，结果是输出表示的大小（5-3 + 2）/ 2 +1 =3。请注意，不能使用步长<strong>S = 3</strong> ，因为在此步长下，接收场将无法捕获图像的一部分。</em>  <em>如果使用公式，则（5-3 + 2）= 4不是3的倍数。在此示例中，神经元的权重为[1、0，-1]（如最右图所示），并且偏移量为零。</em>  <em>这些权重由所有黄色神经元共享。</em> </p><br><p>  <em>使用对齐方式</em> 。 请注意左侧的示例，该示例在输出中包含5个元素，在输出中包含5个元素。 之所以可行，是因为接收场（滤波器）的大小为3，并且我们使用比对<strong>P = 1</strong> 。 如果没有对齐，则输出表示的大小将等于3，因为恰好有那么多神经元适合在那里。 通常，以等于<strong>S = 1</strong>的步长设置对齐大小<strong>P =（F-1）/ 2</strong>可使您获得类似于输入表示形式的输出表示形式的大小。 在实践中经常使用类似的使用对齐的方法，当我们讨论卷积神经网络的体系结构时，我们将在下面讨论原因。 </p><br><p>  <em>步长限制</em> 。 请注意，负责空间排列的超参数也受到限制。 例如，如果输入表示的大小为<strong>W = 10</strong> ， <strong>P = 0</strong>并且接收场的大小<strong>F = 3</strong> ，则由于<strong>（W-F + 2P）/ S + 1 =（10- 3 + 0）/ 2 + 1 = 4.5</strong> ，它给出神经元数量的整数值。 因此，这种超参数配置被认为是无效的，用于卷积神经网络的库将抛出异常，强制对齐甚至切断输入表示。 正如我们将在本章的下一部分中看到的那样，在设计卷积神经网络的体系结构时，通过使用某些建议和“良好音调规则”可以减轻对卷积层超参数的定义。 </p><br><p>  <em>现实生活中的例子</em> 。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">卷积</a>神经网络<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">架构Krizhevsky等。</a> 在2012年赢得ImageNet竞赛的冠军，获得了227x227x3的图像。 在第一卷积层上，她使用大小为<strong>F = 11</strong> ，步长<strong>S = 4</strong>和对齐大小<strong>P = 0</strong>的接收场。 由于（227-11）/ 4 +1 = 55，并且卷积层的深度为<strong>K = 96</strong> ，因此演示文稿的输出尺寸为55x55x96。 此表示形式中的每个55x55x96神经元与输入表示形式中大小为11x11x3的区域相关联。 此外，深列中的所有96个神经元都与相同的11x11x3区域相关，但权重不同。 现在有些幽默-如果您决定结识原始文档（研究），请注意该文档指出输入内容接收到224x224图像，这是不正确的，因为（224-11）/ 4 + 1绝不提供整数值。 对于卷积神经网络故事中的人们，这种情况经常感到困惑。 我的猜测是，Alex使用了对齐大小<strong>P = 3</strong> ，但是忘记在文档中提及这一点。 </p><br><p>  <em>共享选项。</em> 卷积层中共享参数的机制用于控制参数的数量。 请注意上面的示例，因为您可以看到在第一卷积层上有55x55x96 = 290,400个神经元，每个神经元具有11x11x3 = 363权重+ 1偏移值。 总之，如果将这两个值相乘，则<em>仅</em>在卷积神经网络的第一层上获得290400x364 = 105705600参数。 显然，这非常重要！ </p><br><p> 事实证明，可以通过做一个假设来大大减少参数的数量：如果在位置（x，y）中计算的某些属性对我们很重要，那么在位置（x2，y2）中计算的该属性对我们也很重要。 换句话说，将深度的二维“层”表示为“深层”（例如[55x55x96]视图包含96个深层，每个深层的大小为55x55），我们将以相同的权重和位移构建深度的神经元。 通过这种共享参数的方案，我们示例中的第一个卷积层现在将包含96个唯一的权重集（每个深度层集），总共将有96x11x11x3 = 34,848个唯一权重或34,944个参数（+96偏移）。 此外，每个深层中的所有55x55神经元现在都将使用相同的参数。 实际上，在反向传播期间，此表示形式中的每个神经元将计算其自身权重的梯度，但是这些梯度将在每个深度层上求和，并且在每个级别仅更新一组权重。 </p><br><p> 请注意，如果同一深层中的所有神经元都使用相同的权重，则对于通过卷积层的直接传播，将计算神经元权重的值与输入数据之间的卷积。 这就是为什么习惯将单个权重集称为<strong>过滤器（核心）的原因</strong> 。 </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd6/2e1/d75/dd62e1d75bda9b592dabb91627d68aa6.jpg"><br>  <em>通过训练模型Krizhevsky等获得的过滤器示例。</em>  <em>此处显示的96个过滤器中的每个过滤器大小均为11x11x3，并且每个过滤器均由一个深层的所有55x55神经元共享。</em>  <em>请注意，共享相同权重的假设是有道理的：如果水平线的检测在图像的一部分中很重要，那么从直觉上显然，这种检测在图像的另一部分中很重要。</em>  <em>因此，每次重新训练以在卷积层中的图像的55x55的不同位置中找到水平线都是没有意义的。</em> </p><br><p> 应该记住，共享参数的假设可能并不总是有意义。 例如，如果将具有某种中心结构的图像馈送到卷积神经网络的输入，则我们希望能够在图像的一部分中学习一种属性，而在图像的另一部分中学习另一种属性。 一个实际的例子是居中的人脸图像。 可以假设可以在图像的不同区域中识别出不同的眼睛或头发迹象，因此，在这种情况下，可以使用权重松弛，并将该层称为<strong>局部连接</strong> 。 </p><br><p>  <strong>脾气暴躁的例子</strong> 。 先前的讨论应该转移到细节层面和带有代码的示例上。 假设输入表示形式是<code>X</code>一个<code>numpy</code>数组<code>X</code> 然后： </p><br><ul><li> 位置<code>(x,y)</code> ）处<em>的深列</em> （ <em>线程</em> ）将表示为<code>X[x,y,:]</code> 。 </li><li>  <em>较深的层</em> ，或我们之前称为这样的层-深度<code>d</code>处<em>的激活图</em>将表示为<code>X[:,:,d]</code> 。 </li></ul><br><p>  <em>卷积层的一个例子</em> 。 ,    <code>X</code>   <code>X.shape: (11,11,4)</code> .   ,    <strong>P=1</strong> ,    () <strong>F=5</strong>   <strong>S=1</strong> .     44,     — (11-5)/2+1=4.     (  <code>V</code> ),     (      ): </p><br><ul><li> <code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code> </li> <li> <code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code> </li> <li> <code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code> </li> <li> <code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code> </li> </ul><br><p> ,   <code>numpy</code> ,  <code>*</code>      .    ,    <code>W0</code>      <code>b0</code>  .    <code>W0</code>   <code>W0.shape: (5,5,4)</code> ,      5,    4.                   .       ,           ,        2  ( ).             : </p><br><ul><li> <code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code> </li> <li> <code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code> </li> <li> <code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code> </li> <li> <code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code> </li> <li> <code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (,       <code>y</code> ) </li><li> <code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (,      ) </li></ul><br><p>         —    <code>W1</code>   <code>b1</code> .      ,               <code>V</code> .    ,         , , <code>ReLU</code> ,        .      . </p><br><p> <strong></strong> .     : </p><br><ul><li>      <strong>W1 x H1 x D1</strong> </li><li>  4 -: <br><ul><li>   <strong>K</strong> , </li><li>    <strong>F</strong> , </li><li>   <strong>S</strong> , </li><li>   <strong>P</strong> . </li></ul></li><li>     <strong>W2 x H2 x D2</strong> ,  <br><ul><li> <strong>W2 = (W1 — F + 2P)/S + 1</strong> </li><li> <strong>H2 = (H1 — F + 2P)/S + 1</strong> </li><li> <strong>D2 = K</strong> </li></ul></li><li>      <strong>F x F x D1</strong>    ,  <strong>(F x F x D1) x K</strong>   <strong>K</strong> . </li><li>   , <code>d</code> - ( <strong>W2 x H2</strong> )       <code>d</code> -      <strong>S</strong>      <code>d</code> -. </li></ul><br><p>    -  <strong>F = 3, S = 1, P = 1</strong> .        .      "   ". </p><br><p> <strong>.</strong>        .   3D-   ( —  ,  — ,  —  ),        —   .    <strong>W1 = 5, H1 = 5, D1 = 3</strong> ,     <strong>K = 2, F = 3, S = 2, P = 1</strong> . ,       33,     2.        (5 — 3 + 2)/2 + 1 = 3.  ,  ,   <strong>P = 1</strong>        .         ,         ()  ,   . </p><br><p> (   ,     html+css   ,       ) </p><br><p> <strong>    </strong> .               ().                   : </p><br><ol><li>        <strong>im2col</strong> . ,        227x227x3         11113   4,           11113 = 363 .   ,     4    ,   (227 — 11) / 4 + 1 = 55     ,          <strong>X_col</strong>  3633025,               3025.  ,   ,    ,  (),           . </li><li>          . ,    96   11113,       <strong>W_row</strong>  96363. </li><li>               — <strong>np.dot(W_row, X_col)</strong> ,           .         963025. </li><li>              555596. </li></ol><br><p>   , ,     —              ,    . ,    ,      —        (,    BLAS API).  ,    <strong>im2col</strong>        ,        . </p><br><p> <strong> </strong> .   (  )   (   ,    )     (  - ).     ,     . </p><br><p> <strong>11 </strong> .          11,       <a href="">Network in Network</a> .  ,      11,   ,       . ,    2-  ,   11    (     ).         ,       ,         3-  ,         . ,     32323,          11, ,  ,        3  (R, G, B —  , ). </p><br><p> <strong>   </strong> .      -      <em></em> .           .           ,   <em></em> .       <strong>w</strong>  3     <strong>x</strong> : <strong>w[0] <em>x[0] + w[1]</em> x[1] + w[2] <em>x[2] <strong>.      0.    1       :</strong> w[0]</em> x[0] + w[1] <em>x[2] + w[2]</em> x[4]</strong> .      ""  1    .        ,             . ,    2    33,     ,             55 (   55 <em>  </em> ).              . </p><br><h1>   </h1><br><p>   —           .         ,             ,        .          ,       MAX.     22   2,        2 ,    75% .   MAX            22.     .   ,  : </p><br><ul><li>    <strong>W1 x H1 x D1</strong> </li><li>  2 -: <br><ul><li>    <strong>F</strong> , </li><li>  <strong>S</strong> , </li></ul></li><li>    <strong>W2 x H2 x D2</strong> , : <br><ul><li> <strong>W2 = (W1 — F)/S + 1</strong> </li><li> <strong>H2 = (H1 — F)/S + 1</strong> </li><li> <strong>D2 = D1</strong> </li></ul></li><li>   ,         </li><li>       (zero-padding    ). </li></ul><br><p>    ,           :    <strong>F=3, S=2</strong> (   <em> </em> ),    — <strong>F=2, S=2</strong> .      -   . </p><br><p> <strong>   </strong> .      ,       , ,       L2-.         ,           ,      . </p><br><div class="scrollable-table"><table><thead><tr><th> #1 </th><th> #2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/cd7/174/14d/cd717414dcf32dac4df73c00f1e7c6c3.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/1a4/b2a/379/1a4b2a3795d8f073e921d766e70ce6ec.jpg"></td></tr></tbody></table></div><br><p> <em>              . <strong></strong> :       22422464        22   2,     11211264.  ,      . <strong></strong> :     —    (max-pooling),      2.      4  (   22)</em> </p><br><p> <strong> </strong> .      ,     max(a,b)    —            .  ,              (  <em></em> ),        . </p><br><p> <strong>  </strong> .        ,           . ,   <a href="">  :   </a> ,           .             .              ,     (VAEs)     (GANs). ,     -   ,    . </p><br><h1>   </h1><br><p>            ,      ,       . ,      ,            .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a> . </p><br><h1>   </h1><br><p>           ,       .             . </p><br><h1>       </h1><br><p>  ,           ,                 (  ).      -   ,        .    ,         : </p><br><ul><li>       ,       .  ,   ,   ,   ,                  ,     . </li><li> ,         . ,    <strong>K=4096</strong> ( ),     7712          - <strong>F=7, P=0, S=1, K=4096</strong> .        ,               114096,      . </li></ul><br><p> <strong>    </strong> .    ,            .        ,       2242243                  77512 (     AlexNet,    ,     5  ,           7 — 224/2/2/2/2/2 = 7).   AlexNet      4096 , ,     1000 ,     .               : </p><br><ul><li>    ,  ""    77512,       <strong>F=7</strong> ,       114096. </li><li>           <strong>F=1</strong> ,      114096. </li><li>           <strong>F=1</strong> ,      111000. </li></ul><br><p>    ,  ,     (   )   <strong>W</strong>         . ,       "" ()             . </p><br><p> ,     224224  ,  77512   —    32 ,        384384       1212512,   384/32 = 12.                 ,    ,    ,   661000,   (12 — 7)/1 + 1 = 6.  ,        111000     66    384384 . </p><br><blockquote>        (  )     384384,   224244   32 ,    ,        . </blockquote><p>  ,             ,      36 ,    36    .        ,    ,         .                   . </p><br><p> ,           ,     32 ?          (   ). ,        16 ,             2 :               16     . </p><br><h1>     </h1><br><p>        ,  ,   3   :  ,   (    ,    )   .        ReLU   ,   -  .                . </p><br><p>            CONV-RELU-,    POOL-       ,        .  -      .      , ,   .  ,        : </p><br><pre> <code class="plaintext hljs">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?] * M -&gt; [FC -&gt; RELU]*K -&gt; FC</code> </pre> <br><p>   <code>*</code>  ,  <code>POOL?</code>    .  , <code>N &gt;= 0</code> ( <code>N &lt;= 3</code> ), <code>M &gt;= 0</code> , <code>K &gt;= 0</code> ( <code>K &lt; 3</code> ). ,       ,     : </p><br><ul><li> <code>INPUT -&gt; FC</code> ,   . <code>N = M = K = 0</code> . </li><li> <code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code> </li> <li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC</code> ,           . </li><li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC</code> .     2      .  ,        ,                   . </li></ul><br><p> <em>               </em> .      3    33 ( RELU   ,  ).          ""   33  .      ""   33     ,     —     55.        ""  33     ,    —   77.  ,      33           77.         ""  77 (  )      ,    . -,        ,      3      ,       . -,        <strong>C</strong> ,   ,    77     <strong>(C(77)) = 49xxC</strong> ,        33    <strong>3((33)) = 27</strong> .   ,                 ,            .           —          ,      . </p><br><p> <strong></strong> .    ,       ,       Google,         Microsoft.               . </p><br><p> <strong> :  ,        ImageNet.</strong>                ,    ,   90%       .       — "  ":  ,       ,        ,         ImageNet —   ,       .               . </p><br><h1>      </h1><br><p>         -,        .    ,      : </p><br><p> <strong> </strong> ( )    2  .    32 (, CIFAR-10), 64, 96 (, STL-10),  224 (, ImageNet), 384  512. </p><br><p>  <strong> </strong>      (, 33 ,  55),    <strong>S=1</strong> ,    ,    ,         .  , <strong>F=3</strong>   <strong>P=1</strong>        .  <strong>F=5, P=2</strong> .    <strong>F</strong>   ,   <strong>P=(F-1)/2</strong>     .   -       (  77),                 . </p><br><p> <strong> </strong>      .            22 ( <strong>F=2</strong> )   2 ( <strong>S=2</strong> ). ,     75%    (- ,       ). ,   ,     33 ( )  2 ( ).       33   ,               .       . </p><br><p> <em>      .</em>     ,          ,        .   ,      1       ,                   ,             . </p><br><p> <em>     1   ?</em>       .     ,    1        (       ),        . </p><br><p> <em>  ?</em>                ,    .         ,     ,            ,          . </p><br><p> <em>    </em> .    (        ),       ,     . ,      64     33   1   2242243,       22422464. , ,  10  ,  72   ( ,       ).            GPU,     .  ,             77   2.  ,     AlexNet,    1111   4. </p><br><h1>    </h1><br><p>           .    : </p><br><ul><li> <strong>LeNet</strong> .         Yann LeCun  1990.       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">LeNet</a> ,     ZIP-,   . </li><li> <strong>AlexNet</strong> .  ,        ,  Alex Krizhevsky, Ilya Sutskever  Geoff Hinton. AlexNet     ImageNet ILSVRC  2012         ( : 16%  26%).        LeNet,   ,       (               ). </li><li> <strong>ZFNet</strong> .  ILSVRC 2013       Matthew Zeiler  Rob Fergus.      ZFNet.     AlexNet,     -,                 . </li><li> <strong>GoogLeNet</strong> .  ILSVRC 2014       Szegedy et al.  Google.      Inception-,         (4   60   AlexNet).                ,      ,     .       ,    — Inveption-v4. </li><li> <strong>VGGNet</strong> .    2014 ILSVRC    Karen Simonyan  Andrew Zisserman,       VGGNet.               ,        .      16   +        (33    22   ).            .    VGGNet —        (140).          ,       ,          ,       . </li><li> <strong>ResNet</strong> . Residual-   Kaiming He et al.     ILSVRC 2015.        .          .            (  2016). </li></ul><br><p> <strong>VGGNet  </strong> .   VGGNet    .   VGGNet    ,         33,  1   1,       22   2.          (     )    : </p><br><pre> <code class="plaintext hljs">INPUT: [224x224x3] memory: 224*224*3=150K weights: 0 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864 POOL2: [112x112x64] memory: 112*112*64=800K weights: 0 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456 POOL2: [56x56x128] memory: 56*56*128=400K weights: 0 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 POOL2: [28x28x256] memory: 28*28*256=200K weights: 0 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 POOL2: [14x14x512] memory: 14*14*512=100K weights: 0 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 POOL2: [7x7x512] memory: 7*7*512=25K weights: 0 FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448 FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216 FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000 TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd) TOTAL params: 138M parameters</code> </pre> <br><p>          ,    ,     (  )     ,        .        100     140 . </p><br><h1>   </h1><br><p>           .   GPU  3/4/6  ,   GPU — 12  .      ,    : </p><br><ul><li>  :           ,      (  ). ,      .          ,                         . </li><li>  : ,    ,         .    ,      ,     3  . </li><li>            ,         ,      .. </li></ul><br><p>           (,   ),          .    ,    4     (      4 ,       —  8),       1024  ,    ,      .   " ",        ,        . </p><br><p> …   call-to-action — ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">YouTube的</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">电报</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN456186/">https://habr.com/ru/post/zh-CN456186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN456172/index.html">第2部分：RocketChip：连接RAM</a></li>
<li><a href="../zh-CN456174/index.html">如何组织学生马拉松比赛101.第二部分</a></li>
<li><a href="../zh-CN456178/index.html">Android主题和样式没有魔法。 以及如何使用SwitchCompat烹饪它们</a></li>
<li><a href="../zh-CN456182/index.html">蓝牙音频：有关配置文件，编解码器和设备的最详细信息</a></li>
<li><a href="../zh-CN456184/index.html">软件无线电-它如何工作？ 第8部分</a></li>
<li><a href="../zh-CN456188/index.html">令牌，刷新令牌，并为REST请求创建异步包装</a></li>
<li><a href="../zh-CN456192/index.html">从整体到微服务：M.Video-Eldorado和MegaFon的经验</a></li>
<li><a href="../zh-CN456194/index.html">Go数据结构备忘单</a></li>
<li><a href="../zh-CN456196/index.html">关于SCRUM的基本误解</a></li>
<li><a href="../zh-CN456200/index.html">互联网历史：ARPANET-起源</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>