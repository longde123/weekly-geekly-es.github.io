<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>✋🏼 📓 🧑🏽‍🤝‍🧑🏻 Konfigurasikan kluster Kubernetes HA pada logam kosong dengan kubeadm. Bagian 1/3 🍠 👈🏽 🍏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bagian 2/3 di sini 
 Bagian 3/3 di sini 


 Halo semuanya! Pada artikel ini saya ingin merampingkan informasi dan berbagi pengalaman membuat dan mengg...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Konfigurasikan kluster Kubernetes HA pada logam kosong dengan kubeadm. Bagian 1/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/439562/"><p>  <strong>Bagian 2/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>di sini</strong></a> <br>  <strong>Bagian 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>di sini</strong></a> </p><br><p>  Halo semuanya!  Pada artikel ini saya ingin merampingkan informasi dan berbagi pengalaman membuat dan menggunakan cluster internal Kubernetes. </p><br><p>  Selama beberapa tahun terakhir, teknologi orkestrasi wadah ini telah mengambil langkah besar ke depan dan telah menjadi semacam standar perusahaan bagi ribuan perusahaan.  Beberapa menggunakannya dalam produksi, yang lain hanya mengujinya pada proyek, tetapi gairah di sekitarnya, tidak peduli bagaimana Anda mengatakannya, bersinar dengan serius.  Jika Anda belum pernah menggunakannya sebelumnya, saatnya untuk mulai berkencan. </p><br><h3 id="0-vstuplenie">  0. Pendahuluan </h3><br><p>  Kubernetes adalah teknologi orkestrasi scalable yang dapat dimulai dengan instalasi pada satu node dan mencapai ukuran cluster HA besar berdasarkan beberapa ratus node di dalamnya.  Sebagian besar penyedia cloud populer menyediakan berbagai jenis implementasi Kubernet - ambil dan gunakan.  Tetapi situasinya berbeda, dan ada perusahaan yang tidak menggunakan cloud, dan mereka ingin mendapatkan semua keuntungan dari teknologi orkestrasi modern.  Dan inilah instalasi Kubernetes pada logam telanjang. </p><br><p><img src="https://habrastorage.org/webt/el/ci/ua/elciua9kwxmo0fnnm5yoaabqpvm.jpeg"></p><a name="habracut"></a><br><h3 id="1-vvedenie">  1. Pendahuluan </h3><br><p> Dalam contoh ini, kita akan membuat kluster Kubernetes HA dengan topologi untuk beberapa master, dengan kluster eksternal dll sebagai lapisan dasar dan penyeimbang beban MetalLB di dalamnya.  Pada semua node yang berfungsi, kami akan menggunakan GlusterFS sebagai penyimpanan cluster terdistribusi internal yang sederhana.  Kami juga akan mencoba untuk menyebarkan beberapa proyek pengujian di dalamnya menggunakan registri Docker pribadi kami. </p><br><p>  Secara umum, ada beberapa cara untuk membuat kluster Kubernetes HA: jalur yang sulit dan mendalam yang dijelaskan dalam dokumen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kubernet-cara-keras yang populer</a> , atau cara yang lebih sederhana menggunakan utilitas <strong>kubeadm</strong> . </p><br><p>  Kubeadm adalah alat yang dibuat oleh komunitas Kubernetes khusus untuk menyederhanakan pemasangan Kubernetes dan mempermudah proses.  Sebelumnya, Kubeadm direkomendasikan hanya untuk membuat kelompok uji kecil dengan satu node master, untuk memulai.  Tetapi selama setahun terakhir, banyak yang telah diperbaiki, dan sekarang kita dapat menggunakannya untuk membuat cluster HA dengan beberapa node master.  Menurut berita komunitas Kubernetes, di masa depan, Kubeadm akan direkomendasikan sebagai alat untuk menginstal Kubernetes. </p><br><p>  Dokumentasi Kubeadm menawarkan dua cara dasar untuk mengimplementasikan sebuah cluster, dengan topologi stack dan eksternal dll.  Saya akan memilih jalur kedua dengan node eksternal dll karena toleransi kesalahan dari cluster HA. </p><br><p>  Berikut adalah diagram dari dokumentasi Kubeadm yang menjelaskan jalur ini: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/4y/nh/gd/4ynhgd4h3ireojrvdplimgnsk2u.jpeg"></a> </p><br><p>  Saya akan mengubahnya sedikit.  Pertama, saya akan menggunakan sepasang server HAProxy sebagai load balancer dengan paket Heartbeat, yang akan membagikan alamat IP virtual.  Heartbeat dan HAProxy menggunakan sejumlah kecil sumber daya sistem, jadi saya akan menempatkan mereka pada sepasang node etcd untuk sedikit mengurangi jumlah server untuk cluster kami. </p><br><p>  Untuk skema kluster Kubernetes ini, diperlukan delapan node.  Tiga server untuk cluster eksternal dll (layanan LB juga akan menggunakan beberapa dari mereka), dua untuk node dari bidang kontrol (master node) dan tiga untuk node yang berfungsi.  Ini bisa berupa bare metal atau server VM.  Dalam hal ini, itu tidak masalah.  Anda dapat dengan mudah mengubah skema dengan menambahkan lebih banyak node master dan menempatkan HAProxy dengan Heartbeat pada node yang berbeda, jika ada banyak server gratis.  Meskipun opsi saya untuk implementasi pertama dari cluster HA sudah cukup untuk mata. </p><br><p>  Jika Anda mau, tambahkan server kecil dengan utilitas <strong>kubectl</strong> yang <strong>diinstal</strong> untuk mengelola cluster ini atau gunakan desktop Linux Anda sendiri untuk ini. </p><br><p>  Diagram untuk contoh ini akan terlihat seperti ini: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/r4/5w/uc/r45wucdscdlhmaqcuw-gtr7mrmm.jpeg"></a> </p><br><h3 id="2-trebovaniya">  2. Persyaratan </h3><br><p>  Anda memerlukan dua node master Kubernetes dengan persyaratan sistem minimum yang disarankan: 2 CPU dan 2 GB RAM sesuai dengan dokumentasi <strong>kubeadm</strong> .  Untuk node yang berfungsi, saya sarankan menggunakan server yang lebih kuat, karena kami akan menjalankan semua layanan aplikasi kami pada mereka.  Dan untuk Etcd + LB, kita juga dapat mengambil server dengan dua CPU dan setidaknya 2 GB RAM. </p><br><p>  Pilih jaringan publik atau jaringan pribadi untuk klaster ini;  Alamat IP tidak masalah;  Sangat penting bahwa semua server dapat diakses satu sama lain dan, tentu saja, untuk Anda.  Nantinya, di dalam kluster Kubernetes, kami akan menyiapkan jaringan overlay. </p><br><p>  Persyaratan minimum untuk contoh ini adalah: </p><br><ul><li>  2 server dengan 2 prosesor dan 2 GB RAM untuk master node </li><li>  3 server dengan 4 prosesor dan 4-8 GB RAM untuk node yang berfungsi </li><li>  3 server dengan 2 prosesor dan 2 GB RAM untuk Etcd dan HAProxy </li><li>  192.168.0.0/24 - subnet. </li></ul><br><p>  192.168.0.1 - alamat IP virtual HAProxy, 192.168.0.2 - 4 alamat IP utama dari node Etcd dan HAProxy, 192.168.0.5 - 6 alamat IP utama dari simpul master Kubernetes, 192.168.0.7 - 9 alamat IP utama dari simpul kerja Kubernetes . </p><br><p>  Database Debian 9 diinstal pada semua server. </p><br><blockquote>  Juga ingat bahwa persyaratan sistem tergantung pada seberapa besar dan kuatnya klaster.  Untuk informasi lebih lanjut, lihat dokumentasi Kubernetes. </blockquote><br><h3 id="3-nastroyka-haproxy-i-heartbeat">  3. Konfigurasikan HAProxy dan Detak Jantung. </h3><br><p>  Kami memiliki lebih dari satu simpul master Kubernetes, dan oleh karena itu Anda perlu mengonfigurasi penyeimbang beban HAProxy di depannya - untuk mendistribusikan lalu lintas.  Ini akan menjadi sepasang server HAProxy dengan satu alamat IP virtual bersama.  Toleransi kesalahan disediakan dengan paket Heartbeat.  Untuk penyebaran, kami akan menggunakan dua server etcd pertama. </p><br><p>  Instal dan konfigurasikan HAProxy dengan Heartbeat pada server etcd pertama dan kedua (192.168.0.2–3 dalam contoh ini): </p><br><pre><code class="plaintext hljs">etcd1# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy etcd2# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy</code> </pre> <br><p>  Simpan konfigurasi asli dan buat yang baru: </p><br><pre> <code class="plaintext hljs">etcd1# mv /etc/haproxy/haproxy.cfg{,.back} etcd1# vi /etc/haproxy/haproxy.cfg etcd2# mv /etc/haproxy/haproxy.cfg{,.back} etcd2# vi /etc/haproxy/haproxy.cfg</code> </pre> <br><p>  Tambahkan opsi konfigurasi ini untuk kedua HAProxy: </p><br><pre> <code class="plaintext hljs">global user haproxy group haproxy defaults mode http log global retries 2 timeout connect 3000ms timeout server 5000ms timeout client 5000ms frontend kubernetes bind 192.168.0.1:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server k8s-master-0 192.168.0.5:6443 check fall 3 rise 2 server k8s-master-1 192.168.0.6:6443 check fall 3 rise 2</code> </pre> <br><p>  Seperti yang Anda lihat, kedua layanan HAProxy berbagi alamat IP - 192.168.0.1.  Alamat IP virtual ini akan berpindah di antara server, jadi kami akan sedikit lihai dan <strong>mengaktifkan</strong> parameter <strong>net.ipv4.ip_nonlocal_bind</strong> untuk memungkinkan pengikatan layanan sistem ke alamat IP non-lokal. </p><br><p>  Tambahkan fitur ini ke file <strong>/etc/sysctl.conf</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1 etcd2# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1</code> </pre> <br><p>  Jalankan di kedua server: </p><br><pre> <code class="plaintext hljs">sysctl -p</code> </pre> <br><p>  Jalankan juga HAProxy di kedua server: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl start haproxy etcd2# systemctl start haproxy</code> </pre> <br><p>  Pastikan HAProxy berjalan dan mendengarkan pada alamat IP virtual di kedua server: </p><br><pre> <code class="plaintext hljs">etcd1# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy etcd2# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy</code> </pre> <br><p>  Tudung!  Sekarang instal Heartbeat dan konfigurasikan IP virtual ini. </p><br><pre> <code class="plaintext hljs">etcd1# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat etcd2# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat</code> </pre> <br><p>  Saatnya membuat beberapa file konfigurasi untuknya: untuk server pertama dan kedua mereka pada dasarnya akan sama. </p><br><p>  Pertama buat file <strong>/etc/ha.d/authkeys</strong> , dalam file ini Heartbeat menyimpan data untuk otentikasi bersama.  File harus sama di kedua server: </p><br><pre> <code class="plaintext hljs"># echo -n securepass | md5sum bb77d0d3b3f239fa5db73bdf27b8d29a etcd1# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a etcd2# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a</code> </pre> <br><p>  File ini hanya dapat diakses untuk melakukan root: </p><br><pre> <code class="plaintext hljs">etcd1# chmod 600 /etc/ha.d/authkeys etcd2# chmod 600 /etc/ha.d/authkeys</code> </pre> <br><p>  Sekarang buat file konfigurasi utama untuk Detak Jantung di kedua server: untuk setiap server akan sedikit berbeda. </p><br><p>  Buat <strong>/etc/ha.d/ha.cf</strong> : </p><br><p>  <strong>dlld1</strong> </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.3 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to log/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  <strong>dlld2</strong> </p><br><pre> <code class="plaintext hljs">etcd2# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.2 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to vlog/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  Dapatkan parameter "simpul" untuk konfigurasi ini dengan menjalankan uname -n di kedua server Etcd.  Juga gunakan nama kartu jaringan Anda sebagai ganti EN18. </p><br><p>  Terakhir, Anda perlu membuat file <strong>/etc/ha.d/haresources</strong> di server ini.  Untuk kedua server, file harus sama.  Dalam file ini, kami menetapkan alamat IP umum kami dan menentukan simpul mana yang merupakan master default: </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1 etcd2# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1</code> </pre> <br><p>  Ketika semuanya sudah siap, mulai layanan Detak Jantung di kedua server dan verifikasi bahwa kami menerima ini IP virtual yang dinyatakan pada <strong>node</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl restart heartbeat etcd2# systemctl restart heartbeat etcd1# ip a ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff inet 192.168.0.2/24 brd 192.168.0.255 scope global ens18 valid_lft forever preferred_lft forever inet 192.168.0.1/24 brd 192.168.0.255 scope global secondary</code> </pre> <br><p>  Anda dapat memverifikasi bahwa HAProxy berfungsi dengan baik dengan menjalankan <strong>nc</strong> di 192.168.0.1 6443. Anda harus kehabisan waktu karena Kubernetes API belum mendengarkan di sisi server.  Tetapi ini berarti bahwa HAProxy dan Detak Jantung dikonfigurasikan dengan benar. </p><br><pre> <code class="plaintext hljs"># nc -v 192.168.0.1 6443 Connection to 93.158.95.90 6443 port [tcp/*] succeeded!</code> </pre> <br><h3 id="4-podgotovka-nod-dlya-kubernetes">  4. Persiapan node untuk Kubernetes </h3><br><p>  Langkah selanjutnya adalah mempersiapkan semua node Kubernetes.  Anda perlu menginstal Docker dengan beberapa paket tambahan, menambahkan repositori Kubernetes dan menginstal paket <strong>kubelet</strong> , <strong>kubeadm</strong> , <strong>kubectl</strong> darinya.  Pengaturan ini sama untuk semua node Kubernetes (master, pekerja, dll.) </p><br><blockquote>  Keuntungan utama <strong>Kubeadm</strong> adalah tidak perlu banyak perangkat lunak tambahan.  Instal <strong>kubeadm</strong> di semua host - dan gunakan;  setidaknya buat sertifikat CA. </blockquote><p>  Instal Docker di semua node: </p><br><pre> <code class="plaintext hljs">Update the apt package index # apt-get update Install packages to allow apt to use a repository over HTTPS # apt-get -y install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker's official GPG key # curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add docker apt repository # apt-add-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install docker-ce. # apt-get update &amp;&amp; apt-get -y install docker-ce Check docker version # docker -v Docker version 18.09.0, build 4d60db4</code> </pre> <br><p>  Setelah itu, instal paket Kubernetes di semua node: </p><br><ul><li>  <strong><code>kubeadm</code></strong> : perintah untuk memuat cluster. </li><li>  <strong><code>kubelet</code></strong> : komponen yang berjalan di semua komputer di cluster dan melakukan tindakan seperti meluncurkan perapian dan wadah. </li><li>  <strong><code>kubectl</code></strong> : util command line untuk berkomunikasi dengan cluster. </li><li>  <strong>kubectl</strong> — sesuka hati;  Saya sering menginstalnya di semua node untuk menjalankan beberapa perintah Kubernetes untuk debugging. </li></ul><br><pre> <code class="plaintext hljs"># curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository # cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install packages # apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl Hold back packages # apt-mark hold kubelet kubeadm kubectl Check kubeadm version # kubeadm version kubeadm version: &amp;version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.1", GitCommit:"eec55b9dsfdfgdfgfgdfgdfgdf365bdd920", GitTreeState:"clean", BuildDate:"2018-12-13T10:36:44Z", GoVersion:"go1.11.2", Compiler:"gc", Platform:"linux/amd64"}</code> </pre> <br><p>  Setelah <strong>menginstal kubeadm</strong> dan paket lainnya, jangan lupa untuk menonaktifkan swap. </p><br><pre> <code class="plaintext hljs"># swapoff -a # sed -i '/ swap / s/^/#/' /etc/fstab</code> </pre> <br><p>  Ulangi instalasi pada node yang tersisa.  Paket perangkat lunak adalah sama untuk semua node di cluster, dan hanya konfigurasi berikut yang akan menentukan peran yang akan mereka terima nanti. </p><br><h3 id="5-nastroyka-klastera-ha-etcd">  5. Konfigurasikan HA Etcd Cluster </h3><br><p>  Jadi, setelah menyelesaikan persiapan, kita akan mengkonfigurasi cluster Kubernetes.  Bata pertama adalah HA Etcd cluster, yang juga dikonfigurasi menggunakan alat kubeadm. </p><br><blockquote>  Sebelum kita mulai, pastikan bahwa semua node etcd berkomunikasi melalui port 2379 dan 2380. Selain itu, Anda perlu mengkonfigurasi akses ssh di antara mereka untuk menggunakan <strong>scp</strong> . </blockquote><p>  Mari kita mulai dengan node etcd pertama, dan kemudian salin semua sertifikat dan file konfigurasi yang diperlukan ke server lain. </p><br><p>  Pada semua node <strong>etcd</strong> , Anda perlu menambahkan file konfigurasi <strong>systemd</strong> baru untuk unit <strong>kubelet</strong> dengan prioritas yang lebih tinggi: </p><br><pre> <code class="plaintext hljs">etcd-nodes# cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true Restart=always EOF etcd-nodes# systemctl daemon-reload etcd-nodes# systemctl restart kubelet</code> </pre> <br><p>  Kemudian kita akan membahas ssh ke node <strong>etcd</strong> pertama - kita akan menggunakannya untuk menghasilkan semua konfigurasi <strong>kubeadm yang</strong> diperlukan untuk setiap node <strong>etcd</strong> , dan kemudian menyalinnya. </p><br><pre> <code class="plaintext hljs"># Export all our etcd nodes IP's as variables etcd1# export HOST0=192.168.0.2 etcd1# export HOST1=192.168.0.3 etcd1# export HOST2=192.168.0.4 # Create temp directories to store files for all nodes etcd1# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ etcd1# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) etcd1# NAMES=("infra0" "infra1" "infra2") etcd1# for i in "${!ETCDHOSTS[@]}"; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: "kubeadm.k8s.io/v1beta1" kind: ClusterConfiguration etcd: local: serverCertSANs: - "${HOST}" peerCertSANs: - "${HOST}" extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done</code> </pre> <br><p>  Sekarang buat otoritas sertifikat utama menggunakan <strong>kubeadm</strong> </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase certs etcd-ca</code> </pre> <br><p>  Perintah ini akan membuat dua <strong>file ca.crt &amp; ca.key</strong> di <strong>direktori</strong> <strong>/ etc / kubernetes / pki / etcd /</strong> . </p><br><pre> <code class="plaintext hljs">etcd1# ls /etc/kubernetes/pki/etcd/ ca.crt ca.key</code> </pre> <br><p>  Sekarang kita akan menghasilkan sertifikat untuk semua node <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">### Create certificates for the etcd3 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST2}/ ### cleanup non-reusable certificates etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the etcd2 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST1}/ ### cleanup non-reusable certificates again etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the this local node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1 #kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for this node # clean up certs that should not be copied off this host etcd1# find /tmp/${HOST2} -name ca.key -type f -delete etcd1# find /tmp/${HOST1} -name ca.key -type f -delete</code> </pre> <br><p>  Kemudian salin sertifikat dan konfigurasi kubeadm ke node <strong>etcd2</strong> dan <strong>etcd3</strong> . </p><br><blockquote>  Pertama-tama buat sepasang kunci ssh pada <strong>etcd1</strong> dan tambahkan bagian publik ke <strong>node etcd2</strong> dan <strong>3</strong> .  Dalam contoh ini, semua perintah dijalankan atas nama pengguna yang memiliki semua hak dalam sistem. </blockquote><br><pre> <code class="plaintext hljs">etcd1# scp -r /tmp/${HOST1}/* ${HOST1}: etcd1# scp -r /tmp/${HOST2}/* ${HOST2}: ### login to the etcd2 or run this command remotely by ssh etcd2# cd /root etcd2# mv pki /etc/kubernetes/ ### login to the etcd3 or run this command remotely by ssh etcd3# cd /root etcd3# mv pki /etc/kubernetes/</code> </pre> <br><p>  Sebelum memulai cluster etcd, pastikan file ada di semua node: </p><br><p>  Daftar file yang diperlukan di <strong>etcd1</strong> : </p><br><pre> <code class="plaintext hljs">/tmp/192.168.0.2 └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── ca.key ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key</code> </pre> <br><p>  Untuk node <strong>etcd2,</strong> ini adalah: </p><br><pre> <code class="plaintext hljs">/root └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key</code> </pre> <br><p>  Dan simpul terakhir adalah <strong>etcd3</strong> : </p><br><pre> <code class="plaintext hljs">/root └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key</code> </pre> <br><p>  Ketika semua sertifikat dan konfigurasi sudah ada, kami membuat manifes.  Pada setiap node, jalankan perintah <strong>kubeadm</strong> - untuk menghasilkan manifes statis untuk cluster <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase etcd local --config=/tmp/192.168.0.2/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml</code> </pre> <br><p>  Sekarang <strong>dlld</strong> cluster - dalam teori - dikonfigurasi dan sehat.  Verifikasi dengan menjalankan perintah berikut pada <strong>node</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \ --cert-file /etc/kubernetes/pki/etcd/peer.crt \ --key-file /etc/kubernetes/pki/etcd/peer.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://192.168.0.2:2379 cluster-health ### status output member 37245675bd09ddf3 is healthy: got healthy result from https://192.168.0.3:2379 member 532d748291f0be51 is healthy: got healthy result from https://192.168.0.4:2379 member 59c53f494c20e8eb is healthy: got healthy result from https://192.168.0.2:2379 cluster is healthy</code> </pre> <br><p>  Cluster <strong>etcd</strong> telah meningkat, jadi lanjutkan. </p><br><h3 id="6-nastroyka-master--i-rabochih-nod">  6. Mengkonfigurasi node master dan work </h3><br><p>  Konfigurasikan node master cluster kami - salin file-file ini dari node <strong>etcd</strong> pertama ke node master pertama: </p><br><pre> <code class="plaintext hljs">etcd1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.0.5:</code> </pre> <br><p>  Kemudian pergi ssh ke master node <strong>master1</strong> dan buat file <strong>kubeadm-config.yaml</strong> dengan konten berikut: </p><br><pre> <code class="plaintext hljs">master1# cd /root &amp;&amp; vi kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - "192.168.0.1" controlPlaneEndpoint: "192.168.0.1:6443" etcd: external: endpoints: - https://192.168.0.2:2379 - https://192.168.0.3:2379 - https://192.168.0.4:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</code> </pre> <br><p>  Pindahkan sertifikat dan kunci yang sebelumnya disalin ke direktori yang sesuai pada <strong>node</strong> master1, seperti pada deskripsi pengaturan. </p><br><pre> <code class="plaintext hljs">master1# mkdir -p /etc/kubernetes/pki/etcd/ master1# cp /root/ca.crt /etc/kubernetes/pki/etcd/ master1# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ master1# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/</code> </pre> <br><p>  Untuk membuat master node pertama, lakukan: </p><br><pre> <code class="plaintext hljs">master1# kubeadm init --config kubeadm-config.yaml</code> </pre> <br><p>  Jika semua langkah sebelumnya selesai dengan benar, Anda akan melihat yang berikut: </p><br><pre> <code class="plaintext hljs">You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Salin <strong>output</strong> inisialisasi <strong>kubeadm</strong> ini ke file teks apa pun, kami akan menggunakan token ini di masa mendatang ketika kami melampirkan master kedua dan node yang berfungsi ke cluster kami. </p><br><p>  Saya telah mengatakan bahwa kluster Kubernetes akan menggunakan beberapa jenis jaringan overlay untuk perapian dan layanan lainnya, jadi pada titik ini Anda perlu menginstal beberapa jenis plugin CNI.  Saya merekomendasikan plugin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Weave CNI</a> .  Pengalaman menunjukkan bahwa ini lebih bermanfaat dan tidak terlalu bermasalah, tetapi Anda dapat memilih yang lain, misalnya, Calico. </p><br><p>  Menginstal plugin jaringan Weave pada node master pertama: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" The connection to the server localhost:8080 was refused - did you specify the right host or port? serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created</code> </pre> <br><p>  Tunggu sebentar, lalu masukkan perintah berikut untuk memverifikasi bahwa komponen perapian mulai: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 6m25s coredns-86c58d9df4-xj98p 1/1 Running 0 6m25s kube-apiserver-master1 1/1 Running 0 5m22s kube-controller-manager-master1 1/1 Running 0 5m41s kube-proxy-8ncqw 1/1 Running 0 6m25s kube-scheduler-master1 1/1 Running 0 5m25s weave-net-lvwrp 2/2 Running 0 78s</code> </pre> <br><ul><li>  Disarankan untuk melampirkan node baru dari bidang kontrol hanya setelah inisialisasi node pertama. </li></ul><br><p>  Untuk memeriksa status cluster, lakukan: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 11m v1.13.1</code> </pre> <br><p>  Hebat!  Node master pertama naik.  Sekarang sudah siap, dan kita akan selesai membuat cluster Kubernetes - kita akan menambahkan master node kedua dan node yang berfungsi. <br>  Untuk menambahkan simpul master kedua, buat kunci ssh pada <strong>master1</strong> dan tambahkan bagian publik ke <strong>master2</strong> .  Lakukan login uji coba, dan kemudian salin beberapa file dari master node pertama ke yang kedua: </p><br><pre> <code class="plaintext hljs">master1# scp /etc/kubernetes/pki/ca.crt 192.168.0.6: master1# scp /etc/kubernetes/pki/ca.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.pub 192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.key @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.key @192.168.0.6: master1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.6:etcd-ca.crt master1# scp /etc/kubernetes/admin.conf 192.168.0.6: ### Check that files was copied well master2# ls /root admin.conf ca.crt ca.key etcd-ca.crt front-proxy-ca.crt front-proxy-ca.key sa.key sa.pub</code> </pre> <br><p>  Pada node master kedua, pindahkan sertifikat dan kunci yang sebelumnya disalin ke direktori yang sesuai: </p><br><pre> <code class="plaintext hljs">master2# mkdir -p /etc/kubernetes/pki/etcd mv /root/ca.crt /etc/kubernetes/pki/ mv /root/ca.key /etc/kubernetes/pki/ mv /root/sa.pub /etc/kubernetes/pki/ mv /root/sa.key /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.key /etc/kubernetes/pki/ mv /root/front-proxy-ca.crt /etc/kubernetes/pki/ mv /root/front-proxy-ca.key /etc/kubernetes/pki/ mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /root/admin.conf /etc/kubernetes/admin.conf</code> </pre> <br><p>  Hubungkan node master kedua ke cluster.  Untuk melakukan ini, Anda memerlukan output dari perintah koneksi, yang sebelumnya dikirimkan kepada kami oleh <strong><code>kubeadm init</code></strong> pada node pertama. </p><br><p>  Jalankan master node <strong>master2</strong> : </p><br><pre> <code class="plaintext hljs">master2# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6 --experimental-control-plane</code> </pre> <br><ul><li>  Anda perlu menambahkan <strong><code>--experimental-control-plane</code></strong> .  Ini mengotomatiskan lampiran data master ke sebuah cluster.  Tanpa tanda ini, simpul kerja yang biasa hanya akan ditambahkan. </li></ul><br><p>  Tunggu sedikit hingga node bergabung dengan cluster, dan periksa status baru cluster: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 32m v1.13.1 master2 Ready master 46s v1.13.1</code> </pre> <br><p>  Pastikan juga semua pod dari semua node master dimulai dengan normal: </p><br><pre> <code class="plaintext hljs">master1# kubectl — kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 46m coredns-86c58d9df4-xj98p 1/1 Running 0 46m kube-apiserver-master1 1/1 Running 0 45m kube-apiserver-master2 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 45m kube-controller-manager-master2 1/1 Running 0 15m kube-proxy-8ncqw 1/1 Running 0 46m kube-proxy-px5dt 1/1 Running 0 15m kube-scheduler-master1 1/1 Running 0 45m kube-scheduler-master2 1/1 Running 0 15m weave-net-ksvxz 2/2 Running 1 15m weave-net-lvwrp 2/2 Running 0 41m</code> </pre> <br><p>  Hebat!  Kami hampir selesai dengan konfigurasi cluster Kubernetes.  Dan hal terakhir yang harus dilakukan adalah menambahkan tiga node kerja yang kami siapkan sebelumnya. </p><br><p>  Masukkan node yang berfungsi dan jalankan perintah join kubeadm tanpa <strong><code>--experimental-control-plane</code></strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Periksa status gugus lagi: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h30m v1.13.1 master2 Ready master 1h59m v1.13.1 worker1 Ready &lt;none&gt; 1h8m v1.13.1 worker2 Ready &lt;none&gt; 1h8m v1.13.1 worker3 Ready &lt;none&gt; 1h7m v1.13.1</code> </pre> <br><p>  Seperti yang Anda lihat, kami memiliki kluster Kubernetes HA yang sepenuhnya dikonfigurasi dengan dua master dan tiga node yang berfungsi.  Itu dibangun berdasarkan HA etcd cluster dengan penyeimbang beban gagal-aman di depan master node.  Kedengarannya bagus untukku. </p><br><h3 id="7-nastroyka-udalennogo-upravleniya-klasterom">  7. Mengkonfigurasi manajemen cluster jarak jauh </h3><br><p>  Tindakan lain yang masih harus dipertimbangkan dalam bagian pertama artikel ini adalah mengatur utilitas <strong>kubectl</strong> jarak jauh untuk mengelola cluster.  Sebelumnya, kami menjalankan semua perintah dari master node <strong>master1</strong> , tetapi ini hanya cocok untuk pertama kalinya - saat mengkonfigurasi cluster.  Akan lebih baik untuk mengkonfigurasi node kontrol eksternal.  Anda dapat menggunakan laptop atau server lain untuk ini. </p><br><p>  Masuk ke server ini dan jalankan: </p><br><pre> <code class="plaintext hljs">Add the Google repository key control# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository control# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install kubectl control# apt-get update &amp;&amp; apt-get install -y kubectl In your user home dir create control# mkdir ~/.kube Take the Kubernetes admin.conf from the master1 node control# scp 192.168.0.5:/etc/kubernetes/admin.conf ~/.kube/config Check that we can send commands to our cluster control# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 6h58m v1.13.1 master2 Ready master 6h27m v1.13.1 worker1 Ready &lt;none&gt; 5h36m v1.13.1 worker2 Ready &lt;none&gt; 5h36m v1.13.1 worker3 Ready &lt;none&gt; 5h36m v1.13.1</code> </pre> <br><p>  Ok, sekarang mari kita jalankan tes di bawah di cluster kami dan periksa cara kerjanya. </p><br><pre> <code class="plaintext hljs">control# kubectl create deployment nginx --image=nginx deployment.apps/nginx created control# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-5c7588df-6pvgr 1/1 Running 0 52s</code> </pre> <br><p>  Selamat!  Anda baru saja menggunakan Kubernet.  Dan itu berarti cluster HA baru Anda siap.  Bahkan, proses pengaturan cluster <strong>Kubernetes</strong> menggunakan <strong>kubeadm</strong> cukup sederhana dan cepat. </p><br><p>  Pada bagian selanjutnya dari artikel ini, kami akan menambahkan penyimpanan internal dengan menyiapkan GlusterFS pada semua node yang bekerja, menyiapkan penyeimbang beban internal untuk cluster Kubernetes kami, dan juga menjalankan tes stres tertentu, memutuskan beberapa node, dan memeriksa stabilitas cluster. </p><br><h3 id="posleslovie">  Kata penutup </h3><br><p>  Ya, dengan mengerjakan contoh ini, Anda akan menemukan sejumlah masalah.  Tidak perlu khawatir: untuk membatalkan perubahan dan mengembalikan node ke keadaan semula, jalankan saja <strong>kubeadm reset</strong> - perubahan yang <strong>kubeadm</strong> buat sebelumnya akan diatur ulang, dan Anda dapat mengonfigurasi lagi.  Juga, jangan lupa untuk memeriksa status kontainer Docker pada node cluster - pastikan semuanya mulai dan bekerja tanpa kesalahan.  Untuk informasi lebih lanjut tentang kontainer yang rusak, gunakan perintah <strong>buruh pelabuhan log containerid</strong> . </p><br><p>  Itu saja untuk hari ini.  Semoga beruntung </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id439562/">https://habr.com/ru/post/id439562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id439550/index.html">Hackquest 2018. Hasil & Tulisan. Hari 1-3</a></li>
<li><a href="../id439552/index.html">Ekstensi Chrome berbahaya</a></li>
<li><a href="../id439556/index.html">TDMS Fairway. Metodologi PMBOK dan organisasi desain Rusia</a></li>
<li><a href="../id439558/index.html">Telepon lama baru. Temukan kembali telepon PSTN</a></li>
<li><a href="../id439560/index.html">Adaptor blockchain Ethereum untuk platform data IRIS InterSystems</a></li>
<li><a href="../id439564/index.html">Aplikasi Praktis Transformasi Pohon AST Menggunakan Putout sebagai Contoh</a></li>
<li><a href="../id439566/index.html">Mengapa dokumentasi SRE penting? Bagian 3</a></li>
<li><a href="../id439568/index.html">SSD berbasis QLC - pembunuh hard drive? Tidak juga</a></li>
<li><a href="../id439570/index.html">IPython magic untuk mengedit tag sel Jupyter</a></li>
<li><a href="../id439572/index.html">Desain peralatan elektronik berbantuan komputer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>