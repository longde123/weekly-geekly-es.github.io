<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧗 🏬 🏐 Fonctionnement des lapins (RabbitMQ) dans le mode "Survivre à tout prix" 💆🏿 👉🏿 👨🏻‍🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="" Entreprise " - opérateur télécom PJSC "Megafon" 
 " Noda " est le serveur RabbitMQ. 
 Un « cluster » est une combinaison, dans notre cas de trois, d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Fonctionnement des lapins (RabbitMQ) dans le mode "Survivre à tout prix"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434016/">  " <b>Entreprise</b> " - opérateur télécom PJSC "Megafon" <br>  " <b>Noda</b> " est le serveur RabbitMQ. <br>  Un « <b>cluster</b> » est une combinaison, dans notre cas de trois, de nœuds RabbitMQ fonctionnant dans leur ensemble. <br>  « <b>Contour</b> » - un ensemble de clusters RabbitMQ, dont les règles de travail sont déterminées sur l'équilibreur en face d'eux. <br>  " <b>Balancer</b> ", " <b>hap</b> " - Haproxy - équilibreur qui remplit la fonction de commutation de la charge sur les clusters dans la boucle.  Une paire de serveurs Haproxy fonctionnant en parallèle est utilisée pour chaque boucle. <br>  " <b>Sous-système</b> " - l'éditeur et / ou consommateur de messages transmis par le lapin <br>  " <b>SYSTEM</b> " - un ensemble de sous-systèmes, qui est une solution logicielle et matérielle unique utilisée par la société, caractérisée par sa distribution dans toute la Russie, mais avec plusieurs centres où toutes les informations circulent et où les principaux calculs et calculs ont lieu. <br>  <b>SYSTÈME</b> - un système géographiquement réparti - de Khabarovsk et Vladivostok à Saint-Pétersbourg et Krasnodar.  Sur le plan architectural, il s'agit de plusieurs contours centraux, divisés par les caractéristiques des sous-systèmes qui leur sont connectés. <br><a name="habracut"></a><br><h3>  Quelle est la mission du transport dans les réalités des télécoms? </h3><br>  En bref: la réponse des sous-systèmes à l'action de chaque abonné suit, qui à son tour informe les autres sous-systèmes des événements et des changements ultérieurs.  Les messages sont générés par toutes les actions avec le SYSTÈME, non seulement des abonnés, mais aussi du côté des employés de la Société et des sous-systèmes (un très grand nombre de tâches sont effectuées automatiquement). <br><br>  Caractéristiques du transport dans les télécommunications: grand, pas de mal, GRAND flux de données diverses transmises par transport asynchrone. <br><br>  Certains sous-systèmes vivent sur des clusters séparés en raison de la lourdeur des flux de messages - il n'y a tout simplement plus de ressources sur le cluster, par exemple, avec un flux de messages de 5-6 mille messages / seconde, la quantité de données transférées peut atteindre 170-190 mégaoctets / seconde.  Avec un tel profil de charge, une tentative de débarquer quelqu'un d'autre sur ce cluster entraînera de tristes conséquences: comme il n'y a pas assez de ressources pour traiter toutes les données en même temps, le lapin commencera à conduire les connexions entrantes dans un <b>flux</b> - un processus de publication simple commencera, avec toutes les conséquences pour tous les sous-systèmes et systèmes dans ensemble. <br><br>  Exigences de base pour le transport: <br><br><ol><li>  L'accessibilité des véhicules devrait être de 99,99%.  En pratique, cela se traduit par une exigence opérationnelle 24/7 et la capacité de répondre automatiquement à toutes les situations d'urgence. </li><li>  Sécurité des données: le pourcentage de messages perdus sur le transport devrait tendre à 0. </li></ol><br>  Par exemple, lors du fait d'appeler, plusieurs messages différents transitent par le transport asynchrone.  certains messages sont destinés à des sous-systèmes vivant dans le même circuit, et certains sont destinés à être transmis à des nœuds centraux.  Le même message peut être revendiqué par plusieurs sous-systèmes, par conséquent, au stade de la publication du message chez le lapin, il est copié et envoyé à différents consommateurs.  Et dans certains cas, la copie des messages est obligatoirement mise en œuvre sur un circuit intermédiaire - lorsque des informations doivent être transmises du circuit de Khabarovsk au circuit de Krasnodar.  La transmission se fait par l'un des contours centraux, où des copies des messages sont faites, pour les destinataires centraux. <br><br>  Outre les événements provoqués par les actions de l'abonné, les messages de service qui échangent les sous-systèmes passent par le transport.  Ainsi, plusieurs milliers de routes de messagerie différentes sont obtenues, certaines se croisent, certaines existent isolément.  Il suffit de nommer le nombre de files d'attente impliquées dans les itinéraires sur différents Contours pour comprendre l'échelle approximative de la carte de transport: Sur les circuits centraux 600, 200, 260, 15 ... et sur les Circuits distants 80-100 ... <br><br>  Avec une telle implication du transport, les exigences d'accessibilité à 100% de tous les nœuds de transport ne semblent plus excessives.  Nous passons à la mise en œuvre de ces exigences. <br><br><h3>  Comment nous résolvons les tâches </h3><br>  En plus de <i>RabbitMQ lui</i> - <i>même</i> , <i>Haproxy est</i> utilisé pour équilibrer la charge et fournir une réponse automatique aux urgences. <br><br>  Quelques mots sur l'environnement matériel et logiciel dans lequel nos lapins existent: <br><br><ul><li>  Tous les serveurs de lapin sont virtuels, avec des paramètres de 8-12 CPU, 16 Gb Mem, 200 Gb HDD.  Comme l'expérience l'a montré, même l'utilisation de serveurs non virtuels effrayants avec 90 cœurs et un tas de RAM offre une petite amélioration des performances à des coûts nettement plus élevés.  Versions utilisées: 3.6.6 (en pratique - la plus stable de 3.6) avec un erlang de 18.3, 3.7.6 avec un erlang de 20.1. </li><li>  Pour Haproxy, les exigences sont beaucoup plus faibles: 2 CPU, 4 Gb Mem, la version haproxy est stable 1.8.  La charge sur les ressources sur tous les serveurs haproxy ne dépasse pas 15% CPU / Mem. </li><li>  L'ensemble du zoo est situé dans 14 centres de données répartis sur 7 sites à travers le pays, réunis en un seul réseau.  Dans chacun des centres de données, il existe un cluster de trois nœuds et un concentrateur. </li><li>  Pour les circuits distants, 2 centres de données sont utilisés, pour chacun des circuits centraux - 4. </li><li>  Les circuits centraux interagissent entre eux ainsi qu'avec les circuits distants; à leur tour, les circuits distants fonctionnent uniquement avec les circuits centraux; ils n'ont pas de communication directe entre eux. </li><li>  Les configurations de Haps et Clusters au sein d'un même Circuit sont complètement identiques.  Le point d'entrée pour chaque circuit est un alias pour plusieurs enregistrements A-DNS.  Ainsi, pour éviter que cela ne se produise, au moins un hap et au moins un des clusters (au moins un nœud du cluster) dans chaque circuit seront disponibles.  Étant donné que le cas de défaillance de même 6 serveurs dans deux centres de données en même temps est extrêmement improbable, l'acceptabilité est supposée proche de 100%. </li></ul><br>  Il semble conçu (et mis en œuvre) tout cela comme ceci: <br><br><img src="https://habrastorage.org/webt/f2/rm/9i/f2rm9i0kd5jk1ikaapl2vc63yhc.jpeg" alt="image"><br><br><img src="https://habrastorage.org/webt/az/5t/ks/az5tkse8qkz0fl3ml6znbg4pz18.jpeg" alt="image"><br><br>  Maintenant quelques configs. <br><br><div class="spoiler">  <b class="spoiler_title">Configuration haproxy</b> <div class="spoiler_text"><table><tbody><tr><td>  frontend center-rmq_5672 </td><td></td></tr><tr><td></td><td>  lier </td><td>  *: 5672 </td></tr><tr><td></td><td>  mode </td><td>  TCP </td></tr><tr><td></td><td>  maxconn </td><td>  10 000 </td></tr><tr><td></td><td>  client de délai d'attente </td><td>  3h </td></tr><tr><td></td><td>  option </td><td>  tcpka </td></tr><tr><td></td><td>  option </td><td>  tcplog </td></tr><tr><td></td><td>  default_backend </td><td>  center-rmq_5672 </td></tr><tr><td>  frontend center-rmq_5672_lvl_1 </td><td></td></tr><tr><td></td><td>  lier </td><td>  localhost: 56721 </td></tr><tr><td></td><td>  mode </td><td>  TCP </td></tr><tr><td></td><td>  maxconn </td><td>  10 000 </td></tr><tr><td></td><td>  client de délai d'attente </td><td>  3h </td></tr><tr><td></td><td>  option </td><td>  tcpka </td></tr><tr><td></td><td>  option </td><td>  tcplog </td></tr><tr><td></td><td>  default_backend </td><td>  center-rmq_5672_lvl_1 </td></tr><tr><td>  backend center-rmq_5672 </td></tr><tr><td></td><td>  équilibre </td><td>  lessconn </td></tr><tr><td></td><td>  mode </td><td>  TCP </td></tr><tr><td></td><td>  fullconn </td><td>  10 000 </td></tr><tr><td></td><td>  délai </td><td>  serveur 3h </td></tr><tr><td></td><td>  serveur </td><td>  srv-rmq01 10/10/10/10/106767 vérification inter 5s montée 2 chute 3 arrêt-sauvegarde-sessions marquées </td></tr><tr><td></td><td>  serveur </td><td>  srv-rmq03 10/10/10/2011 11672 vérification inter 5s montée 2 chute 3 arrêt-sauvegarde-sessions balisées </td></tr><tr><td></td><td>  serveur </td><td>  srv-rmq05 10/10/10/126767 contrôle inter 5 s montée 2 chute 3 arrêt-sauvegarde-sessions marquées </td></tr><tr><td></td><td>  serveur </td><td>  localhost 127.0.0.1 ∗ 6721 check inter 5s rise 2 fall 3 backup on-marked-down shutdown-sessions </td></tr><tr><td>  backend center-rmq_5672_lvl_1 </td></tr><tr><td></td><td>  équilibre </td><td>  lessconn </td></tr><tr><td></td><td>  mode </td><td>  TCP </td></tr><tr><td></td><td>  fullconn </td><td>  10 000 </td></tr><tr><td></td><td>  délai </td><td>  serveur 3h </td></tr><tr><td></td><td>  serveur </td><td>  srv-rmq02 10/10/10/136767 contrôle inter 5s montée 2 chute 3 arrêt-sauvegarde-sessions marquées </td></tr><tr><td></td><td>  serveur </td><td>  srv-rmq04 10/10/10/14/1067 check inter 5s montée 2 chute 3 arrêt-sauvegarde-sessions balisées </td></tr><tr><td></td><td>  serveur </td><td>  srv-rmq06 10.10.10.5:0767 vérification inter 5 s montée 2 chute 3 arrêt-sauvegarde-sessions marquées </td></tr></tbody></table><br></div></div><br>  La première section du front décrit le point d'entrée - menant au cluster principal, la deuxième section est conçue pour équilibrer le niveau de réserve.  Si vous décrivez simplement tous les serveurs de sauvegarde de lapin dans la section backend (instruction de sauvegarde), cela fonctionnera de la même manière - si le cluster principal est complètement inaccessible, les connexions iront au serveur de sauvegarde, cependant, toutes les connexions iront au premier serveur de sauvegarde de la liste.  Pour assurer l'équilibrage de la charge sur tous les nœuds de sauvegarde, nous introduisons simplement un front supplémentaire, que nous rendons disponible uniquement avec localhost, et nous lui attribuons le serveur de sauvegarde. <br><br>  L'exemple ci-dessus décrit l'équilibrage de la boucle distante - qui fonctionne dans deux centres de données: le serveur srv-rmq {01,03,05} - vivent dans le centre de données n ° 1, srv-rmq {02,04,06} - dans le centre de données n ° 2.  Ainsi, pour implémenter la solution à quatre codas, nous avons seulement besoin d'ajouter deux fronts locaux supplémentaires et deux sections backend des serveurs rabbit correspondants. <br><br>  Le comportement de l'équilibreur avec cette configuration est le suivant: tant qu'au moins un serveur principal est vivant, nous l'utilisons.  Si les serveurs principaux ne sont pas disponibles, nous travaillons avec une réserve.  Si au moins un serveur principal devient disponible, toutes les connexions aux serveurs de sauvegarde sont déconnectées et, lorsque la connexion est restaurée, elles tombent déjà sur le cluster principal. <br><br>  L'expérience de fonctionnement de cette configuration montre une disponibilité de presque 100% de chacun des circuits.  Cette solution nécessite que les sous-systèmes soient parfaitement légaux et simples: pouvoir se reconnecter avec le lapin après déconnexion. <br><br>  Nous avons donc fourni l'équilibrage de charge à un nombre arbitraire de clusters et basculé automatiquement entre eux, il est temps d'aller directement aux lapins. <br><br>  Chaque cluster est créé à partir de trois nœuds, comme le montre la pratique - le nombre de nœuds le plus optimal, ce qui garantit l'équilibre optimal entre disponibilité / tolérance aux pannes / vitesse.  Étant donné que le lapin n'évolue pas horizontalement (les performances du cluster sont égales aux performances du serveur le plus lent), nous créons tous les nœuds avec les mêmes paramètres optimaux pour CPU / Mem / Hdd.  Nous positionnons les serveurs aussi près que possible les uns des autres - dans notre cas, nous classons les machines virtuelles dans la même batterie de serveurs. <br><br>  En ce qui concerne les conditions préalables, à la suite desquelles de la part des sous-systèmes assureront le fonctionnement le plus stable et le respect de l'exigence de sauvegarde des messages reçus: <br><br><ol><li>  Le travail avec le lapin se fait uniquement via le protocole amqp / amqps - via l'équilibrage.  Autorisation sous les comptes locaux - au sein de chaque cluster (enfin, et tout le circuit) </li><li>  Les sous-systèmes sont connectés au lapin en mode passif: aucune manipulation avec les entités des lapins (création de files d'attente / eschendzhey / bind) n'est autorisée et limitée au niveau des droits de compte - nous ne donnons tout simplement pas de droits de configuration. </li><li>  Toutes les entités nécessaires sont créées de manière centralisée, et non au moyen de sous-systèmes, et sur tous les clusters de cluster sont effectuées de la même manière - pour assurer la commutation automatique vers le cluster de sauvegarde et vice versa.  Sinon, nous pouvons obtenir une image: nous sommes passés à la réserve, mais la file d'attente ou la liaison n'est pas là, et nous pouvons obtenir le choix d'une erreur de connexion ou d'une perte de messages. </li></ol><br><h3>  Maintenant, les paramètres directement sur les lapins: </h3><br><ol><li>  Les hôtes locaux n'ont pas accès à l'interface Web </li><li>  L'accès au Web est organisé via LDAP - nous nous intégrons à AD et obtenons la journalisation de qui et où est allé sur la webcam.  Au niveau de la configuration, nous restreignons les droits des comptes AD, non seulement nous exigeons d'être dans un certain groupe, mais nous ne donnons que les droits de «voir».  Les groupes de surveillance sont plus que suffisants.  Et nous suspendons les droits d'administrateur à un autre groupe dans AD, ainsi le cercle d'influence sur le transport est considérablement limité. </li><li>  Pour faciliter l'administration et le suivi: <br>  Sur tous les VHOST, nous suspendons immédiatement une politique de niveau 0 avec application à toutes les files d'attente (modèle :. *): <br><br><ul><li>  <b><i>ha-mode: all</i></b> - stocke toutes les données sur tous les nœuds du cluster, la vitesse de traitement des messages diminue, mais leur sécurité et leur disponibilité sont assurées. </li><li>  <b><i>ha-sync-mode: automatic</i></b> - demande au robot de synchroniser automatiquement les données sur tous les nœuds du cluster: la sécurité et la disponibilité des données augmentent également. </li><li>  <b><i>mode file d'attente: paresseux</i></b> - peut-être l'une des options les plus utiles apparues chez les lapins depuis la version 3.6 - enregistrement immédiat des messages sur le disque dur.  Cette option réduit considérablement la consommation de RAM et augmente la sécurité des données lors des arrêts / chutes de nœuds ou du cluster dans son ensemble. </li></ul><br></li><li>  Paramètres dans le fichier de configuration ( <i>rabbitmq-main / conf / rabbitmq.config</i> ): <br><br><ul><li>  Section <b>lapin</b> : <i>{vm_memory_high_watermark_paging_ratio, 0,5}</i> - seuil de téléchargement des messages sur le disque à 50%.  Avec <b>paresseux activé, il</b> sert plus d'assurance lorsque nous souscrivons une police, par exemple, niveau 1, dans laquelle nous oublions d'inclure <b>paresseux</b> . </li><li>  <i>{vm_memory_high_watermark, 0,95}</i> - nous <i>limitons</i> le lapin à 95% de la RAM totale, car seul le lapin vit sur les serveurs, cela n'a aucun sens d'introduire des restrictions plus strictes.  5% "geste large" qu'il en soit - quittez le système d'exploitation, la surveillance et d'autres petites choses utiles.  Comme cette valeur est la limite supérieure, il y en a assez pour tout le monde. </li><li>  <i>{cluster_partition_handling, pause_minority}</i> - décrit le comportement du cluster lorsque la partition réseau se produit, pour trois clusters de nœuds ou plus, cet indicateur est recommandé - il permet au cluster de se récupérer. </li><li>  <i>{disk_free_limit, "500MB"}</i> - tout est simple, quand il y a 500 Mo d'espace disque libre - la publication des messages sera arrêtée, seule la soustraction sera disponible. </li><li>  <i>{auth_backends, [rabbit_auth_backend_internal, rabbit_auth_backend_ldap]}</i> - ordre d'autorisation pour les lapins: Tout d'abord, la présence d'une échographie dans la base de données locale est vérifiée, et sinon, allez sur le serveur LDAP. </li><li>  Section <b>rabbitmq_auth_backend_ldap</b> - configuration de l'interaction avec AD: <i>{serveurs, ["srv_dc1", "srv_dc2"]}</i> - une liste de contrôleurs de domaine sur lesquels l'authentification aura lieu. </li><li>  Les paramètres qui décrivent directement l'utilisateur dans AD, le port LDAP, etc. sont purement individuels et sont décrits en détail dans la documentation. </li><li>  La chose la plus importante pour nous est une description des droits et restrictions sur l'administration et l'accès à l'interface Web des lapins: tag_queries: <br>  <i>[{administrateur, {in_group, "cn = rabbitmq-admins, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru"}},</i> <i><br></i>  <i>{surveillance,</i> <i><br></i>  <i>{in_group, "cn = rabbitmq-web, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru"}</i> <i><br></i>  <i>}]</i> - cette conception fournit des privilèges administratifs à tous les utilisateurs du groupe rabbitmq-admins et des droits de surveillance (au minimum suffisants pour visualiser l'accès) pour le groupe rabbitmq-web. <br></li><li>  <b>requête_accès_ressource</b> : <br>  <i>{pour,</i> <i><br></i>  <i>[{permission, configure, {in_group, "cn = rabbitmq-admins, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru"}},</i> <i><br></i>  <i>{permission, write, {in_group, "cn = rabbitmq-admins, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru"}},</i> <i><br></i>  <i>{permission, lecture, {constant, true}}</i> <i><br></i>  <i>]</i> <i><br></i>  <i>}</i> - nous fournissons les droits de configuration et d'écriture uniquement au groupe d'administrateurs, à tous ceux qui se connectent avec succès, les droits sont en lecture seule - il peut lire les messages via l'interface Web. <br></li></ul></li></ol><br>  Nous obtenons un cluster configuré (au niveau du fichier de configuration et des paramètres dans le lapin lui-même) qui maximise la disponibilité et la sécurité des données.  Par cela, nous mettons en œuvre l'exigence - assurer la disponibilité et la sécurité des données ... dans la plupart des cas. <br><br>  Il y a plusieurs points à prendre en compte lors de l'utilisation de tels systèmes très chargés: <br><br><ol><li>  Il est préférable d'organiser toutes les propriétés supplémentaires des files d'attente (TTL, expiration, longueur maximale, etc.) par les politiciens, plutôt que de suspendre les paramètres lors de la création des files d'attente.  Il en résulte une structure personnalisable de manière flexible qui peut être personnalisée à la volée en fonction des réalités changeantes. </li><li>  Utilisation de TTL.  Plus la file d'attente est longue, plus la charge sur le processeur est élevée.  Pour éviter de "percer le plafond", il est préférable de limiter la longueur de la file d'attente via max-length également. </li><li>  En plus du lapin lui-même, un certain nombre d'applications utilitaires tournent sur le serveur, ce qui, curieusement, nécessite également des ressources CPU.  Un lapin gourmand, par défaut, prend tous les noyaux disponibles ... Une situation désagréable peut se révéler: une lutte pour les ressources, qui peut facilement conduire à des freins sur le lapin.  Pour éviter l'occurrence d'une telle situation, par exemple, comme suit: Modifiez les paramètres du lancement de l'erlang - introduisez une limite obligatoire sur le nombre de cœurs utilisés.  Nous procédons comme suit: recherchez le fichier <i>rabbitmq-env</i> , recherchez le paramètre SERVER_ERL_ARGS = et ajoutez + sct L0-Xc0-X + SY: Y.  Où X est le nombre de cœurs-1 (le comptage commence à 0), Y - Le nombre de cœurs -1 (à partir de 1).  + sct L0-Xc0-X - change la liaison aux noyaux, + SY: Y - change le nombre de shedulers lancés par l'erlang.  Donc pour un système de 8 cœurs, les paramètres ajoutés prendront la forme: + sct L0-6c0-6 + S 7: 7.  De cette façon, nous ne donnons au lapin que 7 cœurs et nous nous attendons à ce que le système d'exploitation, en lançant d'autres processus, agisse de manière optimale et les accroche sur un noyau non chargé. </li></ol><br><h3>  Les nuances de l'exploitation du zoo résultant </h3><br>  Ce que n'importe quel paramètre ne peut pas protéger, c'est la base effondrée de la mnésie - malheureusement, cela se produit avec une probabilité non nulle.  Un résultat aussi désastreux n'est pas causé par des défaillances globales (par exemple, une défaillance complète d'un centre de données entier - la charge basculera simplement vers un autre cluster), mais davantage de défaillances locales - au sein du même segment de réseau. <br><br>  De plus, ce sont les défaillances du réseau local qui font peur, car  l'arrêt d'urgence d'un ou deux nœuds n'entraînera pas de conséquences fatales - simplement toutes les demandes iront à un nœud, et comme nous nous en souvenons, les performances dépendent des performances du seul nœud lui-même.  Les pannes de réseau (nous ne prenons pas en compte les petites interruptions de communication - elles se produisent sans douleur) conduisent à une situation où les nœuds commencent le processus de synchronisation les uns avec les autres, puis la connexion se brise encore et encore pendant quelques secondes. <br><br>  Par exemple, plusieurs clignotements du réseau et avec une fréquence de plus de 5 secondes (un tel délai est défini dans les paramètres Hapov, vous pouvez certainement les lire, mais pour vérifier l'efficacité, vous devrez répéter l'échec, ce que personne ne veut). <br><br>  Le cluster peut toujours résister à une ou deux itérations de ce type, mais plus - les chances sont déjà minimes.  Dans une telle situation, l'arrêt d'un nœud tombé peut enregistrer, mais il est presque impossible de le faire manuellement.  Le plus souvent, le résultat n'est pas seulement la perte d'un nœud du cluster avec le message <b>«Partition réseau»</b> , mais aussi l'image lorsque les données de la partie des files d'attente ont vécu uniquement ce nœud et n'ont pas eu le temps de se synchroniser avec les autres.  Visuellement - dans la file d'attente, les données sont <b><i>NaN</i></b> . <br><br>  Et maintenant, c'est un signal non ambigu - basculez vers le cluster de sauvegarde.  La commutation fournira un bonheur, il vous suffit d'arrêter les lapins sur le cluster principal - une question de plusieurs minutes.  En conséquence, nous obtenons la restauration de la capacité de travail du transport et nous pouvons procéder en toute sécurité à l'analyse de l'accident et à son élimination. <br><br>  Afin de retirer une grappe endommagée de sous la charge, afin d'éviter une dégradation supplémentaire, la chose la plus simple est de faire fonctionner le lapin sur des ports autres que 5672. Puisque nous surveillons les lapins par le port régulier, son déplacement, par exemple, par 5673 dans les paramètres du lapin, il vous permettra de lancer complètement le cluster sans douleur et d'essayer de restaurer son opérabilité et les messages qui y restent. <br><br>  Nous le faisons en quelques étapes: <br><br><ol><li>  Arrêtez tous les nœuds du cluster défaillant - le hap basculera la charge vers le cluster de sauvegarde </li><li>  RABBITMQ_NODE_PORT=5673   <i>rabbitmq-env</i> –       ,   Web  -    15672. </li><li>            . </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Au démarrage, les index seront reconstruits et, dans la grande majorité des cas, toutes les données seront entièrement restaurées. Malheureusement, des plantages se produisent, qui </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">obligent</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> à supprimer physiquement tous les messages du disque, ne laissant que la configuration - les répertoires </font><i><font style="vertical-align: inherit;">msg_store_persistent</font></i><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">msg_store_transient</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">files d'attente</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (pour la version 3.6) ou </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">msg_stores</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (pour la version 3.7) </font><font style="vertical-align: inherit;">sont supprimés dans le dossier contenant la base de données </font><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Après une telle thérapie radicale, le cluster est lancé avec la préservation de la structure interne, mais sans messages. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et l'option la plus désagréable (observée une fois): les dommages à la base étaient tels qu'il était nécessaire de supprimer complètement la base entière et de reconstruire le cluster à partir de zéro.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour la commodité de la gestion et de la mise à jour des lapins, pas un assemblage prêt à l'emploi en rpm n'est utilisé, mais un lapin démonté à l'aide de cpio et reconfiguré (changé les chemins dans les scripts). La principale différence: il ne nécessite pas de privilèges root pour installer / configurer, n'est pas installé sur le système (le lapin reconstruit est parfaitement emballé dans tgz) et fonctionne à partir de n'importe quel utilisateur. Cette approche vous permet de mettre à niveau les versions de manière flexible (si elle ne nécessite pas un arrêt complet du cluster - dans ce cas, passez simplement au cluster de sauvegarde et mettez à jour, sans oublier de spécifier le port décalé pour le fonctionnement). Il est même possible d'exécuter plusieurs instances de RabbitMQ sur la même machine - pour tester l'option est très pratique - vous pouvez déployer une copie architecturale réduite du zoo de la bataille.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">À la suite du chamanisme avec cpio et les chemins dans les scripts, nous avons eu une option de construction: deux dossiers rabbitmq-base (dans l'assemblage d'origine - le dossier mnesia) et rabbimq-main - ici, je mets tous les scripts nécessaires du lapin et erlang lui-même. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans rabbimq-main / bin - liens symboliques vers des scripts lapin et erlang et un script de suivi de lapin (description ci-dessous). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans rabbimq-main / init.d - le script rabbitmq-server à travers lequel les journaux démarrent / s'arrêtent / tournent; en lib, le lapin lui-même; dans lib64 - erlang (utilise une version allégée, uniquement pour le lapin, version d'erlang).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il est extrêmement facile de mettre à jour l'assembly résultant lorsque de nouvelles versions sont publiées - ajoutez le contenu de rabbimq-main / lib et rabbimq-main / lib64 à partir des nouvelles versions et remplacez les liens symboliques dans bin. Si la mise à jour affecte également les scripts de contrôle, modifiez simplement les chemins d'accès aux nôtres. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un avantage significatif de cette approche est la continuité complète des versions - tous les chemins, scripts, commandes de contrôle restent inchangés, ce qui vous permet d'utiliser tous les scripts utilitaires auto-écrits sans dopage pour chaque version.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depuis la chute des lapins, bien que rare, mais survenant, il a fallu mettre en place un mécanisme de suivi de leur bien-être - remontée en cas de chute (tout en conservant les logs des raisons de la chute). </font><font style="vertical-align: inherit;">L'échec d'un nœud dans 99% des cas s'accompagne d'une entrée de journal, voire de tuer même laisse des traces, cela a permis de mettre en place une surveillance de l'état du lapin à l'aide d'un simple script. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour les versions 3.6 et 3.7, le script est légèrement différent en raison des différences dans les entrées de journal.</font></font><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour la version 3.6</font></font></b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/python import subprocess import os import datetime import zipfile def LastRow(fileName,MAX_ROW=200): with open(fileName,'rb') as f: f.seek(-min(os.path.getsize(fileName),MAX_ROW),2) return (f.read().splitlines())[-1] if os.path.isfile('/data/logs/rabbitmq/startup_log'): if b'FAILED' in LastRow('/data/logs/rabbitmq/startup_log'): proc = subprocess.Popen("ps x|grep rabbitmq-server|grep -v 'grep'", shell=True, stdout=subprocess.PIPE) out = proc.stdout.readlines() if str(out) == '[]': cur_dt=datetime.datetime.now() try: os.stat('/data/logs/rabbitmq/after_crush') except: os.mkdir('/data/logs/rabbitmq/after_crush') z=zipfile.ZipFile('/data/logs/rabbitmq/after_crush/repair_log'+'-'+str(cur_dt.day).zfill(2)+str(cur_dt.month).zfill(2)+str(cur_dt.year)+'_'+str(cur_dt.hour).zfill(2)+'-'+str(cur_dt.minute).zfill(2)+'-'+str(cur_dt.second).zfill(2)+'.zip','a') z.write('/data/logs/rabbitmq/startup_err','startup_err') proc = subprocess.Popen("~/rabbitmq-main/init.d/rabbitmq-server start", shell=True, stdout=subprocess.PIPE) out = proc.stdout.readlines() z.writestr('res_restart.log',str(out)) z.close() my_file = open("/data/logs/rabbitmq/run.time", "a") my_file.write(str(cur_dt)+"\n") my_file.close()</span></span></code> </pre> <br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Pour 3.7, seules deux lignes sont modifiées</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (os.path.isfile(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_log'</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> (os.path.isfile(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_err'</span></span>)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ((<span class="hljs-string"><span class="hljs-string">b' OK '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> LastRow(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_log'</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> (<span class="hljs-string"><span class="hljs-string">b'FAILED'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> LastRow(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_log'</span></span>))) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> (<span class="hljs-string"><span class="hljs-string">b'Gracefully halting Erlang VM'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> LastRow(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_err'</span></span>)):</code> </pre><br></div></div><br><br>  Nous mettons en place un compte crontab sous lequel le lapin travaillera (par défaut rabbitmq) exécutant ce script (nom du script: check_and_run) toutes les minutes (d'abord, nous demandons à l'administrateur de donner au compte le droit d'utiliser crontab, mais si nous avons des droits root, nous le faisons nous-mêmes): <br>  <b><i>* / 1 * * * * ~ / rabbitmq-main / bin / check_and_run</i></b> <br><br>  Le deuxième point d'utilisation du lapin remonté est la rotation des bûches. <br><br>  Comme nous ne sommes pas liés au système logrotate, nous utilisons la fonctionnalité fournie par le développeur: le <b>script rabbitmq-server</b> de init.d (pour la version 3.6) <br>  En apportant de petites modifications à <i>rotation_logs_rabbitmq ()</i> <br>  Ajouter: <br><br><pre> <code class="bash hljs"> find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.* -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f ! -name <span class="hljs-string"><span class="hljs-string">"*.gz"</span></span> | xargs -i gzip --force {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.*.back -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f | xargs -i gzip {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete</code> </pre><br>  Résultat de l'exécution du script rabbitmq-server avec la clé rotation-logs: les journaux sont compressés par gzip et ne sont stockés que pendant les 30 derniers jours.  <b>http_api</b> - le chemin où le lapin place les journaux http - configuré dans le fichier de configuration: <i>{rabbitmq_management, [{rates_mode, détaillé}, {http_log_dir, path_to_logs / http_api "}]}</i> <br><br>  Dans le même temps, je fais attention à <i>{rates_mode, <b>détaillé</b> }</i> - l'option augmente légèrement la charge, mais elle vous permet de voir des informations sur qui publie des messages dans l'EXCHENGE sur l'interface WEB (et donc de passer par l'API).  L'information est extrêmement nécessaire, car  toutes les connexions passent par l'équilibreur - nous ne verrons que l'IP des équilibreurs eux-mêmes.  Et si vous cassez tous les sous-systèmes qui fonctionnent avec le lapin pour qu'ils remplissent les paramètres des propriétés du client dans les propriétés de leurs connexions aux lapins, il sera possible d'obtenir des informations détaillées au niveau de la connexion qui exactement, où et avec quelle intensité publie les messages. <br><br>  Avec la sortie des nouvelles versions 3.7, il y a eu un rejet complet du script <b>rabbimq-server</b> dans init.d.  Afin de faciliter le fonctionnement (l'uniformité des commandes de contrôle quelle que soit la version du lapin) et une transition plus fluide entre les versions, dans le lapin réassemblé, nous continuons à utiliser ce script.  La vérité est encore une fois: nous <i>allons</i> changer <i>un</i> peu <i>rotation_logs_rabbitmq ()</i> , car le mécanisme de nommage des journaux après rotation a changé dans 3.7: <br><br><pre> <code class="bash hljs"> mv <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/<span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>.log.0 <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/<span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.$(date +%Y%m%d-%H%M%S).back mv <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/$(<span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>)_upgrade.log.0 <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/$(<span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>)_upgrade.log.$(date +%Y%m%d-%H%M%S).back find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.* -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f ! -name <span class="hljs-string"><span class="hljs-string">"*.gz"</span></span> | xargs -i gzip --force {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.* -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f ! -name <span class="hljs-string"><span class="hljs-string">"*.gz"</span></span> | xargs -i gzip --force {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete</code> </pre><br>  Maintenant, il ne reste plus qu'à ajouter la tâche de rotation des journaux à crontab - par exemple, tous les jours à 23h00: <br>  <b><i>00 23 * * * ~ / rabbitmq-main / init.d / rabbitmq-server rotation-journaux</i></b> <br><br>  Passons aux tâches à résoudre dans le cadre du fonctionnement de la "ferme aux lapins": <br><br><ol><li>  Manipulations avec des entités de lapin - création / suppression d'entités de lapin: ekschendzhey, files d'attente, liaisons, pelles, utilisateurs, politiques.  Et pour ce faire, c'est absolument identique sur tous les clusters. </li><li>  Après avoir basculé vers / depuis le cluster de sauvegarde, il est nécessaire de transférer les messages qui y sont restés vers le cluster actuel. </li><li>  Création de copies de sauvegarde des configurations de tous les clusters de tous les circuits </li><li>  Synchronisation complète des configurations de cluster dans le contour </li><li>  Arrêter / démarrer les lapins </li><li>  Pour analyser les flux de données actuels: tous les messages vont-ils et s'ils vont, alors où doivent-ils aller ou ... </li><li>  Trouvez et capturez les messages qui passent en fonction de tous les critères </li></ol><br>  Le fonctionnement de notre zoo et la solution des tâches sondées au moyen du plug-in régulier fourni <i>rabbitmq_management</i> est possible, mais extrêmement gênant, c'est pourquoi un shell a été développé et mis en œuvre pour <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">contrôler toute la variété de lapins</a></b> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr434016/">https://habr.com/ru/post/fr434016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr434004/index.html">Plateforme Wargaming: Hello World</a></li>
<li><a href="../fr434006/index.html">Avons-nous besoin de cookies à l'ère du RGPD? Nous discutons de la situation et des exigences de la loi</a></li>
<li><a href="../fr434008/index.html">Comment arrêter de s'inquiéter et commencer à écrire des tests basés sur les propriétés</a></li>
<li><a href="../fr434010/index.html">Vous développe ou configure les proxys Nginx pour Apache Tomcat sur Ubuntu en 5 minutes avec https et pare-feu</a></li>
<li><a href="../fr434012/index.html">PVS-Studio gratuit pour ceux qui développent des projets open source</a></li>
<li><a href="../fr434018/index.html">Obtenez un certificat de développeur Android associé Google</a></li>
<li><a href="../fr434034/index.html">Bons tutoriels sur YouTube</a></li>
<li><a href="../fr434036/index.html">Happy and end - les boîtes aux lettres sur les domaines du portail Qip.ru ont migré vers Yandex</a></li>
<li><a href="../fr434038/index.html">Ventes de véhicules électriques rechargeables en Chine pour novembre 2018</a></li>
<li><a href="../fr434040/index.html">Recueil universitaire ITMO: parler des projets universitaires, des succès et des réalisations de nos diplômés</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>