<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêøÔ∏è üë®‚Äçüîß üõ•Ô∏è Sie haben das Wort "Hallo" noch nicht gesagt und wir wissen bereits, wer Sie sind ‚ö°Ô∏è üîí üìì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Unser neuronales Netzwerk kann dies tun, indem es eine Person an einer ausgesprochenen Silbe erkennt. Das Thema dieses Artikels steht jedoch nicht in ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sie haben das Wort "Hallo" noch nicht gesagt und wir wissen bereits, wer Sie sind</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neurodatalab/blog/422635/">  Unser neuronales Netzwerk kann dies tun, indem es eine Person an einer ausgesprochenen Silbe erkennt.  Das Thema dieses Artikels steht jedoch nicht in direktem Zusammenhang mit der Sprachidentifikation, obwohl es sich darauf bezieht.  Wir werden √ºber neuronale Netzwerkmerkmale sprechen, den sogenannten d-Vektor, der bei Tonverarbeitungsaufgaben verwendet werden kann: von der Verifizierung √ºber die Spracherkennung bis hin zu Emotionen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/eo/h5/k8/eoh5k8gwhm9ep4wpove1up9gljg.jpeg" alt="Bild"></div><br><a name="habracut"></a><br><h4>  <b>Materiel</b> </h4><br>  Abh√§ngig von der Abtastrate kann eine Sekunde Ton 8 bis 48 Tausend Zahlen enthalten.  Sie k√∂nnen als Abweichungen von der Gleichgewichtsposition der Lautsprechermembran oder des Mikrofons dargestellt werden.  Tats√§chlich ist eine solche Beschreibung des Tons redundant: Die Amplitude des Signals zum n√§chsten Zeitpunkt h√§ngt stark von der vorherigen ab, was darauf hindeutet, dass dieses Signal ohne gro√üen Informationsverlust effektiv komprimiert werden kann.  Es gibt eine Vielzahl von M√∂glichkeiten, die Dimension eines Signals zu verringern. Die meisten davon basieren auf den physikalischen Eigenschaften des Klangs und den Eigenschaften des menschlichen Geh√∂rs. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fh/n4/qj/fhn4qjskpumyiosbtjxzik7e9yg.jpeg" alt="Bild"></div><br>  <i>Meme 1.</i> <br><br>  Bevor die neuronalen Netze (im weitesten Sinne) gut funktionierten, arbeitete die Community mit den sogenannten handgefertigten Attributen.  Die bekanntesten und am weitesten verbreiteten sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pitch</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MFCC</a> .  Die erste hat eine physikalische Bedeutung f√ºr die Frequenz der Schwingungen der Stimmb√§nder, die sich beispielsweise f√ºr verschiedene Personen unterscheiden und auch von der Intonation abh√§ngen.  Die Idee der Cepstralkoeffizienten (MFCC) basiert auf der Nichtlinearit√§t der menschlichen Wahrnehmung von Schall, n√§mlich Frequenz und Lautst√§rke.  Es scheint einer Person, dass ein Ton um einen gewissen Betrag h√∂her ist als ein anderer, wenn sich ihre Frequenzen in Wirklichkeit um eine bestimmte Anzahl von Malen unterscheiden. <br><br>  Diese und andere manuell berechnete Merkmale sind in dem Sinne irreversibel, dass ein Teil des Signals f√ºr immer verloren geht.  Bei einigen Aufgaben ist dies nicht kritisch, aber ich m√∂chte einen universelleren und funktionierenderen Ansatz entwickeln. <br><br>  Der Schl√ºssel zur L√∂sung dieses Problems ist die Fourier-Transformation.  Mit ihm k√∂nnen Sie sich ein Audiosignal als die Summe von Wellen mit unterschiedlichen Frequenzen und Amplituden vorstellen.  Tats√§chlich ist Sprache nicht station√§r in dem Sinne, dass ihr Spektrum zu verschiedenen Zeitpunkten qualitativ unterschiedlich sein wird.  Dies erm√∂glicht es uns, es in der Zeit-Frequenz-Darstellung unter Verwendung von <i>Spektrogrammen</i> zu ber√ºcksichtigen. <br><br>  Um ein Spektrogramm zu erstellen, m√ºssen Sie den Ton in sich √ºberschneidende Abschnitte (√ºberlappende Frames) mit einer L√§nge von mehreren zehn Millisekunden aufteilen. Berechnen Sie f√ºr jeden von ihnen die Fourier-Transformation und schreiben Sie ihre Module in Spalten in die Spektrogramme.  Dar√ºber hinaus ist eine solche Transformation nahezu invers, dh mit der inversen Fourier-Transformation und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem Griffin-Lim-Algorithmus k√∂nnen</a> Sie das urspr√ºngliche Tonsignal wiederherstellen (tats√§chlich tritt ein Informationsverlust auf, da die Fourier-Transformation im allgemeinen Fall komplex ist und das Spektrogramm einen reellen Wert hat, und Zur Ann√§herung an die Phasenwiederherstellung wird normalerweise der iterative Griffin-Lim-Algorithmus verwendet.  Insgesamt erhalten wir, wenn wir den Logarithmus der Amplituden nehmen, diese Bilder: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ux/ig/xz/uxigxzafbu938strrzeepi8s-ro.png" alt="Bild"></div><br>  <i>Spektrogramm 5 Sekunden Sprache.</i> <br><br>  Und sie werden bequem von Faltungsnetzen verarbeitet. <br><br>  Ein solcher Hack wird h√§ufig bei Bildverarbeitungsaufgaben verwendet: Es gibt gro√üe Datenbanken mit Beispielen f√ºr verschiedene Objekte (z. B. ImageNet).  Sie k√∂nnen ein gro√ües Raster trainieren, um sie zu erkennen, und es dann f√ºr unsere spezifische Aufgabe neu trainieren oder das Ausgabeergebnis von einer der inneren, vollst√§ndig verbundenen Ebenen √ºbernehmen.  Es wird angenommen, dass eine solche Architektur gute informative Merkmale f√ºr Eingabebilder berechnet.  Die Erfahrung zeigt, dass die Ergebnisse fast immer besser sind, als wenn wir das neuronale Netzwerk von Grund auf neu trainiert h√§tten. <br><br>  Die Idee von d-Vektoren (im Allgemeinen d-Vektoren, aber manchmal auch als x-Vektoren bezeichnet) √§hnelt der Verwendung von vorab trainierten Gittern in ImageNet, mit der Ausnahme, dass es keine √§hnlichen Grundlagen f√ºr Spektrogramme gibt.  Als m√∂glicher Ausweg k√∂nnen Auto-Encoder in Betracht gezogen werden, die jedoch a priori nicht wissen, wonach sie im Spektrogramm suchen sollen, weshalb sie unbefriedigend arbeiten. <br><br><h4>  <b>Wir m√ºssen tiefer gehen</b> </h4><br>  Achtung, der Hauptteil dieses Artikels beginnt. <br><br>  Die Aufgabe, eine Person durch Stimme zu verifizieren, ist allgemein bekannt, wobei es notwendig ist, durch das eingegebene Sprachsegment zu bestimmen, welche der Personen in der Datenbank dies gesagt hat.  Tats√§chlich ist der Aufbau solcher Systeme eine separate Wissenschaft, und es gibt viele verschiedene Add-Ons (Sprechdauer; muss jeder den gleichen Text sprechen; Inszenierung von eins gegen eins oder eins gegen alle), die unter verschiedenen Bedingungen kritisch sind, aber f√ºr uns Sie m√ºssen auf etwas anderes achten. <br><br>  N√§mlich: Wie gut die Funktionen sind, wenn wir das Raster vorab trainieren, um eine Person zu erkennen.  Alles wird zum Wohle der Zeichen getan. <br><br>  Dies wird uns helfen, Intuition und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel von</a> 2015.  Darin lehren die Autoren das Gitter, eine Person am Gesicht zu erkennen (Gesichtserkennung).  Der Schl√ºssel zu dieser Arbeit ist die Verwendung von Triplet Loss. <br><br>  Seine Idee ist sehr einfach: Wir normalisieren Merkmale aus der vorletzten Ebene so, dass sie auf einer Einheitskugel liegen, und verlangen, dass Punkte einer Klasse nahe und weit entfernt von verschiedenen liegen.  Dies kann wie folgt erreicht werden: F√ºr jedes Trainingsbeispiel (Anker) finden wir zwei weitere aus derselben und einer anderen Klasse in der Stichprobe - positiv und negativ.  Dann bilden wir f√ºr diese Dreifachpunkte einen Verlust: <br><br>  \ begin {Gleichung} <br>  \ Big [\ Vert f (x ^ a) - f (x ^ p) \ Vert - \ Vert f (x ^ a) - f (x ^ n) \ Vert + \ alpha \ Big] _ +, <br>  \ end {Gleichung} <br><br>  Dabei ist x das Eingabebild, f die Rasterausgabe nach der Normalisierung, Alpha der manuell ausgew√§hlte Parameter, [] _ ‚Äã‚Äã{+} die ReLU-Funktion.  Qualitativ ist der Wert dieses Verlusts Null, wenn der Abstand zwischen Anker und positiven Punkten um mindestens Alpha gr√∂√üer als der Abstand zwischen Anker und Negativ ist und je gr√∂√üer der Unterschied zwischen zwei verschiedenen Klassen ist. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fp/k6/yy/fpk6yyh_hd1hb00bsdcg39xyuqc.png" alt="Bild"></div><br>  <i>Eine Illustration dessen, was nach dem Training mit Triplet Loss mit Funktionen passiert.</i> <br><br>  √úbrigens k√∂nnen Sie auf intelligente Weise Tripel bilden.  Irgendwann wird das Ausma√ü des Verlusts gering, und um das Lernen zu beschleunigen, k√∂nnen Sie nicht unter allen anderen Klassen nach negativen Beispielen suchen, sondern nur diejenigen, die sich in der N√§he des Ankers befinden.  Bei gro√üen Datenmengen ist dies jedoch schwierig, da Sie paarweise Abst√§nde zwischen Klassen ber√ºcksichtigen m√ºssen, die sich nach jeder Iteration des Netzwerklernens √§ndern. <br><br>  Der Triplettverlust hat einen Vorteil gegen√ºber der kategorialen Kreuzentropie, die bei der herk√∂mmlichen Klassifizierung verwendet wird.  Ein mit Cross-Entropy trainiertes Modell versucht, alle Punkte einer Klasse in einem immer kleineren Bereich zusammenzufassen, und Informationen, die f√ºr eine bestimmte Aufgabe √ºberfl√ºssig sind, k√∂nnen verloren gehen.  Dies m√∂chten wir jedoch nicht, da wir das neuronale Netzwerk als Feature-Generator und nicht zur √úberpr√ºfung verwenden werden.  Triplettverlust hat diese Eigenschaft in viel geringerem Ma√üe: Es ist wichtiger, verschiedene Klassen in verschiedenen Bereichen auf einer einzigen Kugel zu verteilen, als eine Klasse zu tragen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pn/pt/nz/pnptnzxqejabade7orpavbtx5se.jpeg" alt="Bild"></div><br>  <i>Meme 2.</i> <br><br>  Bevor Sie den Feature-Generator auf Spektrogramme trainieren, m√ºssen Sie zun√§chst deren Gr√∂√üe bestimmen.  Offensichtlich ist die Genauigkeit der Klassifizierung umso h√∂her, je l√§nger der von uns betrachtete Zeitraum ist, aber desto mehr ‚Äûgemittelte‚Äú Zeichen werden sich herausstellen.  Daher ist es sinnvoll, eine solche Signall√§nge so zu verwenden, dass 1-3 Phoneme (Silben) hineinfallen - eine halbe Sekunde erscheint angemessen. <br><br>  F√ºr das Training nehmen wir den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VoxCeleb2-</a> Datensatz, in dem f√ºr jeden der 6300 Lautsprecher mehrere separate Audioaufnahmen von jeweils mehreren Minuten (unter verschiedenen Bedingungen erstellt) vorhanden sind.  Wir verwenden einen Teil der Audiodateien f√ºr das Training und den Rest f√ºr die Validierung, w√§hlen die Faltungsnetzwerkarchitektur aus, f√ºgen Triplet Loss hinzu und lernen. <br><br>  Die Ergebnisse waren sehr cool.  In fast 2 Wochen Training bei 1080Ti (ja, so lange) erreichte die Klassifizierungsgenauigkeit 55%.  Es scheint nicht sehr viel zu sein, aber die Genauigkeit der Top-5 betr√§gt 78%. Wenn wir nur die lauteste H√§lfte der Fragmente betrachten, bei denen es sich haupts√§chlich um betonte Vokale handelt, steigt die Genauigkeit der Top-5 auf 91%.  Wir k√∂nnen sagen, dass wir eine Person anhand einer ihrer Phrasen mit angemessener Genauigkeit identifizieren k√∂nnen.  Aber es spielt keine Rolle. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vz/el/21/vzel21arcsvyrco_kg_95usxziy.jpeg" alt="Bild"></div><br>  <i>Meme 3.</i> <br><br>  Schlie√ülich wurde alles f√ºr Merkmale gestartet, die als Ausweg aus dem vorletzten erhalten werden k√∂nnen, bevor die neuronale Netzwerkschicht klassifiziert wird.  Wir haben sie auf unsere Aufgaben getestet und √ºberall waren die Ergebnisse besser als bei der Berechnung von Attributen mit klassischen Ans√§tzen.  Beim Problem der Emotionserkennung konnten wir beispielsweise durch die Verwendung von d-Vektoren den Stand der Technik um 4% umgehen, und der entsprechende Artikel wurde auf der FICC 2019-Konferenz angenommen. Die Erkennung von Emotionen ist jedoch eine v√∂llig andere Geschichte, √ºber die wir sp√§ter sprechen werden. <br><br>  Gepostet von <b>Gregory Sterling</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">sterling239</a> , Deep Learning-Experte, Neurodata Lab. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422635/">https://habr.com/ru/post/de422635/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422625/index.html">Wir haben mit Troy Miles gesprochen - dem Programmierer von "Neuromancer"</a></li>
<li><a href="../de422627/index.html">MongoDB und IT-Arbeitsmarktforschung</a></li>
<li><a href="../de422629/index.html">H√∂r auf, die Holzf√§ller zu f√ºttern! Gib mehr Modifikatoren! Lazy Static Final Fields. Entwurf einer Feature-Skizze</a></li>
<li><a href="../de422631/index.html">QIWI-Terminals. So holen Sie das Beste aus einfachen Technologien heraus</a></li>
<li><a href="../de422633/index.html">Wie wir die √úberwachung der Arbeit der Mitarbeiter des Tankstellennetzes des Bundes automatisiert haben</a></li>
<li><a href="../de422637/index.html">Geeks Geschenk: Auto-Alkash-Schutz</a></li>
<li><a href="../de422641/index.html">Polarnacht, Wasserpumpen und Smart Safe: 5 Studentenprojekte im Bereich IoT</a></li>
<li><a href="../de422643/index.html">Neue Ger√§te mit IFA 2018</a></li>
<li><a href="../de422645/index.html">Welche Bedeutung hat 196 884 = 196 883 + 1? Wie erkl√§rt man es an den Fingern?</a></li>
<li><a href="../de422649/index.html">Multiplayer VR: Wie implementiere ich?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>