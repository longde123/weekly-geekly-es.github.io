<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐰 👨🏾‍✈️ ⏫ Unity的机器学习代理 👁‍🗨 🤨 📂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="这篇关于Unity机器学习代理的文章是由技术创新者，Unity的活跃开发人员，许多Unity游戏，图形项目和书籍的顾问，经理和作者Michael Lanham撰写的。 

 Unity开发人员已实现了对机器学习的支持，尤其是增强学习，以为游戏和模拟开发人员创建深度增强学习（DRL）SDK。 幸运的是...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Unity的机器学习代理</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454612/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/6fe/055/9aa6fe055c20cde3642bb0f0782f62d3.jpg" alt="图片"></div><br>  <em>这篇关于Unity机器学习代理的文章是由技术创新者，Unity的活跃开发人员，许多Unity游戏，图形项目和书籍的顾问，经理和作者Michael Lanham撰写的。</em> <br><br>  Unity开发人员已实现了对机器学习的支持，尤其是增强学习，以为游戏和模拟开发人员创建深度增强学习（DRL）SDK。 幸运的是，由丹尼·兰格（Danny Lange）领导的Unity团队已成功实施了可靠的现代DRL引擎，能够带来令人印象深刻的结果。  Unity使用近端策略优化（PPO）模型作为DRL引擎的基础； 该模型要复杂得多，并且在某些方面可能有所不同。 <br><br> 在本文中，我将向您介绍在游戏和模拟中创建DRL代理的工具和SDK。 尽管该工具具有新颖性和强大功能，但它易于使用，并且具有辅助工具，可让您随时随地学习机器学习概念。 要使用本教程，您需要安装Unity引擎。 <br><a name="habracut"></a><br><h2> 安装ML-Agent </h2><br> 在本节中，我将简要讨论安装ML-Agents SDK所必须采取的步骤。 该材料仍处于测试阶段，并且可能因版本而异。 请按照下列步骤操作： <br><br><ol><li> 在计算机上安装Git； 它可以从命令行运行。  Git是一个非常流行的源代码管理系统，并且Internet上有很多关于跨平台安装和使用Git的资源。 安装Git之后，请通过创建任何存储库的克隆来确保它能正常工作。 </li><li> 打开命令提示符或常规shell。  Windows用户可以打开Anaconda窗口。 </li><li> 转到要放置新代码的工作文件夹，然后输入以下命令（Windows用户可以选择C：\ ML-Agents）： <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents </pre></li><li> 因此，您可以在计算机上克隆ml-agents存储库并创建一个具有相同名称的新文件夹。 您也可以在文件夹名称中添加版本号。 就像几乎整个人工智能世界一样，团结正在不断变化，至少目前如此。 这意味着新的变化不断出现。 在撰写本文时，我们正在将存储库克隆到ml-agents.6文件夹中： <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents ml-agents.6 </pre></li><li> 为ml-agents创建一个新的虚拟环境并指定版本3.6，如下所示： <br><br><pre>  #Windows 
 conda创建-n ml-agents python = 3.6
 
 #Mac
将文档用于您的首选环境 </pre></li><li> 使用Anaconda再次激活您的环境： <br><br><pre> 激活ml代理 </pre></li><li> 安装TensorFlow 在Anaconda中，可以使用以下命令完成此操作： <br><br><pre>  pip install tensorflow == 1.7.1 </pre></li><li> 安装Python软件包。 在Anaconda中，输入以下内容： <br><br><pre><code class="plaintext hljs">cd ML-Agents #from root folder cd ml-agents or cd ml-agents.6 #for example cd ml-agents pip install -e . or pip3 install -e .</code> </pre> </li><li> 因此，您将安装所有必需的Agents SDK程序包。 这可能需要几分钟。 不要关闭窗户，它将很快派上用场。 </li></ol><br> 因此，我们为ML-Agent安装并配置了Unity Python SDK。 在下一节中，我们将学习如何设置和训练Unity提供的众多环境之一。 <br><br><h2> 代理商培训 </h2><br> 现在，我们可以立即着手开展业务，并探索使用深度强化学习（DRL）的示例。 幸运的是，新代理的工具包中有几个示例可以演示引擎的功能。 打开Unity或Unity Hub，然后执行以下步骤： <br><br><ol><li> 单击“项目”对话框顶部的“打开项目”按钮。 </li><li> 找到并打开UnitySDK项目文件夹，如屏幕截图所示： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/371/706/016/3717060168f78fccd271c064f0e055ce.png"></div><br>  <i>打开Unity SDK项目</i> </li><li> 等待项目加载，然后打开编辑器底部的“项目”窗口。 如果打开一个窗口，要求您更新项目，则选择是或继续。 当前，所有代理代码都向后兼容。 </li><li> 找到并打开GridWorld场景，如屏幕截图所示： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0aa/6bd/ab5/0aa6bdab5b8d07cdf414e59255862c05.png"></div><br>  <em>打开GridWorld场景的示例</em> </li><li> 在“层次结构”窗口中选择GridAcademy对象。 </li><li> 转到“检查器”窗口，然后在“大脑”字段旁边单击图标以打开“大脑选择”对话框： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08c/cda/126/08ccda1263b74d131c70cff42edae92c.png"></div></li><li> 选择GridWorldPlayer的大脑。 这个大脑属于玩家，也就是说，玩家（您）可以控制游戏。 </li><li> 单击编辑器顶部的“播放”按钮，然后观察环境。 由于现在已将游戏设置为控制玩家，因此可以使用WASD键移动立方体。 任务是将蓝色立方体移至绿色+符号，同时避开红色X。 </li></ol><br> 在游戏中变得舒适。 请注意，该游戏只能在特定时间段内运行，并且不基于回合制。 在下一节中，我们将学习如何使用DRL代理运行此示例。 <br><br><h2> 大脑里有什么？ </h2><br>  ML-Agents平台的令人惊奇的方面之一是能够快速轻松地从播放器管理切换到AI /代理管理的能力。 为此，Unity使用“大脑”的概念。 大脑可以由玩家控制，也可以由代理（学习大脑）控制。 最令人惊奇的是，您可以组装游戏并将其作为玩家进行测试，然后在RL代理的控制下进行游戏。 因此，可以使用AI来控制任何书面游戏。 <br><br> 在Unity中设置和启动RL代理培训的过程非常简单。  Unity使用外部Python建立学习大脑的模型。 使用Python非常有意义，因为已经围绕它构建了多个深度学习（DL）库。 要在GridWorld中训练代理，请完成以下步骤： <br><br><ol><li> 再次选择GridAcademy，然后在Brains字段而不是GridWorldPlayer中选择GridWorldLearning大脑： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/869/fc8/b42/869fc8b42b64d8b3ccc785eb9a6e765e.png"></div><br>  <em>切换到使用GridWorldLearning Brain</em> </li><li> 选中右侧的控制框。 这个简单的参数表明大脑可以从外部进行控制。 必须启用此选项。 </li><li> 在“层次结构”窗口中选择trueAgent对象，然后在“检查器”窗口中，将“网格代理”组件中的Brain属性更改为GridWorldLearning脑： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/ff3/ed2/832ff3ed2bc17c16cc5ed823d10a7156.png"></div><br>  <em>GridWorldLearning大脑工作的代理</em> </li><li> 在此示例中，我们需要Academy和Agent都使用相同的GridWorldLearning大脑。 切换到Anaconda或Python窗口，然后选择ML-Agents / ml-agents文件夹。 </li><li> 使用ml-agents虚拟环境在Anaconda或Python窗口中运行以下命令： <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = firstRun --train </pre></li><li> 这将启动Unity PPO培训模型和具有指定配置的示例代理。 在某个时刻，命令提示符窗口将要求您使用加载的环境启动Unity编辑器。 </li><li> 在Unity编辑器中单击“播放”以启动GridWorld环境。 不久之后，您应该会看到代理培训并输出到Python脚本窗口： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/583/cc2/fd9/583cc2fd94c49e4a1039d9af64312f68.png"></div><br>  <em>在学习模式下运行GridWorld</em> </li><li> 请注意，mlagents-learn脚本是构建RL模型以运行代理的Python代码。 从脚本的输出中可以看到，需要配置多个参数（超参数）。 </li><li> 让代理学习几千次迭代，并注意其学习速度。 事实证明，这里使用的称为PPO的内部模型是非常有效的学习模型，可以完成许多不同的任务，非常适合游戏开发。 借助功能强大的设备，代理商可以在不到一个小时的时间内完成学习。 </li></ol><br> 让代理进一步学习，并探索其他方式来跟踪代理的学习过程，如下一节所述。 <br><br><h2> 使用TensorBoard监控学习 </h2><br> 使用RL模型或任何DL模型训练代理通常是一项艰巨的任务，需要注意细节。 幸运的是，TensorFlow有一套称为TensorBoard的图表工具，您可以使用它们来监视学习过程。 请按照以下步骤启动TensorBoard： <br><br><ol><li> 打开Anaconda或Python窗口。 激活ml-agents虚拟环境。 不要关闭正在运行训练模型的窗口； 我们需要它继续。 </li><li> 转到ML-Agents / ml-agents文件夹并运行以下命令： <br><br><pre>  tensorboard --logdir =摘要 </pre></li><li> 因此，我们在自己的内置Web服务器上启动TensorBoard。 您可以使用上一条命令后显示的URL加载页面。 </li><li> 输入窗口中所示的TensorBoard的URL，或在浏览器中键入localhost：6006或计算机名：6006。 大约一个小时后，您应该会看到以下内容： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/619/b62/964/619b62964a88d9ee4b7635073799caf8.png"></div><br>  <em>TensorBoard图表窗口</em> </li><li> 上一个屏幕截图显示了图形，每个图形都显示了培训的一个独立方面。 要了解我们的代理商的培训方式，您需要处理以下每个图，因此我们将分析每个部分的输出： </li></ol><br><ul><li> 环境：此部分显示代理如何在整个环境中表现自己。 以下是具有首选趋势的图表的更详细视图： </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/596/956/bc15969569f3d959bc550c2ee629ac3d.png"></div><br>  <em>“环境”部分的图形的详细图片</em> <br><br><ul><li> 累积奖励：这是使代理最大化的总奖励。 通常，它有必要增加，但是由于某些原因它可能会减少。 始终最好将1到-1之间的奖励最大化。 如果进度奖励超出此范围，则也需要解决此问题。 </li><li> 剧集长度：如果该值减小，则通常是一个好兆头。 最终，情节越短，训练就越多。 但是，请记住，如果有必要，情节的长度可能会增加，因此图片可能会有所不同。 </li><li> 经验教训：此图可以清楚地表明座席在哪个课程中； 它旨在用于课程学习。 </li><li> 损失：此部分显示了代表保险单和价值的计算损失或成本的图表。 以下是此部分的屏幕截图，其中的箭头指向最佳设置： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aaa/8c5/3f6/aaa8c53f67533a8d349b62d216c15a1b.png"></div><br>  <em>损失和首选培训</em> </li></ul><br><ul><li> 策略损失：此图表确定随时间变化的策略量。 政治是定义行动的要素，在一般情况下，此时间表应趋于下降，表明政治正在做出更好的决策。 </li><li> 价值损失：这是价值函数的平均损失。 本质上，它对代理预测其下一状态的值的能力进行建模。 最初，该值应增加，而在薪酬稳定后，该值应减小。 </li><li> 政策：为了评估PPO中行动的质量，使用的是政策的概念，而不是模型。 下面的屏幕快照显示了策略图表和首选趋势： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/0ce/2c0/8450ce2c0e0a95bb39e92fe698dbee7c.png"></div><br>  <i>政策图表和首选趋势</i> </li><li> 熵：此图显示了研究代理的大小。 该值需要降低，因为代理可以了解更多有关环境的知识，而需要的研究较少。 </li><li> 学习率：在这种情况下，该值应逐渐线性减小。 </li><li> 价值估算：这是所有座席州访问的平均值。 为了反映代理人不断增长的知识，此价值必须先增长然后稳定。 </li></ul><br>  6.保持代理运行直到完成，并且不要关闭TensorBoard。 <br>  7.返回到训练大脑的Anaconda / Python窗口并运行以下命令： <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = secondRun --train </pre><br>  8.再次要求您在编辑器中单击“播放”。 这样吧。 让代理开始培训并进行几次会议。 在此过程中，观看TensorBoard窗口，并注意secondRun在图表上的显示方式。 您可以让该代理运行直到完成，但是可以根据需要停止它。 <br><br> 在早期版本的ML-Agent中，您必须首先将Unity可执行文件构建为游戏的学习环境，然后再运行它。  Python的外脑应该以相同的方式工作。 这种方法很难调试代码或游戏中的问题。 在新技术中，消除了所有这些困难。 <br><br> 既然我们已经知道设置和训练代理是多么容易，我们将进入下一部分，在该部分中，我们将学习如何在没有Python外来知识的情况下运行代理，并直接在Unity中执行它。 <br><br><h2> 代理启动 </h2><br>  Python训练很棒，但是您不能在真实游戏中使用它。 理想情况下，我们希望构建一个TensorFlow图表并在Unity中使用它。 幸运的是，创建了TensorFlowSharp库，该库允许.NET使用TensorFlow图形。 这使我们能够构建离线TFModels模型，然后将它们注入游戏中。 不幸的是，我们只能使用经过训练的模型，而不能以这种方式进行训练，至少目前还没有。 <br><br> 让我们用我们刚刚为GridWorld环境训练的图的例子来看看它是如何工作的。 将其用作Unity中的内在大脑。 请按照以下部分中的步骤设置和使用您的内脑： <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="external nofollow">从此处</a>下载TFSharp插件 </li><li> 从编辑器菜单中，选择资产| 进口包装 定制包装... </li><li> 找到您刚刚下载的资产包，然后使用导入对话框将插件加载到项目中。 </li><li> 从菜单中，选择编辑| 项目设置。  ``设置''窗口打开（出现在2018.3版中） </li><li> 在“播放器”选项中找到“脚本定义符号”字符，并将文本更改为ENABLE_TENSORFLOW，然后启用“允许不安全代码”，如屏幕截图所示： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/ecf/c31/79decfc310a7f6cf38b33d1998c14c1f.png"></div><br>  <em>设置标志ENABLE_TENSORFLOW</em> </li><li> 在“层次结构”窗口中找到GridWorldAcademy对象，并确保它使用Brains |  GridWorldLearning。 在Grid Academy脚本的Brains部分中禁用Control选项。 </li><li> 在Assets / Examples / GridWorld / Brains文件夹中找到GridWorldLearning大脑，并确保设置了Inspector窗口中的Model参数，如屏幕快照所示： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/7c8/fef/fb47c8fefd97d4e855e8a2c029e4da5d.png"></div><br>  <em>大脑的模型任务</em> </li><li>  GridWorldLearning应该已经设置为模型。 在此示例中，我们使用GridWorld示例随附的TFModel。 </li><li> 单击“播放”以启动编辑器，然后查看代理如何管理多维数据集。 </li></ol><br> 我们现在正在启动Unity预训练环境。 在下一节中，我们将学习如何使用上一节中训练的大脑。 <br><br><h2> 受过训练的大脑负荷 </h2><br> 所有Unity示例都具有经过预训练的大脑，可用于研究示例。 当然，我们希望能够将自己的TF图加载到Unity中并运行它们。 要加载训练图，请按照下列步骤操作： <br><br><ol><li> 转到ML-Agents / ml-agents / models / firstRun-0文件夹。 在此文件夹中是GridWorldLearning.bytes文件。 将此文件拖到Unity编辑器中的Project / Assets / ML-Agents / Examples / GridWorld / TFModels文件夹中： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f51/955/ef5/f51955ef56aae53e7946378048b9e13e.png"></div><br>  <em>将字节图拖入Unity</em> </li><li> 因此，我们将图形作为资源导入到Unity项目中，并将其重命名为GridWorldLearning1。引擎这样做是因为默认模型已经具有相同的名称。 </li><li> 在brain文件夹中找到GridWorldLearning，在Inspector窗口中将其选中，然后将新的GridWorldLearning 1模型拖到Brain Parameters的Model字段中： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/784/8a3/9b7/7848a39b79b02ffb11d968f835054295.png"></div><br>  <em>将大脑加载到“图形模型”字段中</em> </li><li> 在此阶段，我们不需要更改任何其他参数，而是要特别注意大脑的配置方式。 目前，标准设置即可。 </li><li> 在Unity编辑器中单击“播放”，然后查看代理如何在游戏中成功移动。 </li><li> 代理在游戏中的成功取决于他的训练时间。 如果您允许他完成培训，则该代理将类似于经过全面培训的Unity代理。 </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN454612/">https://habr.com/ru/post/zh-CN454612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN454600/index.html">我们如何在Freelansim上达成安全交易：做出选择，削减功能，比较佣金</a></li>
<li><a href="../zh-CN454604/index.html">在几分钟内生成带有GraphQL后端的React应用程序</a></li>
<li><a href="../zh-CN454606/index.html">移动操作系统和浏览器的inputmode属性的功能</a></li>
<li><a href="../zh-CN454608/index.html">服务水平协议：我们为其他人编写SLA，或与电信运营商签订SLA</a></li>
<li><a href="../zh-CN454610/index.html">内容营销，SEO，测试和民意测验：9种在国外促进创业的工具</a></li>
<li><a href="../zh-CN454614/index.html">XXE：XML外部实体</a></li>
<li><a href="../zh-CN454616/index.html">使用AI提高脑力劳动者的效率</a></li>
<li><a href="../zh-CN454618/index.html">生产力坑：松弛如何损害我们的工作流程</a></li>
<li><a href="../zh-CN454620/index.html">#NoDeployFriday：是有益还是有害？</a></li>
<li><a href="../zh-CN454622/index.html">Kreisel EVEX 910e：历史模型-新生活</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>