<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö£üèæ üåÜ üí£ G√©n√©rateur d√©lirant: cr√©ez des textes dans n'importe quelle langue √† l'aide d'un r√©seau de neurones üòò üöò üßÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, Habr. 

 Cet article sera dans un format un peu "vendredi", aujourd'hui nous traiterons de la PNL. Pas le PNL sur lequel les livres sont vendus...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>G√©n√©rateur d√©lirant: cr√©ez des textes dans n'importe quelle langue √† l'aide d'un r√©seau de neurones</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470035/">  Salut, Habr. <br><br>  Cet article sera dans un format un peu "vendredi", aujourd'hui nous traiterons de la PNL.  Pas le PNL sur lequel les livres sont vendus dans les passages souterrains, mais celui que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Natural Language Processing</a> traite les langues naturelles.  Comme exemple d'un tel traitement, la g√©n√©ration de texte utilisant un r√©seau neuronal sera utilis√©e.  Nous pouvons cr√©er des textes dans n'importe quelle langue, du russe ou de l'anglais, au C ++.  Les r√©sultats sont tr√®s int√©ressants, vous pouvez probablement le deviner sur la photo. <br><br><img src="https://habrastorage.org/webt/vc/cy/we/vccywe4c6r0vbryvvx3qiale_j8.jpeg"><br><br>  Pour ceux qui sont int√©ress√©s par ce qui se passe, les r√©sultats et le code source sont sous la coupe. <br><a name="habracut"></a><br><h2>  Pr√©paration des donn√©es </h2><br>  Pour le traitement, nous utiliserons une classe sp√©ciale de r√©seaux de neurones - les r√©seaux de neurones dits r√©currents (RNN).  Ce r√©seau diff√®re de l'habituel en ce qu'en plus des cellules habituelles, il poss√®de des cellules m√©moire.  Cela nous permet d'analyser des donn√©es d'une structure plus complexe, et en fait, plus proche de la m√©moire humaine, car nous ne partons pas non plus de toutes les pens√©es ¬´√† partir de z√©ro¬ª.  Pour √©crire du code, nous utiliserons les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©seaux LSTM</a> (Long Short-Term Memory), car ils sont d√©j√† pris en charge par Keras. <br><br><img src="https://habrastorage.org/webt/ay/ox/ou/ayoxourylcbidznetfkphgdv5ni.jpeg"><br><br>  Le probl√®me suivant qui doit √™tre r√©solu est, en fait, de travailler avec du texte.  Et ici, il y a deux approches - pour soumettre des symboles ou les mots entiers √† l'entr√©e.  Le principe de la premi√®re approche est simple: le texte est divis√© en blocs courts, o√π les ¬´entr√©es¬ª sont un morceau de texte et la ¬´sortie¬ª est le caract√®re suivant.  Par exemple, pour la derni√®re phrase, ¬´les entr√©es sont un morceau de texte¬ª: <br><br> <code>input:    output: "" <br> input:    : output: "" <br> input:    : output:"" <br> input:    : output: "" <br> input:    : output: "". <br></code> <br>  Et ainsi de suite.  Ainsi, le r√©seau neuronal re√ßoit des fragments de texte en entr√©e et en sortie les caract√®res qu'il doit former. <br><br>  La deuxi√®me approche est fondamentalement la m√™me, seuls des mots entiers sont utilis√©s √† la place des mots.  Tout d'abord, un dictionnaire de mots est compil√© et des nombres sont saisis √† la place des mots √† l'entr√©e du r√©seau. <br><br>  Ceci, bien s√ªr, est une description plut√¥t simplifi√©e.  Keras a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©j√† des</a> exemples de g√©n√©ration de texte, mais d'une part, ils ne sont pas d√©crits en d√©tail, et d'autre part, tous les didacticiels en anglais utilisent des textes plut√¥t abstraits comme Shakespeare, qui sont difficiles √† comprendre pour le natif.  Eh bien, nous testons un r√©seau de neurones sur notre grand et puissant r√©seau, qui, bien s√ªr, sera plus clair et compr√©hensible. <br><br><h2>  Formation r√©seau </h2><br>  En tant que texte d'entr√©e, j'ai utilis√© ... les commentaires de Habr, la taille du fichier source est de 1 Mo (il y a en fait plus de commentaires, bien s√ªr, mais je n'ai d√ª utiliser qu'une partie, sinon le r√©seau de neurones aurait √©t√© form√© pendant une semaine et les lecteurs n'auraient pas vu ce texte vendredi).  Permettez-moi de vous rappeler que seules les lettres sont aliment√©es √† l'entr√©e d'un r√©seau neuronal, le r√©seau ne ¬´sait¬ª rien du langage ou de sa structure.  Allons-y, commen√ßons la formation du r√©seau. <br><br>  <b>5 minutes de formation:</b> <br><br>  Jusqu'√† pr√©sent, rien n'est clair, mais vous pouvez d√©j√† voir des combinaisons de lettres reconnaissables: <br><br> <code>                          .                   .                                                     .          ¬´                    <br></code> <br>  <b>15 minutes de formation:</b> <br><br>  Le r√©sultat est d√©j√† nettement meilleur: <br><br> <code>                                                                                                                                 <br></code> <br>  <b>1 heure de formation:</b> <br><br> <code>                                                                  ¬´ ¬ª ‚Äî                                                            ¬´     ¬ª ¬ª ‚Äî             </code> <br> <br>  Pour une raison quelconque, tous les textes se sont r√©v√©l√©s √™tre sans points et sans majuscules, peut-√™tre que le traitement utf-8 n'a pas √©t√© fait correctement.  Mais dans l'ensemble, c'est impressionnant.  En analysant et en se souvenant uniquement des codes de symboles, le programme a effectivement appris ¬´ind√©pendamment¬ª des mots russes et peut g√©n√©rer un texte d'aspect assez cr√©dible. <br><br>  Non moins int√©ressant est le fait que le programme m√©morise assez bien le style de texte.  Dans l'exemple suivant, le texte d'une loi a √©t√© utilis√© comme outil p√©dagogique.  Temps de formation r√©seau 5 minutes. <br><br> <code>  ""  ,  ,  ,  ,  ,  , ,  ,                 <br></code> <br>  Et ici, les annotations m√©dicales pour les m√©dicaments ont √©t√© utilis√©es comme un ensemble d'entr√©e.  Temps de formation r√©seau 5 minutes. <br><br> <code>  <br>      <br>                                          ,    ,             <br></code> <br>  Ici, nous voyons des phrases presque enti√®res.  Cela est d√ª au fait que le texte original est court et que le r√©seau neuronal a en fait "m√©moris√©" certaines phrases dans leur ensemble.  Cet effet est appel√© ¬´recyclage¬ª et doit √™tre √©vit√©.  Id√©alement, vous devez tester un r√©seau de neurones sur de grands ensembles de donn√©es, mais la formation dans ce cas peut prendre de nombreuses heures, et malheureusement je n'ai pas de supercalculateur suppl√©mentaire. <br><br>  Un exemple amusant d'utilisation d'un tel r√©seau est la g√©n√©ration de noms.  Apr√®s avoir t√©l√©charg√© une liste de noms masculins et f√©minins dans le fichier, j'ai obtenu de nouvelles options assez int√©ressantes qui conviendraient tout √† fait √† un roman de science-fiction: Rlar, Laaa, Aria, Arera, Aelia, Ninran, Air.  Quelque chose en eux sent le style d'Efremov et de la n√©buleuse d'Androm√®de ... <br><br><h2>  C ++ </h2><br>  Ce qui est int√©ressant, c'est que dans l'ensemble, un r√©seau de neurones, c'est comme se souvenir.  L'√©tape suivante consistait √† v√©rifier comment le programme g√®re le code source.  Comme test, j'ai pris diff√©rentes sources C ++ et les ai combin√©es en un seul fichier texte. <br><br>  Honn√™tement, le r√©sultat a surpris encore plus que dans le cas de la langue russe. <br><br>  <b>5 minutes de formation</b> <br><br>  Merde, c'est presque du vrai C ++. <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( snd_pcm_state_channels = <span class="hljs-number"><span class="hljs-number">0</span></span> ) { errortext_ = <span class="hljs-string"><span class="hljs-string">"rtapialsa::probedeviceopen: esror stream_.buffer stream!"</span></span>; errortext_ = errorstream_.str(); <span class="hljs-keyword"><span class="hljs-keyword">goto</span></span> unlock; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( stream_.mode == input &amp;&amp; stream_.mode == output || <span class="hljs-literal"><span class="hljs-literal">false</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( stream_.state == stream_stopped ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;stream_.nuserbuffer[i] ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (j=<span class="hljs-number"><span class="hljs-number">0</span></span>; j&lt;info.channels; } } }</code> </pre><br>  <b>30 minutes de formation</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> maxirecorder::stopstream() { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; ainchannels; i++ ) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> input=(stream, null; conternallock( pthread_cond_wate); <span class="hljs-comment"><span class="hljs-comment">// this should not ported to the device is a structure are is and the stream the buffer of the interleaved as and the probe the arauliored format for the the stream os and the software and the supported of the is so aa is set the directsound of allocates the reset and real the read the setup the buffer // this is the setup to the the devices. // asio will open the supported by the supported buffer are asioned structure. // this should not have to setup the counting and the proper in the position we mast it alw the aftware configuration. // this is the the stream and probe the supported be audio for sched/ond changed to this the sample rate it in the counting the fres // mecorend. windevoppart( null, 0, &amp;phase ); possint=&gt;buffersize( 1, bufferlist); } } } ///********************************************* void maxirecorder::abortdevice( device ) { if (cout &lt;&lt; aindeff( thread_cont_start); for (int i = 0; i &lt; sample_rates[0]) { if (count == int) { std::cout &lt;&lt; "checksing of convernion allocation handler (" &lt;&lt; device &lt;&lt; ")."; errortext_ = errorstream_.str(); return failure; } } } if ( stream_.userbuffer[i] ) { errortext_ = "rtapicore::probedeviceopen: system error (" &lt;&lt; geterrorcode( result ) &lt;&lt; ") getting stream configuration info state."; error( rterror::warning ); return; } if ( stream_.mode == output || stream_.mode == duplex ) { callbackinfo *info = (const char *) callback( stream_.userbuffer[1], stream_.convertinfo[0] ); return info; } // setup buffer callback for conversion on do any the supported be audio bet that al alinums be of support of the supported buffer // the buffer size a stream and buffers. info.name = dsdevices[ device ]; return 0; }</span></span></code> </pre> <br>  Comme vous pouvez le voir, le programme a "appris" √† √©crire des fonctions enti√®res.  En m√™me temps, il s√©parait compl√®tement ¬´humainement¬ª les fonctions par un commentaire avec des ast√©risques, mettait des commentaires dans le code, et tout √ßa.  Je voudrais apprendre un nouveau langage de programmation avec une telle vitesse ... Bien s√ªr, il y a des erreurs dans le code, et bien s√ªr, il ne se compilera pas.  Et au fait, je n'ai pas format√© le code, le programme a √©galement appris √† mettre des parenth√®ses et des retraits "moi-m√™me". <br><br>  Bien s√ªr, ces programmes n'ont pas l'essentiel - le <i>sens</i> , et ont donc l'air surr√©aliste, comme s'ils √©taient √©crits dans un r√™ve, ou qu'ils n'√©taient pas √©crits par une personne en parfaite sant√©.  N√©anmoins, les r√©sultats sont impressionnants.  Et peut-√™tre qu'une √©tude plus approfondie de la g√©n√©ration de diff√©rents textes aidera √† mieux comprendre certaines des maladies mentales de vrais patients.  Par ailleurs, comme sugg√©r√© dans les commentaires, une telle maladie mentale dans laquelle une personne parle dans un texte grammaticalement li√© mais compl√®tement d√©nu√© de sens ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">schizophasie</a> ) existe. <br><br><h2>  Conclusion </h2><br>  Les r√©seaux de neurones r√©cr√©atifs sont consid√©r√©s comme tr√®s prometteurs, et c'est en effet un grand pas en avant par rapport aux r√©seaux ¬´ordinaires¬ª comme MLP, qui n'ont pas de m√©moire.  En effet, les capacit√©s des r√©seaux de neurones √† stocker et traiter des structures assez complexes sont impressionnantes.  C'est apr√®s ces tests que j'ai pens√© pour la premi√®re fois qu'Ilon Mask avait probablement raison quand j'ai √©crit que l'IA √† l'avenir pourrait √™tre "le plus grand risque pour l'humanit√©" - m√™me si un simple r√©seau de neurones peut facilement se souvenir et se reproduire mod√®les assez complexes, que peut faire un r√©seau de milliards de composants?  Mais d'un autre c√¥t√©, n'oubliez pas que notre r√©seau de neurones ne peut pas <i>penser</i> , il ne se souvient essentiellement que m√©caniquement de s√©quences de caract√®res, ne comprenant pas leur signification.  C'est un point important - m√™me si vous entra√Ænez un r√©seau neuronal sur un supercalculateur et un √©norme ensemble de donn√©es, au mieux il apprendra √† g√©n√©rer des phrases grammaticalement correctes √† 100%, mais compl√®tement d√©nu√©es de sens. <br><br>  Mais il ne sera pas supprim√© en philosophie, l'article s'adresse toujours plus aux praticiens.  Pour ceux qui veulent exp√©rimenter par eux-m√™mes, le <b>code source</b> de Python 3.7 est sous le spoiler.  Ce code est une compilation de divers projets github, et n'est pas un √©chantillon du meilleur code, mais il semble accomplir sa t√¢che. <br><br>  L'utilisation du programme ne n√©cessite pas de comp√©tences en programmation, il suffit de savoir installer Python.  Exemples de d√©marrage √† partir de la ligne de commande: <br>  - Cr√©ation et formation de mod√®les et g√©n√©ration de texte: <br>  python. \ keras_textgen.py --text = text_habr.txt --epochs = 10 --out_len = 4000 <br>  - G√©n√©ration de texte uniquement sans formation de mod√®le: <br>  python. \ keras_textgen.py --text = text_habr.txt --epochs = 10 --out_len = 4000 --generate <br><br><div class="spoiler">  <b class="spoiler_title">keras_textgen.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># Force CPU os.environ["CUDA_VISIBLE_DEVICES"] = "-1" os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed from keras.callbacks import LambdaCallback from keras.models import Sequential from keras.layers import Dense, Dropout, Embedding, LSTM, TimeDistributed from keras.optimizers import RMSprop from keras.utils.data_utils import get_file import keras from collections import Counter import pickle import numpy as np import random import sys import time import io import re import argparse # Transforms text to vectors of integer numbers representing in text tokens and back. Handles word and character level tokenization. class Vectorizer: def __init__(self, text, word_tokens, pristine_input, pristine_output): self.word_tokens = word_tokens self._pristine_input = pristine_input self._pristine_output = pristine_output tokens = self._tokenize(text) # print('corpus length:', len(tokens)) token_counts = Counter(tokens) # Sort so most common tokens come first in our vocabulary tokens = [x[0] for x in token_counts.most_common()] self._token_indices = {x: i for i, x in enumerate(tokens)} self._indices_token = {i: x for i, x in enumerate(tokens)} self.vocab_size = len(tokens) print('Vocab size:', self.vocab_size) def _tokenize(self, text): if not self._pristine_input: text = text.lower() if self.word_tokens: if self._pristine_input: return text.split() return Vectorizer.word_tokenize(text) return text def _detokenize(self, tokens): if self.word_tokens: if self._pristine_output: return ' '.join(tokens) return Vectorizer.word_detokenize(tokens) return ''.join(tokens) def vectorize(self, text): """Transforms text to a vector of integers""" tokens = self._tokenize(text) indices = [] for token in tokens: if token in self._token_indices: indices.append(self._token_indices[token]) else: print('Ignoring unrecognized token:', token) return np.array(indices, dtype=np.int32) def unvectorize(self, vector): """Transforms a vector of integers back to text""" tokens = [self._indices_token[index] for index in vector] return self._detokenize(tokens) @staticmethod def word_detokenize(tokens): # A heuristic attempt to undo the Penn Treebank tokenization above. Pass the # --pristine-output flag if no attempt at detokenizing is desired. regexes = [ # Newlines (re.compile(r'[ ]?\\n[ ]?'), r'\n'), # Contractions (re.compile(r"\b(can)\s(not)\b"), r'\1\2'), (re.compile(r"\b(d)\s('ye)\b"), r'\1\2'), (re.compile(r"\b(gim)\s(me)\b"), r'\1\2'), (re.compile(r"\b(gon)\s(na)\b"), r'\1\2'), (re.compile(r"\b(got)\s(ta)\b"), r'\1\2'), (re.compile(r"\b(lem)\s(me)\b"), r'\1\2'), (re.compile(r"\b(mor)\s('n)\b"), r'\1\2'), (re.compile(r"\b(wan)\s(na)\b"), r'\1\2'), # Ending quotes (re.compile(r"([^' ]) ('ll|'re|'ve|n't)\b"), r"\1\2"), (re.compile(r"([^' ]) ('s|'m|'d)\b"), r"\1\2"), (re.compile(r'[ ]?‚Äù'), r'"'), # Double dashes (re.compile(r'[ ]?--[ ]?'), r'--'), # Parens and brackets (re.compile(r'([\[\(\{\&lt;]) '), r'\1'), (re.compile(r' ([\]\)\}\&gt;])'), r'\1'), (re.compile(r'([\]\)\}\&gt;]) ([:;,.])'), r'\1\2'), # Punctuation (re.compile(r"([^']) ' "), r"\1' "), (re.compile(r' ([?!\.])'), r'\1'), (re.compile(r'([^\.])\s(\.)([\]\)}&gt;"\']*)\s*$'), r'\1\2\3'), (re.compile(r'([#$]) '), r'\1'), (re.compile(r' ([;%:,])'), r'\1'), # Starting quotes (re.compile(r'(‚Äú)[ ]?'), r'"') ] text = ' '.join(tokens) for regexp, substitution in regexes: text = regexp.sub(substitution, text) return text.strip() @staticmethod def word_tokenize(text): # Basic word tokenizer based on the Penn Treebank tokenization script, but # setup to handle multiple sentences. Newline aware, ie newlines are # replaced with a specific token. You may want to consider using a more robust # tokenizer as a preprocessing step, and using the --pristine-input flag. regexes = [ # Starting quotes (re.compile(r'(\s)"'), r'\1 ‚Äú '), (re.compile(r'([ (\[{&lt;])"'), r'\1 ‚Äú '), # Punctuation (re.compile(r'([:,])([^\d])'), r' \1 \2'), (re.compile(r'([:,])$'), r' \1 '), (re.compile(r'\.\.\.'), r' ... '), (re.compile(r'([;@#$%&amp;])'), r' \1 '), (re.compile(r'([?!\.])'), r' \1 '), (re.compile(r"([^'])' "), r"\1 ' "), # Parens and brackets (re.compile(r'([\]\[\(\)\{\}\&lt;\&gt;])'), r' \1 '), # Double dashes (re.compile(r'--'), r' -- '), # Ending quotes (re.compile(r'"'), r' ‚Äù '), (re.compile(r"([^' ])('s|'m|'d) "), r"\1 \2 "), (re.compile(r"([^' ])('ll|'re|'ve|n't) "), r"\1 \2 "), # Contractions (re.compile(r"\b(can)(not)\b"), r' \1 \2 '), (re.compile(r"\b(d)('ye)\b"), r' \1 \2 '), (re.compile(r"\b(gim)(me)\b"), r' \1 \2 '), (re.compile(r"\b(gon)(na)\b"), r' \1 \2 '), (re.compile(r"\b(got)(ta)\b"), r' \1 \2 '), (re.compile(r"\b(lem)(me)\b"), r' \1 \2 '), (re.compile(r"\b(mor)('n)\b"), r' \1 \2 '), (re.compile(r"\b(wan)(na)\b"), r' \1 \2 '), # Newlines (re.compile(r'\n'), r' \\n ') ] text = " " + text + " " for regexp, substitution in regexes: text = regexp.sub(substitution, text) return text.split() def _create_sequences(vector, seq_length, seq_step): # Take strips of our vector at seq_step intervals up to our seq_length # and cut those strips into seq_length sequences passes = [] for offset in range(0, seq_length, seq_step): pass_samples = vector[offset:] num_pass_samples = pass_samples.size // seq_length pass_samples = np.resize(pass_samples, (num_pass_samples, seq_length)) passes.append(pass_samples) # Stack our sequences together. This will technically leave a few "breaks" # in our sequence chain where we've looped over are entire dataset and # return to the start, but with large datasets this should be neglegable return np.concatenate(passes) def shape_for_stateful_rnn(data, batch_size, seq_length, seq_step): """ Reformat our data vector into input and target sequences to feed into our RNN. Tricky with stateful RNNs. """ # Our target sequences are simply one timestep ahead of our input sequences. # eg with an input vector "wherefore"... # targets: herefore # predicts ^ ^ ^ ^ ^ ^ ^ ^ # inputs: wherefor inputs = data[:-1] targets = data[1:] # We split our long vectors into semi-redundant seq_length sequences inputs = _create_sequences(inputs, seq_length, seq_step) targets = _create_sequences(targets, seq_length, seq_step) # Make sure our sequences line up across batches for stateful RNNs inputs = _batch_sort_for_stateful_rnn(inputs, batch_size) targets = _batch_sort_for_stateful_rnn(targets, batch_size) # Our target data needs an extra axis to work with the sparse categorical # crossentropy loss function targets = targets[:, :, np.newaxis] return inputs, targets def _batch_sort_for_stateful_rnn(sequences, batch_size): # Now the tricky part, we need to reformat our data so the first # sequence in the nth batch picks up exactly where the first sequence # in the (n - 1)th batch left off, as the RNN cell state will not be # reset between batches in the stateful model. num_batches = sequences.shape[0] // batch_size num_samples = num_batches * batch_size reshuffled = np.zeros((num_samples, sequences.shape[1]), dtype=np.int32) for batch_index in range(batch_size): # Take a slice of num_batches consecutive samples slice_start = batch_index * num_batches slice_end = slice_start + num_batches index_slice = sequences[slice_start:slice_end, :] # Spread it across each of our batches in the same index position reshuffled[batch_index::batch_size, :] = index_slice return reshuffled def load_data(data_file, word_tokens, pristine_input, pristine_output, batch_size, seq_length=50, seq_step=25): global vectorizer try: with open(data_file, encoding='utf-8') as input_file: text = input_file.read() except FileNotFoundError: print("No input.txt in data_dir") sys.exit(1) skip_validate = True # try: # with open(os.path.join(data_dir, 'validate.txt'), encoding='utf-8') as validate_file: # text_val = validate_file.read() # skip_validate = False # except FileNotFoundError: # pass # Validation text optional # Find some good default seed string in our source text. # self.seeds = find_random_seeds(text) # Include our validation texts with our vectorizer all_text = text if skip_validate else '\n'.join([text, text_val]) vectorizer = Vectorizer(all_text, word_tokens, pristine_input, pristine_output) data = vectorizer.vectorize(text) x, y = shape_for_stateful_rnn(data, batch_size, seq_length, seq_step) print("Word_tokens:", word_tokens) print('x.shape:', x.shape) print('y.shape:', y.shape) if skip_validate: return x, y, None, None, vectorizer data_val = vectorizer.vectorize(text_val) x_val, y_val = shape_for_stateful_rnn(data_val, batch_size, seq_length, seq_step) print('x_val.shape:', x_val.shape) print('y_val.shape:', y_val.shape) return x, y, x_val, y_val, vectorizer def make_model(batch_size, vocab_size, embedding_size=64, rnn_size=128, num_layers=2): # Conversely if your data is large (more than about 2MB), feel confident to increase rnn_size and train a bigger model (see details of training below). # It will work significantly better. For example with 6MB you can easily go up to rnn_size 300 or even more. model = Sequential() model.add(Embedding(vocab_size, embedding_size, batch_input_shape=(batch_size, None))) for layer in range(num_layers): model.add(LSTM(rnn_size, stateful=True, return_sequences=True)) model.add(Dropout(0.2)) model.add(TimeDistributed(Dense(vocab_size, activation='softmax'))) model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) return model def train(model, x, y, x_val, y_val, batch_size, num_epochs): print('Training...') # print("Shape:", x.shape, y.shape) # print(num_epochs, batch_size, x[0], y[0]) train_start = time.time() validation_data = (x_val, y_val) if (x_val is not None) else None callbacks = None model.fit(x, y, validation_data=validation_data, batch_size=batch_size, shuffle=False, epochs=num_epochs, verbose=1, callbacks=callbacks) # self.update_sample_model_weights() train_end = time.time() print('Training time', train_end - train_start) def sample_preds(preds, temperature=1.0): """ Samples an unnormalized array of probabilities. Use temperature to flatten/amplify the probabilities. """ preds = np.asarray(preds).astype(np.float64) # Add a tiny positive number to avoid invalid log(0) preds += np.finfo(np.float64).tiny preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def generate(model, vectorizer, seed, length=100, diversity=0.5): seed_vector = vectorizer.vectorize(seed) # Feed in seed string print("Seed:", seed, end=' ' if vectorizer.word_tokens else '') model.reset_states() preds = None for char_index in np.nditer(seed_vector): preds = model.predict(np.array([[char_index]]), verbose=0) sampled_indices = [] # np.array([], dtype=np.int32) # Sample the model one token at a time for i in range(length): char_index = 0 if preds is not None: char_index = sample_preds(preds[0][0], diversity) sampled_indices.append(char_index) # = np.append(sampled_indices, char_index) preds = model.predict(np.array([[char_index]]), verbose=0) sample = vectorizer.unvectorize(sampled_indices) return sample if __name__ == "__main__": batch_size = 32 # Batch size for each train num_epochs = 10 # Number of epochs of training out_len = 200 # Length of the output phrase seq_length = 50 # 50 # Determines, how long phrases will be used for training use_words = False # Use words instead of characters (slower speed, bigger vocabulary) data_file = "text_habr.txt" # Source text file seed = "A" # Initial symbol of the text parser = argparse.ArgumentParser() parser.add_argument("-t", "--text", action="store", required=False, dest="text", help="Input text file") parser.add_argument("-e", "--epochs", action="store", required=False, dest="epochs", help="Number of training epochs") parser.add_argument("-p", "--phrase_len", action="store", required=False, dest="phrase_len", help="Phrase analyse length") parser.add_argument("-o", "--out_len", action="store", required=False, dest="out_len", help="Output text length") parser.add_argument("-g", "--generate", action="store_true", required=False, dest='generate', help="Generate output only without training") args = parser.parse_args() if args.text is not None: data_file = args.text if args.epochs is not None: num_epochs = int(args.epochs) if args.phrase_len is not None: seq_length = int(args.phrase_len) if args.out_len is not None: out_len = int(args.out_len) # Load text data pristine_input, pristine_output = False, False x, y, x_val, y_val, vectorizer = load_data(data_file, use_words, pristine_input, pristine_output, batch_size, seq_length) model_file = data_file.lower().replace('.txt', '.h5') if args.generate is False: # Make model model = make_model(batch_size, vectorizer.vocab_size) # Train model train(model, x, y, x_val, y_val, batch_size, num_epochs) # Save model to file model.save(filepath=model_file) model = keras.models.load_model(model_file) predict_model = make_model(1, vectorizer.vocab_size) predict_model.set_weights(model.get_weights()) # Generate phrases res = generate(predict_model, vectorizer, seed=seed, length=out_len) print(res)</span></span></code> </pre><br></div></div><br>  Je pense que cela s'est av√©r√© √™tre un g√©n√©rateur de texte de travail tr√®s <s>g√©nial</s> , <s>qui est utile pour √©crire des articles sur Habr</s> .  Il est particuli√®rement int√©ressant de tester des textes volumineux et un grand nombre d'it√©rations de formation.Si quelqu'un a acc√®s √† des ordinateurs rapides, il serait int√©ressant de voir les r√©sultats. <br><br>  Si quelqu'un veut √©tudier le sujet plus en d√©tail, une bonne description de l'utilisation de RNN avec des exemples d√©taill√©s peut √™tre trouv√©e √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a> . <br><br>  PS: Et enfin, quelques versets;) Il est int√©ressant de noter que ce n'est pas moi qui ai fait la mise en forme du texte ou m√™me l'ajout d'√©toiles, "c'est moi-m√™me".  La prochaine √©tape est int√©ressante pour v√©rifier la possibilit√© de dessiner des images et de composer de la musique.  Je pense que les r√©seaux de neurones sont assez prometteurs ici. <br><br>  <i>xxx</i> <i><br><br></i>  <i>pour certains, √™tre pris dans des biscuits - tout porte bonheur dans une cour √† pain.</i> <i><br></i>  <i>et sous la soir√©e de tamaki</i> <i><br></i>  <i>sous une bougie, prenez une montagne.</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>bient√¥t fils mons √† petachas en tram</i> <i><br></i>  <i>la lumi√®re invisible sent la joie</i> <i><br></i>  <i>c'est pourquoi je frappe ensemble grandit</i> <i><br></i>  <i>vous ne serez pas malade d'une inconnue.</i> <i><br><br></i>  <i>coeur √† cueillir dans l'ogora d√©cal√©,</i> <i><br></i>  <i>ce n'est pas si vieux que les c√©r√©ales mangent,</i> <i><br></i>  <i>Je garde le pont vers le ballon pour voler.</i> <i><br><br></i>  <i>de la m√™me mani√®re Darina √† Doba,</i> <i><br></i>  <i>J'entends dans mon c≈ìur de neige sur ma main.</i> <i><br></i>  <i>notre chant blanc combien doux dumina</i> <i><br></i>  <i>J'ai d√©tourn√© le volot de la b√™te de minerai.</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>v√©t√©rinaire crucifier fretters avec un sort</i> <i><br></i>  <i>et renvers√© sous l'oubli.</i> <i><br></i>  <i>et vous mettez, comme avec les branches de cuba</i> <i><br></i>  <i>brille en elle.</i> <i><br></i>  <i>o plaisir √† zakoto</i> <i><br></i>  <i>avec le vol de lait.</i> <i><br><br></i>  <i>oh tu es une rose, lumi√®re</i> <i><br></i>  <i>lumi√®re des nuages ‚Äã‚Äã√† port√©e de main:</i> <i><br></i>  <i>et roul√© √† l'aube</i> <i><br></i>  <i>comment allez-vous, mon cavalier!</i> <i><br><br></i>  <i>il sert le soir, pas jusqu'aux os,</i> <i><br></i>  <i>la nuit √† Tanya la lumi√®re bleue</i> <i><br></i>  <i>comme une sorte de tristesse.</i> <br><br>  Et les derniers versets dans l'apprentissage par mot.  Ici, la rime a disparu, mais une signification est apparue (?). <br><br>  <i>et toi, de la flamme</i> <i><br></i>  <i>les √©toiles.</i> <i><br></i>  <i>parl√© √† des individus √©loign√©s.</i> <i><br><br></i>  <i>vous inqui√®te rus ,, vous ,, demain.</i> <i><br></i>  <i>"Pluie de colombe,</i> <i><br></i>  <i>et la maison des meurtriers,</i> <i><br></i>  <i>pour la princesse</i> <i><br></i>  <i>son visage.</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>oh berger, agite les chambres</i> <i><br></i>  <i>sur un bosquet au printemps.</i> <i><br><br></i>  <i>Je traverse le coeur de la maison jusqu'√† l'√©tang,</i> <i><br></i>  <i>et les souris guilleret</i> <i><br></i>  <i>Cloches de Nijni Novgorod.</i> <i><br><br></i>  <i>mais n'ayez crainte, le vent du matin,</i> <i><br></i>  <i>avec un chemin, avec un club de fer,</i> <i><br></i>  <i>et pens√© avec l'hu√Ætre</i> <i><br></i>  <i>h√©berg√© sur un √©tang</i> <i><br></i>  <i>en rakit appauvri.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr470035/">https://habr.com/ru/post/fr470035/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr470019/index.html">Fiabilit√© du flash: attendue et inattendue. Partie 1. XIV conf√©rence de l'association USENIX. Technologies de stockage de fichiers</a></li>
<li><a href="../fr470021/index.html">Mod√®le an√©mique et riche dans le contexte des mod√®les GRASP</a></li>
<li><a href="../fr470023/index.html">Nous √©crivons un paiement pour un bot de t√©l√©gramme en python en utilisant la biblioth√®que de telebot partie 3</a></li>
<li><a href="../fr470029/index.html">P√©dagogie extr√™me: ¬´Nous savons¬ª sur le travail avec les enfants sous traitement √† long terme</a></li>
<li><a href="../fr470033/index.html">F # 2: Environnement FSI</a></li>
<li><a href="../fr470037/index.html">F # 3: Formatage du texte</a></li>
<li><a href="../fr470043/index.html">La science derri√®re comment notre cerveau fonctionne le mieux et comment la technologie et notre environnement peuvent aider</a></li>
<li><a href="../fr470045/index.html">Visual Studio pour Mac: principales fonctionnalit√©s du nouvel √©diteur</a></li>
<li><a href="../fr470047/index.html">J'ai regard√© en arri√®re pour voir si elle avait regard√© en arri√®re - 2 ou dans mon propre centre de donn√©es via AWS</a></li>
<li><a href="../fr470049/index.html">Pr√©sentation de la gestion des packages NuGet au niveau de la solution dans Visual Studio pour Mac</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>