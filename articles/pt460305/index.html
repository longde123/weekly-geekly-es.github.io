<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüëß üëç üéì Motor AERODISK: Catastr√≥fico. Parte 2. Metrocluster ü§∂üèø üåµ üéµ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° leitores do Habr! Em um artigo anterior, falamos sobre uma ferramenta simples de toler√¢ncia a desastres nos sistemas de armazenamento do AERODISK ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Motor AERODISK: Catastr√≥fico. Parte 2. Metrocluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/460305/"><p><img src="https://habrastorage.org/webt/ft/el/xz/ftelxzp4fmlci1fn9endwmjffew.jpeg"></p><br><p>  Ol√° leitores do Habr!  Em um artigo anterior, falamos sobre uma ferramenta simples de toler√¢ncia a desastres nos sistemas de armazenamento do AERODISK ENGINE - sobre replica√ß√£o.  Neste artigo, abordaremos um t√≥pico mais complexo e interessante - o cluster metro, ou seja, um meio de prote√ß√£o automatizada contra desastres para dois data centers, que permite que os data centers trabalhem no modo ativo-ativo.  Vamos contar, mostrar, quebrar e consertar. </p><a name="habracut"></a><br><h2 id="kak-obychno-v-nachale-teoriya">  Como sempre, no come√ßo da teoria </h2><br><p>  Um cluster metropolitano √© um cluster espa√ßado em v√°rios sites dentro de uma cidade ou distrito.  A palavra "cluster" indica claramente que o complexo √© automatizado, ou seja, a troca de n√≥s do cluster em caso de falhas ocorre automaticamente. </p><br><p>  √â aqui que reside a principal diferen√ßa entre o cluster metro e a replica√ß√£o comum.  Automa√ß√£o de opera√ß√µes.  Ou seja, no caso de certos incidentes (falha do datacenter, canais interrompidos etc.), o sistema de armazenamento executar√° independentemente as a√ß√µes necess√°rias para manter a disponibilidade dos dados.  Ao usar r√©plicas regulares, essas a√ß√µes s√£o executadas total ou parcialmente manualmente pelo administrador. </p><br><h3 id="dlya-chego-eto-nuzhno">  Para que √© isso? </h3><br><p> O principal objetivo que os clientes buscam usar uma ou outra implementa√ß√£o do cluster metropolitano √© minimizar o RTO (Objetivo de Tempo de Recupera√ß√£o).  Ou seja, minimize o tempo de recupera√ß√£o dos servi√ßos de TI ap√≥s uma falha.  Se voc√™ usar a replica√ß√£o normal, o tempo de recupera√ß√£o sempre ser√° maior que o tempo de recupera√ß√£o com o cluster metropolitano.  Porque  Muito simples  O administrador deve estar no local de trabalho e alternar a replica√ß√£o manualmente, e o cluster metro faz isso automaticamente. </p><br><p>  Se voc√™ n√£o tiver um administrador dedicado de plant√£o que n√£o durma, coma, fume ou fique doente e observe o status de armazenamento 24 horas por dia, n√£o h√° como garantir que o administrador esteja dispon√≠vel para troca manual durante uma falha. </p><br><p>  Consequentemente, a RTO na aus√™ncia de um cluster metropolitano ou <del>  administrador imortal n√≠vel 99 </del>  O servi√ßo de servi√ßo do administrador ser√° igual √† soma do tempo de comuta√ß√£o de todos os sistemas e do per√≠odo m√°ximo ap√≥s o qual √© garantido que o administrador comece a trabalhar com sistemas de armazenamento e sistemas relacionados. </p><br><p>  Assim, chegamos √† conclus√£o √≥bvia de que o cluster metropolitano deve ser usado se o requisito de RTO for minutos, n√£o horas ou dias, ou seja, quando, no caso da pior queda do data center, o departamento de TI deve fornecer √†s empresas tempo para restaurar o acesso √† TI -servi√ßos em minutos ou at√© segundos. </p><br><h3 id="kak-eto-rabotaet">  Como isso funciona? </h3><br><p>  No n√≠vel inferior, o cluster metro usa o mecanismo de replica√ß√£o de dados s√≠ncrona, que descrevemos em um artigo anterior (consulte o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> ).  Como a replica√ß√£o √© s√≠ncrona, os requisitos para ela s√£o apropriados, ou melhor: </p><br><ul><li>  fibra como f√≠sica, 10 gigabit Ethernet (ou superior); </li><li>  a dist√¢ncia entre os data centers n√£o √© superior a 40 quil√¥metros; </li><li>  Atraso do canal √≥ptico entre data centers (entre sistemas de armazenamento) at√© 5 milissegundos (idealmente 2). </li></ul><br><p>  Todos esses requisitos s√£o de natureza consultiva, ou seja, o cluster metropolitano funcionar√° mesmo que esses requisitos n√£o sejam atendidos, mas deve-se entender que as conseq√º√™ncias do n√£o cumprimento desses requisitos s√£o iguais √† desacelera√ß√£o de ambos os sistemas de armazenamento no cluster metropolitano. </p><br><p>  Portanto, uma r√©plica s√≠ncrona √© usada para transferir dados entre sistemas de armazenamento e como as r√©plicas s√£o automaticamente trocadas e, o mais importante, como evitar o c√©rebro dividido?  Para isso, no n√≠vel acima, uma entidade adicional √© usada - o √°rbitro. </p><br><h3 id="kak-rabotaet-arbitr-i-v-chem-ego-zadacha">  Como o √°rbitro trabalha e qual √© a sua tarefa? </h3><br><p>  O √°rbitro √© uma pequena m√°quina virtual, ou um cluster de hardware, que deve ser executado na terceira plataforma (por exemplo, no escrit√≥rio) e fornecer acesso ao armazenamento via ICMP e SSH.  Ap√≥s o lan√ßamento, o √°rbitro deve definir o IP e, em seguida, do lado do armazenamento, indicar seu endere√ßo, al√©m dos endere√ßos dos controladores remotos que participam do cluster do metro.  Depois disso, o √°rbitro est√° pronto para trabalhar. </p><br><p>  O √°rbitro monitora constantemente todos os sistemas de armazenamento no cluster metro e, se um sistema de armazenamento estiver indispon√≠vel, ap√≥s confirmar a inacessibilidade de outro membro do cluster (um dos sistemas de armazenamento "ativos"), toma a decis√£o de iniciar o procedimento para alternar regras e mapeamento de replica√ß√£o. </p><br><p>  Um ponto muito importante.  O √°rbitro deve sempre estar em um site diferente daquele em que o armazenamento est√° localizado, ou seja, no data center-1, onde o armazenamento 1 est√° localizado, nem no data center-2, onde o armazenamento 2 est√° instalado. </p><br><p>  Porque  Porque a √∫nica maneira de um √°rbitro, com a ajuda de um dos sistemas de armazenamento sobreviventes, poder determinar de maneira inequ√≠voca e precisa a queda de qualquer um dos dois sites em que os sistemas de armazenamento est√£o instalados.  Quaisquer outras maneiras de colocar um √°rbitro podem resultar em um c√©rebro dividido. </p><br><h3 id="teper-pogruzimsya-v-detali-raboty-arbitra">  Agora mergulhe nos detalhes do √°rbitro </h3><br><p>  O √°rbitro executa v√°rios servi√ßos que s√£o constantemente interrogados por todos os controladores de armazenamento.  Se o resultado da pesquisa for diferente do anterior (dispon√≠vel / inacess√≠vel), ele ser√° registrado em um pequeno banco de dados, que tamb√©m funciona como √°rbitro. </p><br><p>  <strong>Considere a l√≥gica do √°rbitro em mais detalhes.</strong> </p><br><p>  <u>Etapa 1. Determina√ß√£o da inacessibilidade.</u>  Um sinal de evento sobre a falha do sistema de armazenamento √© a aus√™ncia de ping dos dois controladores do mesmo sistema de armazenamento por 5 segundos. </p><br><p>  <u>Etapa 2. Inicie o procedimento de comuta√ß√£o.</u>  Depois que o √°rbitro entende que um dos sistemas de armazenamento n√£o est√° dispon√≠vel, ele envia uma solicita√ß√£o ao sistema de armazenamento ‚Äúativo‚Äù para garantir que o sistema de armazenamento ‚Äúmorto‚Äù realmente tenha morrido. </p><br><p>  Ap√≥s receber esse comando do √°rbitro, o segundo sistema de armazenamento (ativo) verifica adicionalmente a disponibilidade do primeiro sistema de armazenamento ca√≠do e, se n√£o estiver, envia ao √°rbitro a confirma√ß√£o de suas suposi√ß√µes.  O armazenamento n√£o est√° realmente dispon√≠vel. </p><br><p>  Ap√≥s receber essa confirma√ß√£o, o √°rbitro inicia o procedimento remoto para alternar a replica√ß√£o e aumentar o mapeamento nas r√©plicas que estavam ativas (principais) no armazenamento descartado e envia um comando para o segundo armazenamento para fazer essas r√©plicas de secund√°rias para prim√°rias e aumentar o mapeamento.  Bem, o segundo sistema de armazenamento, respectivamente, executa esses procedimentos, ap√≥s o qual fornece acesso √†s LUNs perdidas por si mesmo. </p><br><p>  Por que preciso de verifica√ß√£o adicional?  Para quorum.  Ou seja, a maioria do n√∫mero √≠mpar total (3) de membros do cluster deve confirmar a queda de um dos n√≥s do cluster.  Somente ent√£o essa decis√£o ser√° exatamente correta.  Isso √© necess√°rio para evitar a troca incorreta e, consequentemente, o c√©rebro dividido. </p><br><p>  A etapa 2 leva aproximadamente de 5 a 10 segundos, portanto, levando em considera√ß√£o o tempo necess√°rio para determinar a inacessibilidade (5 segundos), dentro de 10 a 15 segundos ap√≥s o acidente, os LUNs com armazenamento interrompido estar√£o automaticamente dispon√≠veis para trabalhar com armazenamento ativo. </p><br><p>  √â claro que, para evitar desconectar os hosts, voc√™ tamb√©m deve cuidar da configura√ß√£o correta dos tempos limite nos hosts.  O tempo limite recomendado √© de pelo menos 30 segundos.  Isso n√£o permitir√° que o host se desconecte do sistema de armazenamento durante a transfer√™ncia de carga durante um acidente e ser√° capaz de garantir que n√£o haja interrup√ß√£o da entrada-sa√≠da. </p><br><blockquote>  Apenas um segundo, se tudo estiver bem com o cluster metro, por que voc√™ precisa de replica√ß√£o regular? </blockquote><p>  De fato, nem tudo √© t√£o simples. </p><br><h3 id="rassmotrim-plyusy-i-minusy-metroklastera">  Considere os pr√≥s e os contras do cluster metro </h3><br><p>  Portanto, percebemos que as vantagens √≥bvias do cluster metro em compara√ß√£o com a replica√ß√£o convencional s√£o: </p><br><ul><li>  Automa√ß√£o total, proporcionando tempo m√≠nimo de recupera√ß√£o em caso de desastre; </li><li>  E √© isso :-). </li></ul><br><p>  E agora, aten√ß√£o, contras: </p><br><ul><li>  O custo da decis√£o.  Embora o cluster metro nos sistemas Aerodisk n√£o exija licenciamento adicional (a mesma licen√ßa √© usada para a r√©plica), o custo da solu√ß√£o ainda ser√° maior do que o uso da replica√ß√£o s√≠ncrona.  Ser√° necess√°rio implementar todos os requisitos para r√©plica s√≠ncrona, al√©m dos requisitos para o cluster metropolitano relacionados a comuta√ß√£o adicional e site adicional (consulte o planejamento do cluster metropolitano); </li><li>  A complexidade da decis√£o.  O cluster metropolitano √© muito mais complexo que uma r√©plica regular e requer muito mais aten√ß√£o e m√£o-de-obra no planejamento, configura√ß√£o e documenta√ß√£o. </li></ul><br><p>  No final  <strong>O cluster Metro √©, obviamente, uma solu√ß√£o muito tecnol√≥gica e boa quando voc√™ realmente precisa fornecer o RTO em segundos ou minutos.</strong>  Mas se n√£o houver essa tarefa, e o RTO em horas for bom para os neg√≥cios, n√£o h√° sentido em atirar pardais no canh√£o.  A replica√ß√£o habitual dos camponeses √© suficiente, porque o cluster metropolitano incorrer√° em custos adicionais e complicar√° a infraestrutura de TI. </p><br><h2 id="planirovanie-metroklastera">  Planejamento do Metro Cluster </h2><br><p>  Esta se√ß√£o n√£o afirma ser um guia abrangente para o design do cluster metropolitano, mas mostra apenas as principais dire√ß√µes que devem ser elaboradas se voc√™ decidir construir esse sistema.  Portanto, com a implementa√ß√£o real do cluster do metro, envolva o fabricante dos sistemas de armazenamento (ou seja, n√≥s) e outros sistemas relacionados para consulta. </p><br><h3 id="ploschadki">  Plataformas </h3><br><p>  Conforme indicado acima, s√£o necess√°rios no m√≠nimo tr√™s locais para um cluster de metr√¥.  Dois data centers, onde os sistemas de armazenamento e sistemas relacionados funcionar√£o, e uma terceira plataforma, onde o √°rbitro funcionar√°. </p><br><p>  A dist√¢ncia recomendada entre os data centers n√£o √© superior a 40 quil√¥metros.  √â prov√°vel que dist√¢ncias maiores causem atrasos adicionais, que no caso de um cluster metropolitano s√£o altamente indesej√°veis.  Lembre-se, os atrasos devem ser de at√© 5 milissegundos, embora seja desej√°vel atender a 2. </p><br><p>  Tamb√©m √© recomend√°vel verificar atrasos durante o processo de planejamento.  Qualquer fornecedor mais ou menos adulto que forne√ßa fibra entre os data centers, uma verifica√ß√£o de qualidade pode ser organizada rapidamente. </p><br><p>  Quanto aos atrasos antes do √°rbitro (ou seja, entre a terceira plataforma e as duas primeiras), o limite de atraso recomendado √© de at√© 200 milissegundos, ou seja, √© adequada uma conex√£o VPN corporativa corporativa pela Internet. </p><br><h3 id="kommutaciya-i-set">  Comuta√ß√£o e Rede </h3><br><p>  Ao contr√°rio de um esquema de replica√ß√£o, em que √© suficiente interconectar sistemas de armazenamento de sites diferentes, um esquema com um cluster de metr√¥ exige a conex√£o de hosts com os dois sistemas de armazenamento em sites diferentes.  Para esclarecer qual √© a diferen√ßa, os dois esquemas est√£o listados abaixo. </p><br><p><img src="https://habrastorage.org/webt/qq/tg/x-/qqtgx-ze1zjj9fhbb7drzz6zabw.png"></p><br><p><img src="https://habrastorage.org/webt/l3/vs/es/l3vsesenlgm7lalbws0gdqhysuy.png"></p><br><p>  Como voc√™ pode ver no diagrama, os hosts no site 1 est√£o olhando para o SHD1 e o SHD 2. Al√©m disso, pelo contr√°rio, os hosts da plataforma 2 est√£o olhando para o SHD 2 e o SHD1.  Ou seja, cada host v√™ os dois sistemas de armazenamento.  Este √© um pr√©-requisito para a opera√ß√£o do cluster de metr√¥. </p><br><p>  Obviamente, n√£o h√° necessidade de puxar cada host com um cabo √≥ptico para um data center diferente; nenhuma porta e cadar√ßo ser√° suficiente.  Todas essas conex√µes devem ser feitas atrav√©s dos comutadores Ethernet 10G + ou FibreChannel 8G + (FC apenas para conectar hosts e armazenamento para E / S, o canal de replica√ß√£o est√° atualmente dispon√≠vel apenas por IP (Ethernet 10G +). </p><br><p>  Agora, algumas palavras sobre a topologia de rede.  Um ponto importante √© a configura√ß√£o correta das sub-redes.  Voc√™ deve identificar imediatamente v√°rias sub-redes para os seguintes tipos de tr√°fego: </p><br><ul><li>  A sub-rede para replica√ß√£o na qual os dados entre os sistemas de armazenamento ser√£o sincronizados.  Pode haver v√°rios, neste caso, n√£o importa, tudo depende da topologia de rede atual (j√° implementada).  Se houver dois deles, obviamente, o roteamento entre eles deve ser configurado; </li><li>  Sub-redes de armazenamento atrav√©s das quais os hosts acessar√£o recursos de armazenamento (se for iSCSI).  Deve haver uma dessas sub-redes em cada data center; </li><li>  Sub-redes de controle, ou seja, tr√™s sub-redes rote√°veis ‚Äã‚Äãem tr√™s sites a partir dos quais o gerenciamento de armazenamento √© realizado, e tamb√©m h√° um √°rbitro. </li></ul><br><p>  N√£o consideramos sub-redes para acessar recursos do host aqui, pois eles s√£o altamente dependentes de tarefas. </p><br><p>  Separar tr√°fego diferente em sub-redes diferentes √© extremamente importante (√© especialmente importante separar a r√©plica da E / S), porque se voc√™ misturar todo o tr√°fego em uma sub-rede ‚Äúespessa‚Äù, ser√° imposs√≠vel controlar esse tr√°fego e, nas condi√ß√µes de dois data centers, ainda poder√° causar problemas diferentes. op√ß√µes de colis√£o de rede.  N√£o abordaremos muito esse assunto na estrutura deste artigo, pois voc√™ pode ler sobre o planejamento de uma rede estendida entre data centers com os recursos dos fabricantes de equipamentos de rede, onde √© descrito em detalhes. </p><br><h3 id="konfiguraciya-arbitra">  Configura√ß√£o do √Årbitro </h3><br><p>  O √°rbitro deve fornecer acesso a todas as interfaces de gerenciamento de armazenamento por meio dos protocolos ICMP e SSH.  Voc√™ tamb√©m deve considerar a toler√¢ncia a falhas do √°rbitro.  H√° uma nuance. </p><br><p>  A toler√¢ncia a falhas do √°rbitro √© muito desej√°vel, mas opcional.  E o que acontece se o √°rbitro falhar na hora errada? </p><br><ul><li>  A opera√ß√£o do cluster metro no modo normal n√£o ser√° alterada, porque  O arbtir n√£o afeta a opera√ß√£o do cluster de metr√¥ no modo normal de forma alguma (sua tarefa √© alternar oportunamente a carga entre os datacenters) </li><li>  Al√©m disso, se o √°rbitro, por um motivo ou outro, cair e "acordar" o acidente no data center, nenhuma troca ocorrer√°, porque n√£o haver√° ningu√©m para dar os comandos necess√°rios para trocar e organizar um quorum.  Nesse caso, o cluster do metr√¥ se transformar√° em um esquema de replica√ß√£o regular, que precisar√° ser alternado manualmente durante um desastre, o que afetar√° a RTO. </li></ul><br><p>  O que se segue disso?  Se voc√™ realmente precisa garantir um RTO m√≠nimo, precisa garantir a toler√¢ncia a falhas do √°rbitro.  Existem duas op√ß√µes para isso: </p><br><ul><li>  Execute uma m√°quina virtual com um √°rbitro em um hypervisor de failover, pois todos os hipervisores adultos suportam failover; </li><li>  Se estiver no terceiro site (em um escrit√≥rio condicional) <del>  pregui√ßa de colocar um cluster normal </del>  Como n√£o existe um cluster de hypervizor, fornecemos uma vers√£o de hardware do √°rbitro, feita em uma caixa 2U, na qual dois servidores x-86 comuns funcionam e que podem sobreviver a uma falha local. </li></ul><br><p>  √â altamente recomend√°vel que o √°rbitro seja tolerante a falhas, apesar de o cluster metro n√£o precisar dele no modo normal.  Mas tanto a teoria quanto a pr√°tica mostram que, se voc√™ construir uma infra-estrutura √† prova de desastre verdadeiramente confi√°vel, √© melhor jogar com seguran√ßa.  √â melhor proteger voc√™ e sua empresa da "lei da maldade", isto √©, da falha do √°rbitro e de um dos sites onde o sistema de armazenamento est√° localizado. </p><br><h3 id="arhitektura-resheniya">  Arquitetura da solu√ß√£o </h3><br><p>  Considerando os requisitos acima, obtemos a seguinte arquitetura geral da solu√ß√£o. </p><br><p><img src="https://habrastorage.org/webt/di/wt/oq/diwtoqu2jdf7ik-nsigr4ypx37s.png"></p><br><p>  Os LUNs devem ser distribu√≠dos igualmente em dois sites para evitar congestionamentos pesados.  Ao mesmo tempo, ao dimensionar nos dois datacenters, √© necess√°rio estabelecer n√£o apenas um volume duplo (que √© necess√°rio para armazenar dados simultaneamente em dois sistemas de armazenamento), mas tamb√©m um desempenho duplo em IOPS e MB / s, para evitar a degrada√ß√£o de aplicativos no caso de falha de um dos datacenters - ov. </p><br><p>  Separadamente, observamos que, com uma abordagem adequada ao dimensionamento (ou seja, desde que tenhamos fornecido os limites superiores adequados para IOPS e MB / s, bem como os recursos necess√°rios de CPU e RAM), se um dos sistemas de armazenamento falhar no cluster do metro, n√£o haver√° redu√ß√£o grave no desempenho trabalho tempor√°rio em um sistema de armazenamento. </p><br><p>  Isso ocorre pelo fato de que, nas condi√ß√µes de dois sites simultaneamente, o trabalho de replica√ß√£o s√≠ncrona ‚Äúconsome‚Äù metade do desempenho da grava√ß√£o, pois cada transa√ß√£o precisa ser gravada em dois sistemas de armazenamento (semelhante ao RAID-1/10).  Portanto, se um dos sistemas de armazenamento falhar, o efeito da replica√ß√£o temporariamente (at√© o sistema com falha no armazenamento) desaparecer√° e obteremos um aumento duplo no desempenho da grava√ß√£o.  Depois que os LUNs do sistema de armazenamento com falha foram reiniciados em um sistema de armazenamento em funcionamento, esse aumento duplo desaparece devido √† carga dos LUNs de outro sistema de armazenamento e retornamos ao mesmo n√≠vel de desempenho que tivemos antes da "queda", mas apenas dentro da estrutura de uma plataforma. </p><br><p>  Com a ajuda do dimensionamento competente, √© poss√≠vel fornecer condi√ß√µes sob as quais os usu√°rios n√£o sintam a falha de todo um sistema de armazenamento.  Mas, novamente, isso requer um dimensionamento muito cuidadoso, para o qual, ali√°s, voc√™ pode entrar em contato conosco gratuitamente :-). </p><br><h2 id="nastroyka-metroklastera">  Configura√ß√£o do Metro Cluster </h2><br><p>  A configura√ß√£o de um cluster intermedi√°rio √© muito semelhante √† configura√ß√£o da replica√ß√£o regular, descrita em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo anterior</a> .  Portanto, focamos apenas nas diferen√ßas.  Montamos um estande no laborat√≥rio com base na arquitetura acima, apenas na vers√£o m√≠nima: dois sistemas de armazenamento conectados pela Ethernet 10G entre si, dois comutadores 10G e um host que examina os comutadores nas duas portas de armazenamento com portas 10G.  O √°rbitro √© executado em uma m√°quina virtual. </p><br><p><img src="https://habrastorage.org/webt/gy/-v/ci/gy-vci08eyyimjl6lwa2mciot8k.png"></p><br><p>  Ao configurar IPs virtuais (VIPs) para uma r√©plica, selecione o tipo VIP para o cluster do metro. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ym/aw/89/ymaw89gofhplngagm5nvveg4oe0.png"></a> </p><br><p>  Criamos dois links de replica√ß√£o para dois LUNs e os distribu√≠mos por dois sistemas de armazenamento: LUN TEST Primary no SHD1 (link METRO), LUN TEST2 Primary no SHD2 (link METRO2). </p><br><p><img src="https://habrastorage.org/webt/rz/7w/6e/rz7w6esdq4bued37xobjg1obcha.jpeg"></p><br><p>  Para eles, estabelecemos dois destinos id√™nticos (no nosso caso, iSCSI, mas o FC tamb√©m √© suportado, a l√≥gica da configura√ß√£o √© a mesma). </p><br><p>  SHD1: </p><br><p><img src="https://habrastorage.org/webt/d8/r-/dw/d8r-dw3gglptcowecdjabpewdve.jpeg"></p><br><p>  SHD2: </p><br><p><img src="https://habrastorage.org/webt/lf/ys/_6/lfys_6aohva42tp89ddqcjhakes.jpeg"></p><br><p>  Para conex√µes de replica√ß√£o, eles fizeram mapeamentos em cada sistema de armazenamento. </p><br><p>  SHD1: </p><br><p><img src="https://habrastorage.org/webt/fw/qe/5q/fwqe5qqhcir_4rlbj3usmea1hgg.jpeg"></p><br><p>  SHD2: </p><br><p><img src="https://habrastorage.org/webt/rn/68/ap/rn68apxwkixmyh3dpv70vo-9pk8.jpeg"></p><br><p>  Multipath configurado e apresentado ao host. </p><br><p><img src="https://habrastorage.org/webt/gx/ii/wj/gxiiwjndb_fzeywc5hrxdfbqnrk.jpeg"></p><br><p><img src="https://habrastorage.org/webt/c6/df/5j/c6df5jjxnggdqk8wnjfbebb1rty.jpeg"></p><br><h3 id="nastraivaem-arbitra">  Configure o √°rbitro </h3><br><p>  Voc√™ n√£o precisa fazer nada de especial com o √°rbitro, apenas habilit√°-lo na terceira plataforma, definir um IP para ele e configurar o acesso a ele via ICMP e SSH.  A pr√≥pria configura√ß√£o √© realizada a partir dos pr√≥prios sistemas de armazenamento.  Nesse caso, basta configurar o √°rbitro uma vez em qualquer um dos controladores de armazenamento no cluster metro, essas configura√ß√µes ser√£o distribu√≠das a todos os controladores automaticamente. </p><br><p>  Na se√ß√£o Replica√ß√£o Remota &gt;&gt; Metrocluster (em qualquer controlador) &gt;&gt; bot√£o Configurar. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/sy/h0/x-/syh0x-gywrz2yu8w9mk0us5mwcg.jpeg"></a> </p><br><p>  Introduzimos o IP do √°rbitro, bem como as interfaces de controle dos dois controladores do sistema de armazenamento remoto. </p><br><p><img src="https://habrastorage.org/webt/rs/u2/dt/rsu2dt3tt18k1it_qvmdsdnxldy.jpeg"></p><br><p>  Depois disso, voc√™ precisa ativar todos os servi√ßos (o bot√£o "Reiniciar tudo").  No caso de uma reconfigura√ß√£o no futuro, os servi√ßos devem ser reiniciados para que as configura√ß√µes tenham efeito. </p><br><p><img src="https://habrastorage.org/webt/kj/sb/7u/kjsb7u9rrzlk6cttl7amr_ebxma.jpeg"></p><br><p>  Verifique se todos os servi√ßos est√£o em execu√ß√£o. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/oh/mw/xl/ohmwxl5jry0dufgfamey0ddxris.jpeg"></a> </p><br><p>  <strong>Isso conclui a configura√ß√£o do cluster do metro.</strong> </p><br><h2 id="krash-test">  Teste de colis√£o </h2><br><p>  O teste de colis√£o no nosso caso ser√° bastante simples e r√°pido, pois a funcionalidade de replica√ß√£o (comuta√ß√£o, consist√™ncia etc.) foi considerada em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo anterior</a> .  Portanto, para testar a confiabilidade de um cluster de metr√¥, basta verificar a automa√ß√£o da detec√ß√£o de acidentes, comuta√ß√£o e a aus√™ncia de perdas de registro (paradas de E / S). </p><br><p>  Para fazer isso, simulamos a falha completa de um dos sistemas de armazenamento, desligando fisicamente os dois controladores, iniciando a c√≥pia preliminar de um arquivo grande para o LUN, que deve ser ativado no outro sistema de armazenamento. </p><br><p><img src="https://habrastorage.org/webt/f-/lb/fo/f-lbfonzy8n4iawaihumetqtkmo.png"></p><br><p>  Desative um armazenamento.  No segundo sistema de armazenamento, vemos alertas e mensagens nos logs de que a conex√£o com o sistema vizinho desapareceu.  Se voc√™ configurou alertas para monitoramento SMTP ou SNMP, o administrador receber√° alertas apropriados. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/jo/xv/aejoxv6piwk4fvl-nc1leklbxce.jpeg"></a> </p><br><p>  Exatamente 10 segundos depois (visto nas duas capturas de tela), o link de replica√ß√£o do METRO (aquele que era Principal no sistema de armazenamento com falha) tornou-se automaticamente Principal no sistema de armazenamento em execu√ß√£o.  Usando o mapeamento existente, o LUN TEST permaneceu dispon√≠vel para o host, a grava√ß√£o caiu um pouco (dentro dos 10% prometidos), mas n√£o foi interrompida. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/v_/gj/d9/v_gjd9sgoi5hwrgjdkwxwtdomeu.jpeg"></a> </p><br><p><img src="https://habrastorage.org/webt/ih/lq/rg/ihlqrgv7dq-4fv6dlnhair6nib4.jpeg"></p><br><p>  <strong>Teste conclu√≠do com sucesso.</strong> </p><br><h2 id="podvodim-itog">  Resumir </h2><br><p>  A implementa√ß√£o atual do metrocluster nos sistemas de armazenamento da s√©rie N do AERODISK Engine permite solucionar completamente os problemas onde √© necess√°rio eliminar ou minimizar o tempo de inatividade dos servi√ßos de TI e garantir sua opera√ß√£o 24/7/365 com custos m√≠nimos de m√£o-de-obra. </p><br><p>  , ,    ,      ‚Ä¶       ,      ,    .      ,         ,        ,         . </p><br><p> ,   . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt460305/">https://habr.com/ru/post/pt460305/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt460287/index.html">Visualiza√ß√£o de not√≠cias de runas</a></li>
<li><a href="../pt460291/index.html">Problemas no processamento em lote de solicita√ß√µes e suas solu√ß√µes (parte 1)</a></li>
<li><a href="../pt460295/index.html">O que significa inseguro em Rust?</a></li>
<li><a href="../pt460297/index.html">WeakRef - proposta de inclus√£o no padr√£o ECMAScript</a></li>
<li><a href="../pt460301/index.html">Nova gera√ß√£o de l√¢mpadas LED de alta pot√™ncia</a></li>
<li><a href="../pt460307/index.html">Experi√™ncia de modelagem da equipe Computer Vision Mail.ru</a></li>
<li><a href="../pt460311/index.html">Hora de uma nova teoria do dinheiro</a></li>
<li><a href="../pt460313/index.html">Diferentes hits t√™m algo em comum?</a></li>
<li><a href="../pt460319/index.html">Hunt for Space Inspectors</a></li>
<li><a href="../pt460321/index.html">Galeria dos melhores notebooks de ML e Data Science</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>