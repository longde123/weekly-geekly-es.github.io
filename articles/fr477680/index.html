<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶í ü§∏üèº üóúÔ∏è Nos mains ne sont pas pour l'ennui: restaurer le cluster Rook dans les K8 üòö üôçüèª üåÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous avons d√©j√† expliqu√© comment / pourquoi nous aimons Rook: dans une large mesure, il simplifie l'utilisation du stockage dans les clusters Kubernet...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nos mains ne sont pas pour l'ennui: restaurer le cluster Rook dans les K8</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/477680/"><img src="https://habrastorage.org/webt/x_/rb/jm/x_rbjmnxk6egjidxf_fvk1otkqe.png"><br><br>  Nous avons <a href="https://habr.com/ru/company/flant/blog/451818/">d√©j√† expliqu√©</a> comment / pourquoi nous aimons Rook: dans une large mesure, il simplifie l'utilisation du stockage dans les clusters Kubernetes.  Cependant, avec cette simplicit√©, certaines difficult√©s surviennent.  Nous esp√©rons que le nouveau mat√©riel aidera √† mieux comprendre ces difficult√©s avant m√™me qu'elles ne se manifestent. <br><br>  Et pour le lire c'√©tait plus int√©ressant, on commence par les <i>cons√©quences d'un</i> probl√®me hypoth√©tique dans le cluster. <a name="habracut"></a><br><br><h2>  "Tout est parti!" </h2><br>  Imaginez que vous ayez une fois configur√© et lanc√© Rook dans votre cluster K8s, il √©tait satisfait de son travail, mais √† un moment ¬´merveilleux¬ª, les √©v√©nements suivants se produisent: <br><br><ul><li>  Les nouveaux pods ne peuvent pas monter de RBD Ceph. </li><li>  Les commandes comme <code>lsblk</code> et <code>df</code> ne fonctionnent pas sur les h√¥tes Kubernetes.  Cela signifie automatiquement ¬´quelque chose ne va pas¬ª avec les images RBD mont√©es sur le n≈ìud.  Je ne peux pas les lire, ce qui indique l'inaccessibilit√© des moniteurs ... </li><li>  Oui, il n'y a aucun moniteur op√©rationnel dans le cluster.  De plus - il n'y a m√™me pas de pods avec OSD, ni de pods de MGR. </li></ul><br>  Quand a √©t√© lanc√© le pod <code>rook-ceph-operator</code> ?  Il n'y a pas si longtemps qu'il √©tait d√©ploy√©.  Pourquoi?  L'op√©rateur tour a d√©cid√© de cr√©er un nouveau cluster ... Comment pouvons-nous maintenant restaurer le cluster et les donn√©es qu'il contient? <br><br>  Pour commencer, allons <s>plus</s> loin, apr√®s avoir men√© une enqu√™te approfondie sur les ¬´internes¬ª de Rook et une restauration √©tape par √©tape de ses composants.  Bien s√ªr, il existe une m√©thode correcte <s>plus courte</s> : utiliser les sauvegardes.  Comme vous le savez, les administrateurs sont divis√©s en deux types: ceux qui ne font pas de sauvegardes et ceux qui les font d√©j√† ... Mais plus √† ce sujet apr√®s l'enqu√™te. <br><br><h2>  Un peu de pratique ou un long chemin </h2><br><h3>  Jetez un ≈ìil et restaurez les moniteurs </h3><br>  Alors, regardons la liste des ConfigMaps: il y a <code>rook-ceph-config</code> et <code>rook-config-override</code> n√©cessaires pour la sauvegarde.  Ils apparaissent lors du d√©ploiement r√©ussi du cluster. <br><br>  <i><b>NB</b> : Dans les nouvelles versions, apr√®s l'adoption de <a href="https://github.com/rook/rook/pull/3573">ce PR</a> , ConfigMaps a cess√© d'√™tre un indicateur de la r√©ussite d'un d√©ploiement de cluster.</i> <br><br>  Pour effectuer d'autres actions, nous avons besoin d'un red√©marrage mat√©riel de tous les serveurs qui ont mont√© des images RBD ( <code>ls /dev/rbd*</code> ).  Cela doit √™tre fait via sysrq (ou "√† pied" jusqu'au centre de donn√©es).  Cette exigence est caus√©e par la t√¢che de d√©connecter les RBD mont√©s, pour laquelle un red√©marrage r√©gulier ne fonctionnera pas (il tentera sans succ√®s de les d√©monter normalement). <br><br>  Le th√©√¢tre commence par un cintre et le cluster Ceph commence par des moniteurs.  Regardons-les. <br><br>  Rook monte les entit√©s suivantes dans le module moniteur: <br><br><pre> <code class="plaintext hljs">Volumes: rook-ceph-config: Type: ConfigMap (a volume populated by a ConfigMap) Name: rook-ceph-config rook-ceph-mons-keyring: Type: Secret (a volume populated by a Secret) SecretName: rook-ceph-mons-keyring rook-ceph-log: Type: HostPath (bare host directory volume) Path: /var/lib/rook/kube-rook/log ceph-daemon-data: Type: HostPath (bare host directory volume) Path: /var/lib/rook/mon-a/data Mounts: /etc/ceph from rook-ceph-config (ro) /etc/ceph/keyring-store/ from rook-ceph-mons-keyring (ro) /var/lib/ceph/mon/ceph-a from ceph-daemon-data (rw) /var/log/ceph from rook-ceph-log (rw)</code> </pre> <br>  Voyons quel est le secret de <code>rook-ceph-mons-keyring</code> : <br><br><pre> <code class="plaintext hljs">kind: Secret data: keyring: LongBase64EncodedString=</code> </pre> <br>  Nous d√©codons et obtenons le trousseau de cl√©s habituel avec des droits pour l'administrateur et les moniteurs: <br><br><pre> <code class="plaintext hljs">[mon.] key = AQAhT19dlUz0LhBBINv5M5G4YyBswyU43RsLxA== caps mon = "allow *" [client.admin] key = AQAhT19d9MMEMRGG+wxIwDqWO1aZiZGcGlSMKp== caps mds = "allow *" caps mon = "allow *" caps osd = "allow *" caps mgr = "allow *"</code> </pre> <br>  N'oubliez pas.  Regardez maintenant le trousseau de cl√©s dans le trousseau secret <code>rook-ceph-admin-keyring</code> : <br><br><pre> <code class="plaintext hljs">kind: Secret data: keyring: anotherBase64EncodedString=</code> </pre> <br>  Qu'y a-t-il dedans? <br><br><pre> <code class="plaintext hljs">[client.admin] key = AQAhT19d9MMEMRGG+wxIwDqWO1aZiZGcGlSMKp== caps mds = "allow *" caps mon = "allow *" caps osd = "allow *" caps mgr = "allow *"</code> </pre> <br>  Le m√™me.  Voyons plus ... Voici, par exemple, le secret de <code>rook-ceph-mgr-a-keyring</code> : <br><br><pre> <code class="plaintext hljs">[mgr.a] key = AQBZR19dbVeaIhBBXFYyxGyusGf8x1bNQunuew== caps mon = "allow *" caps mds = "allow *" caps osd = "allow *"</code> </pre> <br>  En fin de compte, nous trouvons quelques secrets de plus dans ConfigMap <code>rook-ceph-mon</code> : <br><br><pre> <code class="plaintext hljs">kind: Secret data: admin-secret: AQAhT19d9MMEMRGG+wxIwDqWO1aZiZGcGlSMKp== cluster-name: a3ViZS1yb29r fsid: ZmZiYjliZDMtODRkOS00ZDk1LTczNTItYWY4MzZhOGJkNDJhCg== mon-secret: AQAhT19dlUz0LhBBINv5M5G4YyBswyU43RsLxA==</code> </pre> <br>  Et voici la liste initiale avec porte-cl√©s, d'o√π viennent tous les secrets d√©crits ci-dessus. <br><br>  Comme vous le savez (voir <code>dataDirHostPath</code> dans la <a href="https://rook.github.io/docs/rook/master/ceph-cluster-crd.html">documentation</a> ), Rook stocke ces donn√©es √† deux endroits.  Par cons√©quent, allons aux n≈ìuds pour examiner le trousseau de cl√©s se trouvant dans les r√©pertoires mont√©s dans des pods avec des moniteurs et OSD.  Pour ce faire, recherchez les n≈ìuds <code>/var/lib/rook/mon-a/data/keyring</code> et voyez: <br><br><pre> <code class="plaintext hljs"># cat /var/lib/rook/mon-a/data/keyring [mon.] key = AXAbS19d8NNUXOBB+XyYwXqXI1asIzGcGlzMGg== caps mon = "allow *"</code> </pre> <br>  <b>Soudain, le</b> secret s'est av√©r√© diff√©rent - pas comme dans ConfigMap. <br><br>  Et le trousseau d'admin?  Nous l'avons √©galement: <br><br><pre> <code class="plaintext hljs"># cat /var/lib/rook/kube-rook/client.admin.keyring [client.admin] key = AXAbR19d8GGSMUBN+FyYwEqGI1aZizGcJlHMLgx= caps mds = "allow *" caps mon = "allow *" caps osd = "allow *" caps mgr = "allow *"</code> </pre> <br>  Voici le probl√®me.  Il y a eu un √©chec: le cluster a √©t√© recr√©√© ... mais en r√©alit√© non. <br><br>  Il devient clair que le trousseau de cl√©s nouvellement g√©n√©r√© est stock√© dans des secrets, et ils <b>ne</b> sont <b>pas</b> de notre ancien cluster.  Par cons√©quent: <br><br><ul><li>  nous prenons le trousseau de cl√©s du moniteur √† partir du fichier <code>/var/lib/rook/mon-a/data/keyring</code> (ou de la sauvegarde); </li><li>  changer le trousseau de cl√©s dans le trousseau de cl√©s secret <code>rook-ceph-mons-keyring</code> ; </li><li>  enregistrer le trousseau de cl√©s de l'administrateur et du moniteur dans ConfigMap'e <code>rook-ceph-mon</code> ; </li><li>  supprimer les contr√¥leurs de pod avec des moniteurs. </li></ul><br>  Le miracle ne prendra pas longtemps: des moniteurs appara√Ætront et d√©marreront.  Hourra, un d√©but! <br><br><h3>  Restaurer l'OSD </h3><br>  Nous allons dans l' <code>rook-operator</code> pod: appeler <code>ceph mon dump</code> montre que tous les moniteurs sont en place, et <code>ceph -s</code> qu'ils sont dans un quorum.  Cependant, si vous regardez l'arborescence OSD (arborescence <code>ceph osd tree</code> ), vous verrez quelque chose d'√©trange: l'OSD a commenc√© √† appara√Ætre, mais ils sont vides.  Il s'av√®re qu'ils doivent √©galement √™tre en quelque sorte restaur√©s.  Mais comment? <br><br>  Pendant ce temps, <code>rook-ceph-config</code> et <code>rook-config-override</code> , ainsi que de nombreux autres ConfigMap avec des noms comme <code>rook-ceph-osd-$nodename-config</code> , sont apparus dans ConfigMap si n√©cessaire.  Regardons-les: <br><br><pre> <code class="plaintext hljs">kind: ConfigMap data: osd-dirs: '{"/mnt/osd1":16,"/mnt/osd2":18}'</code> </pre> <br>  Tout va mal, tout est m√©lang√©! <br><br>  Redimensionnez le module op√©rateur √† z√©ro, supprimez les modules de d√©ploiement g√©n√©r√©s de l'OSD et corrigez ces cartes de configuration.  Mais o√π obtenir la <b>bonne</b> carte OSD par n≈ìuds? <br><br><ul><li>  Essayons de fouiller √† nouveau dans les r√©pertoires <code>/mnt/osd[1-2]</code> sur les n≈ìuds - dans l'espoir que nous pourrons y attraper quelque chose. </li><li>  Il existe 2 sous-r√©pertoires dans le r√©pertoire <code>/mnt/osd1</code> : <code>osd0</code> et <code>osd16</code> .  Le dernier est juste cet ID qui est sp√©cifi√© dans ConfigMap (16)? </li><li>  V√©rifions <code>osd0</code> taille et voyons que <code>osd0</code> beaucoup plus grand que <code>osd16</code> . </li></ul><br>  Nous concluons que <code>osd0</code> est l'OSD requis, qui a √©t√© sp√©cifi√© comme <code>/mnt/osd1</code> dans ConfigMap (car nous utilisons <a href="https://github.com/rook/rook/issues/3379">osd bas√© sur un r√©pertoire</a> .) <br><br>  √âtape par √©tape, nous v√©rifions tous les n≈ìuds et modifions ConfigMap.  Apr√®s toutes les instructions, vous pouvez ex√©cuter le pod de l'op√©rateur Rook et lire ses journaux.  Et tout est merveilleux en eux: <br><br><ul><li>  Je suis un op√©rateur de cluster; </li><li>  J'ai trouv√© des disques sur des n≈ìuds; </li><li>  J'ai trouv√© des moniteurs; </li><li>  les moniteurs sont devenus amis, c'est-√†-dire  form√© un quorum; </li><li>  ex√©cution de d√©ploiements OSD ... </li></ul><br>  Revenons au pod de l'op√©rateur Rook et v√©rifions la vivacit√© du cluster ... oui, nous avons fait une petite erreur avec les conclusions sur les noms OSD sur certains n≈ìuds!  Peu importe: ils ont √† nouveau corrig√© les ConfigMaps, supprim√© les r√©pertoires suppl√©mentaires des nouveaux OSD et sont arriv√©s √† l'√©tat tant attendu de <code>HEALTH_OK</code> ! <br><br>  V√©rifiez les images dans la piscine: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># rbd ls -p kube pvc-9cfa2a98-b878-437e-8d57-acb26c7118fb pvc-9fcc4308-0343-434c-a65f-9fd181ab103e pvc-a6466fea-bded-4ac7-8935-7c347cff0d43 pvc-b284d098-f0fc-420c-8ef1-7d60e330af67 pvc-b6d02124-143d-4ce3-810f-3326cfa180ae pvc-c0800871-0749-40ab-8545-b900b83eeee9 pvc-c274dbe9-1566-4a33-bada-aabeb4c76c32 ‚Ä¶</span></span></code> </pre> <br>  Tout est en place - le cluster est enregistr√©! <br><br><h2>  Je suis <s>paresseux √†</s> faire des sauvegardes ou la voie rapide </h2><br>  Si des sauvegardes ont √©t√© effectu√©es pour Rook, la proc√©dure de r√©cup√©ration devient beaucoup plus simple et se r√©sume √† ce qui suit: <br><br><ol><li>  √âvoluer vers un d√©ploiement nul de l'op√©rateur Rook; </li><li>  Nous supprimons tous les d√©ploiements √† l'exception de l'op√©rateur Rook; </li><li>  Nous restaurons tous les secrets et ConfigMaps √† partir de la sauvegarde; </li><li>  Restaurez le contenu des <code>/var/lib/rook/mon-*</code> sur les n≈ìuds; </li><li>  Restaurer (si soudainement perdu) CRD <code>CephCluster</code> , <code>CephFilesystem</code> , <code>CephBlockPool</code> , <code>CephNFS</code> , <code>CephObjectStore</code> ; </li><li>  Redimensionnez le d√©ploiement de l'op√©rateur Rook √† 1. </li></ol><br><h2>  Conseils utiles </h2><br>  Faites des sauvegardes! <br><br>  Et pour √©viter les situations o√π vous devez vous remettre d'eux: <br><br><ol><li>  Avant de travailler √† grande √©chelle avec le cluster, consistant en des red√©marrages du serveur, mettez √† l'√©chelle l'op√©rateur Rook √† z√©ro afin qu'il n'en fasse pas trop. </li><li>  Sur les moniteurs, <a href="">ajoutez</a> √† l'avance <a href="">nodeAffinity</a> . </li><li>  Faites attention √† <code>ROOK_MON_HEALTHCHECK_INTERVAL</code> <a href="">les d√©lais d'attente</a> <code>ROOK_MON_HEALTHCHECK_INTERVAL</code> et <code>ROOK_MON_OUT_TIMEOUT</code> . </li></ol><br><h2>  Au lieu d'une conclusion </h2><br>  Il ne sert √† rien de pr√©tendre que Rook, √©tant une ¬´couche¬ª suppl√©mentaire (dans le sch√©ma g√©n√©ral d'organisation du stockage dans Kubernetes), autant simplifie, il ajoute √©galement de nouvelles difficult√©s et des probl√®mes potentiels dans l'infrastructure.  La chose reste ¬´petite¬ª: faire un choix √©quilibr√© et √©clair√© entre ces risques d'une part et les avantages que la solution apporte dans votre cas particulier, d'autre part. <br><br>  Par ailleurs, la section ¬´Adopter un cluster Rook Ceph existant dans un nouveau cluster Kubernetes¬ª a r√©cemment <a href="https://github.com/rook/rook/commit/b651239d3f9a793c95b5c06668b7f28771254082">√©t√© ajout√©e √† la</a> documentation Rook.  Il d√©crit plus en d√©tail ce qui doit √™tre fait pour d√©placer les donn√©es existantes vers un nouveau cluster Kubernetes ou pour restaurer un cluster qui s'est effondr√© pour une raison ou une autre. <br><br><h2>  PS </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  " <a href="https://habr.com/ru/company/flant/blog/451818/">Tour ou pas Tour - c'est la question</a> "; </li><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/348044/">Rook est un entrep√¥t de donn√©es¬´ libre-service ¬ªpour Kubernetes</a> ¬ª; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/474208/">Longhorn, le stockage distribu√© de Rancher pour K8, transf√©r√© √† CNCF</a> ." </li><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/329666/">Cr√©ation d'un stockage persistant avec provisioning dans Kubernetes bas√© sur Ceph</a> .¬ª </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr477680/">https://habr.com/ru/post/fr477680/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr477668/index.html">29 novembre, 18 h - devleads-mitap</a></li>
<li><a href="../fr477670/index.html">Ce qui donne l'automatisation des tests</a></li>
<li><a href="../fr477672/index.html">Droits et obligations des membres de l'√©quipe: aspects juridiques et culturels</a></li>
<li><a href="../fr477674/index.html">AI signifie amour?</a></li>
<li><a href="../fr477678/index.html">Perspectives pour la t√©l√©vision num√©rique en Russie</a></li>
<li><a href="../fr477682/index.html">Services h√©rit√©s dans votre infrastructure</a></li>
<li><a href="../fr477684/index.html">Angular: le meilleur compagnon de construction pour les applications interactives</a></li>
<li><a href="../fr477686/index.html">Notre √† la conf√©rence AI Journey</a></li>
<li><a href="../fr477688/index.html">R√©capitulatif des √©v√©nements informatiques de d√©cembre</a></li>
<li><a href="../fr477692/index.html">Exp√©rience avec ZGC et Shenandoah GC en production</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>