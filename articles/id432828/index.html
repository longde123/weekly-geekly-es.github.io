<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî≤ üîΩ üîª Menguji dan men-debug MapReduce ü•ù üÜí üë©üèæ‚Äçü§ù‚Äçüë©üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Di Rostelecom, kami menggunakan Hadoop untuk menyimpan dan memproses data yang diunduh dari berbagai sumber menggunakan aplikasi java. Kami sekarang t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Menguji dan men-debug MapReduce</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/rostelecom/blog/432828/"> Di Rostelecom, kami menggunakan Hadoop untuk menyimpan dan memproses data yang diunduh dari berbagai sumber menggunakan aplikasi java.  Kami sekarang telah pindah ke versi baru hadoop dengan Kerberos Authentication.  Saat pindah, saya menemui sejumlah masalah, termasuk penggunaan API BENANG.  Pekerjaan Hadoop dengan Otentikasi Kerberos layak mendapatkan artikel terpisah, tetapi dalam artikel ini kita akan berbicara tentang debugging Hadoop MapReduce. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/517/ccf/f7f/517ccff7f9688538cbffdd7ab838470e.png"><br><a name="habracut"></a><br>  Saat menjalankan tugas di gugus, memulai debugger menjadi rumit oleh fakta bahwa kita tidak tahu simpul mana yang akan memproses bagian input data ini atau itu, dan kami tidak dapat mengkonfigurasi debugger kami sebelumnya. <br><br>  Anda dapat menggunakan <code>System.out.println("message")</code> teruji waktu.  Tetapi bagaimana cara menganalisis output <code>System.out.println("message")</code> tersebar di node-node ini? <br><br>  Kami dapat menampilkan pesan ke aliran kesalahan standar.  Semua yang ditulis dalam stdout atau stderr, <br>  dikirim ke file log yang sesuai, yang dapat ditemukan pada halaman web informasi tugas yang diperluas atau dalam file log. <br><br>  Kami juga dapat menyertakan alat debugging dalam kode kami, memperbarui pesan status tugas, dan menggunakan penghitung khusus untuk membantu kami memahami skala bencana. <br><br>  Aplikasi Hadoop MapReduce dapat di-debug dalam ketiga mode di mana Hadoop dapat bekerja: <br><br><ul><li>  mandiri <br></li><li>  mode pseudo-distributed <br></li><li>  sepenuhnya terdistribusi <br></li></ul><br>  Secara lebih rinci kita akan fokus pada dua yang pertama. <br><br><h3>  Mode pseudo-didistribusikan </h3><br>  Mode pseudo-distributed digunakan untuk mensimulasikan sebuah cluster nyata.  Dan itu dapat digunakan untuk pengujian di lingkungan sedekat mungkin dengan produktif.  Dalam mode ini, semua daemon Hadoop akan bekerja pada satu simpul! <br><br>  Jika Anda memiliki server dev atau kotak pasir lainnya (misalnya, Mesin Virtual dengan lingkungan pengembangan yang disesuaikan, seperti Hortonworks Sanbox dengan HDP), maka Anda dapat men-debug program kontrol menggunakan alat debugging jarak jauh. <br><br>  Untuk memulai debugging, Anda perlu mengatur nilai variabel lingkungan: <code>YARN_OPTS</code> .  Berikut ini adalah contohnya.  Untuk kenyamanan, Anda dapat membuat file startWordCount.sh dan menambahkan parameter yang diperlukan untuk meluncurkan aplikasi. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash source /etc/hadoop/conf/yarn-env.sh export YARN_OPTS='-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=6000 ${YARN_OPTS}' yarn jar wordcount-0.0.1.jar ru.rtc.example.WordCount /input /output</span></span></code> </pre><br>  Sekarang, jalankan skrip <code>`./startWordCount.sh`</code> , kita akan melihat pesan <br><br><pre> <code class="plaintext hljs">Listening for transport dt_socket at address: 6000</code> </pre><br>  Masih mengkonfigurasi IDE untuk debugging jarak jauh.  Saya menggunakan IDEA intellij.  Buka menu Run -&gt; Edit Configurations ... Tambahkan konfigurasi <code>Remote</code> baru. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/344/574/fe7/344574fe735f992742437f5ba3660c31.png"><br><br>  Atur breakpoint ke main dan jalankan. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e8a/cc9/8f5/e8acc98f5739b25b54630170fcd3c057.png"><br><br>  Itu dia, sekarang kita bisa men-debug program seperti biasa. <br><blockquote>  PERHATIAN  Anda harus memastikan bahwa Anda bekerja dengan versi terbaru dari kode sumber.  Jika tidak, maka Anda mungkin memiliki perbedaan pada baris di mana debugger berhenti. <br></blockquote><br>  Dalam versi Hadoop sebelumnya, kelas khusus disediakan yang memungkinkan Anda untuk memulai kembali tugas yang gagal - isolationRunner.  Data yang menyebabkan kegagalan disimpan ke disk di alamat yang ditentukan dalam variabel lingkungan Hadoop mapred.local.dir.  Sayangnya, dalam versi terbaru Hadoop, kelas ini tidak lagi disediakan. <br><br><h3>  Standalone (awal lokal) </h3><br>  Standalone adalah mode standar di mana Hadoop bekerja.  Sangat cocok untuk debugging di mana HDFS tidak digunakan.  Dengan debugging seperti itu, Anda dapat menggunakan input dan output melalui sistem file lokal.  Mode mandiri biasanya merupakan mode Hadoop tercepat karena menggunakan sistem file lokal untuk semua data input dan output. <br><br>  Seperti yang disebutkan sebelumnya, Anda dapat menyuntikkan alat debugging ke dalam kode Anda, seperti penghitung.  Penghitung ditentukan oleh Java <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">enum</a> .  Nama enumerasi menentukan nama grup, dan bidang enumerasi menentukan nama-nama penghitung.  Penghitung dapat berguna untuk mengevaluasi masalah, <br>  dan dapat digunakan sebagai tambahan untuk debug output. <br><br>  Deklarasi dan penggunaan konter: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">package</span></span> ru.rt.example; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.IntWritable; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.LongWritable; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.Text; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapreduce.Mapper; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Map</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Mapper</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LongWritable</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Text</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Text</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">IntWritable</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> Text word = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Text(); <span class="hljs-keyword"><span class="hljs-keyword">enum</span></span> Word {   TOTAL_WORD_COUNT, } <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">map</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(LongWritable key, Text value, Context context)</span></span></span><span class="hljs-function"> </span></span>{   String[] stringArr = value.toString().split(<span class="hljs-string"><span class="hljs-string">"\\s+"</span></span>);   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (String str : stringArr) {     word.set(str);     context.getCounter(Word.TOTAL_WORD_COUNT).increment(<span class="hljs-number"><span class="hljs-number">1</span></span>);   } } } }</code> </pre><br>  Untuk menambah penghitung, gunakan metode <code>increment(1)</code> . <br><br><pre> <code class="java hljs">... context.getCounter(Word.TOTAL_WORD_COUNT).increment(<span class="hljs-number"><span class="hljs-number">1</span></span>); ...</code> </pre><br>  Setelah MapReduce selesai dengan sukses, tugas menampilkan penghitung di akhir. <br><br><pre> <code class="plaintext hljs">    Shuffle Errors           BAD_ID=0           CONNECTION=0           IO_ERROR=0           WRONG_LENGTH=0           WRONG_MAP=0           WRONG_REDUCE=0   ru.rt.example.Map$Word           TOTAL_WORD_COUNT=655</code> </pre><br>  Data yang salah dapat berupa output ke stderr atau stdout, atau untuk menulis output ke hdfs menggunakan kelas <code>MultipleOutputs</code> untuk analisis lebih lanjut.  Data yang diterima dapat dikirim ke input aplikasi dalam mode mandiri atau saat menulis unit test. <br><br>  Hadoop memiliki perpustakaan MRUnit, yang digunakan bersama dengan kerangka kerja pengujian (mis. JUnit).  Saat menulis unit test, kami memverifikasi bahwa fungsi menghasilkan hasil yang diharapkan pada output.  Kami menggunakan kelas MapDriver dari paket MRUnit, di properti yang kami atur kelas yang diuji.  Untuk melakukan ini, gunakan metode <code>withMapper()</code> , nilai input <code>withInputValue()</code> dan hasil yang diharapkan <code>withOutput()</code> atau <code>withMultiOutput()</code> jika beberapa output digunakan. <br><br>  Ini tes kami. <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">package</span></span> ru.rt.example; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.IntWritable; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.LongWritable; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.Text; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mrunit.mapreduce.MapDriver; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mrunit.types.Pair; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.junit.Before; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.junit.Test; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.io.IOException; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestWordCount</span></span></span><span class="hljs-class"> </span></span>{   <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> MapDriver&lt;Object, Text, Text, IntWritable&gt; mapDriver;   <span class="hljs-meta"><span class="hljs-meta">@Before</span></span>  <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">setUp</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{     Map mapper = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Map();     mapDriver.setMapper(mapper)  }   <span class="hljs-meta"><span class="hljs-meta">@Test</span></span>  <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mapperTest</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{     mapDriver.withInput(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> LongWritable(<span class="hljs-number"><span class="hljs-number">0</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Text(<span class="hljs-string"><span class="hljs-string">"msg1"</span></span>));     mapDriver.withOutput(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Pair&lt;Text, IntWritable&gt;(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Text(<span class="hljs-string"><span class="hljs-string">"msg1"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> IntWritable(<span class="hljs-number"><span class="hljs-number">1</span></span>)));     mapDriver.runTest();  } }</code> </pre><br><h3>  Mode terdistribusi penuh </h3><br>  Seperti namanya, ini adalah mode di mana semua kekuatan Hadoop digunakan.  Program MapReduce yang diluncurkan dapat berjalan di 1000 server.  Selalu sulit untuk men-debug program MapReduce, karena Anda memiliki Mappers yang berjalan pada mesin yang berbeda dengan data input yang berbeda. <br><br><h2>  Kesimpulan </h2><br>  Ternyata, pengujian MapReduce tidak semudah kelihatannya pada pandangan pertama. <br>  Untuk menghemat waktu mencari kesalahan di MapReduce, saya menggunakan semua metode yang tercantum di atas dan saya menyarankan semua orang untuk menerapkannya juga.  Ini sangat berguna dalam kasus instalasi besar, seperti yang bekerja di Rostelecom. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id432828/">https://habr.com/ru/post/id432828/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id432818/index.html">Homemade plotter: tips untuk pemula, bekerja dengan grbl-firmware</a></li>
<li><a href="../id432820/index.html">Pengujian dinamis aplikasi Android</a></li>
<li><a href="../id432822/index.html">Saya merusak kode hidup saya untuk pengembang dan saya tidak ingin melakukannya lagi</a></li>
<li><a href="../id432824/index.html">Mempercepat pembuatan ConcurrentReferenceHashMap</a></li>
<li><a href="../id432826/index.html">Pengembangan Android modern di Kotlin. Bagian 2</a></li>
<li><a href="../id432830/index.html">Sistem otomatis untuk mengenakan denda untuk sampah yang ditinggalkan</a></li>
<li><a href="../id432832/index.html">Cara "merekatkan" server berbasis Intel dan mengatasi peningkatan skala 8 prosesor</a></li>
<li><a href="../id432834/index.html">Tautan internal dan eksternal dalam C ++</a></li>
<li><a href="../id432836/index.html">Bola lampu bagus pertama dari Aliexpress</a></li>
<li><a href="../id432838/index.html">Pengembangan perangkat lunak melalui prisma percobaan Milgram "Pengajuan ke otoritas"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>