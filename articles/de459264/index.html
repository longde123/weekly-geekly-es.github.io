<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèΩ üë®‚Äçüíº üïé Verlassen von Tarantool-Netzwerken. Knotensynchronisation beim Filtern des Datenverkehrs üöµüèæ ü§µüèΩ üïî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Variti ist auf den Schutz vor Bots und DDoS-Angriffen spezialisiert und f√ºhrt auch Stress- und Lasttests durch. Da wir als internationaler Dienst arbe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verlassen von Tarantool-Netzwerken. Knotensynchronisation beim Filtern des Datenverkehrs</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/variti/blog/459264/"><img src="https://habrastorage.org/getpro/habr/post_images/479/4d3/c95/4794d3c95e7b9d2fe9155a3083f674eb.jpg" alt="Bild"><br><br>  <i>Variti ist auf den Schutz vor Bots und DDoS-Angriffen spezialisiert und f√ºhrt auch Stress- und Lasttests durch.</i>  <i>Da wir als internationaler Dienst arbeiten, ist es f√ºr uns √§u√üerst wichtig, einen unterbrechungsfreien Informationsaustausch zwischen Servern und Clustern in Echtzeit sicherzustellen.</i>  <i>Auf der Saint HighLoad ++ 2019-Konferenz erkl√§rte der Variti-Entwickler Anton Barabanov, wie wir UDP und Tarantool verwenden, warum wir so viel genommen haben und wie wir das Tarantool-Modul von Lua nach C umschreiben mussten.</i> <br><br>  Sie k√∂nnen die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zusammenfassung des</a> Berichts auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√ºber den</a> Link lesen und das folgende Video unter dem Spoiler sehen. <br><br><div class="spoiler">  <b class="spoiler_title">Video melden</b> <div class="spoiler_text"><iframe width="560" height="315" src="https://www.youtube.com/embed/R0-POaXC0lI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br></div></div><br>  Als wir mit der Erstellung eines Verkehrsfilterdienstes begannen, entschieden wir uns sofort, uns nicht mit IP-Transit zu befassen, sondern HTTP-, API- und Spieledienste zu sch√ºtzen.  Daher beenden wir den Verkehr auf L7-Ebene im TCP-Protokoll und geben ihn weiter.  Der gleichzeitige Schutz von L3 und 4 erfolgt automatisch.  Das folgende Diagramm zeigt das Servicediagramm: Anforderungen von Personen durchlaufen einen Cluster, dh Server und Netzwerkger√§te, und Bots (als Geist dargestellt) werden gefiltert. <br><br><img src="https://habrastorage.org/webt/g0/eg/qd/g0egqdye_eghswvgjsdpkek9cda.jpeg"><br><br>  Zum Filtern ist es erforderlich, den Datenverkehr in separate Anforderungen aufzuteilen, die Sitzung genau und schnell zu analysieren. Da wir nicht nach IP-Adressen blockieren, definieren Sie Bots und Personen innerhalb der Verbindung von derselben IP-Adresse aus. <a name="habracut"></a><br><br><h4>  Was passiert im Cluster? </h4><br>  Innerhalb des Clusters haben wir unabh√§ngige Filterknoten, dh jeder Knoten arbeitet f√ºr sich und nur mit seinem eigenen Datenverkehr.  Der Datenverkehr wird zuf√§llig auf die Knoten verteilt: Wenn beispielsweise 10 Verbindungen von einem Benutzer empfangen werden, unterscheiden sich alle auf verschiedenen Servern. <br><br>  Wir haben sehr hohe Leistungsanforderungen, da sich unsere Kunden in verschiedenen L√§ndern befinden.  Und wenn beispielsweise ein Benutzer aus der Schweiz eine franz√∂sische Site besucht, ist er aufgrund einer Zunahme der Verkehrsroute bereits mit einer Netzwerkverz√∂gerung von 15 Millisekunden konfrontiert.  Daher sind wir nicht berechtigt, weitere 15 bis 20 Millisekunden in unserem Verarbeitungszentrum hinzuzuf√ºgen - die Anfrage wird noch sehr lange dauern.  Wenn wir jede HTTP-Anforderung f√ºr 15 bis 20 Millisekunden verarbeiten, summiert sich ein einfacher Angriff von 20.000 RPS auf den gesamten Cluster.  Dies ist nat√ºrlich nicht akzeptabel. <br><br>  Eine weitere Anforderung f√ºr uns war nicht nur die Verfolgung der Anfrage, sondern auch das Verst√§ndnis des Kontexts.  Angenommen, ein Benutzer √∂ffnet eine Webseite und sendet eine Schr√§gstrichanforderung.  Danach wird die Seite geladen, und wenn es sich um HTTP / 1.1 handelt, √∂ffnet der Browser 10 Verbindungen zum Backend und fordert in 10 Streams Statik und Dynamik an, stellt Ajax-Anfragen und Unterabfragen.  Wenn Sie beim Zur√ºckgeben der Seite nicht eine Unterabfrage vertreten, sondern mit dem Browser interagieren und versuchen, ihm beispielsweise JS Challenge f√ºr die Unterabfrage zu geben, wird die Seite h√∂chstwahrscheinlich unterbrochen.  Bei der allerersten Anfrage k√∂nnen Sie CAPTCHA- (obwohl dies schlecht ist) oder JS-Herausforderungen stellen, eine Weiterleitung vornehmen, und dann verarbeitet jeder Browser alles korrekt.  Nach dem Testen m√ºssen Informationen zu allen Clustern verbreitet werden, f√ºr die die Sitzung legitim ist.  Wenn zwischen den Clustern kein Informationsaustausch stattfindet, empfangen die anderen Knoten die Sitzung von der Mitte und wissen nicht, ob sie √ºbersprungen werden sollen oder nicht. <br><br>  Es ist auch wichtig, schnell auf alle Lastst√∂√üe und √Ñnderungen im Verkehr zu reagieren.  Wenn etwas auf einen Knoten gesprungen ist, tritt nach 50-100 Millisekunden ein Sprung auf allen anderen Knoten auf.  Daher ist es besser, wenn die Knoten die √Ñnderungen im Voraus kennen und die Schutzparameter im Voraus einstellen, damit auf allen anderen Knoten kein Sprung auftritt. <br>  Ein zus√§tzlicher Dienst zum Schutz vor Bots war der Post-Markup-Dienst: Wir haben ein Pixel auf der Website platziert, Bot- / Personeninformationen geschrieben und diese Daten √ºber die API gesendet.  Diese Urteile m√ºssen irgendwo aufbewahrt werden.  Das hei√üt, wenn wir fr√ºher √ºber die Synchronisation innerhalb eines Clusters gesprochen haben, f√ºgen wir jetzt auch die Synchronisation von Informationen zwischen Clustern hinzu.  Nachfolgend zeigen wir das Schema des Dienstes auf L7-Ebene. <br><br><img src="https://habrastorage.org/webt/_t/lj/ah/_tljahe28yfpufxw6suxf9ay14y.jpeg"><br><br><h4>  Zwischen Clustern </h4><br>  Nachdem wir den Cluster erstellt hatten, begannen wir mit der Skalierung.  Wir arbeiten mit BGP anycast, dh unsere Subnetze werden von allen Clustern angek√ºndigt und der Datenverkehr kommt zum n√§chsten.  Einfach ausgedr√ºckt, eine Anfrage wird von Frankreich an einen Cluster in Frankfurt und von St. Petersburg an einen Cluster in Moskau gesendet.  Cluster sollten unabh√§ngig sein.  Netzwerkstr√∂me sind unabh√§ngig voneinander zul√§ssig. <br><br>  Warum ist das wichtig?  Angenommen, eine Person f√§hrt Auto, arbeitet mit einer Website aus dem mobilen Internet und √ºberquert ein bestimmtes Rubikon. Danach wechselt der Datenverkehr pl√∂tzlich zu einem anderen Cluster.  Oder ein anderer Fall: Die Verkehrsroute wurde neu erstellt, weil irgendwo der Switch oder Router durchgebrannt ist, etwas heruntergefallen ist und das Netzwerksegment getrennt wurde.  In diesem Fall stellen wir dem Browser (z. B. in Cookies) ausreichende Informationen zur Verf√ºgung, damit beim Wechsel zu einem anderen Cluster die erforderlichen Parameter √ºber die bestandenen oder fehlgeschlagenen Tests informiert werden k√∂nnen. <br>  Dar√ºber hinaus m√ºssen Sie den Schutzmodus zwischen Clustern synchronisieren.  Dies ist wichtig bei Angriffen mit geringem Volumen, die am h√§ufigsten unter dem Deckmantel von √úberschwemmungen durchgef√ºhrt werden.  Da Angriffe parallel ausgef√ºhrt werden, glauben die Benutzer, dass ihre Website die Flut bricht, und sehen keinen Angriff mit geringem Volumen.  F√ºr den Fall, dass ein Cluster nur eine geringe Lautst√§rke aufweist und ein anderer √ºberflutet wird, ist eine Synchronisierung des Schutzmodus erforderlich. <br><br>  Und wie bereits erw√§hnt, synchronisieren wir zwischen den Clustern genau die Urteile, die sich ansammeln und von der API gegeben werden.  In diesem Fall kann es viele Urteile geben, die zuverl√§ssig synchronisiert werden m√ºssen.  Im Schutzmodus k√∂nnen Sie etwas innerhalb des Clusters verlieren, jedoch nicht zwischen den Clustern. <br><br>  Es ist erw√§hnenswert, dass zwischen den Clustern eine gro√üe Latenz besteht: Im Fall von Moskau und Frankfurt sind dies 20 Millisekunden.  Synchrone Anforderungen k√∂nnen hier nicht gestellt werden, alle Interaktionen m√ºssen im asynchronen Modus erfolgen. <br><br>  Nachfolgend zeigen wir die Interaktion zwischen den Clustern.  M, l, p sind einige technische Parameter f√ºr einen Austausch.  U1, u2 ist ein Benutzer-Markup als unzul√§ssig und legitim. <br><br><img src="https://habrastorage.org/webt/a1/xl/si/a1xlsi20avhzuiv_s1uljdmubic.jpeg"><br><br><h4>  Interne Interaktion zwischen Knoten </h4><br>  Zu Beginn des Dienstes wurde die Filterung auf L7-Ebene nur auf einem Knoten gestartet.  Dies funktionierte gut f√ºr zwei Kunden, aber nicht mehr.  Bei der Skalierung wollten wir maximale Reaktionsf√§higkeit und minimale Latenz erreichen. <br><br>  Es war wichtig, die f√ºr die Verarbeitung von Paketen aufgewendeten CPU-Ressourcen zu minimieren, damit eine Interaktion √ºber beispielsweise HTTP nicht geeignet w√§re.  Es war auch notwendig, einen minimalen Overhead-Verbrauch von nicht nur Rechenressourcen, sondern auch der Paketrate sicherzustellen.  Trotzdem geht es um das Filtern von Angriffen, und dies sind Situationen, in denen offensichtlich nicht gen√ºgend Leistung vorhanden ist.  Normalerweise reicht beim Erstellen eines Webprojekts x3 oder x4 f√ºr die Last aus, aber wir haben immer x1, da immer ein gro√ü angelegter Angriff erfolgen kann. <br><br>  Eine weitere Voraussetzung f√ºr die Interaktionsschnittstelle ist das Vorhandensein eines Ortes, an dem wir Informationen schreiben und von dem aus wir dann berechnen k√∂nnen, in welchem ‚Äã‚ÄãZustand wir uns gerade befinden.  Es ist kein Geheimnis, dass C ++ h√§ufig zur Entwicklung von Filtersystemen verwendet wird.  Leider st√ºrzen in C ++ geschriebene Programme manchmal ab.  Manchmal m√ºssen solche Programme neu gestartet werden, um sie zu aktualisieren, oder zum Beispiel, weil die Konfiguration nicht erneut gelesen wurde.  Und wenn wir den angegriffenen Knoten neu starten, m√ºssen wir irgendwo den Kontext nehmen, in dem dieser Knoten existiert hat.  Das hei√üt, der Dienst sollte nicht staatenlos sein, sondern sich daran erinnern, dass es eine bestimmte Anzahl von Personen gibt, die wir blockiert haben und die wir √ºberpr√ºfen.  Es muss dieselbe interne Kommunikation vorhanden sein, damit der Dienst einen prim√§ren Satz von Informationen empfangen kann.  Wir hatten Gedanken, in die N√§he einer bestimmten Datenbank zu gehen, zum Beispiel SQLite, aber wir haben eine solche L√∂sung schnell verworfen, da es seltsam ist, Input-Output auf jeden Server zu schreiben. Dies funktioniert schlecht im Speicher. <br><br>  Tats√§chlich arbeiten wir mit nur drei Operationen.  Die erste Funktion ist "Senden" an alle Knoten.  Dies gilt beispielsweise f√ºr Nachrichten zur Synchronisierung der aktuellen Last: Jeder Knoten muss die Gesamtlast der Ressource innerhalb des Clusters kennen, um Spitzenwerte verfolgen zu k√∂nnen.  Die zweite Operation ist das ‚ÄûSpeichern‚Äú, es handelt sich um √úberpr√ºfungsurteile.  Und die dritte Operation ist eine Kombination aus "An alle senden" und "Speichern".  Hier geht es um Status√§nderungsnachrichten, die wir an alle Knoten senden und dann speichern, um subtrahieren zu k√∂nnen.  Unten sehen Sie das resultierende Interaktionsschema, in dem wir Parameter zum Speichern hinzuf√ºgen m√ºssen. <br><br><img src="https://habrastorage.org/webt/s7/__/8c/s7__8c-cnmitihkranlrso0ukza.jpeg"><br><br><h4>  Optionen und Ergebnis </h4><br>  Welche M√∂glichkeiten zur Bewahrung von Urteilen haben wir gepr√ºft?  Zun√§chst haben wir √ºber die Klassiker RabbitMQ, RedisMQ und unseren eigenen TCP-basierten Service nachgedacht.  Wir haben diese Entscheidungen abgelehnt, weil sie langsam funktionieren.  Das gleiche TCP erh√∂ht die Paketrate um x2.  Wenn wir eine Nachricht von einem Knoten an alle anderen senden, ben√∂tigen wir entweder viele Sendeknoten, oder dieser Knoten kann 1/16 der Nachrichten vergiften, die 16 Computer an ihn senden k√∂nnen.  Es ist klar, dass dies nicht akzeptabel ist. <br><br>  Aus diesem Grund haben wir UDP-Multicast verwendet, da das Sendezentrum in diesem Fall eine Netzwerkausr√ºstung ist, deren Leistung nicht eingeschr√§nkt ist und die es Ihnen erm√∂glicht, Probleme mit der Sende- und Empfangsgeschwindigkeit vollst√§ndig zu l√∂sen.  Es ist klar, dass wir bei UDP nicht an Textformate denken, sondern Bin√§rdaten senden. <br><br>  Zus√§tzlich haben wir sofort Verpackung und eine Datenbank hinzugef√ºgt.  Wir haben Tarantool gew√§hlt, weil zum einen alle drei Gr√ºnder des Unternehmens Erfahrung mit dieser Datenbank hatten und zum anderen ist sie so flexibel wie m√∂glich, dh es handelt sich auch um eine Art Anwendungsservice.  Dar√ºber hinaus verf√ºgt Tarantool √ºber CAPI, und die F√§higkeit, in C zu schreiben, ist f√ºr uns eine Grundsatzfrage, da zum Schutz vor DDoS ein maximaler Schutz erforderlich ist.  Im Gegensatz zu C kann keine interpretierte Sprache eine ausreichende Leistung erbringen. <br><br>  In der folgenden Abbildung haben wir eine Datenbank innerhalb des Clusters hinzugef√ºgt, in der die Zust√§nde f√ºr die interne Kommunikation gespeichert sind. <br><br><img src="https://habrastorage.org/webt/k3/py/60/k3py60c1-_z4uirabzit7-otlri.jpeg"><br><br><h4>  Datenbank hinzuf√ºgen </h4><br>  In der Datenbank speichern wir den Status in Form eines Anrufprotokolls.  Als wir herausfanden, wie Informationen gespeichert werden k√∂nnen, gab es zwei M√∂glichkeiten.  Es war m√∂glich, einen Status mit st√§ndiger Aktualisierung und √Ñnderung zu speichern, aber die Implementierung ist ziemlich schwierig.  Deshalb haben wir einen anderen Ansatz gew√§hlt. <br><br>  Tatsache ist, dass die Struktur der √ºber UDP gesendeten Daten einheitlich ist: Es gibt Timing, eine Art Code, drei oder vier Datenfelder.  Also haben wir begonnen, diese Struktur in Space Tarantool zu schreiben und dort einen TTL-Datensatz hinzugef√ºgt, der deutlich macht, dass die Struktur veraltet ist und gel√∂scht werden muss.  Somit wird in Tarantool ein Nachrichtenprotokoll akkumuliert, das wir mit dem angegebenen Timing l√∂schen.  Um alte Daten zu l√∂schen, haben wir zun√§chst expirationd verwendet.  Anschlie√üend mussten wir es aufgeben, da es bestimmte Probleme verursachte, die wir weiter unten diskutieren werden.  Bisher das Schema: Darauf wurden zwei Datenbanken zu unserer Struktur hinzugef√ºgt. <br><br><img src="https://habrastorage.org/webt/p-/li/-m/p-li-mkvdsa7ks9bxqz00ywlrey.jpeg"><br><br>  Wie bereits erw√§hnt, m√ºssen neben der Speicherung von Clusterzust√§nden auch Urteile synchronisiert werden.  Urteile synchronisieren wir Intercluster.  Dementsprechend musste eine zus√§tzliche Installation von Tarantool hinzugef√ºgt werden.  Es w√§re seltsam, eine andere L√∂sung zu verwenden, da Tarantool bereits vorhanden ist und sich ideal f√ºr unseren Service eignet.  In der neuen Installation haben wir begonnen, Urteile zu schreiben und sie mit anderen Clustern zu replizieren.  In diesem Fall verwenden wir nicht Master / Slave, sondern Master / Master.  Jetzt gibt es in Tarantool nur einen asynchronen Master / Master, was in vielen F√§llen nicht geeignet ist, aber f√ºr uns ist dieses Modell optimal.  Bei minimaler Latenz zwischen Clustern w√ºrde die synchrone Replikation im Weg stehen, w√§hrend die asynchrone Replikation keine Probleme verursacht. <br><br><h4>  Die Probleme </h4><br>  Aber wir hatten viele Probleme.  <i>Der erste Komplexit√§tsblock bezieht sich auf UDP</i> : Es ist kein Geheimnis, dass das Protokoll Pakete schlagen und verlieren kann.  Wir haben diese Probleme mit der Strau√ümethode gel√∂st, dh wir haben einfach unsere K√∂pfe im Sand versteckt.  Trotzdem ist eine Paketbesch√§digung und Neuordnung ihrer Orte bei uns nicht m√∂glich, da die Kommunikation im Rahmen eines einzelnen Switches erfolgt und es keine instabilen Verbindungen und instabilen Netzwerkger√§te gibt. <br><br>  Es kann ein Problem mit dem Paketverlust auftreten, wenn ein Computer einfriert, irgendwo eine Eingabe / Ausgabe auftritt oder ein Knoten √ºberlastet ist.  Wenn ein solcher Hang f√ºr einen kurzen Zeitraum, beispielsweise 50 Millisekunden, aufgetreten ist, ist dies schrecklich, wird jedoch durch erh√∂hte Sysctl-Warteschlangen gel√∂st.  Das hei√üt, wir nehmen sysctl, konfigurieren die Gr√∂√üe der Warteschlangen und erhalten einen Puffer, in dem alles liegt, bis der Knoten wieder funktioniert.  Wenn ein l√§ngeres Einfrieren auftritt, ist das Problem nicht der Verlust der Konnektivit√§t, sondern ein Teil des Datenverkehrs, der zum Knoten geht.  Bisher hatten wir einfach keine solchen F√§lle. <br><br>  <i>Asynchrone Replikationsprobleme von Tarantool</i> waren viel komplexer.  Anfangs haben wir nicht Master / Master genommen, sondern ein traditionelleres Modell f√ºr den Betrieb von Master / Slave.  Und alles funktionierte genau so lange, bis der Slave lange Zeit die Hauptlast √ºbernahm.  Infolgedessen funktionierte expirationd und l√∂schte Daten auf dem Master, auf dem Slave jedoch nicht.  Dementsprechend sammelten sich beim mehrmaligen Wechsel von Master zu Slave und zur√ºck so viele Daten auf dem Slave an, dass irgendwann alles kaputt ging.  F√ºr eine vollst√§ndige Fehlertoleranz musste ich also auf asynchrone Master / Master-Replikation umsteigen. <br><br>  Und auch hier traten wieder Schwierigkeiten auf.  Erstens k√∂nnen sich Schl√ºssel zwischen verschiedenen Replikaten schneiden.  Angenommen, wir haben innerhalb des Clusters Daten an einen Master geschrieben. Zu diesem Zeitpunkt wurde die Verbindung unterbrochen. Wir haben alles an den zweiten Master geschrieben. Nachdem wir die asynchrone Replikation durchgef√ºhrt hatten, stellte sich heraus, dass derselbe Prim√§rschl√ºssel im Speicher und die Replikation auseinander fielen. <br><br>  Wir haben dieses Problem einfach gel√∂st: Wir haben ein Modell genommen, in dem der Prim√§rschl√ºssel unbedingt den Namen des Tarantool-Knotens enth√§lt, auf den wir schreiben.  Aufgrund dessen traten keine Konflikte mehr auf, aber eine Situation ist m√∂glich geworden, wenn Benutzerdaten dupliziert werden.  Dies ist ein √§u√üerst seltener Fall, daher haben wir ihn erneut einfach vernachl√§ssigt.  Wenn h√§ufig Duplikationen auftreten, verf√ºgt Tarantool √ºber viele verschiedene Indizes, sodass Sie jederzeit eine Deduplizierung durchf√ºhren k√∂nnen. <br><br>  Ein weiteres Problem betrifft die Bewahrung von Urteilen und tritt auf, wenn die auf einem Master aufgezeichneten Daten noch nicht auf einem anderen Master erschienen sind und beim ersten Master bereits eine Anfrage eingegangen ist.  Um ehrlich zu sein, haben wir dieses Problem noch nicht gel√∂st und verz√∂gern lediglich das Urteil.  Wenn dies nicht akzeptabel ist, werden wir eine Art Push zur Datenbereitschaft organisieren.  So haben wir mit der Master / Master-Replikation und ihren Problemen umgegangen. <br><br>  <i>Es gab eine Reihe von Problemen, die direkt mit Tarantool</i> , seinen Treibern und <i>dem Ablaufmodul zusammenh√§ngen</i> .  Einige Zeit nach dem Start kamen jeden Tag Angriffe auf uns zu. Die Anzahl der Nachrichten, die wir zur Synchronisierung und Speicherung des Kontexts in der Datenbank speichern, ist sehr gro√ü geworden.  Und w√§hrend des Strippens wurden so viele Daten gel√∂scht, dass der Garbage Collector die Bew√§ltigung stoppte.  Wir haben dieses Problem gel√∂st, indem wir in C unser eigenes Ablaufmodul namens IExpire geschrieben haben. <br><br>  Mit expirationd gibt es jedoch eine weitere Schwierigkeit, mit der wir noch nicht fertig geworden sind, und die darin liegt, dass expirationd nur auf einem Master funktioniert.  Wenn der Knoten expirationd ausf√§llt, verliert der Cluster wichtige Funktionen.  Angenommen, wir bereinigen alle Daten, die √§lter als eine Stunde sind. Wenn ein Knoten beispielsweise f√ºnf Stunden liegt, betr√§gt die Datenmenge x5 gegen√ºber dem √ºblichen Wert.  Und wenn in diesem Moment ein gro√üer Angriff erfolgt, dh zwei schlimme F√§lle zusammenfallen, f√§llt der Cluster.  Wir wissen noch nicht, wie wir damit umgehen sollen. <br><br>  Schlie√ülich gab es weiterhin Schwierigkeiten mit dem Tarantool-Treiber f√ºr C. Als wir den Dienst ausfielen (z. B. aufgrund der Rennbedingungen), dauerte es lange, den Grund und das Debugging zu finden.  Deshalb haben wir gerade unseren Tarantool-Treiber geschrieben.  Wir haben f√ºnf Tage gebraucht, um das Protokoll zusammen mit dem Testen, Debuggen und Ausf√ºhren in der Produktion zu implementieren, aber wir hatten bereits unseren eigenen Code f√ºr die Arbeit mit dem Netzwerk. <br><br><h4>  Probleme drau√üen </h4><br>  Denken Sie daran, dass wir die Tarantool-Replikation bereits bereit haben. Wir wissen bereits, wie Urteile synchronisiert werden, aber es gibt keine Infrastruktur f√ºr die √úbertragung von Nachrichten √ºber Angriffe oder Probleme zwischen Clustern. <br>  Wir hatten viele verschiedene Gedanken √ºber die Infrastruktur, einschlie√ülich des Gedankens, unseren eigenen TCP-Dienst zu schreiben.  Trotzdem gibt es ein Tarantool-Warteschlangenmodul vom Tarantool-Team.  Dar√ºber hinaus hatten wir bereits Tarantool mit cluster√ºbergreifender Replikation. Die ‚ÄûL√∂cher‚Äú waren verdreht, dh es war nicht erforderlich, zu den Administratoren zu gehen und nach Ports zu fragen oder den Datenverkehr zu steuern.  Auch hier war die Integration in die Softwarefiltration bereit. <br><br>  Es gab eine Schwierigkeit mit dem Hostknoten.  Angenommen, ein Cluster enth√§lt n unabh√§ngige Knoten, und Sie m√ºssen den Knoten ausw√§hlen, der mit der Schreibwarteschlange interagiert.  Andernfalls werden 16 Nachrichten gesendet oder 16 Mal dieselbe Nachricht von der Warteschlange abgezogen.  Wir haben dieses Problem einfach gel√∂st: Wir registrieren einen verantwortlichen Knoten im Raum Tarantool, und wenn der Knoten abbrennt, √§ndern wir einfach den Raum, wenn wir nicht vergessen.  Aber wenn wir vergessen, dann ist dies ein Problem, das wir auch in Zukunft l√∂sen wollen. <br><br>  Unten sehen Sie ein bereits detailliertes Diagramm eines Clusters mit einer Interaktionsschnittstelle. <br><br><img src="https://habrastorage.org/webt/8f/f3/9o/8ff39og9opl3e4rjwydmtpdsola.jpeg"><br><br><h4>  Was ich verbessern und hinzuf√ºgen m√∂chte </h4><br> -,     open source IExpire.  ,   ,         ,   expirationd,     overhead.      ,      tuple.      ,     Tarantool   ‚Äî  ‚Äú‚Äù,       -  .        CAPI,     . <br><br>      ,  ,     .    ,    expirationd ,         expirationd  .      ,   .  , ,       Tarantool. <br><br>             Tarantool.   ,   Tarantool Queue       ‚Äú--‚Äù.   ,  , , ,     100,   ,    ,   ,    - . -,  ,  Tarantool   . <br><br><h4>  Schlussfolgerungen </h4><br>    UDP multicast  Tarantool. Multicast    ,     ‚Äî  ,   .   ,     ,   50   ,   .    ,  ,      .    UDP multicast ,         . <br><br>   ‚Äî Tarantool.      go, php   ,    Tarantool   .      ,   .   ,        :   Oracle,   PostgeSQL. <br><br> ,  ,     ,      ,     : Redis  ,  go, python   .  .   ,     ,     open source, ,     ,    ,     ,     .  ,    .       Tarantool,      ,      ,   Redis,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de459264/">https://habr.com/ru/post/de459264/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de459252/index.html">Sicherheitswoche 28: Ein Smart Home hacken</a></li>
<li><a href="../de459254/index.html">Noch bessere Rei√üverschlussbombe</a></li>
<li><a href="../de459256/index.html">Wie wir unser Themenkrankenhaus f√ºr verschiedene Plattformen optimiert haben</a></li>
<li><a href="../de459258/index.html">14.000 Meilen nicht haken</a></li>
<li><a href="../de459262/index.html">Mit 22 in den Ruhestand getreten</a></li>
<li><a href="../de459272/index.html">Schreiben einer API f√ºr React-Komponenten, Teil 1: Erstellen Sie keine widerspr√ºchlichen Requisiten</a></li>
<li><a href="../de459274/index.html">Sicherheitsanf√§lligkeit bez√ºglich Bildschirmsperre in Astra Linux Special Edition (Smolensk)</a></li>
<li><a href="../de459276/index.html">Epic Fail Resistance 2 oder warum Sie sich mit FireFox-Plugins nicht auf den Datenschutz einlassen sollten</a></li>
<li><a href="../de459280/index.html">Warum lieben Entwickler es, native Apps zu erstellen?</a></li>
<li><a href="../de459284/index.html">Kurze Einf√ºhrung in die Priorisierung von Produktstrategien und Funktionen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>