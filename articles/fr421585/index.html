<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüè´ üë©üèª‚Äçüåæ ü§ôüèΩ Comment le concours r√©tro des Cosaques a-t-il d√©cid√© üïû üê∂ üíÜüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ce printemps, un important concours r√©tro OpenAI a eu lieu, consacr√© √† l'apprentissage par renforcement, au m√©ta-apprentissage et, bien s√ªr, √† Sonic. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment le concours r√©tro des Cosaques a-t-il d√©cid√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/421585/"><p>  Ce printemps, un important concours r√©tro OpenAI a eu lieu, consacr√© √† l'apprentissage par renforcement, au m√©ta-apprentissage et, bien s√ªr, √† Sonic.  Notre √©quipe a pris la 4e place sur 900+ √©quipes.  Le domaine de la formation avec renforcement est l√©g√®rement diff√©rent de l'apprentissage automatique standard, et ce concours √©tait diff√©rent d'une comp√©tition RL typique.  Je demande des d√©tails sous chat. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sx/pt/0r/sxpt0rdvj5g3xn6eohh6vmusj1o.jpeg" alt="image"></div><br><hr><a name="habracut"></a><br><h2 id="tldr">  TL; DR </h2><br><p>  Une ligne de base correctement r√©gl√©e n'a pas besoin de trucs suppl√©mentaires ... pratiquement. </p><br><h2 id="intro-v-obuchenie-s-podkrepleniem">  Introduction √† la formation de renforcement </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/nb/vn/yxnbvndwcscdelo4q_m7zxxhfsu.png" alt="image"></div><br><p>  L'apprentissage renforc√© est un domaine qui combine la th√©orie du contr√¥le optimal, la th√©orie des jeux, la psychologie et la neurobiologie.  Dans la pratique, l'apprentissage par renforcement est utilis√© pour r√©soudre des probl√®mes de prise de d√©cision et rechercher des strat√©gies comportementales optimales ou des politiques trop complexes pour une programmation ¬´directe¬ª.  Dans ce cas, l'agent est form√© sur l'historique des interactions avec l'environnement.  L'environnement, √† son tour, √©valuant les actions de l'agent, lui fournit une r√©compense (scalaire) - meilleur est le comportement de l'agent, meilleure est la r√©compense.  En cons√©quence, la meilleure politique est apprise de l'agent qui a appris √† maximiser la r√©compense totale pendant toute la dur√©e de l'interaction avec l'environnement. </p><br><p>  √Ä titre d'exemple simple, vous pouvez jouer √† BreakOut.  Dans ce bon vieux jeu de la s√©rie Atari, une personne / un agent doit contr√¥ler la plate-forme horizontale inf√©rieure, frapper la balle et casser progressivement tous les blocs sup√©rieurs avec elle.  Le plus renvers√© - plus la r√©compense est grande.  En cons√©quence, ce qu'une personne / un agent voit est une image d'√©cran et il est n√©cessaire de d√©cider dans quelle direction d√©placer la plate-forme inf√©rieure. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/an/ke/wiankeye_8wb0cxtrhpsfdf5sdm.gif" alt="image"></div><br><p>  Si vous √™tes int√©ress√© par le sujet de la formation de renforcement, je vous conseille un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cours d'introduction</a> sympa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">de HSE</a> , ainsi que son <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">homologue open source</a> plus d√©taill√©.  Si vous voulez quelque chose que vous pouvez lire, mais avec des exemples - un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">livre</a> inspir√© de ces deux cours.  J'ai r√©vis√© / termin√© / aid√© √† cr√©er tous ces cours, et par cons√©quent, je sais par ma propre exp√©rience qu'ils fournissent une excellente base. </p><br><h2 id="pro-zadachu">  √Ä propos de la t√¢che </h2><br><p>  L'objectif principal de cette comp√©tition √©tait de trouver un agent capable de bien jouer dans la s√©rie de jeux SEGA - Sonic The Hedgehog.  OpenAI commen√ßait tout juste √† importer des jeux de SEGA dans sa plateforme de formation d'agents RL, et a donc d√©cid√© de promouvoir un peu ce moment.  M√™me l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article a √©t√©</a> publi√© avec l'appareil de tout et une description d√©taill√©e des m√©thodes de base. </p><br><p>  Les 3 jeux Sonic ont √©t√© pris en charge, chacun avec 9 niveaux, auxquels, en jetant une larme en larmes, vous pouvez m√™me jouer, en vous souvenant de votre enfance (apr√®s les avoir achet√©s sur Steam en premier). </p><br><p>  L'√©tat de l'environnement (ce que l'agent a vu) √©tait l'image du simulateur - une image RVB, et en tant qu'action, l'agent a √©t√© invit√© √† choisir le bouton du joystick virtuel sur lequel appuyer - sauter / gauche / droite et ainsi de suite.  L'agent a re√ßu des points de r√©compense ainsi que dans le jeu original, c'est-√†-dire  pour la collecte des anneaux, ainsi que pour la vitesse de passage du niveau.  En fait, nous avions une sonique originale devant nous, seulement il fallait la passer avec l'aide de notre agent. </p><br><p>  Le concours s'est d√©roul√© du 5 avril au 5 juin, soit  seulement 2 mois, ce qui semble assez petit.  Notre √©quipe n'a pu se r√©unir et participer √† la comp√©tition qu'en mai, ce qui nous a fait beaucoup apprendre en d√©placement. </p><br><h2 id="baselines">  Baselines </h2><br><p>  √Ä titre de r√©f√©rence, des guides de formation complets pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la</a> formation <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Rainbow</a> (approche DQN) et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PPO</a> (approche Gradient politique) √† l'un des niveaux possibles dans Sonic et la soumission de l'agent r√©sultant ont √©t√© fournis. </p><br><p>  La version Rainbow √©tait bas√©e sur le projet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">anyrl</a> peu connu, mais PPO utilisait les bonnes vieilles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lignes</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">base</a> d'OpenAI et nous semblait beaucoup plus pr√©f√©rable. </p><br><p>  Les lignes de base publi√©es diff√®rent des approches d√©crites dans l'article par leur plus grande simplicit√© et leurs petits ensembles de ¬´hacks¬ª pour acc√©l√©rer l'apprentissage.  Ainsi, les organisateurs ont lanc√© des id√©es et fix√© la direction, mais la d√©cision sur l'utilisation et la mise en ≈ìuvre de ces id√©es a √©t√© laiss√©e au participant au concours. </p><br><p>  En ce qui concerne les id√©es, je voudrais remercier OpenAI pour son ouverture et individuellement John Schulman pour les conseils, id√©es et suggestions qu'il a exprim√©s au tout d√©but de ce concours.  Nous, comme de nombreux participants (et plus encore les nouveaux arrivants dans le monde RL), cela nous a permis de mieux nous concentrer sur l'objectif principal de la comp√©tition - le m√©ta-apprentissage et l'am√©lioration de la g√©n√©ralisation des agents, dont nous parlerons maintenant. </p><br><h2 id="osobennosti-ocenivaniya-resheniy">  Caract√©ristiques de l'√©valuation des d√©cisions </h2><br><p>  La chose la plus int√©ressante a commenc√© au moment de l'√©valuation des agents.  Dans les comp√©titions / benchmarks RL typiques, les algorithmes sont test√©s dans le m√™me environnement o√π ils ont √©t√© form√©s, ce qui contribue √† des algorithmes qui se souviennent bien et qui ont de nombreux hyperparam√®tres.  Dans le m√™me concours, le test de l'algorithme a √©t√© r√©alis√© aux nouveaux niveaux Sonic (qui n'ont jamais √©t√© montr√©s √† personne), d√©velopp√©s par l'√©quipe OpenAI sp√©cifiquement pour ce concours.  La cerise sur le g√¢teau √©tait le fait que dans le processus de test, l'agent a √©galement re√ßu une r√©compense lors du passage du niveau, ce qui a permis de se recycler directement dans le processus de test.  Cependant, dans ce cas, il valait la peine de se rappeler que les tests √©taient limit√©s √† la fois dans le temps - 24 heures et dans les ticks de jeu - 1 million.  Dans le m√™me temps, OpenAI a fortement soutenu la cr√©ation d'agents qui pourraient rapidement se recycler √† de nouveaux niveaux.  Comme d√©j√† mentionn√©, l'obtention et l'√©tude de telles solutions √©taient le principal objectif d'OpenAI lors de ce concours. </p><br><p>  Dans le milieu universitaire, la direction de l'√©tude des politiques qui peuvent s'adapter rapidement √† de nouvelles conditions est appel√©e m√©ta-apprentissage et, ces derni√®res ann√©es, elle s'est activement d√©velopp√©e. </p><br><p>  De plus, contrairement aux comp√©titions kaggle habituelles, o√π toute la soumission se r√©sume √† l'envoi de votre fichier de r√©ponses, dans cette comp√©tition (et en effet dans les comp√©titions RL), l'√©quipe devait emballer sa solution dans un conteneur docker avec l'API donn√©e, la collecter et l'envoyer image de docker.  Cela a augment√© le seuil d'entr√©e dans la comp√©tition, mais a rendu le processus de d√©cision beaucoup plus honn√™te - les ressources et le temps pour l'image de docker √©taient respectivement limit√©s, des algorithmes trop lourds et / ou lents n'ont tout simplement pas r√©ussi la s√©lection.  Il me semble que cette approche de l'√©valuation est beaucoup plus pr√©f√©rable, car elle permet aux chercheurs sans ¬´cluster maison de DGX et AWS¬ª de rivaliser avec les amateurs de mod√®les en verre 50000.  J'esp√®re voir plus de ce genre de comp√©tition √† l'avenir. </p><br><h2 id="komanda">  L'√©quipe </h2><br><p>  Kolesnikov Sergey ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">scitateur</a> ) <br>  Passionn√© de RL.  Au moment du concours, un √©tudiant de l'Institut de physique et de technologie de Moscou, MIPT, a √©crit et d√©fendu un dipl√¥me du concours NIPS: Learning to Run de l'ann√©e derni√®re (un article sur lequel devrait √©galement √™tre √©crit). <br>  Senior Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dbrain</a> - Nous proposons des concours pr√™ts √† la production avec docker et des ressources limit√©es dans le monde r√©el. </p><br><p>  Pavlov Mikhail ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">fgvbrt</a> ) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©veloppeur</a> principal de recherche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DiphakLab</a> .  A particip√© √† plusieurs reprises et remport√© des prix dans des hackathons et des comp√©titions d'entra√Ænement renforc√©es. </p><br><p>  Sergeev Ilya ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sergeevii123</a> ) <br>  Passionn√© de RL.  J'ai frapp√© l'un des hackathons RL de Deephack et tout a commenc√©.  Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Avito.ru</a> - vision par ordinateur pour divers projets internes. </p><br><p>  Sorokin Ivan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1ytique</a> ) <br>  Engag√© dans la reconnaissance vocale dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">speechpro.ru</a> . </p><br><h2 id="podhody-i-reshenie">  Approches et solutions </h2><br><p>  Apr√®s avoir test√© rapidement les bases de r√©f√©rence propos√©es, notre choix s'est port√© sur l'approche OpenAI - PPO, comme une option plus form√©e et int√©ressante pour d√©velopper notre solution.  De plus, √† en juger par leur article pour ce concours, l'agent PPO a fait face √† la t√¢che un peu mieux.  √Ä partir du m√™me article, les premi√®res am√©liorations que nous avons utilis√©es dans notre solution sont n√©es, mais d'abord: </p><br><ol><li><p>  Formation PPO collaborative √† tous les niveaux disponibles </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/oc/lx/iaoclxlyzkgmdgiqkkksiwzcoli.png" alt="image"></div><br><p>  La ligne de base pr√©sent√©e a √©t√© form√©e √† un seul des 27 niveaux Sonic disponibles.  Cependant, √† l'aide de petites modifications, il a √©t√© possible de parall√©liser la formation √† la fois √† tous les 27 niveaux.  En raison de la plus grande diversit√© dans la formation, l'agent r√©sultant avait une g√©n√©ralisation beaucoup plus grande et une meilleure compr√©hension de l'appareil du monde sonique, et a donc mieux g√©r√© un ordre de grandeur. </p><br></li><li><p>  Formation suppl√©mentaire pendant les tests <br>  Revenant √† l'id√©e principale de la comp√©tition, le m√©ta-apprentissage, il fallait trouver une approche qui aurait la g√©n√©ralisation maximale et pourrait facilement s'adapter √† de nouveaux environnements.  Et pour l'adaptation, il a fallu recycler l'agent existant pour l'environnement de test, ce qui a en fait √©t√© fait (√† chaque niveau de test, l'agent a pris 1 million de pas, ce qui a suffi pour s'adapter √† un niveau sp√©cifique).  √Ä la fin de chacun des jeux de test, l'agent a √©valu√© le prix re√ßu et optimis√© sa politique en utilisant l'histoire qui vient d'√™tre re√ßue.  Il est important de noter ici qu'avec cette approche, il est important de ne pas oublier toute votre exp√©rience ant√©rieure et de ne pas se d√©grader dans des conditions sp√©cifiques, ce qui, en substance, est le principal int√©r√™t du m√©ta-apprentissage, car un tel agent perd imm√©diatement toute sa capacit√© existante √† g√©n√©raliser. </p><br></li><li><p>  Bonus d'exploration <br>  En approfondissant les conditions de r√©mun√©ration d'un niveau, l'agent a re√ßu une r√©compense pour avoir progress√© le long de la coordonn√©e x, respectivement, il pouvait rester bloqu√© √† certains niveaux, lorsque vous deviez d'abord avancer puis reculer.  Il a √©t√© d√©cid√© de faire un ajout √† la r√©compense de l'agent, la soi-disant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exploration bas√©e sur le nombre</a> , lorsque l'agent a re√ßu une petite r√©compense s'il √©tait dans un √©tat o√π il n'√©tait pas encore.  Deux types de bonus d'exploration ont √©t√© mis en ≈ìuvre: bas√© sur l'image et bas√© sur la coordonn√©e x de l'agent.  Une r√©compense bas√©e sur une image a √©t√© calcul√©e comme suit: pour chaque emplacement de pixel dans l'image, il a √©t√© compt√© combien de fois chaque valeur s'est produite pour un √©pisode, la r√©compense √©tait inversement proportionnelle au produit pour tous les emplacements de pixel et combien de fois les valeurs de ces emplacements se sont rencontr√©es pour un √©pisode.  La r√©compense bas√©e sur la coordonn√©e x a √©t√© consid√©r√©e de la m√™me mani√®re: pour chaque coordonn√©e x (avec une certaine pr√©cision), il a √©t√© compt√© combien de fois l'agent √©tait dans cette coordonn√©e pour l'√©pisode, la r√©compense est inversement proportionnelle √† ce montant pour la coordonn√©e x actuelle. </p><br></li><li><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Exp√©riences mixtes</a> <br>  Dans ¬´l'enseignement avec un enseignant¬ª r√©cemment, une m√©thode simple mais efficace d'augmentation des donn√©es,  m√©lange.  L'id√©e est tr√®s simple: l'ajout de deux images d'entr√©e arbitraires est effectu√© et une somme pond√©r√©e des √©tiquettes correspondantes est affect√©e √† cette nouvelle image (par exemple, 0,7 <em>chien + 0,3</em> chat).  Dans des t√¢ches telles que la classification d'images et la reconnaissance vocale, le mixage montre de bons r√©sultats.  Par cons√©quent, il √©tait int√©ressant de tester cette m√©thode pour RL.  L'augmentation a √©t√© effectu√©e dans chaque gros lot, compos√© de plusieurs √©pisodes.  Les images d'entr√©e √©taient m√©lang√©es en pixels, mais avec des balises, tout n'√©tait pas si simple.  Les valeurs return, values ‚Äã‚Äãet neglogpacs ont √©t√© m√©lang√©es par une somme pond√©r√©e, mais l'action (actions) a √©t√© choisie dans l'exemple avec le coefficient maximum.  Une telle solution n'a pas montr√© d'augmentation tangible (bien qu'il semblerait qu'il aurait d√ª y avoir une augmentation de la g√©n√©ralisation), mais elle n'a pas aggrav√© la ligne de base.  Les graphiques ci-dessous comparent l'algorithme PPO avec mixup (rouge) et sans mixup (bleu): en haut se trouve la r√©compense pendant l'entra√Ænement, en bas se trouve la dur√©e de l'√©pisode. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bk/h-/jb/bkh-jbhhzowcl3gpazkbn8mun2a.jpeg" alt="image"></div><br></li><li><p>  S√©lection de la meilleure politique initiale <br>  Cette am√©lioration a √©t√© l'une des derni√®res et a apport√© une contribution tr√®s significative au r√©sultat final.  Au niveau de la formation, plusieurs politiques diff√©rentes avec diff√©rents hyperparam√®tres ont √©t√© form√©es.  Au niveau du test, pour les premiers √©pisodes, chacun d'eux a √©t√© test√©, et pour une formation compl√©mentaire, la politique qui a donn√© la r√©compense maximale au test pour son √©pisode a √©t√© choisie. </p><br></li></ol><br><h2 id="bloopers">  Bloopers </h2><br><p>  Et maintenant sur la question de ce qui a √©t√© essay√©, mais "n'a pas vol√©."  Apr√®s tout, ce n'est pas un nouvel article SOTA pour cacher quelque chose. </p><br><ol><li>  Modification de l'architecture du r√©seau: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">activation SELU</a> , auto-attention, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">blocs SE</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Neuro√©volution</a> </li><li>  Cr√©ation de vos propres niveaux Sonic - tout √©tait pr√©par√©, mais il n'y avait pas assez de temps </li><li>  M√©ta-formation via <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MAML</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">REPTILE</a> </li><li>  Ensemble de plusieurs mod√®les et formation continue lors du test de chaque mod√®le √† l'aide d'un √©chantillonnage d'importance </li></ol><br><h2 id="itogi">  R√©sum√© </h2><br><p>  3 semaines apr√®s la fin de la comp√©tition, OpenAI a publi√© les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©sultats</a> .  √Ä 11 niveaux suppl√©mentaires suppl√©mentaires, notre √©quipe a re√ßu une honorable 4√®me place, apr√®s avoir saut√© du 8√®me lors d'un test public et d√©pass√© les lignes de base obscures d'OpenAI. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/om/iw/yvomiwj1jmffvmlicjmhe7vjp9y.png" alt="image"></div><br><p>  Les principaux points distinctifs qui ont ¬´vol√©¬ª dans le premier 3ki: </p><br><ol><li>  Syst√®me d'actions am√©lior√© (est venu avec leurs propres boutons, supprim√© les suppl√©mentaires); </li><li>  Investigation des √©tats par hachage √† partir de l'image d'entr√©e; </li><li>  Plus de niveaux de formation; </li></ol><br><p>  De plus, je tiens √† noter que dans ce concours, en plus de gagner, la description de leurs d√©cisions, ainsi que le mat√©riel qui a aid√© les autres participants ont √©t√© activement encourag√©s - il y avait aussi une nomination s√©par√©e pour cela.  Ce qui, encore une fois, a augment√© le concours de lampes. </p><br><h2 id="posleslovie">  Postface </h2><br><p>  Personnellement, j'ai vraiment aim√© ce concours, ainsi que le th√®me du m√©ta apprentissage.  Pendant la participation, j'ai pris connaissance d'une longue liste d'articles (je n'ai m√™me <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pas oubli√©</a> certains d'entre eux) et j'ai appris un grand nombre d'approches diff√©rentes que j'esp√®re appliquer √† l'avenir. </p><br><p>  Dans la meilleure tradition de participation au concours, tout le code est disponible et affich√© sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr421585/">https://habr.com/ru/post/fr421585/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr421573/index.html">Food Design Digest ao√ªt 2018</a></li>
<li><a href="../fr421575/index.html">La confrontation entre Yandex et Roskomnadzor se pr√©pare, dans une journ√©e le moteur de recherche pourrait √™tre partiellement bloqu√©</a></li>
<li><a href="../fr421577/index.html">Un exploit pour une vuln√©rabilit√© non divulgu√©e dans le Planificateur de t√¢ches Windows est publi√© (traduction)</a></li>
<li><a href="../fr421579/index.html">Organisation d'une interaction efficace des microservices</a></li>
<li><a href="../fr421583/index.html">O√π aller au coll√®ge pour √©tudier pour un informaticien? + enqu√™te</a></li>
<li><a href="../fr421587/index.html">[Iekaterinbourg, annonce] Java Mitap - JUG.EKB</a></li>
<li><a href="../fr421589/index.html">M√©tamorphoses: programmation mol√©culaire de la forme</a></li>
<li><a href="../fr421591/index.html">Syst√®me de budget pour la vid√©osurveillance autonome (sans fil) sans fil (Wi-Fi)</a></li>
<li><a href="../fr421593/index.html">SandboxEscaper / PoC-LPE: que contient-il?</a></li>
<li><a href="../fr421595/index.html">Comment les informaticiens trouvent un emploi aux √âtats-Unis et dans l'UE: les 9 meilleures ressources</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>