<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§µüèº üßíüèæ üë©‚Äçüåæ R√©servation Kubernetes: elle existe üßôüèæ ‚õ∞Ô∏è üë©üèº‚Äçü§ù‚Äçüë®üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Je m'appelle Sergey, je viens d'ITSumma et je veux vous dire comment nous abordons la r√©servation √† Kubernetes. R√©cemment, j'ai effectu√© de nombreux t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©servation Kubernetes: elle existe</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/452078/">  Je m'appelle Sergey, je viens d'ITSumma et je veux vous dire comment nous abordons la r√©servation √† Kubernetes.  R√©cemment, j'ai effectu√© de nombreux travaux de conseil sur la mise en ≈ìuvre d'une vari√©t√© de solutions devops pour diff√©rentes √©quipes, et, en particulier, je travaille en √©troite collaboration sur des projets utilisant des K8.  Lors de la conf√©rence Uptime day 4, d√©di√©e √† la redondance dans les architectures complexes, j'ai fait une pr√©sentation sur le cube redondant, et voici sa nouvelle version gratuite.  Je ne pr√©viendrai qu'√† l'avance qu'il n'est pas un guide direct de l'action, mais plut√¥t une g√©n√©ralisation des r√©flexions sur ce sujet. <br><br><img src="https://habrastorage.org/webt/fi/pj/ox/fipjoxmwx-hrd0tvm0_bvgjaa-e.jpeg"><br><br>  En principe, le suivi et la redondance sont les deux principaux outils pour augmenter la r√©silience de tout projet.  Mais dans le cuber, tout est √©quilibr√© par lui-m√™me, dites-vous, tout est mis √† l'√©chelle par lui-m√™me, et si quelque chose se produit, il augmentera de lui-m√™me ... ? "  Beaucoup de gens pensent qu'un cuber est une chose tellement magique qui √©limine tous les probl√®mes d'infrastructure et fait que le projet ne tombe jamais.  Mais ... le monde n'est pas ce qu'il semble. <br><a name="habracut"></a><br>  Comment avons-nous abord√© le processus de sauvegarde auparavant?  Nous avions des plates-formes de placement identiques - soit des machines virtuelles, soit des serveurs de fer, auxquels nous avons appliqu√© trois pratiques de base: <br><br><ol><li>  synchronisation de code et statique </li><li>  synchronisation de la configuration </li><li>  r√©plication de base de donn√©es </li></ol><br>  Et le tour est jou√©: √† tout moment on passe au site de r√©serve, tout le monde est content, on se l√®ve, on n'est pas d'accord. <br><br><img src="https://habrastorage.org/webt/mw/ym/zw/mwymzwrfdl9vsaf_hg-wm3wvphm.jpeg"><br><br>  Et que nous proposent-ils pour augmenter la disponibilit√© constante de notre application kubernetes?  La premi√®re chose que dit la documentation non officielle est de mettre beaucoup de machines, de faire beaucoup de ma√Ætres - leur nombre doit satisfaire les conditions pour atteindre un quorum au sein du cluster, et pour que etcd, api, MC, le planificateur soit √©lev√© sur chacun des ma√Ætres ... Et, il semble, tout va bien : lorsque plusieurs n≈ìuds ou ma√Ætres actifs √©chouent, notre cluster est r√©√©quilibr√© et l'application continue de fonctionner.  Ressemble √† nouveau √† la magie!  Mais souvent, notre cluster est situ√© dans le m√™me centre de donn√©es, ce qui peut poser certaines questions.  Et si une excavatrice arrivait et d√©tenait un c√¢ble, la foudre frappait, il y avait une inondation universelle?  Tout est couvert, notre cluster n'est plus.  Comment aborder la r√©servation en tenant compte de ce c√¥t√© du probl√®me? <br><br>  Tout d'abord, vous devez avoir un autre cluster dans la r√©serve chaude, c'est-√†-dire un cluster auquel vous pouvez basculer √† tout moment.  En m√™me temps, du point de vue du cuber, les infrastructures doivent √™tre compl√®tement identiques.  Autrement dit, s'il existe des plugins non standard pour travailler avec le syst√®me de fichiers, des solutions personnalis√©es pour l'entr√©e, ils devraient √™tre compl√®tement identiques sur vos deux (ou trois, ou dix, il y a assez d'argent et de force d'administrateur).  Il est n√©cessaire de d√©finir clairement deux ensembles d'applications (deployment'ov, statefulset'ov, daemonset'ov, cronjob'ov, etc.): lesquels d'entre eux peuvent fonctionner en permanence sur une r√©serve, et lesquels sont pr√©f√©rables de ne pas d√©marrer avant la commutation directe. <br><br>  Notre cluster de sauvegarde doit-il donc √™tre compl√®tement identique √† notre cluster de combat?  Non.  Auparavant, dans le cadre du travail avec des projets monolithiques, avec des infrastructures en fer, nous avons gard√© un environnement presque compl√®tement identique, mais dans le cadre du cuber, je pense que cela ne devrait pas √™tre le cas.  Voyons pourquoi. <br><br>  Par exemple, commen√ßons par les entit√©s de base de kubernetes - les d√©ploiements - elles doivent √™tre identiques.  Des applications devraient √™tre lanc√©es qui peuvent intercepter le traitement du trafic √† tout moment et permettre √† notre projet de continuer √† vivre.  Si nous parlons de fichiers de configuration, nous devons voir ici s'ils doivent √™tre identiques ou non.  Autrement dit, si nous, les gens intelligents, n'utilisons aucune substance interdite et ne gardons pas la base dans les K8, alors dans les cartes de configuration, nous devrions avoir des param√®tres d'acc√®s √† la base de combat (dont le processus de sauvegarde est construit s√©par√©ment).  Par cons√©quent, pour garantir l'acc√®s √† l'instance de base de donn√©es de sauvegarde, nous devons disposer d'un fichier de configuration distinct (configmap).  Exactement de la m√™me mani√®re que nous travaillons avec les secrets: mots de passe pour acc√©der √† la base de donn√©es, cl√©s api;  √† tout moment, un secret de combat ou une r√©serve, on peut travailler avec nous.  Au total, nous avons d√©j√† deux entit√©s kubernetes dont les versions de sauvegarde ne doivent pas √™tre identiques √† celles de combat.  La prochaine entit√© qui m√©rite d'√™tre √©tudi√©e est cronjob.  Les cronjobs en r√©serve ne doivent en aucun cas √™tre identiques √† l'ensemble des clusters de production de cronjobs!  Si nous augmentons le cluster de sauvegarde et l'√©l√©vons compl√®tement avec tous les cronjob activ√©s, alors, par exemple, les gens recevront deux lettres de vous en m√™me temps au lieu d'une.  Ou une sorte de synchronisation des donn√©es avec des sources externes aura lieu deux fois, respectivement, nous commen√ßons √† nous blesser, √† pleurer, √† crier et √† jurer. <br><br><img src="https://habrastorage.org/webt/m3/n2/yx/m3n2yxmvocsvsfiavdmevq4vx18.jpeg"><br><br>  Mais comment les internautes nous proposent-ils d'organiser un cluster de sauvegarde?  La deuxi√®me r√©ponse la plus populaire apr√®s ¬´pourquoi?¬ª  - utilisation de la F√©d√©ration Kubernetes. <br><br>  Qu'est ce que c'est  Il s'agit, disons, d'un grand m√©ta-cluster.  Si nous imaginons l'architecture du cuber - o√π nous avons un ma√Ætre, plusieurs n≈ìuds - alors du point de vue de la f√©d√©ration, nous avons aussi un ma√Ætre et plusieurs n≈ìuds, seul chaque n≈ìud est un cluster s√©par√©.  Autrement dit, nous travaillons avec les m√™mes entit√©s, avec les m√™mes primitives qu'avec un seul cuber, mais nous tournons et tournons non pas nos machines physiques, mais des grappes enti√®res.  Dans le cadre de la f√©d√©ration, nous sommes en totale synchronisation des ressources f√©d√©rales des parents aux descendants.  Par exemple, si nous avons lanc√© un d√©ploiement via la f√©d√©ration, il sera d√©ploy√© sur chacun de nos clusters subsidiaires.  Si nous prenons une configmap, le secret est de la faire rouler vers la f√©d√©ration - elle se r√©pandra dans tous nos clusters enfants;  en m√™me temps, la f√©d√©ration nous permet de personnaliser nos ressources pour les enfants.  Autrement dit, nous avons pris une configmap, l'avons d√©ploy√©e via la f√©d√©ration, puis, si nous devons corriger quelque chose sur des clusters sp√©cifiques, nous allons √©diter sur un cluster s√©par√©, et cette modification ne sera synchronis√©e nulle part. <br><br>  La F√©d√©ration Kubernetes n'est pas si longtemps un outil existant et ne prend pas en charge l'ensemble des ressources que K8 fournit lui-m√™me: au moment de la publication de l'une des premi√®res versions de la documentation, il s'agissait de ne prendre en charge que les cartes de configuration, le d√©ploiement d'un jeu de r√©pliques, l'entr√©e.  Les secrets n'√©taient pas pris en charge, le travail avec le volume n'√©tait pas non plus pris en charge.  Ensemble trop limit√©.  Surtout si nous aimons nous amuser, par exemple, gr√¢ce √† la d√©finition de ressource personnalis√©e pour transf√©rer nos propres ressources vers les kubernetes, nous ne les poussons pas dans la f√©d√©ration.  C'est, pour ainsi dire ... une d√©cision tr√®s similaire √† la v√©rit√©, mais elle nous fait p√©riodiquement nous tirer une balle dans le pied.  D'autre part, la f√©d√©ration vous permet de g√©rer de mani√®re flexible notre jeu de r√©pliques.  Par exemple, nous voulons que 10 r√©pliques de notre application soient lanc√©es, par d√©faut, la f√©d√©ration divisera ce nombre proportionnellement entre le nombre de clusters.  Et tout cela peut √©galement √™tre configur√©!  Autrement dit, vous pouvez sp√©cifier que vous devez conserver 6 r√©pliques de notre application sur le cluster de combat, et seulement 4 r√©pliques de notre application sur le cluster de sauvegarde, pour √©conomiser des ressources ou pour votre propre divertissement.  Ce qui est √©galement assez pratique.  Mais avec la f√©d√©ration, nous devons utiliser de nouvelles solutions, terminer quelque chose en cours et nous forcer √† r√©fl√©chir un peu plus ... <br><br>  Est-il possible d'aborder le processus de r√©servation d'un cuber en quelque sorte plus simplement?  Quels outils avons-nous du tout? <br><br>  Premi√®rement, nous avons toujours une sorte de syst√®me ci / cd, c'est-√†-dire que nous ne faisons pas le tour manuellement, n'√©crivons pas cr√©er / appliquer sur les serveurs.  Le syst√®me g√©n√®re des yaml'ics pour nos conteneurs. <br><br>  Deuxi√®mement, il existe plusieurs clusters, nous avons un ou plusieurs registres (si nous sommes intelligents), que nous avons √©galement pris et r√©serv√©s.  Et il existe un merveilleux utilitaire kubectl qui peut fonctionner avec plusieurs clusters simultan√©ment. <br><br><img src="https://habrastorage.org/webt/vs/ib/ed/vsibeda9uobcn8jtrfztjkln3ag.jpeg"><br><br>  Donc: √† mon avis, la solution la plus simple et la plus correcte pour cr√©er un cluster de sauvegarde est un d√©ploiement parall√®le primitif.  Il existe une sorte de pipeline dans le syst√®me ci / cd;  nous construisons d'abord nos conteneurs, testons et d√©ployons des applications via kubectl vers plusieurs clusters ind√©pendants.  Nous pouvons construire des calculs simultan√©s sur plusieurs clusters.  En cons√©quence, nous d√©cidons √©galement de livrer des configurations √† ce stade.  Vous pouvez pr√©d√©finir l'ensemble de configurations pour notre cluster de combat, l'ensemble de configurations pour le cluster de sauvegarde et au niveau ci / cd du syst√®me rouler le pro-environnement vers le pro-cluster, l'environnement de sauvegarde vers le cluster de sauvegarde.  Par rapport √† la f√©d√©ration, vous n'avez pas besoin de d√©finir une ressource f√©d√©rale pour chaque cluster enfant et de red√©finir quelque chose.  Nous l'avons fait √† l'avance.  Quels bons amis nous sommes. <br><br>  Mais ... il y a ... j'ai √©crit, il y a la "racine de tout mal", mais il y en a en fait deux.  Le premier est le syst√®me de fichiers.  Il existe une sorte de PV, ou nous utilisons un stockage externe.  Si nous stockons des fichiers √† l'int√©rieur du cluster, nous devons suivre les anciennes pratiques laiss√©es par les infrastructures de fer: par exemple, synchroniser avec lsync.  Eh bien, ou toute autre b√©quille que vous pr√©f√©rez personnellement.  Nous roulons tout sur d'autres voitures et vivons. <br><br>  Deuxi√®mement, et, en fait, une pierre d'achoppement encore plus importante est la base de donn√©es.  Si nous sommes des gens intelligents et que nous ne gardons pas la base de donn√©es dans le cube, le processus de sauvegarde des donn√©es selon le m√™me ancien sch√©ma est la r√©plication ma√Ætre-esclave, puis la commutation, nous rattraperons la r√©plique et nous vivrons bien.  Mais si nous gardons notre base de donn√©es √† l'int√©rieur du cluster, il existe en principe de nombreuses solutions toutes faites pour organiser la m√™me r√©plique ma√Ætre-esclave, de nombreuses solutions pour √©lever la base de donn√©es √† l'int√©rieur du cube. <br>  Un milliard de rapports ont d√©j√† √©t√© lus sur les sauvegardes de bases de donn√©es, un milliard d'articles ont √©t√© √©crits, rien de nouveau n'est n√©cessaire ici, en fait.  En g√©n√©ral, suivez votre r√™ve, vivez comme vous le souhaitez, inventez aussi des b√©quilles compliqu√©es, mais pensez √† la fa√ßon dont vous r√©serverez tout cela. <br><br>  Et maintenant, comment en principe, nous aurons le processus de basculement vers le site de sauvegarde en cas d'incendie.  Tout d'abord, nous d√©ployons des applications sans √©tat en parall√®le.  Ils n'affectent pas la logique m√©tier de nos applications, notre projet, nous pouvons constamment garder deux ensembles d'applications en cours d'ex√©cution, et ils peuvent commencer √† recevoir du trafic.  Il est tr√®s important dans le processus de basculement vers le site de sauvegarde de voir d√©finitivement si les configurations doivent √™tre red√©finies  Par exemple, nous avons un cluster de vente kubernetes, un cluster de sauvegarde kubernetes, une base de donn√©es ma√Ætre externe et une base de donn√©es ma√Ætre de sauvegarde.  Nous avons quatre options sur la fa√ßon dont ces applications dans le prod peuvent commencer √† interagir les unes avec les autres.  Notre base peut changer, et il s'av√®re que nous devons transf√©rer le trafic vers la nouvelle base dans le cluster prod, ou nous pouvons obtenir le cluster et nous sommes pass√©s √† la r√©serve, mais nous continuons √† travailler avec la base pro, enfin, la troisi√®me option, quand nous l'avons , puis c'est foutu, et nous changeons les deux applications, red√©finissons notre configuration pour que les nouvelles applications fonctionnent d√©j√† avec la nouvelle base de donn√©es. <br><br>  Eh bien, en fait, quelles conclusions peut-on tirer de tout cela? <br><br><img src="https://habrastorage.org/webt/rl/81/0a/rl810a1jyyc78gkz0cop0gjzhma.jpeg"><br><br>  Premi√®re conclusion: bien vivre avec une r√©serve.  Mais cher.  Mais id√©alement, vivre avec plus d'une r√©serve.  Id√©alement, vous devez g√©n√©ralement vivre avec plusieurs r√©serves.  Premi√®rement, la r√©serve ne devrait pas √™tre au moins dans un DC, et deuxi√®mement, au moins, dans un autre h√©bergeur.  Cela arrivait souvent - et dans ma pratique, √ßa l'√©tait.  Malheureusement, je ne peux pas nommer les projets, juste au moment o√π il y a eu un incendie dans le centre de donn√©es ... Je suis comme √ßa: on passe √† la r√©serve!  Et les serveurs de sauvegarde dans le m√™me rack se tenaient ... <br><br>  Ou imaginez qu'Amazon a √©t√© interdit en Russie (et c'√©tait le cas).  Et tout: le sens du fait que dans une autre amazone se trouve notre r√©serve?  Il est √©galement indisponible.  Je r√©p√®te donc: nous gardons la r√©serve, au moins, dans un autre DC, et de pr√©f√©rence avec un autre h√¥te. <br><br>  Deuxi√®me conclusion: si votre application communique avec certaines sources externes (il peut s'agir d'une base de donn√©es ou d'une API externe), assurez-vous de la d√©finir comme un service avec un point de terminaison externe afin de ne pas avoir √† le r√©parer au moment du basculement 15 de vos applications qui frappent sur la m√™me base.  D√©finissez la base de donn√©es en tant que service distinct, frappez dessus comme si elle se trouvait √† l'int√©rieur de votre cluster: si vous avez une base de donn√©es, vous changez ip en un seul endroit et continuez √† vivre heureux. <br><br>  Et enfin: j'adore le ¬´cube¬ª, ainsi que ses exp√©riences.  J'aime aussi partager les r√©sultats de ces exp√©riences et, en g√©n√©ral, mon exp√©rience personnelle.  Par cons√©quent, j'ai enregistr√© une s√©rie de webinaires sur les K8, bien sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notre cha√Æne YouTube</a> pour plus de d√©tails. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr452078/">https://habr.com/ru/post/fr452078/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr452066/index.html">Excelsior JET arr√™te le d√©veloppement de son compilateur AOT apr√®s 18 ans de travail</a></li>
<li><a href="../fr452068/index.html">12. Check Point Getting Started R80.20. Journaux et rapports</a></li>
<li><a href="../fr452072/index.html">Nous impl√©mentons CircularRevealAnimation sur Flutter et publions simultan√©ment la biblioth√®que sur pub.dev</a></li>
<li><a href="../fr452074/index.html">Le premier jeu sur l'unit√© ou ce qu'il m'a fallu six mois</a></li>
<li><a href="../fr452076/index.html">Briser le navigateur UC</a></li>
<li><a href="../fr452082/index.html">Flux flexible des mises √† jour int√©gr√©es: acc√©l√©rez le processus de mise √† jour des applications sur Android</a></li>
<li><a href="../fr452086/index.html">Ce qui est dans mon pixel pour vous: cr√©er des nanopixels en utilisant des m√©tasurfaces plasmon</a></li>
<li><a href="../fr452088/index.html">Reconnaissance routi√®re par segmentation s√©mantique</a></li>
<li><a href="../fr452090/index.html">Cr√©ation d'un g√©n√©rateur de puzzle proc√©dural</a></li>
<li><a href="../fr452092/index.html">Mises √† jour int√©gr√©es √† l'application: acc√©l√©ration des mises √† jour des applications Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>