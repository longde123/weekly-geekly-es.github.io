<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî• üèÅ üë¥üèª R√©seaux de neurones et apprentissage profond, chapitre 3, partie 2: pourquoi la r√©gularisation contribue-t-elle √† r√©duire le recyclage? ‚úäüèø üöî üåó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Table des mati√®res 

- Chapitre 1: utiliser les r√©seaux de neurones pour reconna√Ætre les nombres manuscrits 
- Chapitre 2: comment fonctionne l'algori...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©seaux de neurones et apprentissage profond, chapitre 3, partie 2: pourquoi la r√©gularisation contribue-t-elle √† r√©duire le recyclage?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/459816/"><div class="spoiler">  <b class="spoiler_title">Table des mati√®res</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 1: utiliser les r√©seaux de neurones pour reconna√Ætre les nombres manuscrits</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 2: comment fonctionne l'algorithme de r√©tropropagation</a> </li><li>  Chapitre 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1: am√©liorer la m√©thode de formation des r√©seaux de neurones</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: Pourquoi la r√©gularisation contribue-t-elle √† r√©duire le recyclage?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 3: comment choisir les hyperparam√®tres de r√©seau neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 4: preuve visuelle que les r√©seaux de neurones sont capables de calculer n'importe quelle fonction</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 5: Pourquoi les r√©seaux de neurones profonds sont-ils si difficiles √† former?</a> </li><li>  Chapitre 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: progr√®s r√©cents dans la reconnaissance d'images</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Postface: existe-t-il un algorithme simple pour cr√©er de l'intelligence?</a> </li></ul></div></div><br>  Empiriquement, nous avons vu que la r√©gularisation permet de r√©duire la reconversion.  C'est inspirant - mais, malheureusement, il n'est pas √©vident pourquoi la r√©gularisation aide.  Habituellement, les gens l'expliquent d'une certaine mani√®re: dans un sens, les poids plus petits ont moins de complexit√©, ce qui fournit une explication plus simple et plus efficace des donn√©es, donc elles devraient √™tre pr√©f√©r√©es.  Cependant, cette explication est trop courte et certaines parties peuvent sembler douteuses ou myst√©rieuses.  D√©plions cette histoire et examinons-la d'un ≈ìil critique.  Pour ce faire, supposons que nous ayons un ensemble de donn√©es simple pour lequel nous voulons cr√©er un mod√®le: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0f/h2/4p/0fh24p1sl8wmgoov1ewqmqbv900.png"></div><a name="habracut"></a><br>  En termes de signification, nous √©tudions ici le ph√©nom√®ne du monde r√©el, et x et y d√©signent des donn√©es r√©elles.  Notre objectif est de construire un mod√®le qui nous permette de pr√©dire y en fonction de x.  Nous pourrions essayer d'utiliser un r√©seau de neurones pour cr√©er un tel mod√®le, mais je sugg√®re quelque chose de plus simple: je vais essayer de mod√©liser y comme un polyn√¥me en x.  Je le ferai √† la place des r√©seaux de neurones, car l'utilisation de polyn√¥mes rend l'explication particuli√®rement claire.  D√®s que nous traiterons du cas du polyn√¥me, nous passerons √† l'Assembl√©e nationale.  Il y a dix points sur le graphique ci-dessus, ce qui signifie que nous pouvons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">trouver un polyn√¥me unique du</a> 9√®me ordre y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... + a <sub>9</sub> qui correspond exactement aux donn√©es.  Et voici le graphique de ce polyn√¥me. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o6/lj/6j/o6lj6j82dn6rly8xnv-fmzvvwn0.png"></div><br>  Coup parfait.  Mais nous pouvons obtenir une bonne approximation en utilisant le mod√®le lin√©aire y = 2x <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/qk/sq/-3qksq4rmlcq54obwxcxy4_dvtm.png"></div><br>  Lequel est le meilleur?  Quel est le plus susceptible d'√™tre vrai?  Lequel sera mieux g√©n√©ralis√© √† d'autres exemples du m√™me ph√©nom√®ne du monde r√©el? <br><br>  Des questions difficiles.  Et on ne peut y r√©pondre exactement sans informations suppl√©mentaires sur le ph√©nom√®ne sous-jacent du monde r√©el.  Cependant, regardons deux possibilit√©s: (1) un mod√®le avec un polyn√¥me du 9√®me ordre d√©crit vraiment le ph√©nom√®ne du monde r√©el, et donc, se g√©n√©ralise parfaitement;  (2) le mod√®le correct est y = 2x, mais nous avons un bruit suppl√©mentaire associ√© √† l'erreur de mesure, donc le mod√®le ne s'adapte pas parfaitement. <br><br>  A priori, on ne peut dire laquelle des deux possibilit√©s est correcte (ou qu'il n'y en a pas de troisi√®me).  Logiquement, n'importe lequel d'entre eux peut s'av√©rer vrai.  Et la diff√©rence entre eux n'est pas anodine.  Oui, sur la base des donn√©es disponibles, on peut dire qu'il n'y a qu'une l√©g√®re diff√©rence entre les mod√®les.  Mais supposons que nous voulons pr√©dire la valeur de y correspondant √† une grande valeur de x, beaucoup plus grande que n'importe laquelle de celles montr√©es dans le graphique.  Si nous essayons de le faire, une √©norme diff√©rence appara√Ætra entre les pr√©dictions des deux mod√®les, car le terme x <sup>9</sup> domine dans le polyn√¥me du 9√®me ordre, et le mod√®le lin√©aire reste lin√©aire. <br><br>  Un point de vue sur ce qui se passe est de d√©clarer qu'une explication plus simple devrait √™tre utilis√©e en science, si possible.  Lorsque nous trouvons un mod√®le simple qui explique de nombreux points de r√©f√©rence, nous voulons juste crier: "Eur√™ka!"  Apr√®s tout, il est peu probable qu'une explication simple apparaisse purement par accident.  Nous pensons que le mod√®le devrait produire une certaine v√©rit√© associ√©e au ph√©nom√®ne.  Dans ce cas, le mod√®le y = 2x + bruit semble beaucoup plus simple que y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... Il serait surprenant que la simplicit√© apparaisse par hasard, nous soup√ßonnons donc que le bruit y = 2x + exprime certains v√©rit√© sous-jacente.  De ce point de vue, le mod√®le du 9e ordre √©tudie simplement l'effet du bruit local.  Bien que le mod√®le du 9e ordre fonctionne parfaitement pour ces points de r√©f√©rence sp√©cifiques, il ne peut pas se g√©n√©raliser √† d'autres points, ce qui permet au mod√®le lin√©aire avec bruit d'avoir de meilleures capacit√©s de pr√©diction. <br><br>  Voyons ce que ce point de vue signifie pour les r√©seaux de neurones.  Supposons que, dans notre r√©seau, il y ait principalement des poids faibles, comme c'est g√©n√©ralement le cas dans les r√©seaux r√©gularis√©s.  En raison de son faible poids, le comportement du r√©seau ne change pas beaucoup lorsque plusieurs entr√©es al√©atoires sont modifi√©es ici et l√†.  En cons√©quence, le r√©seau r√©gularis√© est difficile √† apprendre les effets du bruit local pr√©sent dans les donn√©es.  Cela est similaire √† la volont√© de s'assurer que les preuves individuelles n'affectent pas consid√©rablement la sortie du r√©seau dans son ensemble.  Au lieu de cela, le r√©seau r√©gularis√© est form√© pour r√©pondre aux preuves que l'on trouve souvent dans les donn√©es de formation.  Inversement, un r√©seau avec des poids importants peut changer son comportement assez fortement en r√©ponse √† de petits changements dans les donn√©es d'entr√©e.  Par cons√©quent, un r√©seau irr√©gulier peut utiliser des poids importants pour former un mod√®le complexe qui contient beaucoup d'informations sur le bruit dans les donn√©es d'entra√Ænement.  En bref, les limites des r√©seaux r√©gularis√©s leur permettent de cr√©er des mod√®les relativement simples bas√©s sur des mod√®les que l'on trouve souvent dans les donn√©es d'entra√Ænement, et ils r√©sistent aux √©carts caus√©s par le bruit dans les donn√©es d'entra√Ænement.  On esp√®re que cela permettra √† nos r√©seaux d'√©tudier le ph√©nom√®ne lui-m√™me et de mieux g√©n√©raliser les connaissances acquises. <br><br>  Cela dit, l'id√©e de privil√©gier des explications plus simples devrait vous rendre nerveux.  Parfois, les gens appellent cette id√©e ¬´le rasoir d'Occam¬ª et l'appliquent avec z√®le, comme si elle avait le statut d'un principe scientifique g√©n√©ral.  Mais ce n'est bien s√ªr pas un principe scientifique g√©n√©ral.  Il n'y a aucune raison a priori logique de pr√©f√©rer des explications simples √† des explications complexes.  Parfois, une explication plus compliqu√©e est correcte. <br><br>  Permettez-moi de d√©crire deux exemples de la fa√ßon dont une explication plus complexe s'est av√©r√©e correcte.  Dans les ann√©es 40, le physicien Marcel Shane a annonc√© la d√©couverte d'une nouvelle particule.  L'entreprise pour laquelle il travaillait, General Electric, √©tait ravie et a largement diffus√© la publication de cet √©v√©nement.  Cependant, le physicien Hans Bethe √©tait sceptique.  Bethe a visit√© Shane et a √©tudi√© les plaques avec des traces de la nouvelle particule de Shane.  Shane a montr√© Beta plaque apr√®s plaque, mais Bete a trouv√© sur chacun d'eux un probl√®me qui indiquait la n√©cessit√© de refuser ces donn√©es.  Enfin, Shane a montr√© √† Beta un record qui semblait en forme.  Bethe a dit que c'√©tait probablement juste une d√©viation statistique.  Shane: "Oui, mais les chances que cela soit d√ª aux statistiques, m√™me selon votre propre formule, sont de un sur cinq."  Bethe: "Cependant, j'ai d√©j√† regard√© cinq disques."  Enfin, Shane a d√©clar√©: "Mais vous avez expliqu√© chacun de mes enregistrements, chaque bonne image avec une autre th√©orie, et j'ai une hypoth√®se qui explique tous les enregistrements √† la fois, d'o√π il r√©sulte que nous parlons d'une nouvelle particule."  Bethe a r√©pondu: ¬´La seule diff√©rence entre mes explications et les v√¥tres est que les v√¥tres ont tort et les miennes sont correctes.  Votre seule explication est incorrecte et toutes mes explications sont correctes. ¬ª  Par la suite, il s'est av√©r√© que la nature √©tait d'accord avec Bethe, et la particule de Shane s'est √©vapor√©e. <br><br>  Dans le deuxi√®me exemple, en 1859, l‚Äôastronome Urbain Jean Joseph Le Verrier a d√©couvert que la forme de l‚Äôorbite de Mercure ne correspond pas √† la th√©orie de Newton de la gravitation universelle.  Il y avait un petit √©cart par rapport √† cette th√©orie, puis plusieurs options pour r√©soudre le probl√®me ont √©t√© propos√©es, ce qui se r√©sumait au fait que la th√©orie de Newton dans son ensemble est correcte et ne n√©cessite qu'un l√©ger changement.  Et en 1916, Einstein a montr√© que cette d√©viation peut √™tre bien expliqu√©e en utilisant sa th√©orie g√©n√©rale de la relativit√©, radicalement diff√©rente de la gravit√© newtonienne et bas√©e sur des math√©matiques beaucoup plus complexes.  Malgr√© cette complexit√© suppl√©mentaire, il est g√©n√©ralement admis aujourd'hui que l'explication d'Einstein est correcte, et la gravit√© newtonienne est incorrecte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">m√™me sous une forme modifi√©e</a> .  Cela se produit, en particulier, parce que nous savons aujourd'hui que la th√©orie d'Einstein explique de nombreux autres ph√©nom√®nes avec lesquels la th√©orie de Newton a eu des difficult√©s.  De plus, encore plus √©tonnant, la th√©orie d'Einstein pr√©dit avec pr√©cision plusieurs ph√©nom√®nes que la gravit√© newtonienne ne pr√©disait pas du tout.  Cependant, ces qualit√©s impressionnantes n'√©taient pas √©videntes dans le pass√©.  A en juger par la simple simplicit√©, une forme modifi√©e de la th√©orie newtonienne aurait alors sembl√© plus attrayante. <br><br>  Trois morales peuvent √™tre tir√©es de ces histoires.  Premi√®rement, il est parfois assez difficile de d√©cider laquelle des deux explications sera ¬´la plus facile¬ª.  Deuxi√®mement, m√™me si nous avons pris une telle d√©cision, la simplicit√© doit √™tre guid√©e avec beaucoup de prudence!  Troisi√®mement, le v√©ritable test du mod√®le n'est pas la simplicit√©, mais la qualit√© avec laquelle il pr√©dit de nouveaux ph√©nom√®nes dans de nouvelles conditions de comportement. <br><br>  Compte tenu de tout cela et en faisant attention, nous accepterons un fait empirique - les SN r√©gularis√©es sont g√©n√©ralement mieux g√©n√©ralis√©es que les SN irr√©guli√®res.  Par cons√©quent, plus tard dans le livre, nous utiliserons souvent la r√©gularisation.  Les histoires mentionn√©es ne sont n√©cessaires que pour expliquer pourquoi personne n'a encore d√©velopp√© une explication th√©orique compl√®tement convaincante de la raison pour laquelle la r√©gularisation aide les r√©seaux √† se g√©n√©raliser.  Les chercheurs continuent de publier des travaux o√π ils essaient d'essayer diff√©rentes approches de la r√©gularisation, de les comparer, d'examiner ce qui fonctionne le mieux et d'essayer de comprendre pourquoi diff√©rentes approches fonctionnent plus ou moins bien.  La r√©gularisation peut donc √™tre trait√©e comme un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nuage</a> .  Quand cela aide assez souvent, nous n'avons pas une compr√©hension syst√©mique compl√®tement satisfaisante de ce qui se passe - seulement des r√®gles heuristiques et pratiques incompl√®tes. <br><br>  Ici se trouve un ensemble plus profond de probl√®mes qui vont au c≈ìur m√™me de la science.  Il s'agit d'un probl√®me de g√©n√©ralisation.  La r√©gularisation peut nous donner une baguette magique de calcul qui aide nos r√©seaux √† mieux g√©n√©raliser les donn√©es, mais elle ne donne pas une compr√©hension de base du fonctionnement de la g√©n√©ralisation et de la meilleure approche. <br><br>  Ces probl√®mes remontent au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">probl√®me de l'induction</a> , dont une interpr√©tation bien connue a √©t√© r√©alis√©e par le philosophe √©cossais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">David Hume</a> dans le livre " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">A Study on Human Cognition</a> " (1748).  Le probl√®me d'induction fait l'objet du ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">th√©or√®me sur l'absence de repas gratuits</a> ¬ª de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">David Walpert et William Macredie</a> (1977). <br><br>  Et cela est particuli√®rement ennuyeux, car dans la vie ordinaire, les gens sont ph√©nom√©nalement bien capables de g√©n√©raliser des donn√©es.  Montrez quelques images de l'√©l√©phant √† l'enfant, et il apprendra rapidement √† reconna√Ætre d'autres √©l√©phants.  Bien s√ªr, il peut parfois faire une erreur, par exemple, confondre un rhinoc√©ros avec un √©l√©phant, mais en g√©n√©ral, ce processus fonctionne de mani√®re √©tonnamment pr√©cise.  Maintenant, nous avons un syst√®me - le cerveau humain - avec une √©norme quantit√© de param√®tres libres.  Et apr√®s avoir vu une ou plusieurs images d'entra√Ænement, le syst√®me apprend √† les g√©n√©raliser √† d'autres images.  Notre cerveau, dans un sens, est incroyablement bon pour r√©gulariser!  Mais comment fait-on cela?  Pour le moment, cela nous est inconnu.  Je pense qu'√† l'avenir, nous d√©velopperons des technologies de r√©gularisation plus puissantes dans les r√©seaux de neurones artificiels, des techniques qui permettront finalement √† l'Assembl√©e nationale de g√©n√©raliser des donn√©es bas√©es sur des ensembles de donn√©es encore plus petits. <br><br>  En fait, nos r√©seaux se g√©n√©ralisent d√©j√† beaucoup mieux que ce √† quoi on pourrait s'attendre a priori.  Un r√©seau de 100 neurones cach√©s a pr√®s de 80 000 param√®tres.  Nous n'avons que 50 000 images dans les donn√©es d'entra√Ænement.  Cela revient √† essayer d'√©tirer un polyn√¥me de 80 000 ordres sur 50 000 points de r√©f√©rence.  De toute √©vidence, notre r√©seau doit se recycler terriblement.  Et pourtant, comme nous l'avons vu, un tel r√©seau se g√©n√©ralise en fait assez bien.  Pourquoi cela se produit-il?  Ce n'est pas tout √† fait clair.  Il a √©t√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©mis l'hypoth√®se</a> que ¬´la dynamique de l'apprentissage par descente de gradient dans les r√©seaux multicouches est soumise √† l'autor√©gulation¬ª.  C'est une fortune extr√™me, mais aussi un fait assez inqui√©tant, car on ne comprend pas pourquoi cela se produit.  Entre-temps, nous adopterons une approche pragmatique et nous recourrons √† la r√©gularisation dans la mesure du possible.  Cela sera b√©n√©fique pour notre Assembl√©e nationale. <br><br>  Permettez-moi de terminer cette section en revenant √† ce que je n'ai pas expliqu√© auparavant: que la r√©gularisation de L2 ne limite pas les d√©placements.  Naturellement, il serait facile de changer la proc√©dure de r√©gularisation pour r√©gulariser les d√©placements.  Mais empiriquement, cela ne change souvent pas les r√©sultats de mani√®re notable, donc, dans une certaine mesure, traiter la r√©gularisation des biais, ou non, est une question d'accord.  Cependant, il convient de noter qu'un grand d√©placement ne rend pas un neurone sensible aux entr√©es comme les gros poids.  Par cons√©quent, nous n'avons pas √† nous soucier des d√©calages importants qui permettent √† nos r√©seaux d'apprendre le bruit dans les donn√©es d'entra√Ænement.  En m√™me temps, en permettant de grands d√©placements, nous rendons nos r√©seaux plus flexibles dans leur comportement - en particulier, les grands d√©placements facilitent la saturation des neurones, ce que nous souhaiterions.  Pour cette raison, nous n'incluons g√©n√©ralement pas les compensations dans la r√©gularisation. <br><br><h2>  Autres techniques de r√©gularisation </h2><br>  Il existe de nombreuses techniques de r√©gularisation en plus de L2.  En fait, tant de techniques ont d√©j√† √©t√© d√©velopp√©es que, avec tout le d√©sir, je ne pouvais pas toutes les d√©crire bri√®vement.  Dans cette section, je d√©crirai bri√®vement trois autres approches pour r√©duire le recyclage: r√©gulariser la L1, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">abandonner</a> et augmenter artificiellement l'ensemble de formation.  Nous ne les √©tudierons pas aussi profond√©ment que les sujets pr√©c√©dents.  Au lieu de cela, nous apprenons √† les conna√Ætre et appr√©cions en m√™me temps la vari√©t√© des techniques de r√©gularisation existantes. <br><br><h3>  R√©gularisation L1 </h3><br>  Dans cette approche, nous modifions la fonction de co√ªt irr√©gulier en ajoutant la somme des valeurs absolues des poids: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>95</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="40.447ex" height="2.66ex" viewBox="0 -832 17414.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-43" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-3D" x="1038" y="0"></use><g transform="translate(2094,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2B" x="3486" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="4736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="5287" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="5738" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="6268" y="0"></use><g transform="translate(6701,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6E" x="10140" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-73" x="10991" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-75" x="11460" y="0"></use><g transform="translate(12033,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="1242" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-7C" x="13518" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="13797" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-7C" x="14513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="15042" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="15403" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="15933" y="0"></use><g transform="translate(16413,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-35" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>95</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> C = C_0 + \ frac {\ lambda} {n} \ sum_w | w | \ tag {95} </script></p><br><br>  Intuitivement, cela est similaire √† la r√©gularisation de L2, qui inflige des amendes pour les poids √©lev√©s et fait que le r√©seau pr√©f√®re les poids faibles.  Bien s√ªr, le terme de r√©gularisation L1 n'est pas comme le terme de r√©gularisation L2, vous ne devez donc pas vous attendre exactement au m√™me comportement.  Essayons de comprendre en quoi le comportement d'un r√©seau form√© √† la r√©gularisation L1 diff√®re d'un r√©seau form√© √† la r√©gularisation L2. <br><br>  Pour ce faire, regardez les d√©riv√©es partielles de la fonction de co√ªt.  En diff√©renciant (95), on obtient: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mspace width=&quot;thinmathspace&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>96</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="86.162ex" height="2.66ex" viewBox="0 -832 37097.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-43" x="3269" y="0"></use></g><g transform="translate(6245,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-3D" x="10508" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="11815" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="12365" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="12817" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="13346" y="0"></use><g transform="translate(13780,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(18218,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2B" x="22427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="23677" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="24228" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="24679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="25209" y="0"></use><g transform="translate(25642,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6E" x="29081" y="0"></use><g transform="translate(29849,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="701" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-73" x="1580" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="2049" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6E" x="2530" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-28" x="32979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="33369" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-29" x="34085" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="34725" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="35086" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="35616" y="0"></use><g transform="translate(36096,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-36" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mspace width="thinmathspace"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>96</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> \ frac {\ partial C} {\ partial w} = \ frac {\ partial C_0} {\ partial w} + \ frac {\ lambda} {n} \, {\ rm sgn} (w) \ tag {96 } </script></p><br><br>  o√π sgn (w) est le signe de w, c'est-√†-dire +1 si w est positif et -1 si w est n√©gatif.  En utilisant cette expression, nous modifions l√©g√®rement la propagation arri√®re afin qu'elle effectue une descente de gradient stochastique en utilisant la r√©gularisation L1.  La r√®gle de mise √† jour finale pour le r√©seau r√©gularis√© L1: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>97</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.249ex" height="2.78ex" viewBox="0 -883.9 39718.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2212" x="9100" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="10901" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="11353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="11882" y="0"></use><g transform="translate(12316,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6E" x="17362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="18213" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-62" x="19091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6F" x="19521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-78" x="20006" y="0"></use><g transform="translate(20579,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6E" x="950" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-28" x="22129" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="22519" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-29" x="23235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2212" x="23847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-65" x="25098" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="25564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="25926" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="26705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="27256" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="27707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="28237" y="0"></use><g transform="translate(28670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33109,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="37345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="37707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="38236" y="0"></use><g transform="translate(38717,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-37" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>97</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \ rightarrow w '= w- \ frac {\ eta \ lambda} {n} \ mbox {sgn} (w) - \ eta \ frac {\ partial C_0} {\ partial w} \ tag {97} </script></p><br><br>  o√π, comme d'habitude, ‚àÇC / ‚àÇw peut √©ventuellement √™tre estim√© en utilisant la valeur moyenne du mini-paquet.  Comparez cela avec la r√®gle de mise √† jour de r√©gularisation L2 (93): <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>98</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.272ex" height="2.78ex" viewBox="0 -883.9 39728.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="9128" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-65" x="9426" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="9893" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="10443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-28" x="10805" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-31" x="11194" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2212" x="11917" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="13168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="13718" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="14170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="14699" y="0"></use><g transform="translate(15133,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6E" x="20179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="21030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="21481" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="21827" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-68" x="22307" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="22884" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-29" x="23245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2212" x="23857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-65" x="25108" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="25574" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="25936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-66" x="26715" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="27266" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="27717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-63" x="28247" y="0"></use><g transform="translate(28680,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33119,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="37355" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="37717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="38246" y="0"></use><g transform="translate(38727,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-38" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>98</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> w \ rightarrow w '= w \ left (1 - \ frac {\ eta \ lambda} {n} \ right) - \ eta \ frac {\ partial C_0} {\ partial w} \ tag {98} </script></p><br><br>  Dans les deux expressions, la r√©gularisation a pour effet de r√©duire les poids.  Cela co√Øncide avec la notion intuitive que les deux types de r√©gularisation p√©nalisent les poids importants.  Cependant, les poids sont r√©duits de diff√©rentes mani√®res.  Dans la r√©gularisation de L1, les poids diminuent d'une valeur constante, tendant vers 0. Dans la r√©gularisation de L2, les poids diminuent d'une valeur proportionnelle √† w.  Par cons√©quent, lorsqu'un poids a une grande valeur | w |, la r√©gularisation de L1 r√©duit le poids pas autant que L2.  Et vice versa, quand | w |  petite, la r√©gularisation de L1 r√©duit le poids beaucoup plus que la r√©gularisation de L2.  En cons√©quence, la r√©gularisation de L1 a tendance √† concentrer les pond√©rations du r√©seau dans un nombre relativement petit d'obligations de grande importance, tandis que les autres pond√©rations tendent √† z√©ro. <br><br>  J'ai l√©g√®rement liss√© un probl√®me dans la discussion pr√©c√©dente - la d√©riv√©e partielle ‚àÇC / ‚àÇw n'est pas d√©finie lorsque w = 0.  En effet, la fonction | w |  il y a un ¬´kink¬ª aigu au point w = 0, il ne peut donc pas √™tre diff√©renci√©.  Mais ce n'est pas effrayant.  Nous appliquons simplement la r√®gle irr√©guli√®re habituelle pour la descente de gradient stochastique lorsque w = 0.  Intuitivement, il n'y a rien de mal √† cela - la r√©gularisation devrait r√©duire les poids, et √©videmment elle ne peut pas r√©duire les poids d√©j√† √©gaux √† 0. Plus pr√©cis√©ment, nous utiliserons les √©quations (96) et (97) √† la condition que sgn (0) = 0.  Cela nous donnera une r√®gle pratique et compacte pour la descente de gradient stochastique avec r√©gularisation L1. <br><br><h3>  Exception [abandon] </h3><br>  Une exception est une technique de r√©gularisation compl√®tement diff√©rente.  Contrairement √† la r√©gularisation de L1 et L2, l'exception ne concerne pas un changement dans la fonction de co√ªt.  Au lieu de cela, nous changeons le r√©seau lui-m√™me.  Permettez-moi d'expliquer la m√©canique de base du fonctionnement d'une exception avant d'aborder la question de savoir pourquoi cela fonctionne et avec quels r√©sultats. <br><br>  Supposons que nous essayons de former un r√©seau: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae5/671/838/ae5671838ebc48f82eb01c1a839b60a7.png"></div><br>  En particulier, supposons que nous ayons l'entr√©e d'apprentissage x et la sortie souhait√©e correspondante y.  Habituellement, nous l'entra√Ænions en distribuant directement x sur le r√©seau, puis en nous propageant en retour pour d√©terminer la contribution du gradient.  Une exception modifie ce processus.  Nous commen√ßons par supprimer al√©atoirement et temporairement la moiti√© des neurones du r√©seau cach√©s, laissant les neurones d'entr√©e et de sortie inchang√©s.  Apr√®s cela, nous aurons environ un tel r√©seau.  Notez que les neurones exclus, ceux qui sont temporairement supprim√©s, sont toujours marqu√©s dans le diagramme: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec3/51c/b6a/ec351cb6a877c8f0688718e0d5980088.png"></div><br>  Nous passons x par distribution directe sur le r√©seau modifi√©, puis distribuons le r√©sultat, √©galement sur le r√©seau modifi√©.  Apr√®s avoir fait cela avec un mini-package d'exemples, nous mettons √† jour les poids et d√©calages correspondants.  Ensuite, nous r√©p√©tons ce processus, en restaurant d'abord les neurones exclus, puis en choisissant un nouveau sous-ensemble al√©atoire de neurones cach√©s √† supprimer, en √©valuant le gradient pour un autre mini-paquet et en mettant √† jour les poids et les d√©calages du r√©seau. <br><br>  En r√©p√©tant ce processus encore et encore, nous obtenons un r√©seau qui a appris certains poids et d√©placements.  Naturellement, ces poids et d√©placements ont √©t√© appris dans des conditions dans lesquelles la moiti√© des neurones cach√©s √©taient exclus.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et lorsque nous lancerons le r√©seau dans son int√©gralit√©, nous aurons deux fois plus de neurones cach√©s actifs. </font><font style="vertical-align: inherit;">Pour compenser cela, nous divisons par deux les poids provenant des neurones cach√©s.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La proc√©dure d'exclusion peut sembler √©trange et arbitraire. Pourquoi devrait-elle aider √† la r√©gularisation? Pour expliquer ce qui se passe, je veux que vous oubliez l'exception pendant un certain temps et pr√©sentiez la formation de l'Assembl√©e nationale de mani√®re standard. En particulier, imaginez que nous formons plusieurs NS diff√©rents en utilisant les m√™mes donn√©es d'entra√Ænement. Bien s√ªr, les r√©seaux peuvent varier au d√©but, et parfois la formation peut produire des r√©sultats diff√©rents. Dans de tels cas, nous pourrions appliquer une sorte de calcul de moyenne ou de vote pour d√©cider lequel des r√©sultats accepter. Par exemple, si nous avons form√© cinq r√©seaux et que trois d'entre eux classent le nombre en ¬´3¬ª, c'est probablement le vrai trois. Et les deux autres r√©seaux se trompent probablement. Un tel sch√©ma de moyenne est souvent un moyen utile (quoique co√ªteux) de r√©duire le recyclage. La raison en estque diff√©rents r√©seaux peuvent se recycler de diff√©rentes mani√®res, et la moyenne peut aider √† √©liminer ce recyclage.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comment tout cela se rapporte-t-il √† l'exception? Heuristiquement, lorsque nous excluons diff√©rents ensembles de neutrons, c'est comme si nous formions diff√©rents NS. Par cons√©quent, la proc√©dure d'exclusion est similaire √† la moyenne des effets sur un tr√®s grand nombre de r√©seaux diff√©rents. Diff√©rents r√©seaux se recyclent de diff√©rentes mani√®res, on esp√®re donc que l'effet moyen de l'exclusion r√©duira le recyclage. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Une explication heuristique connexe des avantages de l'exclusion est donn√©e dans l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un des premiers travaux</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en utilisant cette technique: ¬´Cette technique r√©duit l'adaptation articulaire complexe des neurones, car le neurone ne peut pas compter sur la pr√©sence de certains voisins. En fin de compte, il doit apprendre des traits plus fiables qui peuvent √™tre utiles pour travailler avec de nombreux sous-ensembles al√©atoires diff√©rents de neurones. ¬ª En d'autres termes, si nous imaginons notre Assembl√©e nationale comme un mod√®le faisant des pr√©dictions, alors une exception sera un moyen de garantir la stabilit√© du mod√®le √† la perte de pi√®ces individuelles de preuves. En ce sens, la technique ressemble aux r√©gularisations de L1 et L2, qui cherchent √† r√©duire les poids, et √† rendre ainsi le r√©seau plus r√©sistant √† la perte de toute connexion individuelle dans le r√©seau. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturellement, la v√©ritable mesure de l'utilit√© de l'exclusion est son √©norme succ√®s dans l'am√©lioration de l'efficacit√© des r√©seaux de neurones. Dans l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">≈ìuvre originale</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l√† o√π cette m√©thode a √©t√© introduite, elle a √©t√© appliqu√©e √† de nombreuses t√¢ches diff√©rentes. </font><font style="vertical-align: inherit;">Nous sommes particuli√®rement int√©ress√©s par le fait que les auteurs ont appliqu√© l'exception √† la classification des nombres du MNIST, en utilisant un r√©seau de distribution directe simple similaire √† celui que nous avons examin√©. </font><font style="vertical-align: inherit;">Le papier note que jusque-l√†, le meilleur r√©sultat pour une telle architecture √©tait une pr√©cision de 98,4%. </font><font style="vertical-align: inherit;">Ils l'ont am√©lior√© √† 98,7% en utilisant une combinaison d'exclusion et une forme modifi√©e de r√©gularisation L2. </font><font style="vertical-align: inherit;">Des r√©sultats tout aussi impressionnants ont √©t√© obtenus pour de nombreuses autres t√¢ches, notamment la reconnaissance des formes et de la parole, et le traitement du langage naturel. </font><font style="vertical-align: inherit;">L'exception a √©t√© particuli√®rement utile pour la formation de grands r√©seaux profonds, o√π le probl√®me de la reconversion se pose souvent.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Extension artificielle de l'ensemble de donn√©es de formation </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous avons vu plus t√¥t que notre pr√©cision de classification MNIST a chut√© √† 80%, lorsque nous avons utilis√© seulement 1 000 images d'entra√Ænement. Et pas √©tonnant - avec moins de donn√©es, notre r√©seau rencontrera moins d'options pour √©crire des nombres par des personnes. Essayons de former notre r√©seau de 30 neurones cach√©s, en utilisant diff√©rents volumes de l'ensemble d'entra√Ænement pour √©tudier le changement d'efficacit√©. Nous nous entra√Ænons en utilisant la taille du mini-paquet de 10, la vitesse d'apprentissage Œ∑ = 0,5, le param√®tre de r√©gularisation Œª = 5,0 et la fonction de co√ªt avec entropie crois√©e. Nous formerons un r√©seau de 30 √©poques √† l'aide d'un ensemble complet de donn√©es, et augmenterons le nombre d'√©poques proportionnellement √† la diminution du volume des donn√©es de formation. Pour garantir le m√™me facteur de r√©duction de poids pour diff√©rents ensembles de donn√©es d'entra√Ænement, nous utiliserons le param√®tre de r√©gularisation Œª = 5,0 avec un ensemble de formation complet, et r√©duisez-le proportionnellement avec une diminution des volumes de donn√©es.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d6/a9f/6db/7d6a9f6db97493f1d716450344dd877f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On peut voir que la pr√©cision de la classification augmente consid√©rablement avec l'augmentation des donn√©es d'entra√Ænement. Cette croissance devrait se poursuivre avec une nouvelle augmentation des volumes. Bien s√ªr, √† en juger par le graphique ci-dessus, nous approchons de la saturation. Cependant, supposons que nous refaisons ce graphique √† une d√©pendance logarithmique sur la quantit√© de donn√©es d'entra√Ænement:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On peut voir qu'en fin de compte le graphique a encore tendance √† monter. Cela sugg√®re que si nous prenons une quantit√© beaucoup plus massive de donn√©es - par exemple, des millions voire des milliards d'exemples manuscrits, plut√¥t que 50 000 - alors nous obtiendrons probablement un r√©seau de travail bien meilleur, m√™me de si petite taille. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obtenir plus de donn√©es d'entra√Ænement est une excellente id√©e. Malheureusement, cela peut √™tre co√ªteux, donc en pratique ce n'est pas toujours possible. Cependant, il existe une autre id√©e qui peut fonctionner presque aussi bien - augmenter artificiellement l'ensemble de donn√©es. Par exemple, supposons que nous prenions une image de cinq de MNIST et la tournions un peu, degr√©s de 15:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcf/555/69f/bcf55569f0ecfda9357d6c1ce1f3e9fb.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/408/bcc/32a/408bcc32a8b3b03094eb4f2b7fdea833.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C'est clairement le m√™me chiffre. Mais au niveau des pixels, c'est tr√®s diff√©rent des images disponibles dans la base de donn√©es MNIST. Il est raisonnable de supposer que l'ajout de cette image √† l'ensemble de donn√©es de formation peut aider notre r√©seau √† en savoir plus sur la classification des images. De plus, nous ne sommes √©videmment pas limit√©s √† la possibilit√© d'ajouter une seule image. Nous pouvons √©tendre nos donn√©es d'entra√Ænement en effectuant quelques petits tours de toutes les images d'entra√Ænement du MNIST, puis en utilisant l'ensemble √©tendu de donn√©es d'entra√Ænement pour augmenter l'efficacit√© du r√©seau. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cette id√©e est tr√®s puissante et largement utilis√©e. Regardons les r√©sultats </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des travaux scientifiques</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qui a appliqu√© plusieurs variantes de cette id√©e au MNIST. L'une des architectures des r√©seaux consid√©r√©s par eux √©tait similaire √† celle que nous utilisons - un r√©seau de distribution directe avec 800 neurones cach√©s, utilisant la fonction de co√ªt avec entropie crois√©e. En lan√ßant ce r√©seau avec l'ensemble de formation MNIST standard, ils ont obtenu une pr√©cision de classification de 98,4%. Mais ensuite, ils ont √©largi les donn√©es d'entra√Ænement, en utilisant non seulement la rotation que j'ai d√©crite ci-dessus, mais aussi le transfert et la distorsion des images. Apr√®s avoir form√© le r√©seau aux donn√©es avanc√©es, ils ont augment√© sa pr√©cision √† 98,9%. Ils ont √©galement exp√©riment√© le soi-disant ¬´Distorsion √©lastique¬ª, un type sp√©cial de distorsion d'image, con√ßu pour √©liminer les vibrations al√©atoires des muscles de la main. En utilisant des distorsions √©lastiques pour √©tendre les donn√©es, ils ont atteint une pr√©cision de 99,3%. En substance, ils ont √©largi leur exp√©rience de r√©seau,lui donnant diverses variations manuscrites trouv√©es dans la vraie √©criture.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Des variantes de cette id√©e peuvent √™tre utilis√©es pour am√©liorer les performances de nombreuses t√¢ches d'apprentissage, pas seulement pour la reconnaissance de l'√©criture manuscrite. Le principe g√©n√©ral est d'√©tendre les donn√©es d'entra√Ænement en leur appliquant des op√©rations qui refl√®tent les variations rencontr√©es dans la r√©alit√©. De telles variations sont faciles √† trouver. Supposons que nous cr√©ons NS pour la reconnaissance vocale. Les gens peuvent reconna√Ætre la parole m√™me avec des distorsions telles que le bruit de fond. Par cons√©quent, vous pouvez √©tendre les donn√©es en ajoutant du bruit de fond. Nous sommes √©galement capables de reconna√Ætre un discours acc√©l√©r√© et lent. C'est une autre fa√ßon d'√©largir les donn√©es de formation. Ces techniques ne sont pas toujours utilis√©es - par exemple, au lieu d'√©tendre l'ensemble d'apprentissage en ajoutant du bruit, il peut √™tre plus efficace de nettoyer l'entr√©e en leur appliquant un filtre de bruit. Et pourtant, il convient de garder √† l‚Äôesprit l‚Äôid√©e d‚Äô√©largir l‚Äôensemble de formation,et cherchez des moyens de l'utiliser.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exercice </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme nous l'avons vu ci-dessus, une fa√ßon d'√©tendre les donn√©es d'entra√Ænement du MNIST consiste √† utiliser de petites rotations des images d'entra√Ænement. </font><font style="vertical-align: inherit;">Quel probl√®me peut appara√Ætre si nous autorisons la rotation des images sous tous les angles?</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Digression des m√©gadonn√©es et signification de la comparaison de l'exactitude de la classification </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Examinons √† nouveau comment la pr√©cision de notre NS varie en fonction de la taille de l'ensemble de formation: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supposons qu'au lieu d'utiliser NS, nous utiliserions une autre technologie d'apprentissage automatique pour classer les nombres. Par exemple, essayons d'utiliser la m√©thode de support vector machine (SVM), que nous avons bri√®vement rencontr√©e au chapitre 1. Comme alors, ne vous inqui√©tez pas si vous n'√™tes pas familier avec SVM, nous n'avons pas besoin de comprendre ses d√©tails. Nous utiliserons SVM via la biblioth√®que scikit-learn. Voici comment l'efficacit√© de SVM varie en fonction de la taille de l'ensemble d'entra√Ænement. A titre de comparaison, j'ai mis le calendrier et les r√©sultats de l'Assembl√©e nationale.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a6/21b/a21/9a621ba21ebb087e64fc7c68d3238ef5.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Probablement la premi√®re chose qui attire votre attention - NS d√©passe SVM dans n'importe quelle taille de l'ensemble d'entra√Ænement. C'est bien, m√™me si cela ne vaut pas la peine d'en tirer des conclusions de grande envergure, car j'ai utilis√© les param√®tres pr√©d√©finis de scikit-learn et nous avons travaill√© tr√®s s√©rieusement sur notre NS. Un fait moins vif, mais plus int√©ressant, qui d√©coule du graphique, est que si nous entra√Ænons notre SVM √† l'aide de 50000 images, cela fonctionnera mieux (94,48% de pr√©cision) que notre NS form√© avec 5000 images ( 93,24%). En d'autres termes, une augmentation du volume des donn√©es d'entra√Ænement compense parfois la diff√©rence d'algorithmes MO.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelque chose de plus int√©ressant peut arriver. Supposons que nous essayons de r√©soudre un probl√®me en utilisant deux algorithmes MO, A et B. Parfois, il arrive que l'algorithme A soit en avance sur l'algorithme B sur un ensemble de donn√©es d'apprentissage, et l'algorithme B est en avance sur l'algorithme A sur un autre ensemble de donn√©es d'apprentissage. Nous ne l'avons pas vu ci-dessus - alors les graphiques se croiseraient - mais </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cela se produit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . La bonne r√©ponse √† la question: "L'algorithme A est-il sup√©rieur √† l'algorithme B?" en fait, ceci: "Quel ensemble de donn√©es de formation utilisez-vous?"</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tout cela doit √™tre pris en compte, tant lors du d√©veloppement que lors de la lecture des articles scientifiques. De nombreux travaux se concentrent sur la recherche de nouvelles astuces pour obtenir de meilleurs r√©sultats sur des ensembles de donn√©es de mesure standard. ¬´Notre technologie de superaliments nous a donn√© une am√©lioration de X% par rapport √† l'ensemble comparatif standard Y¬ª - le formulaire de demande canonique dans une telle √©tude. Parfois, de telles d√©clarations sont en fait int√©ressantes, mais il vaut la peine de comprendre qu'elles ne sont applicables que dans le contexte d'un ensemble de formation sp√©cifique. Imaginez une histoire alternative dans laquelle les personnes qui ont initialement cr√©√© un ensemble comparatif ont re√ßu une subvention de recherche plus importante. Ils pourraient utiliser de l'argent suppl√©mentaire pour collecter des donn√©es suppl√©mentaires. Il est possible que ¬´l'am√©lioration¬ª de la technologie de super-duper disparaisse sur un ensemble de donn√©es plus important. En d'autres termes,l'essence de l'am√©lioration peut simplement √™tre un accident. √Ä partir de l√†, la morale suivante doit √™tre prise dans le domaine de l'application pratique: nous avons besoin √† la fois d'algorithmes am√©lior√©s et de donn√©es d'entra√Ænement am√©lior√©es. Il n'y a rien de mal √† rechercher des algorithmes am√©lior√©s, mais assurez-vous de ne pas vous concentrer sur cela, en ignorant le moyen le plus simple de gagner en augmentant le volume ou la qualit√© des donn√©es d'entra√Ænement.</font></font><br><br><h3>  D√©fi </h3><br><ul><li>  .              ?                 .        ‚Äì  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   </a> ,  ,   ,      .   ,             .        -   ?   ,      . </li></ul><br><h3>  R√©sum√© </h3><br>  Nous avons termin√© notre immersion en recyclage et r√©gularisation.  Bien s√ªr, nous reviendrons sur ces probl√®mes.  Comme je l'ai d√©j√† mentionn√© √† plusieurs reprises, le recyclage est un gros probl√®me dans le domaine de la NS, d'autant plus que les ordinateurs deviennent plus puissants et que nous pouvons former des r√©seaux plus importants.  En cons√©quence, il est urgent de d√©velopper des techniques de r√©gularisation efficaces pour r√©duire le recyclage, ce domaine est donc tr√®s actif aujourd'hui. <br><br><h2>  Initialisation du poids </h2><br>  Lorsque nous cr√©ons notre NS, nous devons choisir les valeurs initiales des poids et des d√©calages.  Jusqu'√† pr√©sent, nous les avons choisis selon les directives bri√®vement d√©crites au chapitre 1. Permettez-moi de vous rappeler que nous avons choisi des poids et des d√©calages bas√©s sur une distribution gaussienne ind√©pendante avec une attente math√©matique de 0 et un √©cart-type de 1. Cette approche a bien fonctionn√©, mais elle semble plut√¥t arbitraire, donc √ßa vaut le coup r√©visez-le et pensez s'il est possible de trouver une meilleure fa√ßon d'affecter les poids et d√©placements initiaux et, peut-√™tre, d'aider nos NS √† apprendre plus rapidement. <br><br>  Il s'av√®re que le processus d'initialisation peut √™tre s√©rieusement am√©lior√© par rapport √† la distribution gaussienne normalis√©e.  Pour comprendre cela, disons que nous travaillons avec un r√©seau avec un grand nombre de neurones d'entr√©e, disons, de 1000. Et disons que nous avons utilis√© la distribution gaussienne normalis√©e pour initialiser les poids connect√©s √† la premi√®re couche cach√©e.  Jusqu'√† pr√©sent, je vais me concentrer uniquement sur les √©chelles reliant les neurones d'entr√©e au premier neurone de la couche cach√©e, et ignorer le reste du r√©seau: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac3/c9d/62f/ac3c9d62f301e85234b7cd4bbcbd0e0d.png"></div><br>  Pour simplifier, imaginons que nous essayons de former le r√©seau avec l'entr√©e x, dans laquelle la moiti√© des neurones d'entr√©e sont activ√©s, c'est-√†-dire qu'ils ont une valeur de 1, et la moiti√© sont d√©sactiv√©s, c'est-√†-dire qu'ils ont une valeur de 0. L'argument suivant fonctionne dans un cas plus g√©n√©ral, mais c'est plus facile pour vous le comprendra sur cet exemple particulier.  Consid√©rons la somme pond√©r√©e z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b des entr√©es pour un neurone cach√©.  500 membres de la somme disparaissent parce que les x <sub>j</sub> correspondants sont 0. Par cons√©quent, z est la somme de 501 variables al√©atoires gaussiennes normalis√©es, 500 poids et 1 d√©calage suppl√©mentaire.  Par cons√©quent, la valeur z elle-m√™me a une distribution gaussienne avec une esp√©rance math√©matique de 0 et un √©cart-type de ‚àö501 ‚âà 22,4.  Autrement dit, z a une distribution gaussienne assez large, sans pics nets: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/an/fj/_3/anfj_3qej8-fcxrnig7beewu3qm.png"></div><br>  En particulier, ce graphique montre que | z | est susceptible d'√™tre assez grand, c'est-√†-dire z ‚â´ 1 ou z ‚â´ -1.  Dans ce cas, la sortie des neurones cach√©s œÉ (z) sera tr√®s proche de 1 ou 0. Cela signifie que notre neurone cach√© sera satur√©.  Et lorsque cela se produit, comme nous le savons d√©j√†, de petits changements de poids produiront de minuscules changements dans l'activation d'un neurone cach√©.  Ces petits changements, √† leur tour, n'affecteront pratiquement pas les neutrons restants dans le r√©seau, et nous verrons les petits changements correspondants dans la fonction de co√ªt.  Par cons√©quent, ces poids seront entra√Æn√©s tr√®s lentement lorsque nous utiliserons l'algorithme de descente de gradient.  Ceci est similaire √† la t√¢che que nous avons d√©j√† discut√©e dans ce chapitre, dans laquelle les neurones de sortie satur√©s de valeurs incorrectes ralentissent l'apprentissage.  Nous avions l'habitude de r√©soudre ce probl√®me en choisissant intelligemment une fonction de co√ªt.  Malheureusement, bien que cela ait aid√© avec les neurones de sortie satur√©s, cela n'aide pas du tout avec la saturation des neurones cach√©s. <br><br>  Maintenant, j'ai parl√© des √©chelles entrantes de la premi√®re couche cach√©e.  Naturellement, les m√™mes arguments s'appliquent aux couches cach√©es suivantes: si les poids des couches cach√©es ult√©rieures sont initialis√©s √† l'aide de distributions gaussiennes normalis√©es, leur activation sera souvent proche de 0 ou 1, et la formation se d√©roulera tr√®s lentement. <br><br>  Existe-t-il un moyen de choisir les meilleures options d'initialisation pour les pond√©rations et les d√©calages, afin de ne pas obtenir une telle saturation et d'√©viter les retards d'apprentissage?  Supposons que nous ayons un neurone avec le nombre de poids entrants n po.  Ensuite, nous devons initialiser ces poids avec des distributions gaussiennes al√©atoires avec une esp√©rance math√©matique de 0 et un √©cart-type de 1 / ‚àön <sub>in</sub> .  Autrement dit, nous compressons les gaussiens et r√©duisons la probabilit√© de saturation du neurone.  Ensuite, nous choisissons une distribution gaussienne pour les d√©placements avec une esp√©rance math√©matique de 0 et un √©cart-type de 1, pour des raisons que je reviendrai un peu plus loin.  Apr√®s avoir fait ce choix, nous constatons √† nouveau que z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b sera une variable al√©atoire avec une distribution gaussienne avec une attente math√©matique de 0, mais avec un pic beaucoup plus prononc√© qu'auparavant.  Supposons, comme pr√©c√©demment, que 500 entr√©es soient 0 et 500 sont 1. Ensuite, il est facile de montrer (voir l'exercice ci-dessous) que z a une distribution gaussienne avec une esp√©rance math√©matique de 0 et un √©cart-type de ‚àö (3/2) = 1,22 ... Ce graphique a un pic beaucoup plus net, √† tel point que m√™me dans l'image ci-dessous, la situation est quelque peu sous-estim√©e, car j'ai d√ª changer l'√©chelle de l'axe vertical par rapport au graphique pr√©c√©dent: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/ee/xk/5eeexk-kyxhmi3pdoslo9w_al44.png"></div><br>  Un tel neurone sera satur√© avec une probabilit√© beaucoup plus faible et, par cons√©quent, il sera moins susceptible de rencontrer un ralentissement de l'apprentissage. <br><br><h3>  Exercice </h3><br><ul><li>  Confirmer que l'√©cart type de z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b par rapport au paragraphe pr√©c√©dent est ‚àö (3/2).  Consid√©rations en faveur de ceci: la variance de la somme des variables al√©atoires ind√©pendantes est √©gale √† la somme des variances des variables al√©atoires individuelles;  la variance est √©gale au carr√© de l'√©cart type. </li></ul><br>  J'ai mentionn√© ci-dessus que nous continuerons √† initialiser les d√©placements, comme pr√©c√©demment, sur la base d'une distribution gaussienne ind√©pendante avec une attente math√©matique de 0 et un √©cart-type de 1. Et c'est normal, car cela n'augmente pas consid√©rablement la probabilit√© de saturation de nos neurones.  En fait, l'initialisation des d√©calages n'a pas beaucoup d'importance si nous parvenons √† √©viter le probl√®me de saturation.  Certains tentent m√™me d'initialiser tous les d√©calages √† z√©ro et s'appuient sur le fait que la descente en gradient peut apprendre les d√©calages appropri√©s.  Mais comme la probabilit√© que cela affecte quelque chose est faible, nous continuerons √† utiliser la m√™me proc√©dure d'initialisation qu'auparavant. <br><br>  Comparons les r√©sultats des anciennes et des nouvelles approches pour initialiser les poids en utilisant la t√¢che de classer les nombres du MNIST.  Comme pr√©c√©demment, nous utiliserons 30 neurones cach√©s, un mini-paquet de taille 10, un param√®tre de r√©gularisation &amp; lambda = 5,0, et une fonction de co√ªt avec entropie crois√©e.  Nous r√©duirons progressivement la vitesse d'apprentissage de Œ∑ = 0,5 √† 0,1, car de cette fa√ßon les r√©sultats seront l√©g√®rement mieux visibles sur les graphiques.  Vous pouvez apprendre en utilisant l'ancienne m√©thode d'initialisation du poids: <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.large_weight_initializer() &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Vous pouvez √©galement apprendre √† utiliser la nouvelle approche pour initialiser les poids.  C'est encore plus simple, car par d√©faut, network2 initialise les poids en utilisant une nouvelle approche.  Cela signifie que nous pouvons omettre l'appel net.large_weight_initializer () plus t√¥t: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  On trace (en utilisant le programme weight_initialization.py): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/640/4ab/59d6404ab3c5e01946106b26743ae130.png"></div><br>  Dans les deux cas, une pr√©cision de classement de 96% est obtenue.  La pr√©cision r√©sultante est presque la m√™me dans les deux cas.  Mais la nouvelle technique d'initialisation atteint ce point beaucoup, beaucoup plus rapidement.  √Ä la fin de la derni√®re √®re de formation, l'ancienne approche d'initialisation des poids atteint une pr√©cision de 87% et la nouvelle approche approche d√©j√† 93%.  Apparemment, une nouvelle approche pour initialiser les poids part d'une position bien meilleure, donc nous obtenons de bons r√©sultats beaucoup plus rapidement.  Le m√™me ph√©nom√®ne s'observe si l'on construit les r√©sultats d'un r√©seau de 100 neurones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/72d/8ef/ccd72d8effddc7815bd526dcb1434acd.png"></div><br>  Dans ce cas, deux courbes ne se produisent pas.  Cependant, mes exp√©riences disent que si vous ajoutez un peu plus d'√©poques, la pr√©cision commence √† presque co√Øncider.  Par cons√©quent, sur la base de ces exp√©riences, nous pouvons dire que l'am√©lioration de l'initialisation des poids ne fait qu'acc√©l√©rer la formation, mais ne modifie pas l'efficacit√© globale du r√©seau.  Cependant, au chapitre 4, nous verrons des exemples de NS dans lesquels l'efficacit√© √† long terme est consid√©rablement am√©lior√©e en raison de l'initialisation des pond√©rations √† 1 / ‚àön <sub>in</sub> .  Par cons√©quent, il am√©liore non seulement la vitesse d'apprentissage, mais parfois l'efficacit√© qui en r√©sulte. <br><br>  L'approche de l'initialisation des poids via 1 / ‚àön <sub>in</sub> permet d'am√©liorer la formation des r√©seaux de neurones.  D'autres techniques d'initialisation des poids ont √©t√© propos√©es, dont beaucoup sont bas√©es sur cette id√©e de base.  Je ne les consid√©rerai pas ici, car 1 / ‚àön <sub>in</sub> fonctionne bien pour nos besoins.  Si vous √™tes int√©ress√©, je vous recommande de lire la discussion des pages 14 et 15 dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de 2012 de</a> Yoshua Benggio. <br><br><h3>  D√©fi </h3><br><ul><li>  La combinaison de la r√©gularisation et d'une m√©thode d'initialisation de poids am√©lior√©e.  Parfois, la r√©gularisation de L2 nous donne automatiquement des r√©sultats similaires √† une nouvelle m√©thode d'initialisation des poids.  Disons que nous utilisons l'ancienne approche pour initialiser les poids.  Esquissez un argument heuristique prouvant que: (1) si Œª n'est pas trop petit, alors dans les premi√®res √©poques d'entra√Ænement, l'affaiblissement des poids dominera presque compl√®tement;  (2) si Œ∑Œª ‚â™ n, alors les poids affaibliront e <sup>‚àíŒ∑Œª / m</sup> fois √† l'√©poque;  (3) si Œª n'est pas trop grand, l'affaiblissement des poids ralentit lorsque les poids diminuent √† environ 1 / ‚àön, o√π n est le nombre total de poids dans le r√©seau.  D√©montrer que ces conditions sont remplies dans les exemples pour lesquels les graphiques sont construits dans cette section. </li></ul><br><br><h2>  Retour √† la reconnaissance de l'√©criture manuscrite: code </h2><br>  Impl√©mentons les id√©es d√©crites dans ce chapitre.  Nous allons d√©velopper un nouveau programme, network2.py, une version am√©lior√©e du programme network.py que nous avons cr√©√© au chapitre 1. Si vous n'avez pas vu son code depuis longtemps, il peut √™tre utile de le parcourir rapidement.  Ce ne sont que 74 lignes de code, et c'est facile √† comprendre. <br><br>  Comme avec network.py, l'√©toile de network2.py est la classe Network, que nous utilisons pour repr√©senter nos NS.  Nous initialisons l'instance de classe avec une liste de tailles des couches r√©seau correspondantes, et avec le choix de la fonction de co√ªt, par d√©faut ce sera l'entropie crois√©e: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes, cost=CrossEntropyCost)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost</code> </pre> <br>  Les deux premi√®res lignes de la m√©thode __init__ sont les m√™mes que network.py et sont comprises par elles-m√™mes.  Les deux lignes suivantes sont nouvelles et nous devons comprendre en d√©tail ce qu'elles font. <br><br>  Commen√ßons par la m√©thode default_weight_initializer.  Il utilise une nouvelle approche am√©lior√©e pour initialiser les poids.  Comme nous l'avons vu, dans cette approche, les poids entrant dans le neurone sont initialis√©s sur la base d'une distribution gaussienne ind√©pendante avec une attente math√©matique de 0 et un √©cart-type de 1 divis√© par la racine carr√©e du nombre de liens entrants vers le neurone.  De plus, cette m√©thode initialisera les d√©calages en utilisant la distribution gaussienne avec une moyenne de 0 et un √©cart-type de 1. Voici le code: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">default_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Pour le comprendre, vous devez vous rappeler que np est une biblioth√®que Numpy traitant de l'alg√®bre lin√©aire.  Nous l'avons import√© au d√©but du programme.  Notez √©galement que nous n'initialisons pas les d√©placements dans la premi√®re couche de neurones.  La premi√®re couche est entrante, les d√©calages ne sont donc pas utilis√©s.  Le m√™me √©tait network.py. <br><br>  En plus de la m√©thode default_weight_initializer, nous allons cr√©er une m√©thode large_weight_initializer.  Il initialise les pond√©rations et les d√©calages en utilisant l'ancienne approche du chapitre 1, o√π les pond√©rations et les compensations sont initialis√©es sur la base d'une distribution gaussienne ind√©pendante avec une attente math√©matique de 0 et un √©cart-type de 1. Ce code, bien s√ªr, n'est pas tr√®s diff√©rent de default_weight_initializer: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">large_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  J'ai inclus cette m√©thode principalement parce qu'il √©tait plus pratique pour nous de comparer les r√©sultats de ce chapitre et du chapitre 1. Je ne peux imaginer de r√©elles options dans lesquelles je recommanderais de l'utiliser! <br><br>  La deuxi√®me nouveaut√© de la m√©thode __init__ sera l'initialisation de l'attribut cost.  Pour comprendre comment cela fonctionne, regardons la classe que nous utilisons pour repr√©senter la fonction de co√ªt d'entropie crois√©e (la directive @staticmethod indique √† l'interpr√©teur que cette m√©thode est ind√©pendante de l'objet, donc le param√®tre self n'est pas transmis aux m√©thodes fn et delta). <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropyCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number"><span class="hljs-number">1</span></span>-y)*np.log(<span class="hljs-number"><span class="hljs-number">1</span></span>-a))) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay)</code> </pre> <br>  Voyons cela.  La premi√®re chose que l'on peut voir ici est que, bien que l'entropie crois√©e soit une fonction d'un point de vue math√©matique, nous l'impl√©mentons en tant que classe python, pas en tant que fonction python.  Pourquoi ai-je d√©cid√© de faire √ßa?  Dans notre r√©seau, la valeur joue deux r√¥les diff√©rents.  √âvident - c'est une mesure de la mesure dans laquelle l'activation de la sortie a correspond √† la sortie souhait√©e y.  Ce r√¥le est fourni par la m√©thode CrossEntropyCost.fn.  (Soit dit en passant, notez que l'appel de np.nan_to_num dans CrossEntropyCost.fn garantit que Numpy traite correctement le logarithme des nombres proches de z√©ro).  Cependant, la fonction de co√ªt est utilis√©e dans notre r√©seau de la deuxi√®me mani√®re.  Nous rappelons du chapitre 2 que lors du d√©marrage de l'algorithme de r√©tropropagation, nous devons consid√©rer l'erreur de sortie du r√©seau Œ¥ <sup>L.</sup>  La forme de l'erreur de sortie d√©pend de la fonction de co√ªt: diff√©rentes fonctions de co√ªt auront diff√©rentes formes d'erreur de sortie.  Pour l'entropie crois√©e, l'erreur de sortie, comme suit de l'√©quation (66), sera √©gale √†: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>&amp;#x2212;</mo><mi>y</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>99</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.761ex" height="2.901ex" viewBox="0 -987.6 9799.8 1249" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-65" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-6C" x="1240" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="1538" y="0"></use><g transform="translate(1900,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-3D" x="3289" y="0"></use><g transform="translate(4345,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-2212" x="5679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-79" x="6679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-74" x="7427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-61" x="7788" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMATHI-67" x="8318" y="0"></use><g transform="translate(8798,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgS8Ul_DdfdBdzE-blltx3z2K9KWQ#MJMAIN-39" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>‚àí</mo><mi>y</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>99</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> \ delta ^ L = a ^ L-y \ tag {99} </script></p><br><br>  Par cons√©quent, je d√©finis une deuxi√®me m√©thode, CrossEntropyCost.delta, dont le but est d'expliquer au r√©seau comment calculer l'erreur de sortie.  Et puis nous combinons ces deux m√©thodes en une seule classe contenant tout ce que notre r√©seau doit savoir sur la fonction de co√ªt. <br><br>  Pour une raison similaire, network2.py contient une classe qui repr√©sente une fonction de co√ªt quadratique.  Y compris pour comparaison avec les r√©sultats du chapitre 1, car √† l'avenir, nous utiliserons principalement l'entropie crois√©e.  Le code est ci-dessous.  La m√©thode QuadraticCost.fn est un calcul simple du co√ªt quadratique associ√© √† la sortie a et √† la sortie souhait√©e y.  La valeur renvoy√©e par QuadraticCost.delta est bas√©e sur l'expression (30) de l'erreur de sortie de la valeur quadratique, que nous avons d√©riv√©e au chapitre 2. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">QuadraticCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*np.linalg.norm(ay)**<span class="hljs-number"><span class="hljs-number">2</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay) * sigmoid_prime(z)</code> </pre> <br>  Nous avons maintenant compris les principales diff√©rences entre network2.py et network2.py.  Tout est tr√®s simple.  Il y a d'autres petits changements que je d√©crirai ci-dessous, notamment la mise en place de la r√©gularisation de L2.  Avant cela, regardons le code network2.py complet.  Il n'est pas n√©cessaire de l'√©tudier en d√©tail, mais il vaut la peine de comprendre la structure de base, en particulier, de lire les commentaires pour comprendre ce que fait chacun des √©l√©ments du programme.  Bien s√ªr, je n'interdit pas de vous plonger dans cette question autant que vous le souhaitez!  Si vous vous perdez, essayez de lire le texte apr√®s le programme et revenez au code.  En g√©n√©ral, c'est ici: <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"""network2.py ~~~~~~~~~~~~~~   network.py,            .   ‚Äì      , ,   .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import json import random import sys #  import numpy as np ####   ,      class QuadraticCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. """ return 0.5*np.linalg.norm(ay)**2 @staticmethod def delta(z, a, y): """  delta   .""" return (ay) * sigmoid_prime(z) class CrossEntropyCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. np.nan_to_num    .  ,   ``a``  ``y``      1.0,   (1-y)*np.log(1-a)  nan. np.nan_to_num ,       (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """  delta   .  ``z``    ,          delta     . """ return (ay) ####   Network class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].       ,   ``self.default_weight_initializer`` (.  ). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """            0    1,       ,       .          0    1.    ,         ,           . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """          0    1.          0    1.    ,         ,           .         1,    .       . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """  ,  ``a``  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """     -    . ``training_data`` ‚Äì   ``(x, y)``,       .       ,    ``lmbda``.    ``evaluation_data``,     ,   .         ,     ,   .      :   ,    ,   ,              .  ,      30 ,        30 ,        .     ,   . """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: {}".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: {} / {}".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: {}".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: {} / {}".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """    ,          -. ``mini_batch`` ‚Äì    ``(x, y)``, ``eta`` ‚Äì  , ``lmbda`` -  , ``n`` -     .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """  l      ,      . l = 1    , l = 2 ‚Äì ,   .    ,   python      . """ for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """    ``data``,      .   ‚Äì        .  ``convert``  False,    ‚Äì    ( )  True,   .    - ,  ``y`` -     .  ,       .           .          ?     ‚Äì       ,      .   ,      .       mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """      ``data``.  ``convert``   False,   ‚Äì  (),   True,   ‚Äì   . .    ,      ``accuracy``, . """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """    ``filename``.""" data = {"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)} f = open(filename, "w") json.dump(data, f) f.close() ####  Network def load(filename): """    ``filename``.    Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net ####   def vectorized_result(j): """  10-    1.0   j     .      (0..9)     . """ e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Parmi les changements les plus int√©ressants figure l'inclusion de la r√©gularisation L2.  Bien qu'il s'agisse d'un grand changement conceptuel, il est si facile √† impl√©menter que vous ne le remarquerez peut-√™tre pas dans le code.  Pour la plupart, cela passe simplement le param√®tre lmbda √† diff√©rentes m√©thodes, en particulier Network.SGD.  Tout le travail est effectu√© sur une ligne du programme, la quatri√®me √† partir de la fin dans la m√©thode Network.update_mini_batch.  L√†, nous changeons la r√®gle de mise √† jour de la descente de gradient pour inclure la r√©duction de poids.  Le changement est minime, mais affecte s√©rieusement les r√©sultats! <br><br>  Soit dit en passant, cela se produit souvent lors de la mise en ≈ìuvre de nouvelles techniques dans les r√©seaux de neurones.  Nous avons pass√© des milliers de mots √† discuter de la r√©gularisation.  Conceptuellement, c'est une chose assez subtile et difficile √† comprendre.  Cependant, il peut √™tre trivialement ajout√© au programme!  De fa√ßon inattendue, des techniques complexes peuvent √™tre mises en ≈ìuvre avec des modifications mineures de code. <br><br>  Un autre changement petit mais important dans le code est l'ajout de plusieurs indicateurs optionnels √† la m√©thode de descente de gradient stochastique Network.SGD.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ces drapeaux permettent de suivre les co√ªts et la pr√©cision sur training_data ou evaluation_data, qui peuvent √™tre transmis √† Network.SGD. </font><font style="vertical-align: inherit;">Plus t√¥t dans le chapitre, nous avons souvent utilis√© ces indicateurs, mais permettez-moi de donner un exemple de leur utilisation, juste pour rappel:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous d√©finissons evaluation_data via validation_data. Cependant, nous pourrions suivre les performances sur test_data et tout autre ensemble de donn√©es. Nous avons √©galement quatre indicateurs qui sp√©cifient la n√©cessit√© de suivre les co√ªts et la pr√©cision √† la fois sur evaluation_data et training_data. Ces indicateurs sont d√©finis sur False par d√©faut, mais ils sont inclus ici pour suivre l'efficacit√© du r√©seau. De plus, la m√©thode Network.SGD de network2.py renvoie un tuple √† quatre √©l√©ments repr√©sentant les r√©sultats du suivi. Vous pouvez l'utiliser comme ceci:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>evaluation_cost, evaluation_accuracy, ... training_cost, training_accuracy = net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ainsi, par exemple, evaluation_cost sera une liste de 30 √©l√©ments contenant le co√ªt des donn√©es estim√©es √† la fin de chaque √®re. Ces informations sont extr√™mement utiles pour comprendre le comportement d'un r√©seau neuronal. Ces informations sont extr√™mement utiles pour comprendre le comportement du r√©seau. Il peut, par exemple, √™tre utilis√© pour tracer des graphiques de l'apprentissage r√©seau au fil du temps. C'est ainsi que j'ai construit tous les graphiques de ce chapitre. Cependant, si l'un des indicateurs n'est pas d√©fini, l'√©l√©ment de tuple correspondant sera une liste vide.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">D'autres ajouts de code incluent la m√©thode Network.save, qui enregistre l'objet Network sur le disque, et la fonction de le charger en m√©moire. L'enregistrement et le chargement se font via JSON, pas les modules Python pickle ou cPickle, qui sont g√©n√©ralement utilis√©s pour enregistrer sur le disque et charger en python. L'utilisation de JSON n√©cessite plus de code que ce qui serait n√©cessaire pour pickle ou cPickle. Pour comprendre pourquoi j'ai choisi JSON, imaginez qu'√† un moment donn√© dans le futur, nous avons d√©cid√© de changer notre classe de r√©seau afin qu'il y ait plus que des neurones sigmo√Ødes. Pour impl√©menter ce changement, nous changerions tr√®s probablement les attributs d√©finis dans la m√©thode Network .__ init__. Et si nous utilisions juste du cornichon pour √©conomiser, notre fonction de chargement ne fonctionnerait pas. L'utilisation de JSON avec une s√©rialisation explicite nous permet de garantir facilementque les anciennes versions de l'objet R√©seau peuvent √™tre t√©l√©charg√©es.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il y a beaucoup de petits changements dans le code, mais ce ne sont que de petites variations de network.py. </font><font style="vertical-align: inherit;">Le r√©sultat final est une extension de notre programme de 74 lignes √† un programme beaucoup plus fonctionnel de 152 lignes.</font></font><br><br><h3>  D√©fi </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modifiez le code ci-dessous en introduisant la r√©gularisation L1, et utilisez-le pour classer les chiffres MNIST par un r√©seau avec 30 neurones cach√©s. </font><font style="vertical-align: inherit;">Pouvez-vous choisir un param√®tre de r√©gularisation qui vous permet d'am√©liorer le r√©sultat par rapport √† un r√©seau sans r√©gularisation?</font></font></li><li>    Network.cost_derivative method  network.py.      .        ?     ,       ?  network2.py      Network.cost_derivative,     CrossEntropyCost.delta.      ? </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr459816/">https://habr.com/ru/post/fr459816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr459802/index.html">Habr Weekly # 9 / Burnout chez les jeunes, interfaces japonaises, r√©seau de neurones Battle.net, jeux et cruaut√©</a></li>
<li><a href="../fr459804/index.html">Cr√©ez des cartes d'aide de crowdsourcing sur WordPress + shMapper</a></li>
<li><a href="../fr459806/index.html">Comment nous avons trait√© le chat Lapuna</a></li>
<li><a href="../fr459810/index.html">Microservices ou monolith: √† la recherche d'une solution</a></li>
<li><a href="../fr459814/index.html">Que faites-vous, moteur de rendu? Ou comment fonctionne le module d'affichage du navigateur</a></li>
<li><a href="../fr459820/index.html">Il suffit de glisser la carte: comment OS / 2 est utilis√© dans le m√©tro de New York</a></li>
<li><a href="../fr459822/index.html">Un exemple d'un r√©seau neuronal simple, par cons√©quent, comprendre ce qui est</a></li>
<li><a href="../fr459824/index.html">Liste de contr√¥le pour l'√©criture de superbes extensions Visual Studio</a></li>
<li><a href="../fr459828/index.html">Nouvelles hebdomadaires: prix du billet Hyperloop en Russie, exploitation mini√®re grand public des ordinateurs Apollo, bot AI dans StarCraft II</a></li>
<li><a href="../fr459830/index.html">Bien s√ªr, ils ont donn√© le pouvoir et une ligne de mitrailleuse. Cancer et plus ... exp√©rience avec la m√©decine</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>