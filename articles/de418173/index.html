<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëßüèø üë®‚Äçüíª üë®üèª‚Äçüé® Roundtrip f√ºr neuronale Netze oder eine √úberpr√ºfung der Verwendung von Auto-Encodern in der Textanalyse üêè ü§æüèΩ ü§ôüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir haben bereits im ersten Artikel unseres Unternehmensblogs dar√ºber geschrieben, wie der Algorithmus zur Erkennung √ºbertragbarer Kredite funktionier...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Roundtrip f√ºr neuronale Netze oder eine √úberpr√ºfung der Verwendung von Auto-Encodern in der Textanalyse</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/antiplagiat/blog/418173/"> Wir haben bereits im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Artikel unseres Unternehmensblogs</a> dar√ºber geschrieben, wie der Algorithmus zur Erkennung √ºbertragbarer Kredite funktioniert.  Nur ein paar Abs√§tze in diesem Artikel widmen sich dem Thema des Vergleichens von Texten, obwohl die Idee eine viel detailliertere Beschreibung verdient.  Wie Sie wissen, kann man jedoch nicht sofort √ºber alles erz√§hlen, obwohl man es wirklich will.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">Oleg_Bakhteev</a> und ich haben diese Rezension geschrieben, um diesem Thema und der Architektur des Netzwerks namens " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">Auto-Encoder</a> ", dem wir sehr herzliche Gef√ºhle entgegenbringen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">Tribut zu zollen</a> . <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Learning f√ºr NLP (ohne Magie)</a> <br><br>  Wie wir in diesem Artikel erw√§hnt haben, war der Vergleich von Texten ‚Äûsemantisch‚Äú - wir haben nicht die Textfragmente selbst verglichen, sondern die ihnen entsprechenden Vektoren.  Solche Vektoren wurden als Ergebnis des Trainings eines neuronalen Netzwerks erhalten, das ein Textfragment beliebiger L√§nge in einen Vektor einer gro√üen, aber festen Dimension zeigte.  Wie man eine solche Zuordnung erh√§lt und wie man dem Netzwerk beibringt, die gew√ºnschten Ergebnisse zu erzielen, ist ein separates Thema, das weiter unten er√∂rtert wird. <br><a name="habracut"></a><br><h1>  Was ist ein Auto-Encoder? </h1><br>  Formal wird ein neuronales Netzwerk als Auto-Encoder (oder Auto-Encoder) bezeichnet, der trainiert, um am Netzwerkeingang empfangene Objekte wiederherzustellen. <br><img src="https://habrastorage.org/webt/jy/jw/ip/jyjwipnzwzlyidenzeovey3jba4.png"><br>  Der Auto-Encoder besteht aus zwei Teilen: einem Encoder <b>f</b> , der die Probe <b>X</b> in seine interne Darstellung <b>H</b> codiert, und einem Decoder <b>g</b> , der die urspr√ºngliche Probe wiederherstellt.  Daher versucht der Autocoder, die wiederhergestellte Version jedes Beispielobjekts mit dem urspr√ºnglichen Objekt zu kombinieren. <br><br>  Beim Training eines Auto-Encoders wird die folgende Funktion minimiert: <br><img src="https://habrastorage.org/webt/9f/ay/cm/9faycmmbldgcxvehefjrurcusyq.png"><br><br>  Wobei <b>r</b> f√ºr die wiederhergestellte Version des Originalobjekts steht: <br><img src="https://habrastorage.org/webt/zf/oe/oi/zfoeoiwfxnrscvv0n5cv4keku5k.png"><br><br>  Betrachten Sie das Beispiel in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">blog.keras.io</a> : <br><img src="https://habrastorage.org/webt/lw/wp/i_/lwwpi_kn0wyrduexhkqyrg9jttk.png"><br>  Das Netzwerk empf√§ngt ein Objekt <b>x</b> als Eingabe (in unserem Fall die Nummer 2). <br><br>  Unser Netzwerk verschl√ºsselt dieses Objekt in einen verborgenen Zustand.  Dann wird gem√§√ü dem latenten Zustand die Rekonstruktion des Objekts <b>r</b> wiederhergestellt, die x √§hnlich sein sollte.  Wie wir sehen, ist das wiederhergestellte Bild (rechts) verschwommener geworden.  Dies erkl√§rt sich aus der Tatsache, dass wir versuchen, nur die wichtigsten Zeichen des Objekts in einer verborgenen Ansicht zu halten, sodass das Objekt mit Verlusten wiederhergestellt wird. <br><br>  Das Autocoder-Modell ist nach dem Prinzip eines besch√§digten Telefons trainiert, bei dem eine Person (Encoder) Informationen <b>(x</b> ) an die zweite Person (Decoder) √ºbertr√§gt und diese wiederum an die dritte Person <b>(r (x)) weiterleitet</b> . <br><br>  Einer der Hauptzwecke solcher Auto-Encoder besteht darin, die Dimension des Quellraums zu verringern.  Wenn es sich um Auto-Encoder handelt, merkt sich das Auto-Encoder durch das Trainingsverfahren f√ºr neuronale Netze selbst die Hauptmerkmale von Objekten, von denen es einfacher ist, die urspr√ºnglichen Beispielobjekte wiederherzustellen. <br><br>  Hier k√∂nnen wir eine Analogie zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Methode der Hauptkomponenten ziehen</a> : Dies ist eine Methode zur Reduzierung der Dimension, deren Ergebnis die Projektion der Probe auf einen Unterraum ist, in dem die Varianz dieser Probe maximal ist. <br><br>  In der Tat ist der Auto-Encoder eine Verallgemeinerung der Hauptkomponentenmethode: Wenn wir uns auf die Betrachtung linearer Modelle beschr√§nken, geben der Auto-Encoder und die Hauptkomponentenmethode die gleichen Vektordarstellungen.  Der Unterschied ergibt sich, wenn wir komplexere Modelle, beispielsweise mehrschichtige, vollst√§ndig verbundene neuronale Netze, als Codierer und Decodierer betrachten. <br><br>  Ein Beispiel f√ºr einen Vergleich der Hauptkomponentenmethode und des Auto-Encoders finden Sie im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reduzieren der Dimensionalit√§t von Daten mit neuronalen Netzen</a> : <br><img src="https://habrastorage.org/webt/bj/ap/hq/bjaphq8tpjla39pbn2djsrm84y4.png"><br><br>  Hier werden die Ergebnisse des Trainings des Auto-Encoders und der Hauptkomponentenmethode zum Abtasten von Bildern menschlicher Gesichter demonstriert.  Die erste Zeile zeigt die Gesichter von Personen aus der Kontrollprobe, d.h.  aus einem speziell zur√ºckgestellten Teil der Stichprobe, der von den Algorithmen im Lernprozess nicht verwendet wurde.  In der zweiten und dritten Zeile befinden sich die wiederhergestellten Bilder aus den verborgenen Zust√§nden des Auto-Encoders bzw. der Hauptkomponentenmethode derselben Dimension.  Hier k√∂nnen Sie deutlich sehen, wie viel besser der Auto-Encoder funktioniert hat. <br><br>  Im selben Artikel ein weiteres anschauliches Beispiel: Vergleich der Ergebnisse des Auto-Encoders und der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LSA-</a> Methode f√ºr die Aufgabe des Informationsabrufs.  Die LSA-Methode ist wie die Hauptkomponentenmethode eine klassische Methode des maschinellen Lernens und wird h√§ufig bei Aufgaben im Zusammenhang mit der Verarbeitung nat√ºrlicher Sprache verwendet. <br><img src="https://habrastorage.org/webt/di/h5/j3/dih5j3wsflfohomgzzrjo9e6n7k.png"><br>  Die Abbildung zeigt eine 2D-Projektion mehrerer Dokumente, die mit dem Auto-Encoder und der LSA-Methode erstellt wurden.  Farben geben das Thema des Dokuments an.  Es ist ersichtlich, dass die Projektion vom Auto-Encoder Dokumente gut nach Themen aufschl√ºsselt, w√§hrend der LSA ein viel lauteres Ergebnis erzeugt. <br><br>  Eine weitere wichtige Anwendung von Auto- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Encodern ist das Netzwerk-Pre-Training</a> .  Das Netzwerk-Pre-Training wird verwendet, wenn das optimierte Netzwerk tief genug ist.  In diesem Fall kann es ziemlich schwierig sein, das Netzwerk ‚Äûvon Grund auf neu‚Äú zu trainieren. Daher wird zun√§chst das gesamte Netzwerk als eine Kette von Encodern dargestellt. <br><br>  Der Pre-Training-Algorithmus ist recht einfach: F√ºr jede Schicht trainieren wir unseren eigenen Auto-Encoder und stellen dann ein, dass der Ausgang des n√§chsten Encoders gleichzeitig der Eingang f√ºr die n√§chste Netzwerkschicht ist.  Das resultierende Modell besteht aus einer Kette von Encodern, die darauf trainiert sind, die wichtigsten Merkmale von Objekten eifrig auf ihrer eigenen Ebene zu erhalten.  Das Vorschulungsprogramm wird im Folgenden vorgestellt: <br><img src="https://habrastorage.org/webt/yy/mc/ui/yymcuigpqgegoa_7gwndzfxulia.png"><br>  Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">psyyz10.github.io</a> <br><br>  Diese Struktur wird als gestapelter Autoencoder bezeichnet und h√§ufig als ‚Äû√úbertaktung‚Äú verwendet, um das vollst√§ndige Deep-Network-Modell weiter zu trainieren.  Die Motivation f√ºr ein solches Training eines neuronalen Netzwerks besteht darin, dass ein tiefes neuronales Netzwerk eine nicht konvexe Funktion ist: W√§hrend des Trainings eines Netzwerks kann die Optimierung von Parametern in einem lokalen Minimum ‚Äûstecken bleiben‚Äú.  Durch das gierige Vortraining von Netzwerkparametern k√∂nnen Sie einen guten Ausgangspunkt f√ºr das endg√ºltige Training finden und versuchen, solche lokalen Minima zu vermeiden. <br><br>  Nat√ºrlich haben wir nicht alle m√∂glichen Strukturen ber√ºcksichtigt, da es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sparse Autoencoder</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Denoising Autoencoder</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Contractive Autoencoder</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reconstruction Contractive Autoencoder gibt</a> .  Sie unterscheiden sich untereinander durch die Verwendung verschiedener Fehlerfunktionen und Strafbegriffe.  Alle diese Architekturen verdienen unserer Meinung nach eine gesonderte Pr√ºfung.  In unserem Artikel zeigen wir zun√§chst das allgemeine Konzept von Auto-Encodern und die spezifischen Aufgaben der Textanalyse, die damit gel√∂st werden. <br><br><h2>  Wie funktioniert es in Texten? </h2><br>  Wir wenden uns nun spezifischen Beispielen f√ºr die Verwendung von Autocodern f√ºr Textanalyse-Aufgaben zu.  Wir interessieren uns f√ºr beide Seiten der Anwendung - beide Modelle zum Erhalten interner Darstellungen und die Verwendung dieser internen Darstellungen als Attribute, zum Beispiel f√ºr das weitere Klassifizierungsproblem.  In Artikeln zu diesem Thema werden h√§ufig Aufgaben wie die Stimmungsanalyse oder die Erkennung von Umformulierungen behandelt. Es gibt jedoch auch Arbeiten, die die Verwendung von Auto-Encodern zum Vergleichen von Texten in verschiedenen Sprachen oder zur maschinellen √úbersetzung beschreiben. <br><br>  Bei den Aufgaben der Textanalyse ist das Objekt meistens der Satz, d.h.  geordnete Folge von W√∂rtern.  Somit empf√§ngt der Auto-Encoder genau diese Folge von W√∂rtern oder vielmehr Vektordarstellungen dieser W√∂rter, die aus einem zuvor trainierten Modell stammen.  Was sind Vektordarstellungen von W√∂rtern, wurde Habr√© beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> hinreichend detailliert betrachtet.  Daher muss der Auto-Encoder, der eine Folge von W√∂rtern als Eingabe verwendet, eine interne Darstellung des gesamten Satzes trainieren, die den f√ºr uns wichtigen Merkmalen entspricht, basierend auf der Aufgabe.  Bei Problemen der Textanalyse m√ºssen wir S√§tze Vektoren so zuordnen, dass sie im Sinne einer Distanzfunktion nahe sind, meistens ein Kosinusma√ü: <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Learning f√ºr NLP (ohne Magie)</a> <br><br>  Einer der ersten Autoren, der den erfolgreichen Einsatz von Auto-Encodern in der Textanalyse zeigte, war <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Richard Socher</a> . <br><br>  In seinem Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dynamisches Pooling und Entfalten rekursiver Autoencoder f√ºr die Paraphrase-Erkennung</a> beschreibt er eine neue Autocodierungsstruktur - Entfalten rekursiver Autoencoder (Unfolding RAE) (siehe Abbildung unten). <br><img src="https://habrastorage.org/webt/zn/va/o9/znvao90juxs6ywwmm5ywwe6nrdm.png"><br>  RAE entfalten <br><br>  Es wird angenommen, dass die Satzstruktur von einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">syntaktischen Parser</a> definiert wird.  Die einfachste Struktur wird betrachtet - die Struktur eines Bin√§rbaums.  Ein solcher Baum besteht aus Bl√§ttern - W√∂rtern eines Fragments, internen Knoten (Verzweigungsknoten) - Phrasen und einem Endscheitelpunkt.  Unter Verwendung der Folge von W√∂rtern (x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> ) als Eingabe (drei Vektordarstellungen von W√∂rtern in diesem Beispiel) codiert der Autocoder in diesem Fall sequentiell von rechts nach links Vektordarstellungen von Satzw√∂rtern in Vektordarstellungen von Phrasen und dann in Vektoren Pr√§sentation des gesamten Angebots.  Insbesondere verketten wir in diesem Beispiel zuerst die Vektoren x <sub>2</sub> und x <sub>3</sub> und multiplizieren sie dann mit der Matrix <i>W <sub>e</sub></i> mit der <i>verborgenen</i> Dimension <i>√ó 2</i> , wobei <i>versteckt</i> die Gr√∂√üe der verborgenen internen Darstellung ist, <i>sichtbar</i> ist die Dimension des <i>Wortvektors</i> .  Daher reduzieren wir die Dimension und f√ºgen dann mithilfe der tanh-Funktion Nichtlinearit√§t hinzu.  Im ersten Schritt erhalten wir eine versteckte Vektordarstellung f√ºr die Phrase zwei W√∂rter <i>x <sub>2</sub></i> und <i>x <sub>3</sub></i> : <i>h <sub>1</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [x <sub>2</sub> , x <sub>3</sub> ] + b <sub>e</sub> )</i> .  Im zweiten kombinieren wir es und das verbleibende Wort <i>h <sub>2</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [h <sub>1</sub> , x <sub>1</sub> ] + b <sub>e</sub> )</i> und erhalten eine Vektordarstellung f√ºr den gesamten Satz - <i>h <sub>2</sub></i> .  Wie oben erw√§hnt, m√ºssen wir bei der Definition eines Auto-Encoders den Fehler zwischen Objekten und ihren wiederhergestellten Versionen minimieren.  In unserem Fall sind dies W√∂rter.  Nachdem wir die endg√ºltige Vektordarstellung des gesamten Satzes <i>h <sub>2 erhalten haben</sub></i> , werden wir seine wiederhergestellten Versionen (x <sub>1</sub> ', x <sub>2</sub> ', x <sub>3</sub> ') dekodieren.  Der Decoder arbeitet hier nach dem gleichen Prinzip wie der Encoder, nur die Parametermatrix und der Verschiebungsvektor unterscheiden sich hier: <i>W <sub>d</sub></i> und <i>b <sub>d</sub></i> . <br><br>  Mit der Struktur eines Bin√§rbaums k√∂nnen Sie S√§tze beliebiger L√§nge in einen Vektor fester Dimension codieren. Wir kombinieren immer ein Vektorpaar derselben Dimension mit derselben Parametermatrix <i>W <sub>e</sub></i> .  Im Fall eines nicht-bin√§ren Baums m√ºssen Sie die Matrizen nur im Voraus initialisieren, wenn Sie mehr als zwei W√∂rter kombinieren m√∂chten - 3, 4, ... n. In diesem Fall hat die Matrix nur die <i>verborgene</i> Dimension <i>√ó unsichtbar</i> . <br><br>  Es ist bemerkenswert, dass in diesem Artikel trainierte Vektordarstellungen von Phrasen nicht nur zur L√∂sung des Klassifizierungsproblems verwendet werden - einige S√§tze werden umformuliert oder nicht.  Die Daten eines Experiments zur Suche nach den n√§chsten Nachbarn werden ebenfalls dargestellt - nur basierend auf dem empfangenen Angebotsvektor werden die n√§chsten Vektoren in der Stichprobe gesucht, deren Bedeutung nahe beieinander liegt: <br><br><img src="https://habrastorage.org/webt/d6/pv/ae/d6pvaevnd18k2ouizgs3t9k0xbk.png"><br><br>  Niemand st√∂rt uns jedoch, andere Netzwerkarchitekturen zum Codieren und Decodieren zu verwenden, um W√∂rter nacheinander zu S√§tzen zu kombinieren. <br><br>  Hier ist ein Beispiel aus einem NIPS 2017-Artikel - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deconvolutional Paragraph Representation Learning</a> : <br><img src="https://habrastorage.org/webt/g9/u7/0m/g9u70mec8rpyrbnxuqtnflyxcfo.png"><br><br>  Wir sehen, dass die Codierung der Probe <b>X</b> in die verborgene Darstellung <b>h unter</b> Verwendung eines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungs-Neuronalen Netzwerks</a> erfolgt und der Decodierer nach dem gleichen Prinzip arbeitet. <br><br>  Oder hier ist ein Beispiel f√ºr die Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GRU-GRU</a> im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úberspringen von Gedankenvektoren</a> . <br><br>  Ein interessantes Merkmal hierbei ist, dass das Modell mit dreifachen S√§tzen arbeitet: ( <i>s <sub>i-1</sub> , s <sub>i</sub> , s <sub>i + 1</sub></i> ).  Der Satz <i>s <sub>i</sub></i> wird unter Verwendung von Standard-GRU-Formeln codiert, und der Decodierer versucht unter Verwendung der internen Darstellungsinformationen <i>s <sub>i</sub></i> , <i>s <sub>i-1</sub></i> und <i>s <sub>i + 1</sub></i> ebenfalls unter Verwendung von GRU zu decodieren. <br><br><img src="https://habrastorage.org/webt/ed/od/-r/edod-radj66y43mbsgu31zxo7nu.png"><br><br>  Das Funktionsprinzip √§hnelt in diesem Fall dem Standardmodell der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">maschinellen √úbersetzung neuronaler Netze</a> , das nach dem Encoder-Decoder-Schema arbeitet.  Da wir hier jedoch keine zwei Sprachen haben, senden wir eine Phrase in einer Sprache an die Eingabe unserer Codiereinheit und versuchen, sie wiederherzustellen.  W√§hrend des Lernprozesses werden einige interne Qualit√§tsfunktionen minimiert (dies ist nicht immer ein Rekonstruktionsfehler). Bei Bedarf werden vorab trainierte Vektoren als Merkmale in einem anderen Problem verwendet. <br><br>  In einem weiteren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zweisprachige rekursive Autoencoder f√ºr die statistische maschinelle √úbersetzung</a> , wird eine Architektur vorgestellt, die einen neuen Blick auf die maschinelle √úbersetzung wirft.  Zun√§chst werden rekursive Autocoder f√ºr zwei Sprachen separat trainiert (gem√§√ü dem oben beschriebenen Prinzip - wo Unfolding RAE eingef√ºhrt wurde).  Dann wird zwischen ihnen ein dritter Auto-Encoder trainiert - eine Zuordnung zwischen zwei Sprachen.  Eine solche Architektur hat einen klaren Vorteil: Wenn Sie Texte in verschiedenen Sprachen in einem gemeinsamen verborgenen Raum anzeigen, k√∂nnen Sie sie vergleichen, ohne die maschinelle √úbersetzung als Zwischenschritt zu verwenden. <br><br><img src="https://habrastorage.org/webt/tj/rq/un/tjrqunxjtnbgsivzz7iohnm8ibs.png"><br><br>  Das Training von Auto-Encodern f√ºr Textfragmente findet sich h√§ufig in Artikeln zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ranking-Training</a> .  Auch hier ist die Tatsache wichtig, dass wir die endg√ºltige Funktion der Qualit√§t des Rankings trainieren. Wir trainieren zuerst den Auto-Encoder, um die Vektoren der an die Netzwerkeingabe gesendeten Anforderungen und Antworten besser zu initialisieren. <br><br><img src="https://habrastorage.org/webt/5t/a1/qn/5ta1qnx9gqzl9dypb0t4nhgcjtg.jpeg"><br><br>  Und nat√ºrlich k√∂nnen wir <a href="">Variational Autoencoder</a> oder <a href="">VAEs nur</a> als generative Modelle erw√§hnen.  Am besten sehen Sie sich nat√ºrlich nur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Vortragseintrag von Yandex an</a> .  Es reicht aus, Folgendes zu sagen: Wenn wir Objekte aus dem verborgenen Raum eines herk√∂mmlichen Auto-Encoders <i>erzeugen</i> m√∂chten, ist die Qualit√§t einer solchen Erzeugung gering, da wir nichts √ºber die Verteilung der verborgenen Variablen wissen.  Sie k√∂nnen den Auto-Encoder jedoch sofort auf die Generierung trainieren und eine Verteilungsannahme einf√ºhren. <br><br>  Und dann k√∂nnen Sie mit VAE Texte aus diesem verborgenen Raum generieren, wie es beispielsweise die Autoren des Artikels <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Generieren von S√§tzen aus einem kontinuierlichen Raum</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem hybriden Faltungsvariations-Autoencoder f√ºr die Texterzeugung tun</a> . <br><br>  Die generativen Eigenschaften von VAE eignen sich auch gut f√ºr Aufgaben, bei denen Texte in verschiedenen Sprachen verglichen werden. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Ansatz zur variablen automatischen Codierung zur Induzierung mehrsprachiger Worteinbettungen ist</a> ein hervorragendes Beispiel daf√ºr. <br><br>  Abschlie√üend m√∂chten wir eine kleine Prognose abgeben.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repr√§sentationslernen</a> - Das Training in internen Repr√§sentationen mit genau VAE, insbesondere in Verbindung mit den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Generative Adversarial Networks</a> , ist einer der am weitesten entwickelten Ans√§tze der letzten Jahre. Dies l√§sst sich anhand der h√§ufigsten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikelthemen</a> der neuesten Top- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICLR 2018-Konferenzen zum</a> maschinellen Lernen beurteilen und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICML 2018</a> .  Dies ist ziemlich logisch, da seine Verwendung dazu beigetragen hat, die Qualit√§t bei einer Reihe von Aufgaben zu verbessern, und nicht nur bei Texten.  Aber das ist das Thema einer ganz anderen Rezension ... </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de418173/">https://habr.com/ru/post/de418173/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de418163/index.html">Produktionstests: Netflix Chaos Automation Platform</a></li>
<li><a href="../de418165/index.html">Quasar, Sobaken und Ungeziefer: Enth√ºllung von Details der laufenden Cyberspionagekampagne</a></li>
<li><a href="../de418167/index.html">ScadaPy: F√ºgen Sie das IEC 60870-5-104-Protokoll hinzu</a></li>
<li><a href="../de418169/index.html">Was ist neu in Veeam Availability Console 2.0 Update 1?</a></li>
<li><a href="../de418171/index.html">Auf welche Metriken sollten Sie sich verlassen, wenn Benutzer nur wenige Conversions auf der Website durchf√ºhren?</a></li>
<li><a href="../de418177/index.html">Bearbeiten von .heic-Bildern ohne Farbverlust</a></li>
<li><a href="../de418183/index.html">Anwendung der Sprachanalyse in der Wirtschaft</a></li>
<li><a href="../de418185/index.html">Eine Autopsie-Geschichte: Wie wir Hancitor r√ºckg√§ngig gemacht haben</a></li>
<li><a href="../de418187/index.html">In Amerika wurde vorgeschlagen, alle Bibliotheken durch Amazon-Hubs zu ersetzen. Die √ñffentlichkeit ist emp√∂rt</a></li>
<li><a href="../de418189/index.html">Erbe des Zeus: Warum der IcedID-Trojaner f√ºr Bankkunden gef√§hrlich ist</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>