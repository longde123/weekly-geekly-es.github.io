<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçüç≥ üï£ üòå So funktioniert das Rendern von 3D-Spielen: Rasterisierung und Raytracing üë©üèª‚ÄçüöÄ ü§∑üèø üë®‚Äçüöí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 1: Vertexverarbeitung 

 In diesem Artikel werden wir genauer untersuchen, was mit der 3D-Welt geschieht, nachdem alle ihre Scheitelpunkte verarb...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So funktioniert das Rendern von 3D-Spielen: Rasterisierung und Raytracing</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480338/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ad/2f0/8c9/3ad2f08c927f84fd66245141de4ad34d.png" alt="Bild"></div><br>  <a href="https://habr.com/ru/post/472688/">Teil 1: Vertexverarbeitung</a> <br><br>  In diesem Artikel werden wir genauer untersuchen, was mit der 3D-Welt geschieht, nachdem alle ihre Scheitelpunkte verarbeitet wurden.  Wir m√ºssen wieder den Staub aus den Mathe-Lehrb√ºchern absch√ºtteln, uns an die Geometrie der Pyramidenst√ºmpfe gew√∂hnen und das R√§tsel der Perspektiven l√∂sen.  Wir werden auch kurz auf die Physik von Raytracing, Beleuchtung und Materialien eingehen. <br><br>  Das Hauptthema dieses Artikels ist eine wichtige Rendering-Phase, in der die dreidimensionale Welt der Punkte, Segmente und Dreiecke zu einem zweidimensionalen Raster aus mehrfarbigen Bl√∂cken wird.  Sehr oft scheint dieser Prozess unsichtbar zu sein, da die Konvertierung von 3D zu 2D nicht sichtbar ist, im Gegensatz zu dem im <a href="https://www.techspot.com/article/1857-how-to-3d-rendering-vertex-processing/">vorherigen Artikel</a> beschriebenen Prozess, bei dem wir den Einfluss von Vertex-Shadern und Tessellation sofort erkennen konnten.  Wenn Sie noch nicht dazu bereit sind, k√∂nnen Sie mit unserem Artikel <a href="https://www.techspot.com/article/1851-3d-game-rendering-explained/">3D Game Rendering 101 beginnen</a> . <br><br><h2>  Vorbereitung f√ºr zwei Messungen </h2><br>  Die √ºberwiegende Mehrheit der Leser liest diese Website auf einem vollst√§ndig flachen Monitor oder Smartphone-Bildschirm.  Aber selbst wenn Sie √ºber eine moderne Technik verf√ºgen - einen gekr√ºmmten Monitor - besteht das von ihm angezeigte Bild ebenfalls aus einem flachen Raster aus mehrfarbigen Pixeln.  Wenn Sie jedoch den neuen Call of Mario: Deathduty Battleyard spielen, wirken die Bilder dreidimensional.  Objekte bewegen sich in der Szene, werden gr√∂√üer oder kleiner und n√§hern sich der Kamera und entfernen sich von ihr. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/46b/506/855/46b506855796dd802382dc4fde347751.jpg"></div><br>  Am Beispiel von Bethesdas <a href="https://www.techspot.com/review/1089-fallout-4-benchmarks/">Fallout 4 aus dem Jahr</a> 2014 k√∂nnen wir leicht sehen, wie die Peaks verarbeitet werden, wodurch ein Gef√ºhl von Tiefe und Distanz entsteht.  Dies macht sich insbesondere im Wireframe-Modus bemerkbar (siehe oben). <br><br>  Wenn Sie in den letzten zwei Jahrzehnten ein 3D-Spiel spielen, f√ºhrt fast jedes dieselbe Abfolge von Aktionen aus, um die 3D-Welt der Scheitelpunkte in ein 2D-Pixelarray umzuwandeln.  Diese Konvertierung wird oft als <em>Rasterisierung bezeichnet</em> , dies ist jedoch nur einer von vielen Schritten im gesamten Prozess. <br><br>  Wir m√ºssen die verschiedenen Stadien analysieren und die in ihnen verwendeten Techniken und Berechnungen studieren.  Als Referenz verwenden wir die in Direct3D verwendete Sequenz.  Das Bild unten zeigt, was mit jedem Scheitelpunkt der Welt passiert: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/774/621/43c/77462143c6d436fe0ad9145815956cf5.png"></div><br>  <i>Direct3D-Konvertierungspipeline</i> <br><br>  Im <a href="https://www.techspot.com/article/1857-how-to-3d-rendering-vertex-processing/">ersten Artikel</a> [ <a href="https://habr.com/ru/post/472688/">√úbersetzung</a> auf Habr√©] haben wir gesehen, was im Weltraum (World Space) passiert: Hier werden die Eckpunkte mithilfe verschiedener Matrixberechnungen transformiert und farbig dargestellt.  Wir werden den n√§chsten Schritt √ºberspringen, da im Kamerabereich nur die Scheitelpunkte konvertiert und nach dem Verschieben angepasst werden, sodass die Kamera zu einem Referenzpunkt wird. <br><br>  Die folgenden Schritte sind zu kompliziert, um sie zu √ºberspringen, da sie f√ºr den √úbergang von 3D zu 2D unbedingt erforderlich sind. Bei korrekter Implementierung betrachtet unser Gehirn einen Flachbildschirm, "sieht" jedoch eine Szene mit Tiefe und Skalierung.  Wenn alles falsch gemacht wird, wird das Bild sehr seltsam sein! <br><br><h2>  Es geht nur um Perspektive </h2><br>  Der erste Schritt in dieser Sequenz ist das Einstellen des Bereichs aus Sicht der Kamera.  Dazu m√ºssen Sie zuerst die Winkel des horizontalen und vertikalen Sichtfelds einstellen - die ersten √Ñnderungen treten h√§ufig in Spielen auf, da die Menschen die horizontale periphere Sicht besser als die vertikale entwickelt haben. <br><br>  Wir k√∂nnen das herausfinden, indem wir das Bild mit dem Blickfeld einer Person betrachten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/754/37e/69b/75437e69be055c93670bb139fc2e47d9.png"></div><br>  Zwei Ecken des Sichtfelds (Sichtfeld, fov) definieren die Form der <em>Kegelstumpfpyramide</em> - eine 3D-Pyramide mit einer quadratischen Basis, die von der Kamera ausgeht.  Die erste Ecke legt das <em>vertikale</em> Sichtfeld fest, die zweite das <em>horizontale</em> .  wir bezeichnen sie mit den Symbolen <em>Œ±</em> und <em>Œ≤</em> .  In der Tat sehen wir die Welt nicht ganz so, aber aus der Sicht der Berechnungen ist es viel einfacher, mit der Trunkierungspyramide zu arbeiten, als zu versuchen, ein realistisches Ma√ü an Sichtbarkeit zu generieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cc/62f/e5f/0cc62fe5fdb171e47cc3428fa5bdd427.png"></div><br>  Sie m√ºssen auch zwei weitere Parameter angeben - die Position der nahen (oder vorderen) und der fernen (hinteren) <em>Schnittebene (Schnittebene)</em> .  Die erste Option schneidet die Oberseite der Pyramide ab, bestimmt jedoch im Wesentlichen, wie nah an der Kameraposition alles gezeichnet wird.  Letzteres macht dasselbe, bestimmt aber, wie weit die Grundelemente von der Kamera entfernt gerendert werden. <br><br>  Die Gr√∂√üe und Position der nahen Schnittebene ist sehr wichtig, da sie zu einem so genannten <em>Ansichtsfenster wird</em> .  In der Tat sehen wir dies auf dem Monitor, d.h.  gerenderten Rahmen und in den meisten Grafik-APIs wird das Ansichtsfenster von der oberen linken Ecke gezeichnet.  Im Bild unten ist der Punkt (a1, b2) der Ursprung der Ebene: Die Breite und H√∂he der Ebene werden relativ dazu gemessen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cf8/d66/f95/cf8d66f95ab085a290a35d996f94ed3d.png"></div><br>  <em>Das Seitenverh√§ltnis des</em> Ansichtsfensters ist nicht nur wichtig, um die gerenderte Welt anzuzeigen, sondern auch, um das Seitenverh√§ltnis des Monitors anzupassen.  F√ºr viele Jahre war der Standard 4: 3 (oder 1,3333 ... als Dezimalzahl).  Heutzutage spielt die Mehrheit jedoch in einem 16: 9- oder 21: 9-Seitenverh√§ltnis, das als Widescreen und Ultra-Widescreen bezeichnet wird. <br><br>  Die Koordinaten jedes Scheitelpunkts im Kameraraum m√ºssen so transformiert werden, dass sie alle auf die nahe K√ºrzungsebene passen, wie unten gezeigt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ba3/73f/69c/ba373f69c98de8b0000785606d1d56df.png"></div><br>  <i>Pyramidenseite und -oberseite trimmen</i> <br><br>  Die Transformation wird unter Verwendung einer anderen Matrix ausgef√ºhrt, die als <em>perspektivische Projektionsmatrix bezeichnet wird</em> .  Im folgenden Beispiel verwenden wir zum Ausf√ºhren der Transformationen die Winkel des Bereichs und die Position der K√ºrzungsebenen.  Sie k√∂nnen jedoch stattdessen die Gr√∂√üe des Ansichtsfensters verwenden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d76/32e/2e1/d7632e2e15dd515df9f624a62f8fdce9.png"></div><br>  Der Vertex-Positionsvektor wird mit dieser Matrix multipliziert, wodurch wir einen neuen Satz transformierter Koordinaten erhalten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/267/8d6/a1d/2678d6a1d623c710e22675d54d05eaa2.png"></div><br>  Voila!  Jetzt sind alle Scheitelpunkte so geschrieben, dass die Quellwelt als 3D-Perspektive dargestellt wird und die Primitiven in der N√§he der vorderen K√ºrzungsebene gr√∂√üer erscheinen als diejenigen, die n√§her an der fernen Ebene liegen. <br><br>  Die Gr√∂√üe des Ansichtsfensters und der Betrachtungswinkel h√§ngen zwar zusammen, sie k√∂nnen jedoch einzeln verarbeitet werden.  Mit anderen Worten, Sie k√∂nnen die K√ºrzungspyramide so einstellen, dass Sie eine nahe K√ºrzungsebene erhalten, die sich in Gr√∂√üe und Seitenverh√§ltnis vom Ansichtsfenster unterscheidet.  Dazu ist ein zus√§tzlicher Schritt in der Operationskette erforderlich, bei dem die Scheitelpunkte in der nahen K√ºrzungsebene erneut transformiert werden m√ºssen, um diesen Unterschied zu ber√ºcksichtigen. <br><br>  Dies kann jedoch zu einer Verzerrung der sichtbaren Perspektive f√ºhren.  Am Beispiel des Bethesda <a href="https://www.techspot.com/products/pc-games/the-elder-scrolls-5-skyrim.4195/">Skyrim 2011-Spiels</a> k√∂nnen wir sehen, wie sich das √Ñndern des horizontalen Winkels des Sichtbarkeitsbereichs <em>Œ≤</em> bei gleichem Seitenverh√§ltnis des Ansichtsfensters stark auf die Szene auswirkt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fe9/2b8/4f7/fe92b84f7381a559d307517a2d46258a.jpg"></div><br>  In diesem ersten Bild setzen wir <em>Œ≤</em> = 75 ¬∞ und die Szene sieht v√∂llig normal aus.  Versuchen wir nun <em>Œ≤</em> = 120 ¬∞ zu setzen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e0/bf9/a3b/4e0bf9a3b44c400447fb73393a26916d.jpg"></div><br>  Zwei Unterschiede sind sofort erkennbar - erstens sehen wir jetzt viel mehr auf den Seiten unseres "Sichtfeldes";  zweitens scheinen Objekte jetzt viel weiter entfernt zu sein (insbesondere B√§ume).  Der visuelle Effekt auf der Wasseroberfl√§che sieht jetzt jedoch falsch aus, da der Prozess nicht f√ºr einen solchen Sichtbereich ausgelegt war. <br><br>  Stellen wir uns nun vor, unser Charakter h√§tte fremde Augen und setze <em>Œ≤</em> = 180 ¬∞! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a6e/548/b9f/a6e548b9faa9cba6e3fa80a33a95444d.jpg"></div><br>  Ein derartiger Sichtbereich erzeugt eine fast panoramische Szene, die Sie jedoch mit einer erheblichen Verzerrung der an den R√§ndern gerenderten Objekte bezahlen m√ºssen.  Dies geschah erneut aufgrund der Tatsache, dass die Spieleentwickler eine solche Situation nicht vorausgesehen und die Ressourcen und visuellen Effekte des Spiels f√ºr einen solchen Betrachtungswinkel nicht erstellt haben (der Standardwert betr√§gt ungef√§hr 70 ¬∞). <br><br>  Es mag den Anschein haben, dass sich die Kamera in den obigen Bildern bewegt hat, aber dies ist nicht der Fall. Die einzige √Ñnderung besteht darin, die Pyramidenabschneidung zu modifizieren, wodurch sich wiederum die Abmessungen der nahen Abschneidungsebene ge√§ndert haben.  Bei jedem Bild bleibt das Seitenverh√§ltnis des Ansichtsfensters gleich, sodass die Skalierungsmatrix auf die Scheitelpunkte angewendet wird, sodass alles hineinpasst. <br><br><h2>  Bleibst du oder gehst du? </h2><br>  Nachdem wir die Transformationen in der Projektionsphase durchgef√ºhrt haben, fahren wir mit dem sogenannten <em>Clip-Raum fort</em> .  Obwohl dies <em>nach der</em> Projektion erfolgt, ist es einfacher zu zeigen, was passiert, wenn wir die Vorg√§nge im Voraus ausf√ºhren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8d1/5aa/921/8d15aa92153b56847f0a3eaba62000a0.png"></div><br>  In der Abbildung oben sehen wir, dass sich bei der Gummiente, einem der Flederm√§use und einem Teil der B√§ume, die Dreiecke innerhalb der Pyramidenst√ºmpfe befinden.  Die andere Fledermaus und der am weitesten entfernte Baum befinden sich jedoch au√üerhalb der Grenzen der Pyramidenst√ºmpfe.  Obwohl die Eckpunkte, aus denen diese Objekte bestehen, bereits verarbeitet wurden, werden sie im Ansichtsfenster nicht angezeigt.  Dies bedeutet, dass sie <em>abgeschnitten sind</em> . <br><br>  Beim <em>K√ºrzen entlang der Pyramide (Kegelstumpfbeschneidung) werden</em> alle Primitive au√üerhalb der <em>Kegelstumpfpyramide</em> vollst√§ndig gel√∂scht und die an den R√§ndern liegenden in neue Primitive umgewandelt.  Durch das Abschneiden wird die Leistung nicht wesentlich verbessert, da alle diese unsichtbaren Scheitelpunkte bereits vor dieser Phase in Scheitelpunktshadern usw. verarbeitet wurden.  Bei Bedarf kann der gesamte K√ºrzungsschritt sogar vollst√§ndig √ºbersprungen werden, diese Funktion wird jedoch nicht von allen APIs unterst√ºtzt (z. B. l√§sst die Standard-OpenGL das √úberspringen nicht zu, dies kann jedoch mit der API-Erweiterung erfolgen). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a5/e07/57c/7a5e0757c7f8368a6d419c59468ec03e.png"></div><br>  Es ist anzumerken, dass die Position der Fernk√ºrzungsebene in Spielen nicht immer gleich der <em>Ziehstrecke ist</em> , da letztere von der Spielmaschine selbst gesteuert wird.  Die Engine <em>schneidet auch die Pyramide ab (kegelstumpff√∂rmiges Keulen).</em> Sie f√ºhrt einen Code aus, der festlegt, ob das Objekt in die Pyramidenstumpfzeichnung einbezogen wird und ob sichtbare Objekte betroffen sind.  Wenn die Antwort <em>Nein</em> lautet, wird das Objekt nicht zum Rendern √ºbertragen.  Dies ist nicht dasselbe wie Frustrum-Clipping, da es auch Primitive au√üerhalb der Pyramide verwirft, aber sie haben die Vertex-Verarbeitungsstufe bereits durchlaufen.  Beim Keulen werden sie √ºberhaupt nicht verarbeitet, was eine Menge Ressourcen einspart. <br><br>  Wir haben alle Transformationen und K√ºrzungen vorgenommen, und es scheint, dass die Eckpunkte endlich f√ºr den n√§chsten Schritt in der Rendering-Sequenz bereit sind.  Tats√§chlich ist dies jedoch nicht der Fall, da alle Berechnungen, die in der Scheitelpunktverarbeitungsphase und bei den Transformationsoperationen vom Weltraum zum Trunkierungsraum durchgef√ºhrt werden, in einem einheitlichen Koordinatensystem durchgef√ºhrt werden m√ºssen (d. H. Jeder Scheitelpunkt hat 4 Komponenten, nicht 3). .  Das Ansichtsfenster ist jedoch vollst√§ndig zweidimensional, dh die API erwartet, dass die Scheitelpunktinformationen nur die Werte f√ºr <em>x, y enthalten</em> (obwohl der Wert f√ºr die Tiefe <em>z</em> gespeichert ist). <br><br>  Um die vierte Komponente loszuwerden, wird eine <em>perspektivische Unterteilung</em> durchgef√ºhrt, bei der jede Komponente durch den Wert von <em>w geteilt wird</em> .  Diese Operation beschr√§nkt <em>x</em> und <em>y auf das</em> Intervall m√∂glicher Werte [-1,1] und <em>z</em> auf das Intervall [0,1].  Diese werden als <em>normalisierte Ger√§tekoordinaten</em> (NDC) bezeichnet. <br><br>  Wenn Sie mehr √ºber das, was wir gerade erkl√§rt haben, erfahren m√∂chten und Mathe m√∂gen, lesen Sie das <a href="http://www.songho.ca/opengl/gl_transform.html">hervorragende Tutorial</a> zu diesem Thema Song Ho An.  Lassen Sie uns nun diese Eckpunkte in Pixel umwandeln! <br><br><h2>  Wir beherrschen die Rasterung </h2><br>  Wie bei Transformationen werden wir uns die Regeln und Prozesse ansehen, die verwendet werden, um ein Ansichtsfenster in ein Pixelraster umzuwandeln, wobei Direct3D als Beispiel dient.  Diese Tabelle √§hnelt einer Excel-Tabelle mit Zeilen und Spalten, in denen jede Zelle unterschiedliche Datenwerte enth√§lt (z. B. Farbe, Tiefenwerte, Texturkoordinaten usw.).  Normalerweise wird dieses Raster als <em>Rasterbild bezeichnet</em> , und der Prozess seiner Erzeugung wird als <em>Rasterisierung bezeichnet</em> .  Im Artikel <a href="https://www.techspot.com/article/1851-3d-game-rendering-explained/">3D-Rendering 101 haben</a> wir diese Prozedur vereinfacht: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3b6/01f/0e7/3b601f0e71774ce64eeb0a8f75a83f67.png"></div><br>  Das obige Bild vermittelt den Eindruck, dass die Grundelemente einfach in kleine Bl√∂cke geschnitten werden, in Wirklichkeit gibt es jedoch viel mehr Operationen.  Der allererste Schritt besteht darin, zu bestimmen, ob das Grundelement der Kamera zugewandt ist. Beispielsweise sind in dem obigen Bild mit einer Pyramidenstumpf die Grundelemente, aus denen der R√ºcken des grauen Kaninchens besteht, nicht sichtbar.  Daher m√ºssen sie nicht gerendert werden, obwohl sie im Ansichtsfenster vorhanden sind. <br><br>  Wir k√∂nnen uns ungef√§hr vorstellen, wie es aussieht, wenn wir uns das Diagramm unten ansehen.  Der W√ºrfel wurde verschiedenen Transformationen unterzogen, um das 3D-Modell im 2D-Raum des Bildschirms zu platzieren. Aus Sicht der Kamera sind einige Fl√§chen des W√ºrfels nicht sichtbar.  Wenn wir annehmen, dass alle Oberfl√§chen undurchsichtig sind, k√∂nnen einige dieser Grundelemente ignoriert werden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/693/60a/bf9/69360abf9888e28d60622cf189cbb6c3.png"></div><br>  <i>Von links nach rechts: Weltraum&gt; Kameraraum&gt; Projektionsraum&gt; Leinwandraum</i> <br><br>  In Direct3D kann dies implementiert werden, indem dem System <em>mitgeteilt</em> wird, wie der <em>Rendering-Status</em> lauten soll. Mit dieser Anweisung wird klargestellt, dass die Seiten jedes primitiven Elements entfernt ( <em>abgeschnitten</em> ) werden m√ºssen, die nach vorne oder hinten schauen (oder nicht vollst√§ndig abgeschnitten werden m√ºssen, z. B. im <em>Drahtgittermodus</em> ). .  Aber woher wei√ü sie, welche Seite vorw√§rts oder r√ºckw√§rts schaut?  Als wir die <a href="https://www.techspot.com/article/1857-how-to-3d-rendering-vertex-processing/">Mathematik der Eckenverarbeitung untersuchten</a> , stellten wir fest, dass Dreiecke (oder vielmehr Ecken) normale Vektoren haben, die dem System mitteilen, in welche Richtung es schaut.  Dank dieser Informationen k√∂nnen Sie eine einfache √úberpr√ºfung durchf√ºhren. Wenn das Grundelement fehlschl√§gt, wird es aus der Rendering-Kette entfernt. <br><br>  Jetzt ist es Zeit, das Pixelraster anzuwenden.  Dies ist wiederum ein unerwartet komplexer Vorgang, da das System verstehen muss, ob sich das Pixel innerhalb des Grundelements befindet - vollst√§ndig, teilweise oder √ºberhaupt nicht.  Dazu wird der <em>Coverage-Test</em> durchgef√ºhrt.  Die folgende Abbildung zeigt, wie Dreiecke in Direct3D 11 gerastert werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/858/967/ee6/858967ee6e11144896c35c2c726ec606.png"></div><br>  Die Regel ist recht einfach: Ein Pixel wird als innerhalb des Dreiecks liegend betrachtet, wenn die Mitte des Pixels eine Pr√ºfung besteht, die Microsoft als <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d11/d3d10-graphics-programming-guide-rasterizer-stage-rules">Regel "oben links" bezeichnet</a> .  "Oben" bezieht sich auf die √úberpr√ºfung der horizontalen Linie;  Die Mitte des Pixels sollte <em>auf</em> dieser Linie liegen.  "Links" bezieht sich auf nicht horizontale Linien, und die Mitte des Pixels sollte links von einer solchen Linie liegen.  Es gibt andere Regeln, die sich auf Nicht-Primitive beziehen, z. B. einfache Segmente und Punkte, und bei Verwendung von <em>Multisampling</em> zus√§tzliche, wenn in den Regeln Bedingungen auftreten. <br><br>  Wenn Sie sich die Microsoft-Dokumentation genau ansehen, k√∂nnen Sie feststellen, dass die durch die Pixel erzeugten Formen den urspr√ºnglichen Grundelementen nicht sehr √§hnlich sind.  Dies liegt daran, dass die Pixel zu gro√ü sind, um ein realistisches Dreieck zu erstellen. Das Bitmap-Bild enth√§lt nicht gen√ºgend Daten zu den Originalobjekten. Dies f√ºhrt zu einem Ph√§nomen namens <em>Aliasing</em> . <br><br>  Schauen wir uns das Aliasing <a href="https://benchmarks.ul.com/legacy-benchmarks">anhand</a> eines Beispiels f√ºr <a href="https://benchmarks.ul.com/legacy-benchmarks">UL Benchmark 3DMark03 an</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f11/9d2/289/f119d2289c67082f66e4467385bc07be.jpg"></div><br>  <i>720 x 480 Pixel Rasterisierung</i> <br><br>  Im ersten Bild hat das Rasterbild eine sehr niedrige Aufl√∂sung - 720 x 480 Pixel.  Das Aliasing ist auf dem Gel√§nder und im Schatten der Waffen des oberen Soldaten deutlich zu erkennen.  Vergleichen Sie dies mit dem Ergebnis, das w√§hrend der Rasterung mit einer 24-fachen Erh√∂hung der Pixelanzahl erhalten wurde: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ea8/eba/d3a/ea8ebad3a86c108326f069fc343baed7.jpg"></div><br>  <i>Rasterisierung 3840 x 2160 Pixel</i> <br><br>  Hier sehen wir, dass das Aliasing auf dem Gel√§nder und dem Schatten vollst√§ndig verschwunden ist.  Es scheint, dass Sie immer eine gro√üe Bitmap verwenden sollten, aber die Rastergr√∂√üe sollte von dem Monitor unterst√ºtzt werden, auf dem der Rahmen angezeigt wird.  Unter Ber√ºcksichtigung der Tatsache, dass all diese Pixel verarbeitet werden m√ºssen, ist es offensichtlich, dass die Leistung sinken wird. <br><br>  Multisampling kann hier helfen.  So funktioniert es in Direct3D: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/64b/79e/62f/64b79e62fbcd12197ba9ff2c769045ae.png"></div><br>  Anstatt zu √ºberpr√ºfen, ob die Mitte des Pixels den Rasterungsregeln entspricht, werden mehrere Punkte in jedem Pixel (Subpixel-Abtastwerte oder Sub-Abtastwerte genannt) <em>√ºberpr√ºft</em> , und wenn einige von ihnen die Anforderungen erf√ºllen, bilden sie einen Teil der Figur.  Es mag den Anschein haben, als g√§be es keinen Vorteil, und das Aliasing wird sogar verbessert. Bei Verwendung von Multisampling werden jedoch Informationen dar√ºber, welche Unterabtastwerte vom Grundelement abgedeckt werden und die Ergebnisse der Verarbeitung von Pixeln in einem Pufferspeicher gespeichert. <br><br>  Dieser Puffer wird dann verwendet, um diese Unterabtastwerte und Pixel zu mischen, so dass die Kanten des Grundelements weniger zerrissen werden.  Wir werden das Aliasing in einem anderen Artikel ausf√ºhrlicher betrachten, aber diese Informationen reichen vorerst aus, um zu verstehen, was Multisampling tun kann, wenn zu wenige Pixel gerastert werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/703/466/b61/703466b61fe655bfe9956003b707d9d5.jpg"></div><br>  Wie Sie sehen, hat sich das Ausma√ü des Aliasing an den R√§ndern verschiedener Formen erheblich verringert.  Eine Rasterung mit h√∂herer Aufl√∂sung ist definitiv besser, aber ein Leistungsabfall kann dazu f√ºhren, dass Sie Multisampling verwenden. <br><br>  Auch w√§hrend der Rasterung wird ein <em>Okklusionstest</em> durchgef√ºhrt.  Dies ist erforderlich, da das Ansichtsfenster mit √ºberlagerten Grundelementen gef√ºllt wird. In der obigen Abbildung √ºberlappen beispielsweise die vorausschauenden Dreiecke, aus denen der Soldat im Vordergrund besteht, dieselben Dreiecke eines anderen Soldaten.  Neben der √úberpr√ºfung, ob das Grundelement ein Pixel abdeckt, k√∂nnen Sie auch die relativen Tiefen vergleichen. Befindet sich eine Oberfl√§che hinter der anderen, muss sie aus dem verbleibenden Renderprozess entfernt werden. <br><br>  Wenn jedoch das nahe Grundelement transparent ist, bleibt das ferne Grundelement sichtbar, obwohl es den √úberlappungstest nicht besteht.  Aus diesem Grund f√ºhren fast alle 3D-Engines <em>vor dem</em> Senden von Daten an die GPU √úberlappungspr√ºfungen durch und erstellen stattdessen einen sogenannten <em>Z-Puffer</em> , der Teil des Renderprozesses ist.  Hier wird der Rahmen auf die √ºbliche Weise erstellt, aber anstatt die vorgefertigten Pixelfarben im Speicher zu speichern, speichert die GPU nur die Tiefenwerte.  Sp√§ter k√∂nnen sie in Shadern verwendet werden, um die Sichtbarkeit zu √ºberpr√ºfen und Aspekte in Bezug auf √ºberlappende Objekte mit gro√üer Kontrolle und Genauigkeit zu √ºberpr√ºfen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f53/2bc/c01/f532bcc017258005f427aaed4e3e482e.png"></div><br>  Je dunkler die Pixelfarbe in dem oben gezeigten Bild ist, desto n√§her ist das Motiv an der Kamera.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Frame wird einmal gerendert, um einen Z-Puffer zu erstellen, und dann erneut gerendert. Diesmal wird jedoch w√§hrend der Verarbeitung der Pixel ein Shader gestartet, der sie auf Werte im Z-Puffer √ºberpr√ºft. Wenn es unsichtbar ist, wird die Pixelfarbe nicht in den Puffer des fertigen Rahmens geschrieben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bisher wird unser letzter Hauptschritt die </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Interpolation von Scheitelpunktattributen sein</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - im urspr√ºnglichen vereinfachten Schema war das Grundelement ein vollst√§ndiges Dreieck, aber vergessen Sie nicht, dass das Ansichtsfenster nur mit den Ecken der Figuren und nicht mit den Figuren selbst gef√ºllt ist. Das hei√üt, das System muss bestimmen, welche Farbe, Tiefe und Textur des Grundelements zwischen den Scheitelpunkten liegen soll, und diese Operation wird als </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Interpolation bezeichnet</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Wie Sie vielleicht erraten haben, ist dies eine weitere Berechnung, und es ist nicht so einfach.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obwohl der gerasterte Bildschirm in 2D dargestellt wird, repr√§sentieren die darin enthaltenen Strukturen eine 3D-Perspektive. </font><font style="vertical-align: inherit;">Wenn die Linien wirklich zweidimensional w√§ren, k√∂nnten wir eine einfache </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lineare Gleichung verwenden</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , um Farben und andere Dinge zu berechnen </font><font style="vertical-align: inherit;">, da wir uns von einem Scheitelpunkt zum anderen bewegen. </font><font style="vertical-align: inherit;">Aufgrund des 3D-Aspekts der Szene muss die Interpolation diese Perspektive ber√ºcksichtigen. </font><font style="vertical-align: inherit;">Um mehr √ºber diesen Prozess zu erfahren, lesen Sie den </font></font><a href="http://simonstechblog.blogspot.com/2012/04/software-rasterizer-part-2.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ausgezeichneten Artikel von Simon Young</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Damit ist die Aufgabe erledigt - die 3D-Welt der Eckpunkte verwandelt sich in ein 2D-Raster aus bunten Bl√∂cken. </font><font style="vertical-align: inherit;">Aber wir sind noch nicht ganz fertig.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Von vorne nach hinten (mit einigen Ausnahmen) </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bevor wir mit der Rasterisierung fertig sind, m√ºssen wir √ºber die Reihenfolge der Rendering-Sequenz sprechen. Wir sprechen nicht √ºber die Phase, in der beispielsweise eine Tessellation in der Verarbeitungssequenz auftritt. Wir meinen die Reihenfolge, in der Primitive verarbeitet werden. Objekte werden normalerweise in der Reihenfolge verarbeitet, in der sie sich im Indexpuffer befinden (ein Speicherblock, der dem System mitteilt, wie Scheitelpunkte gruppiert sind). Dies kann die Verarbeitung transparenter Objekte und Effekte erheblich beeinflussen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Grund daf√ºr ist, dass die Primitiven einzeln verarbeitet werden. Wenn Sie die Primitiven zuerst vorne rendern, sind alle dahinter liegenden Elemente unsichtbar (hier kommt Okklusions-Culling ins Spiel) und k√∂nnen aus dem Prozess entfernt werden (um zu sparen) Leistung). Dies wird normalerweise als </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Front-to-Back-</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Rendering bezeichnet </font><font style="vertical-align: inherit;">, und f√ºr diesen Prozess muss der Indexpuffer auf diese Weise sortiert werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn jedoch einige dieser Grundelemente vor der Kamera transparent sind, f√ºhrt das Rendern von vorne nach hinten zum Verlust von Objekten, die hinter der Kamera transparent sind. Eine L√∂sung ist das Rendern von hinten nach vorne, wobei transparente Grundelemente und Effekte zuletzt berechnet werden.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ca/c02/6f7/3cac026f78b545088b8de84fcdd11ee4.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Von links nach rechts: Die Reihenfolge in der Szene, Rendern von vorne nach hinten, Rendern von hinten nach vorne</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Das hei√üt, in allen modernen Spielen wird das Rendern von hinten nach vorne ausgef√ºhrt? Wie auch immer - vergessen Sie nicht, dass das Rendern jedes einzelnen Grundelements zu einem viel st√§rkeren Leistungsabfall f√ºhrt als das Rendern nur des, was wir sehen. Es gibt andere M√∂glichkeiten, transparente Objekte zu verarbeiten, aber im Allgemeinen gibt es keine ideale L√∂sung, die f√ºr jedes System geeignet ist, und jede Situation muss separat betrachtet werden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf diese Weise k√∂nnen wir die wichtigsten Vor- und Nachteile der Rasterung verstehen. Bei modernen Ger√§ten handelt es sich um einen schnellen und effizienten Vorgang, der jedoch nur ann√§hernd das widerspiegelt, was wir sehen. </font><font style="vertical-align: inherit;">In der realen Welt kann jedes Objekt Licht absorbieren, reflektieren und manchmal brechen, und all dies beeinflusst das endg√ºltige Erscheinungsbild der angezeigten Szene. </font><font style="vertical-align: inherit;">Wenn wir die Welt in Primitive aufteilen und nur Teile davon rendern, werden wir schnell. </font><font style="vertical-align: inherit;">aber ein sehr grobes Ergebnis. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nun, wenn es einen anderen Weg g√§be ...</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ein anderer Weg ist: Raytracing! </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vor fast f√ºnfzig Jahren arbeitete ein Informatiker namens Arthur Eppel an einem System zum Rendern von Bildern auf einem Computer, bei dem ein Lichtstrahl von der Kamera in einer geraden Linie ausgestrahlt wurde, bis er mit einem Objekt kollidierte. Nach der Kollision √§nderten die Eigenschaften des Materials (Farbe, Reflexionsverm√∂gen usw.) die Helligkeit des Lichtstrahls. F√ºr jedes Pixel im gerenderten Bild wurde ein Strahl ausgesendet, und der Algorithmus f√ºhrte eine Reihe von Berechnungen durch, um die Farbe des Pixels zu bestimmen. Eppels Verfahren nennt man </font></font><a href="https://en.wikipedia.org/wiki/Ray_casting"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ray Casting</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Etwa zehn Jahre sp√§ter wurde ein anderer Wissenschaftler namens </font></font><a href="https://en.wikipedia.org/wiki/J._Turner_Whitted"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">John Whited</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entwickelten einen mathematischen Algorithmus, der den Eppel-Prozess implementiert. Wenn jedoch ein Strahl mit einem Objekt kollidiert, werden je nach Material des Objekts zus√§tzliche Strahlen erzeugt, die in verschiedene Richtungen divergieren. Da dieses System bei jeder Interaktion mit Objekten neue Strahlen erzeugt, war der Algorithmus von Natur aus rekursiv und rechenintensiver. Es hatte jedoch einen signifikanten Vorteil gegen√ºber Eppels Methode, da er Reflexionen, Brechungen und Schatten korrekt ber√ºcksichtigen konnte. Dieses Verfahren nennt man </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Raytracing (ray Player Tracing)</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (genau genommen ist es </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die umgekehrte</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Strahlverfolgung, weil wir den Strahl der Kamera und nicht durch die Objekte folgen) , </font><font style="vertical-align: inherit;">und seitdem es hat sich zu </font><font style="vertical-align: inherit;">einem heiligen Gral f√ºr Computergrafik und </font></font><a href="https://graphics.pixar.com/library/RayTracingCars/paper.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Filme</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f0/733/8a8/5f07338a8846abdebe390b99bd364ff8.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aus dem obigen Bild k√∂nnen Sie ersehen, wie der Whited-Algorithmus funktioniert. F√ºr jedes Pixel im Bild wird ein Strahl von der Kamera ausgesendet und bewegt sich bis zur Oberfl√§che. In diesem Beispiel ist die Oberfl√§che durchscheinend, sodass Licht reflektiert und durch sie gebrochen werden kann. In beiden F√§llen werden Sekund√§rstrahlen erzeugt, die sich fortbewegen, bis sie mit der Oberfl√§che kollidieren. Neue Sekund√§rstrahlen werden ebenfalls erzeugt, um die Farbe der Lichtquellen und die von ihnen erzeugten Schatten zu ber√ºcksichtigen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die rekursive Natur des Prozesses besteht darin, dass jedes Mal, wenn ein neuer emittierter Strahl die Oberfl√§che schneidet, Sekund√§rstrahlen erzeugt werden k√∂nnen. Dies kann schnell au√üer Kontrolle geraten, sodass die Anzahl der erzeugten Sekund√§rstrahlen immer begrenzt ist. Nach Abschluss des Strahlengangs wird die Farbe an jedem Endpunkt anhand der Materialeigenschaften dieser Oberfl√§che berechnet. Dieser Wert wird dann entlang des vorherigen Strahls √ºbertragen und √§ndert die Farbe f√ºr diese Oberfl√§che usw., bis wir den Startpunkt des Prim√§rstrahls erreichen, n√§mlich das Pixel im Frame.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein solches System kann √§u√üerst komplex sein und selbst einfache Szenen k√∂nnen einen gro√üen Rechenaufwand verursachen. Gl√ºcklicherweise gibt es Tricks, die die Arbeit vereinfachen: Erstens k√∂nnen Sie Ger√§te verwenden, die speziell f√ºr die Beschleunigung dieser mathematischen Operationen entwickelt wurden, √§hnlich wie dies bei der Matrixmathematik in der Vertex-Verarbeitung der Fall ist (dazu sp√§ter mehr). Ein weiterer wichtiger Trick ist der Versuch, den Prozess der Bestimmung des Objekts, in das der Strahl gefallen ist, und der genauen Stelle ihres Schnittpunkts zu beschleunigen. Wenn das Objekt aus vielen Dreiecken besteht, kann diese Aufgabe √ºberraschend schwierig sein:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de4/3f8/dac/de43f8dac76ca9bda84700c168cde9ff.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelle: Raytracing in Echtzeit mit Nvidia RTX:</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Anstatt jedes einzelne Dreieck in jedem Objekt zu √ºberpr√ºfen, wird vor dem Raytracing eine Liste der Bounding Volumes (BVs) erstellt. F√ºr verschiedene Strukturen innerhalb des Objekts werden zyklisch kleinere Begrenzungsvolumina erstellt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zum Beispiel wird der erste BV das gesamte Kaninchen sein. Das n√§chste Paar beschreibt seinen Kopf, seine Beine, seinen K√∂rper, seinen Schwanz usw .; Jedes Volumen wird wiederum eine andere Sammlung von Volumen f√ºr kleinere Strukturen des Kopfes, des K√∂rpers usw. sein, und die letzte Volumenstufe wird eine kleine Anzahl von Dreiecken zur √úberpr√ºfung enthalten. Alle diese Volumes sind h√§ufig in einer geordneten Liste angeordnet ( </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BV-Hierarchie genannt)</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">oder BVH); </font><font style="vertical-align: inherit;">Dank dessen pr√ºft das System jedes Mal einen relativ kleinen BV-Betrag:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/345/319/454/3453194547f780d91a4231529f6f68b4.jpg"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obwohl die Verwendung von BVH die Strahlverfolgung nicht beschleunigt, ist die Generierung einer Hierarchie und des erforderlichen anschlie√üenden Suchalgorithmus im Allgemeinen viel schneller als die √úberpr√ºfung des Schnittpunkts eines Strahls mit einem von Millionen Dreiecken in der 3D-Welt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Heutzutage verwenden Programme wie </font></font><a href="https://www.blender.org/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blender</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="http://www.povray.org/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">POV-Ray</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ray Tracing mit zus√§tzlichen Algorithmen (wie Photon Tracing und Radiosity), um sehr realistische Bilder zu erzeugen:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/b2c/bf6/cfab2cbf6216eaa3e007bdd38f867cd6.jpg"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die naheliegende Frage k√∂nnte sich stellen: Wenn Ray Tracing so gut ist, warum wird es nicht √ºberall verwendet? Die Antwort liegt in zwei Bereichen: Erstens erzeugt bereits eine einfache Strahlenverfolgung Millionen von Strahlen, die immer wieder berechnet werden m√ºssen. Das System startet mit nur einem Strahl pro Bildschirmpixel, dh mit einer Aufl√∂sung von 800 x 600 erzeugt es 480.000 Prim√§rstrahlen und dann erzeugt jeder von ihnen viele Sekund√§rstrahlen. Dies ist selbst f√ºr moderne Desktop-PCs eine sehr schwierige Aufgabe. Das zweite Problem ist, dass die einfache Strahlverfolgung nicht sehr realistisch ist und f√ºr ihre ordnungsgem√§√üe Implementierung eine ganze Reihe zus√§tzlicher, sehr komplexer Gleichungen erforderlich ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selbst mit moderner Ausr√ºstung ist der Arbeitsaufwand in 3D-Spielen f√ºr die Echtzeitimplementierung unerreichbar. In </font></font><a href="https://www.techspot.com/article/1851-3d-game-rendering-explained/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-Rendering 101</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben gesehen, dass ein Raytracing-Benchmark einige zehn Sekunden ben√∂tigt, um ein einzelnes Bild mit niedriger Aufl√∂sung zu erstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie hat der erste </font></font><a href="https://www.techspot.com/downloads/5842-wolfenstein-3d.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wolfenstein 3D</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1992 Ray Casting durchgef√ºhrt und warum bieten Spiele wie </font></font><a href="https://www.techspot.com/review/1759-ray-tracing-benchmarks-vol-2/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Battlefield V</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://www.techspot.com/article/1793-metro-exodus-ray-tracing-benchmark/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Metro Exodus</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die 2019 ver√∂ffentlicht wurden, Ray Tracing-Funktionen? </font><font style="vertical-align: inherit;">F√ºhren sie eine Rasterung oder Raytracing durch? </font><font style="vertical-align: inherit;">Nach und nach von beiden.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ein hybrider Ansatz f√ºr Gegenwart und Zukunft </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im M√§rz 2018 gab Microsoft die Ver√∂ffentlichung einer neuen API-Erweiterung f√ºr Direct3D 12 mit dem Namen DXR (DirectX Raytracing) bekannt. </font><font style="vertical-align: inherit;">Es war eine neue Grafik-Pipeline, die Standard-Rasterisierungs- und Berechnungs-Pipelines erg√§nzt. </font><font style="vertical-align: inherit;">Zus√§tzliche Funktionen wurden durch das Hinzuf√ºgen von Shadern, Datenstrukturen usw. bereitgestellt, erforderten jedoch keine Hardwareunterst√ºtzung, mit Ausnahme derjenigen, die bereits f√ºr Direct3D 12 ben√∂tigt wurde.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7c9/b88/e83/7c9b88e83184b2412b431ff990e39f89.png"></div><br>  Auf der gleichen Spieleentwicklerkonferenz, auf der <a href="https://devblogs.microsoft.com/directx/wp-content/uploads/sites/42/2018/03/GDC_DXR_deck.pdf">Microsoft √ºber DXR sprach</a> , sprach Electronic Arts √ºber sein <a href="https://www.ea.com/seed/news/seed-project-picapica">Pica Pica-Projekt</a> - ein Experiment mit einer 3D-Engine, die DXR verwendet.  Das Unternehmen hat gezeigt, dass Raytracing verwendet werden kann, jedoch nicht zum Rendern des gesamten Frames.  Der Gro√üteil der Arbeit verwendet traditionelle Rastertechniken und computerbasierte Shader, w√§hrend DXR in bestimmten Bereichen verwendet wird.  Das hei√üt, die Anzahl der erzeugten Strahlen ist viel geringer als f√ºr die gesamte Szene. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ad/2f0/8c9/3ad2f08c927f84fd66245141de4ad34d.png"></div><br>  Dieser hybride Ansatz wurde in der Vergangenheit verwendet, wenn auch in geringerem Umfang.  Beispielsweise verwendete Wolfenstein 3D <a href="https://permadi.com/1996/05/ray-casting-tutorial-table-of-contents/">Ray Casting</a> , um einen Frame zu rendern, aber es wurde mit einem Strahl pro Pixelspalte und nicht mit einem Pixel gearbeitet.  Es mag immer noch beeindruckend erscheinen, es sei denn, Sie erinnern sich, dass das Spiel mit einer Aufl√∂sung von 640 x 480 [ca.  eigentlich 320 x 200], dh es wurden gleichzeitig nicht mehr als 640 Strahlen ausgesendet. <br><br>  Anfang 2018 erf√ºllten Grafikkarten wie die AMD Radeon RX 580 oder die Nvidia GeForce 1080 Ti die DXR-Anforderungen, aber trotz ihrer Rechenleistung gab es Bedenken, dass sie nicht leistungsf√§hig genug sein k√∂nnten, um DXR sinnvoll zu machen. <br><br>  Die Situation √§nderte sich im August 2018, als Nvidia seine neueste GPU-Architektur <a href="http://www.techspot.com/news/75952-nvidia-turing-here-next-gen-architecture-first-real.html">mit dem Codenamen Turing ver√∂ffentlichte</a> .  Das wichtigste Merkmal dieses Chips war das Auftreten der sogenannten RT-Kerne: separate Logikbl√∂cke zur Beschleunigung der Berechnungen des Schnittpunkts der Strahlendreiecke und des Durchlaufs der Hierarchie der Grenzvolumina (BVH).  Diese beiden Verfahren sind zeitaufw√§ndige Verfahren zur Bestimmung der Wechselwirkungspunkte von Licht mit Dreiecken, aus denen Szenenobjekte bestehen.  Da es sich bei RT-Kernen um einzigartige Turing-Prozessoreinheiten handelte, konnte der Zugriff nur √ºber die propriet√§re Nvidia-API erfolgen. <br><br>  Das erste Spiel, das dieses Feature unterst√ºtzte, war EAs Battlefield V.  <a href="https://www.techspot.com/review/1749-battlefield-ray-tracing-benchmarks/">Als wir DXR darin testeten</a> , waren wir beeindruckt von der Verbesserung der Reflexionen im Wasser, auf Gras und Metallen sowie einer entsprechenden Abnahme der Leistung: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1cc/e3e/aa1/1cce3eaa1c5db3688910ec0a1c052ce4.png"></div><br>  Um ehrlich zu sein, haben die nachfolgenden Patches die Situation verbessert, aber die Geschwindigkeit beim Rendern von Frames war (und ist) immer noch gesunken.  Bis 2019 gab es einige andere Spiele, die diese API unterst√ºtzen und Raytracing f√ºr einzelne Teile des Frames durchf√ºhren.  Wir haben <a href="https://www.techspot.com/article/1793-metro-exodus-ray-tracing-benchmark/">Metro Exodus</a> und <a href="https://www.techspot.com/article/1814-sotr-ray-tracing/">Shadow of the Tomb Raider</a> in der gleichen Situation getestet - durch die aktive Nutzung von DXR wird die Framerate erheblich reduziert. <br><br>  Etwa zur gleichen Zeit k√ºndigten UL Benchmarks die Erstellung eines DXR-Funktionstests f√ºr <a href="https://www.techspot.com/downloads/5775-3dmark.html">3DMark an</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/612/60a/a0d/61260aa0d10ef6ed6c69731a6a3038f2.jpg"></div><br>  <i>DXR wird in der Nvidia Titan X (Pascal) -Grafikkarte verwendet - ja, das Ergebnis sind 8 fps</i> <br><br>  Eine Studie zu Spielen mit DXR-Unterst√ºtzung und dem 3DMark-Test hat jedoch gezeigt, dass Ray Tracing auch im Jahr 2019 f√ºr die GPU eine sehr schwierige Aufgabe bleibt, selbst bei einem Preis von mehr als 1.000 US-Dollar.  Bedeutet dies, dass wir keine wirklichen Alternativen zur Rasterisierung haben? <br><br>  Fortschrittliche Funktionen in Consumer-3D-Grafiktechnologien sind oft sehr teuer und ihre anf√§ngliche Unterst√ºtzung f√ºr neue API-Funktionen kann sehr fragmentiert oder langsam sein (wie wir <a href="https://www.techspot.com/review/537-max-payne-3-performance/page6.html">beim Testen von Max Payne 3</a> auf verschiedenen Direct3D-Versionen im Jahr 2012 herausgefunden haben).  Das letztere Problem tritt normalerweise auf, weil Spieleentwickler versuchen, so viele moderne Funktionen wie m√∂glich in ihre Produkte zu integrieren, manchmal ohne ausreichende Erfahrung. <br><br>  Vertex- und Pixel-Shader, Tessellation, HDR-Rendering und die Verschleierung des Bildschirmraums waren jedoch auch einmal kostspielige Techniken, die nur f√ºr leistungsstarke GPUs geeignet waren. Jetzt sind sie der Standard f√ºr Spiele, und viele Grafikkarten werden unterst√ºtzt.  Dasselbe wird mit Raytracing passieren;  Im Laufe der Zeit wird es einfach zu einem weiteren Detailparameter, der f√ºr die meisten Spieler standardm√§√üig aktiviert ist. <br><br><h2>  Abschlie√üend </h2><br>  Wir sind also am Ende des zweiten Teils der Analyse angelangt, in dem wir uns eingehender mit der Welt der 3D-Grafik befasst haben.  Wir haben gelernt, wie Welten und Modelle aus drei Dimensionen in ein flaches 2D-Bild umgewandelt werden.  Wir haben gesehen, dass wir den Umfang ber√ºcksichtigen m√ºssen, und festgestellt, welche Auswirkungen dies hat.  Wir untersuchten den Konvertierungsprozess dieser Verines in Pixel und schlossen mit einem kurzen Blick auf Alternativen zum herk√∂mmlichen Rasterungsprozess. <br><br>  Wie im vorherigen Artikel war es unwahrscheinlich, dass wir alle Themen enth√ºllen konnten, und wir haben einige Details √ºbersehen - am Ende ist dies kein Lehrbuch!  Aber wir hoffen, dass Sie etwas Neues gelernt haben und nun die Arbeit von Programmierern und Ingenieuren respektieren, die Computer und Wissenschaft verwendet haben, um all dies in Ihren Lieblings-3D-Spielen umzusetzen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de480338/">https://habr.com/ru/post/de480338/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de480324/index.html">String-Typ-Implementierung in CPython</a></li>
<li><a href="../de480326/index.html">Die F5 Networks Corporation sendet ihren Kunden Briefe, in denen sie √ºber die aktuelle Situation mit NGINX informiert werden</a></li>
<li><a href="../de480328/index.html">Wie man Freunde macht PyTorch und C ++. TorchScript verwenden</a></li>
<li><a href="../de480330/index.html">Ideales Tool zur Mitarbeiterbewertung</a></li>
<li><a href="../de480332/index.html">Analyse der Daten der Blockchain-Abstimmung 2019 in der Moskauer Stadtduma</a></li>
<li><a href="../de480340/index.html">Ich habe mich gegen einen inkompetenten Manager ausgesprochen, und dann wurde er bef√∂rdert</a></li>
<li><a href="../de480342/index.html">Entwicklungsparadigma durch Kommentieren</a></li>
<li><a href="../de480348/index.html">Deep Fake Science, die Krise der Reproduzierbarkeit und woher kommen leere Repositories?</a></li>
<li><a href="../de480350/index.html">Die Zusammenstellung interessanter Materialien f√ºr den mobilen Entwickler # 326 (9. - 15. Dezember)</a></li>
<li><a href="../de480352/index.html">Der Harvard-Genetiker entwickelt eine Prototyp-DNA-Analyse-Dating-App</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>