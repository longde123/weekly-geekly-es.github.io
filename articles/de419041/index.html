<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎢 🙎🏾 🛀🏿 Die neun Elasticsearch-Rechen, auf die ich getreten bin 🈯️ 🎒 ⛓️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="„Eine ausgebildete Person tritt auch auf einen Rechen. 
 Aber auf der anderen Seite, wo der Stift ist. “ 

 Elasticsearch ist ein großartiges Werkzeug...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die neun Elasticsearch-Rechen, auf die ich getreten bin</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/419041/"><img src="https://habrastorage.org/webt/ap/2k/jc/ap2kjcsehhaliahrmgg6a3r27xw.jpeg" alt="Illustration von Anton Gudim"><br><br><br>  <i>„Eine ausgebildete Person tritt auch auf einen Rechen.</i> <i><br></i>  <i>Aber auf der anderen Seite, wo der Stift ist. “</i> <br><br>  Elasticsearch ist ein großartiges Werkzeug, aber jedes Werkzeug erfordert nicht nur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Optimierung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wartung</a> , sondern auch Liebe zum Detail.  Einige sind unbedeutend und liegen an der Oberfläche, während andere so tief verborgen sind, dass die Suche mehr als einen Tag dauert, nicht ein Dutzend Tassen Kaffee und nicht einen Kilometer Nerven.  In diesem Artikel erzähle ich Ihnen von neun wunderbaren Rechen in den elastischen Einstellungen, auf die ich getreten bin. <br><a name="habracut"></a><br>  Ich werde den Rechen in absteigender Reihenfolge der Beweise anordnen.  Von denen, die beim Aufbau und Eintritt in einen Cluster im Produktionszustand vorausgesehen und umgangen werden können, bis zu sehr seltsamen, die die meiste Erfahrung bringen (und Sterne in den Augen). <br><br><h2>  Datenknoten müssen gleich sein </h2><br>  "Der Cluster läuft mit der Geschwindigkeit des langsamsten Datenknotens" - ein qualvolles Axiom.  Es gibt jedoch noch einen weiteren offensichtlichen Punkt, der nicht mit der Leistung zusammenhängt: Der Elastiker denkt nicht im Speicherplatz, sondern in Shards und versucht, sie gleichmäßig auf die Datenknoten zu verteilen.  Wenn einige der Datenknoten mehr Speicherplatz als andere haben, ist es sinnlos, untätig zu bleiben. <br><br><h2>  Deprecation.log </h2><br>  Es kann vorkommen, dass jemand nicht die modernsten Mittel zum Senden von Daten an das Gummiband verwendet, wodurch der Inhaltstyp bei der Ausführung von Abfragen nicht festgelegt werden kann.  In dieser Liste zum Beispiel heka oder wenn die Protokolle die Geräte mit ihren eingebauten Mitteln verlassen).  In diesem Fall Verfall.  Das Protokoll beginnt mit alarmierender Geschwindigkeit zu wachsen, und für jede Anforderung werden die folgenden Zeilen angezeigt: <br><br><pre><code class="hljs markdown">[<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,659</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,670</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,671</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,673</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,677</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController </span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [Content-Type] header.</code> </pre> <br>  Anfragen kommen durchschnittlich alle 5-10 ms - und jedes Mal, wenn eine neue Zeile zum Protokoll hinzugefügt wird.  Dies wirkt sich negativ auf die Leistung des Festplattensubsystems aus und erhöht iowait.  Deprecation.log kann deaktiviert werden, ist aber nicht sinnvoll.  Um elastische Protokolle darin zu sammeln, aber nicht zu verschmutzen, deaktiviere ich nur Protokolle der Klasse oedrRestController. <br><br>  Fügen Sie dazu logs4j2.properties die folgende Konstruktion hinzu: <br><br><pre> <code class="hljs pgsql">logger.restcontroller.name = org.elasticsearch.deprecation.rest.RestController logger.restcontroller.<span class="hljs-keyword"><span class="hljs-keyword">level</span></span> = error</code> </pre><br>  Dadurch werden die Protokolle dieser Klasse auf die Fehlerstufe angehoben, und sie fallen nicht mehr in deprecation.log. <br><br><h2>  .kibana </h2><br>  Wie sieht ein typischer Cluster-Installationsprozess aus?  Wir setzen die Knoten, kombinieren sie zu einem Cluster, setzen das X-Pack (wer es braucht) und natürlich Kibana.  Wir starten, überprüfen, ob alles funktioniert und Kibana den Cluster sieht, und konfigurieren weiter.  Das Problem ist, dass in einem frisch installierten Cluster die Standardvorlage ungefähr so ​​aussieht: <br><br><pre> <code class="hljs json">{ <span class="hljs-attr"><span class="hljs-attr">"default"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"order"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"template"</span></span>: <span class="hljs-string"><span class="hljs-string">"*"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"settings"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"number_of_shards"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"0"</span></span> } }, <span class="hljs-attr"><span class="hljs-attr">"mappings"</span></span>: {}, <span class="hljs-attr"><span class="hljs-attr">"aliases"</span></span>: {} }</code> </pre> <br>  Der .kibana-Index, in dem alle Einstellungen gespeichert sind, wird in einer einzigen Kopie erstellt. <br><br>  Es gab einmal einen Fall, in dem aufgrund eines Hardwarefehlers einer der Datenknoten im Cluster getötet wurde.  Es kam schnell zu einem konsistenten Zustand und erzeugte Replikate von Shards von benachbarten Datenknoten, aber glücklicherweise befand sich auf diesem Datenknoten der einzige Shard mit dem .kibana-Index.  Die Situation ist eine Pattsituation - der Cluster ist in Betrieb, in einem funktionierenden Zustand, und Kibana befindet sich in einem roten Status, und mein Telefon ist von Anrufen von Mitarbeitern zerrissen, die ihre Protokolle dringend benötigen. <br><br>  All dies wird einfach gelöst.  Bisher ist nichts gefallen: <br><br><pre> <code class="hljs objectivec">XPUT .kibana/_settings { <span class="hljs-string"><span class="hljs-string">"index"</span></span>: { <span class="hljs-string"><span class="hljs-string">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;__&gt;"</span></span> } }</code> </pre> <br><h2>  XMX / XMS </h2><br>  In der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> steht zu Recht „Nicht mehr als 32 GB“.  Es ist aber auch richtig, dass Sie nicht in den Diensteinstellungen installieren müssen <br><pre> <code class="hljs powershell"><span class="hljs-literal"><span class="hljs-literal">-Xms32g</span></span> <span class="hljs-literal"><span class="hljs-literal">-Xmx32g</span></span></code> </pre> <br>  Weil es bereits mehr als 32 Gigabyte sind und wir hier auf eine interessante Nuance von Java stoßen, das mit Speicher arbeitet.  Ab einem bestimmten Grenzwert verwendet Java keine komprimierten Zeiger mehr und verbraucht unangemessen viel Speicher.  Es ist sehr einfach zu überprüfen, ob komprimierte Zeiger einen Java-Computer verwenden, auf dem Elasticsearch ausgeführt wird.  Wir schauen im Serviceprotokoll nach: <br><br><pre> <code class="hljs powershell">[<span class="hljs-number"><span class="hljs-number">2018</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">29</span></span><span class="hljs-type"><span class="hljs-type">T15</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span>:<span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">041</span></span>][<span class="hljs-type"><span class="hljs-type">INFO</span></span>][<span class="hljs-type"><span class="hljs-type">oeeNodeEnvironment</span></span>][<span class="hljs-type"><span class="hljs-type">log</span></span>-<span class="hljs-type"><span class="hljs-type">elastic</span></span>-<span class="hljs-type"><span class="hljs-type">hot3</span></span>] heap size [<span class="hljs-number"><span class="hljs-number">31.6</span></span><span class="hljs-type"><span class="hljs-type">gb</span></span>], compressed ordinary object pointers [<span class="hljs-type"><span class="hljs-type">true</span></span>]</code> </pre> <br>  Die Speichermenge, die nicht überschritten werden darf, hängt unter anderem von der verwendeten Java-Version ab.  Informationen zur Berechnung des genauen Volumens in Ihrem Fall finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> . <br><br>  Jetzt habe ich auf allen Datenknoten des Gummibandes installiert: <br><br><pre> <code class="hljs powershell"><span class="hljs-literal"><span class="hljs-literal">-Xms32766m</span></span> <span class="hljs-literal"><span class="hljs-literal">-Xmx32766m</span></span></code> </pre> <br>  Es scheint eine banale Tatsache zu sein, und die Dokumentation ist gut beschrieben, aber ich stoße regelmäßig auf Elasticsearch-Installationen, bei denen ich diesen Punkt verpasst habe, und Xms / Xmx sind auf 32 g eingestellt. <br><br><h2>  / var / lib / elasticsearch </h2><br>  Dies ist der Standardpfad zum Speichern von Daten in Elasticsearch.  yml: <br><br><pre> <code class="hljs kotlin">path.<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>: /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch</code> </pre> <br>  Dort mounte ich normalerweise ein großes RAID-Array, und hier ist der Grund: Wir geben ES verschiedene Möglichkeiten zum Speichern von Daten an, zum Beispiel wie folgt: <br><br><pre> <code class="hljs kotlin">path.<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>: /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch/data1, /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch/data2</code> </pre> <br>  In data1 und data2 sind verschiedene Festplatten oder RAID-Arrays gemountet.  Das Gummiband gleicht sich jedoch nicht aus und verteilt die Last nicht auf diese Pfade.  Zuerst füllt er einen Abschnitt aus und beginnt dann, in einen anderen zu schreiben, damit die Belastung des Speichers ungleichmäßig ist.  Da ich das wusste, traf ich eine eindeutige Entscheidung - ich kombinierte alle Festplatten in RAID0 / 1 und mounte sie in den in path.data angegebenen Pfad. <br><br><h2>  verfügbare_Prozessoren </h2><br>  Und nein, ich meine jetzt nicht Prozessoren auf Aufnahmeknoten.  Wenn Sie sich die Eigenschaften eines laufenden Knotens (über die _nodes-API) ansehen, sehen Sie Folgendes: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"os"</span></span>. { <span class="hljs-string"><span class="hljs-string">"refresh_interval_in_millis"</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"arch"</span></span>: <span class="hljs-string"><span class="hljs-string">"amd64"</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"4.4.0-87-generic"</span></span>, <span class="hljs-string"><span class="hljs-string">"available_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"allocated_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span> }</code> </pre> <br>  Es ist ersichtlich, dass der Knoten auf einem Host mit 28 Kernen ausgeführt wird und das Gummiband seine Anzahl korrekt bestimmt und auf allen gestartet hat.  Aber wenn es mehr als 32 Kerne gibt, passiert es manchmal so: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"os"</span></span>: { <span class="hljs-string"><span class="hljs-string">"refresh_interval_in_millis"</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"arch"</span></span>: <span class="hljs-string"><span class="hljs-string">"amd64"</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"4.4.0-116-generic"</span></span>, <span class="hljs-string"><span class="hljs-string">"available_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">72</span></span>, <span class="hljs-string"><span class="hljs-string">"allocated_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">32</span></span> }</code> </pre> <br>  Sie müssen die Anzahl der Prozessoren erzwingen, die dem Dienst zur Verfügung stehen - dies wirkt sich positiv auf die Leistung des Knotens aus. <br><br><pre> <code class="hljs">processors: 72</code> </pre> <br><h2>  thread_pool.bulk.queue_size </h2><br>  Im Abschnitt thread_pool.bulk.rejected des letzten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikels gab es</a> eine solche Metrik - die Anzahl der Fehler bei Anforderungen zum Hinzufügen von Daten. <br><br>  Ich habe geschrieben, dass das Wachstum dieses Indikators ein sehr schlechtes Zeichen ist, und die Entwickler empfehlen, keine Thread-Pools einzurichten, sondern neue Knoten zum Cluster hinzuzufügen - angeblich löst dies Leistungsprobleme.  Aber die Regeln werden benötigt, um sie manchmal zu brechen.  Und es ist nicht immer möglich, "das Problem mit Eisen zu lösen". Eine der Maßnahmen zur Bekämpfung von Fehlern bei Massenanfragen besteht darin, die Größe dieser Warteschlange zu erhöhen. <br><br>  Standardmäßig sehen die Warteschlangeneinstellungen folgendermaßen aus: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"thread_pool"</span></span>: { <span class="hljs-string"><span class="hljs-string">"bulk"</span></span>: { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"fixed"</span></span>, <span class="hljs-string"><span class="hljs-string">"min"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"max"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"queue_size"</span></span>: <span class="hljs-number"><span class="hljs-number">200</span></span> } }</code> </pre> <br>  Der Algorithmus ist wie folgt: <br><br><ol><li>  Wir sammeln Statistiken über die durchschnittliche Warteschlangengröße während des Tages (der Sofortwert wird in thread_pool.bulk.queue gespeichert). </li><li>  Erhöhen Sie queue_size vorsichtig auf Größen, die geringfügig größer als die durchschnittliche Größe der aktiven Warteschlange sind, da ein Fehler auftritt, wenn er überschritten wird. </li><li>  Wir vergrößern den Pool - dies ist nicht notwendig, aber akzeptabel. </li></ol><br>  Fügen Sie dazu den Host-Einstellungen Folgendes hinzu (Sie haben natürlich Ihre eigenen Werte): <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">thread_pool</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.bulk</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.size</span></span>: 32 <span class="hljs-selector-tag"><span class="hljs-selector-tag">thread_pool</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.bulk</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.queue_size</span></span>: 500</code> </pre> <br>  Und nach dem Neustart des Knotens werden wir definitiv die Last, die E / A und den Speicherverbrauch überwachen.  und alles, was möglich ist, um die Einstellungen bei Bedarf zurückzusetzen. <br><br>  <i>Wichtig: Diese Einstellungen sind nur auf Knoten sinnvoll, die neue Daten empfangen.</i> <br><br><h2>  Vorläufige Indexerstellung </h2><br>  Wie ich im ersten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel der</a> Serie sagte, verwenden wir Elasticsearch, um die Protokolle aller Microservices zu speichern.  Das Endergebnis ist einfach: Ein Index speichert die Protokolle einer Komponente an einem Tag. <br><br>  Daraus folgt, dass jeden Tag neue Indizes durch die Anzahl der Mikrodienste erstellt werden - daher fiel das Gummiband früher jede Nacht für etwa 8 Minuten in den Clinch, während hundert neue Indizes erstellt wurden, mehrere hundert neue Shards, der Zeitplan für das Laden der Festplatte „ins Regal“ ging und die Warteschlangen wuchsen Zabbix blühte mit Warnungen wie ein Weihnachtsbaum. <br><br>  Um dies zu vermeiden, war es normal, ein Python-Skript zu schreiben, um Indizes vorab zu erstellen.  Das Skript funktioniert folgendermaßen: Es findet die Indizes für heute, extrahiert ihre Zuordnungen und erstellt neue Indizes mit denselben Zuordnungen, jedoch für den kommenden Tag.  Es läuft auf Cron, läuft in den Stunden, in denen der Elastic am wenigsten belastet ist.  Das Skript verwendet die Elasticsearch-Bibliothek und ist auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> verfügbar. <br><br><h2>  Transparente übergeordnete riesige Seiten </h2><br>  Als wir feststellten, dass die elastischen Knoten, die den Datenempfang bedienen, während der Stoßzeiten unter Last zu hängen begannen.  Und mit sehr seltsamen Symptomen: Die Verwendung aller Prozessorkerne sinkt auf Null, aber der Dienst bleibt im Speicher hängen, hört ordnungsgemäß auf den Port, tut nichts, reagiert nicht auf Anforderungen und fällt nach einiger Zeit aus dem Cluster heraus.  Der Dienst reagiert nicht auf einen Neustart des Systems.  Nur der gute alte Kill −9 hilft. <br><br>  Dies wird von Standardüberwachungstools in den Diagrammen erst im Moment des Sturzes erfasst, in dem das reguläre Bild in den Serviceprotokollen leer ist.  Der Speicherauszug der Java-Maschine war zu diesem Zeitpunkt ebenfalls nicht möglich. <br><br>  Aber wie sie sagen: "Wir sind Profis, also haben wir nach einiger Zeit die Lösung gegoogelt."  Ein ähnliches Problem wurde im Thread auf " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Diskussion.elastic.co" behandelt</a> und stellte sich als Kernel-Fehler heraus, der mit transparenten großen Seiten zusammenhängt.  Alles wurde gelöst, indem thp im Kernel mit dem sysfsutils-Paket deaktiviert wurde. <br><br>  Es ist einfach zu überprüfen, ob transparente große Seiten aktiviert sind: <br><br><pre> <code class="hljs powershell">cat /sys/kernel/mm/transparent_hugepage/enabled always madvise [<span class="hljs-type"><span class="hljs-type">never</span></span>]</code> </pre> <br>  Wenn [immer] da ist, sind Sie möglicherweise gefährdet. <br><br><h2>  Fazit </h2><br>  Dies ist der Hauptrechen (tatsächlich gab es natürlich noch mehr), den ich als Administrator des Elasticsearch-Clusters anderthalb Jahre lang betreten habe.  Ich hoffe, diese Informationen sind nützlich auf dem schwierigen und mysteriösen Weg zum idealen Elasticsearch-Cluster. <br><br>  Vielen Dank für die Illustration, Anton Gudim - in seinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Instagram</a> steckt noch viel Gutes. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de419041/">https://habr.com/ru/post/de419041/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de419027/index.html">Informationssicherheit bei bargeldlosen Bankzahlungen. Teil 6 - Analyse der Bankenkriminalität</a></li>
<li><a href="../de419029/index.html">Fortnite ist zu einem sozialen Phänomen geworden. Eltern stellen zunehmend Trainer für ihre Kinder ein und spielen mit ihnen</a></li>
<li><a href="../de419033/index.html">Ein kleiner Hinweis zum Thema Ausführen von vue.js im kubernetes-Cluster</a></li>
<li><a href="../de419035/index.html">Buch „Head First Agile. Flexibles Projektmanagement “</a></li>
<li><a href="../de419037/index.html">PPPOS-Implementierung bei stm32f4-Discovery</a></li>
<li><a href="../de419043/index.html">Das schwer fassbare Problem mit dem Frame-Timing</a></li>
<li><a href="../de419047/index.html">Reddit gehackte, durchgesickerte Datenbank mit Passwörtern und E-Mail für 2005-2007</a></li>
<li><a href="../de419049/index.html">GeekBrains startet den kostenlosen Online-Bildungsmarathon „Find Yourself in Digital“</a></li>
<li><a href="../de419051/index.html">Wie Flant Anfängern hilft</a></li>
<li><a href="../de419053/index.html">Testen der Adaptec RAID Cache-Technologie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>