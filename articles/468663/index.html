<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë≤üèΩ üë®‚Äçüë©‚Äçüë¶ üë®üèª‚Äçüç≥ End2 End Approach en tareas de reconocimiento autom√°tico de voz üë¥ üë©üèº‚Äçü§ù‚Äçüë®üèΩ üßöüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬øQu√© es el reconocimiento de voz End2End y por qu√© es necesario? ¬øCu√°l es su diferencia con el enfoque cl√°sico? Y por qu√©, para entrenar un buen model...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>End2 End Approach en tareas de reconocimiento autom√°tico de voz</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/468663/">  ¬øQu√© es el reconocimiento de voz End2End y por qu√© es necesario?  ¬øCu√°l es su diferencia con el enfoque cl√°sico?  Y por qu√©, para entrenar un buen modelo basado en End2End, necesitamos una gran cantidad de datos, en nuestra publicaci√≥n de hoy. <br><br><h4>  El enfoque cl√°sico para el reconocimiento de voz. </h4><br>  Antes de hablar sobre el enfoque End2End, primero debe hablar sobre el enfoque cl√°sico para el reconocimiento de voz.  ¬øC√≥mo es √©l? <br><br><img src="https://habrastorage.org/webt/xk/xm/sc/xkxmscrxxx0n2hvkoxdxyflqaoq.png"><br><a name="habracut"></a><br><h4>  Extracci√≥n de caracter√≠sticas </h4><br>  De hecho, esta no es una secuencia completamente lineal de bloques de acci√≥n.  Deteng√°monos en cada bloque con m√°s detalle.  Tenemos alg√∫n tipo de discurso de entrada, cae en el primer bloque: extracci√≥n de caracter√≠sticas.  Este es un bloqueo que extrae signos del habla.  Hay que tener en cuenta que el discurso en s√≠ es algo bastante complicado.  Debe poder trabajar con √©l de alguna manera, por lo que existen m√©todos est√°ndar para aislar caracter√≠sticas de la teor√≠a del procesamiento de se√±ales.  Por ejemplo, coeficientes Mel-cepstrales (MFCC), etc. <br><br><h4>  Modelo ac√∫stico </h4><br>  El siguiente componente es el modelo ac√∫stico.  Se puede basar en redes neuronales profundas o en mezclas de distribuciones gaussianas y modelos ocultos de Markov.  Su objetivo principal es obtener de una secci√≥n de la se√±al ac√∫stica las distribuciones de probabilidad de varios fonemas en esta secci√≥n. <br><br>  Luego viene el decodificador, que busca la ruta m√°s probable en el gr√°fico en funci√≥n del resultado del √∫ltimo paso.  Restaurar es el toque final en reconocimiento, cuya tarea principal es volver a sopesar las hip√≥tesis y producir el resultado final. <br><br><img src="https://habrastorage.org/webt/pp/hh/2i/pphh2ietnprdh5aeqtbvh06kzpu.png"><br><br>  Deteng√°monos con m√°s detalle en el modelo ac√∫stico.  Como es ella  Tenemos algunas grabaciones de voz que entran en un determinado sistema basado en GMM (mezcla de gausovy monoaural) o HMM.  Es decir, tenemos representaciones en forma de fonemas, utilizamos mon√≥fonos, es decir, fonemas independientes del contexto.  M√°s all√° de esto, hacemos mezclas de distribuciones gaussianas basadas en fonemas sensibles al contexto.  Utiliza clustering basado en √°rboles de decisi√≥n. <br><br>  Luego tratamos de construir la alineaci√≥n.  Tal m√©todo completamente no trivial nos permite obtener un modelo ac√∫stico.  No suena muy simple, de hecho es a√∫n m√°s complicado, hay muchos matices, caracter√≠sticas.  Pero como resultado, un modelo entrenado en cientos de horas es muy capaz de simular ac√∫stica. <br><br><img src="https://habrastorage.org/webt/vt/-x/gl/vt-xgltq9lf3fujagn2jkqdriqy.png"><br><br><h4>  Decodificador </h4><br>  ¬øQu√© es un decodificador?  Este es el m√≥dulo que selecciona la ruta de transici√≥n m√°s probable de acuerdo con el gr√°fico HCLG, que consta de 4 partes: <br><br>  M√≥dulo H basado en HMM <br>  M√≥dulo de dependencia de contexto C <br>  M√≥dulo de pronunciaci√≥n L <br>  M√≥dulo de modelo de lenguaje G <br><br>  Construimos un gr√°fico sobre estos cuatro componentes, en base al cual decodificaremos nuestras caracter√≠sticas ac√∫sticas en ciertas construcciones verbales. <br><br>  M√°s o menos, est√° claro que el enfoque cl√°sico es bastante engorroso y dif√≠cil, es dif√≠cil de entrenar, ya que consta de una gran cantidad de partes separadas, para cada una de las cuales necesita preparar sus propios datos para el entrenamiento. <br><br><h4>  II Enfoque End2End </h4><br>  Entonces, ¬øqu√© es el reconocimiento de voz End2End y por qu√© es necesario?  Este es un cierto sistema, que est√° dise√±ado para reflejar directamente la secuencia de signos ac√∫sticos en la secuencia de grafemas (letras) o palabras.  Tambi√©n puede decir que este es un sistema que optimiza los criterios que afectan directamente la m√©trica final de evaluaci√≥n de calidad.  Por ejemplo, nuestra tarea es espec√≠ficamente la tasa de error de palabras.  Como dije, solo hay una motivaci√≥n: presentar estos componentes complejos de m√∫ltiples etapas como un componente simple que mostrar√° directamente, generar√° palabras o grafemas a partir del discurso de entrada. <br><br><h4>  Problema de simulaci√≥n </h4><br>  Aqu√≠ tenemos un problema de inmediato: el discurso sonoro es una secuencia, y en la salida tambi√©n necesitamos dar una secuencia.  Y hasta 2006, no hab√≠a una forma adecuada de modelar esto.  ¬øCu√°l es el problema del modelado?  Era necesario que cada registro creara un marcado complejo, lo que implica en qu√© segundo pronunciamos un sonido o letra en particular.  Este es un dise√±o complejo muy engorroso y, por lo tanto, no se han realizado una gran cantidad de estudios sobre este tema.  En 2006, se public√≥ un interesante art√≠culo de Alex Graves "Clasificaci√≥n temporal conexionista" (CTC), en el que este problema se resuelve, en principio.  Pero el art√≠culo fue publicado, y no hab√≠a suficiente potencia inform√°tica en ese momento.  Y los algoritmos reales de reconocimiento de voz en funcionamiento aparecieron mucho m√°s tarde. <br><br>  En total, tenemos: el algoritmo CTC fue propuesto por Alex Graves hace trece a√±os, como una herramienta que le permite entrenar / entrenar modelos ac√∫sticos sin la necesidad de este marcado complejo: alineaci√≥n de los cuadros de secuencia de entrada y salida.  Con base en este algoritmo, inicialmente apareci√≥ un trabajo que no fue completo end2end; como resultado se emitieron fonemas.  Vale la pena se√±alar que los fonemas sensibles al contexto basados ‚Äã‚Äãen STS logran uno de los mejores resultados en el reconocimiento de la libertad de expresi√≥n.  Pero tambi√©n vale la pena se√±alar que este algoritmo, aplicado directamente a las palabras, se queda atr√°s en este momento. <br><br><img src="https://habrastorage.org/webt/ns/oe/e2/nsoee2tomucbndnfsaj__7va-o4.png"><br><br><h4>  ¬øQu√© es STS? </h4><br>  Ahora hablaremos un poco m√°s en detalle sobre qu√© es el STS y por qu√© es necesario, qu√© funci√≥n realiza.  STS es necesario para entrenar el modelo ac√∫stico sin la necesidad de una alineaci√≥n cuadro por cuadro entre el sonido y la transcripci√≥n.  La alineaci√≥n cuadro por cuadro es cuando decimos que un cuadro particular de un sonido corresponde a dicho cuadro de la transcripci√≥n.  Tenemos un codificador convencional que acepta signos ac√∫sticos como entrada; proporciona alg√∫n tipo de ocultaci√≥n del estado, en base al cual obtenemos probabilidades condicionales usando softmax.  El codificador generalmente consta de varias capas de LSTM u otras variaciones de RNN.  Vale la pena se√±alar que STS funciona adem√°s de los caracteres ordinarios con un car√°cter especial llamado car√°cter vac√≠o o un s√≠mbolo en blanco.  Para resolver el problema que surge debido al hecho de que no todos los cuadros ac√∫sticos tienen un cuadro en transcripci√≥n y viceversa (es decir, tenemos letras o sonidos que suenan mucho m√°s tiempo y hay sonidos cortos, sonidos repetitivos), y all√≠ Este s√≠mbolo en blanco. <br><br>  El STS en s√≠ est√° destinado a maximizar la probabilidad final de secuencias de caracteres y a generalizar la posible alineaci√≥n.  Dado que queremos usar este algoritmo en redes neuronales, se entiende que debemos entender c√≥mo funcionan sus modos de operaci√≥n hacia adelante y hacia atr√°s.  No nos detendremos en la justificaci√≥n matem√°tica y las caracter√≠sticas del funcionamiento de este algoritmo, de lo contrario, llevar√° mucho tiempo. <br><br>  Qu√© tenemos: el primer ASR basado en el algoritmo STS aparece en 2014.  Nuevamente, Alex Graves present√≥ una publicaci√≥n basada en el STS car√°cter por car√°cter que muestra directamente el discurso de entrada en una secuencia de palabras.  Uno de los comentarios que hicieron en este art√≠culo es que usar un modelo de sonido externo es importante para obtener un buen resultado. <br><br><h4>  5 formas de mejorar el algoritmo </h4><br>  Hay muchas variaciones y mejoras diferentes al algoritmo anterior.  Aqu√≠ est√°n, por ejemplo, los cinco m√°s populares recientemente. <br><br>  ‚Ä¢ El modelo de idioma se incluye en la decodificaci√≥n durante la primera pasada. <br>  o [Hannun et al., 2014] [Maas et al., 2015]: decodificaci√≥n directa de primer paso con un LM en lugar de restaurar como en [Graves &amp; Jaitly, 2014] <br>  o [Miao et al., 2015]: marco EESEN para decodificar con WFST, kit de herramientas de c√≥digo abierto <br>  ‚Ä¢ Entrenamiento a gran escala en la GPU;  Aumento de datos  varios idiomas <br>  o [Hannun et al., 2014;  DeepSpeech] [Amodei et al., 2015;  DeepSpeech2]: entrenamiento de GPU a gran escala;  Aumento de datos;  Mandar√≠n e ingl√©s <br>  ‚Ä¢ Uso de unidades largas: palabras en lugar de caracteres. <br>  o [Soltau et al., 2017]: objetivos CTC a nivel de palabra, capacitados en 125,000 horas de discurso.  ¬°Rendimiento cercano o mejor que un sistema convencional, incluso sin usar un LM! <br>  o [Audhkhasi et al., 2017]: Modelos directos de ac√∫stica a palabra en la centralita <br><br>  Vale la pena prestar atenci√≥n a la implementaci√≥n de DeepSpeach como un buen ejemplo de una soluci√≥n CTC end2end y a una variaci√≥n que utiliza un nivel verbal.  Pero hay una advertencia: para entrenar un modelo de este tipo, necesita 125 mil horas de datos etiquetados, lo que en realidad es bastante en realidades duras. <br><br><h4>  Lo que es importante tener en cuenta sobre STS </h4><br><ul><li>  Problemas u omisiones.  Para mayor eficiencia, es importante hacer suposiciones sobre la independencia.  Es decir, el STS supone que la salida de la red en diferentes tramas es condicionalmente independiente, lo que en realidad es incorrecto.  Pero esta suposici√≥n se hace para simplificar, sin ella, todo se vuelve mucho m√°s complicado. </li><li>  Para lograr un buen rendimiento del modelo STS, se requiere el uso de un modelo de lenguaje externo, ya que la decodificaci√≥n codiciosa directa no funciona muy bien. </li></ul><br><h4>  Atencion </h4><br>  ¬øQu√© alternativa tenemos para este STS?  Probablemente no sea ning√∫n secreto para nadie que exista algo como Atenci√≥n o "Atenci√≥n", que revolucion√≥ hasta cierto punto y pas√≥ directamente de las tareas de traducci√≥n autom√°tica.  Y ahora, la mayor√≠a de las decisiones de modelado de secuencia de secuencia se basan en este mecanismo.  ¬øC√≥mo es √©l?  Tratemos de resolverlo.  Por primera vez sobre Atenci√≥n en tareas de reconocimiento de voz, las publicaciones aparecieron en 2015.  Alguien Chen y Cherowski emitieron dos publicaciones similares y diferentes al mismo tiempo. <br><br>  Deteng√°monos en el primero: se llama Escuchar, asistir y deletrear.  En nuestra simulaci√≥n cl√°sica, en la secuencia donde tenemos un codificador y decodificador, se agrega otro elemento, que se llama atenci√≥n.  El ecualizador realizar√° las funciones que el modelo ac√∫stico sol√≠a realizar.  Su tarea es convertir el discurso de entrada en caracter√≠sticas ac√∫sticas de alto nivel.  Nuestro decodificador realizar√° las tareas que previamente realizamos con el modelo de lenguaje y el modelo de pronunciaci√≥n (l√©xico), y pronosticar√° autom√°ticamente cada token de salida, en funci√≥n de los anteriores.  Y la atenci√≥n misma dir√° directamente qu√© marco de entrada es m√°s relevante / importante para predecir esta salida. <br><br><img src="https://habrastorage.org/webt/gi/hd/1c/gihd1cppd9nsldy12rqffbuheoe.png"><br><br>  ¬øQu√© son estos bloques?  El codificador ecol√≥gico del art√≠culo se describe como un oyente, es un RNN bidireccional cl√°sico basado en LSTM u otra cosa.  En general, nada nuevo: el sistema simplemente simula la secuencia de entrada en caracter√≠sticas complejas. <br><br>  La atenci√≥n, por otro lado, crea un determinado vector de contexto C a partir de estos vectores, que ayudar√° a decodificar el decodificador correctamente directamente, el decodificador en s√≠, que es, por ejemplo, tambi√©n algunos LSTM que se decodificar√°n en la secuencia de entrada desde esta capa de atenci√≥n, que ya ha resaltado los signos de estado m√°s importantes, alguna secuencia de salida de caracteres. <br><br>  Tambi√©n hay diferentes representaciones de esta Atenci√≥n misma, que es la diferencia entre estas dos publicaciones emitidas por Chen y Charowski.  Usan diferente atenci√≥n.  Chen usa la atenci√≥n de productos de punto, y Charowski usa la atenci√≥n aditiva. <br><br><img src="https://habrastorage.org/webt/r4/oa/tu/r4oatu1zxse9xn13cmmbx01viem.png"><br><br><h4>  ¬øA d√≥nde ir despu√©s? </h4><br>  Este es un plus o menos de todos los logros importantes recibidos hasta la fecha en materia de reconocimiento de voz no en l√≠nea.  ¬øQu√© mejoras son posibles aqu√≠?  ¬øA d√≥nde ir despu√©s?  Lo m√°s obvio es el uso de un modelo en palabras en lugar de usar grafemas directamente.  Pueden ser algunos morfemas separados u otra cosa. <br><br>  ¬øCu√°l es la motivaci√≥n para usar cortes de palabras?  T√≠picamente, los modelos de lenguaje del nivel verbal tienen mucha menos perplejidad en comparaci√≥n con el nivel de grafema.  Modelar piezas de palabras le permite construir un decodificador m√°s fuerte del modelo de lenguaje.  Y modelar elementos m√°s largos puede mejorar la eficiencia de la memoria en un decodificador basado en LSTM.  Tambi√©n le permite recordar potencialmente la aparici√≥n de palabras de frecuencia.  Los elementos m√°s largos permiten la decodificaci√≥n en menos pasos, lo que acelera directamente la inferencia de este modelo. <br><br>  Adem√°s, el modelo sobre piezas de palabras nos permite resolver el problema de las palabras OOV (fuera del vocabulario) que surgen en el modelo de lenguaje, ya que podemos modelar cualquier palabra usando piezas de palabras.  Y vale la pena se√±alar que tales modelos est√°n entrenados para maximizar la probabilidad de un modelo de lenguaje sobre un conjunto de datos de entrenamiento.  Estos modelos dependen de la posici√≥n, y podemos usar el algoritmo codicioso para la decodificaci√≥n. <br><br>  ¬øQu√© otras mejoras adem√°s del modelo de palabras pueden ser?  Hay un mecanismo llamado atenci√≥n de m√∫ltiples cabezas.  Se describi√≥ por primera vez en 2017 para la traducci√≥n autom√°tica.  La atenci√≥n de m√∫ltiples cabezas implica un mecanismo que tiene varias llamadas cabezas que le permiten generar una distribuci√≥n diferente de esta misma atenci√≥n, lo que mejora los resultados directamente. <br><br><h4>  Modelos en l√≠nea </h4><br>  Pasamos a la parte m√°s interesante: estos son modelos en l√≠nea.  Es importante tener en cuenta que LAS no est√° transmitiendo.  Es decir, este modelo no puede funcionar en modo de decodificaci√≥n en l√≠nea.  Consideraremos los dos modelos en l√≠nea m√°s populares hasta la fecha.  Transductor RNN y Transductor Neural. <br><br>  El transductor RNN fue propuesto por Graves en 2012-2017.  La idea principal es complicar un poco nuestro modelo STS con la ayuda de un modelo recursivo. <br><br><img src="https://habrastorage.org/webt/5c/y6/lc/5cy6lcuv2l7q1nymrr9j4idp5pi.png"><br><br>  Vale la pena se√±alar que ambos componentes se entrenan juntos en los datos ac√∫sticos disponibles.  Al igual que STS, este enfoque no requiere alineaci√≥n de trama en el conjunto de datos de entrenamiento.  Como vemos en la imagen: a la izquierda est√° nuestro STS cl√°sico, y a la derecha est√° el Transductor RNN.  Y tenemos dos elementos nuevos: la <b>red prevista</b> y la <b>red de uni√≥n</b> . <br><br>  El codificador STS es exactamente el mismo: este es el nivel de entrada RNN, que determina la distribuci√≥n en todas las alineaciones con todas las secuencias de salida que no exceden la longitud de la secuencia de entrada; esto fue descrito por Graves en 2006.  Sin embargo, la tarea de tales conversiones de texto a voz tambi√©n se excluye, donde la secuencia de entrada m√°s larga que la secuencia de entrada del STS no modela la relaci√≥n entre las salidas.  El transductor expande este mismo STS, determinando la distribuci√≥n de las secuencias de salida de todas las longitudes y modelando conjuntamente la dependencia de la entrada-salida y la salida-salida. <br><br>  Resulta que nuestro modelo es capaz de manejar las dependencias de la salida de la entrada y la salida de la salida del √∫ltimo paso. <br><br>  Entonces, ¬øqu√© es una <b>red</b> pronosticada o una red predictiva?  Ella trata de simular cada elemento teniendo en cuenta los anteriores, por lo tanto, es similar al RNN est√°ndar con el pron√≥stico del siguiente paso.  Solo con la capacidad adicional de hacer hip√≥tesis nulas. <br><br>  Como vemos en la imagen, tenemos una red pronosticada, que recibe el valor anterior de la salida, y hay un codificador, que recibe el valor actual de la entrada.  Y a la salida nuevamente, tal tiene el valor actual <img src="https://habrastorage.org/webt/x4/db/gw/x4dbgwm67dwzli8xp3ysvhqm-tu.png">  . <br><br>  <b>Transductor Neural</b> .  Esta es una complicaci√≥n del enfoque cl√°sico seq-2seq.  El codificador procesa la secuencia ac√∫stica de entrada para crear vectores de estado ocultos en cada paso de tiempo.  Todo parece ser como siempre.  Pero hay un elemento Transductor adicional que recibe un bloque de entrada en cada paso y genera hasta tokens de salida M utilizando el modelo basado en seq-2seq sobre esta entrada.  El transductor mantiene su estado en bloques mediante el uso de conexiones peri√≥dicas con los pasos de tiempo anteriores. <br><br><img src="https://habrastorage.org/webt/hh/pn/wx/hhpnwx3l2sco6phy37tmuawvcni.png"><br><br>  <i>La figura muestra el Transductor, produciendo tokens para el bloque para la secuencia utilizada en el bloque del Ym correspondiente.</i> <br><br>  Entonces, examinamos el estado actual del reconocimiento de voz basado en el enfoque End2End.  Vale la pena decir que, desafortunadamente, estos enfoques requieren hoy una gran cantidad de datos.  Y los resultados reales que se logran con el enfoque cl√°sico, que requieren de 200 a 500 horas de grabaciones de sonido marcadas para entrenar un buen modelo basado en End2End, requerir√°n varios, o tal vez decenas de veces m√°s datos.  Ahora este es el mayor problema con estos enfoques.  Pero quiz√°s pronto todo cambie. <br><br>  <i>Desarrollador l√≠der del centro AI MTS Nikita Semenov.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/468663/">https://habr.com/ru/post/468663/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../468639/index.html">Novedades de las consolas web 2019</a></li>
<li><a href="../468645/index.html">Dos p√°ginas fueron suficientes para probar la hip√≥tesis de 30 a√±os del campo de la inform√°tica.</a></li>
<li><a href="../468647/index.html">M√∫sica arriesgada en una vieja impresora de l√≠nea de mainframe de IBM</a></li>
<li><a href="../468653/index.html">¬øCu√°l es la resoluci√≥n del ojo humano (o cu√°ntos megap√≠xeles vemos en un momento dado)</a></li>
<li><a href="../468657/index.html">Bailes con apoyo: tipos y formas de apoyo. Sistemas de soporte trabajando en batalla</a></li>
<li><a href="../468665/index.html">¬øPero es hora de comprar un irrigador?</a></li>
<li><a href="../468673/index.html">Taller "Garantizar la seguridad de los datos personales" - 3 de octubre, San Petersburgo</a></li>
<li><a href="../468677/index.html">El anuncio del tel√©fono inteligente Xiaomi Mi Mix Alpha</a></li>
<li><a href="../468679/index.html">El ABC de la seguridad en Kubernetes: autenticaci√≥n, autorizaci√≥n, auditor√≠a</a></li>
<li><a href="../468683/index.html">Teor√≠a y pr√°ctica de la estandarizaci√≥n de los servicios de Docker.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>