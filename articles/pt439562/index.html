<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüîß üëÉüèΩ üéûÔ∏è Configure o cluster de alta disponibilidade do Kubernetes no bare metal com o kubeadm. Parte 1/3 üê¨ üßõüèæ üë®‚Äçüöí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 2/3 aqui 
 Parte 3/3 aqui 


 Ol√° pessoal! Neste artigo, quero otimizar as informa√ß√µes e compartilhar a experi√™ncia de cria√ß√£o e uso do cluster ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configure o cluster de alta disponibilidade do Kubernetes no bare metal com o kubeadm. Parte 1/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/439562/"><p>  <strong>Parte 2/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>aqui</strong></a> <br>  <strong>Parte 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>aqui</strong></a> </p><br><p>  Ol√° pessoal!  Neste artigo, quero otimizar as informa√ß√µes e compartilhar a experi√™ncia de cria√ß√£o e uso do cluster interno do Kubernetes. </p><br><p>  Nos √∫ltimos anos, essa tecnologia de orquestra√ß√£o de cont√™ineres deu um grande passo √† frente e se tornou um tipo de padr√£o corporativo para milhares de empresas.  Alguns o usam na produ√ß√£o, outros apenas o testam em projetos, mas as paix√µes em torno dele, n√£o importa como voc√™ diz, brilham com seriedade.  Se voc√™ nunca o usou antes, √© hora de come√ßar a namorar. </p><br><h3 id="0-vstuplenie">  0. Introdu√ß√£o </h3><br><p>  O Kubernetes √© uma tecnologia de orquestra√ß√£o escal√°vel que pode come√ßar com a instala√ß√£o em um √∫nico n√≥ e atingir o tamanho de grandes clusters de HA com base em v√°rias centenas de n√≥s no interior.  Os provedores de nuvem mais populares fornecem diferentes tipos de implementa√ß√µes do Kubernetes - uso e uso.  Mas as situa√ß√µes s√£o diferentes e existem empresas que n√£o usam nuvens e desejam obter todas as vantagens das modernas tecnologias de orquestra√ß√£o.  E aqui vem a instala√ß√£o do Kubernetes no bare metal. </p><br><p><img src="https://habrastorage.org/webt/el/ci/ua/elciua9kwxmo0fnnm5yoaabqpvm.jpeg"></p><a name="habracut"></a><br><h3 id="1-vvedenie">  1. Introdu√ß√£o </h3><br><p> Neste exemplo, criaremos um cluster de HA do Kubernetes com a topologia para v√°rios mestres, com um cluster externo etcd como a camada base e um balanceador de carga do MetalLB.  Em todos os n√≥s em funcionamento, implantaremos o GlusterFS como um simples armazenamento em cluster distribu√≠do interno.  Tamb√©m tentaremos implantar v√°rios projetos de teste usando nosso registro pessoal do Docker. </p><br><p>  Em geral, existem v√°rias maneiras de criar um cluster de alta disponibilidade do Kubernetes: o caminho dif√≠cil e detalhado descrito no popular documento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubernetes da maneira mais dif√≠cil</a> , ou a maneira mais simples de usar o utilit√°rio <strong>kubeadm</strong> . </p><br><p>  O Kubeadm √© uma ferramenta criada pela comunidade Kubernetes especificamente para simplificar a instala√ß√£o do Kubernetes e facilitar o processo.  Anteriormente, o Kubeadm era recomendado apenas para criar pequenos clusters de teste com um n√≥ principal, para come√ßar.  Mas, no ano passado, muito foi aprimorado e agora podemos us√°-lo para criar clusters de alta disponibilidade com v√°rios n√≥s principais.  De acordo com as not√≠cias da comunidade Kubernetes, no futuro, o Kubeadm ser√° recomendado como uma ferramenta para instalar o Kubernetes. </p><br><p>  A documenta√ß√£o do Kubeadm oferece duas maneiras b√°sicas de implementar um cluster, com topologias de pilha e etcd externas.  Escolherei o segundo caminho com os n√≥s externos etcd devido √† toler√¢ncia a falhas do cluster de alta disponibilidade. </p><br><p>  Aqui est√° um diagrama da documenta√ß√£o do Kubeadm que descreve este caminho: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/4y/nh/gd/4ynhgd4h3ireojrvdplimgnsk2u.jpeg"></a> </p><br><p>  Vou mudar um pouco.  Primeiro, usarei um par de servidores HAProxy como balanceadores de carga com o pacote Heartbeat, que compartilhar√° o endere√ßo IP virtual.  O Heartbeat e o HAProxy usam uma pequena quantidade de recursos do sistema, portanto, eu os colocarei em um par de n√≥s etcd para reduzir um pouco o n√∫mero de servidores para nosso cluster. </p><br><p>  Para esse esquema de cluster do Kubernetes, s√£o necess√°rios oito n√≥s.  Tr√™s servidores para um cluster externo etcd (os servi√ßos LB tamb√©m usar√£o alguns), dois para n√≥s do plano de controle (n√≥s principais) e tr√™s para n√≥s em funcionamento.  Pode ser bare metal ou um servidor VM.  Nesse caso, isso n√£o importa.  Voc√™ pode alterar facilmente o esquema adicionando mais n√≥s principais e colocando o HAProxy com Heartbeat em n√≥s separados, se houver muitos servidores gratuitos.  Embora minha op√ß√£o para a primeira implementa√ß√£o do cluster HA seja suficiente para os olhos. </p><br><p>  Se desejar, adicione um servidor pequeno com o utilit√°rio <strong>kubectl instalado</strong> para gerenciar este cluster ou use sua pr√≥pria √°rea de trabalho Linux. </p><br><p>  O diagrama para este exemplo ser√° parecido com este: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/r4/5w/uc/r45wucdscdlhmaqcuw-gtr7mrmm.jpeg"></a> </p><br><h3 id="2-trebovaniya">  2. Requisitos </h3><br><p>  Voc√™ precisar√° de dois n√≥s principais do Kubernetes com os requisitos m√≠nimos de sistema recomendados: 2 CPUs e 2 GB de RAM, de acordo com a documenta√ß√£o do <strong>kubeadm</strong> .  Para n√≥s de trabalho, recomendo usar servidores mais poderosos, pois executaremos todos os nossos servi√ßos de aplicativos neles.  E para Etcd + LB, tamb√©m podemos receber servidores com duas CPUs e pelo menos 2 GB de RAM. </p><br><p>  Selecione uma rede p√∫blica ou rede privada para este cluster;  Endere√ßos IP n√£o importam;  √â importante que todos os servidores estejam acess√≠veis um para o outro e, √© claro, para voc√™.  Mais tarde, dentro do cluster Kubernetes, configuraremos uma rede de sobreposi√ß√£o. </p><br><p>  Os requisitos m√≠nimos para este exemplo s√£o: </p><br><ul><li>  2 servidores com 2 processadores e 2 GB de RAM para o n√≥ principal </li><li>  3 servidores com 4 processadores e 4-8 GB de RAM para n√≥s de trabalho </li><li>  3 servidores com 2 processadores e 2 GB de RAM para Etcd e HAProxy </li><li>  192.168.0.0/24 - a sub-rede. </li></ul><br><p>  192.168.0.1 - o endere√ßo IP virtual do HAProxy, 192.168.0.2 - 4 endere√ßos IP principais dos n√≥s Etcd e HAProxy, 192.168.0.5 - 6 endere√ßos IP principais do n√≥ principal do Kubernetes, 192.168.0.7 - 9 endere√ßos IP principais dos n√≥s de trabalho do Kubernetes . </p><br><p>  O banco de dados Debian 9 est√° instalado em todos os servidores. </p><br><blockquote>  Lembre-se tamb√©m de que os requisitos do sistema dependem de qu√£o grande e poderoso √© o cluster.  Para mais informa√ß√µes, consulte a documenta√ß√£o do Kubernetes. </blockquote><br><h3 id="3-nastroyka-haproxy-i-heartbeat">  3. Configure o HAProxy e a pulsa√ß√£o. </h3><br><p>  Temos mais de um n√≥ mestre do Kubernetes e, portanto, voc√™ precisa configurar um balanceador de carga HAProxy na frente deles - para distribuir o tr√°fego.  Este ser√° um par de servidores HAProxy com um endere√ßo IP virtual compartilhado.  A toler√¢ncia a falhas √© fornecida com o pacote de pulsa√ß√£o.  Para implanta√ß√£o, usaremos os dois primeiros servidores etcd. </p><br><p>  Instale e configure o HAProxy com Heartbeat no primeiro e no segundo servidores etcd (192.168.0.2‚Äì3 neste exemplo): </p><br><pre><code class="plaintext hljs">etcd1# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy etcd2# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy</code> </pre> <br><p>  Salve a configura√ß√£o original e crie uma nova: </p><br><pre> <code class="plaintext hljs">etcd1# mv /etc/haproxy/haproxy.cfg{,.back} etcd1# vi /etc/haproxy/haproxy.cfg etcd2# mv /etc/haproxy/haproxy.cfg{,.back} etcd2# vi /etc/haproxy/haproxy.cfg</code> </pre> <br><p>  Adicione estas op√ß√µes de configura√ß√£o para o HAProxy: </p><br><pre> <code class="plaintext hljs">global user haproxy group haproxy defaults mode http log global retries 2 timeout connect 3000ms timeout server 5000ms timeout client 5000ms frontend kubernetes bind 192.168.0.1:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server k8s-master-0 192.168.0.5:6443 check fall 3 rise 2 server k8s-master-1 192.168.0.6:6443 check fall 3 rise 2</code> </pre> <br><p>  Como voc√™ pode ver, os dois servi√ßos HAProxy compartilham o endere√ßo IP - 192.168.0.1.  Esse endere√ßo IP virtual se mover√° entre os servidores, portanto, seremos um pouco espertos e <strong>habilitaremos o</strong> par√¢metro <strong>net.ipv4.ip_nonlocal_bind</strong> para permitir a liga√ß√£o de servi√ßos do sistema a um endere√ßo IP n√£o local. </p><br><p>  Adicione esse recurso ao arquivo <strong>/etc/sysctl.conf</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1 etcd2# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1</code> </pre> <br><p>  Execute nos dois servidores: </p><br><pre> <code class="plaintext hljs">sysctl -p</code> </pre> <br><p>  Execute tamb√©m o HAProxy nos dois servidores: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl start haproxy etcd2# systemctl start haproxy</code> </pre> <br><p>  Verifique se o HAProxy est√° em execu√ß√£o e escutando o endere√ßo IP virtual nos dois servidores: </p><br><pre> <code class="plaintext hljs">etcd1# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy etcd2# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy</code> </pre> <br><p>  Hood!  Agora instale o Heartbeat e configure esse IP virtual. </p><br><pre> <code class="plaintext hljs">etcd1# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat etcd2# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat</code> </pre> <br><p>  Est√° na hora de criar v√°rios arquivos de configura√ß√£o: para o primeiro e o segundo servidores, eles ser√£o basicamente os mesmos. </p><br><p>  Primeiro, crie o arquivo <strong>/etc/ha.d/authkeys</strong> , neste arquivo o Heartbeat armazena dados para autentica√ß√£o m√∫tua.  O arquivo deve ser o mesmo nos dois servidores: </p><br><pre> <code class="plaintext hljs"># echo -n securepass | md5sum bb77d0d3b3f239fa5db73bdf27b8d29a etcd1# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a etcd2# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a</code> </pre> <br><p>  Este arquivo deve estar acess√≠vel apenas para a raiz: </p><br><pre> <code class="plaintext hljs">etcd1# chmod 600 /etc/ha.d/authkeys etcd2# chmod 600 /etc/ha.d/authkeys</code> </pre> <br><p>  Agora crie o arquivo de configura√ß√£o principal do Heartbeat nos dois servidores: para cada servidor, ele ser√° um pouco diferente. </p><br><p>  Crie <strong>/etc/ha.d/ha.cf</strong> : </p><br><p>  <strong>etcd1</strong> </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.3 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to log/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  <strong>etcd2</strong> </p><br><pre> <code class="plaintext hljs">etcd2# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.2 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to vlog/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  Obtenha os par√¢metros "node" para esta configura√ß√£o executando uname -n nos dois servidores Etcd.  Use tamb√©m o nome da sua placa de rede em vez de ens18. </p><br><p>  Por fim, voc√™ precisa criar o arquivo <strong>/etc/ha.d/haresources</strong> nesses servidores.  Para os dois servidores, o arquivo deve ser o mesmo.  Neste arquivo, definimos nosso endere√ßo IP comum e determinamos qual n√≥ √© o mestre padr√£o: </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1 etcd2# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1</code> </pre> <br><p>  Quando tudo estiver pronto, inicie os servi√ßos de pulsa√ß√£o nos dois servidores e verifique se recebemos esse IP virtual declarado no <strong>n√≥</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl restart heartbeat etcd2# systemctl restart heartbeat etcd1# ip a ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff inet 192.168.0.2/24 brd 192.168.0.255 scope global ens18 valid_lft forever preferred_lft forever inet 192.168.0.1/24 brd 192.168.0.255 scope global secondary</code> </pre> <br><p>  Voc√™ pode verificar se o HAProxy est√° funcionando bem executando <strong>nc</strong> em 192.168.0.1 6443. Voc√™ deve ter expirado o tempo limite porque a API do Kubernetes ainda n√£o est√° escutando no servidor.  Mas isso significa que o HAProxy e o Heartbeat est√£o configurados corretamente. </p><br><pre> <code class="plaintext hljs"># nc -v 192.168.0.1 6443 Connection to 93.158.95.90 6443 port [tcp/*] succeeded!</code> </pre> <br><h3 id="4-podgotovka-nod-dlya-kubernetes">  4. Prepara√ß√£o de n√≥s para Kubernetes </h3><br><p>  O pr√≥ximo passo √© preparar todos os n√≥s do Kubernetes.  Voc√™ precisa instalar o Docker com alguns pacotes adicionais, adicionar o reposit√≥rio Kubernetes e instalar os <strong>pacotes kubelet</strong> , <strong>kubeadm</strong> , <strong>kubectl</strong> .  Essa configura√ß√£o √© a mesma para todos os n√≥s do Kubernetes (mestre, trabalhadores, etc.) </p><br><blockquote>  A principal vantagem do <strong>Kubeadm</strong> √© que n√£o h√° muito software adicional necess√°rio.  Instale o <strong>kubeadm</strong> em todos os hosts - e use-o;  pelo menos gere certificados de autoridade de certifica√ß√£o. </blockquote><p>  Instale o Docker em todos os n√≥s: </p><br><pre> <code class="plaintext hljs">Update the apt package index # apt-get update Install packages to allow apt to use a repository over HTTPS # apt-get -y install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker's official GPG key # curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add docker apt repository # apt-add-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install docker-ce. # apt-get update &amp;&amp; apt-get -y install docker-ce Check docker version # docker -v Docker version 18.09.0, build 4d60db4</code> </pre> <br><p>  Depois disso, instale os pacotes Kubernetes em todos os n√≥s: </p><br><ul><li>  <strong><code>kubeadm</code></strong> : comando para carregar o cluster. </li><li>  <strong><code>kubelet</code></strong> : um componente que √© executado em todos os computadores do cluster e executa a√ß√µes como iniciar fog√µes e cont√™ineres. </li><li>  <strong><code>kubectl</code></strong> comando <strong><code>kubectl</code></strong> : util para se comunicar com o cluster. </li><li>  <strong>kubectl</strong> - √† vontade;  Costumo instal√°-lo em todos os n√≥s para executar alguns comandos do Kubernetes para depura√ß√£o. </li></ul><br><pre> <code class="plaintext hljs"># curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository # cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install packages # apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl Hold back packages # apt-mark hold kubelet kubeadm kubectl Check kubeadm version # kubeadm version kubeadm version: &amp;version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.1", GitCommit:"eec55b9dsfdfgdfgfgdfgdfgdf365bdd920", GitTreeState:"clean", BuildDate:"2018-12-13T10:36:44Z", GoVersion:"go1.11.2", Compiler:"gc", Platform:"linux/amd64"}</code> </pre> <br><p>  Ap√≥s <strong>instalar o kubeadm</strong> e outros pacotes, n√£o se esque√ßa de desativar o swap. </p><br><pre> <code class="plaintext hljs"># swapoff -a # sed -i '/ swap / s/^/#/' /etc/fstab</code> </pre> <br><p>  Repita a instala√ß√£o nos n√≥s restantes.  Os pacotes de software s√£o os mesmos para todos os n√≥s no cluster e apenas a seguinte configura√ß√£o determinar√° as fun√ß√µes que eles receber√£o posteriormente. </p><br><h3 id="5-nastroyka-klastera-ha-etcd">  5. Configure o HA Etcd Cluster </h3><br><p>  Assim, ap√≥s concluir os preparativos, configuraremos o cluster Kubernetes.  O primeiro bloco ser√° o cluster HA Etcd, que tamb√©m √© configurado usando a ferramenta kubeadm. </p><br><blockquote>  Antes de come√ßar, verifique se todos os n√≥s etcd se comunicam pelas portas 2379 e 2380. Al√©m disso, voc√™ precisa configurar o acesso ssh entre eles para usar o <strong>scp</strong> . </blockquote><p>  Vamos come√ßar com o primeiro n√≥ etcd e depois copiar todos os certificados e arquivos de configura√ß√£o necess√°rios para os outros servidores. </p><br><p>  Em todos os n√≥s <strong>etcd</strong> , voc√™ precisa adicionar um novo arquivo de configura√ß√£o do <strong>systemd</strong> para a unidade <strong>kubelet</strong> com uma prioridade mais alta: </p><br><pre> <code class="plaintext hljs">etcd-nodes# cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true Restart=always EOF etcd-nodes# systemctl daemon-reload etcd-nodes# systemctl restart kubelet</code> </pre> <br><p>  Depois, passaremos o ssh para o primeiro n√≥ <strong>etcd</strong> - vamos us√°-lo para gerar todas as configura√ß√µes necess√°rias do <strong>kubeadm</strong> para cada n√≥ <strong>etcd</strong> e depois copi√°-las. </p><br><pre> <code class="plaintext hljs"># Export all our etcd nodes IP's as variables etcd1# export HOST0=192.168.0.2 etcd1# export HOST1=192.168.0.3 etcd1# export HOST2=192.168.0.4 # Create temp directories to store files for all nodes etcd1# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ etcd1# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) etcd1# NAMES=("infra0" "infra1" "infra2") etcd1# for i in "${!ETCDHOSTS[@]}"; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: "kubeadm.k8s.io/v1beta1" kind: ClusterConfiguration etcd: local: serverCertSANs: - "${HOST}" peerCertSANs: - "${HOST}" extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done</code> </pre> <br><p>  Agora crie a autoridade de certifica√ß√£o principal usando o <strong>kubeadm</strong> </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase certs etcd-ca</code> </pre> <br><p>  Este comando criar√° dois <strong>arquivos ca.crt e ca.key</strong> no <strong>diret√≥rio</strong> <strong>/ etc / kubernetes / pki / etcd /</strong> . </p><br><pre> <code class="plaintext hljs">etcd1# ls /etc/kubernetes/pki/etcd/ ca.crt ca.key</code> </pre> <br><p>  Agora vamos gerar certificados para todos os n√≥s <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">### Create certificates for the etcd3 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST2}/ ### cleanup non-reusable certificates etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the etcd2 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST1}/ ### cleanup non-reusable certificates again etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the this local node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1 #kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for this node # clean up certs that should not be copied off this host etcd1# find /tmp/${HOST2} -name ca.key -type f -delete etcd1# find /tmp/${HOST1} -name ca.key -type f -delete</code> </pre> <br><p>  Em seguida, copie os certificados e configura√ß√µes do kubeadm para os n√≥s <strong>etcd2</strong> e <strong>etcd3</strong> . </p><br><blockquote>  Primeiro gere um par de chaves ssh no <strong>etcd1</strong> e adicione a parte p√∫blica aos <strong>n√≥s etcd2</strong> e <strong>3</strong> .  Neste exemplo, todos os comandos s√£o executados em nome de um usu√°rio que possui todos os direitos no sistema. </blockquote><br><pre> <code class="plaintext hljs">etcd1# scp -r /tmp/${HOST1}/* ${HOST1}: etcd1# scp -r /tmp/${HOST2}/* ${HOST2}: ### login to the etcd2 or run this command remotely by ssh etcd2# cd /root etcd2# mv pki /etc/kubernetes/ ### login to the etcd3 or run this command remotely by ssh etcd3# cd /root etcd3# mv pki /etc/kubernetes/</code> </pre> <br><p>  Antes de iniciar o cluster etcd, verifique se os arquivos existem em todos os n√≥s: </p><br><p>  Lista de arquivos necess√°rios no <strong>etcd1</strong> : </p><br><pre> <code class="plaintext hljs">/tmp/192.168.0.2 ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ ca.key ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Para o n√≥ <strong>etcd2,</strong> √© o seguinte: </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  E o √∫ltimo n√≥ √© <strong>etcd3</strong> : </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Quando todos os certificados e configura√ß√µes est√£o em vigor, criamos manifestos.  Em cada n√≥, execute o comando <strong>kubeadm</strong> - para gerar um manifesto est√°tico para o cluster <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase etcd local --config=/tmp/192.168.0.2/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml</code> </pre> <br><p>  Agora o cluster <strong>etcd</strong> - em teoria - est√° configurado e √≠ntegro.  Verifique executando o seguinte comando no <strong>n√≥</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \ --cert-file /etc/kubernetes/pki/etcd/peer.crt \ --key-file /etc/kubernetes/pki/etcd/peer.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://192.168.0.2:2379 cluster-health ### status output member 37245675bd09ddf3 is healthy: got healthy result from https://192.168.0.3:2379 member 532d748291f0be51 is healthy: got healthy result from https://192.168.0.4:2379 member 59c53f494c20e8eb is healthy: got healthy result from https://192.168.0.2:2379 cluster is healthy</code> </pre> <br><p>  O cluster <strong>etcd</strong> aumentou, ent√£o siga em frente. </p><br><h3 id="6-nastroyka-master--i-rabochih-nod">  6. Configurando n√≥s principais e de trabalho </h3><br><p>  Configure os n√≥s principais do nosso cluster - copie esses arquivos do primeiro n√≥ <strong>etcd</strong> para o primeiro n√≥ principal: </p><br><pre> <code class="plaintext hljs">etcd1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.0.5:</code> </pre> <br><p>  Em seguida, v√° ssh para o n√≥ principal <strong>master1</strong> e crie o <strong>arquivo kubeadm-config.yaml</strong> com o seguinte conte√∫do: </p><br><pre> <code class="plaintext hljs">master1# cd /root &amp;&amp; vi kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - "192.168.0.1" controlPlaneEndpoint: "192.168.0.1:6443" etcd: external: endpoints: - https://192.168.0.2:2379 - https://192.168.0.3:2379 - https://192.168.0.4:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</code> </pre> <br><p>  Mova os certificados e chaves copiados anteriormente para o diret√≥rio apropriado no <strong>n√≥</strong> master1, como na descri√ß√£o da configura√ß√£o. </p><br><pre> <code class="plaintext hljs">master1# mkdir -p /etc/kubernetes/pki/etcd/ master1# cp /root/ca.crt /etc/kubernetes/pki/etcd/ master1# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ master1# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/</code> </pre> <br><p>  Para criar o primeiro n√≥ principal, fa√ßa: </p><br><pre> <code class="plaintext hljs">master1# kubeadm init --config kubeadm-config.yaml</code> </pre> <br><p>  Se todas as etapas anteriores forem conclu√≠das corretamente, voc√™ ver√° o seguinte: </p><br><pre> <code class="plaintext hljs">You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Copie esta <strong>sa√≠da de</strong> inicializa√ß√£o do <strong>kubeadm</strong> para qualquer arquivo de texto; usaremos esse token no futuro quando anexarmos o segundo n√≥ principal e os n√≥s de trabalho ao nosso cluster. </p><br><p>  Eu j√° disse que o cluster Kubernetes usar√° algum tipo de rede de sobreposi√ß√£o para lareiras e outros servi√ßos, portanto, neste ponto, voc√™ precisa instalar algum tipo de plugin CNI.  Eu recomendo o plugin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Weave CNI</a> .  A experi√™ncia demonstrou que √© mais √∫til e menos problem√°tica, mas voc√™ pode escolher outro, por exemplo, o Calico. </p><br><p>  Instalando o plug-in de rede Weave no primeiro n√≥ principal: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" The connection to the server localhost:8080 was refused - did you specify the right host or port? serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created</code> </pre> <br><p>  Aguarde um momento e, em seguida, digite o seguinte comando para verificar se as lareiras dos componentes s√£o iniciadas: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 6m25s coredns-86c58d9df4-xj98p 1/1 Running 0 6m25s kube-apiserver-master1 1/1 Running 0 5m22s kube-controller-manager-master1 1/1 Running 0 5m41s kube-proxy-8ncqw 1/1 Running 0 6m25s kube-scheduler-master1 1/1 Running 0 5m25s weave-net-lvwrp 2/2 Running 0 78s</code> </pre> <br><ul><li>  √â recomend√°vel anexar novos n√≥s do plano de controle somente ap√≥s a inicializa√ß√£o do primeiro n√≥. </li></ul><br><p>  Para verificar o status do cluster, fa√ßa: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 11m v1.13.1</code> </pre> <br><p>  √ìtimo!  O primeiro n√≥ principal subiu.  Agora est√° pronto e concluiremos a cria√ß√£o do cluster Kubernetes - adicionaremos um segundo n√≥ mestre e os n√≥s em funcionamento. <br>  Para adicionar um segundo n√≥ mestre, crie uma chave ssh em <strong>master1</strong> e adicione a parte p√∫blica em <strong>master2</strong> .  Execute um logon de teste e copie alguns arquivos do primeiro n√≥ mestre para o segundo: </p><br><pre> <code class="plaintext hljs">master1# scp /etc/kubernetes/pki/ca.crt 192.168.0.6: master1# scp /etc/kubernetes/pki/ca.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.pub 192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.key @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.key @192.168.0.6: master1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.6:etcd-ca.crt master1# scp /etc/kubernetes/admin.conf 192.168.0.6: ### Check that files was copied well master2# ls /root admin.conf ca.crt ca.key etcd-ca.crt front-proxy-ca.crt front-proxy-ca.key sa.key sa.pub</code> </pre> <br><p>  No segundo n√≥ principal, mova os certificados e chaves copiados anteriormente para os diret√≥rios apropriados: </p><br><pre> <code class="plaintext hljs">master2# mkdir -p /etc/kubernetes/pki/etcd mv /root/ca.crt /etc/kubernetes/pki/ mv /root/ca.key /etc/kubernetes/pki/ mv /root/sa.pub /etc/kubernetes/pki/ mv /root/sa.key /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.key /etc/kubernetes/pki/ mv /root/front-proxy-ca.crt /etc/kubernetes/pki/ mv /root/front-proxy-ca.key /etc/kubernetes/pki/ mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /root/admin.conf /etc/kubernetes/admin.conf</code> </pre> <br><p>  Conecte o segundo n√≥ principal ao cluster.  Para fazer isso, voc√™ precisa da sa√≠da do comando connection, que anteriormente foi passado para n√≥s pelo <strong><code>kubeadm init</code></strong> no primeiro n√≥. </p><br><p>  Execute o n√≥ principal <strong>master2</strong> : </p><br><pre> <code class="plaintext hljs">master2# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6 --experimental-control-plane</code> </pre> <br><ul><li>  Voc√™ precisa adicionar o <strong><code>--experimental-control-plane</code></strong> .  Ele automatiza a anexa√ß√£o de dados mestre a um cluster.  Sem esse sinalizador, o n√≥ de trabalho usual ser√° simplesmente adicionado. </li></ul><br><p>  Aguarde um pouco at√© o n√≥ ingressar no cluster e verifique o novo estado do cluster: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 32m v1.13.1 master2 Ready master 46s v1.13.1</code> </pre> <br><p>  Verifique tamb√©m se todos os pods de todos os n√≥s principais s√£o iniciados normalmente: </p><br><pre> <code class="plaintext hljs">master1# kubectl ‚Äî kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 46m coredns-86c58d9df4-xj98p 1/1 Running 0 46m kube-apiserver-master1 1/1 Running 0 45m kube-apiserver-master2 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 45m kube-controller-manager-master2 1/1 Running 0 15m kube-proxy-8ncqw 1/1 Running 0 46m kube-proxy-px5dt 1/1 Running 0 15m kube-scheduler-master1 1/1 Running 0 45m kube-scheduler-master2 1/1 Running 0 15m weave-net-ksvxz 2/2 Running 1 15m weave-net-lvwrp 2/2 Running 0 41m</code> </pre> <br><p>  √ìtimo!  Estamos quase terminando a configura√ß√£o de cluster do Kubernetes.  E a √∫ltima coisa a fazer √© adicionar os tr√™s n√≥s de trabalho que preparamos anteriormente. </p><br><p>  Digite os n√≥s de trabalho e execute o comando kubeadm join sem o <strong><code>--experimental-control-plane</code></strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Verifique o estado do cluster novamente: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h30m v1.13.1 master2 Ready master 1h59m v1.13.1 worker1 Ready &lt;none&gt; 1h8m v1.13.1 worker2 Ready &lt;none&gt; 1h8m v1.13.1 worker3 Ready &lt;none&gt; 1h7m v1.13.1</code> </pre> <br><p>  Como voc√™ pode ver, temos um cluster de alta disponibilidade do Kubernetes com dois n√≥s principais e tr√™s n√≥s de trabalho.  Ele √© constru√≠do com base no cluster etcd HA com um balanceador de carga √† prova de falhas na frente dos n√≥s principais.  Parece muito bom para mim. </p><br><h3 id="7-nastroyka-udalennogo-upravleniya-klasterom">  7. Configurando o gerenciamento de cluster remoto </h3><br><p>  Outra a√ß√£o que ainda deve ser considerada nesta primeira parte do artigo √© configurar o utilit√°rio <strong>kubectl</strong> remoto para gerenciar o cluster.  Anteriormente, executamos todos os comandos do n√≥ mestre <strong>master1</strong> , mas isso √© adequado apenas pela primeira vez - ao configurar o cluster.  Seria bom configurar um n√≥ de controle externo.  Voc√™ pode usar um laptop ou outro servidor para isso. </p><br><p>  Efetue login neste servidor e execute: </p><br><pre> <code class="plaintext hljs">Add the Google repository key control# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository control# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install kubectl control# apt-get update &amp;&amp; apt-get install -y kubectl In your user home dir create control# mkdir ~/.kube Take the Kubernetes admin.conf from the master1 node control# scp 192.168.0.5:/etc/kubernetes/admin.conf ~/.kube/config Check that we can send commands to our cluster control# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 6h58m v1.13.1 master2 Ready master 6h27m v1.13.1 worker1 Ready &lt;none&gt; 5h36m v1.13.1 worker2 Ready &lt;none&gt; 5h36m v1.13.1 worker3 Ready &lt;none&gt; 5h36m v1.13.1</code> </pre> <br><p>  Ok, agora vamos executar um teste em nosso cluster e verificar como ele funciona. </p><br><pre> <code class="plaintext hljs">control# kubectl create deployment nginx --image=nginx deployment.apps/nginx created control# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-5c7588df-6pvgr 1/1 Running 0 52s</code> </pre> <br><p>  Parab√©ns!  Voc√™ acabou de implantar o Kubernetes.  E isso significa que seu novo cluster de alta disponibilidade est√° pronto.  De fato, o processo de configura√ß√£o de um cluster <strong>Kubernetes</strong> usando o <strong>kubeadm √©</strong> bastante simples e r√°pido. </p><br><p>  Na pr√≥xima parte do artigo, adicionaremos armazenamento interno configurando o GlusterFS em todos os n√≥s em funcionamento, configurando um balanceador de carga interno para o cluster Kubernetes e executando tamb√©m alguns testes de estresse, desconectando alguns n√≥s e verificando a estabilidade do cluster. </p><br><h3 id="posleslovie">  Posf√°cio </h3><br><p>  Sim, trabalhando neste exemplo, voc√™ encontrar√° v√°rios problemas.  N√£o precisa se preocupar: para desfazer as altera√ß√µes e retornar os n√≥s ao seu estado original, basta executar a <strong>redefini√ß√£o do kubeadm</strong> - as altera√ß√µes feitas pelo <strong>kubeadm</strong> anteriormente ser√£o redefinidas e voc√™ poder√° configurar novamente.  Al√©m disso, n√£o esque√ßa de verificar o status dos cont√™ineres do Docker nos n√≥s do cluster - verifique se todos eles iniciam e funcionam sem erros.  Para obter mais informa√ß√µes sobre cont√™ineres danificados, use o <strong>comando docker logs containerid</strong> . </p><br><p>  Isso √© tudo por hoje.  Boa sorte </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt439562/">https://habr.com/ru/post/pt439562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt439550/index.html">Hackquest 2018. Resultados e revis√µes. Dia 1-3</a></li>
<li><a href="../pt439552/index.html">Extens√µes maliciosas do Chrome</a></li>
<li><a href="../pt439556/index.html">TDMS Fairway. Metodologias do PMBOK e organiza√ß√µes de design russas</a></li>
<li><a href="../pt439558/index.html">Novo telefone antigo. Reinvente o telefone PSTN</a></li>
<li><a href="../pt439560/index.html">Adaptador blockchain Ethereum para a plataforma de dados InterSystems IRIS</a></li>
<li><a href="../pt439564/index.html">Aplica√ß√£o pr√°tica da transforma√ß√£o de √°rvore AST usando Putout como exemplo</a></li>
<li><a href="../pt439566/index.html">Por que a documenta√ß√£o do SRE √© importante. Parte 3</a></li>
<li><a href="../pt439568/index.html">SSDs baseados em QLC - um assassino de disco r√≠gido? Realmente n√£o</a></li>
<li><a href="../pt439570/index.html">Magia IPython para editar tags de c√©lulas Jupyter</a></li>
<li><a href="../pt439572/index.html">Projeto assistido por computador de equipamentos eletr√¥nicos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>