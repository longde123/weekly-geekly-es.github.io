<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òïÔ∏è üë®üèæ‚Äçüíª ü§≤üèº Oh, este m√©todo de Newton ü•¶ üë®üèº‚Äçüéì üï¥üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Muito foi escrito sobre m√©todos de otimiza√ß√£o num√©rica. Isso √© compreens√≠vel, especialmente no contexto dos sucessos recentemente demonstrados por red...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Oh, este m√©todo de Newton</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469877/">  Muito foi escrito sobre m√©todos de otimiza√ß√£o num√©rica.  Isso √© compreens√≠vel, especialmente no contexto dos sucessos recentemente demonstrados por redes neurais profundas.  E √© muito gratificante que pelo menos alguns entusiastas estejam interessados ‚Äã‚Äãn√£o apenas em como bombardear sua rede neural nas estruturas que ganharam popularidade nessa Internet, mas tamb√©m em como e por que tudo funciona.  No entanto, recentemente, observei que, ao fazer perguntas relacionadas ao treinamento de redes neurais (e n√£o apenas com treinamento, e n√£o apenas redes), inclusive em Habr√©, cada vez mais frequentemente s√£o utilizadas diversas declara√ß√µes "conhecidas" para encaminhamento, cuja validade √© para dizer o m√≠nimo, duvidoso.  Entre essas declara√ß√µes duvidosas: <br><br><ol><li>  Os m√©todos da segunda e mais ordens n√£o funcionam bem nas tarefas de treinamento de redes neurais.  Porque isso. </li><li>  O m√©todo de Newton requer uma defini√ß√£o positiva da matriz Hessiana (segundas derivadas) e, portanto, n√£o funciona bem. <br></li><li>  O m√©todo de Levenberg-Marquardt √© um compromisso entre a descida do gradiente e o m√©todo de Newton e geralmente √© heur√≠stico. <br></li></ol><br>  etc.  Do que continuar esta lista, √© melhor come√ßar a trabalhar.  Neste post, consideraremos a segunda declara√ß√£o, j√° que eu o conheci pelo menos duas vezes em Habr√©.  Vou abordar a primeira quest√£o apenas na parte referente ao m√©todo de Newton, uma vez que √© muito mais extenso.  O terceiro e o restante ser√£o deixados at√© tempos melhores. <br><a name="habracut"></a><br>  O foco de nossa aten√ß√£o ser√° a tarefa de otimiza√ß√£o incondicional <img src="https://habrastorage.org/getpro/habr/post_images/823/e50/c93/823e50c935e4cc19a175f53c17ca79af.gif" title="&quot;f (x) \ rightarrow \ min&quot;">  onde <img src="https://habrastorage.org/getpro/habr/post_images/bc7/838/e10/bc7838e10e143195c0381efbf0671cce.gif" title="&quot;x = (x_ {1}, x_ {2}, \ pontos)&quot;">  - um ponto do espa√ßo vetorial, ou simplesmente - um vetor.  Naturalmente, essa tarefa √© mais f√°cil de resolver, mais sabemos sobre <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  .  √â geralmente assumido como diferenci√°vel em rela√ß√£o a cada argumento <img src="https://habrastorage.org/getpro/habr/post_images/6d8/d4e/07d/6d8d4e07d259325d5dd652e4b3b97af6.gif" title="&quot;x_ {k}&quot;">  e quantas vezes for necess√°rio para nossas a√ß√µes sujas.  √â sabido que uma condi√ß√£o necess√°ria para isso em um ponto <img src="https://habrastorage.org/getpro/habr/post_images/8ae/e32/d7e/8aee32d7e93fb189b268894bf91622b0.gif" title="&quot;x ^ {*}&quot;">  o m√≠nimo √© atingido, √© a igualdade do gradiente da fun√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/735/1d5/454/7351d54544ca7acc4b7a9bff7a2c2f6a.gif" title="&quot;\ bigtriangledown f (x ^ {*})&quot;">  neste ponto zero.  A partir daqui, obtemos instantaneamente o seguinte m√©todo de minimiza√ß√£o: <br><br>  Resolva a equa√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/d59/223/ac1/d59223ac159bfe660ea26a1d60f8f33f.gif" title="&quot;\ bigtriangledown f (x) = 0&quot;">  . <br><br>  A tarefa, para dizer o m√≠nimo, n√£o √© f√°cil.  Definitivamente n√£o √© mais f√°cil que o original.  No entanto, neste ponto, podemos notar imediatamente a conex√£o entre o problema de minimiza√ß√£o e o problema de resolver um sistema de equa√ß√µes n√£o lineares.  Essa conex√£o voltar√° para n√≥s quando considerarmos o m√©todo Levenberg-Marquardt (quando chegarmos a isso).  Enquanto isso, lembre-se (ou descubra) que um dos m√©todos mais usados ‚Äã‚Äãpara resolver sistemas de equa√ß√µes n√£o-lineares √© o m√©todo de Newton.  Consiste no fato de que para resolver a equa√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  n√≥s partimos de alguma aproxima√ß√£o inicial <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  construir uma sequ√™ncia <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a0/23d/4a2/3a023d4a27cdff86f8cf3bc78d5b3a21.gif" title="&quot;x_ {i + 1} = x_ {i} -H ^ {- 1} (x_ {i}) F (x_ {i})&quot;">  - m√©todo expl√≠cito de Newton <br><br>  ou <br><br><img src="https://habrastorage.org/getpro/habr/post_images/116/6fa/27b/1166fa27b4038fed75d435daaaab53fe.gif" title="&quot;\ begin {cases} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + p_ {i} \ end {cases}&quot;">  - m√©todo impl√≠cito de Newton <br><br>  onde <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  - matriz composta de derivadas parciais de uma fun√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Naturalmente, no caso geral, quando o sistema de equa√ß√µes n√£o lineares √© simplesmente dado a n√≥s em sensa√ß√µes, requer algo da matriz <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  n√≥s n√£o temos direito.  No caso em que a equa√ß√£o √© uma condi√ß√£o m√≠nima para alguma fun√ß√£o, podemos afirmar que a matriz <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  sim√©trico.  Mas n√£o mais. <br><br>  O m√©todo de Newton para resolver sistemas de equa√ß√µes n√£o lineares tem sido bastante estudado.  E aqui est√° a quest√£o: para sua converg√™ncia, a defini√ß√£o positiva da matriz n√£o √© necess√°ria <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  .  Sim, e n√£o pode ser necess√°rio - caso contr√°rio, ele teria sido in√∫til.  Em vez disso, existem outras condi√ß√µes que garantem a converg√™ncia local desse m√©todo e que n√£o consideraremos aqui, enviando as pessoas interessadas para a literatura especializada (ou no coment√°rio).  Entendemos que a afirma√ß√£o 2 √© falsa. <br><br>  Ent√£o <br><br>  Sim e n√£o  A emboscada aqui na palavra √© converg√™ncia local antes da palavra.  Isso significa que a aproxima√ß√£o inicial <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  deve estar "pr√≥ximo o suficiente" da solu√ß√£o; caso contr√°rio, a cada passo, estaremos cada vez mais afastados dela.  O que fazer?  N√£o entrarei em detalhes de como esse problema √© resolvido para sistemas de equa√ß√µes n√£o lineares de uma forma geral.  Em vez disso, retorne √† nossa tarefa de otimiza√ß√£o.  O primeiro erro da afirma√ß√£o 2 √© que, geralmente falando do m√©todo de Newton em problemas de otimiza√ß√£o, significa sua modifica√ß√£o - o m√©todo de Newton amortecido, no qual a sequ√™ncia de aproxima√ß√µes √© constru√≠da de acordo com a regra <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23d/ca8/404/23dca84042b77a1560b8cd2db607e8ae.gif" title="&quot;x_ {i + 1} = x_ {i} - \ alpha_ {i} H ^ {- 1} (x_ {i}) F (x_ {i})&quot;">  - M√©todo amortecido expl√≠cito de Newton <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9d4/67c/c96/9d467cc96266cf1179d3e553718f5bee.gif" title="&quot;\ begin {cases} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + \ alpha_ {i} p_ {i} \ final {cases} &quot;">  - M√©todo amortecido impl√≠cito de Newton <br><br>  Aqui est√° a sequ√™ncia <img src="https://habrastorage.org/getpro/habr/post_images/c70/738/fd1/c70738fd1eb4d9bfff34f20904f41bbf.gif" title="&quot;\ {\ alpha_ {i} \}&quot;">  √© um par√¢metro do m√©todo e sua constru√ß√£o √© uma tarefa separada.  Em problemas de minimiza√ß√£o, natural ao escolher <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  haver√° um requisito de que, a cada itera√ß√£o, o valor da fun√ß√£o f diminua, ou seja, <img src="https://habrastorage.org/getpro/habr/post_images/3ae/e45/ba0/3aee45ba0097ca8bdc8a23ef6a465f21.gif" title="&quot;f (x_ {i + 1}) &amp; lt; f (x_ {i})&quot;">  .  Surge uma quest√£o l√≥gica: existe tal (positivo) <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  ?  E se a resposta a esta pergunta for positiva, ent√£o <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  chamado de dire√ß√£o de descida.  Ent√£o a quest√£o pode ser colocada desta maneira: <br>  <i>Quando a dire√ß√£o gerada pelo m√©todo de Newton √© a dire√ß√£o da descida?</i> <br>  E para respond√™-lo, voc√™ ter√° que examinar o problema de minimiza√ß√£o de outro lado. <br><br><h2>  M√©todos de descida </h2><br>  Para o problema de minimiza√ß√£o, essa abordagem parece bastante natural: a partir de algum ponto arbitr√°rio, escolhemos a dire√ß√£o p de alguma forma e damos um passo nessa dire√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/fbf/01e/b21/fbf01eb21703831c5dd0e196a2efccc2.gif" title="&quot;\ alpha p&quot;">  .  Se <img src="https://habrastorage.org/getpro/habr/post_images/bd9/6f9/580/bd96f95806b05f65a5766db233a85653.gif" title="&quot;f (x + \ alpha p) &amp; lt; f (x)&quot;">  ent√£o pegue <img src="https://habrastorage.org/getpro/habr/post_images/b87/e59/538/b87e59538ed10c96ec3db2e7bad8dc85.gif" title="&quot;x + \ alpha p&quot;">  como um novo ponto de partida e repita o procedimento.  Se a dire√ß√£o for escolhida arbitrariamente, esse m√©todo √†s vezes √© chamado de m√©todo de passeio aleat√≥rio.  √â poss√≠vel tomar vetores de base unit√°ria como uma dire√ß√£o - ou seja, para dar um passo em apenas uma coordenada, esse m√©todo √© chamado de m√©todo de descida de coordenadas.  Escusado ser√° dizer que eles s√£o ineficazes?  Para que essa abordagem funcione bem, precisamos de algumas garantias adicionais.  Para isso, introduzimos uma fun√ß√£o auxiliar <img src="https://habrastorage.org/getpro/habr/post_images/8bf/1d5/4e1/8bf1d54e1f36dd4c9dfd5720437af51c.gif" title="&quot;g (p) = f (x + p)&quot;">  .  Eu acho √≥bvio que a minimiza√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  completamente equivalente a minimizar <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  .  Se <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  diferenci√°vel ent√£o <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  pode ser representado como <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/615/d31/e47615d310276ab67a9163889a2335a5.gif" title="&quot;g (p) = f (x) + \ bigtriangledown f ^ {T} (x) p + o (\ paralelo p \ paralelo ^ {2})&quot;"><br><br>  e se <img src="https://habrastorage.org/getpro/habr/post_images/2a7/342/acb/2a7342acbe0772f75af6eee281c247d0.gif" title="&quot;\ paralelo p \ paralelo&quot;">  pequeno o suficiente, ent√£o <img src="https://habrastorage.org/getpro/habr/post_images/2ef/8a9/23f/2ef8a923f49cf84264effb5f3f703c31.gif" title="&quot;g (p) \ aprox \ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p&quot;">  .  Agora podemos tentar substituir o problema de minimiza√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/076/563/484/076563484d4e576c5c48098bfa94d45c.gif" title="&quot;g (p)&quot;">  a tarefa de minimizar sua aproxima√ß√£o (ou <i>modelo</i> ) <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  .  A prop√≥sito, todos os m√©todos baseados no uso do modelo <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  chamado gradiente.  Mas o problema √©, <img src="https://habrastorage.org/getpro/habr/post_images/462/957/dda/462957dda265f4fb8be04327f1c12b0f.gif" title="&quot;\ bar {g}&quot;">  √â uma fun√ß√£o linear e, portanto, n√£o tem um m√≠nimo.  Para resolver esse problema, adicionamos uma restri√ß√£o √† dura√ß√£o da etapa que queremos executar.  Nesse caso, esse √© um requisito completamente natural - porque nosso modelo descreve mais ou menos corretamente a fun√ß√£o objetivo apenas em uma vizinhan√ßa suficientemente pequena.  Como resultado, obtemos um problema adicional de otimiza√ß√£o condicional: <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/8a2/c43/279/8a2c4327974067619cfad20b7ea1e821.gif" title="\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ paralelo p \ parallel_ {2} = \ Delta"></a> <br><br>  Esta tarefa tem uma solu√ß√£o √≥bvia: <img src="https://habrastorage.org/getpro/habr/post_images/3ff/1f6/a21/3ff1f6a2117e5d9a99603bcc8fde4f69.gif" title="&quot;p = - \ beta \ bigtriangledown f (x)&quot;">  onde <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  - fator que garante o cumprimento da restri√ß√£o.  As itera√ß√µes do m√©todo descent assumem a forma <br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/987/c25/966987c257a50df1855a50ea363350dd.gif" title="&quot;x_ {i + 1} = x_ {i} - \ beta \ bigtriangledown f (x_ {i})&quot;">  , <br><br>  em que aprendemos o conhecido <b>m√©todo de descida em gradiente</b> .  Par√¢metro <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , que normalmente √© chamado de velocidade de descida, agora adquiriu um significado compreens√≠vel, e seu valor √© determinado a partir da condi√ß√£o de que o novo ponto esteja na esfera de um determinado raio, circunscrito em torno do ponto antigo. <br><br>  Com base nas propriedades do modelo constru√≠do da fun√ß√£o objetivo, podemos argumentar que existe <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  , mesmo que muito pequeno, e se <img src="https://habrastorage.org/getpro/habr/post_images/84b/5fd/00f/84b5fd00fe1f4ca32b7cd7bd095a1490.gif" title="&quot;\ bar {g} (p) &amp; lt; \ bar {g} (0)&quot;">  ent√£o <img src="https://habrastorage.org/getpro/habr/post_images/553/80b/dc5/55380bdc5a434366df6d181078d6a8b7.gif" title="&quot;g (p) &amp; lt; g (0)&quot;">  .  Vale ressaltar que, neste caso, a dire√ß√£o em que iremos nos mover n√£o depende do tamanho do raio dessa esfera.  Em seguida, podemos escolher uma das seguintes maneiras: <br><br><ol><li>  Selecione de acordo com algum m√©todo o valor <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  . </li><li>  Defina a tarefa de escolher o valor apropriado <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , fornecendo uma diminui√ß√£o no valor da fun√ß√£o objetivo. </li></ol><br>  A primeira abordagem √© t√≠pica para os <i>m√©todos da regi√£o de confian√ßa</i> , a segunda leva √† formula√ß√£o do problema auxiliar dos chamados  <i>pesquisa linear (LineSearch)</i> .  Nesse caso em particular, as diferen√ßas entre essas abordagens s√£o pequenas e n√£o as consideraremos.  Em vez disso, preste aten√ß√£o ao seguinte: <br><br>  <b><i>por que, de fato, estamos procurando uma compensa√ß√£o</i></b> <b><i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i></b>  <b><i>deitado exatamente na esfera?</i></b> <br><br>  De fato, poder√≠amos substituir essa restri√ß√£o pelo requisito, por exemplo, de que p pertence √† superf√≠cie do cubo, ou seja, <img src="https://habrastorage.org/getpro/habr/post_images/cf1/a35/92e/cf1a3592ebe97c9e262a083ea44c594c.gif" title="&quot;\ paralelo p \ paralelo _ {\ infty} = \ Delta&quot;">  (neste caso, n√£o √© muito razo√°vel, mas por que n√£o) ou alguma superf√≠cie el√≠ptica?  Isso j√° parece bastante l√≥gico, se lembrarmos dos problemas que surgem ao minimizar as fun√ß√µes de barranco.  A ess√™ncia do problema √© que, ao longo de algumas linhas de coordenadas, a fun√ß√£o muda muito mais rapidamente do que em outras.  Por isso, obtemos que, se o incremento pertencer √† esfera, a quantidade <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  na qual a ‚Äúdescida‚Äù √© fornecida deve ser muito pequena.  E isso leva ao fato de que atingir um m√≠nimo exigir√° um n√∫mero muito grande de etapas.  Mas se, em vez disso, tomarmos uma elipse adequada como vizinhan√ßa, esse problema ser√° magicamente in√∫til. <br><br>  Pela condi√ß√£o de que os pontos da superf√≠cie el√≠ptica pertencem, pode ser escrito como <img src="https://habrastorage.org/getpro/habr/post_images/6ff/1b4/930/6ff1b49309ec84aa656d848764359b4e.gif" title="&quot;\ paralelo p \ paralelo_ {B} = \ sqrt {p ^ {T} Bp} = \ Delta&quot;">  onde <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  √â uma matriz definida positiva, tamb√©m chamada de m√©trica.  Norma <img src="https://habrastorage.org/getpro/habr/post_images/ab8/b42/711/ab8b42711a932f9129bdb193b6a74360.gif" title="&quot;\ paralelo \ cdot \ paralelo_ {B}&quot;">  chamada norma el√≠ptica induzida pela matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Que tipo de matriz √© essa e de onde obt√™-la - consideraremos mais adiante e agora chegamos a uma nova tarefa. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/f4c/cc0/757/f4ccc0757e09fb304ff10a9a8c4751b6.gif" title="\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ dfrac {1} {2} \ paralelo p \ paralelo_ {B} ^ {2} = \ Delta"></a> <br><br>  O quadrado da norma e o fator 1/2 est√£o aqui apenas por conveni√™ncia, para n√£o mexer com as ra√≠zes.  Aplicando o m√©todo multiplicador de Lagrange, obtemos o problema vinculado da otimiza√ß√£o incondicional <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/605/36d/d5e/60536dd5e2297580940a5b926760a3ce.gif" title="f (x) + \ bigtriangledown f ^ {T} (x) p + \ dfrac {\ lambda} {2} p ^ {T} Bp- \ lambda \ Delta \ rightarrow \ min"></a> <br><br>  Uma condi√ß√£o necess√°ria para um m√≠nimo, pois √© <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/4c5/f7e/921/4c5f7e921637c73fcb992c5d9b9efcd6.gif" title="\ bigtriangledown f (x) + \ lambda Bp = 0"></a>  ou <img src="https://habrastorage.org/getpro/habr/post_images/a80/6c9/ff7/a806c9ff7ce22ea27c87b6a61a4c8fed.gif" title="&quot;B \ esquerda (\ lambda p \ direita) = - \ bigtriangledown f (x)&quot;">  de onde <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/b50/032/3a9/b500323a970c7ae821295450627bdad2.gif" title="p = - \ dfrac {1} {\ lambda} B ^ {- 1} \ bigtriangledown f (x) = \ dfrac {1} {\ lambda} \ bar {p}"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/697/906/5d7/6979065d729033e0093ffad8475e80a6.gif" title="\\ dfrac {1} {\ lambda ^ {2}} \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) ^ {T} B \ left (B ^ {- 1} \ bigtriangledown f (x) \ right) = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} BB ^ {- 1} \ bigtriangledown f (x) = \ \ = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x) = \ Delta"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/41f/689/d89/41f689d890b92c84f05fdd0ede8a8114.gif" title="\ lambda = \ sqrt {\ dfrac {1} {\ Delta} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x)}> 0"></a> <br><br>  Mais uma vez, vemos que a dire√ß√£o <img src="https://habrastorage.org/getpro/habr/post_images/45b/686/bb0/45b686bb0219a74b212cfdeaf1998653.gif" title="&quot;\ bar {p} = - B ^ {- 1} \ bigtriangledown f (x)&quot;">  , na qual iremos nos mover, n√£o depende do valor <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  - apenas da matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  E, novamente, podemos pegar <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  que est√° cheio de necessidade de calcular <img src="https://habrastorage.org/getpro/habr/post_images/99d/394/e7d/99d394e7d0b74248114405067e0ffd51.gif" title="&quot;\ lambda&quot;">  invers√£o de matriz expl√≠cita <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  ou resolva o problema auxiliar de encontrar um vi√©s adequado <img src="https://habrastorage.org/getpro/habr/post_images/b7b/6a7/371/b7b6a73716dc8f4e40a52c1c5ef0e6b4.gif" title="&quot;x_ {i + 1} = x_ {i} + \ beta \ bar {p} _ {i}&quot;">  .  Desde <img src="https://habrastorage.org/getpro/habr/post_images/0b8/52f/d1b/0b852fd1bbc20f2966bf757a56186312.gif" title="&quot;\ lambda &amp; gt; 0&quot;">  , a solu√ß√£o para esse problema auxiliar est√° garantida. <br><br>  Ent√£o, o que deveria ser para a matriz B?  N√≥s nos restringimos a id√©ias especulativas.  Se a fun√ß√£o objetivo <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  - quadr√°tico, ou seja, tem a forma <img src="https://habrastorage.org/getpro/habr/post_images/974/f7f/cf6/974f7fcf6345b91ef8466f2cabba6efe.gif" title="&quot;f (x) = a + b ^ {T} x + x ^ {T} Hx&quot;">  onde <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  positivo, √© √≥bvio que o melhor candidato para o papel da matriz <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  √© hessiano <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  , pois nesse caso √© necess√°ria uma itera√ß√£o do m√©todo de descida que constru√≠mos.  Se H n√£o for positivo definido, ent√£o n√£o poder√° ser uma m√©trica, e as itera√ß√µes constru√≠das com ela s√£o itera√ß√µes do m√©todo de Newton amortecido, mas n√£o s√£o itera√ß√µes do m√©todo de descida.  Finalmente, podemos dar uma resposta rigorosa a <br><br>  <b>Pergunta:</b> <i>A matriz hessiana no m√©todo de Newton precisa ser definida positivamente?</i> <br>  <b>Resposta:</b> <i>n√£o, n√£o √© necess√°rio no m√©todo de Newton padr√£o ou amortecido.</i>  <i>Mas se essa condi√ß√£o for atendida, o m√©todo de Newton amortecido √© um m√©todo de descida e tem a propriedade de converg√™ncia <i>global</i> , e n√£o apenas local.</i> <br><br>  Como ilustra√ß√£o, vamos ver como as regi√µes de confian√ßa ficam ao minimizar a conhecida fun√ß√£o Rosenbrock usando a descida de gradiente e os m√©todos de Newton, e como a forma das regi√µes afeta a converg√™ncia do processo. <br><br><img src="https://habrastorage.org/webt/_x/30/nx/_x30nxs-eyrixuan0-diyvwixww.gif" width="600"><br><br>  √â assim que o m√©todo de descida se comporta com uma regi√£o de confian√ßa esf√©rica; tamb√©m √© uma descida de gradiente.  Tudo √© como um livro - estamos presos em um canyon. <br><br><img src="https://habrastorage.org/webt/9x/ik/td/9xiktd4lapdka-uk010evfvlcdm.gif" width="600"><br><br>  E isso √© obtido se a regi√£o de confian√ßa tiver o formato de uma elipse definida pela matriz de Hessian.  Isso nada mais √© do que uma itera√ß√£o do m√©todo de Newton amortecido. <br><br>  Apenas a quest√£o do que fazer se a matriz hessiana n√£o for positiva definida permaneceu sem solu√ß√£o.  Existem muitas op√ß√µes.  O primeiro √© marcar.  Talvez voc√™ tenha sorte e as itera√ß√µes de Newton convergir√£o sem essa propriedade.  Isso √© bastante real, especialmente nos est√°gios finais do processo de minimiza√ß√£o, quando voc√™ j√° est√° perto o suficiente de uma solu√ß√£o.  Nesse caso, itera√ß√µes do m√©todo padr√£o de Newton podem ser usadas sem se preocupar com a busca de uma vizinhan√ßa admiss√≠vel para descida.  Ou use itera√ß√µes do m√©todo Newton amortecido no caso de <img src="https://habrastorage.org/getpro/habr/post_images/6da/2c0/bc5/6da2c0bc54434a64d7630c142d0c7bf9.gif" title="&quot;\ beta = 0&quot;">  , ou seja, no caso em que a dire√ß√£o obtida n√£o √© a dire√ß√£o da descida, altere-a, digamos, para um anti-gradiente.  <i>S√≥ n√£o √© necess√°rio verificar explicitamente se o Hessian √© definido positivamente de acordo com o crit√©rio de Sylvester</i> , como foi feito <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui !!!</a>  .  √â um desperd√≠cio e in√∫til. <br>  M√©todos mais sutis envolvem a constru√ß√£o de uma matriz, em certo sentido pr√≥xima √† matriz hessiana, mas possuindo a propriedade de definitividade positiva, em particular, corrigindo valores pr√≥prios.  Um t√≥pico separado s√£o os m√©todos quasi-newtonianos, ou m√©todos m√©tricos vari√°veis, que garantem a defini√ß√£o positiva da matriz B e n√£o requerem o c√°lculo das segundas derivadas.  Em geral, uma discuss√£o detalhada desses problemas vai muito al√©m do escopo deste artigo. <br><br>  Sim, e a prop√≥sito, segue-se do que foi dito que <i>o m√©todo amortecido de Newton com defini√ß√£o positiva do Hessiano √© um m√©todo gradiente</i> .  Bem como m√©todos quase-newtonianos.  E muitos outros, com base em uma escolha separada de dire√ß√£o e tamanho do passo.  Portanto, contrastar o m√©todo de Newton com a terminologia de gradiente est√° incorreto. <br><br><h2>  Resumir </h2><br>  O m√©todo de Newton, que √© frequentemente lembrado ao discutir m√©todos de minimiza√ß√£o, geralmente n√£o √© o m√©todo de Newton em seu sentido cl√°ssico, mas o m√©todo de descida com a m√©trica especificada pelo hessiano da fun√ß√£o objetivo.  E sim, converge globalmente se o hessiano estiver em toda parte positivo.  Isso √© poss√≠vel apenas para fun√ß√µes convexas, que s√£o muito menos comuns na pr√°tica do que gostar√≠amos; portanto, no caso geral, sem as modifica√ß√µes apropriadas, a aplica√ß√£o do m√©todo Newton (n√£o vamos nos afastar do coletivo e continuar chamando assim) n√£o garante o resultado correto.  O aprendizado de redes neurais, mesmo as rasas, geralmente leva a problemas de otimiza√ß√£o n√£o convexos com muitos m√≠nimos locais.  E aqui est√° uma nova emboscada.  O m√©todo de Newton geralmente converge (se converge) rapidamente.  Quero dizer muito r√°pido.  E isso, curiosamente, √© ruim, porque chegamos ao m√≠nimo local em v√°rias itera√ß√µes.  E para fun√ß√µes com terrenos complexos pode ser muito pior que o global.  A descida de gradiente com pesquisa linear converge muito mais lentamente, mas √© mais prov√°vel que ‚Äúpule‚Äù as cristas da fun√ß√£o objetivo, o que √© muito importante nos est√°gios iniciais da minimiza√ß√£o.  Se voc√™ j√° reduziu muito o valor da fun√ß√£o objetivo, e a converg√™ncia da descida do gradiente diminuiu significativamente, uma mudan√ßa na m√©trica pode acelerar o processo, mas isso √© para os est√°gios finais. <br><br>  Certamente, esse argumento n√£o √© universal, n√£o √© indiscut√≠vel e, em alguns casos, nem √© incorreto.  Assim como a afirma√ß√£o de que os m√©todos de gradiente funcionam melhor nos problemas de aprendizagem. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt469877/">https://habr.com/ru/post/pt469877/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt469861/index.html">Estruturas de dados para programadores de jogos: dados em massa</a></li>
<li><a href="../pt469865/index.html">700 funcion√°rios e v√°rios continentes: como a Alconost construiu um modelo de neg√≥cios n√£o oficial</a></li>
<li><a href="../pt469869/index.html">Por que voc√™ deve fazer overclock de RAM (√© f√°cil!)</a></li>
<li><a href="../pt469871/index.html">Quando os teclados eram mesas</a></li>
<li><a href="../pt469875/index.html">Como proteger suas senhas em 2019</a></li>
<li><a href="../pt469879/index.html">VPN dupla em um clique. Como dividir facilmente o endere√ßo IP de um ponto de entrada e sa√≠da</a></li>
<li><a href="../pt469881/index.html">Os tr√™s primeiros dias de vida de um post sobre Habr√©</a></li>
<li><a href="../pt469885/index.html">Desative o console local ao usar x11vnc</a></li>
<li><a href="../pt469889/index.html">SamsPcbGuide, parte 12: Tecnologia - gabinetes do tipo BGA, pl√°stico e espa√ßo II</a></li>
<li><a href="../pt469893/index.html">Casa com elementos de alta tecnologia para um gato de rua</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>