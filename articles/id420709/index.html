<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏻‍💻 👲🏼 📚 T2F: proyek untuk mengubah teks menjadi gambar wajah dengan pembelajaran yang mendalam 👩🏽‍🔬 👩‍👩‍👦‍👦 👽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kode proyek tersedia di repositori. 

 Pendahuluan 
 Ketika saya membaca deskripsi penampilan karakter dalam buku, saya selalu tertarik pada bagaimana...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>T2F: proyek untuk mengubah teks menjadi gambar wajah dengan pembelajaran yang mendalam</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/420709/"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/703/0df/5ae7030df8270466b01b81aad0ace49f.jpg"><br><br>  <i>Kode proyek tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori.</a></i> <br><br><h2>  Pendahuluan </h2><br>  Ketika saya membaca deskripsi penampilan karakter dalam buku, saya selalu tertarik pada bagaimana mereka terlihat dalam kehidupan.  Sangat mungkin untuk membayangkan seseorang secara keseluruhan, tetapi deskripsi detail yang paling jelas adalah tugas yang sulit, dan hasilnya bervariasi dari orang ke orang.  Sering kali saya tidak bisa membayangkan apa pun kecuali wajah karakter yang sangat buram sampai akhir pekerjaan.  Hanya ketika buku itu diubah menjadi film, wajah buram itu penuh dengan detail.  Misalnya, saya tidak pernah bisa membayangkan bagaimana wajah Rahel terlihat dari buku " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Girl on the Train</a> ".  Tetapi ketika film itu keluar, saya bisa mencocokkan wajah Emily Blunt dengan karakter Rachel.  Tentu saja, orang yang terlibat dalam pemilihan aktor membutuhkan banyak waktu untuk menggambarkan karakter dalam naskah dengan benar. <br><a name="habracut"></a><br>  Masalah ini menginspirasi dan memotivasi saya untuk menemukan solusi.  Setelah itu, saya mulai mempelajari literatur tentang pembelajaran mendalam untuk mencari sesuatu yang serupa.  Untungnya, ada beberapa studi tentang sintesis gambar dari teks.  Berikut adalah beberapa yang saya bangun di atas: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv.org/abs/1605.05396</a> "Teks Adversarial Generatif ke Sintesis Gambar" </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv.org/abs/1612.03242</a> "StackGAN: Teks ke Foto-realistis Sintesis Gambar dengan Stacked Generative Adversarial Networks" </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv.org/abs/1710.10916</a> “StackGAN ++: Sintesis Gambar Realistis dengan Stacked Generative Adversarial Networks” </li></ul><br>  [ <i>proyek menggunakan jaringan permusuhan generatif, GSS (Jaringan permusuhan generatif, GAN) / kira-kira.</i>  <i>perev.</i>  ] <br><br>  Setelah mempelajari literatur, saya memilih arsitektur yang disederhanakan dibandingkan dengan StackGAN ++, dan mengatasi masalah saya dengan cukup baik.  Di bagian berikut, saya akan menjelaskan bagaimana saya memecahkan masalah ini dan membagikan hasil awal.  Saya juga akan menjelaskan beberapa detail pemrograman dan pelatihan yang saya habiskan banyak waktu. <br><br><h2>  Analisis data </h2><br>  Tidak diragukan lagi, aspek terpenting dari pekerjaan ini adalah data yang digunakan untuk melatih model.  Seperti yang dikatakan Profesor Andrew Eun dalam kursus deeplearning.ai-nya: "Di bidang pembelajaran mesin, bukan orang yang memiliki algoritma terbaik, tetapi orang yang memiliki data terbaik."  Maka mulailah pencarian saya untuk dataset pada wajah dengan deskripsi tekstual yang baik, kaya dan beragam.  Saya menemukan set data yang berbeda - baik itu hanya wajah, atau wajah dengan nama, atau wajah dengan deskripsi warna mata dan bentuk wajah.  Tetapi tidak ada yang saya butuhkan.  Pilihan terakhir saya adalah menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">proyek awal</a> - menghasilkan deskripsi data struktural dalam bahasa alami.  Tetapi opsi seperti itu akan menambah noise tambahan ke set data yang sudah sangat bising. <br><br>  Waktu berlalu, dan pada beberapa titik proyek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Face2Text</a> baru <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">muncul</a> .  Itu adalah kumpulan database deskripsi teks rinci tentang orang.  Saya berterima kasih kepada penulis proyek untuk set data yang disediakan. <br><br>  Set data berisi deskripsi tekstual dari 400 gambar yang dipilih secara acak dari database LFW (wajah yang ditandai).  Deskripsi dibersihkan untuk menghilangkan karakteristik ambigu dan minor.  Beberapa deskripsi tidak hanya berisi informasi tentang wajah-wajah, tetapi juga beberapa kesimpulan yang dibuat berdasarkan gambar - misalnya, "orang dalam foto mungkin adalah penjahat".  Semua faktor ini, serta ukuran kecil dari kumpulan data, telah mengarah pada fakta bahwa proyek saya sejauh ini hanya menunjukkan bukti pengoperasian arsitektur.  Selanjutnya, model ini dapat ditingkatkan ke set data yang lebih besar dan lebih beragam. <br><br><h2>  Arsitektur </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c70/ea1/6de/c70ea16de2bbfb674e618fa556cfc9ff.jpg"><br><br>  Arsitektur proyek T2F menggabungkan dua arsitektur stackGAN untuk pengkodean teks yang disempurnakan bersyarat, dan ProGAN ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pertumbuhan GSS progresif</a> ) untuk mensintesis gambar wajah.  Arsitektur stackgan ++ asli menggunakan beberapa GSS dengan resolusi spasial yang berbeda, dan saya memutuskan bahwa ini adalah pendekatan yang terlalu serius untuk tugas distribusi korespondensi.  Tetapi ProGAN hanya menggunakan satu GSS, semakin terlatih pada resolusi yang lebih detail.  Saya memutuskan untuk menggabungkan kedua pendekatan ini. <br><br>  Ada penjelasan tentang aliran data melalui: deskripsi teks dikodekan ke dalam vektor akhir dengan menanamkan ke jaringan LSTM (Embedding) (psy_t) (lihat diagram).  Kemudian, embedding ditransmisikan melalui blok Augmentasi Pengkondisian (satu lapisan linear) untuk mendapatkan bagian teks dari vektor eigen (menggunakan teknik reparameterisasi VAE) untuk GSS sebagai input.  Bagian kedua vektor eigen adalah noise Gaussian acak.  Vektor eigen yang dihasilkan diumpankan ke generator GSS, dan embedding diumpankan ke lapisan diskriminator terakhir untuk distribusi korespondensi bersyarat.  Pelatihan proses GSS berjalan persis sama seperti pada artikel tentang ProGAN - berlapis, dengan peningkatan resolusi spasial.  Lapisan baru diperkenalkan menggunakan teknik fade-in untuk menghindari penghapusan hasil pembelajaran sebelumnya. <br><br><h2>  Implementasi dan detail lainnya </h2><br>  Aplikasi ini ditulis dalam python menggunakan kerangka PyTorch.  Saya dulu bekerja dengan paket tensorflow dan keras, tetapi sekarang saya ingin mencoba PyTorch.  Saya suka menggunakan debugger python bawaan untuk bekerja dengan arsitektur jaringan - semua berkat strategi eksekusi awal.  Tensorflow juga baru-baru ini mengaktifkan mode eksekusi bersemangat.  Namun, saya tidak ingin menilai kerangka mana yang lebih baik, saya hanya ingin menekankan bahwa kode untuk proyek ini ditulis menggunakan PyTorch. <br><br>  Cukup banyak bagian dari proyek yang menurut saya dapat digunakan kembali, terutama ProGAN.  Oleh karena itu, saya menulis kode terpisah untuk mereka sebagai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perpanjangan dari</a> modul PyTorch, dan itu dapat digunakan pada set data lain juga.  Hanya perlu menunjukkan kedalaman dan ukuran fitur GSS.  GSS dapat dilatih secara progresif untuk kumpulan data apa pun. <br><br><h2>  Detail Pelatihan </h2><br>  Saya melatih beberapa versi jaringan menggunakan hiperparameter yang berbeda.  Rincian pekerjaan adalah sebagai berikut: <br><br><ol><li>  Diskriminator tidak memiliki operasi batch-norma atau lapisan-norma, sehingga hilangnya WGAN-GP dapat tumbuh eksplosif.  Saya menggunakan penalti melayang dengan lambda sama dengan 0,001. </li><li>  Untuk mengontrol keragaman Anda sendiri, yang diperoleh dari teks yang disandikan, perlu menggunakan jarak Kullback - Leibler dalam kerugian Generator. </li><li>  Untuk membuat gambar yang dihasilkan lebih cocok dengan distribusi teks yang masuk, lebih baik menggunakan versi WGAN dari diskriminator (Matching-Aware) yang sesuai. </li><li>  Waktu fade-in untuk level atas harus melebihi waktu fade-in untuk level atas.  Saya menggunakan 85% sebagai nilai fade-in saat pelatihan. </li><li>  Saya menemukan bahwa contoh resolusi yang lebih tinggi (32 x 32 dan 64 x 64) menghasilkan lebih banyak kebisingan latar belakang daripada contoh resolusi yang lebih rendah.  Saya pikir ini karena kurangnya data. </li><li>  Selama latihan progresif, lebih baik menghabiskan lebih banyak waktu untuk resolusi yang lebih rendah, dan mengurangi waktu yang dihabiskan untuk bekerja dengan resolusi yang lebih tinggi. </li></ol><br>  Video menunjukkan selang waktu Generator.  Video ini dikompilasi dari gambar dengan resolusi spasial berbeda yang diperoleh selama pelatihan GSS. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO_l87rPDb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Kesimpulan </h2><br>  Menurut hasil awal, dapat dinilai bahwa proyek T2F dapat dikerjakan dan memiliki aplikasi yang menarik.  Misalkan dapat digunakan untuk menyusun photobots.  Atau untuk kasus-kasus ketika perlu untuk meningkatkan imajinasi.  Saya akan terus bekerja pada penskalaan proyek ini pada set data seperti Flicker8K, keterangan Coco, dan sebagainya. <br><br>  Pertumbuhan GSS progresif adalah teknologi fenomenal untuk pelatihan GSS yang lebih cepat dan lebih stabil.  Dapat dikombinasikan dengan berbagai teknologi modern yang disebutkan dalam artikel lain.  GSS dapat digunakan di berbagai bidang MO. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id420709/">https://habr.com/ru/post/id420709/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id420697/index.html">Tema abadi dengan PHP dan MySQL</a></li>
<li><a href="../id420701/index.html">HSBI mengundang ke malam kuliah tentang desain game pada 29 Agustus</a></li>
<li><a href="../id420703/index.html">Sinopsis buku “Negosiasi tanpa kekalahan. Metode Harvard</a></li>
<li><a href="../id420705/index.html">8 ide mendalam dari Tribors of Mentor Tim Ferris</a></li>
<li><a href="../id420707/index.html">Startup JITX menggunakan AI untuk mengotomatisasi pengembangan papan sirkuit tercetak yang kompleks</a></li>
<li><a href="../id420711/index.html">Jurusan Ilmu Data Moskow: pengumuman dan pendaftaran</a></li>
<li><a href="../id420713/index.html">Bagaimana Chuck Hull menciptakan pencetakan 3D</a></li>
<li><a href="../id420715/index.html">Kebenaran sulit tentang beratnya belajar</a></li>
<li><a href="../id420725/index.html">Bagaimana saya mengajar AI bermain Tetris untuk NES. Bagian 1: analisis kode permainan</a></li>
<li><a href="../id420729/index.html">Buka webinar "Naive Bayes Classifier"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>