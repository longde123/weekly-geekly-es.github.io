<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî∫ ‚ÜóÔ∏è üèà Teor√≠a general y arqueolog√≠a de virtualizaci√≥n x86 üñï ‚ùáÔ∏è üëâüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduccion 
 Equipo de autores 
 Publicado por Anton Zhbankov ( AntonVirtual , cloudarchitect.cc ) 
 Coautores: Grigory Pryalukhin , Evgeny Parfenov...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teor√≠a general y arqueolog√≠a de virtualizaci√≥n x86</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/474776/"><h2>  Introduccion </h2><br><h4>  Equipo de autores </h4><br>  Publicado por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Anton Zhbankov</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">AntonVirtual</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cloudarchitect.cc</a> ) <br>  Coautores: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Grigory Pryalukhin</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Evgeny Parfenov</a> <br><br><h3>  Conceptos generales de virtualizaci√≥n </h3><br>  Tuve que ver muchas interpretaciones de lo que <i>es la virtualizaci√≥n</i> y escuchar mucha controversia, no un poco m√°s cerca de discutir el resultado pr√°ctico.  Y como saben, el argumento de dos personas inteligentes se reduce a un debate sobre las definiciones.  Definamos qu√© es la virtualizaci√≥n y qu√© proviene de ella. <br><br>  Probablemente la definici√≥n m√°s cercana de virtualizaci√≥n ser√° "abstraer" de la programaci√≥n orientada a objetos.  O, si se traduce al ruso normal, esto oculta la implementaci√≥n detr√°s de una interfaz abstracta.  Lo cual, por supuesto, explicaba todo a la vez.  Intentemos nuevamente, pero para aquellos que no han estudiado programaci√≥n. <br><blockquote>  Virtualizaci√≥n: esconde una implementaci√≥n espec√≠fica detr√°s de un m√©todo estandarizado universal de acceso a recursos / datos. </blockquote><br>  Si intenta poner en pr√°ctica esta definici√≥n, resulta que funciona en temas completamente inesperados.  Digamos el reloj.  Entonces, se invent√≥ un reloj de sol hace varios miles de a√±os, y en la Edad Media se invent√≥ uno mec√°nico.  ¬øQu√© hay en com√∫n?  El sol y algunos engranajes?  Alg√∫n tipo de tonter√≠a.  Y luego osciladores de cuarzo y todo lo dem√°s. <br>  La conclusi√≥n es que tenemos una interfaz est√°ndar: un puntero o puntero digital, que en una forma est√°ndar universal indica la hora actual.  ¬øPero nos importa cu√°n espec√≠ficamente se implemente este mecanismo dentro de la caja, si el tiempo se indica con suficiente precisi√≥n para nosotros? <br>  ‚ÄúPerm√≠tanme‚Äù, pueden decir, ‚Äú¬°pero pens√© que la virtualizaci√≥n se trataba de m√°quinas, procesadores, etc.! <br>  S√≠, se trata de autom√≥viles y procesadores, pero este es solo un caso especial.  Miremos m√°s ampliamente, ya que el art√≠culo afirma audazmente una teor√≠a general. <br><a name="habracut"></a><br><h2>  POZOR! </h2><br><h3>  Uwaga!  Achtung!  Pozor! </h3><br>  Este art√≠culo tiene un objetivo <b>educativo general</b> para vincular un conjunto completo de tecnolog√≠as y palabras de miedo junto con la historia en una determinada estructura, y debido a esta circunstancia contiene una cantidad significativa de simplificaciones <b>intencionales</b> .  Por supuesto, tambi√©n contiene una gran cantidad de omisiones molestas e incluso errores cursis con errores tipogr√°ficos.  La cr√≠tica constructiva es bienvenida, especialmente en la forma de "D√©jame traerte esta parte a la mente". <br><br><h2>  Tipos de virtualizaci√≥n </h2><br>  Regresemos de conceptos completamente abstractos a los m√°s familiares para nuestras queridas computadoras. <br><br><h3>  Virtualizaci√≥n de almacenamiento </h3><br>  El primero, probablemente, es el tipo de virtualizaci√≥n que encuentra un geek novato: la virtualizaci√≥n de un sistema de almacenamiento de datos.  En este caso, el sistema de almacenamiento no se utiliza en el sentido de una gran matriz con discos conectados a trav√©s de un canal de fibra, sino como un subsistema l√≥gico responsable del almacenamiento de datos a largo plazo. <br><br><h4>  FS -&gt; LBA -&gt; CHS </h4><br>  Tomemos el caso m√°s simple de un sistema de almacenamiento en un solo disco magn√©tico duro.  El formato habitual para trabajar con datos son los archivos que est√°n en la unidad l√≥gica.  El archivo se puede abrir, leer, cerrar.  Pero un objeto como un archivo simplemente no existe f√≠sicamente: solo hay una manera de acceder a ciertos bloques de datos utilizando el m√©todo de direccionamiento del formulario "unidad: \ carpeta1 \ carpeta2 \ archivo".  Es decir  Nos encontramos con la primera capa de virtualizaci√≥n: desde lo mnemot√©cnico y comprensible hasta los humanos, traducimos todo en direcciones entendibles por el sistema.  En las tablas de metadatos, el controlador del sistema de archivos busca qu√© tipo de bloques de datos hay y obtenemos la direcci√≥n en el sistema de direccionamiento de bloques l√≥gicos (LBA).  En el sistema LBA, los bloques tienen un tama√±o fijo y se siguen linealmente, es decir,  de alguna manera puede tener que ver con el almacenamiento de datos en cinta magn√©tica, ¬°pero el disco duro es de alguna manera completamente diferente!  Y aqu√≠ vamos a la segunda capa de virtualizaci√≥n: la traducci√≥n del direccionamiento LBA a CHS (cilindro / cabezal / sector). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e95/940/bd4/e95940bd4389a0187c2bf3d82406e118.png" alt="imagen"><br><br>  CHS, a su vez, ya en el controlador del disco duro comienza a traducirse en par√°metros f√≠sicos para la lectura, pero esta es una historia completamente diferente. <br>  Incluso en un simple acceso al archivo para, por ejemplo, ver una vidosik con memasics, nos encontramos con tres capas de virtualizaci√≥n de inmediato. <br>  Todo ser√≠a demasiado simple si las capas no comenzaran a superponerse en orden aleatorio y en una variedad de formas. <br><br><h4>  RAID </h4><br>  La siguiente capa de virtualizaci√≥n, que muchas personas err√≥neamente no consideran virtualizaci√≥n, es RAID (matriz redundante de discos econ√≥micos / independientes). <br><br>  La caracter√≠stica clave de RAID en el contexto de los conceptos discutidos no es su capacidad para proteger los datos de la falla de un disco f√≠sico en particular.  RAID proporciona un segundo nivel de direccionamiento LBA adem√°s de varias (a veces much√≠simas) direcciones LBA independientes.  Como podemos acceder al RAID, independientemente del nivel de RAID, exactamente de la misma manera que un solo disco sin RAID, podemos decir con confianza: <blockquote>  RAID es virtualizaci√≥n de disco. </blockquote><br>  Adem√°s, el controlador RAID no solo crea un gran disco virtual a partir de varios discos f√≠sicos, sino que puede crear un n√∫mero arbitrario de ellos al agregar otra capa de virtualizaci√≥n. <br><br><h3>  Ver virtualizaci√≥n </h3><br>  El siguiente tipo de virtualizaci√≥n, que muchos de nosotros usamos casi todos los d√≠as, pero no lo consideramos virtualizaci√≥n, es una conexi√≥n remota al escritorio. <br><br>  Los servidores de terminal, VDI e incluso solo RDP a trav√©s de VPN para el servidor son todos virtualizaci√≥n de sesi√≥n.  Usando una interfaz est√°ndar (monitor, teclado, mouse) trabajamos con una m√°quina real o con un dise√±o incomprensible desde un escritorio virtual en un clon vinculado con una aplicaci√≥n en contenedor, desde la cual transferimos datos a trav√©s de un b√∫fer a una aplicaci√≥n con entrega de transmisi√≥n.  O no, ¬øqui√©n lo resolver√°, adem√°s del que lo dise√±√≥? <br><br><h2>  Introducci√≥n a la virtualizaci√≥n x86 </h2><br><h3>  Historia y resumen de procesadores </h3><br><h4>  Ejecuci√≥n del programa </h4><br>  En la primera lecci√≥n de un curso especial de programaci√≥n, Vladimir Denisovich Lelyukh (descanse en paz para √©l) les dijo a los estudiantes: la computadora, a pesar de su nombre, no puede contar, puede pretender que puede contar.  Pero si algo parece un pato, camina como un pato y grazna como un pato, desde un punto de vista pr√°ctico, es un pato. <br><br>  Intentemos recordar esto para un uso pr√°ctico adicional. <br><br>  La computadora, y espec√≠ficamente el procesador, en realidad no hace nada: solo espera algunos par√°metros de entrada en ciertos lugares, y luego, a trav√©s de la terrible magia negra, da algunos resultados en ciertos lugares. <br><br>  Un programa en este caso es un cierto flujo de comandos ejecutados estrictamente secuencialmente, como resultado de lo cual esperamos ver un cierto resultado. <br>  Pero si el programa se est√° ejecutando, ¬øc√≥mo se pueden ingresar los datos?  ¬øY en general, de alguna manera interactuar en una computadora? <br><br>  Para esto, se inventaron las interrupciones de hardware.  El usuario presiona una tecla: el controlador del teclado lo se√±ala y hay una interrupci√≥n en la ejecuci√≥n del hilo de c√≥digo actual.  Las direcciones de los manejadores de interrupciones se registran en un √°rea de memoria espec√≠fica, y despu√©s de guardar el estado actual, el control se transfiere al manejador de interrupciones.  A su vez, el controlador debe, en teor√≠a, procesar r√°pidamente todo, luego √©l y el controlador, escribir la tecla presionada en el b√∫fer deseado y devolver el control.  Por lo tanto, la aplicaci√≥n parece estar ejecut√°ndose y podemos interactuar con el sistema. <br><br>  Los controladores de interrupciones (y el tipo principal de controladores son controladores de dispositivos) tienen la oportunidad de ingresar a un modo de procesador especial, cuando no se pueden implementar otras interrupciones antes de salir de este modo.  Lo que al final a menudo condujo a un problema de bloqueo: un error en el controlador no permiti√≥ salir de la interrupci√≥n. <br><br><h4>  Multitarea </h4><br>  ¬øQu√© hacer en una situaci√≥n si es necesario ejecutar varios programas (secuencias de c√≥digo con sus estructuras de datos y memoria) al mismo tiempo?  Obviamente, si hay m√°s flujos de c√≥digo que dispositivos capaces de ejecutarlos, entonces esto es un problema. <br><br>  La pseudo-multitarea aparece cuando se ejecuta una tarea al cambiar directamente a ella. <br><br>  En el futuro, aparece una cooperativa (multitarea no preventiva): la tarea ejecutable en s√≠ misma comprende que ya no necesita recursos del procesador y le da el control a otra persona.  Pero todo esto no es suficiente. <br><br>  Y aqu√≠ nuevamente, las interrupciones + la capacidad de fingir nos rescatan.  Realmente no le importa al usuario que se ejecuten estrictamente simult√°neamente, es suficiente para verse as√≠. <br>  Por lo tanto, simplemente se cuelga un controlador para interrumpir el temporizador, que comienza a controlar qu√© secuencia de c√≥digo se debe ejecutar a continuaci√≥n.  Si el temporizador se activa con bastante frecuencia (digamos 15 ms), para el usuario todo parece una operaci√≥n paralela.  Y entonces hay una multitarea moderna desplazada. <br><br><h4>  Modo real </h4><br>  El modo de procesador real en este art√≠culo se puede describir de manera bastante simple: toda la memoria est√° disponible para todos.  Cualquier aplicaci√≥n, incluido el malware (malware, software malicioso), puede acceder a cualquier lugar, tanto para leer como para escribir. <br><br>  Este es el modo de operaci√≥n inicial de la familia de procesadores Intel x86. <br><br><h4>  Modo protegido </h4><br>  En 1982, apareci√≥ una innovaci√≥n en el procesador Intel 80286 (en adelante, simplemente 286), un modo de operaci√≥n protegido, que trajo consigo innovaciones en la organizaci√≥n del trabajo con memoria (por ejemplo, asignaci√≥n de tipos de segmentos de memoria: c√≥digo, datos, pila).  Pero lo m√°s importante que el procesador 286 trajo al mundo x86 es el concepto de anillos de protecci√≥n, que todav√≠a utilizamos. <br><br>  El concepto de anillos de protecci√≥n apareci√≥ originalmente en el sistema operativo Multics para el mainframe GE645 (1967) con una implementaci√≥n parcial de software y hardware completo ya en 1970 en el sistema Honeywell 6180. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a9/249/522/2a9249522d8bdb211dbb4eb0aec70b75.png" alt="imagen"><br><br>  La idea b√°sica de los anillos de defensa se asemeja a las fortalezas medievales de varios niveles; las m√°s valiosas se encuentran en el centro detr√°s de las m√∫ltiples paredes.  En este caso, lo m√°s valioso es el acceso directo ilimitado a cualquier √°rea de RAM y el control sobre todos los procesos.  Est√°n pose√≠dos por procesos que operan en el anillo cero de protecci√≥n.  Detr√°s de la pared, en el primer anillo, funcionan procesos menos importantes, como controladores de dispositivos y, en el √∫ltimo, aplicaciones de usuario.  El principio es simple: desde adentro puedes salir, pero desde afuera est√° prohibido.  Es decir  ning√∫n proceso de usuario puede acceder a la memoria del n√∫cleo del sistema operativo, como era posible en modo real anteriormente. <br><br>  En la primera implementaci√≥n completa del Honeywell 6180, se implementaron 8 anillos de protecci√≥n, pero Intel decidi√≥ simplificar el circuito a 4, de los cuales en la pr√°ctica los fabricantes de sistemas operativos comenzaron a usar solo dos: cero y tercero. <br><br><h4>  32bit </h4><br>  En 1985, se lanz√≥ otro procesador extremadamente arquitect√≥nicamente importante en la l√≠nea x86: 80386 (en adelante 386), que implement√≥ el direccionamiento de memoria de 32 bits y utiliz√≥ instrucciones de 32 bits.  Y, por supuesto, la virtualizaci√≥n de la memoria.  Como ya se mencion√≥, la virtualizaci√≥n es la ocultaci√≥n de la implementaci√≥n real a trav√©s de la provisi√≥n de recursos artificiales "virtuales".  En este caso, estamos hablando de direccionamiento de memoria.  El segmento de memoria tiene su propio direccionamiento, que no tiene nada que ver con la ubicaci√≥n real de las celdas de memoria. <br>  El procesador result√≥ tener tanta demanda que se produjo antes de 2007. <br>  La arquitectura en t√©rminos de Intel se llama IA32. <br><br><h4>  64bit </h4><br>  Por supuesto, incluso sin virtualizaci√≥n a mediados de la d√©cada de 2000, la industria ya estaba llegando a los l√≠mites de 32 bits.  Hubo soluciones parciales en forma de PAE (Extensi√≥n de direcci√≥n f√≠sica), pero complicaron y ralentizaron el c√≥digo.  La transici√≥n a 64 bits fue una conclusi√≥n inevitable. <br><br>  AMD present√≥ su versi√≥n de la arquitectura, que se llama AMD64.  En Intel, esperaban la plataforma IA64 (Intel Architecture 64), que tambi√©n conocemos con el nombre de Itanium.  Sin embargo, el mercado conoci√≥ esta arquitectura sin mucho entusiasmo, y como resultado, Intel se vio obligado a implementar su propio soporte para las instrucciones AMD64, que primero se llamaba EM64T, y luego solo Intel 64. <br><br>  En definitiva, todos conocemos esta arquitectura como AMD64, x86-64, x86_64 o, a veces, x64. <br><br>  Dado que se supon√≠a que el uso principal de los servidores en ese momento era f√≠sico, sin virtualizaci√≥n, sucedi√≥ algo t√©cnico divertido con los primeros procesadores de 64 bits en virtualizaci√≥n.  Los hipervisores anidados a menudo se usaban como servidores de laboratorio; no todos pod√≠an permitirse varios grupos de servidores f√≠sicos.  Y al final result√≥ que la VM de carga en el hipervisor incorporado solo pod√≠a funcionar en modo de 32 bits. <br><br>  En los primeros procesadores x86-64, los desarrolladores, manteniendo una compatibilidad total con el modo operativo de 32 bits, descartaron una parte importante de la funcionalidad en el modo de 64 bits.  En este caso, el problema era simplificar enormemente la segmentaci√≥n de la memoria.  Se elimin√≥ la capacidad de garantizar la inviolabilidad de un peque√±o trozo de memoria en la VM donde funcionaba el controlador de excepciones del hipervisor.  En consecuencia, el sistema operativo invitado pudo modificarlo. <br>  Posteriormente, AMD devolvi√≥ la posibilidad de limitar segmentos, e Intel simplemente esper√≥ la introducci√≥n de la virtualizaci√≥n de hardware. <br><br><h4>  UMA </h4><br>  Los sistemas multiprocesador X86 comenzaron a funcionar con el modo UMA (Acceso uniforme a la memoria), en el que la distancia desde cualquier procesador (retraso en el acceso a una celda de memoria) a cualquier barra de memoria es la misma.  En los procesadores Intel, este esquema de trabajo se conserv√≥ incluso despu√©s de la aparici√≥n de procesadores multin√∫cleo hasta la generaci√≥n 54xx (Harpertown).  Comenzando con la generaci√≥n 55xx (Nehalem), los procesadores han cambiado a la arquitectura NUMA. <br><br>  Desde el punto de vista de la l√≥gica de ejecuci√≥n, esta es la aparici√≥n de subprocesos de hardware adicionales, a los que puede asignar secuencias de c√≥digo para su ejecuci√≥n en paralelo. <br><br><h4>  NUMA </h4><br>  NUMA (acceso no uniforme a la memoria): arquitectura con acceso desigual a la memoria.  Dentro de esta arquitectura, cada procesador tiene su propia memoria local, a la que se accede directamente con baja latencia.  Se accede indirectamente a la memoria de otros procesadores con mayores retrasos, lo que conduce a un rendimiento reducido. <br><br>  Para los procesadores Intel Xeon escalables v2 para 2019, la arquitectura interna sigue siendo UMA dentro del z√≥calo, convirti√©ndose en NUMA para otros z√≥calos (aunque no realmente, y solo pretende serlo).  Los procesadores Opteron de AMD ten√≠an arquitectura NUMA incluso durante la √©poca del UMA Xeon m√°s antiguo, y luego NUMA se convirti√≥ incluso dentro del z√≥calo hasta la √∫ltima generaci√≥n de Roma, en la que volvieron a NUMA = z√≥calo. <br><br><h3>  M√°quina virtual </h3><br>  M√°quina virtual (VM, de la m√°quina virtual en ingl√©s): un sistema de software y / o hardware que emula el hardware de alguna plataforma (el objetivo es la plataforma objetivo o invitada) y ejecuta programas para la plataforma objetivo en la plataforma host (el host es la plataforma host , la plataforma host), o virtualizando alguna plataforma y creando entornos en ella que a√≠slen programas e incluso sistemas operativos entre s√≠.  Wikipedia <br>  En este art√≠culo diremos "m√°quina virtual", que significa "m√°quinas virtuales de sistema", lo que permite simular completamente todos los recursos y hardware en forma de construcciones de software. <br>  Hay dos tipos principales de software para crear m√°quinas virtuales: con full y resp.  virtualizaci√≥n incompleta <br><br>  <b>La virtualizaci√≥n completa</b> es un enfoque en el que se emula todo el hardware, incluido el procesador.  Le permite crear entornos independientes del hardware y ejecutar, por ejemplo, el sistema operativo y el software de aplicaci√≥n para la plataforma x86 en sistemas SPARC, o los conocidos emuladores Spectrum con el procesador Z80 en el conocido x86.  La otra cara de la independencia total es la alta sobrecarga para virtualizar el procesador y el bajo rendimiento general. <br><br>  <b>La virtualizaci√≥n incompleta</b> es un enfoque en el que no se virtualiza el 100% del hardware.  Dado que la virtualizaci√≥n incompleta es la m√°s com√∫n en la industria, hablaremos de ello.  Acerca de las plataformas y tecnolog√≠as de m√°quinas virtuales de sistema con virtualizaci√≥n incompleta para la arquitectura x86.  En este caso, virtualizaci√≥n incompleta del procesador, es decir  con la excepci√≥n de la sustituci√≥n parcial u ocultaci√≥n de ciertas llamadas al sistema, el procesador ejecuta el c√≥digo binario de la m√°quina virtual directamente. <br><br><h4>  Virtualizaci√≥n de software </h4><br>  La consecuencia obvia de la arquitectura del procesador y los h√°bitos de los sistemas operativos para trabajar en el anillo cero fue el problema: el n√∫cleo del sistema operativo invitado no puede funcionar en el lugar habitual.  El anillo cero est√° ocupado por el hipervisor, y solo tiene que dejar que el sistema operativo invitado llegue all√≠ tambi√©n; por un lado, volvimos al modo real con todas las consecuencias, y por otro lado, el sistema operativo invitado no espera a nadie all√≠, y destruir√° instant√°neamente todas las estructuras de datos y dejar√° caer el autom√≥vil. <br><br>  Pero todo se decidi√≥ simplemente: dado que para el hipervisor el sistema operativo invitado es solo un conjunto de p√°ginas de memoria con acceso directo completo, y el procesador virtual es solo una cola de comandos, ¬øpor qu√© no reescribirlos?  Sobre la marcha, el hipervisor arroja de la cola de instrucciones para la ejecuci√≥n en el procesador virtual todas las instrucciones que requieren privilegios de anillo cero, reemplaz√°ndolos por otros menos privilegiados.  Pero el resultado de estas instrucciones se presenta exactamente de la misma manera que si el SO hu√©sped estuviera en el anillo cero.  Por lo tanto, puede virtualizar cualquier cosa, hasta la ausencia total de un sistema operativo invitado. <br>  Este enfoque fue implementado por el equipo de desarrollo en 1999 en el producto VMware Workstation, y luego en 2001 en los hipervisores del servidor GSX (el segundo tipo, como Workstation) y ESX (el primer tipo). <br><br><h4>  Paravirtualizaci√≥n </h4><br>  <b>La paravirtualizaci√≥n</b> es un concepto muy simple, que supone que el SO hu√©sped sabe que est√° en una m√°quina virtual y sabe c√≥mo acceder al SO host para ciertas funciones del sistema.        ‚Äî   ,         . <br>   x86   2003     Linux Xen. <br><br>                 ,         . ,  VMware ESXi     SCSI  PVSCSI,          ,    .         ( VMware Tools),      Linux (open-vm-tools). <br><br><h4>   </h4><br>                 ,          . <br><br>      ‚Äî     Intel VT-x  AMD-V ,     ,      .            . <br><br><h3>   </h3><br><h4>  2 (hosted) </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los hipervisores del segundo tipo son aplicaciones que se ejecutan sobre el sistema operativo host. Todas las llamadas de m√°quina virtual son manejadas por el sistema operativo host ascendente. Los hipervisores del segundo tipo tienen un rendimiento muy limitado, ya que la aplicaci√≥n del hipervisor, al no tener derecho a la asignaci√≥n exclusiva de recursos inform√°ticos, se ve obligada a competir por ellos con otras aplicaciones de usuario. En t√©rminos de seguridad, los hipervisores tipo 2 dependen directamente de las pol√≠ticas de seguridad del sistema operativo del usuario y su vulnerabilidad a los ataques. Hoy en d√≠a, existe una opini√≥n un√°nime en la industria de que tales plataformas de virtualizaci√≥n para el nivel empresarial no son adecuadas. Sin embargo, son muy adecuados para el desarrollo multiplataforma y la implementaci√≥n de stands directamente en las m√°quinas de los desarrolladores de software, ya que son f√°ciles de administrar e implementar.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejemplos del segundo tipo de hipervisor: VMware Workstation / Fusion, Oracle VM VirtualBox, Parallels Desktop, VMware Server (ex-GSX), Microsoft Virtual Server 2005 </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tipo 1 (metal desnudo) </font></font></h4><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los hipervisores del primer tipo no requieren un sistema operativo de prop√≥sito general, a diferencia de los anteriores. El hipervisor en s√≠ es un monolito que controla tanto la asignaci√≥n de recursos inform√°ticos como la E / S. En el anillo de seguridad cero, hay un micro-n√∫cleo, sobre el cual funcionan todas las estructuras de control. En esta arquitectura, el hipervisor controla la distribuci√≥n de los recursos inform√°ticos y controla todas las llamadas de las m√°quinas virtuales a los dispositivos. VMware ESX fue considerado el primer hipervisor del primer tipo para x86 durante mucho tiempo, aunque ahora lo atribuir√≠amos a 1+. El √∫nico representante "honesto" de este tipo hoy en d√≠a es VMware ESXi, el sucesor de ESX, despu√©s de que fuera un poco fuera de la secci√≥n principal con RHEL.</font></font><br><br>      ESXi.      API ,    VMkernel.  ,      ,    .     ,             . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/361/f83/77d/361f8377d29345aa2498b31b9af66030.jpg" alt="imagen"><br><br>     :   ‚Äú‚Äù             ,       HCL (hardware compatibility list). <br><br><h4>  1+ ( ) </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los hipervisores de tipo h√≠brido (tambi√©n son tipos 1+, 1a, 1.5) se caracterizan por el aislamiento del sistema operativo base en una entidad especial llamada partici√≥n primaria (partici√≥n primaria en terminolog√≠a Microsoft Hyper-V) o un dominio primario (dominio dom0 en terminolog√≠a Xen). Entonces, despu√©s de instalar la funci√≥n del hipervisor, el n√∫cleo ingresa al modo de soporte de virtualizaci√≥n y el hipervisor es responsable de asignar recursos en el host. Pero la secci√≥n principal se encarga de la funci√≥n de procesamiento de llamadas a los controladores de dispositivos y operaciones de E / S.</font></font><br><br>  ,          .         :       ,     ESXi,  ,         HCL.               ,      . <br><br>     1+  : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89c/b74/eb3/89cb74eb3f97b6a8af063fbd447b1c87.jpg" alt="imagen"><br><br>     :  VMware ESX, Microsoft Hyper-V, Xen-based  (Citrix XenServer   Xen    Linux). ,  Citrix XenServer ‚Äì    RHEL-based OS,             Red-Hat Enterprise Linux.      Xen    :     Linux    Xen      dom0.    ,  Xen-based           1 . <br><br><h2>     </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se tomar√° la base de la terminolog√≠a de VMware, como la plataforma de virtualizaci√≥n m√°s avanzada tecnol√≥gicamente. </font><font style="vertical-align: inherit;">En este art√≠culo, nos restringimos a las tecnolog√≠as de los propios hipervisores y al sistema de control b√°sico. </font><font style="vertical-align: inherit;">Toda la funcionalidad avanzada implementada por productos adicionales por dinero adicional se quedar√° detr√°s de escena. </font><font style="vertical-align: inherit;">Las tecnolog√≠as se agrupan en grupos condicionales para el prop√≥sito principal, como le pareci√≥ al autor, con quien tiene derecho a estar en desacuerdo.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> SLA </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Esta es una colecci√≥n de tecnolog√≠as que afectan principalmente el rendimiento de los SLA para la accesibilidad (RPO / RTO). </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alta disponibilidad: una tecnolog√≠a para garantizar la alta disponibilidad de m√°quinas virtuales en un cl√∫ster mediante un hipervisor. </font><font style="vertical-align: inherit;">En el caso de una muerte del host, la VM se reinicia autom√°ticamente en los hosts supervivientes. </font><font style="vertical-align: inherit;">Efecto: minimizar RTO antes del tiempo de espera de HA + reiniciar OS / servicios.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> FT </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tolerancia a fallos: tecnolog√≠a para garantizar el funcionamiento continuo de las m√°quinas virtuales, incluso en caso de muerte del hu√©sped. </font><font style="vertical-align: inherit;">Se crea una VM virtual en el segundo host, que es completamente id√©ntica a la principal y repite las instrucciones detr√°s de ella. </font><font style="vertical-align: inherit;">Por lo tanto, la diferencia en los estados de VM se mide en decenas o cientos de milisegundos, lo cual es bastante aceptable para muchos servicios. </font><font style="vertical-align: inherit;">Cuando el host muere, la ejecuci√≥n cambia autom√°ticamente a Shadow VM. </font><font style="vertical-align: inherit;">Efecto: minimizar RTO a cero.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tco </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Esta es una colecci√≥n de tecnolog√≠as que influyen principalmente en el TCO. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vMotion </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vMotion es una tecnolog√≠a para la migraci√≥n en vivo de un punto de ejecuci√≥n de VM de un host totalmente funcional a otro. </font><font style="vertical-align: inherit;">Al mismo tiempo, el punto de conmutaci√≥n del punto de ejecuci√≥n es menor que los tiempos de espera de conexi√≥n de red, lo que nos permite considerar la migraci√≥n como en vivo, es decir. </font><font style="vertical-align: inherit;">sin interrupci√≥n en el trabajo de servicios productivos. </font><font style="vertical-align: inherit;">Efecto: reducir el RTO a cero para interrupciones planificadas para el mantenimiento del servidor y, como resultado, la eliminaci√≥n parcial de las interrupciones en s√≠ mismas.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VMotion de almacenamiento </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Storage vMotion es una tecnolog√≠a para la migraci√≥n en vivo de un punto de almacenamiento de VM de un almacenamiento totalmente funcional a otro. </font><font style="vertical-align: inherit;">Al mismo tiempo, el trabajo con el sistema de discos no se detiene, lo que permite que la migraci√≥n se considere en vivo. </font><font style="vertical-align: inherit;">Efecto: reducir el RTO a cero para las interrupciones planificadas para el mantenimiento del almacenamiento y, como resultado, la eliminaci√≥n parcial de las interrupciones en s√≠ mismas.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DPM </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Administraci√≥n de energ√≠a distribuida: tecnolog√≠a para controlar el nivel de carga del host y el encendido / apagado de los hosts a medida que cambia la carga en el cl√∫ster. </font><font style="vertical-align: inherit;">Requiere DRS para su funcionamiento. </font><font style="vertical-align: inherit;">Efecto: reducci√≥n general del consumo de energ√≠a.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VSwitch distribuido </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VSwitch distribuido es una tecnolog√≠a para la administraci√≥n centralizada de la configuraci√≥n de red de conmutadores de host virtual. </font><font style="vertical-align: inherit;">Efecto: reduciendo el volumen y la complejidad del trabajo en la reconfiguraci√≥n del subsistema de red, reduciendo los riesgos de errores.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> EVC </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La compatibilidad mejorada de vMotion es una tecnolog√≠a que permite enmascarar las instrucciones de procesador disponibles para m√°quinas virtuales en modo autom√°tico. </font><font style="vertical-align: inherit;">Se utiliza para alinear el trabajo de las m√°quinas virtuales en un cl√∫ster desigual con la familia de procesadores m√°s antigua, proporcionando la capacidad de migrar m√°quinas virtuales a cualquier host. </font><font style="vertical-align: inherit;">Efecto: ahorro en la complejidad de la infraestructura al tiempo que aumenta gradualmente la capacidad / actualizaci√≥n parcial de los cl√∫steres.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> QoS </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Esta es una colecci√≥n de tecnolog√≠as que influyen principalmente en el rendimiento de SLA en t√©rminos de calidad de servicio. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vNUMA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vNUMA es una tecnolog√≠a que permite que el SO hu√©sped se comunique con la topolog√≠a virtual NUMA de VM para m√°quinas anchas (vCPU o vRAM&gt; nodo NUMA). </font><font style="vertical-align: inherit;">Efecto: la falta de una penalizaci√≥n en el rendimiento del software de aplicaci√≥n que admite NUMA.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Grupo de recursos </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Grupos de recursos: la tecnolog√≠a de combinar varias m√°quinas virtuales en un solo grupo de recursos para controlar el consumo o garantizar la asignaci√≥n de recursos. </font><font style="vertical-align: inherit;">Efecto: simplificar la administraci√≥n, proporcionar un nivel de servicio.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> L√≠mite / reserva </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La memoria / procesador limitante y redundante le permite limitar la asignaci√≥n de recursos, o viceversa, para garantizar su asignaci√≥n en una situaci√≥n de escasez y competencia para garantizar el mantenimiento de m√°quinas virtuales / grupos de alta prioridad. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DRS </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Programador din√°mico de recursos: equilibrio autom√°tico de m√°quinas virtuales por parte de hosts seg√∫n la carga para reducir la fragmentaci√≥n de recursos en el cl√∫ster y proporcionar un nivel de servicio para m√°quinas virtuales. </font><font style="vertical-align: inherit;">Requiere soporte de vMotion.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Control de IO de almacenamiento </font></font></h4><br> Storage IO control ‚Äî ,   ‚Äú ‚Äù,      ,          .   ‚Äî   /     . <br><br><h4> Network IO Control </h4><br> Network IO Control ‚Äî ,   ‚Äú ‚Äù,      . <br><br><h3> Storage Integration (VAAI etc) </h3><br>       : <br><ul><li>                /   ,      . </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Integraci√≥n de nivel de protocolo - VAAI, ODX. </font><font style="vertical-align: inherit;">Estas tecnolog√≠as le permiten descargar el subsistema de disco, transfiriendo parte de la carga est√°ndar a la disposici√≥n de almacenamiento inteligente. </font><font style="vertical-align: inherit;">Por ejemplo, esta categor√≠a incluye operaciones tales como bloques de puesta a cero, clonaci√≥n de m√°quinas virtuales, etc. </font><font style="vertical-align: inherit;">Debido a esto, el canal hacia el sistema de almacenamiento est√° significativamente descargado, y el sistema de almacenamiento realiza las operaciones de disco de una manera m√°s √≥ptima.</font></font></li></ul><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seguridad </font></font></h3><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Microsegmentaci√≥n </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La microsegmentaci√≥n de una red virtual en uso pr√°ctico es la capacidad de construir un firewall distribuido virtual que controla las redes virtuales dentro del host. </font><font style="vertical-align: inherit;">Mejora extremadamente la seguridad de la red virtual.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AV sin agente </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Soporte de tecnolog√≠a antivirus sin agentes. </font><font style="vertical-align: inherit;">En lugar de ser verificado por agentes en el SO hu√©sped, el tr√°fico de las operaciones de disco de la VM es dirigido por el hipervisor a la VM del servicio seleccionado. </font><font style="vertical-align: inherit;">Reduce significativamente la carga en los procesadores y el sistema de disco, matando efectivamente las "tormentas antivirus".</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sistemas hiperconvergentes </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los sistemas convergentes, como su nombre indica, son sistemas con una combinaci√≥n de funciones. Y en este caso, nos referimos a la combinaci√≥n del almacenamiento y la ejecuci√≥n de la VM. Parece simple, pero el marketing de repente interrumpe. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por primera vez con el t√©rmino sistemas convergentes, los especialistas en marketing entran al mercado. Los sistemas convergentes vend√≠an servidores cl√°sicos ordinarios + almacenamiento + conmutadores. Justo debajo de un n√∫mero de socio. O ni siquiera estaban vendiendo, pero se produjo un art√≠culo llamado "arquitectura de referencia". Condenamos sinceramente este enfoque y pasamos a la consideraci√≥n arquitect√≥nica.</font></font><br><br><h3>  Arquitectura </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Manteniendo la convergencia como un principio arquitect√≥nico, obtenemos una combinaci√≥n del punto de almacenamiento y el punto de ejecuci√≥n de la VM en un solo sistema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La arquitectura convergente, en otras palabras, implica el uso de los mismos servicios de hardware tanto para ejecutar m√°quinas virtuales como para almacenarlos en discos locales. </font><font style="vertical-align: inherit;">Bueno, dado que deber√≠a haber tolerancia a fallas, en una arquitectura convergente hay una capa de SDS distribuido.</font></font><br><br>  Obtenemos: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El sistema cl√°sico: software, almacenamiento, conmutaci√≥n y servidores provienen de diferentes lugares, combinados por las manos del cliente / integrador. </font><font style="vertical-align: inherit;">Contratos de soporte separados.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sistema convergente: todo de una fuente, un soporte, un n√∫mero de socio. </font><font style="vertical-align: inherit;">No debe confundirse con el autoensamblaje de un proveedor.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Y resulta que el t√©rmino para nuestra arquitectura convergente ya est√° tomado. </font><font style="vertical-align: inherit;">Exactamente la misma situaci√≥n que con el supervisor. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sistema hiperconvergente</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : un sistema convergente con arquitectura convergente. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por supuesto, no fue sin la segunda venida de los vendedores. </font><font style="vertical-align: inherit;">Aparecieron sistemas convergentes en los que no hab√≠a combinaci√≥n de almacenamiento, pero hay nodos de almacenamiento dedicados bajo el control de SDS distribuido. </font><font style="vertical-align: inherit;">Dentro del marco de las guerras de marketing, incluso apareci√≥ el t√©rmino especial HCI desagregado (infraestructura hiperverg√©nica desagregada). </font><font style="vertical-align: inherit;">En particular, por ejemplo, NetApp con un sistema similar al principio luch√≥ intensamente por el derecho a llamar a su sistema hiperconvergente, pero finalmente se rindi√≥. </font><font style="vertical-align: inherit;">NetApp HCI hoy (finales de 2019): infraestructura de nube h√≠brida.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Opciones de implementaci√≥n </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Debido al hecho de que los sistemas hiperconvergentes funcionan con virtualizaci√≥n, en realidad hay dos opciones y media para la implementaci√≥n. </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. El m√≥dulo del n√∫cleo. </font><font style="vertical-align: inherit;">SDS funciona como un monolito en el n√∫cleo del hipervisor, por ejemplo vSAN + ESXi</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1.5 M√≥dulo de la secci√≥n principal. </font><font style="vertical-align: inherit;">SDS funciona como un servicio como parte de la secci√≥n principal del hipervisor, por ejemplo S2D + Hyper-V</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. La m√°quina virtual. </font><font style="vertical-align: inherit;">SDS se implementa como una m√°quina virtual dedicada en cada host. </font><font style="vertical-align: inherit;">Nutanix, Cisco Hyperflex, HPE Simplivity.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obviamente, adem√°s de los problemas discutidos con el efecto de incrustar en el rendimiento, existe un problema muy importante de aislamiento y soporte de hipervisores de terceros. </font><font style="vertical-align: inherit;">En el caso 1, es obvio que este puede ser solo un sistema del proveedor del hipervisor, mientras que 2 puede funcionar en cualquier hipervisor.</font></font><br><br><h2>  Contenedores </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La virtualizaci√≥n en contenedores, aunque t√©cnicamente es muy diferente de la virtualizaci√≥n completa, parece bastante simple en estructura. </font><font style="vertical-align: inherit;">Al igual que con el modelo de red OSI, la pregunta es el nivel. </font><font style="vertical-align: inherit;">La virtualizaci√≥n de contenedores es un nivel superior, a nivel del entorno de la aplicaci√≥n, y no a la f√≠sica. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La tarea principal de la virtualizaci√≥n de contenedores es dividir el sistema operativo en partes independientes, desde las cuales las aplicaciones aisladas no podr√≠an interferir entre s√≠. </font><font style="vertical-align: inherit;">La virtualizaci√≥n completa no es compartida por el sistema operativo, sino por un servidor f√≠sico.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> VM vs Contenedor </font></font></h3><br>  Los pros y los contras de ambos enfoques son bastante simples y directamente opuestos. <br><br>  La virtualizaci√≥n completa (VM) brinda total independencia al nivel de hierro, incluido el sistema operativo totalmente independiente, el disco y las pilas de red.  Por otro lado, cada aplicaci√≥n, debido a que nos adherimos al esquema 1 aplicaci√≥n = 1 servidor, requiere su propio sistema operativo, su propio disco y pila de red.  es decir  Hay un gasto m√∫ltiple de recursos. <br><br>  Los contenedores tienen discos comunes y pilas de red con el sistema operativo host, y todos juntos usan el mismo n√∫cleo en todo el servidor f√≠sico (bueno o virtual, como en los √∫ltimos tiempos), lo que en su conjunto le permite ahorrar bastante recursos en paisajes homog√©neos. <br><br>  Hist√≥ricamente, x86 inicialmente ten√≠a contenedores para todo, junto con servidores f√≠sicos.  Despu√©s del advenimiento de la virtualizaci√≥n completa, la importancia de los contenedores disminuy√≥ dr√°sticamente en casi 15 a√±os, y las m√°quinas virtuales gruesas rein√≥ en el mundo corporativo.  En ese momento, los contenedores se encontraban en servidores que proporcionaban cientos de servidores web del mismo tipo, donde su ligereza era muy solicitada.  Pero en los √∫ltimos a√±os, desde aproximadamente 2015, los contenedores han vuelto a la realidad corporativa en forma de aplicaciones nativas de la nube. <br><br><h3>  Contenedores 0.1 </h3><br><h4>  chroot </h4><br>  El prototipo de contenedores en 1979 fue chroot. <br><br>  ‚ÄúChroot es la operaci√≥n de cambiar el directorio ra√≠z en sistemas operativos tipo Unix.  Un programa lanzado con un directorio ra√≠z modificado solo tendr√° acceso a los archivos contenidos en este directorio ". <br><br>  Es decir  de hecho, el aislamiento es solo a nivel del sistema de archivos, de lo contrario, es solo un proceso normal en el sistema operativo. <br><br><h4>  C√°rcel de Freebsd </h4><br>  Significativamente m√°s avanzada fue la prisi√≥n de BSD libre, que apareci√≥ en 1999.  Jail le permiti√≥ crear instancias de SO virtuales completas con sus propios conjuntos de aplicaciones y archivos de configuraci√≥n basados ‚Äã‚Äãen la base FreeBSD.  Seguramente hay quienes dicen, ¬°y qu√© hace la c√°rcel en contenedores, porque esto es paravirtualizaci√≥n!  Y estar√°n parcialmente en lo cierto. <br><br>  Sin embargo, antes de la virtualizaci√≥n completa (y su variante en forma de paravirtualizaci√≥n) la c√°rcel carece de la capacidad de ejecutar el n√∫cleo de una versi√≥n diferente en la VM invitada y de agruparse con la migraci√≥n de la VM a otro sistema host. <br><br><h4>  Zonas de Solaris </h4><br>  Solaris Zones es una tecnolog√≠a de virtualizaci√≥n del sistema operativo (virtualizaci√≥n de contenedores), introducida en 2004 en Sun Solaris.  El principio b√°sico es una baja sobrecarga de virtualizaci√≥n. <br><br>  Sin ganar mucha popularidad, migr√≥ a OpenSolaris y las distribuciones basadas en √©l, disponibles en 2019. <br><br><h3>  Contenedores 1.0 </h3><br>  En la era de los contenedores 1.0, han aparecido dos direcciones principales de contenedorizaci√≥n: estos son productos comerciales para proveedores de alojamiento y la contenedorizaci√≥n de aplicaciones. <br><br><h4>  Virtuozzo / OpenVZ </h4><br>  SWsoft ruso present√≥ en 2001 su primera versi√≥n de virtualizaci√≥n de contenedores Virtuozzo, dirigida al mercado de proveedores de hosting.  Debido a la determinaci√≥n y al p√∫blico objetivo comercial espec√≠fico, el producto result√≥ ser bastante exitoso y gan√≥ popularidad.  Tecnol√≥gicamente, en 2002, se demostr√≥ la operaci√≥n simult√°nea de 2500 contenedores en un servidor de 8 procesadores. <br><br>  En 2005, apareci√≥ una versi√≥n abierta de contenedores Virtuozzo para Linux llamada OpenVZ.  Y casi se convirti√≥ en el est√°ndar de oro para alojar VPS. <br><br><h4>  Lxc </h4><br>  LinuX Containers (LXC) es otra virtualizaci√≥n de contenedores bien conocida basada en espacios de nombres y grupos c, que apareci√≥ en 2008. Es la base de los dockers actualmente populares, etc. <br><br><h3>  Contenedores 1.1 (virtualizaci√≥n de aplicaciones) </h3><br>  Si los contenedores restantes est√°n dise√±ados para dividir el sistema operativo base en segmentos, entonces, ¬øpor qu√© no cortar esta capa del sistema y empaquetarla en una sola caja con la aplicaci√≥n y todo su entorno?  Y luego este paquete listo para usar se puede iniciar como una aplicaci√≥n normal a nivel de usuario. <br><br><h4>  App-v </h4><br>  Microsoft Application Virtualization (App-V), anteriormente Softricity SoftGrid: tecnolog√≠a para contener aplicaciones espec√≠ficas (el contenedor es al rev√©s) en un entorno aislado, luego Microsoft.  En 2006, Microsoft adquiri√≥ el inicio de Softricity, que en realidad cambi√≥ el contenedor. <br><br><h4>  Thinapp </h4><br>  VMware ThinApp (anteriormente Thinstall) es un producto de contenedorizaci√≥n de aplicaciones de Jilt adquirido por VMware en 2008.  VMware estima que el 90-95% de todas las aplicaciones empaquetadas en el mundo usan esta tecnolog√≠a. <br><br><h3>  Contenedores 2.0 </h3><br>  La historia de la aparici√≥n de contenedores 2.0 est√° muy asociada con un cambio en el proceso de desarrollo de software.  El deseo de la empresa de reducir un par√°metro tan importante como el tiempo de comercializaci√≥n oblig√≥ a los desarrolladores a reconsiderar los enfoques para crear productos de software.  La metodolog√≠a de desarrollo de Waterfall (ciclos de lanzamiento largos, toda la aplicaci√≥n se actualiza) se reemplaza por Agile (ciclos de lanzamiento cortos y fijos, los componentes de la aplicaci√≥n se actualizan independientemente) y obliga a los desarrolladores a separar las aplicaciones monol√≠ticas en componentes.  Si bien los componentes de las aplicaciones monol√≠ticas a√∫n son bastante grandes y no hay muchos de ellos que se puedan colocar en m√°quinas virtuales, pero cuando una aplicaci√≥n consta de decenas o cientos de componentes, las m√°quinas virtuales ya no son muy adecuadas.  Adem√°s, tambi√©n surge el problema de las versiones de software auxiliar, bibliotecas y dependencias, a menudo hay una situaci√≥n en la que diferentes componentes requieren diferentes versiones o variables de entorno configuradas de manera diferente.  Dichos componentes deben distribuirse a diferentes m√°quinas virtuales, porque  Es casi imposible ejecutar simult√°neamente varias versiones de software dentro del mismo sistema operativo.  El n√∫mero de VM comienza a crecer como una avalancha.  Aqu√≠, los contenedores aparecen en el escenario, lo que permite en el marco de un sistema operativo invitado crear varios entornos aislados para iniciar componentes de aplicaciones.  La contenerizaci√≥n de aplicaciones le permite continuar la segmentaci√≥n de una aplicaci√≥n monol√≠tica en componentes a√∫n m√°s peque√±os y pasar al paradigma de una tarea = un componente: un contenedor, esto se denomina enfoque de microservicio, y cada uno de esos componentes es un microservicio. <br><br><h4>  Contenedor debajo del cap√≥ </h4><br>  Si observa el contenedor con una mirada del administrador del sistema, estos son solo procesos de Linux que tienen sus propios pids, etc.  ¬øQu√© hace posible aislar procesos que se ejecutan en contenedores entre s√≠ y consumir los recursos del SO hu√©sped juntos?  Dos mecanismos est√°ndar presentes en el n√∫cleo de cualquier distribuci√≥n moderna de Linux.  El primero, Linux Namespaces, que garantiza que cada proceso vea su propia representaci√≥n del sistema operativo (sistema de archivos, interfaces de red, nombre de host, etc.) y el segundo, Linux Control Groups (cgroups), que restringe el proceso al consumo de recursos del sistema operativo invitado (CPU, memoria ancho de banda de red, etc.). <br><br><h4>  Espacios de nombres de Linux </h4><br>  Por defecto, cada sistema Linux contiene un solo espacio de nombres.  Todos los recursos del sistema, como sistemas de archivos, identificadores de proceso (ID de proceso), identificadores de usuario (ID de usuario), interfaces de red pertenecen a este espacio de nombres.  Pero nadie nos impide crear espacios de nombres adicionales y redistribuir los recursos del sistema entre ellos. <br><br>  Cuando se inicia un nuevo proceso, se inicia en un espacio de nombres, est√°ndar del sistema o uno de los creados.  Y este proceso ver√° solo aquellos recursos que est√°n disponibles en el espacio de nombres utilizado para ejecutarlo. <br><br>  Pero no todo es tan simple, cada proceso no pertenece a un solo espacio de nombres, sino a un espacio de nombres en cada una de las categor√≠as: <br><br><ul><li>  Monte (mnt) </li><li>  ID de proceso (pid) </li><li>  Red (net) </li><li>  Comunicaci√≥n entre procesos (ipc) </li><li>  UTS </li><li>  ID de usuario (usuario) </li></ul><br>  Cada tipo de espacio de nombres a√≠sla un grupo de recursos correspondiente.  Por ejemplo, el espacio UTS define el nombre de host y el nombre de dominio visibles para los procesos.  Por lo tanto, dos procesos dentro del sistema operativo invitado pueden suponer que se ejecutan en servidores diferentes. <br><br>  El espacio de nombres de red determina la visibilidad de las interfaces de red, el proceso en el interior solo ver√° las interfaces que pertenecen a este espacio de nombres. <br><br><h4>  Grupos de control de Linux (cgroups) </h4><br>  Linux Control Groups (cgroups) es el mecanismo del sistema del kernel (Kernel) de los sistemas Linux que limita el consumo de recursos del sistema por parte de los procesos.  Cada proceso o grupo de procesos no podr√° obtener m√°s recursos (CPU, memoria, ancho de banda de red, etc.) de los que est√° asignado, y no podr√° capturar los "otros" recursos: los recursos de los procesos vecinos. <br><br><h3>  Docker </h3><br>  Como se indic√≥ anteriormente, Docker no invent√≥ los contenedores como tales.  Los contenedores han existido durante muchos a√±os (incluidos los basados ‚Äã‚Äãen LXC), pero Docker los hizo muy populares al crear el primer sistema que facilit√≥ y facilit√≥ la transferencia de contenedores entre diferentes m√°quinas.  Docker ha creado una herramienta para crear contenedores: empaquetar la aplicaci√≥n y sus dependencias y ejecutar contenedores en cualquier sistema Linux con Docker instalado. <br><br>  Una caracter√≠stica importante de Docker es la portabilidad no solo de la aplicaci√≥n en s√≠ y sus dependencias entre distribuciones de Linux completamente diferentes, sino tambi√©n la portabilidad del entorno y el sistema de archivos.  Por ejemplo, un contenedor creado en CentOS puede ejecutarse en un sistema Ubuntu.  En este caso, dentro del contenedor lanzado, el sistema de archivos se heredar√° de CentOS, y la aplicaci√≥n considerar√° que se ejecuta sobre CentOS.  Esto es algo similar a una imagen OVF de una m√°quina virtual, pero el concepto de una imagen Docker usa capas.  Esto significa que cuando se actualiza solo una parte de la imagen, no es necesario volver a descargar la imagen completa, es suficiente descargar solo la capa modificada, como si en la imagen OVF fuera posible actualizar el sistema operativo sin actualizar toda la imagen. <br><br>  Docker ha creado un ecosistema para crear, almacenar, transferir y lanzar contenedores.  Hay tres componentes clave para el mundo Docker: <br><br><ul><li>  Im√°genes: una imagen, esta es la entidad que contiene su aplicaci√≥n, el entorno necesario y otros metadatos necesarios para iniciar el contenedor; </li><li>  Registros: repositorio, lugar de almacenamiento para im√°genes Docker.  Hay una variedad de repositorios, que van desde el oficial - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hub.docker.com</a> y terminan con los privados implementados en la infraestructura de la compa√±√≠a; </li><li>  Contenedores: un contenedor, contenedor de Linux creado a partir de una imagen de Docker.  Como se mencion√≥ anteriormente, este es un proceso de Linux que se ejecuta en un sistema Linux con Docker instalado, aislado de otros procesos y del sistema operativo en s√≠. </li></ul><br>  Considere el ciclo de vida del contenedor.  Inicialmente, un desarrollador crea una imagen de Docker con su aplicaci√≥n (el comando de compilaci√≥n de Docker), completamente desde cero o utilizando im√°genes ya creadas como base (recuerde las capas).  Adem√°s, el desarrollador puede iniciar esta imagen directamente en su propia m√°quina o puede transferirla a otra m√°quina: el servidor.  Para la portabilidad, a menudo se utilizan repositorios (el comando push docker): cargan la imagen en el repositorio.  Despu√©s de eso, la imagen se puede descargar a cualquier otra m√°quina o servidor (docker pull).  Finalmente, cree un contenedor de trabajo (Docker Run) a partir de esta imagen. <br><br><h3>  Kubernetes </h3><br>  Como ya dijimos, el concepto de microservicios significa dividir una aplicaci√≥n monol√≠tica en muchos servicios peque√±os, generalmente realizando una sola funci√≥n.  Bueno, cuando hay docenas de tales servicios, todav√≠a se pueden administrar manualmente a trav√©s de, por ejemplo, Docker.  Pero, ¬øqu√© hacer cuando hay cientos y miles de tales servicios?  Adem√°s del entorno industrial, necesita un entorno de prueba y entornos adicionales para diferentes versiones del producto, es decir.  multiplique por 2, por 3 o incluso m√°s.  Google tambi√©n enfrent√≥ los mismos problemas, sus ingenieros fueron uno de los primeros en utilizar contenedores a escala industrial.  As√≠ naci√≥ Kubernetes (K8s), creado bajo el nombre de Borg en los muros del producto de Google, luego dado al p√∫blico en general y renombrado. <br><br>  K8s es un sistema que facilita la implementaci√≥n, administraci√≥n y monitoreo de aplicaciones en contenedores (microservicios).  Como ya sabemos, cualquier m√°quina Linux es adecuada para lanzar contenedores y los contenedores est√°n aislados unos de otros, respectivamente, y los K8 pueden administrar diferentes servidores con diferentes hardware y bajo el control de diferentes distribuciones de Linux.  Todo esto nos ayuda a usar el hardware disponible de manera efectiva.  Al igual que la virtualizaci√≥n, K8s nos proporciona un conjunto com√∫n de recursos para lanzar, administrar y monitorear nuestros microservicios. <br><br>  Dado que este art√≠culo est√° destinado principalmente a ingenieros de virtualizaci√≥n, para una comprensi√≥n general de los principios de operaci√≥n y los componentes principales de K8, le recomendamos que lea el art√≠culo que establece el paralelismo entre K8 y VMware vSphere: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://medium.com/@pryalukhin/kubernetes-introduction-for-vmware- usuarios-232cc2f69c58</a> <br><br><h2>  Historia de virtualizaci√≥n industrial X86 </h2><br><h3>  VMware </h3><br>  VMware apareci√≥ en 1998, comenzando con el desarrollo de un segundo tipo de hipervisor, que m√°s tarde se conocer√≠a como VMware Workstation. <br><br>  La compa√±√≠a ingres√≥ al mercado de servidores en 2001 con dos hipervisores: GSX (Ground Storm X, segundo tipo) y ESX (Elastic Sky X, primer tipo).  Con el tiempo, las perspectivas del segundo tipo en aplicaciones de servidor se han vuelto obvias, es decir  Ninguno.  Y el GSX pagado se convirti√≥ primero en un servidor VMware gratuito, y luego se detuvo y enterr√≥ por completo. <br><br>  En 2003, apareci√≥ el sistema de administraci√≥n central del Centro Virtual, la tecnolog√≠a vSMP y la migraci√≥n en vivo de m√°quinas virtuales. <br><br>  En 2004, VMware fue adquirido por EMC, un gigante de almacenamiento, pero dej√≥ de funcionar de manera independiente. <br><br>  En 2008, convirti√©ndose en el est√°ndar de facto de la industria, VMware estimul√≥ el r√°pido crecimiento de las ofertas competitivas: Citrix, Microsoft, etc. Se hace evidente la necesidad de obtener una versi√≥n gratuita del hipervisor, lo que era imposible, ya que una secci√≥n principal en ESX utilizaba RHEL bastante comercial.  El proyecto para reemplazar RHEL con algo m√°s f√°cil y gratuito se implement√≥ en 2008 con el sistema busybox.  El resultado es ESXi, conocido por todos hoy. <br><br>  Paralelamente, la compa√±√≠a se est√° desarrollando a trav√©s de proyectos internos y adquisiciones de startups.  Hace unos a√±os, una lista de productos de VMware ocupaba un par de p√°ginas A4, as√≠ que digamos.  VMware para 2019 sigue siendo el est√°ndar de facto en el mercado de virtualizaci√≥n corporativa corporativa local con una cuota de mercado de m√°s del 70% y un l√≠der absoluto en tecnolog√≠a, y una revisi√≥n detallada de la historia merece un art√≠culo muy extenso. <br><br><h3>  Connectix </h3><br>  Fundada en 1988, Connectix trabaj√≥ en una variedad de utilidades del sistema hasta que tom√≥ la virtualizaci√≥n.  En 1997, se cre√≥ el primer producto VirtualPC para Apple Macintosh, permitiendo que Windows se ejecute en una m√°quina virtual.  La primera versi√≥n de VirtualPC para Windows apareci√≥ en 2001. <br><br>  En 2003, Microsoft compr√≥ VirtualPC y, de acuerdo con Connectix, los desarrolladores se cambiaron a Microsoft.  Despu√©s de eso, Connectix cerr√≥. <br><br>  El formato VHD (disco duro virtual) fue desarrollado por Connectix para VirtualPC, y como recordatorio, los discos virtuales de las m√°quinas Hyper-V contienen "conectix" en su firma. <br>  La PC virtual, como puede suponer, es un hipervisor de escritorio cl√°sico del segundo tipo. <br><br><h3>  Microsoft </h3><br>  El viaje de Microsoft hacia la virtualizaci√≥n industrial comenz√≥ con la compra de Connectix y el cambio de marca de Connectix Virtual PC en Microsoft Virtual PC 2004. Virtual PC desarrollado por un tiempo, se incluy√≥ bajo el nombre de Windows Virtual PC en Windows 7. En Windows 8 y posterior, Virtual PC fue reemplazado por versi√≥n de escritorio de Hyper-V. <br><br>  Basado en Virtual PC, se cre√≥ el hipervisor del servidor Virtual Server, que existi√≥ hasta principios de 2008.  Debido a la evidente p√©rdida tecnol√≥gica antes de VMware ESX, se decidi√≥ restringir el desarrollo del segundo tipo de hipervisor en favor de su propio primer tipo de hipervisor, que se convirti√≥ en Hyper-V.  Existe una opini√≥n no oficial en la industria de que Hyper-V es sorprendentemente similar a Xen en arquitectura.  Aproximadamente lo mismo que .Net en Java. <br><blockquote>  "Por supuesto, podr√≠as pensar que Microsoft rob√≥ la idea de Java".  Pero esto no es cierto, ¬°Microsoft la inspir√≥!  - (de un discurso de un representante de Microsoft en la presentaci√≥n de Windows 2003 Server) </blockquote><br>  Desde los momentos curiosos, se puede observar que dentro de Microsoft, el uso de productos de virtualizaci√≥n patentados en los a√±os cero fue, por decirlo suavemente, opcional.  Hay capturas de pantalla de Technet de art√≠culos sobre virtualizaci√≥n, donde el logotipo de VMware Tools est√° claramente presente en la bandeja.  Adem√°s, Mark Russinovich en la Plataforma 2009 en Mosc√∫ realiz√≥ una demostraci√≥n con VMware Workstation. <br><br>  En un esfuerzo por ingresar a nuevos mercados, Microsoft cre√≥ su propia nube p√∫blica, Azure, utilizando un Nano Server altamente modificado con Hyper-V, S2D y SDN como plataforma.  Vale la pena se√±alar que inicialmente, Azure en algunos puntos estaba muy por detr√°s de los sistemas locales.  Por ejemplo, el soporte para m√°quinas virtuales de segunda generaci√≥n (con soporte para arranque seguro, arranque desde particiones GPT, arranque PXE, etc.) apareci√≥ en Azure solo en 2018.  Mientras se encuentra en las instalaciones, las m√°quinas virtuales de segunda generaci√≥n se conocen desde Windows Server 2012R2.  Lo mismo ocurre con las soluciones de portal: hasta 2017, Azure y Windows Azure Pack (soluci√≥n en la nube Multi-Tenancy con SDN y compatibilidad con VM protegida, que reemplaz√≥ a System Center App Controller en 2013) utilizaron el mismo dise√±o de portal.  Despu√©s de que Microsoft anunci√≥ un curso sobre nubes p√∫blicas, Azure dio un paso adelante para desarrollar e implementar varios conocimientos.  Alrededor del a√±o 2016, puede observar una imagen completamente l√≥gica: ahora todas las innovaciones en Windows Server provienen de Azure, pero no en la direcci√≥n opuesta.  El hecho de copiar partes de la documentaci√≥n de Azure en las instalaciones "tal cual" lo indica (consulte la documentaci√≥n en Azure SDN y Network Controller), que por un lado sugiere la actitud hacia las soluciones locales y, por el otro, indica la relaci√≥n de las soluciones en t√©rminos de entidades y arquitectura.  Qui√©n copi√≥ de qui√©n y c√≥mo es realmente: una pregunta discutible. <br><br>  En marzo de 2018, Satya Nadela (CEO de Microsoft) anunci√≥ oficialmente que la nube p√∫blica se estaba convirtiendo en la prioridad de la compa√±√≠a.  Lo que, obviamente, simboliza el plegado gradual y el desvanecimiento de la l√≠nea del servidor para productos locales (sin embargo, el estancamiento se observ√≥ en 2016, pero se confirm√≥ con la primera versi√≥n beta de Windows Server y otras l√≠neas de productos locales), con la excepci√≥n de Azure Edge, el servidor m√≠nimo requerido Infraestructura en la oficina del cliente para servicios que no se pueden llevar a la nube. <br><br><h3>  Plancha virtual </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fundada en 2003, Virtual Iron ofreci√≥ una versi√≥n comercial de Xen y fue una de las primeras en ofrecer al mercado soporte completo de virtualizaci√≥n de hardware. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2009, Oracle asumi√≥ el control para desarrollar su propia l√≠nea de virtualizaci√≥n Oracle VM y expandirla en x86. </font><font style="vertical-align: inherit;">Antes de esto, Oracle VM solo se ofrec√≠a en la plataforma SPARC.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Innotek </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A principios de 2007, Innotek GmbH lanz√≥ el hipervisor de escritorio propietario de segundo tipo, VirtualBox, que es gratuito para uso no comercial. En el mismo a√±o, se lanz√≥ una versi√≥n de c√≥digo abierto. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2008, fue adquirida por Sun, que a su vez fue adquirida por Oracle. Oracle ha mantenido el uso gratuito del producto para fines no comerciales. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VirtualBox admite tres formatos de discos virtuales: VDI (nativo), VMDK (VMware), VHD (Microsoft). Como sistema operativo host, se admiten Windows, macOS, Linux, Solaris y OpenSolaris. La bifurcaci√≥n de VirtualBox para FreeBSD es conocida.</font></font><br><br><h3>  Ibm </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El mainframe es la computadora principal del centro de datos con una gran cantidad de memoria interna y externa (para referencia: en los a√±os 60, 1 MB de memoria se consideraba irrealmente grande). En realidad, el mainframe era un centro de c√≥mputo: las primeras computadoras ocupaban salas de m√°quinas enteras y consist√≠an en enormes bastidores. En estos d√≠as se llama centros de datos. Pero en los centros de datos en la misma sala de m√°quinas puede haber miles de computadoras, y en los albores de la tecnolog√≠a inform√°tica, una computadora ocupaba una sala completa. Cada bastidor vend√≠a un (!) Dispositivo de computadora (bastidores separados con memoria, bastidores separados con dispositivos de almacenamiento y dispositivos perif√©ricos por separado). El n√∫cleo de esta enorme m√°quina era un bastidor con un procesador, se llamaba main o mainframe.Despu√©s de cambiar a los circuitos integrados de transistores, el tama√±o de este milagro del pensamiento cient√≠fico y de ingenier√≠a disminuy√≥ significativamente, y la unidad central de IBM y sus an√°logos comenzaron a entenderse como la unidad central.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En los a√±os 60 del siglo XX, el alquiler de la potencia inform√°tica de todo el mainframe, sin mencionar su compra, cost√≥ mucho dinero. Muy pocas empresas e instituciones pueden permitirse ese lujo. El poder de c√≥mputo de arrendamiento era por hora (el prototipo del modelo moderno de pago por uso en nubes p√∫blicas, ¬øno?). El acceso a los inquilinos para la inform√°tica se otorg√≥ de forma secuencial. La soluci√≥n l√≥gica era paralelizar la carga computacional y aislar los c√°lculos de los inquilinos entre s√≠.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por primera vez, el Centro de Ciencias de Cambridge de IBM propuso la idea de aislar varias instancias de sistemas operativos en un mainframe basado en el mainframe IBM System / 360-67. El desarrollo se llam√≥ CP / CMS y, de hecho, fue el primer hipervisor y proporcion√≥ paravirtualizaci√≥n. CP (Programa de control): el propio hipervisor, que cre√≥ varias "m√°quinas virtuales" (VM) independientes. CMS (originalmente el Cambridge Monitor System, luego renombrado como Conversational Monitor System) era un sistema operativo liviano para un solo usuario. Curiosamente, CMS todav√≠a est√° vivo y todav√≠a se usa en la √∫ltima generaci√≥n de mainframes z / VM. Vale la pena se√±alar que en ese momento y hasta los a√±os 90, una m√°quina virtual significaba una separaci√≥n l√≥gica de discos f√≠sicos (se compart√≠an discos o dispositivos de almacenamiento,el hipervisor no proporcion√≥ almacenamiento para sus propias necesidades) con una pieza dedicada de memoria virtual y tiempo de procesador utilizando la tecnolog√≠a Time-Sharing. Las m√°quinas virtuales no proporcionaban interacci√≥n con la red, porque las m√°quinas virtuales de esa √©poca se refer√≠an a la computaci√≥n y el almacenamiento de datos, y no a la transferencia de ellos. En este sentido, las m√°quinas virtuales de esa √©poca eran m√°s como contenedores que m√°quinas virtuales en el sentido moderno.</font></font><br><br>      CP/CMS   VM/370     System/370 2  1972 .       ‚Äì VM,       VM     IBM.      ,          (           ) ‚Äì         VM/370.  :         ()     System/370     VM/370     (   ! ‚Äì      ).        . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los a√±os 80 se pueden llamar con seguridad la "era del mainframe". VM fue un √©xito con los desarrolladores de sistemas operativos, se escribieron aplicaciones para ello y se hicieron c√°lculos. Esta fue la d√©cada en que la proporci√≥n de bases de datos dominadas por el sistema operativo VM comenz√≥ a prevalecer en los mainframes. Uno de los cambios m√°s importantes fue el recurso de acceso de partici√≥n l√≥gica (LPAR), que en realidad proporcion√≥ dos niveles de virtualizaci√≥n. Los clientes ahora pueden usar el mismo conjunto de procesadores, dispositivos de E / S y m√≥dems en sistemas VM que se ejecutan en diferentes LPAR y permiten que los recursos se migren de un sistema VM a otro. Esto permiti√≥ a las organizaciones de TI ofrecer un rendimiento constante al procesar picos de carga de trabajo. Para racionalizar la creciente base de clientes, VM se dividi√≥ en tres productos separados,disponible a finales de los 80:</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VM / SP: el sistema operativo de virtualizaci√≥n multiprop√≥sito habitual para servidores System z </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HPO (Opci√≥n de alto rendimiento): VM / SP de alto rendimiento para modelos anteriores de servidor System z </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VM / XA (arquitectura extendida): variante VM compatible con la arquitectura S / S extendida 370</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A principios de los 90, la simplicidad y la conveniencia de la arquitectura x86 se volvieron m√°s atractivas para los clientes, y los mainframes estaban perdiendo relevancia r√°pidamente. </font><font style="vertical-align: inherit;">Los mainframes han sido reemplazados por sistemas de cl√∫ster, como el grunge, que reemplaz√≥ al glam metal al mismo tiempo. </font><font style="vertical-align: inherit;">Sin embargo, para una determinada clase de tareas, por ejemplo, al construir un almac√©n de datos centralizado, los mainframes se justifican tanto en t√©rminos de productividad como desde un punto de vista econ√≥mico. </font><font style="vertical-align: inherit;">Por lo tanto, algunas empresas a√∫n usan mainframes en sus infraestructuras, y IBM dise√±a, lanza y da soporte a las nuevas generaciones.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Linux Xen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Xen (pronunciado zen) es un hipervisor desarrollado en el Laboratorio de Computaci√≥n de la Universidad de Cambridge bajo la direcci√≥n de Ian Pratt y distribuido bajo la GPL. </font><font style="vertical-align: inherit;">La primera versi√≥n p√∫blica apareci√≥ en 2003. </font><font style="vertical-align: inherit;">Posteriormente, Ian continu√≥ trabajando en el hipervisor en su versi√≥n comercial, estableciendo la empresa XenSource. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2013, Xen qued√≥ bajo el control de la Fundaci√≥n Linux.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> XenSource </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tras haber existido durante varios a√±os en el mercado con los productos XenServer y XenEnterprise, a finales de 2007 fue adquirida por Citrix. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Citrix XenServer </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Habiendo absorbido XenSource por $ 500 millones, Citrix no pudo comercializar el problema. </font><font style="vertical-align: inherit;">M√°s precisamente, realmente no intent√© hacerlo, sin considerar a XenServer como el producto principal, y confiando en lo barato de las licencias permanentes. </font><font style="vertical-align: inherit;">Despu√©s de ventas francamente infructuosas en medio del exitoso VMware ESX, se decidi√≥ lanzar XenServer al mundo de forma gratuita y con c√≥digo abierto completo en 2009. </font><font style="vertical-align: inherit;">Sin embargo, el c√≥digo del sistema de administraci√≥n patentado de XenCenter no se abri√≥. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cabe se√±alar una interesante coincidencia cronol√≥gica de las iniciativas de Citrix y Microsoft en el campo de la virtualizaci√≥n industrial, a pesar de que las empresas siempre estuvieron muy unidas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A pesar de su nombre comercial com√∫n, Citrix XenApp y XenDesktop no tienen nada que ver con el hipervisor Xen.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Amazonas </font></font></h4><br> Amazon      IaaS   EC2 (Elastic Compute)  2006 .   EC2   Xen,   Amazon     ,          ,         . <br><br>  2017       EC2  KVM   .  ,       EC2  KVM   . <br><br><h3> Linux QEMU / KVM </h3><br> QEMU (Quick EMUlator) ‚Äî        ,    GPL v2.  x86   ARM, MIPS, RISC-V, PowerPC, SPARC, SPARC64.       QEMU   ,    .    QEMU  x86    ,          KVM (Kernel-based Virtual Machine)  Qumranet. <br><br>  KVM ‚Äî  QEMU KVM,       qcow2 (QEMU copy-on-write 2)       KVM. <br>   ,   QEMU     , QEMU / KVM    . <br><br><h3> Qumranet </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una compa√±√≠a israel√≠, un antiguo desarrollador y patrocinador principal del hipervisor KVM y el protocolo SPICE. Fundada en 2005, gan√≥ fama despu√©s de incorporar KVM en el kernel de Linux. 4 de septiembre de 2008, adquirido por Red Hat.</font></font><br><br><h3>  Sombrero rojo </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como todos los fabricantes de distribuci√≥n GNU / Linux, hasta 2010, Red Hat ten√≠a soporte incorporado para el hipervisor Xen en sus distribuciones. </font><font style="vertical-align: inherit;">Sin embargo, al ser un jugador importante en el mercado y una marca seria, pens√© en mi propia implementaci√≥n del hipervisor. </font><font style="vertical-align: inherit;">La base fue tomada por el hipervisor KVM poco notable pero prometedor. </font><font style="vertical-align: inherit;">La primera versi√≥n de Red Hat Enterprise Virtualization 2.2 (RHEV) se introdujo en 2010 con la pretensi√≥n de competir por una parte del mercado de soluciones VDI con Citrix y VMware debido al desarrollo de Qumranet, que se adquiri√≥ dos a√±os antes. </font><font style="vertical-align: inherit;">Fuera de la caja, estaban disponibles cl√∫steres de alta disponibilidad, migraci√≥n en vivo y herramientas de migraci√≥n M2M (solo RHEL). </font><font style="vertical-align: inherit;">Es de destacar que, a juzgar por la documentaci√≥n de esa √©poca, Red Hat retuvo la notaci√≥n Xen al describir la arquitectura de la soluci√≥n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El 28 de octubre de 2018, IBM anunci√≥ la compra de Red Hat. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> OpenStack </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hist√≥ricamente, el proyecto OpenStack surgi√≥ como una iniciativa para contrastar algo con el monopolio real de VMware en el campo de la virtualizaci√≥n de servidores pesados ‚Äã‚Äãx86. </font><font style="vertical-align: inherit;">El proyecto apareci√≥ en 2010 gracias a los esfuerzos conjuntos de Rackspace Hosting (un proveedor de la nube) y la NASA (que abri√≥ el c√≥digo para su propia plataforma Nebula). </font><font style="vertical-align: inherit;">El picante de la situaci√≥n se debi√≥ al hecho de que en 2012 VMware se uni√≥ a la gesti√≥n del proyecto OpenStack y caus√≥ una ola de indignaci√≥n entre los activistas fundadores. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Con el tiempo, Canonical (Ubuntu Linux), Debian, SUSE, Red Hat, HP, Oracle se unieron al proyecto. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, no todo fue sencillo. </font><font style="vertical-align: inherit;">En 2012, la NASA abandon√≥ el proyecto, optando por AWS. </font><font style="vertical-align: inherit;">A principios de 2016, HPE cerr√≥ por completo su proyecto Helion basado en OpenStack.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como parte del proyecto OpenStack, KVM ha sido adoptado como el hipervisor est√°ndar. Sin embargo, debido a la modularidad del enfoque, se puede implementar un sistema basado en OpenStack utilizando otros hipervisores, dejando, por ejemplo, solo un sistema de control de OpenStack. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Existe una amplia gama de opiniones con respecto al proyecto OpenStack, desde el culto entusiasta hasta el escepticismo serio y las duras cr√≠ticas. La cr√≠tica no carece de raz√≥n: se registr√≥ un n√∫mero significativo de problemas y p√©rdidas de datos al usar OpenStack. Lo que, sin embargo, no impide que los fan√°ticos nieguen todo y se refieran a la curvatura en la implementaci√≥n y operaci√≥n de los sistemas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El proyecto OpenStack no se limita √∫nicamente a la virtualizaci√≥n, sino que con el tiempo se ha convertido en un n√∫mero significativo de varios subproyectos y componentes para la expansi√≥n en el √°rea de la pila de servicios en la nube p√∫blica. </font><font style="vertical-align: inherit;">Adem√°s, la importancia de OpenStack probablemente deber√≠a evaluarse con precisi√≥n en esta parte: estos componentes se han convertido en clave en muchos productos y sistemas comerciales tanto en el campo de la virtualizaci√≥n como m√°s all√°. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En Rusia, OpenStack fuera de las nubes p√∫blicas es conocido principalmente por su papel en la sustituci√≥n de importaciones. </font><font style="vertical-align: inherit;">OpenStack incluye la mayor parte de las soluciones y productos de virtualizaci√≥n, incluidos los sistemas hiperconvergentes, con diversos grados de refinamiento.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nutanix AHV </font></font></h4><br> Nutanix          VMware vSphere.   -      ,  -      VMware       ,           .       KVM,       AHV (Acropolis HyperVisor). <br><br><h4>  Paralelos </h4><br>  7  Virtuozzo       KVM. <br><br><h4> Proxmox </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Proxmox VE (Virtual Environment) es un proyecto de c√≥digo abierto de la compa√±√≠a austriaca Proxmox Server Solutions GmbH basado en Debian Linux. </font><font style="vertical-align: inherit;">El primer lanzamiento fue en 2008. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El producto admite la virtualizaci√≥n de contenedores LXC (anteriormente OpenVZ) y la virtualizaci√≥n completa con el hipervisor KVM.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Paralelos / Virtuozzo / Rosplatform </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fundada en 1999 por Sergey Belousov, SWsoft tom√≥ el software de gesti√≥n de hosting. En 2003, se adquiri√≥ la compa√±√≠a rival de Novosibirsk, Plesk. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2004, SWsoft adquiri√≥ la compa√±√≠a rusa Parallels Nikolai Dobrovolsky con su producto Parallels Workstation (hipervisor de escritorio del segundo tipo en Windows). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La compa√±√≠a combinada conserva su nombre Parallels y pronto explotar√° el mercado con Parallels Desktop para Mac (hipervisor de escritorio del segundo tipo para MacOS).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como parte de la virtualizaci√≥n del servidor, el enfoque contin√∫a en el alojamiento de proveedores y centros de datos, en lugar del uso corporativo. Debido a las caracter√≠sticas espec√≠ficas de este mercado en particular, los contenedores Virtuozzo y OpenVZ, en lugar de las m√°quinas virtuales del sistema, se convirtieron en el producto clave. Posteriormente, Parallels, sin mucho √©xito, est√° tratando de ingresar al mercado de virtualizaci√≥n de servidores empresariales con el producto Parallels Bare Metal Server (posteriormente Parallels Hypervisor y Cloud Server, y luego Virtuozzo), agrega hiperconvergencia con su Cloud Storage. El trabajo contin√∫a en la automatizaci√≥n y orquestaci√≥n de proveedores de hosting.</font></font><br><br>  2015           ‚Äî  (    )   Virtuozzo,         .       Depo  IBS     -. <br><br>   7 Virtuozzo    ,  7      KVM. ,  ‚Äî     KVM. <br>   ,        2019 . <br><br> Parallels Desktop    Parallels   Corel.      Odin   IngramMicro.      Virtuozzo / . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/474776/">https://habr.com/ru/post/474776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../474760/index.html">Fijamos ngx-translate en la aplicaci√≥n angular. Tutorial pr√°ctico</a></li>
<li><a href="../474762/index.html">Seminario: Soluciones de TI h√≠bridas para empresas. 14 de noviembre, Mosc√∫</a></li>
<li><a href="../474768/index.html">Transmisi√≥n abierta del Main Hall HighLoad ++ 2019</a></li>
<li><a href="../474770/index.html">C√≥mo llevamos a cabo las pruebas de regresi√≥n de n√≥mina en SAP HCM</a></li>
<li><a href="../474772/index.html">Una startup que us√≥ IA para desarrollar una cura en 21 d√≠as</a></li>
<li><a href="../474782/index.html">Descripci√≥n general de la tecnolog√≠a de s√≠ntesis de voz</a></li>
<li><a href="../474784/index.html">Arcade Stick Story</a></li>
<li><a href="../474788/index.html">Organizaci√≥n de rutas en Laravel</a></li>
<li><a href="../474790/index.html">Negociador Cuentos</a></li>
<li><a href="../474792/index.html">6-8 de diciembre - Rosbank Tech.Madness Hackathon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>