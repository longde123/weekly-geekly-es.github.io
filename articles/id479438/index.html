<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕦 💤 🦂 Postgres-Tuesday # 5: “PostgreSQL dan Kubernetes. CI / CD. Otomasi Uji » 💔 🤛🏼 🏒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pada akhir tahun lalu, siaran langsung lain komunitas Rusia PostgreSQL #RuPostgres terjadi , di mana co-founder-nya Nikolai Samokhvalov berbicara deng...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Postgres-Tuesday # 5: “PostgreSQL dan Kubernetes. CI / CD. Otomasi Uji »</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/479438/"><img src="https://habrastorage.org/webt/qm/rm/ln/qmrmlnm8gjj_8gih4dzhy2ybrvy.jpeg"><br><br>  Pada akhir tahun lalu, siaran langsung lain komunitas Rusia PostgreSQL <a href="https://www.meetup.com/postgresqlrussia/">#RuPostgres terjadi</a> , di mana co-founder-nya Nikolai Samokhvalov berbicara dengan direktur teknis Flanta Dmitry Stolyarov tentang DBMS ini dalam konteks Kubernetes. <br><br>  Kami menerbitkan transkrip dari badan utama diskusi ini, dan video lengkap telah diposting <a href="https://www.youtube.com/channel/UC0SBGSNmBLrTZIkbN-lJHnw">di saluran YouTube komunitas</a> : <a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qXc9VTr4TFc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Basis data dan Kubernet </h2><br>  <i><b>NS</b> : Kami tidak akan membicarakan VACUUM dan CHECKPOINTs hari ini.</i>  <i>Kami ingin berbicara tentang Kubernetes.</i>  <i>Saya tahu bahwa Anda memiliki pengalaman bertahun-tahun.</i>  <i>Saya menonton video Anda dan bahkan meninjau beberapa bagian ... Mari kita mulai: mengapa Postgres atau MySQL di K8s?</i> <br><br>  <b>DS</b> : Tidak ada jawaban tunggal untuk pertanyaan ini dan tidak mungkin.  Tetapi secara umum, itu adalah kesederhanaan dan kenyamanan ... potensi.  Lagi pula, semua orang menginginkan layanan yang dikelola. <br><br>  <i><b>NS</b> : <a href="https://aws.amazon.com/rds/">Suka RDS</a> , hanya di rumah?</i> <br><br>  <b>DS</b> : Ya: menyukai RDS, hanya di mana saja. <br><br>  <i><b>NS</b> : "Di mana saja" adalah poin yang bagus.</i>  <i>Di perusahaan besar, semuanya terletak di tempat yang berbeda.</i>  <i>Dan mengapa, jika ini adalah perusahaan besar, jangan mengambil solusi yang sudah jadi?</i>  <i>Misalnya, Nutanix memiliki perkembangannya sendiri, sementara perusahaan lain (VMware ...) memiliki "RDS, hanya di rumah" yang sama.</i> <br><br>  <b>DS</b> : Tetapi kita berbicara tentang satu implementasi yang hanya akan bekerja dalam kondisi tertentu.  Dan jika kita berbicara tentang Kubernetes, maka ada banyak sekali infrastruktur (yang bisa di K8).  Ini pada dasarnya adalah standar untuk API ke cloud ... <br><br>  <i><b>NS</b> : Gratis juga!</i> <br><br>  <b>DS</b> : Ini tidak begitu penting.  Gratis tidak penting untuk segmen pasar yang sangat besar.  Hal lain yang penting ... Anda mungkin ingat laporan " <a href="https://habr.com/ru/company/flant/blog/431500/">Database dan Kubernetes</a> "? <br><br>  <i><b>NS</b> : Ya.</i> <br><br>  <b>DS</b> : Saya menyadari bahwa dia dianggap sangat ambigu.  Beberapa orang berpikir bahwa saya berkata: "Teman-teman, kami pergi semua database ke Kubernet!", Sementara yang lain memutuskan bahwa mereka semua sepeda yang mengerikan.  Dan saya ingin mengatakan sesuatu yang lain sama sekali: “Lihatlah apa yang terjadi, apa masalahnya dan bagaimana mereka dapat diselesaikan.  Sekarang pergi pangkalan di Kubernetes?  Produksi?  Ya, hanya jika Anda suka ... melakukan hal-hal tertentu.  Tetapi bagi dev, saya dapat mengatakan bahwa saya merekomendasikannya.  Bagi dev, menciptakan / menghapus lingkungan secara dinamis sangat penting. ” <br><br>  <i>NS: By dev, maksud Anda semua lingkungan yang bukan prod?</i>  <i>Pementasan, QA ...</i> <br><br>  <b>DS</b> : Jika kita berbicara tentang perf stand, maka mungkin belum, karena persyaratannya spesifik di sana.  Jika kita berbicara tentang kasus-kasus khusus di mana database yang sangat besar diperlukan untuk pementasan, maka mungkin tidak terlalu ... Jika ini adalah lingkungan statis, berumur panjang, lalu apa manfaatnya memiliki basis yang terletak di K8? <br><br>  <i><b>NS</b> : Tidak ada.</i>  <i>Tetapi di mana kita melihat lingkungan statis?</i>  <i>Lingkungan statis sudah usang besok.</i> <br><br>  <b>DS</b> : Pementasan bisa statis.  Kami memiliki klien ... <br><br>  <i><b>NS</b> : Ya, saya juga punya.</i>  <i>Masalah besar adalah jika Anda memiliki basis 10 TB dan pementasan - 200 GB ...</i> <br><br>  <b>DS</b> : Saya punya case yang sangat keren!  Pada pementasan ada basis prod'ovy di mana perubahan dilakukan.  Dan tombol disediakan: "mulai produksi".  Perubahan ini - delta - ditambahkan (tampaknya, mereka hanya disinkronkan oleh API) dalam produksi.  Ini adalah pilihan yang sangat eksotis. <br><br>  <i><b>NS</b> : Saya melihat startup di Valley yang duduk di RDS, atau bahkan di Heroku - ini adalah kisah 2-3 tahun yang lalu - dan mereka mengunduh dump ke laptop mereka.</i>  <i>Karena basisnya hanya 80 GB sejauh ini, dan ada tempat di laptop.</i>  <i>Kemudian mereka membeli disk untuk semua orang, sehingga mereka memiliki 3 pangkalan, sehingga mereka dapat melakukan pengembangan yang berbeda.</i>  <i>Ini juga terjadi.</i>  <i>Saya juga melihat bahwa mereka tidak takut untuk menyalin prod ke dalam pementasan - itu sangat tergantung pada perusahaan.</i>  <i>Tetapi dia melihat bahwa mereka sangat takut, dan seringkali mereka tidak punya cukup waktu dan tangan.</i>  <i>Tetapi sebelum kita beralih ke topik ini, saya ingin mendengar tentang Kubernetes.</i>  <i>Saya mengerti benar bahwa dalam prod'e sejauh ini tidak ada?</i> <br><br>  <b>DS</b> : Kami memiliki basis kecil di prod.  Kita berbicara tentang volume puluhan gigabyte dan layanan non-kritis, yang terlalu malas untuk membuat replika (dan tidak ada kebutuhan seperti itu).  Dan asalkan di bawah Kubernetes ada penyimpanan normal.  Basis data ini bekerja di mesin virtual - kondisional dalam VMware, di atas penyimpanan.  Kami menempatkannya di <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PV</a> dan sekarang kami dapat mentransfernya dari mobil ke mobil. <br><br>  <i><b>NS</b> : Pangkalan dengan ukuran ini, hingga 100 GB, pada disk yang bagus dan dengan jaringan yang bagus dapat diluncurkan dalam beberapa menit, bukan?</i>  <i>Kecepatan 1 GB per detik tidak lagi eksotis.</i> <br><br>  <b>DS</b> : Ya, untuk operasi linier ini bukan masalah. <br><br>  <i><b>NS</b> : Oke, kita seharusnya memikirkan prod saja.</i>  <i>Dan jika kita mempertimbangkan Kubernetes untuk lingkungan non-prod - bagaimana melakukannya?</i>  <i>Saya melihat bahwa di Zalando <a href="https://github.com/zalando/postgres-operator">mereka membuat operator</a> , di Crunchy mereka <a href="https://github.com/CrunchyData/postgres-operator">menggergaji</a> , ada beberapa opsi lain.</i>  <i>Dan ada <a href="https://ongres.com/">OnGres</a> - ini adalah teman baik kami Alvaro dari Spanyol: pada kenyataannya, mereka bukan hanya <a href="https://habr.com/ru/company/flant/blog/326414/">operator</a> , tetapi seluruh distribusi ( <a href="https://gitlab.com/ongresinc/stackgres">StackGres</a> ), di mana, selain Postgres sendiri, mereka juga memutuskan untuk mendorong cadangan, utusan Utusan ...</i> <br><br>  <b>DS</b> : Utusan untuk apa?  Perimbangan lalu lintas postgres tepatnya? <br><br>  <i><b>NS</b> : Ya.</i>  <i>Yaitu, mereka melihatnya sebagai: jika Anda mengambil distribusi Linux dan kernel, maka PostgreSQL yang biasa adalah kernel, dan mereka ingin membuat distribusi yang ramah cloud dan berjalan di Kubernetes.</i>  <i>Mereka merapat komponen (cadangan, dll.) Dan debug agar berfungsi dengan baik.</i> <br><br>  <b>DS</b> : Sangat keren!  Intinya, ini adalah perangkat lunak untuk membuat Postgres terkelola Anda. <br><br>  <i><b>NS</b> : Distribusi Linux memiliki masalah abadi: cara membuat driver sehingga semua perangkat keras didukung.</i>  <i>Dan mereka memiliki gagasan bahwa mereka akan bekerja di Kubernetes.</i>  <i>Saya tahu bahwa di operator Zalando kami baru-baru ini melihat bola mata pada AWS dan ini tidak terlalu baik.</i>  <i>Seharusnya tidak ada ikatan dengan infrastruktur tertentu - lalu apa gunanya?</i> <br><br>  <b>DS</b> : Saya tidak tahu dalam situasi spesifik apa Zalando terlibat, tetapi dalam penyimpanan Kubernetes sekarang dibuat sedemikian rupa sehingga tidak mungkin untuk menghapus cadangan disk dengan cara yang umum.  Baru-baru ini, standar - dalam versi terbaru dari <a href="https://habr.com/ru/company/flant/blog/465417/">spesifikasi CSI</a> - membuat kemungkinan snapshot, tetapi di mana itu diterapkan?  Jujur, ini masih mentah ... Kami mencoba CSI di atas AWS, GCE, Azure, vSphere, tetapi kami mulai menggunakannya sedikit, karena Anda dapat melihat bahwa itu belum siap. <br><br>  <i><b>NS</b> : Karena itu, terkadang Anda harus terikat dengan infrastruktur.</i>  <i>Saya pikir ini masih tahap awal - masalah pertumbuhan.</i>  <i>Pertanyaan: apa yang akan Anda rekomendasikan untuk pemula yang ingin mencoba PgSQL di K8s?</i>  <i>Operator yang mana mungkin?</i> <br><br>  <b>DS</b> : Masalahnya bagi kami Postgres adalah 3%.  Kami masih memiliki daftar perangkat lunak yang sangat besar di Kubernetes, saya bahkan tidak akan menuliskan semuanya.  Misalnya, Elasticsearch.  Ada banyak operator: ada yang berkembang aktif, ada yang tidak.  Bagi kami sendiri, kami membuat persyaratan yang harus ada di operator, sehingga kami menganggapnya serius.  Operator ini khusus untuk Kubernetes - bukan “operator untuk melakukan sesuatu dalam kondisi Amazon ..." Faktanya, kami menggunakan satu operator yang cukup luas (= untuk hampir semua klien) - <a href="https://github.com/spotahome/redis-operator">untuk Redis</a> <i>(kami akan segera menerbitkan artikel tentang hal itu)</i> . <br><br>  <i><b>NS</b> : Tapi untuk MySQL juga?</i>  <i>Saya tahu Percona ... karena mereka sekarang terlibat dalam MySQL, MongoDB, dan Postgres, mereka harus memiliki semacam universal gash: untuk semua database, untuk semua penyedia cloud.</i> <br><br>  <b>DS</b> : Kami tidak punya waktu untuk melihat pernyataan untuk MySQL.  Bagi kami, ini bukan fokus utama sekarang.  MySQL berfungsi dengan baik di standalone.  Mengapa operator, jika Anda baru saja memulai database ... Anda dapat memulai wadah Docker dengan Postrges, atau Anda dapat memulainya dengan cara yang sederhana. <br><br>  <i><b>NS</b> : Ini juga pertanyaan.</i>  <i>Tidak ada operator sama sekali?</i> <br><br>  <b>DS</b> : Ya, 100% dari kita menjalankan PostgreSQL tanpa operator.  Sejauh ini.  Kami secara aktif menggunakan operator untuk Prometheus, untuk Redis.  Kami memiliki rencana untuk menemukan operator untuk Elasticsearch - ini yang paling terbakar karena kami ingin menginstalnya dalam 100% kasus di Kubernetes.  Sama seperti kami ingin memastikan bahwa MongoDB selalu diinstal di Kubernetes juga.  Daftar Keinginan tertentu muncul di sini - ada perasaan bahwa dalam kasus ini sesuatu dapat dilakukan.  Dan tentang Postgres kami bahkan tidak melihat.  Tentu saja, kita tahu tentang adanya opsi yang berbeda, tetapi sebenarnya kita memiliki standalone. <br><br><h2>  Menguji Basis Data di Kubernetes </h2><br>  <i><b>NS</b> : Mari kita beralih ke topik pengujian.</i>  <i>Cara meluncurkan perubahan dalam database - dari sudut pandang perspektif DevOps.</i>  <i>Ada layanan microser, banyak database, sepanjang waktu sesuatu berubah.</i>  <i>Cara memastikan CI / CD normal sehingga semuanya dalam urutan dari posisi DBMS.</i>  <i>Apa pendekatanmu?</i> <br><br>  <b>DS</b> : Tidak ada jawaban.  Ada beberapa opsi.  Yang pertama adalah ukuran dasar yang ingin kita roll out.  Anda sendiri menyebutkan bahwa perusahaan memiliki sikap yang berbeda untuk memiliki salinan basis produk pada dev dan stage. <br><br>  <i><b>NS</b> : Dan dalam hal GDPR, saya pikir mereka semakin rapi ... Saya dapat mengatakan bahwa di Eropa mereka sudah mulai baik-baik saja.</i> <br><br>  <b>DS</b> : Tetapi Anda sering dapat menulis perangkat lunak yang membuang produksi dan mengaburkannya.  Ternyata data prod'ovye (snapshot, dump, binary copy ...), tetapi mereka anonim.  Sebagai gantinya, mungkin ada skrip generasi: bisa berupa skrip atau hanya skrip yang menghasilkan basis data besar.  Masalahnya adalah apa: berapa lama gambar dasar diperlukan untuk dibuat?  Dan berapa banyak waktu untuk menggunakannya di lingkungan yang tepat? <br><br>  Kami sampai pada skema: jika klien memiliki dataset fixture (versi minimum dari database), maka secara default kami menggunakannya.  Jika kita berbicara tentang lingkungan ulasan, ketika kita membuat cabang, kita telah menggunakan contoh aplikasi - kita meluncurkan basis data kecil di sana.  Tapi <a href="https://habr.com/ru/company/flant/blog/417509/">pilihannya</a> ternyata baik, ketika kita membuang dump dari produksi sekali sehari (pada malam hari) dan mengumpulkan atas dasar wadah Docker dengan PostgreSQL dan MySQL dengan data yang dimuat ini.  Jika Anda perlu menggunakan basis 50 kali dari gambar ini, ini dilakukan cukup sederhana dan cepat. <br><br>  <i><b>NS</b> : <b>Menyalin</b> sederhana?</i> <br><br>  <b>DS</b> : Data disimpan langsung di gambar Docker.  Yaitu  kami memiliki gambar yang sudah jadi, meskipun 100 GB.  Berkat lapisan di Docker, kami dapat dengan cepat menyebarkan gambar ini sebanyak yang diperlukan.  Metodenya bodoh, tetapi berhasil dengan cukup baik. <br><br>  <i><b>NS</b> : Lebih lanjut, saat pengujian, itu berubah tepat di dalam Docker, kan?</i>  <i>Copy-on-write di dalam Docker - membuangnya dan pergi lagi, semuanya baik-baik saja.</i>  <i>Kelas!</i>  <i>Dan Anda sudah menggunakannya dengan kekuatan dan main?</i> <br><br>  <b>DS</b> : Untuk waktu yang lama. <br><br>  <i><b>NS</b> : Kami melakukan hal yang sangat mirip.</i>  <i>Hanya kami tidak menggunakan copy-on-write Docker, tetapi beberapa lagi.</i> <br><br>  <b>JS</b> : Dia bukan generik.  Dan Docker'ny bekerja di mana-mana. <br><br>  <i><b>NS</b> : Secara teori, ya.</i>  <i>Tetapi kami juga memiliki modul di sana, Anda dapat membuat berbagai modul dan bekerja dengan sistem file yang berbeda.</i>  <i>Beberapa saat.</i>  <i>Dari Postgres, kami melihat semua ini secara berbeda.</i>  <i>Sekarang saya melihat dari sisi Docker dan melihat bahwa semuanya bekerja untuk Anda.</i>  <i>Tetapi jika database sangat besar, misalnya, 1 TB, maka ini semua panjang: baik operasi di malam hari dan segala sesuatu di Docker ... Dan jika 5 TB diisi di Docker ... Atau apakah semuanya normal?</i> <br><br>  <b>DS</b> : Apa bedanya: itu gumpalan, hanya bit dan byte. <br><br>  <i><b>NS</b> : Perbedaannya adalah ini: apakah Anda melakukan ini melalui dump dan restore?</i> <br><br>  <b>DS</b> : Sama sekali tidak perlu.  Metode untuk menghasilkan gambar ini bisa berbeda. <br><br>  <i><b>NS</b> : Untuk beberapa klien, kami membuatnya sehingga alih-alih secara teratur menghasilkan gambar dasar, kami terus memperbaruinya.</i>  <i>Ini pada dasarnya adalah replika, tetapi data tidak diterima langsung dari master, tetapi melalui arsip.</i>  <i>Arsip biner, tempat WAL digulirkan setiap hari, cadangan juga dihapus di sana ... WAL ini kemudian terbang - dengan sedikit penundaan (harfiah 1-2 detik) - ke gambar dasar.</i>  <i>Kami mengkloningnya dengan cara apa pun - sekarang kami memiliki ZFS secara default.</i> <br><br>  <b>DS</b> : Tetapi dengan ZFS Anda terbatas pada satu simpul. <br><br>  <i><b>NS</b> : Ya.</i>  <i>Tetapi ZFS juga memiliki <a href="https://docs.oracle.com/cd/E18752_01/html/819-5461/gbchx.html">pengiriman</a> ajaib: Anda dapat mengirim snapshot dengannya dan bahkan (saya belum benar-benar mengujinya, tapi ...) Anda dapat mengirim delta antara dua <code>PGDATA</code> .</i>  <i>Faktanya, kami memiliki alat lain yang tidak kami pertimbangkan secara khusus untuk tugas-tugas tersebut.</i>  <i>PostgreSQL memiliki <a href="https://www.postgresql.org/docs/12/app-pgrewind.html">pg_rewind</a> , yang berfungsi sebagai rsync "pintar", melewatkan banyak hal yang tidak perlu Anda tonton, karena tidak ada yang berubah di sana pasti.</i>  <i>Kita dapat melakukan sinkronisasi cepat antara dua server dan mundur dengan cara yang persis sama.</i> <br><br>  <i>Jadi, kami mencoba ini, lebih banyak DBA'noy, pihak untuk membuat alat yang memungkinkan Anda untuk melakukan hal yang sama seperti yang Anda katakan: kami memiliki satu basis, tetapi kami ingin menguji sesuatu 50 kali, hampir pada saat yang sama.</i> <br><br>  <b>DS</b> : 50 kali berarti Anda perlu memesan 50 instance Spot. <br><br>  <i><b>NS</b> : Tidak, kami melakukan semuanya dengan satu mesin.</i> <br><br>  <b>DS</b> : Tapi bagaimana Anda menggunakan 50 kali jika basis yang satu ini, katakanlah, satu terabyte.  Kemungkinan besar dia membutuhkan 256 GB RAM bersyarat? <br><br>  <i><b>NS</b> : Ya, kadang-kadang banyak memori diperlukan - ini normal.</i>  <i>Tapi contoh seperti itu dari kehidupan.</i>  <i>Mesin produksi memiliki 96 core dan 600 GB.</i>  <i>Pada saat yang sama, 32 core digunakan untuk database (bahkan 16 core kadang-kadang sekarang digunakan) dan memori 100-120 GB.</i> <br><br>  <b>DS</b> : Dan 50 salinan ada di sana? <br><br>  <i><b>NS</b> : Jadi hanya ada satu salinan, lalu copy-on-write (ZFS'ny) berfungsi ... Saya akan memberi tahu Anda lebih banyak.</i> <br><br>  <i>Misalnya, kami memiliki basis 10 TB.</i>  <i>Mereka membuat disk untuk itu, ZFS masih diperas ukurannya 30-40.</i>  <i>Karena kami tidak melakukan pengujian beban, waktu respons yang tepat tidak penting bagi kami: biarkan hingga 2 kali lebih lambat - tidak apa-apa.</i> <br><br>  <i>Kami mengaktifkan programmer, QA, DBA, dll.</i>  <i>Lakukan pengujian dalam 1-2 utas.</i>  <i>Misalnya, mereka dapat memulai beberapa jenis migrasi.</i>  <i>Tidak memerlukan 10 core sekaligus - perlu 1 postgres backend, 1 core.</i>  <i>Migrasi akan dimulai - mungkin <a href="https://www.postgresql.org/docs/12/routine-vacuuming.html">autovacuum</a> masih <a href="https://www.postgresql.org/docs/12/routine-vacuuming.html">akan</a> mulai, kemudian inti kedua diaktifkan.</i>  <i>Kami telah mengalokasikan 16-32 core, sehingga 10 orang dapat bekerja secara bersamaan, tidak ada masalah.</i> <br><br>  <i>Karena <code>PGDATA</code> fisik sama, ternyata kita sebenarnya membodohi Postgres.</i>  <i>Kuncinya adalah ini: dimulai, misalnya, 10 Postgres pada saat yang sama.</i>  <i>Masalah apa biasanya apa?</i>  <i>Mereka <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html">menempatkan shared_buffers</a> , katakanlah, 25%.</i>  <i>Dengan demikian, ini adalah 200 GB.</i>  <i>Anda tidak akan memulai lebih dari tiga, karena memori akan berakhir.</i> <br><br>  <i>Tetapi pada titik tertentu kami menyadari bahwa ini tidak perlu: kami menetapkan shared_buffers ke 2 GB.</i>  <i>PostgreSQL memiliki efektif_cache_size, dan pada kenyataannya hanya memengaruhi <a href="https://en.wikipedia.org/wiki/Query_plan">rencana</a> .</i>  <i>Kami menempatkannya di 0,5 Tb.</i>  <i>Dan itu tidak masalah bahwa mereka tidak benar-benar ada: dia membuat rencana seolah-olah mereka ada.</i> <br><br>  <i>Karena itu, ketika kami menguji beberapa jenis migrasi, kami dapat mengumpulkan semua rencana - kami akan melihat bagaimana itu akan terjadi dalam produksi.</i>  <i>Detik akan ada yang berbeda (lebih lambat), tetapi data yang benar-benar kita baca, dan rencana itu sendiri (seperti BERGABUNG, dll.) Diperoleh persis sama seperti pada produksi.</i>  <i>Dan secara paralel, Anda dapat menjalankan banyak pemeriksaan ini pada satu mesin.</i> <br><br>  <b>DS</b> : Apakah menurut Anda ada beberapa masalah?  Yang pertama adalah solusi yang hanya berfungsi pada PostgreSQL.  Pendekatan ini sangat pribadi, tidak generik.  Yang kedua - Kubernetes (dan di situlah cloud sekarang) melibatkan banyak node, dan node-node ini bersifat fana.  Dan dalam kasus Anda itu adalah node persisten stateful.  Hal-hal ini bertentangan dengan saya. <br><br>  <i><b>NS</b> : Pertama - Saya setuju, ini murni cerita Postgres.</i>  <i>Saya pikir jika kita memiliki IO langsung dan buffer pool untuk hampir semua memori, pendekatan ini tidak akan berhasil - akan ada rencana yang berbeda.</i>  <i>Tetapi kami hanya bekerja dengan Postgres untuk saat ini, kami tidak memikirkan orang lain.</i> <br><br>  <i>Tentang Kubernetes.</i>  <i>Anda sendiri selalu mengatakan bahwa kami memiliki basis yang gigih.</i>  <i>Jika instance crash, hal utama adalah menyimpan disk.</i>  <i>Di sini kita juga memiliki seluruh platform di Kubernetes, dan komponen dengan Postgres terpisah (walaupun suatu hari akan ada di sana).</i>  <i>Karena itu, semuanya begitu: instance jatuh, tetapi kami menyimpannya PV dan hanya terhubung ke instance (baru) lainnya, seolah-olah tidak ada yang terjadi.</i> <br><br>  <b>DS</b> : Dari sudut pandang saya, kami membuat pod di Kubernetes.  K8s - elastic: komponen dipesan sendiri sesuai kebutuhan.  Tugasnya adalah hanya membuat pod dan mengatakan bahwa ia membutuhkan sumber daya X, dan kemudian K8 akan mengetahuinya.  Tetapi dukungan penyimpanan di Kubernetes masih tidak stabil: di <a href="https://habr.com/ru/company/flant/blog/467477/">1.16</a> , di <a href="https://habr.com/ru/company/flant/blog/476998/">1.17</a> (rilis ini dirilis <i>minggu</i> lalu), fitur-fitur ini hanya menjadi beta. <br><br>  Enam bulan atau satu tahun akan berlalu - itu akan menjadi lebih atau kurang stabil, atau setidaknya akan dinyatakan demikian.  Maka kemungkinan snapshot dan ukurannya sudah menyelesaikan masalah Anda sepenuhnya.  Karena Anda punya basis.  Ya, ini mungkin tidak terlalu cepat, tetapi kecepatan tergantung pada apa yang "di bawah tenda," karena beberapa implementasi dapat menyalin dan menyalin pada tingkat subsistem disk. <br><br>  <i><b>NS</b> : Juga penting bagi semua mesin (Amazon, Google ...) untuk mulai mendukung versi ini - itu juga membutuhkan waktu.</i> <br><br>  <b>DS</b> : Meskipun kami tidak menggunakannya.  Kami menggunakan milik kami. <br><br><h2>  Pengembangan lokal di bawah Kubernetes </h2><br>  <i><b>NS</b> : Pernahkah Anda menemukan Wishlist seperti itu ketika Anda perlu menaikkan semua pod pada satu mesin dan melakukan sedikit pengujian.</i>  <i>Untuk mendapatkan bukti konsep dengan cepat, lihat bahwa aplikasi berfungsi di Kubernetes, tanpa mengalokasikan banyak mesin untuknya.</i>  <i>Apakah ada Minikube, kan?</i> <br><br>  <b>DS</b> : Bagi saya kasus ini - menyebar pada satu node - secara eksklusif tentang pengembangan lokal.  Atau beberapa manifestasi dari pola semacam itu.  Ada <a href="https://habr.com/ru/company/flant/blog/333470/">Minikube</a> , ada <a href="https://k3s.io/">k3</a> , <a href="https://github.com/kubernetes-sigs/kind">JENIS</a> .  Kita akan menggunakan Kubernetes IN Docker.  Sekarang mereka mulai bekerja dengannya untuk tes. <br><br>  <i><b>NS</b> : Dulu saya berpikir bahwa ini adalah upaya untuk membungkus semua pod dalam satu gambar Docker.</i>  <i>Tapi ternyata ini tentang hal lain.</i>  <i>Pokoknya, ada wadah yang terpisah, pod yang terpisah - hanya di Docker.</i> <br><br>  <b>DS</b> : Ya.  Dan ada tiruan yang agak lucu dilakukan, tetapi intinya adalah ... Kami memiliki alat penyebaran - <a href="https://werf.io/">werf</a> .  Kami ingin membuat mode di dalamnya - dengan syarat tidak <code>werf up</code> : "Naikkan saya Kubernet lokal".  Dan kemudian jalankan <code>werf follow</code> bersyarat <code>werf follow</code> .  Kemudian pengembang akan dapat mengedit dalam IDE, dan sebuah proses diluncurkan dalam sistem yang melihat perubahan dan menyusun kembali gambar, mengubahnya menjadi K8 lokal.  Jadi kami ingin mencoba menyelesaikan masalah pembangunan lokal. <br><br><h2>  Snapshots dan kloning basis data dalam realitas K8s </h2><br>  <i><b>NS</b> : Jika Anda kembali ke copy-on-write.</i>  <i>Saya perhatikan bahwa awan juga memiliki foto.</i>  <i>Mereka bekerja secara berbeda.</i>  <i>Misalnya, di GCP: Anda memiliki instance multi-terabyte di pantai timur Amerika Serikat.</i>  <i>Anda melakukan snapshot secara berkala.</i>  <i>Anda mengambil salinan disk di pantai barat dari sebuah snapshot - dalam beberapa menit semuanya sudah siap, ia bekerja sangat cepat, hanya cache yang perlu diisi dalam memori.</i>  <i>Tapi klon ini (snapshot) - untuk 'ketentuan' itu volume baru.</i>  <i>Ini bagus ketika Anda perlu membuat banyak contoh.</i> <br><br>  <i>Tetapi untuk tes, menurut saya, snapshot yang Anda bicarakan di Docker atau yang saya bicarakan di ZFS, btrfs dan bahkan LVM ... - mereka memungkinkan Anda untuk tidak membuat data yang benar-benar baru pada mesin yang sama.</i>  <i>Di awan, Anda masih harus membayar untuk mereka setiap waktu dan menunggu bukan menit, tetapi menit (dan dalam kasus <a href="https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-prewarming-data-into-volumes-created-snapshots/">beban malas</a> , mungkin berjam-jam).</i> <br><br>  <i>Alih-alih, Anda bisa mendapatkan data ini dalam satu atau dua detik, uji coba dan buang.</i>  <i>Snapshots ini memecahkan masalah yang berbeda.</i>  <i>Dalam kasus pertama - untuk skala dan mendapatkan replika baru, dan yang kedua - untuk tes.</i> <br><br>  <b>DS</b> : Saya tidak setuju.  Volume kloning biasanya adalah tugas cloud.  Saya tidak melihat implementasi mereka, tetapi saya tahu bagaimana kami melakukannya pada perangkat keras.  Kami memiliki Ceph, di dalamnya Anda dapat memberi tahu volume fisik apa pun ( <a href="https://docs.ceph.com/docs/master/rbd/">RBD</a> ) untuk <i>dikloning</i> dan mendapatkan volume kedua dengan karakteristik yang sama, <a href="https://en.wikipedia.org/wiki/IOPS">IOPS</a> , dll., Dalam puluhan milidetik.  Anda harus memahami bahwa ada copy-on-write yang rumit di dalamnya.  Mengapa cloud tidak melakukan hal yang sama?  Saya yakin mereka entah bagaimana mencoba melakukan ini. <br><br>  <i><b>NS</b> : Tetapi mereka masih akan membutuhkan detik, puluhan detik untuk meningkatkan instance, membawa Docker ke sana, dll.</i> <br><br>  <b>DS</b> : Mengapa perlu mengangkat seluruh instance?  Tapi kami memiliki instance untuk 32 core, untuk 16 ... dan entah bagaimana cocok ke dalamnya - misalnya, empat.  Saat kami memesan yang kelima, instance akan naik, dan kemudian akan dihapus. <br><br>  <i><b>NS</b> : Ya, yang menarik, Kubernetes memiliki cerita yang berbeda.</i>  <i>Database kami tidak dalam K8s, dan satu contoh.</i>  <i>Tetapi mengkloning basis data multi-terabyte tidak lebih dari dua detik.</i> <br><br>  <b>DS</b> : Itu keren.  Tetapi pesan awal saya adalah ini bukan solusi umum.  Ya, itu keren, tetapi hanya Postgres yang cocok dan hanya pada satu node. <br><br>  <i><b>NS</b> : Sangat cocok tidak hanya untuk Postgres: rencana-rencana ini, seperti yang saya jelaskan, hanya akan berfungsi seperti itu.</i>  <i>Tetapi jika Anda tidak repot dengan rencana, tetapi kami hanya membutuhkan semua data untuk pengujian fungsional, maka ini cocok untuk DBMS apa pun.</i> <br><br>  <b>DS</b> : Bertahun-tahun lalu kami melakukan ini pada snapshot LVM.  Ini klasik.  Pendekatan ini telah sangat aktif digunakan.  Hanya node stateful yang menyakitkan.  Karena mereka tidak perlu dijatuhkan, selalu ingat tentang mereka ... <br><br>  <i><b>NS</b> : Apakah Anda melihat kemungkinan hibrida di sini?</i>  <i>Katakanlah stateful adalah semacam pod, ini berfungsi untuk beberapa orang (banyak penguji).</i>  <i>Kami memiliki satu volume, tetapi berkat sistem file, klon adalah lokal.</i>  <i>Jika pod jatuh, disk tetap - pod naik, ia mempertimbangkan informasi tentang semua klon, mengembalikan semuanya dan berkata: "Ini klon Anda di port ini, mulai bekerja dengan mereka lebih jauh."</i> <br><br>  <b>DS</b> : Secara teknis, ini berarti bahwa di dalam Kubernetes, ini adalah satu pod, di dalamnya kita menjalankan banyak Postgres. <br><br>  <i><b>NS</b> : Ya.</i>  <i>Dia memiliki batas: misalkan, pada saat yang sama, tidak lebih dari 10 orang bekerja dengannya.</i>  <i>Jika Anda membutuhkan 20 - jalankan pod seperti kedua.</i>  <i>Kloning sepenuhnya realistis, setelah menerima volume penuh kedua, itu akan memiliki 10 klon "tipis" yang sama.</i>  <i>Tidak melihat peluang seperti itu?</i> <br><br>  <b>DS</b> : Kami harus menambahkan masalah keamanan di sini.  Opsi organisasi semacam itu menyiratkan bahwa pod ini memiliki kemampuan tinggi karena dapat melakukan operasi non-standar pada sistem file ... Tapi saya ulangi: Saya percaya bahwa dalam jangka menengah, penyimpanan akan diperbaiki di Kubernetes, keseluruhan cerita dengan volume akan diperbaiki di awan. - semuanya akan "hanya berfungsi."  Ini akan mengubah ukuran, mengkloning ... Ada volume - kita katakan: "Buat yang baru berdasarkan itu" - dan setelah satu setengah detik kita mendapatkan apa yang kita butuhkan. <br><br>  <i><b>NS</b> : Saya tidak percaya dalam satu setengah detik untuk banyak terabyte.</i>  <i>Di Ceph, Anda melakukannya sendiri, dan berbicara tentang awan.</i>  <i>Pergi ke cloud, di EC2, buat klon volume EBS banyak terabyte dan lihat bagaimana kinerjanya.</i>  <i>Tidak butuh beberapa detik.</i>  <i>Saya sangat tertarik ketika mereka mencapai indikator seperti itu.</i>  <i>Saya mengerti apa yang Anda bicarakan, tetapi biarkan saya tidak setuju.</i> <br><br>  <b>DS</b> : Ok, tapi saya katakan dalam jangka menengah, bukan jangka pendek.  Selama beberapa tahun. <br><br><h2>  Operator pro untuk PostgreSQL dari Zalando </h2><br>  Di tengah pertemuan ini, Alexey Klyukin, mantan pengembang dari Zalando, yang berbicara tentang sejarah operator PostgreSQL, juga bergabung dengannya: <br><br><blockquote>  Sangat menyenangkan bahwa secara umum topik ini disentuh: Postgres dan Kubernetes.  Ketika kami mulai melakukannya di Zalando pada 2017, itu adalah topik yang ingin dilakukan semua orang, tetapi tidak ada yang melakukannya.  Semua orang sudah memiliki Kubernetes, tetapi ketika ditanya apa yang harus dilakukan dengan database, bahkan orang-orang seperti <a href="https://github.com/kelseyhightower">Kelsey Hightower</a> yang memberitakan K8 mengatakan sesuatu seperti ini: <br><br>  <i>"Pergi ke layanan yang dikelola dan menggunakannya, jangan memulai database di Kubernetes.</i>  <i>Kalau tidak, K8 Anda akan memutuskan, misalnya, untuk memutakhirkan, mengeluarkan semua node, dan data Anda akan terbang jauh, jauh sekali. "</i> <br><br>  Kami memutuskan untuk membuat operator yang, berlawanan dengan saran ini, akan meluncurkan database Postgres di Kubernetes.  Dan kami memiliki fondasi yang bagus - <a href="https://github.com/zalando/patroni">Patroni</a> .  Ini adalah failover otomatis untuk PostgreSQL, dilakukan dengan benar, mis.  menggunakan etcd, consul atau ZooKeeper sebagai repositori untuk informasi cluster.  Repositori yang akan diberikan kepada semua orang yang bertanya, misalnya, pemimpin seperti apa sekarang, informasi yang sama - terlepas dari kenyataan bahwa kita telah mendistribusikan semuanya - sehingga tidak ada otak yang terpisah.  Plus, kami memiliki <a href="https://github.com/zalando/patroni/tree/master/docker">gambar Docker</a> untuknya. <br><br>  Secara umum, kebutuhan untuk failover otomatis di perusahaan muncul setelah migrasi dari pusat data besi internal ke cloud.  Cloud didasarkan pada solusi kepemilikan PaaS (Platform-as-a-Service).  Ini adalah Open Source, tetapi untuk meningkatkannya, Anda harus bekerja keras.  Itu disebut <a href="https://stups.io/">STUPS</a> . <br><br>  Awalnya, tidak ada Kubernet.  Lebih tepatnya, ketika solusi sendiri dikerahkan, K8 sudah, tetapi sangat kasar sehingga tidak cocok untuk produksi.  Itu menurut saya, 2015 atau 2016.  Pada 2017, Kubernet menjadi lebih atau kurang matang - ada kebutuhan untuk bermigrasi di sana. <br><br>  Dan kami sudah memiliki wadah buruh pelabuhan.  Ada PaaS yang menggunakan Docker.  Mengapa tidak mencoba K8?  Mengapa tidak menulis pernyataan Anda sendiri?  Murat Kabilov, yang datang kepada kami dari Avito, memulai ini sebagai proyek atas inisiatifnya sendiri - "bermain" - dan proyek "lepas landas". <br><br>  Tetapi secara umum, saya ingin berbicara tentang AWS.  Mengapa ada kode terkait AWS secara historis ... <br><br>  Ketika Anda menjalankan sesuatu di Kubernetes, Anda perlu memahami bahwa K8 adalah pekerjaan yang sedang berjalan.  Itu terus berkembang, meningkat dan bahkan secara berkala pecah.  Anda perlu memonitor semua perubahan di Kubernet dengan hati-hati, Anda harus siap untuk membenamkan diri di dalamnya, dan mencari tahu cara kerjanya secara detail - mungkin lebih dari yang Anda inginkan.  Pada prinsipnya, ini adalah platform apa pun di mana Anda menjalankan basis data ... <br><br>  Jadi, ketika kami melakukan pernyataan, kami memiliki Postgres, yang bekerja dengan volume eksternal (dalam hal ini, EBS, sejak kami bekerja di AWS).  Basis data tumbuh, pada beberapa titik itu perlu untuk mengubah ukuran: misalnya, ukuran asli EBS adalah 100 TB, basis data telah berkembang, sekarang kami ingin membuat EBS dalam 200 TB.  Bagaimana?  Misalkan Anda dapat melakukan dump / restore ke instance baru, tetapi ini lama dan dengan downtime. <br><br>  Oleh karena itu, saya ingin mengubah ukuran yang akan memperluas partisi EBS dan kemudian memberitahu sistem file untuk menggunakan ruang baru.  Dan kami melakukannya, tetapi pada saat itu Kubernetes tidak memiliki API untuk operasi pengubahan ukuran.  Karena kami bekerja pada AWS, kami menulis kode untuk API-nya. <br><br>  Tidak ada yang mau melakukan hal yang sama untuk platform lain.  Tidak ada komplikasi dalam pernyataan bahwa itu hanya dapat dijalankan pada AWS, dan itu tidak akan bekerja pada yang lainnya.  Secara umum, ini adalah proyek Open Source: jika ada yang ingin mempercepat kemunculan penggunaan API baru, kami dipersilakan.  Ada <a href="https://github.com/zalando/postgres-operator">GitHub</a> , tarik-permintaan - tim Zalando berusaha untuk dengan cepat merespons mereka dan mempromosikan operator.  Sejauh yang saya tahu, proyek ini <a href="https://summerofcode.withgoogle.com/archive/2019/organizations/6187982082539520/">berpartisipasi</a> dalam Google Summer of Code dan beberapa inisiatif serupa lainnya.  Zalando sangat aktif di dalamnya. <br></blockquote><br><h2>  Bonus PS! </h2><br>  Jika Anda tertarik dengan topik PostgreSQL dan Kubernetes, maka kami juga menarik perhatian pada fakta bahwa minggu lalu Postgres berikutnya terjadi, di mana <b>Alexander Kukushkin dari Zalando</b> berbicara dengan Nikolai.  Video darinya tersedia di <a href="https://www.youtube.com/watch%3Fv%3DFE0xi7SBqsg">sini</a> . <br><br><h2>  PPS </h2><br>  Baca juga di blog kami: <br><br><ul><li>  " <a href="https://habr.com/ru/company/flant/blog/431500/">Database dan Kubernetes (review dan laporan video)</a> "; </li><li>  “ <a href="https://habr.com/ru/company/flant/blog/475036/">Migrasi Cassandra ke Kubernetes: Fitur dan Solusi</a> ”; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/461149/">Migrasi MongoDB ke Kubernet yang tidak terhalang</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/450662/">Migrasi RabbitMQ yang tidak terhalang ke Kubernetes</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id479438/">https://habr.com/ru/post/id479438/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id479422/index.html">[Video animasi] Dunia kabel: bagaimana dalam 35 tahun jaringan kabel laut menyelimuti dunia</a></li>
<li><a href="../id479426/index.html">Security Week 50: Man-in-the-middle attacks di Confluence dan Linux</a></li>
<li><a href="../id479428/index.html">Acara digital di Moskow dari 9 hingga 15 Desember</a></li>
<li><a href="../id479430/index.html">Acara digital di St. Petersburg dari 9 hingga 15 Desember</a></li>
<li><a href="../id479432/index.html">Yandex.Maps: Saya pergi ke pengontrol kartu - Saya langsung mendapatkan posisi pengguna (oke, sekarang serius)</a></li>
<li><a href="../id479442/index.html">Alexey Savvateev: Game-theoretic model of social schism (+ nginx survey)</a></li>
<li><a href="../id479446/index.html">Mobil sudah di depan orang dalam tes membaca; tetapi apakah mereka mengerti apa yang mereka baca?</a></li>
<li><a href="../id479450/index.html">AppCode 2019.3: bekerja lebih cepat, memahami Swift lebih baik, tahu tentang Mac Catalyst, dengan mudah menampilkan pesan perakitan</a></li>
<li><a href="../id479452/index.html">Bagaimana Sistem Nama Domain Dikembangkan: Era ARPANET</a></li>
<li><a href="../id479458/index.html">Keindahan atau kepraktisan di ruang server</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>