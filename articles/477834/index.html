<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçüíª üìª üö£ Principios para construir sistemas de an√°lisis de transmisi√≥n üñáÔ∏è üöÑ üëâüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El dise√±o de an√°lisis de transmisi√≥n y sistemas de procesamiento de datos de transmisi√≥n tiene sus propios matices, sus propios problemas y su propia ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Principios para construir sistemas de an√°lisis de transmisi√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477834/"><img src="https://habrastorage.org/webt/rk/pz/c5/rkpzc5nw7uyyv0vsp_00trtag44.jpeg" alt="imagen"><br><br>  El dise√±o de an√°lisis de transmisi√≥n y sistemas de procesamiento de datos de transmisi√≥n tiene sus propios matices, sus propios problemas y su propia pila tecnol√≥gica.  Hablamos de esto en la pr√≥xima <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">lecci√≥n abierta</a> , realizada en la v√≠spera del lanzamiento del curso de <a href="https://otus.pw/IxY2/">Ingeniero de Datos</a> . <br><br>  En el seminario web discutido: <br><br><ul><li>  cuando se necesita procesamiento de transmisi√≥n; </li><li>  qu√© elementos est√°n en SPOD, qu√© herramientas podemos usar para implementar estos elementos; </li><li>  c√≥mo construir su propio sistema de an√°lisis de clickstream. </li></ul><br>  Profesor: <a href="https://otus.ru/teacher/370/">Yegor Mateshuk</a> , ingeniero de datos s√©nior en MaximaTelecom. <br><a name="habracut"></a><br><h3>  ¬øCu√°ndo se necesita la transmisi√≥n?  Stream vs Batch </h3><br>  En primer lugar, debemos determinar cu√°ndo necesitamos la transmisi√≥n y el procesamiento por lotes.  Expliquemos las fortalezas y debilidades de estos enfoques. <br><br>  <b>Entonces, las desventajas del procesamiento por lotes:</b> <br><br><ul><li>  los datos se entregan con retraso.  Dado que tenemos un cierto per√≠odo de c√°lculos, para este per√≠odo siempre nos quedamos atr√°s en tiempo real.  Y cuanto m√°s iteraci√≥n, m√°s nos quedamos atr√°s.  Por lo tanto, tenemos un retraso de tiempo, que en algunos casos es cr√≠tico; </li><li>  Se crea la carga m√°xima sobre el hierro.  Si calculamos un lote en modo por lotes, al final del per√≠odo (d√≠a, semana, mes) tenemos una carga m√°xima, porque necesita calcular muchas cosas.  ¬øA qu√© conduce esto?  Primero, comenzamos a descansar contra l√≠mites que, como saben, no son infinitos.  Como resultado, el sistema se ejecuta peri√≥dicamente hasta el l√≠mite, lo que a menudo resulta en fallas.  En segundo lugar, dado que todos estos trabajos comienzan al mismo tiempo, compiten y se calculan bastante lentamente, es decir, no puede contar con un resultado r√°pido. </li></ul><br>  <b>Pero el procesamiento por lotes tiene sus ventajas:</b> <br><br><ul><li>  Alta eficiencia.  No profundizaremos, ya que la eficiencia est√° asociada con la compresi√≥n, con los marcos y con el uso de formatos de columna, etc. El hecho es que el procesamiento por lotes, si toma la cantidad de registros procesados ‚Äã‚Äãpor unidad de tiempo, ser√° m√°s eficiente; </li><li>  facilidad de desarrollo y soporte.  Puede procesar cualquier parte de los datos mediante pruebas y recuentos seg√∫n sea necesario. </li></ul><br>  <b>Ventajas del procesamiento de transmisi√≥n de datos (transmisi√≥n):</b> <br><br><ul><li>  resultado en tiempo real.  No esperamos el final de ning√∫n per√≠odo: tan pronto como nos lleguen los datos (incluso una cantidad muy peque√±a), podemos procesarlos de inmediato y transmitirlos.  Es decir, el resultado, por definici√≥n, es luchar por el tiempo real; </li><li>  Carga uniforme sobre hierro.  Est√° claro que hay ciclos diarios, etc., sin embargo, la carga todav√≠a se distribuye durante todo el d√≠a y resulta m√°s uniforme y predecible. </li></ul><br>  <b>La principal desventaja del procesamiento de transmisi√≥n:</b> <br><ul><li>  complejidad de desarrollo y soporte.  Primero, probar, administrar y recuperar datos es un poco m√°s dif√≠cil en comparaci√≥n con el lote.  La segunda dificultad (de hecho, este es el problema m√°s b√°sico) est√° asociada con las reversiones.  Si los trabajos no funcionaron y hubo un fracaso, es muy dif√≠cil capturar exactamente el momento en que todo se rompi√≥.  Y resolver el problema requerir√° m√°s esfuerzo y recursos que el procesamiento por lotes. </li></ul><br>  Entonces, si est√° pensando <b>si necesita transmisiones</b> , responda las siguientes preguntas por s√≠ mismo: <br><br><ol><li>  ¬øRealmente necesitas en tiempo real? </li><li>  ¬øHay muchas fuentes de transmisi√≥n? </li><li>  ¬øPerder un r√©cord es cr√≠tico? </li></ol><br>  Veamos <b>dos ejemplos</b> : <br><br>  <i>Ejemplo 1. An√°lisis de existencias para el comercio minorista:</i> <br><ul><li>  la exhibici√≥n de bienes no cambia en tiempo real; </li><li>  los datos se entregan con mayor frecuencia en modo por lotes; </li><li>  La p√©rdida de informaci√≥n es cr√≠tica. </li></ul><br>  En este ejemplo, es mejor usar lote. <br><br>  <i>Ejemplo 2. An√°lisis para un portal web:</i> <br><br><ul><li>  la velocidad anal√≠tica determina el tiempo de respuesta a un problema; </li><li>  los datos llegan en tiempo real; </li><li>  Las p√©rdidas de una peque√±a cantidad de informaci√≥n de actividad del usuario son aceptables. </li></ul><br>  Imagine que el an√°lisis refleja c√≥mo se sienten los visitantes de un portal web al usar su producto.  Por ejemplo, lanz√≥ una nueva versi√≥n y necesita comprender dentro de 10-30 minutos si todo est√° en orden, si alguna caracter√≠stica personalizada se ha roto.  Supongamos que el texto del bot√≥n "Pedido" ha desaparecido: los an√°lisis le permitir√°n responder r√°pidamente a una fuerte ca√≠da en el n√∫mero de pedidos, e inmediatamente comprender√° que necesita retroceder. <br><br>  Por lo tanto, en el segundo ejemplo, es mejor usar flujos. <br><br><h3>  Elementos SPOD </h3><br>  Los ingenieros de procesamiento de datos capturan, mueven, entregan, convierten y almacenan estos mismos datos (s√≠, ¬°el almacenamiento de datos tambi√©n es un proceso activo!). <br>  Por lo tanto, para construir un sistema de procesamiento de datos de transmisi√≥n (SPOD), necesitaremos los siguientes elementos: <br><br><ol><li>  <b>cargador de datos</b> (medios de entrega de datos al almacenamiento); </li><li>  <b>bus de intercambio de datos</b> (no siempre es necesario, pero no hay forma de transmitirlo porque necesita un sistema a trav√©s del cual intercambiar√° datos en tiempo real); </li><li>  <b>almacenamiento de datos</b> (como sin √©l); </li><li>  <b>Motor ETL</b> (necesario para realizar varias operaciones de filtrado, clasificaci√≥n y otras operaciones); </li><li>  <b>BI</b> (para mostrar resultados); </li><li>  <b>orquestador</b> (vincula todo el proceso, organizando el procesamiento de datos en varias etapas). </li></ol><br>  En nuestro caso, consideraremos la situaci√≥n m√°s simple y nos centraremos solo en los primeros tres elementos. <br><br><h3>  Herramientas de procesamiento de flujo de datos </h3><br>  Tenemos varios "candidatos" para el rol de <b>cargador</b> de <b>datos</b> : <br><br><ul><li>  Canal de apache </li><li>  Apache nifi </li><li>  Streamset </li></ul><br><h4>  Canal de apache </h4><br>  El primero del que hablaremos es <b>Apache Flume</b> , una herramienta para transportar datos entre diferentes fuentes y repositorios. <br><br><img src="https://habrastorage.org/webt/dg/by/a3/dgbya30snrkaceq0y7bvtct59wc.png" alt="imagen"><br><br>  Pros: <br><br><ul><li>  hay casi en todas partes </li><li>  usado durante mucho tiempo </li><li>  lo suficientemente flexible y extensible </li></ul><br>  Contras: <br><br><ul><li>  configuraci√≥n inconveniente </li><li>  dif√≠cil de monitorear </li></ul><br>  En cuanto a su configuraci√≥n, se parece a esto: <br><br><img src="https://habrastorage.org/webt/hf/-i/bz/hf-ibz-bp5n8ydo3c1viwsxw9qe.png" alt="imagen"><br><br>  Arriba, creamos un canal simple que se encuentra en el puerto, toma datos de all√≠ y simplemente lo registra.  En principio, para describir un proceso, esto sigue siendo normal, pero cuando tiene docenas de dichos procesos, el archivo de configuraci√≥n se convierte en un infierno.  Alguien agrega algunos configuradores visuales, pero ¬øpor qu√© molestarse si hay herramientas que lo hacen fuera de la caja?  Por ejemplo, el mismo NiFi y StreamSets. <br><br><h4>  Apache nifi </h4><br>  De hecho, desempe√±a el mismo papel que Flume, pero con una interfaz visual, que es una gran ventaja, especialmente cuando hay muchos procesos. <br><br>  Un par de hechos sobre NiFi <br><br><ul><li>  desarrollado originalmente en la NSA; </li><li>  Hortonworks ahora es compatible y desarrollado; </li><li>  parte de HDF de Hortonworks; </li><li>  tiene una versi√≥n especial de MiNiFi para recopilar datos de dispositivos. </li></ul><br>  El sistema se parece a esto: <br><br><img src="https://habrastorage.org/webt/jz/1k/l7/jz1kl7seqymd9rxx2tog4k88wni.png" alt="imagen"><br><br>  Tenemos un campo de creatividad y etapas de procesamiento de datos que arrojamos all√≠.  Hay muchos conectores para todos los sistemas posibles, etc. <br><br><h4>  Streamset </h4><br>  Tambi√©n es un sistema de control de flujo de datos con una interfaz visual.  Fue desarrollado por personas de Cloudera, se instala f√°cilmente como Parcel en CDH, tiene una versi√≥n especial de SDC Edge para recopilar datos de dispositivos. <br><br>  Consta de dos componentes: <br><br><ul><li>  SDC: un sistema que realiza el procesamiento directo de datos (gratis); </li><li>  StreamSets Control Hub: un centro de control para varios SDC con caracter√≠sticas adicionales para el desarrollo de l√≠neas de pago (de pago). </li></ul><br>  Se parece a esto: <br><br><img src="https://habrastorage.org/webt/kx/3z/jw/kx3zjwqx_ijbfxdlg7hvizllnt4.png" alt="imagen"><br><br>  Momento desagradable: los StreamSets tienen partes gratuitas y de pago. <br><br><h4>  Bus de datos </h4><br>  Ahora veamos d√≥nde cargaremos estos datos.  Solicitantes: <br><br><ul><li>  Apache kafka </li><li>  Rabbitmq </li><li>  NATS </li></ul><br>  Apache Kafka es la mejor opci√≥n, pero si tiene RabbitMQ o NATS en su empresa, y necesita agregar un poco de an√°lisis, entonces implementar Kafka desde cero no ser√° muy rentable. <br><br>  En todos los dem√°s casos, Kafka es una gran opci√≥n.  De hecho, es un agente de mensajes con escala horizontal y gran ancho de banda.  Est√° perfectamente integrado en todo el ecosistema de herramientas para trabajar con datos y puede soportar cargas pesadas.  Tiene una interfaz universal y es el sistema circulatorio de nuestro procesamiento de datos. <br><br>  En el interior, Kafka se divide en Tema: una cierta secuencia de datos separada de los mensajes con el mismo esquema o, al menos, con el mismo prop√≥sito. <br><br>  Para analizar el siguiente matiz, debe recordar que las fuentes de datos pueden variar ligeramente.  El formato de datos es muy importante: <br><br><img src="https://habrastorage.org/webt/fq/kn/ci/fqkncilmsox7hmoye289ronvbyk.png" alt="imagen"><br><br>  El formato de serializaci√≥n de datos Apache Avro merece una menci√≥n especial.  El sistema usa JSON para determinar la estructura de datos (esquema) que se serializa en un <b>formato binario compacto</b> .  Por lo tanto, ahorramos una gran cantidad de datos y la serializaci√≥n / deserializaci√≥n es m√°s barata. <br><br>  Todo parece estar bien, pero la presencia de archivos separados con circuitos plantea un problema, ya que necesitamos intercambiar archivos entre diferentes sistemas.  Parece que es simple, pero cuando trabajas en diferentes departamentos, los chicos del otro lado pueden cambiar algo y calmarse, y todo se romper√° por ti. <br><br>  Para no transferir todos estos archivos a unidades flash, disquetes y pinturas rupestres, hay un servicio especial: registro de esquemas.  Este es un servicio para sincronizar avro-esquemas entre servicios que escriben y leen desde Kafka. <br><br><img src="https://habrastorage.org/webt/do/jf/qd/dojfqd1m6nf5wr53xmvmg5hee_a.png" alt="imagen"><br><br>  En t√©rminos de Kafka, el productor es quien escribe, el consumidor es el que consume (lee) los datos. <br><br><h4>  Almac√©n de datos </h4><br>  Retadores (de hecho, hay muchas m√°s opciones, pero solo unas pocas): <br><br><ul><li>  HDFS + Colmena </li><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  Antes de elegir un repositorio, recuerde qu√© <b>es la idempotencia</b> .  Wikipedia dice que la idempotencia (idem en lat√≠n - los mismos + potentes - capaces) - la propiedad de un objeto u operaci√≥n cuando se aplica la operaci√≥n al objeto nuevamente, da el mismo resultado que el primero.  En nuestro caso, el proceso de procesamiento de transmisi√≥n debe construirse de modo que al rellenar los datos de origen, el resultado permanezca correcto. <br><br>  <b>C√≥mo lograr esto</b> en los sistemas de transmisi√≥n: <br><br><ul><li>  Identificar una identificaci√≥n √∫nica (puede ser compuesta) </li><li>  use esta identificaci√≥n para deduplicar datos </li></ul><br>  El almacenamiento HDFS + Hive <b>no proporciona idempotencia</b> para la transmisi√≥n de grabaciones "fuera de la caja", por lo que tenemos: <br><br><ul><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  <b>Kudu</b> es un repositorio adecuado para consultas anal√≠ticas, pero con una clave primaria, para deduplicaci√≥n.  <b>Impala</b> es la interfaz SQL para este repositorio (y muchos otros). <br><br>  En cuanto a ClickHouse, esta es una base de datos anal√≠tica de Yandex.  Su objetivo principal es el an√°lisis en una tabla llena de una gran corriente de datos sin procesar.  De las ventajas: hay un motor ReplacingMergeTree para la deduplicaci√≥n clave (la deduplicaci√≥n est√° dise√±ada para ahorrar espacio y puede dejar duplicados en algunos casos, debe tener en cuenta los <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/replacingmergetree/">matices</a> ). <br><br>  Queda por a√±adir algunas palabras sobre <b>Divolte</b> .  Si recuerdas, hablamos sobre el hecho de que algunos datos deben ser capturados.  Si necesita organizar de forma r√°pida y sencilla los an√°lisis para un portal, Divolte es un excelente servicio para capturar eventos de usuarios en una p√°gina web a trav√©s de JavaScript. <br><br><img src="https://habrastorage.org/webt/wo/7m/ol/wo7molaoelkzjjatvyzzaihkrmm.png" alt="imagen"><br><br><h3>  Ejemplo pr√°ctico </h3><br>  ¬øQu√© estamos tratando de hacer?  <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4016">Intentemos crear una</a> tuber√≠a para recopilar datos de Clickstream en tiempo real.  <b>Clickstream</b> es una huella virtual que deja un usuario mientras est√° en su sitio.  Capturaremos datos usando Divolte y los escribiremos en Kafka. <br><br><img src="https://habrastorage.org/webt/uj/mt/h-/ujmth-4jh9catnqqxt0ikp0w9xa.png" alt="imagen"><br><br>  Necesita Docker para trabajar, adem√°s necesita clonar el <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example">siguiente repositorio</a> .  Todo lo que ocurra ser√° lanzado en contenedores.  Para ejecutar varios contenedores a la vez, se <a href="">usar√° docker-compose.yml</a> .  Adem√°s, hay un <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/blob/master/Dockerfile">Dockerfile que</a> compila nuestros StreamSets con ciertas dependencias. <br><br>  Tambi√©n hay tres carpetas: <br><br><ol><li>  <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">los datos de clickhouse</a> se escribir√°n en <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">datos de clickhouse</a> </li><li>  exactamente el mismo pap√° ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/sdc-data">datos sdc</a> ) que tendremos para StreamSets, donde el sistema puede almacenar configuraciones </li><li>  la tercera carpeta ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/examples">ejemplos</a> ) incluye un archivo de solicitud y un archivo de configuraci√≥n de tuber√≠a para StreamSets </li></ol><br><br><img src="https://habrastorage.org/webt/-r/xt/2r/-rxt2rlhxzdqqeyz7qv3z3dugxw.png" alt="imagen"><br><br>  Para comenzar, ingrese el siguiente comando: <br><br><pre><code class="bash hljs">docker-compose up</code> </pre> <br>  Y disfrutamos de lo lento pero seguro que comienzan los contenedores.  Despu√©s de comenzar, podemos ir a la direcci√≥n <a href="http://localhost:8290/">http: // localhost: 18630 ‚Äã‚Äã/</a> e inmediatamente tocar Divolte: <br><br><img src="https://habrastorage.org/webt/cc/1m/im/cc1mimjszmzdoyiwb-elb2c_igu.png" alt="imagen"><br><br>  Entonces, tenemos Divolte, que ya recibi√≥ algunos eventos y los grab√≥ en Kafka.  Intentemos calcularlos usando StreamSets: <a href="http://localhost:18630/">http: // localhost: 18630 ‚Äã‚Äã/</a> (contrase√±a / inicio de sesi√≥n - admin / admin). <br><br><img src="https://habrastorage.org/webt/fc/hk/qz/fchkqzqe9pzpdws-xilz3ftdcb8.png" alt="imagen"><br><br>  Para no sufrir, es mejor <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4425">importar</a> <b>Pipeline</b> , nombr√°ndolo, por ejemplo, <b>clickstream_pipeline</b> .  Y de la carpeta de ejemplos importamos <b>clickstream.json</b> .  Si todo est√° bien, <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4701">veremos la siguiente imagen</a> : <br><br><img src="https://habrastorage.org/webt/o8/z9/x5/o8z9x5eaacqkehcfcgxattekpba.png" alt="imagen"><br><br>  Entonces, creamos una conexi√≥n con Kafka, registramos qu√© Kafka necesitamos, registramos qu√© tema nos interesa, luego seleccionamos los campos que nos interesan, luego pusimos un drenaje en Kafka, registrando qu√© Kafka y qu√© tema.  Las diferencias son que en un caso, el formato de datos es Avro, y en el segundo es solo JSON. <br><br>  Sigamos adelante.  Podemos, por ejemplo, <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4768">hacer una vista previa</a> que capture ciertos registros en tiempo real de Kafka.  Luego escribimos todo. <br><br>  Despu√©s del lanzamiento, veremos que una secuencia de eventos vuela a Kafka, y esto sucede en tiempo real: <br><br><img src="https://habrastorage.org/webt/kr/a7/0n/kra70nxdaplug8-oywal1wb23oi.png" alt="imagen"><br><br>  Ahora puede hacer un repositorio para estos datos en ClickHouse.  Para trabajar con ClickHouse, puede usar un cliente nativo simple ejecutando el siguiente comando: <br><br><pre> <code class="bash hljs">docker run -it --rm --network divolte-ss-ch_default yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Tenga en cuenta que esta l√≠nea indica la red a la que desea conectarse.  Y dependiendo de c√≥mo nombre la carpeta con el repositorio, el nombre de su red puede diferir.  En general, el comando ser√° el siguiente: <br><br><pre> <code class="bash hljs">docker run -it --rm --network {your_network_name} yandex/clickhouse-client --host clickhouse</code> </pre> <br>  La lista de redes se puede ver con el comando: <br><br><pre> <code class="bash hljs">docker network ls</code> </pre> <br>  Bueno, no queda nada: <br><br>  1. <b>Primero, "firme" nuestro ClickHouse a Kafka</b> , " <b>expl√≠quele</b> " qu√© formato tenemos los datos que necesitamos all√≠: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> clickstream_topic ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = Kafka <span class="hljs-keyword"><span class="hljs-keyword">SETTINGS</span></span> kafka_broker_list = <span class="hljs-string"><span class="hljs-string">'kafka:9092'</span></span>, kafka_topic_list = <span class="hljs-string"><span class="hljs-string">'clickstream'</span></span>, kafka_group_name = <span class="hljs-string"><span class="hljs-string">'clickhouse'</span></span>, kafka_format = <span class="hljs-string"><span class="hljs-string">'JSONEachRow'</span></span>;</code> </pre><br>  2. <b>Ahora crearemos una tabla real</b> donde pondremos los datos finales: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> clickstream ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = ReplacingMergeTree() <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> (<span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span>, pageViewId);</code> </pre> <br>  3. <b>Y luego proporcionaremos una relaci√≥n entre estas dos tablas</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">MATERIALIZED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">VIEW</span></span> clickstream_consumer <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> clickstream <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream_topic;</code> </pre> <br>  4. <b>Y ahora seleccionaremos los campos necesarios</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream;</code> </pre> <br>  Como resultado, la elecci√≥n de la tabla de destino nos dar√° el resultado que necesitamos. <br><br><img src="https://habrastorage.org/webt/wn/sr/qe/wnsrqei2fo7iydrf41zattwbddy.png"><br><br>  Eso es todo, fue el Clickstream m√°s simple que puedes construir.  Si desea completar los pasos anteriores usted mismo, <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">vea el video</a> completo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/477834/">https://habr.com/ru/post/477834/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../477816/index.html">C√≥mo encontrar un trabajo con un buen contrato</a></li>
<li><a href="../477818/index.html">C√≥mo convertirse en un cient√≠fico de datos en 2019</a></li>
<li><a href="../477820/index.html">VMware, Hyper-V, OpenStack, Kubernetes, Swarm: monitoreo desde una √∫nica interfaz en Quest Foglight</a></li>
<li><a href="../477824/index.html">Vivamos hasta el lunes o c√≥mo sobrevivir el viernes negro</a></li>
<li><a href="../477826/index.html">Descripci√≥n general y comparaci√≥n de las tecnolog√≠as V2X</a></li>
<li><a href="../477836/index.html">C√≥mo probamos el WD ActiveScale P100 para nuestro almacenamiento S3</a></li>
<li><a href="../477838/index.html">PVS-Studio Static Analyzer como herramienta para la protecci√≥n contra vulnerabilidades de d√≠a cero</a></li>
<li><a href="../477840/index.html">Analizador de c√≥digo est√°tico PVS-Studio como protecci√≥n contra vulnerabilidades de d√≠a cero</a></li>
<li><a href="../477842/index.html">Historias de Gennady Zelenko y Sergey Popov - divulgadores de tecnolog√≠a en la URSS</a></li>
<li><a href="../477844/index.html">5 pasos desde la idea hasta la aplicaci√≥n pr√°ctica del aprendizaje autom√°tico con SAP Data Intelligence</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>