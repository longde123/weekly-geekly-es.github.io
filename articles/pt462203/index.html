<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìí üë®üèø‚Äçüéì üåü KVM (sub) VDI com m√°quinas virtuais √∫nicas usando bash üÜñ ü§∏üèæ üåè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Para quem √© este artigo? 
 Este artigo pode ser do interesse de administradores de sistemas que foram confrontados com a tarefa de criar um servi√ßo de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KVM (sub) VDI com m√°quinas virtuais √∫nicas usando bash</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462203/"><h4>  Para quem √© este artigo? </h4><br>  Este artigo pode ser do interesse de administradores de sistemas que foram confrontados com a tarefa de criar um servi√ßo de trabalhos "√∫nicos". <br><br><h4>  Pr√≥logo </h4><br>  Solicitou-se ao departamento de suporte de TI de uma empresa jovem em desenvolvimento din√¢mico com uma pequena rede regional que organizasse "esta√ß√µes de autoatendimento" para uso de seus clientes externos.  Os dados da esta√ß√£o deveriam ser usados ‚Äã‚Äãpara registro nos portais externos da empresa, para baixar dados de dispositivos externos e trabalhar com portais do governo. <br><br>  Um aspecto importante foi o fato de a maioria do software ser "aprimorada" no MS Windows (por exemplo, "Declara√ß√£o") e, apesar do movimento em dire√ß√£o a formatos abertos, o MS Office continua sendo o padr√£o dominante na troca de documentos eletr√¥nicos.  Portanto, n√£o foi poss√≠vel recusar o MS Windows ao resolver esse problema. <br><a name="habracut"></a><br>  O principal problema era a possibilidade de acumular v√°rios dados das sess√µes do usu√°rio, o que poderia levar ao vazamento para terceiros.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Esta situa√ß√£o j√° decepcionou o MFC</a> .  Mas, diferentemente do MFC quase judicial (institui√ß√£o aut√¥noma do estado), as organiza√ß√µes n√£o estatais ser√£o punidas muito mais por essas defici√™ncias.  A pr√≥xima quest√£o cr√≠tica foi o requisito de trabalhar com m√≠dia de armazenamento externo, na qual, por todos os meios, haver√° um monte de malware malicioso.  A probabilidade de entrada de malware da Internet foi considerada menos prov√°vel devido √† restri√ß√£o de acesso √† Internet por meio de uma lista branca de endere√ßos.Os funcion√°rios de outros departamentos se uniram para elaborar os requisitos, fazendo seus requisitos e desejos, os requisitos finais foram os seguintes: <br><br>  <b>Requisitos de SI</b> <br><br><ul><li>  Ap√≥s o uso, todos os dados do usu√°rio (incluindo arquivos tempor√°rios e chaves do registro) devem ser exclu√≠dos. </li><li>  Todos os processos iniciados pelo usu√°rio devem ser conclu√≠dos no final do trabalho. </li><li>  Acesso √† Internet atrav√©s de uma lista branca de endere√ßos. </li><li>  Restri√ß√µes √† capacidade de executar c√≥digo de terceiros. </li><li>  Se a sess√£o estiver ociosa por mais de 5 minutos, a sess√£o deve terminar automaticamente, a esta√ß√£o deve executar uma limpeza. </li></ul><br>  <b>Requisitos do cliente</b> <br><br><ul><li>  O n√∫mero de esta√ß√µes clientes por filial n√£o √© superior a 4. </li><li>  O tempo m√≠nimo de espera para a prontid√£o do sistema, desde o momento em que "me sentei em uma cadeira" at√© o in√≠cio do trabalho com o software cliente. </li><li>  A capacidade de conectar dispositivos perif√©ricos (scanners, unidades flash) diretamente do local de instala√ß√£o da "esta√ß√£o de autoatendimento". </li><li>  Desejos do cliente </li><li>  Demonstra√ß√£o de materiais publicit√°rios (fotos) no momento do encerramento do complexo. </li></ul><cut></cut><br><h4>  Farinha de criatividade </h4><br>  Tendo jogado o suficiente com o Windows LiveCD, chegamos √† conclus√£o un√¢nime de que a solu√ß√£o resultante n√£o satisfaz pelo menos tr√™s pontos cr√≠ticos.  Eles s√£o carregados por um longo per√≠odo de tempo, ou n√£o s√£o totalmente ao vivo, ou sua personaliza√ß√£o foi associada a uma dor intensa.  Talvez tenhamos pesquisado mal, e voc√™ pode aconselhar um conjunto de algumas ferramentas, ficarei grato. <br><br>  Al√©m disso, come√ßamos a olhar para a VDI, mas para esta tarefa, a maioria das solu√ß√µes √© muito cara ou requer muita aten√ß√£o.  E eu queria uma ferramenta simples com uma quantidade m√≠nima de m√°gica, a maioria dos problemas que poderiam ser resolvidos simplesmente reiniciando / reiniciando o servi√ßo.  Felizmente, t√≠nhamos equipamentos de servidor, de classe baixa nas filiais, do servi√ßo desativado, que poder√≠amos usar para a base tecnol√≥gica. <br><br>  Qual √© o resultado?  Mas n√£o poderei contar o que aconteceu no final, porque a NDA, mas no processo de pesquisa, desenvolvemos um esquema interessante que se mostrou bem em testes de laborat√≥rio, embora n√£o tenha entrado em s√©rie. <br><br>  Algumas isen√ß√µes de responsabilidade: o autor n√£o afirma que a solu√ß√£o proposta resolve completamente todas as tarefas e o faz voluntariamente e com a m√∫sica.  O autor concorda com a afirma√ß√£o de que Sein Englishe sprache √© zehr schlecht.  Como a solu√ß√£o n√£o se desenvolve mais, voc√™ n√£o pode contar com uma corre√ß√£o de bug ou uma altera√ß√£o na funcionalidade, tudo est√° em suas m√£os.  O autor sup√µe que voc√™ esteja pelo menos um pouco familiarizado com o KVM e leia um artigo de revis√£o sobre o protocolo Spice, e voc√™ trabalhou um pouco com o Centos ou outra distribui√ß√£o GNU Linux. <br><br>  Neste artigo, eu gostaria de analisar a espinha dorsal da solu√ß√£o resultante, a saber, a intera√ß√£o do cliente e do servidor e a ess√™ncia dos processos no ciclo de vida das m√°quinas virtuais na estrutura da solu√ß√£o em quest√£o.  Se o artigo for interessante para o p√∫blico, descreverei os detalhes da implementa√ß√£o de imagens ao vivo para criar thin clients baseados no Fedora e falarei sobre os detalhes do ajuste de m√°quinas virtuais e servidores KVM para otimizar o desempenho e a seguran√ßa. <br><br>  Se voc√™ pegar papel colorido, <br>  Tintas, pinc√©is e cola, <br>  E um pouco mais de destreza ... <br>  Voc√™ pode fazer cem rublos! <br><br><h4>  Esquema e descri√ß√£o da bancada de testes </h4><br><img src="https://habrastorage.org/webt/pu/tu/rk/puturkaiwqcpbp4wld7ezdk_lcw.png"><br><br>  Todo o equipamento est√° localizado dentro da rede da filial, apenas o canal da Internet sai.  Historicamente, j√° existe um servidor proxy, n√£o √© nada extraordin√°rio.  Mas √© nele, entre outras coisas, que o tr√°fego das m√°quinas virtuais ser√° filtrado (abrevie VM mais adiante no texto).  Nada impede a coloca√ß√£o desse servi√ßo no servidor KVM, a √∫nica coisa que voc√™ precisa observar √© como a carga dele no subsistema de disco √© alterada. <br><br>  Esta√ß√£o cliente - de fato, ‚Äúesta√ß√µes de autoatendimento‚Äù, ‚Äúfront-end‚Äù do nosso servi√ßo.  S√£o nettops do Lenovo IdeaCentre.  Para que serve esta unidade?  Sim, quase todo mundo, especialmente satisfeito com o grande n√∫mero de conectores USB e leitores de cart√£o no painel frontal.  Em nosso esquema, um cart√£o SD com prote√ß√£o contra grava√ß√£o de hardware √© inserido no leitor de cart√µes, no qual a imagem ao vivo modificada do Fedora 28 √© gravada. √â claro que um monitor, teclado e mouse est√£o conectados ao nettop. <br><br>  Switch - um switch de hardware normal do segundo n√≠vel, fica na sala do servidor e pisca com as luzes.  N√£o est√° conectado a nenhuma rede, exceto a rede de "esta√ß√µes de autoatendimento". <br><br>  O KVM_Server √© o n√∫cleo do circuito; nos testes de bancada do Core 2 Quad Q9650 com 8 GB de RAM, ele usou com confian√ßa tr√™s m√°quinas virtuais Windows 10.  Subsistema de disco - adaptec 3405 2 unidades Raid 1 + SSD.  Em testes de campo do Xeon 1220, o LSI 9260 + SSD mais s√©rio conseguiu facilmente 5-6 VMs.  Obter√≠amos o servidor do servi√ßo aposentado, n√£o haveria muitos custos de capital.  O sistema de virtualiza√ß√£o KVM com o pool de m√°quinas virtuais pool_Vm √© implementado neste (s) servidor (es). <br><br>  VM √© uma m√°quina virtual, o back-end do nosso servi√ßo.  √â o trabalho do usu√°rio. <br><br>  Enp5s0 √© uma interface de rede que olha para a rede de "esta√ß√µes de autoatendimento", dhcpd, ntpd, httpd ao vivo e o xinetd ouve a porta "sinal". <br><br>  Lo0 √© a pseudo-interface de loopback.  Standard. <br><br>  Spice_console - Uma coisa muito interessante, o fato √© que, diferentemente do RDP cl√°ssico, quando voc√™ ativa o pacote de protocolos KVM + Spice, uma entidade adicional aparece - a porta do console da m√°quina virtual.  De fato, ao conectar-se a essa porta TCP, obtemos o console do Vm, sem a necessidade de conectar-se ao Vm atrav√©s de sua interface de rede.  Toda intera√ß√£o com o Vm para transmiss√£o de sinal, o servidor assume.  A fun√ß√£o anal√≥gica mais pr√≥xima √© a IPKVM.  I.e.  Uma imagem de um monitor de VM √© transferida para essa porta, os dados sobre o movimento do mouse s√£o transmitidos a ela e (o mais importante) a intera√ß√£o via protocolo Spice permite redirecionar perfeitamente dispositivos USB para uma m√°quina virtual, como se esse dispositivo estivesse conectado √† pr√≥pria VM.  Testado para flash drives, scanners, webcams. <br><br>  Vnet0, virbr0 e placas de rede virtual Vm formam uma rede de m√°quinas virtuais. <br><br><h4>  Como isso funciona </h4><br>  Da esta√ß√£o do cliente <br><br>  A esta√ß√£o cliente √© inicializada no modo gr√°fico a partir da imagem ao vivo modificada do Fedora 28, recebe o endere√ßo IP por dhcp do espa√ßo de endere√ßo de rede 169.254.24.0/24.  Durante o processo de inicializa√ß√£o, s√£o criadas regras de firewall que permitem conex√µes com as portas do servidor "sinal" e "tempero".  Ap√≥s a conclus√£o do download, a esta√ß√£o aguarda a autoriza√ß√£o do usu√°rio do Cliente.  Ap√≥s a autoriza√ß√£o do usu√°rio, o gerenciador da √°rea de trabalho "openbox" √© iniciado e o script de inicializa√ß√£o autom√°tica √© executado em nome do usu√°rio autorizado.  Entre outras coisas, o script de execu√ß√£o autom√°tica executa o script remote.sh. <br><br><div class="spoiler">  <b class="spoiler_title">$ HOME / .config / openbox / scripts / remote.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh server_ip=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "server_ip" \ |/usr/bin/cut -d "=" -f2) vdi_signal_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_signal_port" \ |/usr/bin/cut -d "=" -f2) vdi_spice_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_spice_port" \ |/usr/bin/cut -d "=" -f2) animation_folder=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "animation_folder" \ |/usr/bin/cut -d "=" -f2) process=/usr/bin/remote-viewer while true do if [ -z `/usr/bin/pidof feh` ] then /usr/bin/echo $animation_folder /usr/bin/feh -N -x -D1 $animation_folder &amp; else /usr/bin/echo fi /usr/bin/nc -i 1 $server_ip $vdi_signal_port |while read line do if /usr/bin/echo "$line" |/usr/bin/grep "RULE ADDED, CONNECT NOW!" then /usr/bin/killall feh pid_process=$($process "spice://$server_ip:$vdi_spice_port" \ "--spice-disable-audio" "--spice-disable-effects=animation" \ "--spice-preferred-compression=auto-glz" "-k" \ "--kiosk-quit=on-disconnect" | /bin/echo $!) /usr/bin/wait $pid_process /usr/bin/killall -u $USER exit else /usr/bin/echo $line &gt;&gt; /var/log/remote.log fi done done</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/client.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">server_ip=169.254.24.1 vdi_signal_port=5905 vdi_spice_port=5906 animation_folder=/usr/share/backgrounds/animation background_folder=/usr/share/backgrounds2/fedora-workstation</code> </pre><br></div></div><br>  Descri√ß√£o das vari√°veis ‚Äã‚Äãdo arquivo client.conf <br>  server_ip - endere√ßo KVM_Server <br>  vdi_signal_port - porta KVM_Server na qual o xinetd "fica" <br>  vdi_spice_port - porta de rede KVM_Server, da qual a solicita√ß√£o de conex√£o ser√° redirecionada do cliente visualizador remoto para a porta de especiarias do Vm selecionado (detalhes abaixo) <br>  animation_folder - pasta de onde as imagens s√£o tiradas para demonstra√ß√£o de anima√ß√£o besteira <br>  background_folder - a pasta de onde as imagens s√£o tiradas para apresenta√ß√µes em espera.  Mais sobre anima√ß√£o na pr√≥xima parte do artigo. <br><br>  O script remote.sh obt√©m as configura√ß√µes do arquivo de configura√ß√£o /etc/client.conf e usa nc para conectar-se √† porta ‚Äúvdi_signal_port‚Äù do servidor KVM e recebe um fluxo de dados do servidor, entre os quais espera a sequ√™ncia ‚ÄúRULE ADDED, CONNECT NOW‚Äù.  Quando a linha desejada √© recebida, o processo do visualizador remoto √© iniciado no modo quiosque, estabelecendo uma conex√£o com a porta do servidor ‚Äúvdi_spice_port‚Äù.  A execu√ß√£o do script √© suspensa at√© o final da execu√ß√£o do visualizador remoto. <br><br>  O visualizador remoto conectado √† porta "vdi_spice_port", devido a um redirecionamento no lado do servidor, chega √† porta "spice_console" da interface lo0, ou seja,  para o console da m√°quina virtual e o trabalho do usu√°rio ocorre diretamente.  Enquanto aguarda a conex√£o, o usu√°rio recebe uma anima√ß√£o besteira, na forma de uma apresenta√ß√£o de slides de arquivos jpeg, o caminho para o diret√≥rio com imagens √© determinado pelo valor da vari√°vel animation_folder do arquivo de configura√ß√£o. <br><br>  Se a conex√£o com a porta "spice_console" da m√°quina virtual for perdida, o que sinaliza o desligamento / reinicializa√ß√£o da m√°quina virtual (ou seja, o final real da sess√£o do usu√°rio), todos os processos em nome do usu√°rio autorizado ser√£o encerrados, o que levar√° ao rein√≠cio do lightdm e retornar√° √† tela de autoriza√ß√£o . <br><br><h4>  Do lado do servidor KVM </h4><br>  Na porta "sinal" da placa de rede, enp5s0 est√° aguardando a conex√£o xinetd.  Ap√≥s conectar-se √† porta "signal", o xinetd executa o script vm_manager.sh sem passar nenhum par√¢metro de entrada para ele e redireciona o resultado do script para a sess√£o nc da Esta√ß√£o Cliente. <br><br><div class="spoiler">  <b class="spoiler_title">/etc/xinetd.d/test-server</b> <div class="spoiler_text"><pre> <code class="bash hljs">service vdi_signal { port = 5905 socket_type = stream protocol = tcp <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> = no user = root server = /home/admin/scripts_vdi_new/vm_manager.sh }</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_manager.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" export "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL export SRV_START_PORT_POOL=$SRV_START_PORT_POOL SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR date=$(/usr/bin/date) #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# /usr/bin/echo "# $date START EXECUTE VM_MANAGER.SH #" make_connect_to_vm() { #&lt;READING CLEAR.LIST AND CHECK PORT FOR NETWORK STATE&gt;# /usr/bin/echo "READING CLEAN.LIST AND CHECK PORT STATE" #&lt;CHECK FOR NO ONE PORT IN CLEAR.LIST&gt;# if [ -z `/usr/bin/cat $SRV_TMP_DIR/clear.list` ] then /usr/bin/echo "NO AVALIBLE PORTS IN CLEAN.LIST FOUND" /usr/bin/echo "Will try to make housekeeper, and create new vm" make_housekeeper else #&lt;MINIMUN ONE PORT IN CLEAR.LIST FOUND&gt;# /usr/bin/cat $SRV_TMP_DIR/clear.list |while read line do clear_vm_port=$(($line)) /bin/echo "FOUND PORT $clear_vm_port IN CLEAN.LIST. TRY NETSTAT" \ "CHECK FOR PORT=$clear_vm_port" #&lt;NETSTAT LISTEN CHECK FOR PORT FROM CLEAN.LIST&gt;# if /usr/bin/netstat -lnt |/usr/bin/grep ":$clear_vm_port" &gt; /dev/null then /bin/echo "$clear_vm_port IS LISTEN" #&lt;PORT IS LISTEN. CHECK FOR IS CONNECTED NOW&gt;# if /usr/bin/netstat -nt |/usr/bin/grep ":$clear_vm_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then #&lt;PORT LISTEN AND ALREADY CONNECTED! MOVE PORT FROM CLEAR.LIST # TO WASTE.LIST&gt;# /bin/echo "$clear_vm_port IS ALREADY CONNECTED, MOVE PORT TO WASTE.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list else #&lt;PORT LISTEN AND NO ONE CONNECT NOW. MOVE PORT FROM CLEAR.LIST TO # CONN_WAIT.LIST AND CREATE IPTABLES RULES&gt;## /usr/bin/echo "OK, $clear_vm_port IS NOT ALREADY CONNECTED" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/conn_wait.list $SRV_SCRIPTS_DIR/vm_connect.sh $clear_vm_port #&lt;TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW WM&gt;# /bin/echo "TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW VM" make_housekeeper /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH#" exit fi else #&lt;PORT IS NOT A LISTEN. MOVE PORT FROM CLEAR.LIST TO WASTE.LIST&gt;# /bin/echo " "$clear_vm_port" is NOT LISTEN. REMOVE PORT FROM CLEAR.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list make_housekeeper fi done fi } make_housekeeper() { /usr/bin/echo "=Execute housekeeper=" /usr/bin/cat $SRV_TMP_DIR/waste.list |while read line do /usr/bin/echo "$line" if /usr/bin/netstat -lnt |/usr/bin/grep ":$line" &gt; /dev/null then /bin/echo "port_alive, vm is running" if /usr/bin/netstat -nt |/usr/bin/grep ":$line" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "port_in_use can't delete vm!!!" else /bin/echo "port_not in use. Deleting vm" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh $line fi else /usr/bin/echo "posible vm is already off. Deleting vm" /usr/bin/echo "MOVE VM IN OFF STATE $line FROM WASTE.LIST TO" \ "RECYCLE.LIST AND DELETE VM" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh "$line" fi done create_clear_vm } create_clear_vm() { /usr/bin/echo "=Create new VM=" while [ $SRV_POOL_SIZE -gt 0 ] do new_vm_port=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) /usr/bin/echo "new_vm_port=$new_vm_port" if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/clear.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in clear.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/waste.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in waste.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/recycle.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN RECYCLE LIST" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/conn_wait.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN CONN_WAIT LIST" else /usr/bin/echo "PORT IN NOT DEFINED IN NO ONE LIST WILL CREATE" \ "VM ON PORT $new_vm_port" /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_create.sh $new_vm_port fi fi fi fi SRV_POOL_SIZE=$(($SRV_POOL_SIZE-1)) done /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH #" } make_connect_to_vm |/usr/bin/tee -a /var/log/vm_manager.log</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/vm_manager.conf</b> <div class="spoiler_text">  srv_scripts_dir = / home / admin / scripts_vdi_new <br>  srv_pool_size = 4 <br>  srv_start_port_pool = 5920 <br>  srv_tmp_dir = / tmp / vm_state <br>  base_host = win10_2 <br>  input_iface = enp5s0 <br>  vdi_spice_port = 5906 <br>  count_conn_tryes = 10 <br></div></div><br><br>  Descri√ß√£o das vari√°veis ‚Äã‚Äãdo arquivo de configura√ß√£o vm_manager.conf <br>  srv_scripts_dir - pasta de local do script vm_manager.sh, vm_connect.sh, vm_delete.sh, vm_create.sh, vm_clear.sh <br>  srv_pool_size - tamanho do pool Vm <br>  srv_start_port_pool - a porta inicial, ap√≥s a qual as portas de tempero dos consoles da m√°quina virtual ser√£o iniciadas <br>  srv_tmp_dir - pasta para arquivos tempor√°rios <br>  base_host - base Vm (imagem dourada) a partir da qual os clones Vm ser√£o transformados no pool <br>  input_iface - a interface de rede do servidor, olhando para esta√ß√µes cliente <br>  vdi_spice_port - a porta de rede do servidor a partir da qual a solicita√ß√£o de conex√£o ser√° redirecionada do cliente do visualizador remoto para a porta de especiarias da VM selecionada <br>  count_conn_tryes - um timer de espera, ap√≥s o qual se considera que n√£o ocorreu uma conex√£o com o Vm (para obter detalhes, consulte vm_connect.sh) <br><br>  O script vm_manager.sh l√™ o arquivo de configura√ß√£o do arquivo vm_manager.conf, avalia o estado das m√°quinas virtuais no conjunto de acordo com v√°rios par√¢metros, a saber: quantas VMs s√£o implementadas, se existem VMs limpas gratuitas.  Para fazer isso, ele l√™ o arquivo clear.list que cont√©m os n√∫meros de portas "spice_console" das m√°quinas virtuais "rec√©m-criadas" (consulte o ciclo de cria√ß√£o da VM abaixo) e verifica se h√° uma conex√£o estabelecida com elas.  Se uma porta com uma conex√£o de rede estabelecida for detectada (o que absolutamente n√£o deveria ser), um aviso ser√° exibido e a porta ser√° transferida para waste.list Quando a primeira porta for encontrada no arquivo clear.list com o qual n√£o h√° conex√£o no momento, vm_manager.sh chama o script vm_connect.sh e passa ele como par√¢metro o n√∫mero dessa porta. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_connect.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh date=$(/usr/bin/date) /usr/bin/echo "#" "$date" "START EXECUTE VM_CONNECT.SH#" #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# free_port="$1" input_iface=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "input_iface" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "input_iface=$input_iface" vdi_spice_port=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "vdi_spice_port" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "vdi_spice_port=$vdi_spice_port" count_conn_tryes=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "count_conn_tryes" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "count_conn_tryes=$count_conn_tryes" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# #&lt;CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# /usr/bin/echo "create rule for port" $free_port /usr/sbin/iptables -I INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -I OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/sbin/iptables -t nat -I PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/bin/echo "RULE ADDED, CONNECT NOW!" #&lt;/CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# #&lt;WAIT CONNECT ESTABLISHED AND ACTIVATE CONNECT TIMER&gt;# while [ $count_conn_tryes -gt 0 ] do if /usr/bin/netstat -nt |/usr/bin/grep ":$free_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "$free_port NOW in use!!!" /usr/bin/sleep 1s /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/waste.list return else /usr/bin/echo "$free_port NOT IN USE" /usr/bin/echo "RULE ADDED, CONNECT NOW!" /usr/bin/sleep 1s fi count_conn_tryes=$((count_conn_tryes-1)) done #&lt;/WAIT CONNECT ESTABLISED AND ACTIVATE CONNECT TIMER&gt;# #&lt;IF COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT \ # VM TO CLEAR.LIST&gt;# /usr/bin/echo "REVERT IPTABLES RULE AND REVERT VM TO CLEAN \ LIST $free_port" /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport $free_port \ -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/clear.list #&lt;/COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT VM \ #TO CLEAR.LIST&gt;# /usr/bin/echo "#" "$date" "END EXECUTE VM_CONNECT.SH#" # Attention! Must Be! sysctl net.ipv4.conf.all.route_localnet=1</span></span></code> </pre><br></div></div><br>  O script vm_connect.sh apresenta regras de firewall que criam um redirecionamento "vdi_spice_port" da porta do servidor da interface enp5s0 para a "porta do console de especiarias" da VM localizada na interface do servidor lo0, transmitida como par√¢metro de inicializa√ß√£o.  A porta √© transferida para conn_wait.list, a VM √© considerada uma conex√£o pendente.  A linha REGRA ADICIONADA, CONECTAR AGORA √© enviada para a sess√£o da Esta√ß√£o Cliente na porta de "sinal" do servidor, o que √© esperado pelo script remote.sh em execu√ß√£o.  Um ciclo de espera de conex√£o come√ßa com o n√∫mero de tentativas determinadas pelo valor da vari√°vel "count_conn_tryes" do arquivo de configura√ß√£o.  A cada segundo da sess√£o nc, a sequ√™ncia "REGRA ADICIONADA, CONECTAR AGORA" ser√° fornecida e a conex√£o estabelecida com a porta "spice_console" ser√° verificada. <br><br>  Se a conex√£o falhar para o n√∫mero definido de tentativas, a porta spice_console ser√° transferida de volta para clear.list. A execu√ß√£o do vm_connect.sh ser√° conclu√≠da, a execu√ß√£o do vm_manager.sh ser√° retomada, iniciando o ciclo de limpeza. <br><br>  Se a Esta√ß√£o Cliente se conectar √† porta spice_console na interface lo0, as regras do firewall que criar√£o um redirecionamento entre a porta do servidor spice e a porta spice_console ser√£o exclu√≠das e a conex√£o ser√° mantida por um mecanismo para determinar o status do firewall.  No caso de uma conex√£o desconectada, a reconex√£o com a porta spice_console falhar√°.  A porta spice_console √© transferida para waste.list, a VM √© considerada suja e n√£o pode retornar ao conjunto de m√°quinas virtuais limpas sem passar pela limpeza.  A execu√ß√£o do vm_connect.sh √© conclu√≠da, a execu√ß√£o do vm_manager.sh √© retomada, o que inicia o ciclo de limpeza. <br><br>  O ciclo de limpeza come√ßa examinando o arquivo waste.list, para o qual s√£o transferidos os n√∫meros spice_console das portas da m√°quina virtual para as quais a conex√£o √© estabelecida.  A presen√ßa de uma conex√£o ativa √© determinada em cada porta spice_console da lista.  Se n√£o houver conex√£o, considera-se que a m√°quina virtual n√£o est√° mais em uso e a porta √© transferida para recycle.list e o processo de exclus√£o da m√°quina virtual (veja abaixo) ao qual essa porta pertence √© iniciado.  Se uma conex√£o de rede ativa for detectada na porta, presume-se que a m√°quina virtual esteja sendo usada, nenhuma a√ß√£o ser√° executada.  Se a porta n√£o for tocada, presume-se que a VM esteja desligada e n√£o seja mais necess√°ria.  A porta √© transferida para recycle.list e o processo de remo√ß√£o da m√°quina virtual √© iniciado.  Para fazer isso, o script vm_delete.sh √© chamado, para o qual o n√∫mero "spice_console" √© transferido para a porta da VM como o par√¢metro, que deve ser exclu√≠do. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_delete.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh #&lt;Set local VARIABLES&gt;# port_to_delete="$1" date=$(/usr/bin/date) #&lt;/Set local VARIABLES&gt;# /usr/bin/echo "# $date START EXECUTE VM_DELETE.SH#" /usr/bin/echo "TRY DELETE VM ON PORT: $vm_port" #&lt;VM NAME SETUP&gt;# vm_name_part1=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep 'base_host' \ |/usr/bin/cut -d'=' -f2) vm_name=$(/usr/bin/echo "$vm_name_part1""-""$port_to_delete") #&lt;/VM NAME SETUP&gt;# #&lt;SHUTDOWN AND DELETE VM&gt;# /usr/bin/virsh destroy $vm_name /usr/bin/virsh undefine $vm_name /usr/bin/rm -f /var/lib/libvirt/images_write/$vm_name.qcow2 /usr/bin/sed -i "/$port_to_delete/d" $SRV_TMP_DIR/recycle.list #&lt;/SHUTDOWN AND DELETE VM&gt;# /usr/bin/echo "VM ON PORT $vm_port HAS BEEN DELETE AND REMOVE" \ "FROM RECYCLE.LIST. EXIT FROM VM_DELETE.SH" /usr/bin/echo "# $date STOP EXECUTE VM_DELETE.SH#" exit</span></span></code> </pre><br></div></div><br>  Remover uma m√°quina virtual √© uma opera√ß√£o bastante trivial, o script vm_delete.sh determina o nome da m√°quina virtual que possui a porta passada como o par√¢metro de inicializa√ß√£o.  A VM √© for√ßada a parar, a VM √© removida do hipervisor, o disco r√≠gido virtual desta VM √© exclu√≠do.  A porta spice_console √© removida de recycle.list.  A execu√ß√£o do vm_delete.sh termina, a execu√ß√£o do vm_manager.sh continua <br><br>  O script vm_manager.sh, no final das opera√ß√µes para limpar m√°quinas virtuais desnecess√°rias da lista waste.list, inicia o ciclo de cria√ß√£o de m√°quinas virtuais no pool. <br><br>  O processo come√ßa com a determina√ß√£o das portas spice_console dispon√≠veis para hospedagem.  Para fazer isso, com base no par√¢metro do arquivo de configura√ß√£o "srv_start_port_pool", que define a porta inicial do pool "spice_console" de m√°quinas virtuais e o par√¢metro "srv_pool_size", que determina o limite do n√∫mero de m√°quinas virtuais, todas as variantes de porta poss√≠veis s√£o enumeradas sequencialmente.  Para cada porta espec√≠fica, √© pesquisada em clear.list, waste.list, conn_wait.list, recycle.list.  Se uma porta for encontrada em qualquer um desses arquivos, a porta ser√° considerada ocupada e ser√° ignorada.  Se a porta n√£o for encontrada nos arquivos especificados, ela ser√° inserida no arquivo recycle.list e o processo de cria√ß√£o de uma nova m√°quina virtual ser√° iniciado.  Para fazer isso, o script vm_create.sh √© chamado para o qual o n√∫mero spice_console da porta para a qual voc√™ deseja criar uma VM √© passado. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_create.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh /usr/bin/echo "#" "$date" "START RUNNING VM_CREATE.SH#" new_vm_port=$1 date=$(/usr/bin/date) a=0 /usr/bin/echo SRV_TMP_DIR=$SRV_TMP_DIR #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# base_host=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "base_host" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "base_host=$base_host" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# hdd_image_locate() { /bin/echo "Run STEP 1 - hdd_image_locate" hdd_base_image=$(/usr/bin/virsh dumpxml $base_host \ |/usr/bin/grep "source file" |/usr/bin/grep "qcow2" |/usr/bin/head -n 1 \ |/usr/bin/cut -d "'" -f2) if [ -z "$hdd_base_image" ] then /bin/echo "base hdd image not found!" else /usr/bin/echo "hdd_base_image found is a $hdd_base_image. Run next step 2" #&lt; CHECK FOR SNAPSHOT ON BASE HDD &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" | /usr/bin/grep -c "Snapshot"` ] then /usr/bin/echo "base image haven't snapshot, run NEXT STEP 3" else /usr/bin/echo "base hdd image have a snapshot, can't use this image" exit fi #&lt;/ CHECK FOR SNAPSHOT ON BASE HDD &gt;# #&lt; CHECK FOR HDD IMAGE IS LINK CLONE &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" |/usr/bin/grep -c "backing file" then /usr/bin/echo "base image is not a linked clone, NEXT STEP 4" /usr/bin/echo "Base image check complete!" else /usr/bin/echo "base hdd image is a linked clone, can't use this image" exit fi fi #&lt;/ CHECK FOR HDD IMAGE IS LINK CLONE &gt;# cloning } cloning() { # &lt;Step_1 turn the base VM off &gt;# /usr/bin/virsh shutdown $base_host &gt; /dev/null 2&gt;&amp;1 # &lt;/Step_1 turn the base VM off &gt;# #&lt;Create_vm_config&gt;# /usr/bin/echo "Free port for Spice VM is $new_vm_port" #&lt;Setup_name_for_new_VM&gt;# new_vm_name=$(/bin/echo $base_host"-"$new_vm_port) #&lt;/Setup_name_for_new_VM&gt;# #&lt;Make_base_config_as_clone_base_VM&gt;# /usr/bin/virsh dumpxml $base_host &gt; $SRV_TMP_DIR/$new_vm_name.xml #&lt;Make_base_config_as_clone_base_VM&gt;# ##&lt;Setup_New_VM_Name_in_config&gt;## /usr/bin/sed -i "s%&lt;name&gt;$base_host&lt;/name&gt;%&lt;name&gt;$new_vm_name&lt;/name&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Setup_New_VM_Name_in_config&gt;# #&lt;UUID Changing&gt;# old_uuid=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "&lt;uuid&gt;") /usr/bin/echo old UUID $old_uuid new_uuid_part1=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 1,2) new_uuid_part2=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 4,5) new_uuid=$(/bin/echo $new_uuid_part1"-"$new_vm_port"-"$new_uuid_part2) /usr/bin/echo $new_uuid /usr/bin/sed -i "s%$old_uuid%$new_uuid%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/UUID Changing&gt;# #&lt;Spice port replace&gt;# old_spice_port=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml \ |/usr/bin/grep "graphics type='spice' port=") /bin/echo old spice port $old_spice_port new_spice_port=$(/usr/bin/echo "&lt;graphics type='spice' port='$new_vm_port' autoport='no' listen='127.0.0.1'&gt;") /bin/echo $new_spice_port /usr/bin/sed -i "s%$old_spice_port%$new_spice_port%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Spice port replace&gt;# #&lt;MAC_ADDR_GENERATE&gt;# mac_new=$(/usr/bin/hexdump -n6 -e '/1 ":%02X"' /dev/random|/usr/bin/sed s/^://g) /usr/bin/echo New Mac is $mac_new #&lt;/MAC_ADDR_GENERATE&gt;# #&lt;GET OLD MAC AND REPLACE&gt;# mac_old=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "mac address=") /usr/bin/echo old mac is $mac_old /usr/bin/sed -i "s%$mac_old%$mac_new%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;GET OLD MAC AND REPLACE&gt;# #&lt;new_disk_create&gt;# /usr/bin/qemu-img create -f qcow2 -b $hdd_base_image /var/lib/libvirt/images_write/$new_vm_name.qcow2 #&lt;/new_disk_create&gt;# #&lt;attach_new_disk_in_confiig&gt;# /usr/bin/echo hdd base image is $hdd_base_image /usr/bin/sed -i "s%&lt;source file='$hdd_base_image'/&gt;%&lt;source file='/var/lib/libvirt/images_write/$new_vm_name.qcow2'/&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/attach_new_disk_in_confiig&gt;# starting_vm #&lt;/Create_vm config&gt;# } starting_vm() { /usr/bin/virsh define $SRV_TMP_DIR/$new_vm_name.xml /usr/bin/virsh start $new_vm_name while [ $a -ne 1 ] do if /usr/bin/virsh list --all |/usr/bin/grep "$new_vm_name" |/usr/bin/grep "running" &gt; /dev/null 2&gt;&amp;1 then a=1 /usr/bin/sed -i "/$new_vm_port/d" $SRV_TMP_DIR/recycle.list /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/clear.list /usr/bin/echo "#" "$date" "VM $new_vm_name IS STARTED #" else /usr/bin/echo "#VM $new_vm_name is not ready#" a=0 /usr/bin/sleep 2s fi done /usr/bin/echo "#$date EXIT FROM VM_CREATE.SH#" exit } hdd_image_locate</span></span></code> </pre><br></div></div><br>  O processo de cria√ß√£o de uma nova m√°quina virtual <br><br>  O script vm_create.sh l√™ no arquivo de configura√ß√£o o valor da vari√°vel "base_host" que determina a m√°quina virtual de amostra com base na qual o clone ser√° feito.  Descarrega a configura√ß√£o xml da VM da base do hipervisor, realiza uma s√©rie de verifica√ß√µes na imagem de disco da VM e, ap√≥s a conclus√£o bem-sucedida, cria o arquivo de configura√ß√£o xml para a nova VM e a imagem de disco "clone vinculado" da nova VM.  Depois disso, a configura√ß√£o xml da nova VM √© carregada no banco de dados do hipervisor e a VM √© iniciada.  A porta spice_console √© transferida de recycle.list para clear.list.  A execu√ß√£o do vm_create.sh termina e a execu√ß√£o do vm_manager.sh termina. <br>  A pr√≥xima vez que voc√™ se conectar, come√ßar√° do come√ßo. <br><br>  Para casos de emerg√™ncia, o kit inclui um script vm_clear.sh que executa for√ßosamente todas as VMs do pool e as remove zerando os valores das listas.  A chamada no est√°gio de carregamento permite iniciar (abaixo) a VDI do zero. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_clear.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #set VARIABLES# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL #Set VARIABLES# /usr/bin/echo "= Cleanup ALL VM=" /usr/bin/mkdir $SRV_TMP_DIR /usr/sbin/service iptables restart /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/clear.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/waste.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/recycle.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/conn_wait.list port_to_delete=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) while [ "$port_to_delete" -gt "$SRV_START_PORT_POOL" ] do $SRV_SCRIPTS_DIR/vm_delete.sh $port_to_delete port_to_delete=$(($port_to_delete-1)) done /usr/bin/echo "= EXIT FROM VM_CLEAR.SH="</span></span></code> </pre><br></div></div><br>  Sobre isso, gostaria de terminar a primeira parte da minha hist√≥ria.  O exposto acima deve ser suficiente para os administradores de sistema experimentarem underVDI nos neg√≥cios.  Se a comunidade achar esse t√≥pico interessante, na segunda parte, falarei sobre a modifica√ß√£o do livecd Fedora e sua transforma√ß√£o em um quiosque. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt462203/">https://habr.com/ru/post/pt462203/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt462181/index.html">Regras para comunica√ß√£o eficaz em bate-papos em grupo</a></li>
<li><a href="../pt462185/index.html">A revolu√ß√£o acabou. Existe uma alternativa para uma bateria de √≠ons de l√≠tio?</a></li>
<li><a href="../pt462189/index.html">Gravando dados com travajs</a></li>
<li><a href="../pt462191/index.html">DataArt Museum: um passeio pelo norte da It√°lia</a></li>
<li><a href="../pt462197/index.html">Dicas sobre como libertar sua mente e aumentar sua criatividade</a></li>
<li><a href="../pt462205/index.html">PhDays vencedores 9 The Standoff: A cr√¥nica da equipe True0xA3</a></li>
<li><a href="../pt462209/index.html">Solu√ß√µes de v√≠deo-confer√™ncia Polycom. Mem√≥rias 6 anos depois ... Etapa 2. Parte 1. RMX1500</a></li>
<li><a href="../pt462213/index.html">Aprender e trabalhar: a experi√™ncia de graduandos da Faculdade de Tecnologia da Informa√ß√£o e Programa√ß√£o</a></li>
<li><a href="../pt462221/index.html">Qu√£o decepcionado estou no Google Play</a></li>
<li><a href="../pt462227/index.html">Moscou, 9 de agosto - Hist√≥rias de back-end 4.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>