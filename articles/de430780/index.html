<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§öüèø üëÉ üë©üèø‚Äç‚úàÔ∏è Sequenz-zu-Sequenz-Teil-1-Modelle üåø üçö üëºüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Guten Tag an alle! 

 Und wir haben wieder einen neuen Stream f√ºr den √ºberarbeiteten Data Scientist- Kurs ge√∂ffnet: einen weiteren hervorragenden Lehr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sequenz-zu-Sequenz-Teil-1-Modelle</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/430780/">  Guten Tag an alle! <br><br>  Und wir haben wieder einen neuen Stream f√ºr den √ºberarbeiteten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Scientist-</a> Kurs ge√∂ffnet: einen weiteren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hervorragenden Lehrer</a> , ein leicht verfeinertes Programm, das auf Updates basiert.  Nun, wie immer interessante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offene Lektionen</a> und Sammlungen interessanter Materialien.  Heute beginnen wir mit der Analyse von seq2seq-Modellen aus Tensor Flow. <br><br>  Lass uns gehen. <br><br>  Wie bereits im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RNN-Lernprogramm</a> erl√§utert (wir empfehlen, dass Sie sich vor dem Lesen dieses Artikels damit vertraut machen), k√∂nnen wiederkehrende neuronale Netze zur Modellierung der Sprache unterrichtet werden.  Und es stellt sich eine interessante Frage: Ist es m√∂glich, das Netzwerk auf bestimmte Daten zu trainieren, um eine aussagekr√§ftige Antwort zu erhalten?  K√∂nnen wir zum Beispiel einem neuronalen Netzwerk beibringen, von Englisch nach Franz√∂sisch zu √ºbersetzen?  Es stellt sich heraus, dass wir k√∂nnen. <br><br>  Diese Anleitung zeigt Ihnen, wie Sie ein solches End-to-End-System erstellen und trainieren.  Kopieren Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das Tensor Flow-Kernrepository</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das TensorFlow-Modellrepository von GitHub</a> .  Anschlie√üend k√∂nnen Sie das √úbersetzungsprogramm starten: <br><br><pre><code class="python hljs">cd models/tutorials/rnn/translate python translate.py --data_dir [your_data_directory]</code> </pre> <br><img src="https://habrastorage.org/webt/ra/j0/rr/raj0rraitsp6itojzydkhrk2yoi.png"><a name="habracut"></a><br><br>  Sie wird die Daten f√ºr die √úbersetzung vom Englischen ins Franz√∂sische von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der WMT'15-Website herunterladen</a> , sie f√ºr das Training vorbereiten und trainieren.  Dies erfordert ungef√§hr 20 GB auf der Festplatte und viel Zeit zum Herunterladen und Vorbereiten, sodass Sie den Vorgang jetzt starten und dieses Tutorial weiterlesen k√∂nnen. <br><br>  Das Handbuch greift auf folgende Dateien zu: <br><br><table><tbody><tr><th>  Datei </th><th>  Was ist drin? </th></tr><tr><td>  tensorflow / tensorflow / python / ops / seq2seq.py </td><td>  Bibliothek zum Erstellen von Sequenz-zu-Sequenz-Modellen </td></tr><tr><td>  models / tutorials / rnn / translate / seq2seq_model.py </td><td>  Neuronale Translationsmodelle von Sequenz zu Sequenz </td></tr><tr><td>  models / tutorials / rnn / translate / data_utils.py </td><td>  Hilfsfunktionen zur Vorbereitung von √úbersetzungsdaten </td></tr><tr><td>  models / tutorials / rnn / translate / translate.py </td><td>  Die Bin√§rdatei, die das √úbersetzungsmodell trainiert und ausf√ºhrt </td></tr></tbody></table><br>  <b>Grundlagen von Sequenz zu Sequenz</b> <br><br>  Das von <a href="">Cho et al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pdf</a> ) vorgestellte grundlegende Sequenz-zu-Sequenz-Modell besteht aus zwei wiederkehrenden neuronalen Netzen (RNNs): einem Codierer (Codierer), der die Eingabedaten verarbeitet, und einem Decodierer (Decodierer), der die Daten generiert Ausgabe.  Die grundlegende Architektur ist unten dargestellt: <br><br><img src="https://habrastorage.org/webt/e-/df/cu/e-dfcuvlsbykvyxvzac9rc0nrow.png"><br><br>  Jedes Rechteck im obigen Bild repr√§sentiert eine Zelle im RNN, normalerweise eine GRU-Zelle - einen kontrollierten Wiederholungsblock oder eine LSTM-Zelle - Langzeit-Kurzzeitged√§chtnis (lesen Sie das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RNN-Tutorial</a> , um mehr dar√ºber zu erfahren).  Encoder und Decoder k√∂nnen gemeinsame Gewichte haben oder h√§ufiger unterschiedliche Parameters√§tze verwenden.  Mehrschichtzellen wurden erfolgreich in Sequenz-zu-Sequenz-Modellen verwendet, um beispielsweise <a href="">Sutskever et al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pdf</a> ) zu √ºbersetzen. <br><br>  In dem oben beschriebenen Grundmodell muss jeder Eingang in einen Zustandsvektor fester Gr√∂√üe codiert werden, da dies das einzige ist, was an den Decoder √ºbertragen wird.  Um dem Decoder einen direkteren Zugriff auf Eingabedaten zu erm√∂glichen, wurde in <a href="">Bahdanau et al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pdf</a> ) ein Aufmerksamkeitsmechanismus eingef√ºhrt.  Wir werden nicht auf Details des Aufmerksamkeitsmechanismus eingehen (dazu k√∂nnen Sie sich hier mit der Arbeit vertraut machen);  Es gen√ºgt zu sagen, dass es dem Decodierer erm√∂glicht, bei jedem Decodierungsschritt in die Eingabedaten zu schauen.  Ein mehrschichtiges Sequenz-zu-Sequenz-Netzwerk mit LSTM-Zellen und dem Aufmerksamkeitsmechanismus im Decoder ist wie folgt: <br><br><img src="https://habrastorage.org/webt/c4/ro/0z/c4ro0zvzu8m-y4qjlnfbhv9x4qa.png"><br><br>  <b>TensorFlow-Bibliothek seq2seq</b> <br><br>  Wie Sie oben sehen k√∂nnen, gibt es verschiedene Sequenz-zu-Sequenz-Modelle.  Alle von ihnen k√∂nnen unterschiedliche RNN-Zellen verwenden, aber alle akzeptieren Encoder-Eingangsdaten und Decoder-Eingangsdaten.  Dies ist die Basis der TensorFlow seq2seq-Bibliotheksschnittstelle (tensorflow / tensorflow / python / ops / seq2seq.py).  Dieses grundlegende RNN-, Codec-Sequenz-zu-Sequenz-Modell funktioniert wie folgt. <br><br><pre> <code class="python hljs">outputs, states = basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)</code> </pre> <br>  In dem oben angegebenen Aufruf ist <code>encoder_inputs</code> eine Liste von Tensoren, die die Eingangsdaten des Encoders darstellen und den Buchstaben A, B, C aus dem obigen Bild entsprechen.  In √§hnlicher Weise sind <code>decoder_inputs</code> Tensoren, die Decoder-Eingangsdaten darstellen.  GO, W, X, Y, Z vom ersten Bild. <br><br>  Das <code>cell</code> ist eine Instanz der Klasse <code>tf.contrib.rnn.RNNCell</code> , die bestimmt, welche Zelle im Modell verwendet wird.  Sie k√∂nnen vorhandene Zellen verwenden, z. B. <code>GRUCell</code> oder <code>LSTMCell</code> , oder Sie k√∂nnen Ihre eigenen schreiben.  Dar√ºber hinaus bietet <code>tf.contrib.rnn</code> Shells zum Erstellen von mehrschichtigen Zellen, zum Hinzuf√ºgen von Ausnahmen zur <code>tf.contrib.rnn</code> und -ausgabe oder zu anderen Transformationen.  Beispiele finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RNN-Tutorial</a> . <br><br>  Der Aufruf <code>basic_rnn_seq2seq</code> gibt zwei Argumente zur√ºck: <code>outputs</code> und <code>states</code> .  Beide repr√§sentieren eine Liste von Tensoren mit der gleichen L√§nge wie <code>decoder_inputs</code> .  <code>outputs</code> entsprechen den Decoderausgangsdaten zu jedem Zeitschritt, im ersten Bild sind es W, X, Y, Z, EOS.  Die zur√ºckgegebenen <code>states</code> repr√§sentieren den internen Zustand des Decoders zu jedem Zeitschritt. <br><br>  In vielen Anwendungen, die das Sequenz-zu-Sequenz-Modell verwenden, wird der Decoderausgang zum Zeitpunkt t zum Zeitpunkt t + 1 an den Eingang zum Decodierer zur√ºckgesendet.  W√§hrend des Testens, w√§hrend der Sequenzdecodierung, wird auf diese Weise eine neue erstellt.  Andererseits ist es w√§hrend des Trainings √ºblich, zu jedem Zeitschritt die richtigen Eingabedaten an den Decoder zu senden, selbst wenn der Decoder zuvor falsch war.  Funktionen in <code>seq2seq.py</code> unterst√ºtzen beide Modi mit dem Argument <code>feed_previous</code> .  Betrachten Sie beispielsweise die folgende Verwendung eines verschachtelten RNN-Modells. <br><br><pre> <code class="python hljs">outputs, states = embedding_rnn_seq2seq( encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, output_projection=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, feed_previous=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br>  Im <code>embedding_rnn_seq2seq</code> Modell sind alle Eingabedaten (sowohl <code>encoder_inputs</code> als auch <code>decoder_inputs</code> ) ganzzahlige Tensoren, die diskrete Werte widerspiegeln.  Sie werden in einer engen Darstellung verschachtelt (Einzelheiten zum Anhang finden Sie im <code>num_encoder_symbols</code> zu <code>num_encoder_symbols</code> diese Anh√§nge zu erstellen, m√ºssen Sie jedoch die maximale Anzahl diskreter Zeichen angeben: <code>num_encoder_symbols</code> auf der Encoderseite und <code>num_decoder_symbols</code> auf der Decoderseite. <br><br>  Im obigen Aufruf setzen wir <code>feed_previous</code> auf False.  Dies bedeutet, dass der Decoder die Tensoren <code>decoder_inputs</code> in der Form verwendet, in der sie bereitgestellt werden.  Wenn wir <code>feed_previous</code> auf True setzen, verwendet der Decoder nur das erste <code>decoder_inputs</code> Element.  Alle anderen Tensoren aus der Liste werden ignoriert und stattdessen der vorherige Wert des Decoderausgangs verwendet.  Dies wird verwendet, um √úbersetzungen in unserem √úbersetzungsmodell zu dekodieren, kann aber auch w√§hrend des Trainings verwendet werden, um die Fehlerstabilit√§t des Modells zu verbessern.  Ungef√§hr wie bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bengio et al., 2015</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pdf</a> ). <br><br>  Ein weiteres wichtiges Argument, das oben verwendet wurde, ist <code>output_projection</code> .  Ohne Klarstellung sind die Schlussfolgerungen des eingebetteten Modells Tensoren in Form der Anzahl der Trainingsmuster pro <code>num_decoder_symbols</code> , da sie die Logiths jedes generierten Symbols darstellen.  Wenn Trainingsmodelle mit gro√üen Ausgabew√∂rterb√ºchern, beispielsweise mit gro√üen <code>num_decoder_symbols</code> , <code>num_decoder_symbols</code> , wird das Speichern dieser gro√üen Tensoren unpraktisch.  Stattdessen ist es besser, kleinere Tensoren zur√ºckzugeben, die anschlie√üend mit <code>output_projection</code> auf den gro√üen Tensor <code>output_projection</code> .  Dies erm√∂glicht es uns, unsere seq2seq-Modelle mit abgetasteten Softmax-Verlusten zu verwenden, wie von <a href="">Jean et.</a>  <a href="">al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pdf</a> ). <br><br>  Zus√§tzlich zu <code>basic_rnn_seq2seq</code> und <code>embedding_rnn_seq2seq</code> gibt es in <code>seq2seq.py</code> mehrere weitere Sequenz-zu-Sequenz-Modelle.  Achten Sie auf sie.  Alle von ihnen haben eine √§hnliche Oberfl√§che, daher werden wir uns nicht mit ihren Details befassen.  Verwenden Sie f√ºr unser √úbersetzungsmodell unten <code>embedding_attention_seq2seq</code> . <br><br>  Fortsetzung folgt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de430780/">https://habr.com/ru/post/de430780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de430768/index.html">Interview mit ADOM-Erfinder Thomas Biscap</a></li>
<li><a href="../de430770/index.html">Backup f√ºr Linux oder wie man einen Snapshot erstellt</a></li>
<li><a href="../de430774/index.html">Bist du bereit f√ºr KI auf Werbetafeln?</a></li>
<li><a href="../de430776/index.html">Eine IP zu erstellen ist der einzige Weg</a></li>
<li><a href="../de430778/index.html">3DEXPERIENCE End-to-End-Entwurfsprozess f√ºr elektrische Systeme</a></li>
<li><a href="../de430782/index.html">Wie viele Programmierer ben√∂tigen Sie, um zuvor geschriebenen Code zu unterst√ºtzen?</a></li>
<li><a href="../de430784/index.html">Vom Junior zum Regisseur: Geschichten einer Wache</a></li>
<li><a href="../de430788/index.html">Meine Interviewhistorie bei IB IT (Java-Entwickler, Investmentbank) in London mit Beispielen f√ºr typische Aufgaben</a></li>
<li><a href="../de430790/index.html">Ledger Nano S: Der Schl√ºssel zu dem Raum, in dem 710 Token und Kryptow√§hrungen liegen k√∂nnen</a></li>
<li><a href="../de430792/index.html">Erstellen einer Gliederung f√ºr LWRP in Unity</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>