<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🎨 👩🏿‍🔬 🤸 Bases de données et Kubernetes (revue et rapport vidéo) 👩‍👩‍👧 💩 🈯️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le 8 novembre, dans le hall principal de la conférence HighLoad ++ 2018 , dans le cadre de la section DevOps et opérations, un rapport a été établi in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bases de données et Kubernetes (revue et rapport vidéo)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/431500/">  Le 8 novembre, dans le hall principal de la conférence <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HighLoad ++ 2018</a> , dans le cadre de la section DevOps et opérations, un rapport a été établi intitulé Bases de données et Kubernetes.  Il parle de la haute disponibilité des bases de données et des approches de tolérance aux pannes pour Kubernetes et avec lui, ainsi que des options pratiques pour placer le SGBD dans les clusters Kubernetes et les solutions existantes pour cela (y compris Stolon pour PostgreSQL). <br><br><img src="https://habrastorage.org/webt/oq/mh/kp/oqmhkpy4pxg-olk9yybf_julwvu.jpeg"><br><br>  Par tradition, nous sommes heureux de présenter une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>vidéo avec un rapport</b></a> (environ une heure, <b>beaucoup plus</b> informatif <b>que l'</b> article) et la principale compression sous forme de texte.  C'est parti! <a name="habracut"></a><br><br><h2>  Théorie </h2><br>  Ce rapport est apparu comme une réponse à l'une des questions les plus populaires que nous avons posées sans relâche ces dernières années à différents endroits: commentaires sur Habr ou YouTube, réseaux sociaux, etc.  Cela semble simple: "Est-il possible d'exécuter la base de données dans Kubernetes?", Et si nous répondions généralement "généralement oui, mais ...", alors il n'y avait clairement pas assez d'explications pour ces "en général" et "mais", mais pour les adapter dans un court message n'a pas réussi. <br><br>  Cependant, pour commencer, je résume le problème de la "base de données [données]" à l'état dans son ensemble.  Un SGBD n'est qu'un cas particulier de décisions avec état, dont une liste plus complète peut être représentée comme suit: <br><br><img src="https://habrastorage.org/webt/px/ps/2_/pxps2_ff80ru5qfth8bduiu_kdw.png"><br><br>  Avant d'examiner des cas spécifiques, je parlerai de trois caractéristiques importantes du travail / de l'utilisation de Kubernetes. <br><br><h3>  1. Philosophie de haute disponibilité de Kubernetes </h3><br>  Tout le monde connaît l'analogie «animaux de compagnie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">vs bétail</a> » et comprend que si Kubernetes est une histoire du monde du troupeau, alors les SGBD classiques ne sont que des animaux de compagnie. <br><br>  Et à quoi ressemblait l'architecture des «animaux de compagnie» dans la version «traditionnelle»?  Un exemple classique d'installation de MySQL est la réplication sur deux serveurs de fer avec une alimentation redondante, un disque, un réseau ... et tout le reste (y compris un ingénieur et divers outils auxiliaires), qui nous aideront à être sûrs que le processus MySQL n'échouera pas, et s'il y a un problème avec l'un des éléments critiques pour ses composants, la tolérance aux pannes sera respectée: <br><br><img src="https://habrastorage.org/webt/0m/qg/s3/0mqgs3e3cco1zukmlah1jg2ubjs.png"><br><br>  À quoi ressemblera la même chose dans Kubernetes?  Ici, il y a généralement beaucoup plus de serveurs de fer, ils sont plus simples et ils n'ont pas d'alimentation et de réseau redondants (dans le sens où la perte d'une machine n'affecte rien) - tout cela est combiné dans un cluster.  Sa tolérance aux pannes est fournie par le logiciel: si quelque chose arrive au nœud, Kubernetes détecte et démarre les instances nécessaires sur l'autre nœud. <br><br>  Quels sont les mécanismes de haute disponibilité dans les K8? <br><br><img src="https://habrastorage.org/webt/n2/gw/mh/n2gwmhiogm3uzv1m5igrzqpyifq.png"><br><br><ol><li>  Contrôleurs  Il en existe de nombreux, mais deux principaux: <code>Deployment</code> (pour les applications sans état) et <code>StatefulSet</code> (pour les applications avec état).  Ils stockent toute la logique des actions entreprises en cas de panne d'un nœud (inaccessibilité du pod). </li><li>  <code>PodAntiAffinity</code> - la possibilité de spécifier des pods spécifiques afin qu'ils ne se trouvent pas sur le même noeud. </li><li>  <code>PodDisruptionBudgets</code> - limite le nombre d'instances de pod qui peuvent être désactivées en même temps en cas de travail planifié. </li></ol><br><h3>  2. Garantie de cohérence Kubernetes </h3><br>  Comment fonctionne le schéma de tolérance de panne à maître unique familier?  Deux serveurs (maître et secours), dont l'un est constamment accessible par l'application, qui à son tour est utilisé via l'équilibreur de charge.  Que se passe-t-il en cas de problème de réseau? <br><br><img src="https://habrastorage.org/webt/6p/1k/ve/6p1kvelnrzyrphtu6sb_agftgaa.gif"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><i>Split-brain</i></a> classique: l'application commence à accéder aux deux instances du SGBD, dont chacune se considère comme la principale.  Pour éviter cela, keepalived a été remplacé par corosync avec déjà trois instances pour atteindre un quorum lors du vote pour le maître.  Cependant, même dans ce cas, il y a des problèmes: si une instance de SGBD tombée en panne tente de "se tuer" de toutes les manières possibles (supprimer l'adresse IP, traduire la base de données en lecture seule ...), alors l'autre partie du cluster ne sait pas ce qui est arrivé au maître - cela pourrait arriver, que ce noeud fonctionne toujours et que les requêtes y parviennent, ce qui signifie que nous ne pouvons toujours pas changer d'assistant. <br><br>  Pour résoudre cette situation, il existe un mécanisme pour isoler le nœud afin de protéger l'ensemble du cluster contre un fonctionnement incorrect - ce processus est appelé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><i>clôture</i></a> .  L'essence pratique se résume au fait que nous essayons par des moyens externes de "tuer" la voiture tombée.  Les approches peuvent être différentes: de la mise hors tension de la machine via IPMI et du blocage du port du commutateur à l'accès à l'API du fournisseur de cloud, etc.  Et ce n'est qu'après cette opération que vous pouvez changer d'assistant.  Cela garantit une garantie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><i>au plus une fois</i></a> qui nous assure la <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cohérence</a></i> . <br><br><img src="https://habrastorage.org/webt/sl/xb/9r/slxb9rwf-lk8mcdaeqiuankz3z8.png"><br><br>  Comment réaliser la même chose dans Kubernetes?  Pour ce faire, il existe déjà des contrôleurs dont le comportement en cas d'inaccessibilité d'un nœud est différent: <br><br><ol><li>  <code>Deployment</code> : "On m'a dit qu'il devrait y avoir 3 pods, et maintenant il n'y a plus que 2 pods - j'en créerai un nouveau"; </li><li>  <code>StatefulSet</code> : "Pod disparu?"  J'attendrai: soit ce nœud reviendra, soit ils nous diront de le tuer ",  les conteneurs eux-mêmes (sans action de l'opérateur) ne sont pas recréés.  C'est ainsi que la même garantie au plus une fois est obtenue. </li></ol><br>  Cependant, ici, dans ce dernier cas, une clôture est nécessaire: nous avons besoin d'un mécanisme qui confirme que ce nœud est définitivement parti.  Le rendre automatique est, d'une part, très difficile (de nombreuses implémentations sont nécessaires), et d'autre part, pire encore, il tue généralement les nœuds lentement (l'accès à IPMI peut prendre des secondes ou des dizaines de secondes, voire des minutes).  Peu de gens sont satisfaits de l'attente par minute pour passer de la base au nouveau maître.  Mais il existe une autre approche qui ne nécessite pas de mécanisme de clôture ... <br><br>  Je vais commencer sa description en dehors de Kubernetes.  Il utilise un équilibreur de charge spécial à travers lequel les backends accèdent au SGBD.  Sa spécificité réside dans le fait qu'il a la propriété de la cohérence, c'est-à-dire  protection contre les pannes de réseau et le split-brain, car il vous permet de supprimer toutes les connexions au maître actuel, d'attendre la synchronisation (réplique) sur un autre nœud et de basculer vers celui-ci.  Je n'ai pas trouvé de terme établi pour cette approche et je l'ai appelé <i>Switchover cohérent</i> . <br><br><img src="https://habrastorage.org/webt/ct/oq/lk/ctoqlkp3efyctjlvsyiheesk2s4.gif"><br><br>  La question principale avec lui est de savoir comment la rendre universelle, en fournissant un support aux fournisseurs de cloud et aux installations privées.  Pour cela, des serveurs proxy sont ajoutés aux applications.  Chacun d'eux acceptera les demandes de sa candidature (et les transmettra au SGBD), et un quorum sera recueilli de chacun d'eux.  Dès qu'une partie du cluster échoue, les mandataires qui ont perdu le quorum suppriment immédiatement leurs connexions au SGBD. <br><br><img src="https://habrastorage.org/webt/nj/y0/-t/njy0-tahnwp8uxeyevxot_tlntq.png"><br><br><h3>  3. Stockage de données et Kubernetes </h3><br>  Le mécanisme principal est le lecteur réseau <i>Network Block Device</i> (aka SAN) dans diverses implémentations pour les options de cloud souhaitées ou le bare metal.  Cependant, le fait de mettre une base de données chargée (par exemple, MySQL, qui nécessite 50 000 IOPS) dans le cloud (AWS EBS) ne fonctionnera pas en raison de la <i>latence</i> . <br><br><img src="https://habrastorage.org/webt/qq/wp/r7/qqwpr7fae-nbek0y2o6ex9lbzsa.png"><br><br>  Kubernetes pour de tels cas a la capacité de connecter un disque dur <i>local</i> - <i>Stockage local</i> .  Si une panne se produit (le disque n'est plus disponible dans le pod), nous sommes alors obligés de réparer cette machine - similaire au schéma classique en cas de panne d'un serveur fiable. <br><br>  Les deux options ( <i>Network Block Device</i> et <i>Local Storage</i> ) appartiennent à la catégorie <i>ReadWriteOnce</i> : le stockage ne peut pas être monté à deux endroits (pods) - pour cette mise à l'échelle, vous devrez créer un nouveau disque et le connecter à un nouveau pod (il existe un mécanisme K8 intégré pour cela) , puis remplissez avec les données nécessaires (déjà effectuées par nos forces). <br><br>  Si nous avons besoin du mode <i>ReadWriteMany</i> , des implémentations <i>Network File System</i> (ou NAS) sont disponibles: pour le cloud public, ce sont <code>AzureFile</code> et <code>AWSElasticFileSystem</code> , et pour leurs installations CephFS et Glusterfs pour les fans de systèmes distribués, ainsi que NFS. <br><br><img src="https://habrastorage.org/webt/eq/y6/gp/eqy6gpf2duz9ljtzc342bj9upg4.png"><br><br><h2>  Pratique </h2><br><h3>  1. Autonome </h3><br>  Cette option concerne le cas où rien ne vous empêche de démarrer le SGBD en mode serveur séparé avec stockage local.  Il n'est pas question de haute disponibilité ... bien qu'elle puisse être dans une certaine mesure (c'est-à-dire suffisante pour cette application) mise en œuvre au niveau du fer.  Il existe de nombreux cas pour cette application.  Tout d'abord, ce sont toutes sortes d'environnements de développement et de développement, mais pas seulement: les services secondaires arrivent également ici, leur désactivation pendant 15 minutes n'est pas critique.  Dans Kubernetes, ceci est implémenté par <code>StatefulSet</code> avec un pod: <br><br><img src="https://habrastorage.org/webt/hl/xk/ha/hlxkhaodepe50imvuobtoilrz6o.png"><br><br>  En général, c'est une option viable, qui, de mon point de vue, n'a aucun inconvénient par rapport à l'installation d'un SGBD sur une machine virtuelle distincte. <br><br><h3>  2. Paire répliquée avec commutation manuelle </h3><br>  <code>StatefulSet</code> nouveau utilisé, mais le schéma général ressemble à ceci: <br><br><img src="https://habrastorage.org/webt/vq/_i/to/vq_itonyvigrh0uezqek_lwtlt4.png"><br><br>  Si l'un des nœuds plante ( <code>mysql-a-0</code> ), un miracle ne se produit pas, mais nous avons une réplique ( <code>mysql-b-0</code> ) vers laquelle nous pouvons commuter le trafic.  Dans ce cas, même avant de commuter le trafic, il est important de ne pas oublier non seulement de supprimer les requêtes SGBD du service <code>mysql</code> , mais également de se connecter manuellement au SGBD et de s'assurer que toutes les connexions sont terminées (les tuer), et aussi d'aller au deuxième nœud du SGBD et reconfigurer la réplique dans la direction opposée. <br><br>  Si vous utilisez actuellement la version classique avec deux serveurs (maître + veille) sans <i>basculement</i> automatique, cette solution est l'équivalent dans Kubernetes.  Convient pour MySQL, PostgreSQL, Redis et autres produits. <br><br><h3>  3. Mise à l'échelle de la charge de lecture </h3><br>  En fait, ce cas n'est pas avec état, car nous ne parlons que de lecture.  Ici, le serveur SGBD principal est en dehors du schéma considéré, et dans le cadre de Kubernetes, une "batterie de serveurs esclaves" est créée, qui est en lecture seule.  Le mécanisme général - l'utilisation de conteneurs init pour remplir les données du SGBD sur chaque nouveau pod de cette batterie (en utilisant un vidage à chaud ou l'habituel avec des actions supplémentaires, etc. - dépend du SGBD utilisé).  Pour être sûr que chaque instance ne traîne pas trop loin du maître, vous pouvez utiliser des tests de vivacité. <br><br><img src="https://habrastorage.org/webt/nz/pd/uk/nzpdukumat3zbax7vnkcs5jtsvs.png"><br><br><h3>  4. Client intelligent </h3><br>  Si vous créez un <code>StatefulSet</code> de trois memcaches, Kubernetes fournit un service spécial qui n'équilibrera pas les demandes, mais créera chaque pod pour son propre domaine.  Le client pourra travailler avec eux s'il est lui-même capable de partitionnement et de réplication. <br><br>  Vous n'avez pas besoin d'aller loin pour un exemple: voici comment le stockage de session fonctionne en PHP.  Pour chaque demande de session, des demandes sont faites simultanément à tous les serveurs, après quoi la réponse la plus pertinente est sélectionnée parmi eux (de manière similaire à un enregistrement). <br><br><img src="https://habrastorage.org/webt/t8/iz/26/t8iz261adru0mbdw7cd2o5i3y8o.png"><br><br><h3>  5. Solutions natives cloud </h3><br>  Il existe de nombreuses solutions qui sont initialement axées sur la défaillance des nœuds, c'est-à-dire  eux-mêmes peuvent effectuer le <i>basculement</i> et la récupération des nœuds, fournir des garanties de <i>cohérence</i> .  Ce n'est pas une liste complète d'entre eux, mais seulement une partie d'exemples populaires: <br><br><img src="https://habrastorage.org/webt/9u/ah/qz/9uahqzayfbdsokgyod153jwfjfs.png"><br><br>  Tous sont simplement placés dans <code>StatefulSet</code> , après quoi les nœuds se retrouvent et forment un cluster.  Les produits eux-mêmes diffèrent dans la façon dont ils mettent en œuvre trois choses: <br><br><ol><li>  Comment les nœuds apprennent-ils les uns des autres?  Il existe des méthodes telles que l'API Kubernetes, les enregistrements DNS, la configuration statique, les nœuds spécialisés (semences), la découverte de services tiers ... </li><li>  Comment le client se connecte-t-il?  Grâce à un équilibreur de charge qui distribue aux hôtes, ou le client doit connaître tous les hôtes, et il décidera de la marche à suivre. </li><li>  Comment s'effectue la mise à l'échelle horizontale?  Pas du tout, complet ou difficile / avec restrictions. </li></ol><br>  Quelles que soient les solutions choisies à ces problèmes, tous ces produits fonctionnent bien avec Kubernetes, car ils ont été créés à l'origine en tant que «troupeau» <i>(bovins)</i> . <br><br><h3>  6. Stolon PostgreSQL </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stolon</a> vous permet en fait de transformer PostgreSQL, créé comme <i>animal</i> de <i>compagnie</i> , en <i>bétail</i> .  Comment y parvient-on? <br><br><img src="https://habrastorage.org/webt/g_/z4/pc/g_z4pcehfw6p985duphcgrbukzo.png"><br><br><ul><li>  Tout d'abord, nous avons besoin d'une découverte de service, dans le rôle de laquelle peut être <b>etcd</b> (d'autres options sont disponibles) - un cluster d'entre eux est placé dans un <code>StatefulSet</code> . </li><li>  Une autre partie de l'infrastructure est <code>StatefulSet</code> avec des instances PostgreSQL.  En plus du SGBD proprement dit, à côté de chaque installation se trouve également un composant appelé <b>keeper</b> , qui effectue la configuration du SGBD. </li><li>  Un autre composant, <b>sentinelle,</b> est déployé en tant que <code>Deployment</code> et surveille la configuration du cluster.  C'est lui qui décide qui sera maître et veille, écrit ces informations sur etcd.  Et le gardien lit les données de etcd et effectue des actions correspondant à l'état actuel avec une instance de PostgreSQL. </li><li>  Un autre composant déployé dans le <code>Deployment</code> et face aux instances PostgreSQL, le <b>proxy,</b> est une implémentation du modèle de <i>basculement cohérent</i> déjà mentionné.  Ces composants sont connectés à etcd, et si cette connexion est perdue, le proxy tue immédiatement les connexions sortantes, car à partir de ce moment il ne connaît pas le rôle de son serveur (est-ce maintenant maître ou standby?). </li><li>  Enfin, les instances proxy font face à l'habituel <code>LoadBalancer</code> Kubernetes. </li></ul><br><h2>  Conclusions </h2><br>  Est-il donc possible de s'installer à Kubernetes?  Oui, bien sûr, c'est possible, dans certains cas ... Et si c'est approprié, c'est fait comme ça (voir le workflow Stolon) ... <br><br>  Tout le monde sait que la technologie évolue par vagues.  Au départ, tout nouvel appareil peut être très difficile à utiliser, mais au fil du temps, tout change: la technologie devient disponible.  Où allons-nous?  Oui, cela restera ainsi à l'intérieur, mais nous ne saurons pas comment cela fonctionnera.  Kubernetes développe activement des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">opérateurs</a> .  Jusqu'à présent, il n'y en a pas beaucoup et ils ne sont pas si bons, mais il y a un mouvement dans cette direction. <br><br><h2>  Vidéos et diapositives </h2><br>  Vidéo de la performance (environ une heure): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BnegHj53pW4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Présentation du rapport: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  PS Nous avons également trouvé sur le net une très (!) Courte <a href="">compression textuelle</a> de ce rapport - merci à Nikolai Volynkin. <br><br><h2>  PPS </h2><br>  Autres reportages sur notre blog: <br><br><ul><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Surveillance et Kubernetes</a> »;  <i>(Dmitry Stolyarov; 28 mai 2018 à RootConf)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Meilleures pratiques CI / CD avec Kubernetes et GitLab</a> »;  <i>(Dmitry Stolyarov; 7 novembre 2017 à HighLoad ++)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Notre expérience avec Kubernetes dans les petits projets</a> »;  <i>(Dmitry Stolyarov; 6 juin 2017 à RootConf)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nous collectons des images Docker pour CI / CD rapidement et facilement avec dapp</a> » <i>(Dmitry Stolyarov; 8 novembre 2016 sur HighLoad ++)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pratiques de livraison continue avec Docker</a> » <i>(Dmitry Stolyarov; 31 mai 2016 à RootConf)</i> . </li></ul><br>  Vous pourriez également être intéressé par les publications suivantes: <br><br><ul><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Trucs et astuces de Kubernetes: accélérer le bootstrap des grandes bases de données</a> »; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CockroachDB DBMS Orchestration in Kubernetes</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr431500/">https://habr.com/ru/post/fr431500/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr431488/index.html">Modulation sonore</a></li>
<li><a href="../fr431490/index.html">Externe - GUI pour Golang</a></li>
<li><a href="../fr431492/index.html">Analyse du quiz du concours React depuis le stand HeadHunter à HolyJs 2018</a></li>
<li><a href="../fr431496/index.html">Comment la technologie aide les enseignants des classes spéciales</a></li>
<li><a href="../fr431498/index.html">WebP reprendra bientôt le Web, mais ce ne sera pas long</a></li>
<li><a href="../fr431502/index.html">Conférence pour les développeurs iOS Kolesa Mobile 3.0. Reportage vidéo</a></li>
<li><a href="../fr431504/index.html">Phishing - fonctionne. Chronique du vol de l'iPhone XS suivi du vol de données iCloud</a></li>
<li><a href="../fr431506/index.html">Xcode et débogage avancé dans LLDB: Partie 1</a></li>
<li><a href="../fr431508/index.html">Gestion efficace des transactions au printemps</a></li>
<li><a href="../fr431510/index.html">Comment collecter des informations sur le contour. Acheter avec du sélénium</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>