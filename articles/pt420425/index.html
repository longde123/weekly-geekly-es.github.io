<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚ÄçüöÄ üßë‚Äçü§ù‚Äçüßë üë®üèΩ‚ÄçüöÄ Teoria e pr√°tica do uso do HBase üöÉ üè¶ üö∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Boa tarde Meu nome √© Danil Lipova, nossa equipe na Sbertech come√ßou a usar o HBase como reposit√≥rio de dados operacionais. Durante seu estudo, foi adq...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teoria e pr√°tica do uso do HBase</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/420425/">  Boa tarde  Meu nome √© Danil Lipova, nossa equipe na Sbertech come√ßou a usar o HBase como reposit√≥rio de dados operacionais.  Durante seu estudo, foi adquirida experi√™ncia que eu queria sistematizar e descrever (esperamos que seja √∫til para muitos).  Todas as experi√™ncias abaixo foram realizadas com vers√µes do HBase 1.2.0-cdh5.14.2 e 2.0.0-cdh6.0.0-beta1. <br><br><ol><li>  Arquitetura geral </li><li>  Gravando dados no HBASE </li><li>  Lendo dados do HBASE </li><li>  Armazenamento em cache de dados </li><li>  Processamento em lote MultiGet / MultiPut </li><li>  Estrat√©gia para dividir tabelas em regi√µes (derramamento) </li><li>  Toler√¢ncia a falhas, compacta√ß√£o e localidade dos dados </li><li>  Configura√ß√µes e desempenho </li><li>  Teste de carga </li><li>  Conclus√µes </li></ol><a name="habracut"></a><br><h2>  1. Arquitetura geral </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y9/wa/vl/y9wavltynzhs9r_7v5sn1d8ff68.png"></div><br>  O mestre em espera ouve a pulsa√ß√£o ativa no n√≥ ZooKeeper e, em caso de desaparecimento, assume as fun√ß√µes do mestre. <br><br><h2>  2. Gravando dados no HBASE </h2><br>  Primeiro, considere o caso mais simples - escrever um objeto de valor-chave em uma determinada tabela usando put (rowkey).  O cliente deve primeiro descobrir onde est√° localizado o servidor da regi√£o raiz (RRS) que armazena a tabela hbase: meta.  Ele recebe essas informa√ß√µes do ZooKeeper.  Em seguida, ele se volta para o RRS e l√™ a tabela hbase: meta, da qual recupera as informa√ß√µes de que o RegionServer (RS) √© respons√°vel por armazenar dados para a chave de linha especificada na tabela de seu interesse.  Para uso futuro, a tabela meta √© armazenada em cache pelo cliente e, portanto, as chamadas subsequentes s√£o mais r√°pidas, diretamente para o RS. <br><br>  Em seguida, o RS, tendo recebido a solicita√ß√£o, primeiro grava-a no WriteAheadLog (WAL), necess√°rio para a recupera√ß√£o no caso de uma falha.  Em seguida, ele salva os dados no MemStore.  Este √© um buffer na mem√≥ria que cont√©m um conjunto classificado de chaves para uma determinada regi√£o.  A tabela pode ser dividida em regi√µes (parti√ß√µes), cada uma das quais cont√©m um conjunto de chaves separado.  Isso permite a coloca√ß√£o de regi√µes em diferentes servidores para obter maior desempenho.  No entanto, apesar da obviedade dessa afirma√ß√£o, veremos mais adiante que isso n√£o funciona em todos os casos. <br><br>  Ap√≥s colocar o registro no MemStore, o cliente recebe uma resposta de que o registro foi salvo com sucesso.  Ao mesmo tempo, ele √© realmente armazenado apenas no buffer e chega ao disco somente ap√≥s um certo per√≠odo de tempo ou quando √© preenchido com novos dados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xt/xi/p7/xtxip7moylyjdgqggsqiu8j_cm4.png"></div><br>  Ao executar a opera√ß√£o ‚ÄúExcluir‚Äù, a exclus√£o de dados f√≠sicos n√£o ocorre.  Eles s√£o simplesmente marcados como exclu√≠dos e a pr√≥pria destrui√ß√£o ocorre quando a principal fun√ß√£o compacta √© chamada, descrita em mais detalhes na Se√ß√£o 7. <br><br>  Os arquivos no formato HFile s√£o acumulados no HDFS e, de tempos em tempos, inicia um pequeno processo compacto, que simplesmente cola arquivos pequenos em arquivos maiores sem excluir nada.  Com o tempo, isso se transforma em um problema que se manifesta apenas ao ler dados (retornaremos a isso mais tarde). <br><br>  Al√©m do processo de inicializa√ß√£o descrito acima, h√° um procedimento muito mais eficiente, que provavelmente √© o lado mais poderoso desse banco de dados - o BulkLoad.  Consiste no fato de criarmos HFiles independentemente e coloc√°-lo no disco, o que nos permite escalar perfeitamente e obter velocidades muito decentes.  De fato, a limita√ß√£o aqui n√£o √© o HBase, mas as possibilidades de ferro.  Abaixo est√£o os resultados do carregamento em um cluster composto por 16 RegionServers e 16 NodeManager YARN (CPU Xeon E5-2680 v4 a 2.40GHz * 64 threads), vers√£o HBase 1.2.0-cdh5.14.2. <br><br><img src="https://habrastorage.org/webt/ro/bu/hf/robuhfegpwqyed6gmwg7he2bmfk.png"><br><br>  Pode-se observar que, aumentando o n√∫mero de parti√ß√µes (regi√µes) na tabela, bem como os execut√°veis ‚Äã‚ÄãSpark, obtemos um aumento na velocidade de download.  Al√©m disso, a velocidade depende da quantidade de grava√ß√£o.  Blocos grandes proporcionam um aumento na medi√ß√£o de MB / s, pequenos no n√∫mero de registros inseridos por unidade de tempo, sendo todas as outras coisas iguais. <br><br>  Voc√™ tamb√©m pode iniciar o carregamento em duas tabelas ao mesmo tempo e obter uma duplica√ß√£o de velocidade.  Pode-se observar abaixo que blocos de 10 KB s√£o gravados em duas tabelas ao mesmo tempo com uma velocidade de cerca de 600 Mb / s cada (total de 1275 Mb / s), o que coincide com a velocidade de grava√ß√£o de 623 MB / s em uma tabela (consulte o n¬∫ 11 acima) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gr/05/vp/gr05vpmhauzclwmn310erbgj22u.png"></div><br>  Mas o segundo lan√ßamento com registros de 50 KB mostra que a velocidade do download j√° est√° crescendo um pouco, o que indica uma aproxima√ß√£o aos valores limite.  Deve-se ter em mente que praticamente n√£o h√° carga no HBASE, tudo o que √© necess√°rio √© fornecer primeiro os dados do hbase: meta e, depois de alinhar os HFiles, liberar os dados do BlockCache e salvar o buffer do MemStore em disco, se n√£o houver. vazio. <br><br><h2>  3. Lendo dados do HBASE </h2><br>  Se assumirmos que todas as informa√ß√µes do hbase: meta j√° possuem um cliente (consulte a se√ß√£o 2), a solicita√ß√£o ser√° enviada imediatamente para o RS onde a chave desejada est√° armazenada.  Primeiro, a pesquisa √© feita no MemCache.  Independentemente de haver dados l√° ou n√£o, a pesquisa tamb√©m √© realizada no buffer do BlockCache e, se necess√°rio, nos HFiles.  Se os dados foram encontrados em um arquivo, eles s√£o colocados no BlockCache e retornados mais rapidamente na pr√≥xima solicita√ß√£o.  As pesquisas de arquivos H s√£o relativamente r√°pidas devido ao uso do filtro Bloom, ou seja,  Depois de ler uma pequena quantidade de dados, ele determina imediatamente se esse arquivo cont√©m a chave desejada e, se n√£o, segue para a pr√≥xima. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8-/zz/wz/8-zzwzcoed7obxmlgzxl3ihrzzw.png"></div><br>  Tendo recebido dados dessas tr√™s fontes, o RS forma uma resposta.  Em particular, ele pode transferir v√°rias vers√µes do objeto encontradas ao mesmo tempo, se o cliente solicitou o controle de vers√£o. <br><br><h2>  4. Armazenamento em cache de dados </h2><br>  Os buffers MemStore e BlockCache ocupam at√© 80% da mem√≥ria RS alocada na pilha (o restante √© reservado para tarefas de servi√ßo RS).  Se o modo de uso t√≠pico for tal que os processos gravem e leiam imediatamente os mesmos dados, faz sentido reduzir o BlockCache e aumentar o MemStore, porque  ao gravar dados no cache de leitura, o uso do BlockCache ocorrer√° com menos frequ√™ncia.  O buffer do BlockCache consiste em duas partes: LruBlockCache (sempre na pilha) e BucketCache (geralmente fora da pilha ou no SSD).  O BucketCache deve ser usado quando houver muitas solicita√ß√µes de leitura e elas n√£o se ajustam ao LruBlockCache, o que leva ao trabalho ativo do Garbage Collector.  Ao mesmo tempo, voc√™ n√£o deve esperar um aumento radical no desempenho usando o cache de leitura, mas retornaremos a isso na Se√ß√£o 8 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/oe/nw/rqoenwgqtngb-a37gsof7sqbgn0.png"></div><br>  O BlockCache √© um para todo o RS e o MemStore possui um para cada tabela (um para cada fam√≠lia de colunas). <br><br>  Conforme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descrito</a> na teoria, ao gravar dados n√£o cai no cache e, de fato, esses par√¢metros CACHE_DATA_ON_WRITE para a tabela e "Cache DATA na Grava√ß√£o" para RS s√£o configurados como false.  No entanto, na pr√°tica, se voc√™ gravar dados no MemStore, liber√°-los para o disco (limpando-os dessa maneira), excluir o arquivo resultante e, em seguida, executando uma solicita√ß√£o de obten√ß√£o, os dados ser√£o recebidos com √™xito.  E mesmo que voc√™ desative completamente o BlockCache e preencha a tabela com novos dados, coloque o MemStore em disco, exclua-os e solicite a partir de outra sess√£o, eles ainda ser√£o buscados em algum lugar.  Portanto, o HBase armazena n√£o apenas dados, mas tamb√©m quebra-cabe√ßas misteriosos. <br><br><pre><code class="bash hljs">hbase(main):001:0&gt; create <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf'</span></span> Created table ns:magic Took 1.1533 seconds hbase(main):002:0&gt; put <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf:c'</span></span>, <span class="hljs-string"><span class="hljs-string">'try_to_delete_me'</span></span> Took 0.2610 seconds hbase(main):003:0&gt; flush <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span> Took 0.6161 seconds hdfs dfs -mv /data/hbase/data/ns/magic/* /tmp/trash hbase(main):002:0&gt; get <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span> cf:c timestamp=1534440690218, value=try_to_delete_me</code> </pre> <br>  DADOS do cache na leitura est√° definido como falso.  Se voc√™ tiver alguma id√©ia, discuta isso nos coment√°rios. <br><br><h2>  5. Processamento em lote de dados MultiGet / MultiPut </h2><br>  O processamento de solicita√ß√µes √∫nicas (Obter / Colocar / Excluir) √© uma opera√ß√£o bastante cara, portanto, voc√™ deve combin√°-las o m√°ximo poss√≠vel em uma Lista ou Lista, o que permite obter um aumento significativo no desempenho.  Isso √© especialmente verdade na opera√ß√£o de grava√ß√£o, mas ao ler, existe a seguinte armadilha.  O gr√°fico abaixo mostra o tempo de leitura de 50.000 registros do MemStore.  A leitura foi feita em um fluxo e o eixo horizontal mostra o n√∫mero de chaves na solicita√ß√£o.  Pode-se observar que, quando voc√™ aumenta para mil chaves em uma solicita√ß√£o, o tempo de execu√ß√£o diminui, ou seja,  velocidade aumenta.  No entanto, quando o modo MSLAB √© ativado por padr√£o, ap√≥s esse limite, uma queda dr√°stica no desempenho √© iniciada e quanto maior a quantidade de dados no registro, maior o tempo. <br><br><img src="https://habrastorage.org/webt/1k/ic/hj/1kichjm1xdpxskbx7avzppmlqty.png"><br><br>  Os testes foram realizados em uma m√°quina virtual, 8 n√∫cleos, vers√£o HBase 2.0.0-cdh6.0.0-beta1. <br><br>  O modo MSLAB foi projetado para reduzir a fragmenta√ß√£o de heap, que ocorre devido √† mistura de dados de nova e velha gera√ß√£o.  Como solu√ß√£o para o problema quando o MSLAB est√° ativado, os dados s√£o colocados em c√©lulas relativamente pequenas (bloco) e processados ‚Äã‚Äãem lotes.  Como resultado, quando o volume no pacote de dados solicitado excede o tamanho alocado, o desempenho cai acentuadamente.  Por outro lado, tamb√©m n√£o √© recomend√°vel desativar esse modo, pois isso leva a paradas devido ao GC durante momentos de trabalho intensivo com dados.  Uma boa sa√≠da √© aumentar o volume da c√©lula, no caso da escrita ativa via put simultaneamente com a leitura.  Vale ressaltar que o problema n√£o ocorre se, ap√≥s a grava√ß√£o, executar o comando flush que libera o MemStore para o disco ou se o carregamento √© realizado usando o BulkLoad.  A tabela abaixo mostra que as consultas dos dados do MemStore de um volume maior (e a mesma quantidade) levam a uma desacelera√ß√£o.  No entanto, aumentar o tamanho do chunks retorna ao tempo de processamento normal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zb/jq/s3/zbjqs3tou2ywnnzc16p93fttbva.png"></div><br>  Al√©m de aumentar o tamanho do peda√ßo, a fragmenta√ß√£o de dados por regi√£o ajuda, ou seja,  divis√£o de mesa.  Isso leva ao fato de que menos solicita√ß√µes chegam a cada regi√£o e, se forem colocadas em uma c√©lula, a resposta permanece boa. <br><br><h2>  6. A estrat√©gia de dividir tabelas em regi√µes (corte) </h2><br>  Como o HBase √© um armazenamento de valor-chave e o particionamento √© realizado por chave, √© extremamente importante compartilhar dados uniformemente em todas as regi√µes.  Por exemplo, particionar essa tabela em tr√™s partes resultar√° na divis√£o dos dados em tr√™s regi√µes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rg/4c/9d/rg4c9dm-gbtodx0dqr3quc9h6we.png"></div><br>  Ocorre que isso leva a uma desacelera√ß√£o acentuada se os dados carregados no futuro parecerem, por exemplo, valores longos, a maioria dos quais come√ßa com o mesmo d√≠gito, por exemplo: <br><br>  1000001 <br>  1000002 <br>  ... <br>  1100003 <br><br>  Como as chaves s√£o armazenadas como uma matriz de bytes, todas elas ser√£o iniciadas da mesma maneira e pertencer√£o √† mesma regi√£o n¬∫ 1 que armazena esse intervalo de chaves.  Existem v√°rias estrat√©gias de divis√£o: <br><br>  HexStringSplit - Transforma a chave em uma string com codifica√ß√£o hexadecimal no intervalo "00000000" =&gt; "FFFFFFFF" e a preenche com zeros √† esquerda. <br><br>  UniformSplit - Transforma uma chave em uma matriz de bytes com codifica√ß√£o hexadecimal no intervalo "00" =&gt; "FF" e a preenche com zeros √† direita. <br><br>  Al√©m disso, voc√™ pode especificar qualquer intervalo ou conjunto de teclas para dividir e configurar a divis√£o autom√°tica.  No entanto, uma das abordagens mais simples e eficazes √© o UniformSplit e o uso de concatena√ß√£o de hash, por exemplo, um par alto de bytes executando uma chave atrav√©s da fun√ß√£o CRC32 (rowkey) e a pr√≥pria keykey: <br><br>  hash + rowkey <br><br>  Todos os dados ser√£o distribu√≠dos uniformemente pelas regi√µes.  Ao ler, os dois primeiros bytes s√£o simplesmente descartados e a chave original permanece.  O RS tamb√©m controla a quantidade de dados e chaves na regi√£o e, quando os limites s√£o excedidos, os divide automaticamente em peda√ßos. <br><br><h2>  7. Toler√¢ncia a falhas e localidade dos dados </h2><br>  Como apenas uma regi√£o √© respons√°vel por cada conjunto de chaves, a solu√ß√£o para os problemas associados a falhas ou descomissionamento do RS √© armazenar todos os dados necess√°rios no HDFS.  Quando o RS trava, o mestre detecta isso atrav√©s da aus√™ncia de um batimento card√≠aco no n√≥ ZooKeeper.  Em seguida, ele atribui a regi√£o atendida a outro RS e, como os HFiles s√£o armazenados em um sistema de arquivos distribu√≠dos, o novo host os l√™ e continua a servir os dados.  No entanto, como alguns dos dados podem estar no MemStore e n√£o tiveram tempo de entrar nos HFiles, os WALs, que tamb√©m s√£o armazenados no HDFS, s√£o usados ‚Äã‚Äãpara restaurar o hist√≥rico da opera√ß√£o.  Ap√≥s o rollover das altera√ß√µes, o RS √© capaz de responder √†s solicita√ß√µes, no entanto, a mudan√ßa leva ao fato de que parte dos dados e seus processos est√£o em n√≥s diferentes, ou seja,  localidade diminu√≠da. <br><br>  A solu√ß√£o para o problema √© a compacta√ß√£o principal - esse procedimento move os arquivos para os n√≥s respons√°veis ‚Äã‚Äãpor eles (onde suas regi√µes est√£o localizadas), como resultado do qual a carga na rede e nos discos aumenta acentuadamente durante esse procedimento.  No entanto, no futuro, o acesso aos dados √© visivelmente acelerado.  Al√©m disso, major_compaction combina todos os HFiles em um arquivo na regi√£o e tamb√©m limpa os dados, dependendo das configura√ß√µes da tabela.  Por exemplo, voc√™ pode especificar o n√∫mero de vers√µes de um objeto que deseja salvar ou sua vida √∫til, ap√≥s as quais o objeto √© exclu√≠do fisicamente. <br><br>  Este procedimento pode ter um efeito muito positivo no HBase.  A imagem abaixo mostra como o desempenho diminuiu como resultado da grava√ß√£o de dados ativa.  Aqui voc√™ pode ver como 40 fluxos foram gravados em uma tabela e 40 fluxos leram dados ao mesmo tempo.  Os fluxos de grava√ß√£o formam cada vez mais HFiles, que s√£o lidos por outros fluxos.  Como resultado, mais e mais dados precisam ser exclu√≠dos da mem√≥ria e, no final, o GC come√ßa a funcionar, o que praticamente paralisa todo o trabalho.  O lan√ßamento da grande compacta√ß√£o levou √† limpeza dos bloqueios resultantes e √† restaura√ß√£o do desempenho. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/x2/ha/ga/x2haga1cohdfilxz5ffu_vatfzy.png"></div><br>  O teste foi realizado em 3 DataNode e 4 RS (CPU Xeon E5-2680 v4 @ 2,40GHz * 64 threads).  HBase Vers√£o 1.2.0-cdh5.14.2 <br><br>  Vale ressaltar que o lan√ßamento da compacta√ß√£o principal foi realizado em uma tabela "ao vivo", na qual os dados foram ativamente gravados e lidos.  Havia uma declara√ß√£o na rede de que isso poderia levar a uma resposta incorreta ao ler dados.  Para verificar, foi iniciado um processo que gerou novos dados e os gravou na tabela.  Depois, li e verifiquei imediatamente se o valor obtido coincidia com o que foi registrado.  Durante esse processo, a compacta√ß√£o principal foi iniciada cerca de 200 vezes e nenhuma falha foi registrada.  Talvez o problema apare√ßa raramente e somente durante alta carga, por isso √© mais seguro interromper os processos de grava√ß√£o e leitura de maneira programada e executar a limpeza sem permitir tais rebaixamentos do GC. <br><br>  Al√©m disso, a compacta√ß√£o principal n√£o afeta o estado do MemStore. Para liber√°-lo para o disco e compactar, √© necess√°rio usar flush (connection.getAdmin (). Flush (TableName.valueOf (tblName))). <br><br><h2>  8. Configura√ß√µes e desempenho </h2><br>  Como j√° mencionado, o HBase mostra o maior sucesso onde n√£o precisa fazer nada ao executar o BulkLoad.  No entanto, isso se aplica √† maioria dos sistemas e pessoas.  No entanto, essa ferramenta √© mais adequada para o empilhamento em massa de dados em grandes blocos, enquanto que se o processo exigir muitas solicita√ß√µes concorrentes de leitura e grava√ß√£o, os comandos Get e Put descritos acima ser√£o usados.  Para determinar os par√¢metros ideais, foram realizados lan√ßamentos com v√°rias combina√ß√µes de par√¢metros e configura√ß√µes da tabela: <br><br><ul><li>  10 threads foram iniciados ao mesmo tempo 3 vezes seguidas (vamos cham√°-lo de um bloco de threads). </li><li>  A m√©dia de tempo de opera√ß√£o de todos os fluxos no bloco foi o resultado final da opera√ß√£o do bloco. </li><li>  Todos os threads funcionaram com a mesma tabela. </li><li>  Antes de cada in√≠cio do bloco de encadeamentos, uma compacta√ß√£o principal era executada. </li><li>  Cada bloco executou apenas uma das seguintes opera√ß√µes: </li></ul><br>  - Coloque <br>  - obtenha <br>  - Get + Put <br><br><ul><li>  Cada bloco realizou 50.000 repeti√ß√µes de sua opera√ß√£o. </li><li>  O tamanho do registro no bloco √© de 100 bytes, 1000 bytes ou 10000 bytes (aleat√≥rio). </li><li>  Os blocos foram lan√ßados com um n√∫mero diferente de chaves solicitadas (uma chave ou 10). </li><li>  Os blocos foram lan√ßados em v√°rias configura√ß√µes da tabela.  Par√¢metros alterados: </li></ul><br>  - BlockCache = ativado ou desativado <br>  - BlockSize = 65 Kb ou 16 Kb <br>  - Parti√ß√µes = 1, 5 ou 30 <br>  - MSLAB = ativado ou desativado <br><br>  Assim, o bloco fica assim: <br><br>  a.  Modo MSLAB ativado / desativado. <br>  b.  Foi criada uma tabela para a qual os seguintes par√¢metros foram definidos: BlockCache = true / none, BlockSize = 65/16 Kb, Parti√ß√µes = 1/5/30. <br>  c.  Defina a compress√£o GZ. <br>  d.  Foram lan√ßados 10 encadeamentos simultaneamente, executando 1/10 das opera√ß√µes put / get / get + put nesta tabela com registros de 100/1000/10000 bytes, executando 50.000 consultas seguidas (chaves aleat√≥rias). <br>  e  O ponto d foi repetido tr√™s vezes. <br>  f.  O tempo de opera√ß√£o de todos os threads foi medido. <br><br>  Todas as combina√ß√µes poss√≠veis foram verificadas.  √â previs√≠vel que, √† medida que o tamanho da grava√ß√£o aumente, a velocidade caia ou a desativa√ß√£o do cache diminua.  No entanto, o objetivo foi compreender o grau e a signific√¢ncia da influ√™ncia de cada par√¢metro, portanto, os dados coletados foram alimentados com a entrada da fun√ß√£o de regress√£o linear, o que possibilita avaliar a confiabilidade usando a estat√≠stica t.  Abaixo est√£o os resultados dos blocos que executam opera√ß√µes Put.  Um conjunto completo de combina√ß√µes 2 * 2 * 3 * 2 * 3 = 144 op√ß√µes + 72 desde  alguns foram realizados duas vezes.  Portanto, um total de 216 lan√ßamentos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/5u/wj/ai5uwj0fvmyo9hyqkjg4-cigceq.png"></div><br>  O teste foi realizado em um mini-cluster composto por 3 DataNode e 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 fluxos).  HBase vers√£o 1.2.0-cdh5.14.2. <br><br>  A velocidade de inser√ß√£o mais alta de 3,7 segundos foi obtida quando o modo MSLAB foi desativado, em uma tabela com uma parti√ß√£o, com o BlockCache ativado, BlockSize = 16, registros de 100 bytes de 10 pe√ßas por pacote. <br>  A velocidade de inser√ß√£o mais baixa de 82,8 segundos foi obtida quando o modo MSLAB foi ativado, em uma tabela com uma parti√ß√£o, com o BlockCache ativado, BlockSize = 16, registros de 10.000 bytes cada. <br><br>  Agora vamos ver o modelo.  Vemos um modelo de boa qualidade para o R2, mas √© claro que a extrapola√ß√£o √© contra-indicada aqui.  O comportamento real do sistema ao alterar os par√¢metros n√£o ser√° linear, este modelo n√£o √© necess√°rio para previs√µes, mas para entender o que aconteceu dentro dos par√¢metros fornecidos.  Por exemplo, aqui vemos pelo crit√©rio de Student que, para a opera√ß√£o Put, os par√¢metros BlockSize e BlockCache n√£o importam (o que geralmente √© previs√≠vel): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/aq/vg/xt/aqvgxt9uyfs_l4m4crnm3adhliy.png"></div><br>  Mas o fato de um aumento no n√∫mero de parti√ß√µes levar a uma diminui√ß√£o no desempenho √© algo inesperado (j√° vimos o efeito positivo de um aumento no n√∫mero de parti√ß√µes com o BulkLoad), embora seja compreens√≠vel.  Primeiro, para o processamento, √© necess√°rio formar consultas para 30 regi√µes, em vez de uma, e a quantidade de dados n√£o √© tal que gera ganho.  Em segundo lugar, o tempo total de opera√ß√£o √© determinado pelo RS mais lento e, como o n√∫mero de DataNode √© menor que o n√∫mero de RS, algumas regi√µes t√™m localidade zero.  Bem, vamos olhar para os cinco primeiros: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bx/md/iu/bxmdiuzfdzqlfe1l8_s_2ktecb0.png"></div><br>  Agora vamos avaliar os resultados da execu√ß√£o dos blocos Get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/2b/no/zl2bnojdyx-byfpty6yr9poebzg.png"></div><br>  O n√∫mero de parti√ß√µes perdeu signific√¢ncia, o que provavelmente se deve ao fato de os dados estarem bem armazenados em cache e o cache de leitura ser o par√¢metro mais significativo (estatisticamente).  Naturalmente, aumentar o n√∫mero de mensagens em uma solicita√ß√£o tamb√©m √© muito √∫til para o desempenho.  Os melhores resultados: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/6d/pu/f06dpurnzlck4po4jw1xpyrphl8.png"></div><br>  Bem, finalmente, veja o modelo do bloco que foi executado primeiro e depois coloque: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/fc/23/aefc23q9mcfbtbumdc5qjmw_qrg.png"></div><br>  Aqui todos os par√¢metros s√£o significativos.  E os resultados dos l√≠deres: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q0/vo/th/q0vothoybhc8k6m1ysefviz3tco.png"></div><br><h2>  9. Teste de carga </h2><br>  Bem, finalmente, lan√ßaremos uma carga mais ou menos decente, mas √© sempre mais interessante quando h√° algo para comparar.  O site do DataStax, um desenvolvedor chave do Cassandra, tem os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">resultados do</a> NT de v√°rios reposit√≥rios NoSQL, incluindo o HBase vers√£o 0.98.6-1.  O carregamento foi realizado por 40 fluxos, tamanho de dados 100 bytes, discos SSD.  O resultado do teste das opera√ß√µes Read-Modify-Write mostrou esses resultados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ha/7b/bw/ha7bbwydc612f04jtwyqvdvg_ae.png"></div><br>  Pelo que entendi, a leitura foi realizada em blocos de 100 registros e, para 16 n√≥s do HBase, o teste DataStax mostrou um desempenho de 10 mil opera√ß√µes por segundo. <br><br>  √â uma sorte que nosso cluster tamb√©m tenha 16 n√≥s, mas n√£o muito ‚Äúfeliz‚Äù que cada um tenha 64 n√∫cleos (threads), enquanto o teste DataStax possui apenas 4. Por outro lado, eles t√™m discos SSD, e temos HDD e mais a nova vers√£o do HBase e a utiliza√ß√£o da CPU durante a carga praticamente n√£o aumentaram significativamente (visualmente em 5 a 10%).  No entanto, tentaremos iniciar essa configura√ß√£o.  Por padr√£o, nas configura√ß√µes da tabela, a leitura √© realizada em um intervalo de teclas de 0 a 50 milh√µes aleatoriamente (isto √©, de fato, cada vez que um novo).  Na tabela, 50 milh√µes de entradas s√£o divididas em 64 parti√ß√µes.  As chaves s√£o hash crc32.  As configura√ß√µes da tabela s√£o padr√£o, o MSLAB est√° ativado.  Iniciando 40 threads, cada thread l√™ um conjunto de 100 chaves aleat√≥rias e grava imediatamente os 100 bytes gerados nessas chaves novamente. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/po/sd/el/posdel66zx7quvvo3kvrjb6uif8.png"></div><br>  Suporte: 16 DataNode e 16 RS (CPU Xeon E5-2680 v4 a 2.40GHz * 64 fluxos).  HBase vers√£o 1.2.0-cdh5.14.2. <br><br>  O resultado m√©dio est√° mais pr√≥ximo de 40 mil opera√ß√µes por segundo, o que √© significativamente melhor do que no teste DataStax.  No entanto, para os fins do experimento, as condi√ß√µes podem ser ligeiramente alteradas.  √â bastante improv√°vel que todo o trabalho seja realizado exclusivamente com uma tabela, bem como apenas com chaves exclusivas.  Suponha que exista um determinado conjunto de chaves "quente" que gere a carga principal.  Portanto, tentaremos criar uma carga com registros maiores (10 KB), tamb√©m em pacotes de 100 cada, em 4 tabelas diferentes e limitar o intervalo de chaves solicitadas a 50 mil.O gr√°fico abaixo mostra o in√≠cio de 40 threads, cada fluxo l√™ um conjunto de 100 chaves e grava imediatamente aleatoriamente 10 KB nessas chaves de volta. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/ec/mr/f7ecmrebgulvlcyru05c7jzytyi.png"></div><br>  Suporte: 16 DataNode e 16 RS (CPU Xeon E5-2680 v4 a 2.40GHz * 64 fluxos).  HBase vers√£o 1.2.0-cdh5.14.2. <br><br>  Durante o carregamento, uma compacta√ß√£o principal foi iniciada v√°rias vezes, conforme mostrado acima, sem esse procedimento; o desempenho ser√° gradualmente degradado; no entanto, um carregamento adicional tamb√©m ocorrer√° durante a execu√ß√£o.  Os rebaixamentos s√£o causados ‚Äã‚Äãpor v√°rios motivos.  √Äs vezes, os threads terminavam e, enquanto reiniciavam, havia uma pausa, outras vezes, aplicativos de terceiros criavam uma carga no cluster. <br><br>  Ler e escrever imediatamente √© um dos cen√°rios de trabalho mais dif√≠ceis para o HBase.  Se voc√™ colocar apenas solicita√ß√µes de coloca√ß√£o de tamanho pequeno, por exemplo, 100 bytes cada, combinando-as em lotes de 10 a 50 mil pe√ßas, poder√° obter centenas de milhares de opera√ß√µes por segundo e a situa√ß√£o √© semelhante √†s solicita√ß√µes somente leitura.  Vale ressaltar que os resultados s√£o radicalmente melhores do que os obtidos no DataStax, principalmente devido a solicita√ß√µes em blocos de 50 mil. <br><br><img src="https://habrastorage.org/webt/kv/_j/bv/kv_jbvizskwbod1nxapokckg9s8.png"><br>  Suporte: 16 DataNode e 16 RS (CPU Xeon E5-2680 v4 a 2.40GHz * 64 fluxos).  HBase vers√£o 1.2.0-cdh5.14.2. <br><br><h2>  10. Conclus√µes </h2><br>  Este sistema √© flex√≠vel o suficiente para configurar, mas o efeito de um grande n√∫mero de par√¢metros ainda √© desconhecido.  Alguns deles foram testados, mas n√£o foram inclu√≠dos no conjunto de testes resultante.  Por exemplo, experimentos preliminares mostraram a insignific√¢ncia de um par√¢metro como DATA_BLOCK_ENCODING, que codifica informa√ß√µes usando valores de c√©lulas vizinhas, o que √© bastante compreens√≠vel para dados gerados aleatoriamente.  No caso de usar um grande n√∫mero de objetos repetidos, o ganho pode ser significativo.  Em geral, podemos dizer que o HBase d√° a impress√£o de um banco de dados bastante s√©rio e bem pensado, que pode ser bastante produtivo ao lidar com grandes blocos de dados.  Especialmente se for poss√≠vel espalhar os processos de leitura e grava√ß√£o no tempo. <br><br>  Se algo na sua opini√£o n√£o for suficientemente divulgado, estou pronto para contar com mais detalhes.  Sugerimos compartilhar sua experi√™ncia ou debater se voc√™ n√£o concorda com algo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt420425/">https://habr.com/ru/post/pt420425/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt420409/index.html">Aprenda o OpenGL. Li√ß√£o 5.7 - HDR</a></li>
<li><a href="../pt420413/index.html">SQLite e NW.js - instru√ß√µes passo a passo para criar amizades fortes</a></li>
<li><a href="../pt420415/index.html">Tudo o que voc√™ queria saber sobre o teste de adaptadores Wi-Fi, mas tinha medo de perguntar</a></li>
<li><a href="../pt420419/index.html">Corredores para quem gosta de humilha√ß√£o ou como mudamos e modificamos o PixJam</a></li>
<li><a href="../pt420423/index.html">Problemas na interface de travessia de terra</a></li>
<li><a href="../pt420429/index.html">USE, RED, PgBouncer, suas configura√ß√µes e monitoramento</a></li>
<li><a href="../pt420431/index.html">Marte Guia pr√°tico de terraforma√ß√£o para donas de casa</a></li>
<li><a href="../pt420433/index.html">‚ÄúFormato sexta-feira‚Äù: estradas musicais - o que √© e por que n√£o est√° na R√∫ssia</a></li>
<li><a href="../pt420435/index.html">In√≠cio r√°pido com o ARM Mbed: desenvolvimento de microcontroladores modernos para iniciantes</a></li>
<li><a href="../pt420437/index.html">Uma introdu√ß√£o pr√°tica ao gerenciador de pacotes do Kubernetes - Helm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>