<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõåüèª üßï üôÜ Buch "Maschinelles Lernen und TensorFlow" üòî „ÄΩÔ∏è üè≥Ô∏è‚Äçüåà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Kenntnis des maschinellen Lernens und der TensorFlow-Bibliothek √§hnelt den ersten Lektionen in einer Fahrschule. Wenn Sie unter parallelem Parken ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Buch "Maschinelles Lernen und TensorFlow"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/437964/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/cy/lg/n3/cylgn38lpjyhq7gveb9cmmt_ml8.jpeg" align="left" alt="Bild"></a>  Die Kenntnis des maschinellen Lernens und der TensorFlow-Bibliothek √§hnelt den ersten Lektionen in einer Fahrschule. Wenn Sie unter parallelem Parken leiden, versuchen Sie, zum richtigen Zeitpunkt den Gang zu wechseln und die Spiegel nicht zu verwechseln. Erinnern Sie sich dabei verzweifelt an die Abfolge der Aktionen, w√§hrend Ihr Fu√ü nerv√∂s auf den Gaspedalen zittert.  Dies ist eine schwierige, aber notwendige √úbung.  Beim maschinellen Lernen: Bevor Sie moderne Gesichtserkennungssysteme oder Prognosealgorithmen an der B√∂rse einsetzen, m√ºssen Sie sich mit den entsprechenden Tools und Anweisungen befassen, um dann problemlos Ihre eigenen Systeme zu erstellen. <br><br>  Anf√§nger im maschinellen Lernen werden die angewandte Ausrichtung dieses Buches zu sch√§tzen wissen, da es darin besteht, die Grundlagen einzuf√ºhren und dann schnell echte Probleme zu l√∂sen.  Nach einer √úberpr√ºfung der Konzepte und Prinzipien des maschinellen Lernens bei der Arbeit mit TensorFlow werden Sie zu grundlegenden Algorithmen √ºbergehen, neuronale Netze untersuchen und in der Lage sein, die Probleme der Klassifizierung, Clusterbildung, Regression und Prognose unabh√§ngig zu l√∂sen. <br><a name="habracut"></a><br><h3>  Auszug.  Faltungsneurale Netze </h3><br>  Das Einkaufen in Gesch√§ften nach einem anstrengenden Tag ist eine sehr beschwerliche Aufgabe.  Meine Augen werden von zu vielen Informationen angegriffen.  Verk√§ufe, Gutscheine, eine Vielzahl von Farben, kleine Kinder, flackernde Lichter und G√§nge voller Menschen - dies sind nur einige Beispiele f√ºr alle Signale, die an den visuellen Kortex des Gehirns gesendet werden, unabh√§ngig davon, ob ich darauf achten m√∂chte oder nicht.  Das visuelle System nimmt eine F√ºlle von Informationen auf. <br><br>  Sicher kennen Sie den Satz "Es ist besser, einmal zu sehen als hundertmal zu h√∂ren."  Dies mag f√ºr Sie und f√ºr mich (dh f√ºr Menschen) zutreffen, aber kann die Maschine in den Bildern einen Sinn finden?  Unsere visuellen Fotorezeptoren w√§hlen Wellenl√§ngen des Lichts aus, aber diese Information erstreckt sich anscheinend nicht auf unser Bewusstsein.  Am Ende kann ich nicht genau sagen, welche Wellenl√§ngen des Lichts ich beobachte.  Auf die gleiche Weise empf√§ngt die Kamera Bildpixel.  Wir m√∂chten aber stattdessen etwas H√∂heres erhalten, zum Beispiel Namen oder Positionen von Objekten.  Wie erhalten wir Informationen, die auf menschlicher Ebene wahrgenommen werden, aus Pixeln? <br><br>  Um eine bestimmte Bedeutung aus den Quelldaten zu erhalten, muss ein neuronales Netzwerkmodell entworfen werden.  In den vorherigen Kapiteln wurden verschiedene Arten von neuronalen Netzwerkmodellen vorgestellt, z. B. vollst√§ndig verbundene Modelle (Kapitel 8) und automatische Encoder (Kapitel 7).  In diesem Kapitel werden wir einen anderen Modelltyp vorstellen, der als Convolutional Neural Network (CNN) bezeichnet wird.  Dieses Modell eignet sich hervorragend f√ºr Bilder und andere sensorische Daten wie Ton.  Beispielsweise kann das CNN-Modell zuverl√§ssig klassifizieren, welches Objekt im Bild angezeigt wird. <br><br>  Das CNN-Modell, das in diesem Kapitel behandelt wird, wird trainiert, um Bilder in eine von 10 m√∂glichen Kategorien zu klassifizieren.  In diesem Fall ist ‚Äûein Bild besser als nur ein Wort‚Äú, da wir nur 10 m√∂gliche Optionen haben.  Dies ist ein winziger Schritt in Richtung Wahrnehmung auf menschlicher Ebene, aber wir m√ºssen mit etwas beginnen, oder? <br><br><h3>  9.1.  Nachteile neuronaler Netze </h3><br>  Maschinelles Lernen ist ein ewiger Kampf um die Entwicklung eines Modells, das ausreichend aussagekr√§ftig ist, um Daten zu pr√§sentieren, aber gleichzeitig nicht so universell ist, dass es darum geht, Muster neu zu trainieren und auswendig zu lernen.  Neuronale Netze werden angeboten, um die Ausdruckskraft zu erh√∂hen.  obwohl sie, wie Sie sich vorstellen k√∂nnen, stark unter den Fallen der Umschulung leiden. <br><br><blockquote>  HINWEIS Eine Umschulung erfolgt, wenn ein trainiertes Modell in einem Trainingsdatensatz au√üergew√∂hnlich genau und in einem Validierungsdatensatz schlecht ist.  Dieses Modell ist wahrscheinlich zu universell f√ºr die geringe Menge verf√ºgbarer Daten und speichert am Ende nur die Trainingsdaten. </blockquote><br>  Um die Vielseitigkeit der beiden Modelle des maschinellen Lernens zu vergleichen, k√∂nnen Sie einen schnellen und groben heuristischen Algorithmus verwenden, um die Anzahl der Parameter zu berechnen, die als Ergebnis des Trainings ermittelt werden m√ºssen.  Wie in Abb.  9.1, ein vollst√§ndig verbundenes neuronales Netzwerk, das ein 256 √ó 256-Bild auf eine Schicht von 10 Neuronen abbildet, hat 256 √ó 256 √ó 10 = 655.360 Parameter!  Vergleichen Sie es mit einem Modell, das nur f√ºnf Parameter enth√§lt.  Es kann angenommen werden, dass ein vollst√§ndig verbundenes neuronales Netzwerk komplexere Daten enth√§lt als ein F√ºnf-Parameter-Modell. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_c/ey/qe/_ceyqe8jc1s-xzasfzbv4vezhum.png" alt="Bild"></div><br>  Im n√§chsten Abschnitt werden Faltungs-Neuronale Netze er√∂rtert, die eine sinnvolle M√∂glichkeit darstellen, die Anzahl der Parameter zu verringern.  Anstatt sich mit vollst√§ndig verbundenen Netzwerken zu befassen, verwendet CNN dieselben Parameter wiederholt. <br><br><h3>  9.2.  Faltungsneurale Netze </h3><br>  Die Hauptidee, die Faltungs-Neuronalen Netzen zugrunde liegt, ist, dass das lokale Verst√§ndnis des Bildes ausreichend ist.  Der praktische Vorteil von Faltungs-Neuronalen Netzen besteht darin, dass mit mehreren Parametern die Trainingszeit sowie die zum Trainieren des Modells erforderliche Datenmenge erheblich reduziert werden k√∂nnen. <br><br>  Anstelle von vollst√§ndig verbundenen Netzwerken mit Gewichten von jedem Pixel verf√ºgt CNN √ºber eine ausreichende Anzahl von Gewichten, die zum Anzeigen eines kleinen Bildfragments erforderlich sind.  Es ist, als w√ºrde man ein Buch mit einer Lupe lesen: Am Ende liest man die gesamte Seite, betrachtet aber zu jedem Zeitpunkt nur ein kleines Fragment davon. <br><br>  Stellen Sie sich ein 256 √ó 256-Bild vor. Anstatt den TensorFlow-Code zu verwenden, der das gesamte Bild auf einmal verarbeitet, k√∂nnen Sie das Bild fragmentweise scannen, z. B. ein 5 √ó 5-Fenster. Ein 5 √ó 5-Fenster gleitet √ºber das Bild (normalerweise von links nach rechts und von oben nach unten) in Abb.  9.2.  Wie schnell es gleitet, nennt man Schrittl√§nge.  Zum Beispiel bedeutet eine Schrittl√§nge von 2, dass ein 5 √ó 5-Schiebefenster jeweils 2 Pixel bewegt, bis das gesamte Bild passiert ist.  In TensorFlow k√∂nnen Sie, wie in K√ºrze gezeigt wird, die Schrittl√§nge und Fenstergr√∂√üe mithilfe der integrierten Funktionsbibliothek anpassen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tl/lj/br/tlljbr-3r61p02umh30yi-wtmhi.png" alt="Bild"></div><br>  Diesem 5 √ó 5-Fenster ist eine 5 √ó 5-Gewichtsmatrix zugeordnet. <br><br><blockquote>  DEFINITION Eine Faltung ist eine gewichtete Summe der Intensit√§tswerte von Pixeln in einem Bild, wenn das Fenster das gesamte Bild durchl√§uft.  Es stellt sich heraus, dass dieser Prozess der Faltung des Bildes mit der Gewichtsmatrix ein anderes Bild erzeugt (von derselben Gr√∂√üe, die von der Faltung abh√§ngt).  Koagulation ist der Prozess der Anwendung von Faltung. </blockquote><br>  Alle Manipulationen des Schiebefensters erfolgen in der Faltungsschicht des neuronalen Netzwerks.  Ein typisches neuronales Faltungsnetzwerk weist mehrere Faltungsschichten auf.  Jede Faltungsschicht erzeugt normalerweise viele zus√§tzliche Faltungen, daher ist die Gewichtungsmatrix ein 5 √ó 5 √ó n-Tensor, wobei n die Anzahl der Faltungen ist. <br><br>  Lassen Sie das Bild beispielsweise eine Faltungsschicht mit einer Gewichtsmatrix von 5 √ó 5 √ó 64 Dimensionen durchlaufen. Dadurch werden 64 Faltungen mit einem 5 √ó 5-Schiebefenster erstellt. Daher weist das entsprechende Modell 5 √ó 5 √ó 64 = 1600 Parameter auf, was erheblich weniger als die Anzahl der Parameter eines vollst√§ndig verbundenen Netzwerks ist : 256 √ó 256 = 65.536. <br><br>  Die Attraktivit√§t von Faltungs-Neuronalen Netzen (CNNs) besteht darin, dass die Anzahl der vom Modell verwendeten Parameter nicht von der Gr√∂√üe des Originalbilds abh√§ngt.  Sie k√∂nnen dasselbe neuronale Faltungsnetzwerk auf 300 √ó 300 Bildern ausf√ºhren, und die Anzahl der Parameter in der Faltungsschicht √§ndert sich nicht! <br><br><h3>  9.3.  Bildvorbereitung </h3><br>  Bereiten Sie einige Bilder vor, bevor Sie das CNN-Modell mit TensorFlow verwenden.  Die Auflistungen in diesem Abschnitt helfen Ihnen beim Einrichten eines Trainingsdatensatzes f√ºr den Rest des Kapitels. <br><br>  Laden Sie zun√§chst den CIFAR-10- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datensatz</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a> herunter.  Dieses Set enth√§lt 60.000 Bilder, die gleichm√§√üig auf 10 Kategorien verteilt sind. Dies ist eine ziemlich gro√üe Ressource f√ºr Klassifizierungsaufgaben.  Dann sollte die Bilddatei im Arbeitsverzeichnis abgelegt werden.  In Abb.  Abbildung 9.3 zeigt Beispiele f√ºr Bilder aus diesem Datensatz. <br><br>  Wir haben den CIFAR-10-Datensatz bereits im vorherigen Kapitel √ºber Auto-Encoder verwendet und sehen uns diesen Code jetzt noch einmal an.  Die folgende Auflistung stammt direkt aus der CIFAR-10-Dokumentation unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.cs.toronto.edu/~kriz/cifar.html</a> .  F√ºgen Sie den Code in die Datei cifar_tools.py ein. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/rd/6n/xsrd6nmuunpim-aeqepzxwj1acy.png" alt="Bild"></div><br>  Listing 9.1.  Laden von Bildern aus einer CIFAR-10-Datei in Python <br><br><pre><code class="plaintext hljs">import pickle def unpickle(file): fo = open(file, 'rb') dict = pickle.load(fo, encoding='latin1') fo.close() return dict</code> </pre> <br>  Neuronale Netze neigen zu Umschulungen, daher ist es wichtig, alles zu tun, um diesen Fehler zu minimieren.  Vergessen Sie dazu nicht, die Daten vor der Verarbeitung zu bereinigen. <br><br>  Die Datenbereinigung ist der Hauptprozess der Pipeline f√ºr maschinelles Lernen.  Der Code in Listing 9.2 verwendet die folgenden drei Schritte, um einen Bildsatz zu bereinigen: <br><br>  1. Wenn Sie ein Bild in Farbe haben, versuchen Sie, es in Graustufen umzuwandeln, um die Dimensionalit√§t der Eingabedaten und damit die Anzahl der Parameter zu verringern. <br><br>  2. Denken Sie daran, das Bild in der Mitte zuzuschneiden, da die Bildr√§nder keine n√ºtzlichen Informationen liefern. <br><br>  3. Normalisieren Sie die Eingabe, indem Sie den Mittelwert subtrahieren und durch die Standardabweichung jeder Datenprobe dividieren, damit sich die Gradienten w√§hrend der R√ºckausbreitung nicht zu stark √§ndern. <br><br>  Die folgende Auflistung zeigt, wie Sie einen Datensatz mit diesen Methoden l√∂schen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ba/nf/al/banfal_9g-9gy2lns0q_qv2qce8.png" alt="Bild"></div><br>  Speichern Sie alle Bilder aus dem CIFAR-10-Dataset und f√ºhren Sie die Bereinigungsfunktion aus.  Die folgende Auflistung definiert eine bequeme Methode zum Lesen, Bereinigen und Strukturieren von Daten zur Verwendung in TensorFlow.  Dort sollten Sie den Code aus der Datei cifar_tools.py einf√ºgen. <br><br>  Listing 9.3.  Vorverarbeitung aller CIFAR-10-Dateien <br><br><pre> <code class="plaintext hljs">def read_data(directory): names = unpickle('{}/batches.meta'.format(directory))['label_names'] print('names', names) data, labels = [], [] for i in range(1, 6): filename = '{}/data_batch_{}'.format(directory, i) batch_data = unpickle(filename) if len(data) &gt; 0: data = np.vstack((data, batch_data['data'])) labels = np.hstack((labels, batch_data['labels'])) else: data = batch_data['data'] labels = batch_data['labels'] print(np.shape(data), np.shape(labels)) data = clean(data) data = data.astype(np.float32) return names, data, labels</code> </pre> <br>  In der Datei using_cifar.py k√∂nnen Sie die Methode verwenden, indem Sie dazu cifar_tools importieren.  Die Listen 9.4 und 9.5 zeigen, wie Sie mehrere Bilder aus einem Datensatz abrufen und visualisieren k√∂nnen. <br><br>  Listing 9.4.  Verwenden der Hilfsfunktion cifar_tools <br><br><pre> <code class="plaintext hljs">import cifar_tools names, data, labels = \ cifar_tools.read_data('your/location/to/cifar-10-batches-py')</code> </pre> <br>  Sie k√∂nnen beliebig mehrere Bilder ausw√§hlen und entsprechend der Beschriftung zeichnen.  Die folgende Auflistung macht genau das, damit Sie die Art der Daten, mit denen Sie sich befassen, besser verstehen k√∂nnen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/3c/ls/9s3clsibt-fy4tqxlnmggitpydm.png" alt="Bild"></div><br>  Wenn Sie diesen Code ausf√ºhren, erstellen Sie die Datei cifar_examples.png, die wie in Abb. 1 dargestellt aussieht.  9.3. <br><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  20% Rabatt-Gutschein f√ºr Stra√üenh√§ndler - <b>Maschinelles Lernen</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437964/">https://habr.com/ru/post/de437964/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437952/index.html">So testen Sie in AutoCar: MindMaps, statische Code-Analyse und MockServer</a></li>
<li><a href="../de437956/index.html">"Implementing Splunk 7" - das erste Buch √ºber Splunk in russischer Sprache</a></li>
<li><a href="../de437958/index.html">Autorisierung in ESIA auf einem Terminalserver mit digitaler Signatur gem√§√ü GOST-2012</a></li>
<li><a href="../de437960/index.html">Beratung durch den technischen Direktor eines IT-Unternehmens f√ºr Bootcamp-Absolventen</a></li>
<li><a href="../de437962/index.html">PVS-Studio ROI</a></li>
<li><a href="../de437968/index.html">PVS-Studio ROI</a></li>
<li><a href="../de437972/index.html">PHP f√ºr Anf√§nger. Die Sitzung</a></li>
<li><a href="../de437974/index.html">Wie gewinne ich digitale WorldSkills? An einem praktischen Beispiel</a></li>
<li><a href="../de437976/index.html">"Vkontakte" darf einzelne Unterlagen vor der Polizei verbergen</a></li>
<li><a href="../de437978/index.html">Willkommen beim SphinxSearch-Meetup SuperJob</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>