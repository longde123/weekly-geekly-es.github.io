<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÅ ü¶ä üëåüèø Crie um pipeline para processamento de dados de streaming. Parte 1 üå∫ üì∂ ‚Ñ¢Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal. Amigos, estamos compartilhando com voc√™ uma tradu√ß√£o de um artigo preparado especialmente para os alunos do curso Data Engineer . Vamos l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Crie um pipeline para processamento de dados de streaming. Parte 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/454186/">  Ol√° pessoal.  Amigos, estamos compartilhando com voc√™ uma tradu√ß√£o de um artigo preparado especialmente para os alunos do curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Data Engineer</a> .  Vamos l√°! <br><br><img src="https://habrastorage.org/webt/fv/mo/wz/fvmowz46naiimojhlq03dhxm38w.png"><br><br><h2>  Apache Beam e DataFlow para pipelines em tempo real </h2><br>  A publica√ß√£o de hoje √© baseada em uma tarefa na qual trabalhei recentemente.  Fiquei muito feliz em traduzi-lo e descrever o trabalho realizado no formato de uma postagem de blog, pois me deu a oportunidade de trabalhar em engenharia de dados e de fazer algo que seria muito √∫til para minha equipe.  H√° pouco tempo, descobri que nossos sistemas armazenam uma quantidade bastante grande de logs de usu√°rios relacionados a um de nossos produtos para trabalhar com dados.  Aconteceu que ningu√©m tinha usado esses dados, ent√£o fiquei imediatamente interessado no que poder√≠amos descobrir se come√ß√°ssemos a analis√°-los regularmente.  No entanto, houve v√°rios problemas ao longo do caminho.  O primeiro problema foi que os dados foram armazenados em muitos arquivos de texto diferentes que n√£o estavam dispon√≠veis para an√°lise instant√¢nea.  O segundo problema era que eles estavam armazenados em um sistema fechado, portanto n√£o pude usar nenhuma das minhas ferramentas favoritas de an√°lise de dados. <a name="habracut"></a><br><br>  Eu tive que decidir como facilitar o acesso para n√≥s e agregar pelo menos algum valor incorporando essa fonte de dados em algumas de nossas solu√ß√µes de intera√ß√£o com o usu√°rio.  Depois de pensar um pouco, decidi construir um pipeline para transferir esses dados para o banco de dados em nuvem, para que eu e a equipe pudessem acess√°-los e come√ßar a gerar conclus√µes.  Depois de concluir minha especializa√ß√£o em Engenharia de Dados na Coursera, h√° algum tempo, eu estava ansioso para usar algumas das ferramentas do curso no projeto. <br><br>  Portanto, colocar dados em um banco de dados na nuvem parecia uma maneira inteligente de resolver meu primeiro problema, mas o que eu poderia fazer com o problema n√∫mero 2?  Felizmente, havia uma maneira de transferir esses dados para um ambiente onde eu podia acessar ferramentas como Python e o Google Cloud Platform (GCP).  No entanto, como foi um processo longo, eu precisava fazer algo que me permitisse continuar o desenvolvimento enquanto aguardava o t√©rmino da transfer√™ncia de dados.  A solu√ß√£o que eu encontrei foi criar dados falsos usando a biblioteca <b>Faker</b> em Python.  Eu nunca tinha usado essa biblioteca antes, mas rapidamente percebi o qu√£o √∫til era.  O uso dessa abordagem me permitiu come√ßar a escrever c√≥digo e testar o pipeline sem dados reais. <br><br>  Com base no exposto, neste post, mostrarei como constru√≠ o pipeline descrito acima, usando algumas das tecnologias dispon√≠veis no GCP.  Em particular, usarei o <b>Apache Beam (vers√£o para Python), Dataflow, Pub / Sub e Big Query</b> para coletar logs do usu√°rio, converter dados e transferi-los para um banco de dados para an√°lise posterior.  No meu caso, eu s√≥ precisava da funcionalidade de lote do Beam, j√° que meus dados n√£o chegaram em tempo real, portanto, Pub / Sub n√£o era necess√°rio.  No entanto, vou me concentrar na vers√£o de streaming, pois √© isso que voc√™ pode encontrar na pr√°tica. <br><br><h2>  Introdu√ß√£o ao GCP e Apache Beam </h2><br>  O Google Cloud Platform fornece um conjunto de ferramentas realmente √∫teis para o processamento de big data.  Aqui est√£o algumas das ferramentas que vou usar: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pub / Sub</a> √© um servi√ßo de mensagens que usa o modelo Publisher-Subscriber que nos permite receber dados em tempo real. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O DataFlow</a> √© um servi√ßo que simplifica a cria√ß√£o de pipelines de dados e resolve automaticamente tarefas como dimensionar a infraestrutura, o que significa que s√≥ podemos focar na escrita de c√≥digo para o nosso pipeline. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O BigQuery</a> √© um data warehouse baseado em nuvem.  Se voc√™ estiver familiarizado com outros bancos de dados SQL, n√£o precisar√° lidar com o BigQuery por muito tempo. </li><li>  E, finalmente, usaremos o Apache Beam, a saber, o foco na vers√£o Python para criar nosso pipeline.  Essa ferramenta nos permitir√° criar um pipeline para streaming ou processamento em lote que se integra ao GCP.  √â especialmente √∫til para processamento paralelo e √© adequado para tarefas como extra√ß√£o, transforma√ß√£o e carregamento (ETL); portanto, se precisarmos mover dados de um local para outro com transforma√ß√µes ou c√°lculos, o Beam √© uma boa op√ß√£o. </li></ul><br><br>  Um grande n√∫mero de ferramentas est√° dispon√≠vel no GCP; portanto, pode ser dif√≠cil abranger todas elas, incluindo sua finalidade, mas, no entanto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui est√° um</a> breve resumo para refer√™ncia. <br><br><h2>  Visualiza√ß√£o do nosso transportador </h2><br>  Vamos visualizar os componentes do nosso pipeline na <i>Figura 1</i> .  Em um n√≠vel alto, queremos coletar dados do usu√°rio em tempo real, process√°-los e transferi-los para o BigQuery.  Os logs s√£o criados quando os usu√°rios interagem com o produto enviando solicita√ß√µes ao servidor, que s√£o registradas.  Esses dados podem ser especialmente √∫teis para entender como os usu√°rios interagem com nosso produto e se eles funcionam corretamente.  Em geral, o transportador conter√° as seguintes etapas: <br><br><ol><li>  Os dados de log de nossos usu√°rios s√£o publicados na se√ß√£o Pub / Sub. </li><li>  Vamos nos conectar ao Pub / Sub e converter os dados para o formato apropriado usando Python e Beam (etapas 3 e 4 na Figura 1). </li><li>  Ap√≥s a convers√£o dos dados, o Beam se conectar√° ao BigQuery e os adicionar√° √† nossa tabela (etapas 4 e 5 na Figura 1). </li><li>  Para an√°lise, podemos conectar-se ao BigQuery usando v√°rias ferramentas, como Tableau e Python. </li></ol><br>  O Beam torna esse processo muito simples, independentemente de termos uma fonte de dados de streaming ou um arquivo CSV e queremos fazer o processamento em lote.  Mais tarde, voc√™ ver√° que o c√≥digo cont√©m apenas as altera√ß√µes m√≠nimas necess√°rias para alternar entre eles.  Esse √© um dos benef√≠cios do uso do Beam. <br><br><img src="https://habrastorage.org/webt/2s/nv/kc/2snvkcz6-jwybsxr2kyz3awipaw.png"><br>  <i>Figura 1: O principal pipeline de dados</i> <br><br><h2>  Criando pseudo dados usando o Faker </h2><br>  Como mencionei anteriormente, devido ao acesso limitado aos dados, decidi criar pseudo-dados no mesmo formato que os reais.  Este foi um exerc√≠cio realmente √∫til, pois eu poderia escrever c√≥digo e testar o pipeline enquanto esperava dados.  Sugiro dar uma olhada na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do Faker <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> se voc√™ quiser saber o que mais esta biblioteca tem a oferecer.  Nossos dados de usu√°rio geralmente ser√£o semelhantes ao exemplo abaixo.  Com base nesse formato, podemos gerar dados linha por linha para simular dados em tempo real.  Esses logs nos fornecem informa√ß√µes como data, tipo de solicita√ß√£o, resposta do servidor, endere√ßo IP etc. <br><br> <code>192.52.197.161 - - [30/Apr/2019:21:11:42] "PUT /tag/category/tag HTTP/1.1" [401] 155 "https://harris-lopez.com/categories/about/" "Mozilla/5.0 (Macintosh; PPC Mac OS X 10_11_2) AppleWebKit/5312 (KHTML, like Gecko) Chrome/34.0.855.0 Safari/5312"</code> <br> <br>  Com base na linha acima, queremos criar nossa vari√°vel <b>LINE</b> usando 7 vari√°veis ‚Äã‚Äãentre chaves abaixo.  Tamb√©m os usaremos como nomes de vari√°veis ‚Äã‚Äãem nosso esquema de tabela um pouco mais tarde. <br><br> <code>LINE = """\ <br> {remote_addr} - - [{time_local}] "{request_type} {request_path} HTTP/1.1" [{status}] {body_bytes_sent} "{http_referer}" "{http_user_agent}"\ <br> """</code> <br> <br>  Se realiz√°ssemos o processamento em lote, o c√≥digo seria muito semelhante, embora precis√°ssemos criar um conjunto de amostras em um determinado intervalo de tempo.  Para usar um falsificador, simplesmente criamos um objeto e chamamos os m√©todos que precisamos.  Em particular, o Faker foi √∫til para criar endere√ßos IP e tamb√©m sites.  Eu usei os seguintes m√©todos: <br><br> <code>fake.ipv4() <br> fake.uri_path() <br> fake.uri() <br> fake.user_agent() <br></code> <br><br><pre> <code class="php hljs">from faker import Faker import time import random import os import numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np from datetime import datetime, timedelta LINE = <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"\ {remote_addr} - - [{time_local}] "</span></span>{request_type} {request_path} HTTP/<span class="hljs-number"><span class="hljs-number">1.1</span></span><span class="hljs-string"><span class="hljs-string">" [{status}] {body_bytes_sent} "</span></span>{http_referer}<span class="hljs-string"><span class="hljs-string">" "</span></span>{http_user_agent}<span class="hljs-string"><span class="hljs-string">"\ "</span></span><span class="hljs-string"><span class="hljs-string">""</span></span> def generate_log_line(): fake = Faker() now = datetime.now() remote_addr = fake.ipv4() time_local = now.strftime(<span class="hljs-string"><span class="hljs-string">'%d/%b/%Y:%H:%M:%S'</span></span>) request_type = random.choice([<span class="hljs-string"><span class="hljs-string">"GET"</span></span>, <span class="hljs-string"><span class="hljs-string">"POST"</span></span>, <span class="hljs-string"><span class="hljs-string">"PUT"</span></span>]) request_path = <span class="hljs-string"><span class="hljs-string">"/"</span></span> + fake.uri_path() status = np.random.choice([<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">401</span></span>, <span class="hljs-number"><span class="hljs-number">404</span></span>], p = [<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>]) body_bytes_sent = random.choice(range(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) http_referer = fake.uri() http_user_agent = fake.user_agent() log_line = LINE.format( remote_addr=remote_addr, time_local=time_local, request_type=request_type, request_path=request_path, status=status, body_bytes_sent=body_bytes_sent, http_referer=http_referer, http_user_agent=http_user_agent ) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> log_line</code> </pre> <br><br>  O fim da primeira parte. <br><br>  Nos pr√≥ximos dias, compartilharemos com voc√™ a continua√ß√£o do artigo, mas agora estamos tradicionalmente aguardando coment√°rios ;-). <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Segunda parte</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt454186/">https://habr.com/ru/post/pt454186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt454176/index.html">Uibook - uma ferramenta de teste visual para componentes React com consultas de m√≠dia</a></li>
<li><a href="../pt454178/index.html">Um exemplo de c√°lculo da pens√£o de um funcion√°rio de TI de Moscou</a></li>
<li><a href="../pt454180/index.html">Schr√∂dinger Cloud Backup</a></li>
<li><a href="../pt454182/index.html">Uma entrevista completa com o decano do departamento Python da GeekBrains - como e por que os iniciantes aprendem a l√≠ngua</a></li>
<li><a href="../pt454184/index.html">KubeCon Europe 2019: Como visitamos o Evento Principal do Kubernetes pela primeira vez</a></li>
<li><a href="../pt454188/index.html">Canais alternativos de recrutamento</a></li>
<li><a href="../pt454190/index.html">O que voc√™ n√£o precisa fazer se seu telefone for roubado</a></li>
<li><a href="../pt454196/index.html">Impress√£o 3D de eletr√¥nicos usando um exemplo de drone: fios e placas n√£o s√£o mais necess√°rios</a></li>
<li><a href="../pt454198/index.html">Criando um projeto Gradle SpringBoot + Angular com v√°rios m√≥dulos no IDEA</a></li>
<li><a href="../pt454204/index.html">O rastreamento comportamental n√£o √© uma panac√©ia?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>