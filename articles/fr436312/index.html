<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüîß üéè ü§≥ Synth√®se de la parole du r√©seau neuronal √† l'aide de l'architecture Tacotron 2, ou ¬´Obtenez l'alignement ou essayez de mourir¬ª üêøÔ∏è ‚Ñ¢Ô∏è üìÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Notre √©quipe a √©t√© charg√©e de: r√©p√©ter les r√©sultats du r√©seau neuronal de synth√®se vocale artificielle Tacotron2 par DeepMind Il s'agit d'une histoir...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Synth√®se de la parole du r√©seau neuronal √† l'aide de l'architecture Tacotron 2, ou ¬´Obtenez l'alignement ou essayez de mourir¬ª</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/436312/"><img src="https://habrastorage.org/webt/yu/y0/2f/yuy02fd0i4fodpxxadfkf0k2ori.jpeg"><br><br>  Notre √©quipe a √©t√© charg√©e de: r√©p√©ter les r√©sultats du r√©seau neuronal de synth√®se vocale <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">artificielle Tacotron2</a> par DeepMind  Il s'agit d'une histoire sur le chemin √©pineux que nous avons parcouru lors de la mise en ≈ìuvre du projet. <br><a name="habracut"></a><br>  La t√¢che de la synth√®se vocale par ordinateur int√©resse depuis longtemps les scientifiques et les experts techniques.  Cependant, les m√©thodes classiques ne permettent pas la synth√®se de la parole, impossible √† distinguer de l'homme.  Et ici, comme dans de nombreux autres domaines, l'apprentissage en profondeur est venu √† la rescousse. <br><br>  Regardons les m√©thodes de synth√®se classiques. <br><br><h2>  Synth√®se vocale concat√©native </h2><br>  Cette m√©thode est bas√©e sur le pr√©-enregistrement de courts fragments audio, qui sont ensuite combin√©s pour cr√©er un discours coh√©rent.  Il s'av√®re √™tre tr√®s propre et clair, mais compl√®tement d√©pourvu de composants √©motionnels et intonatifs, c'est-√†-dire qu'il semble anormal.  Et tout cela parce qu'il est impossible d'obtenir un enregistrement audio de tous les mots possibles prononc√©s dans toutes les combinaisons possibles d'√©motions et de prosodie.  Les syst√®mes concat√©natifs n√©cessitent d'√©normes bases de donn√©es et des combinaisons cod√©es en dur pour former des mots.  Le d√©veloppement d'un syst√®me fiable prend beaucoup de temps. <br><br><h2>  Synth√®se vocale param√©trique </h2><br>  Les applications TTS concat√©nationnelles sont limit√©es en raison des exigences de donn√©es √©lev√©es et du temps de d√©veloppement.  Par cons√©quent, une m√©thode statistique a √©t√© d√©velopp√©e qui explore la nature des donn√©es.  Il g√©n√®re de la parole en combinant des param√®tres tels que la fr√©quence, le spectre d'amplitude, etc. <br><br>  La synth√®se param√©trique comprend deux √©tapes. <br><br><ol><li>  Premi√®rement, les caract√©ristiques linguistiques, telles que les phon√®mes, la dur√©e, etc., sont extraites du texte. </li><li>  Ensuite, pour le vocodeur (le syst√®me g√©n√©rant la forme d'onde), des signes sont extraits qui repr√©sentent le signal de parole correspondant: cepstre, fr√©quence, spectrogramme lin√©aire, spectrogramme de craie. </li><li>  Ces param√®tres configur√©s manuellement, ainsi que les caract√©ristiques linguistiques, sont transf√©r√©s au mod√®le de vocodeur, et il effectue de nombreuses transformations complexes pour g√©n√©rer une onde sonore.  Dans le m√™me temps, le vocodeur √©value les param√®tres de la parole, tels que la phase, la prosodie, l'intonation et autres. </li></ol><br>  Si nous pouvons approximer les param√®tres qui d√©finissent la parole sur chacune de ses unit√©s, alors nous pouvons cr√©er un mod√®le param√©trique.  La synth√®se param√©trique n√©cessite beaucoup moins de donn√©es et de travail acharn√© que les syst√®mes concat√©natifs. <br><br>  Th√©oriquement, tout est simple, mais dans la pratique, il existe de nombreux artefacts qui conduisent √† un discours √©touff√© avec un son "bourdonnant", qui ne sonne pas du tout comme un son naturel. <br><br>  Le fait est qu'√† chaque √©tape de la synth√®se, nous codons en dur certaines fonctionnalit√©s et esp√©rons obtenir un discours r√©aliste.  Mais les donn√©es s√©lectionn√©es sont bas√©es sur notre compr√©hension de la parole, et la connaissance humaine n'est pas absolue, par cons√©quent, les signes pris ne seront pas n√©cessairement la meilleure solution possible. <br><br>  Et ici, le Deep Learning entre en sc√®ne dans toute sa splendeur. <br><br>  Les r√©seaux de neurones profonds sont un outil puissant qui, th√©oriquement, peut approximer une fonction arbitrairement complexe, c'est-√†-dire amener un espace de donn√©es d'entr√©e X dans un espace de donn√©es de sortie Y. Dans le contexte de notre t√¢che, ce seront respectivement du texte et de l'audio avec de la parole. <br><br><h2>  Pr√©traitement des donn√©es </h2><br>  Pour commencer, nous d√©terminerons ce que nous avons en entr√©e et ce que nous voulons obtenir en sortie. <br><br>  L'entr√©e sera du texte et la sortie sera un spectrogramme √† la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">craie</a> .  Il s'agit d'une repr√©sentation de bas niveau obtenue en appliquant la transform√©e de Fourier rapide √† un signal audio discret.  Il convient de noter tout de suite que les spectrogrammes ainsi obtenus <b>doivent</b> encore <b>√™tre normalis√©s</b> en compressant la plage dynamique.  Cela vous permet de r√©duire la relation naturelle entre le son le plus fort et le plus silencieux de l'enregistrement.  Dans nos exp√©riences, l'utilisation de spectrogrammes r√©duits √† la <b>gamme [-4; 4]</b> s'est av√©r√©e la meilleure. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/849/8fe/eb1/8498feeb1bd525506739c85bc5230c65.png"><br>  <i>Figure 1: Spectrogramme √† la craie du signal audio de la parole r√©duit √† la plage [-4; 4].</i> <br><br>  En tant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">qu'ensemble de donn√©es d'</a> entra√Ænement, nous avons choisi l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensemble de donn√©es LJSpeech</a> , qui contient <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">13 100</a> pistes audio pendant 2 √† 10 secondes.  et un fichier avec du texte correspondant √† la parole en anglais enregistr√© sur audio. <br><br>  Le son utilisant les transformations ci-dessus est cod√© en spectrogrammes √† la craie.  Le texte est symbolis√© et transform√©. <br><br>  en une s√©quence d'entiers.  Je dois tout de suite souligner que les textes sont normalis√©s: tous les nombres sont √©crits verbalement, et les abr√©viations possibles sont d√©chiffr√©es, par exemple: ¬´Mme  Robinson ¬ª-¬´ Missis Robinson ¬ª. <br><br>  Ainsi, apr√®s le pr√©traitement, nous obtenons des ensembles de tableaux numpy de s√©quences num√©riques et de spectrogrammes de craie enregistr√©s dans des fichiers npy sur disque. <br><br>  Pour qu'au stade de l'entra√Ænement, toutes les dimensions des tenseurs de patch co√Øncident, nous ajouterons des rembourrages √† de courtes s√©quences.  Pour les s√©quences sous forme de textes, celles-ci seront r√©serv√©es au remplissage 0, et aux spectrogrammes, trames dont les valeurs sont l√©g√®rement inf√©rieures aux spectrogrammes minimaux d√©termin√©s par nos soins.  Ceci est recommand√© pour isoler ces rembourrages, les s√©parer du bruit et du silence. <br><br>  Nous avons maintenant des donn√©es repr√©sentant du texte et de l'audio qui peuvent √™tre trait√©es par un r√©seau neuronal artificiel.  Examinons l'architecture du r√©seau de pr√©diction de fonctionnalit√©s, qui, sous le nom de l'√©l√©ment central de l'ensemble du syst√®me de synth√®se, sera appel√© Tacotron2. <br><br><h2>  L'architecture </h2><br>  Tacotron 2 n'est pas un r√©seau, mais deux: r√©seau de pr√©diction de fonctionnalit√©s et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NNet</a> -vocoder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WaveNet</a> .  L'article original, ainsi que notre propre vision du travail effectu√©, nous permet de consid√©rer Feature prediction net comme le premier violon, tandis que le vocodeur WaveNet joue le r√¥le d'un syst√®me p√©riph√©rique. <br><br>  Tacotron2 est une architecture <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">s√©quence √† s√©quence</a> .  Il se compose d'un <b>encodeur</b> (encodeur), qui cr√©e une repr√©sentation interne du signal d'entr√©e (jetons de symbole), et d'un <b>d√©codeur</b> (d√©codeur), qui transforme cette repr√©sentation en un spectrogramme √† la craie.  Un autre √©l√©ment extr√™mement important du r√©seau est le soi-disant <b>PostNet</b> , qui est con√ßu pour am√©liorer le spectrogramme g√©n√©r√© par le d√©codeur. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/744/8c3/587/7448c35878c1640d1e156e745fa2bd96.png"><br>  <i>Figure 2: Architecture du r√©seau Tacotron 2.</i> <br><br>  Examinons plus en d√©tail les blocs de r√©seau et leurs modules. <br><br>  La premi√®re couche d' <b>encodeur</b> est la couche d'int√©gration.  Bas√© sur une s√©quence de nombres naturels repr√©sentant des caract√®res, il cr√©e des vecteurs multidimensionnels (512 dimensions). <br><br>  Ensuite, les vecteurs d'int√©gration sont introduits dans un bloc de trois couches convolutives unidimensionnelles.  Chaque couche comprend 512 filtres de longueur 5. Cette valeur est une bonne taille de filtre dans ce contexte, car elle capture un certain caract√®re, ainsi que ses deux voisins pr√©c√©dents et deux suivants.  Chaque couche convolutionnelle est suivie d'une normalisation en mini-lots et d'une activation ReLU. <br><br>  Les tenseurs obtenus apr√®s le bloc convolutionnel sont appliqu√©s √† des couches LSTM bidirectionnelles, 256 neurones chacune.  Les r√©sultats avant et arri√®re sont concat√©n√©s. <br><br>  <b>Le d√©codeur</b> a une architecture r√©currente, c'est-√†-dire qu'√† chaque √©tape suivante, la sortie de l'√©tape pr√©c√©dente est utilis√©e.  Ici, ils seront une image du spectrogramme.  Un autre √©l√©ment important, sinon essentiel, de ce syst√®me est le m√©canisme de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">attention</a> douce (entra√Æn√©e) - une technique relativement nouvelle qui gagne de plus en plus en popularit√©.  A chaque √©tape du d√©codeur, attention √† former un vecteur de contexte et √† mettre √† jour le poids d'attention utilise: <br><br><ul><li>  la projection de l'√©tat cach√© pr√©c√©dent du r√©seau RNN du d√©codeur sur une couche enti√®rement connect√©e, </li><li>  projection de la sortie du codeur sur une couche enti√®rement connect√©e, </li><li>  ainsi que des poids d'attention suppl√©mentaires (accumul√©s √† chaque pas de temps du d√©codeur). </li></ul><br>  L'id√©e d'attention doit √™tre comprise comme suit: ¬´quelle partie des donn√©es du codeur doit √™tre utilis√©e √† l'√©tape actuelle du d√©codeur¬ª. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/f4d/410/e47f4d41040e5926c719ef909d21cc99.png"><br>  <i>Figure 3: Sch√©ma du m√©canisme d'attention.</i> <br><br>  √Ä chaque √©tape du d√©codeur, le vecteur de contexte <i>C <sub>i est</sub></i> calcul√© (indiqu√© dans la figure ci-dessus comme "sorties codeur assist√©"), qui est un produit de la sortie codeur ( <i>h</i> ) et des pond√©rations d'attention ( <i>Œ±</i> ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b27/8dc/00e/b278dc00e38919c05351ff95c03dc309.png"><br><br>  o√π <i>Œ± <sub>ij</sub></i> sont les poids d'attention calcul√©s par la formule: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3cc/344/b9f/3cc344b9f86ba736045a83a7cae8b77e.png"><br><br>  o√π <i>e <sub>ij</sub></i> est la soi-disant "√©nergie", dont la formule de calcul d√©pend du type de m√©canisme d'attention que vous utilisez (dans notre cas, ce sera un type hybride, utilisant √† la fois l'attention bas√©e sur la localisation et l'attention bas√©e sur le contenu).  L'√©nergie est calcul√©e par la formule: <br><br>  <i>e <sub>ij</sub> = v <sub>aT</sub> tanh (Ws <sub>i-1</sub> + Vh <sub>j</sub> + Uf <sub>i, j</sub> + b)</i> <br><br>  o√π: <br><ul><li>  <i>s <sub>i-1</sub></i> - √©tat cach√© pr√©c√©dent du r√©seau LSTM du d√©codeur, </li><li>  <i>Œ± <sub>i-1</sub></i> - poids d'attention pr√©c√©dents, </li><li>  <i>h <sub>j</sub></i> est le j√®me √©tat cach√© de l'encodeur, </li><li>  <i>W</i> , <i>V</i> , <i>U</i> , <i>v <sub>a</sub></i> et <i>b</i> sont des param√®tres d'apprentissage, </li><li>  <i>f <sub>i, j</sub></i> - signes de localisation calcul√©s par la formule: <br><br>  <i>f <sub>i</sub> = F * Œ± <sub>i-1</sub></i> <br><br>  o√π <i>F</i> est l'op√©ration de convolution. </li></ul><br><br>  Pour bien comprendre ce qui se passe, nous ajoutons que certains des modules d√©crits ci-dessous supposent l'utilisation des informations de l'√©tape pr√©c√©dente du d√©codeur.  Mais s'il s'agit de la premi√®re √©tape, les informations seront des tenseurs de valeurs nulles, ce qui est une pratique courante lors de la cr√©ation de structures de r√©currence. <br><br>  Consid√©rons maintenant <b>l'algorithme d'op√©ration</b> . <br><br>  Tout d'abord, la sortie du d√©codeur du pas de temps pr√©c√©dent est introduite dans un petit module PreNet, qui est un empilement de deux couches enti√®rement connect√©es de 256 neurones, en alternance avec des couches de d√©crochage avec un taux de 0,5.  Une caract√©ristique distinctive de ce module est que le d√©crochage y est utilis√© non seulement au stade de la formation du mod√®le, mais √©galement au stade de la sortie. <br><br>  La sortie PreNet en concat√©nation avec le vecteur de contexte obtenu √† la suite du m√©canisme d'attention est envoy√©e √† l'entr√©e dans un r√©seau LSTM unidirectionnel √† deux couches, 1024 neurones dans chaque couche. <br><br>  Ensuite, la concat√©nation de la sortie des couches LSTM avec le m√™me vecteur de contexte (et √©ventuellement diff√©rent) est introduite dans une couche enti√®rement connect√©e avec 80 neurones, ce qui correspond au nombre de canaux du spectrogramme.  Cette derni√®re couche du d√©codeur forme le spectrogramme pr√©vu trame par trame.  Et d√©j√† sa sortie est fournie en entr√©e au prochain pas de temps du d√©codeur dans PreNet. <br><br>  Pourquoi avons-nous mentionn√© dans le paragraphe pr√©c√©dent que le vecteur de contexte peut d√©j√† √™tre diff√©rent?  Une des approches possibles est de recalculer le vecteur de contexte apr√®s que l'√©tat latent du r√©seau LSTM est obtenu √† cette √©tape.  Cependant, dans nos exp√©riences, cette approche ne s'est pas justifi√©e. <br><br>  En plus de la projection sur une couche enti√®rement connect√©e √† 80 neurones, la concat√©nation de la sortie des couches LSTM avec un vecteur de contexte est introduite dans une couche enti√®rement connect√©e avec un neurone, suivie d'une activation sigmo√Øde - il s'agit d'une couche de ¬´pr√©diction de jeton d'arr√™t¬ª.  Il pr√©dit la probabilit√© que la trame cr√©√©e √† cette √©tape du d√©codeur soit d√©finitive.  Cette couche est con√ßue pour g√©n√©rer un spectrogramme de longueur non fixe mais arbitraire √† l'√©tape de sortie du mod√®le.  C'est-√†-dire qu'au niveau de la sortie, cet √©l√©ment d√©termine le nombre de pas du d√©codeur.  Il peut √™tre consid√©r√© comme un classificateur binaire. <br><br>  La sortie du d√©codeur de toutes ses √©tapes sera le spectrogramme pr√©vu.  Mais ce n'est pas tout.  Pour am√©liorer la qualit√© du spectrogramme, il est transmis via le module PostNet, qui est une pile de cinq couches convolutionnelles unidimensionnelles avec 512 filtres dans chacune et avec une taille de filtre de 5. La normalisation des lots et l'activation tangente suivent chaque couche (sauf la derni√®re).  Pour revenir √† la dimension du spectrogramme, nous passons les donn√©es de sortie post-net √† travers une couche enti√®rement connect√©e avec 80 neurones et ajoutons les donn√©es re√ßues avec le r√©sultat initial du d√©codeur.  Nous obtenons le spectrogramme √† la craie g√©n√©r√© √† partir du texte.  B√©n√©fice <br><br>  Tous les modules convolutifs sont r√©gularis√©s avec des couches de d√©crochage avec un taux de 0,5, et des couches de r√©currence avec la nouvelle m√©thode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Zoneout</a> avec un taux de 0,1.  C'est assez simple: au lieu d'appliquer l'√©tat latent et l'√©tat de cellule obtenus √† l'√©tape en cours √† l'√©tape de temps suivante du r√©seau LSTM, nous rempla√ßons une partie des donn√©es par les valeurs de l'√©tape pr√©c√©dente.  Cela se fait √† la fois au stade de la formation et au stade du retrait.  Dans ce cas, seul l'√©tat cach√© (qui est pass√© √† l'√©tape LSTM suivante) est expos√© √† la m√©thode Zoneout √† chaque √©tape, tandis que la sortie de la cellule LSTM √† l'√©tape actuelle reste inchang√©e. <br><br>  Nous avons choisi PyTorch comme cadre d'apprentissage en profondeur.  Bien qu'au moment de la mise en ≈ìuvre du r√©seau, il √©tait dans un √©tat de pr√©-version, mais c'√©tait d√©j√† un outil tr√®s puissant pour la construction et la formation de r√©seaux de neurones artificiels.  Dans notre travail, nous utilisons d'autres frameworks tels que TensorFlow et Keras.  Cependant, ce dernier a √©t√© rejet√© en raison de la n√©cessit√© d'impl√©menter des structures personnalis√©es non standard, et si nous comparons TensorFlow et PyTorch, alors lors de l'utilisation du second, il n'y a aucun sentiment que le mod√®le est arrach√© au langage Python.  Cependant, nous ne nous engageons pas √† affirmer que l'un d'eux est meilleur et l'autre pire.  L'utilisation d'un cadre particulier peut d√©pendre de divers facteurs. <br><br>  Le r√©seau est entra√Æn√© par la m√©thode de propagation arri√®re.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ADAM est</a> utilis√© comme optimiseur, l'erreur quadratique moyenne avant et apr√®s PostNet, ainsi que l'entropie crois√©e binaire sur les valeurs r√©elles et pr√©vues de la couche Stop Token Prediction, sont utilis√©es comme fonctions d'erreur.  L'erreur r√©sultante est une simple somme de ces trois. <br><br>  Le mod√®le a √©t√© form√© sur un seul GPU GeForce 1080Ti avec 11 Go de m√©moire. <br><br><h2>  Visualisation </h2><br>  Lorsque vous travaillez avec un si grand mod√®le, il est important de voir comment se d√©roule le processus d'apprentissage.  Et ici, TensorBoard est devenu un outil pratique.  Nous avons suivi la valeur de l'erreur dans les it√©rations de formation et de validation.  De plus, nous avons affich√© les spectrogrammes cibles, les spectrogrammes pr√©dits au stade de la formation, les spectrogrammes pr√©dits au stade de la validation et l'alignement, qui est un poids d'attention cumul√© cumul√© √† toutes les √©tapes de la formation. <br><br>  Il est possible qu'au d√©but votre attention ne soit pas trop informative: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4ce/600/7be/4ce6007bef5b7fe17aa8df56472904bc.png"><br>  <i>Figure 4: Un exemple d'√©chelles d'attention mal form√©es.</i> <br><br>  Mais apr√®s que tous vos modules commencent √† fonctionner comme une montre suisse, vous obtiendrez enfin quelque chose comme ceci: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8e7/5d9/ff0/8e75d9ff0e0da78915c1a16f623a99e8.png"><br>  <i>Figure 5: Exemple d'√©chelles d'attention correctement form√©es.</i> <br><br>  Que signifie ce tableau?  A chaque √©tape du d√©codeur, nous essayons de d√©coder une trame du spectrogramme.  Cependant, il n'est pas clair quelles informations le codeur doit utiliser √† chaque √©tape du d√©codeur.  On peut supposer que cette correspondance sera directe.  Par exemple, si nous avons une s√©quence de texte d'entr√©e de 200 caract√®res et un spectrogramme correspondant de 800 images, alors il y aura 4 images pour chaque caract√®re.  Cependant, vous devez admettre que la parole g√©n√©r√©e sur la base d'un tel spectrogramme serait compl√®tement d√©pourvue de naturel.  Nous pronon√ßons certains mots plus rapidement, certains plus lentement, quelque part nous nous arr√™tons, mais quelque part nous ne le faisons pas.  Et consid√©rer tous les contextes possibles n'est pas possible.  C'est pourquoi l'attention est un √©l√©ment cl√© de tout le syst√®me: elle √©tablit la correspondance entre l'√©tape du d√©codeur et les informations du codeur afin d'obtenir les informations n√©cessaires pour g√©n√©rer une trame sp√©cifique.  Et plus les pond√©rations d'attention sont √©lev√©es, plus ¬´une attention doit √™tre port√©e¬ª √† la partie correspondante des donn√©es du codeur lors de la g√©n√©ration de la trame du spectrogramme. <br><br>  Au stade de la formation, il sera √©galement utile de g√©n√©rer de l'audio, et pas seulement d'√©valuer visuellement la qualit√© des spectrogrammes et l'attention.  Cependant, ceux qui ont travaill√© avec WaveNet conviendront que son utilisation comme vocodeur au stade de la formation serait un luxe inacceptable en termes de temps.  Par cons√©quent, il est recommand√© d'utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l'algorithme Griffin-Lim</a> , qui permet une reconstruction partielle du signal apr√®s des transformations de Fourier rapides.  Pourquoi partiellement?  Le fait est que lorsque nous convertissons le signal en spectrogrammes, nous perdons des informations de phase.  Cependant, la qualit√© de l'audio ainsi obtenue sera tout √† fait suffisante pour comprendre dans quelle direction vous vous d√©placez. <br><br><h2>  Le√ßons apprises </h2><br>  Ici, nous partagerons quelques r√©flexions sur la construction du processus de d√©veloppement, en les soumettant sous forme de conseils.  Certains d'entre eux sont assez g√©n√©raux, d'autres sont plus sp√©cifiques. <br><br>  <b>√Ä propos de l'organisation du workflow</b> : <br><br><ul><li>  Utilisez le syst√®me de contr√¥le de version, d√©crivez clairement et clairement tous les changements.  Cela peut sembler une recommandation √©vidente, mais quand m√™me.  Lors de la recherche de l'architecture optimale, des changements se produisent constamment.  Et apr√®s avoir re√ßu un r√©sultat interm√©diaire satisfaisant, assurez-vous de vous faire un point de contr√¥le afin que vous puissiez effectuer les modifications suivantes en toute s√©curit√©. <br></li><li>  De notre point de vue, dans de telles architectures, il faut respecter les principes d'encapsulation: une classe - un module Python.  Cette approche n'est pas courante dans les t√¢ches ML, mais elle vous aidera √† structurer votre code et √† acc√©l√©rer le d√©bogage et le d√©veloppement.  Dans le code et dans votre vision de l'architecture, divisez-le en blocs, blocs en modules et modules en couches.  Si le module a du code qui remplit un r√¥le sp√©cifique, combinez-le dans une m√©thode de classe de module.  Ce sont des v√©rit√©s courantes, mais nous n'avons pas √©t√© trop paresseux pour en parler √† nouveau. <br></li><li>  Fournissez de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> aux classes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">style numpy</a> .  Cela simplifiera consid√©rablement le travail pour vous et vos coll√®gues qui liront votre code. <br></li><li>  Dessinez toujours l'architecture de votre mod√®le.  D'une part, il vous aidera √† lui donner un sens, et d'autre part, une vue lat√©rale de l'architecture et des hyperparam√®tres du mod√®le vous permettra d'identifier rapidement les inexactitudes de votre approche. <br></li><li>  Mieux vaut travailler en √©quipe.  Si vous travaillez seul, rassemblez toujours des coll√®gues et discutez de votre travail.  Au minimum, ils peuvent vous poser une question qui vous am√®nera √† quelques r√©flexions, mais au maximum ils pointeront vers une inexactitude sp√©cifique qui ne vous permettra pas de r√©ussir la formation du mod√®le. <br></li><li>  Une autre astuce utile est d√©j√† associ√©e au pr√©traitement des donn√©es.  Supposons que vous d√©cidiez de tester une hypoth√®se et d'apporter les modifications appropri√©es au mod√®le.  Mais recommencer l'entra√Ænement, surtout avant le week-end, sera risqu√©.  L'approche peut √™tre initialement erron√©e et vous perdrez du temps.  Que faire alors?  Augmentez la taille de la fen√™tre Transformation de Fourier rapide.  Le param√®tre par d√©faut est 1024;  l'augmenter de 4, voire 8 fois.  Cela ¬´pressera¬ª les spectrogrammes dans le nombre appropri√© de fois et acc√©l√©rera consid√©rablement l'apprentissage.  Le son r√©cup√©r√© aura une qualit√© inf√©rieure, mais ce n'est pas votre t√¢che maintenant?  En 2-3 heures, vous pouvez d√©j√† obtenir l'alignement (¬´alignement¬ª des √©chelles d'attention, comme le montre la figure ci-dessus), cela t√©moignera de l'exactitude architecturale de l'approche et elle peut d√©j√† √™tre test√©e sur des donn√©es volumineuses. <br></li></ul><br>  <b>Mod√®les de construction et de formation</b> : <br><br><ul><li>  Nous avons sugg√©r√© que si les lots n'√©taient pas form√©s de mani√®re al√©atoire, mais en fonction de leur longueur, ils acc√©l√©reraient le processus d'apprentissage du mod√®le et am√©lioreraient les spectrogrammes g√©n√©r√©s.  L'hypoth√®se logique, qui est bas√©e sur l'hypoth√®se que plus un signal utile (et non de remplissage) est envoy√© au r√©seau d'entra√Ænement, mieux c'est.  Cependant, cette approche ne se justifiait pas; dans nos exp√©riences, nous n'avons pas pu former le r√©seau de cette mani√®re.  Cela est probablement d√ª √† la perte d'al√©atoire dans la s√©lection des instances pour la formation. <br></li><li>  Utilisez des algorithmes d'initialisation des param√®tres r√©seau modernes avec des √©tats initiaux optimis√©s.  Par exemple, dans nos exp√©riences, nous avons utilis√© l'initialisation du poids uniforme Xavier.  Si dans votre module vous devez utiliser la normalisation par mini-batch et une fonction d'activation, ils doivent alterner les uns avec les autres dans cet ordre.  En effet, si nous appliquons, par exemple, l'activation ReLU, nous perdons imm√©diatement tout le signal n√©gatif qui devrait √™tre impliqu√© dans le processus de normalisation des donn√©es d'un lot particulier. <br></li><li>  √Ä partir d'une √©tape d'apprentissage sp√©cifique, utilisez un taux d'apprentissage dynamique.  Cela aide vraiment √† r√©duire la valeur d'erreur et √† augmenter la qualit√© des spectrogrammes g√©n√©r√©s. <br></li><li>  Apr√®s avoir cr√©√© le mod√®le et tent√© sans succ√®s de le former sur des lots √† partir de l'ensemble de donn√©es, il sera utile d'essayer de le recycler sur un lot.    ,   alignment,           (    ).  ,      ,      . <br><br>    .        .  ,        ‚Äì      .    ,            .       ,       . </li><li>    RNN-              .      . ,           .        ?             LSTM-     -. <br></li><li>       ,   LSTM-,      ¬´ ¬ª: ¬´ <i>       ,         LSTM-.      ¬´¬ª  bf.   ,        ,    ,   LSTM-     ft  1/2.   ,        :    ,        ¬´¬ª  1/2,         .    bf    ,  1   2:     ft                 </i> ¬ª. <br></li><li>   seq2seq-         .       ‚Äî       ,         .           ?           ,        ( ). <br></li><li> Maintenant une recommandation sp√©cifique pour le framework PyTorch.  Bien que la couche LSTM dans le d√©codeur soit essentiellement sa propre cellule LSTM, qui re√ßoit des informations pour un seul √©l√©ment de la s√©quence √† chaque √©tape du d√©codeur, il est recommand√© d'utiliser la classe <code>torch.nn.LSTM</code> plut√¥t que <code>torch.nn.LSTMCell</code> .  La raison en est que le backend LSTM est impl√©ment√© dans la biblioth√®que CUDNN en C et LSTMCell en Python.  Cette astuce vous permettra d'augmenter consid√©rablement la vitesse du syst√®me. </li></ul><br>  Et √† la fin de l'article, nous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">partagerons des exemples de g√©n√©ration de discours √† partir de textes qui n'√©taient pas contenus dans l'ensemble de formation.</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436312/">https://habr.com/ru/post/fr436312/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436302/index.html">Exp√©rience de la substitution r√©elle des importations √† l'aide du syst√®me de stockage russe AERODISK</a></li>
<li><a href="../fr436304/index.html">Zimbra Collaboration Suite et la lutte contre le phishing</a></li>
<li><a href="../fr436306/index.html">Apprentissage automatique pour Vertica</a></li>
<li><a href="../fr436308/index.html">Rostelecom pourrait devenir un monopole sur le march√© des centres de donn√©es</a></li>
<li><a href="../fr436310/index.html">Comme l'a fait Ivan Metrics DevOps. Objet d'influence</a></li>
<li><a href="../fr436314/index.html">Un h√¥tel-robot japonais a "licenci√©" la moiti√© de ses robots en raison des probl√®mes qu'ils cr√©ent</a></li>
<li><a href="../fr436316/index.html">Comment les cartes √† puce aident √† stimuler les projets informatiques</a></li>
<li><a href="../fr436318/index.html">Nouvelles fonctionnalit√©s d'automatisation r√©seau dans Red Hat Ansible</a></li>
<li><a href="../fr436320/index.html">De nombreuses propri√©t√©s ou propri√©t√©-objet: crit√®res de s√©lection</a></li>
<li><a href="../fr436322/index.html">@Pythonetc d√©cembre 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>