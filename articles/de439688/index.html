<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚶🏿 🕋 🍼 Überblick über AI & ML-Lösungen im Jahr 2018 und Prognosen für 2019: Teil 1 - NLP, Computer Vision 🖖🏿 🗃️ 😴</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! Ich präsentiere Ihnen eine Übersetzung des Analytics Vidhya- Artikels mit einem Überblick über AI / ML-Ereignisse in den Trends 2018...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Überblick über AI & ML-Lösungen im Jahr 2018 und Prognosen für 2019: Teil 1 - NLP, Computer Vision</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439688/"><blockquote> Hallo allerseits!  Ich präsentiere Ihnen eine Übersetzung des <i>Analytics Vidhya-</i> Artikels mit einem Überblick über AI / ML-Ereignisse in den Trends 2018 und 2019.  Das Material ist ziemlich groß und daher in zwei Teile unterteilt.  Ich hoffe, dass der Artikel nicht nur spezialisierte Spezialisten interessiert, sondern auch diejenigen, die sich für das Thema KI interessieren.  Viel Spaß beim Lesen! <br><br><div class="spoiler">  <b class="spoiler_title">Artikelnavigation</b> <div class="spoiler_text">  <b>Teil 1</b> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verarbeitung natürlicher Sprache (NLP)</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NLP-Trends für 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Computer Vision</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trends in der Bildverarbeitung für 2019</a> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tools und Bibliotheken</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AutoML-Trends für 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verstärkungslernen</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verstärkungstrends für 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">KI für gute Jungs - Bewegung in Richtung „ethische“ KI</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ethische Trends in der KI für 2019</a> <br></div></div></blockquote><br><h2>  Einführung </h2><br>  Die letzten Jahre für KI-Enthusiasten und Profis des maschinellen Lernens sind auf der Suche nach einem Traum vergangen.  Diese Technologien sind keine Nischen mehr, haben sich zum Mainstream entwickelt und wirken sich bereits jetzt auf das Leben von Millionen von Menschen aus.  KI-Ministerien wurden in verschiedenen Ländern eingerichtet [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">weitere Details hier</a> - ca.  per.] und Budgets werden zugewiesen, um mit diesem Rennen Schritt zu halten. <br><br>  Gleiches gilt für Data-Science-Profis.  Vor ein paar Jahren konnte man sich wohl fühlen, wenn man ein paar Werkzeuge und Tricks kannte, aber diesmal ist es vorbei.  Die Anzahl der jüngsten Ereignisse in der Datenwissenschaft und die Menge an Wissen, die erforderlich ist, um mit der Zeit in diesem Bereich Schritt zu halten, sind erstaunlich. <br><br>  Ich beschloss, einen Schritt zurückzutreten und die Entwicklungen in einigen Schlüsselbereichen auf dem Gebiet der künstlichen Intelligenz aus Sicht von Data-Science-Experten zu betrachten.  Welche Ausbrüche sind aufgetreten?  Was ist 2018 passiert und was erwartet Sie 2019?  Lesen Sie diesen Artikel für Antworten! <a name="habracut"></a><br><br>  PS Wie in jeder Prognose sind nachfolgend meine persönlichen Schlussfolgerungen aufgeführt, die auf Versuchen beruhen, einzelne Fragmente zu einem Gesamtbild zu kombinieren.  Wenn sich Ihre Sichtweise von meiner unterscheidet, würde ich mich freuen, Ihre Meinung darüber zu erfahren, was sich 2019 in der Datenwissenschaft noch ändern könnte. <br><br>  Die Bereiche, die wir in diesem Artikel behandeln werden, sind: <br><br>  - Natürliche Sprachprozesse (NLP) <br>  - Computer Vision <br>  - Tools und Bibliotheken <br>  - Verstärkungslernen <br>  - Ethikprobleme in der KI <br><br><a name="NLP"></a><h2>  Verarbeitung natürlicher Sprache (NLP) </h2><br>  Maschinen zu zwingen, Wörter und Sätze zu analysieren, schien immer ein Wunschtraum zu sein.  Es gibt viele Nuancen und Merkmale in Sprachen, die selbst für Menschen manchmal schwer zu verstehen sind, aber 2018 war ein echter Wendepunkt für NLP. <br><br>  Wir haben einen großartigen Durchbruch nach dem anderen gesehen: ULMFiT, ELMO, OpenAl Transformer, Google BERT, und dies ist keine vollständige Liste.  Die erfolgreiche Anwendung des Transfer-Lernens (die Kunst, vorab trainierte Modelle auf Daten anzuwenden) hat NLP die Tür für eine Vielzahl von Aufgaben geöffnet. <br><blockquote>  Transferlernen - Ermöglicht die Anpassung eines vorab trainierten Modells / Systems an Ihre spezifische Aufgabe mit relativ geringen Datenmengen. </blockquote>  Schauen wir uns einige dieser Schlüsselentwicklungen genauer an. <br><br><h3>  ULMFiT </h3><br>  ULMFiT wurde von Sebastian Ruder und Jeremy Howard (fast.ai) entwickelt und war das erste Framework, das in diesem Jahr Transferlernen erhielt.  Für die Uneingeweihten steht das Akronym ULMFiT für „Universal Language Model Fine-Tuning“.  Jeremy und Sebastian haben zu Recht das Wort „universal“ zu ULMFiT hinzugefügt - dieses Framework kann auf fast jede NLP-Aufgabe angewendet werden! <br><br>  Das Beste an ULMFiT ist, dass Sie Modelle nicht von Grund auf neu trainieren müssen!  Forscher haben bereits das Schwierigste für Sie getan - nehmen Sie an und bewerben Sie sich in Ihren Projekten.  ULMFiT übertraf andere Methoden in sechs Textklassifizierungsaufgaben. <br><br>  Sie können <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das</a> Tutorial von Pratek Joshi [Pateek Joshi - ca.  trans.], wie Sie ULMFiT für jede Aufgabe der Textklassifizierung verwenden können. <br><br><h3>  ELMo </h3><br>  Ratet mal, was die Abkürzung ELMo bedeutet?  Akronym für Einbettungen aus Sprachmodellen [Anhänge aus Sprachmodellen - ca.  trans.].  Und ELMo erregte direkt nach der Veröffentlichung die Aufmerksamkeit der ML-Community. <br><br>  ELMo verwendet Sprachmodelle, um Anhänge für jedes Wort zu erhalten, und berücksichtigt auch den Kontext, in dem das Wort in einen Satz oder Absatz passt.  Der Kontext ist ein kritischer Aspekt von NLP, bei dem die meisten Entwickler zuvor versagt haben.  ELMo verwendet bidirektionale LSTMs, um Anhänge zu erstellen. <br><blockquote>  Das Langzeitgedächtnis (LSTM) ist eine Art Architektur von wiederkehrenden neuronalen Netzen, die 1997 von Sepp Hochreiter und Jürgen Schmidhuber vorgeschlagen wurde.  Wie die meisten wiederkehrenden neuronalen Netze ist ein LSTM-Netz in dem Sinne universell, dass es mit einer ausreichenden Anzahl von Netzelementen jede Berechnung durchführen kann, zu der ein normaler Computer in der Lage ist, was eine geeignete Gewichtsmatrix erfordert, die als Programm betrachtet werden kann.  Im Gegensatz zu herkömmlichen wiederkehrenden neuronalen Netzen eignet sich das LSTM-Netz gut zum Training der Probleme der Klassifizierung, Verarbeitung und Vorhersage von Zeitreihen in Fällen, in denen wichtige Ereignisse durch Zeitverzögerungen mit unbestimmter Dauer und Grenzen getrennt sind. <br><br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wikipedia</a> </blockquote>  Wie ULMFiT verbessert ELMo die Produktivität bei der Lösung einer großen Anzahl von NLP-Aufgaben erheblich, z. B. bei der Analyse der Textstimmung oder der Beantwortung von Fragen. <br><br><h3>  BERT von Google </h3><br>  Viele Experten stellen fest, dass die Veröffentlichung von BERT den Beginn einer neuen Ära in NLP markiert.  Nach ULMFiT und ELMo übernahm BERT die Führung und zeigte hohe Leistung.  In der ursprünglichen Ankündigung heißt es: „BERT ist konzeptionell einfach und empirisch leistungsfähig.“ <br><br>  BERT hat in 11 NLP-Aufgaben hervorragende Ergebnisse gezeigt!  Siehe die Ergebnisse in SQuAD-Tests: <br><br><img src="https://habrastorage.org/webt/rf/6n/cz/rf6nczjjvbcz1cg4nxfeo-lm7ou.png"><br><br>  Willst du es versuchen?  Sie können die Neuimplementierung in PyTorch oder den TensorFlow-Code von Google verwenden und versuchen, das Ergebnis auf Ihrem Computer zu wiederholen. <br><br><h3>  Facebook PyText </h3><br>  Wie konnte sich Facebook von diesem Rennen fernhalten?  Das Unternehmen bietet ein eigenes Open-Source-NLP-Framework namens PyText an.  Laut einer von Facebook veröffentlichten Studie hat PyText die Genauigkeit von Konversationsmodellen um 10% erhöht und die Trainingszeit verkürzt. <br><br>  PyText steht tatsächlich hinter mehreren eigenen Facebook-Produkten wie Messenger.  Die Zusammenarbeit mit ihm wird Ihrem Portfolio also einen guten Punkt und unschätzbares Wissen hinzufügen, das Sie zweifellos gewinnen werden. <br><br>  Sie können es selbst versuchen und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Code von GitHub herunterladen</a> . <br><br><h3>  Google Duplex </h3><br>  Es ist kaum zu glauben, dass Sie noch nichts von Google Duplex gehört haben.  Hier ist eine Demo, die lange Zeit in den Schlagzeilen aufblitzte: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO0-5MuJvew" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Da es sich um ein Google-Produkt handelt, besteht kaum eine Chance, dass der Code früher oder später für alle veröffentlicht wird.  Natürlich wirft diese Demonstration viele Fragen auf: von ethischen zu Datenschutzfragen, aber wir werden später darüber sprechen.  Genießen Sie vorerst nur, wie weit wir mit ML in den letzten Jahren gekommen sind. <br><br><a name="NLPtrends"></a><h2>  NLP-Trends 2019 </h2><br>  Wer kann besser als Sebastian Ruder selbst eine Vorstellung davon geben, wohin die NLP 2019 steuert?  Hier sind seine Ergebnisse: <br><blockquote><ol><li>  Die Verwendung von vorgefertigten Sprachinvestitionsmodellen wird weit verbreitet sein.  Fortgeschrittene Modelle ohne Unterstützung werden sehr selten sein. </li><li>  Es werden vorab trainierte Ansichten angezeigt, die spezielle Informationen codieren können, die die Anhänge des Sprachmodells ergänzen.  Abhängig von den Anforderungen der Aufgabe können wir verschiedene Arten von vorgefertigten Präsentationen gruppieren. </li><li>  Weitere Arbeiten werden im Bereich mehrsprachiger Anwendungen und mehrsprachiger Modelle erscheinen.  Insbesondere wenn wir uns auf die Einbettung von Wörtern in verschiedenen Sprachen stützen, werden wir die Entstehung tief vorgefertigter Darstellungen in verschiedenen Sprachen sehen. </li></ol></blockquote><a name="cv"></a><h2>  Computer Vision </h2><br><img src="https://habrastorage.org/webt/pu/aj/_c/puaj_c89feaiultos4yynrcj7x4.jpeg"><br><br>  Computer Vision ist heute das beliebteste Gebiet im Bereich des tiefen Lernens.  Es scheint, dass die ersten Früchte der Technologie bereits erhalten wurden und wir uns im Stadium der aktiven Entwicklung befinden.  Unabhängig davon, ob es sich um ein Bild oder ein Video handelt, entstehen viele Frameworks und Bibliotheken, mit denen sich die Probleme der Bildverarbeitung leicht lösen lassen. <br><br>  Hier ist meine Liste der besten Lösungen, die dieses Jahr zu sehen waren. <br><br><h3>  BigGANs raus </h3><br>  Ian Goodfellow entwarf die GANs im Jahr 2014 und das Konzept brachte eine Vielzahl von Anwendungen hervor.  Jahr für Jahr beobachteten wir, wie das ursprüngliche Konzept für die Anwendung in realen Fällen fertiggestellt wurde.  Eines blieb jedoch bis zu diesem Jahr unverändert: Computergenerierte Bilder waren zu leicht zu unterscheiden.  Im Rahmen trat immer eine gewisse Inkonsistenz auf, die den Unterschied sehr deutlich machte. <br><br>  In den letzten Monaten haben sich Verschiebungen in diese Richtung ergeben, und mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schaffung von BigGAN</a> können solche Probleme ein für alle Mal gelöst werden.  Schauen Sie sich die mit dieser Methode erzeugten Bilder an: <br><br><img src="https://habrastorage.org/webt/mo/w7/ow/mow7owldedw4r1jtwex6wbfwwje.png"><br><br>  Ohne ein Mikroskop ist es schwer zu sagen, was mit diesen Bildern nicht stimmt.  Natürlich wird jeder für sich selbst entscheiden, aber es besteht kein Zweifel daran, dass das GAN die Art und Weise verändert, wie wir digitale Bilder (und Videos) wahrnehmen. <br><br>  Als Referenz: Diese Modelle wurden zuerst auf dem ImageNet-Datensatz und dann auf dem JFT-300M trainiert, um zu demonstrieren, dass diese Modelle gut von einem Datensatz auf einen anderen übertragen werden.  Hier ist ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link zu einer Seite</a> aus der GAN-Mailingliste, auf der erklärt wird, wie das GAN visualisiert und verstanden wird. <br><br><h3>  Model Fast.ai trainierte in 18 Minuten auf ImageNet </h3><br>  Dies ist eine wirklich coole Implementierung.  Es ist weit verbreitet, dass Sie für die Durchführung von Deep-Learning-Aufgaben Terabyte an Daten und große Computerressourcen benötigen.  Gleiches gilt für das Training des Modells von Grund auf auf ImageNet-Daten.  Die meisten von uns dachten genauso, bevor ein paar Leute auf fast.ai nicht jedem das Gegenteil beweisen konnten. <br><br>  Ihr Modell ergab eine Genauigkeit von 93% mit beeindruckenden 18 Minuten.  Die von ihnen verwendete Hardware, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ihrem Blog</a> ausführlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beschrieben</a> wurde, bestand aus 16 öffentlichen AWS-Cloud-Instanzen mit jeweils 8 NVIDIA V100-GPUs.  Sie erstellten einen Algorithmus unter Verwendung der Bibliotheken fast.ai und PyTorch. <br><br>  Die Gesamtkosten für die Montage betrugen nur 40 US-Dollar!  Jeremy hat ihre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ansätze und Methoden hier</a> ausführlicher beschrieben.  Dies ist ein gemeinsamer Sieg! <br><br><h3>  vid2vid von NVIDIA </h3><br>  In den letzten 5 Jahren hat die Bildverarbeitung große Fortschritte gemacht, aber was ist mit Video?  Die Methoden zum Konvertieren von einem statischen in einen dynamischen Frame erwiesen sich als etwas komplizierter als erwartet.  Können Sie eine Folge von Bildern aus einem Video entnehmen und vorhersagen, was im nächsten Bild passieren wird?  Solche Studien wurden bereits früher durchgeführt, aber die Veröffentlichungen waren bestenfalls vage. <br><br><img src="https://habrastorage.org/webt/hz/ox/hj/hzoxhjbehlnlzl8ivc-bgiz0vh0.png"><br><br>  NVIDIA hat beschlossen, seine Entscheidung Anfang dieses Jahres öffentlich zugänglich zu machen [2018 - ca.  per.], die von der Gesellschaft positiv bewertet wurde.  Der Zweck von vid2vid besteht darin, eine Anzeigefunktion aus einem bestimmten Eingabevideo abzuleiten, um ein Ausgabevideo zu erstellen, das den Inhalt des Eingabevideos mit unglaublicher Genauigkeit wiedergibt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S1OwOd-war8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Sie können ihre Implementierung auf PyTorch ausprobieren und hier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu GitHub bringen</a> . <br><br><a name="cvtrends"></a><h2>  Bildverarbeitungstrends für 2019 </h2><br>  Wie ich bereits erwähnt habe, werden wir 2019 eher die Entwicklung der Trends für 2018 als neue Durchbrüche sehen: selbstfahrende Autos, Gesichtserkennungsalgorithmen, virtuelle Realität und mehr.  Können Sie mir nicht zustimmen, wenn Sie eine andere Sichtweise oder Ergänzungen haben, teilen Sie es uns mit, was können wir 2019 noch erwarten? <br><br>  Das Thema Drohnen könnte in Erwartung der Zustimmung von Politikern und Regierung endlich grünes Licht in den Vereinigten Staaten bekommen (Indien liegt in dieser Angelegenheit weit zurück).  Persönlich möchte ich, dass mehr Forschung in realen Szenarien durchgeführt wird.  Konferenzen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CVPR</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICML</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bieten</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen</a> guten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Überblick</a> über die neuesten Errungenschaften in diesem Bereich, aber wie nah die Projekte an der Realität sind, ist nicht sehr klar. <br><br>  "Visuelle Fragen beantworten" und "visuelle Dialogsysteme" könnten endlich mit einem lang erwarteten Debüt herauskommen.  Diesen Systemen fehlt die Fähigkeit zur Verallgemeinerung, aber es wird erwartet, dass wir bald einen integrierten multimodalen Ansatz sehen werden. <br><br><img src="https://habrastorage.org/webt/s5/bn/uy/s5bnuydmsc8hf37vm26icbmwrgc.jpeg"><br><br>  Das Selbsttraining stand in diesem Jahr im Vordergrund.  Ich wette, dass es nächstes Jahr in einer viel größeren Anzahl von Studien Anwendung finden wird.  Dies ist eine wirklich coole Richtung: Zeichen werden direkt aus den Eingabedaten ermittelt, anstatt Zeit damit zu verschwenden, die Bilder manuell zu markieren.  Drücken wir die Daumen! <br><br><h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lesen Sie mehr: Teil 2 - Tools und Bibliotheken, AutoML, Reinforcement Learning, Ethik in der KI</a> </h4></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439688/">https://habr.com/ru/post/de439688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439676/index.html">Reagieren Sie auf Native- und C ++ - Integration für iOS und Android</a></li>
<li><a href="../de439678/index.html">Senden Sie an die Applied F # Challenge</a></li>
<li><a href="../de439680/index.html">Etwa 50% der Russen sind bereit, ihre persönlichen Daten zu verkaufen</a></li>
<li><a href="../de439682/index.html">Schulung Cisco 200-125 CCNA v3.0. Cisco Certified Network Specialist (CCNA). Tag 4. Gateway-Geräte</a></li>
<li><a href="../de439684/index.html">Bewerben Sie sich für die Applied F # Challenge</a></li>
<li><a href="../de439690/index.html">Vergleich der Leistung virtueller Maschinen von 6 Cloud-Plattformen: Selectel, MCS, I. Cloud, Google Cloud, AWS und Azure</a></li>
<li><a href="../de439692/index.html">AT & T wurde wegen Änderung des Netzwerksymbols von 4G auf 5G E verklagt</a></li>
<li><a href="../de439694/index.html">Intelligentes Gewebe, das auf Änderungen der Körpertemperatur reagiert</a></li>
<li><a href="../de439696/index.html">Auf dem Wellenkamm oder "Ich will Mainstream" - aber lohnt es sich?</a></li>
<li><a href="../de439698/index.html">Einführung in die Programmierung: Ein einfacher 3D-Shooter von Grund auf über das Wochenende, Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>