<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüíº ü§üüèΩ üë©üèø‚Äç‚öïÔ∏è Einf√ºhrung in die neuronale ODE ü¶å üå∑ üìÑ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neuronale gew√∂hnliche Differentialgleichungen 
 Ein erheblicher Teil der Prozesse wird durch Differentialgleichungen beschrieben. Dies kann die zeitli...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Einf√ºhrung in die neuronale ODE</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/442002/"><h1>  Neuronale gew√∂hnliche Differentialgleichungen </h1><br>  Ein erheblicher Teil der Prozesse wird durch Differentialgleichungen beschrieben. Dies kann die zeitliche Entwicklung des physischen Systems, der Gesundheitszustand des Patienten, grundlegende Merkmale der B√∂rse usw. sein.  Die Daten zu solchen Prozessen sind konsistenter und kontinuierlicher Natur, in dem Sinne, dass Beobachtungen lediglich Manifestationen eines sich st√§ndig √§ndernden Zustands sind. <br><br>  Es gibt auch eine andere Art von seriellen Daten, n√§mlich diskrete Daten, z. B. NLP-Aufgabendaten.  Der Status in solchen Daten variiert diskret: von einem Zeichen oder Wort zum anderen. <br><br>  Jetzt werden beide Arten solcher serieller Daten normalerweise von rekursiven Netzwerken verarbeitet, obwohl sie unterschiedlicher Natur sind und unterschiedliche Ans√§tze zu erfordern scheinen. <br><br>  Auf der letzten <em>NIPS-Konferenz</em> wurde ein sehr interessanter Artikel vorgestellt, der zur L√∂sung dieses Problems beitragen kann.  Die Autoren schlagen einen Ansatz vor, den sie als <strong>neuronale ODEs bezeichnen</strong> . <br><br>  Hier habe ich versucht, die Ergebnisse dieses Artikels zu reproduzieren und zusammenzufassen, um ihre Idee etwas einfacher kennenzulernen.  Es scheint mir, dass diese neue Architektur neben Faltungsnetzwerken und wiederkehrenden Netzwerken durchaus einen Platz in den Standardwerkzeugen eines Datenwissenschaftlers finden k√∂nnte. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7e/65/hf/7e65hfxs1amdqyy_uy6emdwulkg.png"></div><br><a name="habracut"></a><br><i></i><p>  <em>Abbildung</em> 1: Kontinuierliche Gradienten- <em>Backpropagation</em> erfordert die zeitliche L√∂sung der erweiterten Differentialgleichung. <br><br>  Die Pfeile repr√§sentieren die Anpassung von r√ºckw√§rts propagierten Gradienten durch Gradienten aus den Beobachtungen. <br><br>  Illustration aus dem Originalartikel. </p><br><h2>  Problemstellung </h2><br>  Es gebe einen Prozess, der einer unbekannten ODE gehorcht, und es gebe mehrere (verrauschte) Beobachtungen entlang der Flugbahn des Prozesses <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/438/dde/8ca/438dde8cabcfd6a826ed0257752e6499.svg" alt="\ frac {dz} {dt} = f (z (t), t) \;  (1)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/190/072/e64/190072e645d65dc29d6160aaf0dde065.svg" alt="\ {(z_0, t_0), (z_1, t_1), ..., (z_M, t_M) \} - \ text {Beobachtungen}"></div><br>  So finden Sie eine Ann√§herung <img src="https://habrastorage.org/getpro/habr/post_images/287/bb8/c63/287bb8c637a8a70238be4ea690a0a8e1.svg" alt="\ widehat {f} (z, t, \ theta)">  Lautsprecherfunktionen <img src="https://habrastorage.org/getpro/habr/post_images/6ad/6ab/1fa/6ad6ab1fa56e2c8b6d987dc5c1f68004.svg" alt="f (z, t)">  ? <br><br>  Betrachten Sie zun√§chst eine einfachere Aufgabe: Es gibt nur zwei Beobachtungen am Anfang und am Ende der Flugbahn. <img src="https://habrastorage.org/getpro/habr/post_images/547/808/5c2/5478085c21b85393c8e9e0f78c2821b3.svg" alt="(z_0, t_0), (z_1, t_1)">  . <br><br>  Die Systementwicklung beginnt mit dem Zustand <img src="https://habrastorage.org/getpro/habr/post_images/7e2/d13/d9a/7e2d13d9a9cc05a84422c3b63260dd83.svg" alt="z_0, t_0">  auf Zeit <img src="https://habrastorage.org/getpro/habr/post_images/4fc/683/cd0/4fc683cd033494d093a0e2c6f021805b.svg" alt="t_1 - t_0">  mit einigen parametrisierten Dynamikfunktionen unter Verwendung einer beliebigen Evolutionsmethode von ODE-Systemen.  Nachdem sich das System in einem neuen Zustand befindet <img src="https://habrastorage.org/getpro/habr/post_images/56d/8d1/11e/56d8d111e514029cf892aff24a82c768.svg" alt="\ hat {z_1}, t_1">  wird es mit dem Staat verglichen <img src="https://habrastorage.org/getpro/habr/post_images/f80/709/9a5/f807099a57f179abaa3ab5e70cee4c9c.svg" alt="z_1">  und der Unterschied zwischen ihnen wird durch Variieren der Parameter minimiert <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ Theta">  Dynamikfunktionen. <br><br>  Oder formeller in Betracht ziehen, die Verlustfunktion zu minimieren <img src="https://habrastorage.org/getpro/habr/post_images/b03/557/43f/b0355743fe8383284ef53436870494da.svg" alt="L (\ hat {z_1})">  :: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/251/944/489/251944489756d64d40b61f0dc0d7cb0b.svg" alt="L (z (t_1)) = L \ Big (\ int_ {t_0} ^ {t_1} f (z (t), t, \ theta) dt \ Big) = L \ big (\ text {ODESolve} (z () t_0), f, t_0, t_1, \ theta) \ big) \; (2)"></div><br>  Zu minimieren <img src="https://habrastorage.org/getpro/habr/post_images/899/a67/551/899a675510d28419769a9b42281f0c65.svg" alt="L.">  m√ºssen Sie die Verl√§ufe f√ºr alle Parameter berechnen: <img src="https://habrastorage.org/getpro/habr/post_images/194/e4f/6e5/194e4f6e59bbfefc52efebce3bb857a3.svg" alt="z (t_0), t_0, t_1, \ theta">  .  Dazu m√ºssen Sie zun√§chst festlegen, wie <img src="https://habrastorage.org/getpro/habr/post_images/899/a67/551/899a675510d28419769a9b42281f0c65.svg" alt="L.">  h√§ngt zu jedem Zeitpunkt vom Zustand ab <img src="https://habrastorage.org/getpro/habr/post_images/823/f33/fc8/823f33fc81685e76b1d88f48c68eea32.svg" alt="(z (t))">  :: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e79/64f/b7f/e7964fb7f074d4f9c16893b53a1562c6.svg" alt="a (t) = - \ frac {\ partielles L} {\ partielles z (t)} \; (3)"></div><br><img src="https://habrastorage.org/getpro/habr/post_images/2e3/af6/935/2e3af69359cb79517739f0ccf9a8bc3e.svg" alt="a (t)">  wird ein <em>adjungierter</em> Zustand genannt, dessen Dynamik durch eine andere Differentialgleichung gegeben ist, die als kontinuierliches Analogon der Differenzierung einer komplexen Funktion ( <em>Kettenregel</em> ) betrachtet werden kann: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/b66/5d5/006b665d51719470f25859e6271463dc.svg" alt="\ frac {da (t)} {d t} = -a (t) \ frac {\ partielles f (z (t), t, \ theta)} {\ partielles z} \; (4)"></div><br>  Die Ausgabe dieser Formel finden Sie im Anhang des Originalartikels. <br><br>  <i>Die Vektoren in diesem Artikel sollten als Kleinbuchstabenvektoren betrachtet werden, obwohl der urspr√ºngliche Artikel sowohl eine Zeilen- als auch eine Spaltendarstellung verwendet.</i> <br><br>  Wenn wir diffur (4) in der Zeit zur√ºckl√∂sen, erhalten wir eine Abh√§ngigkeit vom Ausgangszustand <img src="https://habrastorage.org/getpro/habr/post_images/9ec/7ef/827/9ec7ef8276505d510f609b983c58fdc3.svg" alt="z (t_0)">  :: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/556/75a/7f0/55675a7f0c820f2a1da88e0802c97dc7.svg" alt="\ frac {\ partielles L} {\ partielles z (t_0)} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partielles f (z (t), t, \ theta)} {\ partielles z} dt \; (5)"></div><br>  Berechnung des Gradienten in Bezug auf <img src="https://habrastorage.org/getpro/habr/post_images/2a3/102/4ea/2a31024ea1803c34a47496e24a53a1ef.svg" alt="t">  und <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ Theta">  k√∂nnen Sie sie einfach als Teil des Staates betrachten.  Dieser Zustand wird als <em>erweitert bezeichnet</em> .  Die Dynamik dieses Zustands ergibt sich trivial aus der urspr√ºnglichen Dynamik: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/161/f11/79c/161f1179cbb6dfe3c18057d8abd0f45b.svg" alt="\ frac {d} {dt} \ begin {bmatrix} z ‚Äã‚Äã\\ \ theta \\ t \ end {bmatrix} (t) = f _ {\ text {aug}} ([z, \ theta, t]): = \ begin {bmatrix} f ([z, \ theta, t]) \\ 0 \\ 1 \ end {bmatrix} \; (6)"></div><br>  Dann der konjugierte Zustand zu diesem erweiterten Zustand: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7dc/7d0/553/7dc7d055304addbb695d1f23a9e640e6.svg" alt="a _ {\ text {aug}}: = \ begin {bmatrix} a \\ a _ {\ theta} \\ a_t \ end {bmatrix}, a _ {\ theta} (t): = \ frac {\ partielles L} { \ partiell \ theta (t)}, a_t (t): = \ frac {\ partiell L} {\ partiell t (t)} \; (7)"></div><br>  Gradient Augmented Dynamics: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/985/437/7a4/9854377a42cda72fa53bedb928a9d023.svg" alt="\ frac {\ partielle f _ {\ text {aug}}} {\ partielle [z, \ theta, t]} = \ begin {bmatrix} \ frac {\ partielle f} {\ partielle z} &amp;; \ frac {\ partielle f} {\ partielle \ theta} &amp;; \ frac {\ partielle f} {\ partielle t} \\ 0 &amp;; 0 &amp;; 0 \\ 0 &amp;; 0 &amp;; 0 \ end {bmatrix} \; (8)"></div><br>  Die Differentialgleichung des konjugierten Augmented State aus Formel (4) lautet dann: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bd3/b9f/7f3/bd3b9f7f3e6ec08881594346a8a3e4f3.svg" alt="\ frac {d a _ {\ text {aug}}} {dt} = - \ begin {bmatrix} a \ frac {\ partielle f} {\ partielle z} &amp;; a \ frac {\ partielles f} {\ partielles \ theta} &amp;; a \ frac {\ partielles f} {\ partielles t} \ end {bmatrix} \; (9)"></div><br>  Das L√∂sen dieser ODE in der Zeit ergibt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a0/01e/3d9/5a001e3d928ce08d05ff084353134d78.svg" alt="\ frac {\ partielles L} {\ partielles z (t_0)} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partielles f (z (t), t, \ theta)} {\ partielles z} dt \; (10)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e09/ba3/cc6/e09ba3cc6674830a8847894c39020ec0.svg" alt="\ frac {\ partielles L} {\ partielles \ Theta} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partielles f (z (t), t, \ theta)} {\ partielles \ Theta } dt \; (11)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/022/f84/715/022f84715f62b101017a6dcb591d5de3.svg" alt="\ frac {\ partielles L} {\ partielles t_0} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partielles f (z (t), t, \ theta)} {\ partielles t} dt \; (12)"></div><br>  Was ist mit <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0f9/b43/8a8/0f9b438a88408c879af759bcaf4d1d5e.svg" alt="\ frac {\ partielles L} {\ partielles t_1} = - a (t) \ frac {\ partielles f (z (t), t, \ theta)} {\ partielles t} \; (13)"></div><br>  <em>Gibt</em> dem <em>ODESolve</em> ODE- <em>Solver</em> Farbverl√§ufe in allen Eingabeparametern. <br><br>  Alle Gradienten (10), (11), (12), (13) k√∂nnen zusammen in einem <em>ODESolve-</em> Aufruf mit der Dynamik des konjugierten erweiterten Zustands (9) berechnet werden. <br><br><img src="https://habrastorage.org/webt/8k/pz/uk/8kpzukmizpmezmywov4b3zm29lc.png"><br>  <i>Illustration aus dem Originalartikel.</i> <br><br>  Der obige Algorithmus beschreibt die umgekehrte Ausbreitung des Gradienten der ODE-L√∂sung f√ºr aufeinanderfolgende Beobachtungen. <br><br>  Bei mehreren Beobachtungen auf einer Trajektorie wird alles auf die gleiche Weise berechnet, aber zum Zeitpunkt der Beobachtungen muss die Umkehrung des propagierten Gradienten mit Gradienten aus der aktuellen Beobachtung angepasst werden, wie in <em>Abbildung 1 gezeigt</em> . <br><br><h1>  Implementierung </h1><br>  Der folgende Code ist meine Implementierung von <strong>neuronalen ODEs</strong> .  Ich habe es nur getan, um besser zu verstehen, was passiert.  Es kommt jedoch dem sehr nahe, was im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository</a> der Autoren des Artikels implementiert ist.  Es enth√§lt den gesamten Code, den Sie zum Verst√§ndnis ben√∂tigen, an einer Stelle und ist etwas auskommentierter.  F√ºr reale Anwendungen und Experimente ist es immer noch besser, die Implementierung des Originalartikels durch die Autoren zu verwenden. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm_notebook <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns sns.color_palette(<span class="hljs-string"><span class="hljs-string">"bright"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tensor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable use_cuda = torch.cuda.is_available()</code> </pre> <br>  Zun√§chst m√ºssen Sie eine Methode zur Entwicklung von ODE-Systemen implementieren.  Der Einfachheit halber wird hier die Euler-Methode implementiert, obwohl jede explizite oder implizite Methode geeignet ist. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ode_solve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z0, t0, t1, f)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""     -   """</span></span> h_max = <span class="hljs-number"><span class="hljs-number">0.05</span></span> n_steps = math.ceil((abs(t1 - t0)/h_max).max().item()) h = (t1 - t0)/n_steps t = t0 z = z0 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i_step <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_steps): z = z + h * f(z, t) t = t + h <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z</code> </pre><br>  Es beschreibt auch die Oberklasse einer parametrisierten Dynamikfunktion mit einigen n√ºtzlichen Methoden. <br><br>  Erstens: Sie m√ºssen alle Parameter, von denen die Funktion abh√§ngt, in Form eines Vektors zur√ºckgeben. <br><br>  Zweitens: Es ist notwendig, die erweiterte Dynamik zu berechnen.  Diese Dynamik h√§ngt vom Gradienten der parametrisierten Funktion in Bezug auf Parameter und Eingabedaten ab.  Um den Gradienten nicht mit jeder Hand f√ºr jede neue Architektur registrieren zu m√ºssen, verwenden wir die Methode <strong>torch.autograd.grad</strong> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward_with_grad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, z, t, grad_outputs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""Compute f and a df/dz, a df/dp, a df/dt"""</span></span> batch_size = z.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] out = self.forward(z, t) a = grad_outputs adfdz, adfdt, *adfdp = torch.autograd.grad( (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a), allow_unused=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, retain_graph=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) <span class="hljs-comment"><span class="hljs-comment">#  grad       , #  expand   if adfdp is not None: adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0) adfdp = adfdp.expand(batch_size, -1) / batch_size if adfdt is not None: adfdt = adfdt.expand(batch_size, 1) / batch_size return out, adfdz, adfdt, adfdp def flatten_parameters(self): p_shapes = [] flat_parameters = [] for p in self.parameters(): p_shapes.append(p.size()) flat_parameters.append(p.flatten()) return torch.cat(flat_parameters)</span></span></code> </pre><br>  Der folgende Code beschreibt die Vorw√§rts- und R√ºckw√§rtsausbreitung f√ºr <em>neuronale ODEs</em> .  Es ist erforderlich, diesen Code vom Hauptmodul <strong><em>torch.nn.Module</em></strong> in Form der Funktion <strong><em>torch.autograd.Function zu trennen</em></strong> , da Sie in letzterem im Gegensatz zu einem Modul eine beliebige <strong><em>Backpropagation-</em></strong> Methode implementieren k√∂nnen.  Das ist also nur eine Kr√ºcke. <br><br>  Diese Funktion liegt dem gesamten Ansatz der <em>neuronalen ODE</em> zugrunde. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ODEAdjoint</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(torch.autograd.Function)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx, z0, t, flat_parameters, func)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> isinstance(func, ODEF) bs, *z_shape = z0.size() time_len = t.size(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): z = torch.zeros(time_len, bs, *z_shape).to(z0) z[<span class="hljs-number"><span class="hljs-number">0</span></span>] = z0 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i_t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(time_len - <span class="hljs-number"><span class="hljs-number">1</span></span>): z0 = ode_solve(z0, t[i_t], t[i_t+<span class="hljs-number"><span class="hljs-number">1</span></span>], func) z[i_t+<span class="hljs-number"><span class="hljs-number">1</span></span>] = z0 ctx.func = func ctx.save_for_backward(t, z.clone(), flat_parameters) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx, dLdz)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" dLdz shape: time_len, batch_size, *z_shape """</span></span> func = ctx.func t, z, flat_parameters = ctx.saved_tensors time_len, bs, *z_shape = z.size() n_dim = np.prod(z_shape) n_params = flat_parameters.size(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   , #       def augmented_dynamics(aug_z_i, t_i): """   -     t_i -   : bs, 1 aug_z_i -   : bs, n_dim*2 + n_params + 1 """ #     z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim] # Unflatten z and a z_i = z_i.view(bs, *z_shape) a = a.view(bs, *z_shape) with torch.set_grad_enabled(True): t_i = t_i.detach().requires_grad_(True) z_i = z_i.detach().requires_grad_(True) faug = func.forward_with_grad(z_i, t_i, grad_outputs=a) func_eval, adfdz, adfdt, adfdp = faug adfdz = adfdz if adfdz is not None else torch.zeros(bs, *z_shape) adfdp = adfdp if adfdp is not None else torch.zeros(bs, n_params) adfdt = adfdt if adfdt is not None else torch.zeros(bs, 1) adfdz = adfdz.to(z_i) adfdp = adfdp.to(z_i) adfdt = adfdt.to(z_i) # Flatten f and adfdz func_eval = func_eval.view(bs, n_dim) adfdz = adfdz.view(bs, n_dim) return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1) dLdz = dLdz.view(time_len, bs, n_dim) # flatten dLdz   with torch.no_grad(): ##      #    , #       adj_z = torch.zeros(bs, n_dim).to(dLdz) adj_p = torch.zeros(bs, n_params).to(dLdz) #    z  p,        adj_t = torch.zeros(time_len, bs, 1).to(dLdz) for i_t in range(time_len-1, 0, -1): z_i = z[i_t] t_i = t[i_t] f_i = func(z_i, t_i).view(bs, n_dim) #      dLdz_i = dLdz[i_t] dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] #     adj_z += dLdz_i adj_t[i_t] = adj_t[i_t] - dLdt_i #      aug_z = torch.cat(( z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z) adj_t[i_t]), dim=-1 ) #  ()      aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics) #       adj_z[:] = aug_ans[:, n_dim:2*n_dim] adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params] adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:] del aug_z, aug_ans ##         #    dLdz_0 = dLdz[0] dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] #  adj_z += dLdz_0 adj_t[0] = adj_t[0] - dLdt_0 return adj_z.view(bs, *z_shape), adj_t, adj_p, None</span></span></code> </pre><br>  <strong>Schlie√üen Sie</strong> diese Funktion der <strong>Einfachheit halber</strong> in <strong>nn.Module ein</strong> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NeuralODE</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, func)</span></span></span><span class="hljs-function">:</span></span> super(NeuralODE, self).__init__() <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> isinstance(func, ODEF) self.func = func <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, z0, t=Tensor</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">([</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">0.</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">1.</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">])</span></span></span></span><span class="hljs-function"><span class="hljs-params">, return_whole_sequence=False)</span></span></span><span class="hljs-function">:</span></span> t = t.to(z0) z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> return_whole_sequence: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z[<span class="hljs-number"><span class="hljs-number">-1</span></span>]</code> </pre><br><br><h1>  Anwendung </h1><br><h2>  Wiederherstellung der realen Dynamikfunktion (Ann√§herungsverifizierung) </h2><br>  Lassen Sie uns nun als <strong>Basistest</strong> √ºberpr√ºfen, ob die <strong>neuronale ODE</strong> mithilfe von Beobachtungsdaten die wahre Funktion der Dynamik wiederherstellen kann. <br><br>  Dazu bestimmen wir zun√§chst die Dynamikfunktion der ODE, entwickeln die darauf basierende Trajektorie weiter und versuchen dann, sie aus der zuf√§llig parametrisierten Dynamikfunktion wiederherzustellen. <br><br>  Lassen Sie uns zun√§chst den einfachsten Fall einer linearen ODE √ºberpr√ºfen.  Die Funktion der Dynamik ist einfach die Wirkung einer Matrix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eb4/fdc/86a/eb4fdc86a1c5f56ab1b6e37e575e7c72.svg" alt="\ frac {dz} {dt} = \ begin {bmatrix} -0.1 &amp;; -1,0 \\ 1,0 &amp;; -0.1 \ end {bmatrix} z"></div><br>  Die trainierte Funktion wird durch eine Zufallsmatrix parametrisiert. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nj/q6/es/njq6eswuevh4rhx-8nhzyz-cq_k.gif"></div><br>  Weiter etwas ausgefeiltere Dynamik (ohne GIF, weil der Lernprozess nicht so sch√∂n ist :)) <br>  Die Lernfunktion ist hier ein vollst√§ndig verbundenes Netzwerk mit einer verborgenen Schicht. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tr/4n/eb/tr4nebjdrs4pt4v5vlzleai8exe.png"></div><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LinearODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, W)</span></span></span><span class="hljs-function">:</span></span> super(LinearODEF, self).__init__() self.lin = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.lin.weight = nn.Parameter(W) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.lin(x)</code> </pre><br>  Die Dynamikfunktion ist nur eine Matrix <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SpiralFunctionExample</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LinearODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> matrix = Tensor([[<span class="hljs-number"><span class="hljs-number">-0.1</span></span>, <span class="hljs-number"><span class="hljs-number">-1.</span></span>], [<span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">-0.1</span></span>]]) super(SpiralFunctionExample, self).__init__(matrix)</code> </pre><br>  Zuf√§llig parametrisierte Matrix <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomLinearODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LinearODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(RandomLinearODEF, self).__init__(torch.randn(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)/<span class="hljs-number"><span class="hljs-number">2.</span></span>)</code> </pre><br>  Dynamik f√ºr anspruchsvollere Flugbahnen <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, A, B, x0)</span></span></span><span class="hljs-function">:</span></span> super(TestODEF, self).__init__() self.A = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.A.weight = nn.Parameter(A) self.B = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.B.weight = nn.Parameter(B) self.x0 = nn.Parameter(x0) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> xTx0 = torch.sum(x*self.x0, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dxdt</code> </pre><br>  Lerndynamik in Form eines vollst√§ndig vernetzten Netzwerks <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NNODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, in_dim, hid_dim, time_invariant=False)</span></span></span><span class="hljs-function">:</span></span> super(NNODEF, self).__init__() self.time_invariant = time_invariant <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> time_invariant: self.lin1 = nn.Linear(in_dim, hid_dim) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.lin1 = nn.Linear(in_dim+<span class="hljs-number"><span class="hljs-number">1</span></span>, hid_dim) self.lin2 = nn.Linear(hid_dim, hid_dim) self.lin3 = nn.Linear(hid_dim, in_dim) self.elu = nn.ELU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> self.time_invariant: x = torch.cat((x, t), dim=<span class="hljs-number"><span class="hljs-number">-1</span></span>) h = self.elu(self.lin1(x)) h = self.elu(self.lin2(h)) out = self.lin3(h) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_np</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x.detach().cpu().numpy() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_trajectories</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(obs=None, times=None, trajs=None, save=None, figsize=</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">16</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">8</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=figsize) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> obs <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> times <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: times = [<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>] * len(obs) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> o, t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(obs, times): o, t = to_np(o), to_np(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(o.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]): plt.scatter(o[:, b_i, <span class="hljs-number"><span class="hljs-number">0</span></span>], o[:, b_i, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=t[:, b_i, <span class="hljs-number"><span class="hljs-number">0</span></span>], cmap=cm.plasma) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> trajs <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> z <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trajs: z = to_np(z) plt.plot(z[:, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], z[:, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], lw=<span class="hljs-number"><span class="hljs-number">1.5</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> save <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: plt.savefig(save) plt.show() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">conduct_experiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ode_true, ode_trained, n_steps, name, plot_freq=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Create data z0 = Variable(torch.Tensor([[0.6, 0.3]])) t_max = 6.29*5 n_points = 200 index_np = np.arange(0, n_points, 1, dtype=np.int) index_np = np.hstack([index_np[:, None]]) times_np = np.linspace(0, t_max, num=n_points) times_np = np.hstack([times_np[:, None]]) times = torch.from_numpy(times_np[:, :, None]).to(z0) obs = ode_true(z0, times, return_whole_sequence=True).detach() obs = obs + torch.randn_like(obs) * 0.01 # Get trajectory of random timespan min_delta_time = 1.0 max_delta_time = 5.0 max_points_num = 32 def create_batch(): t0 = np.random.uniform(0, t_max - max_delta_time) t1 = t0 + np.random.uniform(min_delta_time, max_delta_time) idx = sorted(np.random.permutation( index_np[(times_np &gt; t0) &amp; (times_np &lt; t1)] )[:max_points_num]) obs_ = obs[idx] ts_ = times[idx] return obs_, ts_ # Train Neural ODE optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01) for i in range(n_steps): obs_, ts_ = create_batch() z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True) loss = F.mse_loss(z_, obs_.detach()) optimizer.zero_grad() loss.backward(retain_graph=True) optimizer.step() if i % plot_freq == 0: z_p = ode_trained(z0, times, return_whole_sequence=True) plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=f"assets/imgs/{name}/{i}.png") clear_output(wait=True) ode_true = NeuralODE(SpiralFunctionExample()) ode_trained = NeuralODE(RandomLinearODEF()) conduct_experiment(ode_true, ode_trained, 500, "linear") func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]])) ode_true = NeuralODE(func) func = NNODEF(2, 16, time_invariant=True) ode_trained = NeuralODE(func) conduct_experiment(ode_true, ode_trained, 3000, "comp", plot_freq=30)</span></span></code> </pre><br></div></div><br>  Wie Sie sehen k√∂nnen, leistet <em>Neural ODE</em> einen ziemlich guten Beitrag zur Wiederherstellung der Dynamik.  Das hei√üt, das Konzept als Ganzes funktioniert. <br>  √úberpr√ºfen Sie nun ein etwas komplizierteres Problem (MNIST, haha). <br><br><h2>  Von ResNets inspirierte neuronale ODE </h2><br>  In ResNet'ax √§ndert sich der versteckte Zustand gem√§√ü der Formel <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/30a/6b2/d3c/30a6b2d3c27bb27afaeb4736072e4446.svg" alt="h_ {t + 1} = h_ {t} + f (h_ {t}, \ theta_ {t})"></div><br>  wo <img src="https://habrastorage.org/getpro/habr/post_images/7ca/860/97d/7ca86097dc5dc0d9cbb9b454168de928.svg" alt="t \ in \ {0 ... T \}">  Ist die Blocknummer und <img src="https://habrastorage.org/getpro/habr/post_images/eb2/5f7/ddf/eb25f7ddf77e46681e256b28fecaef35.svg" alt="f">  Dies ist eine Funktion, die von den Ebenen innerhalb des Blocks gelernt wird. <br><br>  Wenn wir im Grenzfall eine unendliche Anzahl von Bl√∂cken mit immer kleineren Schritten nehmen, erhalten wir die kontinuierliche Dynamik der verborgenen Schicht in Form einer ODE, genau wie oben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/629/67a/d41/62967ad41e5f50cc65e1c1e55a2860d3.svg" alt="\ frac {dh (t)} {dt} = f (h (t), t, \ theta)"></div><br>  Ausgehend von der Eingabeebene <img src="https://habrastorage.org/getpro/habr/post_images/c33/9b9/bc0/c339b9bc0244968c806390c2e66fdb62.svg" alt="h (0)">  Wir k√∂nnen die Ausgabeebene definieren <img src="https://habrastorage.org/getpro/habr/post_images/b7e/dec/ef3/b7edecef308ce0df4f3d1c8aaa753617.svg" alt="h (T)">  als L√∂sung f√ºr diese ODE zum Zeitpunkt T. <br><br>  Jetzt k√∂nnen wir z√§hlen <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ Theta">  als verteilte ( <em>gemeinsam genutzte</em> ) Parameter zwischen allen infinitesimalen Bl√∂cken. <br><br><h3>  Validierung der neuronalen ODE-Architektur auf MNIST </h3><br>  In diesem Teil werden wir die F√§higkeit von <em>Neural ODE</em> testen, als Komponenten in bekannteren Architekturen verwendet zu werden. <br><br>  Insbesondere werden wir die Restbl√∂cke im MNIST-Klassifikator durch <em>Neural ODE</em> ersetzen. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7c/gw/ia/7cgwiawqx6-_kxk82qpenqplowi.png" width="400"></div><br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">norm</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dim)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.BatchNorm2d(dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">conv3x3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_feats, out_feats, stride=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.Conv2d(in_feats, out_feats, kernel_size=<span class="hljs-number"><span class="hljs-number">3</span></span>, stride=stride, padding=<span class="hljs-number"><span class="hljs-number">1</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_time</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_tensor, t)</span></span></span><span class="hljs-function">:</span></span> bs, c, w, h = in_tensor.shape <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.cat((in_tensor, t.expand(bs, <span class="hljs-number"><span class="hljs-number">1</span></span>, w, h)), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConvODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, dim)</span></span></span><span class="hljs-function">:</span></span> super(ConvODEF, self).__init__() self.conv1 = conv3x3(dim + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim) self.norm1 = norm(dim) self.conv2 = conv3x3(dim + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim) self.norm2 = norm(dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> xt = add_time(x, t) h = self.norm1(torch.relu(self.conv1(xt))) ht = add_time(h, t) dxdt = self.norm2(torch.relu(self.conv2(ht))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dxdt <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ContinuousNeuralMNISTClassifier</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, ode)</span></span></span><span class="hljs-function">:</span></span> super(ContinuousNeuralMNISTClassifier, self).__init__() self.downsampling = nn.Sequential( nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), norm(<span class="hljs-number"><span class="hljs-number">64</span></span>), nn.ReLU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), nn.Conv2d(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), norm(<span class="hljs-number"><span class="hljs-number">64</span></span>), nn.ReLU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), nn.Conv2d(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), ) self.feature = ode self.norm = norm(<span class="hljs-number"><span class="hljs-number">64</span></span>) self.avg_pool = nn.AdaptiveAvgPool2d((<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) self.fc = nn.Linear(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = self.downsampling(x) x = self.feature(x) x = self.norm(x) x = self.avg_pool(x) shape = torch.prod(torch.tensor(x.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:])).item() x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, shape) out = self.fc(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out func = ConvODEF(<span class="hljs-number"><span class="hljs-number">64</span></span>) ode = NeuralODE(func) model = ContinuousNeuralMNISTClassifier(ode) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: model = model.cuda() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torchvision img_std = <span class="hljs-number"><span class="hljs-number">0.3081</span></span> img_mean = <span class="hljs-number"><span class="hljs-number">0.1307</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span> train_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(<span class="hljs-string"><span class="hljs-string">"data/mnist"</span></span>, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(<span class="hljs-string"><span class="hljs-string">"data/mnist"</span></span>, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) optimizer = torch.optim.Adam(model.parameters()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(epoch)</span></span></span><span class="hljs-function">:</span></span> num_items = <span class="hljs-number"><span class="hljs-number">0</span></span> train_losses = [] model.train() criterion = nn.CrossEntropyLoss() print(<span class="hljs-string"><span class="hljs-string">f"Training Epoch </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{epoch}</span></span></span><span class="hljs-string">..."</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(enumerate(train_loader), total=len(train_loader)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: data = data.cuda() target = target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() train_losses += [loss.item()] num_items += data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] print(<span class="hljs-string"><span class="hljs-string">'Train loss: {:.5f}'</span></span>.format(np.mean(train_losses))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_losses <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">test</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> accuracy = <span class="hljs-number"><span class="hljs-number">0.0</span></span> num_items = <span class="hljs-number"><span class="hljs-number">0</span></span> model.eval() criterion = nn.CrossEntropyLoss() print(<span class="hljs-string"><span class="hljs-string">f"Testing..."</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(enumerate(test_loader), total=len(test_loader)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: data = data.cuda() target = target.cuda() output = model(data) accuracy += torch.sum(torch.argmax(output, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) == target).item() num_items += data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] accuracy = accuracy * <span class="hljs-number"><span class="hljs-number">100</span></span> / num_items print(<span class="hljs-string"><span class="hljs-string">"Test Accuracy: {:.3f}%"</span></span>.format(accuracy)) n_epochs = <span class="hljs-number"><span class="hljs-number">5</span></span> test() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_epochs + <span class="hljs-number"><span class="hljs-number">1</span></span>): train_losses += train(epoch) test() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) history = pd.DataFrame({<span class="hljs-string"><span class="hljs-string">"loss"</span></span>: train_losses}) history[<span class="hljs-string"><span class="hljs-string">"cum_data"</span></span>] = history.index * batch_size history[<span class="hljs-string"><span class="hljs-string">"smooth_loss"</span></span>] = history.loss.ewm(halflife=<span class="hljs-number"><span class="hljs-number">10</span></span>).mean() history.plot(x=<span class="hljs-string"><span class="hljs-string">"cum_data"</span></span>, y=<span class="hljs-string"><span class="hljs-string">"smooth_loss"</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), title=<span class="hljs-string"><span class="hljs-string">"train error"</span></span>)</code> </pre><br></div></div><br><pre> <code class="plaintext hljs">Testing... 100% 79/79 [00:01&lt;00:00, 45.69it/s] Test Accuracy: 9.740% Training Epoch 1... 100% 1875/1875 [01:15&lt;00:00, 24.69it/s] Train loss: 0.20137 Testing... 100% 79/79 [00:01&lt;00:00, 46.64it/s] Test Accuracy: 98.680% Training Epoch 2... 100% 1875/1875 [01:17&lt;00:00, 24.32it/s] Train loss: 0.05059 Testing... 100% 79/79 [00:01&lt;00:00, 46.11it/s] Test Accuracy: 97.760% Training Epoch 3... 100% 1875/1875 [01:16&lt;00:00, 24.63it/s] Train loss: 0.03808 Testing... 100% 79/79 [00:01&lt;00:00, 45.65it/s] Test Accuracy: 99.000% Training Epoch 4... 100% 1875/1875 [01:17&lt;00:00, 24.28it/s] Train loss: 0.02894 Testing... 100% 79/79 [00:01&lt;00:00, 45.42it/s] Test Accuracy: 99.130% Training Epoch 5... 100% 1875/1875 [01:16&lt;00:00, 24.67it/s] Train loss: 0.02424 Testing... 100% 79/79 [00:01&lt;00:00, 45.89it/s] Test Accuracy: 99.170%</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zx/zv/5x/zxzv5x4ktqyy9lt19of-d2i4few.png"></div><br>  Nach einem sehr rauen Training in nur 5 Epochen und 6 Minuten Training hat das Modell bereits einen Testfehler von weniger als 1% erreicht.  Wir k√∂nnen sagen, dass sich <em>neuronale ODEs</em> <em>als</em> Komponente gut in traditionellere Netzwerke integrieren lassen. <br><br>  In ihrem Artikel vergleichen die Autoren diesen Klassifikator (ODE-Net) auch mit einem regul√§ren, vollst√§ndig verbundenen Netzwerk, mit ResNet mit einer √§hnlichen Architektur und mit genau derselben Architektur, in der sich der Gradient direkt durch Operationen in <em>ODESolve ausbreitet</em> (ohne die konjugierte Gradientenmethode) ( RK-Net). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vn/41/--/vn41--frzbdkftca4ehe7hh-kea.png"></div><br>  <i>Illustration aus dem Originalartikel.</i> <br><br>  Demnach weist ein vollst√§ndig verbundenes 1-Schicht-Netzwerk mit ungef√§hr der gleichen Anzahl von Parametern wie <em>Neural ODE</em> einen viel h√∂heren Fehler im Test auf, ResNet mit dem gleichen Fehler hat viel mehr Parameter und RK-Net ohne die konjugierte Gradientenmethode weist einen etwas h√∂heren Fehler auf und mit linear steigendem Speicherverbrauch (je kleiner der zul√§ssige Fehler ist, desto mehr Schritte muss <em>ODESolve ausf√ºhren</em> , wodurch sich der Speicherverbrauch linear mit der Anzahl der Schritte erh√∂ht). <br><br>  Die Autoren verwenden bei ihrer Implementierung die implizite Runge-Kutta-Methode mit adaptiver Schrittgr√∂√üe, im Gegensatz zur einfacheren Euler-Methode hier.  Sie untersuchen auch einige Eigenschaften der neuen Architektur. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/li/fq/u7lifqdsf72otgcyfigm6-fiyww.png"></div><br>  <i>ODE-Net-Funktion (NFE Forward - die Anzahl der Funktionsberechnungen in einem direkten Durchgang)</i> <i><br></i>  <i>Illustration aus dem Originalartikel.</i> <br><br><ul><li>  (a) Durch √Ñndern des akzeptablen Niveaus des numerischen Fehlers wird die Anzahl der Schritte bei der direkten Verteilung ge√§ndert. <br></li><li>  (b) Die f√ºr die direkte Verteilung aufgewendete Zeit ist proportional zur Anzahl der Berechnungen der Funktion. <br></li><li>  (c) Die Anzahl der Berechnungen der Funktion f√ºr die R√ºckausbreitung betr√§gt ungef√§hr die H√§lfte der direkten Ausbreitung, was darauf hinweist, dass das konjugierte Gradientenverfahren rechnerisch effizienter sein kann als die Ausbreitung des Gradienten direkt durch <em>ODESolve</em> . <br></li><li>  (d) Da ODE-Net immer mehr trainiert wird, erfordert es immer mehr Berechnungen einer Funktion (ein immer kleinerer Schritt), die sich m√∂glicherweise an die zunehmende Komplexit√§t des Modells anpassen. <br></li></ul><br><br><h2>  Versteckte generative Funktion f√ºr die Zeitreihenmodellierung </h2><br>  Neuronale ODE eignet sich zur Verarbeitung kontinuierlicher serieller Daten, selbst wenn der Pfad in einem unbekannten verborgenen Raum liegt. <br><br>  In diesem Abschnitt werden wir experimentieren <em>und</em> die Erzeugung kontinuierlicher Sequenzen mit <em>neuronaler ODE</em> √§ndern und einen Blick auf den erlernten verborgenen Raum werfen. <br><br>  Die Autoren vergleichen dies auch mit √§hnlichen Sequenzen, die von wiederkehrenden Netzwerken erzeugt werden. <br><br>  Das Experiment hier unterscheidet sich geringf√ºgig von dem entsprechenden Beispiel im Autoren-Repository. Hier gibt es eine vielf√§ltigere Reihe von Trajektorien. <br><br><h3>  Daten </h3><br>  Die Trainingsdaten bestehen aus zuf√§lligen Spiralen, von denen die H√§lfte im Uhrzeigersinn und die zweite gegen den Uhrzeigersinn sind.  Ferner werden zuf√§llige Teilsequenzen aus diesen Spiralen abgetastet, die vom Codierungswiederholungsmodell in die entgegengesetzte Richtung verarbeitet werden, was zu einem verborgenen Startzustand f√ºhrt, der sich dann entwickelt und eine Flugbahn im verborgenen Raum erzeugt.  Dieser latente Pfad wird dann auf den Datenraum abgebildet und mit der abgetasteten Teilsequenz verglichen.  Auf diese Weise lernt das Modell, Trajektorien √§hnlich einem Datensatz zu generieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4z/lg/or4zlgwkqjm-kzhvlmqtzmeqnl4.png"></div><br>  <i>Beispiele f√ºr Datensatzspiralen</i> <br><br><h3>  VAE als generatives Modell </h3><br>  Generatives Modell durch ein Stichprobenverfahren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fde/a91/b7b/fdea91b7b77af8c87a50e6c9e361f13b.svg" alt="z_ {t_0} \ sim \ mathcal {N} (0, I)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb0/9b2/ad7/fb09b2ad71d820266be632be7c5b5f97.svg" alt="z_ {t_1}, z_ {t_2}, ..., z_ {t_M} = \ text {ODESolve} (z_ {t_0}, f, \ theta_f, t_0, ..., t_M)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/477/5a1/439/4775a14397c7842e67c0756d585c3d5b.svg" alt="x_ {t_i} \ sim p (x \ mid z_ {t_i}; \ theta_x)"></div><br>  Welches mit dem Variations-Auto-Encoder-Ansatz trainiert werden kann. <br><ol><li>  Gehen Sie durch einen wiederkehrenden Encoder durch eine Zeitsequenz in der Zeit zur√ºck, um die Parameter zu erhalten <img src="https://habrastorage.org/getpro/habr/post_images/26b/84a/1dd/26b84a1dd2d618b4dd85d805e95b5db3.svg" alt="\ mu_ {z_ {t_0}}">  , <img src="https://habrastorage.org/getpro/habr/post_images/a1f/9e2/8ac/a1f9e28acee2636547023da51c1af36e.svg" alt="\ sigma_ {z_ {t_0}}">  Variations-Posterior-Verteilung und dann Probe daraus: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/816/997/3be/8169973bece43135c717f8abd5088e4c.svg" alt="z_ {t_0} \ sim q \ left (z_ {t_0} \ mid x_ {t_0}, ..., x_ {t_M}; t_0, ..., t_M; \ theta_q \ right) = \ mathcal {N} \ links (z_ {t_0} \ mid \ mu_ {z_ {t_0}} \ sigma_ {z_ {t_0}} \ rechts)"></div><br><ol><li>  Holen Sie sich versteckte Flugbahn: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/810/833/416810833bb304242b3ec7bccfeb91ad.svg" alt="z_ {t_1}, z_ {t_2}, ..., z_ {t_N} = \ text {ODESolve} (z_ {t_0}, f, \ theta_f, t_0, ..., t_N), \ text {where} \ frac {dz} {dt} = f (z, t; \ theta_f)"></div><br><ol><li>  Ordnen Sie einen versteckten Pfad einem Pfad in den Daten mithilfe eines anderen neuronalen Netzwerks zu: <img src="https://habrastorage.org/getpro/habr/post_images/fd1/2b0/73d/fd12b073dd1cede0a98b312c0e3240d1.svg" alt="\ hat {x_ {t_i}} (z_ {t_i}, t_i; \ theta_x)"><br></li><li>  Maximieren Sie die Bewertung der unteren G√ºltigkeitsgrenze (ELBO) f√ºr den abgetasteten Pfad: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9cf/838/1e3/9cf8381e35978cf70219a1f3bea211b6.svg" alt="\ text {ELBO} \ ungef√§hr N \ Big (\ sum_ {i = 0} ^ {M} \ log p (x_ {t_i} \ mid z_ {t_i} (z_ {t_0}; \ theta_f); \ theta_x) + KL \ left (q (z_ {t_0} \ mid x_ {t_0}, ..., x_ {t_M}; t_0, ..., t_M; \ theta_q) \ parallel \ mathcal {N} (0, I) \ rechts) \ Big)"></div><br>  Und im Fall einer Gau√üschen posterioren Verteilung <img src="https://habrastorage.org/getpro/habr/post_images/9a9/404/aa0/9a9404aa0267a6c257815abc794e95c3.svg" alt="p (x \ mid z_ {t_i}; \ theta_x)">  und bekannter Ger√§uschpegel <img src="https://habrastorage.org/getpro/habr/post_images/87a/7c9/aba/87a7c9abaa12e58e75ef64aa8312050e.svg" alt="\ sigma_x">  :: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7b0/a0d/c43/7b0a0dc43667f287d584b6865afa1cfb.svg" alt="\ text {ELBO} \ ca. -N \ Big (\ sum_ {i = 1} ^ {M} \ frac {(x_i - \ hat {x_i}) ^ 2} {\ sigma_x ^ 2} - \ log \ sigma_ { z_ {t_0}} ^ 2 + \ mu_ {z_ {t_0}} ^ 2 + \ sigma_ {z_ {t_0}} ^ 2 \ Big) + C."></div><br>  Das Berechnungsdiagramm eines versteckten ODE-Modells kann wie folgt dargestellt werden <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/57/ui/zl57uisgnvjy7-y94fkker9m1eq.png"></div><br>  <i>Illustration aus dem Originalartikel.</i> <br><br>  Dieses Modell kann dann getestet werden, wie es den Pfad nur unter Verwendung der anf√§nglichen Beobachtungen interpoliert. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><h3>  Modelle definieren </h3><br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RNNEncoder</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_dim, hidden_dim, latent_dim)</span></span></span><span class="hljs-function">:</span></span> super(RNNEncoder, self).__init__() self.input_dim = input_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim self.rnn = nn.GRU(input_dim+<span class="hljs-number"><span class="hljs-number">1</span></span>, hidden_dim) self.hid2lat = nn.Linear(hidden_dim, <span class="hljs-number"><span class="hljs-number">2</span></span>*latent_dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Concatenate time to input t = t.clone() t[1:] = t[:-1] - t[1:] t[0] = 0. xt = torch.cat((x, t), dim=-1) _, h0 = self.rnn(xt.flip((0,))) # Reversed # Compute latent dimension z0 = self.hid2lat(h0[0]) z0_mean = z0[:, :self.latent_dim] z0_log_var = z0[:, self.latent_dim:] return z0_mean, z0_log_var class NeuralODEDecoder(nn.Module): def __init__(self, output_dim, hidden_dim, latent_dim): super(NeuralODEDecoder, self).__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim func = NNODEF(latent_dim, hidden_dim, time_invariant=True) self.ode = NeuralODE(func) self.l2h = nn.Linear(latent_dim, hidden_dim) self.h2o = nn.Linear(hidden_dim, output_dim) def forward(self, z0, t): zs = self.ode(z0, t, return_whole_sequence=True) hs = self.l2h(zs) xs = self.h2o(hs) return xs class ODEVAE(nn.Module): def __init__(self, output_dim, hidden_dim, latent_dim): super(ODEVAE, self).__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim self.encoder = RNNEncoder(output_dim, hidden_dim, latent_dim) self.decoder = NeuralODEDecoder(output_dim, hidden_dim, latent_dim) def forward(self, x, t, MAP=False): z_mean, z_log_var = self.encoder(x, t) if MAP: z = z_mean else: z = z_mean + torch.randn_like(z_mean) * torch.exp(0.5 * z_log_var) x_p = self.decoder(z, t) return x_p, z, z_mean, z_log_var def generate_with_seed(self, seed_x, t): seed_t_len = seed_x.shape[0] z_mean, z_log_var = self.encoder(seed_x, t[:seed_t_len]) x_p = self.decoder(z_mean, t) return x_p</span></span></code> </pre><br><br><h3>  Datensatzgenerierung </h3><br><br><pre> <code class="python hljs">t_max = <span class="hljs-number"><span class="hljs-number">6.29</span></span>*<span class="hljs-number"><span class="hljs-number">5</span></span> n_points = <span class="hljs-number"><span class="hljs-number">200</span></span> noise_std = <span class="hljs-number"><span class="hljs-number">0.02</span></span> num_spirals = <span class="hljs-number"><span class="hljs-number">1000</span></span> index_np = np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n_points, <span class="hljs-number"><span class="hljs-number">1</span></span>, dtype=np.int) index_np = np.hstack([index_np[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]]) times_np = np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, t_max, num=n_points) times_np = np.hstack([times_np[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]] * num_spirals) times = torch.from_numpy(times_np[:, :, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]).to(torch.float32) <span class="hljs-comment"><span class="hljs-comment"># Generate random spirals parameters normal01 = torch.distributions.Normal(0, 1.0) x0 = Variable(normal01.sample((num_spirals, 2))) * 2.0 W11 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05 W22 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05 W21 = -1.0 * normal01.sample((num_spirals,)).abs() W12 = 1.0 * normal01.sample((num_spirals,)).abs() xs_list = [] for i in range(num_spirals): if i % 2 == 1: # Make it counter-clockwise W21, W12 = W12, W21 func = LinearODEF(Tensor([[W11[i], W12[i]], [W21[i], W22[i]]])) ode = NeuralODE(func) xs = ode(x0[i:i+1], times[:, i:i+1], return_whole_sequence=True) xs_list.append(xs) orig_trajs = torch.cat(xs_list, dim=1).detach() samp_trajs = orig_trajs + torch.randn_like(orig_trajs) * noise_std samp_ts = times fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 9)) axes = axes.flatten() for i, ax in enumerate(axes): ax.scatter(samp_trajs[:, i, 0], samp_trajs[:, i, 1], c=samp_ts[:, i, 0], cmap=cm.plasma) plt.show() import numpy.random as npr def gen_batch(batch_size, n_sample=100): n_batches = samp_trajs.shape[1] // batch_size time_len = samp_trajs.shape[0] n_sample = min(n_sample, time_len) for i in range(n_batches): if n_sample &gt; 0: probs = [1. / (time_len - n_sample)] * (time_len - n_sample) t0_idx = npr.multinomial(1, probs) t0_idx = np.argmax(t0_idx) tM_idx = t0_idx + n_sample else: t0_idx = 0 tM_idx = time_len frm, to = batch_size*i, batch_size*(i+1) yield samp_trajs[t0_idx:tM_idx, frm:to], samp_ts[t0_idx:tM_idx, frm:to]</span></span></code> </pre><br><br><h3>  Schulung </h3><br><br><pre> <code class="python hljs">vae = ODEVAE(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) vae = vae.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: vae = vae.cuda() optim = torch.optim.Adam(vae.parameters(), betas=(<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.999</span></span>), lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>) preload = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">20000</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">100</span></span> plot_traj_idx = <span class="hljs-number"><span class="hljs-number">1</span></span> plot_traj = orig_trajs[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] plot_obs = samp_trajs[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] plot_ts = samp_ts[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: plot_traj = plot_traj.cuda() plot_obs = plot_obs.cuda() plot_ts = plot_ts.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> preload: vae.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">"models/vae_spirals.sd"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): losses = [] train_iter = gen_batch(batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train_iter: optim.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: x, t = x.cuda(), t.cuda() max_len = np.random.choice([<span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>]) permutation = np.random.permutation(t.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) np.random.shuffle(permutation) permutation = np.sort(permutation[:max_len]) x, t = x[permutation], t[permutation] x_p, z, z_mean, z_log_var = vae(x, t) z_var = torch.exp(z_log_var) kl_loss = <span class="hljs-number"><span class="hljs-number">-0.5</span></span> * torch.sum(<span class="hljs-number"><span class="hljs-number">1</span></span> + z_log_var - z_mean**<span class="hljs-number"><span class="hljs-number">2</span></span> - z_var, <span class="hljs-number"><span class="hljs-number">-1</span></span>) loss = <span class="hljs-number"><span class="hljs-number">0.5</span></span> * ((x-x_p)**<span class="hljs-number"><span class="hljs-number">2</span></span>).sum(<span class="hljs-number"><span class="hljs-number">-1</span></span>).sum(<span class="hljs-number"><span class="hljs-number">0</span></span>) / noise_std**<span class="hljs-number"><span class="hljs-number">2</span></span> + kl_loss loss = torch.mean(loss) loss /= max_len loss.backward() optim.step() losses.append(loss.item()) print(<span class="hljs-string"><span class="hljs-string">f"Epoch </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{epoch_idx}</span></span></span><span class="hljs-string">"</span></span>) frm, to, to_seed = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span> seed_trajs = samp_trajs[frm:to_seed] ts = samp_ts[frm:to] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: seed_trajs = seed_trajs.cuda() ts = ts.cuda() samp_trajs_p = to_np(vae.generate_with_seed(seed_trajs, ts)) fig, axes = plt.subplots(nrows=<span class="hljs-number"><span class="hljs-number">3</span></span>, ncols=<span class="hljs-number"><span class="hljs-number">3</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(axes): ax.scatter(to_np(seed_trajs[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(seed_trajs[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>]), c=to_np(ts[frm:to_seed, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), cmap=cm.plasma) ax.plot(to_np(orig_trajs[frm:to, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(orig_trajs[frm:to, i, <span class="hljs-number"><span class="hljs-number">1</span></span>])) ax.plot(samp_trajs_p[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>], samp_trajs_p[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>]) plt.show() print(np.mean(losses), np.median(losses)) clear_output(wait=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) spiral_0_idx = <span class="hljs-number"><span class="hljs-number">3</span></span> spiral_1_idx = <span class="hljs-number"><span class="hljs-number">6</span></span> homotopy_p = Tensor(np.linspace(<span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]) vae = vae <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: homotopy_p = homotopy_p.cuda() vae = vae.cuda() spiral_0 = orig_trajs[:, spiral_0_idx:spiral_0_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] spiral_1 = orig_trajs[:, spiral_1_idx:spiral_1_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] ts_0 = samp_ts[:, spiral_0_idx:spiral_0_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] ts_1 = samp_ts[:, spiral_1_idx:spiral_1_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: spiral_0, ts_0 = spiral_0.cuda(), ts_0.cuda() spiral_1, ts_1 = spiral_1.cuda(), ts_1.cuda() z_cw, _ = vae.encoder(spiral_0, ts_0) z_cc, _ = vae.encoder(spiral_1, ts_1) homotopy_z = z_cw * (<span class="hljs-number"><span class="hljs-number">1</span></span> - homotopy_p) + z_cc * homotopy_p t = torch.from_numpy(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>*np.pi, <span class="hljs-number"><span class="hljs-number">200</span></span>)) t = t[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].expand(<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)[:, :, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].cuda() t = t.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> t hom_gen_trajs = vae.decoder(homotopy_z, t) fig, axes = plt.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, ncols=<span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(axes): ax.plot(to_np(hom_gen_trajs[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(hom_gen_trajs[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>])) plt.show() torch.save(vae.state_dict(), <span class="hljs-string"><span class="hljs-string">"models/vae_spirals.sd"</span></span>)</code> </pre><br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Das passiert nach einer Nacht des Trainings </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fv/pn/pi/fvpnpimixf3sq0cezhepgzh2txy.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Punkte sind verrauschte Beobachtungen der urspr√ºnglichen Flugbahn (blau), </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gelb sind rekonstruierte und interpolierte Flugbahnen, wobei Punkte als Eingaben verwendet werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Farbe des Punktes zeigt die Zeit. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rekonstruktionen einiger Beispiele sehen nicht besonders gut aus. </font><font style="vertical-align: inherit;">Vielleicht ist das Modell nicht komplex genug oder nicht lange genug untersucht. </font><font style="vertical-align: inherit;">In jedem Fall sieht die Rekonstruktion sehr vern√ºnftig aus. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nun wollen wir sehen, was passiert, wenn wir eine versteckte Variable im Uhrzeigersinn in einen Anti-Uhr-Pfad interpolieren.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/g_/7e/atg_7ecofins-uohnv99qccfxfq.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Autoren vergleichen auch Rekonstruktionen und Pfadinterpolationen zwischen </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neuronaler ODE</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und einem einfachen rekursiven Netzwerk.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kj/ak/-e/kjak-evgr-7mrrtpgimf0gfmfxq.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Illustration aus dem Originalartikel.</font></font></i> <br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kontinuierliche Normalisierung von Streams </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Originalartikel bringt auch viel zum Thema Normalisieren von Streams. Normalisierende Fl√ºsse werden verwendet, wenn Sie eine Stichprobe aus einer komplexen Verteilung erstellen m√ºssen, die durch eine √Ñnderung von Variablen aus einer einfachen Verteilung (z. B. Gau√ü) entsteht, und dennoch die Wahrscheinlichkeitsdichte am Punkt jeder Stichprobe kennen. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Autoren zeigen, dass die Verwendung der kontinuierlichen Variablensubstitution viel rechnerisch effizienter und interpretierbarer ist als fr√ºhere Methoden. </font></font><br><br> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Normalisieren von Fl√ºssen ist</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in Modellen wie </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Variation AutoCoders</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bayesian Neural Networks</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und anderen aus dem Bayesian-Ansatz </font><font style="vertical-align: inherit;">sehr n√ºtzlich </font><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieses Thema ist jedoch √ºber den Rahmen </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieses</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel, und diejenigen, die interessiert sind, sollten den urspr√ºnglichen wissenschaftlichen Artikel lesen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">F√ºr Saatgut:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nd/e_/wn/nde_wn590scz9dff_0hzxo1-tgg.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualisierung der Transformation von Rauschen (einfache Verteilung) zu Daten (komplexe Verteilung) f√ºr zwei Datens√§tze; </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die X-Achse zeigt die Transformation von Dichte und Proben im Verlauf von ‚ÄûZeit‚Äú (f√ºr NS) und ‚ÄûTiefe‚Äú (f√ºr NS). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Illustration aus dem Originalartikel</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vielen Dank an </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bekemax</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> f√ºr die Hilfe bei der Bearbeitung der englischen Version des Textes und f√ºr interessante physische Kommentare. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Damit ist meine kleine Forschung zu </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neuronalen ODEs abgeschlossen</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font> Vielen Dank f√ºr Ihre Aufmerksamkeit! <br><br><h1>  N√ºtzliche Links </h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Repository mit Laptop + Englisch</font></font></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Version</a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Originalartikel</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Autoren-Repository</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Variationsschlussfolgerung</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mein Artikel √ºber VAE (russisch)</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VAE Erkl√§rung</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weitere Informationen zum Normalisieren von Fl√ºssen</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Variationsinferenz mit normalisierendem Flusspapier</font></font></a> <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442002/">https://habr.com/ru/post/de442002/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441990/index.html">Der effektivste Weg zur Bek√§mpfung der Piraterie - bequeme und kosteng√ºnstige juristische Dienstleistungen</a></li>
<li><a href="../de441992/index.html">Ein Geschenk f√ºr ein Geek Girl ausw√§hlen</a></li>
<li><a href="../de441994/index.html">NASA: Die Anzahl der bewohnbaren Planeten in unserer Galaxie ist viel geringer als allgemein angenommen</a></li>
<li><a href="../de441996/index.html">Technologie aus den 80er Jahren: Wer Waferscale-Prozessoren wiederbelebt</a></li>
<li><a href="../de441998/index.html">"Die Container haben den Kampf gewonnen, aber den Krieg gegen die Architektur ohne Server verloren" - Simon Wardley</a></li>
<li><a href="../de442004/index.html">SVG-Filtereffekte. Teil 7. Vorw√§rts</a></li>
<li><a href="../de442006/index.html">Dateiverwaltung falsch gemacht - Teil 2: Masterpiece of Shit</a></li>
<li><a href="../de442008/index.html">k3s ist ein kleines, aber zertifiziertes Kubernetes von Rancher Labs</a></li>
<li><a href="../de442010/index.html">Python und FPGA. Testen</a></li>
<li><a href="../de442012/index.html">Experiment: Wir sammeln ein Verzeichnis von Einheiten, die einen Pass ausgestellt haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>