<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☝️ 🐝 🏜️ Présentation de la méthode de vecteur de référence d'algorithme d'apprentissage automatique (SVM) 👵🏻 🦎 ⛑️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Préface 


 Dans cet article, nous explorerons plusieurs aspects de SVM: 



- composante théorique de SVM; 
- comment l'algorithme fonctionne sur des...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Présentation de la méthode de vecteur de référence d'algorithme d'apprentissage automatique (SVM)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428503/"><h2>  Préface </h2><br><img src="https://habrastorage.org/webt/eg/fk/rq/egfkrqshjcqqzkc7ktnnyyx9iuy.jpeg"><br><br>  Dans cet article, nous explorerons plusieurs aspects de SVM: <br><br><ul><li>  composante théorique de SVM; </li><li>  comment l'algorithme fonctionne sur des échantillons qui ne peuvent pas être divisés en classes de façon linéaire; </li><li>  Exemple Python et implémentation de l'algorithme dans la bibliothèque SciKit Learn. </li></ul><a name="habracut"></a><br>  Dans les articles suivants, je vais essayer de parler de la composante mathématique de cet algorithme. <br><br>  Comme vous le savez, les tâches d'apprentissage automatique sont divisées en deux catégories principales: la classification et la régression.  En fonction de laquelle de ces tâches nous sommes confrontés et de l'ensemble de données dont nous disposons pour cette tâche, nous choisissons l'algorithme à utiliser. <br><br>  La méthode Support Vector Machines ou SVM (de l'anglais Support Vector Machines) est un algorithme linéaire utilisé dans les problèmes de classification et de régression.  Cet algorithme est largement utilisé dans la pratique et peut résoudre à la fois des problèmes linéaires et non linéaires.  L'essence des «Machines» des vecteurs de support est simple: l'algorithme crée une ligne ou un hyperplan qui divise les données en classes. <br><br><h4>  Théorie </h4><br>  La tâche principale de l'algorithme est de trouver la ligne ou l'hyperplan le plus correct, en divisant les données en deux classes.  SVM est un algorithme qui reçoit des données à l'entrée et renvoie une telle ligne de division. <br><br>  Prenons l'exemple suivant.  Supposons que nous ayons un ensemble de données et que nous voulons classer et séparer les carrés rouges des cercles bleus (disons positifs et négatifs).  Le but principal de cette tâche sera de trouver la ligne «idéale» qui séparera ces deux classes. <br><br><img src="https://habrastorage.org/webt/lj/e4/oy/lje4oybbp_pbe_slxkvhm6yqaoy.png"><br><br>  Trouvez la ligne ou l'hyperplan parfait qui divise l'ensemble de données en classes bleues et rouges. <br><br>  À première vue, ce n'est pas si difficile, non? <br><br>  Mais, comme vous pouvez le voir, il n'y a pas de ligne unique qui résoudrait un tel problème.  Nous pouvons prendre un nombre infini de lignes qui peuvent séparer ces deux classes.  Comment SVM trouve-t-il exactement la ligne «idéale» et qu'est-ce qui est «idéal» dans sa compréhension? <br><br>  Jetez un œil à l'exemple ci-dessous et pensez à laquelle des deux lignes (jaune ou verte) sépare le mieux les deux classes, et correspond à la description de «idéal»? <br><br><img src="https://habrastorage.org/webt/w4/_f/kz/w4_fkz5krspejxz1o73l1yjnidy.png"><br><br>  Selon vous, quelle ligne sépare mieux l'ensemble de données? <br><br>  Si vous avez choisi la ligne jaune, je vous félicite: c'est la ligne que l'algorithme choisirait.  Dans cet exemple, nous pouvons comprendre intuitivement que la ligne jaune sépare et classe en conséquence les deux classes mieux que la verte. <br><br>  Dans le cas de la ligne verte - elle est située trop près de la classe rouge.  Malgré le fait qu'elle ait correctement classé tous les objets de l'ensemble de données actuel, une telle ligne ne sera pas généralisée - elle ne se comportera pas aussi bien avec un ensemble de données inconnu.  La tâche de trouver une généralisation séparant deux classes est l'une des tâches principales de l'apprentissage automatique. <br><br><h4>  Comment SVM trouve la meilleure ligne </h4><br>  L'algorithme SVM est conçu de manière à rechercher des points sur le graphique qui sont situés directement sur la ligne de séparation la plus proche.  Ces points sont appelés vecteurs de référence.  Ensuite, l'algorithme calcule la distance entre les vecteurs de support et le plan de division.  C'est la distance appelée l'écart.  L'objectif principal de l'algorithme est de maximiser la distance de dégagement.  Le meilleur hyperplan est considéré comme un hyperplan pour lequel cet écart est le plus grand possible. <br><br><img src="https://habrastorage.org/webt/ps/iy/he/psiyhexemtrhnqukbvmvaqzafvi.png"><br><br>  Assez simple, non?  Prenons l'exemple suivant, avec un ensemble de données plus complexe qui ne peut pas être divisé linéairement. <br><br><img src="https://habrastorage.org/webt/jh/5v/bx/jh5vbxwn7vfzyzeuxibxpleejyk.png"><br><br>  De toute évidence, cet ensemble de données ne peut pas être divisé de façon linéaire.  Nous ne pouvons pas tracer une ligne droite qui classerait ces données.  Mais, cet ensemble de données peut être divisé linéairement, en ajoutant une dimension supplémentaire, que nous appellerons l'axe Z. Imaginez que les coordonnées sur l'axe Z soient régulées par la restriction suivante: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">z</span><span class="MJXp-mo" id="MJXp-Span-3" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">x</span><span class="MJXp-mrow" id="MJXp-Span-5"><span class="MJXp-mo" id="MJXp-Span-6" style="margin-left: 0em; margin-right: 0em;">²</span></span><span class="MJXp-mo" id="MJXp-Span-7" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">y</span><span class="MJXp-mrow" id="MJXp-Span-9"><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0em; margin-right: 0em;">²</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.717ex" height="2.419ex" viewBox="0 -780.1 4614.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-7A" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMAIN-3D" x="746" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-78" x="1802" y="0"></use><g transform="translate(2375,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">²</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMAIN-2B" x="2856" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-79" x="3857" y="0"></use><g transform="translate(4354,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">²</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> z = x² + y² </script></p><br>  Ainsi, l'ordonnée Z est représentée à partir du carré de la distance du point au début de l'axe. <br>  Ci-dessous, une visualisation du même jeu de données sur l'axe Z. <br><br><img src="https://habrastorage.org/webt/vd/nj/ce/vdnjce7p5csbhfp12tkaj6t-4-s.png"><br><br>  Maintenant, les données peuvent être divisées linéairement.  Supposons que la ligne magenta séparant les données z = k, où k est une constante.  Si <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12">z</span><span class="MJXp-mo" id="MJXp-Span-13" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">x</span><span class="MJXp-mrow" id="MJXp-Span-15"><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0em;">²</span></span><span class="MJXp-mo" id="MJXp-Span-17" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">y</span><span class="MJXp-mrow" id="MJXp-Span-19"><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0em; margin-right: 0em;">²</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.717ex" height="2.419ex" viewBox="0 -780.1 4614.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-7A" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMAIN-3D" x="746" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-78" x="1802" y="0"></use><g transform="translate(2375,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">²</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMAIN-2B" x="2856" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-79" x="3857" y="0"></use><g transform="translate(4354,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">²</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> z = x² + y² </script></p>  alors <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-21"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22">k</span><span class="MJXp-mo" id="MJXp-Span-23" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24">x</span><span class="MJXp-mrow" id="MJXp-Span-25"><span class="MJXp-mo" id="MJXp-Span-26" style="margin-left: 0em; margin-right: 0em;">²</span></span><span class="MJXp-mo" id="MJXp-Span-27" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28">y</span><span class="MJXp-mrow" id="MJXp-Span-29"><span class="MJXp-mo" id="MJXp-Span-30" style="margin-left: 0em; margin-right: 0em;">²</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.84ex" height="2.419ex" viewBox="0 -780.1 4667.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-6B" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMAIN-3D" x="799" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-78" x="1855" y="0"></use><g transform="translate(2428,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">²</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMAIN-2B" x="2909" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhh31x1y-N_Gd3OutW7uxIF32pODiQ#MJMATHI-79" x="3910" y="0"></use><g transform="translate(4407,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">²</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> k = x² + y² </script></p>  - formule du cercle.  Ainsi, nous pouvons projeter notre diviseur linéaire, revenir au nombre d'origine de dimensions d'échantillon, en utilisant cette transformation. <br><br><img src="https://habrastorage.org/webt/we/nu/zn/wenuznqe4e7n4isscmunomrwzfw.png"><br><br>  Par conséquent, nous pouvons classer un ensemble de données non linéaire en lui ajoutant une dimension supplémentaire, puis le ramener à sa forme d'origine à l'aide de la transformation mathématique.  Cependant, pas avec tous les ensembles de données, il est tout aussi facile de lancer une telle transformation.  Heureusement, l'implémentation de cet algorithme dans la bibliothèque sklearn résout ce problème pour nous. <br><br><h4>  Hyperplan </h4><br>  Maintenant que nous nous sommes familiarisés avec la logique de l'algorithme, nous passons à la définition formelle d'un hyperplan <br><br>  Un hyperplan est un sous-plan de dimension n-1 dans un espace euclidien à n dimensions qui divise l'espace en deux parties distinctes. <br><br>  Par exemple, imaginez que notre ligne est représentée comme un espace euclidien unidimensionnel (c'est-à-dire que notre ensemble de données se trouve sur une ligne droite).  Sélectionnez un point sur cette ligne.  Ce point divisera l'ensemble de données, dans notre cas la ligne, en deux parties.  La ligne a une mesure et le point a 0 mesure.  Par conséquent, un point est un hyperplan d'une ligne. <br><br>  Pour l'ensemble de données bidimensionnel que nous avons rencontré précédemment, la ligne de démarcation était le même hyperplan.  Autrement dit, pour un espace à n dimensions, il y a un hyperplan à n dimensions qui divise cet espace en deux parties. <br><br>  CODE <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np X = np.array([[<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]) y = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])</code> </pre> <br>  Les points sont représentés comme un tableau de X et les classes auxquelles ils appartiennent comme un tableau de y. <br>  Nous allons maintenant former notre modèle avec cet échantillon.  Pour cet exemple, j'ai défini le paramètre linéaire du «noyau» du classificateur (noyau). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SVC clf = SVC(kernel=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>) clf = SVC.fit(X, y)</code> </pre><br>  Prédiction de classe d'un nouvel objet <br><br><pre> <code class="python hljs">prediction = clf.predict([[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>]])</code> </pre> <br><h4>  Réglage des paramètres </h4><br>  Les paramètres sont les arguments que vous transmettez lors de la création du classificateur.  Ci-dessous, j'ai fourni certains des paramètres SVM personnalisés les plus importants: <br><br>  <b>"C"</b> <br><br>  Ce paramètre permet d'ajuster cette fine ligne entre la «douceur» et la précision de la classification des objets dans l'échantillon d'apprentissage.  Plus la valeur «C» est élevée, plus les objets de l'ensemble d'apprentissage seront correctement classés. <br><br><img src="https://habrastorage.org/webt/rq/01/6s/rq016soikp4qfockp86li66s5mq.png"><br><br>  Dans cet exemple, il existe plusieurs seuils de décision que nous pouvons définir pour cet échantillon particulier.  Faites attention au seuil de décision direct (présenté sur le graphique comme une ligne verte).  C'est assez simple, et pour cette raison, plusieurs objets ont été mal classés.  Ces points qui ont été mal classés sont appelés valeurs aberrantes dans les données. <br><br>  Nous pouvons également ajuster les paramètres de telle manière qu'à la fin, nous obtenons une ligne plus courbe (seuil de décision bleu clair), qui classera absolument toutes les données de l'échantillon d'apprentissage correctement.  Bien sûr, dans ce cas, les chances que notre modèle soit en mesure de généraliser et d'afficher des résultats tout aussi bons sur de nouvelles données sont catastrophiques.  Par conséquent, si vous essayez d'atteindre la précision lors de la formation du modèle, vous devez viser quelque chose de plus uniforme, direct.  Plus le nombre «C» est élevé, plus l'hyperplan est enchevêtré dans votre modèle, mais plus le nombre d'objets correctement classés dans l'ensemble d'apprentissage est élevé.  Par conséquent, il est important de «tordre» les paramètres du modèle pour un ensemble de données spécifique afin d'éviter de se recycler mais, en même temps, d'atteindre une grande précision. <br><br>  <b>Gamma</b> <br><br>  Dans la documentation officielle, la bibliothèque SciKit Learn indique que le gamma détermine dans quelle mesure chacun des éléments de l'ensemble de données a une influence sur la détermination de la «ligne idéale».  Plus le gamma est bas, plus les éléments, même ceux qui sont suffisamment éloignés de la ligne de démarcation, participent au processus de choix de cette même ligne.  Si le gamma est élevé, l'algorithme ne «s'appuiera» que sur les éléments les plus proches de la ligne elle-même. <br>  Si le niveau gamma est trop élevé, seuls les éléments les plus proches de la ligne participeront au processus décisionnel sur l'emplacement de la ligne.  Cela aidera à ignorer les valeurs aberrantes dans les données.  L'algorithme SVM est conçu pour que les points situés les plus proches les uns des autres aient plus de poids lors de la prise de décision.  Cependant, avec le réglage correct de "C" et "gamma", un résultat optimal peut être obtenu qui construira un hyperplan plus linéaire qui ignore les valeurs aberrantes et, par conséquent, est plus généralisable. <br><br><h4>  Conclusion </h4><br>  J'espère sincèrement que cet article vous a aidé à comprendre l'essence du travail de SVM ou de la méthode des vecteurs de référence.  J'attends de vous tout commentaire et conseil.  Dans des publications ultérieures, je parlerai de la composante mathématique de SVM et des problèmes d'optimisation. <br><br>  Sources: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation SVM officielle dans SciKit Learn</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://towardsdatascience.com/">Vers le blog DataScience</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Siraj Raval: Support des machines à vecteurs</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intro to Machine Learning Udacity SVM: Vidéo du cours Gamma</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wikipédia: SVM</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr428503/">https://habr.com/ru/post/fr428503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr428493/index.html">Git subrepo</a></li>
<li><a href="../fr428495/index.html">Comment j'ai fait un simulateur de football pendant 13 ans</a></li>
<li><a href="../fr428497/index.html">Gradateur sans fil personnalisé Noolite SUF-1-300</a></li>
<li><a href="../fr428499/index.html">Des géants bleus effrayants peuvent révéler les secrets de l'évolution des étoiles</a></li>
<li><a href="../fr428501/index.html">DartUP: la première conférence en langue russe sur Dart et Flutter le 1er décembre à Saint-Pétersbourg</a></li>
<li><a href="../fr428505/index.html">Obtenir des liens vers l'audio sans VKApi</a></li>
<li><a href="../fr428507/index.html">Nous écrivons un chat bot pour VKontakte sur python en utilisant longpoll</a></li>
<li><a href="../fr428509/index.html">Comment H&M essaie de se sauver avec l'IA et le Big Data</a></li>
<li><a href="../fr428511/index.html">Énergie hydrogène: le début d'un long chemin</a></li>
<li><a href="../fr428513/index.html">500 pointeurs laser au même endroit</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>