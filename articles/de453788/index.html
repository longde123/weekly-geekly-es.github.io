<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüè´ üçÅ üìµ Neuronale Netze bevorzugen Texturen und wie man damit umgeht. üè™ ü§∞üèæ üèí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In letzter Zeit gab es mehrere Artikel, in denen ImageNet kritisiert wurde, der vielleicht ber√ºhmteste Satz von Bildern, die zum Trainieren neuronaler...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze bevorzugen Texturen und wie man damit umgeht.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/453788/"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  In letzter Zeit gab es mehrere Artikel, in denen ImageNet kritisiert wurde, der vielleicht ber√ºhmteste Satz von Bildern, die zum Trainieren neuronaler Netze verwendet werden. </p><br><p>  Im ersten Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">funktioniert die Approximation von CNNs mit Modellen mit vielen lokalen Merkmalen in ImageNet √ºberraschend gut. Die</a> Autoren verwenden ein Modell, das dem Wort mit √§hnlichen W√∂rtern √§hnelt, und verwenden Fragmente aus dem Bild als ‚ÄûW√∂rter‚Äú.  Diese Fragmente k√∂nnen bis zu 9 x 9 Pixel gro√ü sein.  Gleichzeitig erhalten die Autoren bei einem solchen Modell, bei dem Informationen √ºber die r√§umliche Anordnung dieser Fragmente vollst√§ndig fehlen, eine Genauigkeit von 70 bis 86% (z. B. betr√§gt die Genauigkeit eines regul√§ren ResNet-50 ~ 93%). </p><br><p>  Im zweiten Artikel von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet-trainierten CNNs sind die</a> Autoren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf Textur ausgerichtet. Die</a> Autoren kommen zu dem Schluss, dass der gesamte ImageNet-Datensatz und die Art und Weise, wie Menschen und neuronale Netze Bilder wahrnehmen, fehlerhaft sind, und schlagen die Verwendung eines neuen Datensatzes vor - Stylized-ImageNet. </p><br><p>  Im Detail dar√ºber, was Menschen in Bildern sehen und welche neuronalen Netze <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  Der ImageNet-Datensatz wurde 2006 von Professor Fei-Fei Li erstellt und entwickelt sich bis heute weiter.  Derzeit enth√§lt es etwa 14 Millionen Bilder, die zu mehr als 20.000 verschiedenen Kategorien geh√∂ren. </p><br><p>  Seit 2010 wird eine Teilmenge dieses Datensatzes, bekannt als ImageNet 1K mit ~ 1 Million Bildern und Tausenden von Klassen, in der ImageNet Large Scale Visual Recognition Challenge (ILSVRC) verwendet.  In diesem Wettbewerb schoss AlexNet, ein neuronales Faltungsnetzwerk, 2012 mit einer Top-1-Genauigkeit von 60% und einer Top-5-Genauigkeit von 80%. <br>  In dieser Teilmenge des Datensatzes messen Personen aus dem akademischen Umfeld <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ihre SOTA,</a> wenn sie neue Netzwerkarchitekturen anbieten. </p><br><p>  Ein wenig √ºber den Lernprozess in diesem Datensatz.  Wir werden √ºber das Trainingsprotokoll auf ImageNet im akademischen Umfeld sprechen.  Das hei√üt, wenn uns die Ergebnisse eines SE-Block-, ResNeXt- oder DenseNet-Netzwerks im Artikel gezeigt werden, sieht der Prozess ungef√§hr so ‚Äã‚Äãaus: Das Netzwerk lernt f√ºr 90 Epochen, die Lerngeschwindigkeit verringert sich als Optimierer alle 10 Mal um die 30. und 60. Epoche Es wird eine gew√∂hnliche SGD mit einem geringen Gewichtsabfall ausgew√§hlt. Es werden nur RandomCrop und HorizontalFlip aus Erweiterungen verwendet. Die Bildgr√∂√üe wird normalerweise auf 224 x 224 Pixel ge√§ndert. </p><br><p>  Hier ist ein Beispiel f√ºr ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch-Skript</a> f√ºr das Training in ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Kehren wir zu den zuvor erw√§hnten Artikeln zur√ºck.  Im ersten Fall wollten die Autoren ein Modell, das einfacher zu interpretieren ist als gew√∂hnliche tiefe Netzwerke.  Inspiriert von der Idee der Bag-of-Feature-Modelle kreieren sie ihre eigene Modellfamilie - BagNets.  Auf Basis des √ºblichen ResNet-50-Netzwerks. </p><br><p>  Durch das Ersetzen einiger 3x3-Faltungen durch 1x1 in ResNet-50 wird sichergestellt, dass das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Empfangsfeld von</a> Neuronen auf der letzten Faltungsschicht bis zu 9x9 Pixel erheblich reduziert wird.  Daher beschr√§nken sie die Informationen, die einem einzelnen Neuron zur Verf√ºgung stehen, auf ein sehr kleines Fragment des gesamten Bildes - einen Patch mit mehreren Pixeln.  Es ist zu beachten, dass f√ºr das makellose ResNet-50 die Gr√∂√üe des Empfangsfelds <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehr als 400 Pixel betr√§gt,</a> wodurch das Bild vollst√§ndig abgedeckt wird, das normalerweise eine Gr√∂√üe von 224 x 224 Pixel hat. </p><br><p>  Dieser Patch ist das <strong>maximale</strong> Fragment des Bildes, aus dem das Modell r√§umliche Daten extrahieren kann.  Am Ende des Modells wurden alle Daten einfach zusammengefasst und das Modell konnte in keiner Weise wissen, wo sich die einzelnen Patches im Verh√§ltnis zu anderen Patches befinden. <br>  Insgesamt wurden drei Varianten von Netzwerken mit den Empfangsfeldern 9x9, 17x17 und 33x33 getestet.  Und trotz des v√∂lligen Mangels an r√§umlichen Informationen konnten solche Modelle eine gute Genauigkeit bei der Klassifizierung in ImageNet erzielen.  Die Top-5-Genauigkeit f√ºr Patches 9x9 betrug 70%, f√ºr 17x17 - 80%, f√ºr 33x33 - 86%.  Zum Vergleich betr√§gt die ResNet-50 Top-5-Genauigkeit ungef√§hr 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  Die Struktur des Modells ist in der obigen Abbildung dargestellt.  Jeder Patch aus qxqx3 Pixeln, der aus dem Bild ausgeschnitten wurde, wird vom Netzwerk in einen 2048-Vektor umgewandelt. Als n√§chstes wird dieser Vektor dem Eingang eines linearen Klassifikators zugef√ºhrt, der f√ºr jede der 1000 Klassen Punktzahlen erzeugt.  Durch Sammeln der Ergebnisse jedes Patches in einem 2D-Array k√∂nnen Sie eine Heatmap f√ºr jede Klasse und jedes Pixel des Originalbilds erhalten.  Die endg√ºltigen Ergebnisse f√ºr das Bild wurden durch Summieren der Heatmap jeder Klasse erhalten. </p><br><p>  Beispiele f√ºr Heatmaps f√ºr einige Klassen: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="Heatmaps"><br>  Wie Sie sehen k√∂nnen, leisten Patches an den R√§ndern von Objekten den gr√∂√üten Beitrag zum Nutzen einer bestimmten Klasse.  Patches aus dem Hintergrund werden fast ignoriert.  Bisher l√§uft alles gut. </p><br><p>  Schauen wir uns die informativsten Patches an: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="InformativePatches"></p><br><p>  Zum Beispiel nahmen die Autoren vier Klassen.  F√ºr jeden von ihnen wurden 2x7 signifikanteste Patches ausgew√§hlt (dh Patches, bei denen die Punktzahl dieser Klasse am h√∂chsten war).  Die obere Reihe von 7 Patches wird nur aus Bildern der entsprechenden Klasse entnommen, die untere aus dem gesamten Bildbeispiel. </p><br><p>  Was auf diesen Bildern zu sehen ist, ist bemerkenswert.  Beispielsweise sind f√ºr die Schleienklasse (Schleie, Fisch) Finger ein charakteristisches Merkmal.  Ja, gew√∂hnliche menschliche Finger auf gr√ºnem Hintergrund.  Und das alles, weil es in fast allen Bildern einen Fischer dieser Klasse gibt, der diesen Fisch tats√§chlich in den H√§nden h√§lt und eine Troph√§e vorf√ºhrt. </p><br><div class="spoiler">  <b class="spoiler_title">Beispiele aus ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  Ein charakteristisches Merkmal von Laptops sind die Buchstabentasten.  Schreibmaschinentasten z√§hlen auch f√ºr diese Klasse. </p><br><p>  Ein charakteristisches Merkmal eines Buchumschlags sind Buchstaben auf farbigem Hintergrund.  Lassen Sie es sogar eine Inschrift auf einem T-Shirt oder auf einer Tasche sein. </p><br><p>  Es scheint, dass dieses Problem uns nicht st√∂ren sollte.  Da es nur einer engen Klasse von Netzwerken mit einem sehr begrenzten Empfangsfeld inh√§rent ist.  Dar√ºber hinaus berechneten die Autoren die Korrelation zwischen Protokollen (Netzwerkausgaben vor dem endg√ºltigen Softmax), die jeder BagNet-Klasse mit unterschiedlichem Empfangsfeld zugewiesen wurden, und Protokollen von VGG-16, das ein ziemlich gro√ües Empfangsfeld aufweist.  Und sie fanden sie ziemlich hoch. </p><br><div class="spoiler">  <b class="spoiler_title">Korrelation zwischen BagNets und VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  Die Autoren fragten sich, ob BagNet Hinweise darauf enth√§lt, wie andere Netzwerke Entscheidungen treffen. </p><br><p>  F√ºr einen der Tests verwendeten sie eine Technik wie Image Scrambling.  Was darin bestand, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Texturgenerator zu verwenden, der auf Gramm-Matrizen basiert</a> , um ein Bild zu erstellen, in dem Texturen gespeichert werden, aber r√§umliche Informationen fehlen. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="ScrambledImages"></p><br><p>  VGG-16, trainiert auf gew√∂hnlichen, vollwertigen Bildern, kam mit solchen durcheinandergemischten Bildern ziemlich gut zurecht.  Die Top-5-Genauigkeit sank von 90% auf 80%.  Das hei√üt, selbst Netzwerke mit einem ziemlich gro√üen Empfangsfeld bevorzugen es immer noch, sich Texturen zu merken und r√§umliche Informationen zu ignorieren.  Daher fiel ihre Genauigkeit bei verschl√ºsselten Bildern nicht stark ab. </p><br><p>  Die Autoren f√ºhrten eine Reihe von Experimenten durch, in denen sie verglichen, welche Teile der Bilder f√ºr BagNet und andere Netzwerke (VGG-16, ResNet-50, ResNet-152 und DenseNet-169) am wichtigsten sind.  Alles deutete darauf hin, dass andere Netzwerke wie BagNet auf kleine Bildfragmente angewiesen sind und bei Entscheidungen etwa dieselben Fehler machen.  Dies machte sich insbesondere bei nicht sehr tiefen Netzwerken wie VGG bemerkbar. </p><br><p>  Diese Tendenz von Netzwerken, Entscheidungen auf der Grundlage von Texturen zu treffen, veranlasste im Gegensatz zu uns Menschen, die Form bevorzugen (siehe Abbildung unten), die Autoren des zweiten Artikels, einen neuen Datensatz auf der Basis von ImageNet zu erstellen. </p><br><h3 id="stylized-imagenet">  Stilisiertes ImageNet </h3><br><p>  Zun√§chst erstellten die Autoren des Artikels mithilfe der Stil√ºbertragung eine Reihe von Bildern, bei denen sich Form (r√§umliche Daten) und Texturen in einem Bild widersprachen.  Und wir verglichen die Ergebnisse von Menschen und tiefen Faltungsnetzwerken verschiedener Architekturen mit einem synthetisierten Datensatz aus 16 Klassen. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  Ganz rechts sehen die Menschen eine Katze, ein Netzwerk - einen Elefanten. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Vergleich der Ergebnisse von Menschen und neuronalen Netzen. </p><br><p>  Wie Sie sehen k√∂nnen, st√ºtzten sich Personen beim Zuweisen eines Objekts zu einer bestimmten Klasse auf die Form von Objekten, neuronale Netze auf Texturen.  In der Abbildung oben sahen die Menschen eine Katze, ein Netzwerk - einen Elefanten. </p><br><blockquote>  Ja, hier kann man daran bem√§ngeln, dass die Netze auch etwas richtig sind und dies zum Beispiel ein Elefant sein k√∂nnte, der aus n√§chster N√§he mit einem Tattoo einer geliebten Katze fotografiert wurde.  Aufgrund der Tatsache, dass sich die Netzwerke bei Entscheidungen anders verhalten als die Menschen, haben die Autoren das Problem in Betracht gezogen und nach Wegen gesucht, es zu l√∂sen. </blockquote><p>  Wie oben erw√§hnt, kann das Netzwerk nur mit Texturen ein gutes Ergebnis bei einer Top-5-Genauigkeit von 86% erzielen.  Dabei geht es nicht um mehrere Klassen, in denen Texturen helfen, Bilder korrekt zu klassifizieren, sondern um die meisten Klassen. </p><br><p>  Das Problem liegt in ImageNet selbst, da sp√§ter gezeigt wird, dass das Netzwerk die Form lernen kann, dies jedoch nicht, da Texturen f√ºr diesen Datensatz ausreichen und sich die f√ºr die Texturen verantwortlichen Neuronen auf flachen Schichten befinden. die sind viel einfacher zu trainieren. </p><br><p>  Mit diesem etwas anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AdaIN-Schnell√ºbertragungsmechanismus</a> erstellten die Autoren einen neuen Datensatz - Stylized ImageNet.  Die Form der Objekte wurde aus ImageNet √ºbernommen und die Texturen aus diesem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wettbewerb auf Kaggle</a> .  Das Skript zur Generierung ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unter dem Link</a> verf√ºgbar. </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  Der K√ºrze halber wird ImageNet als <strong>IN</strong> , Stylized ImageNet als <strong>SIN bezeichnet</strong> . </p><br><p>  Die Autoren nahmen ResNet-50 und drei BagNet mit unterschiedlichem Empfangsfeld und trainierten f√ºr jeden Datensatz ein eigenes Modell. </p><br><p>  Und hier ist, was sie getan haben: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  Was wir hier sehen.  Auf IN trainiertes ResNet-50 ist auf SIN vollst√§ndig au√üer Gefecht gesetzt.  Dies best√§tigt teilweise, dass sich das Netzwerk beim Training auf IN an Texturen anpasst und die Form von Objekten ignoriert.  Gleichzeitig kommt das auf SIN trainierte ResNet-50 sowohl mit SIN als auch mit IN perfekt zurecht.  Das hei√üt, wenn ihm ein einfacher Pfad entzogen wird, folgt das Netzwerk einem schwierigen Pfad - es lehrt die Form von Objekten. <br>  BagNet hat endlich begonnen, sich wie erwartet zu verhalten, insbesondere bei kleinen Patches, da es nichts zu klammern hat - Texturinformationen fehlen einfach in der SIN. </p><br><p>  In diesen 16 Klassen, die zuvor erw√§hnt wurden, begann ResNet-50, das auf SIN geschult war, Antworten zu geben, die denen √§hnlicher waren, die die Leute gaben: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  Zus√§tzlich zum einfachen Training von ResNet-50 auf SIN versuchten die Autoren, das Netzwerk auf einem gemischten Satz von SIN und IN zu trainieren, einschlie√ülich der separaten Feinabstimmung auf reinem IN. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  Wie Sie sehen, haben sich bei Verwendung von SIN + IN f√ºr das Training die Ergebnisse nicht nur bei der Hauptaufgabe - der Bildklassifizierung in ImageNet - verbessert, sondern auch bei der Erkennung von Objekten im PASCAL VOC 2007-Dataset. </p><br><p>  Dar√ºber hinaus sind von SIN trainierte Netzwerke widerstandsf√§higer gegen verschiedene Datenst√∂rungen. </p><br><h3 id="zaklyuchenie">  Fazit </h3><br><p>  Selbst jetzt, im Jahr 2019, nach sieben Jahren des Erfolgs mit AlexNet, als neuronale Netze in der Bildverarbeitung weit verbreitet sind und ImageNet 1K de facto zum Standard f√ºr die Bewertung der Leistung von Modellen im akademischen Umfeld wurde, ist der Mechanismus, wie neuronale Netze Entscheidungen treffen, nicht ganz klar .  Und wie die Datens√§tze, auf denen diese Netzwerke trainiert wurden, dies beeinflussen. </p><br><p>  Die Autoren des ersten Artikels versuchten zu beleuchten, wie solche Entscheidungen in Netzwerken mit einer Bag-of-Features-Architektur mit einem begrenzten Empfangsfeld getroffen werden, das leichter zu interpretieren ist.  Beim Vergleich der Antworten von BagNet und den √ºblichen tiefen neuronalen Netzen kamen wir zu dem Schluss, dass die Entscheidungsprozesse in ihnen ziemlich √§hnlich sind. </p><br><p>  Die Autoren des zweiten Artikels verglichen, wie Menschen und neuronale Netze Bilder wahrnehmen, in denen sich Form und Textur widersprechen.  Und sie schlugen vor, ein neues Dataset, Stylized ImageNet, zu verwenden, um Wahrnehmungsunterschiede zu verringern.  Als Bonus eine Erh√∂hung der Genauigkeit der Klassifizierung in ImageNet und der Erkennung in Datens√§tzen von Drittanbietern erhalten. </p><br><p>  Die Hauptschlussfolgerung kann wie folgt gezogen werden: Netzwerke, die aus Bildern lernen und sich an √ºbergeordnete r√§umliche Eigenschaften von Objekten erinnern k√∂nnen, bevorzugen einen einfacheren Weg, um das Ziel zu erreichen - eine √úberanpassung an Texturen.  Wenn der Datensatz, auf dem sie trainieren, dies zul√§sst. </p><br><p>  Neben dem akademischen Interesse ist das Problem der Textur√ºberanpassung f√ºr alle von uns wichtig, die vorgefertigte Modelle f√ºr das Transferlernen in ihren Aufgaben verwenden. <br>  Eine wichtige Konsequenz all dessen ist f√ºr uns, dass Sie dem Gewicht von Modellen, die im Allgemeinen in ImageNet vorab trainiert wurden, nicht vertrauen sollten, da f√ºr die meisten von ihnen relativ einfache Erweiterungen verwendet wurden, die in keiner Weise dazu beitragen, √úberanpassungen zu beseitigen.  Und wenn m√∂glich, ist es besser, Modelle mit ernsthafteren Erweiterungen oder stilisiertem ImageNet + ImageNet im Nest trainieren zu lassen.  Um immer vergleichen zu k√∂nnen, welches f√ºr unsere aktuelle Aufgabe am besten geeignet ist. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de453788/">https://habr.com/ru/post/de453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de453776/index.html">Wir pumpen Designer im Unternehmen: vom Junior bis zum Art Director</a></li>
<li><a href="../de453778/index.html">Wie wir eine Online-Bank f√ºr Unternehmen geschaffen haben. Erster Teil: Rebranding</a></li>
<li><a href="../de453780/index.html">Wie w√§hlt man ein Grandstream SIP-Telefon - im Allgemeinen und im Besonderen?</a></li>
<li><a href="../de453782/index.html">Unendliche UIScrollView</a></li>
<li><a href="../de453784/index.html">Wie der DevOps-Spezialist der Automatisierung zum Opfer f√§llt</a></li>
<li><a href="../de453790/index.html">"Der Kunde ist weg - ist es f√ºr immer?" Wie man die Kundenabwanderung in SaaS z√§hlt und was mit grundlegenden Metriken nicht stimmt</a></li>
<li><a href="../de453792/index.html">Empfehlungssysteme: Ideen, Ans√§tze, Aufgaben</a></li>
<li><a href="../de453796/index.html">Brauchen die Leute Mathe?</a></li>
<li><a href="../de453800/index.html">Wie man "Minesweeper" l√∂st (und es besser macht)</a></li>
<li><a href="../de453804/index.html">Das Buch "Wettbewerbsf√§higkeit und Parallelit√§t auf der .NET-Plattform. Effektive Designmuster ‚Äú</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>