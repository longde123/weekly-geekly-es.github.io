<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍🏫 🍁 📵 Neuronale Netze bevorzugen Texturen und wie man damit umgeht. 🏪 🤰🏾 🏒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In letzter Zeit gab es mehrere Artikel, in denen ImageNet kritisiert wurde, der vielleicht berühmteste Satz von Bildern, die zum Trainieren neuronaler...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze bevorzugen Texturen und wie man damit umgeht.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/453788/"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  In letzter Zeit gab es mehrere Artikel, in denen ImageNet kritisiert wurde, der vielleicht berühmteste Satz von Bildern, die zum Trainieren neuronaler Netze verwendet werden. </p><br><p>  Im ersten Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">funktioniert die Approximation von CNNs mit Modellen mit vielen lokalen Merkmalen in ImageNet überraschend gut. Die</a> Autoren verwenden ein Modell, das dem Wort mit ähnlichen Wörtern ähnelt, und verwenden Fragmente aus dem Bild als „Wörter“.  Diese Fragmente können bis zu 9 x 9 Pixel groß sein.  Gleichzeitig erhalten die Autoren bei einem solchen Modell, bei dem Informationen über die räumliche Anordnung dieser Fragmente vollständig fehlen, eine Genauigkeit von 70 bis 86% (z. B. beträgt die Genauigkeit eines regulären ResNet-50 ~ 93%). </p><br><p>  Im zweiten Artikel von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet-trainierten CNNs sind die</a> Autoren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf Textur ausgerichtet. Die</a> Autoren kommen zu dem Schluss, dass der gesamte ImageNet-Datensatz und die Art und Weise, wie Menschen und neuronale Netze Bilder wahrnehmen, fehlerhaft sind, und schlagen die Verwendung eines neuen Datensatzes vor - Stylized-ImageNet. </p><br><p>  Im Detail darüber, was Menschen in Bildern sehen und welche neuronalen Netze <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  Der ImageNet-Datensatz wurde 2006 von Professor Fei-Fei Li erstellt und entwickelt sich bis heute weiter.  Derzeit enthält es etwa 14 Millionen Bilder, die zu mehr als 20.000 verschiedenen Kategorien gehören. </p><br><p>  Seit 2010 wird eine Teilmenge dieses Datensatzes, bekannt als ImageNet 1K mit ~ 1 Million Bildern und Tausenden von Klassen, in der ImageNet Large Scale Visual Recognition Challenge (ILSVRC) verwendet.  In diesem Wettbewerb schoss AlexNet, ein neuronales Faltungsnetzwerk, 2012 mit einer Top-1-Genauigkeit von 60% und einer Top-5-Genauigkeit von 80%. <br>  In dieser Teilmenge des Datensatzes messen Personen aus dem akademischen Umfeld <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ihre SOTA,</a> wenn sie neue Netzwerkarchitekturen anbieten. </p><br><p>  Ein wenig über den Lernprozess in diesem Datensatz.  Wir werden über das Trainingsprotokoll auf ImageNet im akademischen Umfeld sprechen.  Das heißt, wenn uns die Ergebnisse eines SE-Block-, ResNeXt- oder DenseNet-Netzwerks im Artikel gezeigt werden, sieht der Prozess ungefähr so ​​aus: Das Netzwerk lernt für 90 Epochen, die Lerngeschwindigkeit verringert sich als Optimierer alle 10 Mal um die 30. und 60. Epoche Es wird eine gewöhnliche SGD mit einem geringen Gewichtsabfall ausgewählt. Es werden nur RandomCrop und HorizontalFlip aus Erweiterungen verwendet. Die Bildgröße wird normalerweise auf 224 x 224 Pixel geändert. </p><br><p>  Hier ist ein Beispiel für ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch-Skript</a> für das Training in ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Kehren wir zu den zuvor erwähnten Artikeln zurück.  Im ersten Fall wollten die Autoren ein Modell, das einfacher zu interpretieren ist als gewöhnliche tiefe Netzwerke.  Inspiriert von der Idee der Bag-of-Feature-Modelle kreieren sie ihre eigene Modellfamilie - BagNets.  Auf Basis des üblichen ResNet-50-Netzwerks. </p><br><p>  Durch das Ersetzen einiger 3x3-Faltungen durch 1x1 in ResNet-50 wird sichergestellt, dass das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Empfangsfeld von</a> Neuronen auf der letzten Faltungsschicht bis zu 9x9 Pixel erheblich reduziert wird.  Daher beschränken sie die Informationen, die einem einzelnen Neuron zur Verfügung stehen, auf ein sehr kleines Fragment des gesamten Bildes - einen Patch mit mehreren Pixeln.  Es ist zu beachten, dass für das makellose ResNet-50 die Größe des Empfangsfelds <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehr als 400 Pixel beträgt,</a> wodurch das Bild vollständig abgedeckt wird, das normalerweise eine Größe von 224 x 224 Pixel hat. </p><br><p>  Dieser Patch ist das <strong>maximale</strong> Fragment des Bildes, aus dem das Modell räumliche Daten extrahieren kann.  Am Ende des Modells wurden alle Daten einfach zusammengefasst und das Modell konnte in keiner Weise wissen, wo sich die einzelnen Patches im Verhältnis zu anderen Patches befinden. <br>  Insgesamt wurden drei Varianten von Netzwerken mit den Empfangsfeldern 9x9, 17x17 und 33x33 getestet.  Und trotz des völligen Mangels an räumlichen Informationen konnten solche Modelle eine gute Genauigkeit bei der Klassifizierung in ImageNet erzielen.  Die Top-5-Genauigkeit für Patches 9x9 betrug 70%, für 17x17 - 80%, für 33x33 - 86%.  Zum Vergleich beträgt die ResNet-50 Top-5-Genauigkeit ungefähr 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  Die Struktur des Modells ist in der obigen Abbildung dargestellt.  Jeder Patch aus qxqx3 Pixeln, der aus dem Bild ausgeschnitten wurde, wird vom Netzwerk in einen 2048-Vektor umgewandelt. Als nächstes wird dieser Vektor dem Eingang eines linearen Klassifikators zugeführt, der für jede der 1000 Klassen Punktzahlen erzeugt.  Durch Sammeln der Ergebnisse jedes Patches in einem 2D-Array können Sie eine Heatmap für jede Klasse und jedes Pixel des Originalbilds erhalten.  Die endgültigen Ergebnisse für das Bild wurden durch Summieren der Heatmap jeder Klasse erhalten. </p><br><p>  Beispiele für Heatmaps für einige Klassen: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="Heatmaps"><br>  Wie Sie sehen können, leisten Patches an den Rändern von Objekten den größten Beitrag zum Nutzen einer bestimmten Klasse.  Patches aus dem Hintergrund werden fast ignoriert.  Bisher läuft alles gut. </p><br><p>  Schauen wir uns die informativsten Patches an: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="InformativePatches"></p><br><p>  Zum Beispiel nahmen die Autoren vier Klassen.  Für jeden von ihnen wurden 2x7 signifikanteste Patches ausgewählt (dh Patches, bei denen die Punktzahl dieser Klasse am höchsten war).  Die obere Reihe von 7 Patches wird nur aus Bildern der entsprechenden Klasse entnommen, die untere aus dem gesamten Bildbeispiel. </p><br><p>  Was auf diesen Bildern zu sehen ist, ist bemerkenswert.  Beispielsweise sind für die Schleienklasse (Schleie, Fisch) Finger ein charakteristisches Merkmal.  Ja, gewöhnliche menschliche Finger auf grünem Hintergrund.  Und das alles, weil es in fast allen Bildern einen Fischer dieser Klasse gibt, der diesen Fisch tatsächlich in den Händen hält und eine Trophäe vorführt. </p><br><div class="spoiler">  <b class="spoiler_title">Beispiele aus ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  Ein charakteristisches Merkmal von Laptops sind die Buchstabentasten.  Schreibmaschinentasten zählen auch für diese Klasse. </p><br><p>  Ein charakteristisches Merkmal eines Buchumschlags sind Buchstaben auf farbigem Hintergrund.  Lassen Sie es sogar eine Inschrift auf einem T-Shirt oder auf einer Tasche sein. </p><br><p>  Es scheint, dass dieses Problem uns nicht stören sollte.  Da es nur einer engen Klasse von Netzwerken mit einem sehr begrenzten Empfangsfeld inhärent ist.  Darüber hinaus berechneten die Autoren die Korrelation zwischen Protokollen (Netzwerkausgaben vor dem endgültigen Softmax), die jeder BagNet-Klasse mit unterschiedlichem Empfangsfeld zugewiesen wurden, und Protokollen von VGG-16, das ein ziemlich großes Empfangsfeld aufweist.  Und sie fanden sie ziemlich hoch. </p><br><div class="spoiler">  <b class="spoiler_title">Korrelation zwischen BagNets und VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  Die Autoren fragten sich, ob BagNet Hinweise darauf enthält, wie andere Netzwerke Entscheidungen treffen. </p><br><p>  Für einen der Tests verwendeten sie eine Technik wie Image Scrambling.  Was darin bestand, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Texturgenerator zu verwenden, der auf Gramm-Matrizen basiert</a> , um ein Bild zu erstellen, in dem Texturen gespeichert werden, aber räumliche Informationen fehlen. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="ScrambledImages"></p><br><p>  VGG-16, trainiert auf gewöhnlichen, vollwertigen Bildern, kam mit solchen durcheinandergemischten Bildern ziemlich gut zurecht.  Die Top-5-Genauigkeit sank von 90% auf 80%.  Das heißt, selbst Netzwerke mit einem ziemlich großen Empfangsfeld bevorzugen es immer noch, sich Texturen zu merken und räumliche Informationen zu ignorieren.  Daher fiel ihre Genauigkeit bei verschlüsselten Bildern nicht stark ab. </p><br><p>  Die Autoren führten eine Reihe von Experimenten durch, in denen sie verglichen, welche Teile der Bilder für BagNet und andere Netzwerke (VGG-16, ResNet-50, ResNet-152 und DenseNet-169) am wichtigsten sind.  Alles deutete darauf hin, dass andere Netzwerke wie BagNet auf kleine Bildfragmente angewiesen sind und bei Entscheidungen etwa dieselben Fehler machen.  Dies machte sich insbesondere bei nicht sehr tiefen Netzwerken wie VGG bemerkbar. </p><br><p>  Diese Tendenz von Netzwerken, Entscheidungen auf der Grundlage von Texturen zu treffen, veranlasste im Gegensatz zu uns Menschen, die Form bevorzugen (siehe Abbildung unten), die Autoren des zweiten Artikels, einen neuen Datensatz auf der Basis von ImageNet zu erstellen. </p><br><h3 id="stylized-imagenet">  Stilisiertes ImageNet </h3><br><p>  Zunächst erstellten die Autoren des Artikels mithilfe der Stilübertragung eine Reihe von Bildern, bei denen sich Form (räumliche Daten) und Texturen in einem Bild widersprachen.  Und wir verglichen die Ergebnisse von Menschen und tiefen Faltungsnetzwerken verschiedener Architekturen mit einem synthetisierten Datensatz aus 16 Klassen. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  Ganz rechts sehen die Menschen eine Katze, ein Netzwerk - einen Elefanten. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Vergleich der Ergebnisse von Menschen und neuronalen Netzen. </p><br><p>  Wie Sie sehen können, stützten sich Personen beim Zuweisen eines Objekts zu einer bestimmten Klasse auf die Form von Objekten, neuronale Netze auf Texturen.  In der Abbildung oben sahen die Menschen eine Katze, ein Netzwerk - einen Elefanten. </p><br><blockquote>  Ja, hier kann man daran bemängeln, dass die Netze auch etwas richtig sind und dies zum Beispiel ein Elefant sein könnte, der aus nächster Nähe mit einem Tattoo einer geliebten Katze fotografiert wurde.  Aufgrund der Tatsache, dass sich die Netzwerke bei Entscheidungen anders verhalten als die Menschen, haben die Autoren das Problem in Betracht gezogen und nach Wegen gesucht, es zu lösen. </blockquote><p>  Wie oben erwähnt, kann das Netzwerk nur mit Texturen ein gutes Ergebnis bei einer Top-5-Genauigkeit von 86% erzielen.  Dabei geht es nicht um mehrere Klassen, in denen Texturen helfen, Bilder korrekt zu klassifizieren, sondern um die meisten Klassen. </p><br><p>  Das Problem liegt in ImageNet selbst, da später gezeigt wird, dass das Netzwerk die Form lernen kann, dies jedoch nicht, da Texturen für diesen Datensatz ausreichen und sich die für die Texturen verantwortlichen Neuronen auf flachen Schichten befinden. die sind viel einfacher zu trainieren. </p><br><p>  Mit diesem etwas anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AdaIN-Schnellübertragungsmechanismus</a> erstellten die Autoren einen neuen Datensatz - Stylized ImageNet.  Die Form der Objekte wurde aus ImageNet übernommen und die Texturen aus diesem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wettbewerb auf Kaggle</a> .  Das Skript zur Generierung ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unter dem Link</a> verfügbar. </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  Der Kürze halber wird ImageNet als <strong>IN</strong> , Stylized ImageNet als <strong>SIN bezeichnet</strong> . </p><br><p>  Die Autoren nahmen ResNet-50 und drei BagNet mit unterschiedlichem Empfangsfeld und trainierten für jeden Datensatz ein eigenes Modell. </p><br><p>  Und hier ist, was sie getan haben: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  Was wir hier sehen.  Auf IN trainiertes ResNet-50 ist auf SIN vollständig außer Gefecht gesetzt.  Dies bestätigt teilweise, dass sich das Netzwerk beim Training auf IN an Texturen anpasst und die Form von Objekten ignoriert.  Gleichzeitig kommt das auf SIN trainierte ResNet-50 sowohl mit SIN als auch mit IN perfekt zurecht.  Das heißt, wenn ihm ein einfacher Pfad entzogen wird, folgt das Netzwerk einem schwierigen Pfad - es lehrt die Form von Objekten. <br>  BagNet hat endlich begonnen, sich wie erwartet zu verhalten, insbesondere bei kleinen Patches, da es nichts zu klammern hat - Texturinformationen fehlen einfach in der SIN. </p><br><p>  In diesen 16 Klassen, die zuvor erwähnt wurden, begann ResNet-50, das auf SIN geschult war, Antworten zu geben, die denen ähnlicher waren, die die Leute gaben: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  Zusätzlich zum einfachen Training von ResNet-50 auf SIN versuchten die Autoren, das Netzwerk auf einem gemischten Satz von SIN und IN zu trainieren, einschließlich der separaten Feinabstimmung auf reinem IN. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  Wie Sie sehen, haben sich bei Verwendung von SIN + IN für das Training die Ergebnisse nicht nur bei der Hauptaufgabe - der Bildklassifizierung in ImageNet - verbessert, sondern auch bei der Erkennung von Objekten im PASCAL VOC 2007-Dataset. </p><br><p>  Darüber hinaus sind von SIN trainierte Netzwerke widerstandsfähiger gegen verschiedene Datenstörungen. </p><br><h3 id="zaklyuchenie">  Fazit </h3><br><p>  Selbst jetzt, im Jahr 2019, nach sieben Jahren des Erfolgs mit AlexNet, als neuronale Netze in der Bildverarbeitung weit verbreitet sind und ImageNet 1K de facto zum Standard für die Bewertung der Leistung von Modellen im akademischen Umfeld wurde, ist der Mechanismus, wie neuronale Netze Entscheidungen treffen, nicht ganz klar .  Und wie die Datensätze, auf denen diese Netzwerke trainiert wurden, dies beeinflussen. </p><br><p>  Die Autoren des ersten Artikels versuchten zu beleuchten, wie solche Entscheidungen in Netzwerken mit einer Bag-of-Features-Architektur mit einem begrenzten Empfangsfeld getroffen werden, das leichter zu interpretieren ist.  Beim Vergleich der Antworten von BagNet und den üblichen tiefen neuronalen Netzen kamen wir zu dem Schluss, dass die Entscheidungsprozesse in ihnen ziemlich ähnlich sind. </p><br><p>  Die Autoren des zweiten Artikels verglichen, wie Menschen und neuronale Netze Bilder wahrnehmen, in denen sich Form und Textur widersprechen.  Und sie schlugen vor, ein neues Dataset, Stylized ImageNet, zu verwenden, um Wahrnehmungsunterschiede zu verringern.  Als Bonus eine Erhöhung der Genauigkeit der Klassifizierung in ImageNet und der Erkennung in Datensätzen von Drittanbietern erhalten. </p><br><p>  Die Hauptschlussfolgerung kann wie folgt gezogen werden: Netzwerke, die aus Bildern lernen und sich an übergeordnete räumliche Eigenschaften von Objekten erinnern können, bevorzugen einen einfacheren Weg, um das Ziel zu erreichen - eine Überanpassung an Texturen.  Wenn der Datensatz, auf dem sie trainieren, dies zulässt. </p><br><p>  Neben dem akademischen Interesse ist das Problem der Texturüberanpassung für alle von uns wichtig, die vorgefertigte Modelle für das Transferlernen in ihren Aufgaben verwenden. <br>  Eine wichtige Konsequenz all dessen ist für uns, dass Sie dem Gewicht von Modellen, die im Allgemeinen in ImageNet vorab trainiert wurden, nicht vertrauen sollten, da für die meisten von ihnen relativ einfache Erweiterungen verwendet wurden, die in keiner Weise dazu beitragen, Überanpassungen zu beseitigen.  Und wenn möglich, ist es besser, Modelle mit ernsthafteren Erweiterungen oder stilisiertem ImageNet + ImageNet im Nest trainieren zu lassen.  Um immer vergleichen zu können, welches für unsere aktuelle Aufgabe am besten geeignet ist. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de453788/">https://habr.com/ru/post/de453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de453776/index.html">Wir pumpen Designer im Unternehmen: vom Junior bis zum Art Director</a></li>
<li><a href="../de453778/index.html">Wie wir eine Online-Bank für Unternehmen geschaffen haben. Erster Teil: Rebranding</a></li>
<li><a href="../de453780/index.html">Wie wählt man ein Grandstream SIP-Telefon - im Allgemeinen und im Besonderen?</a></li>
<li><a href="../de453782/index.html">Unendliche UIScrollView</a></li>
<li><a href="../de453784/index.html">Wie der DevOps-Spezialist der Automatisierung zum Opfer fällt</a></li>
<li><a href="../de453790/index.html">"Der Kunde ist weg - ist es für immer?" Wie man die Kundenabwanderung in SaaS zählt und was mit grundlegenden Metriken nicht stimmt</a></li>
<li><a href="../de453792/index.html">Empfehlungssysteme: Ideen, Ansätze, Aufgaben</a></li>
<li><a href="../de453796/index.html">Brauchen die Leute Mathe?</a></li>
<li><a href="../de453800/index.html">Wie man "Minesweeper" löst (und es besser macht)</a></li>
<li><a href="../de453804/index.html">Das Buch "Wettbewerbsfähigkeit und Parallelität auf der .NET-Plattform. Effektive Designmuster “</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>