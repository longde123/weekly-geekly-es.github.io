<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüåæ ‚úåüèº üöª Aplicaci√≥n de aprendizaje autom√°tico a redes neuronales con arquitectura de transformador üë®üèø‚Äçü§ù‚Äçüë®üèæ üíç üöã</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Desde el blog de Google AI 

 Desde la publicaci√≥n de informaci√≥n sobre ellos en 2017, las redes neuronales de arquitectura transformadora se han apli...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aplicaci√≥n de aprendizaje autom√°tico a redes neuronales con arquitectura de transformador</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460099/"> <i>Desde el blog de Google AI</i> <br><br>  Desde la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n de informaci√≥n</a> sobre ellos en 2017, las redes neuronales de arquitectura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">transformadora</a> se han aplicado a tareas de diversos tipos, desde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">generar textos de estilo fantas√≠a</a> hasta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">escribir armon√≠as musicales</a> .  Lo que es importante, la alta calidad del trabajo de los "transformadores" ha demostrado que cuando se aplica a tareas secuenciales, como el modelado y la traducci√≥n del lenguaje, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las redes neuronales de distribuci√≥n directa</a> pueden ser tan efectivas como las recurrentes.  Aunque la popularidad de los transformadores y otros modelos de distribuci√≥n directa utilizados en tareas secuenciales est√° creciendo, sus arquitecturas casi siempre se crean manualmente, en contraste con el campo de la visi√≥n por computadora, donde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">los</a> enfoques de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprendizaje autom√°tico</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">avanzado</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AOM</a> ) ya han descubierto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelos avanzados</a> que est√°n por delante de los expuestos ajuste manual  Naturalmente, est√°bamos interesados ‚Äã‚Äãen saber si la aplicaci√≥n de AOM a tareas secuenciales puede lograr el mismo √©xito. <br><a name="habracut"></a><br>  Despu√©s de realizar una b√∫squeda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">evolutiva</a> de neuroarquitectura (NAS) y usar la traducci√≥n como ejemplo de tareas secuenciales, descubrimos un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">transformador en evoluci√≥n</a> (ET), una nueva arquitectura de transformador que demuestra mejoras en varias tareas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">procesamiento del lenguaje natural</a> (OYA).  ET no solo logra resultados de vanguardia en la traducci√≥n, sino que tambi√©n demuestra una mayor eficiencia en el modelado del lenguaje en comparaci√≥n con el transformador original.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Publicamos un</a> nuevo modelo en la biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensor2Tensor</a> , donde puede usarse para cualquier tarea secuencial. <br><br><h2>  Desarrollo tecnico </h2><br>  Para comenzar la b√∫squeda evolutiva de la neuroarquitectura, necesit√°bamos desarrollar nuevas t√©cnicas, ya que la tarea utilizada para evaluar la "aptitud" de cada una de las arquitecturas, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">traducci√≥n del ingl√©s al alem√°n WMT'14</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">requer√≠a</a> recursos inform√°ticos.  Como resultado, estas b√∫squedas resultan ser m√°s exigentes que b√∫squedas similares en el campo de la visi√≥n por computadora, que pueden operar con bases de datos m√°s peque√±as, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CIFAR-10</a> .  La primera de estas t√©cnicas es un comienzo c√°lido, sembrando la poblaci√≥n evolutiva original con arquitecturas de tipo transformador en lugar de modelos aleatorios.  Esto ayuda a concentrar las b√∫squedas en el √°rea obviamente fuerte del espacio de b√∫squeda, lo que nos permite encontrar r√°pidamente los mejores modelos. <br><br>  La segunda t√©cnica es un nuevo m√©todo desarrollado por nosotros llamado Progressive Dynamic Hurdles (PDH).  Este algoritmo complementa la b√∫squeda evolutiva, permiti√©ndole asignar m√°s recursos a los candidatos m√°s fuertes, a diferencia de trabajos anteriores, donde a cada modelo de candidato en el NAS se le asign√≥ la misma cantidad de recursos.  PDH nos permite terminar de evaluar un modelo antes si es terriblemente malo, al tiempo que recompensa arquitecturas prometedoras con muchos recursos. <br><br><h2>  Transformador evolucionado </h2><br>  Con estos m√©todos, realizamos una b√∫squeda NAS a gran escala en nuestra tarea de traducci√≥n y descubrimos extraterrestres.  Como la mayor√≠a de las arquitecturas de redes neuronales del tipo "secuencia a secuencia" (secuencia a secuencia, seq2seq), tiene un codificador que codifica la secuencia de entrada en los insertos, y un decodificador que usa estos insertos para crear la secuencia de salida.  En el caso de una traducci√≥n, la secuencia de entrada es una oferta de traducci√≥n, y la secuencia de salida es una traducci√≥n. <br><br>  La caracter√≠stica m√°s interesante de los ET son las capas convolucionales en la parte inferior de los m√≥dulos tanto del codificador como del decodificador, agregadas de manera similar a la ramificaci√≥n en ambos lugares (es decir, las entradas pasan por dos capas convolucionales diferentes antes de plegarse). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/62d/1d1/b8a62d1d155203861756f0960becaaf0.png"><br>  <i>Comparaci√≥n de la arquitectura del codificador convencional y los codificadores ET.</i>  <i>Preste atenci√≥n a la estructura convolucional de ramificaci√≥n en la parte inferior del m√≥dulo, formada independientemente tanto en el codificador como en el decodificador.</i>  <i>El decodificador se describe en detalle en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestro trabajo</a> .</i> <br><br>  Esto es especialmente interesante ya que el codificador y el decodificador durante el NAS no comparten arquitecturas entre s√≠, y la utilidad de esta arquitectura se descubri√≥ de forma independiente en el codificador y el decodificador, que habla a favor de dicho esquema.  Si el transformador original depend√≠a completamente de la atenci√≥n prestada a los mismos datos que √©l mismo gener√≥ [auto-atenci√≥n], ET es un h√≠brido que aprovecha tanto la auto atenci√≥n como la gran convoluci√≥n. <br><br><h2>  Puntaje ET </h2><br>  Para probar la efectividad de esta nueva arquitectura, primero la comparamos con el transformador original, que trabaj√≥ con la tarea de traducir del ingl√©s al alem√°n, que usamos durante la b√∫squeda.  Descubrimos que ET tiene los mejores indicadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BLEU</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conectividad</a> en todos los tama√±os de par√°metros, y la mayor ganancia de tama√±o es comparable a los dispositivos m√≥viles (~ 7 millones de par√°metros), lo que indica el uso eficiente de los par√°metros.  En tama√±os m√°s grandes, ET logra resultados de vanguardia en WMT '14 En-De con un BLEU de 29.8 y un SacreBLEU de 29.2. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1a2/4c4/c60/1a24c4c6085c167a5398fdcea218f75c.png"></div><br>  <i>Comparaci√≥n de ET y el transformador original en WMT'14 En-De con diferentes vol√∫menes.</i>  <i>La mayor ventaja se logra con tama√±os peque√±os, mientras que ET muestra un buen rendimiento en tama√±os m√°s grandes, por delante del transformador m√°s grande con un 37,6% menos de par√°metros (los modelos comparables est√°n en c√≠rculos).</i> <br><br>  Para verificar la posibilidad de generalizaci√≥n, comparamos ET con un transformador en problemas adicionales de procesamiento del lenguaje natural.  Primero, verificamos las traducciones para diferentes pares de idiomas, y encontramos que la efectividad de ET es mayor, y su separaci√≥n es aproximadamente la misma que la demostrada en la traducci√≥n ingl√©s-alem√°n;  y nuevamente, gracias al uso eficiente de los par√°metros, la mayor brecha se observa en los modelos de tama√±o mediano.  Tambi√©n comparamos los decodificadores de ambos modelos en el modelado de idiomas en <a href="">LM1B</a> , y vimos una mejora significativa en la conectividad. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c5/c96/1b6/6c5c961b6a09ba83905d9f886c9063ea.png"><br><br><h2>  Planes futuros </h2><br>  Estos resultados son el primer paso para explorar la aplicaci√≥n de b√∫squeda de arquitectura para modelos de distribuci√≥n directa secuencial.  ET se distribuye como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo abierto</a> en el marco del proyecto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://www.google.com/url%3Fq%3D">Tensor2Tensor</a> , donde se puede utilizar en cualquier problema consecutivo.  Para mejorar la reproducibilidad, tambi√©n abrimos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el c√≥digo de espacio de b√∫squeda</a> que usamos en nuestra b√∫squeda, y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colab</a> con la implementaci√≥n PDH.  ¬°Esperamos los resultados de la comunidad de investigaci√≥n, armados con nuevos modelos, y esperamos que otros puedan tomar estas nuevas t√©cnicas de b√∫squeda como base! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/460099/">https://habr.com/ru/post/460099/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../460087/index.html">Clasificaci√≥n piramidal (HeapSort)</a></li>
<li><a href="../460089/index.html">Actualizaci√≥n segura de Zimbra Collaboration Suite</a></li>
<li><a href="../460091/index.html">Impresi√≥n directa en camisetas con Epson SureColor SC - F y su diferencia con la serigraf√≠a, la calcoman√≠a y la sublimaci√≥n</a></li>
<li><a href="../460095/index.html">Atrapado una prohibici√≥n para fork deepNude en gitlab.com</a></li>
<li><a href="../460097/index.html">The Matrix lo tiene a usted: una descripci√≥n general de los proyectos que utilizan MITER ATT & CK</a></li>
<li><a href="../460101/index.html">Operaci√≥n XSS basada en cookies | $ 2300 Bug Bounty story</a></li>
<li><a href="../460107/index.html">Sistema ISP, perdona y adi√≥s! Por qu√© y c√≥mo escribimos nuestro panel de control del servidor</a></li>
<li><a href="../460109/index.html">Angular: cuando necesita ver la aplicaci√≥n, pero el backend a√∫n no est√° listo</a></li>
<li><a href="../460111/index.html">Versi√≥n actualizada de SAP Business One 9.3: lo que ha cambiado</a></li>
<li><a href="../460113/index.html">Algunas historias de la vida de JSOC CERT, o an√°lisis forense de Unbanal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>