<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüî¨ ‚òπÔ∏è üè¥ ¬øAdministrador sin brazos = hiperconvergencia? üçæ ü•´ üçä</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este es un mito que es bastante com√∫n en el campo del hardware del servidor. En la pr√°ctica, las soluciones hiperconvergentes (cuando est√°n todas junt...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øAdministrador sin brazos = hiperconvergencia?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croc/blog/471508/"><img src="https://habrastorage.org/webt/_a/wu/gy/_awugy9unrbypap90hlhc86-maa.png"><br><img src="https://habrastorage.org/webt/_k/kx/sa/_kkxsa4xs7rgz-fzktgy10mdeuc.png"><br><br>  Este es un mito que es bastante com√∫n en el campo del hardware del servidor.  En la pr√°ctica, las soluciones hiperconvergentes (cuando est√°n todas juntas) necesitan mucho para qu√©.  Hist√≥ricamente, las primeras arquitecturas fueron desarrolladas por Amazon y Google para sus servicios.  Entonces, la idea era hacer una granja inform√°tica de los mismos nodos, cada uno de los cuales tiene sus propios discos.  Todo esto fue combinado por un software de formaci√≥n de sistemas (hipervisor) y ya estaba dividido en m√°quinas virtuales.  La tarea principal es un m√≠nimo esfuerzo para mantener un nodo y un m√≠nimo de problemas de escala: acabamos de comprar otros mil o dos de los mismos servidores y nos conectamos cerca.  En la pr√°ctica, estos son casos aislados, y mucho m√°s a menudo estamos hablando de un n√∫mero menor de nodos y una arquitectura ligeramente diferente. <br><br>  Pero la ventaja sigue siendo la misma: la incre√≠ble facilidad de escala y control.  Menos: diferentes tareas consumen recursos de manera diferente, y en alg√∫n lugar habr√° muchos discos locales, en alg√∫n lugar habr√° poca RAM, y as√≠ sucesivamente, es decir, con diferentes tipos de tareas, la utilizaci√≥n de recursos disminuir√°. <br><br>  Result√≥ que pagas 10-15% m√°s para facilitar la configuraci√≥n.  Esto caus√≥ el mito principal.  Buscamos durante mucho tiempo d√≥nde se aplicar√° la tecnolog√≠a de manera √≥ptima, y ‚Äã‚Äãla encontramos.  El hecho es que Tsiska no ten√≠a sus propios sistemas de almacenamiento, pero quer√≠an un mercado completo de servidores.  E hicieron Cisco Hyperflex, una soluci√≥n de almacenamiento local en nodos. <br><br>  Y esto de repente result√≥ ser una muy buena soluci√≥n para los centros de datos de respaldo (recuperaci√≥n ante desastres).  Por qu√© y c√≥mo, ahora lo dir√©.  Y mostrar√© pruebas de cl√∫ster. <a name="habracut"></a><br><br><h3>  A donde </h3><br>  La hiperconvergencia es: <br><br><ol><li>  Transfiera discos a nodos de c√≥mputo. </li><li>  Integraci√≥n completa del subsistema de almacenamiento con el subsistema de virtualizaci√≥n. </li><li>  Transferencia / integraci√≥n con el subsistema de red. </li></ol><br>  Tal combinaci√≥n le permite implementar muchas caracter√≠sticas de los sistemas de almacenamiento a nivel de virtualizaci√≥n y todo desde una ventana de control. <br><br>  En nuestra empresa, los proyectos para dise√±ar centros de datos de respaldo tienen una gran demanda, y a menudo se elige la soluci√≥n hiperconvergente debido a la gran cantidad de opciones de replicaci√≥n (hasta el cl√∫ster de metro) listas para usar. <br><br>  En el caso de los centros de datos de respaldo, generalmente se trata de una instalaci√≥n remota en un sitio al otro lado de la ciudad o en otra ciudad en general.  Le permite restaurar sistemas cr√≠ticos en caso de una falla parcial o completa del centro de datos principal.  Los datos de ventas se replican constantemente all√≠, y esta replicaci√≥n puede ser a nivel de aplicaci√≥n o a nivel de dispositivo de bloque (SHD). <br><br>  As√≠ que ahora hablar√© sobre el dispositivo y las pruebas del sistema, y ‚Äã‚Äãluego sobre un par de escenarios de la vida real con datos sobre ahorros. <br><br><h3>  Pruebas </h3><br>  Nuestra copia consta de cuatro servidores, cada uno de los cuales tiene 10 discos SSD por 960 GB.  Hay un disco dedicado para almacenar en cach√© las operaciones de escritura y el almacenamiento de la m√°quina virtual de servicio.  La soluci√≥n en s√≠ es la cuarta versi√≥n.  El primero es francamente crudo (a juzgar por las revisiones), el segundo est√° h√∫medo, el tercero ya es bastante estable, y este puede llamarse un lanzamiento despu√©s del final de las pruebas beta para el p√∫blico en general.  Durante la prueba de los problemas que no vi, todo funciona como un reloj. <br><br><div class="spoiler">  <b class="spoiler_title">Cambios en v4</b> <div class="spoiler_text">  Se corrigieron un mont√≥n de errores. <br><br>  Inicialmente, la plataforma solo pod√≠a funcionar con el hipervisor VMware ESXi y admit√≠a una peque√±a cantidad de nodos.  Adem√°s, el proceso de implementaci√≥n no siempre finaliz√≥ con √©xito, tuve que reiniciar algunos pasos, hubo problemas para actualizar desde versiones anteriores, los datos en la GUI no siempre se mostraban correctamente (aunque todav√≠a no estoy contento con mostrar gr√°ficos de rendimiento), a veces hubo problemas en la interfaz con la virtualizaci√≥n . <br><br>  Ahora que se han reparado todas las llagas de los ni√±os, HyperFlex puede hacer ESXi y Hyper-V, y esto es posible: <br><br><ol><li>  Crear un cl√∫ster extendido. </li><li>  Crear un cl√∫ster para oficinas sin usar Fabric Interconnect, de dos a cuatro nodos (solo compramos servidores). </li><li>  Capacidad para trabajar con almacenamiento externo. </li><li>  Soporte para contenedores y Kubernetes. </li><li>  Creaci√≥n de zonas de accesibilidad. </li><li>  Integraci√≥n con VMware SRM, si la funcionalidad incorporada no es adecuada. </li></ol><br></div></div><br>  La arquitectura no es muy diferente de las decisiones de los principales competidores, no crearon una bicicleta.  Todo funciona en la plataforma de virtualizaci√≥n VMware o Hyper-V.  Hardware alojado en servidores propietarios de Cisco UCS.  Hay quienes odian la plataforma por la relativa complejidad de la configuraci√≥n inicial, muchos botones, un sistema no trivial de plantillas y dependencias, pero tambi√©n hay quienes aprendieron Zen, se inspiraron en la idea y ya no quieren trabajar con otros servidores. <br><br>  Consideraremos la soluci√≥n espec√≠ficamente para VMware, ya que la soluci√≥n se cre√≥ originalmente para √©l y tiene m√°s funcionalidad, se agreg√≥ Hyper-V en el camino para mantenerse al d√≠a con los competidores y cumplir con las expectativas del mercado. <br><br>  Hay un grupo de servidores llenos de discos.  Hay discos para el almacenamiento de datos (SSD o HDD, a su gusto y necesidades), hay un disco SSD para el almacenamiento en cach√©.  Cuando los datos se escriben en el almac√©n de datos, los datos se guardan en la capa de almacenamiento en cach√© (disco SSD dedicado y servicio de RAM de VM).  Paralelamente, el bloque de datos se env√≠a a los nodos en el cl√∫ster (el n√∫mero de nodos depende del factor de replicaci√≥n del cl√∫ster).  Despu√©s de la confirmaci√≥n de todos los nodos sobre la grabaci√≥n exitosa, la confirmaci√≥n de la grabaci√≥n se env√≠a al hipervisor y luego a la VM.  Los datos grabados en segundo plano se deduplican, comprimen y escriben en discos de almacenamiento.  Al mismo tiempo, siempre se escribe un bloque grande en los discos de almacenamiento y de forma secuencial, lo que reduce la carga en los discos de almacenamiento. <br><br>  La deduplicaci√≥n y la compresi√≥n siempre est√°n activadas y no se pueden desactivar.  Los datos se leen directamente desde discos de almacenamiento o desde la memoria cach√© de RAM.  Si se usa una configuraci√≥n h√≠brida, la lectura tambi√©n se almacena en cach√© en el SSD. <br><br>  Los datos no est√°n vinculados a la ubicaci√≥n actual de la m√°quina virtual y se distribuyen de manera uniforme entre los nodos.  Este enfoque le permite cargar por igual todas las unidades e interfaces de red.  La desventaja obvia es: no podemos minimizar el retraso de lectura, ya que no hay garant√≠a de disponibilidad de datos a nivel local.  Pero creo que este es un sacrificio insignificante en comparaci√≥n con las ventajas recibidas.  Adem√°s, los retrasos en la red han alcanzado tales valores que pr√°cticamente no afectan el resultado general. <br><br>  Para toda la l√≥gica del subsistema de disco, es responsable una VM de servicio especial del controlador Cisco HyperFlex Data Platform, que se crea en cada nodo de almacenamiento.  En nuestra configuraci√≥n de VM de servicio, se asignaron ocho vCPU y 72 GB de RAM, que no es tan peque√±o.  Perm√≠tame recordarle que el host en s√≠ tiene 28 n√∫cleos f√≠sicos y 512 GB de RAM. <br><br>  El servicio VM tiene acceso a discos f√≠sicos directamente al reenviar el controlador SAS a la VM.  La comunicaci√≥n con el hipervisor se produce a trav√©s de un m√≥dulo especial IOVisor, que intercepta las operaciones de E / S, y utiliza un agente que le permite transferir comandos a la API del hipervisor.  El agente es responsable de trabajar con instant√°neas y clones de HyperFlex. <br><br>  En el hipervisor, los recursos de disco se montan como una bola NFS o SMB (dependiendo del tipo de hipervisor, adivine cu√°l).  Y bajo el cap√≥, este es un sistema de archivos distribuido que le permite agregar caracter√≠sticas de sistemas de almacenamiento completos para adultos: asignaci√≥n de volumen delgado, compresi√≥n y deduplicaci√≥n, instant√°neas que utilizan la tecnolog√≠a Redirect-on-Write, replicaci√≥n s√≠ncrona / as√≠ncrona. <br><br>  Service VM proporciona acceso a la interfaz WEB de la gesti√≥n del subsistema HyperFlex.  Hay integraci√≥n con vCenter, y la mayor√≠a de las tareas diarias se pueden realizar desde √©l, pero los almacenes de datos, por ejemplo, son m√°s convenientes para cortar desde una c√°mara web separada si ya ha cambiado a una interfaz HTML5 r√°pida, o usar un cliente Flash completo con integraci√≥n completa.  En la c√°mara web de servicio, puede ver el rendimiento y el estado detallado del sistema. <br><br><img src="https://habrastorage.org/webt/o0/bj/od/o0bjod1zrf25ubnrtis5q4e1ywc.png"><br><br>  Hay otro tipo de nodo en un cl√∫ster: nodos computacionales.  Pueden ser servidores rack o blade sin unidades integradas.  En estos servidores, puede ejecutar m√°quinas virtuales cuyos datos se almacenan en servidores con discos.  Desde el punto de vista del acceso a los datos, no hay diferencia entre los tipos de nodos, porque la arquitectura implica la abstracci√≥n de la ubicaci√≥n f√≠sica de los datos.  La relaci√≥n m√°xima de nodos de c√°lculo y nodos de almacenamiento es 2: 1. <br><br>  El uso de nodos computacionales aumenta la flexibilidad al escalar los recursos del cl√∫ster: no tenemos que comprar nodos con discos si solo necesitamos CPU / RAM.  Adem√°s, podemos agregar una canasta blade y ahorrar espacio en el servidor del rack. <br><br>  Como resultado, tenemos una plataforma hiperconvergente con las siguientes caracter√≠sticas: <br><br><ul><li>  Hasta 64 nodos en un cl√∫ster (hasta 32 nodos de almacenamiento). </li><li>  El n√∫mero m√≠nimo de nodos en un cl√∫ster es tres (dos para un cl√∫ster Edge). </li><li>  Mecanismo de redundancia de datos: duplicaci√≥n con factor de replicaci√≥n 2 y 3. </li><li>  Metro cluster. </li><li>  Replicaci√≥n as√≠ncrona de VM a otro cl√∫ster HyperFlex. </li><li>  Orquestaci√≥n de cambio de m√°quinas virtuales a un centro de datos remoto. </li><li>  Instant√°neas nativas utilizando la tecnolog√≠a Redirect-on-Write. </li><li>  Hasta 1 PB de espacio utilizable con factor de replicaci√≥n 3 y sin deduplicaci√≥n.  No tenemos en cuenta el factor de replicaci√≥n 2, ya que esta no es una opci√≥n para ventas serias. </li></ul><br>  Otra gran ventaja es la facilidad de administraci√≥n y despliegue.  Todas las complejidades de la configuraci√≥n de servidores UCS son manejadas por una VM especializada preparada por ingenieros de Cisco. <br><br><h3>  Configuraci√≥n del banco de pruebas: </h3><br><ul><li>  2 x Cisco UCS Fabric Interconnect 6248UP como un cl√∫ster de gesti√≥n y componentes de red (48 puertos que funcionan en modo Ethernet 10G / FC 16G). </li><li>  Cuatro servidores Cisco UCS HXAF240 M4. </li></ul><br>  Caracter√≠sticas del servidor: <br><p></p><div class="scrollable-table"><table><tbody><tr><td><br><p>  CPU </p><br></td><td><br><p>  2 x Intel¬Æ Xeon¬Æ E5-2690 v4 </p><br></td></tr><tr><td><br><p>  RAM </p><br></td><td><br><p>  16 x 32 GB DDR4-2400 MHz RDIMM / PC4-19200 / doble rango / x4 / 1.2v </p><br></td></tr><tr><td><br><p>  Red </p><br></td><td><br><p>  UCSC-MLOM-CSC-02 (VIC 1227).  2 x 10G Ethernet </p><br></td></tr><tr><td><br><p>  Almacenamiento hba </p><br></td><td><br><p>  Cisco 12G Modular SAS Pass a trav√©s del controlador </p><br></td></tr><tr><td><br><p>  Discos de almacenamiento </p><br></td><td><br><p>  1 x SSD Intel S3520 120 GB, 1 x SSD Samsung MZ-IES800D, 10 x SSD Samsung PM863a 960 GB </p><br></td></tr></tbody></table></div><br><br><div class="spoiler">  <b class="spoiler_title">M√°s opciones de configuraci√≥n</b> <div class="spoiler_text">  Adem√°s del hierro seleccionado, las siguientes opciones est√°n actualmente disponibles: <br><br><ul><li>  HXAF240c M5. </li><li>  Una o dos CPU que van desde Intel Silver 4110 hasta Intel Platinum I8260Y.  La segunda generaci√≥n est√° disponible. </li><li>  24 ranuras de memoria, listones de 16 GB RDIMM 2600 a 128 GB LRDIMM 2933. </li><li>  De 6 a 23 discos para datos, un disco de almacenamiento en cach√©, un sistema y un disco de arranque. </li></ul><br>  <b>Unidades de capacidad</b> <br><br><ul><li>  HX-SD960G61X-EV 960GB 2.5 pulgadas Enterprise Value 6G SSD SATA (1X resistencia) SAS 960 GB. </li><li>  HX-SD38T61X-EV 3.8TB 2.5 pulgadas Enterprise Value 6G SATA SSD (1X resistencia) SAS 3.8 TB. </li><li>  Controladores de cach√© </li><li>  HX-NVMEXPB-I375 375GB 2.5 pulgadas Intel Optane Drive, Extreme Perf &amp; Endurance. </li><li>  HX-NVMEHW-H1600 * 1.6TB 2.5 pulgadas Ent.  Perf  NVMe SSD (resistencia 3X) NVMe 1.6 TB. </li><li>  HX-SD400G12TX-EP 400GB 2.5 pulgadas Ent.  Perf  SSD SAS 12G (resistencia 10X) SAS 400 GB. </li><li>  HX-SD800GBENK9 ** 800GB 2.5 pulgadas Ent.  Perf  SSD SED SAS 12G (resistencia 10X) SAS 800 GB. </li><li>  HX-SD16T123X-EP 1.6TB 2.5 pulgadas Enterprise performance 12G SAS SSD (resistencia 3X). </li></ul><br>  <b>Sistema / Log Drives</b> <br><br><ul><li>  HX-SD240GM1X-EV 240GB SSD SATA Enterprise Value 6G de 2.5 pulgadas (Requiere actualizaci√≥n). </li></ul><br>  <b>Controladores de arranque</b> <br><br><ul><li>  HX-M2-240GB 240GB SATA M.2 SSD SATA 240 GB. </li></ul><br></div></div><br>  Conexi√≥n a una red en puertos Ethernet 40G, 25G o 10G. <br><br>  Como FI puede ser HX-FI-6332 (40G), HX-FI-6332-16UP (40G), HX-FI-6454 (40G / 100G). <br><br><h3>  Probarse </h3><br>  Para probar el subsistema de disco, utilic√© HCIBench 2.2.1.  Esta es una utilidad gratuita que le permite automatizar la creaci√≥n de carga desde m√∫ltiples m√°quinas virtuales.  La carga en s√≠ es generada por fio regular. <br><br>  Nuestro cl√∫ster consta de cuatro nodos, factor de replicaci√≥n 3, todas las unidades Flash. <br><br>  Para las pruebas, cre√© cuatro almacenes de datos y ocho m√°quinas virtuales.  Para las pruebas de escritura, se supone que el disco de almacenamiento en cach√© no est√° lleno. <br><br>  Los resultados de la prueba son los siguientes: <br><div class="scrollable-table"><table><tbody><tr><td></td><td colspan="5"><br><p>  100% Lectura 100% Aleatorio </p><br></td><td colspan="5"><br><p>  0% Lectura 100% Aleatorio </p><br></td></tr><tr><td><br><p>  Bloque / profundidad de cola </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td></tr><tr><td><br><p>  4K </p><br></td><td><br><p>  0,59 ms 213804 IOPS </p><br></td><td><br><p>  0,84 ms 303540 IOPS </p><br></td><td><br><p>  1.36ms 374348 IOPS </p><br></td><td><br><p>  2,47 ms 414116 IOPS </p><br></td><td><br><p>  <b>4.86ms 420180 IOPS</b> </p><br></td><td><br><p>  2,22 ms 57408 IOPS </p><br></td><td><br><p>  3,09 ms 82744 IOPS </p><br></td><td><br><p>  5.02 ms 101824 IPOS </p><br></td><td><br><p>  8,75 ms 116912 IOPS </p><br></td><td><br><p>  <b>17.2 ms 118592 IOPS</b> </p><br></td></tr><tr><td><br><p>  8K </p><br></td><td><br><p>  0,67 ms 188416 IOPS </p><br></td><td><br><p>  0,93 ms 273280 IOPS </p><br></td><td><br><p>  1,7 ms 299932 IOPS </p><br></td><td><br><p>  2,72 ms 376,484 IOPS </p><br></td><td><br><p>  <b>5,47 ms 373,176 IOPS</b> </p><br></td><td><br><p>  3,1 ms 41148 IOPS </p><br></td><td><br><p>  4,7 ms 54396 IOPS </p><br></td><td><br><p>  7,09 ms 72192 IOPS </p><br></td><td><br><p>  <b>12,77 ms 80.132 IOPS</b> </p><br></td><td></td></tr><tr><td><br><p>  16K </p><br></td><td><br><p>  0,77 ms 164116 IOPS </p><br></td><td><br><p>  1,12 ms 228328 IOPS </p><br></td><td><br><p>  1.9 ms 268140 IOPS </p><br></td><td><br><p>  <b>3,96 ms 258480 IOPS</b> </p><br></td><td></td><td><br><p>  3,8 ms 33640 IOPS </p><br></td><td><br><p>  6,97 ms 36696 IOPS </p><br></td><td><br><p>  <b>11,35 ms 45060 IOPS</b> </p><br></td><td></td><td></td></tr><tr><td><br><p>  32K </p><br></td><td><br><p>  1,07 ms 119292 IOPS </p><br></td><td><br><p>  1,79 ms 142888 IOPS </p><br></td><td><br><p>  <b>3,56 ms 143760 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  7,17 ms 17810 IOPS </p><br></td><td><br><p>  <b>11,96 ms 21396 IOPS</b> </p><br></td><td></td><td></td><td></td></tr><tr><td><br><p>  64K </p><br></td><td><br><p>  1,84 ms 69440 IOPS </p><br></td><td><br><p>  3,6 ms 71008 IOPS </p><br></td><td><br><p>  <b>7,26 ms 70404 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  <b>11,37 ms 11248 IOPS</b> </p><br></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><br>  <i>Se indican valores en negrita, despu√©s de lo cual no hay aumento en la productividad, a veces incluso la degradaci√≥n es visible.</i>  <i>Debido al hecho de que descansamos en el rendimiento de la red / controladores / unidades.</i> <br><br><ul><li>  Lectura secuencial de 4432 MB / s. </li><li>  Escritura secuencial de 804 MB / s. </li><li>  Si falla un controlador (m√°quina virtual o falla del host), la reducci√≥n del rendimiento se duplica. </li><li>  Si la unidad de almacenamiento falla, la reducci√≥n es 1/3.  Rebild disk toma el 5% de los recursos de cada controlador. </li></ul><br>  En un bloque peque√±o, nos encontramos con el rendimiento del controlador (m√°quina virtual), su CPU est√° cargada al 100%, al tiempo que aumenta el bloque que corremos en el ancho de banda del puerto.  10 Gbps no son suficientes para desbloquear el potencial del sistema AllFlash.  Desafortunadamente, los par√°metros del soporte de demostraci√≥n proporcionado no permiten verificar el trabajo a 40 Gb / s. <br><br>  En mi impresi√≥n de las pruebas y el estudio de la arquitectura, debido al algoritmo que coloca los datos entre todos los hosts, obtenemos un rendimiento predecible escalable, pero esto tambi√©n es una limitaci√≥n al leer, ya que ser√≠a posible exprimir m√°s de los discos locales y m√°s, aqu√≠ para guardar una red m√°s productiva, por ejemplo, hay 40 Gbps FI disponibles. <br><br>  Adem√°s, un disco para el almacenamiento en cach√© y la deduplicaci√≥n puede ser una limitaci√≥n; de hecho, en este soporte podemos escribir en cuatro discos SSD.  Ser√≠a genial poder aumentar el n√∫mero de discos en cach√© y ver la diferencia. <br><br><h3>  Uso real </h3><br>  Se pueden usar dos enfoques para organizar un centro de datos de respaldo (no consideramos colocar el respaldo en un sitio remoto): <br><br><ol><li>  Pasivo activo  Todas las aplicaciones est√°n alojadas en el centro de datos principal.  La replicaci√≥n es s√≠ncrona o as√≠ncrona.  En caso de una ca√≠da en el centro de datos principal, debemos activar el de respaldo.  Esto se puede hacer manualmente / scripts / aplicaciones de orquestaci√≥n.  Aqu√≠ obtenemos un RPO acorde con la frecuencia de replicaci√≥n, y el RTO depende de la reacci√≥n y las habilidades del administrador y la calidad del desarrollo / depuraci√≥n del plan de conmutaci√≥n. </li><li>  Activo activo  En este caso, solo est√° presente la replicaci√≥n sincr√≥nica, la disponibilidad de los centros de datos est√° determinada por un qu√≥rum / √°rbitro, ubicado estrictamente en la tercera plataforma.  RPO = 0, y RTO puede alcanzar 0 (si la aplicaci√≥n lo permite) o igual al tiempo de conmutaci√≥n por error de un nodo en un cl√∫ster de virtualizaci√≥n.  En el nivel de virtualizaci√≥n, se crea un cl√∫ster ampliado (Metro) que requiere almacenamiento activo-activo. </li></ol><br>  Por lo general, vemos con los clientes una arquitectura ya implementada con almacenamiento cl√°sico en el centro de datos principal, por lo que dise√±amos otra para la replicaci√≥n.  Como mencion√©, Cisco HyperFlex ofrece replicaci√≥n asincr√≥nica y la creaci√≥n de un cl√∫ster de virtualizaci√≥n extendido.  Al mismo tiempo, no necesitamos un sistema de almacenamiento dedicado de rango medio o superior con las costosas funciones de replicaci√≥n y acceso de datos activo-activo en dos sistemas de almacenamiento. <br><br>  <b>Escenario 1:</b> Tenemos centros de datos primarios y de respaldo, una plataforma de virtualizaci√≥n en VMware vSphere.  Todos los sistemas productivos se encuentran principalmente en el centro de datos, y la replicaci√≥n de la m√°quina virtual se realiza a nivel de hipervisor, esto permitir√° no mantener las m√°quinas virtuales encendidas en el centro de datos de respaldo.  Replicamos bases de datos y aplicaciones especiales con herramientas integradas y mantenemos las m√°quinas virtuales encendidas.  Si el centro de datos principal falla, iniciamos el sistema en el centro de datos de respaldo.  Creemos que tenemos alrededor de 100 m√°quinas virtuales.  Mientras el centro de datos principal est√© funcionando, los entornos de prueba y otros sistemas se pueden iniciar en el centro de datos de respaldo, que se puede deshabilitar si se cambia el centro de datos principal.  Tambi√©n es posible que usemos la replicaci√≥n bidireccional.  Desde el punto de vista del equipo, nada cambiar√°. <br><br>  En el caso de la arquitectura cl√°sica, colocaremos un sistema de almacenamiento h√≠brido en cada centro de datos con acceso a trav√©s de FibreChannel, desgarro, deduplicaci√≥n y compresi√≥n (pero no en l√≠nea), 8 servidores por sitio, 2 conmutadores FibreChannel y Ethernet 10G.  Para el control de replicaci√≥n y conmutaci√≥n en una arquitectura cl√°sica, podemos usar herramientas VMware (Replication + SRM) o herramientas de terceros que ser√°n un poco m√°s baratas y, a veces, m√°s convenientes. <br><br>  La figura muestra un diagrama. <br><br><img src="https://habrastorage.org/webt/ej/an/92/ejan92jvmtt1ejtbtd1eyytngw8.png"><br><br>  Si usa Cisco HyperFlex, obtiene la siguiente arquitectura: <br><br><img src="https://habrastorage.org/webt/9w/xs/hl/9wxshlz45c53uyitqulmrmwlraq.png"><br><br>  Para HyperFlex, utilic√© servidores con grandes recursos de CPU / RAM, como  parte de los recursos ir√°n a la VM del controlador HyperFlex, incluso he recargado un poco la configuraci√≥n de HyperFlex en la CPU y la memoria, para no jugar junto a Cisco y garantizar recursos para el resto de las VM.  Pero podemos rechazar los conmutadores FibreChannel, y no necesitamos puertos Ethernet para cada servidor, el tr√°fico local se conmuta dentro de FI. <br><br>  El resultado es la siguiente configuraci√≥n para cada centro de datos: <br><div class="scrollable-table"><table><tbody><tr><td><br><p>  Servidores </p><br></td><td><br><p>  8 x 1U Server (384 GB de RAM, 2 x Intel Gold 6132, FC HBA) </p><br></td><td><br><p>  8 x HX240C-M5L (512 GB de RAM, 2 x Intel Gold 6150, SSD de 3.2 GB, 10 x 6 TB NL-SAS) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  Almacenamiento h√≠brido con FC Front-End (SSD de 20 TB, NL-SAS de 130 TB) </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  2 x conmutador Ethernet 10G 12 puertos </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  2 x conmutador FC 32 / 16Gb 24 puertos </p><br></td><td><br><p>  2 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Licencias </p><br></td><td><br><p>  VMware Ent Plus </p><br><p>  Replicaci√≥n y / u orquestaci√≥n de VM </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  Para Hyperflex, no promet√≠ licencias de software de replicaci√≥n, ya que est√° disponible de forma inmediata con nosotros. <br><br>  Para la arquitectura cl√°sica, tom√© un vendedor que se estableci√≥ como un fabricante de calidad y econ√≥mico.  Para ambas opciones utilic√© un est√°ndar para un pat√≠n de soluci√≥n espec√≠fico, a la salida obtuve precios reales. <br><br>  La soluci√≥n en Cisco HyperFlex fue un 13% m√°s barata. <br><br>  <b>Escenario 2:</b> crear dos centros de datos activos.  En este escenario, dise√±amos un cl√∫ster extendido en VMware. <br><br>  La arquitectura cl√°sica consiste en servidores de virtualizaci√≥n, SAN (protocolo FC) y dos sistemas de almacenamiento que pueden leer y escribir en el que se extiende entre ellos.  En cada SHD ponemos una capacidad √∫til para la cerradura. <br><br><img src="https://habrastorage.org/webt/lm/1i/yu/lm1iyuqivylf7dl0xrmff21swlw.png"><br><br>  En HyperFlex, simplemente creamos un Stretch Cluster con el mismo n√∫mero de nodos en ambos sitios.  En este caso, se utiliza el factor de replicaci√≥n 2 + 2. <br><br><img src="https://habrastorage.org/webt/e7/mq/sd/e7mqsd7codjtfbqefl40icsohpu.png"><br><br>  La siguiente configuraci√≥n ha resultado: <br><div class="scrollable-table"><table><tbody><tr><td></td><td><br><p>  Arquitectura clasica </p><br></td><td><br><p>  Hyperflex </p><br></td></tr><tr><td><br><p>  Servidores </p><br></td><td><br><p>  16 x 1U Server (384 GB de RAM, 2 x Intel Gold 6132, FC HBA, 2 x 10G NIC) </p><br></td><td><br><p>  16 x HX240C-M5L (512 GB de RAM, 2 x Intel Gold 6132, 1.6 TB NVMe, 12 x 3.8 TB SSD, VIC 1387) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  2 x almacenamiento AllFlash (SSD de 150 TB) </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  4 x conmutador Ethernet 10G 24 puertos </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  4 x conmutador FC 32 / 16Gb 24 puertos </p><br></td><td><br><p>  4 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Licencias </p><br></td><td><br><p>  VMware Ent Plus </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  En todos los c√°lculos, no tom√© en cuenta la infraestructura de red, los costos del centro de datos, etc., ser√°n los mismos para la arquitectura cl√°sica y para la soluci√≥n HyperFlex. <br><br>  Al costo, HyperFlex result√≥ ser un 5% m√°s caro.  Vale la pena se√±alar aqu√≠ que para los recursos de CPU / RAM, obtuve un sesgo para Cisco, porque en la configuraci√≥n llenaba los canales de los controladores de memoria de manera uniforme.   ,    ,   ,     ¬´  ¬ª,         .      ,      Cisco UCS     . <br><br>        SAN  , -  ,      (, ,   ‚Äî ),   (    ),  . <br><br>   ,         ‚Äî Cisco.        Cisco UCS,    ,  HyperFlex     ,    .          ,     .       : ¬´    ,  ?¬ª  ¬´  - ,     . !¬ª ‚Äî           , : ¬´    ¬ª   . <br><br><h3>  Referencias </h3><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> -</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">-   </a> </li><li>   ‚Äî StGeneralov@croc.ru </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/471508/">https://habr.com/ru/post/471508/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../471498/index.html">El mapa de las c√°maras de fijaci√≥n de carreteras se hace p√∫blico: ¬øalegrarse o llorar?</a></li>
<li><a href="../471500/index.html">Devoluci√≥n de llamada o "Aumento de la lealtad del cliente"</a></li>
<li><a href="../471502/index.html">Granja de ideas</a></li>
<li><a href="../471504/index.html">D√∫o bidimensional: creaci√≥n de heteroestructuras de borofeno-grafeno</a></li>
<li><a href="../471506/index.html">Redondeo correcto de n√∫meros decimales en c√≥digo binario</a></li>
<li><a href="../471512/index.html">28 de octubre, Ekaterimburgo - Comunicaci√≥n de calidad</a></li>
<li><a href="../471514/index.html">El t√≠tulo "Leer art√≠culos para usted". Enero - junio 2019</a></li>
<li><a href="../471516/index.html">Intel 665p: SSD con QLC NAND de 96 capas</a></li>
<li><a href="../471518/index.html">Apple en 2019 es Linux en 2000</a></li>
<li><a href="../471520/index.html">El libro "Tareas de inform√°tica cl√°sica en Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>