<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐂 🤟🏽 🤱🏾 Guide Kubernetes, Partie 2: Création et utilisation d'un cluster 👨🏾‍🤝‍👨🏽 🅿️ 🤱</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La dernière fois, nous avons examiné deux approches pour travailler avec des microservices. En particulier, l'un d'eux implique l'utilisation de conte...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Guide Kubernetes, Partie 2: Création et utilisation d'un cluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/438984/">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La dernière</a> fois, nous avons examiné deux approches pour travailler avec des microservices.  En particulier, l'un d'eux implique l'utilisation de conteneurs Docker, dans lesquels vous pouvez exécuter le code des microservices et des programmes auxiliaires.  Aujourd'hui, en utilisant nos images de conteneurs existantes, nous travaillerons avec Kubernetes. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/13/lv/dr/13lvdrwhhap-ouchegvweul0fg0.jpeg"></a> <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Présentation de Kubernetes</font> </h2><br>  Je promets, et je n'exagère pas du tout que lorsque vous lisez cet article, demandez-vous: "Pourquoi les Kubernetes ne s'appellent-ils pas Supernetes?" <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d76/9f9/e76/d769f9e7670a725759dd7415949177a0.png"></div><br>  <i><font color="#999999">Supernetes</font></i> <br><br>  Si vous lisez la partie précédente de ce document, vous savez que nous avons examiné beaucoup de choses liées à la préparation des demandes de conteneurisation et au travail avec les conteneurs Docker.  Il peut vous sembler que la chose la plus difficile vous attend maintenant, mais, en fait, ce dont nous allons parler ici est beaucoup plus simple que ce que nous avons déjà compris.  La seule raison pour laquelle l'apprentissage de Kubernetes peut sembler une tâche intimidante pour quelqu'un est la quantité d'informations supplémentaires dont vous avez besoin pour comprendre Kubernetes et l'utiliser efficacement.  Nous avons déjà discuté de toutes les "informations supplémentaires" nécessaires au succès du développement de Kubernetes. <br><br><h3>  <font color="#3AC1EF">HatQu'est-ce que Kubernetes?</font> </h3><br>  Dans la première partie de cet article, après avoir lancé des microservices dans des conteneurs, on vous a demandé de réfléchir à la question de la mise à l'échelle des applications conteneurisées. <br>  Je propose d'y réfléchir ensemble, sous forme de questions et réponses: <br><br>  <b>Question:</b> Comment les applications conteneurisées évoluent-elles? <br>  <b>Réponse:</b> lancez des conteneurs supplémentaires. <br><br>  <b>Question:</b> Et comment la charge est-elle répartie entre eux?  Que faire si un certain serveur est déjà utilisé au maximum et que le conteneur doit être déployé sur un autre serveur?  Comment trouver le moyen le plus efficace d'utiliser du matériel? <br>  <b>Réponse:</b> Alors ... je vais regarder sur Internet ... <br><br>  <b>Question:</b> Comment mettre à jour des programmes sans perturber le système?  Et, si la mise à jour contient une erreur, comment revenir à la version de travail de l'application? <br><br>  En fait, c'est la technologie Kubernetes qui donne des réponses valables à ces questions et à bien d'autres.  Je vais essayer de limiter la définition de Kubernetes à une phrase: "Kubernetes est un système de gestion de conteneurs qui résume l'infrastructure sous-jacente (l'environnement dans lequel les conteneurs s'exécutent)." <br><br>  Je pense que maintenant vous n'êtes pas particulièrement clair sur le concept de «gestion des conteneurs», même si nous l'avons déjà mentionné.  Ci-dessous, nous considérerons cette technologie dans la pratique.  Cependant, le concept «d'abstraire l'infrastructure de base» est d'abord rencontré.  Par conséquent, nous allons maintenant l'examiner. <br><br><h3>  <font color="#3AC1EF">▍ Abstraction des infrastructures de base</font> </h3><br>  Kubernetes permet aux applications de s'éloigner de l'infrastructure, nous donnant une API simple à laquelle vous pouvez envoyer des demandes.  Kubernetes essaie de répondre à ces demandes en utilisant toutes ses capacités.  Par exemple, dans une langue normale, une requête similaire peut être décrite comme suit: «Kubernetes, développez 4 conteneurs d'images X».  Après avoir reçu la commande, Kubernetes trouvera des nœuds qui ne sont pas trop occupés (ils sont également appelés "nœuds" - de l'anglais "nœud"), sur lesquels vous pouvez déployer de nouveaux conteneurs. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/735/b88/a2a/735b88a2a717e9c01bfc197f3c1b20fd.png"></div><br>  <i><font color="#999999">Demande de serveur API</font></i> <br><br>  Qu'est-ce que cela signifie pour le développeur?  Cela signifie qu'il n'a pas besoin de se soucier du nombre de nœuds, de l'endroit exact où les conteneurs sont lancés ou de la façon dont ils interagissent.  Il n'a pas à gérer l'optimisation matérielle ni à s'inquiéter des nœuds qui pourraient mal fonctionner (et quelque chose de similaire, selon la loi de Murphy, se produira certainement), car, si nécessaire, de nouveaux nœuds peuvent être ajoutés au cluster Kubernetes.  Si quelque chose ne va pas avec certains nœuds existants, Kubernetes déploiera des conteneurs sur les nœuds qui sont toujours dans un état sain. <br><br>  Une grande partie de ce qui est illustré dans la figure précédente vous est déjà familière.  Mais il y a aussi quelque chose de nouveau: <br><br><ul><li>  Serveur API  Faire des appels à ce serveur est le seul moyen d'interagir avec le cluster que nous avons, qu'il s'agisse de démarrer ou d'arrêter des conteneurs, de vérifier l'état du système, de travailler avec des journaux ou d'effectuer d'autres actions. </li><li>  Kubelet.  Il s'agit d'un agent qui surveille les conteneurs à l'intérieur du nœud et interagit avec le nœud principal. </li></ul><br>  Veuillez noter que dans quelques phrases précédentes, nous utilisons le terme «conteneur», mais ici, il serait plus correct d'utiliser le terme «pod».  Ces entités sont souvent appelées «pods» dans les publications en langue russe, et parfois «pods», dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> , clarifiant le concept de «pod», elles parlent de «troupeau de baleines» (pod de baleines) ou de «pod de pois» mais personne ne les appelle "troupeaux" ou "pods".  En parlant d'eux, nous utiliserons le mot «sous».  Maintenant, vous pouvez les considérer comme des conteneurs, nous parlerons plus en détail des pods ci-dessous. <br><br>  Nous nous arrêterons ici pour le moment, car nous pouvons parler de tout cela plus loin, et, en plus, il y a beaucoup de bons matériaux concernant la théorie de Kubernetes.  Par exemple, il s'agit d'une documentation officielle, bien qu'elle ne soit pas facile à lire, ou de livres comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">celui-ci</a> . <br><br><h3>  <font color="#3AC1EF">▍ Standardisation du travail avec les fournisseurs de services cloud</font> </h3><br>  Une autre force de Kubernetes réside dans le fait que cette technologie contribue à la standardisation du travail avec les prestataires de services cloud (Cloud Service Provider, CSP).  Ceci est une déclaration audacieuse.  Prenons l'exemple suivant.  Un spécialiste qui connaît bien Azure ou Google Cloud Platform doit travailler sur un projet conçu pour un environnement cloud complètement nouveau pour lui, avec lequel il n'est pas familier.  Dans cette situation, beaucoup de choses peuvent mal tourner.  Par exemple, les délais de livraison du projet peuvent être perturbés, la société cliente du projet peut avoir besoin de louer plus de ressources cloud que prévu, etc. <br><br>  Lors de l'utilisation de Kubernetes, un tel problème ne peut tout simplement pas se poser, car, quel que soit le fournisseur de services cloud dont nous parlons, travailler avec Kubernetes a toujours la même apparence.  Le développeur, dans un style déclaratif, indique au serveur API ce dont il a besoin, et Kubernetes travaille avec les ressources du système, ce qui permet au développeur d'ignorer les détails de la mise en œuvre de ce système. <br><br>  Attardez-vous un peu sur cette idée, car il s'agit d'une opportunité Kubernetes très puissante.  Pour les entreprises, cela signifie que leurs décisions ne sont pas liées à un CSP spécifique.  Si une entreprise trouve une meilleure offre sur le marché des services cloud, elle peut librement profiter de cette offre en passant à un nouveau fournisseur.  De plus, l'expérience acquise par les spécialistes de l'entreprise ne se perd nulle part. <br><br>  Parlons maintenant de l'utilisation pratique de Kubernetes <br><br><h2>  <font color="#3AC1EF">Pratique Kubernetes: Pods</font> </h2><br>  Nous avons configuré le lancement de microservices dans des conteneurs, le processus de configuration a été assez fastidieux, mais nous avons réussi à arriver à un système fonctionnel.  De plus, comme déjà mentionné, notre solution ne s'adapte pas bien et ne résiste pas aux pannes.  Nous allons résoudre ces problèmes avec Kubernetes.  Ensuite, nous amènerons notre système sous une forme correspondant au schéma suivant.  À savoir, les conteneurs seront gérés par Kubernetes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53d/19c/3ba/53d19c3bac2f8cdd66213c9b34e7b05b.png"></div><br>  <i><font color="#999999">Les microservices fonctionnent dans un cluster géré par Kubernetes</font></i> <br><br>  Ici, nous utiliserons Minikube pour le déploiement local du cluster et pour tester les capacités de Kubernetes, bien que tout ce que nous ferons ici puisse se faire à l'aide de plateformes cloud telles qu'Azure ou Google Cloud Platform. <br><br><h3>  <font color="#3AC1EF">▍Installation et démarrage de Minikube</font> </h3><br>  Pour installer Minikube, suivez les instructions de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> .  Lors de l'installation de Minikube, vous installerez également Kubectl.  Il s'agit d'un client qui permet de faire des demandes au serveur API Kubernetes. <br><br>  Pour démarrer Minikube, exécutez la commande de <code>minikube start</code> et, une fois celle-ci terminée, exécutez la commande <code>kubectl get nodes</code> .  Par conséquent, vous devriez voir quelque chose comme ceci: <br><br><pre> <code class="plaintext hljs">kubectl get nodes NAME       STATUS ROLES     AGE VERSION minikube   Ready &lt;none&gt;    11m v1.9.0</code> </pre> <br>  Minikube met à notre disposition un cluster composé d'un seul nœud.  Certes, cela nous convient assez bien.  Ceux qui travaillent avec Kubernetes n'ont pas à se soucier du nombre exact de nœuds dans le cluster, car Kubernetes vous permet de faire abstraction de ces détails. <br><br>  Parlons maintenant des pods. <br><br><h3>  <font color="#3AC1EF">▍Pods</font> </h3><br>  J'aime vraiment les conteneurs, et vous les aimez probablement aussi maintenant.  Pourquoi Kubernetes nous propose-t-il d'utiliser des pods, des entités qui sont les unités informatiques minimum déployables dans ce système?  Sous quelles fonctions exerce-t-il?  Le fait est que l'âtre peut comprendre un ou plusieurs conteneurs partageant le même temps d'exécution. <br><br>  Mais est-il nécessaire de réaliser, par exemple, deux conteneurs dans un foyer?  Comment dire ... Habituellement, il n'y a qu'un seul conteneur par conteneur, et c'est ce que nous allons faire.  Mais dans les cas où, par exemple, deux conteneurs ont besoin d'un accès partagé au même entrepôt de données, ou si une connexion est établie entre eux à l'aide de la technique de communication interprocessus, ou s'ils sont étroitement connectés pour une autre raison, tout cela peut être réalisé en les faisant fonctionner dans un seul foyer.  Une autre possibilité que les pods diffèrent est qu'ils n'ont pas à utiliser de conteneurs Docker.  Si nécessaire, vous pouvez appliquer ici d'autres technologies pour la conteneurisation d'applications, par exemple - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Rkt</a> . <br><br>  Le diagramme suivant montre les propriétés numérotées de l'âtre. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f6/2a4/bb1/4f62a4bb18bddccc49a9d224a4aa919d.png"></div><br>  <i><font color="#999999">Propriétés du foyer</font></i> <br><br>  Considérez ces propriétés. <br><br><ol><li>  Chaque pod d'un cluster Kubernetes possède une adresse IP unique. </li><li>  Un foyer peut contenir de nombreux conteneurs.  Ils partagent les numéros de port disponibles, c'est-à-dire qu'ils peuvent par exemple échanger des informations via <code>localhost</code> (naturellement, ils ne peuvent pas utiliser les mêmes ports).  L'interaction avec les conteneurs situés dans d'autres pods est organisée à l'aide des adresses IP de ces pods. </li><li>  Les conteneurs dans les modules partagent les volumes de stockage de données, l'adresse IP, les numéros de port et l'espace de noms IPC. </li></ol><br>  Il convient de noter que les conteneurs ont leurs propres systèmes de fichiers isolés, mais ils peuvent partager des données à l'aide de la ressource Kubernetes appelée <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Volume</a> . <br><br>  Pour nous, ce qui a déjà été dit sur les foyers est suffisant pour continuer à maîtriser les Kubernetes.  En savoir plus à leur sujet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br><h3>  <font color="#3AC1EF">▍ Description du foyer</font> </h3><br>  Ce qui suit est un fichier manifeste pour l'application <code>sa-frontend</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod                                            # 1 metadata: name: sa-frontend                                  # 2 spec:                                                # 3 containers:   - image: rinormaloku/sentiment-analysis-frontend # 4     name: sa-frontend                              # 5     ports:       - containerPort: 80</code> </pre> <br>  Expliquons certains des paramètres qui y sont spécifiés. <br><br><ol><li>  <code>Kind</code> : spécifie le type de ressource Kubernetes que nous voulons créer.  Dans notre cas, c'est <code>Pod</code> . </li><li>  <code>Name</code> : nom de la ressource.  Nous l'avons appelé <code>sa-frontend</code> . </li><li>  <code>Spec</code> : un objet qui décrit l'état souhaité de la ressource.  La propriété la plus importante ici est le tableau de conteneurs. </li><li>  <code>Image</code> : l'image du conteneur que nous voulons exécuter dans ce pod. </li><li>  <code>Name</code> : un nom unique pour le conteneur en dessous. </li><li>  <code>ContainerPort</code> : le port sur lequel le conteneur écoute.  Ce paramètre peut être considéré comme une indication pour qui lit ce fichier (si vous omettez ce paramètre, cela ne limitera pas l'accès au port). </li></ol><br><h3>  <font color="#3AC1EF">▍Création d'un foyer SA-Frontend</font> </h3><br>  Le fichier de description du pod dont nous avons parlé se trouve dans <code>resource-manifests/sa-frontend-pod.yaml</code> .  Vous devez soit accéder à ce dossier à l'aide des outils du terminal, soit, lorsque vous appelez la commande appropriée, spécifier le chemin d'accès complet au fichier.  Voici cette commande et un exemple de réaction du système à celle-ci: <br><br><pre> <code class="plaintext hljs">kubectl create -f sa-frontend-pod.yaml pod "sa-frontend" created</code> </pre> <br>  Afin de savoir si cela fonctionne sous, exécutez la commande suivante: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                          READY STATUS RESTARTS AGE sa-frontend                   1/1 Running 0 7s</code> </pre> <br>  Si l'état de l'âtre pendant l'exécution de cette commande est <code>ContainerCreating</code> , vous pouvez exécuter la même commande avec le <code>--watch</code> .  Pour cette raison, lorsque le foyer est en état de fonctionnement, des informations à ce sujet s'affichent automatiquement. <br><br><h3>  <font color="#3AC1EF">▍Accès à l'application depuis l'extérieur</font> </h3><br>  Afin d'organiser l'accès à l'application depuis l'extérieur, il sera correct de créer une ressource Kubernetes de type Service, dont nous parlerons ci-dessous, mais ici, pour plus de brièveté, nous utiliserons une simple redirection de port: <br><br><pre> <code class="plaintext hljs">kubectl port-forward sa-frontend 88:80 Forwarding from 127.0.0.1:88 -&gt; 80</code> </pre> <br>  Si vous passez maintenant par un navigateur à <code>127.0.0.1:88</code> , vous pouvez voir la page d'application React. <br><br><h3>  <font color="#3AC1EF">▍ Mauvaise approche de mise à l'échelle</font> </h3><br>  Nous avons déjà dit que l'une des capacités de Kubernetes est la mise à l'échelle des applications.  Pour profiter de cette opportunité, nous en exécuterons une autre.  Créez une description d'une autre ressource <code>Pod</code> en plaçant le code suivant dans le fichier <code>sa-frontend-pod2.yaml</code> : <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod                                           metadata: name: sa-frontend2      #   spec:                                                containers:   - image: rinormaloku/sentiment-analysis-frontend     name: sa-frontend                                  ports:       - containerPort: 80</code> </pre> <br>  Comme vous pouvez le voir, si vous comparez cette description avec ce que nous avons examiné ci-dessus, le seul changement est la valeur de la propriété <code>Name</code> . <br><br>  Créez-en un nouveau sous: <br><br><pre> <code class="plaintext hljs">kubectl create -f sa-frontend-pod2.yaml pod "sa-frontend2" created</code> </pre> <br>  Assurez-vous qu'il fonctionne: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                          READY STATUS RESTARTS AGE sa-frontend                   1/1 Running 0 7s sa-frontend2                  1/1 Running 0 7s</code> </pre> <br>  Maintenant, nous avons deux foyers!  Certes, il n'y a rien de spécial à apprécier ici.  Veuillez noter que la solution au problème de mise à l'échelle des applications présentée ici présente de nombreux inconvénients.  Nous verrons comment le faire correctement dans la section sur une autre ressource Kubernetes appelée Déploiement. <br><br>  Considérez maintenant ce que nous avons obtenu après avoir lancé deux foyers identiques.  À savoir, le serveur Web Nginx s'exécute désormais dans deux modules différents.  À cet égard, nous pouvons poser deux questions: <br><br><ol><li>  Comment donner accès à ces serveurs de l'extérieur, par URL? </li><li>  Comment organiser l'équilibrage de charge entre eux? </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ff/3a9/6f4/1ff3a96f4b930fe55727d1063b3c117b.png"></div><br>  <i><font color="#999999">Mauvaise approche de mise à l'échelle</font></i> <br><br>  Parmi les outils Kubernetes, il existe des ressources du formulaire Service.  Parlons-en. <br><br><h2>  <font color="#3AC1EF">Pratique Kubernetes: Services</font> </h2><br>  Les services Kubernetes agissent comme des points d'accès aux ensembles de foyers qui offrent les mêmes fonctionnalités que ces foyers.  Les services effectuent la solution des tâches difficiles consistant à travailler avec des foyers et à équilibrer la charge entre eux. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bbd/95f/bd8/bbd95fbd8562bed4a09ab4930a20f98d.png"></div><br>  <i><font color="#999999">Le service Kubernetes sert des adresses IP</font></i> <br><br>  Dans notre cluster Kubernetes, il y aura des pods qui implémenteront différentes fonctions.  Il s'agit d'une application frontale, d'une application Web Spring et d'une application Flask écrite en Python.  Cela soulève la question de savoir comment le service doit comprendre avec quel type de pods il doit travailler, c'est-à-dire comment trouver sur la base de quelles informations le système doit générer une liste de points de terminaison pour les pods. <br><br>  Cela se fait avec une autre abstraction Kubernetes appelée Label.  Le travail avec les balises comprend deux étapes: <br><br><ol><li>  L'attribution d'étiquette donnera le service avec lequel travailler. </li><li>  En appliquant un «sélecteur» au service, qui détermine les pods auxquels les étiquettes sont affectées, le service fonctionnera. </li></ol><br>  C'est peut-être plus facile à imaginer comme illustration qu'à décrire. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb9/fcf/f0c/bb9fcff0cded591f1a5ab8a0b825245a.png"></div><br>  <i><font color="#999999">Pods étiquetés et leurs fichiers manifestes</font></i> <br><br>  Nous voyons ici deux foyers qui, à l'aide de la construction <code>app: sa-frontend</code> , se voient attribuer les mêmes étiquettes.  Le service s'intéresse aux pods avec de telles marques. <br><br><h3>  <font color="#3AC1EF">▍Étiquettes</font> </h3><br>  Les étiquettes offrent aux développeurs un moyen simple d'organiser les ressources Kubernetes.  Ce sont des paires clé-valeur; vous pouvez les affecter à toutes les ressources.  Modifiez les fichiers de description du foyer de l'application frontale et amenez-les à la vue illustrée dans la figure précédente.  Après cela, enregistrez ces fichiers et exécutez les commandes suivantes: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-pod.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply pod "sa-frontend" configured kubectl apply -f sa-frontend-pod2.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply pod "sa-frontend2" configured</code> </pre> <br>  Lorsque ces commandes sont exécutées, le système émet des avertissements (cela ne nous convient pas que nous utilisons <code>apply</code> au lieu de <code>create</code> , nous le comprenons), mais, après un avertissement, il signale que les pods correspondants sont configurés.  Nous pouvons vérifier si des étiquettes ont été attribuées aux étiquettes, en filtrant les journaux pour lesquels nous voulons afficher des informations: <br><br><pre> <code class="plaintext hljs">kubectl get pod -l app=sa-frontend NAME           READY STATUS    RESTARTS AGE sa-frontend    1/1 Running   0 2h sa-frontend2   1/1 Running   0 2h</code> </pre> <br>  Une autre façon de vérifier que des étiquettes ont bien été attribuées aux étiquettes consiste à attacher la <code>--show-labels</code> à la commande précédente.  Pour cette raison, les informations sur leurs modules incluront également des données sur leurs marques. <br><br>  Maintenant, des balises ont été attribuées et nous sommes prêts à configurer le service pour fonctionner avec eux.  Par conséquent, nous traiterons une description d'un service tel que <code>LoadBalancer</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e0f/081/7e9/e0f0817e9fc090628aa1d0d89577ce80.gif"></div><br>  <i><font color="#999999">Équilibrage de charge à l'aide d'un service tel que LoadBalancer</font></i> <br><br><h3>  <font color="#3AC1EF">▍ Description du service</font> </h3><br>  Voici une description YAML d'un service comme <code>LoadBalancer</code> : <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service              # 1 metadata: name: sa-frontend-lb spec: type: LoadBalancer       # 2 ports: - port: 80               # 3   protocol: TCP          # 4   targetPort: 80         # 5 selector:                # 6   app: sa-frontend       # 7</code> </pre> <br>  Expliquez ce texte: <br><br><ol><li>  <code>Kind</code> : nous créons un service, une ressource <code>Service</code> . </li><li>  <code>Type</code> : le type de ressource indiqué dans sa spécification.  Nous avons choisi le type <code>LoadBalancer</code> , car avec ce service nous voulons résoudre le problème d'équilibrage de la charge entre les foyers. </li><li>  <code>Port</code> : port sur lequel le service accepte les requêtes. </li><li>  <code>Protocol</code> : protocole utilisé par le service. </li><li>  <code>TargetPort</code> : port vers lequel les demandes entrantes sont redirigées. </li><li>  <code>Selector</code> : un objet contenant des informations sur les pods avec lesquels le service devrait fonctionner. </li><li>  <code>app: sa-frontend</code> : cette propriété indique avec quels pods le service fonctionnera.  À savoir, ce sont les pods auxquels l'étiquette <code>app: sa-frontend</code> a été attribuée. </li></ol><br>  Pour créer un service, vous devez exécuter la commande suivante: <br><br><pre> <code class="plaintext hljs">kubectl create -f service-sa-frontend-lb.yaml service "sa-frontend-lb" created</code> </pre> <br>  Vous pouvez vérifier l'état du service comme suit: <br><br><pre> <code class="plaintext hljs">kubectl get svc NAME             TYPE CLUSTER-IP      EXTERNAL-IP PORT(S) AGE sa-frontend-lb   LoadBalancer 10.101.244.40   &lt;pending&gt; 80:30708/TCP 7m</code> </pre> <br>  Ici, vous pouvez voir que la propriété <code>EXTERNAL-IP</code> est dans l'état <code>&lt;pending&gt;</code> , mais vous ne pouvez pas attendre de la changer.  Cela est dû au fait que nous utilisons Minikube.  Si nous avons créé un service similaire en travaillant avec un certain fournisseur de services cloud, tel qu'Azure ou la plateforme Google Cloud, alors le service aurait une adresse IP publique qui permettrait d'y accéder depuis Internet. <br><br>  Malgré cela, Minikube ne nous permettra pas de déconner, nous donnant une commande utile pour le débogage local du système: <br><br><pre> <code class="plaintext hljs">minikube service sa-frontend-lb Opening kubernetes service default/sa-frontend-lb in default browser...</code> </pre> <br>  Grâce à cette commande, un navigateur sera lancé qui accédera au service.  Une fois que le service a reçu la demande, il la redirige vers l'un des foyers (peu importe celui sous lequel il se trouve).  Cette abstraction nous permet de percevoir un groupe de foyers comme une entité unique et de travailler avec eux, en utilisant le service comme point d'accès unique à eux. <br><br>  Dans cette section, nous avons expliqué comment attribuer des étiquettes aux ressources, comment les utiliser lors de la configuration des services en tant que sélecteurs.  Ici, nous avons décrit et créé un service comme <code>LoadBalancer</code> .  Grâce à cela, nous avons résolu le problème de mise à l'échelle de l'application (la mise à l'échelle consiste à ajouter de nouveaux foyers avec les étiquettes correspondantes au cluster) et à organiser l'équilibrage de charge entre les foyers en utilisant le service comme point d'entrée. <br><br><h2>  <font color="#3AC1EF">Pratique Kubernetes: Déploiements</font> </h2><br>  Le déploiement est une abstraction de Kubernetes qui nous permet de contrôler ce qui est toujours présent dans le cycle de vie de l'application.  Il s'agit de gérer les changements d'application.  Les applications qui ne changent pas sont pour ainsi dire des applications «mortes».  Si l'application "vit", vous pouvez rencontrer le fait que ses exigences changent périodiquement, son code se développe, ce code est empaqueté et déployé.  De plus, des erreurs peuvent être commises à chaque étape du processus. <br><br>  Une ressource de type Déploiement vous permet d'automatiser le processus de transition d'une version d'une application à une autre.  Cela se fait sans interrompre le système, et si une erreur se produit pendant ce processus, nous aurons la possibilité de revenir rapidement à la version de travail précédente de l'application. <br><br><h3>  <font color="#3AC1EF">▍Utilisation des déploiements</font> </h3><br>  Maintenant, le cluster dispose de deux foyers et d'un service qui leur donne accès de l'extérieur et équilibre la charge sur eux. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/426/651/0c4/4266510c40a1faa6086178e5db23d20c.png"></div><br>  <i><font color="#999999">État actuel du cluster</font></i> <br><br>  Nous avons parlé du fait que faire fonctionner deux foyers différents avec la même fonctionnalité n'est pas une bonne idée.  Lors de l'utilisation d'un tel schéma, nous devons travailler avec chaque foyer individuellement, créer, mettre à jour, supprimer chaque foyer spécifique, observer son état.  Avec cette approche, il n'est pas nécessaire de parler d'une mise à jour rapide du système ou de la restauration rapide d'une mise à jour infructueuse.  Nous ne sommes pas satisfaits de cet état de fait, nous allons donc recourir à la possibilité de ressources de déploiement, qui vise à résoudre les problèmes ci-dessus. <br><br>  Avant de poursuivre le travail, formulons ses objectifs, qui nous donneront des directives qui seront utiles lors de l'analyse du fichier manifeste de déploiement.  Voici donc ce dont nous avons besoin: <br><br><ol><li>  Nous voulons être en mesure de créer deux foyers basés sur un conteneur <code>rinormaloku/sentiment-analysis-frontend</code> . </li><li>  Nous avons besoin d'un système de déploiement d'applications qui lui permet de fonctionner sans interruption lors de sa mise à jour. </li><li>  Nous voulons que le libellé de l' <code>app: sa-frontend</code> soit attribué <code>app: sa-frontend</code> , qui permettra au service <code>sa-frontend-lb</code> de détecter ces pods. </li></ol><br>  Nous allons maintenant exprimer ces exigences sous la forme d'une description de la ressource de déploiement. <br><br><h3>  <font color="#3AC1EF">▍ Description du déploiement</font> </h3><br>  Voici une description YAML d'une ressource de type déploiement, qui a été créée en tenant compte des exigences système ci-dessus: <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Deployment                                          # 1 metadata: name: sa-frontend spec: replicas: 2                                             # 2 minReadySeconds: 15 strategy:   type: RollingUpdate                                   # 3   rollingUpdate:     maxUnavailable: 1                                   # 4     maxSurge: 1                                         # 5 template:                                               # 6   metadata:     labels:       app: sa-frontend                                  # 7   spec:     containers:       - image: rinormaloku/sentiment-analysis-frontend         imagePullPolicy: Always                         # 8         name: sa-frontend         ports:           - containerPort: 80</code> </pre> <br>  Analysons cette description: <br><br><ol><li>  <code>Kind</code> : il est dit ici que nous décrivons une ressource de la vue <code>Deployment</code> . </li><li>  <code>Replicas</code> : propriété de l'objet de spécification de déploiement qui définit le nombre d'instances (répliques) de foyers à exécuter. </li><li>  <code>Type</code> : décrit la stratégie utilisée dans ce déploiement lors du passage de la version actuelle à une nouvelle.  <code>RollingUpdate</code> stratégie <code>RollingUpdate</code> n'offre aucun temps d'arrêt du système lors des mises à niveau. </li><li>  <code>MaxUnavailable</code> : il s'agit d'une propriété de l'objet <code>RollingUpdate</code> , qui définit le nombre maximal de foyers non disponibles (par rapport au nombre souhaité de foyers) lors de la mise à jour séquentielle du système.  Dans notre déploiement, qui implique la présence de 2 répliques, la valeur de cette propriété indique qu'après l'achèvement d'un pod, un autre sera exécuté, ce qui rend l'application disponible lors de la mise à jour. </li><li>  <code>MaxSurge</code> : il s'agit d'une propriété de l'objet <code>RollingUpdate</code> qui décrit le nombre maximal de foyers pouvant être ajoutés à un déploiement (par rapport à un nombre donné de foyers).  Dans notre cas, sa valeur, 1, signifie que, lors du passage à une nouvelle version du programme, nous pouvons ajouter un autre sous-cluster, ce qui conduira au fait que jusqu'à trois foyers peuvent être lancés simultanément. </li><li>  <code>Template</code> : cet objet définit le modèle de foyer que la ressource de <code>Deployment</code> décrite utilisera pour créer de nouveaux foyers.  Vous trouverez probablement ce paramètre familier. </li><li>  <code>app: sa-frontend</code> : étiquette pour les foyers créés selon un modèle donné. </li><li>  <code>ImagePullPolicy</code> : définit l'ordre de travail avec les images.  Dans notre cas, cette propriété est définie sur <code>Always</code> , c'est-à-dire que lors de chaque déploiement, l'image correspondante sera téléchargée à partir du référentiel. </li></ol><br>  Après avoir examiné tout cela, passons à la pratique.  Exécutez le déploiement: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment.yaml deployment "sa-frontend" created</code> </pre> <br>  Vérifiez l'état du système: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                           READY STATUS RESTARTS AGE sa-frontend                    1/1 Running 0 2d sa-frontend-5d5987746c-ml6m4   1/1 Running 0 1m sa-frontend-5d5987746c-mzsgg   1/1 Running 0 1m sa-frontend2                   1/1 Running 0 2d</code> </pre> <br>  Comme vous pouvez le voir, nous avons maintenant 4 pods.  Deux d'entre eux ont été créés à l'aide de la ressource Déploiement, deux autres sont ceux que nous avons créés nous-mêmes.  Vous pouvez maintenant supprimer les pods que nous avons créés nous-mêmes à l'aide de commandes du type suivant: <br><br><pre> <code class="plaintext hljs">kubectl delete pod &lt;pod-name&gt;</code> </pre> <br>  Au fait, voici une mission pour un travail indépendant.  Supprimez l'un des foyers créés à l'aide de la ressource Déploiement et surveillez le système.  Réfléchissez aux raisons de ce qui se passe avant de continuer à lire. <br><br>  Lors de la suppression d'un foyer, la ressource Déploiement apprend que l'état actuel du système (1 sous) est différent de celui souhaité (2 sous), donc un autre sous est lancé. <br><br>  Quelle est l'utilisation des ressources de déploiement, outre le fait que, lorsqu'elles sont utilisées, le système est maintenu dans le bon état?  Considérez les points forts de ces ressources. <br><br><h3>  <font color="#3AC1EF">▍ Réalisation de déploiements sans interruption du système</font> </h3><br>  Supposons qu'un chef de produit vienne nous voir et signale que le client pour lequel nous avons créé ce produit veut un bouton vert dans l'application cliente.  Les développeurs implémentent cette exigence et nous donnent la seule chose dont nous avons besoin d'eux - un conteneur d'images appelé <code>rinormaloku/sentiment-analysis-frontend:green</code> .  Maintenant vient notre temps.  Nous, l'équipe DevOps, devons déployer le système mis à jour et garantir un temps d'arrêt nul.  Voyons maintenant si les efforts pour développer et configurer la ressource de déploiement sont justifiés. <br><br>  Modifiez le fichier <code>sa-frontend-deployment.yaml</code> , en remplaçant le nom du conteneur d'images par un nouveau, avec <code>rinormaloku/sentiment-analysis-frontend:green</code> , puis enregistrez ce fichier sous <code>sa-frontend-deployment-green.yaml</code> et exécutez la commande suivante: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment-green.yaml --record deployment "sa-frontend" configured</code> </pre> <br>  Vérifiez l'état du système avec la commande suivante: <br><br><pre> <code class="plaintext hljs">kubectl rollout status deployment sa-frontend Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 of 2 updated replicas are available... deployment "sa-frontend" successfully rolled out</code> </pre> <br>  Conformément aux données affichées en réponse à cette commande, nous pouvons conclure que le déploiement de la mise à jour a réussi.  Lors de la mise à niveau, les anciennes répliques, une à la fois, ont été remplacées par de nouvelles.  ,   ,    ,   .     ,    ,    . <br><br><h4>   </h4><br>      ,     ,     : <br><br><pre> <code class="plaintext hljs">minikube service sa-frontend-lb</code> </pre> <br>       ,      . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/777/489/902/777489902694f46438ceae41ce59db9b.png"></div><br> <i><font color="#999999"> </font></i> <br><br>  ,     ,  —    . <br><br><h4>      RollingUpdate </h4><br>  ,     <code>kubectl apply -f sa-frontend-deployment-green.yaml --record</code> , Kubernetes   ,     ,    .         ,       ,    <code>rinormaloku/sentiment-analysis-frontend:green</code> .       ,    ,   . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/465/7ee/290/4657ee29097dc99e4fa2a0ebd9180e7a.png"></div><br> <i><font color="#999999">     </font></i> <br><br>  <code>RollingUpdate</code>       ,  ,     <code>maxUnavailable: 1</code>  <code>maxSurge: 1</code> .  ,   Deployment ,     ,    ,     .  ,    ,    ,         . <br><br>         Deployment.     ,   .      . <br><br><h3> <font color="#3AC1EF">▍    </font> </h3><br>   ,   ,   . «!  !    !», —  .        . ,   ,      : <br><br><pre> <code class="plaintext hljs">kubectl rollout history deployment sa-frontend deployments "sa-frontend" REVISION  CHANGE-CAUSE 1         &lt;none&gt;    2         kubectl.exe apply --filename=sa-frontend-deployment-green.yaml --record=true</code> </pre> <br>          : «,    ,    ?». <br><br> «.  ,   ?», —   . <br><br>  ,         ,     : <br><br><pre> <code class="plaintext hljs">kubectl rollout undo deployment sa-frontend --to-revision=1 deployment "sa-frontend" rolled back</code> </pre> <br>      .   ,      . <br><br>       . <br><br>       . <br><br> ! <br><br>   ,  .   Kubernetes         ,  ,      . ,   ! <br><br>           . ,       .  <code>CHANGE-CAUSE</code>      <code>&lt;none&gt;</code> ,    — <code>kubectl.exe apply –filename=sa-frontend-deployment-green.yaml –record=true</code> ? <br><br>   ,         -- <code>record</code>     ,    . <br><br>       ,   ,  ,      . <br><br><h2> <font color="#3AC1EF">   Kubernetes:    </font> </h2><br>      Kubernetes,    ,     .      ,     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7cb/af4/880/7cbaf4880d435df50761d22508f61e83.png"></div><br> <i><font color="#999999">  </font></i> <br><br>       . <br><br><h3> <font color="#3AC1EF">▍  sa-logic</font> </h3><br>        <code>resource-manifests</code>    : <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-logic-deployment.yaml --record deployment "sa-logic" created</code> </pre> <br>  <code>sa-logic</code>   .     Python-.    <code>app: sa-logic</code> .          <code>sa-logic</code> ,   .   <code>sa-logic-deployment.yaml</code>     . <br><br>  -,        ,      —  <code>sa-logic</code> . <br><br><h3> <font color="#3AC1EF">▍ sa-logic</font> </h3><br>   ,       Service.   ,   Java-,        <code>sa-webapp</code> ,      ,  Python-.  ,    ,       ,     Python-,   .     ,  ,  ,  . <br><br>      , ,    ,       ,   .  ,      <code>sa-logic</code>   ,       <code>sa-logic</code> . <br><br>   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f service-sa-logic.yaml service "sa-logic" created</code> </pre> <br>    ,        . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/693/79b/3e3/69379b3e373ad1bc728242db341411ab.png"></div><br> <i><font color="#999999">  </font></i> <br><br>   <code>sa-logic</code> ,   <code>sa-webapp</code> ,    ,    . <br><br>   <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍  sa-webapp</font> </h3><br>      ,          Deployment    - . ,     <code>sa-web-app-deployment.yaml</code> ,      : <br><br><pre> <code class="plaintext hljs">- image: rinormaloku/sentiment-analysis-web-app imagePullPolicy: Always name: sa-web-app env:   - name: SA_LOGIC_API_URL     value: "http://sa-logic" ports:   - containerPort: 8080</code> </pre> <br>     <code>env</code> ?  ,   ,  ,   <code>SA_LOGIC_API_URL</code>   <code>http://sa-logic</code> .   ,    ,       .    ? <br><br>             kube-dns. <br><br><h3> <font color="#3AC1EF">▍DNS-  Kubernetes</font> </h3><br>  Kubernetes   ,   <code>kube-dns</code> .        DNS-.     <code>kube-dns</code>   ,     DNS-    . <br><br>  ,      <code>sa-logic</code> ,   IP-.  <code>kube-dns</code>        IP- .        <code>http://sa-logic</code>  IP-. <br><br>      Deployment <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍  sa-webapp</font> </h3><br>   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-web-app-deployment.yaml --record deployment "sa-web-app" created</code> </pre> <br>         <code>sa-webapp</code>   ,   .   React-    ,       <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍ sa-webapp</font> </h3><br>     <code>service-sa-web-app-lb.yaml</code> ,  ,  ,    ,   . ,   ,   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f service-sa-web-app-lb.yaml service "sa-web-app-lb" created</code> </pre> <br>    . ,     ,      . ,     <code>sa-frontend</code> ,        Java- <code>sa-webapp</code> ,    <code>http://localhost:8080/sentiment</code> .      ,       ,   <code>sa-webapp</code> ,    React-  ,     Java-. <br><br>           ,     . ,          —  ,    ,     . <br><br>  ,       : <br><br><ol><li>  IP-   <code>sa-webapp</code> ,   : <br><br> <code>minikube service list <br> |-------------|----------------------|-----------------------------| <br> |  NAMESPACE  | NAME         | URL       | <br> |-------------|----------------------|-----------------------------| <br> | default     | kubernetes         | No node port       | <br> | default     | sa-frontend-lb       | http://192.168.99.100:30708 | <br> | default     | sa-logic         | No node port       | <br> | default     | sa-web-app-lb        | http://192.168.99.100:31691 | <br> | kube-system | kube-dns             | No node port | <br> | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | <br> |-------------|----------------------|-----------------------------|</code> </li> <li>   IP-   <code>sa-frontend/src/App.js</code> .   ,     : <br><br><pre> <code class="plaintext hljs">analyzeSentence() {       fetch('http://192.168.99.100:31691/sentiment', { /*    */})           .then(response =&gt; response.json())           .then(data =&gt; this.setState(data));   }</code> </pre> </li><li>  React-,       <code>sa-frontend</code>    <code>npm run build</code> . </li><li>   : <br><br><pre> <code class="plaintext hljs">docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:minikube.</code> </pre> </li><li>     Docker Hub: <br><br><pre> <code class="plaintext hljs">docker push $DOCKER_USER_ID/sentiment-analysis-frontend:minikube</code> </pre> </li><li>   <code>sa-frontend-deployment.yaml</code> ,       . </li><li>   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment.yaml</code> </pre> </li></ol><br>     ,   , ,      ,    <code>minikube service sa-frontend-lb</code> .  ,   - . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53d/19c/3ba/53d19c3bac2f8cdd66213c9b34e7b05b.png"></div><br> <i><font color="#999999">  </font></i> <br><br><h2>  <font color="#3AC1EF">Résumé</font> </h2><br>   Kubernetes     ,        ,   ,   ,     .  Kubernetes   ,     ,           .    Kubernetes  Supernetes. <br><br>    ,   : <br><br><ul><li> ,    ,   React, Java  Python. </li><li>    Docker,  ,        <code>Dockerfile</code> . </li><li>    ,  ,  Docker Hub. </li></ul><br>  ,     Kubernetes: <br><br><ul><li>  </li><li>  </li><li>  </li><li>           </li><li>   </li></ul><br>      ,   ,   Kubernetes. <br><br>  <b>Chers lecteurs!</b>    Kubernetes? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr438984/">https://habr.com/ru/post/fr438984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr438974/index.html">La réalité virtuelle aide à faire face aux troubles mentaux</a></li>
<li><a href="../fr438976/index.html">Le livre "Spring. Tous les modèles de conception »</a></li>
<li><a href="../fr438978/index.html">Apprendre toujours et partout! Podcasts pour les développeurs en anglais</a></li>
<li><a href="../fr438980/index.html">Spring Boot 2: quoi de neuf?</a></li>
<li><a href="../fr438982/index.html">Guide Kubernetes, Partie 1: Applications, microservices et conteneurs</a></li>
<li><a href="../fr438986/index.html">Tutoriel React Partie 14: Atelier sur les composants basés sur les classes, état des composants</a></li>
<li><a href="../fr438988/index.html">Tutoriel React Partie 15: Ateliers sur l'état des composants</a></li>
<li><a href="../fr438992/index.html">Journal du développeur ou mauvaises décisions</a></li>
<li><a href="../fr438994/index.html">Intel Xeon W-3175X, un batteur à chaud. Test</a></li>
<li><a href="../fr438996/index.html">Réseau d'entreprise et MitM. Partie 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>