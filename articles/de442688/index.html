<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💸 🔉 👨🏻‍🎤 Scala-Datenanalyse - ein dringender Bedarf oder eine angenehme Gelegenheit? 🏔️ 🧓🏻 🥀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Traditionelle Tools auf dem Gebiet der Datenwissenschaft sind Sprachen wie R und Python - eine entspannte Syntax und eine große Anzahl von Bibliotheke...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Scala-Datenanalyse - ein dringender Bedarf oder eine angenehme Gelegenheit?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442688/"><p><img src="https://habrastorage.org/webt/5q/1e/fr/5q1efrbkxbuk_ndqoensinfl80a.jpeg"></p><br><p>  Traditionelle Tools auf dem Gebiet der Datenwissenschaft sind Sprachen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">R</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python</a> - eine entspannte Syntax und eine große Anzahl von Bibliotheken für maschinelles Lernen und Datenverarbeitung ermöglichen es Ihnen, schnell einige funktionierende Lösungen zu erhalten.  Es gibt jedoch Situationen, in denen die Einschränkungen dieser Tools zu einem erheblichen Hindernis werden - vor allem dann, wenn eine hohe Verarbeitungsgeschwindigkeit und / oder die Arbeit mit wirklich großen Datenmengen erforderlich sind.  In diesem Fall muss sich der Spezialist widerstrebend an die "dunkle Seite" wenden und Tools in den "industriellen" Programmiersprachen verbinden: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Scala</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Java</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">C ++</a> . </p><br><p> Aber ist diese Seite so dunkel?  Im Laufe der Jahre der Entwicklung haben die Tools der "industriellen" Data Science einen langen Weg zurückgelegt und unterscheiden sich heute erheblich von ihren eigenen Versionen vor 2-3 Jahren.  Versuchen wir am Beispiel der Aufgabe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019</a> herauszufinden, inwieweit das Scala + Spark-Ökosystem Python Data Science entsprechen kann. </p><a name="habracut"></a><br><p>  Im Rahmen des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019</a> lösen die Teilnehmer das Problem, den Newsfeed eines Nutzers eines sozialen Netzwerks in einer von drei „Disziplinen“ zu sortieren: anhand von Daten aus Texten, Bildern oder Feature-Logs.  In dieser Veröffentlichung werden wir untersuchen, wie es in Spark möglich ist, ein Problem anhand eines Zeichenprotokolls mit klassischen maschinellen Lernwerkzeugen zu lösen. </p><br><p>  Bei der Lösung des Problems gehen wir den Standard vor, den jeder Datenanalysespezialist bei der Entwicklung eines Modells durchläuft: </p><br><ul><li>  Wir werden Forschungsdaten analysieren, Diagramme erstellen. </li><li>  Wir analysieren die statistischen Eigenschaften von Zeichen in den Daten und untersuchen deren Unterschiede zwischen Trainings- und Testsätzen. </li><li>  Wir werden eine erste Auswahl von Merkmalen basierend auf statistischen Eigenschaften durchführen. </li><li>  Wir berechnen die Korrelationen zwischen den Zeichen und der Zielvariablen sowie die Kreuzkorrelation zwischen den Zeichen. </li><li>  Wir werden die endgültigen Funktionen bilden, das Modell trainieren und seine Qualität überprüfen. </li><li>  Analysieren wir die interne Struktur des Modells, um Wachstumspunkte zu identifizieren. </li></ul><br><p>  Während unserer „Reise“ werden wir Tools wie das interaktive <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zeppelin-</a> Notizbuch, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spark ML-</a> Bibliothek für maschinelles Lernen und deren Erweiterung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> , das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GraphX-</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grafikpaket</a> , die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vegas-</a> Visualisierungsbibliothek und natürlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Spark</a> in seiner ganzen Pracht kennenlernen: )  Alle Code- und Versuchsergebnisse sind auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl-Plattform für</a> kollaborative Notizbücher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verfügbar</a> . </p><br><h1 id="zagruzka-dannyh">  Laden von Daten </h1><br><p>  Die Besonderheit der beim <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019 bereitgestellten</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daten</a> besteht darin, dass sie direkt mit Python verarbeitet werden können, dies ist jedoch schwierig: Die Originaldaten werden dank der Funktionen des Apache Parquet-Spaltenformats recht effizient gepackt und beim Lesen in den Speicher „über die Stirn“ in mehrere zehn Gigabyte dekomprimiert.  Bei der Arbeit mit Apache Spark müssen die Daten nicht vollständig in den Speicher geladen werden. Die Spark-Architektur ist so konzipiert, dass Daten in Teilen verarbeitet und bei Bedarf von der Festplatte geladen werden. </p><br><p>  Daher kann der erste Schritt - Überprüfen der Datenverteilung nach Tag - problemlos mit Box-Tools ausgeführt werden: </p><br><pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show(train.groupBy($<span class="hljs-string"><span class="hljs-string">"date"</span></span>).agg( functions.count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"count"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"users"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"objects"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"owners"</span></span>)) .orderBy(<span class="hljs-string"><span class="hljs-string">"date"</span></span>))</code> </pre> <br><p>  Was die entsprechende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grafik</a> in Zeppelin anzeigt: </p><br><p><img src="https://habrastorage.org/webt/jp/lh/pw/jplhpwwz4tfgdtrs1u_u-tqsvzi.png"></p><br><p>  Ich muss sagen, dass die Scala-Syntax sehr flexibel ist und derselbe Code beispielsweise folgendermaßen aussehen kann: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show( train groupBy $<span class="hljs-string"><span class="hljs-string">"date"</span></span> agg( count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"count"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"users"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"objects"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"owners"</span></span>) orderBy <span class="hljs-string"><span class="hljs-string">"date"</span></span> )</code> </pre> <br><p>  Hier ist eine wichtige Warnung zu beachten: Wenn man in einem großen Team arbeitet, in dem jeder das Schreiben von Scala-Code ausschließlich aus der Sicht seines eigenen Geschmacks betrachtet, ist die Kommunikation viel schwieriger.  Es ist daher besser, ein einheitliches Konzept für den Codestil zu entwickeln. </p><br><p>  Aber zurück zu unserer Aufgabe.  Eine einfache Analyse am Tag zeigte das Vorhandensein abnormaler Punkte am 17. und 18. Februar;  Wahrscheinlich wurden heutzutage unvollständige Daten gesammelt und die Verteilung der Merkmale kann verzerrt sein.  Dies sollte bei der weiteren Analyse berücksichtigt werden.  Darüber hinaus fällt auf, dass die Anzahl der eindeutigen Benutzer sehr nahe an der Anzahl der Objekte liegt. Daher ist es sinnvoll, die Verteilung der Benutzer mit unterschiedlicher Anzahl von Objekten zu untersuchen: </p><br><pre> <code class="scala hljs">z.show(filteredTrain .groupBy($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).count .groupBy(<span class="hljs-string"><span class="hljs-string">"count"</span></span>).agg(functions.log(functions.count(<span class="hljs-string"><span class="hljs-string">"count"</span></span>)).as(<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>)) .orderBy($<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>.desc) .limit(<span class="hljs-number"><span class="hljs-number">100</span></span>) .orderBy($<span class="hljs-string"><span class="hljs-string">"count"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/dh/i-/pp/dhi-ppdka19zurhimccpq7tij5w.png"></p><br><p>  Es wird eine nahezu exponentielle Verteilung mit einem sehr langen Schwanz erwartet.  Bei solchen Aufgaben ist es in der Regel möglich, die Arbeitsqualität zu verbessern, indem Modelle für Benutzer mit unterschiedlichen Aktivitätsstufen segmentiert werden.  Um zu überprüfen, ob es sich lohnt, dies zu tun, vergleichen Sie die Verteilung der Anzahl der Objekte nach Benutzer im Testsatz: </p><br><p><img src="https://habrastorage.org/webt/du/sy/xt/dusyxten5shm24an736n7akolnm.png"></p><br><p>  Ein Vergleich mit dem Test zeigt, dass Testbenutzer mindestens zwei Objekte in den Protokollen haben (da das Ranking-Problem beim Hackathon gelöst ist, ist dies eine notwendige Voraussetzung für die Bewertung der Qualität).  In Zukunft empfehle ich, die Benutzer im Trainingssatz, für die wir die benutzerdefinierte Funktion mit einem Filter deklarieren, genauer zu betrachten: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//  ,     "",   , //     val testSimilar = sc.broadcast(filteredTrain.groupBy($"instanceId_userId") .agg( functions.count("feedback").as("count"), functions.sum(functions.expr("IF(array_contains(feedback, 'Liked'), 1.0, 0.0)")).as("sum") ) .where("count &gt; sum AND sum &gt; 0") .select("instanceId_userId").rdd.map(_.getInt(0)).collect.sorted) //           // User Defined Function val isTestSimilar = sqlContext.udf.register("isTestSimilar", (x: Int) =&gt; java.util.Arrays.binarySearch(testSimilar.value, x) &gt;= 0)</span></span></code> </pre> <br><p>  Auch hier ist eine wichtige Bemerkung zu machen: Unter dem Gesichtspunkt der Definition von UDF unterscheidet sich die Verwendung von Spark unter Scala / Java und unter Python deutlich.  Während der PySpark-Code die Grundfunktionalität verwendet, funktioniert alles fast genauso schnell, aber wenn überschriebene Funktionen angezeigt werden, verschlechtert sich die Leistung von PySpark um eine Größenordnung. </p><br><h1 id="pervyy-ml-konveyer">  Die erste ML-Pipeline </h1><br><p>  Im nächsten Schritt werden wir versuchen, die grundlegenden Statistiken zu Aktionen und Attributen zu berechnen.  Dafür benötigen wir jedoch die Funktionen von SparkML. Zunächst werden wir uns die allgemeine Architektur ansehen: </p><br><p><img src="https://habrastorage.org/webt/j7/ev/en/j7evenhyzzfvzouqfkbfq1j9hvu.png"></p><br><p>  SparkML basiert auf folgenden Konzepten: </p><br><ul><li>  Transformator - Nimmt einen Datensatz als Eingabe und gibt einen geänderten Satz (Transformation) zurück.  In der Regel wird es zur Implementierung von Vor- und Nachbearbeitungsalgorithmen sowie zur Merkmalsextraktion verwendet und kann auch die resultierenden ML-Modelle darstellen. </li><li>  Schätzer - Nimmt einen Datensatz als Eingabe und gibt Transformer (fit) zurück.  Natürlich kann Estimator den ML-Algorithmus darstellen. </li><li>  Pipeline ist ein Sonderfall von Estimator, der aus einer Kette von Transformatoren und Schätzern besteht.  Wenn die Methode aufgerufen wird, durchläuft fit die Kette, und wenn es einen Transformator sieht, wendet es ihn auf die Daten an, und wenn es einen Schätzer sieht, trainiert es den Transformator damit, wendet ihn auf die Daten an und geht weiter. </li><li>  PipelineModel - Das Ergebnis von Pipeline enthält auch eine Kette im Inneren, die jedoch ausschließlich aus Transformatoren besteht.  Dementsprechend ist PipelineModel selbst auch ein Transformator. </li></ul><br><p>  Ein solcher Ansatz zur Bildung von ML-Algorithmen trägt zu einer klaren modularen Struktur und einer guten Reproduzierbarkeit bei - sowohl Modelle als auch Pipelines können gespeichert werden. </p><br><p>  Zunächst erstellen wir eine einfache Pipeline, mit der wir die Statistik der Verteilung der Aktionen (Feedbackfeld) der Benutzer im Trainingssatz berechnen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> feedbackAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-comment"><span class="hljs-comment">//         (feedback)  one-hot  new MultinominalExtractor().setInputCol("feedback").setOutputCol("feedback"), //       new VectorStatCollector() .setGroupByColumns("date").setInputCol("feedback") .setPercentiles(Array(0.1,0.5,0.9)), //        new VectorExplode().setValueCol("feedback") )).fit(train) z.show(feedbackAggregator .transform(filteredTrain) .orderBy($"date", $"feedback"))</span></span></code> </pre> <br><p>  In dieser Pipeline wird die Funktionalität von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> aktiv genutzt - Bibliotheken mit erweiterten nützlichen Blöcken für SparkML, nämlich: </p><br><ul><li>  MultinominalExtractor wird verwendet, um ein Zeichen vom Typ "Array of Strings" nach dem One-Hot-Prinzip in einen Vektor zu codieren.  Dies ist der einzige Schätzer in der Pipeline (um eine Codierung zu erstellen, müssen Sie eindeutige Zeilen aus dem Datensatz erfassen). </li><li>  Mit VectorStatCollector werden Vektorstatistiken berechnet. </li><li>  Mit VectorExplode wird das Ergebnis in ein für die Visualisierung geeignetes Format konvertiert. </li></ul><br><p>  Das Ergebnis der Arbeit wird eine Grafik sein, die zeigt, dass die Klassen im Datensatz nicht ausgeglichen sind. Das Ungleichgewicht für die Zielklasse "Gefällt mir" ist jedoch nicht extrem: </p><br><p><img src="https://habrastorage.org/webt/aa/db/x4/aadbx4el5s5lhuyput_cj4ns9zo.png"></p><br><p>  Die Analyse einer ähnlichen Verteilung unter Benutzern, die Testbenutzern ähnlich sind (mit "positiv" und "negativ" in den Protokollen), zeigt, dass sie auf die positive Klasse ausgerichtet ist: </p><br><p><img src="https://habrastorage.org/webt/x5/lu/px/x5lupxmayvvodo7lsb3meob1dou.png"></p><br><h1 id="statisticheskiy-analiz-priznakov">  Statistische Analyse von Zeichen </h1><br><p>  In der nächsten Phase werden wir eine detaillierte Analyse der statistischen Eigenschaften der Attribute durchführen.  Diesmal brauchen wir einen größeren Förderer: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> statsAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-comment"><span class="hljs-comment">//          new AutoAssembler() .setColumnsToExclude( (Seq("date", "feedback") ++ train.schema.fieldNames.filter(_.endsWith("Id")) : _*)) .setOutputCol("features"), new VectorStatCollector() .setGroupByColumns("date").setInputCol("features") .setPercentiles(Array(0.1,0.5,0.9)), new VectorExplode().setValueCol("features") ))</span></span></code> </pre> <br><p>  Da wir jetzt nicht mit einem separaten Feld arbeiten müssen, sondern mit allen Attributen gleichzeitig, werden wir zwei weitere nützliche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML-</a> Dienstprogramme verwenden: </p><br><ul><li>  Mit NullToDefaultReplacer können Sie fehlende Elemente in den Daten durch ihre Standardwerte ersetzen (0 für Zahlen, false für logische Variablen usw.).  Wenn Sie diese Konvertierung nicht durchführen, werden NaN-Werte in den resultierenden Vektoren angezeigt, was für viele Algorithmen fatal ist (obwohl beispielsweise XGBoost dies überleben kann).  Eine Alternative zum Ersetzen durch Nullen kann das Ersetzen durch Durchschnittswerte sein. Dies ist in NaNToMeanReplacerEstimator implementiert. </li><li>  AutoAssembler ist ein sehr leistungsfähiges Dienstprogramm, das das Tabellenlayout analysiert und für jede Spalte ein Vektorisierungsschema auswählt, das dem Spaltentyp entspricht. </li></ul><br><p>  Mit der resultierenden Pipeline berechnen wir die Statistiken für drei Sätze (Training, Training mit Benutzerfilter und Test) und speichern sie in separaten Dateien: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (   AutoAssembler  ) val trained = statsAggregator.fit(filteredTrain) //       - ,     . trained .transform(filteredTrain .withColumn("date", //  ,      ,     , //        All   functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/featuresStat") trained .transform(filteredTrain .where(isTestSimilar($"instanceId_userId")) .withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/filteredFeaturesStat") trained .transform(filteredTest.withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(3).write.mode("overwrite").parquet("sna2019/testFeaturesStat")</span></span></code> </pre> <br><p>  Nachdem wir drei Datensätze mit Attributstatistiken erhalten haben, analysieren wir die folgenden Dinge: </p><br><ul><li>  Haben wir Zeichen, für die es große Emissionen gibt? <br>  - Solche Zeichen sollten begrenzt oder Ausreißerdatensätze herausgefiltert werden. </li><li>  Haben wir Zeichen mit einer großen Abweichung des Mittelwerts relativ zum Median? <br>  - Eine solche Verschiebung tritt häufig bei Vorhandensein einer Leistungsverteilung auf. Es ist sinnvoll, diese Vorzeichen zu logarithmieren. </li><li>  Gibt es eine Verschiebung der durchschnittlichen Verteilungen zwischen Trainings- und Testsätzen? </li><li>  Wie dicht unsere Feature-Matrix gefüllt ist. </li></ul><br><p>  Um diese Aspekte zu klären, hilft uns eine solche Anfrage: </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareWithTest</span></span></span></span>(data: <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>) : <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { data.where(<span class="hljs-string"><span class="hljs-string">"date = 'All'"</span></span>) .select( $<span class="hljs-string"><span class="hljs-string">"features"</span></span>, <span class="hljs-comment"><span class="hljs-comment">//         // ( ) functions.log($"features_mean" / $"features_p50").as("skewenes"), //    90-      //    90-  —    functions.log( ($"features_max" - $"features_p90") / ($"features_p90" - $"features_p50")).as("outlieres"), //       ,  //    ($"features_nonZeros" / $"features_count").as("train_fill"), $"features_mean".as("train_mean")) .join(testStat.where("date = 'All'") .select($"features", $"features_mean".as("test_mean"), ($"features_nonZeros" / $"features_count").as("test_fill")), Seq("features")) //          .withColumn("meanDrift", (($"train_mean" - $"test_mean" ) / ($"train_mean" + $"test_mean"))) //      .withColumn("fillDrift", ($"train_fill" - $"test_fill") / ($"train_fill" + $"test_fill")) } //         val comparison = compareWithTest(trainStat).withColumn("mode", functions.lit("raw")) .unionByName(compareWithTest(filteredStat).withColumn("mode", functions.lit("filtered")))</span></span></code> </pre> <br><p>  In dieser Phase ist die Frage nach der Visualisierung dringend: Es ist schwierig, alle Aspekte mit normalen Zeppelin-Werkzeugen sofort anzuzeigen, und Notebooks mit einer großen Anzahl von Grafiken verlangsamen sich aufgrund des aufgeblähten DOM merklich.  Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vegas</a> - DSL - Bibliothek auf Scala zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen von Vega-Lite -</a> Spezifikationen kann dieses Problem lösen.  Vegas bietet nicht nur umfassendere Visualisierungsfunktionen (vergleichbar mit matplotlib), sondern zeichnet sie auch auf Canvas, ohne das DOM aufzublasen :). </p><br><p>  Die Spezifikation des Diagramms, an dem wir interessiert sind, sieht folgendermaßen aus: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(width = <span class="hljs-number"><span class="hljs-number">1024</span></span>, height = <span class="hljs-number"><span class="hljs-number">648</span></span>) <span class="hljs-comment"><span class="hljs-comment">//   .withDataFrame(comparison.na.fill(0.0)) //           .encodeX("meanDrift", Quant, scale = Scale(domainValues = List(-1.0, 1.0), clamp = true)) //   -       .encodeY("train_fill", Quant) //       .encodeColor("outlieres", Quant, scale=Scale( rangeNominals=List("#00FF00", "#FF0000"), domainValues = List(0.0, 5), clamp = true)) //       .encodeSize("skewenes", Quant) //   -   (   ) .encodeShape("mode", Nom) .mark(vegas.Point) .show</span></span></code> </pre> <br><p>  Die folgende Tabelle sollte folgendermaßen lauten: </p><br><ul><li>  Die X-Achse zeigt die Verschiebung der Verteilungszentren zwischen Test- und Trainingssatz (je näher an 0, desto stabiler das Vorzeichen). </li><li>  Der Prozentsatz der Elemente ungleich Null ist entlang der Y-Achse aufgetragen (je höher, desto mehr Daten sind für die größere Anzahl von Punkten pro Attribut vorhanden). </li><li>  Die Größe zeigt die Verschiebung des Durchschnitts relativ zum Median (je größer der Punkt, desto wahrscheinlicher ist die Verteilung des Potenzgesetzes dafür). </li><li>  Farbe zeigt Emissionen an (je röter, desto mehr Emissionen). </li><li>  Nun, das Formular zeichnet sich durch einen Vergleichsmodus aus: mit einem Benutzerfilter im Trainingssatz oder ohne Filter. </li></ul><br><p><img src="https://habrastorage.org/webt/op/hb/6g/ophb6gi3wa_uvsld9rre6ujzquu.png"></p><br><p>  Wir können also die folgenden Schlussfolgerungen ziehen: </p><br><ul><li>  Einige Zeichen benötigen einen Emissionsfilter - wir begrenzen die Maximalwerte für das 90. Perzentil. </li><li>  Einige Zeichen zeigen eine Verteilung nahe der Exponentialverteilung - wir nehmen den Logarithmus. </li><li>  Einige Funktionen werden im Test nicht vorgestellt - wir werden sie vom Training ausschließen. </li></ul><br><h1 id="korrelyacionnyy-analiz">  Korrelationsanalyse </h1><br><p>  Nachdem Sie eine allgemeine Vorstellung davon erhalten haben, wie die Attribute verteilt sind und wie sie sich zwischen den Trainings- und Testsätzen verhalten, versuchen wir, die Korrelationen zu analysieren.  Konfigurieren Sie dazu den Feature-Extraktor basierend auf früheren Beobachtungen: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//             val expressions = filteredTrain.schema.fieldNames //          .filterNot(x =&gt; x == "date" || x == "audit_experiment" || idsColumns(x) || x.contains("vd_")) .map(x =&gt; if(skewedFeautres(x)) { //      s"log($x) AS $x" } else { //     cappedFeatures.get(x).map(capping =&gt; s"IF($x &lt; $capping, $x, $capping) AS $x").getOrElse(x) }) val rawFeaturesExtractor = new Pipeline().setStages(Array( new SQLTransformer().setStatement(s"SELECT ${expressions.mkString(", ")} FROM __THIS__"), new NullToDefaultReplacer(), new AutoAssembler().setOutputCol("features") )) //       val raw = rawFeaturesExtractor.fit(filteredTrain).transform( filteredTrain.where(isTestSimilar($"instanceId_userId")))</span></span></code> </pre> <br><p>  Von den neuen Maschinen in dieser Pipeline fällt das Dienstprogramm SQLTransformer auf, das beliebige SQL-Transformationen der Eingabetabelle ermöglicht. </p><br><p>  Bei der Analyse von Korrelationen ist es wichtig, das Rauschen herauszufiltern, das durch die natürliche Korrelation von One-Hot-Features erzeugt wird.  Dazu möchte ich verstehen, welche Elemente des Vektors welchen Quellenspalten entsprechen.  Diese Aufgabe in Spark wird mithilfe von Spaltenmetadaten (mit Daten gespeichert) und Attributgruppen ausgeführt.  Der folgende Codeblock wird verwendet, um Paare von Attributnamen zu filtern, die aus derselben Spalte vom Typ String stammen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> attributes = <span class="hljs-type"><span class="hljs-type">AttributeGroup</span></span>.fromStructField(raw.schema(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)).attributes.get <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> originMap = filteredTrain .schema.filter(_.dataType == <span class="hljs-type"><span class="hljs-type">StringType</span></span>) .flatMap(x =&gt; attributes.map(_.name.get).filter(_.startsWith(x.name + <span class="hljs-string"><span class="hljs-string">"_"</span></span>)).map(_ -&gt; x.name)) .toMap <span class="hljs-comment"><span class="hljs-comment">//   ,          val isNonTrivialCorrelation = sqlContext.udf.register("isNonTrivialCorrelation", (x: String, y : String) =&gt; //    Scala-quiz   Option originMap.get(x).map(_ != originMap.getOrElse(y, "")).getOrElse(true))</span></span></code> </pre> <br><p>  Wenn Sie einen Datensatz mit einer Vektorspalte zur Hand haben, ist die Berechnung von Kreuzkorrelationen mit Spark recht einfach. Das Ergebnis ist jedoch eine Matrix, für deren Bereitstellung Sie ein wenig in eine Reihe von Paaren spielen müssen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pearsonCorrelation = <span class="hljs-comment"><span class="hljs-comment">//    Pearson  Spearman Correlation.corr(raw, "features", "pearson").rdd.flatMap( //           _.getAs[Matrix](0).rowIter.zipWithIndex.flatMap(x =&gt; { //   ,   (  , //  ) val name = attributes(x._2).name.get //    ,     x._1.toArray.zip(attributes).map(y =&gt; (name, y._2.name.get, y._1)) } //     DataFrame )).toDF("feature1", "feature2", "corr") .na.drop //   .where(isNonTrivialCorrelation($"feature1", $"feature2")) //    . pearsonCorrelation.coalesce(1).write.mode("overwrite") .parquet("sna2019/pearsonCorrelation")</span></span></code> </pre> <br><p>  Und natürlich Visualisierung: Wir brauchen wieder Vegas Hilfe, um eine Heatmap zu zeichnen: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(<span class="hljs-string"><span class="hljs-string">"Pearson correlation heatmap"</span></span>) .withDataFrame(pearsonCorrelation .withColumn(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, $<span class="hljs-string"><span class="hljs-string">"corr"</span></span> &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) .withColumn(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, functions.abs($<span class="hljs-string"><span class="hljs-string">"corr"</span></span>)) .where(<span class="hljs-string"><span class="hljs-string">"feature1 &lt; feature2 AND abs_corr &gt; 0.05"</span></span>) .orderBy(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-string"><span class="hljs-string">"feature2"</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeY(<span class="hljs-string"><span class="hljs-string">"feature2"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeColor(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>, scale=<span class="hljs-type"><span class="hljs-type">Scale</span></span>(rangeNominals=<span class="hljs-type"><span class="hljs-type">List</span></span>(<span class="hljs-string"><span class="hljs-string">"#FFFFFF"</span></span>, <span class="hljs-string"><span class="hljs-string">"#FF0000"</span></span>))) .encodeShape(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Point</span></span>) .show</code> </pre> <br><p>  Das Ergebnis ist besser in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl-e zu sehen</a> .  Für ein allgemeines Verständnis: </p><br><p><img src="https://habrastorage.org/webt/nm/d9/bm/nmd9bmrion4goaov9_vgx5vbjoy.png"></p><br><p>  Die Wärmekarte zeigt, dass einige Korrelationen eindeutig vorhanden sind.  Versuchen wir, die Blöcke der am stärksten korrelierten Merkmale auszuwählen. Dazu verwenden wir die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GraphX-</a> Bibliothek: Wir wandeln die Korrelationsmatrix in ein Diagramm um, filtern die Kanten nach Gewicht. Danach finden wir die verbundenen Komponenten und lassen nur nicht entartete (von mehr als einem Element) übrig.  Ein solches Verfahren ähnelt im Wesentlichen der Anwendung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DBSCAN-</a> Algorithmus und ist wie folgt: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (GrpahX   ID) val featureIndexMap = spearmanCorrelation.select("feature1").distinct.rdd.map( _.getString(0)).collect.zipWithIndex.toMap val featureIndex = sqlContext.udf.register("featureIndex", (x: String) =&gt; featureIndexMap(x)) //    val vertices = sc.parallelize(featureIndexMap.map(x =&gt; x._2.toLong -&gt; x._1).toSeq, 1) //    val edges = spearmanCorrelation.select(featureIndex($"feature1"), featureIndex($"feature2"), $"corr") //     .where("ABS(corr) &gt; 0.7") .rdd.map(r =&gt; Edge(r.getInt(0), r.getInt(1), r.getDouble(2))) //       val components = Graph(vertices, edges).connectedComponents() val reversedMap = featureIndexMap.map(_.swap) //    ,    ,   //   val clusters = components .vertices.map(x =&gt; reversedMap(x._2.toInt) -&gt; reversedMap(x._1.toInt)) .groupByKey().map(x =&gt; x._2.toSeq) .filter(_.size &gt; 1) .sortBy(-_.size) .collect</span></span></code> </pre> <br><p>  Das Ergebnis wird in Form einer Tabelle dargestellt: </p><br><p><img src="https://habrastorage.org/webt/4b/t2/bl/4bt2blcjzmxbzucnm7zynpvpj-q.png"></p><br><p>  Basierend auf den Ergebnissen des Clusters können wir schließen, dass die am meisten korrelierten Gruppen um Zeichen gebildet werden, die mit der Benutzermitgliedschaft in der Gruppe verbunden sind (Membership_status_A) sowie um den Objekttyp (instanceId_objectType).  Für die beste Modellierung der Interaktion von Zeichen ist es sinnvoll, eine Modellsegmentierung anzuwenden, um verschiedene Modelle für verschiedene Objekttypen zu trainieren, getrennt für Gruppen, in denen der Benutzer ist und nicht. </p><br><h1 id="mashinnoe-obuchenie">  Maschinelles Lernen </h1><br><p>  Wir nähern uns dem Interessantesten - dem maschinellen Lernen.  Die Pipeline zum Trainieren des einfachsten Modells (logistische Regression) unter Verwendung von SparkML- und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML-</a> Erweiterungen lautet wie folgt: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement( <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>)))</code> </pre> <br><p>  Hier sehen wir nicht nur viele bekannte Elemente, sondern auch einige neue: </p><br><ul><li>  LogisticRegressionLBFSG ist ein Schätzer mit verteiltem Training der logistischen Regression. </li><li>  Um mit verteilten ML-Algorithmen maximale Leistung zu erzielen.  Daten sollten optimal auf Partitionen verteilt werden.  Das Dienstprogramm UnwrappedStage.repartition hilft dabei, indem es der Pipeline eine Neupartitionsoperation hinzufügt, sodass sie nur in der Schulungsphase verwendet wird (schließlich ist es beim Erstellen von Prognosen nicht mehr erforderlich). </li><li>  Damit könnte das lineare Modell ein gutes Ergebnis liefern.  Daten müssen skaliert werden, für die das Dienstprogramm Scaler.scale verantwortlich ist.  Das Vorhandensein von zwei aufeinanderfolgenden linearen Transformationen (Skalierung und Multiplikation mit den Regressionsgewichten) führt jedoch zu unnötigen Kosten, und es ist wünschenswert, diese Operationen zu reduzieren.  Bei Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML ist</a> die Ausgabe ein sauberes Modell mit einer Transformation :). </li><li>  Natürlich benötigen Sie für solche Modelle ein kostenloses Mitglied, das wir mithilfe der Operation Interceptor.intercept hinzufügen. </li></ul><br><p>  Die resultierende Pipeline, die auf alle Daten angewendet wird, ergibt AUC 0,6889 pro Benutzer (Validierungscode ist auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl</a> verfügbar).  Jetzt müssen wir alle unsere Forschungen anwenden: Daten filtern, Merkmale transformieren und Segmentmodelle.  Die endgültige Pipeline sieht folgendermaßen aus: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">s"SELECT instanceId_userId, instanceId_objectId, </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${expressions.mkString(", ")}</span></span></span><span class="hljs-string"> FROM __THIS__"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label, concat(IF(membership_status = 'A', 'OwnGroup_', 'NonUser_'), instanceId_objectType) AS type FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>, <span class="hljs-string"><span class="hljs-string">"type"</span></span>,<span class="hljs-string"><span class="hljs-string">"instanceId_objectType"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">CombinedModel</span></span>.perType( <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>))), numThreads = <span class="hljs-number"><span class="hljs-number">6</span></span>) ))</code> </pre> <br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> —    CombinedModel.perType.       ,     numThreads = 6.             . </p><br><p> ,   ,  per-user AUC 0.7004.    ?  ,   " "    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">XGBoost</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">XGBoostRegressor</span></span>() .setNumRounds(<span class="hljs-number"><span class="hljs-number">100</span></span>) .setMaxDepth(<span class="hljs-number"><span class="hljs-number">15</span></span>) .setObjective(<span class="hljs-string"><span class="hljs-string">"reg:logistic"</span></span>) .setNumWorkers(<span class="hljs-number"><span class="hljs-number">17</span></span>) .setNthread(<span class="hljs-number"><span class="hljs-number">4</span></span>) .setTrackerConf(<span class="hljs-number"><span class="hljs-number">600000</span></span>L, <span class="hljs-string"><span class="hljs-string">"scala"</span></span>) ))</code> </pre> <br><p> ,     — XGBoost  Spark !         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DLMC</a> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> ,       (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ).  XGboost " "   10     per-user AUC 0.6981. </p><br><h1 id="analiz-rezultatov">   </h1><br><p> ,     ,  ,       .    SparkML     ,      .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a>  :      Parquet            Spark: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//     val perTypeWeights = sqlContext.read.parquet("sna2019/perType/stages/*/weights") //     20    ( //  ) val topFeatures = new TopKTransformer[Double]() .setGroupByColumns("type") .setColumnToOrderGroupsBy("abs_weight") .setTopK(20) .transform(perTypeWeights.withColumn("abs_weight", functions.abs($"unscaled_weight"))) .orderBy("type", "unscaled_weight")</span></span></code> </pre> <br><p>     Parquet,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> — TopKTransformer,           . </p><br><p>      Vegas (   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl</a> ): </p><br><p><img src="https://habrastorage.org/webt/q7/_n/fn/q7_nfnto-hw-9nbdgjf3chhm0oc.png"></p><br><p> ,    -   .      XGBoost? </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> significance = sqlContext.read.parquet( <span class="hljs-string"><span class="hljs-string">"sna2019/xgBoost15_100_raw/stages/*/featuresSignificance"</span></span> vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>() .withDataFrame(significance.na.drop.orderBy($<span class="hljs-string"><span class="hljs-string">"significance"</span></span>.desc).limit(<span class="hljs-number"><span class="hljs-number">40</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"name"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>, sortField = <span class="hljs-type"><span class="hljs-type">Sort</span></span>(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">AggOps</span></span>.<span class="hljs-type"><span class="hljs-type">Mean</span></span>)) .encodeY(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Bar</span></span>) .show</code> </pre> <br><p><img src="https://habrastorage.org/webt/i2/oy/zb/i2oyzbrjlo15x7u1uwadmeswocq.png"></p><br><p>  ,   ,   XGBoost,         ,    .           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .   ,  XGBoost     ,    ,   . </p><br><h1 id="vyvody">  Schlussfolgerungen </h1><br><p>  ,       :).     : </p><br><ol><li>    ,     Scala  Spark    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> . </li><li>    Scala  Spark        Python:    ETL  ML,    ,      ,     . </li><li>  ,   ,   ,    (,  )    ,     ,      . </li><li> ,     ,       .          ,      ,     , -, . </li></ol><br><p> ,       ,    ,        ,    -.        , ,  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   Scala</a> "  Newprolab. </p><br><p> ,  ,      —   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442688/">https://habr.com/ru/post/de442688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442678/index.html">Größtes Projekt in der Stereolithographie: Mammutskelett auf einem 3D-Drucker gedruckt</a></li>
<li><a href="../de442680/index.html">Mit sensorischen Ersatztechnologien können Sie die Welt mithilfe von Geräuschen sehen: Wie die Neuroplastizität des menschlichen Gehirns funktioniert</a></li>
<li><a href="../de442682/index.html">Was Sie eingeben und wie Sie ein C ++ - Projekt zusammenstellen</a></li>
<li><a href="../de442684/index.html">Ausgewogene Site-Leistung. Teil 3: Inhalt</a></li>
<li><a href="../de442686/index.html">DataPower-Lernprogramm</a></li>
<li><a href="../de442690/index.html">Mondmission "Bereshit" - Selfie auf dem Hintergrund der Erde</a></li>
<li><a href="../de442692/index.html">Blockchain ohne Zwischenhändler: Wie wir Wertpapiere an eine verteilte Registrierung gesendet haben</a></li>
<li><a href="../de442694/index.html">Einer der Streaming-Giganten startete in Indien und zog in einer Woche eine Million Nutzer an</a></li>
<li><a href="../de442696/index.html">S for Security: Internetsicherheit von Dingen und Berichten bei InoThings ++ 2019</a></li>
<li><a href="../de442698/index.html">Moskauer U-Bahn-Anwendung für den Windows Store</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>