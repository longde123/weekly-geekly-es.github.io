<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí∏ üîâ üë®üèª‚Äçüé§ Scala-Datenanalyse - ein dringender Bedarf oder eine angenehme Gelegenheit? üèîÔ∏è üßìüèª ü•Ä</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Traditionelle Tools auf dem Gebiet der Datenwissenschaft sind Sprachen wie R und Python - eine entspannte Syntax und eine gro√üe Anzahl von Bibliotheke...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Scala-Datenanalyse - ein dringender Bedarf oder eine angenehme Gelegenheit?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442688/"><p><img src="https://habrastorage.org/webt/5q/1e/fr/5q1efrbkxbuk_ndqoensinfl80a.jpeg"></p><br><p>  Traditionelle Tools auf dem Gebiet der Datenwissenschaft sind Sprachen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">R</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python</a> - eine entspannte Syntax und eine gro√üe Anzahl von Bibliotheken f√ºr maschinelles Lernen und Datenverarbeitung erm√∂glichen es Ihnen, schnell einige funktionierende L√∂sungen zu erhalten.  Es gibt jedoch Situationen, in denen die Einschr√§nkungen dieser Tools zu einem erheblichen Hindernis werden - vor allem dann, wenn eine hohe Verarbeitungsgeschwindigkeit und / oder die Arbeit mit wirklich gro√üen Datenmengen erforderlich sind.  In diesem Fall muss sich der Spezialist widerstrebend an die "dunkle Seite" wenden und Tools in den "industriellen" Programmiersprachen verbinden: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Scala</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Java</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">C ++</a> . </p><br><p> Aber ist diese Seite so dunkel?  Im Laufe der Jahre der Entwicklung haben die Tools der "industriellen" Data Science einen langen Weg zur√ºckgelegt und unterscheiden sich heute erheblich von ihren eigenen Versionen vor 2-3 Jahren.  Versuchen wir am Beispiel der Aufgabe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019</a> herauszufinden, inwieweit das Scala + Spark-√ñkosystem Python Data Science entsprechen kann. </p><a name="habracut"></a><br><p>  Im Rahmen des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019</a> l√∂sen die Teilnehmer das Problem, den Newsfeed eines Nutzers eines sozialen Netzwerks in einer von drei ‚ÄûDisziplinen‚Äú zu sortieren: anhand von Daten aus Texten, Bildern oder Feature-Logs.  In dieser Ver√∂ffentlichung werden wir untersuchen, wie es in Spark m√∂glich ist, ein Problem anhand eines Zeichenprotokolls mit klassischen maschinellen Lernwerkzeugen zu l√∂sen. </p><br><p>  Bei der L√∂sung des Problems gehen wir den Standard vor, den jeder Datenanalysespezialist bei der Entwicklung eines Modells durchl√§uft: </p><br><ul><li>  Wir werden Forschungsdaten analysieren, Diagramme erstellen. </li><li>  Wir analysieren die statistischen Eigenschaften von Zeichen in den Daten und untersuchen deren Unterschiede zwischen Trainings- und Tests√§tzen. </li><li>  Wir werden eine erste Auswahl von Merkmalen basierend auf statistischen Eigenschaften durchf√ºhren. </li><li>  Wir berechnen die Korrelationen zwischen den Zeichen und der Zielvariablen sowie die Kreuzkorrelation zwischen den Zeichen. </li><li>  Wir werden die endg√ºltigen Funktionen bilden, das Modell trainieren und seine Qualit√§t √ºberpr√ºfen. </li><li>  Analysieren wir die interne Struktur des Modells, um Wachstumspunkte zu identifizieren. </li></ul><br><p>  W√§hrend unserer ‚ÄûReise‚Äú werden wir Tools wie das interaktive <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zeppelin-</a> Notizbuch, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spark ML-</a> Bibliothek f√ºr maschinelles Lernen und deren Erweiterung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> , das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GraphX-</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grafikpaket</a> , die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vegas-</a> Visualisierungsbibliothek und nat√ºrlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Spark</a> in seiner ganzen Pracht kennenlernen: )  Alle Code- und Versuchsergebnisse sind auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl-Plattform f√ºr</a> kollaborative Notizb√ºcher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verf√ºgbar</a> . </p><br><h1 id="zagruzka-dannyh">  Laden von Daten </h1><br><p>  Die Besonderheit der beim <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019 bereitgestellten</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daten</a> besteht darin, dass sie direkt mit Python verarbeitet werden k√∂nnen, dies ist jedoch schwierig: Die Originaldaten werden dank der Funktionen des Apache Parquet-Spaltenformats recht effizient gepackt und beim Lesen in den Speicher ‚Äû√ºber die Stirn‚Äú in mehrere zehn Gigabyte dekomprimiert.  Bei der Arbeit mit Apache Spark m√ºssen die Daten nicht vollst√§ndig in den Speicher geladen werden. Die Spark-Architektur ist so konzipiert, dass Daten in Teilen verarbeitet und bei Bedarf von der Festplatte geladen werden. </p><br><p>  Daher kann der erste Schritt - √úberpr√ºfen der Datenverteilung nach Tag - problemlos mit Box-Tools ausgef√ºhrt werden: </p><br><pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show(train.groupBy($<span class="hljs-string"><span class="hljs-string">"date"</span></span>).agg( functions.count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"count"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"users"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"objects"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"owners"</span></span>)) .orderBy(<span class="hljs-string"><span class="hljs-string">"date"</span></span>))</code> </pre> <br><p>  Was die entsprechende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grafik</a> in Zeppelin anzeigt: </p><br><p><img src="https://habrastorage.org/webt/jp/lh/pw/jplhpwwz4tfgdtrs1u_u-tqsvzi.png"></p><br><p>  Ich muss sagen, dass die Scala-Syntax sehr flexibel ist und derselbe Code beispielsweise folgenderma√üen aussehen kann: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show( train groupBy $<span class="hljs-string"><span class="hljs-string">"date"</span></span> agg( count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"count"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"users"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"objects"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"owners"</span></span>) orderBy <span class="hljs-string"><span class="hljs-string">"date"</span></span> )</code> </pre> <br><p>  Hier ist eine wichtige Warnung zu beachten: Wenn man in einem gro√üen Team arbeitet, in dem jeder das Schreiben von Scala-Code ausschlie√ülich aus der Sicht seines eigenen Geschmacks betrachtet, ist die Kommunikation viel schwieriger.  Es ist daher besser, ein einheitliches Konzept f√ºr den Codestil zu entwickeln. </p><br><p>  Aber zur√ºck zu unserer Aufgabe.  Eine einfache Analyse am Tag zeigte das Vorhandensein abnormaler Punkte am 17. und 18. Februar;  Wahrscheinlich wurden heutzutage unvollst√§ndige Daten gesammelt und die Verteilung der Merkmale kann verzerrt sein.  Dies sollte bei der weiteren Analyse ber√ºcksichtigt werden.  Dar√ºber hinaus f√§llt auf, dass die Anzahl der eindeutigen Benutzer sehr nahe an der Anzahl der Objekte liegt. Daher ist es sinnvoll, die Verteilung der Benutzer mit unterschiedlicher Anzahl von Objekten zu untersuchen: </p><br><pre> <code class="scala hljs">z.show(filteredTrain .groupBy($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).count .groupBy(<span class="hljs-string"><span class="hljs-string">"count"</span></span>).agg(functions.log(functions.count(<span class="hljs-string"><span class="hljs-string">"count"</span></span>)).as(<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>)) .orderBy($<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>.desc) .limit(<span class="hljs-number"><span class="hljs-number">100</span></span>) .orderBy($<span class="hljs-string"><span class="hljs-string">"count"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/dh/i-/pp/dhi-ppdka19zurhimccpq7tij5w.png"></p><br><p>  Es wird eine nahezu exponentielle Verteilung mit einem sehr langen Schwanz erwartet.  Bei solchen Aufgaben ist es in der Regel m√∂glich, die Arbeitsqualit√§t zu verbessern, indem Modelle f√ºr Benutzer mit unterschiedlichen Aktivit√§tsstufen segmentiert werden.  Um zu √ºberpr√ºfen, ob es sich lohnt, dies zu tun, vergleichen Sie die Verteilung der Anzahl der Objekte nach Benutzer im Testsatz: </p><br><p><img src="https://habrastorage.org/webt/du/sy/xt/dusyxten5shm24an736n7akolnm.png"></p><br><p>  Ein Vergleich mit dem Test zeigt, dass Testbenutzer mindestens zwei Objekte in den Protokollen haben (da das Ranking-Problem beim Hackathon gel√∂st ist, ist dies eine notwendige Voraussetzung f√ºr die Bewertung der Qualit√§t).  In Zukunft empfehle ich, die Benutzer im Trainingssatz, f√ºr die wir die benutzerdefinierte Funktion mit einem Filter deklarieren, genauer zu betrachten: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//  ,     "",   , //     val testSimilar = sc.broadcast(filteredTrain.groupBy($"instanceId_userId") .agg( functions.count("feedback").as("count"), functions.sum(functions.expr("IF(array_contains(feedback, 'Liked'), 1.0, 0.0)")).as("sum") ) .where("count &gt; sum AND sum &gt; 0") .select("instanceId_userId").rdd.map(_.getInt(0)).collect.sorted) //           // User Defined Function val isTestSimilar = sqlContext.udf.register("isTestSimilar", (x: Int) =&gt; java.util.Arrays.binarySearch(testSimilar.value, x) &gt;= 0)</span></span></code> </pre> <br><p>  Auch hier ist eine wichtige Bemerkung zu machen: Unter dem Gesichtspunkt der Definition von UDF unterscheidet sich die Verwendung von Spark unter Scala / Java und unter Python deutlich.  W√§hrend der PySpark-Code die Grundfunktionalit√§t verwendet, funktioniert alles fast genauso schnell, aber wenn √ºberschriebene Funktionen angezeigt werden, verschlechtert sich die Leistung von PySpark um eine Gr√∂√üenordnung. </p><br><h1 id="pervyy-ml-konveyer">  Die erste ML-Pipeline </h1><br><p>  Im n√§chsten Schritt werden wir versuchen, die grundlegenden Statistiken zu Aktionen und Attributen zu berechnen.  Daf√ºr ben√∂tigen wir jedoch die Funktionen von SparkML. Zun√§chst werden wir uns die allgemeine Architektur ansehen: </p><br><p><img src="https://habrastorage.org/webt/j7/ev/en/j7evenhyzzfvzouqfkbfq1j9hvu.png"></p><br><p>  SparkML basiert auf folgenden Konzepten: </p><br><ul><li>  Transformator - Nimmt einen Datensatz als Eingabe und gibt einen ge√§nderten Satz (Transformation) zur√ºck.  In der Regel wird es zur Implementierung von Vor- und Nachbearbeitungsalgorithmen sowie zur Merkmalsextraktion verwendet und kann auch die resultierenden ML-Modelle darstellen. </li><li>  Sch√§tzer - Nimmt einen Datensatz als Eingabe und gibt Transformer (fit) zur√ºck.  Nat√ºrlich kann Estimator den ML-Algorithmus darstellen. </li><li>  Pipeline ist ein Sonderfall von Estimator, der aus einer Kette von Transformatoren und Sch√§tzern besteht.  Wenn die Methode aufgerufen wird, durchl√§uft fit die Kette, und wenn es einen Transformator sieht, wendet es ihn auf die Daten an, und wenn es einen Sch√§tzer sieht, trainiert es den Transformator damit, wendet ihn auf die Daten an und geht weiter. </li><li>  PipelineModel - Das Ergebnis von Pipeline enth√§lt auch eine Kette im Inneren, die jedoch ausschlie√ülich aus Transformatoren besteht.  Dementsprechend ist PipelineModel selbst auch ein Transformator. </li></ul><br><p>  Ein solcher Ansatz zur Bildung von ML-Algorithmen tr√§gt zu einer klaren modularen Struktur und einer guten Reproduzierbarkeit bei - sowohl Modelle als auch Pipelines k√∂nnen gespeichert werden. </p><br><p>  Zun√§chst erstellen wir eine einfache Pipeline, mit der wir die Statistik der Verteilung der Aktionen (Feedbackfeld) der Benutzer im Trainingssatz berechnen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> feedbackAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-comment"><span class="hljs-comment">//         (feedback)  one-hot  new MultinominalExtractor().setInputCol("feedback").setOutputCol("feedback"), //       new VectorStatCollector() .setGroupByColumns("date").setInputCol("feedback") .setPercentiles(Array(0.1,0.5,0.9)), //        new VectorExplode().setValueCol("feedback") )).fit(train) z.show(feedbackAggregator .transform(filteredTrain) .orderBy($"date", $"feedback"))</span></span></code> </pre> <br><p>  In dieser Pipeline wird die Funktionalit√§t von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> aktiv genutzt - Bibliotheken mit erweiterten n√ºtzlichen Bl√∂cken f√ºr SparkML, n√§mlich: </p><br><ul><li>  MultinominalExtractor wird verwendet, um ein Zeichen vom Typ "Array of Strings" nach dem One-Hot-Prinzip in einen Vektor zu codieren.  Dies ist der einzige Sch√§tzer in der Pipeline (um eine Codierung zu erstellen, m√ºssen Sie eindeutige Zeilen aus dem Datensatz erfassen). </li><li>  Mit VectorStatCollector werden Vektorstatistiken berechnet. </li><li>  Mit VectorExplode wird das Ergebnis in ein f√ºr die Visualisierung geeignetes Format konvertiert. </li></ul><br><p>  Das Ergebnis der Arbeit wird eine Grafik sein, die zeigt, dass die Klassen im Datensatz nicht ausgeglichen sind. Das Ungleichgewicht f√ºr die Zielklasse "Gef√§llt mir" ist jedoch nicht extrem: </p><br><p><img src="https://habrastorage.org/webt/aa/db/x4/aadbx4el5s5lhuyput_cj4ns9zo.png"></p><br><p>  Die Analyse einer √§hnlichen Verteilung unter Benutzern, die Testbenutzern √§hnlich sind (mit "positiv" und "negativ" in den Protokollen), zeigt, dass sie auf die positive Klasse ausgerichtet ist: </p><br><p><img src="https://habrastorage.org/webt/x5/lu/px/x5lupxmayvvodo7lsb3meob1dou.png"></p><br><h1 id="statisticheskiy-analiz-priznakov">  Statistische Analyse von Zeichen </h1><br><p>  In der n√§chsten Phase werden wir eine detaillierte Analyse der statistischen Eigenschaften der Attribute durchf√ºhren.  Diesmal brauchen wir einen gr√∂√üeren F√∂rderer: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> statsAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-comment"><span class="hljs-comment">//          new AutoAssembler() .setColumnsToExclude( (Seq("date", "feedback") ++ train.schema.fieldNames.filter(_.endsWith("Id")) : _*)) .setOutputCol("features"), new VectorStatCollector() .setGroupByColumns("date").setInputCol("features") .setPercentiles(Array(0.1,0.5,0.9)), new VectorExplode().setValueCol("features") ))</span></span></code> </pre> <br><p>  Da wir jetzt nicht mit einem separaten Feld arbeiten m√ºssen, sondern mit allen Attributen gleichzeitig, werden wir zwei weitere n√ºtzliche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML-</a> Dienstprogramme verwenden: </p><br><ul><li>  Mit NullToDefaultReplacer k√∂nnen Sie fehlende Elemente in den Daten durch ihre Standardwerte ersetzen (0 f√ºr Zahlen, false f√ºr logische Variablen usw.).  Wenn Sie diese Konvertierung nicht durchf√ºhren, werden NaN-Werte in den resultierenden Vektoren angezeigt, was f√ºr viele Algorithmen fatal ist (obwohl beispielsweise XGBoost dies √ºberleben kann).  Eine Alternative zum Ersetzen durch Nullen kann das Ersetzen durch Durchschnittswerte sein. Dies ist in NaNToMeanReplacerEstimator implementiert. </li><li>  AutoAssembler ist ein sehr leistungsf√§higes Dienstprogramm, das das Tabellenlayout analysiert und f√ºr jede Spalte ein Vektorisierungsschema ausw√§hlt, das dem Spaltentyp entspricht. </li></ul><br><p>  Mit der resultierenden Pipeline berechnen wir die Statistiken f√ºr drei S√§tze (Training, Training mit Benutzerfilter und Test) und speichern sie in separaten Dateien: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (   AutoAssembler  ) val trained = statsAggregator.fit(filteredTrain) //       - ,     . trained .transform(filteredTrain .withColumn("date", //  ,      ,     , //        All   functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/featuresStat") trained .transform(filteredTrain .where(isTestSimilar($"instanceId_userId")) .withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/filteredFeaturesStat") trained .transform(filteredTest.withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(3).write.mode("overwrite").parquet("sna2019/testFeaturesStat")</span></span></code> </pre> <br><p>  Nachdem wir drei Datens√§tze mit Attributstatistiken erhalten haben, analysieren wir die folgenden Dinge: </p><br><ul><li>  Haben wir Zeichen, f√ºr die es gro√üe Emissionen gibt? <br>  - Solche Zeichen sollten begrenzt oder Ausrei√üerdatens√§tze herausgefiltert werden. </li><li>  Haben wir Zeichen mit einer gro√üen Abweichung des Mittelwerts relativ zum Median? <br>  - Eine solche Verschiebung tritt h√§ufig bei Vorhandensein einer Leistungsverteilung auf. Es ist sinnvoll, diese Vorzeichen zu logarithmieren. </li><li>  Gibt es eine Verschiebung der durchschnittlichen Verteilungen zwischen Trainings- und Tests√§tzen? </li><li>  Wie dicht unsere Feature-Matrix gef√ºllt ist. </li></ul><br><p>  Um diese Aspekte zu kl√§ren, hilft uns eine solche Anfrage: </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareWithTest</span></span></span></span>(data: <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>) : <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { data.where(<span class="hljs-string"><span class="hljs-string">"date = 'All'"</span></span>) .select( $<span class="hljs-string"><span class="hljs-string">"features"</span></span>, <span class="hljs-comment"><span class="hljs-comment">//         // ( ) functions.log($"features_mean" / $"features_p50").as("skewenes"), //    90-      //    90-  ‚Äî    functions.log( ($"features_max" - $"features_p90") / ($"features_p90" - $"features_p50")).as("outlieres"), //       ,  //    ($"features_nonZeros" / $"features_count").as("train_fill"), $"features_mean".as("train_mean")) .join(testStat.where("date = 'All'") .select($"features", $"features_mean".as("test_mean"), ($"features_nonZeros" / $"features_count").as("test_fill")), Seq("features")) //          .withColumn("meanDrift", (($"train_mean" - $"test_mean" ) / ($"train_mean" + $"test_mean"))) //      .withColumn("fillDrift", ($"train_fill" - $"test_fill") / ($"train_fill" + $"test_fill")) } //         val comparison = compareWithTest(trainStat).withColumn("mode", functions.lit("raw")) .unionByName(compareWithTest(filteredStat).withColumn("mode", functions.lit("filtered")))</span></span></code> </pre> <br><p>  In dieser Phase ist die Frage nach der Visualisierung dringend: Es ist schwierig, alle Aspekte mit normalen Zeppelin-Werkzeugen sofort anzuzeigen, und Notebooks mit einer gro√üen Anzahl von Grafiken verlangsamen sich aufgrund des aufgebl√§hten DOM merklich.  Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vegas</a> - DSL - Bibliothek auf Scala zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen von Vega-Lite -</a> Spezifikationen kann dieses Problem l√∂sen.  Vegas bietet nicht nur umfassendere Visualisierungsfunktionen (vergleichbar mit matplotlib), sondern zeichnet sie auch auf Canvas, ohne das DOM aufzublasen :). </p><br><p>  Die Spezifikation des Diagramms, an dem wir interessiert sind, sieht folgenderma√üen aus: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(width = <span class="hljs-number"><span class="hljs-number">1024</span></span>, height = <span class="hljs-number"><span class="hljs-number">648</span></span>) <span class="hljs-comment"><span class="hljs-comment">//   .withDataFrame(comparison.na.fill(0.0)) //           .encodeX("meanDrift", Quant, scale = Scale(domainValues = List(-1.0, 1.0), clamp = true)) //   -       .encodeY("train_fill", Quant) //       .encodeColor("outlieres", Quant, scale=Scale( rangeNominals=List("#00FF00", "#FF0000"), domainValues = List(0.0, 5), clamp = true)) //       .encodeSize("skewenes", Quant) //   -   (   ) .encodeShape("mode", Nom) .mark(vegas.Point) .show</span></span></code> </pre> <br><p>  Die folgende Tabelle sollte folgenderma√üen lauten: </p><br><ul><li>  Die X-Achse zeigt die Verschiebung der Verteilungszentren zwischen Test- und Trainingssatz (je n√§her an 0, desto stabiler das Vorzeichen). </li><li>  Der Prozentsatz der Elemente ungleich Null ist entlang der Y-Achse aufgetragen (je h√∂her, desto mehr Daten sind f√ºr die gr√∂√üere Anzahl von Punkten pro Attribut vorhanden). </li><li>  Die Gr√∂√üe zeigt die Verschiebung des Durchschnitts relativ zum Median (je gr√∂√üer der Punkt, desto wahrscheinlicher ist die Verteilung des Potenzgesetzes daf√ºr). </li><li>  Farbe zeigt Emissionen an (je r√∂ter, desto mehr Emissionen). </li><li>  Nun, das Formular zeichnet sich durch einen Vergleichsmodus aus: mit einem Benutzerfilter im Trainingssatz oder ohne Filter. </li></ul><br><p><img src="https://habrastorage.org/webt/op/hb/6g/ophb6gi3wa_uvsld9rre6ujzquu.png"></p><br><p>  Wir k√∂nnen also die folgenden Schlussfolgerungen ziehen: </p><br><ul><li>  Einige Zeichen ben√∂tigen einen Emissionsfilter - wir begrenzen die Maximalwerte f√ºr das 90. Perzentil. </li><li>  Einige Zeichen zeigen eine Verteilung nahe der Exponentialverteilung - wir nehmen den Logarithmus. </li><li>  Einige Funktionen werden im Test nicht vorgestellt - wir werden sie vom Training ausschlie√üen. </li></ul><br><h1 id="korrelyacionnyy-analiz">  Korrelationsanalyse </h1><br><p>  Nachdem Sie eine allgemeine Vorstellung davon erhalten haben, wie die Attribute verteilt sind und wie sie sich zwischen den Trainings- und Tests√§tzen verhalten, versuchen wir, die Korrelationen zu analysieren.  Konfigurieren Sie dazu den Feature-Extraktor basierend auf fr√ºheren Beobachtungen: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//             val expressions = filteredTrain.schema.fieldNames //          .filterNot(x =&gt; x == "date" || x == "audit_experiment" || idsColumns(x) || x.contains("vd_")) .map(x =&gt; if(skewedFeautres(x)) { //      s"log($x) AS $x" } else { //     cappedFeatures.get(x).map(capping =&gt; s"IF($x &lt; $capping, $x, $capping) AS $x").getOrElse(x) }) val rawFeaturesExtractor = new Pipeline().setStages(Array( new SQLTransformer().setStatement(s"SELECT ${expressions.mkString(", ")} FROM __THIS__"), new NullToDefaultReplacer(), new AutoAssembler().setOutputCol("features") )) //       val raw = rawFeaturesExtractor.fit(filteredTrain).transform( filteredTrain.where(isTestSimilar($"instanceId_userId")))</span></span></code> </pre> <br><p>  Von den neuen Maschinen in dieser Pipeline f√§llt das Dienstprogramm SQLTransformer auf, das beliebige SQL-Transformationen der Eingabetabelle erm√∂glicht. </p><br><p>  Bei der Analyse von Korrelationen ist es wichtig, das Rauschen herauszufiltern, das durch die nat√ºrliche Korrelation von One-Hot-Features erzeugt wird.  Dazu m√∂chte ich verstehen, welche Elemente des Vektors welchen Quellenspalten entsprechen.  Diese Aufgabe in Spark wird mithilfe von Spaltenmetadaten (mit Daten gespeichert) und Attributgruppen ausgef√ºhrt.  Der folgende Codeblock wird verwendet, um Paare von Attributnamen zu filtern, die aus derselben Spalte vom Typ String stammen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> attributes = <span class="hljs-type"><span class="hljs-type">AttributeGroup</span></span>.fromStructField(raw.schema(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)).attributes.get <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> originMap = filteredTrain .schema.filter(_.dataType == <span class="hljs-type"><span class="hljs-type">StringType</span></span>) .flatMap(x =&gt; attributes.map(_.name.get).filter(_.startsWith(x.name + <span class="hljs-string"><span class="hljs-string">"_"</span></span>)).map(_ -&gt; x.name)) .toMap <span class="hljs-comment"><span class="hljs-comment">//   ,          val isNonTrivialCorrelation = sqlContext.udf.register("isNonTrivialCorrelation", (x: String, y : String) =&gt; //    Scala-quiz   Option originMap.get(x).map(_ != originMap.getOrElse(y, "")).getOrElse(true))</span></span></code> </pre> <br><p>  Wenn Sie einen Datensatz mit einer Vektorspalte zur Hand haben, ist die Berechnung von Kreuzkorrelationen mit Spark recht einfach. Das Ergebnis ist jedoch eine Matrix, f√ºr deren Bereitstellung Sie ein wenig in eine Reihe von Paaren spielen m√ºssen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pearsonCorrelation = <span class="hljs-comment"><span class="hljs-comment">//    Pearson  Spearman Correlation.corr(raw, "features", "pearson").rdd.flatMap( //           _.getAs[Matrix](0).rowIter.zipWithIndex.flatMap(x =&gt; { //   ,   (  , //  ) val name = attributes(x._2).name.get //    ,     x._1.toArray.zip(attributes).map(y =&gt; (name, y._2.name.get, y._1)) } //     DataFrame )).toDF("feature1", "feature2", "corr") .na.drop //   .where(isNonTrivialCorrelation($"feature1", $"feature2")) //    . pearsonCorrelation.coalesce(1).write.mode("overwrite") .parquet("sna2019/pearsonCorrelation")</span></span></code> </pre> <br><p>  Und nat√ºrlich Visualisierung: Wir brauchen wieder Vegas Hilfe, um eine Heatmap zu zeichnen: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(<span class="hljs-string"><span class="hljs-string">"Pearson correlation heatmap"</span></span>) .withDataFrame(pearsonCorrelation .withColumn(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, $<span class="hljs-string"><span class="hljs-string">"corr"</span></span> &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) .withColumn(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, functions.abs($<span class="hljs-string"><span class="hljs-string">"corr"</span></span>)) .where(<span class="hljs-string"><span class="hljs-string">"feature1 &lt; feature2 AND abs_corr &gt; 0.05"</span></span>) .orderBy(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-string"><span class="hljs-string">"feature2"</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeY(<span class="hljs-string"><span class="hljs-string">"feature2"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeColor(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>, scale=<span class="hljs-type"><span class="hljs-type">Scale</span></span>(rangeNominals=<span class="hljs-type"><span class="hljs-type">List</span></span>(<span class="hljs-string"><span class="hljs-string">"#FFFFFF"</span></span>, <span class="hljs-string"><span class="hljs-string">"#FF0000"</span></span>))) .encodeShape(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Point</span></span>) .show</code> </pre> <br><p>  Das Ergebnis ist besser in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl-e zu sehen</a> .  F√ºr ein allgemeines Verst√§ndnis: </p><br><p><img src="https://habrastorage.org/webt/nm/d9/bm/nmd9bmrion4goaov9_vgx5vbjoy.png"></p><br><p>  Die W√§rmekarte zeigt, dass einige Korrelationen eindeutig vorhanden sind.  Versuchen wir, die Bl√∂cke der am st√§rksten korrelierten Merkmale auszuw√§hlen. Dazu verwenden wir die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GraphX-</a> Bibliothek: Wir wandeln die Korrelationsmatrix in ein Diagramm um, filtern die Kanten nach Gewicht. Danach finden wir die verbundenen Komponenten und lassen nur nicht entartete (von mehr als einem Element) √ºbrig.  Ein solches Verfahren √§hnelt im Wesentlichen der Anwendung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DBSCAN-</a> Algorithmus und ist wie folgt: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (GrpahX   ID) val featureIndexMap = spearmanCorrelation.select("feature1").distinct.rdd.map( _.getString(0)).collect.zipWithIndex.toMap val featureIndex = sqlContext.udf.register("featureIndex", (x: String) =&gt; featureIndexMap(x)) //    val vertices = sc.parallelize(featureIndexMap.map(x =&gt; x._2.toLong -&gt; x._1).toSeq, 1) //    val edges = spearmanCorrelation.select(featureIndex($"feature1"), featureIndex($"feature2"), $"corr") //     .where("ABS(corr) &gt; 0.7") .rdd.map(r =&gt; Edge(r.getInt(0), r.getInt(1), r.getDouble(2))) //       val components = Graph(vertices, edges).connectedComponents() val reversedMap = featureIndexMap.map(_.swap) //    ,    ,   //   val clusters = components .vertices.map(x =&gt; reversedMap(x._2.toInt) -&gt; reversedMap(x._1.toInt)) .groupByKey().map(x =&gt; x._2.toSeq) .filter(_.size &gt; 1) .sortBy(-_.size) .collect</span></span></code> </pre> <br><p>  Das Ergebnis wird in Form einer Tabelle dargestellt: </p><br><p><img src="https://habrastorage.org/webt/4b/t2/bl/4bt2blcjzmxbzucnm7zynpvpj-q.png"></p><br><p>  Basierend auf den Ergebnissen des Clusters k√∂nnen wir schlie√üen, dass die am meisten korrelierten Gruppen um Zeichen gebildet werden, die mit der Benutzermitgliedschaft in der Gruppe verbunden sind (Membership_status_A) sowie um den Objekttyp (instanceId_objectType).  F√ºr die beste Modellierung der Interaktion von Zeichen ist es sinnvoll, eine Modellsegmentierung anzuwenden, um verschiedene Modelle f√ºr verschiedene Objekttypen zu trainieren, getrennt f√ºr Gruppen, in denen der Benutzer ist und nicht. </p><br><h1 id="mashinnoe-obuchenie">  Maschinelles Lernen </h1><br><p>  Wir n√§hern uns dem Interessantesten - dem maschinellen Lernen.  Die Pipeline zum Trainieren des einfachsten Modells (logistische Regression) unter Verwendung von SparkML- und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML-</a> Erweiterungen lautet wie folgt: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement( <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>)))</code> </pre> <br><p>  Hier sehen wir nicht nur viele bekannte Elemente, sondern auch einige neue: </p><br><ul><li>  LogisticRegressionLBFSG ist ein Sch√§tzer mit verteiltem Training der logistischen Regression. </li><li>  Um mit verteilten ML-Algorithmen maximale Leistung zu erzielen.  Daten sollten optimal auf Partitionen verteilt werden.  Das Dienstprogramm UnwrappedStage.repartition hilft dabei, indem es der Pipeline eine Neupartitionsoperation hinzuf√ºgt, sodass sie nur in der Schulungsphase verwendet wird (schlie√ülich ist es beim Erstellen von Prognosen nicht mehr erforderlich). </li><li>  Damit k√∂nnte das lineare Modell ein gutes Ergebnis liefern.  Daten m√ºssen skaliert werden, f√ºr die das Dienstprogramm Scaler.scale verantwortlich ist.  Das Vorhandensein von zwei aufeinanderfolgenden linearen Transformationen (Skalierung und Multiplikation mit den Regressionsgewichten) f√ºhrt jedoch zu unn√∂tigen Kosten, und es ist w√ºnschenswert, diese Operationen zu reduzieren.  Bei Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML ist</a> die Ausgabe ein sauberes Modell mit einer Transformation :). </li><li>  Nat√ºrlich ben√∂tigen Sie f√ºr solche Modelle ein kostenloses Mitglied, das wir mithilfe der Operation Interceptor.intercept hinzuf√ºgen. </li></ul><br><p>  Die resultierende Pipeline, die auf alle Daten angewendet wird, ergibt AUC 0,6889 pro Benutzer (Validierungscode ist auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl</a> verf√ºgbar).  Jetzt m√ºssen wir alle unsere Forschungen anwenden: Daten filtern, Merkmale transformieren und Segmentmodelle.  Die endg√ºltige Pipeline sieht folgenderma√üen aus: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">s"SELECT instanceId_userId, instanceId_objectId, </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${expressions.mkString(", ")}</span></span></span><span class="hljs-string"> FROM __THIS__"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label, concat(IF(membership_status = 'A', 'OwnGroup_', 'NonUser_'), instanceId_objectType) AS type FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>, <span class="hljs-string"><span class="hljs-string">"type"</span></span>,<span class="hljs-string"><span class="hljs-string">"instanceId_objectType"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">CombinedModel</span></span>.perType( <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>))), numThreads = <span class="hljs-number"><span class="hljs-number">6</span></span>) ))</code> </pre> <br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> ‚Äî    CombinedModel.perType.       ,     numThreads = 6.             . </p><br><p> ,   ,  per-user AUC 0.7004.    ?  ,   " "    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">XGBoost</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">XGBoostRegressor</span></span>() .setNumRounds(<span class="hljs-number"><span class="hljs-number">100</span></span>) .setMaxDepth(<span class="hljs-number"><span class="hljs-number">15</span></span>) .setObjective(<span class="hljs-string"><span class="hljs-string">"reg:logistic"</span></span>) .setNumWorkers(<span class="hljs-number"><span class="hljs-number">17</span></span>) .setNthread(<span class="hljs-number"><span class="hljs-number">4</span></span>) .setTrackerConf(<span class="hljs-number"><span class="hljs-number">600000</span></span>L, <span class="hljs-string"><span class="hljs-string">"scala"</span></span>) ))</code> </pre> <br><p> ,     ‚Äî XGBoost  Spark !         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DLMC</a> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> ,       (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ).  XGboost " "   10     per-user AUC 0.6981. </p><br><h1 id="analiz-rezultatov">   </h1><br><p> ,     ,  ,       .    SparkML     ,      .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a>  :      Parquet            Spark: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//     val perTypeWeights = sqlContext.read.parquet("sna2019/perType/stages/*/weights") //     20    ( //  ) val topFeatures = new TopKTransformer[Double]() .setGroupByColumns("type") .setColumnToOrderGroupsBy("abs_weight") .setTopK(20) .transform(perTypeWeights.withColumn("abs_weight", functions.abs($"unscaled_weight"))) .orderBy("type", "unscaled_weight")</span></span></code> </pre> <br><p>     Parquet,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> ‚Äî TopKTransformer,           . </p><br><p>      Vegas (   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zepl</a> ): </p><br><p><img src="https://habrastorage.org/webt/q7/_n/fn/q7_nfnto-hw-9nbdgjf3chhm0oc.png"></p><br><p> ,    -   .      XGBoost? </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> significance = sqlContext.read.parquet( <span class="hljs-string"><span class="hljs-string">"sna2019/xgBoost15_100_raw/stages/*/featuresSignificance"</span></span> vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>() .withDataFrame(significance.na.drop.orderBy($<span class="hljs-string"><span class="hljs-string">"significance"</span></span>.desc).limit(<span class="hljs-number"><span class="hljs-number">40</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"name"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>, sortField = <span class="hljs-type"><span class="hljs-type">Sort</span></span>(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">AggOps</span></span>.<span class="hljs-type"><span class="hljs-type">Mean</span></span>)) .encodeY(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Bar</span></span>) .show</code> </pre> <br><p><img src="https://habrastorage.org/webt/i2/oy/zb/i2oyzbrjlo15x7u1uwadmeswocq.png"></p><br><p>  ,   ,   XGBoost,         ,    .           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .   ,  XGBoost     ,    ,   . </p><br><h1 id="vyvody">  Schlussfolgerungen </h1><br><p>  ,       :).     : </p><br><ol><li>    ,     Scala  Spark    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> . </li><li>    Scala  Spark        Python:    ETL  ML,    ,      ,     . </li><li>  ,   ,   ,    (,  )    ,     ,      . </li><li> ,     ,       .          ,      ,     , -, . </li></ol><br><p> ,       ,    ,        ,    -.        , ,  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   Scala</a> "  Newprolab. </p><br><p> ,  ,      ‚Äî   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442688/">https://habr.com/ru/post/de442688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442678/index.html">Gr√∂√ütes Projekt in der Stereolithographie: Mammutskelett auf einem 3D-Drucker gedruckt</a></li>
<li><a href="../de442680/index.html">Mit sensorischen Ersatztechnologien k√∂nnen Sie die Welt mithilfe von Ger√§uschen sehen: Wie die Neuroplastizit√§t des menschlichen Gehirns funktioniert</a></li>
<li><a href="../de442682/index.html">Was Sie eingeben und wie Sie ein C ++ - Projekt zusammenstellen</a></li>
<li><a href="../de442684/index.html">Ausgewogene Site-Leistung. Teil 3: Inhalt</a></li>
<li><a href="../de442686/index.html">DataPower-Lernprogramm</a></li>
<li><a href="../de442690/index.html">Mondmission "Bereshit" - Selfie auf dem Hintergrund der Erde</a></li>
<li><a href="../de442692/index.html">Blockchain ohne Zwischenh√§ndler: Wie wir Wertpapiere an eine verteilte Registrierung gesendet haben</a></li>
<li><a href="../de442694/index.html">Einer der Streaming-Giganten startete in Indien und zog in einer Woche eine Million Nutzer an</a></li>
<li><a href="../de442696/index.html">S for Security: Internetsicherheit von Dingen und Berichten bei InoThings ++ 2019</a></li>
<li><a href="../de442698/index.html">Moskauer U-Bahn-Anwendung f√ºr den Windows Store</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>