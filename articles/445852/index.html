<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍺 🐪 😨 Cómo hacer que un DAG se dispare en Airflow usando la API experimental ↗️ 🏥 🎀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Al preparar nuestros programas educativos, periódicamente encontramos dificultades en términos de trabajo con algunas herramientas. Y en ese momento c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cómo hacer que un DAG se dispare en Airflow usando la API experimental</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/newprolab/blog/445852/"><p>  Al preparar nuestros programas educativos, periódicamente encontramos dificultades en términos de trabajo con algunas herramientas.  Y en ese momento cuando los encontramos, no siempre hay suficiente documentación y artículos que ayuden a hacer frente a este problema. </p><br><p>  Este fue el caso, por ejemplo, en 2015, y nosotros, en el programa Big Data Specialist, utilizamos un clúster Hadoop con Spark para 35 usuarios simultáneos.  Cómo cocinarlo en un caso de usuario de este tipo usando YARN no estaba claro.  Como resultado, después de descubrir y caminar por el camino por su cuenta, hicieron una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicación en Habré</a> y también se presentaron en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Moscow Spark Meetup</a> . </p><br><h3 id="predystoriya">  Antecedentes </h3><br><p>  Esta vez hablaremos de otro programa: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ingeniero de datos</a> .  Nuestros participantes construyen dos tipos de arquitectura sobre ella: lambda y kappa.  Y en la arquitectura lamdba, como parte del procesamiento por lotes, Airflow se usa para transferir registros de HDFS a ClickHouse. </p><br><p>  Todo está bien en general.  Déjelos construir sus tuberías.  Sin embargo, existe un "pero": todos nuestros programas son tecnológicos en términos del proceso de aprendizaje en sí.  Para verificar el laboratorio, utilizamos verificadores automáticos: el participante debe ir a su cuenta personal, hacer clic en el botón "Verificar", y después de un tiempo ve algún tipo de retroalimentación extendida sobre lo que ha hecho.  Y es en este momento que comenzamos a abordar nuestro problema. </p><a name="habracut"></a><br><p>  La verificación de este laboratorio se organiza de la siguiente manera: enviamos un paquete de datos de control a Kafka del participante, luego Gobblin transfiere el paquete de datos a HDFS, luego Airflow toma este paquete de datos y lo coloca en ClickHouse.  El truco es que Airflow no debe hacer esto en tiempo real, lo hace en un horario: cada 15 minutos toma un montón de archivos y los arroja. </p><br><p> Resulta que necesitamos de alguna manera activar su DAG por nuestra cuenta a petición del verificador aquí y ahora.  Buscando en Google, descubrimos que para las versiones posteriores de Airflow existe la llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">API Experimental</a> .  La palabra <code>experimental</code> , por supuesto, suena aterradora, pero qué hacer ... De repente, volará. </p><br><p>  A continuación, describimos todo el camino: desde la instalación de Airflow hasta la generación de una solicitud POST que desencadena un DAG utilizando la API experimental.  Trabajaremos con Ubuntu 16.04. </p><br><h3>  1. Instalación del flujo de aire </h3><br><p>  Vamos a comprobar que tenemos Python 3 y virtualenv. </p><br><pre> <code class="python hljs">$ python3 --version Python <span class="hljs-number"><span class="hljs-number">3.6</span></span><span class="hljs-number"><span class="hljs-number">.6</span></span> $ virtualenv --version <span class="hljs-number"><span class="hljs-number">15.2</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span></code> </pre> <br><p>  Si falta algo de esto, instálelo. </p><br><p>  Ahora cree un directorio en el que continuaremos trabajando con Airflow. </p><br><pre> <code class="bash hljs">$ mkdir &lt;your name of directory&gt; $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/your/new/directory $ virtualenv -p <span class="hljs-built_in"><span class="hljs-built_in">which</span></span> python3 venv $ <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> venv/bin/activate (venv) $</code> </pre> <br><p>  Instalar flujo de aire: </p><br><pre> <code class="bash hljs">(venv) $ pip install airflow</code> </pre> <br><p>  La versión en la que trabajamos: 1.10. </p><br><p>  Ahora necesitamos crear el directorio <code>airflow_home</code> donde se <code>airflow_home</code> los archivos DAG y los complementos de Airflow.  Después de crear el directorio, establezca la variable de entorno <code>AIRFLOW_HOME</code> . </p><br><pre> <code class="bash hljs">(venv) $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/my/airflow/workspace (venv) $ mkdir airflow_home (venv) $ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_HOME=&lt;path to airflow_home&gt;</code> </pre> <br><p>  El siguiente paso es ejecutar el comando que creará e inicializará la base de datos de flujo de datos en SQLite: </p><br><pre> <code class="bash hljs">(venv) $ airflow initdb</code> </pre> <br><p>  La base de datos se creará en <code>airflow.db</code> de manera predeterminada. </p><br><p>  Compruebe si Airflow está instalado: </p><br><pre> <code class="bash hljs">$ airflow version [2018-11-26 19:38:19,607] {__init__.py:57} INFO - Using executor SequentialExecutor [2018-11-26 19:38:19,745] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt [2018-11-26 19:38:19,771] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0</code> </pre> <br><p>  Si el comando funcionó, Airflow creó su archivo de configuración <code>airflow.cfg</code> en <code>AIRFLOW_HOME</code> : </p><br><pre> <code class="bash hljs">$ tree . ├── airflow.cfg └── unittests.cfg</code> </pre> <br><p>  Airflow tiene una interfaz web.  Se puede iniciar ejecutando el comando: </p><br><pre> <code class="bash hljs">(venv) $ airflow webserver --port 8081</code> </pre> <br><p>  Ahora puede acceder a la interfaz web en un navegador en el puerto 8081 en el host donde se lanzó Airflow, por ejemplo: <code>&lt;hostname:8081&gt;</code> . </p><br><h3 id="2-rabota-s-experimental-api">  2. Trabajando con la API Experimental </h3><br><p>  En esto, Airflow está configurado y listo para funcionar.  Sin embargo, también necesitamos ejecutar la API experimental.  Nuestras fichas están escritas en Python, por lo que todas las solicitudes se incluirán en la biblioteca de <code>requests</code> . </p><br><p>  De hecho, la API ya está funcionando para consultas simples.  Por ejemplo, una solicitud de este tipo le permite probar su funcionamiento: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests &gt;&gt;&gt; host = &lt;your hostname&gt; &gt;&gt;&gt; airflow_port = <span class="hljs-number"><span class="hljs-number">8081</span></span> <span class="hljs-comment"><span class="hljs-comment">#   ,    8080 &gt;&gt;&gt; requests.get('http://{}:{}/{}'.format(host, airflow_port, 'api/experimental/test').text 'OK'</span></span></code> </pre> <br><p>  Si recibió un mensaje de este tipo en respuesta, esto significa que todo funciona. </p><br><p>  Sin embargo, cuando queremos activar DAG, nos enfrentaremos al hecho de que este tipo de solicitud no se puede realizar sin autenticación. </p><br><p>  Para hacer esto, deberá realizar una serie de acciones. </p><br><p>  En primer lugar, debe agregar esto a la configuración: </p><br><pre> <code class="plaintext hljs">[api] auth_backend = airflow.contrib.auth.backends.password_auth</code> </pre> <br><p>  Luego, debe crear su usuario con derechos de administrador: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.Admin()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'new_user_name'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'set_the_password'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Luego, debe crear un usuario con derechos normales, a quien se le permitirá hacer un disparador DAG. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.User()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'newprolab'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Ahora todo está listo. </p><br><h3 id="3-zapusk-post-zaprosa">  3. Inicio de una solicitud POST </h3><br><p>  La solicitud POST en sí se verá así: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>dag_id = newprolab &gt;&gt;&gt; url = <span class="hljs-string"><span class="hljs-string">'http://{}:{}/{}/{}/{}'</span></span>.format(host, airflow_port, <span class="hljs-string"><span class="hljs-string">'api/experimental/dags'</span></span>, dag_id, <span class="hljs-string"><span class="hljs-string">'dag_runs'</span></span>) &gt;&gt;&gt; data = {<span class="hljs-string"><span class="hljs-string">"conf"</span></span>:<span class="hljs-string"><span class="hljs-string">"{\"key\":\"value\"}"</span></span>} &gt;&gt;&gt; headers = {<span class="hljs-string"><span class="hljs-string">'Content-type'</span></span>: <span class="hljs-string"><span class="hljs-string">'application/json'</span></span>} &gt;&gt;&gt; auth = (<span class="hljs-string"><span class="hljs-string">'newprolab'</span></span>, <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span>) &gt;&gt;&gt; uri = requests.post(url, data=json.dumps(data), headers=headers, auth=auth) &gt;&gt;&gt; uri.text <span class="hljs-string"><span class="hljs-string">'{\n "message": "Created &lt;DagRun newprolab @ 2019-03-27 10:24:25+00:00: manual__2019-03-27T10:24:25+00:00, externally triggered: True&gt;"\n}\n'</span></span></code> </pre> <br><p>  Solicitud procesada con éxito. </p><br><p>  En consecuencia, le damos algo de tiempo al DAG para el procesamiento y hacemos una solicitud a la tabla ClickHouse, tratando de capturar un paquete de datos de control. </p><br><p>  Verificación completada. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/445852/">https://habr.com/ru/post/445852/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445834/index.html">Desarrollar la habilidad de usar agrupación y visualización de datos en Python</a></li>
<li><a href="../445838/index.html">Robótica para niños: ojos de robot</a></li>
<li><a href="../445844/index.html">GitLab 11.9 lanzado con detección de secretos y reglas de resolución de solicitud de Marge múltiple</a></li>
<li><a href="../445846/index.html">Postgres profesionales</a></li>
<li><a href="../445850/index.html">Repare el dispositivo de almacenamiento e impresión Sharp Memowriter EL-7000 Note después de fugas de batería</a></li>
<li><a href="../445854/index.html">Encontrar un marco JS para la generación de UI</a></li>
<li><a href="../445856/index.html">Telegrama después de 5 años.</a></li>
<li><a href="../445858/index.html">Antigüedades: cuando los teléfonos eran raros</a></li>
<li><a href="../445860/index.html">La baja volatilidad de Bitcoin (BTC) conducirá a la próxima Crypto Bull Run</a></li>
<li><a href="../445862/index.html">JS de todos lados: los 10 mejores informes de HolyJS 2018 Moscú</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>