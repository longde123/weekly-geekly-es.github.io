<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>   C贸mo hacer que un DAG se dispare en Airflow usando la API experimental 锔  </title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Al preparar nuestros programas educativos, peri贸dicamente encontramos dificultades en t茅rminos de trabajo con algunas herramientas. Y en ese momento c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C贸mo hacer que un DAG se dispare en Airflow usando la API experimental</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/newprolab/blog/445852/"><p>  Al preparar nuestros programas educativos, peri贸dicamente encontramos dificultades en t茅rminos de trabajo con algunas herramientas.  Y en ese momento cuando los encontramos, no siempre hay suficiente documentaci贸n y art铆culos que ayuden a hacer frente a este problema. </p><br><p>  Este fue el caso, por ejemplo, en 2015, y nosotros, en el programa Big Data Specialist, utilizamos un cl煤ster Hadoop con Spark para 35 usuarios simult谩neos.  C贸mo cocinarlo en un caso de usuario de este tipo usando YARN no estaba claro.  Como resultado, despu茅s de descubrir y caminar por el camino por su cuenta, hicieron una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci贸n en Habr茅</a> y tambi茅n se presentaron en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Moscow Spark Meetup</a> . </p><br><h3 id="predystoriya">  Antecedentes </h3><br><p>  Esta vez hablaremos de otro programa: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ingeniero de datos</a> .  Nuestros participantes construyen dos tipos de arquitectura sobre ella: lambda y kappa.  Y en la arquitectura lamdba, como parte del procesamiento por lotes, Airflow se usa para transferir registros de HDFS a ClickHouse. </p><br><p>  Todo est谩 bien en general.  D茅jelos construir sus tuber铆as.  Sin embargo, existe un "pero": todos nuestros programas son tecnol贸gicos en t茅rminos del proceso de aprendizaje en s铆.  Para verificar el laboratorio, utilizamos verificadores autom谩ticos: el participante debe ir a su cuenta personal, hacer clic en el bot贸n "Verificar", y despu茅s de un tiempo ve alg煤n tipo de retroalimentaci贸n extendida sobre lo que ha hecho.  Y es en este momento que comenzamos a abordar nuestro problema. </p><a name="habracut"></a><br><p>  La verificaci贸n de este laboratorio se organiza de la siguiente manera: enviamos un paquete de datos de control a Kafka del participante, luego Gobblin transfiere el paquete de datos a HDFS, luego Airflow toma este paquete de datos y lo coloca en ClickHouse.  El truco es que Airflow no debe hacer esto en tiempo real, lo hace en un horario: cada 15 minutos toma un mont贸n de archivos y los arroja. </p><br><p> Resulta que necesitamos de alguna manera activar su DAG por nuestra cuenta a petici贸n del verificador aqu铆 y ahora.  Buscando en Google, descubrimos que para las versiones posteriores de Airflow existe la llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">API Experimental</a> .  La palabra <code>experimental</code> , por supuesto, suena aterradora, pero qu茅 hacer ... De repente, volar谩. </p><br><p>  A continuaci贸n, describimos todo el camino: desde la instalaci贸n de Airflow hasta la generaci贸n de una solicitud POST que desencadena un DAG utilizando la API experimental.  Trabajaremos con Ubuntu 16.04. </p><br><h3>  1. Instalaci贸n del flujo de aire </h3><br><p>  Vamos a comprobar que tenemos Python 3 y virtualenv. </p><br><pre> <code class="python hljs">$ python3 --version Python <span class="hljs-number"><span class="hljs-number">3.6</span></span><span class="hljs-number"><span class="hljs-number">.6</span></span> $ virtualenv --version <span class="hljs-number"><span class="hljs-number">15.2</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span></code> </pre> <br><p>  Si falta algo de esto, inst谩lelo. </p><br><p>  Ahora cree un directorio en el que continuaremos trabajando con Airflow. </p><br><pre> <code class="bash hljs">$ mkdir &lt;your name of directory&gt; $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/your/new/directory $ virtualenv -p <span class="hljs-built_in"><span class="hljs-built_in">which</span></span> python3 venv $ <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> venv/bin/activate (venv) $</code> </pre> <br><p>  Instalar flujo de aire: </p><br><pre> <code class="bash hljs">(venv) $ pip install airflow</code> </pre> <br><p>  La versi贸n en la que trabajamos: 1.10. </p><br><p>  Ahora necesitamos crear el directorio <code>airflow_home</code> donde se <code>airflow_home</code> los archivos DAG y los complementos de Airflow.  Despu茅s de crear el directorio, establezca la variable de entorno <code>AIRFLOW_HOME</code> . </p><br><pre> <code class="bash hljs">(venv) $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/my/airflow/workspace (venv) $ mkdir airflow_home (venv) $ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_HOME=&lt;path to airflow_home&gt;</code> </pre> <br><p>  El siguiente paso es ejecutar el comando que crear谩 e inicializar谩 la base de datos de flujo de datos en SQLite: </p><br><pre> <code class="bash hljs">(venv) $ airflow initdb</code> </pre> <br><p>  La base de datos se crear谩 en <code>airflow.db</code> de manera predeterminada. </p><br><p>  Compruebe si Airflow est谩 instalado: </p><br><pre> <code class="bash hljs">$ airflow version [2018-11-26 19:38:19,607] {__init__.py:57} INFO - Using executor SequentialExecutor [2018-11-26 19:38:19,745] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt [2018-11-26 19:38:19,771] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0</code> </pre> <br><p>  Si el comando funcion贸, Airflow cre贸 su archivo de configuraci贸n <code>airflow.cfg</code> en <code>AIRFLOW_HOME</code> : </p><br><pre> <code class="bash hljs">$ tree .  airflow.cfg  unittests.cfg</code> </pre> <br><p>  Airflow tiene una interfaz web.  Se puede iniciar ejecutando el comando: </p><br><pre> <code class="bash hljs">(venv) $ airflow webserver --port 8081</code> </pre> <br><p>  Ahora puede acceder a la interfaz web en un navegador en el puerto 8081 en el host donde se lanz贸 Airflow, por ejemplo: <code>&lt;hostname:8081&gt;</code> . </p><br><h3 id="2-rabota-s-experimental-api">  2. Trabajando con la API Experimental </h3><br><p>  En esto, Airflow est谩 configurado y listo para funcionar.  Sin embargo, tambi茅n necesitamos ejecutar la API experimental.  Nuestras fichas est谩n escritas en Python, por lo que todas las solicitudes se incluir谩n en la biblioteca de <code>requests</code> . </p><br><p>  De hecho, la API ya est谩 funcionando para consultas simples.  Por ejemplo, una solicitud de este tipo le permite probar su funcionamiento: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests &gt;&gt;&gt; host = &lt;your hostname&gt; &gt;&gt;&gt; airflow_port = <span class="hljs-number"><span class="hljs-number">8081</span></span> <span class="hljs-comment"><span class="hljs-comment">#   ,    8080 &gt;&gt;&gt; requests.get('http://{}:{}/{}'.format(host, airflow_port, 'api/experimental/test').text 'OK'</span></span></code> </pre> <br><p>  Si recibi贸 un mensaje de este tipo en respuesta, esto significa que todo funciona. </p><br><p>  Sin embargo, cuando queremos activar DAG, nos enfrentaremos al hecho de que este tipo de solicitud no se puede realizar sin autenticaci贸n. </p><br><p>  Para hacer esto, deber谩 realizar una serie de acciones. </p><br><p>  En primer lugar, debe agregar esto a la configuraci贸n: </p><br><pre> <code class="plaintext hljs">[api] auth_backend = airflow.contrib.auth.backends.password_auth</code> </pre> <br><p>  Luego, debe crear su usuario con derechos de administrador: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.Admin()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'new_user_name'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'set_the_password'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Luego, debe crear un usuario con derechos normales, a quien se le permitir谩 hacer un disparador DAG. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.User()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'newprolab'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Ahora todo est谩 listo. </p><br><h3 id="3-zapusk-post-zaprosa">  3. Inicio de una solicitud POST </h3><br><p>  La solicitud POST en s铆 se ver谩 as铆: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>dag_id = newprolab &gt;&gt;&gt; url = <span class="hljs-string"><span class="hljs-string">'http://{}:{}/{}/{}/{}'</span></span>.format(host, airflow_port, <span class="hljs-string"><span class="hljs-string">'api/experimental/dags'</span></span>, dag_id, <span class="hljs-string"><span class="hljs-string">'dag_runs'</span></span>) &gt;&gt;&gt; data = {<span class="hljs-string"><span class="hljs-string">"conf"</span></span>:<span class="hljs-string"><span class="hljs-string">"{\"key\":\"value\"}"</span></span>} &gt;&gt;&gt; headers = {<span class="hljs-string"><span class="hljs-string">'Content-type'</span></span>: <span class="hljs-string"><span class="hljs-string">'application/json'</span></span>} &gt;&gt;&gt; auth = (<span class="hljs-string"><span class="hljs-string">'newprolab'</span></span>, <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span>) &gt;&gt;&gt; uri = requests.post(url, data=json.dumps(data), headers=headers, auth=auth) &gt;&gt;&gt; uri.text <span class="hljs-string"><span class="hljs-string">'{\n "message": "Created &lt;DagRun newprolab @ 2019-03-27 10:24:25+00:00: manual__2019-03-27T10:24:25+00:00, externally triggered: True&gt;"\n}\n'</span></span></code> </pre> <br><p>  Solicitud procesada con 茅xito. </p><br><p>  En consecuencia, le damos algo de tiempo al DAG para el procesamiento y hacemos una solicitud a la tabla ClickHouse, tratando de capturar un paquete de datos de control. </p><br><p>  Verificaci贸n completada. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/445852/">https://habr.com/ru/post/445852/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445834/index.html">Desarrollar la habilidad de usar agrupaci贸n y visualizaci贸n de datos en Python</a></li>
<li><a href="../445838/index.html">Rob贸tica para ni帽os: ojos de robot</a></li>
<li><a href="../445844/index.html">GitLab 11.9 lanzado con detecci贸n de secretos y reglas de resoluci贸n de solicitud de Marge m煤ltiple</a></li>
<li><a href="../445846/index.html">Postgres profesionales</a></li>
<li><a href="../445850/index.html">Repare el dispositivo de almacenamiento e impresi贸n Sharp Memowriter EL-7000 Note despu茅s de fugas de bater铆a</a></li>
<li><a href="../445854/index.html">Encontrar un marco JS para la generaci贸n de UI</a></li>
<li><a href="../445856/index.html">Telegrama despu茅s de 5 a帽os.</a></li>
<li><a href="../445858/index.html">Antig眉edades: cuando los tel茅fonos eran raros</a></li>
<li><a href="../445860/index.html">La baja volatilidad de Bitcoin (BTC) conducir谩 a la pr贸xima Crypto Bull Run</a></li>
<li><a href="../445862/index.html">JS de todos lados: los 10 mejores informes de HolyJS 2018 Mosc煤</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>