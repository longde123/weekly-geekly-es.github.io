<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üà≥ üë° üë©üèæ‚Äçüíº Les r√©seaux de neurones ont une strat√©gie de classification d'image incroyablement simple. üõÇ üèûÔ∏è üëÜüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les r√©seaux de neurones convolutifs font un excellent travail de classification des images d√©form√©es, contrairement aux humains. 


 Dans cet article,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les r√©seaux de neurones ont une strat√©gie de classification d'image incroyablement simple.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443734/"><h3>  Les r√©seaux de neurones convolutifs font un excellent travail de classification des images d√©form√©es, contrairement aux humains. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/2e6/a7c/4c8/2e6a7c4c8f0ba811e57fa24193375289.jpg"><br><br>  Dans cet article, je montrerai pourquoi les r√©seaux de neurones profonds avanc√©s peuvent parfaitement reconna√Ætre les images d√©form√©es et comment cela aide √† r√©v√©ler la strat√©gie √©tonnamment simple utilis√©e par les r√©seaux de neurones pour classer les photographies naturelles.  Ces d√©couvertes, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">publi√©es</a> dans ICLR 2019, ont de nombreuses cons√©quences: premi√®rement, elles d√©montrent qu'il est beaucoup plus facile de trouver une solution ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ImageNet</a> ¬ª qu'on ne le pensait.  Deuxi√®mement, ils nous aident √† cr√©er des syst√®mes de classification d'images plus interpr√©tables et compr√©hensibles.  Troisi√®mement, ils expliquent plusieurs ph√©nom√®nes observ√©s dans les r√©seaux neuronaux convolutionnels modernes (SNS), par exemple, leur tendance √† rechercher des textures (voir nos autres <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">travaux</a> dans ICLR 2019 et l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">entr√©e de blog</a> correspondante), et ignorant la disposition spatiale de parties de l'objet. <br><a name="habracut"></a><br><h2>  Bons vieux mod√®les "sac de mots" </h2><br>  Au bon vieux temps, avant l'av√®nement de l'apprentissage en profondeur, la reconnaissance des images naturelles √©tait assez simple: nous d√©finissons un ensemble de caract√©ristiques visuelles cl√©s (¬´mots¬ª), d√©terminons la fr√©quence √† laquelle chaque caract√©ristique visuelle se produit dans une image (¬´sac¬ª) et classifions l'image en fonction de celles-ci. chiffres.  Par cons√©quent, de tels mod√®les en vision par ordinateur sont appel√©s le "sac de mots" (sac de mots ou BoW).  Par exemple, supposons que nous ayons deux caract√©ristiques visuelles, l'≈ìil humain et le stylo, et que nous voulons classer les images en deux classes, ¬´personnes¬ª et ¬´oiseaux¬ª.  Le mod√®le BoW le plus simple serait le suivant: pour chaque ≈ìil trouv√© dans l'image, nous augmentons le t√©moignage en faveur de la ¬´personne¬ª de 1. Et vice versa, pour chaque plume nous augmentons le t√©moignage en faveur de ¬´l'oiseau¬ª de 1. Quelle classe gagne plus de preuves, ce sera elle. <br><br>  Une caract√©ristique pratique d'un mod√®le BoW aussi simple est l'interpr√©tabilit√© et la clart√© du processus d√©cisionnel: nous pouvons v√©rifier pr√©cis√©ment quelles caract√©ristiques particuli√®res de l'image parlent en faveur d'une classe particuli√®re, l'int√©gration spatiale des caract√©ristiques est tr√®s simple (par rapport √† l'int√©gration non lin√©aire des caract√©ristiques dans les r√©seaux de neurones profonds), donc il suffit de comprendre comment le mod√®le prend ses d√©cisions. <br><br>  Les mod√®les BoW traditionnels √©taient extr√™mement populaires et fonctionnaient tr√®s bien avant l'invasion de l'apprentissage en profondeur, mais sont rapidement pass√©s de mode en raison de l'efficacit√© relativement faible.  Mais sommes-nous s√ªrs que les r√©seaux de neurones utilisent une strat√©gie de d√©cision fondamentalement diff√©rente de BoW? <br><br><h2>  R√©seau interpr√©t√© en profondeur avec fonctionnalit√©s Bag (BagNet) </h2><br>  Pour tester cette hypoth√®se, nous combinons l'interpr√©tabilit√© et la clart√© des mod√®les BoW avec l'efficacit√© des r√©seaux de neurones.  La strat√©gie ressemble √† ceci: <br><ul><li>  Divisez l'image en petits morceaux qx q. </li><li>  Nous passons les morceaux √† travers le r√©seau neuronal pour obtenir des preuves d'appartenance √† la classe (logits) pour chaque morceau. </li><li>  R√©sumez les preuves dans toutes les pi√®ces pour obtenir une solution au niveau de l'image enti√®re. </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/474/6f5/363/4746f53632bd9e3e7477de9ecb76d396.png"><br><br>  Pour impl√©menter cette strat√©gie, de la mani√®re la plus simple, nous prenons l'architecture ResNet-50 standard et rempla√ßons presque toutes les convolutions 3x3 par des convolutions 1x1.  Par cons√©quent, chaque √©l√©ment cach√© dans la derni√®re couche convolutionnelle ¬´ne voit¬ª qu'une petite partie de l'image (c'est-√†-dire que son champ de perception est beaucoup plus petit que la taille de l'image).  On √©vite ainsi le balisage impos√© de l'image et au plus pr√®s du SNA standard, tout en appliquant une strat√©gie pr√©-planifi√©e.  Nous appelons l'architecture r√©sultante BagNet-q, o√π q d√©signe la taille du champ de perception de la couche tr√®s sup√©rieure (nous avons test√© le mod√®le avec q = 9, 17 et 33).  BagNet-q fonctionne environ 2,5 fois plus longtemps que ResNet-50. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b5e/4b6/1c4/b5e4b61c431e103ac9a6ec3fe4b02894.jpg"><br><br>  Les performances de BagNet sur les donn√©es de la base de donn√©es ImageNet sont impressionnantes m√™me lors de l'utilisation de petits morceaux: des fragments de 17x17 pixels suffisent pour atteindre l'efficacit√© du niveau AlexNet, et des fragments de 33x33 pixels suffisent pour atteindre une pr√©cision de 87%, entrant dans le top 5.  Vous pouvez augmenter l'efficacit√© en pla√ßant les paquets 3x3 plus soigneusement et en ajustant les hyperparam√®tres. <br><br>  Ceci est notre premier r√©sultat majeur: ImageNet peut √™tre r√©solu en utilisant uniquement un ensemble de petites fonctionnalit√©s d'image.  Les relations spatiales √©loign√©es des parties de la composition, telles que la forme des objets ou l'interaction entre les parties de l'objet, peuvent √™tre compl√®tement ignor√©es;  ils ne sont absolument pas n√©cessaires pour r√©soudre le probl√®me. <br><br>  Une caract√©ristique remarquable de BagNet'ov est la transparence de leur syst√®me de prise de d√©cision.  Par exemple, vous pouvez d√©couvrir quelles caract√©ristiques des images seront les plus caract√©ristiques pour une classe donn√©e.  Par exemple, la tanche, un gros poisson, est g√©n√©ralement reconnue par l'image des doigts sur un fond vert.  Pourquoi?  Parce que sur la plupart des photos de cette cat√©gorie, il y a un p√™cheur tenant une tanche comme troph√©e.  Et lorsque BagNet reconna√Æt incorrectement l'image comme une ligne, cela se produit g√©n√©ralement car quelque part sur la photo, il y a des doigts sur un fond vert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/175/197/727/175197727cc15da939117f5c2502d82c.jpg"><br>  <i>Les parties les plus caract√©ristiques des images.</i>  <i>La rang√©e du haut dans chaque cellule correspond √† la reconnaissance correcte, et celle du bas aux fragments distrayants qui ont conduit √† une reconnaissance incorrecte</i> <br><br>  Nous obtenons √©galement la ¬´carte thermique¬ª exacte, qui montre quelles parties de l'image ont contribu√© √† la d√©cision. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89b/510/f5b/89b510f5b5d816a2a1d8590f4487abf6.jpg"><br>  <i>Les cartes thermiques ne sont pas une approximation; elles montrent avec pr√©cision la contribution de chaque partie de l'image.</i> <br><br>  BagNet d√©montre que vous ne pouvez obtenir une grande pr√©cision avec ImageNet que sur la base de faibles corr√©lations statistiques entre les caract√©ristiques locales des images et la cat√©gorie des objets.  Si cela suffit, alors pourquoi les r√©seaux de neurones ResNet-50 standard apprendraient-ils quelque chose de fondamentalement diff√©rent?  Pourquoi ResNet-50 devrait-il √©tudier des relations complexes √† grande √©chelle telles que la forme d'un objet, si l'abondance des caract√©ristiques locales de l'image est suffisante pour r√©soudre le probl√®me? <br><br>  Pour tester l'hypoth√®se selon laquelle les SNA modernes adh√®rent √† une strat√©gie similaire au fonctionnement des r√©seaux BoW les plus simples, nous avons test√© diff√©rents r√©seaux - ResNet, DenseNet et VGG sur les ¬´signes¬ª suivants de BagNet: <br><ul><li>  Les solutions sont ind√©pendantes du brassage spatial des caract√©ristiques de l'image (cela ne peut √™tre v√©rifi√© que sur les mod√®les VGG). </li><li>  Les modifications de diff√©rentes parties de l'image ne doivent pas d√©pendre les unes des autres (dans le sens de leur influence sur l'appartenance √† la classe). </li><li>  Les erreurs commises par le SNA standard et BagNet'ami devraient √™tre similaires. </li><li>  SNS standard et BagNet doivent √™tre sensibles aux fonctionnalit√©s similaires. </li></ul><br><br>  Dans les quatre exp√©riences, nous avons trouv√© des comportements √©tonnamment similaires du SNS et de BagNet.  Par exemple, dans la derni√®re exp√©rience, nous montrons que BagNet est le plus sensible (si, par exemple, ils se chevauchent) aux m√™mes endroits dans les images que le SNA.  En fait, les cartes thermiques BagNet (cartes de sensibilit√© spatiale) pr√©disent mieux la sensibilit√© de DenseNet-169 que les cartes thermiques obtenues par des m√©thodes d'attribution telles que DeepLift (calculant directement les cartes thermiques pour DenseNet-169).  Bien s√ªr, le SNA ne r√©p√®te pas exactement le comportement de BagNet, mais certaines d√©viations le d√©montrent.  En particulier, plus les r√©seaux deviennent profonds, plus les tailles des fonctionnalit√©s sont grandes et plus les d√©pendances s'√©tendent.  Par cons√©quent, les r√©seaux de neurones profonds sont en effet une am√©lioration par rapport aux mod√®les BagNet, mais je ne pense pas que la base de leur classification soit en quelque sorte en train de changer. <br><br><h2>  Aller au-del√† de la classification BoW </h2><br>  L'observation de la prise de d√©cision du SCN dans le style des strat√©gies de BoW peut expliquer certaines des caract√©ristiques √©tranges du SCN.  Tout d'abord, cela explique pourquoi le SCN est si <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">li√© aux textures</a> .  Deuxi√®mement, pourquoi le SNA n'est pas sensible au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">m√©lange de</a> parties de l'image.  Cela peut m√™me expliquer l'existence d'autocollants contradictoires et de perturbations contradictoires: des signaux d√©routants peuvent √™tre plac√©s n'importe o√π dans l'image, et le SNS captera s√ªrement ce signal, qu'il corresponde au reste de l'image. <br><br>  En fait, notre travail montre que le SCN, lors de la reconnaissance d'images, utilise beaucoup de lois statistiques faibles et ne proc√®de pas √† l'int√©gration de parties de l'image au niveau des objets, comme le font les gens.  Il en va probablement de m√™me pour d'autres t√¢ches et modalit√©s sensorielles. <br><br>  Nous devons planifier soigneusement nos architectures, nos t√¢ches et nos m√©thodes de formation pour surmonter la tendance √† utiliser de faibles corr√©lations statistiques.  Une approche consiste √† traduire la distorsion de la formation SNA de petites fonctionnalit√©s locales √† des fonctionnalit√©s plus globales.  Une autre consiste √† supprimer ou √† remplacer les fonctionnalit√©s sur lesquelles le r√©seau de neurones ne devrait pas s'appuyer, comme nous l'avons fait dans une autre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">publication</a> pour l'ICLR 2019, en utilisant le pr√©traitement par transfert de style pour √©liminer la texture d'un objet naturel. <br><br>  Un des plus gros probl√®mes reste cependant la classification des images: si les caract√©ristiques locales sont suffisantes, il n'y a aucune incitation √† √©tudier la v√©ritable "physique" du monde naturel.  Nous devons restructurer la t√¢che de mani√®re √† d√©placer des mod√®les pour √©tudier la nature physique des objets.  Pour ce faire, vous devrez probablement aller au-del√† de l'enseignement purement observationnel jusqu'aux corr√©lations des donn√©es d'entr√©e et de sortie afin que les mod√®les puissent extraire les relations causales. <br><br>  Ensemble, nos r√©sultats sugg√®rent que le SCN peut suivre une strat√©gie de classification extr√™mement simple.  Le fait qu'une telle d√©couverte puisse √™tre faite en 2019 souligne combien nous comprenons encore peu les caract√©ristiques internes du travail des r√©seaux de neurones profonds.  Le manque de compr√©hension ne nous permet pas de d√©velopper des mod√®les et des architectures fondamentalement am√©lior√©s qui comblent le foss√© entre la perception de l'homme et de la machine.  Approfondir notre compr√©hension nous permettra de d√©couvrir des moyens de r√©duire cet √©cart.  Cela peut √™tre extr√™mement utile: en essayant de d√©placer le SNA vers les propri√©t√©s physiques des objets, nous avons soudainement atteint une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©sistance au bruit au</a> niveau humain.  J'attends l'apparition d'un grand nombre d'autres r√©sultats int√©ressants sur notre chemin vers le d√©veloppement du SCN, qui comprend vraiment la nature physique et causale de notre monde. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr443734/">https://habr.com/ru/post/fr443734/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr443722/index.html">Conception et d√©nomination des files d'attente</a></li>
<li><a href="../fr443724/index.html">AMD Radeon VII: puce haut de gamme (partie 1)</a></li>
<li><a href="../fr443726/index.html">Caract√©ristiques de configuration de Palo Alto Networks: VPN SSL</a></li>
<li><a href="../fr443728/index.html">Google Plus cesse de fonctionner. Et alors?</a></li>
<li><a href="../fr443730/index.html">Ctrl-Alt-Del: obsolescence programm√©e des programmeurs</a></li>
<li><a href="../fr443736/index.html">Configuration de Network Management Tools (NUT) √† partir de z√©ro pour g√©rer un onduleur connect√© localement</a></li>
<li><a href="../fr443738/index.html">Comment d√©crocher un emploi en Allemagne pour les professionnels de l'informatique</a></li>
<li><a href="../fr443740/index.html">Premi√®re version de l'outil de test de recherche de produit ouvert</a></li>
<li><a href="../fr443744/index.html">Encyclop√©die de l'√©clairage par Naughty Dog</a></li>
<li><a href="../fr443746/index.html">March√© du jeu, tendances et pr√©visions - Excellentes analyses d'App Annie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>