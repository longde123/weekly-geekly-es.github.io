<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∞üèæ üöÑ üåä Como usar a capacidade de armazenamento dispon√≠vel corretamente üõ°Ô∏è üíª ü§°</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="N√≥s usamos servi√ßos em nuvem h√° muito tempo: correio, armazenamento, redes sociais, mensagens instant√¢neas. Todos eles funcionam remotamente - enviamo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como usar a capacidade de armazenamento dispon√≠vel corretamente</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/480622/">  N√≥s usamos servi√ßos em nuvem h√° muito tempo: correio, armazenamento, redes sociais, mensagens instant√¢neas.  Todos eles funcionam remotamente - enviamos mensagens e arquivos e s√£o armazenados e processados ‚Äã‚Äãem servidores remotos.  Os jogos na nuvem tamb√©m funcionam: o usu√°rio se conecta ao servi√ßo, seleciona o jogo e inicia.  Isso √© conveniente para o jogador, porque os jogos s√£o iniciados quase instantaneamente, n√£o ocupam mem√≥ria e n√£o precisam de um computador poderoso para jogos. <br><br><img src="https://habrastorage.org/webt/ej/k4/oy/ejk4oyjjh1r3riqgzzd239qu_va.jpeg"><br><br>  Para um servi√ßo em nuvem, tudo √© diferente - ele tem problemas de armazenamento de dados.  Cada jogo pode pesar dezenas ou centenas de gigabytes, por exemplo, "The Witcher 3" leva 50 GB e "Call of Duty: Black Ops III" - 113. Ao mesmo tempo, os jogadores n√£o usam o servi√ßo com 2-3 jogos, pelo menos v√°rias dezenas s√£o necess√°rias .  Al√©m de armazenar centenas de jogos, o servi√ßo precisa decidir quanto armazenamento alocar por jogador e escalar quando houver milhares deles. <br><br>  Tudo isso deve ser armazenado em seus servidores: quantos eles precisam, onde colocar os data centers, como "sincronizar" dados entre v√°rios data centers em tempo real?  Compre "nuvens"?  Use m√°quinas virtuais?  √â poss√≠vel armazenar dados do usu√°rio com compress√£o 5 vezes e fornec√™-los em tempo real?  Como excluir qualquer influ√™ncia de usu√°rios um sobre o outro durante o uso consistente da mesma m√°quina virtual? <br><br>  Todas essas tarefas foram resolvidas com sucesso no Playkey.net - uma plataforma de jogos baseada em nuvem.  <strong>Vladimir Ryabov</strong> ( <a href="https://habr.com/ru/users/graymansama/" class="user_link">Graymansama</a> ) - chefe do departamento de administra√ß√£o de sistemas - falar√° em detalhes sobre a tecnologia ZFS para o FreeBSD, que ajudou nisso, e seu novo fork do ZOL (ZFS no Linux). <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SssLwMbMrQ4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Mil servidores da empresa est√£o localizados em data centers remotos em Moscou, Londres e Frankfurt.  Existem mais de 250 jogos no servi√ßo, disputados por 100 mil jogadores por m√™s. <br><br><img src="https://habrastorage.org/webt/s6/mv/nw/s6mvnwi5z_b5ltjl_vico2x7ro4.jpeg"><br><br>  O servi√ßo funciona assim: o jogo √© executado nos servidores da empresa, o usu√°rio recebe um fluxo de controles do teclado, mouse ou gamepad e um fluxo de v√≠deo √© enviado em resposta.  Isso permite que voc√™ jogue jogos modernos de ponta em computadores com hardware fraco, laptops com v√≠deo integrado ou em Macs para os quais esses jogos n√£o s√£o lan√ßados. <br><br><h2>  Os jogos devem ser armazenados e atualizados </h2><br>  Os principais dados do servi√ßo de jogos em nuvem s√£o as distribui√ß√µes de jogos, que podem exceder centenas de GB, e a economia do usu√°rio. <br><br>  Quando √©ramos pequenos, t√≠nhamos apenas uma d√∫zia de servidores e um modesto cat√°logo de 50 jogos.  Armazenamos todos os dados localmente nos servidores, atualizados manualmente, tudo estava bem.  Mas chegou a hora de crescer e partimos <strong>para as nuvens da AWS</strong> . <br><br>  Com a AWS, temos v√°rias centenas de servidores, mas a arquitetura n√£o mudou.  Eles tamb√©m eram servidores, mas agora virtuais, com discos locais nos quais as distribui√ß√µes de jogos estavam.  No entanto, a atualiza√ß√£o manual em cem servidores falhar√°. <br><br>  Come√ßamos a procurar uma solu√ß√£o.  Inicialmente, tentamos atualizar via <strong>rsync</strong> .  Mas aconteceu que isso √© extremamente lento e a carga no n√≥ principal √© demais.  Mas isso nem √© o pior: quando t√≠nhamos pouco online, desligamos algumas das m√°quinas virtuais para n√£o pagar por elas e, ao atualizar, os dados n√£o foram derramados nos servidores desligados.  Todos eles foram deixados sem atualiza√ß√µes. <br><br>  A solu√ß√£o foi torrents - o programa <strong>BTSync</strong> .  Ele permite sincronizar uma pasta em um grande n√∫mero de n√≥s sem especificar explicitamente um n√≥ central. <br><br><h2>  Problemas de crescimento </h2><br>  Por um tempo, tudo isso funcionou maravilhosamente.  Mas o servi√ßo estava em desenvolvimento, havia mais jogos e servidores.  O n√∫mero de dep√≥sitos locais tamb√©m aumentou, tivemos que pagar cada vez mais.  Nas nuvens, √© caro, especialmente para SSDs.  A certa altura, at√© a indexa√ß√£o usual de uma pasta para iniciar sua sincroniza√ß√£o come√ßou a demorar mais de uma hora, e todos os servidores podiam ser atualizados por v√°rios dias. <br><br>  O BTSync criou outro problema com tr√°fego de rede excessivo.  Naquela √©poca, na Amazon, era pago mesmo entre virtuais internos.  Se o iniciador de jogos cl√°ssico fizer pequenas altera√ß√µes em arquivos grandes, o BTSync imediatamente acreditar√° que o arquivo inteiro foi alterado e come√ßar√° a transferi-lo totalmente para todos os n√≥s.  Como resultado, mesmo uma atualiza√ß√£o de 15 MB pode gerar dezenas de GB de tr√°fego de sincroniza√ß√£o. <br><br>  A situa√ß√£o tornou-se cr√≠tica quando o armazenamento aumentou para 1 TB.  Acabou de lan√ßar um novo jogo World of Warships.  Sua distribui√ß√£o tinha v√°rias centenas de milhares de arquivos pequenos.  O BTSync n√£o conseguiu digeri-lo e distribu√≠-lo a todos os outros servidores - isso atrasou a distribui√ß√£o de outros jogos. <br><br>  Todos esses fatores criaram dois problemas: <br><br><ul><li>  produzir armazenamento local √© caro, inconveniente e dif√≠cil de atualizar; </li><li>  as nuvens eram muito caras. </li></ul><br>  Decidimos voltar ao conceito de nossos servidores f√≠sicos. <br><br><h2>  Sistema de armazenamento pr√≥prio </h2><br>  Antes de passar para servidores f√≠sicos, precisamos nos livrar do armazenamento local.  Isso requer seu pr√≥prio <strong>sistema de armazenamento - armazenamento</strong> .  Este √© um sistema que armazena todas as distribui√ß√µes e as distribui centralmente para todos os servidores. <br><br>  Parece que a tarefa √© simples - j√° foi resolvida repetidamente.  Mas com jogos existem nuances.  Por exemplo, a maioria dos jogos simplesmente se recusa a trabalhar se tiver acesso somente leitura.  Mesmo com a inicializa√ß√£o normal usual, eles gostam de escrever algo em seus arquivos e, sem isso, se recusam a trabalhar.  Pelo contr√°rio, se um grande n√∫mero de usu√°rios tiver acesso a um conjunto de distribui√ß√µes, eles come√ßar√£o a superar os arquivos uns dos outros com acesso competitivo. <br><br>  Pensamos no problema, verificamos v√°rias solu√ß√µes poss√≠veis e chegamos ao <strong>ZFS - Zettabyte File System no FreeBSD</strong> . <br><br><h2>  ZFS no FreeBSD </h2><br>  Este n√£o √© um sistema de arquivos comum.  Os sistemas cl√°ssicos s√£o instalados inicialmente em um dispositivo e para trabalhar com v√°rios discos j√° exigem um gerenciador de volume. <br><blockquote>  O ZFS foi originalmente constru√≠do em pools virtuais. </blockquote>  Eles s√£o chamados <strong>zpool</strong> e consistem em grupos de discos ou matrizes RAID.  O volume inteiro desses discos est√° dispon√≠vel para qualquer sistema de arquivos no zpool.  Isso ocorre porque o ZFS foi desenvolvido originalmente como um sistema que funcionar√° com grandes quantidades de dados. <br><br><h3>  Como o ZFS ajudou a resolver nossos problemas </h3><br>  Este sistema possui um <strong>mecanismo</strong> maravilhoso <strong>para criar snapshots e clones</strong> .  Eles s√£o criados <strong>instantaneamente</strong> e pesam apenas alguns KB.  Quando fazemos altera√ß√µes em um dos clones, aumenta o volume dessas altera√ß√µes.  Ao mesmo tempo, os dados nos clones restantes n√£o s√£o alterados e permanecem √∫nicos.  Isso permite que voc√™ distribua um disco de <strong>10 TB</strong> com acesso exclusivo ao usu√°rio final, gastando apenas alguns KB. <br><br>  Se os clones crescerem no processo de fazer altera√ß√µes em uma sess√£o de jogo, eles n√£o ocupar√£o tanto espa√ßo quanto todos os jogos?  N√£o, descobrimos que, mesmo em sess√µes bastante longas, o conjunto de altera√ß√µes raramente excede 100-200 MB - isso n√£o √© cr√≠tico.  Portanto, podemos dar acesso total a um disco r√≠gido de alta capacidade para v√°rias centenas de usu√°rios ao mesmo tempo, gastando apenas 10 TB com uma cauda. <br><br><h3>  Como o ZFS funciona </h3><br>  A descri√ß√£o parece complicada, mas o ZFS funciona de maneira bastante simples.  Vamos analisar seu trabalho com um exemplo simples - criar <code>zpool data</code> partir dos discos <code>zpool create data /dev/da /dev/db /dev/dc</code> dispon√≠veis, <code>zpool create data /dev/da /dev/db /dev/dc</code> . <br><br>  <em>Nota</em>  <em>Isso n√£o √© necess√°rio para a produ√ß√£o, porque se pelo menos um disco morrer, todo o pool ficar√° esquecido.</em>  <em>Melhor usar grupos RAID.</em> <br><br>  N√≥s criamos o <code>zfs create data/games</code> file system, e nele um dispositivo de bloco com o nome <code>data/games/disk</code> de 10 TB.  O dispositivo est√° dispon√≠vel em <code>/dev/zvol/data/games/disk</code> como um disco normal - voc√™ pode executar as mesmas manipula√ß√µes. <br><br>  Ent√£o a divers√£o come√ßa.  Damos esse disco via <strong>iSCSI ao</strong> nosso assistente de atualiza√ß√£o - uma m√°quina virtual comum executando o Windows.  Conectamos o disco e colocamos os jogos nele simplesmente no Steam, como em um computador dom√©stico comum. <br><br>  Encha o disco com jogos.  Agora resta distribuir esses dados para <strong>200 servidores</strong> para usu√°rios finais. <br><br><ul><li>  Crie um instant√¢neo deste disco e chame-o de primeira vers√£o - <code>zfs snapshot data/games/disk@ver1</code> .  <strong>Crie seu clone</strong> <code>zfs clone data/games/disk@ver1 data/games/disk-vm1</code> , que ir√° para a primeira m√°quina virtual. </li><li>  Damos o clone via iSCSI e o <strong>KVM lan√ßa uma</strong> m√°quina virtual <strong>com este disco</strong> .  Carrega, entra em um conjunto de servidores acess√≠veis para os usu√°rios e espera um jogador. </li><li>  Quando a sess√£o do usu√°rio √© conclu√≠da, pegamos todos os salvamentos de usu√°rios dessa m√°quina virtual e os <strong>colocamos em um servidor separado</strong> .  <strong>Desligamos a m√°quina</strong> virtual <strong>e destru√≠mos o clone</strong> - o <code>zfs destroy data/games/disk-vm1</code> . </li><li>  Retornamos ao primeiro passo, crie novamente um clone e inicie a m√°quina virtual. </li></ul><br>  Isso nos permite fornecer a cada pr√≥ximo usu√°rio uma <strong>m√°quina sempre limpa</strong> , na qual n√£o h√° altera√ß√µes em rela√ß√£o ao player anterior.  O disco ap√≥s cada sess√£o do usu√°rio √© exclu√≠do e o espa√ßo que ele ocupava no sistema de armazenamento √© liberado.  Tamb√©m realizamos opera√ß√µes semelhantes com o disco do sistema e com todas as nossas m√°quinas virtuais. <br><br>  Recentemente, me deparei com um v√≠deo no YouTube, onde um usu√°rio satisfeito durante uma sess√£o de jogo formatou nossos discos r√≠gidos em servidores e fiquei muito feliz por ele ter quebrado tudo.  Sim, por favor, apenas para pagar - ele pode jogar e entrar.  De qualquer forma, o pr√≥ximo usu√°rio sempre ter√° uma m√°quina virtual limpa e funcional, independentemente do que o anterior fizer. <br><br>  De acordo com esse esquema, os jogos s√£o distribu√≠dos para apenas 200 servidores.  Calculamos o n√∫mero 200 experimentalmente: este √© o n√∫mero de servidores nos quais n√£o ocorrem cargas cr√≠ticas nas unidades de armazenamento.  Isso ocorre porque os <strong>jogos t√™m um perfil de carga bastante espec√≠fico</strong> : eles l√™em muito no est√°gio de lan√ßamento ou no n√≠vel de carregamento e, durante o jogo, pelo contr√°rio, praticamente n√£o usam um disco.  Se o seu perfil de carga for diferente, a figura ser√° diferente. <br><br>  No esquema antigo, para servi√ßos simult√¢neos a 200 usu√°rios, precisar√≠amos de 2.000 TB de armazenamento local.  Agora podemos gastar um pouco mais de 10 TB no conjunto de dados principal e ainda existem 0,5 TB em estoque para altera√ß√µes do usu√°rio.  Embora o ZFS adore quando possui pelo menos 15% de espa√ßo livre em seu pool, parece-me que economizamos significativamente. <br><br><h3>  E se tivermos v√°rios data centers? </h3><br>  Esse mecanismo funcionar√° apenas dentro de um data center, onde os servidores com um sistema de armazenamento s√£o conectados por pelo menos 10 interfaces de gigabit.  O que fazer se houver v√°rios CDs?  Como atualizar o disco principal com jogos (conjunto de dados) entre eles? <br><br>  Para isso, o ZFS possui sua pr√≥pria solu√ß√£o - <strong>o mecanismo de envio / recebimento</strong> .  O comando de execu√ß√£o √© muito simples: <br><pre> <code class="bash hljs">zfs send -v data/games/disk@ver1 | ssh myzfsuser@myserverip zfs receive data/games/disk</code> </pre> <br>  O mecanismo permite transferir de um sistema de armazenamento para outro um instant√¢neo do sistema principal.  Pela primeira vez, voc√™ precisar√° enviar todos os 10 terabytes de dados gravados no n√≥ mestre para um sistema de armazenamento vazio.  Por√©m, com as pr√≥ximas atualiza√ß√µes, enviaremos apenas altera√ß√µes a partir do momento em que criamos o instant√¢neo anterior. <br><br>  Como resultado, obtemos: <br><br><ul><li>  <strong>Todas as altera√ß√µes s√£o feitas centralmente em um sistema de armazenamento</strong> .  Em seguida, eles se dispersam para todos os outros data centers em qualquer quantidade, e os dados em todos os n√≥s s√£o sempre id√™nticos. </li><li>  <strong>O mecanismo de envio / recebimento n√£o tem medo de uma desconex√£o</strong> .  Os dados n√£o s√£o aplicados ao conjunto de dados principal at√© que sejam completamente transmitidos ao n√≥ escravo.  Se a conex√£o for perdida, √© imposs√≠vel danificar os dados e basta repetir o procedimento de envio. </li><li>  <strong>Qualquer n√≥ pode facilmente se tornar um n√≥ mestre</strong> durante um acidente em apenas alguns minutos, pois os dados em todos os n√≥s s√£o sempre id√™nticos. </li></ul><br><h3>  Desduplica√ß√£o e backups </h3><br>  O ZFS possui outro recurso √∫til - a <strong>desduplica√ß√£o</strong> .  Esta fun√ß√£o ajuda a <strong>n√£o armazenar dois blocos de dados id√™nticos</strong> .  Em vez disso, apenas o primeiro bloco √© armazenado e, no lugar do segundo, um link para o primeiro √© armazenado.  Dois arquivos id√™nticos ocupam espa√ßo como um e, se corresponderem a 90%, preencher√£o 110% do volume original. <br><br>  A fun√ß√£o nos ajudou muito no armazenamento de salvamentos do usu√°rio.  Em um jogo, usu√°rios diferentes t√™m salvamentos semelhantes, muitos arquivos s√£o iguais.  Com o uso da desduplica√ß√£o, podemos armazenar cinco vezes mais dados.  Nossa taxa de deduplica√ß√£o √© de 5,22.  Fisicamente, temos 4,43 terabytes, multiplicamos por um fator e obtemos quase 23 terabytes de dados reais.  Isso economiza espa√ßo, evitando armazenamento duplicado. <br><div class="scrollable-table"><table><tbody><tr><td>  NAME </td><td>  Tamanho </td><td>  ALLOC </td><td>  GR√ÅTIS </td><td>  DEDUP </td></tr><tr><td>  dados </td><td>  7,16 TB </td><td>  4,43 TB </td><td>  2,73 TB </td><td>  5,22x </td></tr></tbody></table></div>  <strong>Instant√¢neos s√£o bons para backups</strong> .  Usamos essa tecnologia em nossos armazenamentos de arquivos.  Por exemplo, se voc√™ salvar uma imagem todos os dias durante um m√™s, poder√° implantar um clone a qualquer momento em qualquer dia desse m√™s e retirar arquivos perdidos ou danificados.  Isso elimina a necessidade de reverter todo o armazenamento ou implantar uma c√≥pia completa dele. <br><br>  <strong>Usamos clones para ajudar nossos desenvolvedores</strong> .  Por exemplo, eles querem experimentar uma migra√ß√£o potencialmente perigosa em uma base de combate.  N√£o √© r√°pido implantar um backup cl√°ssico de um banco de dados com aproximadamente 1 TB.  Portanto, simplesmente removemos o clone do disco base e o adicionamos instantaneamente √† nova inst√¢ncia.  Agora, os desenvolvedores podem testar tudo com seguran√ßa. <br><br><h3>  API do ZFS </h3><br>  Claro, tudo isso deve ser automatizado.  Por que subir nos servidores, trabalhar com as m√£os, escrever scripts, se isso pode ser dado aos programadores?  Portanto, escrevemos nossa <a href="https://github.com/drook/zfsapi">API da Web</a> simples. <br><br>  Envolvemos todas as fun√ß√µes padr√£o do ZFS, cortamos o acesso √†quelas que s√£o potencialmente perigosas e que podem danificar todo o sistema de armazenamento e entregamos tudo isso aos programadores.  Agora <strong>todas as opera√ß√µes de disco s√£o estritamente centralizadas</strong> e executadas por c√≥digo, e <strong>sempre sabemos o status de cada disco</strong> .  Tudo funciona muito bem. <br><br><h2>  ZoL - ZFS no Linux </h2><br>  Centralizamos o sistema e pensamos: √© t√£o bom?  De fato, agora para qualquer extens√£o, precisamos comprar imediatamente v√°rios racks de servidor: eles est√£o vinculados a sistemas de armazenamento e √© irracional dividir o sistema.  O que fazer quando decidimos implantar um pequeno estande de demonstra√ß√£o para mostrar a tecnologia para parceiros em outros pa√≠ses? <br><br>  Pensando, chegamos √† velha id√©ia - <strong>usar unidades locais</strong> , mas apenas com toda a experi√™ncia e conhecimento que recebemos.  Se voc√™ expandir a ideia de maneira mais global, por que n√£o dar a nossos usu√°rios a oportunidade n√£o apenas de usar nossos servidores, mas tamb√©m de alugar seus computadores? <br><br>  A relativamente recente bifurca√ß√£o do <strong>ZFS no Linux - ZoL</strong> nos ajudou muito nisso. <br><blockquote>  Agora cada servidor tem seu pr√≥prio armazenamento. </blockquote>  Somente ele n√£o armazena 10 terabytes de dados, como no caso de uma instala√ß√£o centralizada, mas apenas 1-2 distribui√ß√µes dos jogos que ele serve.  Um SSD √© suficiente para isso.  Tudo isso funciona bem: todo usu√°rio seguinte sempre recebe uma m√°quina virtual limpa, bem como uma instala√ß√£o de combate. <br><br>  No entanto, aqui encontramos dois problemas. <br><br><h3>  Como atualizar? </h3><br>  <strong>Atualize centralmente via SSH, como fazemos nos data centers n√£o funcionar√°</strong> .  Os usu√°rios podem estar conectados √† rede local ou simplesmente desligados, ao contr√°rio dos sistemas de armazenamento, e voc√™ n√£o deseja criar tantas conex√µes SSH. <br><br>  Encontramos os mesmos problemas que ao usar o rsync.  No entanto, os torrents sobre o ZFS n√£o podem mais ser obtidos.  N√≥s pensamos cuidadosamente sobre como o mecanismo de envio funciona: ele envia todos os blocos de dados alterados para o armazenamento final, onde Receive os aplica ao conjunto de dados atual.  Por que n√£o gravar os dados em um arquivo, em vez de envi√°-los para o usu√°rio final? <br><br>  O resultado √© o que chamamos de <strong>diff</strong> .  Este √© um arquivo no qual todos os blocos alterados entre os dois √∫ltimos instant√¢neos s√£o gravados seq√ºencialmente.  Colocamos esse diff em uma CDN e enviamos a todos os nossos usu√°rios via HTTP: ele ligou a m√°quina, viu que havia atualiza√ß√µes, esvaziou e aplicou-a ao conjunto de dados local usando Receive. <br><br><h3>  O que fazer com os motoristas? </h3><br>  Servidores centralizados t√™m a mesma configura√ß√£o e os <strong>usu√°rios finais sempre t√™m computadores e placas de v√≠deo diferentes</strong> .  Mesmo se preenchermos a distribui√ß√£o do sistema operacional com todos os drivers poss√≠veis, tanto quanto poss√≠vel, na primeira vez em que ele iniciar, ele ainda desejar√° instalar esses drivers, depois reiniciar√° e, possivelmente, novamente.  Como toda vez que fornecemos um clone limpo, todo esse carrossel ocorre ap√≥s cada sess√£o do usu√°rio - isso √© ruim. <br><br>  Quer√≠amos fazer alguma inicializa√ß√£o: espere at√© o Windows inicializar, instale todos os drivers, fa√ßa tudo o que ela deseja e s√≥ ent√£o opere nesta unidade.  Mas o problema √© que, se voc√™ fizer altera√ß√µes no conjunto de dados principal, as atualiza√ß√µes ser√£o interrompidas, porque os dados na fonte e no receptor ser√£o diferentes e o diff simplesmente n√£o ser√° aplicado. <br><br>  No entanto, o ZFS √© um sistema flex√≠vel e nos permitiu fazer uma pequena muleta. <br><br><ul><li>  Como de costume, crie uma captura instant√¢nea: <code>zfs snapshot data/games/os@init</code> . </li><li>  Crie seu clone - <code>zfs clone data/games/os@init data/games/os-init</code> - e execute-o no modo de inicializa√ß√£o. </li><li>  Estamos aguardando a instala√ß√£o de todos os drivers e tudo ser√° reiniciado. </li><li>  Desligue a m√°quina virtual e tire um instant√¢neo novamente.  Mas desta vez, n√£o do conjunto de dados original, mas do clone de inicializa√ß√£o: <code>zfs snapshot data/games/os-init@ver1</code> . </li><li>  Criamos um clone do instant√¢neo com todos os drivers instalados.  Ele n√£o ser√° mais reinicializado: <code>zfs clone data/games/os-init@ver1 data/games/os-vm1</code> . </li><li>  Ent√£o trabalhamos no grupo cl√°ssico. </li></ul><br>  Agora este sistema est√° na fase de teste alfa.  Testamos em usu√°rios reais sem o conhecimento do Linux, mas eles conseguem implant√°-lo em casa.  Nosso objetivo final √© que qualquer usu√°rio simplesmente conecte uma unidade flash USB inicializ√°vel em seu computador, conecte uma unidade SSD adicional e alugue-a em nossa plataforma em nuvem. <br><br>  Discutimos apenas uma pequena parte da funcionalidade do ZFS.  Esse sistema pode fazer coisas muito mais interessantes e diferentes, mas poucas pessoas sabem sobre o ZFS - os usu√°rios n√£o querem falar sobre isso.  Espero que, ap√≥s este artigo, novos usu√°rios apare√ßam na comunidade ZFS. <br><br><blockquote>  Assine um <a href="https://t.me/DevOpsConfChannel">canal de telegrama</a> ou <a href="http://eepurl.com/bN_0E1">boletim informativo</a> para aprender sobre novos artigos e v√≠deos da confer√™ncia <a href="https://devopsconf.io/">DevOpsConf</a> .  Al√©m do boletim, coletamos not√≠cias das pr√≥ximas confer√™ncias e informamos, por exemplo, o que ser√° interessante para os f√£s do DevOps no <a href="https://www.highload.ru/spb/2020">Saint HighLoad ++</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt480622/">https://habr.com/ru/post/pt480622/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt480610/index.html">Tabelas em C ++. Parte 2 (heran√ßa virtual + c√≥digo gerado pelo compilador)</a></li>
<li><a href="../pt480612/index.html">Fa√ßa essas altera√ß√µes para atender aos padr√µes de acessibilidade do design da web.</a></li>
<li><a href="../pt480614/index.html">ENUM r√°pido</a></li>
<li><a href="../pt480618/index.html">Jogo eletr√¥nico Tic Tac Toe. O que eu vim para</a></li>
<li><a href="../pt480620/index.html">SD-WAN e DNA para ajudar o administrador: recursos de arquitetura e pr√°tica</a></li>
<li><a href="../pt480626/index.html">Heran√ßa de sistemas e processos legados ou Os primeiros 90 dias no papel de CTO</a></li>
<li><a href="../pt480642/index.html">Introdu√ß√£o aos ELFs do Linux: Entendendo e Analisando</a></li>
<li><a href="../pt480644/index.html">O manifesto sobre a aboli√ß√£o de 146 do C√≥digo Penal e o boicote ao Sberbank e aos detentores de direitos autorais-parasitas. Para c√≥digo-fonte aberto e nginx</a></li>
<li><a href="../pt480646/index.html">Habr - melhores artigos, autores e estat√≠sticas 2019</a></li>
<li><a href="../pt480650/index.html">Cujo cabelo √© mais forte: morfologia capilar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>