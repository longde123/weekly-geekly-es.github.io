<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👘 🧑🏾 🚣🏾 Buku "Belajar mendalam Grok" 🔭 🚬 🤾🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hai, habrozhiteli! Buku ini meletakkan dasar untuk penguasaan lebih lanjut dari teknologi pembelajaran yang mendalam. Ini dimulai dengan deskripsi das...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Buku "Belajar mendalam Grok"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/464509/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/webt/go/gm/1s/gogm1solwetphsozljuyzuizcbs.jpeg" align="left" alt="gambar"></a>  Hai, habrozhiteli!  Buku ini meletakkan dasar untuk penguasaan lebih lanjut dari teknologi pembelajaran yang mendalam.  Ini dimulai dengan deskripsi dasar-dasar jaringan saraf dan kemudian memeriksa secara rinci lapisan tambahan arsitektur. <br><br>  Buku ini ditulis secara khusus dengan tujuan memberikan ambang masuk serendah mungkin.  Anda tidak perlu pengetahuan tentang aljabar linier, metode numerik, optimisasi cembung, dan bahkan pembelajaran mesin.  Semua yang diperlukan untuk memahami pembelajaran mendalam akan diklarifikasi saat Anda pergi. <br><br>  Kami menawarkan Anda untuk berkenalan dengan bagian "Apa itu kerangka pembelajaran yang mendalam?" <br><a name="habracut"></a><br>  <b>Alat yang bagus mengurangi kesalahan, pengembangan kecepatan, dan meningkatkan kecepatan eksekusi.</b> <br><br>  Jika Anda membaca banyak tentang pembelajaran mendalam, maka Anda mungkin menemukan kerangka kerja yang terkenal seperti PyTorch, TensorFlow, Theano (baru-baru ini dinyatakan usang), Keras, Lasagne dan DyNet.  Dalam beberapa tahun terakhir, kerangka kerja telah berevolusi dengan sangat cepat, dan terlepas dari kenyataan bahwa semua kerangka kerja ini didistribusikan secara bebas dan open source, masing-masing dari mereka memiliki semangat kompetisi dan persahabatan. <br><br>  Sampai sekarang, saya telah menghindari membahas kerangka kerja, karena, pertama-tama, sangat penting bagi Anda untuk memahami apa yang terjadi di balik layar, mengimplementasikan algoritma secara manual (hanya menggunakan perpustakaan NumPy).  Tapi sekarang kita akan mulai menggunakan kerangka kerja seperti itu, karena jaringan yang akan kita latih, jaringan dengan memori jangka pendek (LSTM), sangat kompleks, dan kode yang mengimplementasikannya menggunakan NumPy sulit dibaca, digunakan, dan debug (gradien dalam kode ini) ditemukan di mana-mana). <br><br>  Kerumitan inilah yang dirancang untuk ditangani oleh kerangka kerja pembelajaran dalam.  Kerangka belajar yang dalam dapat secara signifikan mengurangi kompleksitas kode (serta mengurangi jumlah kesalahan dan meningkatkan kecepatan pengembangan) dan meningkatkan kecepatan pelaksanaannya, terutama jika Anda menggunakan prosesor grafis (GPU) untuk melatih jaringan saraf, yang dapat mempercepat proses dengan 10-100 kali.  Untuk alasan ini, kerangka kerja digunakan hampir di mana-mana di komunitas penelitian, dan pemahaman tentang fitur pekerjaan mereka akan berguna bagi Anda dalam karir Anda sebagai pengguna dan peneliti pembelajaran yang mendalam. <br><br>  Tetapi kami tidak akan membatasi diri pada kerangka kerja kerangka kerja tertentu, karena ini akan mencegah Anda mempelajari bagaimana semua model yang rumit ini (seperti LSTM) bekerja.  Sebagai gantinya, kami akan membuat kerangka kerja ringan kami sendiri, mengikuti tren terbaru dalam pengembangan kerangka kerja.  Mengikuti jalur ini, Anda akan tahu persis apa yang dilakukan kerangka kerja ketika arsitektur kompleks dibuat dengan bantuan mereka.  Selain itu, upaya untuk membuat kerangka kerja kecil Anda sendiri akan membantu Anda dengan lancar beralih ke penggunaan kerangka kerja pembelajaran yang benar-benar mendalam, karena Anda sudah tahu prinsip-prinsip mengatur antarmuka program (API) dan fungsinya.  Latihan ini sangat berguna bagi saya, dan pengetahuan yang saya peroleh saat membuat kerangka kerja saya sendiri ternyata sangat membantu dalam men-debug model yang bermasalah. <br><br>  Bagaimana kerangka kerja menyederhanakan kode?  Berbicara secara abstrak, itu menghilangkan kebutuhan untuk menulis kode yang sama lagi dan lagi.  Secara khusus, fitur yang paling nyaman dari kerangka pembelajaran yang dalam adalah dukungan untuk backpropagation otomatis dan optimasi otomatis.  Ini memungkinkan Anda untuk menulis hanya kode distribusi langsung, dan kerangka kerja akan secara otomatis menangani distribusi kembali dan koreksi bobot.  Sebagian besar kerangka kerja modern bahkan menyederhanakan kode yang mengimplementasikan distribusi langsung dengan menawarkan antarmuka tingkat tinggi untuk mendefinisikan lapisan dan fungsi kerugian yang khas. <br><br><h3>  Pengantar Tensor </h3><br>  <b>Tensor adalah bentuk abstrak vektor dan matriks</b> <br><br>  Hingga saat ini, kami menggunakan vektor dan matriks sebagai struktur utama.  Biarkan saya mengingatkan Anda bahwa matriks adalah daftar vektor, dan vektor adalah daftar skalar (angka tunggal).  Tensor adalah bentuk abstrak untuk mewakili daftar angka bersarang.  Vektor adalah tensor satu dimensi.  Matriks adalah tensor dua dimensi, dan struktur dengan sejumlah besar dimensi disebut tensor n-dimensional.  Jadi, mari kita mulai membuat kerangka kerja pembelajaran dalam yang baru dengan mendefinisikan tipe dasar, yang akan kita sebut Tensor: <br><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class"> </span></span>= np.array(data) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) print(x) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span>] y = x + x print(y) [<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>]</code> </pre> <br>  Ini adalah versi pertama dari struktur data dasar kami.  Perhatikan bahwa ia menyimpan semua informasi numerik dalam array NumPy (self.data) dan mendukung operasi tensor tunggal (tambahan).  Menambahkan operasi tambahan tidak sulit sama sekali, cukup tambahkan fungsi tambahan dengan fungsionalitas yang sesuai ke kelas Tensor. <br><br><h3>  Pengantar perhitungan gradien otomatis (autograd) </h3><br>  <b>Sebelumnya, kami melakukan propagasi balik manual.</b>  <b>Sekarang mari kita membuatnya otomatis!</b> <br><br>  Dalam bab 4, kami memperkenalkan turunan.  Sejak itu, kami secara manual menghitung turunan ini di setiap jaringan saraf baru.  Biarkan saya mengingatkan Anda bahwa ini dicapai dengan gerakan terbalik melalui jaringan saraf: pertama, gradien pada output jaringan dihitung, maka hasil ini digunakan untuk menghitung turunan dalam komponen sebelumnya, dan seterusnya, hingga gradien yang benar ditentukan untuk semua bobot dalam arsitektur.  Logika ini untuk menghitung gradien juga dapat ditambahkan ke kelas tensor.  Berikut ini menunjukkan apa yang ada dalam pikiran saya. <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">creators</span></span></span></span>=None, creation_op=None): self.data = np.array(data) self.creation_op = creation_op self.creators = creators self.grad = None def backward(self, grad): self.grad = grad <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(self.creation_op == <span class="hljs-string"><span class="hljs-string">"add"</span></span>): self.creators[<span class="hljs-number"><span class="hljs-number">0</span></span>].backward(grad) self.creators[<span class="hljs-number"><span class="hljs-number">1</span></span>].backward(grad) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data, creators=[self,other], creation_op=<span class="hljs-string"><span class="hljs-string">"add"</span></span>) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) y = Tensor([<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>]) z = x + y z.backward(Tensor(np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>])))</code> </pre> <br>  Metode ini memperkenalkan dua inovasi.  Pertama, setiap tensor menerima dua atribut baru.  creator adalah daftar tensor apa pun yang digunakan untuk membuat tensor saat ini (default ke None).  Yaitu, jika tensor z diperoleh dengan menambahkan dua tensor lainnya, x dan y, atribut pencipta tensor z akan berisi tensor x dan y.  creation_op adalah atribut pengiring yang menyimpan operasi yang digunakan dalam proses pembuatan tensor ini.  Artinya, instruksi z = x + y akan membuat grafik komputasi dengan tiga node (x, y dan z) dan dua sisi (z -&gt; x dan z -&gt; y).  Setiap tepi ditandatangani oleh operasi dari creation_op, yaitu, tambahkan.  Grafik ini akan membantu mengatur propagasi gradien mundur rekursif. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/4d/qz/fx4dqzrh6y62rtttfy2vrn1jley.png" alt="gambar"></div><br>  Inovasi pertama dalam implementasi ini adalah pembuatan grafik secara otomatis selama setiap operasi matematika.  Jika kita mengambil z dan melakukan operasi lain, grafik akan dilanjutkan dalam referensi variabel baru z. <br><br>  Inovasi kedua dalam versi kelas Tensor ini adalah kemampuan untuk menggunakan grafik untuk menghitung gradien.  Jika Anda memanggil metode z.backward (), ia akan melewati gradien untuk x dan y, dengan mempertimbangkan fungsi yang digunakan tensor z (add) dibuat.  Seperti yang ditunjukkan pada contoh di atas, kita melewatkan vektor gradien (np.array ([1,1,1,1,1]]) ke z, dan yang itu berlaku untuk orang tuanya.  Seperti yang mungkin Anda ingat dari Bab 4, propagasi mundur melalui penjumlahan berarti menerapkan propagasi mundur.  Dalam hal ini, kami hanya memiliki satu gradien untuk ditambahkan ke x dan y, jadi kami menyalinnya dari z ke x dan y: <br><br><pre> <code class="javascript hljs">print(x.grad) print(y.grad) print(z.creators) print(z.creation_op) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>]), array([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])] add</code> </pre> <br>  Fitur yang paling luar biasa dari bentuk perhitungan gradien otomatis ini adalah bahwa ia bekerja secara rekursif - setiap vektor memanggil metode .backward () dari semua orang tuanya dari daftar self.creators: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bt/ye/if/btyeiflumrhlbprlzqtzjxwipcs.png" alt="gambar"></div><br>  »Informasi lebih lanjut tentang buku ini dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web penerbit</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Isi</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kutipan</a> <br><br>  Kupon diskon 25% untuk pedagang asongan - <b>Deep Learning</b> <br>  Setelah pembayaran versi kertas buku, sebuah buku elektronik dikirim melalui email. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id464509/">https://habr.com/ru/post/id464509/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id464491/index.html">Vivaldi 2.7 - Kehidupan Intens dalam Keheningan</a></li>
<li><a href="../id464495/index.html">Pengembangan tim dan refleksi sebagai komunikasi manajerial pemimpin tim</a></li>
<li><a href="../id464497/index.html">JIRA sebagai obat untuk insomnia dan gangguan saraf</a></li>
<li><a href="../id464499/index.html">"Mat. Wall Street model ”atau upaya untuk mengoptimalkan biaya infrastruktur cloud IT</a></li>
<li><a href="../id464503/index.html">Kata sandi Wi-Fi cocok dengan aircrack-ng</a></li>
<li><a href="../id464511/index.html">Cara mengumpulkan kohort pengguna dalam bentuk grafik di Grafana [+ buruh pelabuhan gambar dengan contoh]</a></li>
<li><a href="../id464513/index.html">Duffle: Transformer dari XD Design</a></li>
<li><a href="../id464515/index.html">Cara Membuat Email dan Tidak Mengacaukan: Kiat Praktis</a></li>
<li><a href="../id464517/index.html">Kartu CUBA Baru</a></li>
<li><a href="../id464523/index.html">Sistem Pembayaran (PSP) untuk bisnis TI: kami bermain besar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>