<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏣 🤝 👨‍👧‍👧 Optimalisasi Strategi Monte Carlo Blackjack ❎ 🤖 🔕</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Terjemahan artikel disiapkan khusus untuk siswa kursus Pembelajaran Mesin . 



 Pelatihan yang diperkuat mengambil dunia Kecerdasan Buatan. Mulai dar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimalisasi Strategi Monte Carlo Blackjack</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>Terjemahan artikel disiapkan khusus untuk siswa kursus <a href="https://otus.pw/Zkti/">Pembelajaran Mesin</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  Pelatihan yang diperkuat mengambil dunia Kecerdasan Buatan.  Mulai dari AlphaGo dan <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> , semakin banyak kegiatan yang sebelumnya didominasi oleh manusia sekarang ditaklukkan oleh agen AI berdasarkan pelatihan penguatan.  Singkatnya, pencapaian ini bergantung pada pengoptimalan tindakan agen dalam lingkungan tertentu untuk mencapai hadiah maksimum.  Dalam beberapa artikel terakhir dari <a href="https://medium.com/gradientcrescent">GradientCrescent,</a> kami telah melihat berbagai aspek mendasar dari pembelajaran yang diperkuat, dari dasar-dasar sistem <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">bandit</a> dan pendekatan berbasis <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">kebijakan</a> hingga mengoptimalkan perilaku berbasis imbalan di <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">lingkungan Markov</a> .  Semua pendekatan ini membutuhkan pengetahuan lengkap tentang lingkungan kita.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">Pemrograman dinamis</a> , misalnya, mengharuskan kita memiliki distribusi probabilitas lengkap dari semua transisi keadaan yang mungkin.  Namun, pada kenyataannya, kami menemukan bahwa sebagian besar sistem tidak dapat sepenuhnya ditafsirkan, dan bahwa distribusi probabilitas tidak dapat diperoleh secara eksplisit karena kompleksitas, ketidakpastian yang melekat, atau keterbatasan dalam kemampuan komputasi.  Sebagai analogi, pertimbangkan tugas ahli meteorologi - jumlah faktor yang terlibat dalam peramalan cuaca bisa sangat besar sehingga tidak mungkin untuk secara akurat menghitung probabilitas. <a name="habracut"></a><br><br>  Untuk kasus seperti itu, metode pengajaran seperti Monte Carlo adalah solusinya.  Istilah Monte Carlo umumnya digunakan untuk menggambarkan pendekatan apa pun terhadap estimasi pengambilan sampel acak.  Dengan kata lain, kita tidak memprediksikan pengetahuan tentang lingkungan kita, tetapi belajar dari pengalaman dengan melalui urutan negara contoh, tindakan dan penghargaan yang diperoleh sebagai hasil interaksi dengan lingkungan.  Metode-metode ini bekerja dengan mengamati secara langsung hadiah yang dikembalikan oleh model selama operasi normal untuk menilai nilai rata-rata dari kondisinya.  Menariknya, bahkan tanpa pengetahuan tentang dinamika lingkungan (yang harus dianggap sebagai distribusi probabilitas transisi negara), kita masih bisa mendapatkan perilaku optimal untuk memaksimalkan imbalan. <br><br>  Sebagai contoh, perhatikan hasil lemparan 12 dadu.  Mempertimbangkan lemparan ini sebagai satu negara, kita dapat merata-rata hasil ini untuk lebih dekat dengan hasil prediksi yang sebenarnya.  Semakin besar sampel, semakin akurat kita akan mendekati hasil yang diharapkan. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>Jumlah rata-rata yang diharapkan pada 12 dadu untuk 60 tembakan adalah 41,57</i> <br><br>  Jenis penilaian berbasis pengambilan sampel ini mungkin tampak akrab bagi pembaca, karena pengambilan sampel seperti itu juga dilakukan untuk sistem k-bandit.  Alih-alih membandingkan bandit yang berbeda, metode Monte Carlo digunakan untuk membandingkan kebijakan yang berbeda di lingkungan Markov, menentukan nilai negara sebagai kebijakan tertentu diikuti sampai pekerjaan selesai. <br><br><h3>  Estimasi nilai Monte Carlo </h3><br>  Dalam konteks pembelajaran penguatan, metode Monte Carlo adalah cara untuk mengevaluasi signifikansi keadaan model dengan rata-rata hasil sampel.  Karena kebutuhan untuk keadaan terminal, metode Monte Carlo secara inheren berlaku untuk lingkungan episodik.  Karena keterbatasan ini, metode Monte Carlo biasanya dianggap "otonom" di mana semua pembaruan dilakukan setelah mencapai keadaan terminal.  Analogi sederhana dengan menemukan jalan keluar dari labirin dapat diberikan - pendekatan otonom akan memaksa agen untuk mencapai akhir sebelum menggunakan pengalaman perantara yang diperoleh untuk mencoba mengurangi waktu yang dibutuhkan untuk melewati labirin.  Di sisi lain, dengan pendekatan online, agen akan terus mengubah perilakunya selama lorong labirin, mungkin dia akan melihat bahwa koridor hijau mengarah ke jalan buntu dan memutuskan untuk menghindarinya, misalnya.  Kami akan membahas pendekatan online di salah satu artikel berikut. <br><br>  Metode Monte Carlo dapat dirumuskan sebagai berikut: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Untuk lebih memahami bagaimana metode Monte Carlo bekerja, pertimbangkan diagram transisi status di bawah ini.  Hadiah untuk setiap transisi negara ditampilkan dalam warna hitam, faktor diskon 0,5 diterapkan untuk itu.  Mari kita kesampingkan nilai aktual negara dan fokus pada menghitung hasil satu lemparan. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Diagram transisi keadaan.</i>  <i>Nomor status ditampilkan dalam warna merah, hasilnya hitam.</i> <br>  Mengingat bahwa keadaan terminal mengembalikan hasil yang sama dengan 0, mari kita menghitung hasil dari setiap keadaan, dimulai dengan keadaan terminal (G5).  Harap dicatat bahwa kami telah menetapkan faktor diskon ke 0,5, yang akan mengarah pada pembobotan terhadap status selanjutnya. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  Atau lebih umum: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Untuk menghindari menyimpan semua hasil dalam daftar, kita dapat melakukan prosedur memperbarui nilai keadaan dalam metode Monte Carlo secara bertahap, menggunakan persamaan yang memiliki beberapa kesamaan dengan penurunan gradien tradisional: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Prosedur pembaruan bertahap Monte Carlo.</i>  <i>S adalah status, V adalah nilainya, G adalah hasilnya, dan A adalah parameter nilai langkah.</i> <br><br>  Sebagai bagian dari pelatihan penguatan, metode Monte Carlo bahkan dapat diklasifikasikan sebagai Kunjungan Pertama atau Setiap Kunjungan.  Singkatnya, perbedaan antara keduanya adalah berapa kali suatu negara bagian dapat dikunjungi sebelum pembaruan Monte Carlo.  Metode Kunjungan Pertama Monte Carlo memperkirakan nilai semua negara bagian sebagai nilai rata-rata hasil setelah kunjungan tunggal ke masing-masing negara bagian sebelum penyelesaian, sedangkan metode setiap kunjungan Monte Carlo rata-rata hasil setelah n kunjungan sampai selesai.  Kami akan menggunakan Kunjungan Pertama Monte Carlo di seluruh artikel ini karena kesederhanaannya. <br><br><h3>  Manajemen Kebijakan Monte Carlo </h3><br>  Jika model tidak dapat menyediakan kebijakan, Monte Carlo dapat digunakan untuk mengevaluasi nilai tindakan negara.  Ini lebih berguna daripada sekadar makna negara, karena gagasan makna setiap tindakan <i>(q)</i> dalam keadaan tertentu memungkinkan agen untuk secara otomatis merumuskan kebijakan dari pengamatan di lingkungan yang tidak dikenal. <br><br>  Secara lebih formal, kita dapat menggunakan Monte Carlo untuk memperkirakan <i>q (s, a, pi)</i> , hasil yang diharapkan ketika mulai dari negara, tindakan a, dan kebijakan selanjutnya <i>Pi</i> .  Metode Monte Carlo tetap sama, kecuali bahwa ada dimensi tindakan tambahan yang diambil untuk keadaan tertentu.  Dipercayai bahwa tindakan negara <i>(s, a)</i> berpasangan dikunjungi selama bagian ini jika keadaan pernah dikunjungi dan tindakan <i>a</i> dilakukan di dalamnya.  Demikian pula, evaluasi tindakan nilai dapat dilakukan dengan menggunakan pendekatan "Kunjungan Pertama" dan "Setiap kunjungan". <br><br>  Seperti dalam pemrograman dinamis, kita dapat menggunakan kebijakan iterasi umum (GPI) untuk membentuk kebijakan dari mengamati nilai tindakan negara. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  Dengan bergantian langkah-langkah evaluasi kebijakan dan peningkatan kebijakan, dan termasuk penelitian untuk memastikan bahwa semua tindakan yang mungkin dikunjungi, kita dapat mencapai kebijakan optimal untuk setiap kondisi.  Untuk GPI Monte Carlo, rotasi ini biasanya dilakukan setelah akhir setiap pass. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>GPI Monte Carlo</i> <br><br><h3>  Strategi blackjack </h3><br>  Untuk lebih memahami bagaimana metode Monte Carlo bekerja dalam praktik dalam tugas menilai berbagai nilai negara, mari kita lakukan demonstrasi langkah demi langkah dari contoh permainan blackjack.  Untuk memulai, mari kita tentukan aturan dan ketentuan permainan kami: <br><br><ul><li>  Kami akan bermain hanya melawan dealer, tidak akan ada pemain lain.  Ini akan memungkinkan kami untuk mempertimbangkan tangan dealer sebagai bagian dari lingkungan. </li><li>  Nilai kartu dengan angka sama dengan nilai nominal.  Nilai kartu gambar: Jack, King and Queen adalah 10. Nilai kartu as dapat 1 atau 11 tergantung pada pilihan pemain. </li><li>  Kedua belah pihak menerima dua kartu.  Dua kartu pemain menghadap ke atas, salah satu kartu dealer juga menghadap ke atas. </li><li>  Tujuan permainan adalah bahwa jumlah kartu yang ada adalah &lt;= 21.  Nilai lebih besar dari 21 adalah angka, jika kedua belah pihak memiliki nilai 21, maka permainan dimainkan secara seri. </li><li>  Setelah pemain melihat kartunya dan kartu dealer pertama, pemain dapat memilih untuk membawanya kartu baru ("belum") atau tidak ("cukup") sampai ia puas dengan jumlah nilai kartu di tangannya. </li><li>  Kemudian dealer menunjukkan kartu keduanya - jika jumlah yang dihasilkan kurang dari 17, ia wajib mengambil kartu hingga mencapai 17 poin, setelah itu ia tidak mengambil kartu lagi. </li></ul><br>  Mari kita lihat bagaimana metode Monte Carlo bekerja dengan aturan-aturan ini. <br><br><h4>  Babak 1. </h4><br>  Anda mendapatkan total 19. Tetapi Anda mencoba untuk menangkap keberuntungan dengan ekor, ambil kesempatan, dapatkan 3 dan bangkrut.  Ketika Anda bangkrut, dealer hanya memiliki satu kartu terbuka dengan jumlah 10. Ini dapat direpresentasikan sebagai berikut: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Jika kami bangkrut, hadiah kami untuk putaran adalah -1.  Mari kita tetapkan nilai ini sebagai hasil pengembalian dari kondisi kedua dari belakang, menggunakan format berikut [Jumlah agen, jumlah dealer, kartu as?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Nah, sekarang kita kurang beruntung.  Mari kita beralih ke babak lain. <br><br><h4>  Babak 2 </h4><br>  Anda mengetik total 19. Kali ini Anda memutuskan untuk berhenti.  Dealer memanggil 13, mengambil kartu dan bangkrut.  Kondisi kedua dari belakang dapat dijelaskan sebagai berikut. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  Mari kita jelaskan kondisi dan penghargaan yang kami terima di babak ini: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Dengan akhir bagian ini, kita sekarang dapat memperbarui nilai semua negara bagian kita di babak ini menggunakan hasil yang dihitung.  Mengambil faktor diskon 1, kami hanya menyebarkan hadiah tangan baru kami, seperti yang dilakukan dengan transisi negara sebelumnya.  Karena negara <i>V (19, 10, tidak)</i> sebelumnya dikembalikan -1, kami menghitung nilai pengembalian yang diharapkan dan menugaskannya ke negara kami: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Nilai status final untuk demonstrasi menggunakan blackjack sebagai contoh</i> . <br><br><h3>  Implementasi </h3><br>  Mari kita menulis permainan blackjack menggunakan metode Monte Carlo kunjungan pertama untuk mengetahui semua nilai status yang mungkin (atau berbagai kombinasi di tangan) dalam permainan menggunakan Python.  Pendekatan kami akan didasarkan pada <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">pendekatan Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Seperti biasa, Anda dapat menemukan semua kode dari artikel di <a href="https://github.com/EXJUSTICE/GradientCrescent)">GitHub kami</a> . <br><br>  Untuk mempermudah implementasi, kami akan menggunakan gym dari OpenAI.  Pikirkan lingkungan sebagai antarmuka untuk memulai blackjack dengan jumlah kode minimal, ini akan memungkinkan kita untuk fokus pada penerapan pembelajaran yang diperkuat.  Mudahnya, semua informasi yang dikumpulkan tentang status, tindakan, dan hadiah disimpan dalam variabel <i>"observasi"</i> , yang diakumulasikan selama sesi permainan saat ini. <br><br>  Mari kita mulai dengan mengimpor semua perpustakaan yang kita butuhkan untuk mendapatkan dan mengumpulkan hasil kita. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Selanjutnya, mari kita inisialisasi lingkungan <i>gym</i> kita dan menetapkan kebijakan yang akan mengoordinasikan tindakan agen kami.  Bahkan, kami akan terus mengambil kartu sampai jumlah di tangan mencapai 19 atau lebih, setelah itu kami berhenti. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  Mari kita tentukan metode untuk menghasilkan data pass menggunakan kebijakan kami.  Kami akan menyimpan informasi tentang status, tindakan yang diambil, dan remunerasi untuk tindakan tersebut. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Akhirnya, mari kita mendefinisikan kunjungan pertama fungsi prediksi Monte Carlo.  Pertama, kami menginisialisasi kamus kosong untuk menyimpan nilai status saat ini dan kamus yang menyimpan jumlah catatan untuk setiap negara bagian dalam lintasan yang berbeda. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  Untuk setiap pass, kami memanggil metode <i>generate_episode</i> kami untuk mendapatkan informasi tentang nilai-nilai status dan hadiah yang diterima setelah keadaan terjadi.  Kami juga menginisialisasi variabel untuk menyimpan hasil tambahan kami.  Kemudian kami mendapatkan hadiah dan nilai status saat ini untuk setiap negara yang dikunjungi selama pass, dan meningkatkan pengembalian variabel kami dengan nilai hadiah per langkah. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) — 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns — value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Biarkan saya mengingatkan Anda bahwa karena kami menerapkan kunjungan pertama Monte Carlo, kami mengunjungi satu negara bagian dalam satu pass.  Oleh karena itu, kami melakukan pemeriksaan bersyarat pada kamus negara bagian untuk melihat apakah negara tersebut telah dikunjungi.  Jika kondisi ini terpenuhi, kami dapat menghitung nilai baru menggunakan prosedur yang didefinisikan sebelumnya untuk memperbarui nilai keadaan menggunakan metode Monte Carlo dan meningkatkan jumlah pengamatan untuk keadaan ini dengan 1. Kemudian kami mengulangi proses untuk lulus berikutnya untuk akhirnya mendapatkan nilai rata-rata dari hasil . <br><br>  Ayo jalankan apa yang kita dapatkan dan lihat hasilnya! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Kesimpulan sampel yang menunjukkan nilai keadaan berbagai kombinasi di tangan dalam blackjack.</i> <br><br>  Kami dapat terus melakukan pengamatan Monte Carlo untuk 5000 lintasan dan membangun distribusi nilai-nilai negara yang menggambarkan nilai-nilai kombinasi apa pun di tangan pemain dan dealer. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualisasi nilai keadaan berbagai kombinasi di blackjack.</i> <br><br>  Jadi, mari kita simpulkan apa yang kita pelajari. <br><br><ul><li>  Metode pembelajaran berbasis sampel memungkinkan kita untuk mengevaluasi nilai keadaan dan kondisi tindakan tanpa dinamika transisi, hanya dengan pengambilan sampel. </li><li>  Pendekatan Monte Carlo didasarkan pada pengambilan sampel acak model, mengamati imbalan yang dikembalikan oleh model, dan mengumpulkan informasi selama operasi normal untuk menentukan nilai rata-rata negara bagian. </li><li>  Menggunakan metode Monte Carlo, kebijakan iterasi umum mungkin. </li><li>  Nilai semua kemungkinan kombinasi di tangan pemain dan dealer di blackjack dapat diperkirakan menggunakan beberapa simulasi Monte Carlo, membuka jalan bagi strategi yang dioptimalkan. </li></ul><br>  Ini menyimpulkan pengantar metode Monte Carlo.  Dalam artikel kami berikutnya, kami akan beralih ke metode pengajaran bentuk pembelajaran Perbedaan Temporal. <br><br><h3>  Sumber: </h3><br>  Sutton et.  al, Pembelajaran Penguatan <br>  White et.  al, Fundamentals of Reinforcement Learning, University of Alberta <br>  Silva et.  al, Pembelajaran Penguatan, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Universitas Northeaster</a> <br><br>  Itu saja.  Sampai jumpa di <a href="https://otus.pw/Zkti/">lapangan</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id477042/">https://habr.com/ru/post/id477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id477022/index.html">Bagaimana kami dapat membantu Anda? Bagaimana Anda bisa membantu kami?</a></li>
<li><a href="../id477026/index.html">JetBrains Hackathon Tahunan Ketujuh</a></li>
<li><a href="../id477032/index.html">Dari blockchain ke DAG: menyingkirkan perantara</a></li>
<li><a href="../id477038/index.html">Bahasa pemrograman terbaik untuk pemula</a></li>
<li><a href="../id477040/index.html">Gartner chart 2019: tentang apa semua kata kunci ini?</a></li>
<li><a href="../id477044/index.html">Otomatisasi Pengujian Akhir-2-End dari sistem informasi terintegrasi. Bagian 2. Teknis</a></li>
<li><a href="../id477046/index.html">.Net Meetup di Raiffeisenbank 28/11 + Broadcast</a></li>
<li><a href="../id477048/index.html">Mengapa perusahaan dengan kapitalisasi $ 55 miliar berpikir untuk meninggalkan bursa</a></li>
<li><a href="../id477050/index.html">Black Friday 2019 untuk pengawasan video dan cloud.</a></li>
<li><a href="../id477052/index.html">Reaktor, WebFlux, Kotlin Coroutines, atau Asynchrony dengan Contoh Sederhana</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>