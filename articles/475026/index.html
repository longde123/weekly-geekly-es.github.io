<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👴🏼 👶🏾 🈹 3 historias de accidentes de Kubernetes en producción: anti-afinidad, cierre elegante, webhook 😮 🧞 🈴</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : Presentamos una mini-selección de autopsias sobre los problemas fatales que enfrentaron los ingenieros de diferentes compañías al operar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>3 historias de accidentes de Kubernetes en producción: anti-afinidad, cierre elegante, webhook</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/475026/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/y7/_c/ra/y7_cracjhm0ke_mtazml1fhzprk.jpeg"></div><br>  <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: Presentamos una mini-selección de autopsias sobre los problemas fatales que enfrentaron los ingenieros de diferentes compañías al operar la infraestructura basada en Kubernetes.</i>  <i>Cada nota habla sobre el problema en sí, sus causas y consecuencias y, por supuesto, sobre una solución que ayuda a evitar situaciones similares en el futuro.</i> <i><br><br></i>  <i>Como sabes, aprender de la experiencia de otra persona es más barato y, por lo tanto, deja que estas historias te ayuden a estar preparado para posibles sorpresas.</i>  <i>Por cierto, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este sitio</a> se publica una selección amplia y actualizada de enlaces a tales "historias de fallas" (según los datos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este repositorio de Git</a> ).</i> <a name="habracut"></a><br><br><h2>  No 1.  Cómo el pánico del núcleo bloqueó un sitio </h2><br>  <i>Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">luz de luna</a> .</i> <br><br>  Entre el 18 y el 22 de enero, el sitio web de Moonlight y API experimentaron fallas intermitentes.  Todo comenzó con errores aleatorios de la API y terminó con un apagado completo.  Los problemas se resolvieron y la aplicación volvió a la normalidad. <br><br><h3>  Información general </h3><br>  Moonlight utiliza un software conocido como Kubernetes.  Kubernetes ejecuta aplicaciones en grupos de servidores.  Estos servidores se llaman nodos.  Las copias de la aplicación que se ejecutan en el nodo se denominan pods.  Kubernetes tiene un planificador que determina dinámicamente qué pods en qué nodos deberían funcionar. <br><br><h3>  Cronograma </h3><br>  Los primeros errores del viernes estuvieron relacionados con problemas para conectarse a la base de datos Redis.  Moonlight API usa Redis para verificar las sesiones para cada solicitud autenticada.  Nuestra herramienta de monitoreo Kubernetes ha notificado que algunos nodos y pods no responden.  Al mismo tiempo, Google Cloud informó un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mal funcionamiento de los servicios de red</a> y decidimos que eran la causa de nuestros problemas. <br><br>  A medida que disminuía el tráfico el fin de semana, los errores parecían resolverse en su mayor parte.  Sin embargo, el martes por la mañana, el sitio de Moonlight cayó y el tráfico externo no llegó al clúster.  Encontramos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otra persona en Twitter</a> con síntomas similares y decidimos que el alojamiento de Google tenía una falla en la red.  Nos contactamos con el servicio de asistencia de Google Cloud, que rápidamente remitió el problema al equipo de asistencia técnica. <br><br>  El equipo de soporte técnico de Google reveló algunos patrones en el comportamiento de los nodos en nuestro clúster de Kubernetes.  La utilización de CPU de los nodos individuales alcanzó el 100%, después de lo cual se produjo el pánico del kernel en la máquina virtual y se bloqueó. <br><br><h3>  Razones </h3><br>  El ciclo que causó la falla fue el siguiente: <br><br><ul><li>  El planificador de Kubernetes alojó varios pods con alto consumo de CPU en el mismo nodo. </li><li>  Los pods se comieron todos los recursos de la CPU en el nodo. </li><li>  Luego vino el pánico del kernel, que condujo a un período de inactividad durante el cual el nodo no respondió al planificador. </li><li>  El programador movió todas las cápsulas caídas a un nuevo nodo, y el proceso se repitió, exacerbando la situación general. </li></ul><br>  Inicialmente, el error ocurrió en el pod de Redis, pero al final todos los pod que trabajan con tráfico cayeron, lo que condujo a un apagado completo.  Los retrasos exponenciales durante la reprogramación han llevado a períodos más largos de inactividad. <br><br><h3>  Solución </h3><br>  Pudimos restaurar el sitio agregando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">reglas anti-afinidad</a> a todas las implementaciones principales.  Distribuyen automáticamente los pods sobre los nodos, aumentando la tolerancia a fallas y el rendimiento. <br><br>  Kubernetes está diseñado como un sistema host tolerante a fallas.  Moonlight utiliza tres nodos en diferentes servidores para mayor estabilidad, y ejecutamos tres copias de cada aplicación que sirve el tráfico.  La idea es tener una copia en cada nodo.  En este caso, incluso una falla de dos nodos no conducirá al tiempo de inactividad.  Sin embargo, Kubernetes a veces colocó las tres cápsulas con el sitio en el mismo nodo, creando así un cuello de botella en el sistema.  Al mismo tiempo, otras aplicaciones que demandaban potencia de procesador (es decir, representación del lado del servidor) estaban en el mismo nodo, y no en uno separado. <br><br>  Se requiere un clúster de Kubernetes correctamente configurado y que funcione correctamente para hacer frente a largos períodos de alta carga de CPU y colocar pods de manera que se maximice el uso de los recursos disponibles.  Continuamos trabajando con el soporte de Google Cloud para identificar y abordar la causa raíz del kernel panic en los servidores. <br><br><h3>  Conclusión </h3><br>  Las reglas anti-afinidad le permiten hacer que las aplicaciones que funcionan con tráfico externo sean más tolerantes a fallas.  Si tiene un servicio similar en Kubernetes, considere agregarlos. <br><br>  Continuamos trabajando con los chicos de Google para encontrar y eliminar la causa de fallas en el núcleo del sistema operativo en los nodos. <br><br><h2>  No 2.  El secreto "sucio" de Kubernetes y el punto final de Ingress </h2><br>  <i>Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Phil Pearl de Ravelin</a> .</i> <br><br><h3>  La elegancia está sobrevalorada </h3><br>  Nosotros en Ravelin migramos a Kubernetes (en GKE).  El proceso ha sido muy exitoso.  Nuestros presupuestos de interrupción de pods están tan completos como siempre, los estados son verdaderamente majestuosos <i>(un juego de palabras difícil de traducir: "nuestros conjuntos con estado son muy majestuosos" - aprox. Transl.)</i> , Y el reemplazo deslizante de los nodos funciona como un reloj. <br><br>  La pieza final del rompecabezas es mover la capa API de máquinas virtuales antiguas al clúster de Kubernetes.  Para hacer esto, necesitamos configurar Ingress para que la API sea accesible desde el mundo exterior. <br><br>  Al principio, la tarea parecía simple.  Simplemente definimos el controlador Ingress, modificamos el Terraform para obtener un cierto número de direcciones IP, y Google se encarga de casi todo lo demás.  Y todo esto funcionará como por arte de magia.  Clase! <br><br>  Sin embargo, con el tiempo, comenzaron a notar que las pruebas de integración reciben periódicamente errores 502. A partir de esto, nuestro viaje comenzó.  Sin embargo, le ahorraré tiempo e iré directamente a las conclusiones. <br><br><h3>  Cierre elegante </h3><br>  Todo el mundo habla de un cierre elegante ("elegante", cierre gradual).  Pero realmente no debes confiar en él en Kubernetes.  O al menos no debería ser el cierre elegante que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://golang.org/pkg/net/">absorbió con la leche de su madre</a> .  En el mundo de Kubernetes, este nivel de "elegancia" es innecesario y amenaza con serios problemas. <br><br><h3>  Mundo perfecto </h3><br>  Así es como, en la vista mayoritaria, el pod se elimina del servicio o del equilibrador de carga en Kubernetes: <br><br><ol><li>  El controlador de replicación decide eliminar el pod. </li><li>  El pod de punto final se elimina del servicio o equilibrador de carga.  Ya no llega nuevo tráfico al pod. </li><li>  Se llama a un gancho previo a la parada, o el pod recibe una señal SIGTERM. </li><li>  El pod "con gracia" está desconectado.  Deja de aceptar conexiones entrantes. </li><li>  La desconexión "agraciada" se completa y el pod se destruye después de que todas sus conexiones existentes se detienen o finalizan. </li></ol><br>  Desafortunadamente, la realidad es completamente diferente. <br><br><h3>  Mundo real </h3><br>  La mayor parte de la documentación sugiere que todo sucede de manera un poco diferente, pero no escriben explícitamente sobre esto en ningún lado.  El principal problema es que el paso 3 no sigue al paso 2. Se producen simultáneamente.  En los servicios ordinarios, la eliminación de puntos finales es tan rápida que la probabilidad de encontrar problemas es extremadamente baja.  Sin embargo, con Ingresss, todo es diferente: generalmente responden mucho más lentamente, por lo que el problema se vuelve obvio.  Pod puede obtener SIGTERM mucho antes de que los cambios en los puntos finales entren en Ingress. <br><br>  Como resultado, un apagado correcto no es lo que se requiere de un pod.  Recibirá nuevas conexiones y debe continuar procesándolas, de lo contrario, los clientes comenzarán a recibir los errores número 500 y toda la maravillosa historia sobre implementaciones y escalas sin complicaciones comenzará a desmoronarse. <br><br>  Esto es lo que realmente sucede: <br><br><ol><li>  El controlador de replicación decide eliminar el pod. </li><li>  El pod de punto final se elimina del servicio o equilibrador de carga.  En el caso de Ingresss, esto puede llevar algún tiempo, y el nuevo tráfico continuará fluyendo hacia el pod. </li><li>  Se llama a un gancho previo a la parada, o el pod recibe una señal SIGTERM. </li><li>  En gran medida, el pod debe ignorar esto, continuar trabajando y mantener nuevas conexiones.  Si es posible, debería indicarles a los clientes que sería bueno cambiarse a otro lugar.  Por ejemplo, en el caso de HTTP, puede enviar <code>Connection: close</code> en los encabezados de respuesta. </li><li>  La cápsula sale solo cuando el período de espera "elegante" expira y SIGKILL la mata. </li><li>  Asegúrese de que este período sea más largo que el tiempo que lleva reprogramar el equilibrador de carga. </li></ol><br>  Si se trata de un código de terceros y no puede cambiar su comportamiento, entonces lo mejor que puede hacer es agregar un gancho previo a la detención que solo duerma durante un período "elegante", de modo que el pod continúe funcionando como si nada sucedió <br><br><h2>  Número 3.  Cómo un webhook simple causó una falla del clúster </h2><br>  <i>Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Jetstack</a> .</i> <br><br>  Jetstack ofrece a sus clientes plataformas multiempresa en Kubernetes.  A veces hay requisitos especiales que no podemos satisfacer con la configuración estándar de Kubernetes.  Para implementarlos, recientemente comenzamos a usar el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Open Policy Agent</a> <i>(escribimos sobre el proyecto con más detalle en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta revisión</a> , aprox. Transl.)</i> Como controlador de acceso para implementar políticas especiales. <br><br>  Este artículo describe el error causado por esta integración mal configurada. <br><br><h3>  Incidente </h3><br>  Nos dedicamos a actualizar el asistente para el clúster de desarrollo, en el que varios equipos probaron sus aplicaciones durante la jornada laboral.  Era un clúster regional en la zona europa-oeste1 en Google Kubernetes Engine (GKE). <br><br>  Se advirtió a los comandos que se estaba realizando una actualización, sin tiempo de inactividad esperado.  Ese mismo día, ya hicimos una actualización similar a otro entorno de preproducción. <br><br>  Comenzamos la actualización usando nuestra tubería GKE Terraform.  La actualización del asistente no se completó hasta que expiró el tiempo de espera de Terraform, que configuramos durante 20 minutos.  Esta fue la primera llamada de atención de que algo salió mal, aunque en la consola GKE el clúster todavía figuraba como "actualización". <br><br>  Reiniciar la tubería condujo al siguiente error <br><br><pre> <code class="bash hljs">google_container_cluster.cluster: Error waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> updating GKE master version: All cluster resources were brought up, but the cluster API is reporting that: component <span class="hljs-string"><span class="hljs-string">"kube-apiserver"</span></span> from endpoint <span class="hljs-string"><span class="hljs-string">"gke-..."</span></span> is unhealthy</code> </pre> <br>  Esta vez, la conexión con el servidor API comenzó a interrumpirse periódicamente y los equipos no pudieron implementar sus aplicaciones. <br><br>  Mientras estábamos tratando de entender lo que estaba sucediendo, todos los nodos comenzaron a destruirse y recrearse en un ciclo sin fin.  Esto ha llevado a una denegación indiscriminada de servicio para todos nuestros clientes. <br><br><h3>  Establecemos la causa raíz de la falla </h3><br>  Con el soporte de Google, pudimos determinar la secuencia de eventos que condujeron a la falla: <br><br><ol><li>  GKE completó la actualización en una instancia del asistente y comenzó a aceptar todo el tráfico al servidor API en él a medida que se actualizaban los siguientes asistentes. </li><li>  Durante la actualización de la segunda instancia del asistente, el servidor API no pudo ejecutar <a href="">PostStartHook</a> para <a href="">registrar la CA.</a> </li><li>  Durante la ejecución de este enlace, el servidor API intentó actualizar ConfigMap llamado <code>extension-apiserver-authentication</code> en <code>kube-system</code> .  No fue posible hacer esto porque el backend para el webhook de comprobación de Open Policy Agent (OPA) que configuramos no respondió. </li><li>  Para que el asistente pase una comprobación de estado, esta operación debe completarse correctamente.  Como esto no sucedió, el segundo maestro ingresó al ciclo de emergencia y detuvo la actualización. </li></ol><br>  El resultado fueron bloqueos periódicos de la API, debido a que los kubelets no pudieron informar el estado del nodo.  A su vez, esto condujo al hecho de que el mecanismo para la restauración automática de los nodos GKE <i>(reparación</i> automática de nodos <i>)</i> comenzó a reiniciar los nodos.  Esta característica se describe en detalle en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentación</a> : <br><br><blockquote>  <i>Un estado no saludable puede significar: Dentro de un tiempo determinado (aproximadamente 10 minutos), el nodo no da ningún estado en absoluto.</i> </blockquote><br><h3>  Solución </h3><br>  Cuando descubrimos que el recurso <code>ValidatingAdmissionWebhook</code> estaba causando un acceso intermitente al servidor API, lo eliminamos y restauramos el clúster para que funcione. <br><br>  Desde entonces, han configurado <code>ValidatingAdmissionWebhook</code> para OPA para monitorear solo aquellos espacios de nombres donde la política es aplicable y a los que los equipos de desarrollo tienen acceso.  También limitamos el webhook a <code>Ingress</code> and <code>Service</code> , los únicos con los que funciona nuestra política. <br><br>  Desde que implementamos la OPA por primera vez, la documentación se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ha actualizado</a> para reflejar este cambio. <br><br>  También agregamos una prueba de vida para asegurar que la OPA se reinicie en caso de que no esté disponible (e hicimos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las modificaciones apropiadas</a> a la documentación). <br><br>  También consideramos deshabilitar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el mecanismo de</a> recuperación automática para los nodos GKE, pero aún así decidimos abandonar esta idea. <br><br><h3>  Resumen </h3><br>  Si habilitamos las alertas de tiempo de respuesta del servidor API, inicialmente podríamos notar su aumento global para todas las solicitudes <code>CREATE</code> y <code>UPDATE</code> después de implementar el webhook para OPA. <br><br>  Esto subraya la importancia de configurar pruebas para todas las cargas de trabajo.  Mirando hacia atrás, podemos decir que el despliegue de OPA fue tan engañosamente simple que ni siquiera nos involucramos en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tabla de Helm</a> (aunque debería).  El cuadro realiza una serie de ajustes más allá de la configuración básica descrita en el manual, incluida la configuración de livenessProbe para contenedores con un controlador de admisión. <br><br>  No fuimos los primeros en encontrar este problema: el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">problema aguas arriba</a> permanece abierto.  La funcionalidad en este asunto puede mejorarse claramente (y haremos un seguimiento de esto). <br><br><h2>  PD del traductor </h2><br>  Lea también en nuestro blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cómo las prioridades de los pods en Kubernetes causaron tiempo de inactividad en Grafana Labs</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">De la vida con Kubernetes: cómo los españoles no se quejaron del servidor HTTP</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">6 errores de sistema entretenidos en el funcionamiento de Kubernetes [y su solución]</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">6 historias prácticas de nuestra vida cotidiana SRE</a> ". </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/475026/">https://habr.com/ru/post/475026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../475016/index.html">QA mitap en Redmadrobot 22 de noviembre</a></li>
<li><a href="../475018/index.html">Alteración de columnas Radiotehnika S-30</a></li>
<li><a href="../475020/index.html">Cómo la tecnología moderna está reemplazando gradualmente a las torres de incendios</a></li>
<li><a href="../475022/index.html">Esquizofrenia arquitectónica Facebook Libra</a></li>
<li><a href="../475024/index.html">Correr es un deporte ideal para un trabajador remoto. Parte 1: el camino hacia la primera carrera de cien kilómetros</a></li>
<li><a href="../475032/index.html">Gartner Hype Cycle 2019: informe</a></li>
<li><a href="../475034/index.html">Gráfico en el navegador para Arduino y STM32</a></li>
<li><a href="../475036/index.html">Migración de Cassandra a Kubernetes: características y soluciones</a></li>
<li><a href="../475038/index.html">El primer conjunto de "Matemática Aplicada y Ciencias de la Computación" en el HSE de San Petersburgo: ¿quiénes son y cómo trabajar con ellos?</a></li>
<li><a href="../475044/index.html">Construyendo su propio servidor sin servidor basado en Fn</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>