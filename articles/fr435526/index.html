<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤼 😒 👩🏽‍🎤 Aventures avec un cluster Home Kubernetes 🐿️ 👩🏿‍🌾 🧔🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : L'auteur de l'article, Marshall Brekka, occupe le poste de directeur de la conception des systèmes chez Fair.com, qui propose son ap...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aventures avec un cluster Home Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/435526/">  <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: L'auteur de l'article, Marshall Brekka, occupe le poste de directeur de la conception des systèmes chez Fair.com, qui propose son application pour la location de voitures.</i>  <i>Pendant son temps libre, il aime mettre à profit sa vaste expérience pour résoudre des problèmes "domestiques" qui ne surprendront probablement pas les geek (par conséquent, la question "Pourquoi?" - en ce qui concerne les actions décrites ci-dessous - est a priori omise).</i>  <i>Ainsi, dans sa publication, Marshall partage les résultats du récent déploiement de Kubernetes sur ... les cartes ARM.</i> <br><br><img src="https://habrastorage.org/webt/ul/nj/do/ulnjdoyysctwv-34jhuyn-wvsp8.png"><br><br>  Comme beaucoup d'autres geeks, au fil des ans, j'ai accumulé une variété de cartes de développement comme le Raspberry Pi.  Et comme beaucoup de geeks, ils se sont époussetés sur les étagères avec la pensée qu'ils pourraient un jour être utiles.  Et maintenant pour moi, ce jour est enfin arrivé! <a name="habracut"></a><br><br>  Pendant les vacances d'hiver, plusieurs semaines en dehors du travail sont apparues, dans lesquelles il y avait suffisamment de temps pour inventorier tout le fer accumulé et décider quoi en faire.  Voici ce que j'avais: <br><br><ul><li>  Boîtier RAID à 5 disques avec connexion USB3; </li><li>  Raspberry Pi modèle B (modèle OG); </li><li>  CubbieBoard 1; </li><li>  Banana Pi M1; </li><li>  Netbook HP (2012?). </li></ul><br>  Sur les 5 composants de fer répertoriés, j'ai utilisé sauf RAID et un netbook comme NAS temporaire.  Cependant, en raison du manque de prise en charge USB3 dans le netbook, RAID n'a pas utilisé le potentiel pleine vitesse. <br><br><h2>  Objectifs de vie </h2><br>  Étant donné que travailler avec RAID n'était pas optimal lors de l'utilisation d'un netbook, j'ai défini les objectifs suivants pour obtenir la meilleure configuration: <br><br><ol><li>  NAS avec USB3 et Gigabit Ethernet; </li><li>  La meilleure façon de gérer les logiciels sur votre appareil </li><li>  (bonus) la possibilité de diffuser du contenu multimédia de RAID vers Fire TV. </li></ol><br>  Étant donné qu'aucun des appareils disponibles ne prend en charge USB3 et Ethernet gigabit, j'ai malheureusement dû faire des achats supplémentaires.  Le choix s'est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">porté</a> sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ROC-RK3328-CC</a> .  Elle possédait toutes les spécifications nécessaires et un support suffisant pour les systèmes d'exploitation. <br><br>  Ayant résolu mes besoins en matériel (et en attendant l'arrivée de cette solution), je suis passé au deuxième objectif. <br><br><h2>  Gestion des logiciels sur l'appareil </h2><br>  En partie, mes projets antérieurs liés aux cartes de développement ont échoué en raison d'une attention insuffisante aux problèmes de reproductibilité et de documentation.  Lors de la création de la prochaine configuration pour mes besoins actuels, je n'ai pas pris la peine de noter les étapes suivies ou les liens vers les articles de blog que j'ai suivis.  Et quand, après des mois ou des années, quelque chose s'est mal passé et que j'ai essayé de résoudre le problème, je ne comprenais pas comment tout était arrangé à l'origine. <br><br>  Alors je me suis dit que cette fois tout sera différent! <br><br><img src="https://habrastorage.org/webt/dm/vb/iv/dmvbivkoa65wfd1ve5mo5wh5jdc.jpeg"><br><br>  Et il s'est tourné vers le fait que je sais assez bien - à Kubernetes. <br><br>  Bien que K8s soit une solution trop difficile à un problème assez simple, après presque trois ans de gestion de clusters à l'aide de divers outils (les miens, kops, etc.) dans mon travail principal, je connais très bien ce système.  De plus, déployer des K8 en dehors d'un environnement cloud, et même sur des appareils ARM - tout cela semblait une tâche intéressante. <br><br>  J'ai également pensé que, puisque le matériel disponible ne répond pas aux exigences nécessaires pour le NAS, j'essaierai au moins d'en assembler un cluster et, peut-être, certains logiciels moins gourmands en ressources pourront fonctionner sur des appareils plus anciens. <br><br><h2>  Kubernetes sur ARM </h2><br>  Au travail, je n'ai pas eu l'occasion d'utiliser l'utilitaire <code>kubeadm</code> pour déployer des clusters, j'ai donc décidé que le moment était venu de l'essayer en action. <br><br>  Raspbian a été choisi comme système d'exploitation, car il est célèbre pour le meilleur support pour mes cartes. <br><br>  J'ai trouvé un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bon article</a> sur la configuration de Kubernetes sur un Raspberry Pi en utilisant HypriotOS.  Comme je n'étais pas sûr de la disponibilité d'HypriotOS pour toutes mes cartes, j'ai adapté ces instructions pour Debian / Raspbian. <br><br><h3>  Composants requis </h3><br>  Tout d'abord, l'installation des outils suivants était requise: <br><br><ul><li>  Docker, </li><li>  kubelet </li><li>  kubeadm, </li><li>  kubectl. </li></ul><br>  Docker doit être installé à l'aide d'un script spécial - script de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">commodité</a> (comme indiqué dans le cas de l'utilisation de Raspbian). <br><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br>  Après cela, j'ai installé les composants Kubernetes selon les instructions du blog Hypriot, en les adaptant pour que des versions spécifiques soient utilisées pour toutes les dépendances: <br><br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h3>  Raspberry pi b </h3><br>  La première difficulté est survenue lors de la tentative d'amorçage d'un cluster sur le Raspberry Pi B: <br><br><pre> <code class="bash hljs">$ kubeadm init Illegal instruction</code> </pre> <br>  Il s'est avéré que Kubernetes a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">supprimé la prise en charge d'ARMv6</a> .  Eh bien, j'ai aussi CubbieBoard et Banana Pi. <br><br><h3>  Banana pi </h3><br>  Initialement, la même séquence d'actions pour Banana Pi semblait être plus réussie, cependant, la commande <code>kubeadm init</code> essayant d'attendre que le plan de contrôle fonctionne: <br><br><pre> <code class="plaintext hljs">error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster</code> </pre> <br>  En découvrant avec <code>docker ps</code> ce qui se passait avec les conteneurs, j'ai vu que <code>kube-controller-manager</code> et <code>kube-scheduler</code> fonctionnaient depuis au moins 4-5 minutes, mais <code>kube-api-server</code> s'est levé il y a seulement 1-2 minutes: <br><br><pre> <code class="bash hljs">$ docker ps CONTAINER ID COMMAND CREATED STATUS de22427ad594 <span class="hljs-string"><span class="hljs-string">"kube-apiserver --au…"</span></span> About a minute ago Up About a minute dc2b70dd803e <span class="hljs-string"><span class="hljs-string">"kube-scheduler --ad…"</span></span> 5 minutes ago Up 5 minutes 60b6cc418a66 <span class="hljs-string"><span class="hljs-string">"kube-controller-man…"</span></span> 5 minutes ago Up 5 minutes 1e1362a9787c <span class="hljs-string"><span class="hljs-string">"etcd --advertise-cl…"</span></span> 5 minutes ago Up 5 minutes</code> </pre> <br>  De toute évidence, le <code>api-server</code> était en train de mourir ou le processus de strontium le tuait et le redémarrait. <br><br>  En vérifiant les journaux, j'ai vu des procédures de démarrage très standard - il y avait un enregistrement du début d'écoute du port sécurisé et une longue pause avant l'apparition de nombreuses erreurs dans les poignées de main TLS: <br><br><pre> <code class="plaintext hljs">20:06:48.604881 naming_controller.go:284] Starting NamingConditionController 20:06:48.605031 establishing_controller.go:73] Starting EstablishingController 20:06:50.791098 log.go:172] http: TLS handshake error from 192.168.1.155:50280: EOF 20:06:51.797710 log.go:172] http: TLS handshake error from 192.168.1.155:50286: EOF 20:06:51.971690 log.go:172] http: TLS handshake error from 192.168.1.155:50288: EOF 20:06:51.990556 log.go:172] http: TLS handshake error from 192.168.1.155:50284: EOF 20:06:52.374947 log.go:172] http: TLS handshake error from 192.168.1.155:50486: EOF 20:06:52.612617 log.go:172] http: TLS handshake error from 192.168.1.155:50298: EOF 20:06:52.748668 log.go:172] http: TLS handshake error from 192.168.1.155:50290: EOF</code> </pre> <br>  Et peu de temps après, le serveur termine son travail.  La recherche sur Google a conduit à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un tel problème</a> , indiquant une raison possible du lent fonctionnement des algorithmes cryptographiques sur certains appareils ARM. <br><br>  Je suis allé plus loin et j'ai pensé que peut-être le <code>api-server</code> recevait trop de demandes répétées du <code>scheduler</code> et du <code>controller-manager</code> . <br><br>  La suppression de ces fichiers du répertoire manifeste indiquera à kubelet d'arrêter l'exécution des pods correspondants: <br><br><pre> <code class="bash hljs">mkdir /etc/kubernetes/manifests.bak mv /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/manifests.bak/ mv /etc/kubernetes/manifests/kube-controller-mananger.yaml /etc/kubernetes/manifests.bak/</code> </pre> <br>  L'affichage des derniers journaux du <code>api-server</code> montré que maintenant le processus allait plus loin, cependant, il est toujours mort après environ 2 minutes.  Ensuite, je me suis souvenu que le manifeste pouvait contenir un échantillon de vivacité avec des délais d'attente ayant des valeurs trop faibles pour un appareil aussi lent. <br><br>  Par conséquent, j'ai vérifié <code>/etc/kubernetes/manifests/kube-api-server.yaml</code> - et dedans, bien sûr ... <br><br><pre> <code class="plaintext hljs">livenessProbe: failureThreshold: 8 httpGet: host: 192.168.1.155 path: /healthz port: 6443 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15</code> </pre> <br>  Pod a été tué après 135 secondes ( <code>initialDelaySeconds</code> + <code>timeoutSeconds</code> * <code>failureThreshold</code> ).  Augmentez <code>initialDelaySeconds</code> à 120 ... <br><br>  <b>Succès!</b>  Eh bien, des erreurs de poignée de main se produisent toujours (probablement à partir de kubelet), mais le lancement a toujours eu lieu: <br><br><pre> <code class="plaintext hljs">20:06:54.957236 log.go:172] http: TLS handshake error from 192.168.1.155:50538: EOF 20:06:55.004865 log.go:172] http: TLS handshake error from 192.168.1.155:50384: EOF 20:06:55.118343 log.go:172] http: TLS handshake error from 192.168.1.155:50292: EOF 20:06:55.252586 cache.go:39] Caches are synced for autoregister controller 20:06:55.253907 cache.go:39] Caches are synced for APIServiceRegistrationController controller 20:06:55.545881 controller_utils.go:1034] Caches are synced for crd-autoregister controller ... 20:06:58.921689 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/cluster-admin 20:06:59.049373 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:discovery 20:06:59.214321 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:basic-user</code> </pre> <br>  Lorsque le <code>api-server</code> s'est levé, j'ai replacé les fichiers YAML pour le contrôleur et le planificateur dans le répertoire manifeste, après quoi ils ont également démarré normalement. <br><br>  Il est maintenant temps de vous assurer que le téléchargement réussira si vous laissez tous les fichiers dans le répertoire d'origine: est-ce suffisant pour modifier le délai d'initialisation de <code>livenessProbe</code> ? <br><br><pre> <code class="plaintext hljs">20:29:33.306983 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.Service: Get https://192.168.1.155:6443/api/v1/services?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.434541 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.ReplicationController: Get https://192.168.1.155:6443/api/v1/replicationcontrollers?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.435799 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolume: Get https://192.168.1.155:6443/api/v1/persistentvolumes?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.477405 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1beta1.PodDisruptionBudget: Get https://192.168.1.155:6443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.493660 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolumeClaim: Get https://192.168.1.155:6443/api/v1/persistentvolumeclaims?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:37.974938 controller_utils.go:1027] Waiting for caches to sync for scheduler controller 20:29:38.078558 controller_utils.go:1034] Caches are synced for scheduler controller 20:29:38.078867 leaderelection.go:205] attempting to acquire leader lease kube-system/kube-scheduler 20:29:38.291875 leaderelection.go:214] successfully acquired lease kube-system/kube-scheduler</code> </pre> <br>  Oui, tout fonctionne, bien que ces vieux appareils, apparemment, n'étaient pas destinés à lancer l'avion de contrôle, car les connexions TLS répétées provoquent des freins importants.  D'une manière ou d'une autre - une installation fonctionnelle de K8 sur ARM est reçue!  Allons plus loin ... <br><br><h3>  Montage RAID </h3><br>  Étant donné que les cartes SD ne sont pas adaptées à l'enregistrement à long terme, j'ai décidé d'utiliser un stockage plus fiable pour les parties les plus volatiles du système de fichiers - dans ce cas, RAID.  4 sections y ont été mises en évidence: <br><br><ul><li>  50 Go; </li><li>  2 × 20 Go; </li><li>  3,9 Tb. </li></ul><br>  Je n'ai pas encore trouvé un objectif spécifique pour les partitions de 20 gigaoctets, mais je voulais laisser des opportunités supplémentaires pour l'avenir. <br><br>  Dans le <code>/etc/fstab</code> pour la partition de 50 Go, le point de montage a été spécifié comme <code>/mnt/root</code> , et pour 3,9 To - <code>/mnt/raid</code> .  Après cela, j'ai monté les répertoires avec etcd et docker sur la partition de 50 Go: <br><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h3>  Arrivée ROC-RK3328-CC </h3><br>  Lorsque la nouvelle carte a été livrée, j'ai installé les composants nécessaires pour les K8 dessus <i>(voir le début de l'article)</i> et lancé <code>kubeadm init</code> .  Quelques minutes d'attente sont le succès et la sortie de la commande <code>join</code> à exécuter sur d'autres nœuds. <br><br>  Super!  Pas de chichi avec les délais. <br><br>  Et comme le RAID sera également utilisé sur cette carte, les supports devront être à nouveau configurés.  Pour résumer toutes les étapes: <br><br><h4>  1. Montez les disques dans / etc / fstab </h4><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h4>  2. Installation des binaires Docker et K8s </h4><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h4>  3. Configuration d'un nom d'hôte unique (important car de nombreux nœuds sont ajoutés) </h4><br><pre> <code class="bash hljs">hostnamectl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-hostname k8s-master-1</code> </pre> <br><h4>  4. Initialisation de Kubernetes </h4><br>  J'omets la phase avec le plan de contrôle, car je veux pouvoir planifier des pods normaux sur ce noeud: <br><br><pre> <code class="bash hljs">kubeadm init --skip-phases mark-control-plane</code> </pre> <br><h4>  5. Installation du plugin réseau </h4><br>  Les informations à ce sujet dans l'article Hypriot étaient un peu datées car le plugin réseau Weave est désormais également <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pris en charge sur ARM</a> : <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f <span class="hljs-string"><span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span></span></span><span class="hljs-string">"</span></span></code> </pre> <br><h4>  6. Ajout d'étiquettes d'hôte </h4><br>  Sur ce nœud, je vais démarrer le serveur NAS, je vais donc le marquer avec des étiquettes pour une éventuelle utilisation future dans le planificateur: <br><br><pre> <code class="bash hljs">kubectl label nodes k8s-master-1 marshallbrekka.raid=<span class="hljs-literal"><span class="hljs-literal">true</span></span> kubectl label nodes k8s-master-1 marshallbrekka.network=gigabit</code> </pre> <br><h3>  Connexion d'autres nœuds au cluster </h3><br>  La configuration d'autres appareils (Banana Pi, CubbieBoard) était tout aussi simple.  Pour eux, vous devez répéter les 3 premières étapes (changer les paramètres de montage des disques / supports flash, selon leur disponibilité) et exécuter la commande <code>kubeadm join</code> au lieu de <code>kubeadm init</code> . <br><br><h2>  Recherche de conteneurs Docker pour ARM </h2><br>  La plupart des conteneurs Docker nécessaires sont construits normalement sur un Mac, mais pour ARM, c'est un peu plus compliqué.  Ayant trouvé de nombreux articles sur la façon d'utiliser QEMU à ces fins, je suis néanmoins parvenu à la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conclusion</a> que la plupart des applications dont j'avais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">besoin</a> sont déjà assemblées, et beaucoup d'entre elles sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">linuxserver</a> . <br><br><h2>  Prochaines étapes </h2><br>  N'obtenant toujours pas la configuration initiale des appareils sous une forme automatisée / scriptée comme je le souhaiterais, j'ai au moins composé un ensemble de commandes de base (montages, <code>docker</code> et <code>kubeadm</code> ) et les <code>kubeadm</code> documentées dans le référentiel Git.  Les autres applications utilisées ont également reçu des configurations YAML pour K8 stockées dans le même référentiel, il est donc très facile d'obtenir la configuration nécessaire à partir de zéro. <br><br>  À l'avenir, j'aimerais atteindre les objectifs suivants: <br><br><ol><li>  Rendre les sites maîtres hautement disponibles </li><li>  ajouter une surveillance / des notifications pour connaître les défaillances de tous les composants; </li><li>  Modifier les paramètres DCHP du routeur pour utiliser un serveur DNS du cluster afin de simplifier la découverte des applications (qui veut se souvenir des adresses IP internes?); </li><li>  exécutez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MetalLB</a> pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transférer</a> les services de cluster vers un réseau privé (DNS, etc.). </li></ol><br><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Trucs et astuces Kubernetes: sur l'allocation des nœuds et la charge sur l'application web</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Trucs et astuces Kubernetes: accès aux sites de développement</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Trucs et astuces de Kubernetes: accélérer le bootstrap des grandes bases de données</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">11 façons de (ne pas) devenir une victime du piratage Kubernetes</a> »; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jouer avec Kubernetes est un service pour apprendre à connaître les K8 dans la pratique</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr435526/">https://habr.com/ru/post/fr435526/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr435510/index.html">Microélectronique, neurophysiologie et apprentissage automatique, secouer mais pas mélanger</a></li>
<li><a href="../fr435512/index.html">Les développeurs de Royole présentent un smartphone flexible pliable</a></li>
<li><a href="../fr435514/index.html">En Russie, ils développent un processeur pour accélérer les réseaux de neurones</a></li>
<li><a href="../fr435520/index.html">Nous écrivons notre langage de programmation, partie 3: Architecture du traducteur. Analyse des structures du langage et des expressions mathématiques</a></li>
<li><a href="../fr435522/index.html">Instantanés d'événements dans Axonframework 3, améliorant les performances</a></li>
<li><a href="../fr435528/index.html">5 raisons de réussir: pourquoi Amazon est devenu l'entreprise la plus chère au monde</a></li>
<li><a href="../fr435530/index.html">Abonnements payants - Dépendance de la connexion automatique à un appareil mobile</a></li>
<li><a href="../fr435532/index.html">Tornado vs Aiohttp: un voyage dans le désert des frameworks asynchrones</a></li>
<li><a href="../fr435534/index.html">Science des données: livres d'entrée de gamme</a></li>
<li><a href="../fr435536/index.html">Robots humanoïdes: avantages et problèmes des mécanismes anthropomorphes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>