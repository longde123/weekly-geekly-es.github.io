<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ˜ ğŸ‘¨ğŸ¼â€ğŸ­ ğŸŒ… VertrauenswÃ¼rdiger Speicher mit DRBD9 und Proxmox (Teil 1: NFS) ğŸ§™ğŸ¾ ğŸš† ğŸ‘©ğŸ¾â€ğŸ’¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wahrscheinlich hat jeder, der mindestens einmal von der Suche nach hochleistungsfÃ¤higem, softwaredefiniertem Speicher verwirrt war, frÃ¼her oder spÃ¤ter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VertrauenswÃ¼rdiger Speicher mit DRBD9 und Proxmox (Teil 1: NFS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417473/"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="Bild"></p><br><p>  Wahrscheinlich hat jeder, der mindestens einmal von der Suche nach hochleistungsfÃ¤higem, <strong>softwaredefiniertem Speicher verwirrt war,</strong> frÃ¼her oder spÃ¤ter von <strong>DRBD</strong> gehÃ¶rt oder sich vielleicht sogar damit befasst. </p><br><p> Auf dem HÃ¶hepunkt der PopularitÃ¤t von <strong>Ceph</strong> und <strong>GlusterFS</strong> , die im Prinzip ziemlich gut funktionieren und vor allem <strong>sofort</strong> <strong>einsatzbereit</strong> sind, haben alle nur ein wenig vergessen.  DarÃ¼ber hinaus unterstÃ¼tzte die vorherige Version die Replikation auf mehr als zwei Knoten nicht, und aus diesem Grund traten hÃ¤ufig Probleme mit dem <strong>geteilten Gehirn</strong> auf, was eindeutig nicht zu seiner PopularitÃ¤t beitrug. </p><br><p>  Die LÃ¶sung ist wirklich nicht neu, aber ziemlich wettbewerbsfÃ¤hig.  Mit relativ geringen Kosten fÃ¼r CPU und RAM bietet <strong>DRBD</strong> eine sehr schnelle und sichere Synchronisation auf <strong>BlockgerÃ¤teebene</strong> .  WÃ¤hrend dieser ganzen Zeit stehen LINBIT-DRBD-Entwickler nicht still und entwickeln sie stÃ¤ndig weiter.  Ab der <strong>DRBD9-</strong> Version ist <strong>es</strong> nicht mehr nur ein Netzwerkspiegel und wird zu etwas mehr. </p><br><p>  Erstens ist die Idee, ein einzelnes <strong>verteiltes BlockgerÃ¤t</strong> fÃ¼r mehrere Server zu erstellen, in den Hintergrund getreten, und jetzt versucht LINBIT, Tools zum Orchestrieren und Verwalten vieler drbd-GerÃ¤te in einem Cluster bereitzustellen, die auf LVM- und <strong>ZFS-Partitionen erstellt werden</strong> . </p><br><p>  DRBD9 unterstÃ¼tzt beispielsweise bis zu 32 Replikate, RDMA, plattenlose Knoten und neue Orchestrierungswerkzeuge, mit denen Sie Snapshots, Online-Migration und vieles mehr verwenden kÃ¶nnen. </p><br><p>  Trotz der Tatsache, dass <strong>DRBD9</strong> Ã¼ber Integrationstools mit <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> und <strong>OpenNebula verfÃ¼gt</strong> , befinden sie sich derzeit in einem Ãœbergangsmodus, in dem neue Tools noch nicht Ã¼berall unterstÃ¼tzt werden und alte sehr bald als <em>veraltet</em> angekÃ¼ndigt werden.  Dies sind <strong>DRBDmanage</strong> und <strong>Linstor</strong> . </p><br><p>  Ich werde diesen Moment nutzen, um nicht viel auf die Details eines jeden von ihnen <strong>einzugehen</strong> , sondern um die Konfiguration und die Prinzipien der Arbeit mit <strong>DRBD9</strong> selbst genauer zu untersuchen. <a name="habracut"></a>  Sie mÃ¼ssen es noch herausfinden, schon allein deshalb, weil die fehlertolerante Konfiguration des Linstor-Controllers die Installation auf einem dieser GerÃ¤te impliziert. </p><br><p>  In diesem Artikel mÃ¶chte ich Sie Ã¼ber <strong>DRBD9</strong> und die MÃ¶glichkeit seiner Verwendung in <strong>Proxmox</strong> ohne Plug-Ins von <strong>Drittanbietern informieren</strong> . </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage und Linstor </h2><br><p>  ZunÃ¤chst ist noch einmal <strong>DRBDmanage</strong> zu erwÃ¤hnen, das sich sehr gut in <strong>Proxmox integriert</strong> .  LINBIT bietet ein vorgefertigtes DRBDmanage-Plugin fÃ¼r Proxmox, mit dem Sie alle Funktionen direkt Ã¼ber die <strong>Proxmox-</strong> OberflÃ¤che nutzen kÃ¶nnen. </p><br><p>  Es sieht wirklich toll aus, hat aber leider einige Nachteile. </p><br><ul><li> ZunÃ¤chst mÃ¼ssen die markierten DatentrÃ¤gernamen, die <strong>LVM-Gruppe</strong> oder der <strong>ZFS-Pool</strong> den Namen <code>drbdpool</code> . </li><li>  UnfÃ¤higkeit, mehr als <strong>einen</strong> Pool pro Knoten zu verwenden </li><li>  Aufgrund der Besonderheiten der LÃ¶sung kann das <strong>Controller-Volume</strong> nur auf einem normalen LVM und nicht anders sein </li><li>  Periodische <strong>dbus-</strong> StÃ¶rungen, die von <strong>DRBDmanage</strong> eng zur Interaktion mit Knoten verwendet werden. </li></ul><br><p>  Infolgedessen entschied sich LINBIT, die gesamte komplexe DRBDmanage-Logik durch eine einfache Anwendung zu ersetzen, die Ã¼ber eine regulÃ¤re <strong>TCP-Verbindung</strong> mit Knoten kommuniziert und dort ohne Magie funktioniert.  Da war also <strong>Linstor</strong> . </p><br><p>  <strong>Linstor</strong> funktioniert wirklich sehr gut.  Leider haben die Entwickler <strong>Java</strong> als Hauptsprache fÃ¼r das Schreiben des Linstor-Servers gewÃ¤hlt, aber lassen Sie sich davon nicht abschrecken, da Linstor sich nur mit der <strong>Verteilung von</strong> DRBD- <strong>Konfigurationen</strong> und dem <strong>Aufteilen von</strong> LVM / ZFS-Partitionen auf Knoten befasst. </p><br><blockquote>  Beide LÃ¶sungen sind kostenlos und werden unter der kostenlosen <strong>GPL3-</strong> Lizenz vertrieben <strong>.</strong> </blockquote><p>  Sie kÃ¶nnen Ã¼ber jeden von ihnen und Ã¼ber das Einrichten des oben genannten Plug- <strong>Ins</strong> fÃ¼r <strong>Proxmox</strong> im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Proxmox-Wiki</a> lesen </p><br><h2 id="otkazoustoychivyy-nfs-server">  Failover-NFS-Server </h2><br><p>  Leider ist <strong>Linstor</strong> zum Zeitpunkt des Schreibens <strong>nur</strong> in <strong>Kubernetes</strong> integriert.  Ende des Jahres werden jedoch Treiber fÃ¼r die Ã¼brigen <strong>Proxmox-</strong> , <strong>OpenNebula-</strong> und <strong>OpenStack-</strong> Systeme erwartet. </p><br><p>  Bisher gibt es jedoch keine vorgefertigte LÃ¶sung, aber wir mÃ¶gen die alte auf die eine oder andere Weise nicht.  Versuchen wir, DRBD9 auf die altmodische Weise zu verwenden, um den <strong>NFS-Zugriff</strong> auf eine gemeinsam genutzte Partition zu organisieren. </p><br><p>  Diese LÃ¶sung wird sich jedoch auch als nicht ohne Vorteile erweisen, da Sie mit dem NFS-Server den <strong>wettbewerbsfÃ¤higen Zugriff</strong> auf das Speicherdateisystem von mehreren Servern aus organisieren kÃ¶nnen, ohne komplexe Cluster-Dateisysteme mit DLM wie OCFS und GFS2 zu benÃ¶tigen. </p><br><p>  In diesem Fall kÃ¶nnen Sie die Rollen des <strong>primÃ¤ren</strong> / <strong>sekundÃ¤ren</strong> Knotens einfach durch Migrieren des Containers mit dem NFS-Server in der Proxmox-Schnittstelle wechseln. </p><br><p>  Sie kÃ¶nnen auch alle Dateien in diesem Dateisystem sowie virtuelle Festplatten und Sicherungen speichern. </p><br><p>  <strong>Wenn</strong> Sie <strong>Kubernetes verwenden</strong> , kÃ¶nnen Sie den <strong>ReadWriteMany-</strong> Zugriff fÃ¼r Ihre <strong>PersistentVolumes</strong> arrangieren. </p><br><h2 id="proxmox-i-lxc-konteynery">  Proxmox- und LXC-Container </h2><br><p>  Die Frage ist nun: Warum Proxmox? </p><br><p>  Um ein solches Schema zu erstellen, kÃ¶nnten wir im Prinzip Kubernetes sowie das Ã¼bliche Schema mit einem Cluster-Manager verwenden.  <strong>Proxmox</strong> bietet jedoch eine vorgefertigte, sehr multifunktionale und gleichzeitig einfache und intuitive BenutzeroberflÃ¤che fÃ¼r fast alles, was Sie benÃ¶tigen.  Es ist sofort einsatzbereit und unterstÃ¼tzt den auf Softdog basierenden <strong>Fencing-</strong> Mechanismus.  Wenn Sie <strong>LXC-Container verwenden,</strong> kÃ¶nnen Sie beim Umschalten minimale ZeitÃ¼berschreitungen erzielen. <br>  Die resultierende LÃ¶sung weist keinen einzigen <strong>Fehlerpunkt auf</strong> . </p><br><p>  TatsÃ¤chlich werden wir Proxmox hauptsÃ¤chlich als <strong>Cluster-Manager verwenden</strong> , wobei wir einen separaten <strong>LXC-Container</strong> als Dienst betrachten kÃ¶nnen, der in einem klassischen HA-Cluster ausgefÃ¼hrt wird, nur mit dem Unterschied, dass das <strong>Root-System</strong> auch mit dem Container <strong>geliefert wird</strong> .  Das heiÃŸt, Sie mÃ¼ssen nicht mehrere Dienstinstanzen auf jedem Server separat installieren, sondern kÃ¶nnen dies nur einmal im Container tun. <br>  Wenn Sie jemals mit <strong>Cluster-Manager-Software gearbeitet</strong> und <strong>HA</strong> fÃ¼r Anwendungen bereitgestellt haben, werden Sie verstehen, was ich meine. </p><br><h2 id="obschaya-shema">  Allgemeines Schema </h2><br><p>  Unsere LÃ¶sung Ã¤hnelt dem Standardreplikationsschema einer Datenbank. </p><br><ul><li>  Wir haben <strong>drei Knoten</strong> </li><li>  Jeder Knoten verfÃ¼gt Ã¼ber ein verteiltes <strong>drbd-GerÃ¤t</strong> . </li><li>  Das GerÃ¤t verfÃ¼gt Ã¼ber ein regulÃ¤res Dateisystem ( <strong>ext4</strong> ) </li><li>  Nur ein Server kann ein <strong>Master sein</strong> </li><li>  Der <strong>NFS-Server</strong> im <strong>LXC-Container wird</strong> im Assistenten gestartet. </li><li>  Alle Knoten greifen ausschlieÃŸlich Ã¼ber <strong>NFS</strong> auf das GerÃ¤t zu <strong>.</strong> </li><li>  Bei Bedarf kann der Assistent zusammen mit dem <strong>NFS-Server</strong> auf einen anderen Knoten wechseln </li></ul><br><p>  <strong>DRBD9</strong> hat eine sehr coole Funktion, die alles stark vereinfacht: <br>  Das drbd-GerÃ¤t wird automatisch <strong>primÃ¤r</strong> , sobald es auf einem Knoten gemountet ist.  Wenn das GerÃ¤t als <strong>primÃ¤r</strong> markiert ist, fÃ¼hrt jeder Versuch, es auf einem anderen Knoten bereitzustellen, zu einem Zugriffsfehler.  Dies gewÃ¤hrleistet eine Blockierung und einen garantierten Schutz gegen gleichzeitigen Zugriff auf das GerÃ¤t. </p><br><p>  Warum vereinfacht sich das alles stark?  Denn wenn der Container gestartet wird, stellt <strong>Proxmox</strong> dieses GerÃ¤t automatisch bereit und es wird auf diesem Knoten <strong>primÃ¤r.</strong> Wenn der Container stoppt, wird es im Gegenteil aufgehoben und das GerÃ¤t wird wieder <strong>sekundÃ¤r</strong> . <br>  Somit mÃ¼ssen wir uns nicht mehr um das Wechseln von <strong>PrimÃ¤r-</strong> / <strong>SekundÃ¤rgerÃ¤ten</strong> kÃ¼mmern, Proxmox erledigt dies <strong>automatisch</strong> , Hurra! </p><br><h2 id="nastroyka-drbd">  DRBD-Setup </h2><br><p>  Nun, wir haben die Idee herausgefunden. Nun gehen wir zur Implementierung Ã¼ber. </p><br><p>  StandardmÃ¤ÃŸig <strong>wird die achte Version von drbd</strong> <strong>mit dem Linux-Kernel</strong> geliefert. Leider passt sie <strong>nicht zu</strong> uns und wir mÃ¼ssen die neunte Version des Moduls installieren. </p><br><p>  Verbinden Sie das LINBIT-Repository und installieren Sie alles, was Sie benÃ¶tigen: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - Kernel-Header, die zum Erstellen des Moduls benÃ¶tigt werden </li><li>  <code>drbd-dkms</code> - Kernelmodul im DKMS-Format </li><li>  <code>drbd-utils</code> - grundlegende DRBD-Verwaltungsdienstprogramme </li><li>  <code>drbdtop</code> ist ein interaktives Tool wie top nur fÃ¼r DRBD </li></ul><br><p>  Nach der Installation des <strong>Moduls</strong> prÃ¼fen <strong>wir</strong> , ob alles in Ordnung ist: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Wenn Sie die <strong>achte Version</strong> in der Ausgabe des Befehls sehen, ist ein Fehler aufgetreten und das Kernelmodul <strong>im Baum</strong> wird geladen.  ÃœberprÃ¼fen Sie den <code>dkms status</code> um herauszufinden, was der Grund ist. </p><br><p>  Auf jedem Knoten, den wir haben, wird dasselbe <strong>drbd-GerÃ¤t</strong> auf regulÃ¤ren Partitionen ausgefÃ¼hrt.  Zuerst mÃ¼ssen wir diesen Abschnitt fÃ¼r drbd auf jedem Knoten vorbereiten. </p><br><p>  Eine solche Partition kann ein beliebiges <strong>BlockgerÃ¤t sein</strong> , es kann sich um lvm, zvol, eine Festplattenpartition oder die gesamte Festplatte handeln.  In diesem Artikel werde ich eine separate NVME-Festplatte mit einer Partition unter drbd verwenden: <code>/dev/nvme1n1p1</code> </p><br><p>  Es ist erwÃ¤hnenswert, dass sich GerÃ¤tenamen manchmal Ã¤ndern. Es ist daher besser, sofort die Gewohnheit zu verwenden, einen konstanten Symlink zum GerÃ¤t zu verwenden. </p><br><p>  Sie kÃ¶nnen einen solchen Symlink fÃ¼r <code>/dev/nvme1n1p1</code> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Wir beschreiben unsere Ressource auf allen drei Knoten: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  Es wird empfohlen, ein <strong>separates Netzwerk</strong> fÃ¼r die drbd-Synchronisation zu verwenden. </p><br><p>  Erstellen Sie nun die Metadaten fÃ¼r drbd und fÃ¼hren Sie sie aus: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  Wiederholen Sie diese Schritte auf allen drei Knoten und Ã¼berprÃ¼fen Sie den Status: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Jetzt befindet sich unsere <strong>inkonsistente</strong> Festplatte auf allen drei Knoten. Dies liegt daran, dass drbd nicht weiÃŸ, welche Festplatte als Original verwendet werden soll.  Wir mÃ¼ssen einen von ihnen als <strong>primÃ¤r</strong> markieren, damit sein Status mit den anderen Knoten synchronisiert wird: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Unmittelbar danach beginnt die <strong>Synchronisation</strong> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  Wir mÃ¼ssen nicht warten, bis es fertig ist, und wir kÃ¶nnen parallel weitere Schritte ausfÃ¼hren.  Sie kÃ¶nnen auf <strong>jedem Knoten</strong> ausgefÃ¼hrt <strong>werden</strong> , unabhÃ¤ngig vom aktuellen Status der lokalen Festplatte in DRBD.  Alle Anforderungen werden automatisch mit dem <strong>Status UpToDate</strong> an das GerÃ¤t <strong>umgeleitet</strong> . </p><br><p>  Vergessen Sie nicht, <strong>die</strong> automatische AusfÃ¼hrung <strong>des</strong> drbd-Dienstes auf den Knoten zu aktivieren: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Konfigurieren eines LXC-Containers </h2><br><p>  Wir werden den Konfigurationsteil des <strong>Proxmox-Clusters</strong> mit drei Knoten weglassen. Dieser Teil ist im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Wiki</a> gut beschrieben </p><br><p>  Wie ich bereits sagte, arbeitet unser <strong>NFS-Server</strong> in einem <strong>LXC-Container</strong> .  Wir werden den Container auf dem GerÃ¤t <code>/dev/drbd100</code> , das wir gerade erstellt haben. </p><br><p>  Zuerst mÃ¼ssen wir ein <strong>Dateisystem</strong> darauf erstellen: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> enthÃ¤lt standardmÃ¤ÃŸig einen <strong>Multimount-Schutz</strong> auf Dateisystemebene. Im Prinzip kÃ¶nnen wir darauf verzichten, da  DRBD hat standardmÃ¤ÃŸig einen eigenen Schutz. Es verbietet lediglich die zweite <strong>PrimÃ¤rdatenbank</strong> fÃ¼r das GerÃ¤t, aber Vorsicht schadet uns nicht. </p><br><p>  Laden Sie jetzt die Ubuntu-Vorlage herunter: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  Und erstellen Sie unseren Container daraus: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=nfs1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  In diesem Befehl geben wir an, dass sich das <code>/dev/drbd100</code> unseres Containers auf dem GerÃ¤t <code>/dev/drbd100</code> und fÃ¼gen den Parameter <code>shared=1</code> , um die <strong>Migration des</strong> Containers zwischen Knoten zu ermÃ¶glichen. </p><br><p>  Wenn etwas schief gelaufen ist, kÃ¶nnen Sie es jederzeit Ã¼ber die <strong>Proxmox-</strong> OberflÃ¤che oder in der Containerkonfiguration <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox entpackt die Vorlage und bereitet <strong>das</strong> Container- <strong>Root-System</strong> fÃ¼r uns vor.  Danach kÃ¶nnen wir unseren Container starten: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Konfigurieren Sie einen NFS-Server. </h2><br><p>  StandardmÃ¤ÃŸig lÃ¤sst Proxmox <strong>nicht zu, dass</strong> der <strong>NFS-Server</strong> im Container ausgefÃ¼hrt wird. Es gibt jedoch verschiedene MÃ¶glichkeiten, ihn zu aktivieren. </p><br><p>  Eine davon ist das HinzufÃ¼gen von <code>lxc.apparmor.profile: unconfined</code> zur <code>/etc/pve/lxc/100.conf</code> unseres Containers. </p><br><p>  Oder wir kÃ¶nnen <strong>NFS</strong> fÃ¼r alle Container fortlaufend <strong>aktivieren.</strong> Dazu mÃ¼ssen wir die Standardvorlage fÃ¼r LXC auf allen Knoten aktualisieren und die folgenden Zeilen zu <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  Starten Sie den Container nach den Ã„nderungen neu: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Jetzt melden wir uns an: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Installieren Sie Updates und <strong>NFS-Server</strong> : </p><br><pre> <code class="hljs powershell">apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> update apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nfs<span class="hljs-literal"><span class="hljs-literal">-kernel</span></span><span class="hljs-literal"><span class="hljs-literal">-server</span></span></code> </pre> <br><p>  <strong>Export</strong> erstellen: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  HA-Setup </h2><br><p>  Zum Zeitpunkt des Schreibens weist der <strong>HA-Manager</strong> von proxmox einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fehler auf</a> , der verhindert, dass der HA-Container seine Arbeit erfolgreich abschlieÃŸt. Infolgedessen verhindern die <strong>Kernel-Space-</strong> Prozesse des <strong>NFS-Servers</strong> , die nicht vollstÃ¤ndig beendet wurden, dass das drbd-GerÃ¤t <strong>Secondary</strong> verlÃ¤sst.  Wenn Sie bereits auf eine solche Situation gestoÃŸen sind, sollten Sie nicht in Panik geraten und einfach <code>killall -9 nfsd</code> auf dem Knoten ausfÃ¼hren, auf dem der Container gestartet wurde. Dann sollte das drbd-GerÃ¤t "freigeben" und es geht zu <strong>Secondary</strong> . </p><br><p>  FÃ¼hren Sie die folgenden Befehle auf allen Knoten aus, um diesen Fehler zu beheben: </p><br><pre> <code class="hljs powershell">sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/share/perl5/PVE/HA/Resources/PVECT.pm systemctl restart pve<span class="hljs-literal"><span class="hljs-literal">-ha</span></span><span class="hljs-literal"><span class="hljs-literal">-lrm</span></span>.service</code> </pre> <br><p>  Jetzt kÃ¶nnen wir mit der <strong>HA-Manager-</strong> Konfiguration fortfahren.  Erstellen wir eine separate HA-Gruppe fÃ¼r unser GerÃ¤t: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd nfs1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Unsere <strong>Ressource</strong> funktioniert nur auf den fÃ¼r diese Gruppe angegebenen Knoten.  FÃ¼gen Sie unseren Container dieser Gruppe hinzu: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=nfs1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  Das ist alles.  Einfach, richtig? </p><br><p>  Der resultierende <strong>NFS-Ball</strong> kann sofort mit Proxmox verbunden werden, um andere virtuelle Maschinen und Container zu speichern und auszufÃ¼hren. </p><br><h2 id="rekomendacii-i-tyuning">  Empfehlungen und Abstimmung </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Wie oben erwÃ¤hnt, ist es immer ratsam, ein separates Netzwerk fÃ¼r die Replikation zu verwenden.  Es wird dringend empfohlen, <strong>10-Gigabit-Netzwerkadapter zu verwenden</strong> , da sonst die <strong>Portgeschwindigkeit beeintrÃ¤chtigt</strong> wird. <br>  Wenn die Replikation langsam genug erscheint, probieren Sie einige der Optionen fÃ¼r <strong>DRBD aus</strong> .  Hier ist die Konfiguration, die meiner Meinung nach fÃ¼r mein <strong>10G-Netzwerk</strong> optimal <strong>ist</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Weitere Informationen zu den einzelnen Parametern finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen DRBD-Dokumentation.</a> </p><br><h5 id="nfs-server">  NFS-Server </h5><br><p>  Um den Betrieb des <strong>NFS-Servers</strong> zu beschleunigen <strong>,</strong> kann es hilfreich sein, die Gesamtzahl der ausgefÃ¼hrten <strong>Instanzen des</strong> NFS-Servers zu erhÃ¶hen.  StandardmÃ¤ÃŸig - <strong>8</strong> , persÃ¶nlich hat es mir geholfen, diese Zahl auf <strong>64</strong> zu erhÃ¶hen. </p><br><p>  Aktualisieren Sie dazu den Parameter <code>RPCNFSDCOUNT=64</code> in <code>/etc/default/nfs-kernel-server</code> . <br>  Und starte die DÃ¤monen neu: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 gegen NFSv4 </h5><br><p>  Kennen Sie den Unterschied zwischen <strong>NFSv3</strong> und <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>NFSv3</strong> ist ein <strong>zustandsloses Protokoll, das</strong> Fehler in der Regel besser toleriert und schneller <strong>wiederherstellt</strong> . </li><li>  <strong>NFSv4</strong> ist ein <strong>Stateful-Protokoll</strong> , es arbeitet schneller und kann an bestimmte TCP-Ports gebunden werden. Aufgrund des vorhandenen Status ist es jedoch empfindlicher gegenÃ¼ber Fehlern.  Es hat auch die MÃ¶glichkeit, die Authentifizierung mit Kerberos und einer Reihe anderer interessanter Funktionen zu verwenden. </li></ul><br><p>  Wenn Sie jedoch <code>showmount -e nfs_server</code> , wird das NFSv3-Protokoll verwendet.  Proxmox verwendet auch NFSv3.  NFSv3 wird auch hÃ¤ufig zum Organisieren von Netzwerkstartmaschinen verwendet. </p><br><p>  Wenn Sie keinen besonderen Grund fÃ¼r die Verwendung von NFSv4 haben, versuchen Sie im Allgemeinen, NFSv3 zu verwenden, da es bei Fehlern aufgrund des Fehlens eines Status als solchen weniger schmerzhaft ist. </p><br><p>  Sie kÃ¶nnen den Ball mit NFSv3 mounten, indem Sie den Parameter <code>-o vers=3</code> fÃ¼r den Befehl <strong>mount</strong> angeben: </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  Wenn Sie mÃ¶chten, kÃ¶nnen Sie NFSv4 fÃ¼r den Server im Allgemeinen deaktivieren. <code>--no-nfs-version 4</code> Variablen <code>--no-nfs-version 4</code> Option <code>--no-nfs-version 4</code> und starten Sie den Server neu. Beispiel: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI und LVM </h2><br><p>  In Ã¤hnlicher Weise kann ein regulÃ¤rer <strong>tgt-Daemon</strong> im Container konfiguriert werden, iSCSI bietet eine deutlich hÃ¶here Leistung fÃ¼r E / A-VorgÃ¤nge und der Container funktioniert reibungsloser, da der tgt-Server vollstÃ¤ndig im Benutzerbereich arbeitet. </p><br><p>  In der Regel wird eine exportierte <strong>LUN</strong> mithilfe von <strong>LVM</strong> in viele Teile geschnitten.  Es sind jedoch mehrere Nuancen zu berÃ¼cksichtigen, z. B. wie LVM- <strong>Sperren</strong> fÃ¼r die gemeinsame Nutzung einer exportierten Gruppe auf mehreren Hosts bereitgestellt werden. </p><br><p>  Vielleicht werde ich diese und andere Nuancen im <strong><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nÃ¤chsten Artikel beschreiben</a></strong> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417473/">https://habr.com/ru/post/de417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar486150/index.html">ØªØ·ÙˆÙŠØ± Ù…Ø¬Ø§Ù„ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø³Ù„ÙˆÙØ§ÙƒÙŠØ§. ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ø¹Ù…Ù„ Ù„Ù„Ù…Ù‡Ù†ÙŠÙŠÙ† Ø§Ù„Ø´Ø¨Ø§Ø¨</a></li>
<li><a href="../ar486156/index.html">ÙƒÙ…Ø§ Ø¹Ù„Ù…Øª ØŒ Ø«Ù… ÙƒØªØ¨ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø¨ÙŠØ«ÙˆÙ†</a></li>
<li><a href="../ar486158/index.html">ØªØµÙˆØ± Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Ù†Ù…Ø§Ø°Ø¬ seq2seq Ù…Ø¹ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…)</a></li>
<li><a href="../ar486164/index.html">ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§ 2019-nCoV. Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø¹Ù† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„ØªÙ†ÙØ³ÙŠ ÙˆØ§Ù„ØªØ·Ù‡ÙŠØ±</a></li>
<li><a href="../ar486174/index.html">Ù„Ø¯ÙŠ ØµÙØ± Ø¯ÙˆØ±Ø§Ù†</a></li>
<li><a href="../de417475/index.html">Glusterfs + LÃ¶schcodierung: Wenn Sie viel brauchen, billig und zuverlÃ¤ssig</a></li>
<li><a href="../de417477/index.html">Hot Desking</a></li>
<li><a href="../de417479/index.html">Schnellere Verkettung von Zeichenfolgen zum Selbermachen in Go</a></li>
<li><a href="../de417481/index.html">Informationen zu Generatoren in JavaScript ES6 und warum es optional ist, sie zu studieren</a></li>
<li><a href="../de417483/index.html">Vergleich von JS-Frameworks: React, Vue und Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>