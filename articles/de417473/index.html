<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòè üë®üèº‚Äçüè≠ üåÖ Vertrauensw√ºrdiger Speicher mit DRBD9 und Proxmox (Teil 1: NFS) üßôüèæ üöÜ üë©üèæ‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wahrscheinlich hat jeder, der mindestens einmal von der Suche nach hochleistungsf√§higem, softwaredefiniertem Speicher verwirrt war, fr√ºher oder sp√§ter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Vertrauensw√ºrdiger Speicher mit DRBD9 und Proxmox (Teil 1: NFS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417473/"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="Bild"></p><br><p>  Wahrscheinlich hat jeder, der mindestens einmal von der Suche nach hochleistungsf√§higem, <strong>softwaredefiniertem Speicher verwirrt war,</strong> fr√ºher oder sp√§ter von <strong>DRBD</strong> geh√∂rt oder sich vielleicht sogar damit befasst. </p><br><p> Auf dem H√∂hepunkt der Popularit√§t von <strong>Ceph</strong> und <strong>GlusterFS</strong> , die im Prinzip ziemlich gut funktionieren und vor allem <strong>sofort</strong> <strong>einsatzbereit</strong> sind, haben alle nur ein wenig vergessen.  Dar√ºber hinaus unterst√ºtzte die vorherige Version die Replikation auf mehr als zwei Knoten nicht, und aus diesem Grund traten h√§ufig Probleme mit dem <strong>geteilten Gehirn</strong> auf, was eindeutig nicht zu seiner Popularit√§t beitrug. </p><br><p>  Die L√∂sung ist wirklich nicht neu, aber ziemlich wettbewerbsf√§hig.  Mit relativ geringen Kosten f√ºr CPU und RAM bietet <strong>DRBD</strong> eine sehr schnelle und sichere Synchronisation auf <strong>Blockger√§teebene</strong> .  W√§hrend dieser ganzen Zeit stehen LINBIT-DRBD-Entwickler nicht still und entwickeln sie st√§ndig weiter.  Ab der <strong>DRBD9-</strong> Version ist <strong>es</strong> nicht mehr nur ein Netzwerkspiegel und wird zu etwas mehr. </p><br><p>  Erstens ist die Idee, ein einzelnes <strong>verteiltes Blockger√§t</strong> f√ºr mehrere Server zu erstellen, in den Hintergrund getreten, und jetzt versucht LINBIT, Tools zum Orchestrieren und Verwalten vieler drbd-Ger√§te in einem Cluster bereitzustellen, die auf LVM- und <strong>ZFS-Partitionen erstellt werden</strong> . </p><br><p>  DRBD9 unterst√ºtzt beispielsweise bis zu 32 Replikate, RDMA, plattenlose Knoten und neue Orchestrierungswerkzeuge, mit denen Sie Snapshots, Online-Migration und vieles mehr verwenden k√∂nnen. </p><br><p>  Trotz der Tatsache, dass <strong>DRBD9</strong> √ºber Integrationstools mit <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> und <strong>OpenNebula verf√ºgt</strong> , befinden sie sich derzeit in einem √úbergangsmodus, in dem neue Tools noch nicht √ºberall unterst√ºtzt werden und alte sehr bald als <em>veraltet</em> angek√ºndigt werden.  Dies sind <strong>DRBDmanage</strong> und <strong>Linstor</strong> . </p><br><p>  Ich werde diesen Moment nutzen, um nicht viel auf die Details eines jeden von ihnen <strong>einzugehen</strong> , sondern um die Konfiguration und die Prinzipien der Arbeit mit <strong>DRBD9</strong> selbst genauer zu untersuchen. <a name="habracut"></a>  Sie m√ºssen es noch herausfinden, schon allein deshalb, weil die fehlertolerante Konfiguration des Linstor-Controllers die Installation auf einem dieser Ger√§te impliziert. </p><br><p>  In diesem Artikel m√∂chte ich Sie √ºber <strong>DRBD9</strong> und die M√∂glichkeit seiner Verwendung in <strong>Proxmox</strong> ohne Plug-Ins von <strong>Drittanbietern informieren</strong> . </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage und Linstor </h2><br><p>  Zun√§chst ist noch einmal <strong>DRBDmanage</strong> zu erw√§hnen, das sich sehr gut in <strong>Proxmox integriert</strong> .  LINBIT bietet ein vorgefertigtes DRBDmanage-Plugin f√ºr Proxmox, mit dem Sie alle Funktionen direkt √ºber die <strong>Proxmox-</strong> Oberfl√§che nutzen k√∂nnen. </p><br><p>  Es sieht wirklich toll aus, hat aber leider einige Nachteile. </p><br><ul><li> Zun√§chst m√ºssen die markierten Datentr√§gernamen, die <strong>LVM-Gruppe</strong> oder der <strong>ZFS-Pool</strong> den Namen <code>drbdpool</code> . </li><li>  Unf√§higkeit, mehr als <strong>einen</strong> Pool pro Knoten zu verwenden </li><li>  Aufgrund der Besonderheiten der L√∂sung kann das <strong>Controller-Volume</strong> nur auf einem normalen LVM und nicht anders sein </li><li>  Periodische <strong>dbus-</strong> St√∂rungen, die von <strong>DRBDmanage</strong> eng zur Interaktion mit Knoten verwendet werden. </li></ul><br><p>  Infolgedessen entschied sich LINBIT, die gesamte komplexe DRBDmanage-Logik durch eine einfache Anwendung zu ersetzen, die √ºber eine regul√§re <strong>TCP-Verbindung</strong> mit Knoten kommuniziert und dort ohne Magie funktioniert.  Da war also <strong>Linstor</strong> . </p><br><p>  <strong>Linstor</strong> funktioniert wirklich sehr gut.  Leider haben die Entwickler <strong>Java</strong> als Hauptsprache f√ºr das Schreiben des Linstor-Servers gew√§hlt, aber lassen Sie sich davon nicht abschrecken, da Linstor sich nur mit der <strong>Verteilung von</strong> DRBD- <strong>Konfigurationen</strong> und dem <strong>Aufteilen von</strong> LVM / ZFS-Partitionen auf Knoten befasst. </p><br><blockquote>  Beide L√∂sungen sind kostenlos und werden unter der kostenlosen <strong>GPL3-</strong> Lizenz vertrieben <strong>.</strong> </blockquote><p>  Sie k√∂nnen √ºber jeden von ihnen und √ºber das Einrichten des oben genannten Plug- <strong>Ins</strong> f√ºr <strong>Proxmox</strong> im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Proxmox-Wiki</a> lesen </p><br><h2 id="otkazoustoychivyy-nfs-server">  Failover-NFS-Server </h2><br><p>  Leider ist <strong>Linstor</strong> zum Zeitpunkt des Schreibens <strong>nur</strong> in <strong>Kubernetes</strong> integriert.  Ende des Jahres werden jedoch Treiber f√ºr die √ºbrigen <strong>Proxmox-</strong> , <strong>OpenNebula-</strong> und <strong>OpenStack-</strong> Systeme erwartet. </p><br><p>  Bisher gibt es jedoch keine vorgefertigte L√∂sung, aber wir m√∂gen die alte auf die eine oder andere Weise nicht.  Versuchen wir, DRBD9 auf die altmodische Weise zu verwenden, um den <strong>NFS-Zugriff</strong> auf eine gemeinsam genutzte Partition zu organisieren. </p><br><p>  Diese L√∂sung wird sich jedoch auch als nicht ohne Vorteile erweisen, da Sie mit dem NFS-Server den <strong>wettbewerbsf√§higen Zugriff</strong> auf das Speicherdateisystem von mehreren Servern aus organisieren k√∂nnen, ohne komplexe Cluster-Dateisysteme mit DLM wie OCFS und GFS2 zu ben√∂tigen. </p><br><p>  In diesem Fall k√∂nnen Sie die Rollen des <strong>prim√§ren</strong> / <strong>sekund√§ren</strong> Knotens einfach durch Migrieren des Containers mit dem NFS-Server in der Proxmox-Schnittstelle wechseln. </p><br><p>  Sie k√∂nnen auch alle Dateien in diesem Dateisystem sowie virtuelle Festplatten und Sicherungen speichern. </p><br><p>  <strong>Wenn</strong> Sie <strong>Kubernetes verwenden</strong> , k√∂nnen Sie den <strong>ReadWriteMany-</strong> Zugriff f√ºr Ihre <strong>PersistentVolumes</strong> arrangieren. </p><br><h2 id="proxmox-i-lxc-konteynery">  Proxmox- und LXC-Container </h2><br><p>  Die Frage ist nun: Warum Proxmox? </p><br><p>  Um ein solches Schema zu erstellen, k√∂nnten wir im Prinzip Kubernetes sowie das √ºbliche Schema mit einem Cluster-Manager verwenden.  <strong>Proxmox</strong> bietet jedoch eine vorgefertigte, sehr multifunktionale und gleichzeitig einfache und intuitive Benutzeroberfl√§che f√ºr fast alles, was Sie ben√∂tigen.  Es ist sofort einsatzbereit und unterst√ºtzt den auf Softdog basierenden <strong>Fencing-</strong> Mechanismus.  Wenn Sie <strong>LXC-Container verwenden,</strong> k√∂nnen Sie beim Umschalten minimale Zeit√ºberschreitungen erzielen. <br>  Die resultierende L√∂sung weist keinen einzigen <strong>Fehlerpunkt auf</strong> . </p><br><p>  Tats√§chlich werden wir Proxmox haupts√§chlich als <strong>Cluster-Manager verwenden</strong> , wobei wir einen separaten <strong>LXC-Container</strong> als Dienst betrachten k√∂nnen, der in einem klassischen HA-Cluster ausgef√ºhrt wird, nur mit dem Unterschied, dass das <strong>Root-System</strong> auch mit dem Container <strong>geliefert wird</strong> .  Das hei√üt, Sie m√ºssen nicht mehrere Dienstinstanzen auf jedem Server separat installieren, sondern k√∂nnen dies nur einmal im Container tun. <br>  Wenn Sie jemals mit <strong>Cluster-Manager-Software gearbeitet</strong> und <strong>HA</strong> f√ºr Anwendungen bereitgestellt haben, werden Sie verstehen, was ich meine. </p><br><h2 id="obschaya-shema">  Allgemeines Schema </h2><br><p>  Unsere L√∂sung √§hnelt dem Standardreplikationsschema einer Datenbank. </p><br><ul><li>  Wir haben <strong>drei Knoten</strong> </li><li>  Jeder Knoten verf√ºgt √ºber ein verteiltes <strong>drbd-Ger√§t</strong> . </li><li>  Das Ger√§t verf√ºgt √ºber ein regul√§res Dateisystem ( <strong>ext4</strong> ) </li><li>  Nur ein Server kann ein <strong>Master sein</strong> </li><li>  Der <strong>NFS-Server</strong> im <strong>LXC-Container wird</strong> im Assistenten gestartet. </li><li>  Alle Knoten greifen ausschlie√ülich √ºber <strong>NFS</strong> auf das Ger√§t zu <strong>.</strong> </li><li>  Bei Bedarf kann der Assistent zusammen mit dem <strong>NFS-Server</strong> auf einen anderen Knoten wechseln </li></ul><br><p>  <strong>DRBD9</strong> hat eine sehr coole Funktion, die alles stark vereinfacht: <br>  Das drbd-Ger√§t wird automatisch <strong>prim√§r</strong> , sobald es auf einem Knoten gemountet ist.  Wenn das Ger√§t als <strong>prim√§r</strong> markiert ist, f√ºhrt jeder Versuch, es auf einem anderen Knoten bereitzustellen, zu einem Zugriffsfehler.  Dies gew√§hrleistet eine Blockierung und einen garantierten Schutz gegen gleichzeitigen Zugriff auf das Ger√§t. </p><br><p>  Warum vereinfacht sich das alles stark?  Denn wenn der Container gestartet wird, stellt <strong>Proxmox</strong> dieses Ger√§t automatisch bereit und es wird auf diesem Knoten <strong>prim√§r.</strong> Wenn der Container stoppt, wird es im Gegenteil aufgehoben und das Ger√§t wird wieder <strong>sekund√§r</strong> . <br>  Somit m√ºssen wir uns nicht mehr um das Wechseln von <strong>Prim√§r-</strong> / <strong>Sekund√§rger√§ten</strong> k√ºmmern, Proxmox erledigt dies <strong>automatisch</strong> , Hurra! </p><br><h2 id="nastroyka-drbd">  DRBD-Setup </h2><br><p>  Nun, wir haben die Idee herausgefunden. Nun gehen wir zur Implementierung √ºber. </p><br><p>  Standardm√§√üig <strong>wird die achte Version von drbd</strong> <strong>mit dem Linux-Kernel</strong> geliefert. Leider passt sie <strong>nicht zu</strong> uns und wir m√ºssen die neunte Version des Moduls installieren. </p><br><p>  Verbinden Sie das LINBIT-Repository und installieren Sie alles, was Sie ben√∂tigen: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - Kernel-Header, die zum Erstellen des Moduls ben√∂tigt werden </li><li>  <code>drbd-dkms</code> - Kernelmodul im DKMS-Format </li><li>  <code>drbd-utils</code> - grundlegende DRBD-Verwaltungsdienstprogramme </li><li>  <code>drbdtop</code> ist ein interaktives Tool wie top nur f√ºr DRBD </li></ul><br><p>  Nach der Installation des <strong>Moduls</strong> pr√ºfen <strong>wir</strong> , ob alles in Ordnung ist: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Wenn Sie die <strong>achte Version</strong> in der Ausgabe des Befehls sehen, ist ein Fehler aufgetreten und das Kernelmodul <strong>im Baum</strong> wird geladen.  √úberpr√ºfen Sie den <code>dkms status</code> um herauszufinden, was der Grund ist. </p><br><p>  Auf jedem Knoten, den wir haben, wird dasselbe <strong>drbd-Ger√§t</strong> auf regul√§ren Partitionen ausgef√ºhrt.  Zuerst m√ºssen wir diesen Abschnitt f√ºr drbd auf jedem Knoten vorbereiten. </p><br><p>  Eine solche Partition kann ein beliebiges <strong>Blockger√§t sein</strong> , es kann sich um lvm, zvol, eine Festplattenpartition oder die gesamte Festplatte handeln.  In diesem Artikel werde ich eine separate NVME-Festplatte mit einer Partition unter drbd verwenden: <code>/dev/nvme1n1p1</code> </p><br><p>  Es ist erw√§hnenswert, dass sich Ger√§tenamen manchmal √§ndern. Es ist daher besser, sofort die Gewohnheit zu verwenden, einen konstanten Symlink zum Ger√§t zu verwenden. </p><br><p>  Sie k√∂nnen einen solchen Symlink f√ºr <code>/dev/nvme1n1p1</code> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Wir beschreiben unsere Ressource auf allen drei Knoten: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  Es wird empfohlen, ein <strong>separates Netzwerk</strong> f√ºr die drbd-Synchronisation zu verwenden. </p><br><p>  Erstellen Sie nun die Metadaten f√ºr drbd und f√ºhren Sie sie aus: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  Wiederholen Sie diese Schritte auf allen drei Knoten und √ºberpr√ºfen Sie den Status: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Jetzt befindet sich unsere <strong>inkonsistente</strong> Festplatte auf allen drei Knoten. Dies liegt daran, dass drbd nicht wei√ü, welche Festplatte als Original verwendet werden soll.  Wir m√ºssen einen von ihnen als <strong>prim√§r</strong> markieren, damit sein Status mit den anderen Knoten synchronisiert wird: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Unmittelbar danach beginnt die <strong>Synchronisation</strong> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  Wir m√ºssen nicht warten, bis es fertig ist, und wir k√∂nnen parallel weitere Schritte ausf√ºhren.  Sie k√∂nnen auf <strong>jedem Knoten</strong> ausgef√ºhrt <strong>werden</strong> , unabh√§ngig vom aktuellen Status der lokalen Festplatte in DRBD.  Alle Anforderungen werden automatisch mit dem <strong>Status UpToDate</strong> an das Ger√§t <strong>umgeleitet</strong> . </p><br><p>  Vergessen Sie nicht, <strong>die</strong> automatische Ausf√ºhrung <strong>des</strong> drbd-Dienstes auf den Knoten zu aktivieren: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Konfigurieren eines LXC-Containers </h2><br><p>  Wir werden den Konfigurationsteil des <strong>Proxmox-Clusters</strong> mit drei Knoten weglassen. Dieser Teil ist im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Wiki</a> gut beschrieben </p><br><p>  Wie ich bereits sagte, arbeitet unser <strong>NFS-Server</strong> in einem <strong>LXC-Container</strong> .  Wir werden den Container auf dem Ger√§t <code>/dev/drbd100</code> , das wir gerade erstellt haben. </p><br><p>  Zuerst m√ºssen wir ein <strong>Dateisystem</strong> darauf erstellen: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> enth√§lt standardm√§√üig einen <strong>Multimount-Schutz</strong> auf Dateisystemebene. Im Prinzip k√∂nnen wir darauf verzichten, da  DRBD hat standardm√§√üig einen eigenen Schutz. Es verbietet lediglich die zweite <strong>Prim√§rdatenbank</strong> f√ºr das Ger√§t, aber Vorsicht schadet uns nicht. </p><br><p>  Laden Sie jetzt die Ubuntu-Vorlage herunter: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  Und erstellen Sie unseren Container daraus: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=nfs1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  In diesem Befehl geben wir an, dass sich das <code>/dev/drbd100</code> unseres Containers auf dem Ger√§t <code>/dev/drbd100</code> und f√ºgen den Parameter <code>shared=1</code> , um die <strong>Migration des</strong> Containers zwischen Knoten zu erm√∂glichen. </p><br><p>  Wenn etwas schief gelaufen ist, k√∂nnen Sie es jederzeit √ºber die <strong>Proxmox-</strong> Oberfl√§che oder in der Containerkonfiguration <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox entpackt die Vorlage und bereitet <strong>das</strong> Container- <strong>Root-System</strong> f√ºr uns vor.  Danach k√∂nnen wir unseren Container starten: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Konfigurieren Sie einen NFS-Server. </h2><br><p>  Standardm√§√üig l√§sst Proxmox <strong>nicht zu, dass</strong> der <strong>NFS-Server</strong> im Container ausgef√ºhrt wird. Es gibt jedoch verschiedene M√∂glichkeiten, ihn zu aktivieren. </p><br><p>  Eine davon ist das Hinzuf√ºgen von <code>lxc.apparmor.profile: unconfined</code> zur <code>/etc/pve/lxc/100.conf</code> unseres Containers. </p><br><p>  Oder wir k√∂nnen <strong>NFS</strong> f√ºr alle Container fortlaufend <strong>aktivieren.</strong> Dazu m√ºssen wir die Standardvorlage f√ºr LXC auf allen Knoten aktualisieren und die folgenden Zeilen zu <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  Starten Sie den Container nach den √Ñnderungen neu: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Jetzt melden wir uns an: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Installieren Sie Updates und <strong>NFS-Server</strong> : </p><br><pre> <code class="hljs powershell">apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> update apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nfs<span class="hljs-literal"><span class="hljs-literal">-kernel</span></span><span class="hljs-literal"><span class="hljs-literal">-server</span></span></code> </pre> <br><p>  <strong>Export</strong> erstellen: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  HA-Setup </h2><br><p>  Zum Zeitpunkt des Schreibens weist der <strong>HA-Manager</strong> von proxmox einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fehler auf</a> , der verhindert, dass der HA-Container seine Arbeit erfolgreich abschlie√üt. Infolgedessen verhindern die <strong>Kernel-Space-</strong> Prozesse des <strong>NFS-Servers</strong> , die nicht vollst√§ndig beendet wurden, dass das drbd-Ger√§t <strong>Secondary</strong> verl√§sst.  Wenn Sie bereits auf eine solche Situation gesto√üen sind, sollten Sie nicht in Panik geraten und einfach <code>killall -9 nfsd</code> auf dem Knoten ausf√ºhren, auf dem der Container gestartet wurde. Dann sollte das drbd-Ger√§t "freigeben" und es geht zu <strong>Secondary</strong> . </p><br><p>  F√ºhren Sie die folgenden Befehle auf allen Knoten aus, um diesen Fehler zu beheben: </p><br><pre> <code class="hljs powershell">sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/share/perl5/PVE/HA/Resources/PVECT.pm systemctl restart pve<span class="hljs-literal"><span class="hljs-literal">-ha</span></span><span class="hljs-literal"><span class="hljs-literal">-lrm</span></span>.service</code> </pre> <br><p>  Jetzt k√∂nnen wir mit der <strong>HA-Manager-</strong> Konfiguration fortfahren.  Erstellen wir eine separate HA-Gruppe f√ºr unser Ger√§t: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd nfs1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Unsere <strong>Ressource</strong> funktioniert nur auf den f√ºr diese Gruppe angegebenen Knoten.  F√ºgen Sie unseren Container dieser Gruppe hinzu: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=nfs1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  Das ist alles.  Einfach, richtig? </p><br><p>  Der resultierende <strong>NFS-Ball</strong> kann sofort mit Proxmox verbunden werden, um andere virtuelle Maschinen und Container zu speichern und auszuf√ºhren. </p><br><h2 id="rekomendacii-i-tyuning">  Empfehlungen und Abstimmung </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Wie oben erw√§hnt, ist es immer ratsam, ein separates Netzwerk f√ºr die Replikation zu verwenden.  Es wird dringend empfohlen, <strong>10-Gigabit-Netzwerkadapter zu verwenden</strong> , da sonst die <strong>Portgeschwindigkeit beeintr√§chtigt</strong> wird. <br>  Wenn die Replikation langsam genug erscheint, probieren Sie einige der Optionen f√ºr <strong>DRBD aus</strong> .  Hier ist die Konfiguration, die meiner Meinung nach f√ºr mein <strong>10G-Netzwerk</strong> optimal <strong>ist</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Weitere Informationen zu den einzelnen Parametern finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen DRBD-Dokumentation.</a> </p><br><h5 id="nfs-server">  NFS-Server </h5><br><p>  Um den Betrieb des <strong>NFS-Servers</strong> zu beschleunigen <strong>,</strong> kann es hilfreich sein, die Gesamtzahl der ausgef√ºhrten <strong>Instanzen des</strong> NFS-Servers zu erh√∂hen.  Standardm√§√üig - <strong>8</strong> , pers√∂nlich hat es mir geholfen, diese Zahl auf <strong>64</strong> zu erh√∂hen. </p><br><p>  Aktualisieren Sie dazu den Parameter <code>RPCNFSDCOUNT=64</code> in <code>/etc/default/nfs-kernel-server</code> . <br>  Und starte die D√§monen neu: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 gegen NFSv4 </h5><br><p>  Kennen Sie den Unterschied zwischen <strong>NFSv3</strong> und <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>NFSv3</strong> ist ein <strong>zustandsloses Protokoll, das</strong> Fehler in der Regel besser toleriert und schneller <strong>wiederherstellt</strong> . </li><li>  <strong>NFSv4</strong> ist ein <strong>Stateful-Protokoll</strong> , es arbeitet schneller und kann an bestimmte TCP-Ports gebunden werden. Aufgrund des vorhandenen Status ist es jedoch empfindlicher gegen√ºber Fehlern.  Es hat auch die M√∂glichkeit, die Authentifizierung mit Kerberos und einer Reihe anderer interessanter Funktionen zu verwenden. </li></ul><br><p>  Wenn Sie jedoch <code>showmount -e nfs_server</code> , wird das NFSv3-Protokoll verwendet.  Proxmox verwendet auch NFSv3.  NFSv3 wird auch h√§ufig zum Organisieren von Netzwerkstartmaschinen verwendet. </p><br><p>  Wenn Sie keinen besonderen Grund f√ºr die Verwendung von NFSv4 haben, versuchen Sie im Allgemeinen, NFSv3 zu verwenden, da es bei Fehlern aufgrund des Fehlens eines Status als solchen weniger schmerzhaft ist. </p><br><p>  Sie k√∂nnen den Ball mit NFSv3 mounten, indem Sie den Parameter <code>-o vers=3</code> f√ºr den Befehl <strong>mount</strong> angeben: </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  Wenn Sie m√∂chten, k√∂nnen Sie NFSv4 f√ºr den Server im Allgemeinen deaktivieren. <code>--no-nfs-version 4</code> Variablen <code>--no-nfs-version 4</code> Option <code>--no-nfs-version 4</code> und starten Sie den Server neu. Beispiel: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI und LVM </h2><br><p>  In √§hnlicher Weise kann ein regul√§rer <strong>tgt-Daemon</strong> im Container konfiguriert werden, iSCSI bietet eine deutlich h√∂here Leistung f√ºr E / A-Vorg√§nge und der Container funktioniert reibungsloser, da der tgt-Server vollst√§ndig im Benutzerbereich arbeitet. </p><br><p>  In der Regel wird eine exportierte <strong>LUN</strong> mithilfe von <strong>LVM</strong> in viele Teile geschnitten.  Es sind jedoch mehrere Nuancen zu ber√ºcksichtigen, z. B. wie LVM- <strong>Sperren</strong> f√ºr die gemeinsame Nutzung einer exportierten Gruppe auf mehreren Hosts bereitgestellt werden. </p><br><p>  Vielleicht werde ich diese und andere Nuancen im <strong><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">n√§chsten Artikel beschreiben</a></strong> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417473/">https://habr.com/ru/post/de417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar486150/index.html">ÿ™ÿ∑ŸàŸäÿ± ŸÖÿ¨ÿßŸÑ ÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÅŸä ÿ≥ŸÑŸàŸÅÿßŸÉŸäÿß. ŸÅŸàÿßÿ¶ÿØ ÿßŸÑÿπŸÖŸÑ ŸÑŸÑŸÖŸáŸÜŸäŸäŸÜ ÿßŸÑÿ¥ÿ®ÿßÿ®</a></li>
<li><a href="../ar486156/index.html">ŸÉŸÖÿß ÿπŸÑŸÖÿ™ ÿå ÿ´ŸÖ ŸÉÿ™ÿ® ÿØŸÑŸäŸÑ ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ŸÅŸä ÿ®Ÿäÿ´ŸàŸÜ</a></li>
<li><a href="../ar486158/index.html">ÿ™ÿµŸàÿ± ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑÿ¢ŸÑŸäÿ© ÿßŸÑÿπÿµÿ®Ÿäÿ© (ŸÜŸÖÿßÿ∞ÿ¨ seq2seq ŸÖÿπ ÿ¢ŸÑŸäÿ© ÿßŸÑÿßŸáÿ™ŸÖÿßŸÖ)</a></li>
<li><a href="../ar486164/index.html">ŸÅŸäÿ±Ÿàÿ≥ ŸÉŸàÿ±ŸàŸÜÿß 2019-nCoV. ÿ£ÿ≥ÿ¶ŸÑÿ© Ÿàÿ£ÿ¨Ÿàÿ®ÿ© ÿπŸÜ ÿ≠ŸÖÿßŸäÿ© ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿßŸÑÿ™ŸÜŸÅÿ≥Ÿä ŸàÿßŸÑÿ™ÿ∑ŸáŸäÿ±</a></li>
<li><a href="../ar486174/index.html">ŸÑÿØŸä ÿµŸÅÿ± ÿØŸàÿ±ÿßŸÜ</a></li>
<li><a href="../de417475/index.html">Glusterfs + L√∂schcodierung: Wenn Sie viel brauchen, billig und zuverl√§ssig</a></li>
<li><a href="../de417477/index.html">Hot Desking</a></li>
<li><a href="../de417479/index.html">Schnellere Verkettung von Zeichenfolgen zum Selbermachen in Go</a></li>
<li><a href="../de417481/index.html">Informationen zu Generatoren in JavaScript ES6 und warum es optional ist, sie zu studieren</a></li>
<li><a href="../de417483/index.html">Vergleich von JS-Frameworks: React, Vue und Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>