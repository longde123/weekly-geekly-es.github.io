<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☝🏻 👉🏿 👽 Apache NiFi: apa itu dan gambaran umum singkat fitur-fiturnya 👨‍👨‍👧‍👦 👨🏼‍🔧 👏🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hari ini di situs asing bertema tentang Big Data Anda dapat menemukan penyebutan alat yang relatif baru untuk ekosistem Hadoop sebagai Apache NiFi. In...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache NiFi: apa itu dan gambaran umum singkat fitur-fiturnya</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/rostelecom/blog/432166/">  Hari ini di situs asing bertema tentang Big Data Anda dapat menemukan penyebutan alat yang relatif baru untuk ekosistem Hadoop sebagai Apache NiFi.  Ini adalah alat ETL open source modern.  Arsitektur terdistribusi untuk pemuatan paralel yang cepat dan pemrosesan data, sejumlah besar plug-in untuk sumber dan transformasi, versi konfigurasi hanyalah sebagian dari kelebihannya.  Dengan semua kekuatannya, NiFi tetap cukup mudah digunakan. <br><br><img src="https://habrastorage.org/webt/9b/zs/ri/9bzsrib2emb_rcdq1cj-d8nubbe.png" alt="gambar"><br><br>  Kami di Rostelecom berusaha mengembangkan kerja dengan Hadoop, jadi kami telah mencoba dan menghargai keunggulan Apache NiFi dibandingkan solusi lain.  Dalam artikel ini saya akan memberi tahu Anda bagaimana alat ini menarik kami dan bagaimana kami menggunakannya. <br><a name="habracut"></a><br><h2>  Latar belakang </h2><br>  Belum lama berselang, kami dihadapkan pada pilihan solusi untuk memuat data dari sumber eksternal ke dalam cluster Hadoop.  Untuk waktu yang lama, kami menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Flume</a> untuk memecahkan masalah seperti itu.  Tidak ada keluhan tentang Flume secara keseluruhan, kecuali beberapa poin yang tidak cocok untuk kami. <br><br>  <i>Hal pertama</i> yang kami, sebagai administrator, tidak suka adalah menulis konfigurasi Flume untuk melakukan unduhan sepele berikutnya tidak dapat dipercayakan kepada pengembang atau analis yang tidak terbenam dalam seluk-beluk alat ini.  Menghubungkan setiap sumber baru diperlukan intervensi wajib dari tim administrasi. <br>  <i>Poin kedua</i> adalah toleransi kesalahan dan penskalaan.  Untuk unduhan berat, misalnya, melalui syslog, perlu mengkonfigurasi beberapa agen Flume dan mengatur penyeimbang di depannya.  Semua ini kemudian harus dipantau dan dipulihkan dalam hal terjadi kegagalan. <br>  <i>Ketiga</i> , Flume tidak mengizinkan mengunduh data dari berbagai DBMS dan bekerja dengan beberapa protokol lain di luar kotak.  Tentu saja, di hamparan jaringan yang luas, Anda dapat menemukan cara untuk membuat Flume bekerja dengan Oracle atau SFTP, tetapi mendukung sepeda seperti itu sama sekali tidak menyenangkan.  Untuk memuat data dari Oracle yang sama, kami harus menggunakan alat lain - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Sqoop</a> . <br>  Terus terang, pada dasarnya saya adalah orang yang malas, dan saya tidak ingin mendukung kebun binatang solusi sama sekali.  Dan saya tidak suka bahwa semua pekerjaan ini harus dilakukan sendiri. <br><br>  Tentu saja ada solusi yang sangat kuat di pasar ETL-tools yang dapat bekerja dengan Hadoop.  Ini termasuk Informatica, IBM Datastage, SAS, dan Pentaho Data Integration.  Ini adalah yang paling sering didengar dari rekan kerja di bengkel dan yang pertama kali muncul di benak Anda.  Omong-omong, kami menggunakan IBM DataStage untuk ETL pada solusi dari kelas Gudang Data.  Tetapi itu terjadi secara historis bahwa tim kami tidak dapat menggunakan DataStage untuk unduhan di Hadoop.  Sekali lagi, kami tidak memerlukan kekuatan penuh solusi tingkat ini untuk melakukan konversi dan unduhan data yang cukup sederhana.  Apa yang kami butuhkan adalah solusi dengan dinamika pengembangan yang baik, dapat bekerja dengan banyak protokol dan memiliki antarmuka yang nyaman dan intuitif yang tidak hanya dapat ditangani oleh administrator yang mengerti semua seluk beluknya, tetapi juga pengembang dengan analis, yang sering kali bagi kami pelanggan dari data itu sendiri. <br><br>  Seperti yang Anda lihat dari judulnya, kami memecahkan masalah di atas dengan Apache NiFi. <br><br><h2>  Apa itu Apache NiFi? </h2><br>  Nama NiFi berasal dari "Niagara Files."  Proyek ini dikembangkan oleh Badan Keamanan Nasional AS selama delapan tahun, dan pada November 2014 kode sumbernya dibuka dan dipindahkan ke Yayasan Perangkat Lunak Apache sebagai bagian dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Program Transfer Teknologi NSA</a> . <br><br>  NiFi adalah alat ETL / ELT open source yang dapat bekerja dengan banyak sistem, dan tidak hanya kelas Big Data dan Data Warehouse.  Berikut adalah beberapa di antaranya: HDFS, Hive, HBase, Solr, Cassandra, MongoDB, ElastcSearch, Kafka, RabbitMQ, Syslog, HTTPS, SFTP.  Anda dapat melihat daftar lengkap di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> resmi. <br><br>  Bekerja dengan DBMS spesifik diimplementasikan dengan menambahkan driver JDBC yang sesuai.  Ada API untuk menulis modul Anda sebagai penerima atau pengonversi data tambahan.  Contohnya dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><h2>  Fitur utama </h2><br>  NiFi menggunakan antarmuka web untuk membuat DataFlow.  Seorang analis yang baru-baru ini mulai bekerja dengan Hadoop, pengembang, dan admin berjanggut akan mengatasinya.  Dua yang terakhir dapat berinteraksi tidak hanya dengan "persegi panjang dan panah", tetapi juga dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">REST API</a> untuk mengumpulkan statistik, memantau dan mengelola komponen DataFlow. <br><br><img src="https://habrastorage.org/webt/zw/dx/iq/zwdxiqh9ovilvva4tak-stj5jpc.png" alt="gambar"><br>  <i>Manajemen Berbasis Web NiFi</i> <br><br>  Di bawah ini saya akan menunjukkan beberapa contoh DataFlow untuk melakukan beberapa operasi umum. <br><br><img src="https://habrastorage.org/webt/jz/cw/v_/jzcwv_nu7infyyarte3skiwvayi.png" alt="gambar"><br>  <i>Contoh mengunduh file dari server SFTP ke HDFS</i> <br><br>  Dalam contoh ini, prosesor ListSFTP melakukan daftar file di server jauh.  Hasil dari daftar ini digunakan untuk memuat file paralel oleh semua node cluster oleh prosesor FetchSFTP.  Setelah itu, atribut ditambahkan ke setiap file, diperoleh dengan menguraikan namanya, yang kemudian digunakan oleh prosesor PutHDFS saat menulis file ke direktori akhir. <br><br><img src="https://habrastorage.org/webt/v-/ei/op/v-eiopqny5-jao0kaqlyexduvx0.png" alt="gambar"><br>  <i>Contoh mengunduh data syslog di Kafka dan HDFS</i> <br><br>  Di sini, menggunakan prosesor ListenSyslog, kita mendapatkan aliran pesan input.  Setelah itu, atribut tentang waktu kedatangan mereka di NiFi dan nama skema di Avro Schema Registry ditambahkan ke setiap grup pesan.  Selanjutnya, cabang pertama dikirim ke input prosesor QueryRecord, yang, berdasarkan skema yang ditentukan, membaca data dan mem-parsingnya menggunakan SQL, dan kemudian mengirimkannya ke Kafka.  Cabang kedua dikirim ke prosesor MergeContent, yang mengumpulkan data selama 10 menit, dan kemudian memberikannya ke prosesor berikutnya untuk konversi ke format Parket dan merekam ke HDFS. <br><br>  Berikut ini adalah contoh bagaimana Anda bisa mendesain DataFlow: <br><img src="https://habrastorage.org/webt/43/kd/2-/43kd2-43rovwudvvoi3sm8hdmuk.png" alt="gambar"><br>  <i>Unduh data syslog ke Kafka dan HDFS.</i>  <i>Menghapus data di Hive</i> <br><br>  Sekarang tentang konversi data.  NiFi memungkinkan Anda mem-parsing data dengan data biasa, menjalankan SQL di dalamnya, memfilter dan menambahkan bidang, dan mengonversi satu format data ke yang lain.  Ini juga memiliki bahasa ekspresi sendiri, kaya akan berbagai operator dan fungsi bawaan.  Dengan itu, Anda dapat menambahkan variabel dan atribut ke data, membandingkan dan menghitung nilai, menggunakannya nanti dalam pembentukan berbagai parameter, seperti jalur untuk menulis ke HDFS atau SQL-query di Hive.  Baca lebih lanjut di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><img src="https://habrastorage.org/webt/a4/7m/b_/a47mb_i_f2mzkfezluoq6qrt6-0.png" alt="gambar"><br>  <i>Contoh menggunakan variabel dan fungsi dalam prosesor UpdateAttribute</i> <br><br>  Pengguna dapat melacak jalur penuh data, mengamati perubahan konten dan atributnya. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a69/e09/44a/a69e0944abb4bb44f3863653994cd891.png"><br>  <i>Visualisasi rantai DataFlow</i> <br><br><img src="https://habrastorage.org/webt/sc/u3/ih/scu3ihzv1nwydvwfjc4ks9yvfoe.png" alt="gambar"><br>  <i>Lihat atribut konten dan data</i> <br><br>  Untuk versi DataFlow ada layanan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Registry NiFi</a> terpisah.  Dengan mengaturnya, Anda mendapatkan kemampuan untuk mengelola perubahan.  Anda dapat menjalankan perubahan lokal, memutar kembali, atau mengunduh versi sebelumnya. <br><br><img src="https://habrastorage.org/webt/ci/uz/ge/ciuzgeuazknrhqm5peopzmekiuo.png" alt="gambar"><br>  <i>Menu Kontrol Versi</i> <br><br>  Di NiFi, Anda dapat mengontrol akses ke antarmuka web dan pemisahan hak pengguna.  Mekanisme otentikasi berikut saat ini didukung: <br><br><ul><li>  Berbasis Sertifikat <br></li><li>  Berdasarkan nama pengguna dan kata sandi melalui LDAP dan Kerberos <br></li><li>  Melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Knox</a> <br></li><li>  Melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenID Connect</a> <br></li></ul><br>  Penggunaan simultan beberapa mekanisme sekaligus tidak didukung.  Untuk mengotorisasi pengguna dalam sistem, FileUserGroupProvider dan LdapUserGroupProvider digunakan.  Baca lebih lanjut tentang ini di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Seperti yang saya katakan, NiFi dapat bekerja dalam mode cluster.  Ini memberikan toleransi kesalahan dan memungkinkan penskalaan beban horizontal.  Tidak ada master node yang diperbaiki secara statis.  Sebaliknya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Zookeeper</a> memilih satu simpul sebagai koordinator dan satu simpul sebagai primer.  Koordinator menerima informasi tentang status mereka dari node lain dan bertanggung jawab untuk koneksi dan pemutusan mereka dari cluster. <br>  Primer-simpul digunakan untuk memulai prosesor terisolasi, yang seharusnya tidak berjalan di semua node secara bersamaan. <br><br><img src="https://habrastorage.org/webt/1w/io/mv/1wiomvdhjbh_ewwa73dgl-yjitg.png" alt="gambar"><br>  <i>Operasi NiFi dalam sebuah cluster</i> <br><br><img src="https://habrastorage.org/webt/du/ty/ea/dutyeaditjnc6bq_qgta6xcexrk.png" alt="gambar"><br>  <i>Memuat distribusi dengan node cluster menggunakan prosesor PutHDFS sebagai contoh</i> <br><br><h2>  Penjelasan Singkat tentang Arsitektur dan Komponen NiFi </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/d68/920/1de/d689201de4c392c562e807fc279cd2ab.png"><br>  <i>Arsitektur Instansi NiFi</i> <br><br>  NiFi didasarkan pada konsep "Flow Based Programming" ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">FBP</a> ).  Berikut adalah konsep dan komponen dasar yang dihadapi setiap pengguna: <br><br>  <b>FlowFile</b> - entitas yang mewakili objek dengan konten dari nol atau lebih byte dan atribut yang sesuai.  Ini bisa berupa data itu sendiri (misalnya, aliran pesan Kafka), atau hasil prosesor (PutSQL, misalnya), yang tidak berisi data seperti itu, tetapi hanya atribut yang dihasilkan sebagai hasil dari kueri.  Atribut adalah metadata FlowFile. <br><br>  <b>FlowFile Processor</b> adalah esensi yang melakukan pekerjaan dasar di NiFi.  Prosesor, sebagai suatu peraturan, memiliki satu atau beberapa fungsi untuk bekerja dengan FlowFile: membuat, membaca / menulis dan mengubah konten, membaca / menulis / mengubah atribut, perutean.  Sebagai contoh, prosesor ListenSyslog menerima data menggunakan protokol syslog, membuat FlowFiles dengan atribut syslog.version, syslog.hostname, syslog.sender dan lainnya.  Prosesor RouteOnAttribute membaca atribut input FlowFile dan memutuskan untuk mengalihkannya ke koneksi yang sesuai dengan prosesor lain, tergantung pada nilai atribut. <br><br>  <b>Connection</b> - menyediakan koneksi flowFile dan transfer antara berbagai prosesor dan beberapa entitas NiFi lainnya.  Koneksi membuat FlowFile dalam antrian, dan kemudian meneruskannya ke rantai.  Anda dapat mengonfigurasi bagaimana FlowFiles dipilih dari antrian, masa pakainya, jumlah maksimum dan ukuran maksimum semua objek dalam antrian. <br><br>  <b>Grup Proses</b> - satu set prosesor, koneksi mereka dan elemen DataFlow lainnya.  Ini adalah mekanisme untuk mengatur banyak komponen menjadi satu struktur logis.  Membantu menyederhanakan pemahaman tentang DataFlow.  Input / Output Port digunakan untuk menerima dan mengirim data dari Grup Proses.  Baca lebih lanjut tentang penggunaannya di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  <b>Repositori FlowFile</b> adalah tempat NiFi menyimpan semua informasi yang diketahuinya tentang setiap FlowFile yang ada dalam sistem. <br><br>  <b>Content Repository</b> - repositori tempat konten semua FlowFiles berada, mis.  data yang dikirimkan itu sendiri. <br><br>  <b>Provenance Repository</b> - Berisi cerita tentang masing-masing FlowFile.  Setiap kali ketika suatu peristiwa terjadi dengan FlowFile (pembuatan, perubahan, dll.), Informasi yang sesuai dimasukkan ke dalam repositori ini. <br><br>  <b>Server Web</b> - menyediakan antarmuka web dan API REST. <br><br><h2>  Kesimpulan </h2><br>  Dengan NiFi, Rostelecom mampu meningkatkan mekanisme pengiriman data ke Data Lake di Hadoop.  Secara umum, seluruh proses menjadi lebih mudah dan dapat diandalkan.  Hari ini, saya dapat dengan yakin mengatakan bahwa NiFi sangat bagus untuk mengunduh ke Hadoop.  Kami tidak memiliki masalah dalam operasinya. <br><br>  Omong-omong, NiFi adalah bagian dari distribusi Arus Data Hortonworks dan secara aktif dikembangkan oleh Hortonworks sendiri.  Ia juga memiliki sub proyek Apache MiNiFi yang menarik, yang memungkinkan Anda mengumpulkan data dari berbagai perangkat dan mengintegrasikannya ke dalam DataFlow di dalam NiFi. <br><br><h2>  Informasi Tambahan Tentang NiFi </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Halaman</a> dokumentasi proyek resmi <br></li><li>  Kumpulan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel menarik</a> tentang NiFi dari salah satu peserta proyek <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Blog tentang</a> salah satu pengembang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">NiFi</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Artikel</a> Hortonworks <br></li></ul><br>  Mungkin itu saja.  Terima kasih atas perhatiannya.  Tulis di komentar jika Anda memiliki pertanyaan.  Saya akan menjawabnya dengan senang hati. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id432166/">https://habr.com/ru/post/id432166/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id432154/index.html">Akses Bersyarat sebagai Mekanisme Kontrol Akses</a></li>
<li><a href="../id432156/index.html">2GIS baru - terhubung ke pengujian publik</a></li>
<li><a href="../id432158/index.html">Menggunakan JIRA dan Confluence dalam Proyek Besar</a></li>
<li><a href="../id432160/index.html">Video dari Android Kolesa Mobile: tentang pengembangan modular, UI yang digerakkan oleh backend dan integrasi berkelanjutan</a></li>
<li><a href="../id432162/index.html">“Kami mencoba untuk memberikan kisah kehidupan nyata”: tentang program Heisenbug 2018 Moscow</a></li>
<li><a href="../id432168/index.html">Otoritas Tiongkok mengumpulkan informasi dari kendaraan listrik warga negara</a></li>
<li><a href="../id432170/index.html">Transport pusat data dalam 14.400 detik</a></li>
<li><a href="../id432172/index.html">Undangan berbahaya, atau Cara pertempuran memuat untuk email phishing</a></li>
<li><a href="../id432174/index.html">Bagaimana mengembangkan produk perangkat lunak secara kompeten dan efektif</a></li>
<li><a href="../id432176/index.html">Bagaimana kami menggandakan kecepatan bekerja dengan Float di Mono</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>