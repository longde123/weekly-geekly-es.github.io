<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÜ üë®üèº‚Äçüíº üòù Neuronale Netze, grundlegende Funktionsprinzipien, Vielfalt und Topologie üñçÔ∏è üéôÔ∏è üÜô</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neuronale Netze haben das Gebiet der Mustererkennung revolutioniert, werden jedoch aufgrund der nicht offensichtlichen Interpretierbarkeit des Funktio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze, grundlegende Funktionsprinzipien, Vielfalt und Topologie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416071/">  Neuronale Netze haben das Gebiet der Mustererkennung revolutioniert, werden jedoch aufgrund der nicht offensichtlichen Interpretierbarkeit des Funktionsprinzips nicht in Bereichen wie Medizin und Risikobewertung eingesetzt.  Es erfordert eine visuelle Darstellung des Netzwerks, wodurch es keine Black Box, sondern zumindest ‚Äûdurchscheinend‚Äú wird.  <b>Cristopher Olah demonstrierte in Neuronale Netze, Mannigfaltigkeiten und Topologie die Prinzipien des neuronalen Netzbetriebs und verband sie mit der mathematischen Theorie der Topologie und Diversit√§t, die als Grundlage f√ºr diesen Artikel diente.</b>  Um den Betrieb eines neuronalen Netzwerks zu demonstrieren, werden niedrigdimensionale tiefe neuronale Netzwerke verwendet. <br><br>  Das Verst√§ndnis des Verhaltens tiefer neuronaler Netze ist im Allgemeinen keine triviale Aufgabe.  Es ist einfacher, niedrigdimensionale tiefe neuronale Netze zu erforschen - Netze, in denen sich nur wenige Neuronen in jeder Schicht befinden.  F√ºr niedrigdimensionale Netzwerke k√∂nnen Sie Visualisierungen erstellen, um das Verhalten und das Training solcher Netzwerke zu verstehen.  Diese Perspektive wird ein tieferes Verst√§ndnis des Verhaltens neuronaler Netze vermitteln und die Verbindung beobachten, die neuronale Netze mit einem Feld der Mathematik kombiniert, das als Topologie bezeichnet wird. <br><br>  Daraus ergeben sich eine Reihe interessanter Dinge, einschlie√ülich der grundlegenden Untergrenzen f√ºr die Komplexit√§t eines neuronalen Netzwerks, das bestimmte Datens√§tze klassifizieren kann. <br><br>  Betrachten Sie das Prinzip des Netzwerks anhand eines Beispiels <br><a name="habracut"></a><br>  Beginnen wir mit einem einfachen Datensatz - zwei Kurven in einer Ebene.  Die Netzwerkaufgabe lernt, die zu Kurven geh√∂renden Punkte zu klassifizieren. <br><br><img src="https://habrastorage.org/webt/m4/od/cv/m4odcvui3bls-vx-zzfhprjatzg.png"><br><br>  Eine naheliegende M√∂glichkeit, das Verhalten eines neuronalen Netzwerks zu visualisieren und zu sehen, wie der Algorithmus alle m√∂glichen Objekte (in unserem Beispiel Punkte) aus einem Datensatz klassifiziert. <br><br>  Beginnen wir mit der einfachsten Klasse neuronaler Netze mit einer Eingabe- und Ausgabeschicht.  Ein solches Netzwerk versucht, zwei Datenklassen zu trennen, indem es sie durch eine Linie teilt. <br><br><img src="https://habrastorage.org/webt/ii/sz/eo/iiszeobw-_fe8mam77ku7o62g7g.png"><br><br>  Ein solches Netzwerk wird in der Praxis nicht verwendet.  Moderne neuronale Netze haben normalerweise mehrere Schichten zwischen ihrer Eingabe und Ausgabe, die als "versteckte" Schichten bezeichnet werden. <br><br><img src="https://habrastorage.org/webt/ej/bp/dw/ejbpdwhjcvzouhxnir9a-zeavye.jpeg"><br><br><h3>  Einfaches Netzwerkdiagramm </h3><br>  Wir visualisieren das Verhalten dieses Netzwerks und beobachten, was es mit verschiedenen Punkten in seinem Bereich macht.  Ein Hidden-Layer-Netzwerk trennt Daten von einer komplexeren Kurve als eine Linie. <br><br><img src="https://habrastorage.org/webt/wp/cr/vf/wpcrvf_a0tiqrhftmxhddovy8tk.png"><br><br>  Mit jeder Schicht transformiert das Netzwerk die Daten und erstellt eine neue Ansicht.  Wir k√∂nnen die Daten in jeder dieser Ansichten sehen und wie das Netzwerk mit einer verborgenen Schicht sie klassifiziert.  Wenn der Algorithmus die endg√ºltige Pr√§sentation erreicht, zieht das neuronale Netzwerk eine Linie durch die Daten (oder in h√∂heren Dimensionen - eine Hyperebene). <br><br>  In der vorherigen Visualisierung werden die Daten in einer Rohansicht ber√ºcksichtigt.  Sie k√∂nnen sich das vorstellen, indem Sie sich die Eingabeebene ansehen.  Betrachten Sie es nun, nachdem es in die erste Ebene konvertiert wurde.  Sie k√∂nnen sich das vorstellen, indem Sie die verborgene Ebene betrachten. <br>  Jede Messung entspricht der Aktivierung eines Neurons in der Schicht. <br><br><img src="https://habrastorage.org/webt/fc/ha/ch/fchachtyqdnmtcgaxonj6d1xiu0.png"><br><br>  Die verborgene Ebene wird in der Ansicht so trainiert, dass die Daten linear trennbar sind. <br><br>  <b>Kontinuierliches Layer-Rendering</b> <br><br>  In dem im vorherigen Abschnitt beschriebenen Ansatz lernen wir, Netzwerke zu verstehen, indem wir uns die Pr√§sentation ansehen, die jeder Schicht entspricht.  Dies gibt uns eine diskrete Liste von Ansichten. <br><br>  Der nicht triviale Teil ist zu verstehen, wie wir uns von einem zum anderen bewegen.  Gl√ºcklicherweise haben neuronale Netzwerkebenen Eigenschaften, die dies erm√∂glichen. <br>  Es gibt viele verschiedene Arten von Schichten, die in neuronalen Netzen verwendet werden. <br><br>  Betrachten Sie eine Tanh-Schicht f√ºr ein bestimmtes Beispiel.  Die Tanh-Tanh-Schicht (Wx + b) besteht aus: <br><br><ol><li>  Die lineare Transformation der "Gewichts" -Matrix W. </li><li>  √úbersetzung mit Vektor b </li><li>  Spot Anwendung von Tanh. </li></ol><br>  Wir k√∂nnen dies als kontinuierliche Transformation wie folgt darstellen: <br><br><img src="https://habrastorage.org/webt/cn/vl/rb/cnvlrblwsnmp9mxxzbawagywuj0.gif"><br><br>  Dieses Funktionsprinzip ist anderen Standardschichten sehr √§hnlich, die aus einer affinen Transformation bestehen, gefolgt von der punktweisen Anwendung einer monotonen Aktivierungsfunktion. <br>  Diese Methode kann verwendet werden, um komplexere Netzwerke zu verstehen.  Das folgende Netzwerk klassifiziert also zwei Spiralen, die sich mithilfe von vier verborgenen Schichten leicht verheddern.  Im Laufe der Zeit ist ersichtlich, dass sich das neuronale Netzwerk von einer Rohansicht auf eine h√∂here Ebene bewegt, die das Netzwerk zur Klassifizierung von Daten untersucht hat.  W√§hrend sich die Spiralen anf√§nglich verheddern, sind sie gegen Ende linear trennbar. <br><br><img src="https://habrastorage.org/webt/g0/-y/kg/g0-ykgrem8udnrx-xiua2rolthq.gif"><br><br>  Auf der anderen Seite das n√§chste Netzwerk, das ebenfalls mehrere Ebenen verwendet, aber nicht zwei Spiralen klassifizieren kann, die sich mehr verwickeln. <br><br><img src="https://habrastorage.org/webt/gv/f3/wc/gvf3wc_-a7yw2h-3odoobbsie7u.gif"><br><br>  Es ist zu beachten, dass diese Aufgaben eine begrenzte Komplexit√§t aufweisen, da niedrigdimensionale neuronale Netze verwendet werden.  Wenn breitere Netzwerke verwendet wurden, wurde die Probleml√∂sung vereinfacht. <br><br><h3>  Tang-Schichten </h3><br>  Jede Schicht streckt und komprimiert den Raum, schneidet aber nie, bricht nicht und faltet ihn nicht.  Intuitiv sehen wir, dass die topologischen Eigenschaften auf jeder Schicht erhalten bleiben. <br><br>  Solche Transformationen, die die Topologie nicht beeinflussen, werden als Homomorphismen bezeichnet (Wiki - Dies ist eine Abbildung des algebraischen Systems A, bei der die grundlegenden Operationen und grundlegenden Beziehungen erhalten bleiben).  Formal sind sie Bijektionen, die kontinuierliche Funktionen in beide Richtungen sind.  Bei einer bijektiven Zuordnung entspricht jedes Element einer Menge genau einem Element einer anderen Menge, und eine inverse Zuordnung mit derselben Eigenschaft wird definiert. <br><br>  <b>Der Satz</b> <br><br>  Schichten mit N Eing√§ngen und N Ausg√§ngen sind Homomorphismen, wenn die Gewichtsmatrix W nicht entartet ist.  (Sie m√ºssen vorsichtig mit der Domain und dem Bereich sein.) <br><br><div class="spoiler">  <b class="spoiler_title">Beweis:</b> <div class="spoiler_text">  1. Angenommen, W hat eine Determinante ungleich Null.  Dann ist es eine bijektive lineare Funktion mit einer linearen Inversen.  Lineare Funktionen sind stetig.  Die Multiplikation mit W ist also ein Hom√∂omorphismus. <br>  2. Abbildungen - Homomorphismen <br>  3. tanh (sowohl Sigmoid als auch Softplus, aber nicht ReLU) sind kontinuierliche Funktionen mit kontinuierlichen Inversen.  Sie sind Bijektionen, wenn wir vorsichtig mit dem Bereich und der Reichweite sind, die wir in Betracht ziehen.  Ihre Verwendung ist punktuell ein Homomorphismus. <br><br>  Wenn also W eine Determinante ungleich Null hat, ist die Faser hom√∂omorph. <br></div></div><br><h3>  Topologie und Klassifizierung </h3><br>  Betrachten Sie einen zweidimensionalen Datensatz mit zwei Klassen A, B‚äÇR2: <br><br>  A = {x |  d (x, 0) &lt;1/3} <br><br>  B = {x |  2/3 &lt;d (x, 0) &lt;1} <br><br><img src="https://habrastorage.org/webt/ow/2l/b1/ow2lb1ozxk5p4d-s4ho_lq6_ds8.png"><br><br>  A rot, B blau <br><br>  Voraussetzung: Ein neuronales Netzwerk kann diesen Datensatz unabh√§ngig von der Breite nicht ohne drei oder mehr versteckte Ebenen klassifizieren. <br><br>  Wie bereits erw√§hnt, entspricht die Klassifizierung mit einer Sigmoidfunktion oder einer Softmax-Schicht dem Versuch, die Hyperebene (oder in diesem Fall die Linie) zu finden, die A und B in der endg√ºltigen Darstellung trennt.  Mit nur zwei versteckten Schichten kann das Netzwerk Daten auf diese Weise topologisch nicht gemeinsam nutzen und ist zum Scheitern in diesem Datensatz verurteilt. <br>  In der n√§chsten Visualisierung beobachten wir eine latente Ansicht, w√§hrend das Netzwerk zusammen mit der Klassifizierungslinie trainiert. <br><br><img src="https://habrastorage.org/webt/ap/nt/xe/apntxeprybgn8yf_jchwskwes44.gif"><br><br>  Denn dieses Trainingsnetz reicht nicht aus, um ein hundertprozentiges Ergebnis zu erzielen. <br>  Der Algorithmus f√§llt in ein unproduktives lokales Minimum, kann jedoch eine Klassifizierungsgenauigkeit von ~ 80% erreichen. <br><br>  In diesem Beispiel gab es nur eine verborgene Ebene, die jedoch nicht funktionierte. <br>  Aussage.  Entweder ist jede Schicht ein Homomorphismus oder die Gewichtsmatrix der Schicht hat die Determinante 0. <br><br><div class="spoiler">  <b class="spoiler_title">Beweis:</b> <div class="spoiler_text">  Wenn dies ein Homomorphismus ist, ist A immer noch von B umgeben, und die Linie kann sie nicht trennen.  Angenommen, es hat eine Determinante von 0: Dann kollabiert der Datensatz auf einer Achse.  Da es sich um etwas handelt, das mit dem urspr√ºnglichen Datensatz hom√∂omorph ist, ist A von B umgeben. Wenn wir auf einer beliebigen Achse kollabieren, werden einige Punkte von A und B gemischt, was eine Unterscheidung unm√∂glich macht. <br></div></div><br>  Wenn wir ein drittes verstecktes Element hinzuf√ºgen, wird das Problem trivial.  Das neuronale Netzwerk erkennt die folgende Darstellung: <br><br><img src="https://habrastorage.org/webt/y1/p8/ol/y1p8olobp-zdo3shlaooa62hy8k.png"><br><br>  Die Ansicht erm√∂glicht es, Datens√§tze mit einer Hyperebene zu trennen. <br>  Um besser zu verstehen, was los ist, schauen wir uns einen noch einfacheren Datensatz an, der eindimensional ist: <br><br><img src="https://habrastorage.org/webt/ud/by/hr/udbyhrfnqgfdr9r9lvh4cg7apw0.png"><br><br>  A = [- 1 / 3,1 / 3] <br>  B = [- 1, ‚Äì2 / 3] ‚à™ [2 / 3,1] <br>  Ohne die Verwendung einer Ebene aus zwei oder mehr versteckten Elementen k√∂nnen wir diesen Datensatz nicht klassifizieren.  Wenn wir jedoch ein Netzwerk mit zwei Elementen verwenden, lernen wir, wie die Daten als gute Kurve dargestellt werden, mit der wir Klassen mithilfe einer Linie trennen k√∂nnen: <br><br><img src="https://habrastorage.org/webt/8_/w-/bo/8_w-boyjlwhtufafsnqpljuo8u0.gif"><br><br>  Was ist los?  Ein verstecktes Element lernt zu feuern, wenn x&gt; -1/2, und eines lernt zu feuern, wenn x&gt; 1/2.  Wenn der erste ausgel√∂st wird, aber nicht der zweite, wissen wir, dass wir uns in A befinden. <br><br><h3>  Sortenvermutung </h3><br>  Gilt dies f√ºr Datens√§tze aus der realen Welt, z. B. Bilds√§tze?  Wenn Sie die Diversit√§tshypothese ernst nehmen, denke ich, dass es wichtig ist. <br><br>  Die mehrdimensionale Hypothese lautet, dass nat√ºrliche Daten im Implantationsraum niedrigdimensionale Mannigfaltigkeiten bilden.  Es gibt sowohl theoretische [1] als auch experimentelle [2] Gr√ºnde zu der Annahme, dass dies wahr ist.  Wenn ja, besteht die Aufgabe des Klassifizierungsalgorithmus darin, das B√ºndel verschr√§nkter Verteiler zu trennen. <br><br>  In den vorherigen Beispielen hat eine Klasse die andere vollst√§ndig umgeben.  Es ist jedoch unwahrscheinlich, dass die Vielfalt der Hundebilder vollst√§ndig von einer Sammlung von Katzenbildern umgeben ist.  Es gibt jedoch noch andere, plausibelere topologische Situationen, die m√∂glicherweise noch auftreten, wie wir im n√§chsten Abschnitt sehen werden. <br><br><h3>  Verbindungen und Homotopien </h3><br>  Ein weiterer interessanter Datensatz sind die beiden verbundenen Tori A und B. <br><br><img src="https://habrastorage.org/webt/wl/er/ns/wlernsr6ibyfnuq2cbkepwgxxq8.png"><br><br>  Wie die vorherigen Datens√§tze, die wir untersucht haben, kann dieser Datensatz nicht ohne Verwendung von n + 1 Dimensionen, n√§mlich der vierten Dimension, geteilt werden. <br><br>  Verbindungen werden in der Knotentheorie auf dem Gebiet der Topologie untersucht.  Manchmal, wenn wir einen Zusammenhang sehen, ist nicht sofort klar, ob es sich um Inkoh√§renz handelt (viele Dinge, die sich verheddern, aber durch kontinuierliche Verformung getrennt werden k√∂nnen) oder nicht. <br><br><img src="https://habrastorage.org/webt/lg/1h/bu/lg1hbu1mehjrj872e61gd73_3vi.png"><br><br>  Relativ einfache Inkoh√§renz. <br><br>  Wenn ein neuronales Netzwerk, das Schichten mit nur drei Einheiten verwendet, es klassifizieren kann, ist es inkoh√§rent.  (Frage: K√∂nnen theoretisch alle Inkoh√§renzen mit nur drei Inkoh√§renzen √ºber das Netzwerk klassifiziert werden?) <br><br>  Aus Sicht dieses Knotens ist die kontinuierliche Visualisierung von Darstellungen, die von einem neuronalen Netzwerk erstellt wurden, ein Verfahren zum Aufl√∂sen von Verbindungen.  In der Topologie nennen wir diese Umgebungsisotopie zwischen der urspr√ºnglichen Verbindung und den getrennten. <br><br>  Formal ist die Isotopie des umgebenden Raums zwischen den Mannigfaltigkeiten A und B eine stetige Funktion F: [0,1] √ó X ‚Üí Y, so dass jedes Ft ein Hom√∂omorphismus von X zu seinem Bereich ist, F0 eine Identit√§tsfunktion ist und F1 A auf B abbildet. T. .e.  Ft geht kontinuierlich von der Karte A zu sich selbst, zur Karte A nach B. <br><br>  Satz: Es gibt eine Isotopie des umgebenden Raums zwischen dem Eingang und der Darstellung der Netzwerkebene, wenn: a) W nicht entartet ist, b) wir bereit sind, Neuronen auf die verborgene Schicht zu √ºbertragen, und c) es mehr als ein verborgenes Element gibt. <br><br><div class="spoiler">  <b class="spoiler_title">Beweis:</b> <div class="spoiler_text">  1. Der schwierigste Teil ist die lineare Transformation.  Um dies zu erm√∂glichen, muss W eine positive Determinante haben.  Unsere Pr√§misse ist, dass es nicht gleich Null ist, und wir k√∂nnen das Vorzeichen umkehren, wenn es negativ ist, indem wir zwei versteckte Neuronen vertauschen, und daher k√∂nnen wir garantieren, dass die Determinante positiv ist.  Der Raum der positiven Determinantenmatrizen ist verbunden, daher existiert p: [0,1] ‚Üí GLn ¬Æ 5, so dass p (0) = Id und p (1) = W. Mit k√∂nnen wir kontinuierlich von der Identit√§tsfunktion zur W-Transformation √ºbergehen Funktionen x ‚Üí p (t) x, wobei x zu jedem Zeitpunkt t mit einer kontinuierlich passierenden Matrix p (t) multipliziert wird. <br>  2. Mit der Funktion x ‚Üí x + tb k√∂nnen wir kontinuierlich von der Identit√§tsfunktion zur b-Map wechseln. <br>  3. Wir k√∂nnen kontinuierlich von der identischen Funktion zur punktweisen Verwendung von œÉ mit der Funktion √ºbergehen: x ‚Üí (1-t) x + tœÉ (x) <br></div></div><br>  Bisher ist es unwahrscheinlich, dass die Beziehungen, von denen wir gesprochen haben, in realen Daten auftreten, aber es gibt Verallgemeinerungen auf einer h√∂heren Ebene.  Es ist plausibel, dass solche Merkmale in realen Daten vorhanden sein k√∂nnen. <br><br>  Verbindungen und Knoten sind eindimensionale Mannigfaltigkeiten, aber wir ben√∂tigen vier Dimensionen, damit die Netzwerke alle entwirren k√∂nnen.  In √§hnlicher Weise kann ein noch h√∂herdimensionaler Raum erforderlich sein, um n-dimensionale Mannigfaltigkeiten erweitern zu k√∂nnen.  Alle n-dimensionalen Verteiler k√∂nnen in 2n + 2 Dimensionen erweitert werden.  [3] <br><br><h3>  Einfacher Ausstieg </h3><br>  Der einfache Weg ist, zu versuchen, die Verteiler auseinander zu ziehen und die Teile zu dehnen, die sich so verwickeln wie m√∂glich.  Obwohl dies einer echten L√∂sung nicht nahe kommt, kann eine solche L√∂sung eine relativ hohe Klassifizierungsgenauigkeit erreichen und ein akzeptables lokales Minimum darstellen. <br><br><img src="https://habrastorage.org/webt/7x/mf/fp/7xmffpy2eilztftxrcbv69xhsf4.png"><br><br>  Solche lokalen Minima sind absolut nutzlos, um topologische Probleme zu l√∂sen, aber topologische Probleme k√∂nnen eine gute Motivation f√ºr die Untersuchung dieser Probleme darstellen. <br><br>  Wenn wir dagegen nur an guten Klassifizierungsergebnissen interessiert sind, ist der Ansatz akzeptabel.  Wenn ein winziger Teil eines Datenverteilers in einem anderen Verteiler erfasst wird, ist dies ein Problem?  Es ist wahrscheinlich, dass es trotz dieses Problems m√∂glich sein wird, beliebig gute Klassifizierungsergebnisse zu erhalten. <br><br>  Verbesserte Schichten zur Manipulation von Verteilern? <br><br>  Es ist schwer vorstellbar, dass Standardschichten mit affinen Transformationen wirklich gut zur Manipulation von Mannigfaltigkeiten geeignet sind. <br><br>  Vielleicht ist es sinnvoll, eine v√∂llig andere Schicht zu haben, die wir in der Komposition mit traditionelleren verwenden k√∂nnen? <br><br>  Die Untersuchung eines Vektorfeldes mit einer Richtung, in die wir die Mannigfaltigkeit verschieben wollen, ist vielversprechend: <br><br><img src="https://habrastorage.org/webt/2z/iw/at/2ziwat9d2bjwlclnjrixwmm5llc.png"><br><br>  Und dann verformen wir den Raum basierend auf dem Vektorfeld: <br><br><img src="https://habrastorage.org/webt/ig/qb/dr/igqbdrqbcl3gy85kv4rzbhirflk.png"><br><br>  Man k√∂nnte das Vektorfeld an festen Punkten untersuchen (einfach einige feste Punkte aus dem Testdatensatz zur Verwendung als Anker nehmen) und irgendwie interpolieren. <br><br><div class="spoiler">  <b class="spoiler_title">Das obige Vektorfeld hat die Form:</b> <div class="spoiler_text">  P (x) = (v0f0 (x) + v1f1 (x)) / (1 + 0 (x) + f1 (x)) <br></div></div><br>  Wobei v0 und v1 Vektoren sind und f0 (x) und f1 (x) n-dimensionale Gau√üsche sind. <br><br><h3>  K-Nearest Neighbor Layers </h3><br>  Die lineare Trennbarkeit kann ein gro√üer und m√∂glicherweise unangemessener Bedarf an neuronalen Netzen sein.  Es ist nat√ºrlich, die Methode der k-n√§chsten Nachbarn (k-NN) zu verwenden.  Der Erfolg von k-NN h√§ngt jedoch stark von der Pr√§sentation ab, die es klassifiziert. Daher ist eine gute Pr√§sentation erforderlich, bevor das k-NN gut funktionieren kann. <br><br>  k-NN ist in Bezug auf die Darstellung, auf die es einwirkt, differenzierbar.  Auf diese Weise k√∂nnen wir das Netzwerk direkt trainieren, um k-NN zu klassifizieren.  Dies kann als eine Art "n√§chster Nachbar" -Schicht angesehen werden, die als Alternative zu Softmax fungiert. <br>  Wir m√∂chten nicht mit unserem gesamten Training f√ºr jede Mini-Party warnen, da dies ein sehr teures Verfahren sein wird.  Der angepasste Ansatz besteht darin, jedes Element des Mini-Loses basierend auf den Klassen der anderen Elemente des Mini-Loses zu klassifizieren, wobei jedes Einheitsgewicht geteilt durch den Abstand vom Klassifizierungsziel angegeben wird. <br><br>  Leider verringert die Verwendung von k-NN selbst bei komplexen Architekturen die Fehlerwahrscheinlichkeit - und die Verwendung einfacherer Architekturen verschlechtert die Ergebnisse. <br><br><h3>  Fazit </h3><br>  Topologische Eigenschaften von Daten, wie z. B. Beziehungen, k√∂nnen die lineare Aufteilung von Klassen mithilfe niedrigdimensionaler Netzwerke unabh√§ngig von der Tiefe unm√∂glich machen.  Auch in F√§llen, in denen dies technisch m√∂glich ist.  Zum Beispiel Spiralen, deren Trennung sehr schwierig sein kann. <br><br>  F√ºr eine genaue Datenklassifizierung ben√∂tigen neuronale Netze breite Schichten.  Dar√ºber hinaus sind die traditionellen Schichten des neuronalen Netzwerks schlecht geeignet, um wichtige Manipulationen mit Mannigfaltigkeiten darzustellen.  Selbst wenn wir die Gewichte manuell einstellen, w√§re es schwierig, die gew√ºnschten Transformationen kompakt darzustellen. <br><br><div class="spoiler">  <b class="spoiler_title">Links zu Quellen und Erkl√§rungen</b> <div class="spoiler_text">  [1] Viele der nat√ºrlichen Transformationen, die Sie m√∂glicherweise an einem Bild ausf√ºhren m√∂chten, z. B. das √úbersetzen oder Skalieren eines Objekts oder das √Ñndern der Beleuchtung, w√ºrden im Bildraum kontinuierliche Kurven bilden, wenn Sie sie kontinuierlich ausf√ºhren. <br><br>  [2] Carlsson et al.  fanden heraus, dass lokale Flecken von Bildern eine kleine Flasche bilden. <br>  [3] Dieses Ergebnis wird im Wikipedia-Unterabschnitt zu Isotopy-Versionen erw√§hnt. <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416071/">https://habr.com/ru/post/de416071/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416059/index.html">Mobio spricht mit Daniil Shuleiko (Yandex.Taxi) √ºber die Fusion mit Uber, den Taximarkt und den Wettbewerb</a></li>
<li><a href="../de416061/index.html">So lala sehe ich alles</a></li>
<li><a href="../de416063/index.html">Verhandlungen √ºber Russen sind nirgends zu verzeichnen</a></li>
<li><a href="../de416067/index.html">√úbersicht √ºber die Sicherheitsanf√§lligkeit in Mikrotik Winbox. Oder eine gro√üe Datei</a></li>
<li><a href="../de416069/index.html">Verlustfreie ElasticSearch-Datenmigration</a></li>
<li><a href="../de416073/index.html">Ein einfacher Kryptow√§hrungs-Handelsbot</a></li>
<li><a href="../de416075/index.html">Der FSB m√∂chte die Verantwortung f√ºr die versteckte Verwendung von Diktierger√§ten und Kameras in Smartphones √ºbernehmen [und nicht nur]</a></li>
<li><a href="../de416077/index.html">PlantUML - Alles, was Business Analysten zum Erstellen von Diagrammen in der Softwaredokumentation ben√∂tigen</a></li>
<li><a href="../de416079/index.html">Corona Native f√ºr Android - Verwenden von benutzerdefiniertem Java-Code in einem in Corona geschriebenen Spiel</a></li>
<li><a href="../de416081/index.html">Mit der R√ºckkehr nach Habr stimmt noch etwas nicht</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>