<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úçüèª üìø üìØ Secuencia a secuencia Modelos de la Parte 2 ‚õìÔ∏è üåì üêè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos! 

 La segunda parte de la traducci√≥n, que publicamos hace un par de semanas, en preparaci√≥n para el lanzamiento de la segunda secuencia ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Secuencia a secuencia Modelos de la Parte 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/432302/"> Hola a todos! <br><br>  La segunda parte de la traducci√≥n, que publicamos hace un par de semanas, en preparaci√≥n para el lanzamiento de la segunda secuencia del curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Cient√≠fico de datos"</a> .  A continuaci√≥n hay otro material interesante y una lecci√≥n abierta. <br><br>  Mientras tanto, nos adentramos en la jungla de modelos. <br><br>  <b>Modelo de traducci√≥n neuronal</b> <br><br>  Si bien el n√∫cleo del modelo de secuencia a secuencia es creado por funciones de <code>tensorflow/tensorflow/python/ops/seq2seq.py</code> , todav√≠a hay un par de trucos utilizados en nuestro modelo de traducci√≥n en <code>models/tutorials/rnn/translate/seq2seq_model.py</code> , acerca de Vale la pena mencionar. <br><br><img src="https://habrastorage.org/webt/3z/s9/fy/3zs9fym7zcpeqweylxlqhknznko.png"><a name="habracut"></a><br><br>  <b>Softmax muestreado y proyecci√≥n de salida</b> <br><br>  Como se mencion√≥ anteriormente, queremos usar el softmax muestreado para trabajar con un diccionario de salida grande.  Para decodificar a partir de √©l, debe seguir la proyecci√≥n de la salida.  Tanto la p√©rdida softmax muestreada como la proyecci√≥n de salida se generan mediante el siguiente c√≥digo en <code>seq2seq_model.py</code> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> num_samples &gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> num_samples &lt; self.target_vocab_size: w_t = tf.get_variable(<span class="hljs-string"><span class="hljs-string">"proj_w"</span></span>, [self.target_vocab_size, size], dtype=dtype) w = tf.transpose(w_t) b = tf.get_variable(<span class="hljs-string"><span class="hljs-string">"proj_b"</span></span>, [self.target_vocab_size], dtype=dtype) output_projection = (w, b) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sampled_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, inputs)</span></span></span><span class="hljs-function">:</span></span> labels = tf.reshape(labels, [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-comment"><span class="hljs-comment"># We need to compute the sampled_softmax_loss using 32bit floats to # avoid numerical instabilities. local_w_t = tf.cast(w_t, tf.float32) local_b = tf.cast(b, tf.float32) local_inputs = tf.cast(inputs, tf.float32) return tf.cast( tf.nn.sampled_softmax_loss( weights=local_w_t, biases=local_b, labels=labels, inputs=local_inputs, num_sampled=num_samples, num_classes=self.target_vocab_size), dtype)</span></span></code> </pre><br>  Primero, tenga en cuenta que solo creamos softmax muestreado si el n√∫mero de muestras (512 por defecto) es menor que el tama√±o del diccionario de destino.  Para diccionarios menores de 512, es mejor usar la p√©rdida est√°ndar de softmax. <br><br>  Luego, cree una proyecci√≥n de la salida.  Este es un par que consiste en una matriz de pesos y un vector de desplazamiento.  Cuando se usa, la celda rnn devuelve los vectores de forma del n√∫mero de muestras de entrenamiento por <code>size</code> , y no el n√∫mero de muestras de entrenamiento por <code>target_vocab_size</code> .  Para restaurar logits, debe multiplicarlo por la matriz de pesos y agregar un desplazamiento, que ocurre en las l√≠neas 124-126 en <code>seq2seq_model.py</code> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output_projection <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(len(buckets)): self.outputs[b] = [tf.matmul(output, output_projection[<span class="hljs-number"><span class="hljs-number">0</span></span>]) + output_projection[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ...]</code> </pre> <br>  <b>Bucketing y acolchado</b> <br><br>  Adem√°s del softmax muestreado, nuestro modelo de traducci√≥n tambi√©n utiliza el <i>bucketing</i> , un m√©todo que le permite administrar de manera eficiente oraciones de diferentes longitudes.  Para comenzar, explica el problema.  Al traducir del ingl√©s al franc√©s, tenemos oraciones en ingl√©s de diferentes longitudes L1 en la entrada y oraciones en franc√©s de diferentes longitudes L1 en la salida.  Como la oraci√≥n en ingl√©s se transmite como <code>encoder_inputs</code> , y la oraci√≥n en franc√©s se muestra como <code>decoder_inputs</code> (con el prefijo del s√≠mbolo GO), es necesario crear un modelo seq2seq para cada par (L1, L2 + 1) de longitudes de oraciones en ingl√©s y franc√©s.  Como resultado, obtenemos un gr√°fico enorme que consta de muchos subgr√°ficos similares.  Por otro lado, podemos "rellenar" cada oraci√≥n con caracteres PAD especiales.  Y luego solo necesitamos un modelo seq2seq para longitudes "empaquetadas".  Pero dicho modelo ser√° ineficaz en oraciones cortas: debe codificar y decodificar muchos caracteres PAD in√∫tiles. <br><br>  Como compromiso entre crear un gr√°fico para cada par de longitudes y rellenarlo en una sola longitud, usamos un cierto n√∫mero de cubos y rellenamos cada oraci√≥n a la longitud del grupo anterior.  En <code>translate.py</code> usamos los siguientes grupos por defecto. <br><br><pre> <code class="python hljs">buckets = [(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>), (<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>), (<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>), (<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>)]</code> </pre> <br><br>  Por lo tanto, si una oraci√≥n en ingl√©s con 3 tokens llega a la entrada, y la oraci√≥n en franc√©s correspondiente contiene 6 tokens en la salida, ir√°n al primer grupo y se completar√°n hasta la longitud 5 en la entrada del codificador y la longitud 10 en la entrada del decodificador.  Y si hay 8 tokens en la oferta en ingl√©s, y en el franc√©s 18 correspondiente, no caer√°n en el grupo (10, 15) y se transferir√°n al grupo (20, 25), es decir, la oferta en ingl√©s aumentar√° a 20 tokens, y la francesa a 25. <br><br>  Recuerde que al crear la entrada del decodificador, agregamos el car√°cter <code>GO</code> especial a la entrada.  Esto sucede en la funci√≥n <code>get_batch()</code> en <code>seq2seq_model.py</code> , que tambi√©n voltea la oraci√≥n en ingl√©s.  La inversi√≥n de la entrada ayud√≥ a mejorar los resultados del modelo de traducci√≥n neural de <a href="">Sutskever et al., 2014 (pdf).</a>  Para finalmente resolverlo, imagine que hay una oraci√≥n "voy". En la entrada, dividida en tokens <code>["I", "go", "."]</code> , Y en la salida hay una oraci√≥n "Je vais", dividida en tokens <code>["Je", "vais", "."]</code> .  Se agregar√°n al grupo (5, 10), con una representaci√≥n del codificador de entrada <code>[PAD PAD "." "go" "I"]</code>  <code>[PAD PAD "." "go" "I"]</code> y la entrada del decodificador <code>[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]</code>  <code>[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]</code> . <br><br>  <b>Ejecutarlo</b> <br><br>  Para entrenar el modelo descrito anteriormente, necesitar√° un gran cuerpo anglo-franc√©s.  Para la capacitaci√≥n, utilizaremos el cuerpo 10 ^ 9 franc√©s-ingl√©s del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sitio web WMT'15</a> , y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">probaremos</a> las noticias del mismo sitio como muestra de trabajo.  Ambos conjuntos de datos se <code>train_dir</code> en <code>train_dir</code> cuando se <code>train_dir</code> el siguiente comando. <br><br><pre> <code class="plaintext hljs">python translate.py --data_dir [your_data_directory] --train_dir [checkpoints_directory] --en_vocab_size=40000 --fr_vocab_size=40000</code> </pre> <br>  Necesitar√° 18 GB de espacio en el disco duro y varias horas para preparar el edificio de capacitaci√≥n.  El caso se desempaqueta, los archivos de diccionario se crean en <code>data_dir,</code> y luego el caso se tokeniza y se convierte en identificadores enteros.  Presta atenci√≥n a los par√°metros responsables del tama√±o del diccionario.  En el ejemplo anterior, todas las palabras fuera de las 40,000 palabras usadas con mayor frecuencia se convertir√°n en un token UNK que representa una palabra desconocida.  Por lo tanto, al cambiar el tama√±o del diccionario, el binario volver√° a reformar la carcasa mediante el token-id.  Despu√©s de la preparaci√≥n de datos comienza el entrenamiento. <br><br>  Los valores especificados en la <code>translate</code> son muy altos por defecto.  Los modelos grandes que aprenden durante mucho tiempo muestran buenos resultados, pero puede llevar demasiado tiempo o demasiada memoria de GPU.  Puede especificar un modelo de entrenamiento m√°s peque√±o, como en el ejemplo a continuaci√≥n. <br><br><pre> <code class="plaintext hljs">python translate.py --data_dir [your_data_directory] --train_dir [checkpoints_directory] --size=256 --num_layers=2 --steps_per_checkpoint=50</code> </pre> <br>  El comando anterior entrenar√° el modelo con dos capas (hay 3 por defecto), cada una de las cuales tiene 256 unidades (1024 por defecto), con un punto de control cada 50 pasos (200 por defecto).  Experimente con estas opciones para ver qu√© modelo de tama√±o es el adecuado para su GPU. <br><br>  Durante el entrenamiento, cada paso del binario <code>steps_per_checkpoin</code> t proporcionar√° estad√≠sticas sobre los pasos anteriores.  Con los par√°metros predeterminados (3 capas de tama√±o 1024), el primer mensaje es el siguiente: <br><br><pre> <code class="plaintext hljs">global step 200 learning rate 0.5000 step-time 1.39 perplexity 1720.62 eval: bucket 0 perplexity 184.97 eval: bucket 1 perplexity 248.81 eval: bucket 2 perplexity 341.64 eval: bucket 3 perplexity 469.04 global step 400 learning rate 0.5000 step-time 1.38 perplexity 379.89 eval: bucket 0 perplexity 151.32 eval: bucket 1 perplexity 190.36 eval: bucket 2 perplexity 227.46 eval: bucket 3 perplexity 238.66</code> </pre> <br>  Tenga en cuenta que cada paso lleva un poco menos de 1,4 segundos, lo que deja perpleja la muestra de entrenamiento y deja perpleja la muestra de trabajo en cada grupo.  Despu√©s de aproximadamente 30 mil pasos, vemos c√≥mo los perplejos de las oraciones cortas (grupos 0 y 1) se vuelven inequ√≠vocos.  El edificio de entrenamiento contiene alrededor de 22 millones de oraciones, una iteraci√≥n (una serie de datos de entrenamiento) toma alrededor de 340 mil pasos con una cantidad de 64 muestras de entrenamiento. En esta etapa, el modelo puede usarse para traducir oraciones en ingl√©s al franc√©s usando la opci√≥n <code>--decode</code> . <br><br><pre> <code class="plaintext hljs">python translate.py --decode --data_dir [your_data_directory] --train_dir [checkpoints_directory] Reading model parameters from /tmp/translate.ckpt-340000 &gt; Who is the president of the United States? Qui est le pr√©sident des √âtats-Unis ?</code> </pre> <br>  <b>Que sigue</b> <br><br>  El ejemplo anterior muestra c√≥mo crear su propio traductor ingl√©s-franc√©s de principio a fin.  Ejec√∫telo y vea c√≥mo funciona el modelo.  La calidad es aceptable, pero no se puede obtener un modelo de traducci√≥n ideal con los par√°metros predeterminados.  Aqu√≠ hay algunas cosas que puede mejorar. <br><br>  Primero, usamos tokenizaci√≥n primitiva, la funci√≥n b√°sica de <code>basic_tokenizer</code> en <code>data_utils</code> .  Se puede encontrar un mejor tokenizador en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web de WMT'15</a> .  Si lo usa y un diccionario grande, puede lograr mejores traducciones. <br><br>  Adem√°s, los par√°metros predeterminados del modelo de traducci√≥n no est√°n configurados perfectamente.  Puede intentar cambiar la velocidad de aprendizaje, la atenuaci√≥n, la inicializaci√≥n de los pesos del modelo.  Tambi√©n puede reemplazar el <code>GradientDescentOptimizer</code> est√°ndar en <code>seq2seq_model.py</code> con algo m√°s avanzado, como <code>AdagradOptimizer</code> .  ¬°Prueba y mira para obtener mejores resultados! <br><br>  Finalmente, el modelo presentado anteriormente puede usarse no solo para la traducci√≥n, sino tambi√©n para cualquier otra tarea de secuencia a secuencia.  Incluso si desea convertir una secuencia en un √°rbol, por ejemplo, generar un √°rbol de an√°lisis, este modelo puede producir resultados de vanguardia, como lo muestran <a href="">Vinyals &amp; Kaiser et al., 2014 (pdf)</a> .  Por lo tanto, puede crear no solo un traductor, sino tambi√©n un analizador, un bot de chat o cualquier otro programa que desee.  Experimento! <br><br>  Eso es todo! <br><br>  Estamos esperando sus comentarios y preguntas aqu√≠ o lo invitamos a preguntarle a su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">maestro</a> en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una lecci√≥n abierta</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es432302/">https://habr.com/ru/post/es432302/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es432292/index.html">Fuentes param√©tricas y variables: win-win para dise√±adores</a></li>
<li><a href="../es432294/index.html">Ansible Tower: plantillas de trabajo de flujo de trabajo</a></li>
<li><a href="../es432296/index.html">Google te mantiene en una "burbuja de b√∫squeda" personal incluso si sales de tu cuenta</a></li>
<li><a href="../es432298/index.html">Timeweb ingres√≥ a los registradores de dominio TOP-10 en la zona .RU</a></li>
<li><a href="../es432300/index.html">Soporte, servicio, dolor de cabeza y todo-todo-todo</a></li>
<li><a href="../es432304/index.html">Un neurocient√≠fico brillante que puede tener una clave para crear verdadera inteligencia artificial.</a></li>
<li><a href="../es432306/index.html">Almacenamiento Clase Memoria en almacenamiento: si la necesita a√∫n m√°s r√°pido</a></li>
<li><a href="../es432308/index.html">Nivel de ciencia ficci√≥n modular UE4: inspirado en Nostromo y Serenity</a></li>
<li><a href="../es432310/index.html">Ktor como cliente HTTP para Android</a></li>
<li><a href="../es432312/index.html">Crear un mapa de formas del mapa de RF en Power BI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>