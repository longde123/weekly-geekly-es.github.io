<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤲🏼 📄 🧔🏿 Onde uma pessoa vê formas, a IA vê texturas ⏮️ 🎪 🦁</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Surpreendentemente, os pesquisadores com algoritmos de aprendizado profundo de visão por computador geralmente não conseguem classificar imagens porqu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Onde uma pessoa vê formas, a IA vê texturas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462951/"> Surpreendentemente, os pesquisadores com algoritmos de aprendizado profundo de visão por computador geralmente não conseguem classificar imagens porque se concentram principalmente nas texturas, e não nas formas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/953/444/41f/95344441f850333698aa94694c254d32.jpg"><br><br>  Se você olhar a foto de um gato, com uma alta probabilidade, poderá reconhecê-lo, independentemente de ser vermelho ou listrado - ou mesmo se a foto for em preto e branco, manchada, maltratada ou manchada.  Você provavelmente poderá notar um gato quando ele se enrolar atrás de um travesseiro ou pular sobre uma mesa, representando apenas uma forma borrada.  Você naturalmente aprendeu a reconhecer gatos em quase qualquer situação.  Mas os sistemas de visão de máquina baseados em redes neurais profundas, embora às vezes possam fornecer às pessoas tarefas de reconhecimento de gatos em condições fixas, podem ser confundidos com imagens que são pelo menos ligeiramente diferentes daquilo que sabem, ou que contêm ruído ou grão forte. <br><a name="habracut"></a><br>  E agora os pesquisadores alemães descobriram uma razão inesperada para isso: se as pessoas prestam atenção às formas dos objetos representados, a visão computacional com aprendizado profundo se apega às texturas dos objetos. <br><br>  Essa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descoberta</a> , apresentada em maio em uma conferência internacional de representações de aprendizado, enfatiza o nítido contraste entre o “pensamento” de pessoas e máquinas e ilustra como podemos estar errados ao entender como a IA funciona.  E também pode nos dizer por que nossa visão se tornou assim como resultado da evolução. <br><br><h2>  Gatos de marfim e aviões de observação </h2><br>  Os algoritmos de aprendizado profundo funcionam ao conduzir milhares de imagens através de uma rede neural que possui ou não um gato.  O sistema procura padrões nesses dados, que são usados ​​para colocar a melhor marca na imagem que não havia encontrado antes.  A arquitetura de rede é um pouco como a estrutura do sistema visual humano, pois possui camadas conectadas que permitem extrair mais e mais recursos abstratos da imagem.  No entanto, o processo de construção de um sistema de associações que leva à resposta correta é uma caixa preta que as pessoas só podem tentar interpretar após o fato.  "Tentamos entender o que leva ao sucesso desses algoritmos de visão computacional de aprendizado profundo e por que eles são tão vulneráveis", disse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Thomas Ditterich</a> , especialista em TI da Universidade de Oregon, que não é afiliado a este estudo. <br><br>  Alguns pesquisadores preferem estudar o que acontece quando tentam enganar a rede alterando ligeiramente a imagem.  Eles descobriram que mesmo pequenas alterações podem fazer com que o sistema marque a imagem incorretamente - e grandes alterações <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">podem não</a> alterar o rótulo.  Enquanto isso, outros especialistas rastreiam mudanças no sistema para analisar como os neurônios individuais respondem à imagem e compõem um " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atlas de ativação</a> " com base nos atributos que o sistema aprendeu. <br><br>  Mas um grupo de cientistas dos laboratórios do neurobiólogo computacional <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Matias Betge</a> e do psicofisiologista <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Felix Wichmann</a> da Universidade de Tübingen, na Alemanha, escolheu uma abordagem qualitativa.  No ano passado, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">equipe relatou</a> que, ao treinar imagens que foram alteradas por um determinado tipo de ruído, a rede começou a reconhecer imagens melhor do que as pessoas que tentavam tirar as mesmas fotos com ruído.  No entanto, as mesmas imagens, modificadas de maneira um pouco diferente, confundiram completamente a rede, embora para as pessoas a nova distorção parecesse quase a mesma que a antiga. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c18/418/8c4/c184188c4f088a155c652e51562c42f6.jpg" width="60%"><br>  <i>Robert Geyros, estudante de pós-graduação em Neurobiologia Computacional pela Universidade de Tübingen</i> <br><br>  Para explicar esse resultado, os pesquisadores se perguntaram qual a qualidade da imagem que mais muda, mesmo com a adição de um pouco de ruído.  A escolha óbvia é a textura.  "A forma de um objeto permanece mais ou menos incólume se você adicionar muito ruído por um longo tempo", disse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Robert Geyros</a> , estudante de graduação nos laboratórios de Betge e Wichmann, principal autor do estudo.  Mas "a estrutura da imagem local é distorcida muito rapidamente quando uma pequena quantidade de ruído é adicionada".  Então eles criaram uma maneira complicada de testar como os sistemas visuais de máquinas e pessoas processam imagens. <br><br>  Geyros, Betge e seus colegas criaram imagens com duas características conflitantes, assumindo a forma de um objeto e a textura de outro: por exemplo, uma silhueta de gato pintada com textura de pele de elefante cinza, ou um urso feito de latas de alumínio ou uma silhueta plana cheia de sobreposições um ao outro com imagens de mostradores.  As pessoas rotulavam centenas dessas imagens com base em suas formas - gato, urso, avião - quase sempre, como pretendido.  No entanto, quatro algoritmos de classificação diferentes estavam inclinados na direção oposta, distribuindo rótulos que refletiam as texturas dos objetos: elefante, latas, relógios. <br><br>  "Isso muda nossa compreensão de como as redes neurais profundas com distribuição direta - sem configurações adicionais, após o processo usual de aprendizado - reconhecem imagens", disse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nikolaus Kriegescorte</a> , neurocientista computacional da Universidade de Columbia que não participou do estudo. <br><br>  À primeira vista, a preferência por texturas AI sobre formas pode parecer estranha, mas faz sentido.  "A textura tem um formato de alta resolução", disse Kriegscorte.  E é mais fácil para o sistema se apegar a essa escala: o número de pixels com informações de textura excede significativamente o número de pixels que compõem o limite do objeto, e as primeiras etapas da rede estão relacionadas ao reconhecimento de recursos locais, como linhas e faces.  "Isso é exatamente o que é textura", disse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">John Tsotsos</a> , especialista em visão computacional da Universidade de York, em Toronto, que não está associado a este estudo.  "Por exemplo, um agrupamento de segmentos alinhados da mesma maneira." <br><br>  Geyros e colegas mostraram que esses sinais locais são suficientes para a rede realizar a classificação.  Esta é a prova de Betge e outro dos autores do estudo, o pós-doc <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Wiland Brendel</a> , que deu uma olhada final no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho</a> , que também foi apresentado na conferência de maio.  Nesse trabalho, eles construíram um sistema de aprendizado profundo que funcionava da mesma maneira que os algoritmos de classificação funcionavam antes da disseminação do aprendizado profundo - com base no princípio da "bolsa de atributos".  O algoritmo divide a imagem em pequenos fragmentos, como os modelos atuais (como Geyros usado em seu experimento), mas, em vez de integrar gradualmente essas informações para extrair sinais de um nível mais alto de abstração, o algoritmo assume imediatamente o conteúdo de cada peça ( “Nesta peça há evidência de bicicleta, nesta - evidência de um pássaro”).  Ele simplesmente dobrou todas as decisões para determinar o objeto ("se mais peças contêm sinais de bicicleta, então esta é uma bicicleta"), sem prestar atenção às relações espaciais das peças.  E, no entanto, ele foi capaz de reconhecer objetos com uma precisão inesperadamente alta. <br><br>  "Este trabalho desafia a suposição de que o aprendizado profundo faz algo completamente diferente", de modelos anteriores, disse Brendel.  “Obviamente, houve um grande salto.  Só estou dizendo que não era tão grande quanto se esperava. " <br><br>  Segundo Amir Rosenfeld, um pós-doutorado da Universidade de York e da Universidade de Toronto, que não participou do estudo, "existe uma grande diferença entre o que as redes neurais devem, em nossa opinião, e o que elas fazem", incluindo o quão bem elas gerenciam reproduzir o comportamento humano. <br><br>  O pretzel falou da mesma maneira.  É fácil assumir que as redes neurais resolverão os problemas da mesma maneira que as pessoas, disse ele.  "No entanto, esquecemos constantemente a existência de outros métodos." <br><br><h2>  Uma mudança para uma perspectiva mais humana </h2><br>  Os métodos modernos de aprendizado profundo podem integrar recursos locais, como texturas, a padrões mais globais, como formulários.  "O que é inesperadamente e muito convincentemente mostrado nesses trabalhos - embora a arquitetura permita classificar imagens padrão, isso não acontece automaticamente se você apenas treinar a rede sobre isso", disse Kriegescorte. <br><br>  Geyros queria ver o que acontece se a equipe obriga os modelos a ignorar as texturas.  A equipe pegou as imagens tradicionalmente usadas para treinar algoritmos de classificação e as pintou em estilos diferentes, privando-as de informações úteis sobre textura.  Quando eles treinaram novamente cada modelo nas novas imagens, os sistemas começaram a contar com padrões globais maiores e mostraram uma tendência maior ao reconhecimento de padrões, mais parecido com as pessoas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c60/18d/44b/c6018d44bb8459f3b0496d19975c6c5d.jpg" width="60%"><br>  <i>Wieland Brendel, neurocientista computacional da Universidade de Tübingen na Alemanha</i> <br><br>  E depois disso, os algoritmos começaram a classificar melhor as imagens ruidosas, mesmo quando não foram treinadas para lidar com essas distorções.  "A rede de reconhecimento de formas tornou-se completamente mais confiável gratuitamente", disse Geyros.  "Isso sugere que o viés certo para a execução de determinadas tarefas, no nosso caso, a propensão a usar formulários, ajuda a generalizar o conhecimento para novas condições". <br><br>  Isso também sugere que em humanos essa tendência poderia ter se formado naturalmente, já que o uso de formas é uma maneira mais confiável de reconhecer o que vemos em condições novas ou barulhentas.  As pessoas vivem em um mundo tridimensional, onde os objetos são visíveis de muitos ângulos sob diversas condições e onde nossos outros sentimentos, como o toque, podem opcionalmente complementar o reconhecimento de objetos.  Portanto, para nossa visão, faz sentido colocar o formulário como uma textura prioritária.  Além disso, alguns psicólogos mostraram uma conexão entre linguagem, aprendizado e tendência a usar formas: quando as crianças eram ensinadas a prestar mais atenção às formas quando estudavam certas categorias de palavras, mais tarde elas foram capazes de desenvolver um vocabulário muito mais extenso de substantivos do que outros. <br><br>  Este trabalho serve como um lembrete de que "os dados têm um efeito mais forte sobre o preconceito e o viés dos modelos do que pensávamos", disse Wichman.  Esta não é a primeira vez que os pesquisadores encontram esse problema: já foi demonstrado que programas de reconhecimento facial, pesquisa automática de currículo e outras redes neurais dão muita importância a sinais inesperados devido a preconceitos profundamente enraizados nos dados em que são treinados.  Eliminar preconceitos indesejados do processo de tomada de decisão provou ser uma tarefa difícil, mas Wichman disse que o novo trabalho demonstra que isso é possível em princípio e é encorajador. <br><br>  No entanto, mesmo os modelos de Geyros que se concentram nos formulários podem ser enganados adicionando muito ruído às imagens ou alterando certos pixels, o que significa que eles ainda têm um longo caminho a percorrer para alcançar uma qualidade comparável à visão humana.  Na mesma linha, um novo trabalho de Rosenfeld, Tsotsos e Marcus Solbach, um estudante de pós-graduação do laboratório Tsotsos, demonstra que os algoritmos de aprendizado de máquina não são capazes de capturar a semelhança de diferentes imagens da mesma maneira que as pessoas.  No entanto, esses trabalhos "ajudam a indicar exatamente em quais aspectos esses modelos ainda não reproduzem aspectos importantes do cérebro humano", disse Kriegscorte.  E Wichman disse que "em alguns casos, pode ser mais importante examinar o conjunto de dados". <br><br>  Sanya Fiedler, especialista em TI da Universidade de Toronto, que não participou do estudo, concorda.  "É nosso trabalho desenvolver dados inteligentes", disse ela.  Ela e seus colegas estão explorando como as tarefas auxiliares podem ajudar as redes neurais a melhorar a qualidade de suas tarefas principais.  Inspirados pelas descobertas de Geyros, eles recentemente treinaram o algoritmo de classificação de imagens não apenas para reconhecer os próprios objetos, mas também para determinar quais pixels pertencem a seus contornos.  E a rede melhorou automaticamente no reconhecimento de objetos.  "Se você recebe apenas uma tarefa, o resultado é atenção seletiva e cegueira em relação a muitas outras coisas", disse Fiedler.  "Se eu lhe der várias tarefas, você aprenderá sobre coisas diferentes, e isso pode não acontecer."  É o mesmo com esses algoritmos. "  A solução de vários problemas os ajuda a “desenvolver uma tendência para várias informações”, semelhante ao que aconteceu no experimento de Geyros com formas e texturas. <br><br>  Todos esses estudos são "um passo muito interessante para aprofundar nossa compreensão do que está acontecendo com o aprendizado profundo, e talvez isso nos ajude a superar as limitações que enfrentamos", disse Dietrich.  "É por isso que amo essa série de trabalhos." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt462951/">https://habr.com/ru/post/pt462951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt462939/index.html">Os 10 principais relatórios C ++ da Rússia e a lista de reprodução da conferência de acesso aberto</a></li>
<li><a href="../pt462943/index.html">Caça aos Wumpus ou experimente escrever um jogo clássico do Android</a></li>
<li><a href="../pt462945/index.html">Gere senhas únicas para 2FA em JS usando a API Web Crypto</a></li>
<li><a href="../pt462947/index.html">A história de como o PVS-Studio encontrou um erro na biblioteca usada em ... PVS-Studio</a></li>
<li><a href="../pt462949/index.html">A história de como o PVS-Studio encontrou um erro na biblioteca usada em ... PVS-Studio</a></li>
<li><a href="../pt462955/index.html">Transformação digital do treinamento e certificação da equipe de campo</a></li>
<li><a href="../pt462957/index.html">Prós e contras: o limite de preço para .org ainda é cancelado</a></li>
<li><a href="../pt462959/index.html">Processamento de linguagem natural de cheques on-line: um curso de lições de mágica para um gato comum e outros problemas</a></li>
<li><a href="../pt462961/index.html">Data Science Digest (agosto de 2019)</a></li>
<li><a href="../pt462963/index.html">Usando a API de contexto no React para criar um tema de aplicativo global</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>