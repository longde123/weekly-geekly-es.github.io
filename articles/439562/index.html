<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§õ ‚õ∑Ô∏è ‚åöÔ∏è Configure el cl√∫ster Kubernetes HA en metal desnudo con kubeadm. Parte 1/3 üõí üßëüèæ‚Äçü§ù‚Äçüßëüèº üéÖüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 2/3 aqu√≠ 
 Parte 3/3 aqu√≠ 


 Hola a todos! En este art√≠culo quiero optimizar la informaci√≥n y compartir la experiencia de crear y usar el cl√∫st...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configure el cl√∫ster Kubernetes HA en metal desnudo con kubeadm. Parte 1/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/439562/"><p>  <strong>Parte 2/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>aqu√≠</strong></a> <br>  <strong>Parte 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>aqu√≠</strong></a> </p><br><p>  Hola a todos!  En este art√≠culo quiero optimizar la informaci√≥n y compartir la experiencia de crear y usar el cl√∫ster interno de Kubernetes. </p><br><p>  En los √∫ltimos a√±os, esta tecnolog√≠a de orquestaci√≥n de contenedores ha dado un gran paso adelante y se ha convertido en una especie de est√°ndar corporativo para miles de empresas.  Algunos lo usan en producci√≥n, otros solo lo prueban en proyectos, pero las pasiones a su alrededor, no importa c√≥mo lo digas, brillan con seriedad.  Si nunca lo has usado antes, es hora de comenzar a salir. </p><br><h3 id="0-vstuplenie">  0. Introducci√≥n </h3><br><p>  Kubernetes es una tecnolog√≠a de orquestaci√≥n escalable que puede comenzar con la instalaci√≥n en un solo nodo y alcanzar el tama√±o de grandes cl√∫steres HA basados ‚Äã‚Äãen varios cientos de nodos en su interior.  Los proveedores de nube m√°s populares proporcionan diferentes tipos de implementaciones de Kubernetes: toma y uso.  Pero las situaciones son diferentes, y hay empresas que no usan las nubes y quieren obtener todas las ventajas de las tecnolog√≠as modernas de orquestaci√≥n.  Y aqu√≠ viene la instalaci√≥n de Kubernetes en metal desnudo. </p><br><p><img src="https://habrastorage.org/webt/el/ci/ua/elciua9kwxmo0fnnm5yoaabqpvm.jpeg"></p><a name="habracut"></a><br><h3 id="1-vvedenie">  1. Introducci√≥n </h3><br><p> En este ejemplo, crearemos un cl√∫ster de Kubernetes HA con la topolog√≠a para varios maestros, con un cl√∫ster externo, etc. como capa base y un equilibrador de carga MetalLB dentro.  En todos los nodos de trabajo, implementaremos GlusterFS como un simple almacenamiento de cl√∫ster distribuido interno.  Tambi√©n intentaremos implementar varios proyectos de prueba en √©l utilizando nuestro registro personal de Docker. </p><br><p>  En general, hay varias formas de crear un cl√∫ster de Kubernetes HA: la ruta dif√≠cil y profunda que se describe en el popular documento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">kubernetes-the-hard-way</a> , o la forma m√°s simple de usar la utilidad <strong>kubeadm</strong> . </p><br><p>  Kubeadm es una herramienta creada por la comunidad de Kubernetes espec√≠ficamente para simplificar la instalaci√≥n de Kubernetes y facilitar el proceso.  Anteriormente, Kubeadm se recomendaba solo para crear peque√±os grupos de prueba con un nodo maestro, para comenzar.  Pero durante el a√±o pasado, se ha mejorado mucho, y ahora podemos usarlo para crear cl√∫steres HA con varios nodos maestros.  Seg√∫n las noticias de la comunidad de Kubernetes, en el futuro, se recomendar√° Kubeadm como herramienta para instalar Kubernetes. </p><br><p>  La documentaci√≥n de Kubeadm ofrece dos formas b√°sicas de implementar un cl√∫ster, con topolog√≠as de pila y externas, etc.  Elegir√© la segunda ruta con nodos externos, etc. debido a la tolerancia a fallas del cl√∫ster HA. </p><br><p>  Aqu√≠ hay un diagrama de la documentaci√≥n de Kubeadm que describe esta ruta: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/4y/nh/gd/4ynhgd4h3ireojrvdplimgnsk2u.jpeg"></a> </p><br><p>  Lo cambiar√© un poco.  Primero, usar√© un par de servidores HAProxy como equilibradores de carga con el paquete Heartbeat, que compartir√° la direcci√≥n IP virtual.  Heartbeat y HAProxy utilizan una peque√±a cantidad de recursos del sistema, por lo que los colocar√© en un par de nodos etcd para reducir ligeramente la cantidad de servidores para nuestro cl√∫ster. </p><br><p>  Para este esquema de cl√∫ster de Kubernetes, se requieren ocho nodos.  Tres servidores para un cl√∫ster externo, etcd (los servicios LB tambi√©n usar√°n un par de ellos), dos para los nodos del plano de control (nodos maestros) y tres para los nodos de trabajo.  Puede ser un metal desnudo o un servidor VM.  En este caso, no importa.  Puede cambiar f√°cilmente el esquema agregando m√°s nodos maestros y colocando HAProxy con Heartbeat en nodos separados, si hay muchos servidores gratuitos.  Aunque mi opci√≥n para la primera implementaci√≥n del cl√∫ster HA es suficiente para los ojos. </p><br><p>  Si lo desea, agregue un peque√±o servidor con la utilidad <strong>kubectl instalada</strong> para administrar este cl√∫ster o use su propio escritorio Linux para esto. </p><br><p>  El diagrama para este ejemplo se ver√° as√≠: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/r4/5w/uc/r45wucdscdlhmaqcuw-gtr7mrmm.jpeg"></a> </p><br><h3 id="2-trebovaniya">  2. Requisitos </h3><br><p>  Necesitar√° dos nodos maestros Kubernetes con los requisitos m√≠nimos del sistema recomendados: 2 CPU y 2 GB de RAM de acuerdo con la documentaci√≥n de <strong>kubeadm</strong> .  Para los nodos de trabajo, recomiendo usar servidores m√°s potentes, ya que ejecutaremos todos nuestros servicios de aplicaciones en ellos.  Y para Etcd + LB, tambi√©n podemos tomar servidores con dos CPU y al menos 2 GB de RAM. </p><br><p>  Seleccione una red p√∫blica o privada para este cl√∫ster;  Las direcciones IP no importan;  Es importante que todos los servidores sean accesibles entre s√≠ y, por supuesto, para usted.  M√°s tarde, dentro del cl√∫ster de Kubernetes, configuraremos una red superpuesta. </p><br><p>  Los requisitos m√≠nimos para este ejemplo son: </p><br><ul><li>  2 servidores con 2 procesadores y 2 GB de RAM para el nodo maestro </li><li>  3 servidores con 4 procesadores y 4-8 GB de RAM para nodos de trabajo </li><li>  3 servidores con 2 procesadores y 2 GB de RAM para Etcd y HAProxy </li><li>  192.168.0.0/24: la subred. </li></ul><br><p>  192.168.0.1 - Direcci√≥n IP virtual HAProxy, 192.168.0.2 - 4 direcciones IP principales de nodos Etcd y HAProxy, 192.168.0.5 - 6 direcciones IP principales del nodo maestro Kubernetes, 192.168.0.7 - 9 direcciones IP principales de nodos de trabajo Kubernetes . </p><br><p>  La base de datos Debian 9 est√° instalada en todos los servidores. </p><br><blockquote>  Recuerde tambi√©n que los requisitos del sistema dependen de cu√°n grande y poderoso sea el cl√∫ster.  Para obtener m√°s informaci√≥n, consulte la documentaci√≥n de Kubernetes. </blockquote><br><h3 id="3-nastroyka-haproxy-i-heartbeat">  3. Configure HAProxy y Heartbeat. </h3><br><p>  Tenemos m√°s de un nodo maestro de Kubernetes y, por lo tanto, debe configurar un equilibrador de carga HAProxy frente a ellos para distribuir el tr√°fico.  Este ser√° un par de servidores HAProxy con una direcci√≥n IP virtual compartida.  La tolerancia a fallos se proporciona con el paquete Heartbeat.  Para la implementaci√≥n, utilizaremos los dos primeros servidores etcd. </p><br><p>  Instale y configure HAProxy con Heartbeat en el primer y segundo servidor etcd (192.168.0.2‚Äì3 en este ejemplo): </p><br><pre><code class="plaintext hljs">etcd1# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy etcd2# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy</code> </pre> <br><p>  Guarde la configuraci√≥n original y cree una nueva: </p><br><pre> <code class="plaintext hljs">etcd1# mv /etc/haproxy/haproxy.cfg{,.back} etcd1# vi /etc/haproxy/haproxy.cfg etcd2# mv /etc/haproxy/haproxy.cfg{,.back} etcd2# vi /etc/haproxy/haproxy.cfg</code> </pre> <br><p>  Agregue estas opciones de configuraci√≥n para ambos HAProxy: </p><br><pre> <code class="plaintext hljs">global user haproxy group haproxy defaults mode http log global retries 2 timeout connect 3000ms timeout server 5000ms timeout client 5000ms frontend kubernetes bind 192.168.0.1:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server k8s-master-0 192.168.0.5:6443 check fall 3 rise 2 server k8s-master-1 192.168.0.6:6443 check fall 3 rise 2</code> </pre> <br><p>  Como puede ver, ambos servicios HAProxy comparten la direcci√≥n IP: 192.168.0.1.  Esta direcci√≥n IP virtual se mover√° entre los servidores, por lo que seremos un poco astutos y <strong>habilitaremos el</strong> par√°metro <strong>net.ipv4.ip_nonlocal_bind</strong> para permitir el enlace de los servicios del sistema a una direcci√≥n IP no local. </p><br><p>  Agregue esta caracter√≠stica al archivo <strong>/etc/sysctl.conf</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1 etcd2# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1</code> </pre> <br><p>  Ejecutar en ambos servidores: </p><br><pre> <code class="plaintext hljs">sysctl -p</code> </pre> <br><p>  Tambi√©n ejecute HAProxy en ambos servidores: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl start haproxy etcd2# systemctl start haproxy</code> </pre> <br><p>  Aseg√∫rese de que HAProxy se est√© ejecutando y escuchando en la direcci√≥n IP virtual en ambos servidores: </p><br><pre> <code class="plaintext hljs">etcd1# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy etcd2# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy</code> </pre> <br><p>  Capucha!  Ahora instale Heartbeat y configure esta IP virtual. </p><br><pre> <code class="plaintext hljs">etcd1# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat etcd2# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat</code> </pre> <br><p>  Es hora de crear varios archivos de configuraci√≥n para √©l: para el primer y el segundo servidor, ser√°n b√°sicamente los mismos. </p><br><p>  Primero cree el archivo <strong>/etc/ha.d/authkeys</strong> , en este archivo Heartbeat almacena datos para la autenticaci√≥n mutua.  El archivo debe ser el mismo en ambos servidores: </p><br><pre> <code class="plaintext hljs"># echo -n securepass | md5sum bb77d0d3b3f239fa5db73bdf27b8d29a etcd1# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a etcd2# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a</code> </pre> <br><p>  Este archivo debe ser accesible solo para root: </p><br><pre> <code class="plaintext hljs">etcd1# chmod 600 /etc/ha.d/authkeys etcd2# chmod 600 /etc/ha.d/authkeys</code> </pre> <br><p>  Ahora cree el archivo de configuraci√≥n principal para Heartbeat en ambos servidores: para cada servidor ser√° ligeramente diferente. </p><br><p>  Cree <strong>/etc/ha.d/ha.cf</strong> : </p><br><p>  <strong>etcd1</strong> </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.3 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to log/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  <strong>etcd2</strong> </p><br><pre> <code class="plaintext hljs">etcd2# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.2 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to vlog/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  Obtenga los par√°metros de "nodo" para esta configuraci√≥n ejecutando uname -n en ambos servidores Etcd.  Utilice tambi√©n el nombre de su tarjeta de red en lugar de ens18. </p><br><p>  Finalmente, debe crear el archivo <strong>/etc/ha.d/haresources</strong> en estos servidores.  Para ambos servidores, el archivo debe ser el mismo.  En este archivo, establecemos nuestra direcci√≥n IP com√∫n y determinamos qu√© nodo es el maestro predeterminado: </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1 etcd2# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1</code> </pre> <br><p>  Cuando todo est√© listo, inicie los servicios Heartbeat en ambos servidores y verifique que hayamos recibido esta IP virtual declarada en el <strong>nodo</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl restart heartbeat etcd2# systemctl restart heartbeat etcd1# ip a ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff inet 192.168.0.2/24 brd 192.168.0.255 scope global ens18 valid_lft forever preferred_lft forever inet 192.168.0.1/24 brd 192.168.0.255 scope global secondary</code> </pre> <br><p>  Puede verificar que HAProxy funciona bien ejecutando <strong>nc</strong> en 192.168.0.1 6443. Debe haber agotado el tiempo de espera porque la API de Kubernetes a√∫n no est√° escuchando en el lado del servidor.  Pero esto significa que HAProxy y Heartbeat est√°n configurados correctamente. </p><br><pre> <code class="plaintext hljs"># nc -v 192.168.0.1 6443 Connection to 93.158.95.90 6443 port [tcp/*] succeeded!</code> </pre> <br><h3 id="4-podgotovka-nod-dlya-kubernetes">  4. Preparaci√≥n de nodos para Kubernetes </h3><br><p>  El siguiente paso es preparar todos los nodos de Kubernetes.  Debe instalar Docker con algunos paquetes adicionales, agregar el repositorio de Kubernetes e instalar los <strong>paquetes kubelet</strong> , <strong>kubeadm</strong> , <strong>kubectl</strong> desde √©l.  Esta configuraci√≥n es la misma para todos los nodos de Kubernetes (maestro, trabajadores, etc.) </p><br><blockquote>  La principal ventaja de <strong>Kubeadm</strong> es que no hay mucha necesidad de software adicional.  Instale <strong>kubeadm</strong> en todos los hosts y <strong>√∫selo</strong> ;  al menos generar certificados de CA. </blockquote><p>  Instale Docker en todos los nodos: </p><br><pre> <code class="plaintext hljs">Update the apt package index # apt-get update Install packages to allow apt to use a repository over HTTPS # apt-get -y install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker's official GPG key # curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add docker apt repository # apt-add-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install docker-ce. # apt-get update &amp;&amp; apt-get -y install docker-ce Check docker version # docker -v Docker version 18.09.0, build 4d60db4</code> </pre> <br><p>  Despu√©s de eso, instale los paquetes de Kubernetes en todos los nodos: </p><br><ul><li>  <strong><code>kubeadm</code></strong> : comando para cargar el cl√∫ster. </li><li>  <strong><code>kubelet</code></strong> : un componente que se ejecuta en todas las computadoras del cl√∫ster y realiza acciones como iniciar hogares y contenedores. </li><li>  <strong><code>kubectl</code></strong> : l√≠nea de comando util para comunicarse con el cl√∫ster. </li><li>  <strong>kubectl</strong> ‚Äî a voluntad;  A menudo lo instalo en todos los nodos para ejecutar algunos comandos de Kubernetes para la depuraci√≥n. </li></ul><br><pre> <code class="plaintext hljs"># curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository # cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install packages # apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl Hold back packages # apt-mark hold kubelet kubeadm kubectl Check kubeadm version # kubeadm version kubeadm version: &amp;version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.1", GitCommit:"eec55b9dsfdfgdfgfgdfgdfgdf365bdd920", GitTreeState:"clean", BuildDate:"2018-12-13T10:36:44Z", GoVersion:"go1.11.2", Compiler:"gc", Platform:"linux/amd64"}</code> </pre> <br><p>  Despu√©s de <strong>instalar kubeadm</strong> y otros paquetes, no olvide deshabilitar el intercambio. </p><br><pre> <code class="plaintext hljs"># swapoff -a # sed -i '/ swap / s/^/#/' /etc/fstab</code> </pre> <br><p>  Repita la instalaci√≥n en los nodos restantes.  Los paquetes de software son los mismos para todos los nodos del cl√∫ster, y solo la siguiente configuraci√≥n determinar√° los roles que recibir√°n m√°s adelante. </p><br><h3 id="5-nastroyka-klastera-ha-etcd">  5. Configurar HA Etcd Cluster </h3><br><p>  Entonces, una vez finalizados los preparativos, configuraremos el cl√∫ster de Kubernetes.  El primer bloque ser√° el cl√∫ster HA Etcd, que tambi√©n se configura con la herramienta kubeadm. </p><br><blockquote>  Antes de comenzar, aseg√∫rese de que todos los nodos etcd se comuniquen a trav√©s de los puertos 2379 y 2380. Adem√°s, debe configurar el acceso ssh entre ellos para usar <strong>scp</strong> . </blockquote><p>  Comencemos con el primer nodo etcd, y luego simplemente copie todos los certificados necesarios y los archivos de configuraci√≥n a los otros servidores. </p><br><p>  En todos los nodos <strong>etcd</strong> , debe agregar un nuevo archivo de configuraci√≥n <strong>systemd</strong> para la unidad <strong>kubelet</strong> con mayor prioridad: </p><br><pre> <code class="plaintext hljs">etcd-nodes# cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true Restart=always EOF etcd-nodes# systemctl daemon-reload etcd-nodes# systemctl restart kubelet</code> </pre> <br><p>  Luego, pasaremos por ssh al primer nodo de <strong>etcd</strong> ; lo usaremos para generar todas las configuraciones de <strong>kubeadm</strong> necesarias para cada nodo de <strong>etcd</strong> y luego copiarlas. </p><br><pre> <code class="plaintext hljs"># Export all our etcd nodes IP's as variables etcd1# export HOST0=192.168.0.2 etcd1# export HOST1=192.168.0.3 etcd1# export HOST2=192.168.0.4 # Create temp directories to store files for all nodes etcd1# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ etcd1# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) etcd1# NAMES=("infra0" "infra1" "infra2") etcd1# for i in "${!ETCDHOSTS[@]}"; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: "kubeadm.k8s.io/v1beta1" kind: ClusterConfiguration etcd: local: serverCertSANs: - "${HOST}" peerCertSANs: - "${HOST}" extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done</code> </pre> <br><p>  Ahora cree la autoridad de certificaci√≥n principal usando <strong>kubeadm</strong> </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase certs etcd-ca</code> </pre> <br><p>  Este comando crear√° dos <strong>archivos ca.crt y ca.key</strong> en el directorio <strong>/ etc / kubernetes / pki / etcd /</strong> . </p><br><pre> <code class="plaintext hljs">etcd1# ls /etc/kubernetes/pki/etcd/ ca.crt ca.key</code> </pre> <br><p>  Ahora generaremos certificados para todos los nodos <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">### Create certificates for the etcd3 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST2}/ ### cleanup non-reusable certificates etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the etcd2 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST1}/ ### cleanup non-reusable certificates again etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the this local node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1 #kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for this node # clean up certs that should not be copied off this host etcd1# find /tmp/${HOST2} -name ca.key -type f -delete etcd1# find /tmp/${HOST1} -name ca.key -type f -delete</code> </pre> <br><p>  Luego copie los certificados y configuraciones de kubeadm a los nodos <strong>etcd2</strong> y <strong>etcd3</strong> . </p><br><blockquote>  Primero genere un par de claves ssh en <strong>etcd1</strong> y agregue la parte p√∫blica a los <strong>nodos etcd2</strong> y <strong>3</strong> .  En este ejemplo, todos los comandos se ejecutan en nombre de un usuario que posee todos los derechos en el sistema. </blockquote><br><pre> <code class="plaintext hljs">etcd1# scp -r /tmp/${HOST1}/* ${HOST1}: etcd1# scp -r /tmp/${HOST2}/* ${HOST2}: ### login to the etcd2 or run this command remotely by ssh etcd2# cd /root etcd2# mv pki /etc/kubernetes/ ### login to the etcd3 or run this command remotely by ssh etcd3# cd /root etcd3# mv pki /etc/kubernetes/</code> </pre> <br><p>  Antes de iniciar el cl√∫ster etcd, aseg√∫rese de que los archivos existan en todos los nodos: </p><br><p>  Lista de archivos requeridos en <strong>etcd1</strong> : </p><br><pre> <code class="plaintext hljs">/tmp/192.168.0.2 ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ ca.key ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Para el nodo <strong>etcd2,</strong> esto es: </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Y el √∫ltimo nodo es <strong>etcd3</strong> : </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Cuando todos los certificados y configuraciones est√°n en su lugar, creamos manifiestos.  En cada nodo, ejecute el comando <strong>kubeadm</strong> para generar un manifiesto est√°tico para el cl√∫ster <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase etcd local --config=/tmp/192.168.0.2/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml</code> </pre> <br><p>  Ahora el cl√∫ster, <strong>etcd</strong> , en teor√≠a, est√° configurado y en buen estado.  Verifique ejecutando el siguiente comando en el <strong>nodo</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \ --cert-file /etc/kubernetes/pki/etcd/peer.crt \ --key-file /etc/kubernetes/pki/etcd/peer.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://192.168.0.2:2379 cluster-health ### status output member 37245675bd09ddf3 is healthy: got healthy result from https://192.168.0.3:2379 member 532d748291f0be51 is healthy: got healthy result from https://192.168.0.4:2379 member 59c53f494c20e8eb is healthy: got healthy result from https://192.168.0.2:2379 cluster is healthy</code> </pre> <br><p>  El cl√∫ster <strong>etcd</strong> ha aumentado, as√≠ que sigue adelante. </p><br><h3 id="6-nastroyka-master--i-rabochih-nod">  6. Configuraci√≥n de nodos maestros y de trabajo </h3><br><p>  Configure los nodos maestros de nuestro cl√∫ster: copie estos archivos del primer nodo <strong>etcd</strong> al primer nodo maestro: </p><br><pre> <code class="plaintext hljs">etcd1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.0.5:</code> </pre> <br><p>  Luego vaya ssh al nodo maestro <strong>master1</strong> y cree el <strong>archivo kubeadm-config.yaml</strong> con el siguiente contenido: </p><br><pre> <code class="plaintext hljs">master1# cd /root &amp;&amp; vi kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - "192.168.0.1" controlPlaneEndpoint: "192.168.0.1:6443" etcd: external: endpoints: - https://192.168.0.2:2379 - https://192.168.0.3:2379 - https://192.168.0.4:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</code> </pre> <br><p>  Mueva los certificados y la clave previamente copiados al directorio apropiado en el <strong>nodo</strong> master1, como en la descripci√≥n de la configuraci√≥n. </p><br><pre> <code class="plaintext hljs">master1# mkdir -p /etc/kubernetes/pki/etcd/ master1# cp /root/ca.crt /etc/kubernetes/pki/etcd/ master1# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ master1# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/</code> </pre> <br><p>  Para crear el primer nodo maestro, haga: </p><br><pre> <code class="plaintext hljs">master1# kubeadm init --config kubeadm-config.yaml</code> </pre> <br><p>  Si todos los pasos anteriores se completan correctamente, ver√° lo siguiente: </p><br><pre> <code class="plaintext hljs">You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Copie esta <strong>salida de</strong> inicializaci√≥n de <strong>kubeadm</strong> en cualquier archivo de texto, usaremos este token en el futuro cuando adjuntemos el segundo maestro y los nodos de trabajo a nuestro cl√∫ster. </p><br><p>  Ya he dicho que el cl√∫ster de Kubernetes usar√° alg√∫n tipo de red superpuesta para hogares y otros servicios, por lo que en este punto debe instalar alg√∫n tipo de complemento CNI.  Recomiendo el complemento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Weave CNI</a> .  La experiencia ha demostrado que es m√°s √∫til y menos problem√°tico, pero puede elegir otro, por ejemplo, Calico. </p><br><p>  Instalaci√≥n del complemento de red Weave en el primer nodo maestro: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" The connection to the server localhost:8080 was refused - did you specify the right host or port? serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created</code> </pre> <br><p>  Espere un momento y luego ingrese el siguiente comando para verificar que se inicien los hogares de componentes: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 6m25s coredns-86c58d9df4-xj98p 1/1 Running 0 6m25s kube-apiserver-master1 1/1 Running 0 5m22s kube-controller-manager-master1 1/1 Running 0 5m41s kube-proxy-8ncqw 1/1 Running 0 6m25s kube-scheduler-master1 1/1 Running 0 5m25s weave-net-lvwrp 2/2 Running 0 78s</code> </pre> <br><ul><li>  Se recomienda adjuntar nuevos nodos del plano de control solo despu√©s de la inicializaci√≥n del primer nodo. </li></ul><br><p>  Para verificar el estado del cl√∫ster, haga lo siguiente: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 11m v1.13.1</code> </pre> <br><p>  Genial  El primer nodo maestro se levant√≥.  Ahora est√° listo y terminaremos de crear el cl√∫ster de Kubernetes: agregaremos un segundo nodo maestro y nodos de trabajo. <br>  Para agregar un segundo nodo maestro, cree una clave ssh en <strong>master1</strong> y agregue la parte p√∫blica a <strong>master2</strong> .  Realice un inicio de sesi√≥n de prueba y luego copie algunos archivos del primer nodo maestro al segundo: </p><br><pre> <code class="plaintext hljs">master1# scp /etc/kubernetes/pki/ca.crt 192.168.0.6: master1# scp /etc/kubernetes/pki/ca.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.pub 192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.key @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.key @192.168.0.6: master1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.6:etcd-ca.crt master1# scp /etc/kubernetes/admin.conf 192.168.0.6: ### Check that files was copied well master2# ls /root admin.conf ca.crt ca.key etcd-ca.crt front-proxy-ca.crt front-proxy-ca.key sa.key sa.pub</code> </pre> <br><p>  En el segundo nodo maestro, mueva los certificados y claves previamente copiados a los directorios apropiados: </p><br><pre> <code class="plaintext hljs">master2# mkdir -p /etc/kubernetes/pki/etcd mv /root/ca.crt /etc/kubernetes/pki/ mv /root/ca.key /etc/kubernetes/pki/ mv /root/sa.pub /etc/kubernetes/pki/ mv /root/sa.key /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.key /etc/kubernetes/pki/ mv /root/front-proxy-ca.crt /etc/kubernetes/pki/ mv /root/front-proxy-ca.key /etc/kubernetes/pki/ mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /root/admin.conf /etc/kubernetes/admin.conf</code> </pre> <br><p>  Conecte el segundo nodo maestro al cl√∫ster.  Para hacer esto, necesita la salida del comando de conexi√≥n, que <strong><code>kubeadm init</code></strong> nos pas√≥ previamente en el primer nodo. </p><br><p>  Ejecute el nodo maestro <strong>master2</strong> : </p><br><pre> <code class="plaintext hljs">master2# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6 --experimental-control-plane</code> </pre> <br><ul><li>  <strong><code>--experimental-control-plane</code></strong> agregar la <strong><code>--experimental-control-plane</code></strong> .  Automatiza la conexi√≥n de datos maestros a un cl√∫ster.  Sin este indicador, simplemente se agregar√° el nodo de trabajo habitual. </li></ul><br><p>  Espere un poco hasta que el nodo se una al cl√∫ster y verifique el nuevo estado del cl√∫ster: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 32m v1.13.1 master2 Ready master 46s v1.13.1</code> </pre> <br><p>  Tambi√©n aseg√∫rese de que todos los pods de todos los nodos maestros se inicien normalmente: </p><br><pre> <code class="plaintext hljs">master1# kubectl ‚Äî kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 46m coredns-86c58d9df4-xj98p 1/1 Running 0 46m kube-apiserver-master1 1/1 Running 0 45m kube-apiserver-master2 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 45m kube-controller-manager-master2 1/1 Running 0 15m kube-proxy-8ncqw 1/1 Running 0 46m kube-proxy-px5dt 1/1 Running 0 15m kube-scheduler-master1 1/1 Running 0 45m kube-scheduler-master2 1/1 Running 0 15m weave-net-ksvxz 2/2 Running 1 15m weave-net-lvwrp 2/2 Running 0 41m</code> </pre> <br><p>  Genial  Ya casi hemos terminado con la configuraci√≥n del cl√∫ster de Kubernetes.  Y lo √∫ltimo que debe hacer es agregar los tres nodos de trabajo que preparamos anteriormente. </p><br><p>  Ingrese los nodos de trabajo y ejecute el comando de uni√≥n kubeadm sin la <strong><code>--experimental-control-plane</code></strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Verifique nuevamente el estado del cl√∫ster: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h30m v1.13.1 master2 Ready master 1h59m v1.13.1 worker1 Ready &lt;none&gt; 1h8m v1.13.1 worker2 Ready &lt;none&gt; 1h8m v1.13.1 worker3 Ready &lt;none&gt; 1h7m v1.13.1</code> </pre> <br><p>  Como puede ver, tenemos un cl√∫ster Kubernetes HA totalmente configurado con dos nodos maestros y tres nodos de trabajo.  Est√° construido sobre la base del cl√∫ster HA etcd con un equilibrador de carga a prueba de fallas frente a los nodos maestros.  Suena bastante bien para mi. </p><br><h3 id="7-nastroyka-udalennogo-upravleniya-klasterom">  7. Configuraci√≥n de la gesti√≥n remota del cl√∫ster </h3><br><p>  Otra acci√≥n que queda por considerar en esta primera parte del art√≠culo es configurar la utilidad <strong>kubectl</strong> remota para administrar el cl√∫ster.  Anteriormente, <strong>ejecutamos</strong> todos los comandos desde el nodo maestro <strong>master1</strong> , pero esto solo es adecuado por primera vez, al configurar el cl√∫ster.  Ser√≠a bueno configurar un nodo de control externo.  Puede usar una computadora port√°til u otro servidor para esto. </p><br><p>  Inicie sesi√≥n en este servidor y ejecute: </p><br><pre> <code class="plaintext hljs">Add the Google repository key control# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository control# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install kubectl control# apt-get update &amp;&amp; apt-get install -y kubectl In your user home dir create control# mkdir ~/.kube Take the Kubernetes admin.conf from the master1 node control# scp 192.168.0.5:/etc/kubernetes/admin.conf ~/.kube/config Check that we can send commands to our cluster control# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 6h58m v1.13.1 master2 Ready master 6h27m v1.13.1 worker1 Ready &lt;none&gt; 5h36m v1.13.1 worker2 Ready &lt;none&gt; 5h36m v1.13.1 worker3 Ready &lt;none&gt; 5h36m v1.13.1</code> </pre> <br><p>  Ok, ahora ejecutemos una prueba en nuestro cl√∫ster y verifiquemos c√≥mo funciona. </p><br><pre> <code class="plaintext hljs">control# kubectl create deployment nginx --image=nginx deployment.apps/nginx created control# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-5c7588df-6pvgr 1/1 Running 0 52s</code> </pre> <br><p>  Felicidades  Acabas de desplegar Kubernetes.  Y eso significa que su nuevo cl√∫ster HA est√° listo.  De hecho, el proceso de configurar un cl√∫ster de <strong>Kubernetes</strong> usando <strong>kubeadm es</strong> bastante simple y r√°pido. </p><br><p>  En la siguiente parte del art√≠culo, agregaremos almacenamiento interno configurando GlusterFS en todos los nodos de trabajo, configurando un equilibrador de carga interno para nuestro cl√∫ster Kubernetes y tambi√©n ejecutando ciertas pruebas de estr√©s, desconectando algunos nodos y verificando la estabilidad del cl√∫ster. </p><br><h3 id="posleslovie">  Ep√≠logo </h3><br><p>  S√≠, trabajando en este ejemplo, encontrar√° una serie de problemas.  No debe preocuparse: para deshacer los cambios y devolver los nodos a su estado original, simplemente ejecute <strong>kubeadm reset</strong> : los cambios que realiz√≥ anteriormente <strong>Kubeadm</strong> se restablecer√°n y podr√° volver a configurarlos.  Adem√°s, no olvide verificar el estado de los contenedores Docker en los nodos del cl√∫ster; aseg√∫rese de que todos se inicien y funcionen sin errores.  Para obtener m√°s informaci√≥n sobre los contenedores da√±ados, use el <strong>comando docker logs containerid</strong> . </p><br><p>  Eso es todo por hoy.  Buena suerte </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/439562/">https://habr.com/ru/post/439562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../439550/index.html">Hackquest 2018. Resultados y rese√±as. D√≠a 1-3</a></li>
<li><a href="../439552/index.html">Extensiones maliciosas de Chrome</a></li>
<li><a href="../439556/index.html">TDMS Fairway. Metodolog√≠as PMBOK y organizaciones de dise√±o rusas.</a></li>
<li><a href="../439558/index.html">Nuevo tel√©fono viejo. Reinventar el tel√©fono PSTN</a></li>
<li><a href="../439560/index.html">Adaptador Ethereum blockchain para la plataforma de datos InterSystems IRIS</a></li>
<li><a href="../439564/index.html">Aplicaci√≥n pr√°ctica de la transformaci√≥n del √°rbol AST utilizando Putout como ejemplo</a></li>
<li><a href="../439566/index.html">Por qu√© es importante la documentaci√≥n de SRE. Parte 3</a></li>
<li><a href="../439568/index.html">SSD basados ‚Äã‚Äãen QLC: ¬øun asesino del disco duro? No realmente</a></li>
<li><a href="../439570/index.html">Magia de IPython para editar etiquetas de celda de Jupyter</a></li>
<li><a href="../439572/index.html">Dise√±o asistido por computadora de equipos electr√≥nicos.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>