<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§£ ‚ò¶Ô∏è üõ¥ Armless Admin = Hyperkonvergenz? üêæ üíê üõåüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dies ist ein Mythos, der auf dem Gebiet der Serverhardware weit verbreitet ist. In der Praxis brauchen hyperkonvergente L√∂sungen (wenn alles in einem)...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Armless Admin = Hyperkonvergenz?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croc/blog/471508/"><img src="https://habrastorage.org/webt/_a/wu/gy/_awugy9unrbypap90hlhc86-maa.png"><br><img src="https://habrastorage.org/webt/_k/kx/sa/_kkxsa4xs7rgz-fzktgy10mdeuc.png"><br><br>  Dies ist ein Mythos, der auf dem Gebiet der Serverhardware weit verbreitet ist.  In der Praxis brauchen hyperkonvergente L√∂sungen (wenn alles in einem) viel f√ºr was.  In der Vergangenheit wurden die ersten Architekturen von Amazon und Google f√ºr ihre Dienste entwickelt.  Dann bestand die Idee darin, eine Computerfarm aus denselben Knoten zu erstellen, von denen jeder seine eigenen Laufwerke hat.  All dies wurde von einer systembildenden Software (Hypervisor) kombiniert und bereits in virtuelle Maschinen unterteilt.  Die Hauptaufgabe ist ein Minimum an Aufwand f√ºr die Wartung eines Knotens und ein Minimum an Skalierungsproblemen: Wir haben gerade weitere tausend oder zwei gleiche Server gekauft und in der N√§he eine Verbindung hergestellt.  In der Praxis handelt es sich um Einzelf√§lle, und viel h√§ufiger handelt es sich um eine geringere Anzahl von Knoten und eine etwas andere Architektur. <br><br>  Das Plus bleibt jedoch das gleiche - die unglaubliche Leichtigkeit der Skalierung und Steuerung.  Minus - verschiedene Aufgaben verbrauchen Ressourcen unterschiedlich, und irgendwo gibt es viele lokale Festplatten, irgendwo gibt es wenig RAM usw., dh bei verschiedenen Arten von Aufgaben sinkt die Ressourcennutzung. <br><br>  Es stellte sich heraus, dass Sie f√ºr eine einfache Einrichtung 10-15% mehr bezahlen.  Dies verursachte den Schlagzeilen-Mythos.  Wir haben lange gesucht, wo die Technologie optimal angewendet wird, und sie gefunden.  Tatsache ist, dass Tsiska keine eigenen Speichersysteme hatte, aber einen vollst√§ndigen Servermarkt wollte.  Und sie haben Cisco Hyperflex zu einer lokalen Speicherl√∂sung auf Knoten gemacht. <br><br>  Und dies stellte sich pl√∂tzlich als sehr gute L√∂sung f√ºr Backup-Rechenzentren heraus (Disaster Recovery).  Warum und wie - jetzt werde ich es erz√§hlen.  Und ich werde Clustertests zeigen. <a name="habracut"></a><br><br><h3>  Wohin? </h3><br>  Hyperkonvergenz ist: <br><br><ol><li>  √úbertragen Sie Datentr√§ger auf Rechenknoten. </li><li>  Vollst√§ndige Integration des Speichersubsystems in das Virtualisierungssubsystem. </li><li>  √úbertragung / Integration mit dem Netzwerksubsystem. </li></ol><br>  Mit einer solchen Kombination k√∂nnen Sie viele Funktionen von Speichersystemen auf Virtualisierungsebene und alle in einem Steuerungsfenster implementieren. <br><br>  In unserem Unternehmen sind Projekte zum Entwerfen redundanter Rechenzentren sehr beliebt, und h√§ufig wird h√§ufig die hyperkonvergente L√∂sung ausgew√§hlt, da viele Replikationsoptionen (bis zum Metro-Cluster) sofort verf√ºgbar sind. <br><br>  Bei Backup-Rechenzentren handelt es sich normalerweise um eine Remote-Einrichtung an einem Standort auf der anderen Seite der Stadt oder in einer anderen Stadt im Allgemeinen.  Sie k√∂nnen kritische Systeme im Falle eines teilweisen oder vollst√§ndigen Ausfalls des Hauptrechenzentrums wiederherstellen.  Dort werden Verkaufsdaten st√§ndig repliziert, und diese Replikation kann auf Anwendungsebene oder auf Blockger√§teebene (SHD) erfolgen. <br><br>  Jetzt werde ich √ºber das Ger√§t und die Tests des Systems sprechen und dann √ºber einige reale Szenarien mit Daten zu Einsparungen. <br><br><h3>  Tests </h3><br>  Unsere Kopie besteht aus vier Servern mit jeweils 10 SSD-Festplatten pro 960 GB.  Es gibt eine dedizierte Festplatte zum Zwischenspeichern von Schreibvorg√§ngen und zum Speichern der virtuellen Dienstmaschine.  Die L√∂sung selbst ist die vierte Version.  Der erste ist ehrlich gesagt roh (nach den Bewertungen zu urteilen), der zweite ist feucht, der dritte ist bereits ziemlich stabil, und dieser kann nach dem Ende des Beta-Tests f√ºr die breite √ñffentlichkeit als Ver√∂ffentlichung bezeichnet werden.  W√§hrend des Testens habe ich keine Probleme gesehen, alles funktioniert wie eine Uhr. <br><br><div class="spoiler">  <b class="spoiler_title">√Ñnderungen in v4</b> <div class="spoiler_text">  Einige Fehler wurden behoben. <br><br>  Anf√§nglich konnte die Plattform nur mit dem VMware ESXi-Hypervisor arbeiten und unterst√ºtzte eine kleine Anzahl von Knoten.  Au√üerdem wurde der Bereitstellungsprozess nicht immer erfolgreich beendet, ich musste einige Schritte neu starten, es gab Probleme beim Aktualisieren von alten Versionen, die Daten in der GUI wurden nicht immer korrekt angezeigt (obwohl ich immer noch nicht mit der Anzeige von Leistungsdiagrammen zufrieden bin), manchmal gab es Probleme an der Schnittstelle mit der Virtualisierung . <br><br>  Nachdem alle Wunden der Kinder behoben wurden, kann HyperFlex sowohl ESXi als auch Hyper-V ausf√ºhren. Dies ist au√üerdem m√∂glich: <br><br><ol><li>  Erstellen eines gestreckten Clusters. </li><li>  Erstellen eines Clusters f√ºr B√ºros ohne Verwendung von Fabric Interconnect mit zwei bis vier Knoten (wir kaufen nur Server). </li><li>  F√§higkeit, mit externem Speicher zu arbeiten. </li><li>  Unterst√ºtzung f√ºr Container und Kubernetes. </li><li>  Erstellung von Barrierefreiheitszonen. </li><li>  Integration mit VMware SRM, wenn die integrierte Funktionalit√§t nicht passt. </li></ol><br></div></div><br>  Die Architektur unterscheidet sich nicht wesentlich von den Entscheidungen der Hauptkonkurrenten, sie haben nicht begonnen, ein Fahrrad zu bauen.  Alles funktioniert auf der VMware- oder Hyper-V-Virtualisierungsplattform.  Hardware, die auf propriet√§ren Cisco UCS-Servern gehostet wird.  Es gibt diejenigen, die die Plattform wegen der relativen Komplexit√§t der Ersteinrichtung hassen, viele Schaltfl√§chen, ein nicht triviales System von Vorlagen und Abh√§ngigkeiten, aber es gibt auch diejenigen, die Zen gelernt haben, von der Idee inspiriert waren und nicht mehr mit anderen Servern arbeiten m√∂chten. <br><br>  Wir werden die L√∂sung speziell f√ºr VMware in Betracht ziehen, da die L√∂sung urspr√ºnglich f√ºr VMware entwickelt wurde und √ºber mehr Funktionen verf√ºgt. Hyper-V wurde hinzugef√ºgt, um mit den Wettbewerbern Schritt zu halten und die Markterwartungen zu erf√ºllen. <br><br>  Es gibt einen Cluster von Servern voller Festplatten.  Es gibt Festplatten zur Datenspeicherung (SSD oder HDD - nach Ihrem Geschmack und Ihren Bed√ºrfnissen), es gibt eine SSD-Festplatte zum Zwischenspeichern.  Wenn Daten in den Datenspeicher geschrieben werden, werden Daten auf der Caching-Schicht (dedizierte SSD-Festplatte und Service-VM-RAM) gespeichert.  Parallel dazu wird der Datenblock an die Knoten im Cluster gesendet (die Anzahl der Knoten h√§ngt vom Clusterreplikationsfaktor ab).  Nach der Best√§tigung aller Knoten √ºber die erfolgreiche Aufzeichnung wird die Best√§tigung der Aufzeichnung an den Hypervisor und dann an die VM gesendet.  Aufgezeichnete Daten im Hintergrund werden dedupliziert, komprimiert und auf Speicherplatten geschrieben.  Gleichzeitig wird ein gro√üer Block immer nacheinander auf Speicherplatten geschrieben, wodurch die Belastung der Speicherplatten verringert wird. <br><br>  Deduplizierung und Komprimierung sind immer aktiviert und k√∂nnen nicht deaktiviert werden.  Daten werden direkt von Speicherplatten oder aus dem RAM-Cache gelesen.  Wenn eine Hybridkonfiguration verwendet wird, wird der Lesevorgang auch auf der SSD zwischengespeichert. <br><br>  Die Daten sind nicht an den aktuellen Standort der virtuellen Maschine gebunden und werden gleichm√§√üig auf die Knoten verteilt.  Mit diesem Ansatz k√∂nnen Sie alle Laufwerke und Netzwerkschnittstellen gleicherma√üen laden.  Das offensichtliche Minus lautet: Wir k√∂nnen die Leseverz√∂gerung nicht minimieren, da keine Garantie f√ºr die lokale Datenverf√ºgbarkeit besteht.  Aber ich glaube, dass dies ein unbedeutendes Opfer im Vergleich zu den erhaltenen Pluspunkten ist.  Dar√ºber hinaus haben Netzwerkverz√∂gerungen solche Werte erreicht, dass sie das Gesamtergebnis praktisch nicht beeinflussen. <br><br>  F√ºr die gesamte Logik des Festplattensubsystems ist eine spezielle Service-VM des Cisco HyperFlex Data Platform-Controllers verantwortlich, die auf jedem Speicherknoten erstellt wird.  In unserer Service-VM-Konfiguration wurden acht vCPUs und 72 GB RAM zugewiesen, was nicht so klein ist.  Ich m√∂chte Sie daran erinnern, dass der Host selbst √ºber 28 physische Kerne und 512 GB RAM verf√ºgt. <br><br>  Die Service-VM hat direkten Zugriff auf physische Festplatten, indem der SAS-Controller an die VM weitergeleitet wird.  Die Kommunikation mit dem Hypervisor erfolgt √ºber ein spezielles IOVisor-Modul, das E / A-Vorg√§nge abf√§ngt, und √ºber einen Agenten, mit dem Sie Befehle an die Hypervisor-API √ºbertragen k√∂nnen.  Der Agent ist f√ºr die Arbeit mit HyperFlex-Snapshots und -Klonen verantwortlich. <br><br>  Im Hypervisor werden Festplattenressourcen als NFS- oder SMB-Ball bereitgestellt (je nach Typ des Hypervisors raten Sie, welcher).  Und unter der Haube ist dies ein verteiltes Dateisystem, mit dem Sie Funktionen vollwertiger Speichersysteme f√ºr Erwachsene hinzuf√ºgen k√∂nnen: Zuweisung d√ºnner Volumes, Komprimierung und Deduplizierung, Snapshots mithilfe der Redirect-on-Write-Technologie, synchrone / asynchrone Replikation. <br><br>  Service VM bietet Zugriff auf die WEB-Schnittstelle der HyperFlex-Subsystemverwaltung.  Es gibt eine Integration mit vCenter, und die meisten t√§glichen Aufgaben k√∂nnen von dort aus ausgef√ºhrt werden. Datenspeicher k√∂nnen jedoch bequemer von einer separaten Webcam abgeschnitten werden, wenn Sie bereits auf eine schnelle HTML5-Oberfl√§che umgestellt haben oder einen vollst√§ndigen Flash-Client mit vollst√§ndiger Integration verwenden.  In der Service-Webcam k√∂nnen Sie die Leistung und den detaillierten Status des Systems anzeigen. <br><br><img src="https://habrastorage.org/webt/o0/bj/od/o0bjod1zrf25ubnrtis5q4e1ywc.png"><br><br>  Es gibt eine andere Art von Knoten in einem Cluster - Rechenknoten.  Es k√∂nnen Rack- oder Blade-Server ohne integrierte Laufwerke sein.  Auf diesen Servern k√∂nnen Sie VMs ausf√ºhren, deren Daten auf Servern mit Festplatten gespeichert sind.  Unter dem Gesichtspunkt des Datenzugriffs gibt es keinen Unterschied zwischen den Knotentypen, da die Architektur das Abstrahieren vom physischen Standort der Daten umfasst.  Das maximale Verh√§ltnis von Rechenknoten und Speicherknoten betr√§gt 2: 1. <br><br>  Die Verwendung von Rechenknoten erh√∂ht die Flexibilit√§t bei der Skalierung von Clusterressourcen: Wir m√ºssen keine Knoten mit Festplatten kaufen, wenn wir nur CPU / RAM ben√∂tigen.  Dar√ºber hinaus k√∂nnen wir einen Blade-Korb hinzuf√ºgen und Platz auf dem Rack-Server sparen. <br><br>  Als Ergebnis haben wir eine hyperkonvergente Plattform mit den folgenden Funktionen: <br><br><ul><li>  Bis zu 64 Knoten in einem Cluster (bis zu 32 Speicherknoten). </li><li>  Die Mindestanzahl von Knoten in einem Cluster betr√§gt drei (zwei f√ºr einen Edge-Cluster). </li><li>  Datenredundanzmechanismus: Spiegelung mit Replikationsfaktor 2 und 3. </li><li>  Metro-Cluster. </li><li>  Asynchrone VM-Replikation auf einen anderen HyperFlex-Cluster. </li><li>  Orchestrierung des Wechsels von VMs zu einem Remote-Rechenzentrum. </li><li>  Native Snapshots mit Redirect-on-Write-Technologie. </li><li>  Bis zu 1 PB nutzbarer Speicherplatz mit Replikationsfaktor 3 und ohne Deduplizierung.  Replikationsfaktor 2 wird nicht ber√ºcksichtigt, da dies keine Option f√ºr ernsthafte Verk√§ufe ist. </li></ul><br>  Ein weiteres gro√ües Plus ist die einfache Verwaltung und Bereitstellung.  Alle Komplexit√§ten bei der Konfiguration von UCS-Servern werden von einer speziellen VM √ºbernommen, die von Cisco-Ingenieuren erstellt wurde. <br><br><h3>  Testbed-Konfiguration: </h3><br><ul><li>  2 x Cisco UCS Fabric Interconnect 6248UP als Verwaltungscluster und Netzwerkkomponenten (48 Ports im Ethernet 10G / FC 16G-Modus). </li><li>  Vier Cisco UCS HXAF240 M4-Server. </li></ul><br>  Serverfunktionen: <br><p></p><div class="scrollable-table"><table><tbody><tr><td><br><p>  CPU </p><br></td><td><br><p>  2 x Intel ¬Æ Xeon ¬Æ E5-2690 v4 </p><br></td></tr><tr><td><br><p>  RAM </p><br></td><td><br><p>  16 x 32 GB DDR4-2400-MHz-RDIMM / PC4-19200 / Dual Rank / x4 / 1,2 V. </p><br></td></tr><tr><td><br><p>  Netzwerk </p><br></td><td><br><p>  UCSC-MLOM-CSC-02 (VIC 1227).  2 x 10G Ethernet </p><br></td></tr><tr><td><br><p>  Speicher hba </p><br></td><td><br><p>  Cisco 12G Modular SAS Pass Through Controller </p><br></td></tr><tr><td><br><p>  Speicherplatten </p><br></td><td><br><p>  1 x SSD Intel S3520 120 GB, 1 x SSD Samsung MZ-IES800D, 10 x SSD Samsung PM863a 960 GB </p><br></td></tr></tbody></table></div><br><br><div class="spoiler">  <b class="spoiler_title">Weitere Konfigurationsoptionen</b> <div class="spoiler_text">  Neben dem ausgew√§hlten B√ºgeleisen stehen derzeit folgende Optionen zur Verf√ºgung: <br><br><ul><li>  HXAF240c M5. </li><li>  Eine oder zwei CPUs von Intel Silver 4110 bis Intel Platinum I8260Y.  Die zweite Generation ist verf√ºgbar. </li><li>  24 Speichersteckpl√§tze, Lamellen von 16 GB RDIMM 2600 bis 128 GB LRDIMM 2933. </li><li>  6 bis 23 Datentr√§ger f√ºr Daten, ein Caching-Datentr√§ger, ein System und ein Startdatentr√§ger. </li></ul><br>  <b>Kapazit√§tslaufwerke</b> <br><br><ul><li>  HX-SD960G61X-EV 960 GB 2,5-Zoll-Enterprise-Value-6G-SATA-SSD (1-fache Lebensdauer) SAS 960 GB. </li><li>  HX-SD38T61X-EV 3,8 TB 2,5 Zoll Enterprise Value 6G SATA SSD (1X Ausdauer) SAS 3,8 TB. </li><li>  Treiber zwischenspeichern </li><li>  HX-NVMEXPB-I375 375 GB 2,5-Zoll-Intel Optane-Laufwerk, extreme Leistung und Ausdauer. </li><li>  HX-NVMEHW-H1600 * 1,6 TB 2,5 Zoll Ent.  Perf  NVMe SSD (3X Ausdauer) NVMe 1,6 TB. </li><li>  HX-SD400G12TX-EP 400 GB 2,5 Zoll Ent.  Perf  12G SAS SSD (10X Ausdauer) SAS 400 GB. </li><li>  HX-SD800GBENK9 ** 800GB 2,5 Zoll Ent.  Perf  12G SAS SED SSD (10X Ausdauer) SAS 800 GB. </li><li>  HX-SD16T123X-EP 1,6 TB 2,5-Zoll-12G-SAS-SSD mit Unternehmensleistung (3-fache Lebensdauer). </li></ul><br>  <b>System- / Protokolllaufwerke</b> <br><br><ul><li>  HX-SD240GM1X-EV 240 GB 2,5 Zoll Enterprise Value 6G SATA-SSD (Upgrade erforderlich). </li></ul><br>  <b>Boot-Treiber</b> <br><br><ul><li>  HX-M2-240 GB 240 GB SATA M.2 SSD SATA 240 GB. </li></ul><br></div></div><br>  Verbindung zu einem Netzwerk √ºber 40G-, 25G- oder 10G-Ethernet-Ports. <br><br>  Da FI HX-FI-6332 (40G), HX-FI-6332-16UP (40G), HX-FI-6454 (40G / 100G) sein kann. <br><br><h3>  Testen Sie sich </h3><br>  Zum Testen des Festplattensubsystems habe ich HCIBench 2.2.1 verwendet.  Dies ist ein kostenloses Dienstprogramm, mit dem Sie die Erstellung von Lasten aus mehreren virtuellen Maschinen automatisieren k√∂nnen.  Die Last selbst wird durch regul√§res fio erzeugt. <br><br>  Unser Cluster besteht aus vier Knoten, Replikationsfaktor 3, allen Flash-Laufwerken. <br><br>  Zum Testen habe ich vier Datenspeicher und acht virtuelle Maschinen erstellt.  Bei Schreibtests wird davon ausgegangen, dass die Caching-Festplatte nicht voll ist. <br><br>  Die Testergebnisse sind wie folgt: <br><div class="scrollable-table"><table><tbody><tr><td></td><td colspan="5"><br><p>  100% 100% zuf√§llig lesen </p><br></td><td colspan="5"><br><p>  0% 100% zuf√§llig lesen </p><br></td></tr><tr><td><br><p>  Block- / Warteschlangentiefe </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td></tr><tr><td><br><p>  4K </p><br></td><td><br><p>  0,59 ms 213804 IOPS </p><br></td><td><br><p>  0,84 ms 303540 IOPS </p><br></td><td><br><p>  1,36 ms 374348 IOPS </p><br></td><td><br><p>  2,47 ms 414116 IOPS </p><br></td><td><br><p>  <b>4,86 ms 420180 IOPS</b> </p><br></td><td><br><p>  2,22 ms 57408 IOPS </p><br></td><td><br><p>  3,09 ms 82744 IOPS </p><br></td><td><br><p>  5,02 ms 101824 IPOS </p><br></td><td><br><p>  8,75 ms 116912 IOPS </p><br></td><td><br><p>  <b>17,2 ms 118592 IOPS</b> </p><br></td></tr><tr><td><br><p>  8K </p><br></td><td><br><p>  0,67 ms 188416 IOPS </p><br></td><td><br><p>  0,93 ms 273280 IOPS </p><br></td><td><br><p>  1,7 ms 299932 IOPS </p><br></td><td><br><p>  2,72 ms 376,484 IOPS </p><br></td><td><br><p>  <b>5,47 ms 373,176 IOPS</b> </p><br></td><td><br><p>  3,1 ms 41148 IOPS </p><br></td><td><br><p>  4,7 ms 54396 IOPS </p><br></td><td><br><p>  7,09 ms 72192 IOPS </p><br></td><td><br><p>  <b>12,77 ms 80,132 IOPS</b> </p><br></td><td></td></tr><tr><td><br><p>  16K </p><br></td><td><br><p>  0,77 ms 164116 IOPS </p><br></td><td><br><p>  1,12 ms 228328 IOPS </p><br></td><td><br><p>  1,9 ms 268140 IOPS </p><br></td><td><br><p>  <b>3,96 ms 258480 IOPS</b> </p><br></td><td></td><td><br><p>  3,8 ms 33640 IOPS </p><br></td><td><br><p>  6,97 ms 36696 IOPS </p><br></td><td><br><p>  <b>11,35 ms 45060 IOPS</b> </p><br></td><td></td><td></td></tr><tr><td><br><p>  32K </p><br></td><td><br><p>  1,07 ms 119292 IOPS </p><br></td><td><br><p>  1,79 ms 142888 IOPS </p><br></td><td><br><p>  <b>3,56 ms 143760 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  7,17 ms 17810 IOPS </p><br></td><td><br><p>  <b>11,96 ms 21396 IOPS</b> </p><br></td><td></td><td></td><td></td></tr><tr><td><br><p>  64K </p><br></td><td><br><p>  1,84 ms 69440 IOPS </p><br></td><td><br><p>  3,6 ms 71008 IOPS </p><br></td><td><br><p>  <b>7,26 ms 70404 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  <b>11,37 ms 11248 IOPS</b> </p><br></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><br>  <i>Fettgedruckte Werte sind angegeben, wonach keine Produktivit√§tssteigerung mehr auftritt, manchmal ist sogar eine Verschlechterung sichtbar.</i>  <i>Aufgrund der Tatsache, dass wir uns auf die Netzwerkleistung / Controller / Laufwerke st√ºtzen.</i> <br><br><ul><li>  Sequentielles Lesen von 4432 MB / s. </li><li>  Sequentielles Schreiben von 804 MB / s. </li><li>  Wenn ein Controller ausf√§llt (Ausfall einer virtuellen Maschine oder eines Hosts), wird der Leistungsabfall verdoppelt. </li><li>  Wenn das Speicherlaufwerk ausf√§llt, betr√§gt der Drawdown 1/3.  Die Rebild-Festplatte beansprucht 5% der Ressourcen jedes Controllers. </li></ul><br>  Auf einem kleinen Block sto√üen wir auf die Leistung des Controllers (virtuelle Maschine), dessen CPU zu 100% ausgelastet ist, w√§hrend wir den Block erh√∂hen, auf den die Portbandbreite l√§uft.  10 Gbit / s reichen nicht aus, um das Potenzial des AllFlash-Systems auszusch√∂pfen.  Leider erlauben die Parameter des bereitgestellten Demostands keine √úberpr√ºfung der Arbeit mit 40 Gbit / s. <br><br>  In meinem Eindruck von den Tests und dem Studium der Architektur erhalten wir aufgrund des Algorithmus, der Daten zwischen allen Hosts platziert, eine skalierbare vorhersagbare Leistung. Dies ist jedoch auch eine Einschr√§nkung beim Lesen, da hier mehr von lokalen Datentr√§gern und mehr gequetscht werden k√∂nnte Um ein produktiveres Netzwerk zu speichern, stehen beispielsweise 40-Gbit / s-FIs zur Verf√ºgung. <br><br>  Au√üerdem kann eine Festplatte f√ºr das Caching und die Deduplizierung eine Einschr√§nkung darstellen. In diesem Stand k√∂nnen wir sogar auf vier SSD-Festplatten schreiben.  Es w√§re gro√üartig, die Anzahl der zwischengespeicherten Festplatten erh√∂hen zu k√∂nnen und den Unterschied zu erkennen. <br><br><h3>  Echte Verwendung </h3><br>  Zum Organisieren eines Backup-Rechenzentrums k√∂nnen zwei Ans√§tze verwendet werden (wir ziehen nicht in Betracht, Backups an einem Remote-Standort zu platzieren): <br><br><ol><li>  Aktiv Passiv  Alle Anwendungen werden im Hauptrechenzentrum gehostet.  Die Replikation ist synchron oder asynchron.  Im Falle eines Sturzes im Hauptrechenzentrum m√ºssen wir das Backup aktivieren.  Dies kann manuell / mit Skripten / Orchestrierungsanwendungen erfolgen.  Hier erhalten wir ein RPO, das der Replikationsh√§ufigkeit entspricht, und das RTO h√§ngt von der Reaktion und den F√§higkeiten des Administrators und der Qualit√§t der Entwicklung / des Debuggens des Switching-Plans ab. </li><li>  Aktiv Aktiv  In diesem Fall ist nur eine synchrone Replikation vorhanden. Die Verf√ºgbarkeit von Rechenzentren wird durch ein Quorum / Arbiter bestimmt, das ausschlie√ülich auf der dritten Plattform platziert ist.  RPO = 0, und RTO kann 0 erreichen (sofern die Anwendung dies zul√§sst) oder gleich der Zeit f√ºr ein Failover eines Knotens in einem Virtualisierungscluster.  Auf der Virtualisierungsebene wird ein gestreckter (Metro) Cluster erstellt, f√ºr den Active-Active-Speicher erforderlich ist. </li></ol><br>  Normalerweise sehen wir bei Kunden eine bereits implementierte Architektur mit klassischem Speicher im Hauptrechenzentrum, daher entwerfen wir eine andere f√ºr die Replikation.  Wie bereits erw√§hnt, bietet Cisco HyperFlex eine asynchrone Replikation und die Erstellung eines erweiterten Virtualisierungsclusters.  Gleichzeitig ben√∂tigen wir kein dediziertes Midrange- oder h√∂heres Speichersystem mit den teuren Funktionen der Replikation und des Active-Active-Datenzugriffs auf zwei Speichersystemen. <br><br>  <b>Szenario 1:</b> Wir haben prim√§re und Backup-Rechenzentren, eine Virtualisierungsplattform auf VMware vSphere.  Alle Produktivsysteme befinden sich haupts√§chlich im Rechenzentrum. Die Replikation der virtuellen Maschine wird auf Hypervisor-Ebene durchgef√ºhrt. Dadurch k√∂nnen die VMs im Sicherungs-Rechenzentrum nicht eingeschaltet bleiben.  Wir replizieren Datenbanken und spezielle Anwendungen mit integrierten Tools und lassen VMs eingeschaltet.  Wenn das Hauptdatenzentrum ausf√§llt, starten wir das System im Sicherungsdatenzentrum.  Wir glauben, dass wir ungef√§hr 100 virtuelle Maschinen haben.  Solange das Hauptdatenzentrum in Betrieb ist, k√∂nnen Testumgebungen und andere Systeme im Sicherungsdatenzentrum gestartet werden, die deaktiviert werden k√∂nnen, wenn das Hauptdatenzentrum umgeschaltet wird.  Es ist auch m√∂glich, dass wir die bidirektionale Replikation verwenden.  Aus Sicht der Ausr√ºstung wird sich nichts √§ndern. <br><br>  Bei der klassischen Architektur werden wir in jedem Rechenzentrum ein hybrides Speichersystem mit Zugriff √ºber FibreChannel, Zerrei√üen, Deduplizieren und Komprimieren (jedoch nicht online), 8 Servern pro Standort, 2 FibreChannel-Switches und Ethernet 10G installieren.  F√ºr die Replikations- und Switching-Steuerung in einer klassischen Architektur k√∂nnen wir VMware-Tools (Replication + SRM) oder Tools von Drittanbietern verwenden, die etwas billiger und manchmal bequemer sind. <br><br>  Die Abbildung zeigt ein Diagramm. <br><br><img src="https://habrastorage.org/webt/ej/an/92/ejan92jvmtt1ejtbtd1eyytngw8.png"><br><br>  Wenn Sie Cisco HyperFlex verwenden, erhalten Sie die folgende Architektur: <br><br><img src="https://habrastorage.org/webt/9w/xs/hl/9wxshlz45c53uyitqulmrmwlraq.png"><br><br>  F√ºr HyperFlex habe ich Server mit gro√üen CPU / RAM-Ressourcen verwendet, z  Ein Teil der Ressourcen wird an die VM des HyperFlex-Controllers gesendet. Ich habe sogar ein wenig in die HyperFlex-Konfiguration auf der CPU und im Speicher neu geladen, um nicht neben Cisco zu spielen und Ressourcen f√ºr den Rest der VMs zu garantieren.  Wir k√∂nnen jedoch FibreChannel-Switches ablehnen, und wir ben√∂tigen nicht f√ºr jeden Server Ethernet-Ports. Der lokale Datenverkehr wird innerhalb von FI umgeschaltet. <br><br>  Das Ergebnis ist die folgende Konfiguration f√ºr jedes Rechenzentrum: <br><div class="scrollable-table"><table><tbody><tr><td><br><p>  Server </p><br></td><td><br><p>  8 x 1U Server (384 GB RAM, 2 x Intel Gold 6132, FC HBA) </p><br></td><td><br><p>  8 x HX240C-M5L (512 GB RAM, 2 x Intel Gold 6150, 3,2 GB SSD, 10 x 6 TB NL-SAS) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  Hybridspeicher mit FC-Front-End (20 TB SSD, 130 TB NL-SAS) </p><br></td><td><br><p>  - - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  2 x Ethernet Switch 10G 12 Ports </p><br></td><td><br><p>  - - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  2 x FC Switch 32 / 16Gb 24 Ports </p><br></td><td><br><p>  2 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Lizenzen </p><br></td><td><br><p>  VMware Ent Plus </p><br><p>  VM-Replikation und / oder Orchestrierung </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  F√ºr Hyperflex habe ich keine Replikationssoftwarelizenzen zugesagt, da diese bei uns sofort verf√ºgbar sind. <br><br>  F√ºr die klassische Architektur habe ich einen Anbieter ausgew√§hlt, der sich als qualitativ hochwertiger und kosteng√ºnstiger Hersteller etabliert hat.  F√ºr beide Optionen habe ich einen Standard f√ºr einen bestimmten L√∂sungs-Skid verwendet, bei der Ausgabe habe ich echte Preise erhalten. <br><br>  Die L√∂sung auf Cisco HyperFlex war 13% billiger. <br><br>  <b>Szenario 2:</b> Erstellen von zwei aktiven Rechenzentren.  In diesem Szenario entwerfen wir einen erweiterten Cluster auf VMware. <br><br>  Die klassische Architektur besteht aus Virtualisierungsservern, SAN (FC-Protokoll) und zwei Speichersystemen, die auf dem zwischen ihnen gespannten lesen und schreiben k√∂nnen.  Auf jedem SHD legen wir eine n√ºtzliche Kapazit√§t f√ºr das Schloss. <br><br><img src="https://habrastorage.org/webt/lm/1i/yu/lm1iyuqivylf7dl0xrmff21swlw.png"><br><br>  Bei HyperFlex erstellen wir einfach einen Stretch-Cluster mit der gleichen Anzahl von Knoten an beiden Standorten.  In diesem Fall wird der Replikationsfaktor 2 + 2 verwendet. <br><br><img src="https://habrastorage.org/webt/e7/mq/sd/e7mqsd7codjtfbqefl40icsohpu.png"><br><br>  Die folgende Konfiguration hat sich herausgestellt: <br><div class="scrollable-table"><table><tbody><tr><td></td><td><br><p>  Klassische Architektur </p><br></td><td><br><p>  Hyperflex </p><br></td></tr><tr><td><br><p>  Server </p><br></td><td><br><p>  16 x 1 HE Server (384 GB RAM, 2 x Intel Gold 6132, FC HBA, 2 x 10 G NIC) </p><br></td><td><br><p>  16 x HX240C-M5L (512 GB RAM, 2 x Intel Gold 6132, 1,6 TB NVMe, 12 x 3,8 TB SSD, VIC 1387) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  2 x AllFlash-Speicher (150 TB SSD) </p><br></td><td><br><p>  - - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  4 x Ethernet Switch 10G 24 Ports </p><br></td><td><br><p>  - - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  4 x FC Switch 32 / 16Gb 24 Ports </p><br></td><td><br><p>  4 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Lizenzen </p><br></td><td><br><p>  VMware Ent Plus </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  Bei allen Berechnungen habe ich die Netzwerkinfrastruktur, die Kosten f√ºr Rechenzentren usw. nicht ber√ºcksichtigt: Sie sind f√ºr die klassische Architektur und f√ºr die HyperFlex-L√∂sung gleich. <br><br>  Zu Anschaffungskosten erwies sich HyperFlex als 5% teurer.  Es ist erw√§hnenswert, dass ich f√ºr die CPU / RAM-Ressourcen eine Tendenz f√ºr Cisco habe, weil es in der Konfiguration die Kan√§le der Speichercontroller gleichm√§√üig f√ºllte.  Die Kosten sind etwas h√∂her, aber nicht um eine Gr√∂√üenordnung, was eindeutig darauf hinweist, dass Hyperkonvergenz nicht unbedingt ein "Spielzeug f√ºr die Reichen" ist, sondern mit dem Standardansatz zum Aufbau eines Rechenzentrums konkurrieren kann.      ,      Cisco UCS     . <br><br>        SAN  , -  ,      (, ,   ‚Äî ),   (    ),  . <br><br>   ,         ‚Äî Cisco.        Cisco UCS,    ,  HyperFlex     ,    .          ,     .       : ¬´    ,  ?¬ª  ¬´  - ,     . !¬ª ‚Äî           , : ¬´    ¬ª   . <br><br><h3>  Referenzen </h3><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> -</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">-   </a> </li><li>   ‚Äî StGeneralov@croc.ru </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de471508/">https://habr.com/ru/post/de471508/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de471498/index.html">Die Karte der Stra√üenfixierungskameras wird ver√∂ffentlicht: sich freuen oder weinen?</a></li>
<li><a href="../de471500/index.html">R√ºckruf oder ‚ÄûSteigerung der Kundenbindung‚Äú</a></li>
<li><a href="../de471502/index.html">Ideenfarm</a></li>
<li><a href="../de471504/index.html">Zweidimensionales Duett: Erzeugung von Borofen-Graphen-Heterostrukturen</a></li>
<li><a href="../de471506/index.html">Korrekte Rundung von Dezimalzahlen im Bin√§rcode</a></li>
<li><a href="../de471512/index.html">28. Oktober, Jekaterinburg - Qualit√§tskommunikation</a></li>
<li><a href="../de471514/index.html">Die √úberschrift "Artikel f√ºr Sie lesen." Januar - Juni 2019</a></li>
<li><a href="../de471516/index.html">Intel 665p - SSD mit 96-Layer-QLC-NAND</a></li>
<li><a href="../de471518/index.html">Apple im Jahr 2019 ist Linux im Jahr 2000</a></li>
<li><a href="../de471520/index.html">Das Buch "Klassische Informatik Aufgaben in Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>