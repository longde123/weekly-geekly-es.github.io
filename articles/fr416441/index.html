<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüîß üò™ üë®‚Äçüë©‚Äçüë¶‚Äçüë¶ Pitch-tracking, ou d√©termination de la fr√©quence de pitch dans la parole, en utilisant Praat, YAAPT et YIN comme exemples üö£ üè£ ü§±üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans le domaine de la reconnaissance des √©motions, la voix est la deuxi√®me source de donn√©es √©motionnelles apr√®s le visage. La voix peut √™tre caract√©r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pitch-tracking, ou d√©termination de la fr√©quence de pitch dans la parole, en utilisant Praat, YAAPT et YIN comme exemples</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neurodatalab/blog/416441/"><img src="https://habrastorage.org/getpro/habr/post_images/37d/3f1/975/37d3f19758eb7d646ccff079d37772f8.png" alt="image"><br><br>  Dans le domaine de la reconnaissance des √©motions, la voix est la deuxi√®me source de donn√©es √©motionnelles apr√®s le visage.  La voix peut √™tre caract√©ris√©e par plusieurs param√®tres.  La hauteur de la voix est l'une des principales caract√©ristiques de ce type, cependant, dans le domaine de la technologie acoustique, il est plus correct d'appeler ce param√®tre la fr√©quence fondamentale. <br><br>  La fr√©quence du ton fondamental est directement li√©e √† ce que nous appelons l'intonation.  Et l'intonation, par exemple, est associ√©e aux caract√©ristiques √©motionnellement expressives de la voix. <br><br>  N√©anmoins, la d√©termination de la fr√©quence du ton fondamental n'est pas une t√¢che compl√®tement triviale avec des nuances int√©ressantes.  Dans cet article, nous discuterons des caract√©ristiques des algorithmes pour sa d√©termination et comparerons les solutions existantes avec des exemples d'enregistrements audio sp√©cifiques. <br><a name="habracut"></a><br>  <b>Pr√©sentation</b> <br><br>  Pour commencer, rappelons quelle est, en substance, la fr√©quence du ton fondamental et dans quelles t√¢ches il peut √™tre n√©cessaire.  <i>La fr√©quence fondamentale</i> , √©galement appel√©e CHOT, Fundamental Frequency ou F0, est la fr√©quence des cordes vocales lorsqu'elles prononcent des sons vocaux.  Lors de la prononciation de sons non tonaux (non vocaux), par exemple, en parlant √† voix basse ou en √©mettant des sifflements et des sifflements, les ligaments n'h√©sitent pas, ce qui signifie que cette caract√©ristique n'est pas pertinente pour eux. <br><br>  * Veuillez noter que la division en sons tonaux et non sonores n'est pas √©quivalente √† la division en voyelles et consonnes. <br><br>  La variabilit√© de la fr√©quence du ton fondamental est assez grande, et elle peut varier consid√©rablement non seulement entre les personnes (pour les voix masculines moyennes inf√©rieures, la fr√©quence est de 70-200 Hz, et pour les voix f√©minines, elle peut atteindre 400 Hz), mais aussi pour une personne, en particulier dans le discours √©motionnel . <br><br>  La d√©termination de la fr√©quence du ton fondamental est utilis√©e pour r√©soudre un large √©ventail de probl√®mes: <br><br><ul><li>  Reconnaissance des √©motions, comme nous l'avons dit plus haut; </li><li>  D√©termination du sexe; </li><li>  Lorsque vous r√©solvez le probl√®me de la segmentation audio √† plusieurs voix ou de la division du discours en phrases; </li><li>  En m√©decine, pour d√©terminer les caract√©ristiques pathologiques de la voix (par exemple, en utilisant les param√®tres acoustiques Jitter et Shimmer).  Par exemple, l'identification des signes de la maladie de Parkinson [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1</a> ].  Jitter et Shimmer peuvent √©galement √™tre utilis√©s pour reconna√Ætre les √©motions [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2</a> ]. </li></ul><br>  Cependant, il existe un certain nombre de difficult√©s pour d√©terminer F0.  Par exemple, il est souvent possible de confondre F0 avec les harmoniques, ce qui peut conduire √† ce que l'on appelle les effets de doublage de hauteur / r√©duction de hauteur [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">3</a> ].  Et dans les enregistrements audio de mauvaise qualit√©, F0 est assez difficile √† calculer, car le pic souhait√© aux basses fr√©quences dispara√Æt presque. <br><br>  Au fait, tu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">te</a> souviens de l'histoire de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Laurel et Yanny</a> ?  Les diff√©rences dans les mots que les gens entendent en √©coutant le m√™me enregistrement audio sont dues pr√©cis√©ment √† la diff√©rence de perception F0, qui est influenc√©e par de nombreux facteurs: l'√¢ge de l'auditeur, le degr√© de fatigue et l'appareil de lecture.  Ainsi, lorsque vous √©coutez des enregistrements dans des haut-parleurs avec une reproduction de haute qualit√© des basses fr√©quences, vous entendrez Laurel, et dans les syst√®mes audio o√π les basses fr√©quences sont mal reproduites, Yanny.  L'effet de transition est visible sur un seul appareil, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Et dans cet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> , le r√©seau neuronal agit comme un auditeur.  Dans un autre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article,</a> vous pouvez lire comment le ph√©nom√®ne Yanny / Laurel est expliqu√© en termes de formation de la parole. <br><br>  √âtant donn√© qu'une analyse d√©taill√©e de toutes les m√©thodes de d√©termination de F0 serait trop volumineuse, l'article est de nature g√©n√©rale et peut aider √† naviguer dans le sujet. <br><br>  <b>M√©thodes de d√©termination de F0</b> <br><br>  Les m√©thodes de d√©termination de F0 peuvent √™tre divis√©es en trois cat√©gories: en fonction de la dynamique temporelle du signal ou du domaine temporel;  bas√© sur la structure de fr√©quence, ou domaine de fr√©quence, ainsi que des m√©thodes combin√©es.  Nous vous sugg√©rons de vous familiariser avec l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de</a> synth√®se sur le sujet, o√π les m√©thodes indiqu√©es pour extraire F0 sont analys√©es en d√©tail. <br><br>  Notez que l'un des algorithmes discut√©s comprend 3 √©tapes principales: <br><br>  Pr√©traitement (filtrage du signal, division en images) <br>  Recherche des valeurs possibles de F0 (candidats) <br>  Le suivi est le choix de la trajectoire la plus probable F0 (puisque pour chaque moment dans le temps nous avons plusieurs candidats en comp√©tition, nous devons trouver la piste la plus probable parmi eux) <br><br>  <b>Domaine temporel</b> <br><br>  Nous soulignons quelques points g√©n√©raux.  Avant d'appliquer les m√©thodes du domaine temporel, le signal est pr√©-filtr√©, ne laissant que les basses fr√©quences.  Des seuils sont d√©finis - les fr√©quences minimale et maximale, par exemple de 75 √† 500 Hz.  La d√©termination de F0 est faite uniquement pour les zones avec parole harmonique, car pour les pauses ou les bruits, cela n'a pas seulement de sens, mais peut √©galement introduire des erreurs dans les trames adjacentes lors de l'interpolation et / ou du lissage.  La longueur de trame est s√©lectionn√©e de mani√®re √† contenir au moins trois p√©riodes. <br><br>  La principale m√©thode, sur la base de laquelle toute une famille d'algorithmes est apparue par la suite, est l'autocorr√©lation.  L'approche est assez simple - il faut calculer la fonction d'autocorr√©lation et prendre son premier maximum.  Il affichera la composante de fr√©quence la plus prononc√©e du signal.  Quelle pourrait √™tre la difficult√© dans le cas de l'autocorr√©lation et pourquoi est-il loin d'√™tre toujours que le premier maximum correspondra √† la fr√©quence souhait√©e?  M√™me dans des conditions proches des conditions id√©ales sur des enregistrements de haute qualit√©, la m√©thode peut √™tre erron√©e en raison de la structure complexe du signal.  Dans des conditions proches du r√©el, o√π, entre autres, on peut rencontrer la disparition du pic souhait√© dans des enregistrements bruyants ou des enregistrements de qualit√© initialement faible, le nombre d'erreurs augmente fortement. <br><br>  Malgr√© les erreurs, la m√©thode d'autocorr√©lation est assez pratique et attrayante en raison de sa simplicit√© de base et de sa logique, c'est pourquoi elle est utilis√©e comme base dans de nombreux algorithmes, y compris YIN.  M√™me le nom de l'algorithme nous renvoie √† un √©quilibre entre la commodit√© et l'inexactitude de la m√©thode d'autocorr√©lation: ¬´Le nom YIN de '' yin '' et '' yang '' de la philosophie orientale fait r√©f√©rence √† l'interaction entre l'autocorr√©lation et l'annulation qu'il implique.¬ª  [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">4</a> ] <br><br>  Les cr√©ateurs de YIN ont tent√© de corriger les faiblesses de l'approche d'autocorr√©lation.  Le premier changement est l'utilisation de la fonction de diff√©rence moyenne normalis√©e cumul√©e, qui devrait r√©duire la sensibilit√© aux modulations d'amplitude, rendre les pics plus prononc√©s: <br><br>  \ commencer {√©quation} <br>  d'_t (\ tau) = <br>  \ begin {cases} <br>  1, &amp; \ tau = 0 \\ <br>  d_t (\ tau) \ bigg / \ bigg [\ frac {1} {\ tau} \ sum \ limits_ {j = 1} ^ {\ tau} d_t (j) \ bigg], &amp; \ text {sinon} <br>  \ end {cases} <br>  \ end {√©quation} <br>  YIN essaie √©galement d'√©viter les erreurs qui se produisent dans les cas o√π la longueur de la fonction de fen√™tre n'est pas compl√®tement divis√©e par la p√©riode d'oscillation.  Pour cela, une interpolation minimale parabolique est utilis√©e.  √Ä la derni√®re √©tape du traitement du signal audio, la fonction Best Local Estimate est ex√©cut√©e pour emp√™cher les sauts brusques des valeurs (que ce soit bon ou mauvais - c'est un point discutable). <br><br>  <b>Domaine de fr√©quence</b> <br><br>  Si nous parlons du domaine fr√©quentiel, alors la structure harmonique du signal appara√Æt, c'est-√†-dire la pr√©sence de pics spectraux √† des fr√©quences qui sont des multiples de F0.  Vous pouvez ¬´r√©duire¬ª ce sch√©ma p√©riodique en un pic clair en utilisant l'analyse cepstrale.  Cepstrum - Transform√©e de Fourier du logarithme du spectre de puissance;  le pic cepstral correspond √† la composante la plus p√©riodique du spectre (on peut le lire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> ). <br><br>  <b>M√©thodes hybrides pour d√©terminer F0</b> <br><br>  Le prochain algorithme, qui m√©rite d'√™tre explor√© plus en d√©tail, porte le nom parlant YAAPT - Yet Another Algorithm of Pitch Tracking - et est en fait hybride, car il utilise √† la fois des informations de fr√©quence et de temps.  Une description compl√®te se trouve dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> , nous ne d√©crivons ici que les principales √©tapes. <br><br><img src="https://habrastorage.org/webt/r2/mu/uj/r2muujzlcxgdgp5a0bqem3t_iuu.png"><br>  <i>Figure 1. Diagramme de l'algorithme YAAPTalgo ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> )</i> . <br><br>  YAAPT comprend plusieurs √©tapes principales, dont la premi√®re est le pr√©traitement.  √Ä ce stade, les valeurs du signal d'origine sont mises au carr√© et une deuxi√®me version du signal est obtenue.  Cette √©tape poursuit le m√™me objectif que la fonction de diff√©rence moyenne normalis√©e cumulative dans YIN - amplification et restauration des pics ¬´bloqu√©s¬ª d'autocorr√©lation.  Les deux versions du signal sont filtr√©es - elles prennent g√©n√©ralement la plage de 50 √† 1500 Hz, parfois de 50 √† 900 Hz. <br><br>  Ensuite, la trajectoire de base F0 est calcul√©e √† partir du spectre du signal converti.  Les candidats pour F0 sont d√©termin√©s √† l'aide de la fonction de corr√©lation des harmoniques spectrales (SHC). <br><br>  \ commencer {√©quation} <br>  SHC (t, f) = \ sum \ limits_ {f '= - WL / 2} ^ {WL / 2} \ prod \ limits_ {r = 1} ^ {NH + 1} S (t, rf + f') <br>  \ end {√©quation} <br>  o√π S (t, f) est le spectre d'amplitude pour la trame t et la fr√©quence f, WL est la longueur de la fen√™tre en Hz, NH est le nombre d'harmoniques (les auteurs recommandent d'utiliser les trois premi√®res harmoniques).  La puissance spectrale est √©galement utilis√©e pour d√©terminer les trames vois√©es-non vois√©es, apr√®s quoi la trajectoire la plus optimale est recherch√©e, et la possibilit√© de doublage / r√©duction de hauteur est prise en compte [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">3</a> , Section II, C]. <br><br>  En outre, les candidats pour F0 sont d√©termin√©s √† la fois pour le signal initial et le signal converti, et au lieu de la fonction d'autocorr√©lation, la corr√©lation crois√©e normalis√©e (NCCF) est utilis√©e ici. <br><br>  \ commencer {√©quation} <br>  NCCF (m) = \ frac {\ sum \ limits_ {n = 0} ^ {Nm-1} x (n) * x (n + m)} {\ sqrt {\ sum \ limits_ {n = 0} ^ {{ Nm-1} x ^ 2 (n) * \ sum \ limits_ {n = 0} ^ {Nm-1} x ^ 2 (n + m)}} \ text {,} \ hspace {0,3 cm} 0 &lt;m &lt;M_ {0} <br>  \ end {√©quation} <br>  L'√©tape suivante consiste √† √©valuer tous les candidats possibles et √† calculer leur importance ou leur poids (m√©rite).  Le poids des candidats obtenus √† partir du signal audio d√©pend non seulement de l'amplitude du pic NCCF, mais √©galement de leur proximit√© avec la trajectoire F0 d√©termin√©e √† partir du spectre.  Autrement dit, le domaine fr√©quentiel est consid√©r√© comme grossier en termes de pr√©cision, mais stable [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">3</a> , Section II, D]. <br><br>  Ensuite, pour toutes les paires des candidats restants, la matrice des co√ªts de transition est calcul√©e - le prix de transition, auquel ils trouvent finalement la trajectoire optimale [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">3</a> , Section II, E]. <br><br>  <b>Des exemples</b> <br><br>  Maintenant, nous appliquons tous les algorithmes ci-dessus √† des enregistrements audio sp√©cifiques.  Comme point de d√©part, nous utiliserons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Praat</a> , un outil fondamental pour de nombreux sp√©cialistes de la parole.  Et puis en Python, nous examinerons l'impl√©mentation de YIN et YAAPT et nous comparerons les r√©sultats re√ßus. <br><br>  En tant que mat√©riel audio, vous pouvez utiliser n'importe quel audio disponible.  Nous avons pris plusieurs extraits de notre base de donn√©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RAMAS</a> - un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensemble de donn√©es</a> multimodal cr√©√© avec la participation d'acteurs VGIK.  Vous pouvez √©galement utiliser du mat√©riel provenant d'autres bases de donn√©es ouvertes, telles que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LibriSpeech</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RAVDESS</a> . <br><br>  Pour un exemple illustratif, nous avons pris des extraits de plusieurs enregistrements avec des voix masculines et f√©minines, √† la fois neutres et √©motionnellement color√©es, et pour plus de clart√©, nous les avons combin√©s en un seul <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">enregistrement</a> .  Regardons notre signal, son spectrogramme, son intensit√© (couleur orange) et F0 (couleur bleue).  Dans Praat, cela peut √™tre fait en utilisant Ctrl + O (Ouvrir - Lire √† partir du fichier) puis le bouton Afficher et modifier. <br><br><img src="https://habrastorage.org/webt/ir/ay/kh/iraykhpzwfetpahhiymdic6pdwi.png"><br>  <i>Figure 2. Spectrogramme, intensit√© (couleur orange), F0 (couleur bleue) √† Praat.</i> <br><br>  L'audio montre assez clairement que dans le discours √©motionnel, la hauteur augmente chez les hommes et les femmes.  Dans le m√™me temps, F0 pour la parole masculine √©motionnelle peut √™tre compar√© √† F0 d'une voix f√©minine. <br><br>  <b>Suivi</b> <br><br>  S√©lectionnez l'onglet Analyze periodicity - to Pitch (ac) dans le menu Praat, c'est-√†-dire la d√©finition de F0 en utilisant l'autocorr√©lation.  Une fen√™tre de d√©finition des param√®tres appara√Ætra dans laquelle il est possible de d√©finir 3 param√®tres pour d√©terminer les candidats pour F0 et 6 autres param√®tres pour l'algorithme de recherche de chemin, qui construit le chemin F0 le plus probable parmi tous les candidats. <br><br><div class="spoiler">  <b class="spoiler_title">De nombreux param√®tres (dans Praat, leur description est √©galement sur le bouton Aide)</b> <div class="spoiler_text"><ul><li>  Seuil de silence - le seuil de l'amplitude relative du signal pour d√©terminer le silence, la valeur standard est 0,03. </li><li>  Seuil de voisement - le poids du candidat non vois√©, la valeur maximale est 1. Plus ce param√®tre est √©lev√©, plus les images seront d√©finies comme non vois√©es, c'est-√†-dire ne contenant pas de sons de tonalit√©.  Dans ces trames, F0 ne sera pas d√©termin√©.  La valeur de ce param√®tre est le seuil des pics de la fonction d'autocorr√©lation.  La valeur par d√©faut est 0,45. </li><li>  Co√ªt en octave - d√©termine combien plus de poids les candidats haute fr√©quence ont par rapport √† ceux basse fr√©quence.  Plus la valeur est √©lev√©e, plus la pr√©f√©rence est donn√©e au candidat haute fr√©quence.  La valeur par d√©faut est 0,01 par octave. </li><li>  Co√ªt du saut d'octave - avec une augmentation de ce coefficient, le nombre de transitions brusques de type saut entre les valeurs successives de F0 diminue.  La valeur par d√©faut est 0,35. </li><li>  Co√ªt vois√© / non factur√© - l'augmentation de ce coefficient diminue le nombre de transitions vois√©es / non vois√©es.  La valeur par d√©faut est 0,14. </li><li>  Plafond du pitch (Hz) - les candidats au-dessus de cette fr√©quence ne sont pas pris en compte.  La valeur par d√©faut est 600 Hz. </li></ul><br></div></div><br>  Une description d√©taill√©e de l'algorithme peut √™tre trouv√©e dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un article de</a> 1993. <br><br>  A quoi ressemble le r√©sultat du tracker (path-finder) peut √™tre vu en cliquant sur OK puis en affichant (View &amp; Edit) le fichier Pitch r√©sultant.  On peut voir qu'en plus de la trajectoire choisie, il y avait encore des candidats assez importants avec une fr√©quence inf√©rieure. <br><br><img src="https://habrastorage.org/webt/wq/rq/vf/wqrqvf_cbnbn8orcajij6sfrasu.png"><br>  <i>Figure 3. PitchPath pour les 1,3 premi√®res secondes d'enregistrement audio.</i> <br><br>  <b>Mais qu'en est-il de Python?</b> <br><br>  Prenons deux biblioth√®ques offrant le suivi de hauteur - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">aubio</a> , dans lequel l'algorithme par d√©faut est YIN, et la biblioth√®que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AMFM_decompsition</a> , qui a une impl√©mentation de l'algorithme YAAPT.  Dans le fichier s√©par√© (fichier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PraatPitch.txt</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ins√©rez les</a> valeurs F0 de Praat (cela peut √™tre fait manuellement: s√©lectionnez le fichier son, cliquez sur View &amp; Edit, s√©lectionnez le fichier entier et s√©lectionnez la liste Pitch-Pitch dans le menu sup√©rieur). <br><br>  Comparez maintenant les r√©sultats pour les trois algorithmes (YIN, YAAPT, Praat). <br><br><div class="spoiler">  <b class="spoiler_title">Beaucoup de code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.basic_tools <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> basic <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> aubio <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> source, pitch <span class="hljs-comment"><span class="hljs-comment"># load audio signal = basic.SignalObj('/home/eva/Documents/papers/habr/media/audio.wav') filename = '/home/eva/Documents/papers/habr/media/audio.wav' # YAAPT pitches pitchY = pYAAPT.yaapt(signal, frame_length=40, tda_frame_length=40, f0_min=75, f0_max=600) # YIN pitches downsample = 1 samplerate = 0 win_s = 1764 // downsample # fft size hop_s = 441 // downsample # hop size s = source(filename, samplerate, hop_s) samplerate = s.samplerate tolerance = 0.8 pitch_o = pitch("yin", win_s, hop_s, samplerate) pitch_o.set_unit("midi") pitch_o.set_tolerance(tolerance) pitchesYIN = [] confidences = [] total_frames = 0 while True: samples, read = s() pitch = pitch_o(samples)[0] pitch = int(round(pitch)) confidence = pitch_o.get_confidence() pitchesYIN += [pitch] confidences += [confidence] total_frames += read if read &lt; hop_s: break # load PRAAT pitches praat = np.genfromtxt('/home/eva/Documents/papers/habr/PraatPitch.txt', filling_values=0) praat = praat[:,1] # plot fig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(12, 8)) ax1.plot(np.asarray(pitchesYIN), label='YIN', color='green') ax1.legend(loc="upper right") ax2.plot(pitchY.samp_values, label='YAAPT', color='blue') ax2.legend(loc="upper right") ax3.plot(praat, label='Praat', color='red') ax3.legend(loc="upper right") plt.show()</span></span></code> </pre> <br></div></div><br><br><img src="https://habrastorage.org/webt/tv/k7/uq/tvk7uqi50ctcnd3j3rjq0sjzl8s.png"><br>  <i>Figure 4. Comparaison du fonctionnement des algorithmes YIN, YAAPT et Praat.</i> <br><br>  Nous voyons qu'avec les param√®tres par d√©faut, YIN est assez √©limin√©, obtenant une trajectoire tr√®s plate avec des valeurs inf√©rieures √† Praat et perdant compl√®tement les transitions entre les voix masculines et f√©minines, ainsi qu'entre la parole √©motionnelle et non √©motionnelle. <br><br>  YAAPT a r√©duit un ton tr√®s √©lev√© dans le discours √©motionnel des femmes, mais dans l'ensemble, il a nettement mieux r√©ussi.  En raison de ses caract√©ristiques sp√©cifiques, YAAPT fonctionne mieux - il est impossible de r√©pondre tout de suite, bien s√ªr, mais on peut supposer que le r√¥le est jou√© en obtenant des candidats de trois sources et un calcul plus m√©ticuleux de leur poids que dans YIN. <br><br>  <b>Conclusion</b> <br><br>  Puisque la question de d√©terminer la fr√©quence du ton fondamental (F0) sous une forme ou une autre se pose avant presque tous ceux qui travaillent avec le son, il existe de nombreuses fa√ßons de le r√©soudre.  La question de la pr√©cision et des caract√©ristiques n√©cessaires du mat√©riel audio dans chaque cas d√©termine la pr√©cision avec laquelle il est n√©cessaire de s√©lectionner les param√®tres, ou dans un autre cas, vous pouvez vous limiter √† une solution de base comme YAAPT.  En prenant Praat comme algorithme standard pour le traitement de la parole (n√©anmoins, un grand nombre de chercheurs l'utilisent), nous pouvons conclure que YAAPT est, √† premi√®re vue, plus fiable et pr√©cis que YIN, bien que notre exemple se soit r√©v√©l√© compliqu√© pour lui. <br><br>  Publi√© par <b>Eva Kazimirova</b> , chercheuse au laboratoire de neurodonn√©es, sp√©cialiste du traitement de la parole. <br><br>  <font color="green"><b>Offtop</b></font> : Aimez-vous l'article?  En fait, nous avons un tas de t√¢ches int√©ressantes en ML, en math√©matiques et en programmation, et nous avons besoin de cerveaux.  √ätes-vous curieux?  Venez chez nous!  Courriel: hr@neurodatalab.com <br><br><div class="spoiler">  <b class="spoiler_title">Les r√©f√©rences</b> <div class="spoiler_text"><ol><li>  Rusz, J., Cmejla, R., Ruzickova, H., Ruzicka, E. Mesures acoustiques quantitatives pour la caract√©risation des troubles de la parole et de la voix dans la maladie de Parkinson pr√©coce non trait√©e.  Le Journal de l'Acoustical Society of America, vol.  129, num√©ro 1 (2011), pp.  350-367.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Acc√®s</a> </li><li>  Farr√∫s, M., Hernando, J., Ejarque, P. Jitter and Shimmer Measurements for Speaker Recognition.  Actes de la conf√©rence annuelle de l'International Speech Communication Association, INTERSPEECH, vol.  2 (2007), pp.  1153-1156.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Acc√®s</a> </li><li>  Zahorian, S., Hu, HA.  M√©thode spectrale / temporelle pour un suivi robuste des fr√©quences fondamentales.  Le Journal de l'Acoustical Society of America, vol.  123, num√©ro 6 (2008), pp.  4559-4571.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Acc√®s</a> </li><li>  De Cheveign√©, A., Kawahara, H. YIN, un estimateur de fr√©quence fondamental pour la parole et la musique.  Le Journal de l'Acoustical Society of America, vol.  111, num√©ro 4 (2002), pp.  1917-1930.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Acc√®s</a> </li></ol></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr416441/">https://habr.com/ru/post/fr416441/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr416431/index.html">Les meilleurs projets de blockchain. ICO juillet 2018 (vote)</a></li>
<li><a href="../fr416433/index.html">Demandez √† Ethan: les pertes de rayonnement des √©toiles peuvent-elles expliquer l'√©nergie sombre?</a></li>
<li><a href="../fr416435/index.html">Pourquoi le cerveau humain est-il si efficace?</a></li>
<li><a href="../fr416437/index.html">Y a-t-il suffisamment de produits chimiques sur les mondes glac√©s pour y maintenir la vie?</a></li>
<li><a href="../fr416439/index.html">iOS 12: regroupement des notifications</a></li>
<li><a href="../fr416443/index.html">9 secrets d'ASP.NET Core</a></li>
<li><a href="../fr416445/index.html">Webinaires Skillbox: les plus int√©ressants - gratuits</a></li>
<li><a href="../fr416449/index.html">.NET Core + Docker sur Raspberry Pi. Est-ce l√©gal?</a></li>
<li><a href="../fr416451/index.html">Les bases de donn√©es de recherche Microsoft d√©sormais accessibles √† tous</a></li>
<li><a href="../fr416453/index.html">Sch√©mas de vol dans les syst√®mes RBS et cinq niveaux de contre-attaque</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>