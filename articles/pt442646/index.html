<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎅🏼 🥐 🤾🏿 Redes Kubernetes: ingresso 👩🏾‍🤝‍👨🏿 👩🏿‍🤝‍👨🏻 🧝🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hoje estamos publicando uma tradução da terceira parte do Kubernetes Networking Guide. A primeira parte foi sobre pods, a segunda sobre serviços, e ho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes Kubernetes: ingresso</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/442646/">  Hoje estamos publicando uma tradução da terceira parte do Kubernetes Networking Guide.  A <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">primeira</a> parte foi sobre pods, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">segunda</a> sobre serviços, e hoje falaremos sobre balanceamento de carga e recursos Kubernetes do tipo Ingress. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/te/wp/ce/tewpcee5cggzqu97irog_mj_qgo.png"></div><a name="habracut"></a><h2>  <font color="#3AC1EF">O roteamento não é balanceamento de carga</font> </h2><br>  No artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">anterior</a> desta série, consideramos uma configuração que consiste em um par de lareiras e um serviço ao qual foi atribuído um endereço IP chamado "IP do cluster".  As consultas destinadas a lares foram enviadas para este endereço.  Aqui continuaremos a trabalhar em nosso sistema de treinamento, começando onde nos graduamos pela última vez.  Lembre-se de que o endereço IP do cluster do serviço, <code>10.3.241.152</code> , pertence a um intervalo de endereços IP diferente do usado na rede da lareira e do usado na rede em que os nós estão localizados.  Chamei a rede definida por esse espaço de endereço de "rede de serviço", embora dificilmente seja digna de um nome especial, pois nenhum dispositivo está conectado a essa rede e seu espaço de endereço, de fato, consiste inteiramente de regras de roteamento.  Foi demonstrado anteriormente como essa rede é implementada com base no componente Kubernetes, chamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kube-proxy,</a> e interage com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">netfilter do</a> módulo do kernel Linux para interceptar e redirecionar o tráfego enviado ao cluster IP para o qual trabalhar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/35f/1ec/61535f1ec0169dbd13732aba4c9a5621.png"></div><br>  <i><font color="#999999">Diagrama de rede</font></i> <br><br>  Até agora, conversamos sobre "conexões" e "solicitações" e até usamos o conceito difícil de interpretar "tráfego", mas para entender os recursos do mecanismo do Kubernetes Ingress, precisamos usar termos mais precisos.  Portanto, as conexões e solicitações funcionam no 4º nível do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo OSI</a> (tcp) ou no 7º nível (http, rpc e assim por diante).  As regras do Netfilter são regras de roteamento, elas funcionam com pacotes IP no terceiro nível.  Todos os roteadores, incluindo o netfilter, tomam decisões mais ou menos com base apenas nas informações contidas no pacote.  Em geral, eles estão interessados ​​em saber de onde o pacote vem e para onde vai.  Portanto, para descrever esse comportamento em termos do terceiro nível do modelo OSI, deve-se dizer que cada pacote destinado ao serviço localizado em <code>10.3.241.152:80</code> , que chega na interface do nó <code>eth0</code> , é processado pelo netfilter e, de acordo com as regras definidas para o nosso serviço são redirecionadas para o endereço IP de uma lareira viável. <br><br>  Parece bastante óbvio que qualquer mecanismo que usamos para permitir que clientes externos acessem pods deve usar a mesma infraestrutura de roteamento.  Como resultado, esses clientes externos acessarão o endereço IP e a porta do cluster, pois são o "ponto de acesso" a todos os mecanismos sobre os quais falamos até agora.  Eles nos permitem não nos preocupar com o local exato em que ele é executado em um determinado momento.  No entanto, não é de todo óbvio como fazer tudo funcionar. <br><br>  O serviço IP de cluster é alcançável apenas com a interface Ethernet do nó.  Nada fora do cluster sabe o que fazer com endereços do intervalo ao qual esse endereço pertence.  Como redirecionar o tráfego de um endereço IP público para um endereço acessível somente se o pacote já chegou ao host? <br><br>  Se tentarmos encontrar uma solução para esse problema, uma das coisas que podem ser feitas no processo de encontrar uma solução será o estudo das regras do netfilter usando o utilitário <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">iptables</a> .  Se você fizer isso, poderá descobrir algo que, à primeira vista, possa parecer incomum: as regras para o serviço não se limitam a uma rede de origem específica.  Isso significa que quaisquer pacotes gerados em qualquer lugar que cheguem à interface Ethernet do nó e tenham um endereço de destino <code>10.3.241.152:80</code> serão reconhecidos como em conformidade com a regra e serão redirecionados para o sub.  Podemos apenas fornecer aos clientes um cluster IP, talvez vinculando-o a um nome de domínio adequado e, em seguida, configurar uma rota que permita organizar a entrega desses pacotes para um dos nós? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f3/6ba/ea7/5f36baea7589e3559632de385f3f2bf6.png"></div><br>  <i><font color="#999999">Cliente e cluster externos</font></i> <br><br>  Se tudo estiver configurado dessa maneira, esse projeto provará estar funcionando.  Os clientes acessam o IP do cluster, os pacotes seguem a rota que leva ao host e são redirecionados para a parte inferior.  Neste momento, pode parecer que essa solução pode ser limitada, mas sofre de alguns problemas sérios.  A primeira é que os nós, de fato, o conceito de efêmero, não são particularmente diferentes, a esse respeito, dos lares.  Eles estão, é claro, um pouco mais próximos do mundo material do que os pods, mas podem migrar para novas máquinas virtuais, os clusters podem aumentar ou diminuir, e assim por diante.  Os roteadores funcionam no terceiro nível do modelo OSI e os pacotes não conseguem distinguir entre os serviços normalmente funcionando e os que não funcionam corretamente.  Eles esperam que a próxima transição na rota seja acessível e estável.  Se o nó estiver inacessível, a rota ficará inoperante e permanecerá assim, na maioria dos casos, por muito tempo.  Mesmo que a rota seja resistente a falhas, esse esquema levará ao fato de que todo o tráfego externo passará por um único nó, o que provavelmente não é o ideal. <br><br>  Não importa como trazemos o tráfego do cliente para o sistema, precisamos fazer isso para que ele não dependa do estado de nenhum nó do cluster.  E, de fato, não há uma maneira confiável de fazer isso usando apenas roteamento, sem alguns meios de gerenciar ativamente o roteador.  De fato, é precisamente esse papel, o papel do sistema de controle, que o kube-proxy desempenha em relação ao netfilter.  Estender a responsabilidade do Kubernetes de gerenciar um roteador externo provavelmente não fazia muito sentido para arquitetos de sistemas, especialmente porque já possuímos ferramentas comprovadas para distribuir o tráfego de clientes em vários servidores.  Eles são chamados de balanceadores de carga e não é de surpreender que sejam a solução realmente confiável para o Kubernetes Ingress.  Para entender exatamente como isso acontece, precisamos sair do terceiro nível do OSI e falar sobre conexões novamente. <br><br>  Para usar o balanceador de carga para distribuir o tráfego do cliente entre os nós do cluster, precisamos de um endereço IP público ao qual os clientes possam se conectar e também precisamos dos endereços dos próprios nós para os quais o balanceador de carga pode redirecionar solicitações.  Pelas razões acima, não podemos apenas criar uma rota estática estável entre o roteador do gateway e os nós usando uma rede baseada em serviço (cluster IP). <br><br>  Entre os outros endereços com os quais você pode trabalhar, apenas os endereços da rede à qual as interfaces Ethernet dos nós estão conectados, ou seja, neste exemplo, <code>10.100.0.0/24</code> , podem ser observados.  O roteador já sabe como encaminhar pacotes para essas interfaces e as conexões enviadas do balanceador de carga para o roteador irão para onde devem ir.  Mas se o cliente quiser se conectar ao nosso serviço na porta 80, não podemos simplesmente enviar pacotes para essa porta nas interfaces de rede dos nós. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4cd/c77/40b/4cdc7740be5ecfc3069a8b3fb157c605.png"></div><br>  <i><font color="#999999">Balanceador de carga, tentativa malsucedida de acessar a porta 80 da interface de rede host</font></i> <br><br>  A razão pela qual isso não pode ser feito é completamente óbvia.  Ou seja, estamos falando do fato de que não há processo aguardando conexões em <code>10.100.0.3:80</code> (e, se houver, esse definitivamente não é o mesmo processo) e as regras do netfilter, que, como esperávamos, interceptariam a solicitação e eles enviam para ele, não trabalham nesse endereço de destino.  Eles respondem apenas a uma rede IP de cluster com base em serviços, ou seja, no endereço <code>10.3.241.152:80</code> .  Como resultado, esses pacotes, após sua chegada, não podem ser entregues no endereço de destino, e o kernel emitirá uma resposta <code>ECONNREFUSED</code> .  Isso nos coloca em uma posição confusa: não é fácil trabalhar com a rede para redirecionar pacotes para o qual o netfilter está configurado ao redirecionar dados do gateway para os nós, e uma rede para a qual o roteamento é fácil de configurar não é a rede para a qual o netfilter redireciona pacotes.  Para resolver esse problema, você pode criar uma ponte entre essas redes.  É exatamente isso que o Kubernetes faz usando um serviço como o NodePort. <br><br><h2>  <font color="#3AC1EF">Serviços como NodePort</font> </h2><br>  O serviço que nós, por exemplo, criamos no artigo anterior, não recebe um tipo, por isso adotou o tipo padrão - <code>ClusterIP</code> .  Existem mais dois tipos de serviços que diferem em recursos adicionais, e o que estamos interessados ​​agora é o <code>NodePort</code> .  Aqui está um exemplo de uma descrição de um serviço desse tipo: <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: NodePort selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Serviços do tipo <code>NodePort</code> são serviços do tipo <code>ClusterIP</code> que têm uma oportunidade adicional: o acesso a eles pode ser obtido pelo endereço IP atribuído ao host e pelo endereço atribuído ao cluster na rede de serviço.  Isso é conseguido de uma maneira bastante simples: quando o Kubernetes cria um serviço NodePort, o kube-proxy aloca uma porta no intervalo de 30000-32767 e abre essa porta na interface <code>eth0</code> de cada nó (daí o nome do tipo de serviço - <code>NodePort</code> ).  As conexões feitas a essa porta (chamaremos essas portas <code>NodePort</code> ) são redirecionadas para o IP do cluster do serviço.  Se criarmos o serviço descrito acima e executarmos o <code>kubectl get svc service-test</code> , podemos ver a porta atribuída a ele. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME           CLUSTER-IP EXTERNAL-IP   PORT(S) AGE service-test   10.3.241.152 &lt;none&gt;        80:32213/TCP 1m</code> </pre> <br>  Nesse caso, o serviço é atribuído ao NodePort <code>32213</code> .  Isso significa que agora podemos nos conectar ao serviço através de qualquer nó em nosso cluster experimental em <code>10.100.0.2:32213</code> ou em <code>10.100.0.3:32213</code> .  Nesse caso, o tráfego será redirecionado para o serviço. <br><br>  Após essa parte do sistema ter ocorrido, temos todos os fragmentos do pipeline para equilibrar a carga criada pelas solicitações do cliente para todos os nós do cluster. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/da5/78e/735/da578e735e468dce8077f8a1d27d8490.png"></div><br>  <i><font color="#999999">Serviço NodePort</font></i> <br><br>  Na figura anterior, o cliente se conecta ao balanceador de carga por meio de um endereço IP público, o balanceador de carga seleciona o nó e se conecta a ele em <code>10.100.0.3:32213</code> , o kube-proxy aceita essa conexão e a redireciona para o serviço acessível via IP <code>10.3.241.152:80</code> cluster .  Aqui, a solicitação é processada com sucesso de acordo com as regras definidas pelo netfilter e é redirecionada para o pod do servidor no endereço <code>10.0.2.2:8080</code> .  Talvez tudo isso possa parecer um pouco complicado, e até certo ponto seja, mas não é fácil encontrar uma solução mais simples que suporte todos os recursos maravilhosos que nos fornecem pods e redes com base em serviços. <br><br>  Esse mecanismo, no entanto, não deixa de ter seus próprios problemas.  O uso de serviços como o <code>NodePort</code> oferece aos clientes acesso a serviços usando uma porta não padrão.  Geralmente, isso não é um problema, pois o balanceador de carga pode fornecer a eles uma porta regular e ocultar o <code>NodePort</code> dos usuários finais.  Mas, em alguns cenários, por exemplo, ao usar um balanceador de carga externo da plataforma Google Cloud, pode ser necessário implantar o <code>NodePort</code> clientes.  Deve-se notar que essas portas, além disso, representam recursos limitados, embora 2768 portas sejam provavelmente suficientes, mesmo para os maiores clusters.  Na maioria dos casos, você pode permitir que o Kubernetes selecione números de porta aleatoriamente, mas você mesmo pode configurá-los, se necessário.  Outro problema são algumas limitações relacionadas ao armazenamento de endereços IP de origem nas solicitações.  Para descobrir como resolver esses problemas, você pode consultar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este</a> material na documentação do Kubernetes. <br><br>  Portas <code>NodePorts</code> é o mecanismo fundamental pelo qual todo o tráfego externo entra no cluster Kubernetes.  No entanto, eles próprios não nos apresentam uma solução pronta.  Pelas razões acima, antes do cluster, se os clientes são entidades internas ou externas localizadas em uma rede pública, é sempre necessário ter algum tipo de balanceador de carga. <br><br>  Os arquitetos da plataforma, percebendo isso, forneceram duas maneiras de configurar o balanceador de carga a partir da própria plataforma Kubernetes.  Vamos discutir isso. <br><br><h2>  <font color="#3AC1EF">Serviços como LoadBalancer e recursos do tipo Ingress</font> </h2><br>  Serviços como o <code>LoadBalancer</code> e recursos do tipo <code>Ingress</code> são alguns dos mecanismos mais complexos do Kubernetes.  No entanto, não gastaremos muito tempo com eles, pois seu uso não leva a mudanças fundamentais em tudo o que falamos até agora.  Todo o tráfego externo, como antes, entra no cluster através do <code>NodePort</code> . <br><br>  Os arquitetos podem parar por aqui, permitindo que aqueles que criam clusters se importem apenas com endereços IP públicos e balanceadores de carga.  De fato, em certas situações, como iniciar um cluster em servidores regulares ou em casa, é exatamente isso que eles fazem.  Mas em ambientes que suportam configurações de recursos de rede controladas por API, o Kubernetes permite configurar tudo o que você precisa em um só lugar. <br><br>  A primeira abordagem para resolver esse problema, a mais simples, é usar serviços do Kubernetes, como o <code>LoadBalancer</code> .  Esses serviços têm todos os recursos de serviços como <code>NodePort</code> e, além disso, têm a capacidade de criar caminhos completos para o tráfego recebido, com base na suposição de que o cluster está sendo executado em ambientes como GCP ou AWS que suportam a configuração de recursos de rede por meio da API. <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: LoadBalancer selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Se excluirmos e recriarmos o serviço do nosso exemplo no Google Kubernetes Engine, logo depois disso, usando o <code>kubectl get svc service-test</code> , poderemos verificar se o IP externo está atribuído. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME      CLUSTER-IP      EXTERNAL-IP PORT(S)          AGE openvpn   10.3.241.52     35.184.97.156 80:32213/TCP     5m</code> </pre> <br>  Foi dito acima que seremos capazes de verificar o fato de atribuir um endereço IP externo "em breve", apesar de a atribuição de um IP externo demorar alguns minutos, o que não é surpreendente, dada a quantidade de recursos que precisam ser trazidos para um estado íntegro.  Na plataforma GCP, por exemplo, isso exige que o sistema crie um endereço IP externo, regras de redirecionamento de tráfego, um servidor proxy de destino, um serviço de back-end e, possivelmente, uma instância de um grupo.  Depois de alocar um endereço IP externo, você pode se conectar ao serviço através desse endereço, atribuir um nome de domínio e informar os clientes.  Até que o serviço seja destruído e recriado (para fazer isso, raramente quando houver um bom motivo), o endereço IP não será alterado. <br><br>  Serviços como o <code>LoadBalancer</code> têm algumas limitações.  Esse serviço não pode ser configurado para descriptografar o tráfego HTTPS.  Você não pode criar hosts virtuais ou configurar o roteamento baseado em caminho; portanto, usando configurações práticas, não pode usar um único balanceador de carga com muitos serviços.  Essas limitações levaram à introdução do Kubernetes 1.1.  Um recurso especial para configurar balanceadores de carga.  Este é um recurso do tipo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ingress</a> .  Serviços como o <code>LoadBalancer</code> visam expandir os recursos de um único serviço para dar suporte a clientes externos.  Por outro lado, os recursos do <code>Ingress</code> são recursos especiais que permitem configurar flexivelmente balanceadores de carga.  A API do Ingress suporta descriptografia do tráfego TLS, hosts virtuais e roteamento baseado em caminho.  Usando essa API, o balanceador de carga pode ser facilmente configurado para funcionar com vários serviços de back-end. <br><br>  A API do recurso do tipo <code>Ingress</code> é muito grande para discutir seus recursos aqui; além disso, não afeta particularmente como os recursos do Ingress funcionam no nível da rede.  A implementação desse recurso segue o padrão usual do Kubernetes: existe um tipo de recurso e um controlador para controlar esse tipo.  O recurso nesse caso é o recurso <code>Ingress</code> , que descreve solicitações para recursos de rede.  Veja como pode ser a descrição de um recurso do <code>Ingress</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations:   kubernetes.io/ingress.class: "gce" spec: tls:   - secretName: my-ssl-secret rules: - host: testhost.com   http:     paths:     - path: /*       backend:         serviceName: service-test         servicePort: 80</code> </pre> <br>  O controlador do Ingress é responsável por executar essas solicitações, trazendo outros recursos para o estado desejado.  Ao usar o Ingress, são criados serviços como o <code>NodePort</code> , após o qual o controlador do Ingress pode tomar decisões sobre como direcionar o tráfego para os nós.  Existe uma implementação do controlador Ingress para balanceadores de carga GCE, balanceadores da AWS, para servidores proxy populares como nginx e haproxy.  Observe que a mistura de recursos e serviços do Ingress como o <code>LoadBalancer</code> pode causar problemas menores em alguns ambientes.  Eles são fáceis de manusear, mas, em geral, é melhor usar o Ingress mesmo para serviços simples. <br><br><h2>  <font color="#3AC1EF">HostPort e HostNetwork</font> </h2><br>  O que falaremos agora, a saber, <code>HostPort</code> e <code>HostNetwork</code> , pode ser atribuído à categoria de raridades interessantes, e não a ferramentas úteis.  De fato, comprometo-me a afirmar que, em 99,99% dos casos, seu uso pode ser considerado um antipadrão, e qualquer sistema no qual eles são usados ​​deve passar por uma verificação obrigatória de sua arquitetura. <br><br>  Eu pensei que não valia a pena falar sobre eles, mas eles são algo parecido com as ferramentas usadas pelos recursos do Ingress para processar o tráfego recebido, então decidi que valia a pena mencioná-los, pelo menos brevemente. <br><br>  Primeiro, <code>HostPort</code> falar sobre o <code>HostPort</code> .  Esta é uma propriedade de contêiner (declarada na estrutura <code>ContainerPort</code> ).  Quando um determinado número de porta é gravado nele, isso leva à abertura dessa porta no nó e ao seu redirecionamento diretamente para o contêiner.  Não há mecanismos de proxy e a porta é aberta apenas nos nós nos quais o contêiner está em execução.  Nos primeiros dias da plataforma, antes que os mecanismos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DaemonSet</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">StatefulSet</a> aparecessem, o <code>HostPort</code> era um truque que permitia que apenas um contêiner de um determinado tipo fosse iniciado em qualquer nó.  Por exemplo, uma vez eu usei isso para criar um cluster do <code>HostPort</code> configurando o <code>HostPort</code> para <code>9200</code> e especificando quantas réplicas havia nós.       ,          Kubernetes,    -     <code>HostPort</code> . <br><br>   <code>NostNetwork</code> , ,   Kubernetes    ,  <code>HostPort</code> .       <code>true</code> ,       - <code>network=host</code>  <code>docker run</code> .    ,           .            <code>eth0</code>    .  ,             .      ,  ,  ,    Kubernetes,     - . <br><br><h2>  <font color="#3AC1EF">Sumário</font> </h2><br>        Kubernetes,   ,          Ingress. ,  ,    ,       Kubernetes. <br><br>  <b>Caros leitores!</b>     Ingress? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt442646/">https://habr.com/ru/post/pt442646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt442636/index.html">Você traz más notícias para a gerência?</a></li>
<li><a href="../pt442638/index.html">Escalonamento de aplicativos Kubernetes com base nas métricas do Prometheus</a></li>
<li><a href="../pt442640/index.html">Bug perfeito: usando confusão de tipo no Flash. Parte 1</a></li>
<li><a href="../pt442642/index.html">O que ler em março: 22 novos livros para profissionais de marketing, gerentes, desenvolvedores e designers</a></li>
<li><a href="../pt442644/index.html">A maioria das habilidades que não são de programação aumentam o valor do desenvolvedor</a></li>
<li><a href="../pt442648/index.html">Ir mecanismos de alocação</a></li>
<li><a href="../pt442650/index.html">Análise e otimização de aplicativos React</a></li>
<li><a href="../pt442652/index.html">Usando Fastify e Preact para prototipar aplicativos da Web rapidamente</a></li>
<li><a href="../pt442654/index.html">Mudando para Next.js e acelerando o carregamento da página inicial do manifold.co 7,5 vezes</a></li>
<li><a href="../pt442658/index.html">8 truques para trabalhar com CSS: paralaxe, rodapé fixo e outros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>