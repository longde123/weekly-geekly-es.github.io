<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÖüèº ü•ê ü§æüèø Redes Kubernetes: ingresso üë©üèæ‚Äçü§ù‚Äçüë®üèø üë©üèø‚Äçü§ù‚Äçüë®üèª üßùüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hoje estamos publicando uma tradu√ß√£o da terceira parte do Kubernetes Networking Guide. A primeira parte foi sobre pods, a segunda sobre servi√ßos, e ho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes Kubernetes: ingresso</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/442646/">  Hoje estamos publicando uma tradu√ß√£o da terceira parte do Kubernetes Networking Guide.  A <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">primeira</a> parte foi sobre pods, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">segunda</a> sobre servi√ßos, e hoje falaremos sobre balanceamento de carga e recursos Kubernetes do tipo Ingress. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/te/wp/ce/tewpcee5cggzqu97irog_mj_qgo.png"></div><a name="habracut"></a><h2>  <font color="#3AC1EF">O roteamento n√£o √© balanceamento de carga</font> </h2><br>  No artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">anterior</a> desta s√©rie, consideramos uma configura√ß√£o que consiste em um par de lareiras e um servi√ßo ao qual foi atribu√≠do um endere√ßo IP chamado "IP do cluster".  As consultas destinadas a lares foram enviadas para este endere√ßo.  Aqui continuaremos a trabalhar em nosso sistema de treinamento, come√ßando onde nos graduamos pela √∫ltima vez.  Lembre-se de que o endere√ßo IP do cluster do servi√ßo, <code>10.3.241.152</code> , pertence a um intervalo de endere√ßos IP diferente do usado na rede da lareira e do usado na rede em que os n√≥s est√£o localizados.  Chamei a rede definida por esse espa√ßo de endere√ßo de "rede de servi√ßo", embora dificilmente seja digna de um nome especial, pois nenhum dispositivo est√° conectado a essa rede e seu espa√ßo de endere√ßo, de fato, consiste inteiramente de regras de roteamento.  Foi demonstrado anteriormente como essa rede √© implementada com base no componente Kubernetes, chamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kube-proxy,</a> e interage com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">netfilter do</a> m√≥dulo do kernel Linux para interceptar e redirecionar o tr√°fego enviado ao cluster IP para o qual trabalhar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/35f/1ec/61535f1ec0169dbd13732aba4c9a5621.png"></div><br>  <i><font color="#999999">Diagrama de rede</font></i> <br><br>  At√© agora, conversamos sobre "conex√µes" e "solicita√ß√µes" e at√© usamos o conceito dif√≠cil de interpretar "tr√°fego", mas para entender os recursos do mecanismo do Kubernetes Ingress, precisamos usar termos mais precisos.  Portanto, as conex√µes e solicita√ß√µes funcionam no 4¬∫ n√≠vel do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo OSI</a> (tcp) ou no 7¬∫ n√≠vel (http, rpc e assim por diante).  As regras do Netfilter s√£o regras de roteamento, elas funcionam com pacotes IP no terceiro n√≠vel.  Todos os roteadores, incluindo o netfilter, tomam decis√µes mais ou menos com base apenas nas informa√ß√µes contidas no pacote.  Em geral, eles est√£o interessados ‚Äã‚Äãem saber de onde o pacote vem e para onde vai.  Portanto, para descrever esse comportamento em termos do terceiro n√≠vel do modelo OSI, deve-se dizer que cada pacote destinado ao servi√ßo localizado em <code>10.3.241.152:80</code> , que chega na interface do n√≥ <code>eth0</code> , √© processado pelo netfilter e, de acordo com as regras definidas para o nosso servi√ßo s√£o redirecionadas para o endere√ßo IP de uma lareira vi√°vel. <br><br>  Parece bastante √≥bvio que qualquer mecanismo que usamos para permitir que clientes externos acessem pods deve usar a mesma infraestrutura de roteamento.  Como resultado, esses clientes externos acessar√£o o endere√ßo IP e a porta do cluster, pois s√£o o "ponto de acesso" a todos os mecanismos sobre os quais falamos at√© agora.  Eles nos permitem n√£o nos preocupar com o local exato em que ele √© executado em um determinado momento.  No entanto, n√£o √© de todo √≥bvio como fazer tudo funcionar. <br><br>  O servi√ßo IP de cluster √© alcan√ß√°vel apenas com a interface Ethernet do n√≥.  Nada fora do cluster sabe o que fazer com endere√ßos do intervalo ao qual esse endere√ßo pertence.  Como redirecionar o tr√°fego de um endere√ßo IP p√∫blico para um endere√ßo acess√≠vel somente se o pacote j√° chegou ao host? <br><br>  Se tentarmos encontrar uma solu√ß√£o para esse problema, uma das coisas que podem ser feitas no processo de encontrar uma solu√ß√£o ser√° o estudo das regras do netfilter usando o utilit√°rio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">iptables</a> .  Se voc√™ fizer isso, poder√° descobrir algo que, √† primeira vista, possa parecer incomum: as regras para o servi√ßo n√£o se limitam a uma rede de origem espec√≠fica.  Isso significa que quaisquer pacotes gerados em qualquer lugar que cheguem √† interface Ethernet do n√≥ e tenham um endere√ßo de destino <code>10.3.241.152:80</code> ser√£o reconhecidos como em conformidade com a regra e ser√£o redirecionados para o sub.  Podemos apenas fornecer aos clientes um cluster IP, talvez vinculando-o a um nome de dom√≠nio adequado e, em seguida, configurar uma rota que permita organizar a entrega desses pacotes para um dos n√≥s? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f3/6ba/ea7/5f36baea7589e3559632de385f3f2bf6.png"></div><br>  <i><font color="#999999">Cliente e cluster externos</font></i> <br><br>  Se tudo estiver configurado dessa maneira, esse projeto provar√° estar funcionando.  Os clientes acessam o IP do cluster, os pacotes seguem a rota que leva ao host e s√£o redirecionados para a parte inferior.  Neste momento, pode parecer que essa solu√ß√£o pode ser limitada, mas sofre de alguns problemas s√©rios.  A primeira √© que os n√≥s, de fato, o conceito de ef√™mero, n√£o s√£o particularmente diferentes, a esse respeito, dos lares.  Eles est√£o, √© claro, um pouco mais pr√≥ximos do mundo material do que os pods, mas podem migrar para novas m√°quinas virtuais, os clusters podem aumentar ou diminuir, e assim por diante.  Os roteadores funcionam no terceiro n√≠vel do modelo OSI e os pacotes n√£o conseguem distinguir entre os servi√ßos normalmente funcionando e os que n√£o funcionam corretamente.  Eles esperam que a pr√≥xima transi√ß√£o na rota seja acess√≠vel e est√°vel.  Se o n√≥ estiver inacess√≠vel, a rota ficar√° inoperante e permanecer√° assim, na maioria dos casos, por muito tempo.  Mesmo que a rota seja resistente a falhas, esse esquema levar√° ao fato de que todo o tr√°fego externo passar√° por um √∫nico n√≥, o que provavelmente n√£o √© o ideal. <br><br>  N√£o importa como trazemos o tr√°fego do cliente para o sistema, precisamos fazer isso para que ele n√£o dependa do estado de nenhum n√≥ do cluster.  E, de fato, n√£o h√° uma maneira confi√°vel de fazer isso usando apenas roteamento, sem alguns meios de gerenciar ativamente o roteador.  De fato, √© precisamente esse papel, o papel do sistema de controle, que o kube-proxy desempenha em rela√ß√£o ao netfilter.  Estender a responsabilidade do Kubernetes de gerenciar um roteador externo provavelmente n√£o fazia muito sentido para arquitetos de sistemas, especialmente porque j√° possu√≠mos ferramentas comprovadas para distribuir o tr√°fego de clientes em v√°rios servidores.  Eles s√£o chamados de balanceadores de carga e n√£o √© de surpreender que sejam a solu√ß√£o realmente confi√°vel para o Kubernetes Ingress.  Para entender exatamente como isso acontece, precisamos sair do terceiro n√≠vel do OSI e falar sobre conex√µes novamente. <br><br>  Para usar o balanceador de carga para distribuir o tr√°fego do cliente entre os n√≥s do cluster, precisamos de um endere√ßo IP p√∫blico ao qual os clientes possam se conectar e tamb√©m precisamos dos endere√ßos dos pr√≥prios n√≥s para os quais o balanceador de carga pode redirecionar solicita√ß√µes.  Pelas raz√µes acima, n√£o podemos apenas criar uma rota est√°tica est√°vel entre o roteador do gateway e os n√≥s usando uma rede baseada em servi√ßo (cluster IP). <br><br>  Entre os outros endere√ßos com os quais voc√™ pode trabalhar, apenas os endere√ßos da rede √† qual as interfaces Ethernet dos n√≥s est√£o conectados, ou seja, neste exemplo, <code>10.100.0.0/24</code> , podem ser observados.  O roteador j√° sabe como encaminhar pacotes para essas interfaces e as conex√µes enviadas do balanceador de carga para o roteador ir√£o para onde devem ir.  Mas se o cliente quiser se conectar ao nosso servi√ßo na porta 80, n√£o podemos simplesmente enviar pacotes para essa porta nas interfaces de rede dos n√≥s. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4cd/c77/40b/4cdc7740be5ecfc3069a8b3fb157c605.png"></div><br>  <i><font color="#999999">Balanceador de carga, tentativa malsucedida de acessar a porta 80 da interface de rede host</font></i> <br><br>  A raz√£o pela qual isso n√£o pode ser feito √© completamente √≥bvia.  Ou seja, estamos falando do fato de que n√£o h√° processo aguardando conex√µes em <code>10.100.0.3:80</code> (e, se houver, esse definitivamente n√£o √© o mesmo processo) e as regras do netfilter, que, como esper√°vamos, interceptariam a solicita√ß√£o e eles enviam para ele, n√£o trabalham nesse endere√ßo de destino.  Eles respondem apenas a uma rede IP de cluster com base em servi√ßos, ou seja, no endere√ßo <code>10.3.241.152:80</code> .  Como resultado, esses pacotes, ap√≥s sua chegada, n√£o podem ser entregues no endere√ßo de destino, e o kernel emitir√° uma resposta <code>ECONNREFUSED</code> .  Isso nos coloca em uma posi√ß√£o confusa: n√£o √© f√°cil trabalhar com a rede para redirecionar pacotes para o qual o netfilter est√° configurado ao redirecionar dados do gateway para os n√≥s, e uma rede para a qual o roteamento √© f√°cil de configurar n√£o √© a rede para a qual o netfilter redireciona pacotes.  Para resolver esse problema, voc√™ pode criar uma ponte entre essas redes.  √â exatamente isso que o Kubernetes faz usando um servi√ßo como o NodePort. <br><br><h2>  <font color="#3AC1EF">Servi√ßos como NodePort</font> </h2><br>  O servi√ßo que n√≥s, por exemplo, criamos no artigo anterior, n√£o recebe um tipo, por isso adotou o tipo padr√£o - <code>ClusterIP</code> .  Existem mais dois tipos de servi√ßos que diferem em recursos adicionais, e o que estamos interessados ‚Äã‚Äãagora √© o <code>NodePort</code> .  Aqui est√° um exemplo de uma descri√ß√£o de um servi√ßo desse tipo: <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: NodePort selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Servi√ßos do tipo <code>NodePort</code> s√£o servi√ßos do tipo <code>ClusterIP</code> que t√™m uma oportunidade adicional: o acesso a eles pode ser obtido pelo endere√ßo IP atribu√≠do ao host e pelo endere√ßo atribu√≠do ao cluster na rede de servi√ßo.  Isso √© conseguido de uma maneira bastante simples: quando o Kubernetes cria um servi√ßo NodePort, o kube-proxy aloca uma porta no intervalo de 30000-32767 e abre essa porta na interface <code>eth0</code> de cada n√≥ (da√≠ o nome do tipo de servi√ßo - <code>NodePort</code> ).  As conex√µes feitas a essa porta (chamaremos essas portas <code>NodePort</code> ) s√£o redirecionadas para o IP do cluster do servi√ßo.  Se criarmos o servi√ßo descrito acima e executarmos o <code>kubectl get svc service-test</code> , podemos ver a porta atribu√≠da a ele. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME           CLUSTER-IP EXTERNAL-IP   PORT(S) AGE service-test   10.3.241.152 &lt;none&gt;        80:32213/TCP 1m</code> </pre> <br>  Nesse caso, o servi√ßo √© atribu√≠do ao NodePort <code>32213</code> .  Isso significa que agora podemos nos conectar ao servi√ßo atrav√©s de qualquer n√≥ em nosso cluster experimental em <code>10.100.0.2:32213</code> ou em <code>10.100.0.3:32213</code> .  Nesse caso, o tr√°fego ser√° redirecionado para o servi√ßo. <br><br>  Ap√≥s essa parte do sistema ter ocorrido, temos todos os fragmentos do pipeline para equilibrar a carga criada pelas solicita√ß√µes do cliente para todos os n√≥s do cluster. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/da5/78e/735/da578e735e468dce8077f8a1d27d8490.png"></div><br>  <i><font color="#999999">Servi√ßo NodePort</font></i> <br><br>  Na figura anterior, o cliente se conecta ao balanceador de carga por meio de um endere√ßo IP p√∫blico, o balanceador de carga seleciona o n√≥ e se conecta a ele em <code>10.100.0.3:32213</code> , o kube-proxy aceita essa conex√£o e a redireciona para o servi√ßo acess√≠vel via IP <code>10.3.241.152:80</code> cluster .  Aqui, a solicita√ß√£o √© processada com sucesso de acordo com as regras definidas pelo netfilter e √© redirecionada para o pod do servidor no endere√ßo <code>10.0.2.2:8080</code> .  Talvez tudo isso possa parecer um pouco complicado, e at√© certo ponto seja, mas n√£o √© f√°cil encontrar uma solu√ß√£o mais simples que suporte todos os recursos maravilhosos que nos fornecem pods e redes com base em servi√ßos. <br><br>  Esse mecanismo, no entanto, n√£o deixa de ter seus pr√≥prios problemas.  O uso de servi√ßos como o <code>NodePort</code> oferece aos clientes acesso a servi√ßos usando uma porta n√£o padr√£o.  Geralmente, isso n√£o √© um problema, pois o balanceador de carga pode fornecer a eles uma porta regular e ocultar o <code>NodePort</code> dos usu√°rios finais.  Mas, em alguns cen√°rios, por exemplo, ao usar um balanceador de carga externo da plataforma Google Cloud, pode ser necess√°rio implantar o <code>NodePort</code> clientes.  Deve-se notar que essas portas, al√©m disso, representam recursos limitados, embora 2768 portas sejam provavelmente suficientes, mesmo para os maiores clusters.  Na maioria dos casos, voc√™ pode permitir que o Kubernetes selecione n√∫meros de porta aleatoriamente, mas voc√™ mesmo pode configur√°-los, se necess√°rio.  Outro problema s√£o algumas limita√ß√µes relacionadas ao armazenamento de endere√ßos IP de origem nas solicita√ß√µes.  Para descobrir como resolver esses problemas, voc√™ pode consultar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este</a> material na documenta√ß√£o do Kubernetes. <br><br>  Portas <code>NodePorts</code> √© o mecanismo fundamental pelo qual todo o tr√°fego externo entra no cluster Kubernetes.  No entanto, eles pr√≥prios n√£o nos apresentam uma solu√ß√£o pronta.  Pelas raz√µes acima, antes do cluster, se os clientes s√£o entidades internas ou externas localizadas em uma rede p√∫blica, √© sempre necess√°rio ter algum tipo de balanceador de carga. <br><br>  Os arquitetos da plataforma, percebendo isso, forneceram duas maneiras de configurar o balanceador de carga a partir da pr√≥pria plataforma Kubernetes.  Vamos discutir isso. <br><br><h2>  <font color="#3AC1EF">Servi√ßos como LoadBalancer e recursos do tipo Ingress</font> </h2><br>  Servi√ßos como o <code>LoadBalancer</code> e recursos do tipo <code>Ingress</code> s√£o alguns dos mecanismos mais complexos do Kubernetes.  No entanto, n√£o gastaremos muito tempo com eles, pois seu uso n√£o leva a mudan√ßas fundamentais em tudo o que falamos at√© agora.  Todo o tr√°fego externo, como antes, entra no cluster atrav√©s do <code>NodePort</code> . <br><br>  Os arquitetos podem parar por aqui, permitindo que aqueles que criam clusters se importem apenas com endere√ßos IP p√∫blicos e balanceadores de carga.  De fato, em certas situa√ß√µes, como iniciar um cluster em servidores regulares ou em casa, √© exatamente isso que eles fazem.  Mas em ambientes que suportam configura√ß√µes de recursos de rede controladas por API, o Kubernetes permite configurar tudo o que voc√™ precisa em um s√≥ lugar. <br><br>  A primeira abordagem para resolver esse problema, a mais simples, √© usar servi√ßos do Kubernetes, como o <code>LoadBalancer</code> .  Esses servi√ßos t√™m todos os recursos de servi√ßos como <code>NodePort</code> e, al√©m disso, t√™m a capacidade de criar caminhos completos para o tr√°fego recebido, com base na suposi√ß√£o de que o cluster est√° sendo executado em ambientes como GCP ou AWS que suportam a configura√ß√£o de recursos de rede por meio da API. <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: LoadBalancer selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Se excluirmos e recriarmos o servi√ßo do nosso exemplo no Google Kubernetes Engine, logo depois disso, usando o <code>kubectl get svc service-test</code> , poderemos verificar se o IP externo est√° atribu√≠do. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME      CLUSTER-IP      EXTERNAL-IP PORT(S)          AGE openvpn   10.3.241.52     35.184.97.156 80:32213/TCP     5m</code> </pre> <br>  Foi dito acima que seremos capazes de verificar o fato de atribuir um endere√ßo IP externo "em breve", apesar de a atribui√ß√£o de um IP externo demorar alguns minutos, o que n√£o √© surpreendente, dada a quantidade de recursos que precisam ser trazidos para um estado √≠ntegro.  Na plataforma GCP, por exemplo, isso exige que o sistema crie um endere√ßo IP externo, regras de redirecionamento de tr√°fego, um servidor proxy de destino, um servi√ßo de back-end e, possivelmente, uma inst√¢ncia de um grupo.  Depois de alocar um endere√ßo IP externo, voc√™ pode se conectar ao servi√ßo atrav√©s desse endere√ßo, atribuir um nome de dom√≠nio e informar os clientes.  At√© que o servi√ßo seja destru√≠do e recriado (para fazer isso, raramente quando houver um bom motivo), o endere√ßo IP n√£o ser√° alterado. <br><br>  Servi√ßos como o <code>LoadBalancer</code> t√™m algumas limita√ß√µes.  Esse servi√ßo n√£o pode ser configurado para descriptografar o tr√°fego HTTPS.  Voc√™ n√£o pode criar hosts virtuais ou configurar o roteamento baseado em caminho; portanto, usando configura√ß√µes pr√°ticas, n√£o pode usar um √∫nico balanceador de carga com muitos servi√ßos.  Essas limita√ß√µes levaram √† introdu√ß√£o do Kubernetes 1.1.  Um recurso especial para configurar balanceadores de carga.  Este √© um recurso do tipo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ingress</a> .  Servi√ßos como o <code>LoadBalancer</code> visam expandir os recursos de um √∫nico servi√ßo para dar suporte a clientes externos.  Por outro lado, os recursos do <code>Ingress</code> s√£o recursos especiais que permitem configurar flexivelmente balanceadores de carga.  A API do Ingress suporta descriptografia do tr√°fego TLS, hosts virtuais e roteamento baseado em caminho.  Usando essa API, o balanceador de carga pode ser facilmente configurado para funcionar com v√°rios servi√ßos de back-end. <br><br>  A API do recurso do tipo <code>Ingress</code> √© muito grande para discutir seus recursos aqui; al√©m disso, n√£o afeta particularmente como os recursos do Ingress funcionam no n√≠vel da rede.  A implementa√ß√£o desse recurso segue o padr√£o usual do Kubernetes: existe um tipo de recurso e um controlador para controlar esse tipo.  O recurso nesse caso √© o recurso <code>Ingress</code> , que descreve solicita√ß√µes para recursos de rede.  Veja como pode ser a descri√ß√£o de um recurso do <code>Ingress</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations:   kubernetes.io/ingress.class: "gce" spec: tls:   - secretName: my-ssl-secret rules: - host: testhost.com   http:     paths:     - path: /*       backend:         serviceName: service-test         servicePort: 80</code> </pre> <br>  O controlador do Ingress √© respons√°vel por executar essas solicita√ß√µes, trazendo outros recursos para o estado desejado.  Ao usar o Ingress, s√£o criados servi√ßos como o <code>NodePort</code> , ap√≥s o qual o controlador do Ingress pode tomar decis√µes sobre como direcionar o tr√°fego para os n√≥s.  Existe uma implementa√ß√£o do controlador Ingress para balanceadores de carga GCE, balanceadores da AWS, para servidores proxy populares como nginx e haproxy.  Observe que a mistura de recursos e servi√ßos do Ingress como o <code>LoadBalancer</code> pode causar problemas menores em alguns ambientes.  Eles s√£o f√°ceis de manusear, mas, em geral, √© melhor usar o Ingress mesmo para servi√ßos simples. <br><br><h2>  <font color="#3AC1EF">HostPort e HostNetwork</font> </h2><br>  O que falaremos agora, a saber, <code>HostPort</code> e <code>HostNetwork</code> , pode ser atribu√≠do √† categoria de raridades interessantes, e n√£o a ferramentas √∫teis.  De fato, comprometo-me a afirmar que, em 99,99% dos casos, seu uso pode ser considerado um antipadr√£o, e qualquer sistema no qual eles s√£o usados ‚Äã‚Äãdeve passar por uma verifica√ß√£o obrigat√≥ria de sua arquitetura. <br><br>  Eu pensei que n√£o valia a pena falar sobre eles, mas eles s√£o algo parecido com as ferramentas usadas pelos recursos do Ingress para processar o tr√°fego recebido, ent√£o decidi que valia a pena mencion√°-los, pelo menos brevemente. <br><br>  Primeiro, <code>HostPort</code> falar sobre o <code>HostPort</code> .  Esta √© uma propriedade de cont√™iner (declarada na estrutura <code>ContainerPort</code> ).  Quando um determinado n√∫mero de porta √© gravado nele, isso leva √† abertura dessa porta no n√≥ e ao seu redirecionamento diretamente para o cont√™iner.  N√£o h√° mecanismos de proxy e a porta √© aberta apenas nos n√≥s nos quais o cont√™iner est√° em execu√ß√£o.  Nos primeiros dias da plataforma, antes que os mecanismos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DaemonSet</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">StatefulSet</a> aparecessem, o <code>HostPort</code> era um truque que permitia que apenas um cont√™iner de um determinado tipo fosse iniciado em qualquer n√≥.  Por exemplo, uma vez eu usei isso para criar um cluster do <code>HostPort</code> configurando o <code>HostPort</code> para <code>9200</code> e especificando quantas r√©plicas havia n√≥s.       ,          Kubernetes,    -     <code>HostPort</code> . <br><br>   <code>NostNetwork</code> , ,   Kubernetes    ,  <code>HostPort</code> .       <code>true</code> ,       - <code>network=host</code>  <code>docker run</code> .    ,           .            <code>eth0</code>    .  ,             .      ,  ,  ,    Kubernetes,     - . <br><br><h2>  <font color="#3AC1EF">Sum√°rio</font> </h2><br>        Kubernetes,   ,          Ingress. ,  ,    ,       Kubernetes. <br><br>  <b>Caros leitores!</b>     Ingress? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt442646/">https://habr.com/ru/post/pt442646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt442636/index.html">Voc√™ traz m√°s not√≠cias para a ger√™ncia?</a></li>
<li><a href="../pt442638/index.html">Escalonamento de aplicativos Kubernetes com base nas m√©tricas do Prometheus</a></li>
<li><a href="../pt442640/index.html">Bug perfeito: usando confus√£o de tipo no Flash. Parte 1</a></li>
<li><a href="../pt442642/index.html">O que ler em mar√ßo: 22 novos livros para profissionais de marketing, gerentes, desenvolvedores e designers</a></li>
<li><a href="../pt442644/index.html">A maioria das habilidades que n√£o s√£o de programa√ß√£o aumentam o valor do desenvolvedor</a></li>
<li><a href="../pt442648/index.html">Ir mecanismos de aloca√ß√£o</a></li>
<li><a href="../pt442650/index.html">An√°lise e otimiza√ß√£o de aplicativos React</a></li>
<li><a href="../pt442652/index.html">Usando Fastify e Preact para prototipar aplicativos da Web rapidamente</a></li>
<li><a href="../pt442654/index.html">Mudando para Next.js e acelerando o carregamento da p√°gina inicial do manifold.co 7,5 vezes</a></li>
<li><a href="../pt442658/index.html">8 truques para trabalhar com CSS: paralaxe, rodap√© fixo e outros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>