<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüî¨ üàπ üë©üèø‚Äçü§ù‚Äçüë©üèæ Curiosidad y dilaci√≥n en el aprendizaje autom√°tico üåà üë®üèæ‚Äçüé® üç§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El aprendizaje reforzado (RL) es una de las t√©cnicas de aprendizaje autom√°tico m√°s prometedoras que se est√° desarrollando activamente. Aqu√≠, el agente...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Curiosidad y dilaci√≥n en el aprendizaje autom√°tico</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427847/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El aprendizaje reforzado</a> (RL) es una de las t√©cnicas de aprendizaje autom√°tico m√°s prometedoras que se est√° desarrollando activamente.  Aqu√≠, el agente de IA recibe una recompensa positiva por las acciones correctas y una recompensa negativa por las acciones incorrectas.  Este m√©todo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">zanahoria y palo</a> es simple y universal.  Con √©l, DeepMind ense√±√≥ el algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DQN</a> para jugar viejos videojuegos de Atari, y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AlphaGoZero</a> para jugar el antiguo juego Go.  As√≠ que OpenAI ense√±√≥ el algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI-Five</a> para jugar al moderno videojuego Dota, y Google ense√±√≥ manos rob√≥ticas para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">capturar nuevos objetos</a> .  A pesar del √©xito de RL, todav√≠a hay muchos problemas que reducen la efectividad de esta t√©cnica. <br><br>  Los algoritmos de RL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tienen dificultades para trabajar</a> en un entorno donde el agente rara vez recibe comentarios.  Pero esto es t√≠pico del mundo real.  Como ejemplo, imagine buscar su queso favorito en un gran laberinto, como un supermercado.  Est√°s buscando un departamento con quesos, pero no lo encuentras.  Si en cada paso no obtiene un "palo" o una "zanahoria", entonces es imposible decir si se est√° moviendo en la direcci√≥n correcta.  En ausencia de una recompensa, ¬øqu√© le impide deambular para siempre?  Nada m√°s que tu curiosidad.  Motiva mudarse al departamento de comestibles, que parece desconocido. <br><a name="habracut"></a><br>  El trabajo cient√≠fico, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Curiosidad epis√≥dica a trav√©s de la accesibilidad",</a> es el resultado de una colaboraci√≥n entre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el equipo de Google Brain</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepMind</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la Escuela T√©cnica Superior Suiza de Zurich</a> .  Ofrecemos un nuevo modelo de recompensa RL epis√≥dica basada en memoria.  Parece una curiosidad que te permite explorar el entorno.  Dado que el agente no solo debe estudiar el entorno, sino tambi√©n resolver el problema inicial, nuestro modelo agrega una bonificaci√≥n a la recompensa inicialmente escasa.  La recompensa combinada ya no es escasa, lo que permite que los algoritmos RL est√°ndar aprendan de ella.  Por lo tanto, nuestro m√©todo de curiosidad ampl√≠a el rango de tareas que pueden resolverse usando RL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e9/462/46c/4e946246c97aafab60f3dbe954ca6ed2.png"><br>  <i><font color="gray">Curiosidad ocasional a trav√©s del alcance: los datos de observaci√≥n se agregan a la memoria, la recompensa se calcula en funci√≥n de cu√°n lejos est√° la observaci√≥n actual de observaciones similares en la memoria.</font></i>  <i><font color="gray">El agente recibe una recompensa mayor por las observaciones que a√∫n no se presentan en la memoria.</font></i> <br><br>  La idea principal del m√©todo es almacenar las observaciones del agente del medio ambiente en la memoria epis√≥dica, as√≠ como recompensar al agente por ver las observaciones que a√∫n no se presentan en la memoria.  "Falta de memoria" es la definici√≥n de novedad en nuestro m√©todo.  La b√∫squeda de tales observaciones significa la b√∫squeda de un extra√±o.  Tal deseo de buscar un extra√±o llevar√° al agente de IA a nuevas ubicaciones, evitando as√≠ deambular en un c√≠rculo y, en √∫ltima instancia, lo ayudar√° a tropezar con el objetivo.  Como discutimos m√°s adelante, nuestra redacci√≥n puede disuadir al agente del comportamiento indeseable al que est√°n sujetas otras palabras.  Para nuestra gran sorpresa, este comportamiento tiene algunas similitudes con lo que un laico llamar√≠a "procrastinaci√≥n". <br><br><h4>  Curiosidad previa </h4><br>  Aunque ha habido muchos intentos de formular curiosidad en el pasado <sup>[1] [2] [3] [4]</sup> , en este art√≠culo nos centraremos en un enfoque natural y muy popular: curiosidad a trav√©s de la sorpresa basada en pron√≥sticos.  Esta t√©cnica se describe en un art√≠culo reciente, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Investigaci√≥n de un entorno utilizando la curiosidad prediciendo bajo su propio control"</a> (generalmente denominado ICM).  Para ilustrar la conexi√≥n entre sorpresa y curiosidad, nuevamente utilizamos la analog√≠a de encontrar queso en un supermercado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b93/003/a3c/b93003a3c6790968f3922777926a494a.jpg"><br>  <i><font color="gray">Ilustraci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Indira Pasko</a> , bajo licencia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CC BY-NC-ND 4.0</a></font></i> <br><br>  Paseando por la tienda, est√° tratando de predecir el futuro ( <i>"Ahora estoy en el departamento de carne, as√≠ que creo que el departamento a la vuelta de la esquina es el departamento de pescado, generalmente est√°n cerca en esta cadena de supermercados"</i> ).  Si el pron√≥stico es incorrecto, se sorprender√° ( <i>"En realidad, hay un departamento de verduras. ¬°No esperaba esto!"</i> ) Y de esta forma se obtiene una recompensa.  Esto aumenta la motivaci√≥n en el futuro para mirar a la vuelta de la esquina, explorar nuevos lugares solo para verificar que sus expectativas sean verdaderas (y posiblemente tropezar con queso). <br><br>  Del mismo modo, el m√©todo ICM crea un modelo predictivo de la din√°mica del mundo y le da al agente una recompensa si el modelo no puede hacer buenas predicciones, un marcador de sorpresa o novedad.  Tenga en cuenta que explorar nuevos lugares no se articula directamente en la curiosidad de ICM.  Para el m√©todo ICM, asistir a ellos es solo una forma de obtener m√°s "sorpresas" y as√≠ maximizar su recompensa general.  Como resultado, en algunos entornos puede haber otras formas de sorprenderse, lo que conduce a resultados inesperados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a56/253/b56/a56253b56c9a991ce20b2c744f42d5a9.gif"></div><br>  <i><font color="gray">Un agente con un sistema de curiosidad basado en sorpresas se congela cuando se re√∫ne con un televisor.</font></i>  <i><font color="gray">Animaci√≥n del video de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deepak Patak</a> , licenciado bajo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CC BY 2.0</a></font></i> <br><br><h4>  El peligro de la dilaci√≥n. </h4><br>  En el art√≠culo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Un estudio a gran escala del aprendizaje basado en la curiosidad", los</a> autores del m√©todo ICM, junto con los investigadores de OpenAI, muestran un peligro oculto de maximizar la sorpresa: los agentes pueden aprender a darse el gusto en lugar de hacer algo √∫til para la tarea.  Para entender por qu√© sucede esto, considere un experimento mental que los autores llaman el "problema del ruido de la televisi√≥n".  Aqu√≠ el agente se coloca en un laberinto con la tarea de encontrar un elemento muy √∫til (como "queso" en nuestro ejemplo).  El entorno tiene un televisor y el agente tiene un control remoto.  Hay un n√∫mero limitado de canales (cada canal tiene una transmisi√≥n separada), y cada vez que se presiona el control remoto, el televisor cambia a un canal aleatorio.  ¬øC√≥mo actuar√° un agente en ese entorno? <br><br>  Si la curiosidad se forma sobre la base de la sorpresa, entonces un cambio de canales dar√° m√°s recompensas, ya que cada cambio es impredecible e inesperado.  Es importante tener en cuenta que incluso despu√©s de una exploraci√≥n c√≠clica de todos los canales disponibles, una selecci√≥n aleatoria de un canal asegura que cada nuevo cambio seguir√° siendo inesperado: el agente hace una predicci√≥n de que mostrar√° TV despu√©s de cambiar el canal, y lo m√°s probable es que el pron√≥stico sea incorrecto, lo que causar√° sorpresa.  Es importante tener en cuenta que incluso si el agente ya ha visto cada transmisi√≥n en cada canal, el cambio sigue siendo impredecible.  Debido a esto, el agente, en lugar de buscar un elemento muy √∫til, finalmente permanecer√° frente al televisor, de manera similar a la dilaci√≥n.  ¬øC√≥mo cambiar la redacci√≥n de la curiosidad para evitar este comportamiento? <br><br><h4>  Curiosidad epis√≥dica </h4><br>  En el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Curiosidad epis√≥dica a trav√©s de accesibilidad",</a> exploramos un modelo de curiosidad epis√≥dica basado en la memoria que es menos propenso al placer instant√°neo.  Por qu√©  Si tomamos el ejemplo anterior, luego de alg√∫n tiempo cambiando de canal, todas las transmisiones terminar√°n en la memoria.  Por lo tanto, el televisor perder√° su atractivo: incluso si el orden en que aparecen los programas en la pantalla es aleatorio e impredecible, ¬°todos est√°n en la memoria!  Esta es la principal diferencia con el m√©todo basado en la sorpresa: nuestro m√©todo ni siquiera trata de predecir el futuro, es dif√≠cil de predecir (o incluso imposible).  En cambio, el agente examina el pasado y verifica si hay observaciones en la memoria <i>como la</i> actual.  Por lo tanto, nuestro agente no es propenso a los placeres instant√°neos, lo que produce un "ruido de televisi√≥n".  El agente tendr√° que ir a explorar el mundo fuera del televisor para obtener m√°s recompensas. <br><br>  Pero, ¬øc√≥mo decidimos si un agente ve lo mismo que est√° almacenado en la memoria?  La comprobaci√≥n de coincidencia exacta no tiene sentido: en un entorno real, un agente rara vez ve lo mismo dos veces.  Por ejemplo, incluso si el agente regresa a la misma habitaci√≥n, seguir√° viendo esta habitaci√≥n desde un √°ngulo diferente. <br><br>  En lugar de buscar coincidencias exactas, utilizamos una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red neuronal profunda</a> que est√° entrenada para medir cu√°n similares son dos experiencias.  Para entrenar esta red, debemos adivinar qu√© tan cerca ocurrieron las observaciones a tiempo.  La proximidad en el tiempo es un buen indicador de si dos observaciones deben considerarse parte de la misma.  Tal aprendizaje conduce a un concepto general de novedad a trav√©s de la accesibilidad, que se ilustra a continuaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/f43/629/52cf436298b7bbf44550ba9770e84f1b.png"><br>  <i><font color="gray">El gr√°fico de accesibilidad define la novedad.</font></i>  <i><font color="gray">En la pr√°ctica, este gr√°fico no est√° disponible; por lo tanto, entrenamos al aproximador de la red neuronal para estimar el n√∫mero de pasos entre observaciones</font></i> <br><br><h4>  Resultados experimentales </h4><br>  Para comparar el rendimiento de diferentes enfoques para describir la curiosidad, los probamos en dos entornos 3D visualmente ricos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ViZDoom</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DMLab</a> .  En estas condiciones, al agente se le asignaron varias tareas, como encontrar un objetivo en el laberinto, recolectar objetos buenos y evitar los malos.  En el entorno DMLab, el agente est√° equipado de manera predeterminada con un dispositivo fant√°stico como un l√°ser, pero si el dispositivo no es necesario para una tarea espec√≠fica, el agente no puede usarlo libremente.  Curiosamente, basado en la sorpresa, el agente ICM realmente us√≥ el l√°ser con mucha frecuencia, ¬°incluso si era in√∫til completar la tarea!  Como en el caso de la TV, en lugar de buscar un objeto valioso en el laberinto, prefer√≠a pasar el tiempo disparando en las paredes, ya que daba muchas recompensas en forma de sorpresa.  Te√≥ricamente, el resultado de un tiro en la pared deber√≠a ser predecible, pero en la pr√°ctica es demasiado dif√≠cil de predecir.  Esto probablemente requiere un conocimiento m√°s profundo de la f√≠sica que el que est√° disponible para el agente de IA est√°ndar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/497/4e6/be3/4974e6be39718b8ab6dbd9cdfcdab273.gif"></div><br>  <i><font color="gray">El agente ICM sorprendido dispara constantemente contra la pared en lugar de explorar el laberinto</font></i> <br><br>  A diferencia de √©l, nuestro agente ha dominado un comportamiento razonable para estudiar el medio ambiente.  Esto sucedi√≥ porque no est√° tratando de predecir el resultado de sus acciones, sino que est√° buscando observaciones que est√°n "m√°s lejos" de las que est√°n en la memoria epis√≥dica.  En otras palabras, el agente persigue impl√≠citamente objetivos que requieren m√°s esfuerzo que un simple tiro al muro. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5a/002/381/b5a00238194970be7a6d6cf8969ac7f7.gif"></div><br>  <i><font color="gray">Nuestro m√©todo demuestra un comportamiento inteligente de exploraci√≥n ambiental.</font></i> <br><br>  Es interesante observar c√≥mo nuestro enfoque de recompensa castiga a un agente que se ejecuta en un c√≠rculo, porque despu√©s de completar el primer c√≠rculo, el agente no encuentra nuevas observaciones y, por lo tanto, no recibe ninguna recompensa: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/489/ef2/a8b/489ef2a8b67c8c087cff940bdfeeee9f.gif"></div><br>  <i><font color="gray">Visualizaci√≥n de recompensa: el rojo corresponde a la recompensa negativa, el verde a positivo.</font></i>  <i><font color="gray">De izquierda a derecha: tarjeta de premio, mapa con ubicaciones en la memoria, vista en primera persona</font></i> <br><br>  Al mismo tiempo, nuestro m√©todo contribuye a un buen estudio del medio ambiente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/ae3/952/075ae39528e77477099fad13610d2e4a.gif"></div><br>  <i><font color="gray">Visualizaci√≥n de recompensa: el rojo corresponde a la recompensa negativa, el verde a positivo.</font></i>  <i><font color="gray">De izquierda a derecha: tarjeta de premio, mapa con ubicaciones en la memoria, vista en primera persona</font></i> <br><br>  Esperamos que nuestro trabajo contribuya a una nueva ola de investigaci√≥n que vaya m√°s all√° del alcance de la t√©cnica de la sorpresa para educar a los agentes sobre un comportamiento m√°s inteligente.  Para un an√°lisis en profundidad de nuestro m√©todo, eche un vistazo a la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">preimpresi√≥n del trabajo cient√≠fico</a> . <br><br><h4>  Agradecimientos </h4><br>  Este proyecto es el resultado de una colaboraci√≥n entre el equipo de Google Brain, DeepMind y la Escuela T√©cnica Superior Suiza de Zurich.  Grupo de investigaci√≥n principal: Nikolay Savinov, Anton Raichuk, Rafael Marinier, Damien Vincent, Mark Pollefeys, Timothy Lillirap y Sylvain Zheli.  Nos gustar√≠a agradecer a Olivier Pietkin, Carlos Riquelme, Charles Blundell y Sergey Levine por discutir este documento.  Agradecemos a Indira Pasco por su ayuda con las ilustraciones. <br><br><h4>  Referencias a la literatura: </h4><br>  [1] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"El estudio del medio ambiente basado en contar con modelos de densidad neural"</a> , Georg Ostrovsky, Mark G. Bellemar, Aaron Van den Oord, Remy Munoz <br>  [2] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚ÄúEntornos de aprendizaje basados ‚Äã‚Äãen</a> conteo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">para el aprendizaje profundo con refuerzo‚Äù</a> , Khaoran Tan, Rain Huthuft, Davis Foot, Adam Knock, Xi Chen, Yan Duan, John Schulman, Philip de Turk, Peter Abbel <br>  [3] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Aprender sin un maestro para localizar metas para la investigaci√≥n motivada internamente",</a> Alexander Pere, Sebastien Forestier, Olivier Sigot, Pierre-Yves Udeye <br>  [4] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"VIME: Inteligencia para maximizar los cambios de informaci√≥n",</a> Rein Huthuft, Xi Chen, Yan Duan, John Schulman, Philippe de Turk, Peter Abbel </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es427847/">https://habr.com/ru/post/es427847/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es427837/index.html">Los primeros d√≠as en el equipo de desarrollo, como sucede con nosotros</a></li>
<li><a href="../es427839/index.html">Autorizaci√≥n de usuario en Django a trav√©s de GSSAPI y delegaci√≥n de derechos de usuario al servidor</a></li>
<li><a href="../es427841/index.html">Estafa de salto m√°gico</a></li>
<li><a href="../es427843/index.html">C√≥mo dormir bien y mal</a></li>
<li><a href="../es427845/index.html">C√≥mo colocar un mill√≥n de estrellas en un iPhone</a></li>
<li><a href="../es427849/index.html">L√≠nea recta con TM. v3.0</a></li>
<li><a href="../es427853/index.html">Reflexiones sobre TDD. ¬øPor qu√© esta metodolog√≠a no es ampliamente reconocida?</a></li>
<li><a href="../es427855/index.html">Mitaps MOSDROID en FunCorp</a></li>
<li><a href="../es427857/index.html">Asuntos fiscales y legales para aut√≥nomos principiantes</a></li>
<li><a href="../es427859/index.html">Por qu√© las habilidades t√©cnicas para el gerente de proyecto: explicar en casos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>