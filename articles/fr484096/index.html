<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§æüèΩ üôáüèæ üë®üèΩ‚Äç‚öñÔ∏è La bataille des deux Yakozun, ou Cassandra vs HBase. Exp√©rience de l'√©quipe Sberbank ‚òùÔ∏è üöö ‚ÄºÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ce n'est m√™me pas une blague, il semble que cette image particuli√®re refl√®te le plus fid√®lement l'essence de ces bases de donn√©es, et √† la fin, il ser...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La bataille des deux Yakozun, ou Cassandra vs HBase. Exp√©rience de l'√©quipe Sberbank</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/484096/">  Ce n'est m√™me pas une blague, il semble que cette image particuli√®re refl√®te le plus fid√®lement l'essence de ces bases de donn√©es, et √† la fin, il sera clair pourquoi: <br><br><img src="https://habrastorage.org/webt/i2/lk/zo/i2lkzo9tq7zpeprcbtgm3-mufk4.png"><br><br>  Selon DB-Engines Ranking, les deux bases de colonnes NoSQL les plus populaires sont Cassandra (ci-apr√®s CS) et HBase (HB). <br><br><img src="https://habrastorage.org/webt/su/rd/39/surd39n7bmrbnxgpn0512tnxamm.png"><br><br>  Par la volont√© du destin, notre √©quipe de gestion du chargement des donn√©es √† Sberbank travaille en √©troite collaboration avec HB depuis <a href="https://habr.com/ru/company/sberbank/blog/420425/">longtemps</a> .  Pendant ce temps, nous avons assez bien √©tudi√© ses forces et ses faiblesses et appris √† le cuisiner.  Cependant, la pr√©sence d'une alternative sous forme de CS tout le temps m'a fait me tourmenter de doutes: avons-nous fait le bon choix?  De plus, les r√©sultats de la <a href="https://www.datastax.com/products/compare/nosql-performance-benchmarks">comparaison</a> effectu√©e par DataStax indiquent que CS bat facilement HB avec presque un score d'√©crasement.  D'un autre c√¥t√©, DataStax est une personne int√©ress√©e, et vous ne devriez pas prendre un mot ici.  De plus, une assez petite quantit√© d'informations sur les conditions de test √©tait embarrassante, nous avons donc d√©cid√© de d√©couvrir de mani√®re ind√©pendante qui √©tait le roi de BigData NoSql, et les r√©sultats √©taient tr√®s int√©ressants. <br><a name="habracut"></a><br>  Cependant, avant de passer aux r√©sultats des tests effectu√©s, il est n√©cessaire de d√©crire les aspects essentiels des configurations de l'environnement.  Le fait est que CS peut √™tre utilis√© en mode de tol√©rance de perte de donn√©es.  C'est-√†-dire  c'est lorsqu'un seul serveur (n≈ìud) est responsable des donn√©es d'une certaine cl√©, et s'il tombe pour une raison quelconque, la valeur de cette cl√© sera perdue.  Pour de nombreuses t√¢ches, ce n'est pas critique, mais pour le secteur bancaire, c'est l'exception plut√¥t que la r√®gle.  Dans notre cas, il est important d'avoir plusieurs copies de donn√©es pour un stockage fiable. <br><br>  Par cons√©quent, seul le mode CS de triple r√©plication a √©t√© consid√©r√©, c'est-√†-dire  la cr√©ation de cas a √©t√© effectu√©e avec les param√®tres suivants: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> KEYSPACE ks <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> <span class="hljs-keyword"><span class="hljs-keyword">REPLICATION</span></span> = {<span class="hljs-string"><span class="hljs-string">'class'</span></span> : <span class="hljs-string"><span class="hljs-string">'NetworkTopologyStrategy'</span></span>, <span class="hljs-string"><span class="hljs-string">'datacenter1'</span></span> : <span class="hljs-number"><span class="hljs-number">3</span></span>};</code> </pre> <br>  De plus, il existe deux fa√ßons d'assurer le niveau de coh√©rence requis.  R√®gle g√©n√©rale: <br>  NW + NR&gt; RF <br><br>  Cela signifie que le nombre de confirmations des n≈ìuds lors de l'√©criture (NW) plus le nombre de confirmations des n≈ìuds lors de la lecture (NR) doit √™tre sup√©rieur au facteur de r√©plication.  Dans notre cas, RF = 3 et donc les options suivantes conviennent: <br>  2 + 2&gt; 3 <br>  3 + 1&gt; 3 <br><br>  Puisqu'il est fondamental pour nous de garder les donn√©es aussi fiables que possible, un sch√©ma 3 + 1 a √©t√© choisi.  De plus, HB travaille sur une base similaire, c'est-√†-dire  une telle comparaison serait plus honn√™te. <br><br>  Il convient de noter que DataStax a fait le contraire dans leur √©tude, ils ont d√©fini RF = 1 pour CS et HB (pour ce dernier en modifiant les param√®tres HDFS).  C'est un aspect vraiment important, car l'impact sur les performances CS dans ce cas est √©norme.  Par exemple, l'image ci-dessous montre l'augmentation du temps requis pour charger les donn√©es dans CS: <br><br><img src="https://habrastorage.org/webt/fw/az/r9/fwazr9muypegpgjpyaq4rnagg8u.png"><br><br>  Nous voyons ici ce qui suit, plus les threads concurrents √©crivent des donn√©es, plus cela prend de temps.  C'est naturel, mais il est important que la d√©gradation des performances pour RF = 3 soit significativement plus √©lev√©e.  En d'autres termes, si nous √©crivons 5 tableaux dans 4 tableaux chacun (20 au total), RF = 3 perd environ 2 fois (150 secondes RF = 3 contre 75 pour RF = 1).  Mais si nous augmentons la charge en chargeant les donn√©es dans 8 tables dans chacun des 5 flux (40 au total), alors perdre RF = 3 est d√©j√† 2,7 fois (375 secondes contre 138). <br><br>  C'est peut-√™tre en partie le secret de la r√©ussite des tests de stress DataStax pour CS, car pour HB sur notre stand, le changement du facteur de r√©plication de 2 √† 3 n'a eu aucun effet.  C'est-√†-dire  les disques ne sont pas le goulot d'√©tranglement pour HB pour notre configuration.  Cependant, il existe de nombreux autres pi√®ges, car il convient de noter que notre version de HB a √©t√© l√©g√®rement corrig√©e et obscurcie, les environnements sont compl√®tement diff√©rents, etc.  Il convient √©galement de noter que je ne sais peut-√™tre pas comment pr√©parer correctement CS et qu'il existe des moyens plus efficaces de travailler avec lui et j'esp√®re que dans les commentaires, nous le d√©couvrirons.  Mais tout d'abord. <br><br>  Tous les tests ont √©t√© effectu√©s sur un cluster de fer compos√© de 4 serveurs, chacun dans une configuration: <br><br>  <i>CPU: Xeon E5-2680 v4 @ 2.40GHz 64 threads.</i> <i><br></i>  <i>Disques: 12 morceaux de disque dur SATA</i> <i><br></i>  <i>version java: 1.8.0_111</i> <i><br></i> <br><br>  Version CS: 3.11.5 <br><br><div class="spoiler">  <b class="spoiler_title">Param√®tres cassandra.yml</b> <div class="spoiler_text">  num_tokens: 256 <br>  hinted_handoff_enabled: true <br>  hinted_handoff_throttle_in_kb: 1024 <br>  max_hints_delivery_threads: 2 <br>  hints_directory: / data10 / cassandra / hints <br>  hints_flush_period_in_ms: 10000 <br>  max_hints_file_size_in_mb: 128 <br>  batchlog_replay_throttle_in_kb: 1024 <br>  authentificateur: AllowAllAuthenticator <br>  authorizer: AllowAllAuthorizer <br>  role_manager: CassandraRoleManager <br>  roles_validity_in_ms: 2000 <br>  permissions_validity_in_ms: 2000 <br>  credentials_validity_in_ms: 2000 <br>  partitionneur: org.apache.cassandra.dht.Murmur3Partitioner <br>  data_file_directories: <br>  - / data1 / cassandra / data # chaque r√©pertoire dataN est un lecteur s√©par√© <br>  - / data2 / cassandra / data <br>  - / data3 / cassandra / data <br>  - / data4 / cassandra / data <br>  - / data5 / cassandra / data <br>  - / data6 / cassandra / data <br>  - / data7 / cassandra / data <br>  - / data8 / cassandra / data <br>  commit_directory: / data9 / cassandra / commitlog <br>  cdc_enabled: false <br>  disk_failure_policy: arr√™ter <br>  commit_failure_policy: arr√™ter <br>  prepare_statements_cache_size_mb: <br>  thrift_prepared_statements_cache_size_mb: <br>  key_cache_size_in_mb: <br>  key_cache_save_period: 14400 <br>  row_cache_size_in_mb: 0 <br>  row_cache_save_period: 0 <br>  counter_cache_size_in_mb: <br>  counter_cache_save_period: 7200 <br>  r√©pertoire_caches_enregistr√©es: / data10 / cassandra / caches_enregistr√©s <br>  commitlog_sync: p√©riodique <br>  commitlog_sync_period_in_ms: 10000 <br>  commitlog_segment_size_in_mb: 32 <br>  seed_provider: <br>  - nom_classe: org.apache.cassandra.locator.SimpleSeedProvider <br>  param√®tres: <br>  - graines: "*, *" <br>  concurrent_reads: 256 # essay√© 64 - aucune diff√©rence not√©e <br>  concurrent_writes: 256 # a essay√© 64 - aucune diff√©rence not√©e <br>  concurrent_counter_writes: 256 # a essay√© 64 - aucune diff√©rence not√©e <br>  concurrent_materialized_view_writes: 32 <br>  memtable_heap_space_in_mb: 2048 # a essay√© 16 Go - √©tait plus lent <br>  memtable_allocation_type: heap_buffers <br>  index_summary_capacity_in_mb: <br>  index_summary_resize_interval_in_minutes: 60 <br>  trickle_fsync: false <br>  trickle_fsync_interval_in_kb: 10240 <br>  stockage_port: 7000 <br>  ssl_storage_port: 7001 <br>  listen_address: * <br>  broadcast_address: * <br>  listen_on_broadcast_address: true <br>  internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator <br>  start_native_transport: true <br>  native_transport_port: 9042 <br>  start_rpc: true <br>  rpc_address: * <br>  rpc_port: 9160 <br>  rpc_keepalive: true <br>  rpc_server_type: synchronisation <br>  thrift_framed_transport_size_in_mb: 15 <br>  incremental_backups: false <br>  snapshot_before_compaction: false <br>  auto_snapshot: true <br>  column_index_size_in_kb: 64 <br>  column_index_cache_size_in_kb: 2 <br>  concurrents_compacteurs: 4 <br>  compaction_throughput_mb_per_sec: 1600 <br>  sstable_preemptive_open_interval_in_mb: 50 <br>  read_request_timeout_in_ms: 100000 <br>  range_request_timeout_in_ms: 200000 <br>  write_request_timeout_in_ms: 40000 <br>  counter_write_request_timeout_in_ms: 100000 <br>  cas_contention_timeout_in_ms: 20000 <br>  truncate_request_timeout_in_ms: 60000 <br>  request_timeout_in_ms: 200000 <br>  slow_query_log_timeout_in_ms: 500 <br>  cross_node_timeout: false <br>  endpoint_snitch: GossipingPropertyFileSnitch <br>  dynamic_snitch_update_interval_in_ms: 100 <br>  dynamic_snitch_reset_interval_in_ms: 600000 <br>  dynamic_snitch_badness_threshold: 0,1 <br>  request_scheduler: org.apache.cassandra.scheduler.NoScheduler <br>  server_encryption_options: <br>  internode_encryption: aucun <br>  client_encryption_options: <br>  activ√©: faux <br>  internode_compression: dc <br>  inter_dc_tcp_nodelay: false <br>  tracetype_query_ttl: 86400 <br>  tracetype_repair_ttl: 604800 <br>  enable_user_defined_functions: false <br>  enable_scripted_user_defined_functions: false <br>  windows_timer_interval: 1 <br>  transparent_data_encryption_options: <br>  activ√©: faux <br>  tombstone_warn_threshold: 1000 <br>  tombstone_failure_threshold: 100000 <br>  batch_size_warn_threshold_in_kb: 200 <br>  batch_size_fail_threshold_in_kb: 250 <br>  unlogged_batch_across_partitions_warn_threshold: 10 <br>  compaction_large_partition_warning_threshold_mb: 100 <br>  gc_warn_threshold_in_ms: 1000 <br>  back_pressure_enabled: false <br>  enable_materialized_views: true <br>  enable_sasi_indexes: true <br></div></div><br>  Param√®tres GC: <br><br><div class="spoiler">  <b class="spoiler_title">### Param√®tres CMS</b> <div class="spoiler_text">  -XX: + UseParNewGC <br>  -XX: + UseConcMarkSweepGC <br>  -XX: + CMSParallelRemarkEnabled <br>  -XX: SurvivorRatio = 8 <br>  -XX: MaxTenuringThreshold = 1 <br>  -XX: CMSInitiatingOccupancyFraction = 75 <br>  -XX: + UseCMSInitiatingOccupancyOnly <br>  -XX: CMSWaitDuration = 10000 <br>  -XX: + CMSParallelInitialMarkEnabled <br>  -XX: + CMSEdenChunksRecordAlways <br>  -XX: + CMSClassUnloadingEnabled <br><br></div></div><br>  La m√©moire jvm.options a √©t√© allou√©e 16 Go (toujours essay√© 32 Go, aucune diff√©rence n'a √©t√© remarqu√©e). <br><br>  La cr√©ation de tables a √©t√© effectu√©e par la commande: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> ks.t1 (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span> <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span> PRIMARY <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>, title <span class="hljs-built_in"><span class="hljs-built_in">text</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> compression = {<span class="hljs-string"><span class="hljs-string">'sstable_compression'</span></span>: <span class="hljs-string"><span class="hljs-string">'LZ4Compressor'</span></span>, <span class="hljs-string"><span class="hljs-string">'chunk_length_kb'</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span>};</code> </pre> <br>  Version HB: 1.2.0-cdh5.14.2 (dans la classe org.apache.hadoop.hbase.regionserver.HRegion nous avons exclu MetricsRegion qui a conduit au GC avec plus de 1000 r√©gions sur RegionServer) <br><br><div class="spoiler">  <b class="spoiler_title">Options HBase non par d√©faut</b> <div class="spoiler_text">  zookeeper.session.timeout: 120000 <br>  hbase.rpc.timeout: 2 minute (s) <br>  hbase.client.scanner.timeout.period: 2 minute (s) <br>  hbase.master.handler.count: 10 <br>  hbase.regionserver.lease.period, hbase.client.scanner.timeout.period: 2 minute (s) <br>  hbase.regionserver.handler.count: 160 <br>  hbase.regionserver.metahandler.count: 30 <br>  hbase.regionserver.logroll.period: 4 heure (s) <br>  hbase.regionserver.maxlogs: 200 <br>  hbase.hregion.memstore.flush.size: 1 Gio <br>  hbase.hregion.memstore.block.multiplier: 6 <br>  hbase.hstore.compactionThreshold: 5 <br>  hbase.hstore.blockingStoreFiles: 200 <br>  hbase.hregion.majorcompaction: 1 jour (s) <br>  Extrait de configuration avanc√©e du service HBase (soupape de s√©curit√©) pour hbase-site.xml: <br>  hbase.regionserver.wal.codecorg.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec <br>  hbase.master.namespace.init.timeout3600000 <br>  hbase.regionserver.optionalcacheflushinterval18000000 <br>  hbase.regionserver.thread.compaction.large12 <br>  hbase.regionserver.wal.enablecompressiontrue <br>  hbase.hstore.compaction.max.size1073741824 <br>  hbase.server.compactchecker.interval.multiplier200 <br>  Options de configuration Java pour HBase RegionServer: <br>  -XX: + UseParNewGC -XX: + UseConcMarkSweepGC -XX: CMSInitiatingOccupancyFraction = 70 -XX: + CMSParallelRemarkEnabled -XX: ReservedCodeCacheSize = 256m <br>  hbase.snapshot.master.timeoutMillis: 2 minute (s) <br>  hbase.snapshot.region.timeout: 2 minute (s) <br>  hbase.snapshot.master.timeout.millis: 2 minute (s) <br>  Taille maximale du journal du serveur HBase REST: 100 Mio <br>  Sauvegardes maximales du fichier journal du serveur HBase REST: 5 <br>  Taille maximale du journal du serveur HBase Thrift: 100 Mio <br>  Sauvegardes maximales des fichiers journaux de HBase Thrift Server: 5 <br>  Taille maximale du journal ma√Ætre: 100 Mio <br>  Sauvegardes maximales du fichier journal principal: 5 <br>  Taille maximale du journal RegionServer: 100 Mio <br>  Sauvegardes maximales du fichier journal de RegionServer: 5 <br>  Fen√™tre de d√©tection de HBase Active Master: 4 minute (s) <br>  dfs.client.hedged.read.threadpool.size: 40 <br>  dfs.client.hedged.read.threshold.millis: 10 milliseconde (s) <br>  hbase.rest.threads.min: 8 <br>  hbase.rest.threads.max: 150 <br>  Descripteurs de fichiers de processus maximum: 180 000 <br>  hbase.thrift.minWorkerThreads: 200 <br>  hbase.master.executor.openregion.threads: 30 <br>  hbase.master.executor.closeregion.threads: 30 <br>  hbase.master.executor.serverops.threads: 60 <br>  hbase.regionserver.thread.compaction.small: 6 <br>  hbase.ipc.server.read.threadpool.size: 20 <br>  Fils de d√©placement de r√©gion: 6 <br>  Taille du segment de m√©moire Java client en octets: 1 Gio <br>  Groupe par d√©faut du serveur HBase REST: 3 Gio <br>  Groupe par d√©faut du serveur HBase Thrift Server: 3 Gio <br>  Taille de tas Java du ma√Ætre HBase en octets: 16 Gio <br>  Taille de segment de m√©moire Java de HBase RegionServer en octets: 32 Gio <br><br>  + ZooKeeper <br>  maxClientCnxns: 601 <br>  maxSessionTimeout: 120000 </div></div><br>  Cr√©ation de tableaux: <br>  <i>hbase org.apache.hadoop.hbase.util.RegionSplitter ns: t1 UniformSplit -c 64 -f cf</i> <i><br></i>  <i>modifier 'ns: t1', {NAME =&gt; 'cf', DATA_BLOCK_ENCODING =&gt; 'FAST_DIFF', COMPRESSION =&gt; 'GZ'}</i> <br><br>  Il y a un point important - la description DataStax ne dit pas combien de r√©gions ont √©t√© utilis√©es pour cr√©er les tables HB, bien que cela soit critique pour les gros volumes.  Par cons√©quent, pour les tests, le nombre = 64 a √©t√© choisi, ce qui permet de stocker jusqu'√† 640 Go, soit  table de taille moyenne. <br><br>  Au moment du test, HBase avait 22 000 tables et 67 000 r√©gions (ce serait mortel pour la version 1.2.0, sinon pour le patch mentionn√© ci-dessus). <br><br>  Maintenant, pour le code.  Puisqu'il n'√©tait pas clair quelles configurations sont plus avantageuses pour une base de donn√©es particuli√®re, les tests ont √©t√© effectu√©s dans diverses combinaisons.  C'est-√†-dire  dans certains tests, la charge est pass√©e simultan√©ment √† 4 tables (les 4 n≈ìuds ont √©t√© utilis√©s pour la connexion).  Dans d'autres tests, ils ont travaill√© avec 8 tables diff√©rentes.  Dans certains cas, la taille du lot √©tait de 100, dans d'autres 200 (param√®tre de lot - voir code ci-dessous).  La taille des donn√©es pour la valeur est de 10 octets ou 100 octets (dataSize).  Au total, 5 millions d'enregistrements ont √©t√© √©crits et soustraits √† chaque fois dans chaque tableau.  Dans le m√™me temps, 5 flux ont √©t√© √©crits / lus dans chaque table (le num√©ro de flux est thNum), chacun utilisant sa propre plage de cl√©s (nombre = 1 million): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"BEGIN BATCH "</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); sb.append(<span class="hljs-string"><span class="hljs-string">"INSERT INTO "</span></span>) .append(tableName) .append(<span class="hljs-string"><span class="hljs-string">"(id, title) "</span></span>) .append(<span class="hljs-string"><span class="hljs-string">"VALUES ("</span></span>) .append(key) .append(<span class="hljs-string"><span class="hljs-string">", '"</span></span>) .append(value) .append(<span class="hljs-string"><span class="hljs-string">"');"</span></span>); key++; } sb.append(<span class="hljs-string"><span class="hljs-string">"APPLY BATCH;"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); session.execute(query); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"SELECT * FROM "</span></span>).append(tableName).append(<span class="hljs-string"><span class="hljs-string">" WHERE id IN ("</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { sb = sb.append(key); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i+<span class="hljs-number"><span class="hljs-number">1</span></span> &lt; batch) sb.append(<span class="hljs-string"><span class="hljs-string">","</span></span>); key++; } sb = sb.append(<span class="hljs-string"><span class="hljs-string">");"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); ResultSet rs = session.execute(query); } }</code> </pre><br>  En cons√©quence, une fonctionnalit√© similaire a √©t√© fournie pour HB: <br><br><pre> <code class="java hljs">Configuration conf = getConf(); HTable table = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HTable(conf, keyspace + <span class="hljs-string"><span class="hljs-string">":"</span></span> + tableName); table.setAutoFlush(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); List&lt;Get&gt; lGet = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); List&lt;Put&gt; lPut = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] cf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"cf"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] qf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lPut.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Put p = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Put(makeHbaseRowKey(key)); String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); p.addColumn(cf, qf, value.getBytes()); lPut.add(p); key++; } table.put(lPut); table.flushCommits(); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lGet.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Get g = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Get(makeHbaseRowKey(key)); lGet.add(g); key++; } Result[] rs = table.get(lGet); } }</code> </pre><br>  √âtant donn√© que le client doit veiller √† la distribution uniforme des donn√©es dans HB, la fonction de salage cl√© ressemblait √† ceci: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] makeHbaseRowKey(<span class="hljs-keyword"><span class="hljs-keyword">long</span></span> key) { <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] nonSaltedRowKey = Bytes.toBytes(key); CRC32 crc32 = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CRC32(); crc32.update(nonSaltedRowKey); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> crc32Value = crc32.getValue(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] salt = Arrays.copyOfRange(Bytes.toBytes(crc32Value), <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ArrayUtils.addAll(salt, nonSaltedRowKey); }</code> </pre><br>  Maintenant, les r√©sultats les plus int√©ressants sont: <br><br><img src="https://habrastorage.org/webt/id/yd/pc/idydpc9plsmulsycf0i-wqy3c3c.png"><br><br>  Identique √† un graphique: <br><br><img src="https://habrastorage.org/webt/72/ag/o1/72ago1u2gdnlufjanqwavk5p1jk.png"><br><br>  L'avantage de HB est tellement incroyable qu'il y a une suspicion d'une sorte de goulot d'√©tranglement dans les param√®tres CS.  Cependant, la recherche sur Google et la torsion des param√®tres les plus √©vidents (comme concurrent_writes ou memtable_heap_space_in_mb) n'ont pas donn√© d'acc√©l√©ration.  En m√™me temps, les b√ªches sont propres, ne jurez rien. <br><br>  Les donn√©es sont r√©parties uniform√©ment sur les n≈ìuds, les statistiques de tous les n≈ìuds sont approximativement les m√™mes. <br><br><div class="spoiler">  <b class="spoiler_title">Voici les statistiques sur la table avec l'un des n≈ìuds</b> <div class="spoiler_text">  Espace cl√©: ks <br>  Nombre de lectures: 9383707 <br>  Latence de lecture: 0,04287025042448576 ms <br>  Nombre d'√©crits: 15462012 <br>  Latence d'√©criture: 0,1350068438699957 ms <br>  Flushs en attente: 0 <br>  Tableau: t1 <br>  Nombre d'STable: 16 <br>  Espace utilis√© (live): 148,59 MiB <br>  Espace utilis√© (total): 148,59 MiB <br>  Espace utilis√© par les instantan√©s (total): 0 octet <br>  M√©moire hors tas utilis√©e (total): 5,17 Mio <br>  Ratio de compression SSTable: 0,5720989576459437 <br>  Nombre de partitions (estimation): 3970323 <br>  Nombre de cellules Memtable: 0 <br>  Taille des donn√©es Memtable: 0 octets <br>  Memtable off heap memory used: 0 octets <br>  Nombre de commutateurs Memtable: 5 <br>  Nombre de lectures locales: 2346045 <br>  Latence de lecture locale: NaN ms <br>  Nombre d'√©crits locaux: 3865503 <br>  Latence d'√©criture locale: NaN ms <br>  Rin√ßages en attente: 0 <br>  Pourcentage r√©par√©: 0,0 <br>  Faux positifs du filtre Bloom: 25 <br>  Faux rapport du filtre Bloom: 0,00000 <br>  Espace de filtrage Bloom utilis√©: 4,57 Mio <br>  Filtre Bloom sur la m√©moire de tas utilis√©e: 4,57 Mio <br>  R√©sum√© de l'index hors de la m√©moire de tas utilis√©e: 590,02 Kio <br>  M√©tadonn√©es de compression hors m√©moire de tas utilis√©es: 19,45 Ko <br>  Octets minimum de partition compact√©e: 36 <br>  Octets maximum de la partition compact√©e: 42 <br>  Octets moyens de la partition compact√©e: 42 <br>  Cellules vivantes moyennes par tranche (cinq derni√®res minutes): NaN <br>  Nombre maximal de cellules vivantes par tranche (cinq derni√®res minutes): 0 <br>  Pierres tombales moyennes par tranche (cinq derni√®res minutes): NaN <br>  Pierres tombales maximum par tranche (cinq derni√®res minutes): 0 <br>  Mutations supprim√©es: 0 octet <br></div></div><br>  Une tentative de r√©duire la taille du lot (jusqu'√† l'envoi un par un) n'a pas eu d'effet, elle n'a fait qu'empirer.  Il est possible qu'en fait ce soit vraiment la performance maximale pour CS, car les r√©sultats obtenus sur CS sont similaires √† ceux obtenus pour DataStax - environ des centaines de milliers d'op√©rations par seconde.  De plus, si vous regardez l'utilisation des ressources, vous verrez que CS utilise beaucoup plus de CPU et de disques: <br><br><img src="https://habrastorage.org/webt/us/fo/i4/usfoi4-mgkktzlosilmz2ogm7uu.png"><br>  <i>La figure montre l'utilisation pendant l'ex√©cution de tous les tests cons√©cutifs pour les deux bases de donn√©es.</i> <br><br>  En ce qui concerne les puissants avantages de lecture de HB.  On peut voir que pour les deux bases de donn√©es, l'utilisation du disque pendant la lecture est extr√™mement faible (les tests de lecture sont la derni√®re partie du cycle de test pour chaque base de donn√©es, par exemple, pour CS de 15:20 √† 15:40).  Dans le cas de HB, la raison est claire - la plupart des donn√©es sont bloqu√©es en m√©moire, dans memstore, et certaines ont √©t√© mises en cache dans blockcache.  Quant √† CS, il n'est pas tr√®s clair comment cela fonctionne, cependant, l'utilisation du disque n'est pas non plus visible, mais juste au cas o√π une tentative a √©t√© faite pour activer le cache row_cache_size_in_mb = 2048 et d√©finir la mise en cache = {'keys': 'ALL', 'row_per_partition': ' 2 000 000 '}, mais cela a encore aggrav√© la situation. <br><br>  Il convient √©galement une fois de plus de dire un point significatif sur le nombre de r√©gions de HB.  Dans notre cas, la valeur 64 √©tait indiqu√©e. Si vous la r√©duisez et la rendez √©gale √† par exemple 4, alors lors de la lecture la vitesse diminue de 2 fois.  La raison en est que memstore se bouchera plus rapidement et les fichiers seront vid√©s plus souvent et lors de la lecture, il devra traiter plus de fichiers, ce qui est une op√©ration assez compliqu√©e pour HB.  En conditions r√©elles, cela peut √™tre trait√© en r√©fl√©chissant √† la strat√©gie de pr√©-plantation et de compactage, en particulier, nous utilisons un utilitaire self-made qui recueille les ordures et compresse les HFiles en permanence en arri√®re-plan.  Il est possible que pour les tests DataStax, g√©n√©ralement 1 r√©gion soit allou√©e par table (ce qui n'est pas correct) et cela clarifierait quelque peu pourquoi HB a tant perdu dans leurs tests de lecture. <br><br>  Les conclusions pr√©liminaires en sont les suivantes.  En supposant qu'aucune erreur grossi√®re n'ait √©t√© commise lors des tests, Cassandra est comme un colosse aux pieds d'argile.  Plus pr√©cis√©ment, alors qu'elle est en √©quilibre sur une jambe, comme sur la photo au d√©but de l'article, elle montre des r√©sultats relativement bons, mais lorsqu'elle combat dans les m√™mes conditions, elle perd carr√©ment.  Dans le m√™me temps, compte tenu de la faible utilisation du CPU sur notre mat√©riel, nous avons appris √† planter deux RegionServer HB par h√¥te et ainsi doubl√© la productivit√©.  C'est-√†-dire  compte tenu de l'utilisation des ressources, la situation de la CS est encore plus d√©plorable. <br><br>  Bien s√ªr, ces tests sont assez synth√©tiques et la quantit√© de donn√©es utilis√©es ici est relativement modeste.  Il est possible que lors du passage aux t√©raoctets, la situation soit diff√©rente, mais si pour HB, nous pouvons charger des t√©raoctets, alors pour CS, cela s'est av√©r√© probl√©matique.  Il lan√ßait souvent une OperationTimedOutException m√™me avec ces volumes, bien que les param√®tres d'attente de r√©ponse aient d√©j√† √©t√© augment√©s de plusieurs fois par rapport aux param√®tres par d√©faut. <br><br>  J'esp√®re que gr√¢ce √† des efforts conjoints, nous trouverons les goulots d'√©tranglement CS et si nous parvenons √† l'acc√©l√©rer, j'ajouterai certainement des informations sur les r√©sultats finaux √† la fin de l'article. <br><br>  <b>UPD: Les</b> directives suivantes ont √©t√© appliqu√©es lors de la configuration de CS: <br><br>  <i>disk_optimization_strategy: rotation</i> <i><br></i>  <i>MAX_HEAP_SIZE = "32G"</i> <i><br></i>  <i>HEAP_NEWSIZE = "3200M"</i> <i><br></i>  <i>-Xms32G</i> <i><br></i>  <i>-Xmx32G</i> <i><br></i>  <i>-XX: + UseG1GC</i> <i><br></i>  <i>-XX: G1RSetUpdatingPauseTimePercent = 5</i> <i><br></i>  <i>-XX: MaxGCPauseMillis = 500</i> <i><br></i>  <i>-XX: InitiatorHeapOccupancyPercent = 70</i> <i><br></i>  <i>-XX: ParallelGCThreads = 32</i> <i><br></i>  <i>-XX: ConcGCThreads = 8</i> <br><br>  Quant aux param√®tres du syst√®me d'exploitation, il s'agit d'une proc√©dure assez longue et compliqu√©e (obtention de root, red√©marrage des serveurs, etc.), donc ces recommandations n'ont pas √©t√© appliqu√©es.  D'un autre c√¥t√©, les deux bases de donn√©es sont dans des conditions √©gales, donc tout est juste. <br><br>  Dans la partie code, un connecteur est cr√©√© pour tous les threads √©crivant dans la table: <br><pre> <code class="java hljs">connector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CassandraConnector(); connector.connect(node, <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>, CL); session = connector.getSession(); session.getCluster().getConfiguration().getSocketOptions().setConnectTimeoutMillis(<span class="hljs-number"><span class="hljs-number">120000</span></span>); KeyspaceRepository sr = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> KeyspaceRepository(session); sr.useKeyspace(keyspace); prepared = session.prepare(<span class="hljs-string"><span class="hljs-string">"insert into "</span></span> + tableName + <span class="hljs-string"><span class="hljs-string">" (id, title) values (?, ?)"</span></span>);</code> </pre> <br><br>  Les donn√©es ont √©t√© envoy√©es via une liaison: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); session.execute(prepared.bind(key, value)); }</code> </pre> <br><br>  Cela n'a pas eu d'impact significatif sur les performances d'enregistrement.  Pour la fiabilit√©, j'ai lanc√© la charge avec l'outil YCSB, absolument le m√™me r√©sultat.  Voici les statistiques pour un thread (sur 4): <br><br>  <i>2020-01-18 14: 41: 53: 180 315 sec: 10 000 000 d'op√©rations;</i>  <i>21589,1 op√©rations courantes / s;</i>  <i>[NETTOYAGE: Nombre = 100, Max = 2236415, Min = 1, Moy = 22356,39, 90 = 4, 99 = 24, 99,9 = 2236415, 99,99 = 2236415] [INS√âRER: Nombre = 119551, Max = 174463, Min = 273, Moy = 2582,71, 90 = 3491, 99 = 16767, 99,9 = 99711, 99,99 = 171263]</i> <i><br></i>  <i>[GLOBAL], RunTime (ms), 315539</i> <i><br></i>  <i>[GLOBAL], D√©bit (ops / sec), 31691.803548848162</i> <i><br></i>  <i>[TOTAL_GCS_PS_Scavenge], nombre, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_Scavenge], temps (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_Scavenge], Heure (%), 0,7710615803434757</i> <i><br></i>  <i>[TOTAL_GCS_PS_MarkSweep], nombre, 0</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_MarkSweep], temps (ms), 0</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_MarkSweep], heure (%), 0,0</i> <i><br></i>  <i>[TOTAL_GCs], nombre, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME], temps (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME_%], heure (%), 0,7710615803434757</i> <i><br></i>  <i>[INS√âRER], Op√©rations, 10 000 000</i> <i><br></i>  <i>[INS√âRER], AverageLatency (us), 3114.2427012</i> <i><br></i>  <i>[INS√âRER], MinLatency (us), 269</i> <i><br></i>  <i>[INS√âRER], MaxLatency (us), 609279</i> <i><br></i>  <i>[INS√âRER], 95thPercentileLatency (us), 5007</i> <i><br></i>  <i>[INS√âRER], 99thPercentileLatency (us), 33439</i> <i><br></i>  <i>[INS√âRER], Retour = OK, 10000000</i> <i><br></i> <br><br>  Ici, vous pouvez voir que la vitesse d'un flux est d'environ 32 000 enregistrements par seconde, 4 flux ont fonctionn√©, il en r√©sulte 128 000. Il semble qu'il n'y ait plus rien √† presser sur les param√®tres actuels du sous-syst√®me de disque. <br><br>  √Ä propos de la lecture plus int√©ressante.  Gr√¢ce aux conseils de camarades, il a pu acc√©l√©rer radicalement.  La lecture a √©t√© effectu√©e non pas en 5 flux, mais en 100. Une augmentation √† 200 n'a pas produit d'effet.  √âgalement ajout√© au g√©n√©rateur: <br>  .withLoadBalancingPolicy (nouveau TokenAwarePolicy (DCAwareRoundRobinPolicy.builder (). build ())) <br><br>  Par cons√©quent, si auparavant le test a montr√© 159 644 op√©rations (5 flux, 4 tables, 100 lots), maintenant: <br>  100 fils, 4 tableaux, lot = 1 (individuellement): 301969 ops <br>  100 fils, 4 tables, lot = 10: 447 608 ops <br>  100 fils, 4 tableaux, lot = 100: 625 655 ops <br><br>  √âtant donn√© que les r√©sultats sont meilleurs avec les lots, j'ai effectu√© des tests similaires * avec HB: <br><img src="https://habrastorage.org/webt/ct/bk/-y/ctbk-yrecbwegasrbpauq6f1vv8.png"><br>  <i>* Puisque lorsque vous travaillez dans 400 threads, la fonction RandomStringUtils, qui √©tait utilis√©e pr√©c√©demment, charge le CPU √† 100%, elle est remplac√©e par un g√©n√©rateur plus rapide.</i> <br><br>  Ainsi, une augmentation du nombre de threads lors du chargement des donn√©es donne une petite augmentation des performances HB. <br><br>  Quant √† la lecture, voici les r√©sultats de plusieurs options.  √Ä la demande de <a href="https://habr.com/ru/users/0x62ash/" class="user_link">0x62ash</a> , la commande flush a √©t√© ex√©cut√©e avant la lecture, et plusieurs autres options sont √©galement donn√©es √† titre de comparaison: <br>  Memstore - lecture √† partir de la m√©moire, c.-√†-d.  avant de vider le disque. <br>  HFile + zip - lecture √† partir de fichiers compress√©s par l'algorithme GZ. <br>  HFile + upzip - lecture √† partir de fichiers sans compression. <br><br>  Une caract√©ristique int√©ressante est √† noter - les petits fichiers (voir le champ ¬´Donn√©es¬ª, o√π 10 octets sont √©crits) sont trait√©s plus lentement, surtout s'ils sont compress√©s.  √âvidemment, cela n'est possible que jusqu'√† une certaine taille, √©videmment un fichier de 5 Go ne sera pas trait√© plus rapidement que 10 Mo, mais cela indique clairement que dans tous ces tests, il n'y a toujours pas de champ labour√© pour rechercher diverses configurations. <br><br>  Par int√©r√™t, j'ai corrig√© le code YCSB pour travailler avec des lots HB de 100 pi√®ces pour mesurer la latence et plus encore.  Ci-dessous est le r√©sultat du travail de 4 copies qui ont √©crit sur leurs tables, chacune avec 100 threads.  Il s'est av√©r√© ce qui suit: <br><div class="spoiler">  <b class="spoiler_title">Une op√©ration = 100 enregistrements</b> <div class="spoiler_text">  [GLOBAL], RunTime (ms), 1165415 <br>  [GLOBAL], D√©bit (ops / sec), 858.06343662987 <br>  [TOTAL_GCS_PS_Scavenge], nombre, 798 <br>  [TOTAL_GC_TIME_PS_Scavenge], temps (ms), 7346 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Heure (%), 0,6303334005483026 <br>  [TOTAL_GCS_PS_MarkSweep], nombre, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], temps (ms), 74 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Heure (%), 0,006349669431061038 <br>  [TOTAL_GCs], nombre, 799 <br>  [TOTAL_GC_TIME], temps (ms), 7420 <br>  [TOTAL_GC_TIME_%], heure (%), 0,6366830699793635 <br>  [INS√âRER], Op√©rations, 1 000 000 <br>  [INS√âRER], AverageLatency (us), 115893.891644 <br>  [INS√âRER], MinLatency (us), 14528 <br>  [INS√âRER], MaxLatency (us), 1470463 <br>  [INS√âRER], 95thPercentileLatency (us), 248319 <br>  [INS√âRER], 99thPercentileLatency (us), 445951 <br>  [INS√âRER], Retour = OK, 1 000 000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Fermeture du zookeeper sessionid = 0x36f98ad0a4ad8cc <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cc ferm√©e <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: arr√™t EventThread <br>  [GLOBAL], RunTime (ms), 1165806 <br>  [GLOBAL], D√©bit (ops / sec), 857.7756504941646 <br>  [TOTAL_GCS_PS_Scavenge], nombre, 776 <br>  [TOTAL_GC_TIME_PS_Scavenge], temps (ms), 7517 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Heure (%), 0,6447899564764635 <br>  [TOTAL_GCS_PS_MarkSweep], nombre, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], temps (ms), 63 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Heure (%), 0,005403986598113236 <br>  [TOTAL_GCs], comte, 777 <br>  [TOTAL_GC_TIME], temps (ms), 7580 <br>  [TOTAL_GC_TIME_%], heure (%), 0,6501939430745767 <br>  [INS√âRER], Op√©rations, 1 000 000 <br>  [INS√âRER], AverageLatency (us), 116042.207936 <br>  [INS√âRER], MinLatency (us), 14056 <br>  [INS√âRER], MaxLatency (us), 1462271 <br>  [INS√âRER], 95thPercentileLatency (us), 250239 <br>  [INS√âRER], 99thPercentileLatency (us), 446719 <br>  [INS√âRER], Retour = OK, 1 000 000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Fermeture du zookeeper sessionid = 0x26f98ad07b6d67e <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Session: 0x26f98ad07b6d67e ferm√©e <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: arr√™t EventThread <br>  [GLOBAL], RunTime (ms), 1165999 <br>  [GLOBAL], D√©bit (ops / sec), 857.63366863951 <br>  [TOTAL_GCS_PS_Scavenge], nombre, 818 <br>  [TOTAL_GC_TIME_PS_Scavenge], temps (ms), 7557 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6481137633908777 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 79 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.006775305982252128 <br> [TOTAL_GCs], Count, 819 <br> [TOTAL_GC_TIME], Time(ms), 7636 <br> [TOTAL_GC_TIME_%], Time(%), 0.6548890693731299 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116172.212864 <br> [INSERT], MinLatency(us), 7952 <br> [INSERT], MaxLatency(us), 1458175 <br> [INSERT], 95thPercentileLatency(us), 250879 <br> [INSERT], 99thPercentileLatency(us), 446463 <br> [INSERT], Return=OK, 1000000 <br><br> 20/01/19 13:19:17 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x36f98ad0a4ad8cd <br> 20/01/19 13:19:17 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cd closed <br> 20/01/19 13:19:17 INFO zookeeper.ClientCnxn: EventThread shut down <br> [OVERALL], RunTime(ms), 1166860 <br> [OVERALL], Throughput(ops/sec), 857.000839860823 <br> [TOTAL_GCS_PS_Scavenge], Count, 707 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7239 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6203829079752499 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 67 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.0057419056270675145 <br> [TOTAL_GCs], Count, 708 <br> [TOTAL_GC_TIME], Time(ms), 7306 <br> [TOTAL_GC_TIME_%], Time(%), 0.6261248136023173 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116230.849308 <br> [INSERT], MinLatency(us), 7352 <br> [INSERT], MaxLatency(us), 1443839 <br> [INSERT], 95thPercentileLatency(us), 250623 <br> [INSERT], 99thPercentileLatency(us), 447487 <br> [INSERT], Return=OK, 1000000 </div></div><br><br> ,    CS AverageLatency(us)   3114,   HB AverageLatency(us) = 1162 (,  1  = 100     ). <br><br>      ‚Äî        HBase.   ,  SSD       .   ,       ,    ,     4 ,  400    ,      .   :  ‚Äî  .  .   ScyllaDB     ,    ‚Ä¶ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr484096/">https://habr.com/ru/post/fr484096/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr484084/index.html">1C-Bitrix et une tentative de l'introduire</a></li>
<li><a href="../fr484088/index.html">D√©fil√© de mots de passe (analyse d'environ 5 milliards de mots de passe de fuites)</a></li>
<li><a href="../fr484090/index.html">Nouvelle infrastructure informatique pour le centre de donn√©es de la poste russe</a></li>
<li><a href="../fr484092/index.html">Princes et nobles un peu habill√©s</a></li>
<li><a href="../fr484094/index.html">Cr√©ez un jeu de tir zombie √† la troisi√®me personne avec DOTS</a></li>
<li><a href="../fr484100/index.html">Utilisation de l'interface dans le SDK Google Maps pour Android</a></li>
<li><a href="../fr484102/index.html">PHP vs Python vs Ruby on Rails: comparaison d√©taill√©e</a></li>
<li><a href="../fr484106/index.html">MVCC dans PostgreSQL-6. Le vide</a></li>
<li><a href="../fr484108/index.html">Encapsuleur Etherblade.net et substitution d'importation pour les composants r√©seau (deuxi√®me partie)</a></li>
<li><a href="../fr484112/index.html">Est-il possible de pirater un avion</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>