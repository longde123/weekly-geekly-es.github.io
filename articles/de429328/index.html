<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ú¥Ô∏è üñêüèº üç∫ Softwaremodul zur Digitalisierung besch√§digter Dokumente üë©üèª‚Äçüîß ‚ôêÔ∏è ü§∂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei der optischen Zeichenerkennung (OCR) werden gedruckte Texte in digitalisiertem Format abgerufen. Wenn Sie einen klassischen Roman auf einem digita...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Softwaremodul zur Digitalisierung besch√§digter Dokumente</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/429328/"><p> Bei der optischen Zeichenerkennung (OCR) werden gedruckte Texte in digitalisiertem Format abgerufen.  Wenn Sie einen klassischen Roman auf einem digitalen Ger√§t gelesen oder einen Arzt gebeten haben, alte Krankenakten √ºber das Computersystem des Krankenhauses abzurufen, haben Sie wahrscheinlich OCR verwendet. </p><br><p>  OCR macht zuvor statische Inhalte bearbeitbar, durchsuchbar und gemeinsam nutzbar.  Viele Dokumente, die digitalisiert werden m√ºssen, enthalten Kaffeeflecken, Seiten mit gekr√§uselten Ecken und viele Falten, die einige gedruckte Dokumente nicht digitalisieren. </p><br><p>  Jeder wei√ü seit langem, dass Millionen alter B√ºcher eingelagert sind.  Die Verwendung dieser B√ºcher ist aufgrund ihres Verfalls und ihrer Altersschw√§che verboten, weshalb die Digitalisierung dieser B√ºcher so wichtig ist. </p><br><p>  Das Papier befasst sich mit der Aufgabe, Text von Rauschen zu befreien, Text in einem Bild zu erkennen und in ein Textformat zu konvertieren. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3b0/14c/324/3b014c324c6ac91f8d8a761ca4e0bbc1.jpg" alt="Bild"></p><br><p>  F√ºr das Training wurden 144 Bilder verwendet.  Die Gr√∂√üe kann unterschiedlich sein, sollte aber vorzugsweise im Rahmen des Zumutbaren liegen.  Bilder m√ºssen im PNG-Format vorliegen.  Nach dem Lesen des Bildes wird die Bin√§risierung verwendet - der Prozess des Konvertierens eines Farbbilds in Schwarzwei√ü, dh jedes Pixel wird auf einen Bereich von 0 bis 255 normalisiert, wobei 0 schwarz und 255 wei√ü ist. </p><br><p>  Um ein Faltungsnetzwerk zu trainieren, ben√∂tigen Sie mehr Bilder als vorhanden.  Es wurde beschlossen, die Bilder in Teile zu teilen.  Da das Trainingsbeispiel aus Bildern unterschiedlicher Gr√∂√üe besteht, wurde jedes Bild auf 448 x 448 Pixel komprimiert.  Das Ergebnis waren 144 Bilder mit einer Aufl√∂sung von 448 x 448 Pixel.  Dann wurden alle in nicht √ºberlappende Fenster mit einer Gr√∂√üe von 112 x 112 Pixel geschnitten. </p><a name="habracut"></a><br><p><img src="https://habrastorage.org/getpro/habr/post_images/07a/c1d/271/07ac1d2716da451215a597dbb8241a9d.jpg" alt="Bild"></p><br><p>  Somit wurden von 144 Anfangsbildern ungef√§hr 2304 Bilder im Trainingssatz erhalten.  Das war aber nicht genug.  F√ºr ein gutes Faltungsnetzwerktraining ist mehr Training erforderlich.  Infolgedessen war es am besten, die Bilder um 90 Grad, dann um 180 und 270 Grad zu drehen.  Infolgedessen wird dem Netzwerkeingang ein Array mit der Gr√∂√üe [16,112,112,1] zugef√ºhrt.  Dabei ist 16 die Anzahl der Bilder, 112 die Breite und H√∂he jedes Bildes, 1 die Farbkan√§le.  Es stellte sich heraus, 9216 Beispiele f√ºr das Training.  Dies reicht aus, um ein Faltungsnetzwerk zu trainieren. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/f7d/320/415/f7d320415a5cba904cf715646dddbe2a.png" alt="Bild"></p><br><p>  Jedes Bild hat eine Gr√∂√üe von 112 x 112 Pixel.  Wenn die Gr√∂√üe zu gro√ü ist, erh√∂ht sich der Rechenaufwand bzw. die Einschr√§nkungen der Antwortgeschwindigkeit werden verletzt, die Bestimmung der Gr√∂√üe in diesem Problem wird durch das Auswahlverfahren gel√∂st.  Wenn Sie eine zu kleine Gr√∂√üe ausw√§hlen, kann das Netzwerk keine Schl√ºsselzeichen identifizieren.  Jedes Bild hat ein Schwarzwei√üformat, daher ist es in einen Kanal unterteilt.  Farbbilder sind in 3 Kan√§le unterteilt: Rot, Blau, Gr√ºn.  Da wir Schwarzwei√übilder haben, betr√§gt die Gr√∂√üe jedes Bildes 112 x 122 x 1 Pixel. </p><br><p>  Zun√§chst ist es notwendig, ein Faltungsnetzwerk auf vorbereiteten, verarbeiteten Bildern zu trainieren.  F√ºr diese Aufgabe wurde die U-Net-Architektur ausgew√§hlt. </p><br><p>  Es wurde eine reduzierte Version der Architektur ausgew√§hlt, die nur aus zwei Bl√∂cken besteht (die urspr√ºngliche Version von vier).  Eine wichtige √úberlegung war die Tatsache, dass eine gro√üe Klasse bekannter Bin√§risierungsalgorithmen in einer solchen Architektur oder einer √§hnlichen Architektur explizit ausgedr√ºckt wird (als Beispiel k√∂nnen wir den Niblack-Algorithmus modifizieren, indem wir die Standardabweichung durch die mittlere Abweichung ersetzen. In diesem Fall wird das Netzwerk besonders einfach aufgebaut). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/43a/928/f6d/43a928f6d29028779c1fa25df30f794c.jpg" alt="Bild"></p><br><p>  Der Vorteil dieser Architektur besteht darin, dass Sie zum Trainieren des Netzwerks eine ausreichende Menge an Trainingsdaten aus einer kleinen Anzahl von Quellbildern erstellen k√∂nnen.  Dar√ºber hinaus weist das Netzwerk aufgrund seiner Faltungsarchitektur eine relativ geringe Anzahl von Gewichten auf.  Aber es gibt einige Nuancen.  Insbesondere das verwendete k√ºnstliche neuronale Netzwerk l√∂st streng genommen das Bin√§risierungsproblem nicht: F√ºr jedes Pixel des Originalbilds wird eine Zahl von 0 bis 1 zugeordnet, die den Grad kennzeichnet, in dem dieses Pixel zu einer der Klassen geh√∂rt (sinnvolle F√ºllung oder Hintergrund) und der erforderlich ist konvertiere immer noch zur endg√ºltigen bin√§ren Antwort.  [1] </p><br><p>  U-Net besteht aus einem Komprimierungs- und Dekomprimierungspfad und "Weiterleiten" zwischen ihnen.  Der Komprimierungspfad in dieser Architektur besteht aus zwei Bl√∂cken (in der urspr√ºnglichen Version von vier).  Jeder Block hat zwei Faltungen mit einem 3x3-Filter (unter Verwendung der Tanh-Aktivierungsfunktion nach der Faltung) und ein Pooling mit einer 2x2-Filtergr√∂√üe in Schritten von 2. Die Anzahl der Kan√§le bei jedem Schritt verdoppelt sich. </p><br><p>  Der Quetschweg besteht ebenfalls aus zwei Bl√∂cken.  Jeder von ihnen besteht aus einem "Sweep" mit einer Filtergr√∂√üe von 2x2, der Halbierung der Anzahl der Kan√§le, der Verkettung mit der entsprechenden abgeschnittenen Feature-Map aus dem Komprimierungspfad ("Weiterleitung") und zwei Faltungen mit einem 3x3-Filter (unter Verwendung der Tanh-Aktivierungsfunktion nach der Faltung).  Als n√§chstes wird auf der letzten Schicht eine 1x1-Faltung (unter Verwendung der Sigmoid-Aktivierungsfunktion) durchgef√ºhrt, um ein flaches Ausgabebild zu erhalten.  Beachten Sie, dass das Trimmen der Feature-Map w√§hrend der Verkettung aufgrund des Verlusts von Grenzpixeln f√ºr jede Faltung wesentlich ist.  Adam wurde als Methode zur stochastischen Optimierung gew√§hlt. </p><br><p>  Im Allgemeinen ist Architektur eine Folge von Faltungs- + Pooling-Schichten, die die r√§umliche Aufl√∂sung des Bildes verringern und dann erh√∂hen, indem sie im Voraus mit den Bilddaten kombiniert werden und die anderen Faltungsschichten durchlaufen.  Somit wirkt das Netzwerk als eine Art Filter.  [2] </p><br><p>  Die Testprobe bestand aus √§hnlichen Bildern, die Unterschiede bestanden nur in der Rauschstruktur und im Text.  Auf diesem Bild wurden Netzwerktests durchgef√ºhrt. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/595/79b/89b/59579b89b072f197c925623e11e712c5.jpg" alt="Bild"></p><br><p>  Am Ausgang des Faltungsnetzwerks wird ein Array von Zahlen mit einer Gr√∂√üe von [16,112,112,1] erhalten.  Jede Nummer ist ein separates Pixel, das vom Netzwerk verarbeitet wird.  Bilder haben ein Format von 112x112 Pixel, wie zuvor wurde es in St√ºcke geschnitten.  Sie muss das urspr√ºngliche Aussehen verraten.  Wir kombinieren die erhaltenen Bilder in einem Teil, so dass das Bild ein Format von 448x448 hat.  Als n√§chstes multiplizieren wir jede Zahl im Array mit 255, um einen Bereich von 0 bis 255 zu erhalten, wobei 0 schwarz und 255 wei√ü ist.  Wir bringen das Bild auf seine urspr√ºngliche Gr√∂√üe zur√ºck, da es zuvor komprimiert wurde.  Das Ergebnis ist das Bild unten in der Abbildung. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/a05/a85/636/a05a856361fdc5cefe5c84e81df33992.jpg" alt="Bild"></p><br><p>  In diesem Beispiel ist zu sehen, dass das Faltungsnetzwerk den gr√∂√üten Teil des Rauschens bew√§ltigte und sich als effizient erwies.  Es ist jedoch deutlich zu erkennen, dass das Bild wolkiger wurde und die fehlenden Ger√§usche sichtbar sind.  Dies kann in Zukunft die Genauigkeit der Texterkennung beeintr√§chtigen. </p><br><p>  Aufgrund dieser Tatsache wurde beschlossen, ein anderes neuronales Netzwerk zu verwenden - ein mehrschichtiges Perzeptron.  Im erwarteten Ergebnis sollte das Netzwerk den Text im Bild klarer machen und das Rauschen entfernen, das im Faltungs-Neuronalen Netzwerk fehlt. </p><br><p>  Ein Bild, das bereits vom Faltungsnetzwerk verarbeitet wurde, wird an den Eingang des mehrschichtigen Perzeptrons gesendet.  In diesem Fall unterscheidet sich das Trainingsmuster f√ºr dieses Netzwerk von dem Beispiel f√ºr das Faltungsnetzwerk, da die Netzwerke das Bild unterschiedlich verarbeiten.  Das Faltungsnetzwerk wird als Hauptnetzwerk betrachtet und entfernt den gr√∂√üten Teil des Bildrauschens, w√§hrend das mehrschichtige Perzeptron verarbeitet, was das Faltungsnetzwerk nicht getan hat. <br>  Hier sind einige Beispiele aus dem Trainingsset f√ºr ein mehrschichtiges Perzeptron. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/505/62f/877/50562f87767ceb31fae90cb98f86dacf.jpg" alt="Bild"></p><br><p>  Bilddaten wurden erhalten, indem die Trainingsprobe f√ºr das Faltungsnetzwerk mit einem mehrschichtigen Perzeptron verarbeitet wurde.  Zur gleichen Zeit wurde das Perzeptron an derselben Probe trainiert, jedoch an einer kleinen Anzahl von Beispielen und einer kleinen Anzahl von Epochen. </p><br><p>  F√ºr das Perzeptron-Training wurden 36 Bilder verarbeitet.  Das Netzwerk wird Pixel f√ºr Pixel trainiert, dh ein Pixel aus dem Bild wird an den Netzwerkeingang gesendet.  Am Ausgang des Netzwerks erhalten wir auch ein Ausgangsneuron - ein Pixel, dh die Netzwerkantwort.  Um die Genauigkeit der Verarbeitung zu erh√∂hen, wurden 29 Eingangsneuronen hergestellt.  Und dem Bild, das nach der Verarbeitung durch das Faltungsnetzwerk erhalten wurde, werden 28 Filter √ºberlagert.  Das Ergebnis sind 29 Bilder mit verschiedenen Filtern.  Wir senden ein Pixel von jedem 29 Bild an den Netzwerkeingang und nur ein Pixel wird am Netzwerkausgang empfangen, dh an der Netzwerkantwort. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d00/628/790/d00628790fa450774472e3293bba3965.jpg" alt="Bild"></p><br><p>  Dies wurde f√ºr ein besseres Training und Networking durchgef√ºhrt.  Danach begann das Netzwerk, die Genauigkeit und den Kontrast des Bildes zu erh√∂hen.  Au√üerdem werden kleinere Fehler behoben, durch die das Faltungsnetzwerk nicht gel√∂scht werden konnte. </p><br><p>  Infolgedessen hat das neuronale Netzwerk 29 Eingangsneuronen, ein Pixel von jedem Bild.  Nach den Experimenten wurde festgestellt, dass nur eine verborgene Schicht ben√∂tigt wurde, in der 500 Neuronen.  Es gibt nur einen Ausweg aus dem Netzwerk.  Da das Training Pixel f√ºr Pixel stattfand, wurde n * m mal auf das Netzwerk zugegriffen, wobei n die Bildbreite bzw. m die H√∂he ist. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/a1a/1b4/2d9/a1a1b42d91fd9ec4ea767e9a72850f11.jpg" alt="Bild"></p><br><p>  Nach der sequentiellen Verarbeitung des Bildes durch zwei neuronale Netze bleibt nur noch die Erkennung des Textes.  Hierzu wurde eine vorgefertigte L√∂sung gew√§hlt, n√§mlich die Python-Bibliothek Pytesseract.  Pytesseract bietet keine echten Python-Bindungen.  Es ist vielmehr ein einfacher Wrapper f√ºr die Tesseract-Bin√§rdatei.  In diesem Fall wird tesseract separat auf dem Computer installiert.  Pytesseract speichert das Bild in einer tempor√§ren Datei auf der Festplatte, ruft dann die Tesseract-Bin√§rdatei auf und schreibt das Ergebnis in eine Datei. </p><br><p>  Dieser Wrapper wurde von Google entwickelt und ist kostenlos und kostenlos zu verwenden.  Es kann sowohl f√ºr eigene als auch f√ºr kommerzielle Zwecke verwendet werden.  Die Bibliothek funktioniert ohne Internetverbindung, unterst√ºtzt viele Sprachen zur Erkennung und beeindruckt durch ihre Geschwindigkeit.  Seine Anwendung kann in verschiedenen popul√§ren Anwendungen gefunden werden. </p><br><p>  Der letzte verbleibende Punkt besteht darin, den erkannten Text in einem f√ºr die Verarbeitung geeigneten Format in eine Datei zu schreiben.  Wir verwenden daf√ºr ein normales Notizbuch, das nach Beendigung des Programms ge√∂ffnet wird.  Au√üerdem wird der Text auf der Testoberfl√§che angezeigt.  Ein gutes Beispiel f√ºr eine Schnittstelle. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3b7/bb0/a49/3b7bb0a49f54d81ecd638b0e074ab24e.jpg" alt="Bild"></p><br><p>  <strong>Referenzliste:</strong> </p><br><ol><li>  Die Siegesgeschichte beim internationalen Dokumentenerkennungswettbewerb des SmartEngines-Teams [Elektronische Ressource].  Zugriffsmodus: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://habr.com/company/smartengines/blog/344550/</a> </li><li>  Bildsegmentierung √ºber ein neuronales Netzwerk: U-Net [Elektronische Ressource].  Zugriffsmodus: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://robocraft.ru/blog/machinelearning/3671.html</a> </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">&gt; <strong>Github-Repository</strong></a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de429328/">https://habr.com/ru/post/de429328/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de429318/index.html">IPhone SMT Solver</a></li>
<li><a href="../de429320/index.html">Live-√úbertragung des Sberbank Data Science Day am 10. November</a></li>
<li><a href="../de429322/index.html">nanoCAD Mechanics 9.0: Die Grundlagen des modernen Designs</a></li>
<li><a href="../de429324/index.html">Unreal Engine 4.21 Release</a></li>
<li><a href="../de429326/index.html">App Store ruft nicht an. Oder wie ich meine Bewerbung gemacht habe, aber sie wird die Benutzer nicht erreichen</a></li>
<li><a href="../de429330/index.html">Mythen und Legenden von Agile - von den Pharaonen bis heute</a></li>
<li><a href="../de429332/index.html">Selbstgemachtes Laserlichtschwert - wie es war, Teil 1</a></li>
<li><a href="../de429336/index.html">Kommunikation zwischen Treiber und Ger√§t √ºber die _HID ACPI-Methode am Beispiel des GPIO des Lynxpoint-Controllers</a></li>
<li><a href="../de429338/index.html">Android-Speicher: intern, extern, entfernbar. Teil 1/3</a></li>
<li><a href="../de429340/index.html">√úberlegen Sie zweimal, bevor Sie Helm verwenden.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>