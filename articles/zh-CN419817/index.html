<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌲 👡 🐛 在Kubernetes中启动RabbitMQ集群 🎇 😉 👩🏻‍🤝‍👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在应用程序的微服务组织的情况下，大量工作取决于微服务的集成连接机制。 而且，这种集成应该是容错的，并且具有高度的可用性。 

 在我们的解决方案中，我们使用与Kafka，gRPC和RabbitMQ的集成。 

 在本文中，我们将分享我们的RabbitMQ集群经验，该集群的节点托管在Kubernete...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>在Kubernetes中启动RabbitMQ集群</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/"> 在应用程序的微服务组织的情况下，大量工作取决于微服务的集成连接机制。 而且，这种集成应该是容错的，并且具有高度的可用性。 <br><br> 在我们的解决方案中，我们使用与Kafka，gRPC和RabbitMQ的集成。 <br><br> 在本文中，我们将分享我们的RabbitMQ集群经验，该集群的节点托管在Kubernetes上。 <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="图片"><br><br> 在RabbitMQ版本3.7之前，将其群集在K8S中并不是一件容易的事，因为它有很多技巧，而且解决方案也不是很好。 在版本3.6中，使用了RabbitMQ社区的自动集群插件。 并在3.7版中出现了Kubernetes Peer Discovery Backend。 它是由RabbitMQ的基本交付中的插件内置的，不需要单独的组装和安装。 <br><br> 在评论正在发生的事情时，我们将描述最终配置的整体。 <br><a name="habracut"></a><br><h2> 理论上 </h2><br> 该插件<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在github上</a>有一个<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">存储库</a> ，其中有<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">一个基本用法示例</a> 。 <br> 此示例不适用于生产，其说明中已明确指出，此外，其中的某些设置与产品中使用的逻辑相反。 另外，在该示例中，根本没有提到存储的持久性，因此在任何紧急情况下，我们的集群都会变得毫无生气。 <br><br><h2> 在实践中 </h2><br> 现在，我们将告诉您您自己面临的挑战以及如何安装和配置RabbitMQ。 <br><br> 让我们描述在K8中作为服务的RabbitMQ各个部分的配置。 我们将立即说明，我们在K8s中将RabbitMQ安装为StatefulSet。 在K8s群集的每个节点上，RabbitMQ的一个实例将始终起作用（传统群集配置中的一个节点）。 我们还将在K8s中安装RabbitMQ控制面板，并允许在集群外部访问该面板。 <br><br><h3> 权利和角色： </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  RabbitMQ的访问权限完全来自示例，而无需进行任何更改。 我们为群集创建一个ServiceAccount，并将其授予Endpoints K8s的读取权限。 <br><br><h3> 永久存储： </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br> 在这里，我们将最简单的情况作为持久性存储-hostPath（每个K8s节点上的常规文件夹），但是您可以使用K8s支持的多种持久卷中的任何一种。 <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br> 在上一步中创建的卷上创建卷声明。 然后，此Claim将在StatefulSet中用作持久数据存储。 <br><br><h3> 服务项目： </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br> 我们创建了一个内部的headless服务，Peer Discovery插件将通过该服务运行。 <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br> 为了使K8s中的应用程序能够与我们的集群一起使用，我们创建了平衡器服务。 <br><br> 由于我们需要访问K8之外的RabbitMQ集群，因此我们将遍历NodePort。 当访问端口31673和30673上的K8s集群的任何节点时，RabbitMQ将可用。在实际工作中，对此没有太大需求。  RabbitMQ管理面板的可用性问题。 <br><br> 在K8s中使用NodePort类型创建服务时，也会隐式创建ClusterIP类型的服务来为其提供服务。 因此，需要与RabbitMQ一起使用的K8中的应用程序将能够通过<i>amqp</i>访问群集<i>：// Rabbitmq：5672</i> <br><br><h3> 配置： </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br> 我们创建RabbitMQ配置文件。 主要魔术。 <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br> 将必要的插件添加到允许下载的插件中。 现在，我们可以在K8S中使用自动对等发现。 <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br> 我们公开了必要的插件作为对等发现的后端。 <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br> 指定可以访问kubernetes apiserver的地址和端口。 在这里，您可以直接指定ip地址，但是这样做会更漂亮。 <br><br> 在名称空间默认情况下，通常会使用名称kubernetes创建服务，该服务将导致k8-apiserver。 在不同的K8S安装选项中，名称空间，服务名称和端口可能不同。 如果特定安装中的某些内容不同，则需要相应地对其进行修复。 <br><br> 例如，我们面临这样一个事实，在某些群集中，该服务位于端口443上，而在某些群集中，该服务位于6443上。可以理解，RabbitMQ启动日志中出现了问题，此处的到此地址的连接时间清楚地突出显示了。 <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br> 默认情况下，该示例通过IP地址指定RabbitMQ集群节点的地址类型。 但是，当您重新启动Pod时，它每次都会获得一个新IP。 惊喜！ 集群快要死了。 <br><br> 将地址更改为主机名。  StatefulSet保证了主机名在整个StatefulSet生命周期内的不变性，这完全适合我们。 <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br> 由于当我们丢失一个节点时，我们假设它早晚会恢复，因此我们通过一群无法访问的节点禁用自删除。 在这种情况下，节点一旦返回联机状态，它将立即进入群集而不会丢失其先前状态。 <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br> 此参数确定在仲裁丢失的情况下群集的操作。 在这里，您只需要阅读<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">有关此主题</a>的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文档，</a>并自己了解更接近特定用例的内容。 <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br> 确定新队列的向导选择。 使用此设置，向导将选择队列数量最少的节点，因此队列将在群集节点之间平均分配。 <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br> 我们命名无头K8s服务（由我们之前创建），RabbitMQ节点将通过该服务彼此通信。 <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br> 在群集中寻址的重要事项是主机名。  K8炉膛的FQDN由短名称（rabbitmq-0，rabbitmq-1）+后缀（域部分）组成。 在这里，我们指出这个后缀。 在K8S中，它看起来像<b>。&lt;服务名称&gt;。&lt;名称空间名称&gt; .svc.cluster.local</b> <br><br>  kube-dns可以将格式RabbitMq-0.rabbitmq-internal.our-namespace.svc.cluster.local解析为特定容器的IP地址，而无需任何其他配置，这使通过主机名进行群集的所有魔术成为可能。 <br><br><h3>  StatefulSet RabbitMQ配置： </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br> 实际上，StatefulSet本身。 我们注意到有趣的观点。 <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br> 我们在StatefulSet中写下Pod通过其进行通信的无头服务的名称。 <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br> 设置集群中的副本数。 在我们国家，它等于K8个工作节点的数量。 <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br> 当K8s节点之一掉落时，有状态集试图保留集合中实例的数量，因此，它在同一K8s节点上创建了多个炉床。 这种行为是完全不希望的，并且原则上是毫无意义的。 因此，我们为来自有状态集的炉床集制定了反亲和性规则。 我们将规则设置为严格（必需），以便在计划吊舱时kube-scheduler不会破坏该规则。 <br><br> 本质上很简单：调度程序禁止在<i>应用程序</i>内（名称空间内）放置多个Pod <i>：</i>每个节点上有<i>Rabbitmq标记</i> 。 我们通过<i>kubernetes.io/hostname</i>标签的值来区分<i>节点</i> 。 现在，如果由于某些原因，工作中的K8S节点的数量少于StatefulSet中所需的副本数，则只有在空闲节点再次出现之前，才会创建新副本。 <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br> 我们注册ServiceAccount，我们的Pod在该帐户下工作。 <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  RabbitMQ的图像是完全标准的，并且是从docker hub获得的；它不需要任何重建和文件修订。 <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br> 来自RabbitMQ的持久数据存储在/ var / lib / rabbitmq / mnesia中。 在这里，我们将持久卷声明安装在此文件夹中，以便在重新启动炉床/节点或什至整个StatefulSet时，数据（包括有关已组装群集的所有服务和用户数据）都是安全无害的。 有一些示例使整个/ var / lib / rabbitmq /文件夹具有持久性。 我们得出的结论是，这不是最好的主意，因为与此同时，Rabbit配置所设置的所有信息都将被记住。 也就是说，为了更改配置文件中的某些内容，您需要清除持久性存储，这在操作中非常不方便。 <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br> 有了这组环境变量，我们首先告诉RabbitMQ使用FQDN名称作为集群成员的标识符，其次，我们设置该名称的格式。 解析配置时，前面已经描述了格式。 <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br> 集群成员之间进行通信的无头服务的名称。 <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  Erlang Cookie的内容在集群的所有节点上都应该相同，您需要注册自己的值。 具有其他Cookie的节点无法进入集群。 <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br> 从先前创建的“持久卷声明”中定义映射的卷。 <br><br> 这就是我们在K8s中完成设置的地方。 结果是RabbitMQ集群，该集群在节点之间平均分配队列，并且可以抵抗运行时环境中的问题。 <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="图片"><br><br> 如果群集节点之一不可用，则包含在其中的队列将停止访问，其他所有节点将继续工作。 节点一旦恢复运行，它将返回集群，并且原来作为主节点的队列将再次运行，并保留其中包含的所有数据（当然，如果持久性存储未损坏）。 所有这些过程都是全自动的，不需要干预。 <br><br><h2> 奖励：自定义HA </h2><br> 其中一个项目是细微差别。 这些要求完善了集群中所有数据的镜像。 这是必要的，以便在至少一个群集节点可运行的情况下，从应用程序的角度来看，一切都将继续工作。 此刻与K8s无关，我们仅将其描述为一个小型操作方法。 <br><br> 要启用完整的HA，您需要在Admin- <i>&gt; Policies</i>选项卡上的RabbitMQ仪表板中创建一个Policy。 名称是任意的，模式为空（所有队列），在“定义”中添加两个参数： <i>ha-mode：all</i> ， <i>ha-sync-mode：automatic</i> 。 <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="图片"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="图片"><br><br> 之后，集群中创建的所有队列将处于高可用性模式：如果主节点不可用，则新向导将自动选择其中一个从站。 并且进入队列的数据将被镜像到集群的所有节点。 实际上，这是必须接收的。 <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="图片"><br><br> 在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>阅读有关RabbitMQ中的HA的更多信息 <br><br><h2> 有用的文献： </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">存储库RabbitMQ对等发现Kubernetes插件</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">K8S中RabbitMQ部署的配置示例</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">描述集群形成的原理，对等发现机制及其插件</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">有关基于主机名的正确发现设置的史诗讨论</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RabbitMQ集群指南</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">裂脑聚类问题和解决方案的描述</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RabbitMQ的高可用性队列</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">配置策略</a> </li></ul><br> 祝你好运！ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN419817/">https://habr.com/ru/post/zh-CN419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN419805/index.html">超级微型编译器-现已推出俄语</a></li>
<li><a href="../zh-CN419807/index.html">青光眼-如何不致盲：让我们谈谈治疗方法...</a></li>
<li><a href="../zh-CN419811/index.html">柔性显示器的发展</a></li>
<li><a href="../zh-CN419813/index.html">Skillbox网络研讨会：星期五的选择</a></li>
<li><a href="../zh-CN419815/index.html">我们前台容错的秘诀</a></li>
<li><a href="../zh-CN419819/index.html">衰老的生物标志物。 面板脆弱。 第二部分</a></li>
<li><a href="../zh-CN419823/index.html">不寻常的二重唱-密码短语和助记符图像</a></li>
<li><a href="../zh-CN419825/index.html">在虚拟环境中测试几种类型的驱动器的性能</a></li>
<li><a href="../zh-CN419829/index.html">OpenSSH的默认密钥加密总比没有好</a></li>
<li><a href="../zh-CN419831/index.html">JS的工作方式：自定义元素</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>