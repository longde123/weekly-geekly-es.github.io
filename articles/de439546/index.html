<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôãüèΩ üîµ üèûÔ∏è Tr√§umen Androiden von elektrischem Punk? Wie ich einem neuronalen Netzwerk das Schreiben von Musik beigebracht habe üèúÔ∏è üë®üèø‚Äçüé§ üé£</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In maschinellen Lernkursen bei Artezio traf ich ein Lernmodell, das Musik machen konnte. Musik ist ein wesentlicher Bestandteil meines Lebens, ich hab...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tr√§umen Androiden von elektrischem Punk? Wie ich einem neuronalen Netzwerk das Schreiben von Musik beigebracht habe</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lanit/blog/439546/">  In maschinellen Lernkursen bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artezio traf</a> ich ein Lernmodell, das Musik machen konnte.  Musik ist ein wesentlicher Bestandteil meines Lebens, ich habe viele Jahre in Gruppen (Punkrock, Reggae, Hip Hop, Rock usw.) gespielt und bin ein fanatischer Zuh√∂rer. <br><br>  Leider haben sich viele Gruppen, von denen ich in meiner Jugend ein gro√üer Fan war, aus verschiedenen Gr√ºnden getrennt.  Oder sie haben sich nicht getrennt, aber was sie gerade aufnehmen ... im Allgemeinen w√§re es besser, wenn sie sich trennen w√ºrden. <br><br>  Ich war neugierig, ob es jetzt ein fertiges Modell gibt, das auf den Spuren einer meiner Lieblingsgruppen lernen und √§hnliche Kompositionen erstellen kann.  Da die Musiker selbst nicht mehr sehr erfolgreich sind, kann das neuronale Netzwerk sie vielleicht handhaben? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/72f/844/01f/72f84401f2af035271021780fc848fe8.png"></div>  <a href="">Quelle</a> <br><a name="habracut"></a><br>  Als ich die fertigen Modelle studierte, stie√ü ich schnell auf einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">solchen Artikel</a> mit einem √úberblick √ºber die sechs bekanntesten Optionen.  Es geht nat√ºrlich um digitale Musikformate.  Aus dem Artikel geht hervor, dass zwei Hauptans√§tze zur Musikgenerierung unterschieden werden k√∂nnen: basierend auf dem digitalisierten Audiostream (dem Ton, den wir aus den Lautsprechern h√∂ren - Roh-Audio, WAV-Dateien) und basierend auf der Arbeit mit MIDI (Musiknotation). <br><br>  Ich habe die Optionen mit rohem Audio gel√∂scht, und deshalb. <br><br><ul><li>  Die Ergebnisse sind nicht beeindruckend - die Verwendung solcher Modelle f√ºr polyphone Musik liefert ein sehr spezifisches Ergebnis.  Das ist ungew√∂hnlich, man kann interessante Bilder erstellen, aber es ist nicht f√ºr meine Zwecke geeignet: Es klingt seltsam, aber ich wollte etwas √Ñhnliches wie das Original h√∂ren. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5d9/229/364/5d9229364d98bee837d9bcf7cb3e1bac.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> <br><br>  Ein gutes Beispiel f√ºr Klaviermusik: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Und mit Orchestermusik oder Rock klingt es viel seltsamer: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Hier haben die Jungs versucht, Black Metal zu verarbeiten und das nicht nur in rohem Audio. <br><br><ul><li>  In den Kompositionen meiner Lieblingsbands klingen verschiedene Instrumente - Gesang, Schlagzeug, Bass, Gitarren, Synthesizer.  Jedes Instrument klingt zusammen mit dem Rest.  Ich suche ein Modell, das auf die gleiche Weise funktioniert, dh nicht nur mit einzelnen Instrumenten funktioniert, sondern auch deren gemeinsamen Klang ber√ºcksichtigt. <br><br>  Wenn ein Musiker einen Teil eines Instruments nach Geh√∂r lernen muss, versucht er, das ben√∂tigte Instrument vom gesamten Klangstrom zu isolieren.  Dann wiederholt er seinen Sound, bis er ein √§hnliches Ergebnis erzielt.  Die Aufgabe ist selbst f√ºr Menschen mit gutem Geh√∂r nicht die einfachste - Musik kann schwierig sein, Instrumente ‚Äûverschmelzen‚Äú. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dcd/5d1/8eb/dcd5d18eb46a6d383ffbc378d3fc7adb.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> <br><br>  Ich bin auf Software-Tools gesto√üen, die versucht haben, ein √§hnliches Problem zu l√∂sen.  Es gibt mehrere Projekte, die dies basierend auf maschinellem Lernen tun.  W√§hrend ich diesen Text schrieb, ver√∂ffentlichte Magenta beispielsweise ein neues Instrument, Wave2Midi2Wave, mit dem Piano-Noten ‚Äûabgenommen‚Äú und realistisch ‚Äûwiedergegeben‚Äú werden k√∂nnen.  Es gibt andere Tools, obwohl diese Aufgabe im Allgemeinen noch nicht gel√∂st wurde. <br><br>  Um einen Teil aus einer Arbeit zu lernen, ist es am einfachsten, vorgefertigte Notizen zu machen.  Dies ist der einfachste Weg.  Es ist logisch anzunehmen, dass es f√ºr neuronale Netze einfacher sein wird, mit der musikalischen Darstellung von Musik zu arbeiten, wobei jedes Instrument durch eine separate Spur dargestellt wird. <br><br><ul><li>  Bei Roh-Audio ist das Ergebnis eine Mischung aller Instrumente. Teile k√∂nnen nicht einzeln in den Sequenzer (Audio-Editor) geladen, korrigiert, der Sound ge√§ndert usw. werden.  Ich bin ziemlich froh, wenn das neuronale Netzwerk einen Treffer komponiert, aber in ein paar Noten einen Fehler macht - wenn ich mit Noten arbeite, kann ich sie leicht korrigieren, mit rohem Audio ist dies fast unm√∂glich. </li></ul><br>  Musiknotation hat auch ihre Nachteile.  Die Masse der Leistungsnuancen wird nicht ber√ºcksichtigt.  Wenn es um MIDI geht, ist nicht immer bekannt, wer diese MIDI-Dateien waren und wie nah sie am Original sind.  Vielleicht hat der Compiler einfach einen Fehler gemacht, weil es keine leichte Aufgabe ist, das Spiel zu "entfernen". <br><br>  Wenn Sie mit polyphonen Noten arbeiten, m√ºssen Sie sicherstellen, dass die Instrumente jederzeit gestimmt sind.  Dar√ºber hinaus ist es wichtig, dass die Abfolge dieser Momente aus menschlicher Sicht logisch ist. <br><br>  Es stellte sich heraus, dass es nicht so viele L√∂sungen gibt, die mit Noten arbeiten k√∂nnen, und sogar nicht mit einem Instrument, sondern mit mehreren gleichzeitig klingenden.  Ich habe das Magenta-Projekt von Google TensorFlow zun√§chst √ºbersehen, weil es als "nicht polyphon" beschrieben wurde.  Zu diesem Zeitpunkt war die MusicVAE-Bibliothek noch nicht ver√∂ffentlicht worden, daher habe ich mich f√ºr das BachBot-Projekt entschieden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b7/fef/993/2b7fef9935afb9f001bf954dd0fc097b.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> <br><br><h2>  Bachbot </h2><br>  Es stellte sich heraus, dass die L√∂sung f√ºr mein Problem bereits existiert.  H√∂ren Sie sich den von BachBot gestimmten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Happy Birthday</a> an, der wie ein Bach-Choral klingt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Chor ist eine spezifische Musik, sie besteht aus vier Stimmen: Sopran, Bratsche, Tenor und Bass.  Jedes der Instrumente kann jeweils eine Note erzeugen.  Hier muss man etwas tiefer in die Musik einsteigen.  Wir werden √ºber Musik in der Dimension von vier Vierteln sprechen. <br><br>  In einer Notenschrift hat eine Note zwei Indikatoren - Tonh√∂he (bis, re, mi ...) und Dauer (Ganzzahl, halbe, achte, sechzehnte, drei√üig Sekunden).  Dementsprechend dauert eine ganze Note einen ganzen Schlag, zwei halbe Noten einen ganzen Schlag, sechzehn Sechzehntel einen ganzen Schlag. <br><br>  Bei der Vorbereitung der Daten f√ºr das Training des neuronalen Netzwerks haben die Entwickler von BachBot Folgendes ber√ºcksichtigt: <br><br><ul><li>  Um das Modell nicht mit Akkorden aus verschiedenen Tonarten niederzuschlagen, die zusammen nicht harmonisch klingen w√ºrden, f√ºhrten alle Ch√∂re zu derselben Tonart. </li><li>  Das neuronale Netzwerk muss mit diskreten Werten versorgt werden, und Musik ist ein kontinuierlicher Prozess, was bedeutet, dass eine Diskretisierung erforderlich ist.  Ein Instrument kann eine lange, ganze Note spielen und das andere gleichzeitig einige Sechzehntel.  Um dieses Problem zu l√∂sen, wurden alle Notizen in Sechzehntel unterteilt.  Mit anderen Worten, wenn eine vierte Note in den Noten vorkommt, kommt sie viermal als dieselbe sechzehnte Eingabe an - das erste Mal mit dem Flag, das gedr√ºckt wurde, und das n√§chste Mal mit dem Flag, das sie fortsetzt. </li></ul><br>  Das Datenformat ist wie folgt: (Tonh√∂he, neue Note | Fortsetzung des Klangs der alten Note) <br><br>  (56, Richtig) # Sopran <br>  (52, False) # Alt <br>  (47, falsch) # Tenor <br>  (38, falsch) # Bass <br><br>  Nachdem die BachBot-Autoren alle Ch√∂re aus dem popul√§ren music21-Datensatz durch dieses Verfahren getrieben hatten, stellten sie fest, dass es in Ch√∂ren nicht viele Kombinationen von Kombinationen von vier Noten gibt (wenn Sie sie auf dieselbe Tonart bringen), obwohl es den Anschein hat, dass es m√∂glicherweise 128 x 128 x geben k√∂nnte 128 x 128 (128 Tonh√∂hen im Midi).  Die Gr√∂√üe eines bedingten W√∂rterbuchs ist nicht so gro√ü.  Dies ist eine merkw√ºrdige Bemerkung, auf die wir zur√ºckkommen werden, wenn wir √ºber MusicVAE sprechen.  Wir haben also die Bach-Ch√∂re in Form von Sequenzen solcher Viere aufgenommen. <br><br>  Es wird oft gesagt, dass Musik eine Sprache ist.  Daher ist es nicht verwunderlich, dass die Entwickler von BachBot die in NLP (Natural Language Processing) beliebte Technologie auf Musik anwendeten, indem sie das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LSTM-Netzwerk</a> auf den generierten Datensatz trainierten und ein Modell erhielten, das ein oder mehrere Instrumente erg√§nzen oder sogar Ch√∂re von Grund auf neu erstellen konnte.  Das hei√üt, Sie stellen Alt, Tenor und Bass ein, und BachBot f√ºgt die Sopran-Melodie f√ºr Sie hinzu, und zusammen klingt es wie Bach. <br><br>  Hier ist ein weiteres Beispiel: <br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  H√∂rt sich toll an! <br><br>  Sie k√∂nnen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dieses Video</a> genauer ansehen.  Dort gibt es eine interessante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Analyse</a> , die auf der Grundlage einer Umfrage auf der Website <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bachbot.com gesammelt wurde</a> <br><br>  Benutzer werden ermutigt, die urspr√ºnglichen Bach-Ch√∂re von der Musik zu unterscheiden, die vom neuronalen Netzwerk erzeugt wird.  Die Ergebnisse erw√§hnen, dass, wenn ein neuronales Netzwerk einen Bass-Part f√ºr alle anderen Einstellungen erstellt, nur die H√§lfte der Benutzer Ch√∂re, die von einem neuronalen Netzwerk erstellt wurden, von den urspr√ºnglichen unterscheiden k√∂nnen.  Witzig, aber vor allem die Musikexperten sind verwirrt.  Mit anderen Tools sieht es etwas besser aus.  F√ºr mich als Bassist klingt das beleidigend - der Geiger scheint vorerst gebraucht zu werden, aber es ist Zeit f√ºr die Bassisten, ihre F√§higkeiten im Trockenbau aufzufrischen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/701/bb9/cc6/701bb9cc6cdff8a20f0ede768324cd81.png"></div><br><h1>  Magenta </h1><br>  Als ich BachBot studierte, stellte ich fest, dass es im Magenta-Projekt (Google TensorFlow) enthalten war.  Ich habe mich n√§her damit befasst und festgestellt, dass im Rahmen von Magenta mehrere interessante Modelle entwickelt wurden, von denen eines nur der Arbeit mit polyphonen Kompositionen gewidmet ist.  Magenta hat ihre wunderbaren Werkzeuge entwickelt und sogar bereits das Plugin f√ºr den Ableton-Audio-Editor ver√∂ffentlicht, was besonders f√ºr die Anwendung f√ºr Musiker von Vorteil ist. <br><br>  Meine Favoriten: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beat Blender</a> (erzeugt Variationen eines bestimmten Drum-Parts) und <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">latente Schleifen</a> (erzeugt √úberg√§nge zwischen Melodien). <br><br>  Die Hauptidee des MusicVAE-Tools, f√ºr das ich mich entschieden habe, ist, dass die Entwickler versucht haben, ein Modell und einen Variations-Autoencoder - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VAE</a> - im LSTM-Netzwerk zu kombinieren. <br><br>  Wenn Sie sich erinnern, haben wir in einem Gespr√§ch √ºber Bach Bot festgestellt, dass das Akkordw√∂rterbuch nicht aus 128x128x128x128 Elementen besteht, sondern viel weniger.  Die Entwickler von MusicVAE bemerkten dies ebenfalls und entschieden sich f√ºr einen komprimierten latenten Raum. <br><br>  √úbrigens, was typisch f√ºr das Training von MusicVAE ist, m√ºssen Sie die Quellen nicht in einen Schl√ºssel √ºbersetzen.  Eine Transponierung ist vermutlich nicht erforderlich, da der Quellcode weiterhin vom Auto-Encoder konvertiert wird und die Tonalit√§tsinformationen verschwinden. <br><br>  VAE ist so konzipiert, dass der Decoder Daten aus dem Trainingsdatensatz effizient wiederherstellen kann, w√§hrend der latente Raum eine reibungslose Verteilung der Merkmale der Eingabedaten darstellt. <br><br>  Dies ist ein sehr wichtiger Punkt.  Dies erm√∂glicht es, √§hnliche Objekte zu erstellen und eine logisch sinnvolle Interpolation durchzuf√ºhren.  Im urspr√ºnglichen Raum haben wir 128x128x128x128 Varianten, um den Klang von vier Noten zu kombinieren, aber tats√§chlich werden nicht alle verwendet (sie klingen gut f√ºr das menschliche Ohr).  Ein Variations-Auto-Encoder verwandelt sie in eine viel kleinere Menge in einem verborgenen Raum, und Sie k√∂nnen mathematische Operationen in diesem Raum entwickeln, die aus Sicht des urspr√ºnglichen Raums eine bedeutungsvolle Bedeutung haben. Beispielsweise sind benachbarte Punkte √§hnliche musikalische Fragmente. <br><br>  Ein gutes Beispiel ist das Hinzuf√ºgen einer Brille zu einem Foto mithilfe eines automatischen Encoders in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> .  Weitere Informationen zur Funktionsweise von Muisc VAE finden Sie auf der offiziellen Magenta-Website in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> . Au√üerdem gibt es einen Link zu arXiv. <br><br>  Wenn das Instrument ausgew√§hlt ist, bleibt es, um es mit meinem urspr√ºnglichen Ziel zu verwenden - neue Musik basierend auf bereits aufgenommenen Tracks zu erstellen und zu bewerten, wie sehr dies wie der Sound der urspr√ºnglichen Gruppe klingen wird.  Magenta funktioniert auf meinem Windows-Laptop nicht und berechnet seit langem ein Modell ohne GPU.  Nachdem ich unter virtuellen Maschinen, einem Docker-Container usw. gelitten hatte, entschied ich mich f√ºr die Cloud. <br><br>  Google bietet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Colab-Notebooks an, in</a> denen Sie sich mit Magenta-Modellen verw√∂hnen lassen k√∂nnen.  In meinem Fall war es jedoch nicht m√∂glich, das Modell zu trainieren. Der Prozess st√ºrzte aufgrund verschiedener Einschr√§nkungen st√§ndig ab - der Menge des verf√ºgbaren Speichers, Timeout-Abschaltungen, dem Fehlen einer normalen Befehlszeile und Root-Rechten zum Installieren der erforderlichen Bibliotheken.  Hypothetisch gibt es sogar die M√∂glichkeit, die GPU zu verwenden, aber ich wiederhole, ich konnte das Modell nicht installieren und starten. <br><br>  Ich habe √ºber den Kauf eines Servers nachgedacht und, oh, viel Gl√ºck, festgestellt, dass Google Google Cloud-Cloud-Dienste mit einer GPU bereitstellt, und es gibt sogar eine kostenlose Testphase.  Es stellte sich heraus, dass sie in Russland offiziell nur juristischen Personen zur Verf√ºgung stehen, aber sie haben mich in einem kostenlosen Testmodus zugelassen. <br><br>  Also habe ich in GoogleCloud eine virtuelle Maschine mit einem GPU-Modul erstellt, im Internet mehrere Midi-Dateien einer meiner Lieblingsgruppen gefunden und in den Midi-Ordner in der Cloud hochgeladen. <br><br>  Installieren Sie Magenta: <br><br><pre><code class="plaintext hljs">pip install magenta-gpu</code> </pre> <br>  Es ist gro√üartig, dass all dies mit einem Team installiert werden kann, dachte ich, aber ... Fehler.  Es scheint, als m√ºssten Sie die Befehlszeile ber√ºhren, sorry. <br><br>  Wir sehen uns Fehler an: Die rtmidi-Bibliothek ist nicht auf dem Cloud-Computer installiert, ohne den Magenta nicht funktioniert. <br><br>  Und es st√ºrzt wiederum aufgrund des Fehlens des libasound2-dev-Pakets ab, und ich habe auch keine Root-Rechte. <br><br>  Nicht so be√§ngstigend: <br><br><pre> <code class="plaintext hljs">sudo su root apt-get install libasound2-dev</code> </pre> <br>  Hurra, jetzt l√§uft pip install rtmidi fehlerfrei, ebenso wie pip install magenta-gpu. <br><br>  Wir finden im Internet und laden die Quelldateien im Midi-Ordner herunter.  Sie klingen ungef√§hr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">so</a> . <br><br>  Wir konvertieren Midi in ein Datenformat, mit dem das Netzwerk bereits arbeiten kann: <br><br><pre> <code class="plaintext hljs">convert_dir_to_note_sequences \ --input_dir=midi\ --hparams=sampling_rate=1000.0\ --output_file=notesequences_R2Midi.tfrecord \ --log=DEBUG \ --recursive</code> </pre> <br>  und mit dem Training beginnen <br><br><pre> <code class="plaintext hljs">music_vae_train \ --config=hier-multiperf_vel_1bar_med \ --run_dir=/home/RNCDtrain/ \ --num_steps=1 \ --checkpoints_to_keep=2 \ --hparams=sampling_rate=1000.0 \ --hparams=batch_size=32,learning_rate=0.0005 \ --num_steps=5000 \ --mode=train \ --examples_path=notesequences_R2Midi.tfrecord</code> </pre> <br>  Wieder das Problem.  Tensorflow st√ºrzt mit einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fehler ab</a> - die Bibliothek kann zum Gl√ºck nicht gefunden werden. Gl√ºcklicherweise hat vor einigen Tagen jemand diesen Fehler bereits beschrieben, und die Python-Quellen k√∂nnen behoben werden. <br><br>  Wir klettern in den Ordner <br><br><pre> <code class="plaintext hljs">/usr/local/lib/python2.7/dist-packages/tensorflow_probability/python/distributions#</code> </pre> <br>  und ersetzen Sie die Importzeile, wie im Fehler auf Github beschrieben. <br><br>  Starten Sie music_vae_train erneut und ... Hurra!  Das Training ist weg! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b5/46c/498/0b546c498699b6d4fbca1b95af41bd51.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> <br><br>  hier-multiperf_vel_1bar_med - Ich verwende ein polyphones Modell (bis zu 8 Instrumente), das jeweils einen Takt erzeugt. <br><br>  Ein wichtiger Parameter ist checkpoints_to_keep = 2, die Festplattenkapazit√§t in den Clouds ist begrenzt. Eines der Probleme besteht darin, dass der Lernprozess aufgrund eines Festplatten√ºberlaufs st√§ndig unterbrochen wurde. Die Pr√ºfpunkte sind ziemlich hoch - jeweils 0,6-1 Gigabyte. <br><br>  Irgendwo in den 5000 Epochen beginnt der Fehler zwischen 40 und 70 zu springen.  Ich wei√ü nicht, ob dies ein gutes Ergebnis ist oder nicht, aber es scheint, dass mit ein wenig Trainingsdaten das Netzwerk weitergebildet wird und es keinen Sinn macht, die Zeit der GPUs, die mir freundlicherweise kostenlos zur Verf√ºgung gestellt werden, in Google-Rechenzentren zu verbringen.  Wir gehen zur Generation √ºber. <br><br>  Aus irgendeinem Grund musste ich bei der Installation von Magenta die Generierungsdatei selbst nicht installieren und sie mit meinen H√§nden in den Ordner zu den anderen legen: <br><br><pre> <code class="plaintext hljs">curl -o music_vae_generate.py https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/models/music_vae/music_vae_generate.py</code> </pre> <br>  Erstellen Sie schlie√ülich die Fragmente: <br><br><pre> <code class="plaintext hljs">music_vae_generate --config=hier-multiperf_vel_1bar_med --checkpoint_file=/home/RNCDtrain/train/ --mode=sample --num_outputs=32 --output_dir=/home/andrey_shagal/  --temperature=0.3</code> </pre> <br>  config - Art der Generierung, genau wie w√§hrend des Trainings - Multitrack, 1 Uhr <br>  checkpoint_file - Ordner, in dem die letzte Datei mit dem trainierten Modell abgerufen wird <br>  mode - sample - Erstelle ein Sample (es gibt eine weitere Option zum Interpolieren - Erstelle ein √úbergangsma√ü zwischen zwei Takten) <br>  num_outputs - wie viele Teile generiert werden sollen <br>  Temperatur - ein Randomisierungsparameter beim Erstellen einer Stichprobe von 0 bis 1. Bei 0 ist das Ergebnis vorhersehbarer, n√§her an der Quelle, bei 1 - Ich bin ein K√ºnstler, wie ich es sehe. <br><br>  Am Ausgang erhalte ich 32 Fragmente pro Takt.  Nachdem ich den Generator mehrmals gestartet habe, h√∂re ich mir die Fragmente an und klebe das Beste in eine Spur: neurancid.mp3. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Also "Ich habe diesen Sommer verbracht."  Ich bin zufrieden.  Nat√ºrlich wird das Radio "Maximum" es wahrscheinlich nicht in die Wiedergabeliste aufnehmen, aber wenn Sie es h√∂ren, sieht es wirklich aus wie die urspr√ºngliche Rancid-Gruppe.  Der Sound unterscheidet sich nat√ºrlich von der Studioaufnahme, aber wir haben haupts√§chlich mit Noten gearbeitet.  Au√üerdem gibt es Raum f√ºr Action - verarbeiten Sie Midi mit verschiedenen VST-Plug-Ins, nehmen Sie Parts mit Live-Musikern neu auf oder warten Sie, bis die Jungs von Wave2Midi2Wave mit einer √úberlastung zu den Gitarren kommen. <br><br>  Es gibt keine Beschwerden √ºber die Notizen.  Idealerweise m√∂chte ich, dass das neuronale Netzwerk ein Meisterwerk oder zumindest einen Hit f√ºr die Billboard Top 100 schafft. Aber w√§hrend sie von Rockern gelernt hat, wie man <s>mit Alkohol und Drogen</s> umgeht <s>,</s> spielt sie den ganzen Beat eine Note in Achteln (eigentlich nicht nur, aber ich bin stolz auf ihre v√§terliche √úbergang von 20 auf 22 Sekunden).  Daf√ºr gibt es Gr√ºnde und mehr. <br><br><ol><li>  Kleine Datenmenge. </li><li>  Das von mir verwendete Modell erzeugt Fragmente in der Gr√∂√üe eines Ma√ües.  Im Punkrock finden in der Regel nicht viele Ereignisse innerhalb einer einzigen Ma√ünahme statt. </li><li>  Interessante √úberg√§nge und Melodien funktionieren nur vor dem Hintergrund von Tonh√∂henriffs, √úberg√§ngen von Akkord zu Akkord, und der Auto-Encoder scheint zusammen mit einer kleinen Datenmenge die meisten Melodien verloren zu haben und sogar alle Riffs auf zwei Konsonanten und mehrere atonale Akkorde reduziert zu haben.  Wir m√ºssen ein Modell ausprobieren, das mit 16 Takten funktioniert. Schade, dass nur drei Stimmen darin verf√ºgbar sind. </li></ol><br>  Ich habe die Entwickler kontaktiert, sie haben empfohlen, die Dimension des latenten Raums zu reduzieren, weil sie ihr Netzwerk auf 200.000 Tracks trainiert haben, und ich habe auf 15 trainiert. Ich konnte den sichtbaren Effekt der Reduzierung des Z-Raums nicht erzielen, aber es gibt immer noch etwas zu basteln. <br><br>  Monotonie und Monotonie sind √ºbrigens keineswegs immer ein Minus.  Von schamanistischen Ritualen bis zu Technopartys, wie Sie wissen, ein Schritt.  Wir m√ºssen versuchen, das Modell auf so etwas zu trainieren - Rave, Techno, Dub, Reggae, Hip-Hop-Nachteile.  Sicherlich gibt es eine Chance, etwas angenehm Zombie zu erschaffen.  Ich fand ungef√§hr 20 Songs von Bob Marley im Midi und, voila la, eine sehr sch√∂ne Schleife: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  √úber den Midi-Parts werden Live-B√§sse und Gitarren neu aufgenommen, die von VST-Synthesizern verarbeitet werden, um das Fragment saftiger klingen zu lassen.  Im Original gab das Netzwerk nur Notizen aus.  Wenn Sie sie mit einem Standard-Midi-Player spielen, klingt dies folgenderma√üen: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Wenn Sie eine Reihe grundlegender thematischer Drum-Zeichnungen erstellen, diese in Beat Blender + grundlegenden Teilen von Bass und Synthesizern mit einem latenten Loop starten (es gab mehr dar√ºber), ist es durchaus m√∂glich, einen Algorithmus f√ºr Techno-Radio auszuf√ºhren, der kontinuierlich neue Tracks oder sogar einen erstellt endlose Spur.  Ewiges Summen! <br><br>  MusicVAE bietet auch die M√∂glichkeit, das Netzwerk zu trainieren, um Trio-Fragmente mit 16 Takten zu erzeugen - Schlagzeug, Bass und Lead.  Auch ganz interessant.  Eingabedaten - Multitrack-MIDI-Dateien - Das System teilt sich in allen m√∂glichen Kombinationen in Tripel auf und trainiert das Modell weiter.  Ein solches Netzwerk ben√∂tigt deutlich mehr Ressourcen, aber das Ergebnis sind sofort 16 Zyklen!  Unm√∂glich zu widerstehen.  Ich versuchte mir vorzustellen, wie eine Gruppe, die etwas zwischen Rancid und NOFX spielt, klingen k√∂nnte, indem ich f√ºr das Training ungef√§hr die gleiche Anzahl von Tracks aus jeder Gruppe lud: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Es gibt auch Midi-Parts, die Live-Gitarren neu aufgenommen haben.  Standard-Midi-Player wie folgt: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Interessant!  Das ist definitiv besser als meine erste Gruppe!  √úbrigens gibt uns dieses Modell einen anst√§ndigen Free Jazz: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://w.soundcloud.com/player/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Die Probleme, auf die ich gesto√üen bin: <br><br><ol><li>  Fehlen eines guten, bequemen Standes, der die Wartezeit auf das Training verk√ºrzt.  Das Modell funktioniert nur unter Linux, das Training ist lang, sehr lange ohne GPU, und ich m√∂chte st√§ndig versuchen, die Parameter zu √§ndern und zu sehen, was passiert.  Beispielsweise z√§hlte ein Cloud-Server mit einem GPU-Prozessor aus 100 Epochen f√ºr das Modell ‚ÄûTrio mit 16 Zyklen‚Äú 8 Stunden. </li><li>  Ein typisches Problem beim maschinellen Lernen ist der Mangel an Daten.  Nur 15 MIDI-Dateien - es ist sehr klein, Musik zu verstehen.  Das neuronale Netzwerk hat, anders als ich in meiner Jugend, vor den L√∂chern keine 6 Rancid-Alben geh√∂rt, ich habe keine Konzerte besucht. Dieses Ergebnis wurde aus 15 Midi-Tracks erzielt, die niemandem bekannt sind, der weit vom Original entfernt ist.  Wenn Sie nun mit Sensoren beim Gitarristen bleiben und jeden Ton aus jeder Note nehmen ... Mal sehen, wie sich die Wave2Midi2Wave-Idee entwickelt.  Vielleicht wird es in ein paar Jahren m√∂glich sein, Notizen zur L√∂sung eines solchen Problems abzulehnen. </li><li>  Der Musiker sollte klar in den Rhythmus fallen, aber nicht perfekt.  Am Midi-Wochenende gibt es keine Dynamik in den Noten (zum Beispiel im Schlagzeug), sie werden alle mit der gleichen Lautst√§rke gespielt, genau mit einem Klick (wie die Musiker sagen, d. H. Genau im Takt), selbst wenn Sie sie zuf√§llig diversifizieren, beginnt die Musik zu klingen lebendiger und angenehmer.  Auch hier befasst sich Wave2Midi2Wave bereits mit diesem Problem. </li></ol><br>  Jetzt haben Sie eine Vorstellung von den M√∂glichkeiten der KI beim Erstellen von Musik und meinen musikalischen Vorlieben.  Welche Rolle erwartet KI Ihrer Meinung nach in Zukunft im kreativen Prozess?  Kann eine Maschine Musik auf gleicher Augenh√∂he oder sogar besser als ein Mensch schaffen, um ein Assistent im kreativen Prozess zu sein?  Oder k√ºnstliche Intelligenz wird im Musikbereich nur f√ºr primitives Handwerk ber√ºhmt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439546/">https://habr.com/ru/post/de439546/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439534/index.html">Ertragsprogrammierung und Einsatz von Drohnen in der √ñlf√∂rderung - 10 Vortr√§ge von der GIS Tech Russia-Konferenz</a></li>
<li><a href="../de439538/index.html">Gesellschaftsprogrammierung</a></li>
<li><a href="../de439540/index.html">Etherblade.net - OpenSource-Projekt zum Erstellen eines Ethernet-Verkehrskapselers auf FPGA (Teil 1)</a></li>
<li><a href="../de439542/index.html">Nintendo macht deutlich, dass nur Piraterie die Geschichte von Videospielen retten kann</a></li>
<li><a href="../de439544/index.html">Kolonie. Kapitel 24: Abreise</a></li>
<li><a href="../de439550/index.html">Hackquest 2018. Ergebnisse & Zuschreibungen. Tag 1-3</a></li>
<li><a href="../de439552/index.html">Sch√§dliche Chrome-Erweiterungen</a></li>
<li><a href="../de439556/index.html">TDMS Fairway. PMBOK-Methoden und russische Designorganisationen</a></li>
<li><a href="../de439558/index.html">Neues altes Telefon. Das PSTN-Telefon neu erfinden</a></li>
<li><a href="../de439560/index.html">Ethereum-Blockchain-Adapter f√ºr die IRIS-Datenplattform von InterSystems</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>