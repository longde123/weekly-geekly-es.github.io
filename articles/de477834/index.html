<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤼 🙅🏿 🛌 Prinzipien zum Aufbau von Streaming-Analytics-Systemen 📠 🕔 👩🏻‍🤝‍👨🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Entwerfen von Streaming-Analytics- und Streaming-Datenverarbeitungssystemen hat seine eigenen Nuancen, seine eigenen Probleme und seinen eigenen t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Prinzipien zum Aufbau von Streaming-Analytics-Systemen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477834/"><img src="https://habrastorage.org/webt/rk/pz/c5/rkpzc5nw7uyyv0vsp_00trtag44.jpeg" alt="Bild"><br><br>  Das Entwerfen von Streaming-Analytics- und Streaming-Datenverarbeitungssystemen hat seine eigenen Nuancen, seine eigenen Probleme und seinen eigenen technologischen Stack.  Wir haben darüber in der nächsten <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">offenen Lektion gesprochen</a> , die am Vorabend des Starts des <a href="https://otus.pw/IxY2/">Data Engineer-</a> Kurses stattfand. <br><br>  Auf dem Webinar diskutiert: <br><br><ul><li>  wenn Streaming-Verarbeitung benötigt wird; </li><li>  Welche Elemente sind in SPOD enthalten, mit welchen Tools können diese Elemente implementiert werden? </li><li>  Wie Sie Ihr eigenes Clickstream-Analysesystem erstellen. </li></ul><br>  Dozent - <a href="https://otus.ru/teacher/370/">Yegor Mateshuk</a> , Senior Data Engineer bei MaximaTelecom. <br><a name="habracut"></a><br><h3>  Wann wird Streaming benötigt?  Stream vs Batch </h3><br>  Zunächst müssen wir herausfinden, wann Streaming und Stapelverarbeitung erforderlich sind.  Lassen Sie uns die Stärken und Schwächen dieser Ansätze erklären. <br><br>  <b>Also, die Nachteile der Stapelverarbeitung:</b> <br><br><ul><li>  Daten werden mit einer Verzögerung geliefert.  Da wir einen bestimmten Berechnungszeitraum haben, hinken wir in diesem Zeitraum immer der Echtzeit nach.  Und je mehr Iteration, desto mehr hinken wir hinterher.  Dadurch erhalten wir eine Zeitverzögerung, die in einigen Fällen kritisch ist; </li><li>  Es entsteht eine Spitzenbelastung des Eisens.  Wenn wir eine Menge im Batch-Modus berechnen, haben wir am Ende des Zeitraums (Tag, Woche, Monat) eine Spitzenlast, weil Sie eine Menge Dinge berechnen müssen.  Wohin führt das?  Erstens beginnen wir uns gegen Grenzen auszuruhen, die, wie Sie wissen, nicht unendlich sind.  Infolgedessen läuft das System regelmäßig bis zum Limit, was häufig zu Fehlern führt.  Zweitens, da alle diese Jobs zur gleichen Zeit starten, konkurrieren sie und werden relativ langsam berechnet. Sie können also nicht mit einem schnellen Ergebnis rechnen. </li></ul><br>  <b>Die Stapelverarbeitung hat jedoch folgende Vorteile:</b> <br><br><ul><li>  hoher Wirkungsgrad.  Wir werden nicht tiefer gehen, da Effizienz mit Komprimierung, Frameworks und der Verwendung von Spaltenformaten usw. verbunden ist. Fakt ist, dass die Stapelverarbeitung effizienter ist, wenn Sie die Anzahl der verarbeiteten Datensätze pro Zeiteinheit berücksichtigen. </li><li> einfache Entwicklung und Unterstützung.  Sie können jeden Teil der Daten verarbeiten, indem Sie sie nach Bedarf testen und neu zählen. </li></ul><br>  <b>Vorteile der Streaming-Datenverarbeitung (Streaming):</b> <br><br><ul><li>  Ergebnis in Echtzeit.  Wir warten nicht auf das Ende eines Zeitraums: Sobald die Daten (auch nur in sehr geringem Umfang) bei uns eintreffen, können wir sie sofort verarbeiten und weitergeben.  Das heißt, das Ergebnis strebt per Definition nach Echtzeit. </li><li>  gleichmäßige Belastung des Eisens.  Es ist klar, dass es tägliche Zyklen usw. gibt, die Last ist jedoch immer noch über den Tag verteilt, und sie fällt gleichmäßiger und vorhersehbarer aus. </li></ul><br>  <b>Der Hauptnachteil der Streaming-Verarbeitung:</b> <br><ul><li>  Komplexität der Entwicklung und Unterstützung.  Erstens ist das Testen, Verwalten und Abrufen von Daten im Vergleich zum Batch etwas schwieriger.  Die zweite Schwierigkeit (tatsächlich ist dies das grundlegendste Problem) ist mit Rollbacks verbunden.  Wenn Jobs nicht funktionierten und ein Fehler aufgetreten ist, ist es sehr schwierig, genau den Moment zu erfassen, in dem alles kaputt gegangen ist.  Die Lösung des Problems erfordert mehr Aufwand und Ressourcen als die Stapelverarbeitung. </li></ul><br>  Wenn Sie sich überlegen, <b>ob Sie Streams benötigen</b> , beantworten Sie die folgenden Fragen für sich selbst: <br><br><ol><li>  Benötigen Sie wirklich Echtzeit? </li><li>  Gibt es viele Streaming-Quellen? </li><li>  Ist es wichtig, einen Datensatz zu verlieren? </li></ol><br>  Schauen wir uns <b>zwei Beispiele an</b> : <br><br>  <i>Beispiel 1. Aktienanalyse für den Einzelhandel:</i> <br><ul><li>  Die Warenanzeige ändert sich nicht in Echtzeit. </li><li>  Daten werden am häufigsten im Batch-Modus geliefert; </li><li>  Informationsverlust ist kritisch. </li></ul><br>  In diesem Beispiel ist es besser, Batch zu verwenden. <br><br>  <i>Beispiel 2. Analyse für ein Webportal:</i> <br><br><ul><li>  Die Analysegeschwindigkeit bestimmt die Reaktionszeit auf ein Problem. </li><li>  Daten kommen in Echtzeit; </li><li>  Der Verlust einer kleinen Menge von Benutzeraktivitätsinformationen ist akzeptabel. </li></ul><br>  Stellen Sie sich vor, dass Analytics widerspiegelt, wie sich Besucher eines Webportals mit Ihrem Produkt fühlen.  Beispielsweise haben Sie eine neue Version herausgebracht und müssen innerhalb von 10 bis 30 Minuten wissen, ob alles in Ordnung ist, wenn benutzerdefinierte Funktionen fehlerhaft sind.  Angenommen, der Text auf der Schaltfläche "Bestellen" ist nicht mehr vorhanden. Mit Analytics können Sie schnell auf einen starken Rückgang der Anzahl der Bestellungen reagieren und sofort erkennen, dass ein Rollback erforderlich ist. <br><br>  Daher ist es im zweiten Beispiel besser, Streams zu verwenden. <br><br><h3>  SPOD-Elemente </h3><br>  Datenverarbeitungsingenieure erfassen, verschieben, liefern, konvertieren und speichern genau diese Daten (ja, die Datenspeicherung ist ebenfalls ein aktiver Prozess!). <br>  Um ein Streaming Data Processing System (SPOD) aufzubauen, benötigen wir daher die folgenden Elemente: <br><br><ol><li>  <b>Datenlader</b> (Mittel zur Datenlieferung an den Speicher); </li><li>  <b>Datenaustauschbus</b> (dies ist nicht immer erforderlich, aber in Streams ist dies nicht möglich, da Sie ein System benötigen, über das Sie Daten in Echtzeit austauschen können.) </li><li>  <b>Datenspeicherung</b> (wie ohne); </li><li>  <b>ETL-Engine</b> (für verschiedene Filter-, Sortier- und andere Vorgänge erforderlich); </li><li>  <b>BI</b> (um Ergebnisse anzuzeigen); </li><li>  <b>Orchestrator</b> (verbindet den gesamten Prozess und organisiert die mehrstufige Datenverarbeitung). </li></ol><br>  In unserem Fall betrachten wir die einfachste Situation und konzentrieren uns nur auf die ersten drei Elemente. <br><br><h3>  Tools zur Datenstromverarbeitung </h3><br>  Wir haben mehrere "Kandidaten" für die Rolle des <b>Datenladers</b> : <br><br><ul><li>  Apache-Gerinne </li><li>  Apache nifi </li><li>  Streamset </li></ul><br><h4>  Apache-Gerinne </h4><br>  Das erste, worüber wir sprechen, ist <b>Apache Flume</b> , ein Tool zum Transportieren von Daten zwischen verschiedenen Quellen und Repositorys. <br><br><img src="https://habrastorage.org/webt/dg/by/a3/dgbya30snrkaceq0y7bvtct59wc.png" alt="Bild"><br><br>  Vorteile: <br><br><ul><li>  es gibt fast überall </li><li>  lange gebraucht </li><li>  flexibel und erweiterbar genug </li></ul><br>  Nachteile: <br><br><ul><li>  unbequeme Konfiguration </li><li>  schwer zu überwachen </li></ul><br>  Die Konfiguration sieht ungefähr so ​​aus: <br><br><img src="https://habrastorage.org/webt/hf/-i/bz/hf-ibz-bp5n8ydo3c1viwsxw9qe.png" alt="Bild"><br><br>  Oben erstellen wir einen einfachen Kanal, der sich auf dem Port befindet, Daten von dort entnimmt und einfach protokolliert.  Prinzipiell ist dies, um einen Prozess zu beschreiben, immer noch normal, aber wenn Sie Dutzende solcher Prozesse haben, verwandelt sich die Konfigurationsdatei in eine Hölle.  Jemand fügt einige visuelle Konfiguratoren hinzu, aber warum sollte man sich die Mühe machen, wenn es Werkzeuge gibt, die es aus der Box schaffen?  Zum Beispiel die gleichen NiFi und StreamSets. <br><br><h4>  Apache nifi </h4><br>  Tatsächlich hat es die gleiche Funktion wie Flume, verfügt jedoch über eine visuelle Benutzeroberfläche, was insbesondere bei vielen Prozessen ein großes Plus darstellt. <br><br>  Einige Fakten zu NiFi <br><br><ul><li>  ursprünglich bei der NSA entwickelt; </li><li>  Hortonworks wird jetzt unterstützt und weiterentwickelt. </li><li>  Teil von HDF von Hortonworks; </li><li>  hat eine spezielle Version von MiNiFi zum Sammeln von Daten von Geräten. </li></ul><br>  Das System sieht ungefähr so ​​aus: <br><br><img src="https://habrastorage.org/webt/jz/1k/l7/jz1kl7seqymd9rxx2tog4k88wni.png" alt="Bild"><br><br>  Wir haben ein Feld für Kreativität und Phasen der Datenverarbeitung, die wir dort werfen.  Es gibt viele Anschlüsse für alle möglichen Systeme usw. <br><br><h4>  Streamset </h4><br>  Es ist auch ein Datenfluss-Kontrollsystem mit einer visuellen Schnittstelle.  Es wurde von Leuten aus Cloudera entwickelt, ist einfach als Paket auf CDH zu installieren und verfügt über eine spezielle Version von SDC Edge zum Sammeln von Daten von Geräten. <br><br>  Besteht aus zwei Komponenten: <br><br><ul><li>  DEZA - ein System zur direkten Datenverarbeitung (kostenlos); </li><li>  StreamSets Control Hub - ein Kontrollzentrum für mehrere DEZA mit zusätzlichen Funktionen für die Entwicklung von Paylines (kostenpflichtig). </li></ul><br>  Es sieht ungefähr so ​​aus: <br><br><img src="https://habrastorage.org/webt/kx/3z/jw/kx3zjwqx_ijbfxdlg7hvizllnt4.png" alt="Bild"><br><br>  Unangenehmer Moment - StreamSets haben sowohl kostenlose als auch kostenpflichtige Teile. <br><br><h4>  Datenbus </h4><br>  Nun wollen wir herausfinden, wo wir diese Daten hochladen werden.  Bewerber: <br><br><ul><li>  Apache kafka </li><li>  Rabbitmq </li><li>  NATS </li></ul><br>  Apache Kafka ist die beste Option, aber wenn Sie RabbitMQ oder NATS in Ihrem Unternehmen haben und ein wenig Analyse hinzufügen müssen, ist die Bereitstellung von Kafka von Grund auf nicht sehr rentabel. <br><br>  In allen anderen Fällen ist Kafka eine gute Wahl.  Tatsächlich handelt es sich um einen Nachrichtenbroker mit horizontaler Skalierung und großer Bandbreite.  Es ist perfekt in das gesamte Ökosystem der Tools für die Arbeit mit Daten integriert und hält hohen Belastungen stand.  Es hat eine universelle Schnittstelle und ist das Kreislaufsystem unserer Datenverarbeitung. <br><br>  Im Inneren ist Kafka in Topic unterteilt - ein bestimmter separater Datenstrom von Nachrichten mit demselben Schema oder zumindest mit demselben Zweck. <br><br>  Um die nächste Nuance zu erörtern, müssen Sie berücksichtigen, dass die Datenquellen geringfügig variieren können.  Das Datenformat ist sehr wichtig: <br><br><img src="https://habrastorage.org/webt/fq/kn/ci/fqkncilmsox7hmoye289ronvbyk.png" alt="Bild"><br><br>  Das Apache Avro-Daten-Serialisierungsformat verdient besondere Erwähnung.  Das System ermittelt mithilfe von JSON die Datenstruktur (Schema), die in ein <b>kompaktes Binärformat</b> serialisiert wird.  Daher sparen wir eine große Datenmenge, und die Serialisierung / Deserialisierung ist billiger. <br><br>  Alles scheint in Ordnung zu sein, aber das Vorhandensein separater Dateien mit Schaltkreisen stellt ein Problem dar, da wir Dateien zwischen verschiedenen Systemen austauschen müssen.  Es scheint einfach zu sein, aber wenn Sie in verschiedenen Abteilungen arbeiten, können die Kollegen am anderen Ende etwas ändern und sich beruhigen, und alles wird für Sie zusammenbrechen. <br><br>  Um nicht alle diese Dateien auf Flash-Laufwerke, Disketten und Höhlenmalereien zu übertragen, gibt es einen speziellen Dienst - Schema-Registry.  Dies ist ein Dienst zum Synchronisieren von Avro-Schemata zwischen Diensten, die von Kafka schreiben und lesen. <br><br><img src="https://habrastorage.org/webt/do/jf/qd/dojfqd1m6nf5wr53xmvmg5hee_a.png" alt="Bild"><br><br>  In Bezug auf Kafka ist der Produzent derjenige, der schreibt, der Konsument derjenige, der die Daten konsumiert (liest). <br><br><h4>  Data Warehouse </h4><br>  Herausforderer (in der Tat gibt es viel mehr Optionen, aber nur wenige): <br><br><ul><li>  HDFS + Hive </li><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  Denken Sie vor der Auswahl eines Repositorys daran, was <b>Idempotenz ist</b> .  Wikipedia sagt, dass die Idempotenz (lat. Idem - das Gleiche + Potenzen - fähig) - die Eigenschaft eines Objekts oder einer Operation, wenn die Operation erneut auf das Objekt angewendet wird, dasselbe Ergebnis wie die erste ergibt.  In unserem Fall sollte der Prozess der Streaming-Verarbeitung so aufgebaut sein, dass beim erneuten Füllen der Quelldaten das Ergebnis korrekt bleibt. <br><br>  <b>So erreichen Sie dies</b> in Streaming-Systemen: <br><br><ul><li>  identifiziere eine eindeutige ID (kann zusammengesetzt sein) </li><li>  Verwenden Sie diese ID, um Daten zu deduplizieren </li></ul><br>  Der HDFS + Hive-Speicher <b>bietet keine Möglichkeit,</b> Aufnahmen <b>sofort</b> zu streamen. Daher haben wir: <br><br><ul><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  <b>Kudu</b> ist ein für analytische Abfragen geeignetes Repository, das jedoch einen Primärschlüssel für die Deduplizierung besitzt.  <b>Impala</b> ist die SQL-Schnittstelle zu diesem Repository (und mehreren anderen). <br><br>  Bei ClickHouse handelt es sich um eine Analysedatenbank von Yandex.  Der Hauptzweck ist die Analyse einer Tabelle, die mit einem großen Strom von Rohdaten gefüllt ist.  Von den Vorteilen - es gibt eine ReplacingMergeTree-Engine für die Schlüsseldeduplizierung (die Deduplizierung ist platzsparend und hinterlässt möglicherweise Duplikate. In einigen Fällen müssen Sie die <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/replacingmergetree/">Nuancen</a> berücksichtigen). <br><br>  Es bleibt noch ein paar Worte über <b>Divolte hinzuzufügen</b> .  Wenn Sie sich erinnern, haben wir darüber gesprochen, dass einige Daten erfasst werden müssen.  Wenn Sie Analysen für ein Portal schnell und einfach organisieren möchten, ist Divolte ein hervorragender Dienst zum Erfassen von Benutzerereignissen auf einer Webseite über JavaScript. <br><br><img src="https://habrastorage.org/webt/wo/7m/ol/wo7molaoelkzjjatvyzzaihkrmm.png" alt="Bild"><br><br><h3>  Praktisches Beispiel </h3><br>  Was versuchen wir zu tun?  <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4016">Versuchen wir, eine</a> Pipeline zu erstellen, um Clickstream-Daten in Echtzeit zu erfassen.  <b>Clickstream</b> ist ein virtueller Footprint, den ein Benutzer auf Ihrer Website hinterlässt.  Wir werden Daten mit Divolte erfassen und in Kafka schreiben. <br><br><img src="https://habrastorage.org/webt/uj/mt/h-/ujmth-4jh9catnqqxt0ikp0w9xa.png" alt="Bild"><br><br>  Sie benötigen Docker, um arbeiten zu können. Außerdem müssen Sie das <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example">folgende Repository</a> klonen.  Alles, was passiert, wird in Containern gestartet.  <a href="">Wenn Sie</a> mehrere Container gleichzeitig <a href="">ausführen möchten,</a> wird <a href="">docker-compose.yml</a> verwendet.  Zusätzlich gibt es eine <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/blob/master/Dockerfile">Docker-Datei</a> , die unsere StreamSets mit bestimmten Abhängigkeiten kompiliert. <br><br>  Es gibt auch drei Ordner: <br><br><ol><li>  <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">clickhouse-Daten</a> werden in <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">clickhouse-Daten geschrieben</a> </li><li>  genau das gleiche daddy ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/sdc-data">sdc-daten</a> ) haben wir für streamsets, wo das system konfigurationen speichern kann </li><li>  Der dritte Ordner ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/examples">Beispiele</a> ) enthält eine Anforderungsdatei und eine Pipe-Konfigurationsdatei für StreamSets </li></ol><br><br><img src="https://habrastorage.org/webt/-r/xt/2r/-rxt2rlhxzdqqeyz7qv3z3dugxw.png" alt="Bild"><br><br>  Geben Sie zum Starten den folgenden Befehl ein: <br><br><pre><code class="bash hljs">docker-compose up</code> </pre> <br>  Und wir freuen uns, wie langsam, aber sicher Container starten.  Nach dem Start können wir zur Adresse <a href="http://localhost:8290/">http: // localhost: 18630 ​​/ gehen</a> und sofort Divolte berühren: <br><br><img src="https://habrastorage.org/webt/cc/1m/im/cc1mimjszmzdoyiwb-elb2c_igu.png" alt="Bild"><br><br>  Wir haben also Divolte, die bereits einige Events erhalten und in Kafka aufgenommen hat.  Versuchen wir, sie mit StreamSets zu berechnen: <a href="http://localhost:18630/">http: // localhost: 18630 ​​/</a> (password / login - admin / admin). <br><br><img src="https://habrastorage.org/webt/fc/hk/qz/fchkqzqe9pzpdws-xilz3ftdcb8.png" alt="Bild"><br><br>  Um nicht zu leiden, ist es besser, <b>Pipeline</b> zu <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4425">importieren</a> und sie beispielsweise als <b>clickstream_pipeline zu bezeichnen</b> .  Und aus dem Beispielordner importieren wir <b>clickstream.json</b> .  Wenn alles in Ordnung ist, sehen <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4701">wir folgendes Bild</a> : <br><br><img src="https://habrastorage.org/webt/o8/z9/x5/o8z9x5eaacqkehcfcgxattekpba.png" alt="Bild"><br><br>  Also stellten wir eine Verbindung zu Kafka her, registrierten, welche Kafka wir brauchten, registrierten, welches Thema uns interessierte, wählten dann die Felder aus, die uns interessierten, und steckten dann Kafka ab, um zu registrieren, welche Kafka und welches Thema wir wollten.  Die Unterschiede bestehen darin, dass in einem Fall das Datenformat Avro und im zweiten Fall nur JSON ist. <br><br>  Lass uns weitermachen.  Wir können zum Beispiel <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4768">eine Vorschau</a> erstellen, die bestimmte Datensätze von Kafka in Echtzeit aufzeichnet.  Dann schreiben wir alles auf. <br><br>  Nach dem Start sehen wir, dass ein Strom von Ereignissen nach Kafka fliegt, und dies geschieht in Echtzeit: <br><br><img src="https://habrastorage.org/webt/kr/a7/0n/kra70nxdaplug8-oywal1wb23oi.png" alt="Bild"><br><br>  Jetzt können Sie in ClickHouse ein Repository für diese Daten erstellen.  Um mit ClickHouse zu arbeiten, können Sie einen einfachen nativen Client verwenden, indem Sie den folgenden Befehl ausführen: <br><br><pre> <code class="bash hljs">docker run -it --rm --network divolte-ss-ch_default yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Bitte beachten Sie, dass diese Zeile das Netzwerk angibt, zu dem Sie eine Verbindung herstellen möchten.  Und je nachdem, wie Sie den Ordner mit dem Repository benennen, kann sich Ihr Netzwerkname unterscheiden.  Im Allgemeinen lautet der Befehl wie folgt: <br><br><pre> <code class="bash hljs">docker run -it --rm --network {your_network_name} yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Die Liste der Netzwerke kann mit dem folgenden Befehl angezeigt werden: <br><br><pre> <code class="bash hljs">docker network ls</code> </pre> <br>  Nun, da ist nichts mehr übrig: <br><br>  1. <b>Unterzeichnen Sie zunächst unser ClickHouse bei Kafka</b> und „erklären Sie ihm“, welches Format die Daten haben, die wir dort benötigen: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> clickstream_topic ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = Kafka <span class="hljs-keyword"><span class="hljs-keyword">SETTINGS</span></span> kafka_broker_list = <span class="hljs-string"><span class="hljs-string">'kafka:9092'</span></span>, kafka_topic_list = <span class="hljs-string"><span class="hljs-string">'clickstream'</span></span>, kafka_group_name = <span class="hljs-string"><span class="hljs-string">'clickhouse'</span></span>, kafka_format = <span class="hljs-string"><span class="hljs-string">'JSONEachRow'</span></span>;</code> </pre><br>  2. <b>Nun erstellen wir eine echte Tabelle,</b> in die wir die endgültigen Daten einfügen: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> clickstream ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = ReplacingMergeTree() <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> (<span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span>, pageViewId);</code> </pre> <br>  3. <b>Und dann stellen wir eine Beziehung zwischen diesen beiden Tabellen her</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">MATERIALIZED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">VIEW</span></span> clickstream_consumer <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> clickstream <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream_topic;</code> </pre> <br>  4. <b>Und jetzt werden wir die erforderlichen Felder auswählen</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream;</code> </pre> <br>  Infolgedessen liefert die Auswahl aus der Zieltabelle das gewünschte Ergebnis. <br><br><img src="https://habrastorage.org/webt/wn/sr/qe/wnsrqei2fo7iydrf41zattwbddy.png"><br><br>  Das ist alles, es war der einfachste Clickstream, den Sie erstellen können.  Wenn Sie die obigen Schritte selbst <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">ausführen</a> möchten, <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">sehen Sie sich das</a> gesamte <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">Video an</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de477834/">https://habr.com/ru/post/de477834/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de477820/index.html">VMware, Hyper-V, OpenStack, Kubernetes, Swarm - Überwachung über eine einzige Oberfläche in Quest Foglight</a></li>
<li><a href="../de477822/index.html">PHP 7.4 veröffentlicht! Wie Badoo Upgrades</a></li>
<li><a href="../de477824/index.html">Lass uns bis Montag leben oder wie man den schwarzen Freitag überlebt</a></li>
<li><a href="../de477826/index.html">Übersicht und Vergleich der V2X-Technologien</a></li>
<li><a href="../de477832/index.html">Wie komme ich mit der Generation Z zurecht?</a></li>
<li><a href="../de477836/index.html">Wie wir den WD ActiveScale P100 für unseren S3-Speicher getestet haben</a></li>
<li><a href="../de477838/index.html">PVS-Studio Static Analyzer als Tool zum Schutz vor Zero-Day-Schwachstellen</a></li>
<li><a href="../de477840/index.html">Static Code Analyzer von PVS-Studio zum Schutz vor Zero-Day-Schwachstellen</a></li>
<li><a href="../de477842/index.html">Geschichten von Gennady Zelenko und Sergey Popov - Technologie-Popularisatoren in der UdSSR</a></li>
<li><a href="../de477844/index.html">5 Schritte von der Idee bis zur praktischen Anwendung des maschinellen Lernens mit SAP Data Intelligence</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>