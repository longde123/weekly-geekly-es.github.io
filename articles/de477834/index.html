<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§º üôÖüèø üõå Prinzipien zum Aufbau von Streaming-Analytics-Systemen üì† üïî üë©üèª‚Äçü§ù‚Äçüë®üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Entwerfen von Streaming-Analytics- und Streaming-Datenverarbeitungssystemen hat seine eigenen Nuancen, seine eigenen Probleme und seinen eigenen t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Prinzipien zum Aufbau von Streaming-Analytics-Systemen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477834/"><img src="https://habrastorage.org/webt/rk/pz/c5/rkpzc5nw7uyyv0vsp_00trtag44.jpeg" alt="Bild"><br><br>  Das Entwerfen von Streaming-Analytics- und Streaming-Datenverarbeitungssystemen hat seine eigenen Nuancen, seine eigenen Probleme und seinen eigenen technologischen Stack.  Wir haben dar√ºber in der n√§chsten <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">offenen Lektion gesprochen</a> , die am Vorabend des Starts des <a href="https://otus.pw/IxY2/">Data Engineer-</a> Kurses stattfand. <br><br>  Auf dem Webinar diskutiert: <br><br><ul><li>  wenn Streaming-Verarbeitung ben√∂tigt wird; </li><li>  Welche Elemente sind in SPOD enthalten, mit welchen Tools k√∂nnen diese Elemente implementiert werden? </li><li>  Wie Sie Ihr eigenes Clickstream-Analysesystem erstellen. </li></ul><br>  Dozent - <a href="https://otus.ru/teacher/370/">Yegor Mateshuk</a> , Senior Data Engineer bei MaximaTelecom. <br><a name="habracut"></a><br><h3>  Wann wird Streaming ben√∂tigt?  Stream vs Batch </h3><br>  Zun√§chst m√ºssen wir herausfinden, wann Streaming und Stapelverarbeitung erforderlich sind.  Lassen Sie uns die St√§rken und Schw√§chen dieser Ans√§tze erkl√§ren. <br><br>  <b>Also, die Nachteile der Stapelverarbeitung:</b> <br><br><ul><li>  Daten werden mit einer Verz√∂gerung geliefert.  Da wir einen bestimmten Berechnungszeitraum haben, hinken wir in diesem Zeitraum immer der Echtzeit nach.  Und je mehr Iteration, desto mehr hinken wir hinterher.  Dadurch erhalten wir eine Zeitverz√∂gerung, die in einigen F√§llen kritisch ist; </li><li>  Es entsteht eine Spitzenbelastung des Eisens.  Wenn wir eine Menge im Batch-Modus berechnen, haben wir am Ende des Zeitraums (Tag, Woche, Monat) eine Spitzenlast, weil Sie eine Menge Dinge berechnen m√ºssen.  Wohin f√ºhrt das?  Erstens beginnen wir uns gegen Grenzen auszuruhen, die, wie Sie wissen, nicht unendlich sind.  Infolgedessen l√§uft das System regelm√§√üig bis zum Limit, was h√§ufig zu Fehlern f√ºhrt.  Zweitens, da alle diese Jobs zur gleichen Zeit starten, konkurrieren sie und werden relativ langsam berechnet. Sie k√∂nnen also nicht mit einem schnellen Ergebnis rechnen. </li></ul><br>  <b>Die Stapelverarbeitung hat jedoch folgende Vorteile:</b> <br><br><ul><li>  hoher Wirkungsgrad.  Wir werden nicht tiefer gehen, da Effizienz mit Komprimierung, Frameworks und der Verwendung von Spaltenformaten usw. verbunden ist. Fakt ist, dass die Stapelverarbeitung effizienter ist, wenn Sie die Anzahl der verarbeiteten Datens√§tze pro Zeiteinheit ber√ºcksichtigen. </li><li> einfache Entwicklung und Unterst√ºtzung.  Sie k√∂nnen jeden Teil der Daten verarbeiten, indem Sie sie nach Bedarf testen und neu z√§hlen. </li></ul><br>  <b>Vorteile der Streaming-Datenverarbeitung (Streaming):</b> <br><br><ul><li>  Ergebnis in Echtzeit.  Wir warten nicht auf das Ende eines Zeitraums: Sobald die Daten (auch nur in sehr geringem Umfang) bei uns eintreffen, k√∂nnen wir sie sofort verarbeiten und weitergeben.  Das hei√üt, das Ergebnis strebt per Definition nach Echtzeit. </li><li>  gleichm√§√üige Belastung des Eisens.  Es ist klar, dass es t√§gliche Zyklen usw. gibt, die Last ist jedoch immer noch √ºber den Tag verteilt, und sie f√§llt gleichm√§√üiger und vorhersehbarer aus. </li></ul><br>  <b>Der Hauptnachteil der Streaming-Verarbeitung:</b> <br><ul><li>  Komplexit√§t der Entwicklung und Unterst√ºtzung.  Erstens ist das Testen, Verwalten und Abrufen von Daten im Vergleich zum Batch etwas schwieriger.  Die zweite Schwierigkeit (tats√§chlich ist dies das grundlegendste Problem) ist mit Rollbacks verbunden.  Wenn Jobs nicht funktionierten und ein Fehler aufgetreten ist, ist es sehr schwierig, genau den Moment zu erfassen, in dem alles kaputt gegangen ist.  Die L√∂sung des Problems erfordert mehr Aufwand und Ressourcen als die Stapelverarbeitung. </li></ul><br>  Wenn Sie sich √ºberlegen, <b>ob Sie Streams ben√∂tigen</b> , beantworten Sie die folgenden Fragen f√ºr sich selbst: <br><br><ol><li>  Ben√∂tigen Sie wirklich Echtzeit? </li><li>  Gibt es viele Streaming-Quellen? </li><li>  Ist es wichtig, einen Datensatz zu verlieren? </li></ol><br>  Schauen wir uns <b>zwei Beispiele an</b> : <br><br>  <i>Beispiel 1. Aktienanalyse f√ºr den Einzelhandel:</i> <br><ul><li>  Die Warenanzeige √§ndert sich nicht in Echtzeit. </li><li>  Daten werden am h√§ufigsten im Batch-Modus geliefert; </li><li>  Informationsverlust ist kritisch. </li></ul><br>  In diesem Beispiel ist es besser, Batch zu verwenden. <br><br>  <i>Beispiel 2. Analyse f√ºr ein Webportal:</i> <br><br><ul><li>  Die Analysegeschwindigkeit bestimmt die Reaktionszeit auf ein Problem. </li><li>  Daten kommen in Echtzeit; </li><li>  Der Verlust einer kleinen Menge von Benutzeraktivit√§tsinformationen ist akzeptabel. </li></ul><br>  Stellen Sie sich vor, dass Analytics widerspiegelt, wie sich Besucher eines Webportals mit Ihrem Produkt f√ºhlen.  Beispielsweise haben Sie eine neue Version herausgebracht und m√ºssen innerhalb von 10 bis 30 Minuten wissen, ob alles in Ordnung ist, wenn benutzerdefinierte Funktionen fehlerhaft sind.  Angenommen, der Text auf der Schaltfl√§che "Bestellen" ist nicht mehr vorhanden. Mit Analytics k√∂nnen Sie schnell auf einen starken R√ºckgang der Anzahl der Bestellungen reagieren und sofort erkennen, dass ein Rollback erforderlich ist. <br><br>  Daher ist es im zweiten Beispiel besser, Streams zu verwenden. <br><br><h3>  SPOD-Elemente </h3><br>  Datenverarbeitungsingenieure erfassen, verschieben, liefern, konvertieren und speichern genau diese Daten (ja, die Datenspeicherung ist ebenfalls ein aktiver Prozess!). <br>  Um ein Streaming Data Processing System (SPOD) aufzubauen, ben√∂tigen wir daher die folgenden Elemente: <br><br><ol><li>  <b>Datenlader</b> (Mittel zur Datenlieferung an den Speicher); </li><li>  <b>Datenaustauschbus</b> (dies ist nicht immer erforderlich, aber in Streams ist dies nicht m√∂glich, da Sie ein System ben√∂tigen, √ºber das Sie Daten in Echtzeit austauschen k√∂nnen.) </li><li>  <b>Datenspeicherung</b> (wie ohne); </li><li>  <b>ETL-Engine</b> (f√ºr verschiedene Filter-, Sortier- und andere Vorg√§nge erforderlich); </li><li>  <b>BI</b> (um Ergebnisse anzuzeigen); </li><li>  <b>Orchestrator</b> (verbindet den gesamten Prozess und organisiert die mehrstufige Datenverarbeitung). </li></ol><br>  In unserem Fall betrachten wir die einfachste Situation und konzentrieren uns nur auf die ersten drei Elemente. <br><br><h3>  Tools zur Datenstromverarbeitung </h3><br>  Wir haben mehrere "Kandidaten" f√ºr die Rolle des <b>Datenladers</b> : <br><br><ul><li>  Apache-Gerinne </li><li>  Apache nifi </li><li>  Streamset </li></ul><br><h4>  Apache-Gerinne </h4><br>  Das erste, wor√ºber wir sprechen, ist <b>Apache Flume</b> , ein Tool zum Transportieren von Daten zwischen verschiedenen Quellen und Repositorys. <br><br><img src="https://habrastorage.org/webt/dg/by/a3/dgbya30snrkaceq0y7bvtct59wc.png" alt="Bild"><br><br>  Vorteile: <br><br><ul><li>  es gibt fast √ºberall </li><li>  lange gebraucht </li><li>  flexibel und erweiterbar genug </li></ul><br>  Nachteile: <br><br><ul><li>  unbequeme Konfiguration </li><li>  schwer zu √ºberwachen </li></ul><br>  Die Konfiguration sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/hf/-i/bz/hf-ibz-bp5n8ydo3c1viwsxw9qe.png" alt="Bild"><br><br>  Oben erstellen wir einen einfachen Kanal, der sich auf dem Port befindet, Daten von dort entnimmt und einfach protokolliert.  Prinzipiell ist dies, um einen Prozess zu beschreiben, immer noch normal, aber wenn Sie Dutzende solcher Prozesse haben, verwandelt sich die Konfigurationsdatei in eine H√∂lle.  Jemand f√ºgt einige visuelle Konfiguratoren hinzu, aber warum sollte man sich die M√ºhe machen, wenn es Werkzeuge gibt, die es aus der Box schaffen?  Zum Beispiel die gleichen NiFi und StreamSets. <br><br><h4>  Apache nifi </h4><br>  Tats√§chlich hat es die gleiche Funktion wie Flume, verf√ºgt jedoch √ºber eine visuelle Benutzeroberfl√§che, was insbesondere bei vielen Prozessen ein gro√ües Plus darstellt. <br><br>  Einige Fakten zu NiFi <br><br><ul><li>  urspr√ºnglich bei der NSA entwickelt; </li><li>  Hortonworks wird jetzt unterst√ºtzt und weiterentwickelt. </li><li>  Teil von HDF von Hortonworks; </li><li>  hat eine spezielle Version von MiNiFi zum Sammeln von Daten von Ger√§ten. </li></ul><br>  Das System sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/jz/1k/l7/jz1kl7seqymd9rxx2tog4k88wni.png" alt="Bild"><br><br>  Wir haben ein Feld f√ºr Kreativit√§t und Phasen der Datenverarbeitung, die wir dort werfen.  Es gibt viele Anschl√ºsse f√ºr alle m√∂glichen Systeme usw. <br><br><h4>  Streamset </h4><br>  Es ist auch ein Datenfluss-Kontrollsystem mit einer visuellen Schnittstelle.  Es wurde von Leuten aus Cloudera entwickelt, ist einfach als Paket auf CDH zu installieren und verf√ºgt √ºber eine spezielle Version von SDC Edge zum Sammeln von Daten von Ger√§ten. <br><br>  Besteht aus zwei Komponenten: <br><br><ul><li>  DEZA - ein System zur direkten Datenverarbeitung (kostenlos); </li><li>  StreamSets Control Hub - ein Kontrollzentrum f√ºr mehrere DEZA mit zus√§tzlichen Funktionen f√ºr die Entwicklung von Paylines (kostenpflichtig). </li></ul><br>  Es sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/kx/3z/jw/kx3zjwqx_ijbfxdlg7hvizllnt4.png" alt="Bild"><br><br>  Unangenehmer Moment - StreamSets haben sowohl kostenlose als auch kostenpflichtige Teile. <br><br><h4>  Datenbus </h4><br>  Nun wollen wir herausfinden, wo wir diese Daten hochladen werden.  Bewerber: <br><br><ul><li>  Apache kafka </li><li>  Rabbitmq </li><li>  NATS </li></ul><br>  Apache Kafka ist die beste Option, aber wenn Sie RabbitMQ oder NATS in Ihrem Unternehmen haben und ein wenig Analyse hinzuf√ºgen m√ºssen, ist die Bereitstellung von Kafka von Grund auf nicht sehr rentabel. <br><br>  In allen anderen F√§llen ist Kafka eine gute Wahl.  Tats√§chlich handelt es sich um einen Nachrichtenbroker mit horizontaler Skalierung und gro√üer Bandbreite.  Es ist perfekt in das gesamte √ñkosystem der Tools f√ºr die Arbeit mit Daten integriert und h√§lt hohen Belastungen stand.  Es hat eine universelle Schnittstelle und ist das Kreislaufsystem unserer Datenverarbeitung. <br><br>  Im Inneren ist Kafka in Topic unterteilt - ein bestimmter separater Datenstrom von Nachrichten mit demselben Schema oder zumindest mit demselben Zweck. <br><br>  Um die n√§chste Nuance zu er√∂rtern, m√ºssen Sie ber√ºcksichtigen, dass die Datenquellen geringf√ºgig variieren k√∂nnen.  Das Datenformat ist sehr wichtig: <br><br><img src="https://habrastorage.org/webt/fq/kn/ci/fqkncilmsox7hmoye289ronvbyk.png" alt="Bild"><br><br>  Das Apache Avro-Daten-Serialisierungsformat verdient besondere Erw√§hnung.  Das System ermittelt mithilfe von JSON die Datenstruktur (Schema), die in ein <b>kompaktes Bin√§rformat</b> serialisiert wird.  Daher sparen wir eine gro√üe Datenmenge, und die Serialisierung / Deserialisierung ist billiger. <br><br>  Alles scheint in Ordnung zu sein, aber das Vorhandensein separater Dateien mit Schaltkreisen stellt ein Problem dar, da wir Dateien zwischen verschiedenen Systemen austauschen m√ºssen.  Es scheint einfach zu sein, aber wenn Sie in verschiedenen Abteilungen arbeiten, k√∂nnen die Kollegen am anderen Ende etwas √§ndern und sich beruhigen, und alles wird f√ºr Sie zusammenbrechen. <br><br>  Um nicht alle diese Dateien auf Flash-Laufwerke, Disketten und H√∂hlenmalereien zu √ºbertragen, gibt es einen speziellen Dienst - Schema-Registry.  Dies ist ein Dienst zum Synchronisieren von Avro-Schemata zwischen Diensten, die von Kafka schreiben und lesen. <br><br><img src="https://habrastorage.org/webt/do/jf/qd/dojfqd1m6nf5wr53xmvmg5hee_a.png" alt="Bild"><br><br>  In Bezug auf Kafka ist der Produzent derjenige, der schreibt, der Konsument derjenige, der die Daten konsumiert (liest). <br><br><h4>  Data Warehouse </h4><br>  Herausforderer (in der Tat gibt es viel mehr Optionen, aber nur wenige): <br><br><ul><li>  HDFS + Hive </li><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  Denken Sie vor der Auswahl eines Repositorys daran, was <b>Idempotenz ist</b> .  Wikipedia sagt, dass die Idempotenz (lat. Idem - das Gleiche + Potenzen - f√§hig) - die Eigenschaft eines Objekts oder einer Operation, wenn die Operation erneut auf das Objekt angewendet wird, dasselbe Ergebnis wie die erste ergibt.  In unserem Fall sollte der Prozess der Streaming-Verarbeitung so aufgebaut sein, dass beim erneuten F√ºllen der Quelldaten das Ergebnis korrekt bleibt. <br><br>  <b>So erreichen Sie dies</b> in Streaming-Systemen: <br><br><ul><li>  identifiziere eine eindeutige ID (kann zusammengesetzt sein) </li><li>  Verwenden Sie diese ID, um Daten zu deduplizieren </li></ul><br>  Der HDFS + Hive-Speicher <b>bietet keine M√∂glichkeit,</b> Aufnahmen <b>sofort</b> zu streamen. Daher haben wir: <br><br><ul><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  <b>Kudu</b> ist ein f√ºr analytische Abfragen geeignetes Repository, das jedoch einen Prim√§rschl√ºssel f√ºr die Deduplizierung besitzt.  <b>Impala</b> ist die SQL-Schnittstelle zu diesem Repository (und mehreren anderen). <br><br>  Bei ClickHouse handelt es sich um eine Analysedatenbank von Yandex.  Der Hauptzweck ist die Analyse einer Tabelle, die mit einem gro√üen Strom von Rohdaten gef√ºllt ist.  Von den Vorteilen - es gibt eine ReplacingMergeTree-Engine f√ºr die Schl√ºsseldeduplizierung (die Deduplizierung ist platzsparend und hinterl√§sst m√∂glicherweise Duplikate. In einigen F√§llen m√ºssen Sie die <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/replacingmergetree/">Nuancen</a> ber√ºcksichtigen). <br><br>  Es bleibt noch ein paar Worte √ºber <b>Divolte hinzuzuf√ºgen</b> .  Wenn Sie sich erinnern, haben wir dar√ºber gesprochen, dass einige Daten erfasst werden m√ºssen.  Wenn Sie Analysen f√ºr ein Portal schnell und einfach organisieren m√∂chten, ist Divolte ein hervorragender Dienst zum Erfassen von Benutzerereignissen auf einer Webseite √ºber JavaScript. <br><br><img src="https://habrastorage.org/webt/wo/7m/ol/wo7molaoelkzjjatvyzzaihkrmm.png" alt="Bild"><br><br><h3>  Praktisches Beispiel </h3><br>  Was versuchen wir zu tun?  <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4016">Versuchen wir, eine</a> Pipeline zu erstellen, um Clickstream-Daten in Echtzeit zu erfassen.  <b>Clickstream</b> ist ein virtueller Footprint, den ein Benutzer auf Ihrer Website hinterl√§sst.  Wir werden Daten mit Divolte erfassen und in Kafka schreiben. <br><br><img src="https://habrastorage.org/webt/uj/mt/h-/ujmth-4jh9catnqqxt0ikp0w9xa.png" alt="Bild"><br><br>  Sie ben√∂tigen Docker, um arbeiten zu k√∂nnen. Au√üerdem m√ºssen Sie das <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example">folgende Repository</a> klonen.  Alles, was passiert, wird in Containern gestartet.  <a href="">Wenn Sie</a> mehrere Container gleichzeitig <a href="">ausf√ºhren m√∂chten,</a> wird <a href="">docker-compose.yml</a> verwendet.  Zus√§tzlich gibt es eine <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/blob/master/Dockerfile">Docker-Datei</a> , die unsere StreamSets mit bestimmten Abh√§ngigkeiten kompiliert. <br><br>  Es gibt auch drei Ordner: <br><br><ol><li>  <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">clickhouse-Daten</a> werden in <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">clickhouse-Daten geschrieben</a> </li><li>  genau das gleiche daddy ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/sdc-data">sdc-daten</a> ) haben wir f√ºr streamsets, wo das system konfigurationen speichern kann </li><li>  Der dritte Ordner ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/examples">Beispiele</a> ) enth√§lt eine Anforderungsdatei und eine Pipe-Konfigurationsdatei f√ºr StreamSets </li></ol><br><br><img src="https://habrastorage.org/webt/-r/xt/2r/-rxt2rlhxzdqqeyz7qv3z3dugxw.png" alt="Bild"><br><br>  Geben Sie zum Starten den folgenden Befehl ein: <br><br><pre><code class="bash hljs">docker-compose up</code> </pre> <br>  Und wir freuen uns, wie langsam, aber sicher Container starten.  Nach dem Start k√∂nnen wir zur Adresse <a href="http://localhost:8290/">http: // localhost: 18630 ‚Äã‚Äã/ gehen</a> und sofort Divolte ber√ºhren: <br><br><img src="https://habrastorage.org/webt/cc/1m/im/cc1mimjszmzdoyiwb-elb2c_igu.png" alt="Bild"><br><br>  Wir haben also Divolte, die bereits einige Events erhalten und in Kafka aufgenommen hat.  Versuchen wir, sie mit StreamSets zu berechnen: <a href="http://localhost:18630/">http: // localhost: 18630 ‚Äã‚Äã/</a> (password / login - admin / admin). <br><br><img src="https://habrastorage.org/webt/fc/hk/qz/fchkqzqe9pzpdws-xilz3ftdcb8.png" alt="Bild"><br><br>  Um nicht zu leiden, ist es besser, <b>Pipeline</b> zu <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4425">importieren</a> und sie beispielsweise als <b>clickstream_pipeline zu bezeichnen</b> .  Und aus dem Beispielordner importieren wir <b>clickstream.json</b> .  Wenn alles in Ordnung ist, sehen <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4701">wir folgendes Bild</a> : <br><br><img src="https://habrastorage.org/webt/o8/z9/x5/o8z9x5eaacqkehcfcgxattekpba.png" alt="Bild"><br><br>  Also stellten wir eine Verbindung zu Kafka her, registrierten, welche Kafka wir brauchten, registrierten, welches Thema uns interessierte, w√§hlten dann die Felder aus, die uns interessierten, und steckten dann Kafka ab, um zu registrieren, welche Kafka und welches Thema wir wollten.  Die Unterschiede bestehen darin, dass in einem Fall das Datenformat Avro und im zweiten Fall nur JSON ist. <br><br>  Lass uns weitermachen.  Wir k√∂nnen zum Beispiel <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4768">eine Vorschau</a> erstellen, die bestimmte Datens√§tze von Kafka in Echtzeit aufzeichnet.  Dann schreiben wir alles auf. <br><br>  Nach dem Start sehen wir, dass ein Strom von Ereignissen nach Kafka fliegt, und dies geschieht in Echtzeit: <br><br><img src="https://habrastorage.org/webt/kr/a7/0n/kra70nxdaplug8-oywal1wb23oi.png" alt="Bild"><br><br>  Jetzt k√∂nnen Sie in ClickHouse ein Repository f√ºr diese Daten erstellen.  Um mit ClickHouse zu arbeiten, k√∂nnen Sie einen einfachen nativen Client verwenden, indem Sie den folgenden Befehl ausf√ºhren: <br><br><pre> <code class="bash hljs">docker run -it --rm --network divolte-ss-ch_default yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Bitte beachten Sie, dass diese Zeile das Netzwerk angibt, zu dem Sie eine Verbindung herstellen m√∂chten.  Und je nachdem, wie Sie den Ordner mit dem Repository benennen, kann sich Ihr Netzwerkname unterscheiden.  Im Allgemeinen lautet der Befehl wie folgt: <br><br><pre> <code class="bash hljs">docker run -it --rm --network {your_network_name} yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Die Liste der Netzwerke kann mit dem folgenden Befehl angezeigt werden: <br><br><pre> <code class="bash hljs">docker network ls</code> </pre> <br>  Nun, da ist nichts mehr √ºbrig: <br><br>  1. <b>Unterzeichnen Sie zun√§chst unser ClickHouse bei Kafka</b> und ‚Äûerkl√§ren Sie ihm‚Äú, welches Format die Daten haben, die wir dort ben√∂tigen: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> clickstream_topic ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = Kafka <span class="hljs-keyword"><span class="hljs-keyword">SETTINGS</span></span> kafka_broker_list = <span class="hljs-string"><span class="hljs-string">'kafka:9092'</span></span>, kafka_topic_list = <span class="hljs-string"><span class="hljs-string">'clickstream'</span></span>, kafka_group_name = <span class="hljs-string"><span class="hljs-string">'clickhouse'</span></span>, kafka_format = <span class="hljs-string"><span class="hljs-string">'JSONEachRow'</span></span>;</code> </pre><br>  2. <b>Nun erstellen wir eine echte Tabelle,</b> in die wir die endg√ºltigen Daten einf√ºgen: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> clickstream ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = ReplacingMergeTree() <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> (<span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span>, pageViewId);</code> </pre> <br>  3. <b>Und dann stellen wir eine Beziehung zwischen diesen beiden Tabellen her</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">MATERIALIZED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">VIEW</span></span> clickstream_consumer <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> clickstream <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream_topic;</code> </pre> <br>  4. <b>Und jetzt werden wir die erforderlichen Felder ausw√§hlen</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream;</code> </pre> <br>  Infolgedessen liefert die Auswahl aus der Zieltabelle das gew√ºnschte Ergebnis. <br><br><img src="https://habrastorage.org/webt/wn/sr/qe/wnsrqei2fo7iydrf41zattwbddy.png"><br><br>  Das ist alles, es war der einfachste Clickstream, den Sie erstellen k√∂nnen.  Wenn Sie die obigen Schritte selbst <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">ausf√ºhren</a> m√∂chten, <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">sehen Sie sich das</a> gesamte <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">Video an</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de477834/">https://habr.com/ru/post/de477834/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de477820/index.html">VMware, Hyper-V, OpenStack, Kubernetes, Swarm - √úberwachung √ºber eine einzige Oberfl√§che in Quest Foglight</a></li>
<li><a href="../de477822/index.html">PHP 7.4 ver√∂ffentlicht! Wie Badoo Upgrades</a></li>
<li><a href="../de477824/index.html">Lass uns bis Montag leben oder wie man den schwarzen Freitag √ºberlebt</a></li>
<li><a href="../de477826/index.html">√úbersicht und Vergleich der V2X-Technologien</a></li>
<li><a href="../de477832/index.html">Wie komme ich mit der Generation Z zurecht?</a></li>
<li><a href="../de477836/index.html">Wie wir den WD ActiveScale P100 f√ºr unseren S3-Speicher getestet haben</a></li>
<li><a href="../de477838/index.html">PVS-Studio Static Analyzer als Tool zum Schutz vor Zero-Day-Schwachstellen</a></li>
<li><a href="../de477840/index.html">Static Code Analyzer von PVS-Studio zum Schutz vor Zero-Day-Schwachstellen</a></li>
<li><a href="../de477842/index.html">Geschichten von Gennady Zelenko und Sergey Popov - Technologie-Popularisatoren in der UdSSR</a></li>
<li><a href="../de477844/index.html">5 Schritte von der Idee bis zur praktischen Anwendung des maschinellen Lernens mit SAP Data Intelligence</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>