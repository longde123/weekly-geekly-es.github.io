<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöØ ‚èπÔ∏è üó®Ô∏è VShard - mise √† l'√©chelle horizontale dans Tarantool üë®üèæ‚Äçüîß ‚ôåÔ∏è ‚Ü©Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mon nom est Vladislav, je participe au d√©veloppement de Tarantool - SGBD et serveur d'application dans une bouteille. Et aujourd'hui, je vais vous exp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - mise √† l'√©chelle horizontale dans Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/436916/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Mon nom est Vladislav, je participe au d√©veloppement de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool</a> - SGBD et serveur d'application dans une bouteille.  Et aujourd'hui, je vais vous expliquer comment nous avons impl√©ment√© la mise √† l'√©chelle horizontale dans Tarantool √† l'aide du module <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VShard</a> . <br><a name="habracut"></a><br>  Tout d'abord, un peu de th√©orie. <br><br>  Il existe deux types de mise √† l'√©chelle: horizontale et verticale.  L'horizontale est divis√©e en deux types: r√©plication et partitionnement.  La r√©plication est utilis√©e pour mettre √† l'√©chelle l'informatique, le sharding est utilis√© pour mettre √† l'√©chelle les donn√©es. <br><br>  Le partage est divis√© en deux types: le partage par plages et le partage par hachages. <br><br>  Lors du partitionnement avec des plages, nous calculons une cl√© de partition √† partir de chaque enregistrement du cluster.  Ces cl√©s de fragments sont projet√©es sur une ligne droite, qui est divis√©e en plages que nous ajoutons √† diff√©rents n≈ìuds physiques. <br><br>  Le partage avec des hachages est plus simple: √† partir de chaque enregistrement du cluster, nous consid√©rons une fonction de hachage, nous ajoutons les entr√©es avec la m√™me valeur de la fonction de hachage √† un n≈ìud physique. <br><br>  Je vais parler de la mise √† l'√©chelle horizontale √† l'aide du d√©coupage de hachage. <br><br><h1>  Impl√©mentation pr√©c√©dente </h1><br>  Le premier module de mise √† l'√©chelle horizontale que nous avions √©tait <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool Shard</a> .  Il s'agit d'un partitionnement par hachage tr√®s simple, qui prend en compte la cl√© de partition de la cl√© primaire de toutes les entr√©es du cluster. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Mais une t√¢che s'est alors impos√©e que Tarantool Shard n'a pas √©t√© en mesure de g√©rer pour trois raisons fondamentales. <br><br>  Premi√®rement, la <b>localisation des donn√©es logiquement li√©es √©tait</b> requise.  Lorsque nous avons des donn√©es qui sont connect√©es logiquement, nous voulons toujours les stocker sur le m√™me n≈ìud physique, quelle que soit la fa√ßon dont la topologie du cluster change ou l'√©quilibrage est effectu√©.  Et Tarantool Shard ne le garantit pas.  Il consid√®re le hachage uniquement par des cl√©s primaires, et lors du r√©√©quilibrage, m√™me les enregistrements avec le m√™me hachage peuvent √™tre divis√©s pendant un certain temps - le transfert n'est pas atomique. <br><br>  Le probl√®me du manque de localisation des donn√©es nous a le plus emp√™ch√©.  Je vais vous donner un exemple.  Il y a une banque dans laquelle le client a ouvert un compte.  Les donn√©es du compte et du client doivent toujours √™tre stock√©es physiquement ensemble afin de pouvoir √™tre lues en une seule demande, √©chang√©es en une seule transaction, par exemple lors du transfert d'argent depuis un compte.  Si vous utilisez le partitionnement classique avec Tarantool Shard, les valeurs des fonctions de partition seront diff√©rentes pour les comptes et les clients.  Les donn√©es peuvent se trouver sur diff√©rents n≈ìuds physiques.  Cela complique grandement la lecture et le travail transactionnel avec un tel client. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre><br>  Dans l'exemple ci-dessus, les champs <code>id</code> peuvent facilement ne pas correspondre aux comptes et aux clients.  Ils sont connect√©s via le champ de compte <code>customer_id</code> et <code>customer_id</code> <code>id</code> .  Le m√™me champ <code>id</code> romprait l'unicit√© de la cl√© primaire du compte.  Et d'une autre mani√®re, Shard n'est pas en mesure de tailler. <br><br>  Le probl√®me suivant est <b>le r√©√©chantillonnage lent</b> .  C'est le probl√®me classique de tous les fragments sur les hachages.  L'essentiel est que lorsque nous modifions la composition d'un cluster, nous changeons g√©n√©ralement la fonction de partition, car cela d√©pend g√©n√©ralement du nombre de n≈ìuds.  Et lorsque la fonction change, vous devez parcourir toutes les entr√©es du cluster et recalculer √† nouveau la fonction de partition.  Peut-√™tre transf√©rer quelques notes.  Et pendant que nous les transf√©rons, nous ne savons pas si les donn√©es n√©cessaires √† la prochaine demande entrante ont d√©j√† √©t√© transf√©r√©es, peut-√™tre sont-elles maintenant en cours de transfert.  Par cons√©quent, lors du nouveau partage, il est n√©cessaire que chaque lecture fasse une demande pour deux fonctions de partition: l'ancienne et la nouvelle.  Les demandes deviennent deux fois plus lentes et pour nous, c'√©tait inacceptable. <br><br>  Une autre caract√©ristique de Tarantool Shard √©tait que lorsque certains n≈ìuds dans les jeux de r√©pliques √©chouaient, cela montrait une <b>mauvaise accessibilit√© en lecture</b> . <br><br><h1>  Nouvelle solution </h1><br>  Pour r√©soudre les trois probl√®mes d√©crits, nous avons cr√©√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool VShard</a> .  Sa principale diff√©rence est que le niveau de stockage des donn√©es est virtualis√©: les stockages virtuels sont apparus au-dessus des stockages physiques et les enregistrements sont r√©partis entre eux.  Ces stockages sont appel√©s bucket'ami.  L'utilisateur n'a pas besoin de penser √† quoi et sur quel n≈ìud physique se trouve.  Le seau est une unit√© de donn√©es atomique indivisible, comme dans le partage classique d'un tuple.  VShard stocke toujours l'int√©gralit√© du compartiment sur un n≈ìud physique et transf√®re toutes les donn√©es d'un compartiment de mani√®re atomique lors du resharding.  Pour cette raison, la localit√© est fournie.  Nous avons juste besoin de mettre les donn√©es dans un seul compartiment, et nous pouvons toujours √™tre s√ªrs que ces donn√©es seront associ√©es √† toutes les modifications du cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  Comment puis-je mettre les donn√©es dans un seul compartiment?  Dans le sch√©ma que nous avons introduit pr√©c√©demment pour le client de banque, nous ajouterons l' <code>bucket id</code> de <code>bucket id</code> aux tables en fonction du nouveau champ.  Si les donn√©es li√©es sont les m√™mes, les enregistrements seront dans le m√™me compartiment.  L'avantage est que nous pouvons stocker ces enregistrements avec le m√™me <code>bucket id</code> de <code>bucket id</code> dans diff√©rents espaces, et m√™me dans diff√©rents moteurs.  La <code>bucket id</code> fournie, quelle que soit la fa√ßon dont ces enregistrements sont stock√©s. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre><br>  Pourquoi sommes-nous si impatients pour cela?  Si nous avons un partage classique, alors les donn√©es peuvent se glisser dans tous les stockages physiques que nous avons seulement.  Dans l'exemple avec la banque, lors de la demande de tous les comptes d'un client, vous devrez vous tourner vers tous les n≈ìuds.  Il s'av√®re la difficult√© de lire O (N), o√π N est le nombre de magasins physiques.  Terriblement lent. <br><br>  Gr√¢ce √† bucket'am et √† la localit√© par <code>bucket id</code> de compartiment <code>bucket id</code> nous pouvons toujours lire les donn√©es d'un n≈ìud en une seule demande, quelle que soit la taille du cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2bb/524/8b7/2bb5248b7757ea6249f47a3dca46a681.png"><br><br>  Vous devez calculer l' <code>bucket id</code> et attribuer vous-m√™me les m√™mes valeurs.  Pour certains, c'est un avantage, pour quelqu'un un d√©savantage.  Je consid√®re qu'il est avantageux que vous puissiez choisir vous-m√™me la fonction de calcul de l' <code>bucket id</code> compartiment. <br><br>  Quelle est la principale diff√©rence entre le sharding classique et le sharding virtuel avec bucket? <br><br>  Dans le premier cas, lorsque nous modifions la composition du cluster, nous avons deux √©tats: l'actuel (ancien) et le nouveau, dans lesquels nous devons aller.  Dans le processus de transition, vous devez non seulement transf√©rer les donn√©es, mais √©galement recalculer les fonctions de hachage pour tous les enregistrements.  Ceci est tr√®s g√™nant, car √† un moment donn√©, nous ne savons pas quelles donn√©es ont d√©j√† √©t√© transf√©r√©es et lesquelles ne le sont pas.  De plus, ce n'est ni fiable ni atomique, car pour le transfert atomique d'un ensemble d'enregistrements avec la m√™me valeur de la fonction de hachage, il est n√©cessaire de stocker de mani√®re persistante l'√©tat de transfert en cas de besoin de restauration.  Il y a des conflits, des erreurs, vous devez red√©marrer la proc√©dure plusieurs fois. <br><br>  Le partage virtuel est beaucoup plus simple.  Nous n'avons pas deux √©tats s√©lectionn√©s du cluster, nous n'avons que l'√©tat du compartiment.  Le cluster devient plus maniable, il passe progressivement d'un √©tat √† l'autre.  Et maintenant, il y a plus de deux √âtats.  Gr√¢ce √† une transition en douceur, vous pouvez modifier l'√©quilibre √† la vol√©e, supprimer le stockage nouvellement ajout√©.  Autrement dit, la contr√¥labilit√© de l'√©quilibrage est consid√©rablement augment√©e, elle devient granulaire. <br><br><h1>  Utiliser </h1><br>  Disons que nous avons choisi une fonction pour l' <code>bucket id</code> et vers√© tellement de donn√©es dans le cluster qu'il n'y avait plus d'espace.  Maintenant, nous voulons ajouter des n≈ìuds, et pour que les donn√©es y soient d√©plac√©es nous-m√™mes.  Dans VShard, cela se fait comme suit.  Tout d'abord, lancez de nouveaux n≈ìuds et Tarantools dessus, puis mettez √† jour la configuration VShard.  Il d√©crit tous les membres du cluster, toutes les r√©pliques, les jeux de r√©plicas, les ma√Ætres, les URI attribu√©s et bien plus encore.  Nous ajoutons de nouveaux n≈ìuds √† la configuration, et en utilisant la fonction <code>VShard.storage.cfg</code> , <code>VShard.storage.cfg</code> utilisons sur tous les n≈ìuds du cluster. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Comme vous vous en souvenez, dans le partage classique avec un changement du nombre de n≈ìuds, la fonction de partage change √©galement.  Dans VShard, cela ne se produit pas, nous avons un nombre fixe de stockages virtuels - bucket'ov.  Il s'agit de la constante que vous s√©lectionnez lors du d√©marrage du cluster.  Il peut sembler qu'√† cause de cela, l'√©volutivit√© est limit√©e, mais pas vraiment.  Vous pouvez choisir un grand nombre de bucket'ov, des dizaines et des centaines de milliers.  L'essentiel est qu'il devrait y avoir au moins deux ordres de grandeur de plus que le nombre maximum de jeux de r√©pliques que vous aurez jamais dans le cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  √âtant donn√© que le nombre de stockages virtuels ne change pas - et la fonction de partition d√©pend uniquement de cette valeur - nous pouvons ajouter autant de stockages physiques que n√©cessaire sans recompter la fonction de partition. <br><br>  Comment les paniers sont-ils distribu√©s seuls dans les magasins physiques?  Lorsque VShard.storage.cfg est appel√© sur l'un des n≈ìuds, le processus de r√©√©quilibrage se r√©veille.  Il s'agit d'un processus analytique qui calcule l'√©quilibre parfait dans un cluster.  Il va √† tous les n≈ìuds physiques, demande qui a combien de bucket'ov, et construit des itin√©raires pour leur mouvement afin de faire la moyenne de la distribution.  Le r√©√©quilibreur envoie des itin√©raires vers des stockages bond√©s et ils commencent √† envoyer des seaux.  Apr√®s un certain temps, le cluster devient √©quilibr√©. <br><br>  Mais dans les projets r√©els, le concept d'√©quilibre parfait peut √™tre diff√©rent.  Par exemple, je souhaite stocker moins de donn√©es sur un jeu de r√©plicas que sur l'autre, car il y a moins d'espace sur le disque dur.  VShard pense que tout est bien √©quilibr√©, et en fait mon stockage est sur le point de d√©border.  Nous avons fourni un m√©canisme d'ajustement des r√®gles d'√©quilibrage √† l'aide de pond√©rations.  Chaque jeu de r√©plicas et r√©f√©rentiel peut √™tre pond√©r√©.  Lorsque l'√©quilibreur d√©cide √† qui envoyer le nombre de bucket'ov, il prend en compte la <b>relation de</b> toutes les paires de poids. <br><br>  Par exemple, un magasin a un poids de 100 et l'autre 200. Ensuite, le premier stockera deux fois moins de bucket'ov que le second.  Veuillez noter que je parle sp√©cifiquement du <b>rapport des</b> poids.  Les significations absolues n'ont aucun effet.  Vous pouvez choisir des pond√©rations bas√©es sur une distribution de cluster √† 100%: un magasin a 30%, un autre a 70%.  Vous pouvez prendre la capacit√© de stockage en gigaoctets comme base, ou vous pouvez mesurer les poids dans le nombre de bucket'ov.  L'essentiel est d'observer l'attitude dont vous avez besoin. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b4e/889/298/b4e889298991781b8c3ee01f4c066a6e.png"><br><br>  Un tel syst√®me a un effet secondaire int√©ressant: si vous attribuez un poids nul √† un magasin, l'√©quilibreur ordonnera au magasin de distribuer tous ses seaux.  Apr√®s cela, vous pouvez supprimer l'ensemble de r√©plicas de la configuration. <br><br><h1>  Transfert de seau atomique </h1><br>  Nous avons un compartiment, il accepte une sorte de demande de lecture et d'√©criture, puis l'√©quilibreur demande de le transf√©rer vers un autre stockage.  Bucket cesse d'accepter les demandes d'enregistrement, sinon ils auront le temps de le mettre √† jour pendant le transfert, puis ils auront le temps de mettre √† jour la mise √† jour portable, puis la mise √† jour portable, et ainsi de suite √† l'infini.  Par cons√©quent, l'enregistrement est bloqu√© et vous pouvez toujours lire √† partir du compartiment.  Le transfert de morceaux vers un nouvel endroit commence.  Une fois le transfert termin√©, le compartiment recommence √† accepter les demandes.  Dans l'ancien emplacement, il se trouve √©galement toujours, mais il a d√©j√† √©t√© marqu√© comme poubelle et par la suite, le garbage collector le supprimera morceau par morceau. <br><br>  Chaque compartiment est associ√© √† des m√©tadonn√©es physiquement stock√©es sur le disque.  Toutes les √©tapes ci-dessus sont enregistr√©es sur le disque et, quoi qu'il arrive avec le r√©f√©rentiel, l'√©tat du compartiment sera automatiquement restaur√©. <br><br>  Vous pourriez avoir des questions: <br><br><ul><li>  <b>Qu'arrivera-t-il aux demandes qui ont fonctionn√© avec le compartiment quand elles ont commenc√© √† le porter?</b> <br><br>  Il existe deux types de liens dans les m√©tadonn√©es de chaque compartiment: lecture et √©criture.  Lorsque l'utilisateur fait une demande au compartiment, il indique comment il va l'utiliser, en lecture seule ou en lecture-√©criture.  Pour chaque demande, le compteur de r√©f√©rence correspondant est incr√©ment√©. <br><br>  Pourquoi ai-je besoin d'un compteur de r√©f√©rence pour lire les demandes?  Disons que le seau est transf√©r√© tranquillement, et ici le ramasse-miettes vient et veut supprimer ce seau.  Il voit que le nombre de liens est sup√©rieur √† z√©ro, vous ne pouvez donc pas le supprimer.  Et lorsque les demandes seront trait√©es, le garbage collector pourra achever son travail. <br><br>  Le compteur de r√©f√©rence pour les demandes d'√©criture garantit que le compartiment ne commence m√™me pas √† √™tre transf√©r√© pendant qu'au moins une demande d'√©criture fonctionne avec.  Mais les demandes d'√©criture peuvent venir en permanence, et le bucket ne sera jamais transf√©r√©.  Le fait est que si l'√©quilibreur a exprim√© le souhait de le transf√©rer, les nouvelles demandes d'enregistrement commenceront √† √™tre bloqu√©es et le syst√®me actuel attendra la fin d'un certain d√©lai.  Si les demandes ne se terminent pas dans le d√©lai imparti, le syst√®me recommencera √† accepter de nouvelles demandes d'√©criture, reportant le transfert du compartiment pendant un certain temps.  Ainsi, l'√©quilibreur effectuera les tentatives de transfert jusqu'√† ce que l'une soit r√©ussie. <br><br>  VShard poss√®de une API bucket_ref de bas niveau au cas o√π vous auriez peu de fonctionnalit√©s de haut niveau.  Si vous voulez vraiment faire quelque chose vous-m√™me, acc√©dez simplement √† cette API √† partir du code. </li><li>  <b>Est-il possible de ne pas bloquer du tout les enregistrements?</b> <br><br>  C'est impossible.  Si le compartiment contient des donn√©es critiques qui n√©cessitent un acc√®s constant en √©criture, vous devrez bloquer compl√®tement son transfert.  Il existe une fonction <code>bucket_pin</code> pour cela, elle attache √©troitement le <code>bucket_pin</code> au jeu de r√©pliques actuel, emp√™chant son transfert.  Dans ce cas, le bucket'y voisin pourra se d√©placer sans restrictions. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Il existe un outil encore plus puissant que <code>bucket_pin</code> - blocage du jeu de r√©plicas.  Cela ne se fait plus en code, mais en configuration.  Le blocage interdit le mouvement de tout bucket'ov de cette r√©plique set'a et la r√©ception de nouveaux.  En cons√©quence, toutes les donn√©es seront disponibles en permanence pour l'enregistrement. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h1>  VShard.router </h1><br>  VShard se compose de deux sous-modules: VShard.storage et VShard.router.  Ils peuvent √™tre cr√©√©s et mis √† l'√©chelle ind√©pendamment, m√™me sur une seule instance.  Lors de l'acc√®s au cluster, nous ne savons pas o√π se trouve le compartiment, et VShard.router le recherchera par <code>bucket id</code> de compartiment pour nous. <br><br>  Regardons un exemple de ce √† quoi cela ressemble.  Nous revenons au cluster bancaire et aux comptes clients.  Je veux pouvoir retirer tous les comptes d'un client particulier du cluster.  Pour ce faire, j'√©cris la fonction habituelle de recherche locale: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f78/e2b/df2/f78e2bdf2d977fcb9fb320b031592171.png"><br><br>  Elle recherche tous les comptes clients par son identifiant.  Maintenant, je dois d√©cider lequel des r√©f√©rentiels appeler cette fonction.  Pour ce faire, je calcule l' <code>bucket id</code> de compartiment √† partir de l'ID client dans ma demande et demande √† VShard.router de m'appeler une telle fonction dans le stockage o√π r√©side le compartiment avec l' <code>bucket id</code> compartiment r√©sultant.  Il existe une table de routage dans le sous-module, dans laquelle l'emplacement du compartiment dans le jeu de r√©plicas est sp√©cifi√©.  Et VShard.router proc√®de par procuration √† ma demande. <br><br>  Bien s√ªr, il peut arriver qu'√† ce moment-l√†, le resharding commence et que le godet commence √† bouger.  Le routeur en arri√®re-plan met progressivement √† jour la table en gros morceaux: il interroge les r√©f√©rentiels pour leurs tables de compartiment actuelles. <br><br>  Il peut m√™me arriver que nous nous tournions vers le bucket qui vient de se d√©placer, et le routeur n'a pas encore r√©ussi √† mettre √† jour sa table de routage.  Ensuite, il se tournera vers l'ancien r√©f√©rentiel, et il indiquera au routeur o√π chercher le compartiment, ou r√©pondra simplement qu'il ne dispose pas des donn√©es n√©cessaires.  Ensuite, le routeur fera le tour de tous les stockages √† la recherche du seau souhait√©.  Et tout cela est transparent pour nous, on ne remarquera m√™me pas un manque dans la table de routage. <br><br><h1>  Lire l'instabilit√© </h1><br>  Rappelez-vous quels probl√®mes nous avions initialement: <br><br><ul><li>  Il n'y avait aucune localit√© de donn√©es.  Nous avons d√©cid√© en ajoutant bucket'ov. </li><li>  Le resharding a tout ralenti et ralenti.  Impl√©mentation du bucket'ami de transfert de donn√©es atomiques, d√©barrass√© des fonctions de recomptage des fragments. </li><li>  Lecture instable. </li></ul><br>  Le dernier probl√®me est r√©solu par VShard.router en utilisant le sous-syst√®me de basculement en lecture automatique. <br><br>  Le routeur envoie r√©guli√®rement un ping au stockage sp√©cifi√© dans la configuration.  Et puis certains d'entre eux ont cess√© de cingler.  Le routeur dispose d'une connexion de sauvegarde √† chaud √† chaque r√©plique et si la connexion actuelle cesse de r√©pondre, elle ira √† une autre.  La demande de lecture sera trait√©e normalement, car nous pouvons lire sur des r√©pliques (mais pas √©crire).  Nous pouvons d√©finir la priorit√© des r√©pliques par lesquelles le routeur doit s√©lectionner le basculement pour les lectures.  Nous le faisons avec le zonage. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c3/5bf/dbd/5c35bfdbddd67fe8217f06730673bd43.png"><br><br>  Nous attribuons un num√©ro de zone √† chaque r√©plique et √† chaque routeur et d√©finissons une table dans laquelle nous indiquons la distance entre chaque paire de zones.  Lorsque le routeur d√©cide o√π envoyer une demande de lecture, il s√©lectionne une r√©plique dans la zone la plus proche de la sienne. <br><br>  A quoi cela ressemble dans la configuration: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/445/799/5ae/4457995ae5c7bc1761684cf7d4f3b2e4.png"><br><br>  Dans le cas g√©n√©ral, vous pouvez faire r√©f√©rence √† une r√©plique arbitraire, mais si le cluster est grand et complexe, tr√®s distribu√©, le zonage est tr√®s utile.  Diff√©rents racks de serveurs peuvent √™tre des zones, afin de ne pas charger le r√©seau de trafic.  Ou il peut s'agir de points g√©ographiquement √©loign√©s les uns des autres. <br><br>  Le zonage permet √©galement de varier les performances des r√©pliques.  Par exemple, dans chaque jeu de r√©plicas, nous avons un r√©plica de sauvegarde, qui ne doit pas accepter les demandes, mais uniquement stocker une copie des donn√©es.  Ensuite, nous le faisons dans la zone, qui sera tr√®s loin de tous les routeurs du tableau, et ils se tourneront vers elle dans le cas le plus extr√™me. <br><br><h1>  Enregistrement de l'instabilit√© </h1><br>  Puisque nous parlons de basculement en lecture, qu'en est-il du basculement en √©criture lors du changement de l'assistant?  Ici, VShard n'est pas si rose: l'√©lection d'un nouveau ma√Ætre n'y est pas impl√©ment√©e, vous devrez le faire vous-m√™me.  Lorsque nous l'avons s√©lectionn√© d'une mani√®re ou d'une autre, il est n√©cessaire que cette instance prenne maintenant l'autorit√© du ma√Ætre.  Nous mettons √† jour la configuration en sp√©cifiant <code>master = false</code> pour l'ancien master, et <code>master = true</code> pour le nouveau, l'appliquons via VShard.storage.cfg et le roulons vers le stockage.  Ensuite, tout se passe automatiquement.  L'ancien ma√Ætre arr√™te d'accepter les demandes d'√©criture et commence √† se synchroniser avec le nouveau, car il peut y avoir des donn√©es qui ont d√©j√† √©t√© appliqu√©es sur l'ancien ma√Ætre, mais la nouvelle n'est pas encore arriv√©e.  Apr√®s cela, le nouveau ma√Ætre entre dans le r√¥le et commence √† accepter les demandes, et l'ancien ma√Ætre devient une r√©plique.  Voici comment fonctionne le basculement en √©criture dans VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><h1>  Comment maintenant suivre toute cette vari√©t√© d'√©v√©nements? </h1><br>  Dans le cas g√©n√©ral, deux poign√©es suffisent - <code>VShard.storage.info</code> et <code>VShard.router.info</code> . <br><br>  VShard.storage.info affiche des informations dans plusieurs sections. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  Le premier est la section de r√©plication.  L'√©tat du jeu de r√©plicas auquel vous avez appliqu√© cette fonction s'affiche: quel d√©calage de r√©plication il a, avec qui il a des connexions et avec qui il n'est pas disponible, qui est disponible et non disponible, quel assistant est configur√© pour quoi, etc. <br><br>  Dans la section Bucket, vous pouvez voir en temps r√©el combien de bucket'ov se d√©placent actuellement vers le jeu de r√©plicas actuel, combien le quittent, combien y travaillent actuellement, combien sont marqu√©s comme ordures, combien sont attach√©s. <br><br>  La section Alert est un tel m√©li-m√©lo de tous les probl√®mes que VShard a pu d√©terminer ind√©pendamment: le ma√Ætre n'est pas configur√©, le niveau de redondance est insuffisant, le ma√Ætre est l√†, mais toutes les r√©pliques ont √©chou√©, etc. <br><br>  Et la derni√®re section est une lumi√®re qui s'allume en rouge lorsque les choses deviennent vraiment mauvaises.  C'est un nombre de z√©ro √† trois, plus c'est pire. <br><br>  VShard.router.info a les m√™mes sections, mais elles signifient un peu diff√©rentes. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  La premi√®re section est la r√©plication.      ,    :    ,  replica set'  ,          ,   ,   replica set'  bucket'     ,     . <br><br>   Bucket    bucket',              ;    bucket'   ;  ,       replica set'. <br><br>   Alert,  ,   ,   failover,   bucket'. <br><br> ,         . <br><br><h1>     VShard? </h1><br>  ‚Äî    bucket'.       <code>int32_max</code> ?     bucket'   ‚Äî  30      16   .     bucket',     .           bucket',          bucket'.    ,          . <br><br>  ‚Äî   -   <code>bucket id</code> .    ,    -   ,   bucket ‚Äî           .      ,   bucket'   ,  VShard    bucket'.       -,      bucket'  bucket,  -.    . <br><br><h1>  R√©sum√© </h1><br> Vshard : <br><br><ul><li>  ; </li><li>  ; </li><li>    ; </li><li>  read failover; </li><li>    bucket'. </li></ul><br> VShard   .  -    .  ‚Äî  <b>   </b> .     ,       .           . <br><br>  ‚Äî <b>lock-free  bucket'</b> .   ,       bucket'      .      ,     . <br><br>  ‚Äî <b>  </b> .          : -    ,   ,    ?        . <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  </a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436916/">https://habr.com/ru/post/fr436916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436904/index.html">Seuil de 32 Ko pour les donn√©es dans la ROM des microcontr√¥leurs AVR</a></li>
<li><a href="../fr436908/index.html">6 fa√ßons de masquer des donn√©es dans une application Android</a></li>
<li><a href="../fr436910/index.html">Conseils pour cr√©er des workflows personnalis√©s dans GitLab CI</a></li>
<li><a href="../fr436912/index.html">Tendances CRM 2019: amusant √† lire, dangereux √† croire</a></li>
<li><a href="../fr436914/index.html">Probl√®mes de croissance de d√©marrage - Surveillance</a></li>
<li><a href="../fr436918/index.html">Cr√©ation d'un jeu pour Game Boy, partie 2</a></li>
<li><a href="../fr436920/index.html">Transpilateur PAS2JS de Pascal √† JavaScript: incompatible avec Delphi et solutions</a></li>
<li><a href="../fr436922/index.html">Optimisation du temps de d√©marrage de Prometheus 2.6.0 avec pprof</a></li>
<li><a href="../fr436924/index.html">Quelques mots sur l'organisation des concours de robotique</a></li>
<li><a href="../fr436926/index.html">H√©ros de l'authentification √† deux facteurs, ou comment "marcher dans la peau des autres"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>