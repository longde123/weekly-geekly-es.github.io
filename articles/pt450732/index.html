<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☝🏽 🖖🏻 🧑🏿‍🤝‍🧑🏽 Entendo, significa que existo: uma revisão do Deep Learning in Computer Vision (parte 1) 🚜 🧙🏼 🏮</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Visão computacional. Agora eles falam muito sobre isso, onde é aplicado e implementado muito. E de alguma forma, há algum tempo, não havia artigos de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Entendo, significa que existo: uma revisão do Deep Learning in Computer Vision (parte 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Visão computacional.  Agora eles falam muito sobre isso, onde é aplicado e implementado muito.  E de alguma forma, há algum tempo, não havia artigos de revisão sobre Habré no CV, com exemplos de arquiteturas e tarefas modernas.  Mas existem muitos, e eles são muito legais!  Se você está interessado no que está acontecendo no Computer Vision agora, não apenas do ponto de vista de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisas e artigos</a> , mas também do ponto de vista dos problemas aplicados, então você é bem-vindo ao gato.  Além disso, o artigo pode ser uma boa introdução para aqueles que há muito desejam começar a entender tudo isso, mas algo está no caminho;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="imagem"><br><a name="habracut"></a><br>  Hoje, na PhysTech, há uma colaboração ativa da "Academia" e dos parceiros industriais.  Em particular, existem muitos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">laboratórios interessantes</a> de empresas como Sberbank, Biocad, 1C, Tinkoff, MTS, Huawei na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Escola de Matemática Aplicada e Ciência da Computação</a> da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PhysTech</a> . <br><br>  Fui inspirado a escrever este artigo trabalhando no Laboratório de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sistemas Inteligentes Híbridos</a> , aberto pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VkusVill</a> .  O laboratório tem uma tarefa ambiciosa - construir uma loja que funcione sem caixas eletrônicos, principalmente com a ajuda da visão computacional.  Durante quase um ano de trabalho, tive a oportunidade de trabalhar em muitas tarefas de visão, que serão discutidas nessas duas partes. <br><br><div class="spoiler">  <b class="spoiler_title">Comprar sem mesas de dinheiro?</b>  <b class="spoiler_title">Em algum lugar eu já ouvi isso ..</b> <div class="spoiler_text">  Provavelmente, caro leitor, você pensou no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Amazon Go</a> .  Em certo sentido, a tarefa é repetir o sucesso deles, mas nossa decisão é mais sobre implementação do que construir uma loja desse tipo do zero por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">muito dinheiro</a> . <br></div></div><br>  Vamos nos mover de acordo com o plano: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Motivação e o que está acontecendo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Classificação como estilo de vida</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Arquiteturas de redes neurais convolucionais: 1000 maneiras de atingir um objetivo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Visualização de redes neurais convolucionais: mostre-me paixão</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Eu próprio sou uma espécie de cirurgião: extraímos recursos de redes neurais</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fique perto: aprendizado de representação para pessoas e indivíduos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: <s>detectando, avaliando a postura e reconhecendo ações</s> sem spoilers</a> </li></ol><br><a name="1"></a><h2>  Motivação e o que está acontecendo </h2><br><div class="spoiler">  <b class="spoiler_title">Para quem é o artigo?</b> <div class="spoiler_text">  O artigo se concentra mais em pessoas que já estão familiarizadas com aprendizado de máquina e redes neurais.  No entanto, aconselho a ler pelo menos as duas primeiras seções - de repente tudo ficará claro :) <br></div></div><br>  Em 2019, todo mundo está falando sobre inteligência artificial, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">quarta revolução industrial</a> e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">abordagem da humanidade para uma singularidade</a> .  Legal, legal, mas eu quero detalhes.  Afinal, somos técnicos curiosos que não acreditam em contos de fadas sobre IA, acreditamos em tarefas formais, matemática e programação.  Neste artigo, falaremos sobre casos específicos de uso da IA ​​muito moderna - o uso de aprendizado profundo (redes neurais convolucionais) em várias tarefas de visão computacional. <br><br>  Sim, falaremos especificamente sobre grades, às vezes mencionando algumas idéias de uma visão "clássica" (chamaremos o conjunto de métodos em visão que foram usados ​​antes das redes neurais, mas isso não significa que eles não sejam usados ​​agora). <br><br><div class="spoiler">  <b class="spoiler_title">Eu quero aprender a visão do computador a partir do zero</b> <div class="spoiler_text">  Eu recomendo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o curso de Anton Konushin "Introdução à visão computacional"</a> .  Pessoalmente, eu passei por sua contraparte no SHAD, que estabeleceu uma base sólida para entender o processamento de imagem e vídeo. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="imagem" width="300"></div><br>  Na minha opinião, a primeira aplicação realmente interessante de redes neurais na visão, abordada na mídia em 1993, é <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o reconhecimento de manuscrito por</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Jan LeCun</a> .  Agora ele é um dos principais IA na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisa de IA do Facebook</a> , a equipe deles já lançou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">muitas coisas úteis de código aberto</a> . <br><br>  Hoje, a visão é usada em muitas áreas.  Vou dar apenas alguns exemplos impressionantes: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="imagem" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="imagem" width="400" height="300"></div><br><br>  <i>Veículos não tripulados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tesla</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="imagem" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Análise de imagens médicas</a> e <a href="">previsão de câncer</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="imagem" width="500"></div><br><br>  <i>Consoles de jogos: Kinect 2.0 (embora também use informações detalhadas, ou seja, imagens RGB-D)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="imagem" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Reconhecimento de rosto: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Apple FaceID</a> (usando vários sensores)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="imagem" width="400"></div><br><br>  <i>Classificação do ponto de cara: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">máscaras do Snapchat</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="imagem" width="400"></div><br><br>  <i>Biometria dos movimentos da face e dos olhos (um exemplo do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto do FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="imagem" width="400"></div><br><br>  <i>Pesquisa por imagem: Yandex e Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="imagem" width="500"></div><br><br>  <i>Reconhecimento do texto na imagem ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reconhecimento óptico de caracteres</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="imagem" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="imagem" width="400"></div><br><br>  <i>Drones e robôs: recebendo e processando informações através da visão</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="imagem" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Odometria</a> : construção de um mapa e planejamento ao mover robôs</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="imagem" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhorando gráficos e texturas em videogames</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="imagem" width="200" height="300"></div><br><br>  <i>Tradução de imagens: Yandex e Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="imagem" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="imagem" width="500"></div><br><br>  <i>Realidade Aumentada: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Leap Motion (Projeto North Star)</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="imagem" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Transferência de estilo e textura: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PicsArt</a></i> <br><br>  Sem mencionar as inúmeras aplicações em várias tarefas internas das empresas.  O Facebook, por exemplo, também usa a visão para filtrar o conteúdo da mídia.  Métodos de visão computacional também são usados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">em testes de qualidade / danos na indústria</a> . <br><br>  A realidade aumentada aqui deve, de fato, receber atenção especial, uma vez <s>que não funciona</s> em um futuro próximo, e pode se tornar uma das principais áreas de aplicação da visão. <br><br>  Motivado.  Cobrado.  Vamos lá: <br><br><a name="2"></a><h2>  Classificação como estilo de vida </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="imagem"><br><br>  Como eu disse, nos anos 90, as redes foram disparadas à vista.  E eles filmaram uma tarefa específica - a tarefa de classificar imagens de números manuscritos (o famoso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conjunto de dados MNIST</a> ).  Historicamente, foi a tarefa de classificar imagens que se tornou a base para resolver quase todas as tarefas subseqüentes em visão.  Considere um exemplo específico: <br><br>  <b>Tarefa</b> : Uma pasta com fotos é dada na entrada, cada foto tem um objeto específico: um gato, um cachorro ou uma pessoa (mesmo que não haja fotos de "lixo", é uma tarefa super-não-vital, mas você precisa começar em algum lugar).  Você precisa decompor as imagens em três pastas: <code>/cats</code> , <code>/dogs</code> e <s><code>/leather_bags</code></s> <code>/humans</code> , colocando apenas fotos com os objetos correspondentes em cada pasta. <br><br><div class="spoiler">  <b class="spoiler_title">O que é uma foto / foto?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="imagem"><br>  Em quase todos os lugares da visão, é comum trabalhar com imagens no formato RGB.  Cada imagem possui uma altura (H), uma largura (W) e uma profundidade de 3 (cores).  Assim, uma imagem pode ser representada como um tensor da dimensão HxWx3 (cada pixel é um conjunto de três números - valores de intensidade nos canais). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Imagine que ainda não estamos familiarizados com a visão computacional, mas sabemos que o aprendizado de máquina.  As imagens são simplesmente tensores numéricos na memória do computador.  Formalizamos a tarefa em termos de aprendizado de máquina: objetos são figuras, seus sinais são valores em pixels, a resposta para cada um dos objetos é um rótulo de classe (gato, cachorro ou pessoa).  Esta é uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tarefa de classificação</a> pura. <br><br><div class="spoiler">  <b class="spoiler_title">Se agora se tornou difícil ..</b> <div class="spoiler_text">  ... então é melhor ler primeiro os 4 primeiros artigos do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenDataScience ML Open Course</a> e ler um artigo mais introdutório sobre visão, por exemplo, uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">boa palestra no Small ShAD</a> . <br></div></div><br>  Você pode usar alguns métodos da visão "clássica" ou do aprendizado de máquina "clássico", ou seja, não de uma rede neural.  Basicamente, esses métodos consistem em destacar as imagens de determinados recursos (pontos especiais) ou regiões locais que caracterizarão a imagem ("conjunto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">de palavras visuais</a> ").  Geralmente tudo se resume a algo como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SVM</a> sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> . <br><br>  Mas nos reunimos aqui para falar sobre redes neurais, por isso não queremos usar os sinais que inventamos, mas queremos que a rede faça tudo por nós.  Nosso classificador pegará os sinais de um objeto como uma entrada e retornará uma previsão (rótulo de classe).  Aqui, os valores de intensidade em pixels atuam como sinais (veja o modelo da imagem em <br>  spoiler acima).  Lembre-se de que uma imagem é um tensor de tamanho (Altura, Largura, 3) (se for colorida).  Ao aprender a entrar na grade, tudo isso geralmente é servido não por uma imagem e não por um conjunto de dados inteiro, mas por lotes, ou seja,  em pequenas porções de objetos (por exemplo, 64 imagens no lote). <br><br>  Assim, a rede recebe um tensor de entrada de tamanho (BATCH_SIZE, H, W, 3).  Você pode "expandir" cada figura em uma linha vetorial de números H * W * 3 e trabalhar com os valores em pixels, assim como os sinais no aprendizado de máquina, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Multilayer Perceptron (MLP) comum</a> faria exatamente isso, mas, francamente, é assim linha de base, já que trabalhar com pixels como uma linha de vetor não leva em conta, por exemplo, a invariância translacional dos objetos na imagem.  O mesmo gato pode estar no meio da foto e, no canto, o MLP não aprenderá esse padrão. <br><br>  Então, você precisa de algo mais inteligente, por exemplo, uma operação de convolução.  E isso é sobre visão moderna, sobre <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">redes neurais convolucionais</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">O código de treinamento da rede de convolução pode se parecer com isso (na estrutura do PyTorch)</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Como agora estamos falando sobre o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinamento com um professor</a> , precisamos de vários componentes para treinar uma rede neural: <br><br><ul><li>  Dados (já existe) </li><li>  Arquitetura de rede (destaque) </li><li>  Uma função de perda que informa como a rede neural deve aprender (aqui será <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entropia cruzada</a> ) </li><li>  Método de otimização (alterará o peso da rede na direção certa) </li><li>  Defina os parâmetros da arquitetura e do otimizador (por exemplo, tamanho da etapa do otimizador, número de neurônios em camadas, coeficientes de regularização) </li></ul><br>  É exatamente isso que é implementado no código: a própria rede neural convolucional é descrita na classe Net (). <br><br>  Se você quiser aprender lentamente e desde o início sobre pacotes e redes de convolução, recomendo uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">palestra na Deep Learning School (MIPT MIPT) (em russo)</a> sobre esse tópico e, é claro, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o curso de Stanford cs231n (em inglês)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Deep Learning School - o que é isso?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Escola de Aprendizagem Profunda</a> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Laboratório de Inovação FPMI MIPT</a> é uma organização que está ativamente envolvida no desenvolvimento de um curso aberto de língua russa em redes neurais.  No artigo, vou me referir a esses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tutoriais em vídeo</a> várias vezes. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="imagem" width="650"></div><br>  Em resumo, a operação de convolução permite encontrar padrões nas imagens com base em sua variabilidade.  Quando treinamos redes neurais convolucionais (por: Redes Neurais Convolucionais), na verdade, encontramos filtros de convolução (pesos de neurônios) que descrevem bem as imagens e tão bem que podemos determinar com precisão a classe a partir delas.  Muitas maneiras foram inventadas para construir essa rede.  Mais do que você pensa ... <br><br><a name="3"></a><h3>  Arquiteturas de redes neurais convolucionais: 1000 maneiras de atingir um objetivo </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="imagem" width="500"></div><br><br>  Sim, sim, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">outra revisão arquitetônica</a> .  Mas aqui vou tentar torná-lo o mais relevante possível! <br><br>  Primeiro, houve o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LeNet</a> , que ajudou Jan LeCun a reconhecer números em 1998.  Esta foi a primeira rede neural convolucional para classificação.  Sua principal característica era que ela basicamente começou a usar operações de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convolução e pool</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="imagem"><br><br>  Depois, houve uma pausa no desenvolvimento de grades, mas o hardware não parou; foram desenvolvidos cálculos eficazes em GPU e <abbr title="Álgebra Linear Acelerada">XLA</abbr> .  Em 2012, a AlexNet apareceu, ela participou da competição ILSVRC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio de reconhecimento visual em grande escala do ImageNet</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Uma pequena digressão sobre o ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O ImageNet</a> foi montado em 2012 e um subconjunto de milhares de fotos e 1000 classes foi usado para a competição ILSVRC.  Atualmente, a ImageNet possui ~ 14 milhões de fotos e 21.841 aulas (tiradas do site oficial), mas para a competição geralmente costumam selecionar apenas um subconjunto.  O ILSVRC se tornou a maior competição anual de classificação de imagens.  A propósito, recentemente descobrimos como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinar no ImageNet em questão de minutos</a> . <br><br>  Foi no ImageNet (no ILSVRC) de 2010 a 2018 que eles receberam redes <abbr title="Estado da arte">SOTA</abbr> na classificação de imagens.  É verdade que, desde 2016, as competições de localização, detecção e compreensão da cena, em vez de classificação, são mais relevantes. <br></div></div><br>  Normalmente, várias <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">revisões de arquitetura</a> lançam luz sobre as primeiras no ILSVRC de 2010 a 2016 e em algumas redes individuais.  Para não confundir a história, coloquei-os no spoiler abaixo, tentando enfatizar as idéias principais: <br><br><div class="spoiler">  <b class="spoiler_title">Arquitetura de 2012 a 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ano </th><th>  Artigo </th><th>  Ideia chave </th><th>  Peso </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Alexnet</a> </td><td>  use dois pacotes consecutivos;  divida o treinamento da rede em dois ramos paralelos </td><td>  240 MB </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Zfnet</a> </td><td>  tamanho do filtro, número de filtros em camadas </td><td>  - </td></tr><tr><td>  2013 </td><td>  <a href="">Overfeat</a> </td><td>  um dos primeiros detectores de redes neurais </td><td>  - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  profundidade da rede (13 a 19 camadas), o uso de vários blocos Conv-Conv-Pool com um tamanho de convolução menor (3x3) </td><td>  549MB (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">Início (v1) (também conhecido como GoogLeNet)</a> </td><td>  Convolução 1x1 (idéia da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">rede em rede</a> ), perdas auxiliares (ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">supervisão profunda</a> ), empilhamento das saídas de várias convoluções (bloco inicial) </td><td>  - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conexões residuais</a> , muito profundas (152 camadas ..) </td><td>  98 MB (ResNet-50), 232 MB (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  As idéias de todas essas arquiteturas (exceto a ZFNet, geralmente é mencionada pouco) ao mesmo tempo eram uma nova palavra em redes neurais para visão.  No entanto, após 2015, houve muitas melhorias mais importantes, por exemplo, Inception-ResNet, Xception, DenseNet, SENet.  Abaixo, tentei colecioná-los em um só lugar. <br><br><div class="spoiler">  <b class="spoiler_title">Arquitetura de 2015 a 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ano </th><th>  Artigo </th><th>  Ideia chave </th><th>  Peso </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Iniciação v2 e v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">decomposição de pacotes em pacotes 1xN e Nx1</a> </td><td>  92 MB </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Inception v4 e Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">combinação de Inception e ResNet</a> </td><td>  215 MB </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Resnext</a> </td><td>  2º lugar ILSVRC, o uso de muitos ramos (bloco de generalização "generalizado") </td><td>  - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convolução separável em profundidade</a> , pesa menos com precisão comparável à </td><td>  88 MB </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloco denso</a>  leve, mas preciso </td><td>  33 MB (DenseNet-121), 80 MB (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloco de aperto e excitação</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">46 MB (SENet-Inception), 440 MB (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  A maioria desses modelos de PyTorch pode ser encontrada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> , e existe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">uma coisa muito legal</a> . <br><br>  Você deve ter notado que a coisa toda pesa bastante (eu gostaria de 20 MB no máximo, ou até menos), enquanto hoje em dia eles usam dispositivos móveis em todos os lugares e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">IoT está</a> ganhando popularidade, o que significa que você também deseja usar grades lá. <br><br><div class="spoiler">  <b class="spoiler_title">Relação entre peso e velocidade do modelo</b> <div class="spoiler_text">  Como as redes neurais em si mesmas apenas multiplicam os tensores, o número de operações de multiplicação (leia-se: o número de pesos) afeta diretamente a velocidade do trabalho (se não for utilizado pós-processamento ou pré-processamento intensivo em mão-de-obra).  A velocidade da rede em si depende da implementação (estrutura), do hardware no qual está sendo executada e do tamanho da imagem de entrada. <br></div></div><br>  Os autores de muitos artigos seguiram o caminho de inventar arquiteturas rápidas, colecionei seus métodos sob o spoiler abaixo: <br><br><div class="spoiler">  <b class="spoiler_title">Arquitetura leve da CNN</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ano </th><th>  Artigo </th><th>  Ideia chave </th><th>  Peso </th><th>  Exemplo de implementação </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Compressão FireModule</a> </td><td>  0,5 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">obtida por uma pesquisa neural de arquiteturas, é uma rede da categoria AutoML</a> </td><td>  23 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conv conventual do grupo, embaralhamento do canal</a> </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convoluções separáveis ​​em profundidade e muitos outros truques</a> </td><td>  16 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Eu recomendo este artigo sobre Habré</a> </td><td>  14 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Squeezenext</a> </td><td>  veja as fotos no repositório original </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisa de arquitetura neural especificamente para dispositivos móveis usando RL</a> </td><td>  ~ 2 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet (v3)</a> </td><td>  ela saiu enquanto eu escrevia um artigo :) </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Os números em todas as tabelas <s>são retirados do teto</s> dos repositórios, da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tabela Aplicativos Keras</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deste artigo</a> . <br><br>  Você pergunta: “Por que você escreveu sobre todo esse“ zoológico ”de modelos?  E por que a tarefa da classificação?  Mas queremos ensinar as máquinas a ver, e a classificação é apenas um tipo de tarefa restrita ... ”.  O fato é que as redes neurais para detectar objetos, avaliar posturas / pontos, re-identificar e pesquisar em uma imagem usam exatamente os modelos de classificação como <b><abbr title="base, literalmente - a coluna vertebral">espinha dorsal</abbr></b> , e 80% do sucesso depende deles. <br><br>  Mas, de alguma forma, quero confiar mais na CNN, ou eles pensaram em caixas pretas, mas o que está "dentro" não é óbvio.  Para entender melhor o mecanismo de funcionamento das redes convolucionais, os pesquisadores propuseram o uso da visualização. <br><br><a name="4"></a><h3>  Visualização de redes neurais convolucionais: mostre-me paixão </h3><br>  Um passo importante para entender o que está acontecendo nas redes convolucionais é o artigo <a href="">“Visualizando e compreendendo redes convolucionais”</a> .  Nele, os autores propuseram várias maneiras de visualizar exatamente o que (em quais partes da imagem) os neurônios respondem às diferentes camadas da CNN (eu também recomendo assistir a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">uma palestra de Stanford sobre esse tópico</a> ).  Os resultados foram impressionantes: os autores mostraram que as primeiras camadas da rede convolucional respondem a algumas “coisas de baixo nível” pelo tipo de arestas / ângulos / linhas, e as últimas camadas já respondem a partes inteiras das imagens (veja a figura abaixo), ou seja, elas já carregam em si alguma semântica. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="imagem"></div><br><br>  Além disso, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto de visualização profunda da Cornell University e da empresa</a> avançou ainda mais na visualização, enquanto o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">famoso DeepDream</a> aprendeu a distorcer em um estilo interessante e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">viciante</a> (abaixo está uma foto do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="imagem" width="500"></div><br><br>  Em 2017, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo muito bom foi publicado no Distill</a> , no qual eles realizaram uma análise detalhada do que cada camada “vê” e, mais recentemente (em março de 2019), o Google inventou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atlas de ativação</a> : mapas exclusivos que podem ser construídos para cada camada de rede, o que nos aproxima da compreensão do quadro geral do trabalho da CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Se você quiser brincar com a visualização, eu recomendaria o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lucid</a> e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TensorSpace</a> . <br><br>  Ok, a CNN parece ser verdadeira até certo ponto.  Precisamos aprender como usar isso em outras tarefas, e não apenas na classificação.  Isso nos ajudará a extrair fotos do Embedding'ov e o Transfer Learning. <br><br><a name="5"></a><h2>  Eu próprio sou uma espécie de cirurgião: extraímos recursos de redes neurais </h2><br>  Imagine que existe uma imagem e queremos encontrar imagens com aparência visual (esta é, por exemplo, a pesquisa em uma imagem no Yandex.Pictures).  Anteriormente (antes das redes neurais), os engenheiros costumavam extrair recursos para isso, por exemplo, inventando algo que descreve bem a imagem e permite que ela seja comparada com outras.  Basicamente, esses métodos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> ) operam com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">gradientes de imagem</a> , geralmente esses itens são chamados de descritores de imagem "clássicos".  De particular interesse, refiro-me ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> e ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">curso de Anton Konushin</a> (isto não é publicidade, apenas um bom curso :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="imagem" width="500"></div><br><br>  Usando redes neurais, não podemos inventar esses recursos e heurísticas, mas treinamos adequadamente o modelo e, em seguida, <b>consideramos a saída de uma ou mais camadas da rede como sinais da imagem</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="imagem" width="650"></div><br>  Uma análise mais detalhada de todas as arquiteturas deixa claro que há duas etapas para classificação na CNN: <br>  1)  Camadas <b>extratoras de recursos</b> para extrair recursos informativos de imagens usando camadas convolucionais <br>  2)  Aprendendo sobre esses recursos Camadas classificadoras <b>totalmente conectadas (FC)</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="imagem" width="500"></div><br><br>  <b>A incorporação de imagens (recursos)</b> é apenas sobre o fato de que você pode pegar seus sinais após o extrator de recursos de uma rede neural convolucional (embora eles possam ser agregados de maneiras diferentes) como uma descrição informativa das imagens.  Ou seja, treinamos a rede para classificação e, em seguida, apenas saímos na frente das camadas de classificação.  Esses sinais são chamados de <i>recursos</i> , <i>descritores de redes neurais</i> ou <i>incorporação de figuras</i> (embora sejam geralmente aceitos na PNL, já que essa é uma visão, falarei sobre <i>características com</i> frequência).  Geralmente, esse é um tipo de vetor numérico, por exemplo, 128 números, com os quais você já pode trabalhar. <br><br><div class="spoiler">  <b class="spoiler_title">Mas e os codificadores automáticos?</b> <div class="spoiler_text">  Sim, de fato, os recursos podem ser obtidos pelos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">codificadores automáticos</a> .  Na minha prática, eles fizeram isso de maneiras diferentes, mas, por exemplo, em artigos sobre re-identificação (que serão discutidos mais adiante), mais frequentemente eles ainda usam recursos após o extrator, em vez de treinar o codificador automático para isso.  Parece-me que vale a pena realizar experimentos em ambas as direções, se a questão é o que funciona melhor. <br></div></div><br>  Assim, o pipeline para resolver o <b>problema de busca por imagem</b> pode ser organizado de maneira simples: executamos as imagens pela CNN, pegamos sinais nas camadas desejadas e comparamos esses recursos entre si a partir de imagens diferentes.  Por exemplo, simplesmente consideramos a distância euclidiana desses vetores. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="imagem" width="500"></div><br><br>  <b>O Transfer Learning</b> é uma técnica bem conhecida para o treinamento eficaz de redes neurais que já são treinadas em um conjunto de dados específico para sua tarefa.  Freqüentemente, eles também dizem Ajuste fino em vez de Transfer Learning, nas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">notas do curso de Stanford, cs231.</a> Esses conceitos são compartilhados, eles dizem: Transfer Learning é uma ideia geral e o Ajuste fino é uma das implementações da técnica.  Isso não é tão importante para nós no futuro, o principal é entender que podemos apenas treinar a rede para prever bem o novo conjunto de dados, começando não com pesos aleatórios, mas com aqueles treinados em algum tipo ImageNet grande.  Isso é especialmente verdade quando há poucos dados e você deseja resolver o problema qualitativamente. <br><br><div class="spoiler">  <b class="spoiler_title">Saiba mais sobre o Transfer Learning</b> <div class="spoiler_text">  <a href="">Artigo original</a> , mas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">por que ler muito texto, se você pode assistir ao vídeo</a> <br></div></div><br>  No entanto, simplesmente pegar os recursos necessários e fazer um treinamento adicional do conjunto de dados para o conjunto de dados pode não ser suficiente, por exemplo, para tarefas de pesquisa de pessoas / pessoas semelhantes / algo específico.  Fotos da mesma pessoa visualmente às vezes podem ser ainda mais diferentes do que fotografias de pessoas diferentes.  É necessário fazer a rede destacar exatamente os sinais que são inerentes a uma pessoa / objeto, mesmo que seja difícil para nós fazer isso com nossos olhos.  Bem-vindo ao mundo do <b>aprendizado</b> de <b>representação</b> . <br><br><a name="6"></a><h2>  Fique perto: aprendizado de representação para pessoas e indivíduos </h2><br><div class="spoiler">  <b class="spoiler_title">Nota de terminologia</b> <div class="spoiler_text">  Se você lê artigos científicos, às vezes parece que alguns autores entendem a frase <b>aprendizado de métricas de maneira</b> diferente, e não há consenso sobre quais métodos chamar de aprendizado de métricas e quais não são.  Por isso, neste artigo, decidi evitar essa frase em particular e usei um <b>aprendizado de representação</b> mais lógica, alguns leitores podem não concordar com isso - terei prazer em discutir nos comentários. <br></div></div><br>  Definimos as tarefas: <br><br><ul><li>  <b>Tarefa 1</b> : existe uma galeria (conjunto) de fotografias dos rostos das pessoas, queremos que a rede seja capaz de responder de acordo com uma nova foto com o nome de uma pessoa da galeria (supostamente é essa) ou disse que não existe essa pessoa na galeria (e, talvez, acrescentemos pessoa nova) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>Tarefa 2</b> : a mesma coisa, mas não estamos trabalhando com fotografias de rostos, mas com pessoas de tamanho completo <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  A primeira tarefa é geralmente chamada de <b>reconhecimento facial</b> , a segunda - <b>re-identificação</b> (abreviada como <i>Reid</i> ).  Combinei-os em um bloco, porque suas soluções usam idéias semelhantes hoje: para aprender a incorporar imagens efetivas que podem lidar com situações bastante difíceis, hoje eles usam diferentes tipos de perdas, como, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perda de trigêmeos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perda de quádruplos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perda do centro contrastivo, perda de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cosseno</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Ainda existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">redes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">siamesas</a> maravilhosas, mas eu sinceramente não as usei.  A propósito, não apenas a perda em si “decide”, mas como provar pares de pontos positivos e negativos para ela, enfatizam os autores do artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Amostragem na aprendizagem de incorporação profunda</a> . <br><br>  A essência de todas essas perdas e redes siamesas é simples - queremos que as imagens de uma classe (pessoa) no espaço latente de recursos (casamentos) sejam "próximas" e de diferentes classes (pessoas) sejam "distantes".  A proximidade é geralmente medida da seguinte forma: capturas de imagens de uma rede neural são realizadas (por exemplo, um vetor de 128 números) e consideramos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">distância euclidiana</a> usual entre esses vetores ou a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">proximidade</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cosseno.</a>  Como medir, é melhor escolher em seu conjunto de dados / tarefa. <br><br>  Uma representação esquemática de um pipeline de solução de problemas no aprendizado de representação é mais ou menos assim: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Mas para ser mais preciso, assim</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        —   .   ,     —     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> <br></div></div><br><br>   <b> </b>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">- ( <b>MUST READ!</b> )</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FaceNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ArcFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CosFace</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="imagem" width="700"></div><br><br>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dlib</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FaceNet repo</a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">       </a> . ,      ArcFace  CosFace (  ,    - ,    - ). <br><br>        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ,   ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="detecção humana cortada de uma fotografia grande"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> Reid —    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 2016 </a> . ,     ,    —   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="imagem" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   —    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TorchReid</a> .      —   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  ,   </a> ).   ?  … <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="imagem" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt450732/">https://habr.com/ru/post/pt450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt450720/index.html">Como desenvolver um aplicativo amigável</a></li>
<li><a href="../pt450724/index.html">Apresentando o Python para camaradas superando a “linguagem A vs. V” língua B "e outros preconceitos</a></li>
<li><a href="../pt450726/index.html">Criando uma ferramenta para escrever de forma rápida e eficiente autotestes no Selenium</a></li>
<li><a href="../pt450728/index.html">NLog: regras e filtros</a></li>
<li><a href="../pt450730/index.html">ok.tech: frontend meetup</a></li>
<li><a href="../pt450734/index.html">Fuzzing é um passo importante no desenvolvimento seguro</a></li>
<li><a href="../pt450736/index.html">"Isolar a Internet é muito mais fácil e mais barato do que fornecer bloqueio externo".</a></li>
<li><a href="../pt450738/index.html">Robôs no data center: como a inteligência artificial pode ser útil?</a></li>
<li><a href="../pt450740/index.html">Base de lâmpada inteligente REDMOND - adicione à casa inteligente</a></li>
<li><a href="../pt450744/index.html">Infraestrutura de bicicleta em Minsk para um expat em TI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>