<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèΩ üññüèª üßëüèø‚Äçü§ù‚ÄçüßëüèΩ Entendo, significa que existo: uma revis√£o do Deep Learning in Computer Vision (parte 1) üöú üßôüèº üèÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vis√£o computacional. Agora eles falam muito sobre isso, onde √© aplicado e implementado muito. E de alguma forma, h√° algum tempo, n√£o havia artigos de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Entendo, significa que existo: uma revis√£o do Deep Learning in Computer Vision (parte 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Vis√£o computacional.  Agora eles falam muito sobre isso, onde √© aplicado e implementado muito.  E de alguma forma, h√° algum tempo, n√£o havia artigos de revis√£o sobre Habr√© no CV, com exemplos de arquiteturas e tarefas modernas.  Mas existem muitos, e eles s√£o muito legais!  Se voc√™ est√° interessado no que est√° acontecendo no Computer Vision agora, n√£o apenas do ponto de vista de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisas e artigos</a> , mas tamb√©m do ponto de vista dos problemas aplicados, ent√£o voc√™ √© bem-vindo ao gato.  Al√©m disso, o artigo pode ser uma boa introdu√ß√£o para aqueles que h√° muito desejam come√ßar a entender tudo isso, mas algo est√° no caminho;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="imagem"><br><a name="habracut"></a><br>  Hoje, na PhysTech, h√° uma colabora√ß√£o ativa da "Academia" e dos parceiros industriais.  Em particular, existem muitos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">laborat√≥rios interessantes</a> de empresas como Sberbank, Biocad, 1C, Tinkoff, MTS, Huawei na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Escola de Matem√°tica Aplicada e Ci√™ncia da Computa√ß√£o</a> da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PhysTech</a> . <br><br>  Fui inspirado a escrever este artigo trabalhando no Laborat√≥rio de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sistemas Inteligentes H√≠bridos</a> , aberto pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VkusVill</a> .  O laborat√≥rio tem uma tarefa ambiciosa - construir uma loja que funcione sem caixas eletr√¥nicos, principalmente com a ajuda da vis√£o computacional.  Durante quase um ano de trabalho, tive a oportunidade de trabalhar em muitas tarefas de vis√£o, que ser√£o discutidas nessas duas partes. <br><br><div class="spoiler">  <b class="spoiler_title">Comprar sem mesas de dinheiro?</b>  <b class="spoiler_title">Em algum lugar eu j√° ouvi isso ..</b> <div class="spoiler_text">  Provavelmente, caro leitor, voc√™ pensou no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Amazon Go</a> .  Em certo sentido, a tarefa √© repetir o sucesso deles, mas nossa decis√£o √© mais sobre implementa√ß√£o do que construir uma loja desse tipo do zero por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">muito dinheiro</a> . <br></div></div><br>  Vamos nos mover de acordo com o plano: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Motiva√ß√£o e o que est√° acontecendo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Classifica√ß√£o como estilo de vida</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Arquiteturas de redes neurais convolucionais: 1000 maneiras de atingir um objetivo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Visualiza√ß√£o de redes neurais convolucionais: mostre-me paix√£o</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Eu pr√≥prio sou uma esp√©cie de cirurgi√£o: extra√≠mos recursos de redes neurais</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fique perto: aprendizado de representa√ß√£o para pessoas e indiv√≠duos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: <s>detectando, avaliando a postura e reconhecendo a√ß√µes</s> sem spoilers</a> </li></ol><br><a name="1"></a><h2>  Motiva√ß√£o e o que est√° acontecendo </h2><br><div class="spoiler">  <b class="spoiler_title">Para quem √© o artigo?</b> <div class="spoiler_text">  O artigo se concentra mais em pessoas que j√° est√£o familiarizadas com aprendizado de m√°quina e redes neurais.  No entanto, aconselho a ler pelo menos as duas primeiras se√ß√µes - de repente tudo ficar√° claro :) <br></div></div><br>  Em 2019, todo mundo est√° falando sobre intelig√™ncia artificial, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">quarta revolu√ß√£o industrial</a> e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">abordagem da humanidade para uma singularidade</a> .  Legal, legal, mas eu quero detalhes.  Afinal, somos t√©cnicos curiosos que n√£o acreditam em contos de fadas sobre IA, acreditamos em tarefas formais, matem√°tica e programa√ß√£o.  Neste artigo, falaremos sobre casos espec√≠ficos de uso da IA ‚Äã‚Äãmuito moderna - o uso de aprendizado profundo (redes neurais convolucionais) em v√°rias tarefas de vis√£o computacional. <br><br>  Sim, falaremos especificamente sobre grades, √†s vezes mencionando algumas id√©ias de uma vis√£o "cl√°ssica" (chamaremos o conjunto de m√©todos em vis√£o que foram usados ‚Äã‚Äãantes das redes neurais, mas isso n√£o significa que eles n√£o sejam usados ‚Äã‚Äãagora). <br><br><div class="spoiler">  <b class="spoiler_title">Eu quero aprender a vis√£o do computador a partir do zero</b> <div class="spoiler_text">  Eu recomendo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o curso de Anton Konushin "Introdu√ß√£o √† vis√£o computacional"</a> .  Pessoalmente, eu passei por sua contraparte no SHAD, que estabeleceu uma base s√≥lida para entender o processamento de imagem e v√≠deo. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="imagem" width="300"></div><br>  Na minha opini√£o, a primeira aplica√ß√£o realmente interessante de redes neurais na vis√£o, abordada na m√≠dia em 1993, √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o reconhecimento de manuscrito por</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Jan LeCun</a> .  Agora ele √© um dos principais IA na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisa de IA do Facebook</a> , a equipe deles j√° lan√ßou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">muitas coisas √∫teis de c√≥digo aberto</a> . <br><br>  Hoje, a vis√£o √© usada em muitas √°reas.  Vou dar apenas alguns exemplos impressionantes: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="imagem" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="imagem" width="400" height="300"></div><br><br>  <i>Ve√≠culos n√£o tripulados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tesla</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="imagem" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">An√°lise de imagens m√©dicas</a> e <a href="">previs√£o de c√¢ncer</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="imagem" width="500"></div><br><br>  <i>Consoles de jogos: Kinect 2.0 (embora tamb√©m use informa√ß√µes detalhadas, ou seja, imagens RGB-D)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="imagem" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Reconhecimento de rosto: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Apple FaceID</a> (usando v√°rios sensores)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="imagem" width="400"></div><br><br>  <i>Classifica√ß√£o do ponto de cara: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">m√°scaras do Snapchat</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="imagem" width="400"></div><br><br>  <i>Biometria dos movimentos da face e dos olhos (um exemplo do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto do FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="imagem" width="400"></div><br><br>  <i>Pesquisa por imagem: Yandex e Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="imagem" width="500"></div><br><br>  <i>Reconhecimento do texto na imagem ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reconhecimento √≥ptico de caracteres</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="imagem" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="imagem" width="400"></div><br><br>  <i>Drones e rob√¥s: recebendo e processando informa√ß√µes atrav√©s da vis√£o</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="imagem" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Odometria</a> : constru√ß√£o de um mapa e planejamento ao mover rob√¥s</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="imagem" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhorando gr√°ficos e texturas em videogames</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="imagem" width="200" height="300"></div><br><br>  <i>Tradu√ß√£o de imagens: Yandex e Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="imagem" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="imagem" width="500"></div><br><br>  <i>Realidade Aumentada: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Leap Motion (Projeto North Star)</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="imagem" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Transfer√™ncia de estilo e textura: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PicsArt</a></i> <br><br>  Sem mencionar as in√∫meras aplica√ß√µes em v√°rias tarefas internas das empresas.  O Facebook, por exemplo, tamb√©m usa a vis√£o para filtrar o conte√∫do da m√≠dia.  M√©todos de vis√£o computacional tamb√©m s√£o usados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">em testes de qualidade / danos na ind√∫stria</a> . <br><br>  A realidade aumentada aqui deve, de fato, receber aten√ß√£o especial, uma vez <s>que n√£o funciona</s> em um futuro pr√≥ximo, e pode se tornar uma das principais √°reas de aplica√ß√£o da vis√£o. <br><br>  Motivado.  Cobrado.  Vamos l√°: <br><br><a name="2"></a><h2>  Classifica√ß√£o como estilo de vida </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="imagem"><br><br>  Como eu disse, nos anos 90, as redes foram disparadas √† vista.  E eles filmaram uma tarefa espec√≠fica - a tarefa de classificar imagens de n√∫meros manuscritos (o famoso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conjunto de dados MNIST</a> ).  Historicamente, foi a tarefa de classificar imagens que se tornou a base para resolver quase todas as tarefas subseq√ºentes em vis√£o.  Considere um exemplo espec√≠fico: <br><br>  <b>Tarefa</b> : Uma pasta com fotos √© dada na entrada, cada foto tem um objeto espec√≠fico: um gato, um cachorro ou uma pessoa (mesmo que n√£o haja fotos de "lixo", √© uma tarefa super-n√£o-vital, mas voc√™ precisa come√ßar em algum lugar).  Voc√™ precisa decompor as imagens em tr√™s pastas: <code>/cats</code> , <code>/dogs</code> e <s><code>/leather_bags</code></s> <code>/humans</code> , colocando apenas fotos com os objetos correspondentes em cada pasta. <br><br><div class="spoiler">  <b class="spoiler_title">O que √© uma foto / foto?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="imagem"><br>  Em quase todos os lugares da vis√£o, √© comum trabalhar com imagens no formato RGB.  Cada imagem possui uma altura (H), uma largura (W) e uma profundidade de 3 (cores).  Assim, uma imagem pode ser representada como um tensor da dimens√£o HxWx3 (cada pixel √© um conjunto de tr√™s n√∫meros - valores de intensidade nos canais). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Imagine que ainda n√£o estamos familiarizados com a vis√£o computacional, mas sabemos que o aprendizado de m√°quina.  As imagens s√£o simplesmente tensores num√©ricos na mem√≥ria do computador.  Formalizamos a tarefa em termos de aprendizado de m√°quina: objetos s√£o figuras, seus sinais s√£o valores em pixels, a resposta para cada um dos objetos √© um r√≥tulo de classe (gato, cachorro ou pessoa).  Esta √© uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tarefa de classifica√ß√£o</a> pura. <br><br><div class="spoiler">  <b class="spoiler_title">Se agora se tornou dif√≠cil ..</b> <div class="spoiler_text">  ... ent√£o √© melhor ler primeiro os 4 primeiros artigos do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenDataScience ML Open Course</a> e ler um artigo mais introdut√≥rio sobre vis√£o, por exemplo, uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">boa palestra no Small ShAD</a> . <br></div></div><br>  Voc√™ pode usar alguns m√©todos da vis√£o "cl√°ssica" ou do aprendizado de m√°quina "cl√°ssico", ou seja, n√£o de uma rede neural.  Basicamente, esses m√©todos consistem em destacar as imagens de determinados recursos (pontos especiais) ou regi√µes locais que caracterizar√£o a imagem ("conjunto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">de palavras visuais</a> ").  Geralmente tudo se resume a algo como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SVM</a> sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> . <br><br>  Mas nos reunimos aqui para falar sobre redes neurais, por isso n√£o queremos usar os sinais que inventamos, mas queremos que a rede fa√ßa tudo por n√≥s.  Nosso classificador pegar√° os sinais de um objeto como uma entrada e retornar√° uma previs√£o (r√≥tulo de classe).  Aqui, os valores de intensidade em pixels atuam como sinais (veja o modelo da imagem em <br>  spoiler acima).  Lembre-se de que uma imagem √© um tensor de tamanho (Altura, Largura, 3) (se for colorida).  Ao aprender a entrar na grade, tudo isso geralmente √© servido n√£o por uma imagem e n√£o por um conjunto de dados inteiro, mas por lotes, ou seja,  em pequenas por√ß√µes de objetos (por exemplo, 64 imagens no lote). <br><br>  Assim, a rede recebe um tensor de entrada de tamanho (BATCH_SIZE, H, W, 3).  Voc√™ pode "expandir" cada figura em uma linha vetorial de n√∫meros H * W * 3 e trabalhar com os valores em pixels, assim como os sinais no aprendizado de m√°quina, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Multilayer Perceptron (MLP) comum</a> faria exatamente isso, mas, francamente, √© assim linha de base, j√° que trabalhar com pixels como uma linha de vetor n√£o leva em conta, por exemplo, a invari√¢ncia translacional dos objetos na imagem.  O mesmo gato pode estar no meio da foto e, no canto, o MLP n√£o aprender√° esse padr√£o. <br><br>  Ent√£o, voc√™ precisa de algo mais inteligente, por exemplo, uma opera√ß√£o de convolu√ß√£o.  E isso √© sobre vis√£o moderna, sobre <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">redes neurais convolucionais</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">O c√≥digo de treinamento da rede de convolu√ß√£o pode se parecer com isso (na estrutura do PyTorch)</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Como agora estamos falando sobre o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinamento com um professor</a> , precisamos de v√°rios componentes para treinar uma rede neural: <br><br><ul><li>  Dados (j√° existe) </li><li>  Arquitetura de rede (destaque) </li><li>  Uma fun√ß√£o de perda que informa como a rede neural deve aprender (aqui ser√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entropia cruzada</a> ) </li><li>  M√©todo de otimiza√ß√£o (alterar√° o peso da rede na dire√ß√£o certa) </li><li>  Defina os par√¢metros da arquitetura e do otimizador (por exemplo, tamanho da etapa do otimizador, n√∫mero de neur√¥nios em camadas, coeficientes de regulariza√ß√£o) </li></ul><br>  √â exatamente isso que √© implementado no c√≥digo: a pr√≥pria rede neural convolucional √© descrita na classe Net (). <br><br>  Se voc√™ quiser aprender lentamente e desde o in√≠cio sobre pacotes e redes de convolu√ß√£o, recomendo uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">palestra na Deep Learning School (MIPT MIPT) (em russo)</a> sobre esse t√≥pico e, √© claro, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o curso de Stanford cs231n (em ingl√™s)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Deep Learning School - o que √© isso?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Escola de Aprendizagem Profunda</a> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Laborat√≥rio de Inova√ß√£o FPMI MIPT</a> √© uma organiza√ß√£o que est√° ativamente envolvida no desenvolvimento de um curso aberto de l√≠ngua russa em redes neurais.  No artigo, vou me referir a esses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tutoriais em v√≠deo</a> v√°rias vezes. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="imagem" width="650"></div><br>  Em resumo, a opera√ß√£o de convolu√ß√£o permite encontrar padr√µes nas imagens com base em sua variabilidade.  Quando treinamos redes neurais convolucionais (por: Redes Neurais Convolucionais), na verdade, encontramos filtros de convolu√ß√£o (pesos de neur√¥nios) que descrevem bem as imagens e t√£o bem que podemos determinar com precis√£o a classe a partir delas.  Muitas maneiras foram inventadas para construir essa rede.  Mais do que voc√™ pensa ... <br><br><a name="3"></a><h3>  Arquiteturas de redes neurais convolucionais: 1000 maneiras de atingir um objetivo </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="imagem" width="500"></div><br><br>  Sim, sim, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">outra revis√£o arquitet√¥nica</a> .  Mas aqui vou tentar torn√°-lo o mais relevante poss√≠vel! <br><br>  Primeiro, houve o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LeNet</a> , que ajudou Jan LeCun a reconhecer n√∫meros em 1998.  Esta foi a primeira rede neural convolucional para classifica√ß√£o.  Sua principal caracter√≠stica era que ela basicamente come√ßou a usar opera√ß√µes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convolu√ß√£o e pool</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="imagem"><br><br>  Depois, houve uma pausa no desenvolvimento de grades, mas o hardware n√£o parou; foram desenvolvidos c√°lculos eficazes em GPU e <abbr title="√Ålgebra Linear Acelerada">XLA</abbr> .  Em 2012, a AlexNet apareceu, ela participou da competi√ß√£o ILSVRC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desafio de reconhecimento visual em grande escala do ImageNet</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Uma pequena digress√£o sobre o ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O ImageNet</a> foi montado em 2012 e um subconjunto de milhares de fotos e 1000 classes foi usado para a competi√ß√£o ILSVRC.  Atualmente, a ImageNet possui ~ 14 milh√µes de fotos e 21.841 aulas (tiradas do site oficial), mas para a competi√ß√£o geralmente costumam selecionar apenas um subconjunto.  O ILSVRC se tornou a maior competi√ß√£o anual de classifica√ß√£o de imagens.  A prop√≥sito, recentemente descobrimos como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinar no ImageNet em quest√£o de minutos</a> . <br><br>  Foi no ImageNet (no ILSVRC) de 2010 a 2018 que eles receberam redes <abbr title="Estado da arte">SOTA</abbr> na classifica√ß√£o de imagens.  √â verdade que, desde 2016, as competi√ß√µes de localiza√ß√£o, detec√ß√£o e compreens√£o da cena, em vez de classifica√ß√£o, s√£o mais relevantes. <br></div></div><br>  Normalmente, v√°rias <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">revis√µes de arquitetura</a> lan√ßam luz sobre as primeiras no ILSVRC de 2010 a 2016 e em algumas redes individuais.  Para n√£o confundir a hist√≥ria, coloquei-os no spoiler abaixo, tentando enfatizar as id√©ias principais: <br><br><div class="spoiler">  <b class="spoiler_title">Arquitetura de 2012 a 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ano </th><th>  Artigo </th><th>  Ideia chave </th><th>  Peso </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Alexnet</a> </td><td>  use dois pacotes consecutivos;  divida o treinamento da rede em dois ramos paralelos </td><td>  240 MB </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Zfnet</a> </td><td>  tamanho do filtro, n√∫mero de filtros em camadas </td><td>  - </td></tr><tr><td>  2013 </td><td>  <a href="">Overfeat</a> </td><td>  um dos primeiros detectores de redes neurais </td><td>  - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  profundidade da rede (13 a 19 camadas), o uso de v√°rios blocos Conv-Conv-Pool com um tamanho de convolu√ß√£o menor (3x3) </td><td>  549MB (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">In√≠cio (v1) (tamb√©m conhecido como GoogLeNet)</a> </td><td>  Convolu√ß√£o 1x1 (id√©ia da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">rede em rede</a> ), perdas auxiliares (ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">supervis√£o profunda</a> ), empilhamento das sa√≠das de v√°rias convolu√ß√µes (bloco inicial) </td><td>  - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conex√µes residuais</a> , muito profundas (152 camadas ..) </td><td>  98 MB (ResNet-50), 232 MB (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  As id√©ias de todas essas arquiteturas (exceto a ZFNet, geralmente √© mencionada pouco) ao mesmo tempo eram uma nova palavra em redes neurais para vis√£o.  No entanto, ap√≥s 2015, houve muitas melhorias mais importantes, por exemplo, Inception-ResNet, Xception, DenseNet, SENet.  Abaixo, tentei colecion√°-los em um s√≥ lugar. <br><br><div class="spoiler">  <b class="spoiler_title">Arquitetura de 2015 a 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ano </th><th>  Artigo </th><th>  Ideia chave </th><th>  Peso </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Inicia√ß√£o v2 e v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">decomposi√ß√£o de pacotes em pacotes 1xN e Nx1</a> </td><td>  92 MB </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Inception v4 e Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">combina√ß√£o de Inception e ResNet</a> </td><td>  215 MB </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Resnext</a> </td><td>  2¬∫ lugar ILSVRC, o uso de muitos ramos (bloco de generaliza√ß√£o "generalizado") </td><td>  - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convolu√ß√£o separ√°vel em profundidade</a> , pesa menos com precis√£o compar√°vel √† </td><td>  88 MB </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloco denso</a>  leve, mas preciso </td><td>  33 MB (DenseNet-121), 80 MB (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bloco de aperto e excita√ß√£o</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">46 MB (SENet-Inception), 440 MB (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  A maioria desses modelos de PyTorch pode ser encontrada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> , e existe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">uma coisa muito legal</a> . <br><br>  Voc√™ deve ter notado que a coisa toda pesa bastante (eu gostaria de 20 MB no m√°ximo, ou at√© menos), enquanto hoje em dia eles usam dispositivos m√≥veis em todos os lugares e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">IoT est√°</a> ganhando popularidade, o que significa que voc√™ tamb√©m deseja usar grades l√°. <br><br><div class="spoiler">  <b class="spoiler_title">Rela√ß√£o entre peso e velocidade do modelo</b> <div class="spoiler_text">  Como as redes neurais em si mesmas apenas multiplicam os tensores, o n√∫mero de opera√ß√µes de multiplica√ß√£o (leia-se: o n√∫mero de pesos) afeta diretamente a velocidade do trabalho (se n√£o for utilizado p√≥s-processamento ou pr√©-processamento intensivo em m√£o-de-obra).  A velocidade da rede em si depende da implementa√ß√£o (estrutura), do hardware no qual est√° sendo executada e do tamanho da imagem de entrada. <br></div></div><br>  Os autores de muitos artigos seguiram o caminho de inventar arquiteturas r√°pidas, colecionei seus m√©todos sob o spoiler abaixo: <br><br><div class="spoiler">  <b class="spoiler_title">Arquitetura leve da CNN</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ano </th><th>  Artigo </th><th>  Ideia chave </th><th>  Peso </th><th>  Exemplo de implementa√ß√£o </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Compress√£o FireModule</a> </td><td>  0,5 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">obtida por uma pesquisa neural de arquiteturas, √© uma rede da categoria AutoML</a> </td><td>  23 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conv conventual do grupo, embaralhamento do canal</a> </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convolu√ß√µes separ√°veis ‚Äã‚Äãem profundidade e muitos outros truques</a> </td><td>  16 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Eu recomendo este artigo sobre Habr√©</a> </td><td>  14 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Squeezenext</a> </td><td>  veja as fotos no reposit√≥rio original </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisa de arquitetura neural especificamente para dispositivos m√≥veis usando RL</a> </td><td>  ~ 2 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet (v3)</a> </td><td>  ela saiu enquanto eu escrevia um artigo :) </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Os n√∫meros em todas as tabelas <s>s√£o retirados do teto</s> dos reposit√≥rios, da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tabela Aplicativos Keras</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deste artigo</a> . <br><br>  Voc√™ pergunta: ‚ÄúPor que voc√™ escreveu sobre todo esse‚Äú zool√≥gico ‚Äùde modelos?  E por que a tarefa da classifica√ß√£o?  Mas queremos ensinar as m√°quinas a ver, e a classifica√ß√£o √© apenas um tipo de tarefa restrita ... ‚Äù.  O fato √© que as redes neurais para detectar objetos, avaliar posturas / pontos, re-identificar e pesquisar em uma imagem usam exatamente os modelos de classifica√ß√£o como <b><abbr title="base, literalmente - a coluna vertebral">espinha dorsal</abbr></b> , e 80% do sucesso depende deles. <br><br>  Mas, de alguma forma, quero confiar mais na CNN, ou eles pensaram em caixas pretas, mas o que est√° "dentro" n√£o √© √≥bvio.  Para entender melhor o mecanismo de funcionamento das redes convolucionais, os pesquisadores propuseram o uso da visualiza√ß√£o. <br><br><a name="4"></a><h3>  Visualiza√ß√£o de redes neurais convolucionais: mostre-me paix√£o </h3><br>  Um passo importante para entender o que est√° acontecendo nas redes convolucionais √© o artigo <a href="">‚ÄúVisualizando e compreendendo redes convolucionais‚Äù</a> .  Nele, os autores propuseram v√°rias maneiras de visualizar exatamente o que (em quais partes da imagem) os neur√¥nios respondem √†s diferentes camadas da CNN (eu tamb√©m recomendo assistir a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">uma palestra de Stanford sobre esse t√≥pico</a> ).  Os resultados foram impressionantes: os autores mostraram que as primeiras camadas da rede convolucional respondem a algumas ‚Äúcoisas de baixo n√≠vel‚Äù pelo tipo de arestas / √¢ngulos / linhas, e as √∫ltimas camadas j√° respondem a partes inteiras das imagens (veja a figura abaixo), ou seja, elas j√° carregam em si alguma sem√¢ntica. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="imagem"></div><br><br>  Al√©m disso, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto de visualiza√ß√£o profunda da Cornell University e da empresa</a> avan√ßou ainda mais na visualiza√ß√£o, enquanto o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">famoso DeepDream</a> aprendeu a distorcer em um estilo interessante e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">viciante</a> (abaixo est√° uma foto do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="imagem" width="500"></div><br><br>  Em 2017, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo muito bom foi publicado no Distill</a> , no qual eles realizaram uma an√°lise detalhada do que cada camada ‚Äúv√™‚Äù e, mais recentemente (em mar√ßo de 2019), o Google inventou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atlas de ativa√ß√£o</a> : mapas exclusivos que podem ser constru√≠dos para cada camada de rede, o que nos aproxima da compreens√£o do quadro geral do trabalho da CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Se voc√™ quiser brincar com a visualiza√ß√£o, eu recomendaria o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lucid</a> e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TensorSpace</a> . <br><br>  Ok, a CNN parece ser verdadeira at√© certo ponto.  Precisamos aprender como usar isso em outras tarefas, e n√£o apenas na classifica√ß√£o.  Isso nos ajudar√° a extrair fotos do Embedding'ov e o Transfer Learning. <br><br><a name="5"></a><h2>  Eu pr√≥prio sou uma esp√©cie de cirurgi√£o: extra√≠mos recursos de redes neurais </h2><br>  Imagine que existe uma imagem e queremos encontrar imagens com apar√™ncia visual (esta √©, por exemplo, a pesquisa em uma imagem no Yandex.Pictures).  Anteriormente (antes das redes neurais), os engenheiros costumavam extrair recursos para isso, por exemplo, inventando algo que descreve bem a imagem e permite que ela seja comparada com outras.  Basicamente, esses m√©todos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> ) operam com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">gradientes de imagem</a> , geralmente esses itens s√£o chamados de descritores de imagem "cl√°ssicos".  De particular interesse, refiro-me ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> e ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">curso de Anton Konushin</a> (isto n√£o √© publicidade, apenas um bom curso :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="imagem" width="500"></div><br><br>  Usando redes neurais, n√£o podemos inventar esses recursos e heur√≠sticas, mas treinamos adequadamente o modelo e, em seguida, <b>consideramos a sa√≠da de uma ou mais camadas da rede como sinais da imagem</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="imagem" width="650"></div><br>  Uma an√°lise mais detalhada de todas as arquiteturas deixa claro que h√° duas etapas para classifica√ß√£o na CNN: <br>  1)  Camadas <b>extratoras de recursos</b> para extrair recursos informativos de imagens usando camadas convolucionais <br>  2)  Aprendendo sobre esses recursos Camadas classificadoras <b>totalmente conectadas (FC)</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="imagem" width="500"></div><br><br>  <b>A incorpora√ß√£o de imagens (recursos)</b> √© apenas sobre o fato de que voc√™ pode pegar seus sinais ap√≥s o extrator de recursos de uma rede neural convolucional (embora eles possam ser agregados de maneiras diferentes) como uma descri√ß√£o informativa das imagens.  Ou seja, treinamos a rede para classifica√ß√£o e, em seguida, apenas sa√≠mos na frente das camadas de classifica√ß√£o.  Esses sinais s√£o chamados de <i>recursos</i> , <i>descritores de redes neurais</i> ou <i>incorpora√ß√£o de figuras</i> (embora sejam geralmente aceitos na PNL, j√° que essa √© uma vis√£o, falarei sobre <i>caracter√≠sticas com</i> frequ√™ncia).  Geralmente, esse √© um tipo de vetor num√©rico, por exemplo, 128 n√∫meros, com os quais voc√™ j√° pode trabalhar. <br><br><div class="spoiler">  <b class="spoiler_title">Mas e os codificadores autom√°ticos?</b> <div class="spoiler_text">  Sim, de fato, os recursos podem ser obtidos pelos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">codificadores autom√°ticos</a> .  Na minha pr√°tica, eles fizeram isso de maneiras diferentes, mas, por exemplo, em artigos sobre re-identifica√ß√£o (que ser√£o discutidos mais adiante), mais frequentemente eles ainda usam recursos ap√≥s o extrator, em vez de treinar o codificador autom√°tico para isso.  Parece-me que vale a pena realizar experimentos em ambas as dire√ß√µes, se a quest√£o √© o que funciona melhor. <br></div></div><br>  Assim, o pipeline para resolver o <b>problema de busca por imagem</b> pode ser organizado de maneira simples: executamos as imagens pela CNN, pegamos sinais nas camadas desejadas e comparamos esses recursos entre si a partir de imagens diferentes.  Por exemplo, simplesmente consideramos a dist√¢ncia euclidiana desses vetores. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="imagem" width="500"></div><br><br>  <b>O Transfer Learning</b> √© uma t√©cnica bem conhecida para o treinamento eficaz de redes neurais que j√° s√£o treinadas em um conjunto de dados espec√≠fico para sua tarefa.  Freq√ºentemente, eles tamb√©m dizem Ajuste fino em vez de Transfer Learning, nas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">notas do curso de Stanford, cs231.</a> Esses conceitos s√£o compartilhados, eles dizem: Transfer Learning √© uma ideia geral e o Ajuste fino √© uma das implementa√ß√µes da t√©cnica.  Isso n√£o √© t√£o importante para n√≥s no futuro, o principal √© entender que podemos apenas treinar a rede para prever bem o novo conjunto de dados, come√ßando n√£o com pesos aleat√≥rios, mas com aqueles treinados em algum tipo ImageNet grande.  Isso √© especialmente verdade quando h√° poucos dados e voc√™ deseja resolver o problema qualitativamente. <br><br><div class="spoiler">  <b class="spoiler_title">Saiba mais sobre o Transfer Learning</b> <div class="spoiler_text">  <a href="">Artigo original</a> , mas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">por que ler muito texto, se voc√™ pode assistir ao v√≠deo</a> <br></div></div><br>  No entanto, simplesmente pegar os recursos necess√°rios e fazer um treinamento adicional do conjunto de dados para o conjunto de dados pode n√£o ser suficiente, por exemplo, para tarefas de pesquisa de pessoas / pessoas semelhantes / algo espec√≠fico.  Fotos da mesma pessoa visualmente √†s vezes podem ser ainda mais diferentes do que fotografias de pessoas diferentes.  √â necess√°rio fazer a rede destacar exatamente os sinais que s√£o inerentes a uma pessoa / objeto, mesmo que seja dif√≠cil para n√≥s fazer isso com nossos olhos.  Bem-vindo ao mundo do <b>aprendizado</b> de <b>representa√ß√£o</b> . <br><br><a name="6"></a><h2>  Fique perto: aprendizado de representa√ß√£o para pessoas e indiv√≠duos </h2><br><div class="spoiler">  <b class="spoiler_title">Nota de terminologia</b> <div class="spoiler_text">  Se voc√™ l√™ artigos cient√≠ficos, √†s vezes parece que alguns autores entendem a frase <b>aprendizado de m√©tricas de maneira</b> diferente, e n√£o h√° consenso sobre quais m√©todos chamar de aprendizado de m√©tricas e quais n√£o s√£o.  Por isso, neste artigo, decidi evitar essa frase em particular e usei um <b>aprendizado de representa√ß√£o</b> mais l√≥gica, alguns leitores podem n√£o concordar com isso - terei prazer em discutir nos coment√°rios. <br></div></div><br>  Definimos as tarefas: <br><br><ul><li>  <b>Tarefa 1</b> : existe uma galeria (conjunto) de fotografias dos rostos das pessoas, queremos que a rede seja capaz de responder de acordo com uma nova foto com o nome de uma pessoa da galeria (supostamente √© essa) ou disse que n√£o existe essa pessoa na galeria (e, talvez, acrescentemos pessoa nova) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>Tarefa 2</b> : a mesma coisa, mas n√£o estamos trabalhando com fotografias de rostos, mas com pessoas de tamanho completo <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  A primeira tarefa √© geralmente chamada de <b>reconhecimento facial</b> , a segunda - <b>re-identifica√ß√£o</b> (abreviada como <i>Reid</i> ).  Combinei-os em um bloco, porque suas solu√ß√µes usam id√©ias semelhantes hoje: para aprender a incorporar imagens efetivas que podem lidar com situa√ß√µes bastante dif√≠ceis, hoje eles usam diferentes tipos de perdas, como, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perda de trig√™meos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perda de qu√°druplos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perda do centro contrastivo, perda de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cosseno</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Ainda existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">redes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">siamesas</a> maravilhosas, mas eu sinceramente n√£o as usei.  A prop√≥sito, n√£o apenas a perda em si ‚Äúdecide‚Äù, mas como provar pares de pontos positivos e negativos para ela, enfatizam os autores do artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Amostragem na aprendizagem de incorpora√ß√£o profunda</a> . <br><br>  A ess√™ncia de todas essas perdas e redes siamesas √© simples - queremos que as imagens de uma classe (pessoa) no espa√ßo latente de recursos (casamentos) sejam "pr√≥ximas" e de diferentes classes (pessoas) sejam "distantes".  A proximidade √© geralmente medida da seguinte forma: capturas de imagens de uma rede neural s√£o realizadas (por exemplo, um vetor de 128 n√∫meros) e consideramos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dist√¢ncia euclidiana</a> usual entre esses vetores ou a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">proximidade</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cosseno.</a>  Como medir, √© melhor escolher em seu conjunto de dados / tarefa. <br><br>  Uma representa√ß√£o esquem√°tica de um pipeline de solu√ß√£o de problemas no aprendizado de representa√ß√£o √© mais ou menos assim: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Mas para ser mais preciso, assim</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        ‚Äî   .   ,     ‚Äî     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> <br></div></div><br><br>   <b> </b>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">- ( <b>MUST READ!</b> )</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FaceNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ArcFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CosFace</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="imagem" width="700"></div><br><br>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dlib</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FaceNet repo</a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">       </a> . ,      ArcFace  CosFace (  ,    - ,    - ). <br><br>        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ,   ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="detec√ß√£o humana cortada de uma fotografia grande"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> Reid ‚Äî    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 2016 </a> . ,     ,    ‚Äî   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="imagem" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   ‚Äî    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TorchReid</a> .      ‚Äî   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  ,   </a> ).   ?  ‚Ä¶ <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="imagem" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt450732/">https://habr.com/ru/post/pt450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt450720/index.html">Como desenvolver um aplicativo amig√°vel</a></li>
<li><a href="../pt450724/index.html">Apresentando o Python para camaradas superando a ‚Äúlinguagem A vs. V‚Äù l√≠ngua B "e outros preconceitos</a></li>
<li><a href="../pt450726/index.html">Criando uma ferramenta para escrever de forma r√°pida e eficiente autotestes no Selenium</a></li>
<li><a href="../pt450728/index.html">NLog: regras e filtros</a></li>
<li><a href="../pt450730/index.html">ok.tech: frontend meetup</a></li>
<li><a href="../pt450734/index.html">Fuzzing √© um passo importante no desenvolvimento seguro</a></li>
<li><a href="../pt450736/index.html">"Isolar a Internet √© muito mais f√°cil e mais barato do que fornecer bloqueio externo".</a></li>
<li><a href="../pt450738/index.html">Rob√¥s no data center: como a intelig√™ncia artificial pode ser √∫til?</a></li>
<li><a href="../pt450740/index.html">Base de l√¢mpada inteligente REDMOND - adicione √† casa inteligente</a></li>
<li><a href="../pt450744/index.html">Infraestrutura de bicicleta em Minsk para um expat em TI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>