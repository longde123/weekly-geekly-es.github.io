<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💂🏼 🤟🏽 👲🏻 Introduciendo Neural ODE 🐹 🖖🏾 😀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ecuaciones diferenciales ordinarias neurales 
 Una proporción significativa de procesos se describe mediante ecuaciones diferenciales, esta puede ser ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introduciendo Neural ODE</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/442002/"><h1>  Ecuaciones diferenciales ordinarias neurales </h1><br>  Una proporción significativa de procesos se describe mediante ecuaciones diferenciales, esta puede ser la evolución del sistema físico a lo largo del tiempo, la condición médica del paciente, las características fundamentales del mercado de valores, etc.  Los datos sobre tales procesos son consistentes y de naturaleza continua, en el sentido de que las observaciones son simplemente manifestaciones de algún tipo de estado continuamente cambiante. <br><br>  También hay otro tipo de datos en serie, son datos discretos, por ejemplo, datos de tareas de PNL.  El estado en dichos datos varía discretamente: de un carácter o palabra a otro. <br><br>  Ahora, ambos tipos de datos en serie suelen ser procesados ​​por redes recursivas, aunque son de naturaleza diferente y parecen requerir enfoques diferentes. <br><br>  Se presentó un artículo muy interesante en la última <em>conferencia de NIPS</em> , que puede ayudar a resolver este problema.  Los autores proponen un enfoque que llamaron <strong>EDO neuronales</strong> . <br><br>  Aquí intenté reproducir y resumir los resultados de este artículo para familiarizarme un poco con su idea.  Me parece que esta nueva arquitectura puede encontrar un lugar en las herramientas estándar de un científico de datos junto con redes convolucionales y recurrentes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7e/65/hf/7e65hfxs1amdqyy_uy6emdwulkg.png"></div><br><a name="habracut"></a><br><i></i><p>  <em>Figura</em> 1: La propagación <em>inversa en</em> gradiente continuo requiere resolver la ecuación diferencial aumentada en el tiempo. <br><br>  Las flechas representan el ajuste de gradientes propagados hacia atrás por gradientes de las observaciones. <br><br>  Ilustración del artículo original. </p><br><h2>  Planteamiento del problema </h2><br>  Que haya un proceso que obedezca a alguna EDO desconocida y que haya varias observaciones (ruidosas) a lo largo de la trayectoria del proceso <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/438/dde/8ca/438dde8cabcfd6a826ed0257752e6499.svg" alt="\ frac {dz} {dt} = f (z (t), t) \; (1)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/190/072/e64/190072e645d65dc29d6160aaf0dde065.svg" alt="\ {(z_0, t_0), (z_1, t_1), ..., (z_M, t_M) \} - \ text {observaciones}"></div><br>  Cómo encontrar una aproximación <img src="https://habrastorage.org/getpro/habr/post_images/287/bb8/c63/287bb8c637a8a70238be4ea690a0a8e1.svg" alt="\ widehat {f} (z, t, \ theta)">  funciones de altavoz <img src="https://habrastorage.org/getpro/habr/post_images/6ad/6ab/1fa/6ad6ab1fa56e2c8b6d987dc5c1f68004.svg" alt="f (z, t)">  ? <br><br>  Primero, considere una tarea más simple: solo hay 2 observaciones, al principio y al final de la trayectoria, <img src="https://habrastorage.org/getpro/habr/post_images/547/808/5c2/5478085c21b85393c8e9e0f78c2821b3.svg" alt="(z_0, t_0), (z_1, t_1)">  . <br><br>  La evolución del sistema comienza desde el estado <img src="https://habrastorage.org/getpro/habr/post_images/7e2/d13/d9a/7e2d13d9a9cc05a84422c3b63260dd83.svg" alt="z_0, t_0">  a tiempo <img src="https://habrastorage.org/getpro/habr/post_images/4fc/683/cd0/4fc683cd033494d093a0e2c6f021805b.svg" alt="t_1 - t_0">  con alguna función dinámica parametrizada utilizando cualquier método de evolución de sistemas ODE.  Después de que el sistema esté en un nuevo estado <img src="https://habrastorage.org/getpro/habr/post_images/56d/8d1/11e/56d8d111e514029cf892aff24a82c768.svg" alt="\ hat {z_1}, t_1">  , se compara con el estado <img src="https://habrastorage.org/getpro/habr/post_images/f80/709/9a5/f807099a57f179abaa3ab5e70cee4c9c.svg" alt="z_1">  y la diferencia entre ellos se minimiza variando los parámetros <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  Funciones dinámicas. <br><br>  O, más formalmente, considere minimizar la función de pérdida <img src="https://habrastorage.org/getpro/habr/post_images/b03/557/43f/b0355743fe8383284ef53436870494da.svg" alt="L (\ hat {z_1})">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/251/944/489/251944489756d64d40b61f0dc0d7cb0b.svg" alt="L (z (t_1)) = L \ Big (\ int_ {t_0} ^ {t_1} f (z (t), t, \ theta) dt \ Big) = L \ big (\ text {ODESolve} (z ( t_0), f, t_0, t_1, \ theta) \ big) \; (2)"></div><br>  Para minimizar <img src="https://habrastorage.org/getpro/habr/post_images/899/a67/551/899a675510d28419769a9b42281f0c65.svg" alt="L">  , necesita calcular los gradientes para todos sus parámetros: <img src="https://habrastorage.org/getpro/habr/post_images/194/e4f/6e5/194e4f6e59bbfefc52efebce3bb857a3.svg" alt="z (t_0), t_0, t_1, \ theta">  .  Para hacer esto, primero debe determinar cómo <img src="https://habrastorage.org/getpro/habr/post_images/899/a67/551/899a675510d28419769a9b42281f0c65.svg" alt="L">  depende del estado en cada momento <img src="https://habrastorage.org/getpro/habr/post_images/823/f33/fc8/823f33fc81685e76b1d88f48c68eea32.svg" alt="(z (t))">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e79/64f/b7f/e7964fb7f074d4f9c16893b53a1562c6.svg" alt="a (t) = - \ frac {\ partial L} {\ partial z (t)} \; (3)"></div><br><img src="https://habrastorage.org/getpro/habr/post_images/2e3/af6/935/2e3af69359cb79517739f0ccf9a8bc3e.svg" alt="a (t)">  se llama estado <em>adjunto</em> , su dinámica viene dada por otra ecuación diferencial, que puede considerarse un análogo continuo de diferenciación de una función compleja ( <em>regla de cadena</em> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/b66/5d5/006b665d51719470f25859e6271463dc.svg" alt="\ frac {d a (t)} {d t} = -a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial z} \; (4)"></div><br>  El resultado de esta fórmula se puede encontrar en el apéndice del artículo original. <br><br>  <i>Los vectores en este artículo deben considerarse vectores en minúsculas, aunque el artículo original usa tanto una representación de fila como de columna.</i> <br><br>  Al resolver diffur (4) en el tiempo, obtenemos una dependencia del estado inicial <img src="https://habrastorage.org/getpro/habr/post_images/9ec/7ef/827/9ec7ef8276505d510f609b983c58fdc3.svg" alt="z (t_0)">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/556/75a/7f0/55675a7f0c820f2a1da88e0802c97dc7.svg" alt="\ frac {\ partial L} {\ partial z (t_0)} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial z} dt \; (5)"></div><br>  Para calcular el gradiente con respecto a <img src="https://habrastorage.org/getpro/habr/post_images/2a3/102/4ea/2a31024ea1803c34a47496e24a53a1ef.svg" alt="t">  y <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  , simplemente puede considerarlos parte del estado.  Esta condición se llama <em>aumentada</em> .  La dinámica de este estado se obtiene trivialmente de la dinámica original: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/161/f11/79c/161f1179cbb6dfe3c18057d8abd0f45b.svg" alt="\ frac {d} {dt} \ begin {bmatrix} z ​​\\ \ theta \\ t \ end {bmatrix} (t) = f _ {\ text {aug}} ([z, \ theta, t]): = \ begin {bmatrix} f ([z, \ theta, t]) \\ 0 \\ 1 \ end {bmatrix} \; (6)"></div><br>  Entonces el estado conjugado a este estado aumentado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7dc/7d0/553/7dc7d055304addbb695d1f23a9e640e6.svg" alt="a _ {\ text {aug}}: = \ begin {bmatrix} a \\ a _ {\ theta} \\ a_t \ end {bmatrix}, a _ {\ theta} (t): = \ frac {\ partial L} { \ partial \ theta (t)}, a_t (t): = \ frac {\ partial L} {\ partial t (t)} \; (7)"></div><br>  Dinámica Aumentada Gradiente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/985/437/7a4/9854377a42cda72fa53bedb928a9d023.svg" alt="\ frac {\ partial f _ {\ text {aug}}} {\ partial [z, \ theta, t]} = \ begin {bmatrix} \ frac {\ partial f} {\ partial z} &amp;; \ frac {\ partial f} {\ partial \ theta} &amp;; \ frac {\ partial f} {\ partial t} \\ 0 &amp;; 0 &amp;; 0 \\ 0 &amp;; 0 &amp;; 0 \ end {bmatrix} \; (8)"></div><br>  La ecuación diferencial del estado aumentado conjugado de la fórmula (4) entonces: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bd3/b9f/7f3/bd3b9f7f3e6ec08881594346a8a3e4f3.svg" alt="\ frac {d a _ {\ text {aug}}} {dt} = - \ begin {bmatrix} a \ frac {\ partial f} {\ partial z} &amp;; a \ frac {\ partial f} {\ partial \ theta} &amp;; a \ frac {\ partial f} {\ partial t} \ end {bmatrix} \; (9)"></div><br>  Resolver esta ODE en el tiempo produce: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a0/01e/3d9/5a001e3d928ce08d05ff084353134d78.svg" alt="\ frac {\ partial L} {\ partial z (t_0)} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial z} dt \; (10)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e09/ba3/cc6/e09ba3cc6674830a8847894c39020ec0.svg" alt="\ frac {\ partial L} {\ partial \ theta} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial \ theta } dt \; (11)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/022/f84/715/022f84715f62b101017a6dcb591d5de3.svg" alt="\ frac {\ partial L} {\ partial t_0} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial t} dt \; (12)"></div><br>  Que pasa con <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0f9/b43/8a8/0f9b438a88408c879af759bcaf4d1d5e.svg" alt="\ frac {\ partial L} {\ partial t_1} = - a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial t} \; (13)"></div><br>  da gradientes en todos los parámetros de entrada al <em>solucionador ODESolve</em> ODE. <br><br>  Todos los gradientes (10), (11), (12), (13) se pueden calcular juntos en una llamada <em>ODESolve</em> con la dinámica del estado aumentado conjugado (9). <br><br><img src="https://habrastorage.org/webt/8k/pz/uk/8kpzukmizpmezmywov4b3zm29lc.png"><br>  <i>Ilustración del artículo original.</i> <br><br>  El algoritmo anterior describe la propagación inversa del gradiente de la solución ODE para observaciones sucesivas. <br><br>  En el caso de varias observaciones en una trayectoria, todo se calcula de la misma manera, pero en los momentos de observaciones, el inverso del gradiente distribuido debe ajustarse con gradientes de la observación actual, como se muestra en la <em>Figura 1</em> . <br><br><h1>  Implementación </h1><br>  El siguiente código es mi implementación de <strong>EDO neuronales</strong> .  Lo hice simplemente para comprender mejor lo que está sucediendo.  Sin embargo, está muy cerca de lo que se implementa en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio de</a> los autores del artículo.  Contiene todo el código que necesita comprender en un solo lugar, también está un poco más comentado.  Para aplicaciones y experimentos reales, aún es mejor utilizar la implementación de los autores del artículo original. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm_notebook <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns sns.color_palette(<span class="hljs-string"><span class="hljs-string">"bright"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tensor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable use_cuda = torch.cuda.is_available()</code> </pre> <br>  Primero debe implementar cualquier método para la evolución de los sistemas ODE.  Para simplificar, el método de Euler se implementa aquí, aunque cualquier método explícito o implícito es adecuado. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ode_solve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z0, t0, t1, f)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""     -   """</span></span> h_max = <span class="hljs-number"><span class="hljs-number">0.05</span></span> n_steps = math.ceil((abs(t1 - t0)/h_max).max().item()) h = (t1 - t0)/n_steps t = t0 z = z0 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i_step <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_steps): z = z + h * f(z, t) t = t + h <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z</code> </pre><br>  También describe la superclase de una función dinámica parametrizada con un par de métodos útiles. <br><br>  Primero: debe devolver todos los parámetros de los que depende la función en forma de vector. <br><br>  En segundo lugar: es necesario calcular la dinámica aumentada.  Esta dinámica depende del gradiente de la función parametrizada en términos de parámetros y datos de entrada.  Para no tener que registrar el gradiente con cada mano para cada nueva arquitectura, utilizaremos el método <strong>torch.autograd.grad</strong> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward_with_grad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, z, t, grad_outputs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""Compute f and a df/dz, a df/dp, a df/dt"""</span></span> batch_size = z.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] out = self.forward(z, t) a = grad_outputs adfdz, adfdt, *adfdp = torch.autograd.grad( (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a), allow_unused=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, retain_graph=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) <span class="hljs-comment"><span class="hljs-comment">#  grad       , #  expand   if adfdp is not None: adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0) adfdp = adfdp.expand(batch_size, -1) / batch_size if adfdt is not None: adfdt = adfdt.expand(batch_size, 1) / batch_size return out, adfdz, adfdt, adfdp def flatten_parameters(self): p_shapes = [] flat_parameters = [] for p in self.parameters(): p_shapes.append(p.size()) flat_parameters.append(p.flatten()) return torch.cat(flat_parameters)</span></span></code> </pre><br>  El siguiente código describe la propagación hacia adelante y hacia atrás para <em>las EDO neuronales</em> .  Es necesario separar este código del <strong><em>módulo torch.nn.Module</em></strong> principal en forma de la función <strong><em>torch.autograd.Function</em></strong> porque en este último puede implementar un método de <strong><em>propagación inversa</em></strong> arbitrario, en lugar de un módulo.  Entonces esto es solo una muleta. <br><br>  Esta característica subyace a todo el enfoque <em>Neural ODE</em> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ODEAdjoint</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(torch.autograd.Function)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx, z0, t, flat_parameters, func)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> isinstance(func, ODEF) bs, *z_shape = z0.size() time_len = t.size(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): z = torch.zeros(time_len, bs, *z_shape).to(z0) z[<span class="hljs-number"><span class="hljs-number">0</span></span>] = z0 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i_t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(time_len - <span class="hljs-number"><span class="hljs-number">1</span></span>): z0 = ode_solve(z0, t[i_t], t[i_t+<span class="hljs-number"><span class="hljs-number">1</span></span>], func) z[i_t+<span class="hljs-number"><span class="hljs-number">1</span></span>] = z0 ctx.func = func ctx.save_for_backward(t, z.clone(), flat_parameters) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx, dLdz)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" dLdz shape: time_len, batch_size, *z_shape """</span></span> func = ctx.func t, z, flat_parameters = ctx.saved_tensors time_len, bs, *z_shape = z.size() n_dim = np.prod(z_shape) n_params = flat_parameters.size(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   , #       def augmented_dynamics(aug_z_i, t_i): """   -     t_i -   : bs, 1 aug_z_i -   : bs, n_dim*2 + n_params + 1 """ #     z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim] # Unflatten z and a z_i = z_i.view(bs, *z_shape) a = a.view(bs, *z_shape) with torch.set_grad_enabled(True): t_i = t_i.detach().requires_grad_(True) z_i = z_i.detach().requires_grad_(True) faug = func.forward_with_grad(z_i, t_i, grad_outputs=a) func_eval, adfdz, adfdt, adfdp = faug adfdz = adfdz if adfdz is not None else torch.zeros(bs, *z_shape) adfdp = adfdp if adfdp is not None else torch.zeros(bs, n_params) adfdt = adfdt if adfdt is not None else torch.zeros(bs, 1) adfdz = adfdz.to(z_i) adfdp = adfdp.to(z_i) adfdt = adfdt.to(z_i) # Flatten f and adfdz func_eval = func_eval.view(bs, n_dim) adfdz = adfdz.view(bs, n_dim) return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1) dLdz = dLdz.view(time_len, bs, n_dim) # flatten dLdz   with torch.no_grad(): ##      #    , #       adj_z = torch.zeros(bs, n_dim).to(dLdz) adj_p = torch.zeros(bs, n_params).to(dLdz) #    z  p,        adj_t = torch.zeros(time_len, bs, 1).to(dLdz) for i_t in range(time_len-1, 0, -1): z_i = z[i_t] t_i = t[i_t] f_i = func(z_i, t_i).view(bs, n_dim) #      dLdz_i = dLdz[i_t] dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] #     adj_z += dLdz_i adj_t[i_t] = adj_t[i_t] - dLdt_i #      aug_z = torch.cat(( z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z) adj_t[i_t]), dim=-1 ) #  ()      aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics) #       adj_z[:] = aug_ans[:, n_dim:2*n_dim] adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params] adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:] del aug_z, aug_ans ##         #    dLdz_0 = dLdz[0] dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] #  adj_z += dLdz_0 adj_t[0] = adj_t[0] - dLdt_0 return adj_z.view(bs, *z_shape), adj_t, adj_p, None</span></span></code> </pre><br>  Ahora, para su comodidad, <strong>ajuste</strong> esta función en <strong>nn.Module</strong> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NeuralODE</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, func)</span></span></span><span class="hljs-function">:</span></span> super(NeuralODE, self).__init__() <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> isinstance(func, ODEF) self.func = func <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, z0, t=Tensor</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">([</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">0.</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">1.</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">])</span></span></span></span><span class="hljs-function"><span class="hljs-params">, return_whole_sequence=False)</span></span></span><span class="hljs-function">:</span></span> t = t.to(z0) z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> return_whole_sequence: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z[<span class="hljs-number"><span class="hljs-number">-1</span></span>]</code> </pre><br><br><h1>  Solicitud </h1><br><h2>  Recuperación de la función dinámica real (verificación de aproximación) </h2><br>  Como prueba básica, verifiquemos ahora si <strong>Neural ODE</strong> puede restaurar la verdadera función de la dinámica utilizando datos de observación. <br><br>  Para hacer esto, primero determinamos la función dinámica del ODE, evolucionamos la trayectoria en función de ella y luego intentamos restaurarla desde la función dinámica parametrizada al azar. <br><br>  Primero, verifiquemos el caso más simple de una EDO lineal.  La función dinámica es solo la acción de una matriz. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eb4/fdc/86a/eb4fdc86a1c5f56ab1b6e37e575e7c72.svg" alt="\ frac {dz} {dt} = \ begin {bmatrix} -0.1 &amp;; -1.0 \\ 1.0 &amp;; -0.1 \ end {bmatrix} z"></div><br>  La función entrenada se parametriza mediante una matriz aleatoria. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nj/q6/es/njq6eswuevh4rhx-8nhzyz-cq_k.gif"></div><br>  Además, una dinámica un poco más sofisticada (sin un gif, porque el proceso de aprendizaje no es tan hermoso :)) <br>  La función de aprendizaje aquí es una red totalmente conectada con una capa oculta. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tr/4n/eb/tr4nebjdrs4pt4v5vlzleai8exe.png"></div><br><div class="spoiler">  <b class="spoiler_title">Código</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LinearODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, W)</span></span></span><span class="hljs-function">:</span></span> super(LinearODEF, self).__init__() self.lin = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.lin.weight = nn.Parameter(W) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.lin(x)</code> </pre><br>  La función dinámica es solo una matriz <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SpiralFunctionExample</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LinearODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> matrix = Tensor([[<span class="hljs-number"><span class="hljs-number">-0.1</span></span>, <span class="hljs-number"><span class="hljs-number">-1.</span></span>], [<span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">-0.1</span></span>]]) super(SpiralFunctionExample, self).__init__(matrix)</code> </pre><br>  Matriz parametrizada al azar <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomLinearODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LinearODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(RandomLinearODEF, self).__init__(torch.randn(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)/<span class="hljs-number"><span class="hljs-number">2.</span></span>)</code> </pre><br>  Dinámica para trayectorias más sofisticadas. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, A, B, x0)</span></span></span><span class="hljs-function">:</span></span> super(TestODEF, self).__init__() self.A = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.A.weight = nn.Parameter(A) self.B = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.B.weight = nn.Parameter(B) self.x0 = nn.Parameter(x0) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> xTx0 = torch.sum(x*self.x0, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dxdt</code> </pre><br>  Dinámica de aprendizaje en forma de una red totalmente conectada. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NNODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, in_dim, hid_dim, time_invariant=False)</span></span></span><span class="hljs-function">:</span></span> super(NNODEF, self).__init__() self.time_invariant = time_invariant <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> time_invariant: self.lin1 = nn.Linear(in_dim, hid_dim) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.lin1 = nn.Linear(in_dim+<span class="hljs-number"><span class="hljs-number">1</span></span>, hid_dim) self.lin2 = nn.Linear(hid_dim, hid_dim) self.lin3 = nn.Linear(hid_dim, in_dim) self.elu = nn.ELU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> self.time_invariant: x = torch.cat((x, t), dim=<span class="hljs-number"><span class="hljs-number">-1</span></span>) h = self.elu(self.lin1(x)) h = self.elu(self.lin2(h)) out = self.lin3(h) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_np</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x.detach().cpu().numpy() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_trajectories</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(obs=None, times=None, trajs=None, save=None, figsize=</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">16</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">8</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=figsize) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> obs <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> times <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: times = [<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>] * len(obs) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> o, t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(obs, times): o, t = to_np(o), to_np(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(o.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]): plt.scatter(o[:, b_i, <span class="hljs-number"><span class="hljs-number">0</span></span>], o[:, b_i, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=t[:, b_i, <span class="hljs-number"><span class="hljs-number">0</span></span>], cmap=cm.plasma) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> trajs <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> z <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trajs: z = to_np(z) plt.plot(z[:, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], z[:, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], lw=<span class="hljs-number"><span class="hljs-number">1.5</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> save <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: plt.savefig(save) plt.show() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">conduct_experiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ode_true, ode_trained, n_steps, name, plot_freq=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Create data z0 = Variable(torch.Tensor([[0.6, 0.3]])) t_max = 6.29*5 n_points = 200 index_np = np.arange(0, n_points, 1, dtype=np.int) index_np = np.hstack([index_np[:, None]]) times_np = np.linspace(0, t_max, num=n_points) times_np = np.hstack([times_np[:, None]]) times = torch.from_numpy(times_np[:, :, None]).to(z0) obs = ode_true(z0, times, return_whole_sequence=True).detach() obs = obs + torch.randn_like(obs) * 0.01 # Get trajectory of random timespan min_delta_time = 1.0 max_delta_time = 5.0 max_points_num = 32 def create_batch(): t0 = np.random.uniform(0, t_max - max_delta_time) t1 = t0 + np.random.uniform(min_delta_time, max_delta_time) idx = sorted(np.random.permutation( index_np[(times_np &gt; t0) &amp; (times_np &lt; t1)] )[:max_points_num]) obs_ = obs[idx] ts_ = times[idx] return obs_, ts_ # Train Neural ODE optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01) for i in range(n_steps): obs_, ts_ = create_batch() z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True) loss = F.mse_loss(z_, obs_.detach()) optimizer.zero_grad() loss.backward(retain_graph=True) optimizer.step() if i % plot_freq == 0: z_p = ode_trained(z0, times, return_whole_sequence=True) plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=f"assets/imgs/{name}/{i}.png") clear_output(wait=True) ode_true = NeuralODE(SpiralFunctionExample()) ode_trained = NeuralODE(RandomLinearODEF()) conduct_experiment(ode_true, ode_trained, 500, "linear") func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]])) ode_true = NeuralODE(func) func = NNODEF(2, 16, time_invariant=True) ode_trained = NeuralODE(func) conduct_experiment(ode_true, ode_trained, 3000, "comp", plot_freq=30)</span></span></code> </pre><br></div></div><br>  Como puede ver, <em>Neural ODE es</em> bastante bueno para restaurar la dinámica.  Es decir, el concepto en su conjunto funciona. <br>  Ahora verifique un problema un poco más complicado (MNIST, jaja). <br><br><h2>  ODE neuronal inspirada en ResNets </h2><br>  En ResNet'ax, el estado latente cambia de acuerdo con la fórmula <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/30a/6b2/d3c/30a6b2d3c27bb27afaeb4736072e4446.svg" alt="h_ {t + 1} = h_ {t} + f (h_ {t}, \ theta_ {t})"></div><br>  donde <img src="https://habrastorage.org/getpro/habr/post_images/7ca/860/97d/7ca86097dc5dc0d9cbb9b454168de928.svg" alt="t \ in \ {0 ... T \}">  Es el número de bloque y <img src="https://habrastorage.org/getpro/habr/post_images/eb2/5f7/ddf/eb25f7ddf77e46681e256b28fecaef35.svg" alt="f">  Esta es una función aprendida por las capas dentro del bloque. <br><br>  En el límite, si tomamos un número infinito de bloques con pasos cada vez más pequeños, obtenemos la dinámica continua de la capa oculta en forma de ODE, al igual que lo que estaba arriba. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/629/67a/d41/62967ad41e5f50cc65e1c1e55a2860d3.svg" alt="\ frac {dh (t)} {dt} = f (h (t), t, \ theta)"></div><br>  Comenzando desde la capa de entrada <img src="https://habrastorage.org/getpro/habr/post_images/c33/9b9/bc0/c339b9bc0244968c806390c2e66fdb62.svg" alt="h (0)">  podemos definir la capa de salida <img src="https://habrastorage.org/getpro/habr/post_images/b7e/dec/ef3/b7edecef308ce0df4f3d1c8aaa753617.svg" alt="h (T)">  como una solución a esta ODE en el momento T. <br><br>  Ahora podemos contar <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  como parámetros distribuidos ( <em>compartidos</em> ) entre todos los bloques infinitesimales. <br><br><h3>  Validación de la arquitectura neuronal ODE en MNIST </h3><br>  En esta parte, probaremos la capacidad de <em>Neural ODE para</em> usarse como componentes en arquitecturas más familiares. <br><br>  En particular, reemplazaremos los bloques residuales con <em>ODE neuronal</em> en el clasificador MNIST. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7c/gw/ia/7cgwiawqx6-_kxk82qpenqplowi.png" width="400"></div><br><br><div class="spoiler">  <b class="spoiler_title">Código</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">norm</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dim)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.BatchNorm2d(dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">conv3x3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_feats, out_feats, stride=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.Conv2d(in_feats, out_feats, kernel_size=<span class="hljs-number"><span class="hljs-number">3</span></span>, stride=stride, padding=<span class="hljs-number"><span class="hljs-number">1</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_time</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_tensor, t)</span></span></span><span class="hljs-function">:</span></span> bs, c, w, h = in_tensor.shape <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.cat((in_tensor, t.expand(bs, <span class="hljs-number"><span class="hljs-number">1</span></span>, w, h)), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConvODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, dim)</span></span></span><span class="hljs-function">:</span></span> super(ConvODEF, self).__init__() self.conv1 = conv3x3(dim + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim) self.norm1 = norm(dim) self.conv2 = conv3x3(dim + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim) self.norm2 = norm(dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> xt = add_time(x, t) h = self.norm1(torch.relu(self.conv1(xt))) ht = add_time(h, t) dxdt = self.norm2(torch.relu(self.conv2(ht))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dxdt <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ContinuousNeuralMNISTClassifier</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, ode)</span></span></span><span class="hljs-function">:</span></span> super(ContinuousNeuralMNISTClassifier, self).__init__() self.downsampling = nn.Sequential( nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), norm(<span class="hljs-number"><span class="hljs-number">64</span></span>), nn.ReLU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), nn.Conv2d(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), norm(<span class="hljs-number"><span class="hljs-number">64</span></span>), nn.ReLU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), nn.Conv2d(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), ) self.feature = ode self.norm = norm(<span class="hljs-number"><span class="hljs-number">64</span></span>) self.avg_pool = nn.AdaptiveAvgPool2d((<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) self.fc = nn.Linear(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = self.downsampling(x) x = self.feature(x) x = self.norm(x) x = self.avg_pool(x) shape = torch.prod(torch.tensor(x.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:])).item() x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, shape) out = self.fc(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out func = ConvODEF(<span class="hljs-number"><span class="hljs-number">64</span></span>) ode = NeuralODE(func) model = ContinuousNeuralMNISTClassifier(ode) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: model = model.cuda() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torchvision img_std = <span class="hljs-number"><span class="hljs-number">0.3081</span></span> img_mean = <span class="hljs-number"><span class="hljs-number">0.1307</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span> train_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(<span class="hljs-string"><span class="hljs-string">"data/mnist"</span></span>, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(<span class="hljs-string"><span class="hljs-string">"data/mnist"</span></span>, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) optimizer = torch.optim.Adam(model.parameters()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(epoch)</span></span></span><span class="hljs-function">:</span></span> num_items = <span class="hljs-number"><span class="hljs-number">0</span></span> train_losses = [] model.train() criterion = nn.CrossEntropyLoss() print(<span class="hljs-string"><span class="hljs-string">f"Training Epoch </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{epoch}</span></span></span><span class="hljs-string">..."</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(enumerate(train_loader), total=len(train_loader)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: data = data.cuda() target = target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() train_losses += [loss.item()] num_items += data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] print(<span class="hljs-string"><span class="hljs-string">'Train loss: {:.5f}'</span></span>.format(np.mean(train_losses))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_losses <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">test</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> accuracy = <span class="hljs-number"><span class="hljs-number">0.0</span></span> num_items = <span class="hljs-number"><span class="hljs-number">0</span></span> model.eval() criterion = nn.CrossEntropyLoss() print(<span class="hljs-string"><span class="hljs-string">f"Testing..."</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(enumerate(test_loader), total=len(test_loader)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: data = data.cuda() target = target.cuda() output = model(data) accuracy += torch.sum(torch.argmax(output, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) == target).item() num_items += data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] accuracy = accuracy * <span class="hljs-number"><span class="hljs-number">100</span></span> / num_items print(<span class="hljs-string"><span class="hljs-string">"Test Accuracy: {:.3f}%"</span></span>.format(accuracy)) n_epochs = <span class="hljs-number"><span class="hljs-number">5</span></span> test() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_epochs + <span class="hljs-number"><span class="hljs-number">1</span></span>): train_losses += train(epoch) test() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) history = pd.DataFrame({<span class="hljs-string"><span class="hljs-string">"loss"</span></span>: train_losses}) history[<span class="hljs-string"><span class="hljs-string">"cum_data"</span></span>] = history.index * batch_size history[<span class="hljs-string"><span class="hljs-string">"smooth_loss"</span></span>] = history.loss.ewm(halflife=<span class="hljs-number"><span class="hljs-number">10</span></span>).mean() history.plot(x=<span class="hljs-string"><span class="hljs-string">"cum_data"</span></span>, y=<span class="hljs-string"><span class="hljs-string">"smooth_loss"</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), title=<span class="hljs-string"><span class="hljs-string">"train error"</span></span>)</code> </pre><br></div></div><br><pre> <code class="plaintext hljs">Testing... 100% 79/79 [00:01&lt;00:00, 45.69it/s] Test Accuracy: 9.740% Training Epoch 1... 100% 1875/1875 [01:15&lt;00:00, 24.69it/s] Train loss: 0.20137 Testing... 100% 79/79 [00:01&lt;00:00, 46.64it/s] Test Accuracy: 98.680% Training Epoch 2... 100% 1875/1875 [01:17&lt;00:00, 24.32it/s] Train loss: 0.05059 Testing... 100% 79/79 [00:01&lt;00:00, 46.11it/s] Test Accuracy: 97.760% Training Epoch 3... 100% 1875/1875 [01:16&lt;00:00, 24.63it/s] Train loss: 0.03808 Testing... 100% 79/79 [00:01&lt;00:00, 45.65it/s] Test Accuracy: 99.000% Training Epoch 4... 100% 1875/1875 [01:17&lt;00:00, 24.28it/s] Train loss: 0.02894 Testing... 100% 79/79 [00:01&lt;00:00, 45.42it/s] Test Accuracy: 99.130% Training Epoch 5... 100% 1875/1875 [01:16&lt;00:00, 24.67it/s] Train loss: 0.02424 Testing... 100% 79/79 [00:01&lt;00:00, 45.89it/s] Test Accuracy: 99.170%</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zx/zv/5x/zxzv5x4ktqyy9lt19of-d2i4few.png"></div><br>  Después de un entrenamiento muy duro durante solo 5 eras y 6 minutos de entrenamiento, el modelo ya ha alcanzado un error de prueba de menos del 1%.  Podemos decir que <em>las ODE neuronales se</em> integran bien <em>como un</em> componente en redes más tradicionales. <br><br>  En su artículo, los autores también comparan este clasificador (ODE-Net) con una red normal totalmente conectada, con ResNet con una arquitectura similar y con la misma arquitectura exacta, en la que el gradiente se propaga directamente a través de operaciones en <em>ODESolve</em> (sin el método de gradiente conjugado) ( RK-Net). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vn/41/--/vn41--frzbdkftca4ehe7hh-kea.png"></div><br>  <i>Ilustración del artículo original.</i> <br><br>  Según ellos, una red de 1 capa totalmente conectada con aproximadamente el mismo número de parámetros que <em>Neural ODE</em> tiene un error mucho mayor en la prueba, ResNet con el mismo error tiene muchos más parámetros, y RK-Net sin el método de gradiente conjugado tiene un error ligeramente mayor y con un consumo de memoria que aumenta linealmente (cuanto menor es el error permitido, más pasos debe seguir <em>ODESolve</em> , lo que aumenta linealmente el consumo de memoria con el número de pasos). <br><br>  Los autores utilizan el método implícito Runge-Kutta con un tamaño de paso adaptativo en su implementación, a diferencia del método más simple de Euler aquí.  También estudian algunas propiedades de la nueva arquitectura. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/li/fq/u7lifqdsf72otgcyfigm6-fiyww.png"></div><br>  <i>Función ODE-Net (NFE Forward - el número de cálculos de funciones en un pase directo)</i> <i><br></i>  <i>Ilustración del artículo original.</i> <br><br><ul><li>  (a) Cambiar el nivel aceptable de error numérico cambia el número de pasos en la distribución directa. <br></li><li>  (b) El tiempo dedicado a la distribución directa es proporcional al número de cálculos de la función. <br></li><li>  (c) El número de cálculos de la función para la propagación inversa es aproximadamente la mitad de la propagación directa, lo que indica que el método de gradiente conjugado puede ser más eficiente computacionalmente que propagar el gradiente directamente a través de <em>ODESolve</em> . <br></li><li>  (d) A medida que ODE-Net se entrena cada vez más, requiere más y más cálculos de una función (un paso cada vez más pequeño), posiblemente adaptándose a la creciente complejidad del modelo. <br></li></ul><br><br><h2>  Función Generadora Oculta para Modelado de Series de Tiempo </h2><br>  Neural ODE es adecuado para procesar datos en serie continuos incluso cuando la ruta se encuentra en un espacio oculto desconocido. <br><br>  En esta sección, experimentaremos <em>y</em> cambiaremos la generación de secuencias continuas usando <em>Neural ODE</em> , y observaremos el espacio oculto aprendido. <br><br>  Los autores también comparan esto con secuencias similares generadas por redes recurrentes. <br><br>  El experimento aquí es ligeramente diferente del ejemplo correspondiente en el repositorio de autores, aquí hay un conjunto más diverso de caminos. <br><br><h3>  Datos </h3><br>  Los datos de entrenamiento consisten en espirales aleatorias, la mitad de las cuales son en sentido horario, y la segunda en sentido antihorario.  Además, las subsecuencias aleatorias se muestrean a partir de estas espirales, procesadas por el modelo de recurrencia de codificación en la dirección opuesta, dando lugar a un estado oculto inicial, que luego evoluciona, creando una trayectoria en el espacio oculto.  Esta ruta latente se asigna al espacio de datos y se compara con la subsecuencia muestreada.  Por lo tanto, el modelo aprende a generar rutas similares a un conjunto de datos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4z/lg/or4zlgwkqjm-kzhvlmqtzmeqnl4.png"></div><br>  <i>Ejemplos de espirales de conjuntos de datos</i> <br><br><h3>  VAE como modelo generativo </h3><br>  Modelo generativo a través de un procedimiento de muestreo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fde/a91/b7b/fdea91b7b77af8c87a50e6c9e361f13b.svg" alt="z_ {t_0} \ sim \ mathcal {N} (0, I)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb0/9b2/ad7/fb09b2ad71d820266be632be7c5b5f97.svg" alt="z_ {t_1}, z_ {t_2}, ..., z_ {t_M} = \ text {ODESolve} (z_ {t_0}, f, \ theta_f, t_0, ..., t_M)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/477/5a1/439/4775a14397c7842e67c0756d585c3d5b.svg" alt="x_ {t_i} \ sim p (x \ mid z_ {t_i}; \ theta_x)"></div><br>  Que se puede entrenar usando el enfoque variador de codificador automático. <br><ol><li>  Ir a través de un codificador recurrente a través de una secuencia de tiempo en el tiempo para obtener los parámetros <img src="https://habrastorage.org/getpro/habr/post_images/26b/84a/1dd/26b84a1dd2d618b4dd85d805e95b5db3.svg" alt="\ mu_ {z_ {t_0}}">  , <img src="https://habrastorage.org/getpro/habr/post_images/a1f/9e2/8ac/a1f9e28acee2636547023da51c1af36e.svg" alt="\ sigma_ {z_ {t_0}}">  distribución posterior variacional, y luego muestra de ella: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/816/997/3be/8169973bece43135c717f8abd5088e4c.svg" alt="z_ {t_0} \ sim q \ left (z_ {t_0} \ mid x_ {t_0}, ..., x_ {t_M}; t_0, ..., t_M; \ theta_q \ right) = \ mathcal {N} \ izquierda (z_ {t_0} \ mid \ mu_ {z_ {t_0}} \ sigma_ {z_ {t_0}} \ right)"></div><br><ol><li>  Obtener trayectoria oculta: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/810/833/416810833bb304242b3ec7bccfeb91ad.svg" alt="z_ {t_1}, z_ {t_2}, ..., z_ {t_N} = \ text {ODESolve} (z_ {t_0}, f, \ theta_f, t_0, ..., t_N), \ text {where} \ frac {dz} {dt} = f (z, t; \ theta_f)"></div><br><ol><li>  Asigne una ruta oculta a una ruta en los datos utilizando otra red neuronal: <img src="https://habrastorage.org/getpro/habr/post_images/fd1/2b0/73d/fd12b073dd1cede0a98b312c0e3240d1.svg" alt="\ hat {x_ {t_i}} (z_ {t_i}, t_i; \ theta_x)"><br></li><li>  Maximice la evaluación del límite inferior de validez (ELBO) para la ruta muestreada: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9cf/838/1e3/9cf8381e35978cf70219a1f3bea211b6.svg" alt="\ text {ELBO} \ approx N \ Big (\ sum_ {i = 0} ^ {M} \ log p (x_ {t_i} \ mid z_ {t_i} (z_ {t_0}; \ theta_f); \ theta_x) + KL \ left (q (z_ {t_0} \ mid x_ {t_0}, ..., x_ {t_M}; t_0, ..., t_M; \ theta_q) \ parallel \ mathcal {N} (0, I) \ derecha) \ Big)"></div><br>  Y en el caso de una distribución posterior gaussiana <img src="https://habrastorage.org/getpro/habr/post_images/9a9/404/aa0/9a9404aa0267a6c257815abc794e95c3.svg" alt="p (x \ mid z_ {t_i}; \ theta_x)">  y nivel de ruido conocido <img src="https://habrastorage.org/getpro/habr/post_images/87a/7c9/aba/87a7c9abaa12e58e75ef64aa8312050e.svg" alt="\ sigma_x">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7b0/a0d/c43/7b0a0dc43667f287d584b6865afa1cfb.svg" alt="\ text {ELBO} \ approx -N \ Big (\ sum_ {i = 1} ^ {M} \ frac {(x_i - \ hat {x_i}) ^ 2} {\ sigma_x ^ 2} - \ log \ sigma_ { z_ {t_0}} ^ 2 + \ mu_ {z_ {t_0}} ^ 2 + \ sigma_ {z_ {t_0}} ^ 2 \ Big) + C"></div><br>  El gráfico de cálculo de un modelo ODE oculto se puede representar de la siguiente manera <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/57/ui/zl57uisgnvjy7-y94fkker9m1eq.png"></div><br>  <i>Ilustración del artículo original.</i> <br><br>  Este modelo se puede probar para ver cómo interpola la ruta utilizando solo las observaciones iniciales. <br><br><div class="spoiler">  <b class="spoiler_title">Código</b> <div class="spoiler_text"><h3>  Definir modelos </h3><br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RNNEncoder</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_dim, hidden_dim, latent_dim)</span></span></span><span class="hljs-function">:</span></span> super(RNNEncoder, self).__init__() self.input_dim = input_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim self.rnn = nn.GRU(input_dim+<span class="hljs-number"><span class="hljs-number">1</span></span>, hidden_dim) self.hid2lat = nn.Linear(hidden_dim, <span class="hljs-number"><span class="hljs-number">2</span></span>*latent_dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Concatenate time to input t = t.clone() t[1:] = t[:-1] - t[1:] t[0] = 0. xt = torch.cat((x, t), dim=-1) _, h0 = self.rnn(xt.flip((0,))) # Reversed # Compute latent dimension z0 = self.hid2lat(h0[0]) z0_mean = z0[:, :self.latent_dim] z0_log_var = z0[:, self.latent_dim:] return z0_mean, z0_log_var class NeuralODEDecoder(nn.Module): def __init__(self, output_dim, hidden_dim, latent_dim): super(NeuralODEDecoder, self).__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim func = NNODEF(latent_dim, hidden_dim, time_invariant=True) self.ode = NeuralODE(func) self.l2h = nn.Linear(latent_dim, hidden_dim) self.h2o = nn.Linear(hidden_dim, output_dim) def forward(self, z0, t): zs = self.ode(z0, t, return_whole_sequence=True) hs = self.l2h(zs) xs = self.h2o(hs) return xs class ODEVAE(nn.Module): def __init__(self, output_dim, hidden_dim, latent_dim): super(ODEVAE, self).__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim self.encoder = RNNEncoder(output_dim, hidden_dim, latent_dim) self.decoder = NeuralODEDecoder(output_dim, hidden_dim, latent_dim) def forward(self, x, t, MAP=False): z_mean, z_log_var = self.encoder(x, t) if MAP: z = z_mean else: z = z_mean + torch.randn_like(z_mean) * torch.exp(0.5 * z_log_var) x_p = self.decoder(z, t) return x_p, z, z_mean, z_log_var def generate_with_seed(self, seed_x, t): seed_t_len = seed_x.shape[0] z_mean, z_log_var = self.encoder(seed_x, t[:seed_t_len]) x_p = self.decoder(z_mean, t) return x_p</span></span></code> </pre><br><br><h3>  Generación de conjunto de datos </h3><br><br><pre> <code class="python hljs">t_max = <span class="hljs-number"><span class="hljs-number">6.29</span></span>*<span class="hljs-number"><span class="hljs-number">5</span></span> n_points = <span class="hljs-number"><span class="hljs-number">200</span></span> noise_std = <span class="hljs-number"><span class="hljs-number">0.02</span></span> num_spirals = <span class="hljs-number"><span class="hljs-number">1000</span></span> index_np = np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n_points, <span class="hljs-number"><span class="hljs-number">1</span></span>, dtype=np.int) index_np = np.hstack([index_np[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]]) times_np = np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, t_max, num=n_points) times_np = np.hstack([times_np[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]] * num_spirals) times = torch.from_numpy(times_np[:, :, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]).to(torch.float32) <span class="hljs-comment"><span class="hljs-comment"># Generate random spirals parameters normal01 = torch.distributions.Normal(0, 1.0) x0 = Variable(normal01.sample((num_spirals, 2))) * 2.0 W11 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05 W22 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05 W21 = -1.0 * normal01.sample((num_spirals,)).abs() W12 = 1.0 * normal01.sample((num_spirals,)).abs() xs_list = [] for i in range(num_spirals): if i % 2 == 1: # Make it counter-clockwise W21, W12 = W12, W21 func = LinearODEF(Tensor([[W11[i], W12[i]], [W21[i], W22[i]]])) ode = NeuralODE(func) xs = ode(x0[i:i+1], times[:, i:i+1], return_whole_sequence=True) xs_list.append(xs) orig_trajs = torch.cat(xs_list, dim=1).detach() samp_trajs = orig_trajs + torch.randn_like(orig_trajs) * noise_std samp_ts = times fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 9)) axes = axes.flatten() for i, ax in enumerate(axes): ax.scatter(samp_trajs[:, i, 0], samp_trajs[:, i, 1], c=samp_ts[:, i, 0], cmap=cm.plasma) plt.show() import numpy.random as npr def gen_batch(batch_size, n_sample=100): n_batches = samp_trajs.shape[1] // batch_size time_len = samp_trajs.shape[0] n_sample = min(n_sample, time_len) for i in range(n_batches): if n_sample &gt; 0: probs = [1. / (time_len - n_sample)] * (time_len - n_sample) t0_idx = npr.multinomial(1, probs) t0_idx = np.argmax(t0_idx) tM_idx = t0_idx + n_sample else: t0_idx = 0 tM_idx = time_len frm, to = batch_size*i, batch_size*(i+1) yield samp_trajs[t0_idx:tM_idx, frm:to], samp_ts[t0_idx:tM_idx, frm:to]</span></span></code> </pre><br><br><h3>  Entrenamiento </h3><br><br><pre> <code class="python hljs">vae = ODEVAE(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) vae = vae.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: vae = vae.cuda() optim = torch.optim.Adam(vae.parameters(), betas=(<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.999</span></span>), lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>) preload = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">20000</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">100</span></span> plot_traj_idx = <span class="hljs-number"><span class="hljs-number">1</span></span> plot_traj = orig_trajs[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] plot_obs = samp_trajs[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] plot_ts = samp_ts[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: plot_traj = plot_traj.cuda() plot_obs = plot_obs.cuda() plot_ts = plot_ts.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> preload: vae.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">"models/vae_spirals.sd"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): losses = [] train_iter = gen_batch(batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train_iter: optim.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: x, t = x.cuda(), t.cuda() max_len = np.random.choice([<span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>]) permutation = np.random.permutation(t.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) np.random.shuffle(permutation) permutation = np.sort(permutation[:max_len]) x, t = x[permutation], t[permutation] x_p, z, z_mean, z_log_var = vae(x, t) z_var = torch.exp(z_log_var) kl_loss = <span class="hljs-number"><span class="hljs-number">-0.5</span></span> * torch.sum(<span class="hljs-number"><span class="hljs-number">1</span></span> + z_log_var - z_mean**<span class="hljs-number"><span class="hljs-number">2</span></span> - z_var, <span class="hljs-number"><span class="hljs-number">-1</span></span>) loss = <span class="hljs-number"><span class="hljs-number">0.5</span></span> * ((x-x_p)**<span class="hljs-number"><span class="hljs-number">2</span></span>).sum(<span class="hljs-number"><span class="hljs-number">-1</span></span>).sum(<span class="hljs-number"><span class="hljs-number">0</span></span>) / noise_std**<span class="hljs-number"><span class="hljs-number">2</span></span> + kl_loss loss = torch.mean(loss) loss /= max_len loss.backward() optim.step() losses.append(loss.item()) print(<span class="hljs-string"><span class="hljs-string">f"Epoch </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{epoch_idx}</span></span></span><span class="hljs-string">"</span></span>) frm, to, to_seed = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span> seed_trajs = samp_trajs[frm:to_seed] ts = samp_ts[frm:to] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: seed_trajs = seed_trajs.cuda() ts = ts.cuda() samp_trajs_p = to_np(vae.generate_with_seed(seed_trajs, ts)) fig, axes = plt.subplots(nrows=<span class="hljs-number"><span class="hljs-number">3</span></span>, ncols=<span class="hljs-number"><span class="hljs-number">3</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(axes): ax.scatter(to_np(seed_trajs[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(seed_trajs[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>]), c=to_np(ts[frm:to_seed, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), cmap=cm.plasma) ax.plot(to_np(orig_trajs[frm:to, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(orig_trajs[frm:to, i, <span class="hljs-number"><span class="hljs-number">1</span></span>])) ax.plot(samp_trajs_p[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>], samp_trajs_p[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>]) plt.show() print(np.mean(losses), np.median(losses)) clear_output(wait=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) spiral_0_idx = <span class="hljs-number"><span class="hljs-number">3</span></span> spiral_1_idx = <span class="hljs-number"><span class="hljs-number">6</span></span> homotopy_p = Tensor(np.linspace(<span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]) vae = vae <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: homotopy_p = homotopy_p.cuda() vae = vae.cuda() spiral_0 = orig_trajs[:, spiral_0_idx:spiral_0_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] spiral_1 = orig_trajs[:, spiral_1_idx:spiral_1_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] ts_0 = samp_ts[:, spiral_0_idx:spiral_0_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] ts_1 = samp_ts[:, spiral_1_idx:spiral_1_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: spiral_0, ts_0 = spiral_0.cuda(), ts_0.cuda() spiral_1, ts_1 = spiral_1.cuda(), ts_1.cuda() z_cw, _ = vae.encoder(spiral_0, ts_0) z_cc, _ = vae.encoder(spiral_1, ts_1) homotopy_z = z_cw * (<span class="hljs-number"><span class="hljs-number">1</span></span> - homotopy_p) + z_cc * homotopy_p t = torch.from_numpy(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>*np.pi, <span class="hljs-number"><span class="hljs-number">200</span></span>)) t = t[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].expand(<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)[:, :, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].cuda() t = t.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> t hom_gen_trajs = vae.decoder(homotopy_z, t) fig, axes = plt.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, ncols=<span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(axes): ax.plot(to_np(hom_gen_trajs[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(hom_gen_trajs[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>])) plt.show() torch.save(vae.state_dict(), <span class="hljs-string"><span class="hljs-string">"models/vae_spirals.sd"</span></span>)</code> </pre><br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Eso es lo que sucede después de una noche de entrenamiento. </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fv/pn/pi/fvpnpimixf3sq0cezhepgzh2txy.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los puntos son observaciones ruidosas de la trayectoria original (azul), el </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">amarillo son trayectorias reconstruidas e interpoladas, utilizando puntos como entradas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El color del punto muestra el tiempo. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las reconstrucciones de algunos ejemplos no se ven muy bien. </font><font style="vertical-align: inherit;">Tal vez el modelo no sea lo suficientemente complejo o no se estudie lo suficiente. </font><font style="vertical-align: inherit;">En cualquier caso, la reconstrucción parece muy razonable. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ahora veamos qué sucede si interpolamos una variable oculta en una trayectoria en sentido horario a una trayectoria en sentido antihorario.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/g_/7e/atg_7ecofins-uohnv99qccfxfq.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los autores también comparan las reconstrucciones y las interpolaciones de ruta entre </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ODE neuronal</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y una red recursiva simple.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kj/ak/-e/kjak-evgr-7mrrtpgimf0gfmfxq.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ilustración del artículo original.</font></font></i> <br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Corrientes de normalización continua </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El artículo original también aporta mucho al tema de Normalización de flujos. Los flujos de normalización se utilizan cuando necesita muestrear a partir de una distribución compleja que aparece a través del cambio de variables a partir de una distribución simple (gaussiana, por ejemplo), y aún así conocer la densidad de probabilidad en el punto de cada muestra. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los autores muestran que el uso de la sustitución continua de variables es mucho más computacionalmente eficiente e interpretable que los métodos anteriores. </font></font><br><br> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los flujos de normalización son</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> muy útiles en modelos como </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Variation AutoCoders</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bayesian Neural Networks</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y otros del enfoque bayesiano. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este tema, sin embargo, queda fuera del alcance de </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artículos, y aquellos que estén interesados ​​deben leer el artículo científico original. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para semillas:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nd/e_/wn/nde_wn590scz9dff_0hzxo1-tgg.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualización de la transformación de ruido (distribución simple) a datos (distribución compleja) para dos conjuntos de datos; </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El eje X muestra la transformación de la densidad y las muestras en el transcurso del "tiempo" (para NS) y la "profundidad" (para NS). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ilustración del artículo original</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gracias a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bekemax</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por su ayuda en la edición de la versión en inglés del texto y por sus interesantes comentarios físicos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esto concluye mi pequeña investigación sobre </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">las EDO neurales</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Gracias por su atencion!</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Enlaces utiles </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Repositorio con laptop + Inglés </font><font style="vertical-align: inherit;">version</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artículo original</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Repositorio del autor</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusión variacional</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mi artículo sobre VAE (ruso)</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Explicación VAE</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Más sobre normalizar flujos</font></font></a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Variational Inference with Normalizing Flows Paper</a> <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442002/">https://habr.com/ru/post/442002/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../441990/index.html">La forma más efectiva de combatir la piratería: servicios legales convenientes y baratos</a></li>
<li><a href="../441992/index.html">Elegir un regalo para una chica geek</a></li>
<li><a href="../441994/index.html">NASA: la cantidad de planetas habitables en nuestra galaxia es mucho menor de lo que comúnmente se cree</a></li>
<li><a href="../441996/index.html">Tecnología de los años 80: quién revive los procesadores de escamas wafers</a></li>
<li><a href="../441998/index.html">"Los contenedores ganaron la batalla, pero pierden la guerra en la arquitectura sin servidor", - Simon Wardley</a></li>
<li><a href="../442004/index.html">Efectos de filtrado SVG. Parte 7. Adelante</a></li>
<li><a href="../442006/index.html">Gestión de archivos mal hecha - Parte 2: Obra maestra de mierda</a></li>
<li><a href="../442008/index.html">k3s es un Kubernetes pequeño pero certificado por Rancher Labs</a></li>
<li><a href="../442010/index.html">Python y FPGA. Prueba</a></li>
<li><a href="../442012/index.html">Experimento: recopilamos un directorio de unidades que emitieron un pasaporte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>