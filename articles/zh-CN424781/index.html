<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👸🏾 👆🏼 👨🏽‍⚕️ 使用Ignite在PyTorch上教学和测试神经网络 🍚 🗿 🆗</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="嗨，Habr，在本文中，我将讨论ignite库，您可以使用该库使用PyTorch框架轻松地训练和测试神经网络。 


使用ignite，您可以编写周期来训练网络，只需几行，从框中添加标准度量计算，保存模型，等等。 好吧，对于那些从TF转到PyTorch的人来说，我们可以说点火库是PyTorch的Ke...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用Ignite在PyTorch上教学和测试神经网络</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/424781/"><p>  <em>嗨，Habr，在本文中，我将讨论<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库，您可以使用该库使用PyTorch框架轻松地训练和测试神经网络。</em> </p><br><p>使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite，</a>您可以编写周期来训练网络，只需几行，从框中添加标准度量计算，保存模型，等等。 好吧，对于那些从TF转到PyTorch的人来说，我们可以说<em>点火</em>库是PyTorch的Keras。 </p><br><p> 本文将详细研究一个使用<em>ignite</em>训练神经网络进行分类任务的示例 </p><br><p><img src="https://habrastorage.org/webt/35/ar/af/35arafc8y9aicrbpz5unazs-y-a.png"></p><a name="habracut"></a><br><h2 id="dobavim-esche-bolshe-ognya-v-pytorch"> 向PyTorch添加更多火力 </h2><br><p> 我不会浪费时间谈论PyTorch框架有多<em>酷</em> 。 任何使用过它的人都知道我在写什么。 但是，尽管具有所有优点，但在用于训练，测试和测试神经网络的编写周期方面仍处于低水平。 </p><br><p> 如果我们看一下使用PyTorch框架的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方示例</a> ，我们将看到至少两个迭代周期，分别是网格训练代码中的时期和成批的训练集： </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, epochs + <span class="hljs-number"><span class="hljs-number">1</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): <span class="hljs-comment"><span class="hljs-comment"># ...</span></span></code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库的主要思想是将这些循环分解为一个类，同时允许用户使用事件处理程序与这些循环进行交互。 </p><br><p> 因此，在标准深度学习任务的情况下，我们可以节省很多代码行。 更少的行-更少的错误！ </p><br><p> 例如，为了进行比较，左侧是使用<em>ignite</em>进行训练和模型验证的代码，右侧是纯PyTorch： <br><img src="https://habrastorage.org/getpro/habr/post_images/914/408/b07/914408b07fa093c696e66cb15ae36bfc.png" alt="图片"></p><br><p> 那么， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">点燃</a>又有什么<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">用</a>呢？ </p><br><ul><li> 您不再需要为每个任务循环编写<code>for epoch in range(n_epochs)</code>和<code>for epoch in range(n_epochs)</code> <code>for batch in data_loader</code> 。 </li><li> 使您可以更好地分解代码 </li><li> 允许您开箱即用地计算基本指标 </li><li> 提供类型的“小圆面包” <br><ul><li> 在训练过程中保存最新和最好的模型（以及优化程序和学习率调度程序）， </li><li> 提前停止学习 </li><li> 等 </li></ul></li><li> 轻松与可视化工具集成：tensorboardX，visdom，... </li></ul><br><p> 从某种意义上说，可以将<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库与所有著名的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Keras</a>及其用于培训和测试网络的API进行比较。 同样，乍看之下的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库与<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">tnt</a>库非常相似，因为最初这两个库都有共同的目标，并且对于实现它们有相似的想法。 </p><br><p> 因此，点亮： </p><br><pre> <code class="hljs sql">pip <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> pytorch-ignite</code> </pre> <br><p> 或 </p><br><pre> <code class="hljs swift">conda install ignite -<span class="hljs-built_in"><span class="hljs-built_in">c</span></span> pytorch</code> </pre> <br><p> 接下来，通过一个特定的示例，我们将熟悉<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">API</a> 。 </p><br><h2 id="zadacha-klassifikacii-s-ignitehttpspytorchorgignite"> 带<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">点火的</a>分类任务 </h2><br><p> 在本文的这一部分中，我们将考虑一个<em>学校</em>示例，该示例使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库为分类问题训练神经网络。 </p><br><p> 因此，让我们使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kaggle</a>拍摄带有水果图片的简单数据集。 任务是将对应的类别与每个水果图片相关联。 </p><br><p> 在使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>之前，让我们定义主要组件： </p><br><p> 数据流 </p><br><ul><li> 培训样品分<code>train_loader</code>装载机<code>train_loader</code> </li><li> 结帐批处理下载器<code>val_loader</code> </li></ul><br><p> 型号： </p><br><ul><li> 从<code>torchvision</code>获得小的squeezeNet网格 </li></ul><br><p> 优化算法： </p><br><ul><li>  sgd </li></ul><br><p> 损失函数： </p><br><ul><li> 交叉熵 </li></ul><br><div class="spoiler">  <b class="spoiler_title">代号</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dataset, DataLoader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data.dataset <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Subset <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageFolder <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.transforms <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Compose, RandomResizedCrop, RandomVerticalFlip, RandomHorizontalFlip <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.transforms <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ColorJitter, ToTensor, Normalize FRUIT360_PATH = Path(<span class="hljs-string"><span class="hljs-string">"."</span></span>).resolve().parent / <span class="hljs-string"><span class="hljs-string">"input"</span></span> / <span class="hljs-string"><span class="hljs-string">"fruits-360_dataset"</span></span> / <span class="hljs-string"><span class="hljs-string">"fruits-360"</span></span> device = <span class="hljs-string"><span class="hljs-string">"cuda"</span></span> train_transform = Compose([ RandomHorizontalFlip(), RandomResizedCrop(size=<span class="hljs-number"><span class="hljs-number">32</span></span>), ColorJitter(brightness=<span class="hljs-number"><span class="hljs-number">0.12</span></span>), ToTensor(), Normalize(mean=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>], std=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) ]) val_transform = Compose([ RandomResizedCrop(size=<span class="hljs-number"><span class="hljs-number">32</span></span>), ToTensor(), Normalize(mean=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>], std=[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) ]) batch_size = <span class="hljs-number"><span class="hljs-number">128</span></span> num_workers = <span class="hljs-number"><span class="hljs-number">8</span></span> train_dataset = ImageFolder((FRUIT360_PATH /<span class="hljs-string"><span class="hljs-string">"Training"</span></span>).as_posix(), transform=train_transform, target_transform=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) val_dataset = ImageFolder((FRUIT360_PATH /<span class="hljs-string"><span class="hljs-string">"Test"</span></span>).as_posix(), transform=val_transform, target_transform=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device) val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchvision.models.squeezenet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> squeezenet1_1 model = squeezenet1_1(pretrained=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">81</span></span>) model.classifier[<span class="hljs-number"><span class="hljs-number">-1</span></span>] = nn.AdaptiveAvgPool2d(<span class="hljs-number"><span class="hljs-number">1</span></span>) model = model.to(device)</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.optim <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD optimizer = SGD(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) criterion = nn.CrossEntropyLoss()</code> </pre> </div></div><br><p> 因此，现在该运行<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignit了</a> ： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Engine, _prepare_batch <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">process_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.train() optimizer.zero_grad() x, y = _prepare_batch(batch, device=device) y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() trainer = Engine(process_function)</code> </pre> <br><p> 让我们看看这段代码的含义。 </p><br><h3 id="dvizhok-engine"> 引擎<code>Engine</code> </h3><br><p>  <code>ignite.engine.Engine</code>类是库框架，此类的对象是<code>ignite.engine.Engine</code> ： </p><br><pre> <code class="python hljs">trainer = Engine(process_function)</code> </pre> <br><p> 它由输入函数<code>process_function</code>定义，用于处理一批，并用于实现训练样本的传递。 在<code>ignite.engine.Engine</code>类中，发生以下情况： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epoch &lt; max_epochs: <span class="hljs-comment"><span class="hljs-comment"># run once on data for batch in data: output = process_function(batch)</span></span></code> </pre> <br><p> 返回<code>process_function</code>函数： </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">process_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.train() optimizer.zero_grad() x, y = _prepare_batch(batch, device=device) y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item()</code> </pre> <br><p> 我们看到，在函数内部，像在模型训练中一样，我们可以计算<code>y_pred</code>预测，计算损失函数， <code>loss</code>和梯度。 后者允许您更新模型权重： <code>optimizer.step()</code> 。 </p><br><p> 通常，对<code>process_function</code>函数的代码没有限制。 我们只注意到它以两个参数作为输入： <code>Engine</code>对象（在我们的例子中是<code>trainer</code> ）和来自数据加载器的批处理。 因此，例如，对于测试神经网络，我们可以定义<code>ignite.engine.Engine</code>类的另一个对象，在该对象中，输入函数仅计算预测值，并一次执行一次测试采样。 稍后再阅读。 </p><br><p> 因此，以上代码仅定义了必要的对象，而没有开始进行培训。 基本上，在一个最小的示例中，您可以调用方法： </p><br><pre> <code class="python hljs">trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><p> 并且此代码足以“安静地”（无需任何中间结果的派生）训练模型。 </p><br><div class="spoiler">  <b class="spoiler_title">笔记</b> <div class="spoiler_text"><p> 还请注意，对于此类任务，库提供了一种方便的方法来创建<code>trainer</code>对象： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_supervised_trainer trainer = create_supervised_trainer(model, optimizer, criterion, device)</code> </pre> </div></div><br><p> 当然，实际上，上面的示例没有什么意义，所以让我们为“教练”添加以下选项： </p><br><ul><li> 每50次迭代显示损失函数值 </li><li> 开始使用固定模型对训练集中的指标进行计算 </li><li> 在每个时代之后开始计算测试样本的指标 </li><li> 在每个时代之后保存模型参数 </li><li> 保留三种最佳模式 </li><li> 学习速度因时代而异（学习率安排） </li><li> 提前停止训练（提前停止） </li></ul><br><h3 id="sobytiya-i-obrabotchiki-sobytiy"> 事件和事件处理程序 </h3><br><p> 为了为“ trainer”添加以上选项， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>库提供了事件系统和自定义事件处理程序的启动。 因此，用户可以在每个阶段控制<code>Engine</code>类的对象： </p><br><ul><li> 引擎启动/完成启动 </li><li> 时代开始/结束 </li><li> 批处理迭代开始/结束 </li></ul><br><p> 并在每个事件上运行您的代码。 </p><br><h4 id="vyvod-na-ekran-znacheniya-funkcii-poter"> 显示损失函数值 </h4><br><p> 为此，您只需要确定输出将在屏幕上显示的功能，并将其添加到“培训师”即可： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Events log_interval = <span class="hljs-number"><span class="hljs-number">50</span></span> @trainer.on(Events.ITERATION_COMPLETED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_training_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> iteration = (engine.state.iteration - <span class="hljs-number"><span class="hljs-number">1</span></span>) % len(train_loader) + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> iteration % log_interval == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">"Epoch[{}] Iteration[{}/{}] Loss: {:.4f}"</span></span> .format(engine.state.epoch, iteration, len(train_loader), engine.state.output))</code> </pre> <br><p> 实际上，有两种添加事件处理程序的方法：通过<code>add_event_handler</code>或通过<code>on</code>装饰器。 与上面相同，可以这样进行： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Events log_interval = <span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_training_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># ... trainer.add_event_handler(Events.ITERATION_COMPLETED, log_training_loss)</span></span></code> </pre> <br><p> 请注意，任何参数都可以传递给事件处理函数。 通常，这样的功能将如下所示： </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">custom_handler</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, *args, **kwargs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> trainer.add_event_handler(Events.ITERATION_COMPLETED, custom_handler, *args, **kwargs) <span class="hljs-comment"><span class="hljs-comment">#  @trainer.on(Events.ITERATION_COMPLETED, *args, **kwargs) def custom_handler(engine, *args, **kwargs): pass</span></span></code> </pre> <br><p> 因此，让我们开始一个时代的训练，看看会发生什么： </p><br><pre> <code class="python hljs">output = trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[50/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.3459</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[100/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.2801</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[150/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.2294</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[200/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 4<span class="hljs-selector-class"><span class="hljs-selector-class">.1467</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[250/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 3<span class="hljs-selector-class"><span class="hljs-selector-class">.8607</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Epoch</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[1]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Iteration</span></span><span class="hljs-selector-attr"><span class="hljs-selector-attr">[300/322]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">Loss</span></span>: 3<span class="hljs-selector-class"><span class="hljs-selector-class">.6688</span></span></code> </pre> <br><p> 还不错！ 让我们走得更远。 </p><br><h4 id="zapusk-rascheta-metrik-na-obuchayuschey-i-testovoy-vyborkah"> 开始计算训练和测试样本的指标 </h4><br><p> 让我们计算以下指标：平均准确性，每个训练阶段和整个测试样本在每个时代之后的平均完整性。 请注意，我们将在每个训练时代之后（而不是在训练期间）在训练样本部分计算指标。 因此，效率模型的计算将更加准确，因为模型在计算过程中不会发生变化。 </p><br><p> 因此，我们定义了指标： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Loss, CategoricalAccuracy, Precision, Recall metrics = { <span class="hljs-string"><span class="hljs-string">'avg_loss'</span></span>: Loss(criterion), <span class="hljs-string"><span class="hljs-string">'avg_accuracy'</span></span>: CategoricalAccuracy(), <span class="hljs-string"><span class="hljs-string">'avg_precision'</span></span>: Precision(average=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), <span class="hljs-string"><span class="hljs-string">'avg_recall'</span></span>: Recall(average=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) }</code> </pre> <br><p> 接下来，我们将使用<code>ignite.engine.create_supervised_evaluator</code>创建两个引擎来评估模型： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.engine <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_supervised_evaluator <span class="hljs-comment"><span class="hljs-comment"># ,  device = “cuda”    train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device) val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)</span></span></code> </pre> <br><p> 我们正在创建两个引擎，以便将附加的事件处理程序进一步附加到其中一个（ <code>val_evaluator</code> ）以保存模型并尽早停止学习（有关以下所有内容）。 </p><br><p> 我们还仔细研究一下如何定义用于评估模型的引擎，即如何定义输入函数<code>process_function</code>以处理一批： </p><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_supervised_evaluator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, metrics={}, device=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> device: model.to(device) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_inference</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> model.eval() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): x, y = _prepare_batch(batch, device=device) y_pred = model(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> y_pred, y engine = Engine(_inference) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> name, metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> metrics.items(): metric.attach(engine, name) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> engine</code> </pre> <br><p> 我们继续。 让我们随机选择训练样本中要计算指标的部分： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data.dataset <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Subset indices = np.arange(len(train_dataset)) random_indices = np.random.permutation(indices)[:len(val_dataset)] train_subset = Subset(train_dataset, indices=random_indices) train_eval_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre> <br><p> 接下来，让我们确定在培训的什么时候开始度量的计算并输出到屏幕： </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@trainer.on(Events.EPOCH_COMPLETED) def compute_and_display_offline_train_metrics(engine): epoch = engine.state.epoch print("Compute train metrics...") metrics = train_evaluator.run(train_eval_loader).metrics print("Training Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}" .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall'])) @trainer.on(Events.EPOCH_COMPLETED) def compute_and_display_val_metrics(engine): epoch = engine.state.epoch print("Compute validation metrics...") metrics = val_evaluator.run(val_loader).metrics print("Validation Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}" .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall']))</span></span></code> </pre><br><p> 你可以跑！ </p><br><pre> <code class="python hljs">output = trainer.run(train_loader, max_epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p> 我们在屏幕上 </p><br><pre> <code class="hljs powershell">Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">3.5112</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.9840</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.8807</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.9285</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.5026</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1944</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">2.1018</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.3699</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.3981</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.3686</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">2.0519</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.3850</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.3578</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.3845</span></span></code> </pre><br><p> 已经更好了！ </p><br><p>  <strong>一些细节</strong> <br> 让我们再看一下前面的代码。 读者可能已经注意到以下代码行： </p><br><pre> <code class="python hljs">metrics = train_evaluator.run(train_eval_loader).metrics</code> </pre> <br><p> 可能存在一个关于从<code>train_evaluator.run(train_eval_loader)</code>获得的对象类型的问题，该对象具有<code>metrics</code>属性。 </p><br><p> 实际上， <code>Engine</code>类包含一个称为<code>state</code> （类型<code>State</code> ）的结构，以便能够在事件处理程序之间传输数据。 此<code>state</code>属性包含有关当前时代，迭代，时代数等的基本信息。 它也可以用于传输任何用户数据，包括度量标准的计算结果。 </p><br><pre> <code class="python hljs">state = train_evaluator.run(train_eval_loader) metrics = state.metrics <span class="hljs-comment"><span class="hljs-comment">#   train_evaluator.run(train_eval_loader) metrics = train_evaluator.state.metrics</span></span></code> </pre> <br><h5 id="raschet-metrik-vo-vremya-obucheniya"> 训练期间指标的计算 </h5><br><p> 如果任务中有大量的训练样本，并且在每个训练时期之后计算指标很昂贵，但是您仍然希望在训练期间看到一些指标变化，则可以从框中使用以下<code>RunningAverage</code>事件处理程序。 例如，我们要计算并显示分类器的准确性： </p><br><pre> <code class="python hljs">acc_metric = RunningAverage(CategoryAccuracy(...), alpha=<span class="hljs-number"><span class="hljs-number">0.98</span></span>) acc_metric.attach(trainer, <span class="hljs-string"><span class="hljs-string">'running_avg_accuracy'</span></span>) @trainer.on(Events.ITERATION_COMPLETED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_running_avg_metrics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> print(<span class="hljs-string"><span class="hljs-string">"running avg accuracy:"</span></span>, engine.state.metrics[<span class="hljs-string"><span class="hljs-string">'running_avg_accuracy'</span></span>])</code> </pre> <br><p> 要使用<code>RunningAverage</code>功能，您需要从以下来源安装<em>ignite</em> ： </p><br><pre> <code class="hljs objectivec">pip install git+https:<span class="hljs-comment"><span class="hljs-comment">//github.com/pytorch/ignite</span></span></code> </pre> <br><h4 id="izmenenie-skorosti-obuchenie-learning-rate-scheduling"> 学习率安排 </h4><br><p> 有多种方法可以使用<em>ignite</em>更改学习速度。 接下来，通过在每个时代开始时调用<code>lr_scheduler.step()</code>函数来考虑最简单的方法。 </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.optim.lr_scheduler <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ExponentialLR lr_scheduler = ExponentialLR(optimizer, gamma=<span class="hljs-number"><span class="hljs-number">0.8</span></span>) @trainer.on(Events.EPOCH_STARTED) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_lr_scheduler</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> lr_scheduler.step() <span class="hljs-comment"><span class="hljs-comment">#    : if len(optimizer.param_groups) == 1: lr = float(optimizer.param_groups[0]['lr']) print("Learning rate: {}".format(lr)) else: for i, param_group in enumerate(optimizer.param_groups): lr = float(param_group['lr']) print("Learning rate (group {}): {}".format(i, lr))</span></span></code> </pre> <br><h4 id="sohranenie-luchshih-modeley-i-drugih-parametrov-vo-vremya-obucheniya"> 在训练过程中保存最佳模型和其他参数 </h4><br><p> 在训练过程中，最好在光盘上记录最佳模型的权重，并定期保存模型权重，优化器参数和用于更改学习速度的参数。 后者对于从上次保存的状态恢复学习很有用。 </p><br><p>  <em>Ignite</em> <code>ModelCheckpoint</code>有一个特殊的<code>ModelCheckpoint</code>类。 因此，让我们创建一个<code>ModelCheckpoint</code>事件<code>ModelCheckpoint</code>并就测试集中的准确性保存最佳模型。 在这种情况下，我们定义一个<code>score_function</code>函数，该函数将精度值提供给事件处理程序，并决定是否保存模型： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.handlers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">score_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine)</span></span></span><span class="hljs-function">:</span></span> val_avg_accuracy = engine.state.metrics[<span class="hljs-string"><span class="hljs-string">'avg_accuracy'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> val_avg_accuracy best_model_saver = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"best_models"</span></span>, filename_prefix=<span class="hljs-string"><span class="hljs-string">"model"</span></span>, score_name=<span class="hljs-string"><span class="hljs-string">"val_accuracy"</span></span>, score_function=score_function, n_saved=<span class="hljs-number"><span class="hljs-number">3</span></span>, save_as_state_dict=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, create_dir=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-comment"><span class="hljs-comment"># "best_models" -    1     #   -&gt; {filename_prefix}_{name}_{step_number}_{score_name}={abs(score_function_result)}.pth # save_as_state_dict=True, #   `state_dict` val_evaluator.add_event_handler(Events.COMPLETED, best_model_saver, {"best_model": model})</span></span></code> </pre> <br><p> 现在创建另一个<code>ModelCheckpoint</code>事件<code>ModelCheckpoint</code> ，以每1000次迭代维护学习状态： </p><br><pre> <code class="python hljs">training_saver = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"checkpoint"</span></span>, filename_prefix=<span class="hljs-string"><span class="hljs-string">"checkpoint"</span></span>, save_interval=<span class="hljs-number"><span class="hljs-number">1000</span></span>, n_saved=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_as_state_dict=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, create_dir=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) to_save = {<span class="hljs-string"><span class="hljs-string">"model"</span></span>: model, <span class="hljs-string"><span class="hljs-string">"optimizer"</span></span>: optimizer, <span class="hljs-string"><span class="hljs-string">"lr_scheduler"</span></span>: lr_scheduler} trainer.add_event_handler(Events.ITERATION_COMPLETED, training_saver, to_save)</code> </pre> <br><p> 因此，几乎一切就绪，添加最后一个元素： </p><br><h4 id="rannyaya-ostanovka-obucheniya-early-stopping"> 提早停止训练（提早停止） </h4><br><p> 让我们添加另一个事件处理程序，如果在10个时代内模型质量没有改善，该事件处理程序将停止学习。 我们将使用score_function <code>score_function</code>再次评估模型的质量。 </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite.handlers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EarlyStopping early_stopping = EarlyStopping(patience=<span class="hljs-number"><span class="hljs-number">10</span></span>, score_function=score_function, trainer=trainer) val_evaluator.add_event_handler(Events.EPOCH_COMPLETED, early_stopping)</code> </pre> <br><h3 id="zapusk-obucheniya"> 开始训练 </h3><br><p> 为了开始训练，我们只要调用<code>run()</code>方法就足够了。 我们将训练模型10个时代： </p><br><pre> <code class="python hljs">max_epochs = <span class="hljs-number"><span class="hljs-number">10</span></span> output = trainer.run(train_loader, max_epochs=max_epochs)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">屏幕输出</b> <div class="spoiler_text"><pre> <code class="hljs powershell">Learning rate: <span class="hljs-number"><span class="hljs-number">0.01</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.7984</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.9736</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">4.3419</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.0261</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1724</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">1</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">2.1599</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">1.5363</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.5177</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.5477</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.5178</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">1</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">1.5116</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.5139</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.5400</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.5140</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.008</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4076</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4892</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.2485</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.6511</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">3.3376</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">2</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.3299</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">2</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">3.2686</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.1977</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.1792</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.1942</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">2</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">3.2772</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.1962</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.1628</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.1918</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.006400000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9016</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.2006</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8892</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8141</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">1.4005</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">3</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8888</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">3</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7368</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7554</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7818</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7554</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">3</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7177</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7623</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7863</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7611</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.005120000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8490</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8493</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.8100</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9165</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.9370</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">4</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6548</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">4</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.7047</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7713</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8040</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7728</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">4</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.6737</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.7778</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.7955</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.7806</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.004096000000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6965</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6196</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6194</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3986</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.6032</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">5</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.7152</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">5</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.5049</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8282</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8393</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8314</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">5</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.5084</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8304</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8386</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8328</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.0032768000000000007</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4433</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4764</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5578</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3684</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4847</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">6</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3811</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">6</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4383</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8474</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8618</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8495</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">6</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4419</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8446</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8532</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8442</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.002621440000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4447</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4602</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5345</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3973</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5023</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">7</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.5303</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">7</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4305</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8579</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8691</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8596</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">7</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.4262</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8590</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8685</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8606</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.002097152000000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4867</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3090</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3721</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4559</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3958</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">8</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4222</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">8</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3432</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8818</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8895</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8817</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">8</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3644</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8713</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8784</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8707</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.001677721600000001</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3557</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3692</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3510</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3446</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3966</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">9</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3451</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">9</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3315</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8954</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.9001</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8982</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">9</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3559</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8818</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8876</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8847</span></span> Learning rate: <span class="hljs-number"><span class="hljs-number">0.0013421772800000006</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3340</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">100</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3370</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">150</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3694</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.3409</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">250</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.4420</span></span> Epoch[<span class="hljs-number"><span class="hljs-number">10</span></span>] Iteration[<span class="hljs-number"><span class="hljs-number">300</span></span>/<span class="hljs-number"><span class="hljs-number">322</span></span>] Loss: <span class="hljs-number"><span class="hljs-number">0.2770</span></span> Compute train metrics... Training Results - Epoch: <span class="hljs-number"><span class="hljs-number">10</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3246</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8921</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8988</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8925</span></span> Compute validation metrics... Validation Results - Epoch: <span class="hljs-number"><span class="hljs-number">10</span></span> Average Loss: <span class="hljs-number"><span class="hljs-number">0.3536</span></span> | Accuracy: <span class="hljs-number"><span class="hljs-number">0.8731</span></span> | Precision: <span class="hljs-number"><span class="hljs-number">0.8785</span></span> | Recall: <span class="hljs-number"><span class="hljs-number">0.8722</span></span></code> </pre> </div></div><br><p> 现在检查保存到磁盘的模型和参数： </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">ls</span></span> best_models/ model_best_model_10_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8730994</span></span>.pth model_best_model_8_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8712978</span></span>.pth model_best_model_9_val_accuracy=<span class="hljs-number"><span class="hljs-number">0.8818188</span></span>.pth</code> </pre> <br><p> 和 </p><br><pre> <code class="hljs pgsql">ls <span class="hljs-keyword"><span class="hljs-keyword">checkpoint</span></span>/ checkpoint_lr_scheduler_3000.pth checkpoint_optimizer_3000.pth checkpoint_model_3000.pth</code> </pre> <br><h3 id="predskazaniya-obuchennoy-modelyu"> 经过训练的模型进行预测 </h3><br><p> 首先，创建一个测试数据加载器（例如，获取一个验证样本），以便数据批处理由图像及其索引组成： </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestDataset</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Dataset)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, ds)</span></span></span><span class="hljs-function">:</span></span> self.ds = ds <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(self.ds) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__getitem__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, index)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.ds[index][<span class="hljs-number"><span class="hljs-number">0</span></span>], index test_dataset = TestDataset(val_dataset) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_workers=num_workers, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pin_memory=<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> device)</code> </pre> <br><p> 使用<em>ignite，</em>我们<em>将为</em>测试数据创建一个新的预测引擎。 为此，我们定义函数<code>inference_update</code> ，该函数返回预测结果和图像索引。 为了提高准确性，我们还将使用众所周知的技巧“测试时间增加”（TTA）。 </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ignite._utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> convert_tensor <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_prepare_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch)</span></span></span><span class="hljs-function">:</span></span> x, index = batch x = convert_tensor(x, device=device) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x, index <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">inference_update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(engine, batch)</span></span></span><span class="hljs-function">:</span></span> x, indices = _prepare_batch(batch) y_pred = model(x) y_pred = F.softmax(y_pred, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> {<span class="hljs-string"><span class="hljs-string">"y_pred"</span></span>: convert_tensor(y_pred, device=<span class="hljs-string"><span class="hljs-string">'cpu'</span></span>), <span class="hljs-string"><span class="hljs-string">"indices"</span></span>: indices} model.eval() inferencer = Engine(inference_update)</code> </pre> <br><p> 接下来，创建事件处理程序，该处理程序将通知有关预测的阶段并将预测保存在专用数组中： </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@inferencer.on(Events.EPOCH_COMPLETED) def log_tta(engine): print("TTA {} / {}".format(engine.state.epoch, n_tta)) n_tta = 3 num_classes = 81 n_samples = len(val_dataset) #     y_probas_tta = np.zeros((n_samples, num_classes, n_tta), dtype=np.float32) @inferencer.on(Events.ITERATION_COMPLETED) def save_results(engine): output = engine.state.output tta_index = engine.state.epoch - 1 start_index = ((engine.state.iteration - 1) % len(test_loader)) * batch_size end_index = min(start_index + batch_size, n_samples) batch_y_probas = output['y_pred'].detach().numpy() y_probas_tta[start_index:end_index, :, tta_index] = batch_y_probas</span></span></code> </pre> <br><p> 在开始该过程之前，让我们下载最佳模型： </p><br><pre> <code class="python hljs">model = squeezenet1_1(pretrained=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">64</span></span>) model.classifier[<span class="hljs-number"><span class="hljs-number">-1</span></span>] = nn.AdaptiveAvgPool2d(<span class="hljs-number"><span class="hljs-number">1</span></span>) model = model.to(device) model_state_dict = torch.load(<span class="hljs-string"><span class="hljs-string">"best_models/model_best_model_10_val_accuracy=0.8730994.pth"</span></span>) model.load_state_dict(model_state_dict)</code> </pre> <br><p> 我们推出： </p><br><pre> <code class="python hljs">inferencer.run(test_loader, max_epochs=n_tta) &gt; TTA <span class="hljs-number"><span class="hljs-number">1</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; TTA <span class="hljs-number"><span class="hljs-number">2</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; TTA <span class="hljs-number"><span class="hljs-number">3</span></span> / <span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p> 接下来，以标准方式，对TTA预测取平均值，并以最高的概率计算类别索引： </p><br><pre> <code class="python hljs">y_probas = np.mean(y_probas_tta, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) y_preds = np.argmax(y_probas, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p> 现在，我们可以根据预测再次计算模型的准确性： </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score y_test_true = [y <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> val_dataset] accuracy_score(y_test_true, y_preds) &gt; <span class="hljs-number"><span class="hljs-number">0.9310369676443035</span></span></code> </pre> <br><p> ,     ,          .   ,   ,      ,    <em>ignite</em>      . </p><br><h2 id="drugie-primery-s-ignitehttpspytorchorgignite">    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a> </h2><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> . </p><br><p>  github      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a>       </p><br><ul><li> fast neural transfer </li><li> reinforcement learning </li><li> dcgan </li></ul><br><h2 id="zaklyuchenie"> 结论 </h2><br><p>    ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ignite</a>      Facebook           (.   ).        0.1.0,   API (Engine, State, Events, Metric, ...)           .       ,      ,     ,     pull request-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> github</a> . </p><br><p> 感谢您的关注！ </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN424781/">https://habr.com/ru/post/zh-CN424781/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN424767/index.html">我们用哈伯做蛋糕。 再来一次</a></li>
<li><a href="../zh-CN424771/index.html">个人经验：从想法和空白表到网站的草稿版本</a></li>
<li><a href="../zh-CN424773/index.html">生物制药和数值模拟：安进的经验和实践</a></li>
<li><a href="../zh-CN424777/index.html">使用领事扩展有状态服务</a></li>
<li><a href="../zh-CN424779/index.html">Python中的多页SPA</a></li>
<li><a href="../zh-CN424787/index.html">2018 RubyRussia会议发言人Aaron Patterson访谈</a></li>
<li><a href="../zh-CN424789/index.html">如何使用HAProxy Ingress，unicorn / puma和Web套接字部署Ruby on Rails应用程序</a></li>
<li><a href="../zh-CN424791/index.html">使用WI-FI扩展可编程继电器的网络功能</a></li>
<li><a href="../zh-CN424793/index.html">如何打印电动机</a></li>
<li><a href="../zh-CN424795/index.html">太阳能无人机有多大？</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>