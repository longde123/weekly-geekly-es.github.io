<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖨️ 👩🏼‍⚕️ 🤞🏼 Stellen Sie verteilten CEPH-Speicher bereit und verbinden Sie ihn mit Kubernetes 💙 🤞🏾 👅</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 1 Wir stellen die Umgebung für die Arbeit mit Microservices bereit. Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian) 
 Hallo, liebe ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Stellen Sie verteilten CEPH-Speicher bereit und verbinden Sie ihn mit Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1 Wir stellen die Umgebung für die Arbeit mit Microservices bereit.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian)</a> </p><br><h2>  Hallo, liebe Leser von Habr! </h2><br><p>  In einem früheren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beitrag habe</a> ich darüber gesprochen, wie ein Kubernetes-Failovercluster bereitgestellt wird.  Tatsache ist jedoch, dass es in Kubernetes praktisch ist, zustandslose Anwendungen bereitzustellen, die ihren Status nicht beibehalten oder nicht mit Daten arbeiten müssen.  In den meisten Fällen müssen wir jedoch Daten speichern und dürfen sie beim Neustart der Herde nicht verlieren. <br>  Kubernetes verwendet für diese Zwecke Volumes.  Wenn wir mit Kubernetes Cloud-Lösungen arbeiten, gibt es keine besonderen Probleme.  Wir müssen nur das erforderliche Volume bei Google, Amazon oder einem anderen Cloud-Anbieter bestellen und gemäß der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> die empfangenen Volumes mit den Pods verbinden. <br>  Wenn wir uns mit Bare Metal beschäftigen, sind die Dinge etwas komplizierter.  Heute möchte ich über eine der Lösungen sprechen, die auf der Verwendung von Ceph basieren. </p><br><p>  In dieser Veröffentlichung werde ich erzählen: </p><br><ul><li>  So stellen Sie verteilten Ceph-Speicher bereit </li><li>  Verwendung von Ceph bei der Arbeit mit Kubernetes <a name="habracut"></a></li></ul><br><h2>  Einführung </h2><br><p>  Zunächst möchte ich erklären, wem dieser Artikel nützlich sein wird.  Erstens für Leser, die den Cluster gemäß meiner ersten Veröffentlichung bereitgestellt haben, um weiterhin eine Microservice-Architektur aufzubauen.  Zweitens für Personen, die versuchen möchten, einen Ceph-Cluster selbst bereitzustellen und seine Leistung zu bewerten. </p><br><p>  In dieser Veröffentlichung werde ich nicht auf das Thema Clusterplanung für irgendwelche Bedürfnisse eingehen, sondern nur auf allgemeine Prinzipien und Konzepte eingehen.  Ich werde mich nicht mit "Tuning" und Deep Tuning befassen, es gibt viele Veröffentlichungen zu diesem Thema, einschließlich des Habr.  Der Artikel wird einführender sein, aber gleichzeitig wird es Ihnen ermöglichen, eine funktionierende Lösung zu erhalten, die Sie in Zukunft an Ihre Bedürfnisse anpassen können. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Liste der Hosts, Hostressourcen, Betriebssystem- und Softwareversionen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ceph-Cluster-Struktur</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konfigurieren Sie Clusterknoten vor der Installation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines Ceph-Clusters</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerkeinrichtung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie Ceph-Pakete</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installation und Initialisierung von Monitoren</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OSD hinzufügen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbinden Sie Ceph mit Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines Datenpools</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Kundengeheimnis erstellen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stellen Sie den ceph rbd-Provisioner bereit</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen einer Speicherklasse</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes + Ceph-Bandtest</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Liste der bei der Erstellung des Artikels verwendeten Materialien</a> </li></ol><br><a name="vm"></a><br><h2>  Hostliste und Systemanforderungen </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Name</b> </th><th>  <b>IP-Adresse</b> </th><th>  <b>Kommentar</b> </th></tr><tr><td>  ceph01-Test </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  ceph02-Test </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  ceph03-Test </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Beim Schreiben eines Artikels verwende ich virtuelle Maschinen mit dieser Konfiguration </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Auf jedem ist ein Debian 9.5-Betriebssystem installiert.  Hierbei handelt es sich um Testmaschinen mit jeweils zwei Festplatten, die erste für das Betriebssystem und die zweite für das OSD-Cef. </p><br><p>  Ich werde den Cluster über das Dienstprogramm ceph-deploy bereitstellen.  Sie können einen Ceph-Cluster im manuellen Modus bereitstellen. Alle Schritte sind in der Dokumentation beschrieben. In diesem Artikel wird jedoch erläutert, wie schnell Sie Ceph bereitstellen und in Kubernetes verwenden können. <br>  Ceph ist ziemlich gefräßig für Ressourcen, insbesondere RAM.  Für eine gute Geschwindigkeit ist es ratsam, SSD-Laufwerke zu verwenden. </p><br><p>  Weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu den</a> Anforderungen finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Ceph-Dokumentation.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Ceph-Cluster-Struktur </h2><br><p>  <strong>MON</strong> <br>  <em>Ein Monitor ist ein Dämon, der als Koordinator fungiert, von dem aus der Cluster beginnt.</em>  <em>Sobald wir mindestens einen funktionierenden Monitor haben, haben wir einen Ceph-Cluster.</em>  <em>Der Monitor speichert Informationen über den Zustand und den Zustand des Clusters, indem verschiedene Karten mit anderen Monitoren ausgetauscht werden.</em>  <em>Clients wenden sich an Monitore, um herauszufinden, in welches OSD Daten geschrieben / gelesen werden sollen.</em>  <em>Wenn Sie einen neuen Speicher bereitstellen, erstellen Sie zunächst einen Monitor (oder mehrere).</em>  <em>Der Cluster kann auf einem Monitor ausgeführt werden. Es wird jedoch empfohlen, 3 oder 5 Monitore zu erstellen, um den Ausfall des gesamten Systems aufgrund des Ausfalls eines einzelnen Monitors zu vermeiden.</em>  <em>Die Hauptsache ist, dass die Anzahl dieser ungerade sein sollte, um Split-Brain-Situationen zu vermeiden.</em>  <em>Monitore arbeiten in einem Quorum. Wenn also mehr als die Hälfte der Monitore ausfällt, wird der Cluster blockiert, um Dateninkonsistenzen zu vermeiden.</em> <br>  <strong>Mgr</strong> <br>  <em>Der Ceph Manager-Daemon arbeitet mit dem Monitor-Daemon zusammen, um zusätzliche Kontrolle bereitzustellen.</em> <em><br></em>  <em>Seit Version 12.x ist der ceph-mgr-Daemon für den normalen Betrieb erforderlich geworden.</em> <em><br></em>  <em>Wenn der mgr-Daemon nicht ausgeführt wird, wird eine Warnung angezeigt.</em> <br>  <strong>OSD (Object Storage Device)</strong> <br>  <em>OSD ist eine Speichereinheit, die die Daten selbst speichert und Clientanforderungen durch Datenaustausch mit anderen OSDs verarbeitet.</em>  <em>Dies ist normalerweise eine Festplatte.</em>  <em>Normalerweise gibt es für jedes OSD einen separaten OSD-Daemon, der auf jedem Computer ausgeführt werden kann, auf dem diese Festplatte installiert ist.</em> </p><br><p>  Alle drei Daemons funktionieren auf jedem Computer in unserem Cluster.  Dementsprechend überwachen und verwalten Sie Daemons als Service- und OSD-Daemons für ein Laufwerk unserer virtuellen Maschine. </p><br><a name="before"></a><br><h2>  Konfigurieren Sie Clusterknoten vor der Installation </h2><br><p>  Die ceph-Dokumentation gibt den folgenden Workflow an: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Ich werde vom ersten Knoten des ceph01-Test-Clusters aus arbeiten, es wird der Admin-Knoten sein, es wird auch Konfigurationsdateien für das Dienstprogramm ceph-deploy enthalten.  Damit das Dienstprogramm ceph-deploy ordnungsgemäß funktioniert, müssen alle Clusterknoten über ssh mit dem Admin-Knoten erreichbar sein.  Der Einfachheit halber schreibe ich in die Hosts Kurznamen für den Cluster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  Und kopieren Sie die Schlüssel auf die anderen Hosts.  Alle Befehle werde ich von root ausführen. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Setup-Dokumentation</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Installieren Sie ceph-deploy </h2><br><p>  Der erste Schritt ist die Installation von ceph-deploy auf dem ceph01-Testcomputer </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  Als Nächstes müssen Sie die Version auswählen, die Sie einfügen möchten.  Aber hier gibt es Schwierigkeiten, derzeit unterstützt ceph für Debian OS nur leuchtende Pakete. <br>  Wenn Sie eine neuere Version einfügen möchten, müssen Sie beispielsweise einen Spiegel verwenden <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  Fügen Sie auf allen drei Knoten ein Repository mit Mimic hinzu </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Wenn Ihnen das Licht ausreicht, können Sie die offiziellen Repositories verwenden </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Wir installieren auch NTP auf allen drei Knoten. </p><br><div class="spoiler">  <b class="spoiler_title">da diese empfehlung in der ceph dokumentation steht</b> <div class="spoiler_text"><p>  Wir empfehlen, NTP auf Ceph-Knoten (insbesondere auf Ceph Monitor-Knoten) zu installieren, um Probleme durch Taktdrift zu vermeiden. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Stellen Sie sicher, dass Sie den NTP-Dienst aktivieren.  Stellen Sie sicher, dass jeder Ceph-Knoten denselben NTP-Server verwendet.  Weitere Details finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> </p><br><a name="ceph-install"></a><br><h2>  Erstellen eines Ceph-Clusters </h2><br><p>  Erstellen Sie ein Verzeichnis für Konfigurationsdateien und Dateien ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Lassen Sie uns eine neue Cluster-Konfiguration erstellen. Geben Sie beim Erstellen an, dass unser Monitor drei Monitore enthält </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Netzwerkeinrichtung </h2><br><p>  Jetzt ist der wichtige Punkt, es ist Zeit, über das Netzwerk für Ceph zu sprechen.  Ceph verwendet zwei öffentliche Netzwerke und ein Clusternetzwerk, um zu arbeiten <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Wie Sie dem Diagramm des öffentlichen Netzwerks entnehmen können, ist dies die Benutzer- und Anwendungsebene, und das Clusternetzwerk ist das Netzwerk, über das Daten repliziert werden. <br>  Es ist sehr wünschenswert, diese beiden Netzwerke voneinander zu trennen.  Außerdem ist ein Netzwerkgeschwindigkeits-Cluster-Netzwerk mit mindestens 10 GB wünschenswert. <br>  Natürlich können Sie alles im selben Netzwerk behalten.  Dies ist jedoch mit der Tatsache behaftet, dass die Netzwerklast SEHR zunimmt, sobald das Replikationsvolumen zwischen OSDs zunimmt, beispielsweise wenn neue OSDs (Festplatten) fallen oder hinzugefügt werden.  Die Geschwindigkeit und Stabilität Ihrer Infrastruktur hängt also stark vom von ceph verwendeten Netzwerk ab. <br>  Leider verfügt mein Virtualisierungscluster nicht über ein separates Netzwerk, und ich werde ein gemeinsames Netzwerksegment verwenden. <br>  Die Netzwerkkonfiguration für den Cluster erfolgt über die Konfigurationsdatei, die wir mit dem vorherigen Befehl generiert haben. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Wie wir sehen können, hat die cef-Bereitstellung nicht die Standardnetzwerkeinstellungen für uns erstellt, daher werde ich den Parameter public network = {public-network / netmask} zum globalen Abschnitt der Konfiguration hinzufügen.  Mein Netzwerk ist 10.73.0.0/16, daher sieht meine Konfiguration nach dem Hinzufügen folgendermaßen aus </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Wenn Sie das Clusternetzwerk von öffentlich trennen möchten, fügen Sie den Parameter cluster network = {cluster-network / netmask} hinzu <br>  Weitere Informationen zu Netzwerken finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Dokumentation.</a> </p><br><a name="ceph-pack"></a><br><h2>  Installieren Sie Ceph-Pakete </h2><br><p>  Mit ceph-deploy installieren wir alle benötigten ceph-Pakete auf unseren drei Knoten. <br>  Führen Sie dazu auf ceph01-test aus <br>  Wenn die Version imitiert ist, dann </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Wenn die Version dann leuchtet </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Und warten Sie, bis alles feststeht. </p><br><a name="ceph-mon"></a><br><h2>  Installation und Initialisierung von Monitoren </h2><br><p>  Nachdem alle Pakete installiert wurden, erstellen und initiieren wir die Monitore unseres Clusters. <br>  C ceph01-Test gehen Sie wie folgt vor </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Dabei werden Monitore erstellt, Daemons gestartet und ceph-deploy das Quorum überprüft. <br>  Streuen Sie nun die Konfigurationen auf die Clusterknoten. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Und überprüfen Sie den Status unseres Clusters. Wenn Sie alles richtig gemacht haben, sollte der Status sein <br>  GESUNDHEIT_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Erstellen Sie mgr </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Und überprüfen Sie den Status erneut </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Eine Linie sollte erscheinen </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Wir schreiben die Konfiguration an alle Hosts im Cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  OSD hinzufügen </h2><br><p>  Im Moment haben wir einen funktionierenden Cluster, aber es gibt noch keine Festplatten (osd in der Ceph-Terminologie) zum Speichern von Informationen. </p><br><p>  OSD kann mit dem folgenden Befehl hinzugefügt werden (Gesamtansicht) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  In meinem Testbed wird disk / dev / sdb unter osd zugewiesen. In meinem Fall lauten die Befehle also wie folgt </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  Überprüfen Sie, ob alle OSDs funktionieren. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Fazit </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Sie können auch einige nützliche Befehle für OSD ausprobieren. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  und </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Wenn alles in Ordnung ist, haben wir einen funktionierenden Ceph-Cluster.  Im nächsten Teil werde ich erklären, wie man Ceph mit Kubernetes benutzt </p><br><a name="kubernetes"></a><br><h1>  Verbinden Sie Ceph mit Kubernetes </h1><br><p>  Leider kann ich den Betrieb von Kubernetes-Volumes in diesem Artikel nicht detailliert beschreiben, daher werde ich versuchen, in einen Absatz zu passen. <br>  Kubernetes verwendet Speicherklassen, um mit Datenmengen zu arbeiten. Jede Speicherklasse verfügt über einen eigenen Provisioner. Sie können ihn als eine Art „Treiber“ für die Arbeit mit verschiedenen Datenspeichermengen betrachten.  Die vollständige Liste, die Kubernetes unterstützt, finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Dokumentation</a> . <br>  Kubernetes selbst unterstützt auch die Arbeit mit rbd, aber im offiziellen kube-controller-manager-Image ist kein rbd-Client installiert, sodass Sie ein anderes Image verwenden müssen. <br>  Es sollte auch beachtet werden, dass als rbd erstellte Volumes (pvc) nur ReadWriteOnce (RWO) und sein können, was bedeutet, dass Sie das erstellte Volume NUR an einem Herd mounten können. </p><br><p>  Damit unser Cluster mit Ceph-Volumes arbeiten kann, benötigen wir: <br>  in einem Ceph-Cluster: </p><br><ul><li>  Erstellen Sie einen Datenpool im Ceph-Cluster </li><li>  Erstellen Sie einen Client und greifen Sie auf den Datenpool zu </li><li>  Holen Sie sich Ceph Admin Geheimnis </li></ul><br><p>  Damit unser Cluster mit Ceph-Volumes arbeiten kann, benötigen wir: <br>  in einem Ceph-Cluster: </p><br><ul><li>  Erstellen Sie einen Datenpool im Ceph-Cluster </li><li>  Erstellen Sie einen Client und greifen Sie auf den Datenpool zu </li><li>  Holen Sie sich Ceph Admin Geheimnis </li></ul><br><p>  im Kubernetes-Cluster: </p><br><ul><li>  Erstellen Sie das Ceph-Administratorgeheimnis und den Ceph-Client-Schlüssel </li><li>  Installieren Sie den ceph rbd-Provisioner oder ändern Sie das kube-controller-manager-Image in ein Image, das rbd unterstützt </li><li>  Erstellen Sie ein Geheimnis mit dem Ceph-Client-Schlüssel </li><li>  Speicherklasse erstellen </li><li>  Installieren Sie Ceph-Common auf Kubernetes Worker Notes </li></ul><br><a name="ceph-pool"></a><br><h2>  Erstellen eines Datenpools </h2><br><p>  Erstellen Sie im Ceph-Cluster einen Pool für die Kubernetes-Volumes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Hier werde ich eine kleine Erklärung abgeben, die Zahlen 8 8 am Ende sind die Zahlen von pg und pgs.  Diese Werte hängen von der Größe Ihres Ceph-Clusters ab.  Es gibt spezielle Taschenrechner, die die Menge an pg und pgs berechnen, zum Beispiel offiziell von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ceph</a> <br>  Zunächst empfehle ich, es standardmäßig zu belassen, wenn dieser Betrag in Zukunft erhöht werden kann (er kann nur gegenüber der Nautilus-Version reduziert werden). </p><br><a name="ceph-key"></a><br><h2>  Erstellen eines Clients für einen Datenpool </h2><br><p>  Erstellen Sie einen Client für den neuen Pool </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Wir werden einen Schlüssel für den Kunden erhalten, in Zukunft werden wir ihn benötigen, um geheime Kubernetes zu erstellen </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Admin-Schlüssel erhalten </h2><br><p>  Und hol dir den Admin-Schlüssel </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>Auf dem Ceph-Cluster sind alle Arbeiten abgeschlossen, und jetzt müssen wir zu einem Computer gehen, der Zugriff auf den Kubernetes-Cluster hat</strong> <br>  Ich werde mit dem master01-Test (10.73.71.25) des von mir in der ersten Veröffentlichung bereitgestellten Clusters arbeiten. </p><br><a name="kubernetes-secrets"></a><br><h2>  Ein Kundengeheimnis erstellen </h2><br><p>  Erstellen Sie eine Datei mit dem Client-Token, das wir erhalten haben (vergessen Sie nicht, sie durch Ihr Token zu ersetzen). </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  Und schaffen Sie ein Geheimnis, das wir in Zukunft nutzen werden </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Admin-Geheimnis erstellen </h2><br><p>  Erstellen Sie eine Datei mit dem Admin-Token (vergessen Sie nicht, sie durch Ihr Token zu ersetzen). </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Danach erstellen Sie ein Admin-Geheimnis </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  Überprüfen Sie, ob Geheimnisse erstellt wurden </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  Methode zuerst bereitstellen ceph rbd Provisioner </h2><br><p>  Wir klonen das Kubernetes-Inkubator / External-Storage-Repository mit Github. Es enthält alles, was Sie benötigen, um den Kubernetes-Cluster mit dem Ceph-Repository anzufreunden. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Fazit </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  Methode zwei: Ersetzen Sie das kube-controller-manager-Image </h2><br><p>  Es gibt keine rbd-Unterstützung im offiziellen kube-controller-manager-Image, daher müssen wir das controller-manager-Image ändern. <br>  Dazu müssen Sie auf jedem der Kubernetes-Assistenten die Datei kube-controller-manager.yaml bearbeiten und das Bild durch gcr.io/google_containers/hyperkube:v1.15.2 ersetzen.  Achten Sie auf die Version des Bildes, die Ihrer Version des Kubernetes-Clusters entsprechen soll. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Danach müssen Sie kube-controller-manager neu starten </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Pods sollten automatisch aktualisiert werden. Wenn dies jedoch aus irgendeinem Grund nicht geschehen ist, können Sie sie durch Löschen manuell neu erstellen. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  Überprüfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - - </p><br><a name="storage-class"></a><br><h2>  Erstellen einer Speicherklasse </h2><br><p>  <strong>Methode eins - wenn Sie den Provisioner ceph.com/rbd verwendet haben</strong> <br>  Erstellen Sie eine Yaml-Datei mit einer Beschreibung unserer Speicherklasse.  Außerdem können alle unten verwendeten Dateien <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in mein Repository</a> im ceph-Verzeichnis heruntergeladen werden </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Und bette ihn in unseren Cluster ein </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  Überprüfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>Methode zwei - wenn Sie den Provisioner kubernetes.io/rbd verwendet haben</strong> <br>  Speicherklasse erstellen </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Und bette ihn in unseren Cluster ein </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  Überprüfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Kubernetes + Ceph-Bandtest </h2><br><p>  Bevor Sie ceph + kubernetes testen, müssen Sie das ceph-common-Paket auf JEDEM Workcode des Clusters installieren. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Erstellen Sie eine Yaml-Datei PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  Töte ihn </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  Überprüfen Sie, ob PersistentVolumeClaim erstellt wurde. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  Und auch automatisch PersistentVolume erstellt. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Lassen Sie uns einen Test-Pod erstellen, in dem wir den erstellten PVC in das Verzeichnis / mnt aufnehmen.  Führen Sie diese Datei /mnt/test.txt mit dem Text "Hello World!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Wir werden ihn töten und überprüfen, ob er seine Aufgabe erfüllt hat </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Warten wir auf den Status </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Lassen Sie uns ein neues erstellen, unser Volume daran anschließen, aber bereits in / mnt / test, und danach sicherstellen, dass die vom ersten Volume erstellte Datei vorhanden ist </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Führen Sie kubectl get po -w aus und warten Sie, bis der Pod ausgeführt wird <br>  Danach gehen wir hinein und überprüfen, ob das Volume verbunden ist und unsere Datei im Verzeichnis / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Vielen Dank für das Lesen bis zum Ende.  Entschuldigen Sie die Verzögerung bei der Veröffentlichung. <br>  Ich bin bereit, alle Fragen in persönlichen Nachrichten oder in sozialen Netzwerken zu beantworten, die in meinem Profil angegeben sind. <br>  In der nächsten kleinen Veröffentlichung werde ich Ihnen erklären, wie Sie S3-Speicher basierend auf dem erstellten Ceph-Cluster und dann gemäß dem Plan aus der ersten Veröffentlichung bereitstellen. </p><br><a name="book"></a><br><p>  Zur Veröffentlichung verwendete Materialien </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Offizielle Ceph-Dokumentation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einführung in das Ceph-Repository in Bildern</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Offizielle Dokumentation von Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-Inkubator / externer Speicher</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes-ceph-percona-Repository mit Beispieldateien</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465399/">https://habr.com/ru/post/de465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465379/index.html">Selbstgemachte drahtlose eigenständige Insulinpumpensteuerung</a></li>
<li><a href="../de465391/index.html">Auf den Spuren der russischen Scala-Bewegung. Teil 1</a></li>
<li><a href="../de465393/index.html">Batterieleistung für MySensors-Geräte</a></li>
<li><a href="../de465395/index.html">Was ist der Hauptunterschied zwischen Dependency Injection und Service Locator?</a></li>
<li><a href="../de465397/index.html">Wie ist der Nitro-Übersetzer entstanden, der Entwicklern bei der Lokalisierung und beim technischen Support hilft?</a></li>
<li><a href="../de465401/index.html">5 Aktivitäten zur Beschleunigung der Problemlösung in einem beliebigen IT-Team</a></li>
<li><a href="../de465403/index.html">Achtung! Neue Kameras unterwegs oder aktuelle Informationen zu Radargeräten und Radarwarnern</a></li>
<li><a href="../de465407/index.html">1. Übersicht über Extreme Enterprise Layer Switches</a></li>
<li><a href="../de465409/index.html">Vue.js Best Practices für die Webentwicklung</a></li>
<li><a href="../de465415/index.html">Wir sprechen über DevOps in einer verständlichen Sprache</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>