<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñ®Ô∏è üë©üèº‚Äç‚öïÔ∏è ü§ûüèº Stellen Sie verteilten CEPH-Speicher bereit und verbinden Sie ihn mit Kubernetes üíô ü§ûüèæ üëÖ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 1 Wir stellen die Umgebung f√ºr die Arbeit mit Microservices bereit. Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian) 
 Hallo, liebe ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Stellen Sie verteilten CEPH-Speicher bereit und verbinden Sie ihn mit Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1 Wir stellen die Umgebung f√ºr die Arbeit mit Microservices bereit.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian)</a> </p><br><h2>  Hallo, liebe Leser von Habr! </h2><br><p>  In einem fr√ºheren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beitrag habe</a> ich dar√ºber gesprochen, wie ein Kubernetes-Failovercluster bereitgestellt wird.  Tatsache ist jedoch, dass es in Kubernetes praktisch ist, zustandslose Anwendungen bereitzustellen, die ihren Status nicht beibehalten oder nicht mit Daten arbeiten m√ºssen.  In den meisten F√§llen m√ºssen wir jedoch Daten speichern und d√ºrfen sie beim Neustart der Herde nicht verlieren. <br>  Kubernetes verwendet f√ºr diese Zwecke Volumes.  Wenn wir mit Kubernetes Cloud-L√∂sungen arbeiten, gibt es keine besonderen Probleme.  Wir m√ºssen nur das erforderliche Volume bei Google, Amazon oder einem anderen Cloud-Anbieter bestellen und gem√§√ü der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> die empfangenen Volumes mit den Pods verbinden. <br>  Wenn wir uns mit Bare Metal besch√§ftigen, sind die Dinge etwas komplizierter.  Heute m√∂chte ich √ºber eine der L√∂sungen sprechen, die auf der Verwendung von Ceph basieren. </p><br><p>  In dieser Ver√∂ffentlichung werde ich erz√§hlen: </p><br><ul><li>  So stellen Sie verteilten Ceph-Speicher bereit </li><li>  Verwendung von Ceph bei der Arbeit mit Kubernetes <a name="habracut"></a></li></ul><br><h2>  Einf√ºhrung </h2><br><p>  Zun√§chst m√∂chte ich erkl√§ren, wem dieser Artikel n√ºtzlich sein wird.  Erstens f√ºr Leser, die den Cluster gem√§√ü meiner ersten Ver√∂ffentlichung bereitgestellt haben, um weiterhin eine Microservice-Architektur aufzubauen.  Zweitens f√ºr Personen, die versuchen m√∂chten, einen Ceph-Cluster selbst bereitzustellen und seine Leistung zu bewerten. </p><br><p>  In dieser Ver√∂ffentlichung werde ich nicht auf das Thema Clusterplanung f√ºr irgendwelche Bed√ºrfnisse eingehen, sondern nur auf allgemeine Prinzipien und Konzepte eingehen.  Ich werde mich nicht mit "Tuning" und Deep Tuning befassen, es gibt viele Ver√∂ffentlichungen zu diesem Thema, einschlie√ülich des Habr.  Der Artikel wird einf√ºhrender sein, aber gleichzeitig wird es Ihnen erm√∂glichen, eine funktionierende L√∂sung zu erhalten, die Sie in Zukunft an Ihre Bed√ºrfnisse anpassen k√∂nnen. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Liste der Hosts, Hostressourcen, Betriebssystem- und Softwareversionen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ceph-Cluster-Struktur</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konfigurieren Sie Clusterknoten vor der Installation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines Ceph-Clusters</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerkeinrichtung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie Ceph-Pakete</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installation und Initialisierung von Monitoren</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OSD hinzuf√ºgen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbinden Sie Ceph mit Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines Datenpools</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Kundengeheimnis erstellen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stellen Sie den ceph rbd-Provisioner bereit</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen einer Speicherklasse</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes + Ceph-Bandtest</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Liste der bei der Erstellung des Artikels verwendeten Materialien</a> </li></ol><br><a name="vm"></a><br><h2>  Hostliste und Systemanforderungen </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Name</b> </th><th>  <b>IP-Adresse</b> </th><th>  <b>Kommentar</b> </th></tr><tr><td>  ceph01-Test </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  ceph02-Test </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  ceph03-Test </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Beim Schreiben eines Artikels verwende ich virtuelle Maschinen mit dieser Konfiguration </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Auf jedem ist ein Debian 9.5-Betriebssystem installiert.  Hierbei handelt es sich um Testmaschinen mit jeweils zwei Festplatten, die erste f√ºr das Betriebssystem und die zweite f√ºr das OSD-Cef. </p><br><p>  Ich werde den Cluster √ºber das Dienstprogramm ceph-deploy bereitstellen.  Sie k√∂nnen einen Ceph-Cluster im manuellen Modus bereitstellen. Alle Schritte sind in der Dokumentation beschrieben. In diesem Artikel wird jedoch erl√§utert, wie schnell Sie Ceph bereitstellen und in Kubernetes verwenden k√∂nnen. <br>  Ceph ist ziemlich gefr√§√üig f√ºr Ressourcen, insbesondere RAM.  F√ºr eine gute Geschwindigkeit ist es ratsam, SSD-Laufwerke zu verwenden. </p><br><p>  Weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu den</a> Anforderungen finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Ceph-Dokumentation.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Ceph-Cluster-Struktur </h2><br><p>  <strong>MON</strong> <br>  <em>Ein Monitor ist ein D√§mon, der als Koordinator fungiert, von dem aus der Cluster beginnt.</em>  <em>Sobald wir mindestens einen funktionierenden Monitor haben, haben wir einen Ceph-Cluster.</em>  <em>Der Monitor speichert Informationen √ºber den Zustand und den Zustand des Clusters, indem verschiedene Karten mit anderen Monitoren ausgetauscht werden.</em>  <em>Clients wenden sich an Monitore, um herauszufinden, in welches OSD Daten geschrieben / gelesen werden sollen.</em>  <em>Wenn Sie einen neuen Speicher bereitstellen, erstellen Sie zun√§chst einen Monitor (oder mehrere).</em>  <em>Der Cluster kann auf einem Monitor ausgef√ºhrt werden. Es wird jedoch empfohlen, 3 oder 5 Monitore zu erstellen, um den Ausfall des gesamten Systems aufgrund des Ausfalls eines einzelnen Monitors zu vermeiden.</em>  <em>Die Hauptsache ist, dass die Anzahl dieser ungerade sein sollte, um Split-Brain-Situationen zu vermeiden.</em>  <em>Monitore arbeiten in einem Quorum. Wenn also mehr als die H√§lfte der Monitore ausf√§llt, wird der Cluster blockiert, um Dateninkonsistenzen zu vermeiden.</em> <br>  <strong>Mgr</strong> <br>  <em>Der Ceph Manager-Daemon arbeitet mit dem Monitor-Daemon zusammen, um zus√§tzliche Kontrolle bereitzustellen.</em> <em><br></em>  <em>Seit Version 12.x ist der ceph-mgr-Daemon f√ºr den normalen Betrieb erforderlich geworden.</em> <em><br></em>  <em>Wenn der mgr-Daemon nicht ausgef√ºhrt wird, wird eine Warnung angezeigt.</em> <br>  <strong>OSD (Object Storage Device)</strong> <br>  <em>OSD ist eine Speichereinheit, die die Daten selbst speichert und Clientanforderungen durch Datenaustausch mit anderen OSDs verarbeitet.</em>  <em>Dies ist normalerweise eine Festplatte.</em>  <em>Normalerweise gibt es f√ºr jedes OSD einen separaten OSD-Daemon, der auf jedem Computer ausgef√ºhrt werden kann, auf dem diese Festplatte installiert ist.</em> </p><br><p>  Alle drei Daemons funktionieren auf jedem Computer in unserem Cluster.  Dementsprechend √ºberwachen und verwalten Sie Daemons als Service- und OSD-Daemons f√ºr ein Laufwerk unserer virtuellen Maschine. </p><br><a name="before"></a><br><h2>  Konfigurieren Sie Clusterknoten vor der Installation </h2><br><p>  Die ceph-Dokumentation gibt den folgenden Workflow an: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Ich werde vom ersten Knoten des ceph01-Test-Clusters aus arbeiten, es wird der Admin-Knoten sein, es wird auch Konfigurationsdateien f√ºr das Dienstprogramm ceph-deploy enthalten.  Damit das Dienstprogramm ceph-deploy ordnungsgem√§√ü funktioniert, m√ºssen alle Clusterknoten √ºber ssh mit dem Admin-Knoten erreichbar sein.  Der Einfachheit halber schreibe ich in die Hosts Kurznamen f√ºr den Cluster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  Und kopieren Sie die Schl√ºssel auf die anderen Hosts.  Alle Befehle werde ich von root ausf√ºhren. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Setup-Dokumentation</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Installieren Sie ceph-deploy </h2><br><p>  Der erste Schritt ist die Installation von ceph-deploy auf dem ceph01-Testcomputer </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  Als N√§chstes m√ºssen Sie die Version ausw√§hlen, die Sie einf√ºgen m√∂chten.  Aber hier gibt es Schwierigkeiten, derzeit unterst√ºtzt ceph f√ºr Debian OS nur leuchtende Pakete. <br>  Wenn Sie eine neuere Version einf√ºgen m√∂chten, m√ºssen Sie beispielsweise einen Spiegel verwenden <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  F√ºgen Sie auf allen drei Knoten ein Repository mit Mimic hinzu </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Wenn Ihnen das Licht ausreicht, k√∂nnen Sie die offiziellen Repositories verwenden </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Wir installieren auch NTP auf allen drei Knoten. </p><br><div class="spoiler">  <b class="spoiler_title">da diese empfehlung in der ceph dokumentation steht</b> <div class="spoiler_text"><p>  Wir empfehlen, NTP auf Ceph-Knoten (insbesondere auf Ceph Monitor-Knoten) zu installieren, um Probleme durch Taktdrift zu vermeiden. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Stellen Sie sicher, dass Sie den NTP-Dienst aktivieren.  Stellen Sie sicher, dass jeder Ceph-Knoten denselben NTP-Server verwendet.  Weitere Details finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> </p><br><a name="ceph-install"></a><br><h2>  Erstellen eines Ceph-Clusters </h2><br><p>  Erstellen Sie ein Verzeichnis f√ºr Konfigurationsdateien und Dateien ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Lassen Sie uns eine neue Cluster-Konfiguration erstellen. Geben Sie beim Erstellen an, dass unser Monitor drei Monitore enth√§lt </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Netzwerkeinrichtung </h2><br><p>  Jetzt ist der wichtige Punkt, es ist Zeit, √ºber das Netzwerk f√ºr Ceph zu sprechen.  Ceph verwendet zwei √∂ffentliche Netzwerke und ein Clusternetzwerk, um zu arbeiten <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Wie Sie dem Diagramm des √∂ffentlichen Netzwerks entnehmen k√∂nnen, ist dies die Benutzer- und Anwendungsebene, und das Clusternetzwerk ist das Netzwerk, √ºber das Daten repliziert werden. <br>  Es ist sehr w√ºnschenswert, diese beiden Netzwerke voneinander zu trennen.  Au√üerdem ist ein Netzwerkgeschwindigkeits-Cluster-Netzwerk mit mindestens 10 GB w√ºnschenswert. <br>  Nat√ºrlich k√∂nnen Sie alles im selben Netzwerk behalten.  Dies ist jedoch mit der Tatsache behaftet, dass die Netzwerklast SEHR zunimmt, sobald das Replikationsvolumen zwischen OSDs zunimmt, beispielsweise wenn neue OSDs (Festplatten) fallen oder hinzugef√ºgt werden.  Die Geschwindigkeit und Stabilit√§t Ihrer Infrastruktur h√§ngt also stark vom von ceph verwendeten Netzwerk ab. <br>  Leider verf√ºgt mein Virtualisierungscluster nicht √ºber ein separates Netzwerk, und ich werde ein gemeinsames Netzwerksegment verwenden. <br>  Die Netzwerkkonfiguration f√ºr den Cluster erfolgt √ºber die Konfigurationsdatei, die wir mit dem vorherigen Befehl generiert haben. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Wie wir sehen k√∂nnen, hat die cef-Bereitstellung nicht die Standardnetzwerkeinstellungen f√ºr uns erstellt, daher werde ich den Parameter public network = {public-network / netmask} zum globalen Abschnitt der Konfiguration hinzuf√ºgen.  Mein Netzwerk ist 10.73.0.0/16, daher sieht meine Konfiguration nach dem Hinzuf√ºgen folgenderma√üen aus </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Wenn Sie das Clusternetzwerk von √∂ffentlich trennen m√∂chten, f√ºgen Sie den Parameter cluster network = {cluster-network / netmask} hinzu <br>  Weitere Informationen zu Netzwerken finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Dokumentation.</a> </p><br><a name="ceph-pack"></a><br><h2>  Installieren Sie Ceph-Pakete </h2><br><p>  Mit ceph-deploy installieren wir alle ben√∂tigten ceph-Pakete auf unseren drei Knoten. <br>  F√ºhren Sie dazu auf ceph01-test aus <br>  Wenn die Version imitiert ist, dann </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Wenn die Version dann leuchtet </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Und warten Sie, bis alles feststeht. </p><br><a name="ceph-mon"></a><br><h2>  Installation und Initialisierung von Monitoren </h2><br><p>  Nachdem alle Pakete installiert wurden, erstellen und initiieren wir die Monitore unseres Clusters. <br>  C ceph01-Test gehen Sie wie folgt vor </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Dabei werden Monitore erstellt, Daemons gestartet und ceph-deploy das Quorum √ºberpr√ºft. <br>  Streuen Sie nun die Konfigurationen auf die Clusterknoten. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Und √ºberpr√ºfen Sie den Status unseres Clusters. Wenn Sie alles richtig gemacht haben, sollte der Status sein <br>  GESUNDHEIT_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Erstellen Sie mgr </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Und √ºberpr√ºfen Sie den Status erneut </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Eine Linie sollte erscheinen </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Wir schreiben die Konfiguration an alle Hosts im Cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  OSD hinzuf√ºgen </h2><br><p>  Im Moment haben wir einen funktionierenden Cluster, aber es gibt noch keine Festplatten (osd in der Ceph-Terminologie) zum Speichern von Informationen. </p><br><p>  OSD kann mit dem folgenden Befehl hinzugef√ºgt werden (Gesamtansicht) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  In meinem Testbed wird disk / dev / sdb unter osd zugewiesen. In meinem Fall lauten die Befehle also wie folgt </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  √úberpr√ºfen Sie, ob alle OSDs funktionieren. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Fazit </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Sie k√∂nnen auch einige n√ºtzliche Befehle f√ºr OSD ausprobieren. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  und </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Wenn alles in Ordnung ist, haben wir einen funktionierenden Ceph-Cluster.  Im n√§chsten Teil werde ich erkl√§ren, wie man Ceph mit Kubernetes benutzt </p><br><a name="kubernetes"></a><br><h1>  Verbinden Sie Ceph mit Kubernetes </h1><br><p>  Leider kann ich den Betrieb von Kubernetes-Volumes in diesem Artikel nicht detailliert beschreiben, daher werde ich versuchen, in einen Absatz zu passen. <br>  Kubernetes verwendet Speicherklassen, um mit Datenmengen zu arbeiten. Jede Speicherklasse verf√ºgt √ºber einen eigenen Provisioner. Sie k√∂nnen ihn als eine Art ‚ÄûTreiber‚Äú f√ºr die Arbeit mit verschiedenen Datenspeichermengen betrachten.  Die vollst√§ndige Liste, die Kubernetes unterst√ºtzt, finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Dokumentation</a> . <br>  Kubernetes selbst unterst√ºtzt auch die Arbeit mit rbd, aber im offiziellen kube-controller-manager-Image ist kein rbd-Client installiert, sodass Sie ein anderes Image verwenden m√ºssen. <br>  Es sollte auch beachtet werden, dass als rbd erstellte Volumes (pvc) nur ReadWriteOnce (RWO) und sein k√∂nnen, was bedeutet, dass Sie das erstellte Volume NUR an einem Herd mounten k√∂nnen. </p><br><p>  Damit unser Cluster mit Ceph-Volumes arbeiten kann, ben√∂tigen wir: <br>  in einem Ceph-Cluster: </p><br><ul><li>  Erstellen Sie einen Datenpool im Ceph-Cluster </li><li>  Erstellen Sie einen Client und greifen Sie auf den Datenpool zu </li><li>  Holen Sie sich Ceph Admin Geheimnis </li></ul><br><p>  Damit unser Cluster mit Ceph-Volumes arbeiten kann, ben√∂tigen wir: <br>  in einem Ceph-Cluster: </p><br><ul><li>  Erstellen Sie einen Datenpool im Ceph-Cluster </li><li>  Erstellen Sie einen Client und greifen Sie auf den Datenpool zu </li><li>  Holen Sie sich Ceph Admin Geheimnis </li></ul><br><p>  im Kubernetes-Cluster: </p><br><ul><li>  Erstellen Sie das Ceph-Administratorgeheimnis und den Ceph-Client-Schl√ºssel </li><li>  Installieren Sie den ceph rbd-Provisioner oder √§ndern Sie das kube-controller-manager-Image in ein Image, das rbd unterst√ºtzt </li><li>  Erstellen Sie ein Geheimnis mit dem Ceph-Client-Schl√ºssel </li><li>  Speicherklasse erstellen </li><li>  Installieren Sie Ceph-Common auf Kubernetes Worker Notes </li></ul><br><a name="ceph-pool"></a><br><h2>  Erstellen eines Datenpools </h2><br><p>  Erstellen Sie im Ceph-Cluster einen Pool f√ºr die Kubernetes-Volumes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Hier werde ich eine kleine Erkl√§rung abgeben, die Zahlen 8 8 am Ende sind die Zahlen von pg und pgs.  Diese Werte h√§ngen von der Gr√∂√üe Ihres Ceph-Clusters ab.  Es gibt spezielle Taschenrechner, die die Menge an pg und pgs berechnen, zum Beispiel offiziell von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ceph</a> <br>  Zun√§chst empfehle ich, es standardm√§√üig zu belassen, wenn dieser Betrag in Zukunft erh√∂ht werden kann (er kann nur gegen√ºber der Nautilus-Version reduziert werden). </p><br><a name="ceph-key"></a><br><h2>  Erstellen eines Clients f√ºr einen Datenpool </h2><br><p>  Erstellen Sie einen Client f√ºr den neuen Pool </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Wir werden einen Schl√ºssel f√ºr den Kunden erhalten, in Zukunft werden wir ihn ben√∂tigen, um geheime Kubernetes zu erstellen </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Admin-Schl√ºssel erhalten </h2><br><p>  Und hol dir den Admin-Schl√ºssel </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>Auf dem Ceph-Cluster sind alle Arbeiten abgeschlossen, und jetzt m√ºssen wir zu einem Computer gehen, der Zugriff auf den Kubernetes-Cluster hat</strong> <br>  Ich werde mit dem master01-Test (10.73.71.25) des von mir in der ersten Ver√∂ffentlichung bereitgestellten Clusters arbeiten. </p><br><a name="kubernetes-secrets"></a><br><h2>  Ein Kundengeheimnis erstellen </h2><br><p>  Erstellen Sie eine Datei mit dem Client-Token, das wir erhalten haben (vergessen Sie nicht, sie durch Ihr Token zu ersetzen). </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  Und schaffen Sie ein Geheimnis, das wir in Zukunft nutzen werden </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Admin-Geheimnis erstellen </h2><br><p>  Erstellen Sie eine Datei mit dem Admin-Token (vergessen Sie nicht, sie durch Ihr Token zu ersetzen). </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Danach erstellen Sie ein Admin-Geheimnis </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  √úberpr√ºfen Sie, ob Geheimnisse erstellt wurden </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  Methode zuerst bereitstellen ceph rbd Provisioner </h2><br><p>  Wir klonen das Kubernetes-Inkubator / External-Storage-Repository mit Github. Es enth√§lt alles, was Sie ben√∂tigen, um den Kubernetes-Cluster mit dem Ceph-Repository anzufreunden. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Fazit </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  Methode zwei: Ersetzen Sie das kube-controller-manager-Image </h2><br><p>  Es gibt keine rbd-Unterst√ºtzung im offiziellen kube-controller-manager-Image, daher m√ºssen wir das controller-manager-Image √§ndern. <br>  Dazu m√ºssen Sie auf jedem der Kubernetes-Assistenten die Datei kube-controller-manager.yaml bearbeiten und das Bild durch gcr.io/google_containers/hyperkube:v1.15.2 ersetzen.  Achten Sie auf die Version des Bildes, die Ihrer Version des Kubernetes-Clusters entsprechen soll. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Danach m√ºssen Sie kube-controller-manager neu starten </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Pods sollten automatisch aktualisiert werden. Wenn dies jedoch aus irgendeinem Grund nicht geschehen ist, k√∂nnen Sie sie durch L√∂schen manuell neu erstellen. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  √úberpr√ºfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - - </p><br><a name="storage-class"></a><br><h2>  Erstellen einer Speicherklasse </h2><br><p>  <strong>Methode eins - wenn Sie den Provisioner ceph.com/rbd verwendet haben</strong> <br>  Erstellen Sie eine Yaml-Datei mit einer Beschreibung unserer Speicherklasse.  Au√üerdem k√∂nnen alle unten verwendeten Dateien <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in mein Repository</a> im ceph-Verzeichnis heruntergeladen werden </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Und bette ihn in unseren Cluster ein </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  √úberpr√ºfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>Methode zwei - wenn Sie den Provisioner kubernetes.io/rbd verwendet haben</strong> <br>  Speicherklasse erstellen </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Und bette ihn in unseren Cluster ein </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  √úberpr√ºfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Kubernetes + Ceph-Bandtest </h2><br><p>  Bevor Sie ceph + kubernetes testen, m√ºssen Sie das ceph-common-Paket auf JEDEM Workcode des Clusters installieren. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Erstellen Sie eine Yaml-Datei PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  T√∂te ihn </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  √úberpr√ºfen Sie, ob PersistentVolumeClaim erstellt wurde. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  Und auch automatisch PersistentVolume erstellt. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Lassen Sie uns einen Test-Pod erstellen, in dem wir den erstellten PVC in das Verzeichnis / mnt aufnehmen.  F√ºhren Sie diese Datei /mnt/test.txt mit dem Text "Hello World!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Wir werden ihn t√∂ten und √ºberpr√ºfen, ob er seine Aufgabe erf√ºllt hat </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Warten wir auf den Status </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Lassen Sie uns ein neues erstellen, unser Volume daran anschlie√üen, aber bereits in / mnt / test, und danach sicherstellen, dass die vom ersten Volume erstellte Datei vorhanden ist </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  F√ºhren Sie kubectl get po -w aus und warten Sie, bis der Pod ausgef√ºhrt wird <br>  Danach gehen wir hinein und √ºberpr√ºfen, ob das Volume verbunden ist und unsere Datei im Verzeichnis / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Vielen Dank f√ºr das Lesen bis zum Ende.  Entschuldigen Sie die Verz√∂gerung bei der Ver√∂ffentlichung. <br>  Ich bin bereit, alle Fragen in pers√∂nlichen Nachrichten oder in sozialen Netzwerken zu beantworten, die in meinem Profil angegeben sind. <br>  In der n√§chsten kleinen Ver√∂ffentlichung werde ich Ihnen erkl√§ren, wie Sie S3-Speicher basierend auf dem erstellten Ceph-Cluster und dann gem√§√ü dem Plan aus der ersten Ver√∂ffentlichung bereitstellen. </p><br><a name="book"></a><br><p>  Zur Ver√∂ffentlichung verwendete Materialien </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Offizielle Ceph-Dokumentation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einf√ºhrung in das Ceph-Repository in Bildern</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Offizielle Dokumentation von Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-Inkubator / externer Speicher</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes-ceph-percona-Repository mit Beispieldateien</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465399/">https://habr.com/ru/post/de465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465379/index.html">Selbstgemachte drahtlose eigenst√§ndige Insulinpumpensteuerung</a></li>
<li><a href="../de465391/index.html">Auf den Spuren der russischen Scala-Bewegung. Teil 1</a></li>
<li><a href="../de465393/index.html">Batterieleistung f√ºr MySensors-Ger√§te</a></li>
<li><a href="../de465395/index.html">Was ist der Hauptunterschied zwischen Dependency Injection und Service Locator?</a></li>
<li><a href="../de465397/index.html">Wie ist der Nitro-√úbersetzer entstanden, der Entwicklern bei der Lokalisierung und beim technischen Support hilft?</a></li>
<li><a href="../de465401/index.html">5 Aktivit√§ten zur Beschleunigung der Probleml√∂sung in einem beliebigen IT-Team</a></li>
<li><a href="../de465403/index.html">Achtung! Neue Kameras unterwegs oder aktuelle Informationen zu Radarger√§ten und Radarwarnern</a></li>
<li><a href="../de465407/index.html">1. √úbersicht √ºber Extreme Enterprise Layer Switches</a></li>
<li><a href="../de465409/index.html">Vue.js Best Practices f√ºr die Webentwicklung</a></li>
<li><a href="../de465415/index.html">Wir sprechen √ºber DevOps in einer verst√§ndlichen Sprache</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>