<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔥 👨🏽‍💻 🍽️ Ceph - de «à genoux» à «production» 🎅🏿 🔟 👩🏿‍🍳</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Choix du CEPH. Partie 1 


 Nous avions cinq racks, dix commutateurs optiques, BGP configuré, une douzaine de SSD et un tas de disques SAS de toutes l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - de «à genoux» à «production»</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456446/"><h1 id="vybor-ceph-chast-1">  Choix du CEPH.  Partie 1 </h1><br><p>  <em>Nous avions cinq racks, dix commutateurs optiques, BGP configuré, une douzaine de SSD et un tas de disques SAS de toutes les couleurs et tailles, ainsi que du proxmox et le désir de mettre toute la statique dans notre propre stockage S3.</em>  <em>Non pas que tout cela était nécessaire pour la virtualisation, mais une fois que vous avez commencé à utiliser opensource, passez à la fin de votre passe-temps.</em>  <em>La seule chose qui me dérangeait était BGP.</em>  <em>Il n'y a personne au monde plus impuissant, irresponsable et immoral que le routage BGP interne.</em>  <em>Et je savais que très bientôt nous y plongerions.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/09e/a36/178/09ea3617814a9598a6aa9784abc14a76.jpg"></p><br><p>  La tâche était courante - il y avait le CEPH, cela ne fonctionnait pas très bien.  Il fallait faire "bien". <br>  Le cluster que j'ai obtenu était hétérogène, fouetté et pratiquement pas réglé.  Il se composait de deux groupes de nœuds différents, avec une grille commune jouant le rôle de cluster et de réseau public.  Les nœuds étaient équipés de quatre types de disques - deux types de SSD, assemblés en deux règles de placement distinctes et deux types de disques durs de tailles différentes, assemblés en un troisième groupe.  Le problème avec différentes tailles a été résolu par différents poids OSD. </p><br><p>  Le paramètre lui-même était divisé en deux parties: le <strong>réglage du système d'exploitation</strong> et le <strong>réglage du CEPH lui</strong> - <strong>même</strong> et de ses paramètres. </p><a name="habracut"></a><br><h2 id="prokachka-os">  Mise à niveau du système d'exploitation </h2><br><h3 id="network">  Réseau </h3><br><p>  Une latence élevée affectait à la fois l'enregistrement et l'équilibrage.  Lors de l'enregistrement - parce que le client ne reçoit pas de réponse sur l'enregistrement réussi jusqu'à ce que les répliques de données dans d'autres groupes de placement confirment le succès.  Étant donné que les règles de distribution des répliques dans la carte CRUSH comportaient une réplique par hôte, le réseau était toujours utilisé. </p><br><p>  Par conséquent, la première chose que j'ai décidé de configurer légèrement le réseau actuel, tout en essayant de me convaincre de passer à des réseaux distincts. </p><br><p>  Tout d'abord, tordu les paramètres des cartes réseau.  Commencé par la mise en place de files d'attente: </p><br><p>  ce qui était: </p><br><div class="spoiler">  <b class="spoiler_title">ethtool -l ens1f1</b> <div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1 root@ceph01:~# ethtool -g ens1f1 Ring parameters for ens1f1: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1</code> </pre> </div></div><br><p>  On voit que les paramètres actuels sont loin d'être des maximums.  Augmenté: </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ethtool -G ens1f0 rx 4096 root@ceph01:~#ethtool -G ens1f0 tx 4096 root@ceph01:~#ethtool -L ens1f0 combined 63</code> </pre> <br><p>  Guidé par un excellent article </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a> </p><br><p>  augmentation de la longueur de la <strong>file</strong> d' <strong>attente</strong> de <strong>txqueuelen</strong> de 1000 à 10 000 </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set ens1f0 txqueuelen 10000</code> </pre> <br><p>  Eh bien, en suivant la documentation de ceph elle-même </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://ceph.com/geen-categorie/ceph-loves-jumbo-frames/</a> </p><br><p>  augmentation du <strong>MTU</strong> à 9000. </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set dev ens1f0 mtu 9000</code> </pre> <br><p>  Ajouté à / etc / network / interfaces, de sorte que tout ce qui précède est chargé au démarrage </p><br><div class="spoiler">  <b class="spoiler_title">cat / etc / network / interfaces</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01:~# cat /etc/network/interfaces auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet manual post-up /sbin/ethtool -G ens1f0 rx 4096 post-up /sbin/ethtool -G ens1f0 tx 4096 post-up /sbin/ethtool -L ens1f0 combined 63 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 auto ens1f1 iface ens1f1 inet manual post-up /sbin/ethtool -G ens1f1 rx 4096 post-up /sbin/ethtool -G ens1f1 tx 4096 post-up /sbin/ethtool -L ens1f1 combined 63 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000</code> </pre> </div></div><br><p>  Après quoi, suivant le même article, il a commencé à remonter soigneusement les poignées du noyau 4.15.  Considérant que sur les nœuds de RAM 128G, nous avons obtenu un certain fichier de configuration pour <strong>sysctl</strong> </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/sysctl.d/50-ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">net.core.rmem_max = 56623104 #        54M net.core.wmem_max = 56623104 #        54M net.core.rmem_default = 56623104 #        . 54M net.core.wmem_default = 56623104 #         54M #    net.ipv4.tcp_rmem = 4096 87380 56623104 # (,  , )    tcp_rmem #  3  ,      TCP. # :   TCP       #   .     #       (moderate memory pressure). #       8  (8192). #  :  ,    #   TCP  .     #  /proc/sys/net/core/rmem_default,   . #       ( ) #  87830 .     65535  #     tcp_adv_win_scale  tcp_app_win = 0, #  ,       tcp_app_win. # :   ,     #     TCP.     , #    /proc/sys/net/core/rmem_max.  «» #     SO_RCVBUF     . net.ipv4.tcp_wmem = 4096 65536 56623104 net.core.somaxconn = 5000 #    ,  . net.ipv4.tcp_timestamps=1 #     (timestamps),    RFC 1323. net.ipv4.tcp_sack=1 #     TCP net.core.netdev_max_backlog=5000 ( 1000) #       ,  #    ,     . net.ipv4.tcp_max_tw_buckets=262144 #   ,    TIME-WAIT . #     – «»     #    . net.ipv4.tcp_tw_reuse=1 #   TIME-WAIT   , #     . net.core.optmem_max=4194304 #   - ALLOCATABLE #    (4096 ) net.ipv4.tcp_low_latency=1 #  TCP/IP      #     . net.ipv4.tcp_adv_win_scale=1 #          , #    TCP-    . #   tcp_adv_win_scale ,     #   : # Bytes- bytes\2  -tcp_adv_win_scale #  bytes –     .   tcp_adv_win_scale # ,       : # Bytes- bytes\2  tcp_adv_win_scale #    .  - – 2, # ..     ¼  ,   # tcp_rmem. net.ipv4.tcp_slow_start_after_idle=0 #    ,     # ,       . #   SSR  ,    #  . net.ipv4.tcp_no_metrics_save=1 #    TCP      . net.ipv4.tcp_syncookies=0 #   syncookie net.ipv4.tcp_ecn=0 #Explicit Congestion Notification (   )  # TCP-.      «» #       .     # -        #    . net.ipv4.conf.all.send_redirects=0 #   ICMP Redirect …  .    #   ,        . #    . net.ipv4.ip_forward=0 #  .   ,     , #    . net.ipv4.icmp_echo_ignore_broadcasts=1 #   ICMP ECHO ,    net.ipv4.tcp_fin_timeout=10 #      FIN-WAIT-2   #   .  60 net.core.netdev_budget=600 # ( 300) #        , #          #  .    NIC ,    . # ,     SoftIRQs # ( )  CPU.    netdev_budget. #    300.    SoftIRQ  # 300   NIC     CPU net.ipv4.tcp_fastopen=3 # TFO TCP Fast Open #        TFO,      #    TCP .     ,  #  )</code> </pre> </div></div><br><p>  Avec <strong>un réseau</strong> lustré <strong>, il a</strong> été alloué sur des interfaces réseau 10 Gbit / s distinctes à un réseau plat distinct.  <strong>Les</strong> cartes réseau <strong>Mellanox</strong> 10/25 Gbit / s à deux ports <strong>livrées</strong> dans deux commutateurs 10 Gbit / s distincts ont été livrées sur chaque machine.  L'agrégation a été réalisée en utilisant OSPF, car la liaison avec lacp pour une raison quelconque a montré une bande passante totale d'un maximum de 16 Gbps, tandis que ospf a utilisé avec succès les deux douzaines sur chaque machine.  Les plans futurs étaient d'utiliser ROCE sur ces mélanoxes pour réduire la latence.  Comment configurer cette partie du réseau: </p><br><ol><li>  Puisque les machines elles-mêmes ont des IP externes sur BGP, nous avons besoin d'un logiciel - <em>(ou plutôt, au moment de la rédaction de ce document, c'était <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">frr = 6.0-1</a> )</em> déjà installé. </li><li>  Au total, les machines avaient deux interfaces réseau avec deux interfaces - un total de 4 ports.  Une carte réseau a regardé l'usine avec deux ports et BGP a été configuré dessus, la seconde - sur deux ports a regardé deux commutateurs différents et OSPF a été réglé dessus </li></ol><br><p>  Détails de configuration OSPF: La tâche principale consiste à agréger deux liens et à avoir une tolérance aux pannes. <br>  deux interfaces réseau sont configurées dans deux réseaux plats simples - 10.10.10.0/24 et 10.10.20.0/24 </p><br><pre> <code class="plaintext hljs">1: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.10.2/24 brd 10.10.10.255 scope global ens1f0 2: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.20.2/24 brd 10.10.20.255 scope global ens1f1</code> </pre> <br><p>  par lequel les voitures se voient. </p><br><h3 id="disk">  DISK </h3><br><p>  L'étape suivante consistait à optimiser les performances des disques.  Pour SSD, j'ai changé le planificateur en <strong>noop</strong> , pour HDD - <strong>date limite</strong> .  Si à peu près - alors NOOP fonctionne sur le principe de "qui s'est levé en premier - et des pantoufles", qui en anglais ressemble à "FIFO (First In, First Out)".  Les demandes sont mises en file d'attente dès qu'elles sont disponibles.  DEADLINE est plus en lecture seule, et le processus de la file d'attente obtient un accès presque exclusif au disque au moment de l'opération.  C'est très bien pour notre système - après tout, un seul processus fonctionne avec chaque disque - le démon OSD. <br>  (Ceux qui souhaitent s'immerger dans le planificateur d'E / S peuvent en lire plus ici: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> </p><br><p>  Je préfère lire en russe: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.opennet.ru/base/sys/linux_shedulers.txt.html</a> ) </p><br><p>  Dans les recommandations pour l'optimisation de Linux, il est conseillé d'augmenter également nr_request </p><br><blockquote>  <em>nr_requests</em> <em><br></em>  <em>La valeur de nr_requests détermine la quantité de demandes d'E / S qui sont mises en mémoire tampon avant que le planificateur d'E / S envoie / reçoive des données au périphérique de bloc, si vous utilisez une carte RAID / périphérique de bloc qui peut gérer une file d'attente plus grande que ce que le I Le planificateur d'E / S est défini sur, l'augmentation de la valeur de nr_requests peut aider à améliorer l'ensemble et à réduire la charge du serveur lorsque de grandes quantités d'E / S se produisent sur le serveur.</em>  <em>Si vous utilisez Deadline ou CFQ comme planificateur, il est conseillé de définir la valeur nr_request sur 2 fois la valeur de la profondeur de file d'attente.</em> </blockquote><p>  MAIS!  Les citoyens eux-mêmes sont les développeurs du CEPH qui nous convainc que leur système de priorité fonctionne mieux </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b7e/fd3/bd0/b7efd3bd03fedc88307e200905c8c6a9.gif"></p><br><div class="spoiler">  <b class="spoiler_title">WBThrottle et / ou nr_requests</b> <div class="spoiler_text"><blockquote>  WBThrottle et / ou nr_requests <br>  Le stockage de fichiers utilise des E / S tamponnées pour écrire;  cela apporte un certain nombre d'avantages si le journal de stockage de fichiers se trouve sur un support plus rapide.  Les demandes des clients sont notifiées dès que les données sont écrites dans le journal, puis vidées sur le disque de données lui-même ultérieurement à l'aide de la fonctionnalité Linux standard.  Cela permet aux disques de broche OSD de fournir des latences d'écriture similaires aux SSD lors de l'écriture en petits paquets.  Cette écriture différée retardée permet également au noyau lui-même de reconstruire les demandes d'E / S sur le disque dans l'espoir de les fusionner ou de permettre aux têtes de disque existantes de choisir un chemin plus optimal au-dessus de leurs plaques.  L'effet final est que vous pouvez extraire légèrement plus d'E / S de chaque disque que ce ne serait possible avec des E / S directes ou synchrones. </blockquote><p>  Cependant, un certain problème se pose si le volume d'enregistrements entrants dans un cluster Ceph donné est en avance sur toutes les capacités des disques sous-jacents.  Dans un tel scénario, le nombre total d'E / S en attente d'écriture en attente sur un disque peut augmenter de manière incontrôlable et entraîner une file d'attente d'opérations d'E / S qui remplissent l'intégralité du disque et les files d'attente Ceph.  Les demandes de lecture fonctionnent particulièrement mal, car elles se bloquent entre les demandes d'écriture, ce qui peut prendre plusieurs secondes pour être vidée sur le disque principal. </p><br><p>  Pour vaincre ce problème, Ceph a un mécanisme de limitation de réécriture appelé WBThrottle intégré au stockage de fichiers.  Il est conçu pour limiter le nombre total d'opérations d'E / S d'écriture en attente qui peuvent être mises en file d'attente et démarrer leur processus de réinitialisation plus tôt que cela ne se produirait naturellement en raison de l'inclusion par le noyau lui-même.  Malheureusement, les tests démontrent que les valeurs par défaut ne peuvent toujours pas réduire le comportement existant à un niveau qui peut réduire cet effet sur la latence des opérations de lecture.  L'ajustement peut changer ce comportement et réduire la longueur totale des files d'attente d'enregistrement et rendre un tel impact peu fort.  Cependant, il y a un compromis: en réduisant le nombre maximal total d'entrées pouvant être mises en file d'attente, vous pouvez réduire la capacité du noyau lui-même à maximiser son efficacité dans la commande des demandes entrantes.  Cela vaut la peine de penser que vous avez besoin de plus pour votre application spécifique, vos charges de travail et ajustez-les en conséquence. </p><br><p>  Pour contrôler la profondeur d'une telle file d'attente d'écriture en attente, vous pouvez soit réduire le nombre maximal total d'opérations d'E / S ayant échoué à l'aide des paramètres WBThrottle, soit diminuer la valeur maximale des opérations ayant échoué au niveau même du bloc de votre noyau.  Cela et un autre peuvent contrôler efficacement le même comportement et ce sont vos préférences qui seront la base de la mise en œuvre de ce paramètre. <br>  Il convient également de noter que la priorité du système d'exploitation Ceph est plus efficace pour les requêtes plus courtes au niveau du disque.  Lors de la réduction de la file d'attente globale à un disque donné, l'emplacement de la file d'attente principale se déplace vers Ceph, où il a plus de contrôle sur la priorité de l'opération d'E / S.  Prenons l'exemple suivant: </p><br><pre> <code class="plaintext hljs">echo 8 &gt; /sys/block/sda/queue/nr_requests</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> </p></div></div><br><h3 id="common">  COMMUN </h3><br><p>  Et quelques paramètres de noyau supplémentaires à faire <del>  votre voiture est douce et soyeuse </del>  presser un peu plus de performances en fer </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/sysctl.d/60-ceph2.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> kernel.pid_max = 4194303 #     25,       kernel.threads-max=2097152 # , , . vm.max_map_count=524288 #      . #        #         # malloc,    mmap, mprotect  madvise,     #  . fs.aio-max-nr=50000000 #   input-output #  Linux     - (AIO), #       - # ,    -  . #     , #      -. #  aio-max-nr     #  . vm.min_free_kbytes=1048576 #       . #  1Gb,       , #    OOM Killer   OSD.     #    ,      vm.swappiness=10 #       10% . #   128G ,  10%  12 .     . #    60%   ,   , #       vm.vfs_cache_pressure=1000 #    100.     #     . vm.zone_reclaim_mode=0 #         #  ,     . #     ,     . #       # ,    , zone_reclaim_mode #  ,   , # ,   ,   . vm.dirty_ratio=20 #   ,     ""  #    : #   128  . #   20  SSD,     CEPH  #     3G . #   40  HDD,      1G # 20%  128  25.6 . ,     , #    2.4G .         #    -    DevOps   . vm.dirty_background_ratio=3 #   ,    dirty pages  , #    pdflush/flush/kdmflush     fs.file-max=524288 #      ,,   ,    .</code> </pre> </div></div><br><h2 id="pogruzhenie-v--ceph">  Plongez au CEPH </h2><br><p>  Paramètres sur lesquels je voudrais m'attarder plus en détail: </p><br><div class="spoiler">  <b class="spoiler_title">chat /etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">osd: journal_aio: true #  ,  journal_block_align: true #  i/o journal_dio: true #   journal_max_write_bytes: 1073714824 #     #      journal_max_write_entries: 10000 #      journal_queue_max_bytes: 10485760000 journal_queue_max_ops: 50000 rocksdb_separate_wal_dir: true #    wal #       # NVMe bluestore_block_db_create: true #       bluestore_block_db_size: '5368709120 #5G' bluestore_block_wal_create: true bluestore_block_wal_size: '1073741824 #1G' bluestore_cache_size_hdd: '3221225472 # 3G' #     #     bluestore_cache_size_ssd: '9663676416 # 9G' keyring: /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap: '1073741824 #1G' osd_disk_thread_ioprio_class: idle osd_disk_thread_ioprio_priority: 7 osd_disk_threads: 2 #        osd_failsafe_full_ratio: 0.95 osd_heartbeat_grace: 5 osd_heartbeat_interval: 3 osd_map_dedup: true osd_max_backfills: 2 #       . osd_max_write_size: 256 osd_mon_heartbeat_interval: 5 osd_op_threads: 16 osd_op_num_threads_per_shard: 1 osd_op_num_threads_per_shard_hdd: 2 osd_op_num_threads_per_shard_ssd: 2 osd_pool_default_min_size: 1 #  .    osd_pool_default_size: 2 #  ,    #     #   osd_recovery_delay_start: 10.000000 osd_recovery_max_active: 2 osd_recovery_max_chunk: 1048576 osd_recovery_max_single_start: 3 osd_recovery_op_priority: 1 osd_recovery_priority: 1 #       osd_recovery_sleep: 2 osd_scrub_chunk_max: 4</code> </pre> </div></div><br><p>  Certains paramètres qui ont été testés sur QA sur la version 12.2.12 sont manquants dans ceph version 12.2.2, par exemple, <strong>osd_recovery_threads.</strong>  Par conséquent, les plans incluaient une mise à jour de la prod au 12.2.12.  La pratique a montré la compatibilité dans un cluster de versions 12.2.2 et 12.2.12, ce qui permet une mise à jour continue. </p><br><h3 id="testovyy-klaster">  Cluster de test </h3><br><p>  Naturellement, pour les tests, il était nécessaire d'avoir la même version que dans la bataille, mais au moment où j'ai commencé à travailler avec le cluster dans le référentiel, il n'y en avait qu'une plus récente.  Voyant que dans la version mineure, vous n'êtes pas très volumineux ( <strong>1393</strong> lignes dans les configurations contre <strong>1436</strong> lignes dans la nouvelle version), nous avons décidé de commencer à tester une nouvelle (elle est toujours mise à jour, pourquoi continuer sur l'ancienne corbeille) </p><br><p>  La seule chose qu'ils ont essayé de quitter l'ancienne version est le <strong>package ceph-deploy,</strong> car une partie des utilitaires (et une partie des employés) a été affinée par sa syntaxe.  La nouvelle version était assez différente, mais elle n'a pas affecté le fonctionnement du cluster lui-même, et les versions <strong>1.5.39 l'ont</strong> laissé </p><br><p>  Étant donné que la commande ceph-disk indique clairement qu'elle est obsolète et que nous utilisons, chère, la commande ceph-volume - nous avons commencé à créer l'OSD avec cette commande, sans perdre de temps sur l'obsolète. </p><br><p>  Le plan était de créer un miroir de deux disques SSD, sur lesquels nous plaçons les journaux OSD, qui, à leur tour, sont situés sur les SAS de la broche.  Ainsi, nous pouvons nous protéger des problèmes de données lorsqu'un disque avec un journal tombe. </p><br><p>  Créer un cluster de documentation acier </p><br><div class="spoiler">  <b class="spoiler_title">chat /etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01-qa:~# cat /etc/ceph/ceph.conf #     [client] rbd_cache = true rbd_cache_max_dirty = 50331648 rbd_cache_max_dirty_age = 2 rbd_cache_size = 67108864 rbd_cache_target_dirty = 33554432 rbd_cache_writethrough_until_flush = true rbd_concurrent_management_ops = 10 rbd_default_format = 2 [global] auth_client_required = cephx auth_cluster_required = cephx auth_service_required = cephx cluster network = 10.10.10.0/24 debug_asok = 0/0 debug_auth = 0/0 debug_buffer = 0/0 debug_client = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_filer = 0/0 debug_filestore = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_journal = 0/0 debug_journaler = 0/0 debug_lockdep = 0/0 debug_mon = 0/0 debug_monc = 0/0 debug_ms = 0/0 debug_objclass = 0/0 debug_objectcatcher = 0/0 debug_objecter = 0/0 debug_optracker = 0/0 debug_osd = 0/0 debug_paxos = 0/0 debug_perfcounter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_rgw = 0/0 debug_throttle = 0/0 debug_timer = 0/0 debug_tp = 0/0 fsid = d0000000d-4000-4b00-b00b-0123qwe123qwf9 mon_host = ceph01-q, ceph02-q, ceph03-q mon_initial_members = ceph01-q, ceph02-q, ceph03-q public network = 8.8.8.8/28 #  ,  )) rgw_dns_name = s3-qa.mycompany.ru #     rgw_host = s3-qa.mycompany.ru #    [mon] mon allow pool delete = true mon_max_pg_per_osd = 300 #     #     #  , ,    , #     OSD.     PG #     -    mon_osd_backfillfull_ratio = 0.9 mon_osd_down_out_interval = 5 mon_osd_full_ratio = 0.95 #   SSD     #   -      #   5%   (   1.2Tb) #   ,     # bluestore_block_db_size     #   mon_osd_nearfull_ratio = 0.9 mon_pg_warn_max_per_osd = 520 [osd] bluestore_block_db_create = true bluestore_block_db_size = 5368709120 #5G bluestore_block_wal_create = true bluestore_block_wal_size = 1073741824 #1G bluestore_cache_size_hdd = 3221225472 # 3G bluestore_cache_size_ssd = 9663676416 # 9G journal_aio = true journal_block_align = true journal_dio = true journal_max_write_bytes = 1073714824 journal_max_write_entries = 10000 journal_queue_max_bytes = 10485760000 journal_queue_max_ops = 50000 keyring = /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap = 1073741824 #1G osd_disk_thread_ioprio_class = idle osd_disk_thread_ioprio_priority = 7 osd_disk_threads = 2 osd_failsafe_full_ratio = 0.95 osd_heartbeat_grace = 5 osd_heartbeat_interval = 3 osd_map_dedup = true osd_max_backfills = 4 osd_max_write_size = 256 osd_mon_heartbeat_interval = 5 osd_op_num_threads_per_shard = 1 osd_op_num_threads_per_shard_hdd = 2 osd_op_num_threads_per_shard_ssd = 2 osd_op_threads = 16 osd_pool_default_min_size = 1 osd_pool_default_size = 2 osd_recovery_delay_start = 10.0 osd_recovery_max_active = 1 osd_recovery_max_chunk = 1048576 osd_recovery_max_single_start = 3 osd_recovery_op_priority = 1 osd_recovery_priority = 1 osd_recovery_sleep = 2 osd_scrub_chunk_max = 4 osd_scrub_chunk_min = 2 osd_scrub_sleep = 0.1 rocksdb_separate_wal_dir = true</code> </pre> </div></div><br><pre> <code class="plaintext hljs">#   root@ceph01-qa:~#ceph-deploy mon create ceph01-q #        root@ceph01-qa:~#ceph-deploy gatherkeys ceph01-q #   .       - ,       # mon_initial_members = ceph01-q, ceph02-q, ceph03-q #         root@ceph01-qa:~#ceph-deploy mon create-initial #        root@ceph01-qa:~#cat ceph.bootstrap-osd.keyring &gt; /var/lib/ceph/bootstrap-osd/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-mgr.keyring &gt; /var/lib/ceph/bootstrap-mgr/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-rgw.keyring &gt; /var/lib/ceph/bootstrap-rgw/ceph.keyring #      root@ceph01-qa:~#ceph-deploy admin ceph01-q #  ,   root@ceph01-qa:~#ceph-deploy mgr create ceph01-q</code> </pre> <br><p> ,        ceph-deploy    12.2.12 —      OSD  db    - </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 blkid could not detect a PARTUUID for device: /dev/md1</code> </pre> <br><p> , blkid   PARTUUID,    : </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#parted /dev/md0 mklabel GPT #   , #  GPT     #        = bluestore_block_db_size: '5368709120 #5G' #    20  OSD,     #    root@ceph01-qa:~#for i in {1..20}; do echo -e "n\n\n\n+5G\nw" | fdisk /dev/md0; done</code> </pre> <br><p>   ,     OSD     (, ,    ) </p><br><p>   OSD  bluestore     WAL,    db </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 stderr: 2019-04-12 10:39:27.211242 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid stderr: 2019-04-12 10:39:27.213185 7eff461b6e00 -1 bdev(0x55824c273680 /var/lib/ceph/osd/ceph-0//block.wal) open open got: (22) Invalid argument stderr: 2019-04-12 10:39:27.213201 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _open_db add block device(/var/lib/ceph/osd/ceph-0//block.wal) returned: (22) Invalid argument stderr: 2019-04-12 10:39:27.999039 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) mkfs failed, (22) Invalid argument stderr: 2019-04-12 10:39:27.999057 7eff461b6e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (22) Invalid argument stderr: 2019-04-12 10:39:27.999141 7eff461b6e00 -1 ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0/: (22) Invalid argumen</code> </pre> <br><p>     -  (   ,  )      WAL      OSD —     (    WAL,  , ,   ). </p><br><p> ,         WAL  NVMe,     . </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sdf --block.wal /dev/md0p2 --block.db /dev/md1p2</code> </pre> <br><p>  ,   OSD.      ,        —    SSD  ,     SAS. </p><br><p>       20 ,     ,  — . <br> , ,   : </p><br><div class="spoiler"> <b class="spoiler_title">ceph osd tree</b> <div class="spoiler_text"><p> root@eph01-q:~# ceph osd tree <br> ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF <br> -1 14.54799 root default <br> -3 9.09200 host ceph01-q <br> 0 ssd 1.00000 osd.0 up 1.00000 1.00000 <br> 1 ssd 1.00000 osd.1 up 1.00000 1.00000 <br> 2 ssd 1.00000 osd.2 up 1.00000 1.00000 <br> 3 ssd 1.00000 osd.3 up 1.00000 1.00000 <br> 4 hdd 1.00000 osd.4 up 1.00000 1.00000 <br> 5 hdd 0.27299 osd.5 up 1.00000 1.00000 <br> 6 hdd 0.27299 osd.6 up 1.00000 1.00000 <br> 7 hdd 0.27299 osd.7 up 1.00000 1.00000 <br> 8 hdd 0.27299 osd.8 up 1.00000 1.00000 <br> 9 hdd 0.27299 osd.9 up 1.00000 1.00000 <br> 10 hdd 0.27299 osd.10 up 1.00000 1.00000 <br> 11 hdd 0.27299 osd.11 up 1.00000 1.00000 <br> 12 hdd 0.27299 osd.12 up 1.00000 1.00000 <br> 13 hdd 0.27299 osd.13 up 1.00000 1.00000 <br> 14 hdd 0.27299 osd.14 up 1.00000 1.00000 <br> 15 hdd 0.27299 osd.15 up 1.00000 1.00000 <br> 16 hdd 0.27299 osd.16 up 1.00000 1.00000 <br> 17 hdd 0.27299 osd.17 up 1.00000 1.00000 <br> 18 hdd 0.27299 osd.18 up 1.00000 1.00000 <br> 19 hdd 0.27299 osd.19 up 1.00000 1.00000 <br> -5 5.45599 host ceph02-q <br> 20 ssd 0.27299 osd.20 up 1.00000 1.00000 <br> 21 ssd 0.27299 osd.21 up 1.00000 1.00000 <br> 22 ssd 0.27299 osd.22 up 1.00000 1.00000 <br> 23 ssd 0.27299 osd.23 up 1.00000 1.00000 <br> 24 hdd 0.27299 osd.24 up 1.00000 1.00000 <br> 25 hdd 0.27299 osd.25 up 1.00000 1.00000 <br> 26 hdd 0.27299 osd.26 up 1.00000 1.00000 <br> 27 hdd 0.27299 osd.27 up 1.00000 1.00000 <br> 28 hdd 0.27299 osd.28 up 1.00000 1.00000 <br> 29 hdd 0.27299 osd.29 up 1.00000 1.00000 <br> 30 hdd 0.27299 osd.30 up 1.00000 1.00000 <br> 31 hdd 0.27299 osd.31 up 1.00000 1.00000 <br> 32 hdd 0.27299 osd.32 up 1.00000 1.00000 <br> 33 hdd 0.27299 osd.33 up 1.00000 1.00000 <br> 34 hdd 0.27299 osd.34 up 1.00000 1.00000 <br> 35 hdd 0.27299 osd.35 up 1.00000 1.00000 <br> 36 hdd 0.27299 osd.36 up 1.00000 1.00000 <br> 37 hdd 0.27299 osd.37 up 1.00000 1.00000 <br> 38 hdd 0.27299 osd.38 up 1.00000 1.00000 <br> 39 hdd 0.27299 osd.39 up 1.00000 1.00000 <br> -7 6.08690 host ceph03-q <br> 40 ssd 0.27299 osd.40 up 1.00000 1.00000 <br> 41 ssd 0.27299 osd.41 up 1.00000 1.00000 <br> 42 ssd 0.27299 osd.42 up 1.00000 1.00000 <br> 43 ssd 0.27299 osd.43 up 1.00000 1.00000 <br> 44 hdd 0.27299 osd.44 up 1.00000 1.00000 <br> 45 hdd 0.27299 osd.45 up 1.00000 1.00000 <br> 46 hdd 0.27299 osd.46 up 1.00000 1.00000 <br> 47 hdd 0.27299 osd.47 up 1.00000 1.00000 <br> 48 hdd 0.27299 osd.48 up 1.00000 1.00000 <br> 49 hdd 0.27299 osd.49 up 1.00000 1.00000 <br> 50 hdd 0.27299 osd.50 up 1.00000 1.00000 <br> 51 hdd 0.27299 osd.51 up 1.00000 1.00000 <br> 52 hdd 0.27299 osd.52 up 1.00000 1.00000 <br> 53 hdd 0.27299 osd.53 up 1.00000 1.00000 <br> 54 hdd 0.27299 osd.54 up 1.00000 1.00000 <br> 55 hdd 0.27299 osd.55 up 1.00000 1.00000 <br> 56 hdd 0.27299 osd.56 up 1.00000 1.00000 <br> 57 hdd 0.27299 osd.57 up 1.00000 1.00000 <br> 58 hdd 0.27299 osd.58 up 1.00000 1.00000 <br> 59 hdd 0.89999 osd.59 up 1.00000 1.00000 </p></div></div><br><p>          : </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket rack01 root #  root root@ceph01-q:~#ceph osd crush add-bucket ceph01-q host #   root@ceph01-q:~#ceph osd crush move ceph01-q root=rack01 #     root@ceph01-q:~#osd crush add 28 1.0 host=ceph02-q #     #       root@ceph01-q:~# ceph osd crush remove osd.4 root@ceph01-q:~# ceph osd crush remove rack01</code> </pre> <br><p> ,      <strong></strong> ,            —  <strong>ceph osd crush move ceph01-host root=rack01</strong> ,      .    CTRL+C     . </p><br><p>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://tracker.ceph.com/issues/23386</a> </p><br><p>    crushmap     <strong>rule replicated_ruleset</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd getcrushmap -o crushmap.row #     root@ceph01-prod:~#crushtool -d crushmap.row -o crushmap.txt #   root@ceph01-prod:~#vim crushmap.txt #,  rule replicated_ruleset root@ceph01-prod:~#crushtool -c crushmap.txt -o new_crushmap.row #  root@ceph01-prod:~#ceph osd setcrushmap -i new_crushmap.row #  </code> </pre> <br><p> <strong>:</strong>      placement group  OSD.    ,   . </p><br><p>  ,        —  ,     OSD ,        ,    root default. <br>  ,   ,      root  ssd     ,          default root.   OSD     . <br> <em>     ,     .     </em> </p><br><h3 id="kak-my-delali-razlichnye-gruppy-po-tipam-diskov">        . </h3><br><p>     root- —  ssd   hdd </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket ssd-root root root@ceph01-q:~#ceph osd crush add-bucket hdd-root root</code> </pre> <br><p>        —          </p><br><pre> <code class="plaintext hljs"># : root@ceph01-q:~#ceph osd crush add-bucket ssd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack02 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack03 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack #  root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph03-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host</code> </pre> <br><p>          </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#   0  3  SSD,   ceph01-q,     root@ceph01-q:~# ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 0 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 1 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 2 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 3 1 host=ssd-ceph01-q root-ceph01-q:~#    </code> </pre> <br><p>     ssd-root  hdd-root   root-default ,     </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush remove default</code> </pre> <br><p>     ,        —      root          —        ,     (    root,    ) </p><br><p>        : <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules</a> </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush rule create-simple rule-ssd ssd-root host firstn root-ceph01-q:~#ceph osd crush rule create-simple rule-hdd hdd-root host firstn root-ceph01-q:~#    ,     root-ceph01-q:~#   -        , root-ceph01-q:~#       root-ceph01-q:~#  ,   ,    root-ceph01-q:~#        : root-ceph01-q:~# ##ceph osd crush rule create-simple rule-ssd ssd-root rack firstn</code> </pre> <br><p>    ,            — PROXMOX: </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~# #ceph osd pool create {NAME} {pg_num} {pgp_num} root-ceph01-q:~# ceph osd pool create ssd_pool 1024 1024 root-ceph01-q:~# ceph osd pool create hdd_pool 1024 1024</code> </pre> <br><p>         </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~#ceph osd crush rule ls #    root-ceph01-q:~#ceph osd crush rule dump rule-ssd | grep rule_id # ID  root-ceph01-q:~#ceph osd pool set ssd_pool crush_rule 2</code> </pre><br><p>               —     ,    (    )   ,    . </p><br><p>      300    ,        —        10 Tb    10 PG —      (pg)   —           ). </p><br><p>        PG —         —     . </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,    CEPH. </p><br><p>  : </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://tracker.ceph.com/issues/23386</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://ceph.com/pgcalc/</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456446/">https://habr.com/ru/post/fr456446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456432/index.html">Security Week 25: vulnérabilité d'Evernote et des centaines de boutiques en ligne piratées</a></li>
<li><a href="../fr456434/index.html">Professions du futur: «Qui travaillerez-vous sur Mars?»</a></li>
<li><a href="../fr456436/index.html">Tâche JS courte pour lundi</a></li>
<li><a href="../fr456440/index.html">Les aventures des Malvari insaisissables, partie I</a></li>
<li><a href="../fr456442/index.html">Admission au programme de premier cycle de l'Université d'État de Saint-Pétersbourg avec le soutien de Yandex et JetBrains</a></li>
<li><a href="../fr456448/index.html">Règles pour choisir un framework JS</a></li>
<li><a href="../fr456450/index.html">DO-RA.Avia pour la surveillance du rayonnement cosmique dans l'aviation</a></li>
<li><a href="../fr456452/index.html">Exemples de code C ++ avant et après les plages</a></li>
<li><a href="../fr456462/index.html">Assemblage d'une bibliothèque de composants angulaires en tant que composants Web</a></li>
<li><a href="../fr456466/index.html">Génériques PHP aujourd'hui (enfin, presque)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>