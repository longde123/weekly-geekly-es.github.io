<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîº üö∂ üéâ Approche End2 End dans les t√¢ches de reconnaissance automatique de la parole ü§πüèæ üé™ üò∞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Qu'est-ce que la reconnaissance vocale End2End et pourquoi est-elle n√©cessaire? Quelle est sa diff√©rence avec l'approche classique? Et pourquoi, pour ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Approche End2 End dans les t√¢ches de reconnaissance automatique de la parole</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/468663/">  Qu'est-ce que la reconnaissance vocale End2End et pourquoi est-elle n√©cessaire?  Quelle est sa diff√©rence avec l'approche classique?  Et pourquoi, pour former un bon mod√®le bas√© sur End2End, nous avons besoin d'une √©norme quantit√© de donn√©es - dans notre article d'aujourd'hui. <br><br><h4>  L'approche classique de la reconnaissance vocale </h4><br>  Avant de parler de l'approche End2End, vous devez d'abord parler de l'approche classique de la reconnaissance vocale.  Comment est-il? <br><br><img src="https://habrastorage.org/webt/xk/xm/sc/xkxmscrxxx0n2hvkoxdxyflqaoq.png"><br><a name="habracut"></a><br><h4>  Extraction de fonctionnalit√©s </h4><br>  En fait, ce n'est pas une s√©quence compl√®tement lin√©aire de blocs d'action.  Arr√™tons-nous sur chaque bloc plus en d√©tail.  Nous avons une sorte de discours d'entr√©e, il tombe sur le premier bloc - Extraction de fonctionnalit√©s.  Il s'agit d'un bloc qui tire les signes de la parole.  Il ne faut pas oublier que la parole elle-m√™me est une chose assez compliqu√©e.  Vous devez pouvoir l'utiliser d'une mani√®re ou d'une autre, il existe donc des m√©thodes standard pour isoler les entit√©s de la th√©orie du traitement du signal.  Par exemple, les coefficients Mel-cepstral (MFCC) et ainsi de suite. <br><br><h4>  Mod√®le acoustique </h4><br>  Le composant suivant est le mod√®le acoustique.  Il peut √™tre bas√© sur des r√©seaux de neurones profonds, ou sur la base de m√©langes de distributions gaussiennes et de mod√®les de Markov cach√©s.  Son objectif principal est d'obtenir √† partir d'une section du signal acoustique les distributions de probabilit√© des diff√©rents phon√®mes de cette section. <br><br>  Vient ensuite le d√©codeur, qui recherche le chemin le plus probable dans le graphique en fonction du r√©sultat de la derni√®re √©tape.  La recotation est la touche finale de reconnaissance, dont la t√¢che principale est de repeser les hypoth√®ses et de produire le r√©sultat final. <br><br><img src="https://habrastorage.org/webt/pp/hh/2i/pphh2ietnprdh5aeqtbvh06kzpu.png"><br><br>  Arr√™tons-nous plus en d√©tail sur le mod√®le acoustique.  Comment est-elle?  Nous avons des enregistrements vocaux qui entrent dans un certain syst√®me bas√© sur GMM (m√©lange monausal gausovy) ou HMM.  Autrement dit, nous avons des repr√©sentations sous forme de phon√®mes, nous utilisons des monophones, c'est-√†-dire des phon√®mes ind√©pendants du contexte.  Plus loin, nous faisons des m√©langes de distributions gaussiennes bas√©es sur des phon√®mes contextuels.  Il utilise le clustering bas√© sur des arbres de d√©cision. <br><br>  Ensuite, nous essayons de construire l'alignement.  Une telle m√©thode tout √† fait non triviale nous permet d'obtenir un mod√®le acoustique.  Cela ne semble pas tr√®s simple, en fait c'est encore plus compliqu√©, il y a beaucoup de nuances, de fonctionnalit√©s.  Mais en cons√©quence, un mod√®le form√© sur des centaines d'heures est tr√®s bien capable de simuler l'acoustique. <br><br><img src="https://habrastorage.org/webt/vt/-x/gl/vt-xgltq9lf3fujagn2jkqdriqy.png"><br><br><h4>  D√©codeur </h4><br>  Qu'est-ce qu'un d√©codeur?  C'est le module qui s√©lectionne le chemin de transition le plus probable selon le graphique HCLG, qui se compose de 4 parties: <br><br>  Module H bas√© sur HMM <br>  Module de d√©pendance du contexte C <br>  Module de prononciation L <br>  Module de mod√®le de langage G <br><br>  Nous construisons un graphe sur ces quatre composantes, √† partir duquel nous d√©coderons nos caract√©ristiques acoustiques en certaines constructions verbales. <br><br>  Plus ou moins, il est clair que l'approche classique est plut√¥t lourde et difficile, elle est difficile √† former, car elle se compose d'un grand nombre de parties distinctes, pour chacune desquelles vous devez pr√©parer vos propres donn√©es pour la formation. <br><br><h4>  II Approche End2End </h4><br>  Qu'est-ce que la reconnaissance vocale End2End et pourquoi est-elle n√©cessaire?  Il s'agit d'un certain syst√®me, qui est con√ßu pour refl√©ter directement la s√©quence de signes acoustiques dans la s√©quence de graph√®mes (lettres) ou de mots.  Vous pouvez √©galement dire qu'il s'agit d'un syst√®me qui optimise les crit√®res qui affectent directement la m√©trique finale de l'√©valuation de la qualit√©.  Par exemple, notre t√¢che est sp√©cifiquement le taux d'erreur sur les mots.  Comme je l'ai dit, il n'y a qu'une seule motivation - pr√©senter ces composants complexes √† plusieurs √©tapes comme un composant simple qui affichera directement, produira des mots ou des graph√®mes √† partir de la parole d'entr√©e. <br><br><h4>  Probl√®me de simulation </h4><br>  Ici, nous avons tout de suite un probl√®me: la parole sonore est une s√©quence, et √† la sortie, nous devons √©galement donner une s√©quence.  Et jusqu'en 2006, il n'y avait aucun moyen ad√©quat de mod√©liser cela.  Quel est le probl√®me de la mod√©lisation?  Il fallait que chaque enregistrement cr√©e un balisage complexe, ce qui implique √† quelle seconde nous pronon√ßons un son ou une lettre en particulier.  Il s'agit d'une configuration complexe tr√®s lourde et, par cons√©quent, un grand nombre d'√©tudes sur ce sujet n'ont pas √©t√© men√©es.  En 2006, un article int√©ressant d'Alex Graves ¬´Classification temporelle connexionniste¬ª (CTC) a √©t√© publi√©, dans lequel ce probl√®me est, en principe, r√©solu.  Mais l'article a √©t√© publi√© et il n'y avait pas assez de puissance de calcul √† l'√©poque.  Et de vrais algorithmes de reconnaissance vocale fonctionnels sont apparus beaucoup plus tard. <br><br>  Au total, nous avons: l'algorithme CTC a √©t√© propos√© par Alex Graves il y a treize ans, comme un outil qui vous permet de former / former des mod√®les acoustiques sans avoir besoin de ce balisage complexe - alignement des trames de s√©quence d'entr√©e et de sortie.  Sur la base de cet algorithme, un travail est apparu initialement qui n'√©tait pas termin√© end2end; des phon√®mes ont donc √©t√© √©mis.  Il convient de noter que les phon√®mes contextuels bas√©s sur STS obtiennent l'un des meilleurs r√©sultats dans la reconnaissance de la libert√© d'expression.  Mais il convient √©galement de noter que cet algorithme, appliqu√© directement aux mots, reste quelque part en retard pour le moment. <br><br><img src="https://habrastorage.org/webt/ns/oe/e2/nsoee2tomucbndnfsaj__7va-o4.png"><br><br><h4>  Qu'est-ce que STS </h4><br>  Maintenant, nous allons parler un peu plus en d√©tail de ce qu'est le STS, et pourquoi il est n√©cessaire, quelle fonction il remplit.  STS est n√©cessaire pour former le mod√®le acoustique sans avoir besoin d'un alignement image par image entre le son et la transcription.  L'alignement image par image, c'est quand on dit qu'une image particuli√®re d'un son correspond √† une telle image de la transcription.  Nous avons un encodeur conventionnel qui accepte les signes acoustiques en entr√©e - il donne une sorte de dissimulation de l'√©tat, sur la base de laquelle nous obtenons des probabilit√©s conditionnelles en utilisant softmax.  Le codeur se compose g√©n√©ralement de plusieurs couches de LSTM ou d'autres variantes de RNN.  Il convient de noter que STS fonctionne en plus des caract√®res ordinaires avec un caract√®re sp√©cial appel√© caract√®re vide ou symbole vide.  Afin de r√©soudre le probl√®me qui se pose du fait que toutes les trames acoustiques n'ont pas de trame en transcription et vice versa (c'est-√†-dire que nous avons des lettres ou des sons qui sonnent beaucoup plus longtemps et qu'il y a des sons courts, des sons r√©p√©titifs), et l√† ce symbole vierge. <br><br>  Le STS lui-m√™me est destin√© √† maximiser la probabilit√© finale de s√©quences de caract√®res et √† g√©n√©raliser l'alignement possible.  Puisque nous voulons utiliser cet algorithme dans les r√©seaux de neurones, il est entendu que nous devons comprendre comment fonctionnent ses modes de fonctionnement avant et arri√®re.  Nous ne nous attarderons pas sur la justification math√©matique et les caract√©ristiques du fonctionnement de cet algorithme, sinon cela prendra tr√®s longtemps. <br><br>  Qu'avons-nous: le premier ASR bas√© sur l'algorithme STS appara√Æt en 2014.  Encore une fois, Alex Graves a pr√©sent√© une publication bas√©e sur le STS caract√®re par caract√®re qui affiche directement le discours d'entr√©e dans une s√©quence de mots.  L'un des commentaires qu'ils ont fait dans cet article est qu'il est important d'utiliser un mod√®le de son externe pour obtenir un bon r√©sultat. <br><br><h4>  5 fa√ßons d'am√©liorer l'algorithme </h4><br>  Il existe de nombreuses variantes et am√©liorations de l'algorithme ci-dessus.  Voici, par exemple, les cinq plus populaires r√©cemment. <br><br>  ‚Ä¢ Le mod√®le de langage est inclus dans le d√©codage lors de la premi√®re passe <br>  o [Hannun et al., 2014] [Maas et al., 2015]: d√©codage direct de premier passage avec un LM par opposition √† un nouveau scoring comme dans [Graves &amp; Jaitly, 2014] <br>  o [Miao et al., 2015]: cadre EESEN pour le d√©codage avec WFST, bo√Æte √† outils open source <br>  ‚Ä¢ Formation √† grande √©chelle sur le GPU;  Augmentation des donn√©es  plusieurs langues <br>  o [Hannun et al., 2014;  DeepSpeech] [Amodei et al., 2015;  DeepSpeech2]: formation GPU √† grande √©chelle;  Augmentation des donn√©es;  Mandarin et anglais <br>  ‚Ä¢ Utilisation d'unit√©s longues: des mots au lieu de caract√®res <br>  o [Soltau et al., 2017]: cibles CTC de niveau Word, form√©es sur 125 000 heures de discours.  Des performances proches ou sup√©rieures √† un syst√®me conventionnel, m√™me sans utiliser de LM! <br>  o [Audhkhasi et al., 2017]: Mod√®les Direct Acoustics-to-Word sur Switchboard <br><br>  Il convient de pr√™ter attention √† la mise en ≈ìuvre de DeepSpeach comme un bon exemple de solution CTC end2end et √† une variation qui utilise un niveau verbal.  Mais il y a une mise en garde: pour former un tel mod√®le, vous avez besoin de 125 000 heures de donn√©es √©tiquet√©es, ce qui est en fait beaucoup dans les dures r√©alit√©s. <br><br><h4>  Ce qui est important √† noter sur STS </h4><br><ul><li>  Probl√®mes ou omissions.  Pour l'efficacit√©, il est important de faire des hypoth√®ses sur l'ind√©pendance.  C'est-√†-dire que le STS suppose que la sortie du r√©seau dans diff√©rentes trames est conditionnellement ind√©pendante, ce qui est en fait incorrect.  Mais cette hypoth√®se est faite pour simplifier, sans elle, tout devient beaucoup plus compliqu√©. </li><li>  Pour obtenir de bonnes performances √† partir du mod√®le STS, l'utilisation d'un mod√®le de langage externe est requise, car le d√©codage gourmand direct ne fonctionne pas tr√®s bien. </li></ul><br><h4>  Attention </h4><br>  Quelle alternative avons-nous pour ce STS?  Ce n'est probablement un secret pour personne qu'il existe une chose telle que l'Attention ou ¬´Attention¬ª, qui a r√©volutionn√© dans une certaine mesure et qui est directement pass√© des t√¢ches de traduction automatique.  Et maintenant, la plupart des d√©cisions de mod√©lisation s√©quence-s√©quence sont bas√©es sur ce m√©canisme.  Comment est-il?  Essayons de le comprendre.  Pour la premi√®re fois sur l'attention dans les t√¢ches de reconnaissance vocale, des publications sont parues en 2015.  Quelqu'un Chen et Cherowski ont publi√© deux publications similaires et dissemblables en m√™me temps. <br><br>  Arr√™tons-nous sur le premier - il s'appelle √©couter, assister et √©peler.  Dans notre simulation classique, dans la s√©quence o√π nous avons un encodeur et un d√©codeur, un autre √©l√©ment est ajout√©, qui est appel√© attention.  L'echnoder remplira les fonctions que le mod√®le acoustique utilisait auparavant.  Sa t√¢che est de transformer le discours d'entr√©e en fonctionnalit√©s acoustiques de haut niveau.  Notre d√©codeur effectuera les t√¢ches que nous avons pr√©c√©demment effectu√©es le mod√®le de langue et le mod√®le de prononciation (lexique), il pr√©dira de mani√®re autor√©gressive chaque jeton de sortie, en fonction des pr√©c√©dents.  Et l'attention elle-m√™me dira directement quelle trame d'entr√©e est la plus pertinente / importante pour pr√©dire cette sortie. <br><br><img src="https://habrastorage.org/webt/gi/hd/1c/gihd1cppd9nsldy12rqffbuheoe.png"><br><br>  Quels sont ces blocs?  L'√©co-encodeur dans l'article est d√©crit comme un auditeur, c'est un RNN bidirectionnel classique bas√© sur des LSTM ou autre chose.  En g√©n√©ral, rien de nouveau - le syst√®me simule simplement la s√©quence d'entr√©e en fonctionnalit√©s complexes. <br><br>  L'attention, d'autre part, cr√©e un certain vecteur de contexte C √† partir de ces vecteurs, ce qui aidera √† d√©coder correctement le d√©codeur directement, le d√©codeur lui-m√™me, qui est, par exemple, √©galement certains LSTM qui seront d√©cod√©s dans la s√©quence d'entr√©e de cette couche d'attention, qui a d√©j√† mis en √©vidence les signes d'√©tat les plus importants, une s√©quence de sortie de caract√®res. <br><br>  Il existe √©galement diff√©rentes repr√©sentations de cette Attention elle-m√™me - ce qui fait la diff√©rence entre ces deux publications publi√©es par Chen et Charowski.  Ils utilisent une attention diff√©rente.  Chen utilise Attention produit par points, et Charowski utilise Attention Additive. <br><br><img src="https://habrastorage.org/webt/r4/oa/tu/r4oatu1zxse9xn13cmmbx01viem.png"><br><br><h4>  O√π aller ensuite? </h4><br>  Il s'agit d'un avantage ou d'un inconv√©nient pour toutes les principales r√©alisations obtenues √† ce jour en mati√®re de reconnaissance vocale non en ligne.  Quelles am√©liorations sont possibles ici?  O√π aller ensuite?  Le plus √©vident est l'utilisation d'un mod√®le sur des morceaux de mots au lieu d'utiliser directement des graph√®mes.  Il peut s'agir de morph√®mes distincts ou d'autre chose. <br><br>  Quelle est la motivation pour utiliser des tranches de mots?  Typiquement, les mod√®les linguistiques du niveau verbal ont beaucoup moins de perplexit√© que le niveau du graph√®me.  La mod√©lisation de morceaux de mots vous permet de construire un d√©codeur plus fort du mod√®le de langage.  Et la mod√©lisation d'√©l√©ments plus longs peut am√©liorer l'efficacit√© de la m√©moire dans un d√©codeur bas√© sur des LSTM.  Il vous permet √©galement de vous souvenir potentiellement de l'occurrence des mots de fr√©quence.  Des √©l√©ments plus longs permettent un d√©codage en moins d'√©tapes, ce qui acc√©l√®re directement l'inf√©rence de ce mod√®le. <br><br>  En outre, un mod√®le sur des morceaux de mots nous permet de r√©soudre le probl√®me des mots OOV (hors vocabulaire) qui se posent dans un mod√®le de langage, car nous pouvons mod√©liser n'importe quel mot avec des morceaux de mots.  Et il convient de noter que ces mod√®les sont form√©s pour maximiser la probabilit√© d'un mod√®le de langage sur un ensemble de donn√©es de formation.  Ces mod√®les d√©pendent de la position et nous pouvons utiliser l'algorithme gourmand pour le d√©codage. <br><br>  Quelles autres am√©liorations que le mod√®le de morceaux de mots peuvent √™tre?  Il existe un m√©canisme appel√© attention multi-t√™tes.  Il a √©t√© d√©crit pour la premi√®re fois en 2017 pour la traduction automatique.  L'attention multi-t√™tes implique un m√©canisme qui a plusieurs soi-disant t√™tes qui vous permettent de g√©n√©rer une distribution diff√©rente de cette m√™me attention, ce qui am√©liore directement les r√©sultats. <br><br><h4>  Mod√®les en ligne </h4><br>  Nous passons √† la partie la plus int√©ressante - ce sont des mod√®les en ligne.  Il est important de noter que LAS ne diffuse pas.  Autrement dit, ce mod√®le ne peut pas fonctionner en mode de d√©codage en ligne.  Nous consid√©rerons les deux mod√®les en ligne les plus populaires √† ce jour.  Transducteur RNN et transducteur neuronal. <br><br>  Le transducteur RNN a √©t√© propos√© par Graves en 2012-2017.  L'id√©e principale est de compliquer un peu notre mod√®le STS √† l'aide d'un mod√®le r√©cursif. <br><br><img src="https://habrastorage.org/webt/5c/y6/lc/5cy6lcuv2l7q1nymrr9j4idp5pi.png"><br><br>  Il convient de noter que les deux composants sont entra√Æn√©s ensemble sur les donn√©es acoustiques disponibles.  Comme STS, cette approche ne n√©cessite pas d'alignement de trame dans l'ensemble de donn√©es d'apprentissage.  Comme nous le voyons sur l'image: √† gauche est notre STS classique, et √† droite est le transducteur RNN.  Et nous avons deux nouveaux √©l√©ments: le <b>r√©seau pr√©dit</b> et le <b>r√©seau Join</b> . <br><br>  L'encodeur STS est exactement le m√™me - c'est le niveau d'entr√©e RNN, qui d√©termine la distribution sur tous les alignements avec toutes les s√©quences de sortie ne d√©passant pas la longueur de la s√©quence d'entr√©e - cela a √©t√© d√©crit par Graves en 2006.  Cependant, la t√¢che de telles conversions texte-parole est √©galement exclue, o√π la s√©quence d'entr√©e plus longue que la s√©quence d'entr√©e du STS ne mod√©lise pas la relation entre les sorties.  Le transducteur √©tend ce STS tr√®s, en d√©terminant la distribution des s√©quences de sortie de toutes longueurs et en mod√©lisant conjointement la d√©pendance de l'entr√©e-sortie et de la sortie-sortie. <br><br>  Il s'av√®re que notre mod√®le est finalement capable de g√©rer les d√©pendances de la sortie de l'entr√©e et de la sortie de la sortie de la derni√®re √©tape. <br><br>  Alors, qu'est-ce qu'un <b>r√©seau pr√©dit</b> ou un r√©seau pr√©dictif?  Elle essaie de mod√©liser chaque √©l√©ment en tenant compte des pr√©c√©dents, par cons√©quent, il est similaire au RNN standard avec la pr√©vision de l'√©tape suivante.  Seulement avec la possibilit√© suppl√©mentaire de faire des hypoth√®ses nulles. <br><br>  Comme nous le voyons dans l'image, nous avons un r√©seau pr√©dit, qui re√ßoit la valeur pr√©c√©dente de la sortie, et il y a un encodeur, qui re√ßoit la valeur actuelle de l'entr√©e.  Et √† la sortie, nous avons √† nouveau, telle a la valeur actuelle <img src="https://habrastorage.org/webt/x4/db/gw/x4dbgwm67dwzli8xp3ysvhqm-tu.png">  . <br><br>  <b>Transducteur neuronal</b> .  Il s'agit d'une complication de l'approche classique seq-2seq.  La s√©quence acoustique d'entr√©e est trait√©e par le codeur pour cr√©er des vecteurs d'√©tat cach√©s √† chaque pas de temps.  Tout semble √™tre comme d'habitude.  Mais il y a un √©l√©ment Transducteur suppl√©mentaire qui re√ßoit un bloc d'entr√©es √† chaque √©tape et g√©n√®re jusqu'√† M jetons de sortie en utilisant un mod√®le bas√© sur seq-2seq au-dessus de cette entr√©e.  Le transducteur conserve son √©tat dans des blocs en utilisant des connexions p√©riodiques avec les pas de temps pr√©c√©dents. <br><br><img src="https://habrastorage.org/webt/hh/pn/wx/hhpnwx3l2sco6phy37tmuawvcni.png"><br><br>  <i>La figure montre le transducteur, produisant des jetons pour le bloc pour la s√©quence utilis√©e dans le bloc du Ym correspondant.</i> <br><br>  Nous avons donc examin√© l'√©tat actuel de la reconnaissance vocale sur la base de l'approche End2End.  Il convient de dire que, malheureusement, ces approches n√©cessitent aujourd'hui une grande quantit√© de donn√©es.  Et les vrais r√©sultats obtenus par l'approche classique, n√©cessitant de 200 √† 500 heures d'enregistrements sonores balis√©s pour la formation d'un bon mod√®le bas√© sur End2End, n√©cessiteront plusieurs, voire des dizaines de fois plus de donn√©es.  Maintenant, c'est le plus gros probl√®me avec ces approches.  Mais peut-√™tre que bient√¥t tout changera. <br><br>  <i>D√©veloppeur principal du centre AI MTS Nikita Semenov.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr468663/">https://habr.com/ru/post/fr468663/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr468641/index.html">Tutoriel pour cr√©er une solution de cha√Æne de blocs sur Hyperledger Composer</a></li>
<li><a href="../fr468645/index.html">Deux pages ont suffi pour prouver l'hypoth√®se de 30 ans dans le domaine de l'informatique.</a></li>
<li><a href="../fr468647/index.html">Musique risqu√©e sur une ancienne imprimante de ligne mainframe IBM</a></li>
<li><a href="../fr468653/index.html">Quelle est la r√©solution de l'≈ìil humain (ou combien de m√©gapixels nous voyons √† un moment donn√©)</a></li>
<li><a href="../fr468657/index.html">Danses avec support: types et formes de support. Syst√®mes de soutien fonctionnant au combat</a></li>
<li><a href="../fr468665/index.html">Mais est-il temps d'acheter un irrigateur?</a></li>
<li><a href="../fr468673/index.html">Atelier "Garantir la s√©curit√© des donn√©es personnelles" - 3 octobre, Saint-P√©tersbourg</a></li>
<li><a href="../fr468677/index.html">L'annonce du smartphone Xiaomi Mi Mix Alpha</a></li>
<li><a href="../fr468679/index.html">L'ABC de la s√©curit√© dans Kubernetes: authentification, autorisation, audit</a></li>
<li><a href="../fr468683/index.html">Th√©orie et pratique de la normalisation des services Docker</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>