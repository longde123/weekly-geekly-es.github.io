<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüî¨ üë®üèº‚Äçüî¨ ü§° VShard - escala horizontal en Tarantool ü•Ö üë©üèº‚Äçüíª ü§æüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mi nombre es Vladislav, participo en el desarrollo de Tarantool - DBMS y servidor de aplicaciones en una botella. Y hoy les contar√© c√≥mo implementamos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - escala horizontal en Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/436916/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Mi nombre es Vladislav, participo en el desarrollo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tarantool</a> - DBMS y servidor de aplicaciones en una botella.  Y hoy les contar√© c√≥mo implementamos el escalado horizontal en Tarantool usando el m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VShard</a> . <br><a name="habracut"></a><br>  Primero, una peque√±a teor√≠a. <br><br>  Hay dos tipos de escalado: horizontal y vertical.  Horizontal se divide en dos tipos: replicaci√≥n y fragmentaci√≥n.  La replicaci√≥n se usa para escalar la computaci√≥n, el fragmentaci√≥n se usa para escalar datos. <br><br>  Sharding se divide en dos tipos: sharding por rangos y sharding por hashes. <br><br>  Al fragmentar con rangos, calculamos alguna clave de fragmento de cada registro en el cl√∫ster.  Estas teclas de fragmentos se proyectan en una l√≠nea recta, que se divide en rangos que agregamos a diferentes nodos f√≠sicos. <br><br>  El fragmentado con hash es m√°s simple: de cada registro en el cl√∫ster consideramos una funci√≥n hash, agregamos las entradas con el mismo valor de la funci√≥n hash a un nodo f√≠sico. <br><br>  Hablar√© sobre el escalado horizontal usando el hash sharding. <br><br><h1>  Implementaci√≥n previa </h1><br>  El primer m√≥dulo de escala horizontal que tuvimos fue el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fragmento de Tarantool</a> .  Este es un particionamiento muy simple por hashes, que considera la clave de fragmento de la clave primaria de todas las entradas en el cl√∫ster. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Pero luego surgi√≥ una tarea que Tarantool Shard no pudo manejar por tres razones fundamentales. <br><br>  Primero, <b>se</b> requer√≠a la <b>localidad de los datos relacionados l√≥gicamente</b> .  Cuando tenemos datos que est√°n conectados l√≥gicamente, siempre queremos almacenarlos en el mismo nodo f√≠sico, sin importar c√≥mo cambie la topolog√≠a del cl√∫ster o se realice el equilibrio.  Y el fragmento de Tarantool no garantiza esto.  Considera el hash solo por las claves primarias y, al reequilibrar, incluso los registros con el mismo hash se pueden dividir durante alg√∫n tiempo; la transferencia no es at√≥mica. <br><br>  El problema de la falta de localidad de los datos nos lo impidi√≥ m√°s.  Dar√© un ejemplo.  Hay un banco en el que el cliente abri√≥ una cuenta.  Los datos de la cuenta y del cliente siempre deben almacenarse f√≠sicamente juntos para que puedan leerse en una solicitud, intercambiarse en una transacci√≥n, por ejemplo, al transferir dinero desde una cuenta.  Si usa un fragmento cl√°sico con el fragmento de Tarantool, los valores de las funciones de fragmento ser√°n diferentes para las cuentas y los clientes.  Los datos pueden estar en diferentes nodos f√≠sicos.  Esto complica enormemente tanto la lectura como el trabajo transaccional con dicho cliente. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre><br>  En el ejemplo anterior, los campos de <code>id</code> no pueden coincidir f√°cilmente con cuentas y clientes.  Est√°n conectados a trav√©s del campo de cuenta <code>customer_id</code> y <code>customer_id</code> <code>id</code> .  El mismo campo de <code>id</code> romper√≠a la unicidad de la clave principal de la cuenta.  Y de otra manera, Shard no puede fragmentarse. <br><br>  El siguiente problema es el <b>lento reajuste</b> .  Este es el problema cl√°sico de todos los fragmentos en hashes.  La conclusi√≥n es que cuando cambiamos la composici√≥n de un cl√∫ster, generalmente cambiamos la funci√≥n de fragmento, porque generalmente depende de la cantidad de nodos.  Y cuando la funci√≥n cambia, debe revisar todas las entradas del cl√∫ster y volver a calcular la funci√≥n de fragmento.  Quiz√°s transfiera algunas notas.  Y mientras los estamos transfiriendo, no sabemos si los datos que necesita la pr√≥xima solicitud entrante ya se han transferido, tal vez ahora est√©n en proceso de transferencia.  Por lo tanto, durante la reorganizaci√≥n, es necesario que cada lectura solicite dos funciones de fragmento: la antigua y la nueva.  Las solicitudes se est√°n volviendo dos veces m√°s lentas, y para nosotros fue inaceptable. <br><br>  Otra caracter√≠stica de Tarantool Shard fue que cuando algunos nodos en los conjuntos de r√©plicas fallan, muestra <b>poca accesibilidad de lectura</b> . <br><br><h1>  Nueva soluci√≥n </h1><br>  Para resolver los tres problemas descritos, creamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tarantool VShard</a> .  Su diferencia clave es que el nivel de almacenamiento de datos est√° virtualizado: los almacenamientos virtuales aparecieron sobre los f√≠sicos, y los registros se distribuyen entre ellos.  Estos almacenamientos se llaman bucket'ami.  El usuario no necesita pensar en qu√© y en qu√© nodo f√≠sico se encuentra.  Bucket es una unidad de datos indivisible at√≥mica, como en la fragmentaci√≥n cl√°sica de una tupla.  VShard siempre almacena el dep√≥sito completo en un nodo f√≠sico y, durante la reorganizaci√≥n, transfiere todos los datos de un dep√≥sito at√≥micamente.  Debido a esto, se proporciona la localidad.  Solo necesitamos poner los datos en un cubo, y siempre podemos estar seguros de que estos datos estar√°n junto con cualquier cambio en el cl√∫ster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  ¬øC√≥mo puedo poner datos en un cubo?  En el esquema que presentamos anteriormente para el cliente del banco, agregaremos el <code>bucket id</code> del <code>bucket id</code> a las tablas de acuerdo con el nuevo campo.  Si los datos vinculados son los mismos, los registros estar√°n en el mismo dep√≥sito.  La ventaja es que podemos almacenar estos registros con la misma <code>bucket id</code> en diferentes espacios, e incluso en diferentes motores.  La <code>bucket id</code> proporciona independientemente de c√≥mo se almacenen estos registros. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre><br>  ¬øPor qu√© estamos tan ansiosos por esto?  Si tenemos fragmentos cl√°sicos, entonces los datos pueden colarse en todos los almacenamientos f√≠sicos que solo tenemos.  En el ejemplo con el banco, al solicitar todas las cuentas de un cliente, deber√° recurrir a todos los nodos.  Resulta la dificultad de leer O (N), donde N es el n√∫mero de tiendas f√≠sicas.  Muy lento <br><br>  Gracias a bucket'am y la localidad por <code>bucket id</code> siempre podemos leer datos de un nodo en una solicitud, independientemente del tama√±o del cl√∫ster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2bb/524/8b7/2bb5248b7757ea6249f47a3dca46a681.png"><br><br>  Debe calcular la <code>bucket id</code> del <code>bucket id</code> y asignar los mismos valores usted mismo.  Para algunos, esto es una ventaja, para alguien una desventaja.  Considero que es una ventaja que pueda elegir la funci√≥n para calcular la <code>bucket id</code> del <code>bucket id</code> usted mismo. <br><br>  ¬øCu√°l es la diferencia clave entre los fragmentos cl√°sicos y los fragmentos virtuales con el cubo? <br><br>  En el primer caso, cuando cambiamos la composici√≥n del cl√∫ster, tenemos dos estados: el actual (antiguo) y el nuevo, en el que debemos ir.  En el proceso de transici√≥n, no solo necesita transferir los datos, sino tambi√©n recalcular las funciones hash para todos los registros.  Esto es muy inconveniente, porque en cualquier momento no sabemos qu√© datos ya se han transferido y cu√°les no.  Adem√°s, esto no es confiable ni at√≥mico, ya que para la transferencia at√≥mica de un conjunto de registros con el mismo valor de la funci√≥n hash, es necesario almacenar persistentemente el estado de transferencia en caso de que sea necesaria la recuperaci√≥n.  Hay conflictos, errores, debe reiniciar el procedimiento muchas veces. <br><br>  El fragmentaci√≥n virtual es mucho m√°s simple.  No tenemos dos estados seleccionados del cl√∫ster, solo tenemos el estado del dep√≥sito.  El grupo se vuelve m√°s maniobrable, gradualmente se mueve de un estado a otro.  Y ahora hay m√°s de dos estados.  Gracias a una transici√≥n sin problemas, puede cambiar el saldo sobre la marcha, eliminar el almacenamiento reci√©n agregado.  Es decir, la capacidad de control del equilibrio aumenta considerablemente, se vuelve granular. <br><br><h1>  Uso </h1><br>  Digamos que elegimos una funci√≥n para la <code>bucket id</code> y vertimos tantos datos en el cl√∫ster que no hab√≠a m√°s espacio.  Ahora queremos agregar nodos y que los datos se trasladen a ellos mismos.  En VShard, esto se hace de la siguiente manera.  Primero, inicie nuevos nodos y Tarantools en ellos, y luego actualice la configuraci√≥n de VShard.  Describe todos los miembros del cl√∫ster, todas las r√©plicas, conjuntos de r√©plicas, maestros, URI asignados y mucho m√°s.  <code>VShard.storage.cfg</code> nuevos nodos a la configuraci√≥n, y usando la funci√≥n <code>VShard.storage.cfg</code> , la usamos en todos los nodos del cl√∫ster. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Como recordar√°, en los fragmentos cl√°sicos con un cambio en el n√∫mero de nodos, la funci√≥n de fragmentos tambi√©n cambia.  En VShard esto no sucede, tenemos un n√∫mero fijo de almacenamientos virtuales: bucket'ov.  Esta es la constante que selecciona al iniciar el cl√∫ster.  Puede parecer que debido a esto, la escalabilidad es limitada, pero en realidad no.  Puede elegir una gran cantidad de bucket'ov, decenas y cientos de miles.  Lo principal es que debe haber al menos dos √≥rdenes de magnitud m√°s que el n√∫mero m√°ximo de conjuntos de r√©plicas que tendr√° en el cl√∫ster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  Dado que el n√∫mero de almacenamientos virtuales no cambia, y la funci√≥n de fragmento depende solo de este valor, podemos agregar tantos almacenamientos f√≠sicos como sea necesario sin volver a contar la funci√≥n de fragmento. <br><br>  ¬øC√≥mo se distribuyen los paquetes entre las tiendas f√≠sicas por su cuenta?  Cuando se llama a VShard.storage.cfg en uno de los nodos, se activa el proceso de reequilibrio.  Este es un proceso anal√≠tico que calcula el equilibrio perfecto en un cl√∫ster.  Acude a todos los nodos f√≠sicos, pregunta qui√©n tiene cu√°ntos bucket'ov y construye rutas para su movimiento con el fin de promediar la distribuci√≥n.  El reequilibrador env√≠a rutas a almacenes llenos de gente, y comienzan a enviar baldes.  Despu√©s de un tiempo, el grupo se equilibra. <br><br>  Pero en proyectos reales, el concepto de equilibrio perfecto puede ser diferente.  Por ejemplo, quiero almacenar menos datos en un conjunto de r√©plicas que en el otro, porque hay menos espacio en el disco duro.  VShard piensa que todo est√° bien equilibrado y, de hecho, mi almacenamiento est√° a punto de desbordarse.  Hemos proporcionado un mecanismo para ajustar las reglas de equilibrio utilizando pesos.  Cada conjunto de r√©plicas y repositorio se pueden ponderar.  Cuando el equilibrador decide a qui√©n enviar cu√°ntos bucket'ov, tiene en cuenta la <b>relaci√≥n de</b> todos los pares de pesas. <br><br>  Por ejemplo, una tienda tiene un peso de 100 y la otra tiene 200. Luego, la primera almacenar√° dos veces menos bucket'ov que la segunda.  Tenga en cuenta que estoy hablando espec√≠ficamente sobre la <b>relaci√≥n de</b> pesos.  Los significados absolutos no tienen efecto.  Puede elegir pesos basados ‚Äã‚Äãen una distribuci√≥n de cl√∫ster del 100%: una tienda tiene el 30%, otra tiene el 70%.  Puede tomar la capacidad de almacenamiento en gigabytes como base, o puede medir pesos en la cantidad de bucket'ov.  Lo principal es observar la actitud que necesita. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b4e/889/298/b4e889298991781b8c3ee01f4c066a6e.png"><br><br>  Tal sistema tiene un efecto secundario interesante: si asigna un peso cero a alguna tienda, entonces el equilibrador ordenar√° a la tienda que distribuya todos sus cubos.  Despu√©s de eso, puede eliminar todo el conjunto de r√©plicas de la configuraci√≥n. <br><br><h1>  Transferencia de cubo at√≥mico </h1><br>  Tenemos un dep√≥sito, acepta alg√∫n tipo de solicitud de lectura y escritura, y luego el equilibrador solicita transferirlo a otro almacenamiento.  Bucket deja de aceptar solicitudes de grabaci√≥n, de lo contrario podr√°n actualizarlo durante la transferencia, luego tendr√°n tiempo para actualizar la actualizaci√≥n port√°til, luego la actualizaci√≥n de la actualizaci√≥n port√°til, y as√≠ hasta el infinito.  Por lo tanto, el registro est√° bloqueado y a√∫n puede leer desde el bucket.  Comienza la transferencia de trozos a un nuevo lugar.  Una vez completada la transferencia, el dep√≥sito volver√° a comenzar a aceptar solicitudes.  En el lugar anterior, tambi√©n sigue mintiendo, pero ya se ha marcado como basura y, posteriormente, el recolector de basura lo eliminar√° trozo por trozo. <br><br>  Cada dep√≥sito est√° asociado con metadatos que se almacenan f√≠sicamente en el disco.  Todos los pasos anteriores se guardan en el disco, y pase lo que pase con el repositorio, el estado del dep√≥sito se restaurar√° autom√°ticamente. <br><br>  Puedes tener preguntas: <br><br><ul><li>  <b>¬øQu√© pasar√° con esas solicitudes que funcionaron con el dep√≥sito cuando comenzaron a portarlo?</b> <br><br>  Hay dos tipos de enlaces en los metadatos de cada dep√≥sito: lectura y escritura.  Cuando el usuario realiza una solicitud al dep√≥sito, indica c√≥mo trabajar√° con √©l, solo lectura o lectura escritura.  Para cada solicitud, se incrementa el contador de referencia correspondiente. <br><br>  ¬øPor qu√© necesito un contador de referencia para leer las solicitudes?  Digamos que el dep√≥sito se transfiere silenciosamente, y aqu√≠ viene el recolector de basura y quiere eliminar este dep√≥sito.  √âl ve que el recuento de enlaces es mayor que cero, por lo que no puede eliminarlo.  Y cuando se procesen las solicitudes, el recolector de basura podr√° completar su trabajo. <br><br>  El contador de referencia para las solicitudes de escritura asegura que el dep√≥sito ni siquiera comienza a transferirse mientras al menos una solicitud de escritura est√° trabajando con √©l.  Pero las solicitudes de escritura pueden venir constantemente, y luego el dep√≥sito nunca se transferir√°.  El hecho es que si el equilibrador ha expresado su deseo de transferirlo, las nuevas solicitudes de grabaci√≥n comenzar√°n a bloquearse y el sistema actual esperar√° a que se complete el tiempo de espera.  Si las solicitudes no se completan en el tiempo asignado, el sistema nuevamente comenzar√° a aceptar nuevas solicitudes de escritura, posponiendo la transferencia del dep√≥sito por un tiempo.  Por lo tanto, el equilibrador realizar√° los intentos de transferencia hasta que uno tenga √©xito. <br><br>  VShard tiene una API bucket_ref de bajo nivel en caso de que tenga pocas caracter√≠sticas de alto nivel.  Si realmente quiere hacer algo usted mismo, solo acceda a esta API desde el c√≥digo. </li><li>  <b>¬øEs posible no bloquear registros en absoluto?</b> <br><br>  Es imposible  Si el dep√≥sito contiene datos cr√≠ticos que necesitan acceso de escritura constante, tendr√° que bloquear su transferencia por completo.  Para esto hay una funci√≥n <code>bucket_pin</code> , que une firmemente el dep√≥sito al conjunto de r√©plica actual, evitando su transferencia.  En este caso, el bucket'y vecino podr√° moverse sin restricciones. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Hay una herramienta a√∫n m√°s poderosa que <code>bucket_pin</code> : bloqueo de conjunto de r√©plicas.  Ya no se hace en c√≥digo, sino a trav√©s de la configuraci√≥n.  El bloqueo proh√≠be el movimiento de cualquier bucket'ov de esta r√©plica set'a y la recepci√≥n de nuevos.  En consecuencia, todos los datos estar√°n constantemente disponibles para la grabaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h1>  VShard.router </h1><br>  VShard consta de dos subm√≥dulos: VShard.storage y VShard.router.  Se pueden crear y escalar independientemente incluso en una instancia.  Al acceder al cl√∫ster, no sabemos d√≥nde se encuentra el dep√≥sito, y VShard.router lo buscar√° por <code>bucket id</code> para nosotros. <br><br>  Veamos un ejemplo de c√≥mo se ve esto.  Regresamos al cl√∫ster bancario y a las cuentas de los clientes.  Quiero poder extraer todas las cuentas de un cliente particular del cl√∫ster.  Para hacer esto, escribo la funci√≥n habitual para la b√∫squeda local: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f78/e2b/df2/f78e2bdf2d977fcb9fb320b031592171.png"><br><br>  Ella busca todas las cuentas de clientes por su identificaci√≥n.  Ahora necesito decidir cu√°l de los repositorios llamar√° a esta funci√≥n.  Para hacer esto, calculo la <code>bucket id</code> del <code>bucket id</code> partir de la identificaci√≥n del cliente en mi solicitud y le pido a VShard.router que me llame a tal funci√≥n en el almacenamiento donde vive el dep√≥sito con la <code>bucket id</code> resultante.  Hay una tabla de enrutamiento en el subm√≥dulo, en la que se especifica la ubicaci√≥n del dep√≥sito en el conjunto de r√©plicas.  Y VShard.router representa mi solicitud. <br><br>  Por supuesto, puede suceder que en este momento la reorganizaci√≥n comenz√≥ y el cubo comenz√≥ a moverse.  El enrutador en segundo plano actualiza gradualmente la tabla en fragmentos grandes: consulta los repositorios para sus tablas de dep√≥sito actuales. <br><br>  Incluso puede suceder que recurramos al dep√≥sito que se acaba de mover, y el enrutador a√∫n no ha logrado actualizar su tabla de enrutamiento.  Luego, recurrir√° al antiguo repositorio y le indicar√° al enrutador d√≥nde buscar el dep√≥sito o simplemente responder√° que no tiene los datos necesarios.  Luego, el enrutador recorrer√° todos los almacenes en busca del cubo deseado.  Y todo esto es transparente para nosotros, ni siquiera notaremos una falla en la tabla de enrutamiento. <br><br><h1>  Leer inestabilidad </h1><br>  Recordemos qu√© problemas tuvimos inicialmente: <br><br><ul><li>  No hubo localidad de datos.  Decidimos agregando bucket'ov. </li><li>  Volver a cargar disminuy√≥ la velocidad y ralentiz√≥ todo.  Implementado la transferencia de datos at√≥micos bucket'ami, se deshizo del recuento de funciones de fragmentos. </li><li>  Lectura inestable. </li></ul><br>  VShard.router resuelve el √∫ltimo problema utilizando el subsistema de conmutaci√≥n por error de lectura autom√°tica. <br><br>  El enrutador hace ping peri√≥dicamente al almacenamiento especificado en la configuraci√≥n.  Y luego algunos de ellos dejaron de hacer ping.  El enrutador tiene una conexi√≥n de respaldo en caliente para cada r√©plica, y si la actual deja de responder, ir√° a otra.  La solicitud de lectura se procesar√° normalmente, porque podemos leer en r√©plicas (pero no escribir).  Podemos establecer la prioridad de las r√©plicas mediante las cuales el enrutador debe seleccionar la conmutaci√≥n por error para las lecturas.  Hacemos esto con la zonificaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c3/5bf/dbd/5c35bfdbddd67fe8217f06730673bd43.png"><br><br>  Asignamos un n√∫mero de zona a cada r√©plica y cada enrutador y establecemos una tabla en la que indicamos la distancia entre cada par de zonas.  Cuando el enrutador decide d√≥nde enviar una solicitud de lectura, seleccionar√° una r√©plica en la zona m√°s cercana a la suya. <br><br>  C√≥mo se ve en la configuraci√≥n: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/445/799/5ae/4457995ae5c7bc1761684cf7d4f3b2e4.png"><br><br>  En el caso general, puede hacer referencia a una r√©plica arbitraria, pero si el cl√∫ster es grande y complejo, muy distribuido, la zonificaci√≥n es muy √∫til.  Los diferentes racks de servidores pueden ser zonas, para no cargar la red con tr√°fico.  O pueden ser puntos geogr√°ficamente distantes entre s√≠. <br><br>  La zonificaci√≥n tambi√©n ayuda a variar el rendimiento de la r√©plica.  Por ejemplo, en cada conjunto de r√©plicas tenemos una r√©plica de respaldo, que no debe aceptar solicitudes, sino solo almacenar una copia de los datos.  Luego lo hacemos en la zona, que estar√° muy lejos de todos los enrutadores de la tabla, y recurrir√°n a √©l en el caso m√°s extremo. <br><br><h1>  Grabaci√≥n de inestabilidad </h1><br>  Dado que estamos hablando de la conmutaci√≥n por error de lectura, ¬øqu√© pasa con la conmutaci√≥n por error de escritura al cambiar el asistente?  Aqu√≠, VShard no es tan optimista: la elecci√≥n de un nuevo maestro no se implementa en √©l, tendr√° que hacerlo usted mismo.  Cuando de alguna manera lo seleccionamos, es necesario que esta instancia ahora se haga cargo de la autoridad del maestro.  Actualizamos la configuraci√≥n especificando <code>master = false</code> para el antiguo maestro y <code>master = true</code> para el nuevo, apl√≠quelo a trav√©s de VShard.storage.cfg y enr√≥llelo en el almacenamiento.  Entonces todo sucede autom√°ticamente.  El viejo maestro deja de aceptar solicitudes de escritura y comienza a sincronizarse con el nuevo, porque puede haber datos que ya se han aplicado en el viejo maestro, pero el nuevo a√∫n no ha llegado.  Despu√©s de eso, el nuevo maestro ingresa al rol y comienza a aceptar solicitudes, y el viejo maestro se convierte en una r√©plica.  As√≠ es como funciona la conmutaci√≥n por error de escritura en VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><h1>  ¬øC√≥mo ahora seguir toda esta variedad de eventos? </h1><br>  En el caso general, dos manejadores son suficientes: <code>VShard.storage.info</code> y <code>VShard.router.info</code> . <br><br>  VShard.storage.info muestra informaci√≥n en varias secciones. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  El primero es la secci√≥n de replicaci√≥n.  Se muestra el estado del conjunto de r√©plicas al que aplic√≥ esta funci√≥n: qu√© retraso de replicaci√≥n tiene, con qui√©n tiene conexiones y con qui√©n no est√° disponible, qui√©n est√° disponible y qu√© no est√° disponible, qu√© asistente est√° configurado para qu√©, etc. <br><br>  En la secci√≥n Bucket, puede ver en tiempo real cu√°ntos bucket'ov se est√°n moviendo actualmente al conjunto de r√©plica actual, cu√°ntos lo est√°n dejando, cu√°ntos est√°n trabajando actualmente en √©l, cu√°ntos est√°n marcados como basura, cu√°ntos est√°n conectados. <br><br>  La secci√≥n Alerta es una mezcla de todos los problemas que VShard pudo determinar de forma independiente: el maestro no est√° configurado, el nivel de redundancia es insuficiente, el maestro est√° all√≠ y todas las r√©plicas han fallado, etc. <br><br>  Y la √∫ltima secci√≥n es una luz que se ilumina en rojo cuando las cosas se ponen realmente mal.  Es un n√∫mero de cero a tres, cuanto m√°s peor. <br><br>  VShard.router.info tiene las mismas secciones, pero significan un poco diferente. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  La primera secci√≥n es la replicaci√≥n.      ,    :    ,  replica set'  ,          ,   ,   replica set'  bucket'     ,     . <br><br>   Bucket    bucket',              ;    bucket'   ;  ,       replica set'. <br><br>   Alert,  ,   ,   failover,   bucket'. <br><br> ,         . <br><br><h1>     VShard? </h1><br>  ‚Äî    bucket'.       <code>int32_max</code> ?     bucket'   ‚Äî  30      16   .     bucket',     .           bucket',          bucket'.    ,          . <br><br>  ‚Äî   -   <code>bucket id</code> .    ,    -   ,   bucket ‚Äî           .      ,   bucket'   ,  VShard    bucket'.       -,      bucket'  bucket,  -.    . <br><br><h1>  Resumen </h1><br> Vshard : <br><br><ul><li>  ; </li><li>  ; </li><li>    ; </li><li>  read failover; </li><li>    bucket'. </li></ul><br> VShard   .  -    .  ‚Äî  <b>   </b> .     ,       .           . <br><br>  ‚Äî <b>lock-free  bucket'</b> .   ,       bucket'      .      ,     . <br><br>  ‚Äî <b>  </b> .          : -    ,   ,    ?        . <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es436916/">https://habr.com/ru/post/es436916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es436904/index.html">Umbral de 32K para datos en ROM de microcontroladores AVR</a></li>
<li><a href="../es436908/index.html">6 formas de ocultar datos en una aplicaci√≥n de Android</a></li>
<li><a href="../es436910/index.html">Consejos para crear flujos de trabajo personalizados en GitLab CI</a></li>
<li><a href="../es436912/index.html">Tendencias de CRM 2019: divertido de leer, peligroso de creer</a></li>
<li><a href="../es436914/index.html">Problemas de crecimiento de inicio - Monitoreo</a></li>
<li><a href="../es436918/index.html">Creando un juego para Game Boy, Parte 2</a></li>
<li><a href="../es436920/index.html">Transpilador PAS2JS de Pascal a JavaScript: incompatible con Delphi y soluciones alternativas</a></li>
<li><a href="../es436922/index.html">Optimizaci√≥n del tiempo de inicio de Prometheus 2.6.0 con pprof</a></li>
<li><a href="../es436924/index.html">Algunas palabras sobre la organizaci√≥n de competiciones de rob√≥tica</a></li>
<li><a href="../es436926/index.html">H√©roes de la autenticaci√≥n de dos factores, o c√≥mo "caminar en la piel de los dem√°s"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>