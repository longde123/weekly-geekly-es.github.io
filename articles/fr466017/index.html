<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤱🏽 🛀🏼 🧗🏿 Livy - le maillon manquant de la chaîne Hadoop Spark Airflow Python 🍰 🙆 ❕</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut tout le monde, quelques informations "sous le capot" sont la date de l'atelier d'ingénierie d'Alfastrakhovaniya - qui excite nos esprits techniq...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livy - le maillon manquant de la chaîne Hadoop Spark Airflow Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/alfastrah/blog/466017/"><p>  Salut tout le monde, quelques informations "sous le capot" sont la date de l'atelier d'ingénierie d'Alfastrakhovaniya - qui excite nos esprits techniques. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/956/f04/a6a/956f04a6ae545bee58a8c34f2938a850.png" alt="image"></p><br><p>  Apache Spark est un merveilleux outil qui vous permet de traiter rapidement et facilement de grandes quantités de données sur des ressources informatiques assez modestes (je veux dire le traitement en cluster). </p><br><p>  Traditionnellement, le cahier jupyter est utilisé dans le traitement de données ad hoc.  En combinaison avec Spark, cela nous permet de manipuler des trames de données à longue durée de vie (Spark traite de l'allocation des ressources, les trames de données vivent quelque part dans le cluster, leur durée de vie est limitée par la durée de vie du contexte Spark). </p><br><p>  Après le transfert du traitement des données vers Apache Airflow, la durée de vie des trames est considérablement réduite - le contexte Spark «vit» dans la même instruction Airflow.  Comment contourner cela, pourquoi se déplacer et qu'est-ce que Livy a à voir avec cela - lire sous la coupe. </p><a name="habracut"></a><br><p>  Regardons un exemple très, très simple: supposons que nous devons dénormaliser les données dans une grande table et enregistrer le résultat dans une autre table pour un traitement ultérieur (un élément typique du pipeline de traitement des données). </p><br><p>  Comment ferions-nous cela: </p><br><ul><li>  données chargées dans la trame de données (sélection à partir d'une grande table et de répertoires) </li><li>  regardé avec des "yeux" le résultat (cela a-t-il fonctionné correctement) </li><li>  trame de données enregistrée dans la table Hive (par exemple) </li></ul><br><p>  Sur la base des résultats de l'analyse, il se peut que nous devions insérer dans la deuxième étape un traitement spécifique (remplacement de dictionnaire ou autre).  En termes de logique, nous avons trois étapes </p><br><ul><li>  étape 1: télécharger </li><li>  étape 2: traitement </li><li>  étape 3: enregistrer </li></ul><br><p>  Voici comment cela fonctionne dans le cahier jupyter - nous pouvons traiter les données téléchargées pendant une durée arbitraire, ce qui donne le contrôle des ressources Spark. </p><br><p> Il est logique de s'attendre à ce qu'une telle partition puisse être transférée vers Airflow.  Autrement dit, pour avoir un graphique de ce genre </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/312/30b/d7e/31230bd7e4c3beb62f92aebb709e2010.png" alt="image"></p><br><p>  Malheureusement, cela n'est pas possible lorsque vous utilisez la combinaison Airflow + Spark: chaque instruction Airflow est exécutée dans son propre interpréteur python, par conséquent, entre autres, chaque instruction doit en quelque sorte "persister" les résultats de ses activités.  Ainsi, notre traitement est «compressé» en une seule étape - «dénormaliser les données». </p><br><p>  Comment ramener la flexibilité du notebook jupyter à Airflow?  Il est clair que l'exemple ci-dessus «n'en vaut pas la peine» (peut-être, au contraire, il s'avère une bonne étape de traitement claire).  Mais encore - comment faire exécuter des instructions Airflow dans le même contexte Spark sur un espace de trame de données commun? </p><br><h2 id="privetstvuem-livy">  Bienvenue Livy </h2><br><p>  Un autre produit de l'écosystème Hadoop vient à la rescousse - Apache Livy. </p><br><p>  Je n'essaierai pas de décrire ici de quel genre de «bête» il s'agit.  S'il est très bref et noir et blanc - Livy vous permet "d'injecter" du code python dans un programme que le pilote exécute: </p><br><ul><li>  nous créons d'abord une session Livy </li><li>  après cela, nous avons la possibilité d'exécuter du code python arbitraire dans cette session (très similaire à l'idéologie jupyter / ipython) </li></ul><br><p>  Et pour tout cela, il y a une API REST. </p><br><p>  Revenons à notre tâche simple: avec Tite-Live, nous pouvons sauvegarder la logique d'origine de notre dénormalisation </p><br><ul><li>  dans la première étape (la première instruction de notre graphique), nous chargerons et exécuterons le code de chargement des données dans la trame de données </li><li>  dans la deuxième étape (deuxième instruction) - exécuter le code pour le traitement supplémentaire nécessaire de cette trame de données </li><li>  dans la troisième étape - le code pour enregistrer la trame de données dans la table </li></ul><br><p>  À quoi pourrait ressembler Airflow: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b9b/255/bcd/b9b255bcd00525201a000ef1a3fbafa3.png" alt="image"></p><br><p>  (puisque l'image est une capture d'écran très réelle, des «réalités» supplémentaires ont été ajoutées - la création du contexte Spark est devenue une opération distincte avec un nom étrange, le «traitement» des données a disparu car il n'était pas nécessaire, etc.) </p><br><p>  Pour résumer, nous obtenons </p><br><ul><li>  déclaration de flux d'air universel qui exécute du code python dans une session Livy </li><li>  la possibilité d '"organiser" le code python en graphiques assez complexes (Airflow pour cela) </li><li>  la capacité de s'attaquer aux optimisations de niveau supérieur, par exemple, dans quel ordre devons-nous effectuer nos transformations afin que Spark puisse conserver les données générales dans la mémoire du cluster aussi longtemps que possible </li></ul><br><p>  Un pipeline typique pour préparer des données pour la modélisation contient environ 25 requêtes sur 10 tables, il est évident que certaines tables sont utilisées plus souvent que d'autres (les mêmes "données générales") et il y a quelque chose à optimiser. </p><br><h2 id="chto-dalshe">  Et ensuite </h2><br><p>  La capacité technique a été testée, nous pensons plus loin - comment traduire plus technologiquement nos transformations dans ce paradigme.  Et comment aborder l'optimisation mentionnée ci-dessus.  Nous sommes encore au début de cette partie de notre voyage - quand il y a quelque chose d'intéressant, nous le partagerons certainement. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr466017/">https://habr.com/ru/post/fr466017/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr465991/index.html">Travailleurs de l'architecture propre et rapide</a></li>
<li><a href="../fr465993/index.html">Pas besoin d'économiser sur la sécurité numérique</a></li>
<li><a href="../fr465995/index.html">LDC - Excursion</a></li>
<li><a href="../fr466001/index.html">"Mobile" Feng Shui, ou on dort correctement (café, cafards et intolérances sur Habré)</a></li>
<li><a href="../fr466015/index.html">Un peu plus sur la trigonométrie en informatique</a></li>
<li><a href="../fr466019/index.html">ABBYY Mobile Web Capture: des photos de documents de haute qualité directement dans le navigateur de votre smartphone</a></li>
<li><a href="../fr466021/index.html">Comment j'ai appris à Yandex Alice à parler de jouets sexuels</a></li>
<li><a href="../fr466027/index.html">Le livre "The Way of Python. Ceinture noire pour le développement, la mise à l'échelle, les tests et le déploiement »</a></li>
<li><a href="../fr466029/index.html">Comment transformer un ordinateur quantique en un générateur de nombres aléatoires parfait</a></li>
<li><a href="../fr466031/index.html">La mission épique de DeepMind pour résoudre le problème scientifique le plus complexe</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>