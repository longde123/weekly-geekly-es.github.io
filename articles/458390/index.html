<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🚀 ⛪️ 🤸🏾 Ceph - de "en la rodilla" a "producción" parte 2 🐷 🕢 ☑️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(primera parte aquí: https://habr.com/en/post/456446/ ) 
 Ceph 
 Introduccion 


 Dado que la red es uno de los elementos clave de Ceph, y es un poco ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - de "en la rodilla" a "producción" parte 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (primera parte aquí: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  Introduccion </h3><br><p>  Dado que la red es uno de los elementos clave de Ceph, y es un poco específica en nuestra empresa, primero le contaremos un poco al respecto. <br>  Habrá muchas menos descripciones de Ceph, principalmente una infraestructura de red.  Solo se describirán los servidores Ceph y algunas características de los servidores de virtualización Proxmox. </p><a name="habracut"></a><br><p>  Entonces: la topología de la red en sí está construida como <strong>Leaf-Spine.</strong>  La arquitectura clásica de tres niveles es una red donde hay <strong>Core</strong> (enrutadores centrales), <strong>Agregación</strong> (enrutadores de agregación) y directamente conectados con clientes de <strong>Access</strong> (enrutadores de acceso): </p><br><p>  <strong>Esquema de tres niveles</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  La topología Leaf-Spine consta de dos niveles: <strong>Spine</strong> (aproximadamente el enrutador principal) y <strong>Leaf</strong> (ramas). </p><br><p>  <strong>Esquema de dos niveles</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Todo el enrutamiento interno y externo se basa en BGP.  El sistema principal que se ocupa del control de acceso, anuncios y más es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>XCloud.</strong></a> <br>  Los servidores para la reserva de canales (y también para su expansión) están conectados a dos conmutadores L3 (la mayoría de los servidores están conectados a conmutadores Leaf, pero algunos servidores con mayor carga de red están conectados directamente a la columna vertebral del conmutador), y a través de BGP anuncian su dirección de unidifusión, así como cualquier dirección de difusión para el servicio si varios servidores sirven el tráfico del servicio y el equilibrio ECMP es suficiente para ellos.  Una característica separada de este esquema, que nos permitió ahorrar en direcciones, pero también requería que los ingenieros se familiarizaran con el mundo IPv6, fue el uso del estándar BGP sin numerar basado en RFC 5549. Durante algún tiempo, Quagga se usó para servidores en BGP para este esquema para servidores y periódicamente hubo problemas con la pérdida de fiestas y conectividad.  Pero después de cambiar a FRRouting (cuyos contribuyentes activos son nuestros proveedores de equipos de red: Cumulus y XCloudNetworks), ya no observamos tales problemas. </p><br><p>  Por conveniencia, llamamos a todo este esquema general una "fábrica". </p><br><h2 id="poisk-puti">  Busca un camino </h2><br><p>  Opciones de configuración de red de clúster: </p><br><p>  1) Segunda red en BGP </p><br><p>  2) La segunda red en dos conmutadores apilados separados con LACP </p><br><p>  3) Segunda red en dos conmutadores aislados separados con OSPF </p><br><h3 id="testy">  Pruebas </h3><br><p>  Las pruebas se llevaron a cabo en dos tipos: </p><br><p>  a) red utilizando las utilidades iperf, qperf, nuttcp </p><br><p>  b) pruebas internas Ceph ceph-gobench, rados bench, creó rbd y las probó usando dd en uno o varios hilos, usando fio </p><br><p>  Todas las pruebas se llevaron a cabo en máquinas de prueba con discos SAS.  Las cifras en el rendimiento de rbd no se analizaron mucho, solo se usaron para comparar.  Interesado en cambios dependiendo del tipo de conexión. </p><br><h3 id="pervyy-variant">  Primera opción </h3><br><p>  <strong>Las tarjetas de red están conectadas a la fábrica, configuradas BGP.</strong> </p><br><p>  El uso de este esquema para la red interna no se consideró la mejor opción: </p><br><p>  En primer lugar, el exceso de elementos intermedios en forma de interruptores que dan latencia adicional (esta fue la razón principal). <br>  En segundo lugar, inicialmente, para emitir estadísticas a través de s3, utilizaron cualquier dirección emitida en varias máquinas con radosgateway.  Esto resultó en el hecho de que el tráfico de las máquinas de front-end a RGW no se distribuyó de manera uniforme, sino que pasó por la ruta más corta, es decir, Nginx de front-end siempre giraba hacia el mismo nodo con RGW que estaba conectado a la hoja compartida con él (esto, por supuesto, era no es el argumento principal: simplemente nos negamos posteriormente de las direcciones anycast para devolver estática).  Pero por la pureza del experimento, decidieron realizar pruebas en dicho esquema para tener datos para comparar. </p><br><p>  Teníamos miedo de ejecutar pruebas para todo el ancho de banda, ya que la fábrica es utilizada por servidores de producción, y si bloqueamos los enlaces entre la hoja y la columna vertebral, esto perjudicaría algunas de las ventas. <br>  En realidad, esta fue otra razón para rechazar tal esquema. <br>  Las pruebas Iperf con un límite de BW de 3 Gbps de 1, 10 y 100 flujos se utilizaron para comparar con otros esquemas. <br>  Las pruebas mostraron los siguientes resultados: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  en <strong>1</strong> flujo, aproximadamente <strong>9.30 - 9.43 Gbits / seg</strong> (en este caso, el número de retransmisiones crece fuertemente, a <strong>39148</strong> ).  La cifra resultó estar cerca del máximo de una interfaz sugiere que se use una de las dos.  El número de retransmisiones es de aproximadamente <strong>500-600.</strong> <br>  <strong>10</strong> transmisiones de <strong>9.63 Gbits / seg</strong> por interfaz, mientras que el número de retransmisiones creció a un promedio de <strong>17045.</strong> <br>  en <strong>100</strong> hilos, el resultado fue peor que en <strong>10</strong> , mientras que el número de retransmisiones es menor: el valor promedio es <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Segunda opción </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Había dos interruptores Juniper EX4500.  Los recogieron en la pila, conectaron el servidor con los primeros enlaces a un conmutador, el segundo al segundo. <br>  La configuración inicial de la unión fue la siguiente: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Las pruebas iperf y qperf mostraron Bw de hasta <strong>16 Gbits / seg.</strong>  Decidimos comparar diferentes tipos de mod: <br>  <strong>rr, balance-xor y 802.3ad.</strong>  También comparamos diferentes tipos de hash <strong>layer2 + 3 y layer3 + 4</strong> (con la esperanza de obtener una ventaja en la computación hash). <br>  También comparamos los resultados para diferentes valores sysctl de la variable <strong>net.ipv4.fib_multipath_hash_policy,</strong> (bueno, jugamos un poco con <strong>net.ipv4.tcp_congestion_control</strong> , aunque no tiene nada que ver con la <strong>vinculación</strong> . Hay un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buen artículo en ValdikSS para</a> esta variable)). </p><br><p>  Pero en todas las pruebas, no funcionó para superar el umbral de <strong>18 Gbits / seg</strong> (esta cifra se logró usando <strong>balance-xor y 802.3ad</strong> , no hubo mucha diferencia entre los resultados de la prueba) y este valor se logró "en salto" por ráfagas. </p><br><h3 id="tretiy-variant">  Tercera opción </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Para configurar esta opción, se eliminó LACP de los conmutadores (se dejó el apilamiento, pero solo se usó para la administración).  En cada conmutador, recopilaron un vlan separado para un grupo de puertos (con miras al futuro de que tanto los servidores QA como PROD se atascarán en los mismos conmutadores). </p><br><p>  Configurado dos redes privadas planas para cada vlan (una interfaz por conmutador).  Encima de estas direcciones está el anuncio de otra dirección de la tercera red privada, que es la red de clúster para CEPH. </p><br><p>  Como la <em>red pública</em> (a través de la cual usamos SSH) funciona en BGP, usamos frr para configurar OSPF, que ya está en el sistema. </p><br><p>  <strong>10.10.10.0/24 y 20.20.20.0/24</strong> : dos redes planas en los conmutadores </p><br><p>  <strong>172.16.1.0/24</strong> - red para anuncio </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Configuración de la máquina: <br>  interfaces <strong>ens1f0 ens1f1</strong> observan una red privada <br>  interfaces <strong>ens4f0 ens4f1</strong> mira la red pública </p><br><p>  La configuración de red en la máquina se ve así: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  Las configuraciones de Frr se ven así: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  En estas configuraciones, las pruebas de red iperf, qperf, etc.  mostró la máxima utilización de ambos canales a <strong>19.8 Gbit / seg,</strong> mientras que la latencia se redujo a <strong>20us</strong> </p><br><p>  <em><strong>Campo Bgp router-id: se</strong> utiliza para identificar el nodo al procesar información de enrutamiento y construir rutas.</em>  <em>Si no se especifica en la configuración, se selecciona una de las direcciones IP del host.</em>  <em>Los diferentes fabricantes de hardware y software pueden tener algoritmos diferentes, en nuestro caso FRR utilizó la dirección IP de bucle invertido más grande.</em>  <em>Esto condujo a dos problemas:</em> <em><br></em>  <em>1) Si tratamos de colgar otra dirección (por ejemplo, privada de la red 172.16.0.0) más que la actual, esto condujo a un cambio en la <strong>identificación</strong> del <strong>enrutador</strong> y, en consecuencia, a reinstalar las sesiones actuales.</em>  <em>Esto significa un breve descanso y pérdida de conectividad de red.</em> <em><br></em>  <em>2) Si intentamos colgar cualquier dirección de difusión compartida por varias máquinas y se seleccionó como una <strong>identificación de</strong> <strong>enrutador,</strong> aparecerían dos nodos con la misma <strong>identificación de enrutador</strong> en la red <strong>.</strong></em> </p><br><h2 id="chast-2">  Parte 2 </h2><br><p>  Después de probar QA, comenzamos a mejorar el combate Ceph. </p><br><h3 id="network">  RED </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Pasando de una red a dos </h3><br><p>  El parámetro de red del clúster es uno de los que no se puede cambiar sobre la marcha especificando el OSD a través de <strong>ceph tell osd. * Injectargs.</strong>  Cambiarlo en la configuración y reiniciar todo el clúster es una solución tolerable, pero realmente no quería tener ni siquiera un pequeño tiempo de inactividad.  También es imposible reiniciar un OSD con un nuevo parámetro de red; en algún momento habríamos tenido dos medios clústeres: OSD antiguos en la red anterior, nuevos en el nuevo.  Afortunadamente, el parámetro de red del clúster (así como public_network, por cierto) es una lista, es decir, puede especificar varios valores.  Decidimos movernos gradualmente: primero agreguemos una nueva red a las configuraciones y luego eliminemos la anterior.  Ceph revisa la lista de redes secuencialmente: OSD comienza a trabajar primero con la red que aparece primero. </p><br><p>  La dificultad era que la primera red funcionaba a través de bgp y estaba conectada a un conmutador, y la segunda, a ospf y conectada a otras que no estaban físicamente conectadas a la primera.  En el momento de la transición, era necesario tener acceso temporal a la red entre las dos redes.  La peculiaridad de configurar nuestra fábrica fue que las ACL no se pueden configurar en la red si no está en la lista de las anunciadas (en este caso es "externa" y las ACL solo se pueden crear externamente. Se creó en spains, pero no llegó en hojas). </p><br><p>  La solución fue una muleta, complicada, pero funcionó: anunciar la red interna a través de bgp, simultáneamente con ospf. </p><br><p>  La secuencia de transición es la siguiente: </p><br><p>  1) Configure la red de clúster para ceph en dos redes: a través de bgp y a través de ospf <br>  En configuraciones frr no era necesario cambiar nada, una línea </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  no nos limita en las direcciones anunciadas, la dirección de la red interna en sí se muestra en la interfaz de bucle invertido, fue suficiente para configurar la recepción del anuncio de esta dirección en los enrutadores. </p><br><p>  2) Agregue una nueva red a la configuración <strong>ceph.conf</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  y comience a reiniciar el OSD de uno en uno hasta que todos <strong>cambien a la</strong> red <strong>172.16.1.0/24.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Luego eliminamos el exceso de red de la configuración </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  y repita el procedimiento. </p><br><p>  Eso es todo, nos mudamos sin problemas a una nueva red. </p><br><p>  Referencias <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458390/">https://habr.com/ru/post/458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458376/index.html">No es otro lenguaje de programación. Parte 1: lógica de dominio</a></li>
<li><a href="../458378/index.html">Usando Avocode para el diseño del sitio. Revisión para principiantes. Bonificación: registre un período de prueba de 30 días</a></li>
<li><a href="../458382/index.html">¿Por qué estamos enseñando esto?</a></li>
<li><a href="../458384/index.html">HP 3D Structured Light Scanner Pro S3 Revisión y prueba</a></li>
<li><a href="../458388/index.html">Deep (Learning + Random) Forest y análisis de artículos</a></li>
<li><a href="../458394/index.html">Asegurar protocolos inalámbricos utilizando LoRaWAN como ejemplo</a></li>
<li><a href="../458396/index.html">Cómo hice que el desarrollo en Vue.js sea conveniente con la representación del lado del servidor</a></li>
<li><a href="../458398/index.html">Higiene del trabajo a distancia o los beneficios de la telepatía.</a></li>
<li><a href="../458400/index.html">Arquitectura e implementación de microservicios, paso a paso, parte 1</a></li>
<li><a href="../458404/index.html">Transición del monolito a los microservicios: historia y práctica.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>