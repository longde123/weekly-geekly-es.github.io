<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>解 锔 じ Ceph - de "en la rodilla" a "producci贸n" parte 2   锔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(primera parte aqu铆: https://habr.com/en/post/456446/ ) 
 Ceph 
 Introduccion 


 Dado que la red es uno de los elementos clave de Ceph, y es un poco ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - de "en la rodilla" a "producci贸n" parte 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (primera parte aqu铆: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  Introduccion </h3><br><p>  Dado que la red es uno de los elementos clave de Ceph, y es un poco espec铆fica en nuestra empresa, primero le contaremos un poco al respecto. <br>  Habr谩 muchas menos descripciones de Ceph, principalmente una infraestructura de red.  Solo se describir谩n los servidores Ceph y algunas caracter铆sticas de los servidores de virtualizaci贸n Proxmox. </p><a name="habracut"></a><br><p>  Entonces: la topolog铆a de la red en s铆 est谩 construida como <strong>Leaf-Spine.</strong>  La arquitectura cl谩sica de tres niveles es una red donde hay <strong>Core</strong> (enrutadores centrales), <strong>Agregaci贸n</strong> (enrutadores de agregaci贸n) y directamente conectados con clientes de <strong>Access</strong> (enrutadores de acceso): </p><br><p>  <strong>Esquema de tres niveles</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  La topolog铆a Leaf-Spine consta de dos niveles: <strong>Spine</strong> (aproximadamente el enrutador principal) y <strong>Leaf</strong> (ramas). </p><br><p>  <strong>Esquema de dos niveles</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Todo el enrutamiento interno y externo se basa en BGP.  El sistema principal que se ocupa del control de acceso, anuncios y m谩s es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>XCloud.</strong></a> <br>  Los servidores para la reserva de canales (y tambi茅n para su expansi贸n) est谩n conectados a dos conmutadores L3 (la mayor铆a de los servidores est谩n conectados a conmutadores Leaf, pero algunos servidores con mayor carga de red est谩n conectados directamente a la columna vertebral del conmutador), y a trav茅s de BGP anuncian su direcci贸n de unidifusi贸n, as铆 como cualquier direcci贸n de difusi贸n para el servicio si varios servidores sirven el tr谩fico del servicio y el equilibrio ECMP es suficiente para ellos.  Una caracter铆stica separada de este esquema, que nos permiti贸 ahorrar en direcciones, pero tambi茅n requer铆a que los ingenieros se familiarizaran con el mundo IPv6, fue el uso del est谩ndar BGP sin numerar basado en RFC 5549. Durante alg煤n tiempo, Quagga se us贸 para servidores en BGP para este esquema para servidores y peri贸dicamente hubo problemas con la p茅rdida de fiestas y conectividad.  Pero despu茅s de cambiar a FRRouting (cuyos contribuyentes activos son nuestros proveedores de equipos de red: Cumulus y XCloudNetworks), ya no observamos tales problemas. </p><br><p>  Por conveniencia, llamamos a todo este esquema general una "f谩brica". </p><br><h2 id="poisk-puti">  Busca un camino </h2><br><p>  Opciones de configuraci贸n de red de cl煤ster: </p><br><p>  1) Segunda red en BGP </p><br><p>  2) La segunda red en dos conmutadores apilados separados con LACP </p><br><p>  3) Segunda red en dos conmutadores aislados separados con OSPF </p><br><h3 id="testy">  Pruebas </h3><br><p>  Las pruebas se llevaron a cabo en dos tipos: </p><br><p>  a) red utilizando las utilidades iperf, qperf, nuttcp </p><br><p>  b) pruebas internas Ceph ceph-gobench, rados bench, cre贸 rbd y las prob贸 usando dd en uno o varios hilos, usando fio </p><br><p>  Todas las pruebas se llevaron a cabo en m谩quinas de prueba con discos SAS.  Las cifras en el rendimiento de rbd no se analizaron mucho, solo se usaron para comparar.  Interesado en cambios dependiendo del tipo de conexi贸n. </p><br><h3 id="pervyy-variant">  Primera opci贸n </h3><br><p>  <strong>Las tarjetas de red est谩n conectadas a la f谩brica, configuradas BGP.</strong> </p><br><p>  El uso de este esquema para la red interna no se consider贸 la mejor opci贸n: </p><br><p>  En primer lugar, el exceso de elementos intermedios en forma de interruptores que dan latencia adicional (esta fue la raz贸n principal). <br>  En segundo lugar, inicialmente, para emitir estad铆sticas a trav茅s de s3, utilizaron cualquier direcci贸n emitida en varias m谩quinas con radosgateway.  Esto result贸 en el hecho de que el tr谩fico de las m谩quinas de front-end a RGW no se distribuy贸 de manera uniforme, sino que pas贸 por la ruta m谩s corta, es decir, Nginx de front-end siempre giraba hacia el mismo nodo con RGW que estaba conectado a la hoja compartida con 茅l (esto, por supuesto, era no es el argumento principal: simplemente nos negamos posteriormente de las direcciones anycast para devolver est谩tica).  Pero por la pureza del experimento, decidieron realizar pruebas en dicho esquema para tener datos para comparar. </p><br><p>  Ten铆amos miedo de ejecutar pruebas para todo el ancho de banda, ya que la f谩brica es utilizada por servidores de producci贸n, y si bloqueamos los enlaces entre la hoja y la columna vertebral, esto perjudicar铆a algunas de las ventas. <br>  En realidad, esta fue otra raz贸n para rechazar tal esquema. <br>  Las pruebas Iperf con un l铆mite de BW de 3 Gbps de 1, 10 y 100 flujos se utilizaron para comparar con otros esquemas. <br>  Las pruebas mostraron los siguientes resultados: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  en <strong>1</strong> flujo, aproximadamente <strong>9.30 - 9.43 Gbits / seg</strong> (en este caso, el n煤mero de retransmisiones crece fuertemente, a <strong>39148</strong> ).  La cifra result贸 estar cerca del m谩ximo de una interfaz sugiere que se use una de las dos.  El n煤mero de retransmisiones es de aproximadamente <strong>500-600.</strong> <br>  <strong>10</strong> transmisiones de <strong>9.63 Gbits / seg</strong> por interfaz, mientras que el n煤mero de retransmisiones creci贸 a un promedio de <strong>17045.</strong> <br>  en <strong>100</strong> hilos, el resultado fue peor que en <strong>10</strong> , mientras que el n煤mero de retransmisiones es menor: el valor promedio es <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Segunda opci贸n </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Hab铆a dos interruptores Juniper EX4500.  Los recogieron en la pila, conectaron el servidor con los primeros enlaces a un conmutador, el segundo al segundo. <br>  La configuraci贸n inicial de la uni贸n fue la siguiente: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Las pruebas iperf y qperf mostraron Bw de hasta <strong>16 Gbits / seg.</strong>  Decidimos comparar diferentes tipos de mod: <br>  <strong>rr, balance-xor y 802.3ad.</strong>  Tambi茅n comparamos diferentes tipos de hash <strong>layer2 + 3 y layer3 + 4</strong> (con la esperanza de obtener una ventaja en la computaci贸n hash). <br>  Tambi茅n comparamos los resultados para diferentes valores sysctl de la variable <strong>net.ipv4.fib_multipath_hash_policy,</strong> (bueno, jugamos un poco con <strong>net.ipv4.tcp_congestion_control</strong> , aunque no tiene nada que ver con la <strong>vinculaci贸n</strong> . Hay un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buen art铆culo en ValdikSS para</a> esta variable)). </p><br><p>  Pero en todas las pruebas, no funcion贸 para superar el umbral de <strong>18 Gbits / seg</strong> (esta cifra se logr贸 usando <strong>balance-xor y 802.3ad</strong> , no hubo mucha diferencia entre los resultados de la prueba) y este valor se logr贸 "en salto" por r谩fagas. </p><br><h3 id="tretiy-variant">  Tercera opci贸n </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Para configurar esta opci贸n, se elimin贸 LACP de los conmutadores (se dej贸 el apilamiento, pero solo se us贸 para la administraci贸n).  En cada conmutador, recopilaron un vlan separado para un grupo de puertos (con miras al futuro de que tanto los servidores QA como PROD se atascar谩n en los mismos conmutadores). </p><br><p>  Configurado dos redes privadas planas para cada vlan (una interfaz por conmutador).  Encima de estas direcciones est谩 el anuncio de otra direcci贸n de la tercera red privada, que es la red de cl煤ster para CEPH. </p><br><p>  Como la <em>red p煤blica</em> (a trav茅s de la cual usamos SSH) funciona en BGP, usamos frr para configurar OSPF, que ya est谩 en el sistema. </p><br><p>  <strong>10.10.10.0/24 y 20.20.20.0/24</strong> : dos redes planas en los conmutadores </p><br><p>  <strong>172.16.1.0/24</strong> - red para anuncio </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Configuraci贸n de la m谩quina: <br>  interfaces <strong>ens1f0 ens1f1</strong> observan una red privada <br>  interfaces <strong>ens4f0 ens4f1</strong> mira la red p煤blica </p><br><p>  La configuraci贸n de red en la m谩quina se ve as铆: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  Las configuraciones de Frr se ven as铆: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  En estas configuraciones, las pruebas de red iperf, qperf, etc.  mostr贸 la m谩xima utilizaci贸n de ambos canales a <strong>19.8 Gbit / seg,</strong> mientras que la latencia se redujo a <strong>20us</strong> </p><br><p>  <em><strong>Campo Bgp router-id: se</strong> utiliza para identificar el nodo al procesar informaci贸n de enrutamiento y construir rutas.</em>  <em>Si no se especifica en la configuraci贸n, se selecciona una de las direcciones IP del host.</em>  <em>Los diferentes fabricantes de hardware y software pueden tener algoritmos diferentes, en nuestro caso FRR utiliz贸 la direcci贸n IP de bucle invertido m谩s grande.</em>  <em>Esto condujo a dos problemas:</em> <em><br></em>  <em>1) Si tratamos de colgar otra direcci贸n (por ejemplo, privada de la red 172.16.0.0) m谩s que la actual, esto condujo a un cambio en la <strong>identificaci贸n</strong> del <strong>enrutador</strong> y, en consecuencia, a reinstalar las sesiones actuales.</em>  <em>Esto significa un breve descanso y p茅rdida de conectividad de red.</em> <em><br></em>  <em>2) Si intentamos colgar cualquier direcci贸n de difusi贸n compartida por varias m谩quinas y se seleccion贸 como una <strong>identificaci贸n de</strong> <strong>enrutador,</strong> aparecer铆an dos nodos con la misma <strong>identificaci贸n de enrutador</strong> en la red <strong>.</strong></em> </p><br><h2 id="chast-2">  Parte 2 </h2><br><p>  Despu茅s de probar QA, comenzamos a mejorar el combate Ceph. </p><br><h3 id="network">  RED </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Pasando de una red a dos </h3><br><p>  El par谩metro de red del cl煤ster es uno de los que no se puede cambiar sobre la marcha especificando el OSD a trav茅s de <strong>ceph tell osd. * Injectargs.</strong>  Cambiarlo en la configuraci贸n y reiniciar todo el cl煤ster es una soluci贸n tolerable, pero realmente no quer铆a tener ni siquiera un peque帽o tiempo de inactividad.  Tambi茅n es imposible reiniciar un OSD con un nuevo par谩metro de red; en alg煤n momento habr铆amos tenido dos medios cl煤steres: OSD antiguos en la red anterior, nuevos en el nuevo.  Afortunadamente, el par谩metro de red del cl煤ster (as铆 como public_network, por cierto) es una lista, es decir, puede especificar varios valores.  Decidimos movernos gradualmente: primero agreguemos una nueva red a las configuraciones y luego eliminemos la anterior.  Ceph revisa la lista de redes secuencialmente: OSD comienza a trabajar primero con la red que aparece primero. </p><br><p>  La dificultad era que la primera red funcionaba a trav茅s de bgp y estaba conectada a un conmutador, y la segunda, a ospf y conectada a otras que no estaban f铆sicamente conectadas a la primera.  En el momento de la transici贸n, era necesario tener acceso temporal a la red entre las dos redes.  La peculiaridad de configurar nuestra f谩brica fue que las ACL no se pueden configurar en la red si no est谩 en la lista de las anunciadas (en este caso es "externa" y las ACL solo se pueden crear externamente. Se cre贸 en spains, pero no lleg贸 en hojas). </p><br><p>  La soluci贸n fue una muleta, complicada, pero funcion贸: anunciar la red interna a trav茅s de bgp, simult谩neamente con ospf. </p><br><p>  La secuencia de transici贸n es la siguiente: </p><br><p>  1) Configure la red de cl煤ster para ceph en dos redes: a trav茅s de bgp y a trav茅s de ospf <br>  En configuraciones frr no era necesario cambiar nada, una l铆nea </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  no nos limita en las direcciones anunciadas, la direcci贸n de la red interna en s铆 se muestra en la interfaz de bucle invertido, fue suficiente para configurar la recepci贸n del anuncio de esta direcci贸n en los enrutadores. </p><br><p>  2) Agregue una nueva red a la configuraci贸n <strong>ceph.conf</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  y comience a reiniciar el OSD de uno en uno hasta que todos <strong>cambien a la</strong> red <strong>172.16.1.0/24.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Luego eliminamos el exceso de red de la configuraci贸n </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  y repita el procedimiento. </p><br><p>  Eso es todo, nos mudamos sin problemas a una nueva red. </p><br><p>  Referencias <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458390/">https://habr.com/ru/post/458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458376/index.html">No es otro lenguaje de programaci贸n. Parte 1: l贸gica de dominio</a></li>
<li><a href="../458378/index.html">Usando Avocode para el dise帽o del sitio. Revisi贸n para principiantes. Bonificaci贸n: registre un per铆odo de prueba de 30 d铆as</a></li>
<li><a href="../458382/index.html">驴Por qu茅 estamos ense帽ando esto?</a></li>
<li><a href="../458384/index.html">HP 3D Structured Light Scanner Pro S3 Revisi贸n y prueba</a></li>
<li><a href="../458388/index.html">Deep (Learning + Random) Forest y an谩lisis de art铆culos</a></li>
<li><a href="../458394/index.html">Asegurar protocolos inal谩mbricos utilizando LoRaWAN como ejemplo</a></li>
<li><a href="../458396/index.html">C贸mo hice que el desarrollo en Vue.js sea conveniente con la representaci贸n del lado del servidor</a></li>
<li><a href="../458398/index.html">Higiene del trabajo a distancia o los beneficios de la telepat铆a.</a></li>
<li><a href="../458400/index.html">Arquitectura e implementaci贸n de microservicios, paso a paso, parte 1</a></li>
<li><a href="../458404/index.html">Transici贸n del monolito a los microservicios: historia y pr谩ctica.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>