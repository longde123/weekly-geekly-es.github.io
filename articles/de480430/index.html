<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ±Ô∏è ‚úçÔ∏è üåû Wie der Videocodec funktioniert. Teil 2. Was, warum, wie üë©üèø‚ÄçüöÄ üßõüèø üëÜüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil I: Video- und Bildgrundlagen 

  

 Was denn Ein Video-Codec ist eine Software / Hardware, die digitales Video komprimiert und / oder dekomprimie...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie der Videocodec funktioniert. Teil 2. Was, warum, wie</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/edison/blog/480430/"><h3>  Teil I: <a href="https://habr.com/ru/company/edison/blog/481418/">Video- und Bildgrundlagen</a> </h3><br><hr><br> <a href="https://habr.com/ru/company/edison/blog/480430/"><img src="https://habrastorage.org/webt/ta/r8/lt/tar8ltvwz2ubkwwudgcjws-sa9a.png" alt="Kodeks Geschichte"></a> <br><br>  <b>Was denn</b>  Ein Video-Codec ist eine Software / Hardware, die digitales Video komprimiert und / oder dekomprimiert. <br><br>  <b>Wof√ºr?</b>  Trotz einiger Einschr√§nkungen in Bezug auf die Bandbreite, <br>  Und was den Speicherplatz angeht, verlangt der Markt immer mehr qualitativ hochwertige Videos.  Erinnern Sie sich, wie wir im letzten Beitrag das erforderliche Minimum f√ºr 30 Bilder pro Sekunde, 24 Bit pro Pixel, mit einer Aufl√∂sung von 480 x 240 berechnet haben?  Empfing 82.944 Mbit / s ohne Komprimierung.  Die Komprimierung ist die einzige M√∂glichkeit, HD / FullHD / 4K auf Fernsehbildschirme und das Internet zu √ºbertragen.  Wie wird das erreicht?  Jetzt werden wir kurz die Hauptmethoden betrachten. <a name="habracut"></a><blockquote> <a href="https://www.edsd.ru/" title="EDISON Software - Webentwicklung"><img align="left" width="153" height="75" src="https://habrastorage.org/webt/w0/zl/to/w0zltoxvysbr0yeinstkfvw1wbg.png" alt="EDISON Software - Webentwicklung"></a> <br clear="right">  Die √úbersetzung wurde mit Unterst√ºtzung von EDISON Software erstellt. <br><br>  Wir besch√§ftigen uns mit der <a href="https://www.edsd.ru/integratsiya-sistem-videonablyudeniya-axxon-next-i-sureview-immix">Integration von Video√ºberwachungssystemen</a> und der <a href="https://www.edsd.ru/prilozhenie-dlya-mikrotomografa">Entwicklung eines Mikrotomographen</a> . </blockquote><h3>  Codec gegen Container </h3><br>  Ein h√§ufiger Anf√§ngerfehler besteht darin, einen digitalen Videocodec und einen digitalen Videocontainer zu verwechseln.  Ein Container hat ein bestimmtes Format.  Ein Wrapper mit Video-Metadaten (und m√∂glicherweise Audio).  Komprimiertes Video kann als Container-Nutzlast betrachtet werden. <br><br>  In der Regel gibt eine Videodateierweiterung einen Containertyp an.  Beispielsweise ist die Datei video.mp4 h√∂chstwahrscheinlich ein <i>MPEG-4 Part 14-</i> Container, und die Datei video.mkv ist h√∂chstwahrscheinlich eine russische <a href="https://ru.wikipedia.org/wiki/Matroska">Puppe</a> .  Sie k√∂nnen <a href="https://www.ffmpeg.org/">FFmpeg</a> oder <a href="https://mediaarea.net/ru/MediaInfo">MediaInfo</a> verwenden, um sich im <a href="https://mediaarea.net/ru/MediaInfo">Codec-</a> und Container-Format voll und ganz sicher zu <a href="https://mediaarea.net/ru/MediaInfo">f√ºhlen</a> . <br><br><h3>  Ein bisschen Geschichte </h3><br>  Bevor wir zu <b>Wie kommen?</b>  Tauchen wir ein bisschen in die Geschichte ein, um ein bisschen mehr √ºber einige alte Codecs zu erfahren. <br><br>  Der <b>H.261</b> -Videocodec erschien 1990 (technisch gesehen 1988) und wurde f√ºr eine Daten√ºbertragungsrate von 64 Kbit / s entwickelt.  Es wurden bereits Ideen wie Farbunterabtastung, Makrobl√∂cke usw. verwendet.  1995 wurde der Video-Codec-Standard <b>H.263</b> ver√∂ffentlicht, der sich bis 2001 entwickelte. <br><br>  Im Jahr 2003 wurde die erste Version von <b>H.264 / AVC</b> fertiggestellt.  Im selben Jahr ver√∂ffentlichte TrueMotion seinen kostenlosen Video-Codec, der verlustbehaftetes Video mit dem Namen <b>VP3</b> komprimiert.  2008 kaufte Google diese Firma und brachte im selben Jahr <b>VP8 heraus</b> .  Im Dezember 2012 ver√∂ffentlichte Google <b>VP9</b> und es wird in etwa ¬æ des Browsermarktes (einschlie√ülich mobiler Ger√§te) unterst√ºtzt. <br><br>  <b>AV1</b> ist ein neuer kostenloser Open-Source-Videocodec, der <b>von der Open Media Alliance</b> ( <b>AOMedia</b> ) entwickelt wurde und zu dem bekannte Unternehmen wie Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel und Cisco geh√∂ren .  Die erste Version des Codec 0.1.0 wurde am 7. April 2016 ver√∂ffentlicht. <br><br><h3>  Geburt von AV1 </h3><br>  Anfang 2015 arbeitete Google an <b>VP10</b> , Xiph (das zu Mozilla geh√∂rt) arbeitete an <b>Daala</b> und Cisco erstellte seinen kostenlosen Video-Codec namens <b>Thor</b> . <br><br>  Dann k√ºndigte <b>MPEG LA</b> zun√§chst j√§hrliche Grenzwerte f√ºr <b>HEVC</b> ( <b>H.265</b> ) und eine 8-fach h√∂here Geb√ºhr als f√ºr H.264 an, √§nderte jedoch bald die Regeln erneut: <br><br>  kein Jahreslimit, <br>  Inhaltsgeb√ºhr (0,5% des Umsatzes) und <br>  Die St√ºckkosten sind etwa zehnmal h√∂her als bei H.264. <br><br>  <i>Die Open Media Alliance</i> wurde von Unternehmen aus verschiedenen Bereichen gegr√ºndet: Ger√§teherstellern (Intel, AMD, ARM, Nvidia, Cisco), Inhaltsanbietern (Google, Netflix, Amazon), Browsern (Google, Mozilla) und anderen. <br><br>  Die Unternehmen hatten ein gemeinsames Ziel - einen Videocodec ohne Lizenzgeb√ºhren.  Dann kommt <b>AV1</b> mit einer wesentlich einfacheren Patentlizenz.  Timothy B. Terriberry hielt eine beeindruckende Pr√§sentation ab, aus der das aktuelle Konzept von AV1 und seines Lizenzmodells hervorging. <br><br>  Sie werden √ºberrascht sein zu erfahren, dass Sie den AV1-Codec √ºber einen Browser analysieren k√∂nnen (Interessenten k√∂nnen zu <a href="http://aomanalyzer.org/">aomanalyzer.org</a> gehen). <br><br><img src="https://habrastorage.org/webt/wa/zz/tv/wazztvoifpc4l11k2_8jzdmiffk.png" alt="Bild"><br><br><h3>  Universeller Codec </h3><br>  Analysieren wir die grundlegenden Mechanismen, die dem universellen Videocodec zugrunde liegen.  Die meisten dieser Konzepte sind n√ºtzlich und werden in modernen Codecs wie <b>VP9</b> , <b>AV1</b> und <b>HEVC verwendet</b> .  Ich warne Sie, dass vieles, was erkl√§rt wird, vereinfacht wird.  Gelegentlich werden reale Beispiele verwendet (wie dies bei H.264 der Fall ist), um Technologie zu demonstrieren. <br><br><h3>  1. Schritt - Aufteilen des Bildes </h3><br>  Der erste Schritt besteht darin, den Frame in mehrere Abschnitte, Unterabschnitte und mehr zu unterteilen. <br><br><img width="420" height="238" src="https://habrastorage.org/webt/lo/ze/q1/lozeq18ppa9po16q2ykeiazibpk.png" alt="Bild"><br><br>  Wof√ºr?  Gr√ºnde gibt es viele.  Wenn wir das Bild teilen, k√∂nnen wir den Bewegungsvektor mit kleinen Abschnitten f√ºr kleine bewegliche Teile genauer vorhersagen.  Bei einem statischen Hintergrund k√∂nnen Sie sich auf gr√∂√üere Abschnitte beschr√§nken. <br><br>  In der Regel organisieren Codecs diese Abschnitte in Abschnitte (oder Fragmente), Makrobl√∂cke (oder Bl√∂cke eines Codierungsbaums) und viele Unterabschnitte.  Die maximale Gr√∂√üe dieser Partitionen variiert, HEVC legt 64 x 64 fest, w√§hrend AVC 16 x 16 verwendet und Unterabschnitte bis zu 4x4 aufgeteilt werden k√∂nnen. <br><br>  Erinnern Sie sich an die Rahmenvarianten aus dem letzten Artikel ?!  Dasselbe kann auf Bl√∂cke angewendet werden, also k√∂nnen wir ein I-Fragment, einen B-Block, einen P-Makroblock usw. haben. <br><br>  Sehen Sie f√ºr diejenigen, die √ºben m√∂chten, wie das Bild in Abschnitte und Unterabschnitte unterteilt wird.  Zu diesem Zweck k√∂nnen Sie den <a href="https://software.intel.com/en-us/video-pro-analyzer">Intel Video Pro Analyzer verwenden, der</a> bereits in einem fr√ºheren Artikel erw√§hnt wurde (der kostenpflichtig ist, aber eine kostenlose Testversion enth√§lt, die auf die ersten 10 Bilder beschr√§nkt ist).  Die Abschnitte von <b>VP9</b> werden hier analysiert: <br><br><img width="711" height="370" src="https://habrastorage.org/webt/ub/n1/yh/ubn1yh-_d5n68swbd0bgc-lxbha.png" alt="Bild"><br><br><h3>  2. Schritt - Prognose </h3><br>  Sobald wir Abschnitte haben, k√∂nnen wir <s>astrologische</s> Vorhersagen dar√ºber machen.  F√ºr die <b>INTER-Vorhersage ist</b> es notwendig, <b>Bewegungsvektoren</b> und den Rest zu √ºbertragen, und f√ºr die INTRA-Vorhersage werden die <b>Richtung der Vorhersage</b> und der Rest √ºbertragen. <br><br><h3>  3. Schritt - Konvertierung </h3><br>  Nachdem wir den Restblock erhalten haben (den vorhergesagten Abschnitt ‚Üí den realen Abschnitt), ist es m√∂glich, ihn so zu transformieren, dass wir wissen, welche Pixel verworfen werden k√∂nnen, w√§hrend die Gesamtqualit√§t erhalten bleibt.  Es gibt einige Transformationen, die genaues Verhalten liefern. <br><br>  Obwohl es andere Methoden gibt, wollen wir die <b>diskrete Cosinustransformation</b> ( <b>DCT</b> - from <i>discrete cosine transform</i> ) genauer betrachten.  Hauptmerkmale von DCT: <br><br><ul><li>  Konvertiert Pixelbl√∂cke in gleich gro√üe Frequenzkoeffizientenbl√∂cke. </li><li>  Versiegelt die Stromversorgung und hilft, r√§umliche Redundanzen zu beseitigen. </li><li>  Bietet Reversibilit√§t. </li></ul><br>  2. Februar 2017 Sintra R.J.  (Cintra, RJ) und Bayer F.M.  (Bayer FM) ver√∂ffentlichte einen Artikel zur DCT-√§hnlichen Konvertierung f√ºr die Bildkomprimierung, der nur 14 Zus√§tze erfordert. <br><br>  Machen Sie sich keine Sorgen, wenn Sie die Vorteile der einzelnen Artikel nicht verstehen.  Anhand konkreter Beispiele werden wir nun ihren wahren Wert √ºberpr√ºfen. <br><br>  Nehmen wir einen 8x8 Pixel Block wie folgt: <br><br><img width="354" height="139" src="https://habrastorage.org/webt/tf/di/up/tfdiuplow4tut1aww2lrfydamyi.png" alt="Bild"><br><br>  Dieser Block wird 8 mal 8 Pixel in das folgende Bild gerendert: <br><br><img width="362" height="380" src="https://habrastorage.org/webt/rq/gj/hj/rqgjhjuplvgdadziupv-ogpdazo.png" alt="Bild"><br><br>  Wenden Sie DCT auf diesen Pixelblock an und erhalten Sie einen Koeffizientenblock der Gr√∂√üe 8x8: <br><br><img width="623" height="141" src="https://habrastorage.org/webt/3c/oh/i8/3cohi8vd2yjmxurbzrfhlabc_ni.png" alt="Bild"><br><br>  Und wenn wir diesen Koeffizientenblock rendern, erhalten wir das folgende Bild: <br><br><img width="401" height="328" src="https://habrastorage.org/webt/np/b0/v2/npb0v2_58_1hxinqekagzf6phzi.png" alt="Bild"><br><br>  Wie Sie sehen k√∂nnen, entspricht dies nicht dem Originalbild.  M√∂glicherweise stellen Sie fest, dass sich der erste Koeffizient von allen anderen stark unterscheidet.  Dieser erste Koeffizient ist als DC-Koeffizient bekannt, der alle Abtastwerte in der Eingabematrix darstellt, √§hnlich wie der Durchschnittswert. <br><br>  Dieser Koeffizientenblock hat eine interessante Eigenschaft: Er trennt hochfrequente von niederfrequenten Komponenten. <br><br><img width="293" height="129" src="https://habrastorage.org/webt/n0/4u/1z/n04u1zqlixaeggiw_yfg3umocsy.jpeg" alt="Bild"><br><br>  Im Bild ist der gr√∂√üte Teil der Leistung auf niedrigere Frequenzen konzentriert. Wenn Sie daher das Bild in seine Frequenzkomponenten umwandeln und die h√∂heren Frequenzkoeffizienten verwerfen, k√∂nnen Sie die zur Beschreibung des Bildes erforderliche Datenmenge reduzieren, ohne die Bildqualit√§t zu stark zu beeintr√§chtigen. <blockquote>  Frequenz bedeutet, wie schnell sich das Signal √§ndert. </blockquote>  Versuchen wir, die im Testbeispiel gewonnenen Erkenntnisse anzuwenden, indem wir das Originalbild mit DCT in seine Frequenz (Koeffizientenblock) umwandeln und dann einige der unwichtigsten Koeffizienten verwerfen. <br><br>  Konvertieren Sie es zun√§chst in den Frequenzbereich. <br><br><img width="623" height="141" src="https://habrastorage.org/webt/3c/oh/i8/3cohi8vd2yjmxurbzrfhlabc_ni.png" alt="Bild"><br><br>  Als n√§chstes verwerfen wir einen Teil (67%) der Koeffizienten, haupts√§chlich die untere rechte Seite. <br><br><img width="624" height="139" src="https://habrastorage.org/webt/h5/dq/ww/h5dqwwheilwuuasgm7xxijdxmt8.png" alt="Bild"><br><br>  Schlie√ülich stellen wir das Bild aus diesem verworfenen Koeffizientenblock wieder her (denken Sie daran, es muss umkehrbar sein) und vergleichen es mit dem Original. <br><br><img src="https://habrastorage.org/webt/6j/c6/1z/6jc61zgpg5r-vxxh3j0dxjeatha.png" alt="Bild"><br><br>  Wir sehen, dass es dem Originalbild √§hnelt, aber es gibt viele Unterschiede zum Original.  Wir haben 67,1875% und haben immer noch etwas, das der urspr√ºnglichen Quelle √§hnelt.  Sie k√∂nnen die Koeffizienten bewusster verwerfen, um ein noch besseres Bild zu erhalten. Dies ist jedoch das n√§chste Thema. <blockquote><h4>  Jeder Koeffizient wird mit allen Pixeln erzeugt. </h4><br><br>  Wichtig: Jeder Koeffizient wird nicht direkt auf einem Pixel angezeigt, sondern ist eine gewichtete Summe aller Pixel.  Diese erstaunliche Grafik zeigt, wie der erste und der zweite Koeffizient unter Verwendung von Gewichten berechnet werden, die f√ºr jeden Index eindeutig sind. <br><br><img width="381" height="550" src="https://habrastorage.org/webt/ix/uv/hw/ixuvhwvqvclyzq6astn1d5vsaac.jpeg" alt="Bild"><br><br>  Sie k√∂nnen auch versuchen, DCT zu visualisieren, indem Sie eine einfache darauf basierende Bildgebung betrachten.  Zum Beispiel ist hier das Symbol A, das mit jedem Koeffizientengewicht erzeugt wird: <br><br><img width="241" height="81" src="https://habrastorage.org/webt/oq/ua/3t/oqua3tnacmh7nsucvodymeqzqa0.gif" alt="Bild"></blockquote><br><br><h3>  4. Schritt - Quantisierung </h3><br>  Nachdem wir im vorherigen Schritt einige Koeffizienten weggeworfen haben, erstellen wir im letzten Schritt (Transformation) eine spezielle Quantisierungsform.  Zu diesem Zeitpunkt ist es zul√§ssig, Informationen zu verlieren.  Oder einfacher gesagt, wir werden die Koeffizienten quantisieren, um eine Komprimierung zu erreichen. <br><br>  Wie kann ein Koeffizientenblock quantisiert werden?  Eine der einfachsten Methoden wird die einheitliche Quantisierung sein, wenn wir einen Block nehmen, durch einen Wert (durch 10) teilen und abrunden, was passiert ist. <br><br><img width="770" height="168" src="https://habrastorage.org/webt/i8/1a/os/i81aosymefwhnp0jboibq0dd_4i.png" alt="Bild"><br><br>  K√∂nnen wir diesen Koeffizientenblock umkehren?  Ja, wir k√∂nnen, indem wir mit demselben Wert multiplizieren, durch den wir dividiert haben. <br><br><img width="770" height="168" src="https://habrastorage.org/webt/ys/vs/51/ysvs514v-u_hrteesekxdrkjp9c.png" alt="Bild"><br><br>  Dieser Ansatz ist nicht der beste, da er nicht die Bedeutung jedes Koeffizienten ber√ºcksichtigt.  Man k√∂nnte die Quantisierermatrix anstelle eines einzelnen Werts verwenden, und diese Matrix k√∂nnte die DCT-Eigenschaft verwenden, um die Mehrheit der unteren rechten und die Minderheit der oberen linken zu quantisieren. <br><br><h3>  5-Stufen-Entropie-Codierung </h3><br>  Nachdem wir die Daten (Bildbl√∂cke, Fragmente, Rahmen) quantisiert haben, k√∂nnen wir sie immer noch ohne Verlust komprimieren.  Es gibt viele algorithmische M√∂glichkeiten, Daten zu komprimieren.  Wir werden einige von ihnen kurz kennenlernen. Zum besseren Verst√§ndnis lesen Sie das Buch ‚Äû <a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/">Komprimierung verstehen: Datenkomprimierung f√ºr moderne Entwickler</a> ‚Äú (‚Äû <a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/">Komprimierung verstehen: Datenkomprimierung f√ºr moderne Entwickler</a> ‚Äú). <br><br><h3>  Videokodierung mit VLC </h3><br>  Angenommen, wir haben einen Zeichenstrom: <b>a</b> , <b>e</b> , <b>r</b> und <b>t</b> .  Die Wahrscheinlichkeit (im Bereich von 0 bis 1), wie oft jedes Symbol im Stream vorkommt, ist in dieser Tabelle dargestellt. <div class="scrollable-table"><table><tbody><tr><th></th><th>  a </th><th>  e </th><th>  r </th><th>  t </th></tr><tr><th>  Wahrscheinlichkeit </th><td align="center">  0,3 </td><td align="center">  0,3 </td><td align="center">  0,2 </td><td align="center">  0,2 </td></tr></tbody></table></div>  Wir k√∂nnen eindeutige Bin√§rcodes (vorzugsweise kleine) den wahrscheinlichsten und gr√∂√üere Codes den unwahrscheinlichsten zuordnen. <div class="scrollable-table"><table><tbody><tr><th></th><th>  a </th><th>  e </th><th>  r </th><th>  t </th></tr><tr><th>  Wahrscheinlichkeit </th><td align="center">  0,3 </td><td align="center">  0,3 </td><td align="center">  0,2 </td><td align="center">  0,2 </td></tr><tr><th>  Bin√§rcode </th><td align="center">  0 </td><td align="center">  10 </td><td align="center">  110 </td><td align="center">  1110 </td></tr></tbody></table></div>  Wir komprimieren den Stream unter der Annahme, dass wir am Ende 8 Bits f√ºr jedes Zeichen ausgeben.  Ohne Komprimierung eines Zeichens w√§ren 24 Bits erforderlich.  Wenn jedes Zeichen durch seinen Code ersetzt wird, erhalten wir Einsparungen! <br><br>  Der erste Schritt ist das Codieren des Zeichens <b>e</b> , das 10 ist, und des zweiten Zeichens <b>a</b> , das (nicht mathematisch) hinzugef√ºgt wird: [10] [0] und schlie√ülich des dritten Zeichens <b>t</b> , das unseren endg√ºltigen komprimierten Bitstrom gleich macht [10] [0] [1110] oder <b>1001110</b> , f√ºr die nur 7 Bits erforderlich sind (3,4-mal weniger Speicherplatz als im Original). <br><br>  Bitte beachten Sie, dass jeder Code ein eindeutiger Code mit einem Pr√§fix sein muss.  <a href="https://en.wikipedia.org/wiki/Huffman_coding">Der Huffman-Algorithmus</a> hilft beim Auffinden dieser Zahlen.  Obwohl diese Methode nicht fehlerfrei ist, gibt es Videocodecs, die diese algorithmische Methode zur Komprimierung anbieten. <br><br>  Sowohl der Codierer als auch der Decodierer m√ºssen Zugriff auf die Symboltabelle mit ihren Bin√§rcodes haben.  Daher ist es auch notwendig, eine Tabelle in der Eingabe zu senden. <br><br><h3>  Arithmetische Codierung </h3><br>  Angenommen, wir haben einen Strom von Zeichen: <b>a</b> , <b>e</b> , <b>r</b> , <b>s</b> und <b>t</b> , und ihre Wahrscheinlichkeit wird durch diese Tabelle dargestellt. <div class="scrollable-table"><table><tbody><tr><th></th><th>  a </th><th>  e </th><th>  r </th><th>  s </th><th>  t </th></tr><tr><th>  Wahrscheinlichkeit </th><td align="center">  0,3 </td><td align="center">  0,3 </td><td align="center">  0,15 </td><td align="center">  0,05 </td><td align="center">  0,2 </td></tr></tbody></table></div>  Mit dieser Tabelle erstellen wir Bereiche, die alle m√∂glichen Zeichen enthalten, sortiert nach der gr√∂√üten Zahl. <br><br><img width="713" height="86" src="https://habrastorage.org/webt/v2/et/wv/v2etwv7gqr0imlymbanupu6k3fw.png" alt="Bild"><br><br>  Codieren wir nun einen Stream mit drei Zeichen: <b>eat</b> . <br><br>  W√§hlen Sie zun√§chst das erste Zeichen <b>e aus</b> , das sich im Unterbereich von 0,3 bis 0,6 (ohne) befindet.  Wir nehmen diesen Unterbereich und teilen ihn wieder in die gleichen Proportionen wie zuvor, jedoch bereits f√ºr diesen neuen Bereich. <br><br><img width="731" height="86" src="https://habrastorage.org/webt/fv/ra/lx/fvralxpih4rt9d6zowmweah5r8m.png" alt="Bild"><br><br>  Lassen Sie uns unseren <b>Eat-</b> Stream weiter codieren.  Nun nehmen wir das zweite Zeichen <b>a</b> , das sich in dem neuen Unterbereich von 0,3 bis 0,39 befindet, und dann nehmen wir unser letztes Zeichen <b>t</b> und wiederholen denselben Vorgang erneut, um das letzte Unterband von 0,354 bis 0,372 zu erhalten. <br><br><img width="735" height="388" src="https://habrastorage.org/webt/eh/bj/nz/ehbjnz3uckrkkfoei5bt2gqh1zk.png" alt="Bild"><br><br>  Wir m√ºssen nur eine Zahl im letzten Unterbereich von 0,354 bis 0,372 ausw√§hlen.  Lassen Sie uns 0,36 w√§hlen (aber Sie k√∂nnen jede andere Zahl in diesem Unterbereich w√§hlen).  Nur mit dieser Nummer k√∂nnen wir unseren urspr√ºnglichen Fluss wiederherstellen.  Es ist, als w√ºrden wir eine Linie innerhalb von Bereichen zeichnen, um unseren Stream zu kodieren. <br><br><img width="712" height="231" src="https://habrastorage.org/webt/a2/y2/ye/a2y2yerdhz8bied1n-4o8pqfiic.png" alt="Bild"><br><br>  Der umgekehrte Vorgang (also das <i>Dekodieren</i> ) ist genauso einfach: Mit unserer Zahl 0.36 und unserem Anfangsbereich k√∂nnen wir den gleichen Vorgang starten.  Aber jetzt zeigen wir unter Verwendung dieser Nummer den Stream, der unter Verwendung dieser Nummer codiert wurde. <br><br>  Beim ersten Bereich stellen wir fest, dass unsere Nummer einem Slice entspricht, daher ist dies unser erstes Zeichen.  Jetzt teilen wir uns wieder dieses Teilband und f√ºhren den gleichen Vorgang wie zuvor durch.  Hier sehen Sie, dass 0,36 dem Zeichen <b>a entspricht</b> , und nach Wiederholung des Vorgangs gelangen wir zum letzten Zeichen <b>t</b> (das unseren urspr√ºnglich codierten Stream <b>eat bildet</b> ). <br><br>  Sowohl der Codierer als auch der Decodierer m√ºssen eine Tabelle mit Symbolwahrscheinlichkeiten haben, daher ist es erforderlich, diese in den Eingabedaten zu senden. <br><br>  Ziemlich elegant, nicht wahr?  Jemand, der sich diese L√∂sung ausgedacht hat, war verdammt schlau.  Einige Video-Codecs verwenden diese Technik (oder bieten sie auf jeden Fall als Option an). <br><br>  Die Idee ist, einen verlustfreien quantisierten Bitstrom zu komprimieren.  Sicherlich gibt es in diesem Artikel nicht jede Menge Details, Gr√ºnde, Kompromisse usw.  Wenn Sie jedoch Entwickler sind, sollten Sie mehr wissen.  Neue Codecs versuchen, andere Entropiecodierungsalgorithmen wie <b>ANS zu verwenden</b> . <br><br><h3>  6-stufiges Bitstream-Format </h3><br>  Nach alledem m√ºssen die komprimierten Frames im Rahmen der durchgef√ºhrten Schritte entpackt werden.  Der Decoder muss ausdr√ºcklich √ºber die vom Encoder getroffenen Entscheidungen informiert werden.  Dem Decoder sollten alle erforderlichen Informationen zur Verf√ºgung gestellt werden: Bittiefe, Farbraum, Aufl√∂sung, Vorhersageinformationen (Bewegungsvektoren, Richtungs-INTER-Vorhersage), Profil, Pegel, Bildrate, Bildtyp, Bildnummer und vieles mehr. <br><br>  Wir werden uns den <b>H.264-</b> Bitstream ansehen.  Unser erster Schritt besteht darin, einen minimalen H.264-Bitstream zu erstellen (FFmpeg f√ºgt standardm√§√üig alle Codierungsparameter wie <b>SEI NAL hinzu</b> - etwas weiter werden wir herausfinden, was es ist).  Wir k√∂nnen dies mit unserem eigenen Repository und FFmpeg tun. <br><br> <code>./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264</code> <br> <br>  Dieser Befehl generiert einen unformatierten <b>H.264-</b> Bitstream mit einem Frame und einer Aufl√∂sung von 64 x 64 mit dem Farbraum <b>YUV420</b> .  Das folgende Bild wird als Rahmen verwendet. <br><br><img width="64" height="64" src="https://habrastorage.org/webt/dk/94/2b/dk942bnujajisjouk6zpwy9p5i8.png" alt="Bild"><br><br><h3>  H.264-Bitstream </h3><br>  Der <b>AVC-Standard</b> ( <b>H.264</b> ) definiert, dass Informationen in Makrorahmen (im Verst√§ndnis des Netzwerks) gesendet werden, die als <b>NAL bezeichnet werden</b> (dies ist eine solche Ebene der Netzwerkabstraktion).  Das Hauptziel von NAL ist die Bereitstellung einer "netzwerkfreundlichen" Videopr√§sentation.  Dieser Standard sollte auf Fernsehger√§ten (basierend auf Streams) und im Internet (basierend auf Paketen) funktionieren. <br><br><img width="484" height="41" src="https://habrastorage.org/webt/rd/8r/ag/rd8rag_9adq5fnhkh3bp1l84ljw.png" alt="Bild"><br><br>  Es gibt eine Synchronisationsmarke zum Definieren der Grenzen von NAL-Elementen.  Jeder Synchronisationsmarker enth√§lt den Wert <nobr><b>0x00 0x00 0x01</b></nobr> mit Ausnahme des allerersten, n√§mlich <nobr><b>0x00 0x00 0x00 0x01.</b></nobr>  Wenn wir <b>Hexdump</b> f√ºr den generierten H.264-Bitstream ausf√ºhren, identifizieren wir mindestens drei NAL-Muster am Anfang der Datei. <br><br><img width="451" height="250" src="https://habrastorage.org/webt/ow/vk/uj/owvkujuxd9qw-yft0b2tecxp3oq.png" alt="Bild"><br><br>  Wie bereits erw√§hnt, muss der Decoder nicht nur die Bilddaten kennen, sondern auch die Details des Videos, des Rahmens, der Farbe, der verwendeten Parameter und vieles mehr.  Das erste Byte jeder NAL definiert ihre Kategorie und ihren Typ. <div class="scrollable-table"><table><tbody><tr><th>  NAL-Typenkennung </th><th>  Beschreibung </th></tr><tr><td align="center">  0 </td><td>  Unbekannter Typ </td></tr><tr><td align="center">  1 </td><td>  Codiertes Bildfragment ohne IDR </td></tr><tr><td align="center">  2 </td><td>  Coded Slice <b>Ein</b> Datenabschnitt </td></tr><tr><td align="center">  3 </td><td>  Coded Slice Data Abschnitt <b>B</b> </td></tr><tr><td align="center">  4 </td><td>  <b>C-</b> codierter Schnittdatenabschnitt </td></tr><tr><td align="center">  5 </td><td>  Codiertes IDR-Fragment eines IDR-Bildes </td></tr><tr><td align="center">  6 </td><td>  Zus√§tzliche Informationen zur SEI-Erweiterung </td></tr><tr><td align="center">  7 </td><td>  SPS-Sequenzparametersatz </td></tr><tr><td align="center">  8 </td><td>  PPS-Bildparametersatz </td></tr><tr><td align="center">  9 </td><td>  Zugriffsbegrenzer </td></tr><tr><td align="center">  10 </td><td>  Ende der Sequenz </td></tr><tr><td align="center">  11 </td><td>  Ende des Streams </td></tr><tr><td align="center">  ... </td><td>  ... </td></tr></tbody></table></div>  Normalerweise ist der erste NAL-Bitstrom <b>SPS</b> .  Diese Art von NAL ist f√ºr das Melden allgemeiner Codierungsvariablen wie Profil, Ebene, Aufl√∂sung und mehr verantwortlich. <br><br>  Wenn wir das erste Synchronisationstoken √ºberspringen, k√∂nnen wir das erste Byte dekodieren, um herauszufinden, welcher NAL-Typ der erste ist. <br><br>  Das erste Byte nach der Synchronisationsmarke ist beispielsweise <i>01100111</i> , wobei sich das erste Bit ( <i>0</i> ) im Feld f <i>orbidden_zero_bit</i> befindet.  Die n√§chsten 2 Bits ( <i>11</i> ) <i>teilen</i> uns das Feld <i>nal_ref_idc mit,</i> das angibt, ob dieses NAL ein Referenzfeld ist oder nicht.  Die verbleibenden 5 Bits ( <i>00111</i> ) <i>teilen</i> uns das Feld <i>nal_unit_type mit.</i> In diesem Fall handelt es sich um einen SPS ( <i>7</i> ) -NAL-Block. <br><br>  Das zweite Byte ( <i>bin√§r</i> = <i>01100100</i> , <i>hex</i> = <i>0x64</i> , <i>dez</i> = <i>100</i> ) in der SPS-NAL ist das Feld <i>profile_idc,</i> das das vom Encoder verwendete Profil anzeigt.  In diesem Fall wurde ein begrenztes hohes Profil verwendet (d. H. Ein hohes Profil ohne Unterst√ºtzung f√ºr ein bidirektionales B-Segment). <br><br><img src="https://habrastorage.org/webt/xo/zt/wl/xoztwl_amtveehtmceuijr3wdsm.png" alt="Bild"><br><br>  Wenn wir uns mit der Spezifikation des <b>H.264-</b> Bitstroms f√ºr SPS NAL vertraut machen, finden wir viele Werte f√ºr den Parameternamen, die Kategorie und die Beschreibung.  Schauen <i>wir</i> <i>uns beispielsweise die Felder</i> <i>pic_width_in_mbs_minus_1</i> und <i>pic_height_in_map_units_minus_1 an</i> . <div class="scrollable-table"><table><tbody><tr><th>  Parametername </th><th>  Kategorie </th><th>  Beschreibung </th></tr><tr><td>  pic_width_in_mbs_minus_1 </td><td align="center">  0 </td><td align="center">  ue (v) </td></tr><tr><td>  pic_height_in_map_units_minus_1 </td><td align="center">  0 </td><td align="center">  ue (v) </td></tr></tbody></table></div>  Wenn wir mit den Werten dieser Felder einige mathematische Operationen durchf√ºhren, erhalten wir die Erlaubnis.  Sie k√∂nnen sich <nobr>1920 x 1080</nobr> vorstellen, <i>wenn</i> Sie <i>pic_width_in_mbs_minus_1</i> mit einem Wert von <nobr>119 ((119 + 1) * macroblock_size = 120 * 16 = 1920) verwenden</nobr> .  Auch hier wurde Platz gespart, anstatt 1920 mit 119 zu programmieren. <br><br>  Wenn Sie unser erstelltes Video weiterhin in bin√§rer Form √ºberpr√ºfen (zum Beispiel: <nobr><i>xxd -b -c 11 v / minimal_yuv420.h264</i></nobr> ), k√∂nnen Sie zur letzten NAL gehen, die das Frame selbst ist. <br><br><img src="https://habrastorage.org/webt/fc/wt/ru/fcwtruw1ol7zkpc4osveloks_ri.png" alt="Bild"><br><br>  Hier sehen wir die ersten 6-Byte-Werte: <i>01100101 10001000 10000100 00000000 00100001 11111111</i> .  Da bekannt ist, dass das erste Byte den Typ der NAL angibt, handelt es sich in diesem Fall ( <i>00101</i> ) um ein IDR-Fragment (5), das dann weiter untersucht werden kann: <br><br><img width="604" height="458" src="https://habrastorage.org/webt/ip/67/gh/ip67gh7uuxizfnt_5oko5oowohk.png" alt="Bild"><br><br>  Unter Verwendung der Spezifikationsinformationen ist es m√∂glich, den Fragmenttyp ( <i>slice_type</i> ) und die <i>Bildnummer</i> ( <i>frame_num</i> ) unter anderen wichtigen Feldern zu decodieren. <br><br>  Um die Werte einiger Felder ( <i>ue</i> ( <i>v</i> ), <i>me</i> ( <i>v</i> ), <i>se</i> ( <i>v</i> ) oder <i>te</i> ( <i>v</i> )) zu erhalten, m√ºssen wir das Fragment mit einem speziellen Decoder decodieren, der auf <a href="https://ru.wikipedia.org/wiki/%25D0%25AD%25D0%25BA%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D1%2586%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BA%25D0%25BE%25D0%25B4_%25D0%2593%25D0%25BE%25D0%25BB%25D0%25BE%25D0%25BC%25D0%25B1%25D0%25B0">dem Golomb-Exponentialcode</a> basiert.  Diese Methode ist sehr effektiv f√ºr die Codierung variabler Werte, insbesondere wenn viele Standardwerte vorhanden sind. <br><br>  Die Werte f√ºr <i>slice_type</i> und <i>frame_num</i> in diesem Video sind 7 (I-Fragment) und 0 (erstes Bild). <br><br>  Bitstream kann als Protokoll betrachtet werden.  Wenn Sie mehr √ºber den Bitstream erfahren m√∂chten, lesen Sie die <b>ITU H.264-</b> Spezifikation.  Hier ist ein Makro, das zeigt, wo sich die Bilddaten befinden ( <b>YUV</b> in komprimierter Form). <br><br><img src="https://habrastorage.org/webt/sh/ka/sf/shkasfvpn80vva4jfeoxk2be1pe.png" alt="Bild"><br><br>  Sie k√∂nnen andere Bitstreams wie <b>VP9</b> , <b>H.265</b> ( <b>HEVC</b> ) oder sogar unseren neuen besten <b>AV1-</b> Bitstream erkunden.  Sind sie alle gleich?  Nein, aber es ist viel einfacher, den Rest zu verstehen, wenn man sich mit mindestens einem befasst hat. <blockquote><h4>  Willst du √ºben?  Entdecken Sie den H.264-Bitstream </h4><br>  Sie k√∂nnen Einzelbildvideos generieren und mithilfe von MediaInfo den <b>H.264-</b> Bitstream untersuchen.  Tats√§chlich hindert Sie nichts daran, sich den Quellcode anzusehen, der den <b>H.264</b> ( <b>AVC</b> ) -Bitstream analysiert. <br><br><img src="https://habrastorage.org/webt/b0/n5/7v/b0n57v_h_gb2l2m0evkrhkukr_s.png" alt="Bild"><br><br>  Zum √úben k√∂nnen Sie Intel Video Pro Analyzer verwenden (ich habe bereits gesagt, dass das Programm kostenpflichtig ist, aber gibt es eine kostenlose Testversion mit einem Limit von 10 Bildern?). <br><br><img src="https://habrastorage.org/webt/l9/63/ni/l963nix2wqornscrgdpzechdjq8.png" alt="Bild"></blockquote><h3>  R√ºckblick </h3><br>  Beachten Sie, dass viele moderne Codecs dasselbe Modell verwenden, das sie gerade gelernt haben.  Schauen wir uns hier das Blockdiagramm des <b>Thor-</b> Video-Codecs an.  Es enth√§lt alle Schritte, die wir unternommen haben.  In diesem Beitrag geht es darum, die Neuerungen und Dokumentationen in diesem Bereich zumindest besser zu verstehen. <br><br><img width="574" height="507" src="https://habrastorage.org/webt/7q/go/1s/7qgo1so0_mld4wglqvhan3ttwei.png" alt="Bild"><br><br>  Bisher wurde gesch√§tzt, dass 139 GB Festplattenspeicher erforderlich sind, um eine einst√ºndige Videodatei mit 720p und 30 fps Qualit√§t zu speichern.  Wenn Sie die in diesem Artikel beschriebenen Methoden verwenden (Inter-Frame- und interne Vorhersagen, Konvertierung, Quantisierung, Entropie-Codierung usw.), k√∂nnen Sie erreichen (vorausgesetzt, wir geben 0,031 Bit pro Pixel aus), dass das Video eine recht zufriedenstellende Qualit√§t aufweist, die es in Anspruch nimmt Nur 367,82 MB, nicht 139 GB Arbeitsspeicher. <br><br><h3>  Wie erreicht H.265 ein besseres Kompressionsverh√§ltnis als H.264? </h3><br>  Da Sie nun mehr √ºber die Funktionsweise von Codecs wissen, ist es einfacher zu verstehen, wie neue Codecs mit weniger Bits eine h√∂here Aufl√∂sung bieten k√∂nnen. <br><br>  Wenn Sie <b>AVC</b> und <b>HEVC vergleichen</b> , sollten Sie nicht vergessen, dass dies fast immer eine Wahl zwischen einer h√∂heren CPU-Last und einem h√∂heren Komprimierungsverh√§ltnis ist. <br><br>  <b>HEVC</b> bietet mehr Optionen f√ºr Abschnitte (und Unterabschnitte) als <b>AVC</b> , mehr Anweisungen f√ºr interne Vorhersagen, verbesserte Entropiecodierung und vieles mehr.  All diese Verbesserungen machten <b>H.265</b> in der Lage, 50% mehr als <b>H.264</b> zu komprimieren. <br><br><img src="https://habrastorage.org/webt/m3/jj/x8/m3jjx88-ppzrakex1sjxlinwjx8.png" alt="Bild"><br><br><hr><br><h3>  Teil I: <a href="https://habr.com/ru/company/edison/blog/481418/">Video- und Bildgrundlagen</a> </h3><br><br><hr><br> <a href="https://habr.com/ru/company/edison/blog/485460/"><img align="right" width="404" height="150" src="https://habrastorage.org/webt/2b/9i/gm/2b9igmgpbxunecpetjj6hhqsa9m.png"></a> <br clear="left"><h4>  Lesen Sie auch den Blog <br>  EDISON Unternehmen: </h4><br>  <a href="https://habr.com/ru/company/edison/blog/485460/"><b>20 Bibliotheken f√ºr</b></a> <a href="https://habr.com/ru/company/edison/blog/485460/"><b><br></b></a>  <a href="https://habr.com/ru/company/edison/blog/485460/"><b>spektakul√§re iOS-Anwendung</b></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de480430/">https://habr.com/ru/post/de480430/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de480412/index.html">Alexei Ragozin √ºber neue und alte Funktionen von Java Flight Recorder in OpenJDK 11 beim Treffen von jug.msk.ru</a></li>
<li><a href="../de480414/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 393 (9. - 15. Dezember 2019)</a></li>
<li><a href="../de480420/index.html">Interview mit Matthew "Oki" O'Connor √ºber Atherosklerose und ihre Behandlung</a></li>
<li><a href="../de480422/index.html">Google Style Guide in C ++. Teil 1</a></li>
<li><a href="../de480428/index.html">Die Methode der einfachsten Abk√ºrzung. Alphabet und Schriftart f√ºr sie.</a></li>
<li><a href="../de480432/index.html">Das Problem des ersten Betrachters oder die schwierige Konvertierung von WebRTC-Videostreams in HLS</a></li>
<li><a href="../de480438/index.html">Digitale Veranstaltungen in Moskau vom 16. bis 22. Dezember</a></li>
<li><a href="../de480440/index.html">Digitale Veranstaltungen in St. Petersburg vom 16. bis 22. Dezember</a></li>
<li><a href="../de480444/index.html">Habra Detektiv: 24 Stunden aus dem Leben von 24 Publikationen</a></li>
<li><a href="../de480446/index.html">Schreiben des Grafana-Reverse-Proxys auf Go</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>