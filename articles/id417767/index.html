<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⭐️ 💎 🔟 Analisis sentimen teks menggunakan jaringan saraf convolutional 🎫 🐸 👩🏿‍🤝‍👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bayangkan Anda memiliki paragraf teks. Mungkinkah untuk memahami emosi apa yang dibawa teks ini: kegembiraan, kesedihan, kemarahan? Kamu bisa. Kami me...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analisis sentimen teks menggunakan jaringan saraf convolutional</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/417767/"><img src="https://habrastorage.org/webt/2u/l3/lw/2ul3lwsbyobovjnol2g_cbvrghi.gif"><br><br>  Bayangkan Anda memiliki paragraf teks.  Mungkinkah untuk memahami emosi apa yang dibawa teks ini: kegembiraan, kesedihan, kemarahan?  Kamu bisa.  Kami menyederhanakan tugas kami dan akan mengklasifikasikan emosi sebagai positif atau negatif, tanpa spesifikasi.  Ada banyak cara untuk mengatasi masalah ini, dan salah satunya adalah <b>jaringan saraf convolutional</b> (Convolutional Neural Networks).  CNN pada awalnya dikembangkan untuk pemrosesan gambar, tetapi mereka berhasil mengatasi tugas-tugas di bidang pemrosesan kata otomatis.  Saya akan memperkenalkan Anda pada analisis biner dari nada suara teks-teks berbahasa Rusia menggunakan jaringan saraf convolutional, di mana representasi vektor kata-kata dibentuk berdasarkan model <b>Word2Vec yang</b> terlatih. <br><br>  Artikel ini bersifat ikhtisar, saya menekankan komponen praktis.  Dan saya ingin segera memperingatkan Anda bahwa keputusan yang dibuat pada setiap tahap mungkin tidak optimal.  Sebelum membaca, saya sarankan Anda membiasakan diri dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel pengantar</a> tentang penggunaan CNN dalam tugas pemrosesan bahasa alami, serta membaca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">materi</a> tentang metode representasi vektor kata-kata. <br><a name="habracut"></a><br><h2>  Arsitektur </h2><br>  Arsitektur CNN yang dipertimbangkan didasarkan pada pendekatan [1] dan [2].  Pendekatan [1], yang menggunakan ansambel jaringan konvolusional dan berulang, pada kompetisi tahunan terbesar dalam linguistik komputer SemEval-2017 mengambil tempat pertama [3] dalam lima nominasi dalam tugas untuk analisis nada suara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tugas 4</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20a/058/4aa/20a0584aa0d0a5a6c8108af970c896fa.png"><br>  <i>Gambar 1. Arsitektur CNN [2].</i> <br><br>  Input CNN (Gbr. 1) adalah matriks dengan ketinggian tetap <i>n</i> , di mana setiap baris adalah pemetaan vektor token ke ruang fitur dimensi <i>k</i> .  Alat semantik distribusi seperti Word2Vec, Glove, FastText, dll. Sering digunakan untuk membentuk ruang fitur. <br><br>  Pada tahap pertama, matriks input diproses oleh lapisan konvolusi.  Sebagai aturan, filter memiliki lebar tetap yang sama dengan dimensi ruang atribut, dan hanya satu parameter yang dikonfigurasi untuk ukuran filter - tinggi <i>h</i> .  Ternyata <i>h</i> adalah ketinggian garis yang berdekatan yang dipertimbangkan bersama oleh filter.  Dengan demikian, dimensi matriks fitur keluaran untuk setiap filter bervariasi tergantung pada ketinggian filter ini <i>h</i> dan tinggi matriks asli <i>n</i> . <br><br>  Selanjutnya, peta fitur yang diperoleh pada output setiap filter diproses oleh lapisan subsampling dengan fungsi pemadatan tertentu (penyatuan 1-maks dalam gambar), yaitu.  mengurangi dimensi peta fitur yang dihasilkan.  Dengan demikian, informasi yang paling penting diekstraksi untuk setiap konvolusi, terlepas dari posisinya dalam teks.  Dengan kata lain, untuk tampilan vektor yang digunakan, kombinasi lapisan konvolusi dan lapisan sub-sampling memungkinkan untuk mengekstrak <i>n-</i> gram paling signifikan dari teks. <br><br>  Setelah ini, peta fitur dihitung pada output dari setiap lapisan sub-sampling digabungkan menjadi satu vektor fitur umum.  Ia diumpankan ke input lapisan tersembunyi, yang terhubung penuh, dan kemudian diumpankan ke lapisan keluaran jaringan saraf, tempat label kelas akhir dihitung. <br><br><h2>  Data Pelatihan </h2><br>  Untuk pelatihan, saya memilih <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kumpulan teks pendek oleh Yulia Rubtsova</a> , yang dibentuk berdasarkan pesan-pesan berbahasa Rusia dari Twitter [4].  Ini berisi 114 991 tweet positif, 111 923 negatif, serta basis tweet yang tidak terisi dengan volume 17 639 674 pesan. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment">#   n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount'] data_positive = pd.read_csv('data/positive.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) data_negative = pd.read_csv('data/negative.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) #    sample_size = min(data_positive.shape[0], data_negative.shape[0]) raw_data = np.concatenate((data_positive['text'].values[:sample_size], data_negative['text'].values[:sample_size]), axis=0) labels = [1] * sample_size + [0] * sample_size</span></span></code> </pre> <br>  Sebelum pelatihan, teks-teks tersebut melewati proses pendahuluan: <br><br><ul><li>  dilemparkan ke huruf kecil; <br></li><li>  penggantian "e" dengan "e"; <br></li><li>  Penggantian tautan ke token “URL”; <br></li><li>  penggantian penyebutan pengguna dengan token USER; <br></li><li>  menghapus tanda baca. <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = text.lower().replace(<span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>) text = re.sub(<span class="hljs-string"><span class="hljs-string">'((www\.[^\s]+)|(https?://[^\s]+))'</span></span>, <span class="hljs-string"><span class="hljs-string">'URL'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'@[^\s]+'</span></span>, <span class="hljs-string"><span class="hljs-string">'USER'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() data = [preprocess_text(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> raw_data]</code> </pre> <br>  Selanjutnya, saya membagi set data menjadi pelatihan dan menguji sampel dalam rasio 4: 1. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><h2>  Tampilan kata-kata vektor </h2><br>  Data input dari jaringan saraf convolutional adalah matriks dengan tinggi tetap <i>n</i> , di mana setiap baris adalah pemetaan vektor dari suatu kata ke dalam ruang fitur dimensi <i>k</i> .  Untuk membentuk lapisan embedding jaringan saraf, saya menggunakan utilitas semantik distributif Word2Vec yang dirancang untuk memetakan makna semantik kata ke dalam ruang vektor.  Word2Vec menemukan hubungan antara kata-kata dengan mengasumsikan bahwa kata-kata yang berhubungan secara semantik ditemukan dalam konteks yang sama.  Anda dapat membaca lebih lanjut tentang Word2Vec di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel asli</a> , dan juga di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Karena tweet ditandai oleh tanda baca dan emotikon penulis, menentukan batas kalimat menjadi tugas yang agak memakan waktu.  Dalam karya ini, saya berasumsi bahwa setiap tweet hanya berisi satu kalimat. <br><br>  Basis tweet yang tidak terisi disimpan dalam format SQL dan berisi lebih dari 17,5 juta catatan.  Untuk kenyamanan, saya mengubahnya menjadi SQLite menggunakan skrip <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sqlite3 <span class="hljs-comment"><span class="hljs-comment">#  SQLite   conn = sqlite3.connect('mysqlite3.db') c = conn.cursor() with open('data/tweets.txt', 'w', encoding='utf-8') as f: #    for row in c.execute('SELECT ttext FROM sentiment'): if row[0]: tweet = preprocess(row[0]) #      print(tweet, file=f)</span></span></code> </pre> <br>  Kemudian, menggunakan perpustakaan Gensim, saya melatih model Word2Vec dengan parameter berikut: <br><br><ul><li>  <i>size = 200</i> - dimensi ruang atribut; <br></li><li>  <i>window = 5</i> - jumlah kata dari konteks yang dianalisis algoritma; <br></li><li>  <i>min_count = 3</i> - kata itu harus muncul setidaknya tiga kali sehingga model memperhitungkannya. <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec logging.basicConfig(format=<span class="hljs-string"><span class="hljs-string">'%(asctime)s : %(levelname)s : %(message)s'</span></span>, level=logging.INFO) <span class="hljs-comment"><span class="hljs-comment">#      data = gensim.models.word2vec.LineSentence('data/tweets.txt') #   model = Word2Vec(data, size=200, window=5, min_count=3, workers=multiprocessing.cpu_count()) model.save("models/w2v/model.w2v")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/uk/zh/h0/ukzhh0kwiptrhim7tygo1vh5vwq.png"><br>  <i>Gambar 2. Visualisasi kelompok kata-kata yang mirip menggunakan t-SNE.</i> <br><br>  Untuk pemahaman yang lebih rinci tentang pengoperasian Word2Vec pada Gambar.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Gambar</a> 2 menunjukkan visualisasi beberapa kelompok kata-kata serupa dari model terlatih, dipetakan ke dalam ruang dua dimensi menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">algoritma visualisasi t-SNE</a> . <br><br><h2>  Tampilan vektor teks </h2><br><img src="https://habrastorage.org/webt/er/de/wc/erdewcunafpymiafxeqgby-8-h8.png"><br>  <i>Gambar 3. Distribusi panjang teks.</i> <br><br>  Pada langkah berikutnya, setiap teks dipetakan ke array pengenal token.  Saya memilih dimensi vektor teks <i>s = 26</i> , karena pada nilai ini 99,71% dari semua teks dalam tubuh yang terbentuk tertutup sepenuhnya (Gbr. 3).  Jika selama analisis jumlah kata dalam tweet melebihi ketinggian matriks, kata-kata yang tersisa dibuang dan tidak diperhitungkan dalam klasifikasi.  Dimensi akhir dari matriks proposal adalah <i>s × d = 26 × 200</i> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences <span class="hljs-comment"><span class="hljs-comment">#   (    ) SENTENCE_LENGTH = 26 #   NUM = 100000 def get_sequences(tokenizer, x): sequences = tokenizer.texts_to_sequences(x) return pad_sequences(sequences, maxlen=SENTENCE_LENGTH) # C    tokenizer = Tokenizer(num_words=NUM) tokenizer.fit_on_texts(x_train) #        x_train_seq = get_sequences(tokenizer, x_train) x_test_seq = get_sequences(tokenizer, x_test)</span></span></code> </pre> <br><h2>  Jaringan Saraf Konvolusional </h2><br>  Untuk membangun jaringan saraf, saya menggunakan perpustakaan Keras, yang bertindak sebagai add-on tingkat tinggi untuk TensorFlow, CNTK, dan Theano.  Keras memiliki dokumentasi yang sangat baik, serta blog yang mencakup banyak tugas pembelajaran mesin, seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menginisialisasi lapisan penyematan</a> .  Dalam kasus kami, lapisan embedding diprakarsai oleh bobot yang diperoleh dengan mempelajari Word2Vec.  Untuk meminimalkan perubahan pada lapisan penyematan, saya membekukannya pada tahap pertama pelatihan. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.embeddings <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Embedding tweet_input = Input(shape=(SENTENCE_LENGTH,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) tweet_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH, weights=[embedding_matrix], trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)(tweet_input)</code> </pre> <br>  Dalam arsitektur yang dikembangkan, filter dengan ketinggian <i>h = (2, 3, 4, 5)</i> digunakan, yang dirancang untuk pemrosesan paralel bigrams, trigram, 4-gram dan 5-gram, masing-masing.  Menambahkan 10 lapisan konvolusional ke setiap jaringan saraf untuk setiap ketinggian filter, fungsi aktivasi adalah ReLU.  Rekomendasi untuk menemukan ketinggian optimal dan jumlah filter dapat ditemukan di [2]. <br><br>  Setelah diproses oleh lapisan konvolusi, peta atribut diumpankan ke lapisan subsampling, di mana operasi 1-max-pooling diterapkan pada mereka, sehingga mengekstraksi n-gram paling signifikan dari teks.  Pada tahap berikutnya, mereka bergabung menjadi vektor fitur umum (menggabungkan lapisan), yang dimasukkan ke dalam lapisan yang terhubung sepenuhnya tersembunyi dengan 30 neuron.  Pada tahap terakhir, peta fitur akhir diumpankan ke lapisan output jaringan saraf dengan fungsi aktivasi sigmoidal. <br><br>  Karena jaringan saraf rentan terhadap pelatihan ulang, setelah lapisan embedding dan sebelum lapisan yang terhubung sepenuhnya tersembunyi, saya menambahkan regularisasi dropout dengan probabilitas ejeksi vertex p = 0,2. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, concatenate, Activation, Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.convolutional <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Conv1D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.pooling <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GlobalMaxPooling1D branches = [] <span class="hljs-comment"><span class="hljs-comment">#  dropout- x = Dropout(0.2)(tweet_encoder) for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]: for i in range(filters_count): #    branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x) #    branch = GlobalMaxPooling1D()(branch) branches.append(branch) #    x = concatenate(branches, axis=1) #  dropout- x = Dropout(0.2)(x) x = Dense(30, activation='relu')(x) x = Dense(1)(x) output = Activation('sigmoid')(x) model = Model(inputs=[tweet_input], outputs=[output])</span></span></code> </pre> <br>  Saya mengkonfigurasi model akhir dengan fungsi optimasi Adam (Adaptive Moment Estimation) dan binary cross-entropy sebagai fungsi kesalahan.  Kualitas penggolong dievaluasi dalam hal akurasi rata-rata makro, kelengkapan dan ukuran-f. <br><br><pre> <code class="python hljs">model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[precision, recall, f1]) model.summary()</code> </pre> <br>  Pada tahap pertama pelatihan, lapisan penanaman dibekukan, semua lapisan lainnya dilatih selama 10 era: <br><br><ul><li>  Ukuran kelompok contoh yang digunakan untuk pelatihan adalah 32. <br></li><li>  Ukuran sampel validasi: 25%. <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint checkpoint = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"models/cnn/cnn-frozen-embeddings-{epoch:02d}-{val_f1:.2f}.hdf5"</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_f1'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'max'</span></span>, period=<span class="hljs-number"><span class="hljs-number">1</span></span>) history = model.fit(x_train_seq, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.25</span></span>, callbacks = [checkpoint])</code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">Log</b> <div class="spoiler_text"> <code>Train on 134307 samples, validate on 44769 samples <br> Epoch 1/10 <br> 134307/134307 [==============================] - 221s 2ms/step - loss: 0.5703 - precision: 0.7006 - recall: 0.6854 - f1: 0.6839 - val_loss: 0.5014 - val_precision: 0.7538 - val_recall: 0.7493 - val_f1: 0.7452 <br> Epoch 2/10 <br> 134307/134307 [==============================] - 218s 2ms/step - loss: 0.5157 - precision: 0.7422 - recall: 0.7258 - f1: 0.7263 - val_loss: 0.4911 - val_precision: 0.7413 - val_recall: 0.7924 - val_f1: 0.7602 <br> Epoch 3/10 <br> 134307/134307 [==============================] - 213s 2ms/step - loss: 0.5023 - precision: 0.7502 - recall: 0.7337 - f1: 0.7346 - val_loss: 0.4825 - val_precision: 0.7750 - val_recall: 0.7411 - val_f1: 0.7512 <br> Epoch 4/10 <br> 134307/134307 [==============================] - 215s 2ms/step - loss: 0.4956 - precision: 0.7545 - recall: 0.7412 - f1: 0.7407 - val_loss: 0.4747 - val_precision: 0.7696 - val_recall: 0.7590 - val_f1: 0.7584 <br> Epoch 5/10 <br> 134307/134307 [==============================] - 229s 2ms/step - loss: 0.4891 - precision: 0.7587 - recall: 0.7492 - f1: 0.7473 - val_loss: 0.4781 - val_precision: 0.8014 - val_recall: 0.7004 - val_f1: 0.7409 <br> Epoch 6/10 <br> 134307/134307 [==============================] - 217s 2ms/step - loss: 0.4830 - precision: 0.7620 - recall: 0.7566 - f1: 0.7525 - val_loss: 0.4749 - val_precision: 0.7877 - val_recall: 0.7411 - val_f1: 0.7576 <br> Epoch 7/10 <br> 134307/134307 [==============================] - 219s 2ms/step - loss: 0.4802 - precision: 0.7632 - recall: 0.7568 - f1: 0.7532 - val_loss: 0.4730 - val_precision: 0.7969 - val_recall: 0.7241 - val_f1: 0.7522 <br> Epoch 8/10 <br> 134307/134307 [==============================] - 215s 2ms/step - loss: 0.4769 - precision: 0.7644 - recall: 0.7605 - f1: 0.7558 - val_loss: 0.4680 - val_precision: 0.7829 - val_recall: 0.7542 - val_f1: 0.7619 <br> Epoch 9/10 <br> 134307/134307 [==============================] - 227s 2ms/step - loss: 0.4741 - precision: 0.7657 - recall: 0.7663 - f1: 0.7598 - val_loss: 0.4672 - val_precision: 0.7695 - val_recall: 0.7784 - val_f1: 0.7682 <br> Epoch 10/10 <br> 134307/134307 [==============================] - 221s 2ms/step - loss: 0.4727 - precision: 0.7670 - recall: 0.7647 - f1: 0.7590 - val_loss: 0.4673 - val_precision: 0.7833 - val_recall: 0.7561 - val_f1: 0.7636</code> <br> </div></div><br><br>  Kemudian ia memilih model dengan ukuran-F tertinggi pada set data validasi, yaitu.  model yang diperoleh dalam zaman pendidikan kedelapan (F <sub>1</sub> = 0,7791).  Model mencairkan lapisan embedding, setelah itu meluncurkan lima era pelatihan lagi. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-comment"><span class="hljs-comment">#    model.load_weights('models/cnn/cnn-frozen-embeddings-09-0.77.hdf5') #  embedding     model.layers[1].trainable = True #  learning rate adam = optimizers.Adam(lr=0.0001) model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[precision, recall, f1]) model.summary() checkpoint = ModelCheckpoint("models/cnn/cnn-trainable-{epoch:02d}-{val_f1:.2f}.hdf5", monitor='val_f1', save_best_only=True, mode='max', period=1) history_trainable = model.fit(x_train_seq, y_train, batch_size=32, epochs=5, validation_split=0.25, callbacks = [checkpoint])</span></span></code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">Log</b> <div class="spoiler_text"> <code>Train on 134307 samples, validate on 44769 samples <br> Epoch 1/5 <br> 134307/134307 [==============================] - 2042s 15ms/step - loss: 0.4495 - precision: 0.7806 - recall: 0.7797 - f1: 0.7743 - val_loss: 0.4560 - val_precision: 0.7858 - val_recall: 0.7671 - val_f1: 0.7705 <br> Epoch 2/5 <br> 134307/134307 [==============================] - 2253s 17ms/step - loss: 0.4432 - precision: 0.7857 - recall: 0.7842 - f1: 0.7794 - val_loss: 0.4543 - val_precision: 0.7923 - val_recall: 0.7572 - val_f1: 0.7683 <br> Epoch 3/5 <br> 134307/134307 [==============================] - 2018s 15ms/step - loss: 0.4372 - precision: 0.7899 - recall: 0.7879 - f1: 0.7832 - val_loss: 0.4519 - val_precision: 0.7805 - val_recall: 0.7838 - val_f1: 0.7767 <br> Epoch 4/5 <br> 134307/134307 [==============================] - 1901s 14ms/step - loss: 0.4324 - precision: 0.7943 - recall: 0.7904 - f1: 0.7869 - val_loss: 0.4504 - val_precision: 0.7825 - val_recall: 0.7808 - val_f1: 0.7762 <br> Epoch 5/5 <br> 134307/134307 [==============================] - 1924s 14ms/step - loss: 0.4256 - precision: 0.7986 - recall: 0.7947 - f1: 0.7913 - val_loss: 0.4497 - val_precision: 0.7989 - val_recall: 0.7549 - val_f1: 0.7703</code> <br> </div></div><br><br>  Indikator tertinggi <i>F <sub>1</sub> = 76,80%</i> dalam sampel validasi dicapai pada pelatihan era ketiga.  Kualitas model yang terlatih pada data uji adalah <i>F1 = 78,1%</i> . <br><br>  Tabel 1. Kualitas analisis sentimen pada data uji. <br><div class="scrollable-table"><table><tbody><tr><td>  Label kelas <br></td><td>  Akurasi <br></td><td>  Kelengkapan <br></td><td>  F <sub>1</sub> <br></td><td>  Jumlah objek <br></td></tr><tr><td>  Negatif <br></td><td>  0,78194 <br></td><td>  0,78243 <br></td><td>  0,78218 <br></td><td>  22457 <br></td></tr><tr><td>  Positif <br></td><td>  0.78089 <br></td><td>  0.78040 <br></td><td>  0.78064 <br></td><td>  22313 <br></td></tr><tr><td>  rata-rata / total <br></td><td>  0,78142 <br></td><td>  0,78142 <br></td><td>  0,78142 <br></td><td>  44770 <br></td></tr></tbody></table></div><br><h2>  Hasil </h2><br>  Sebagai solusi dasar, saya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">melatih</a> classifier Bayes naif dengan model distribusi multinomial, hasil perbandingan disajikan dalam tabel.  2. <br><br>  Tabel 2. Perbandingan kualitas analisis nada suara. <br><div class="scrollable-table"><table><tbody><tr><td>  Pengklasifikasi <br></td><td>  Presisi <br></td><td>  Ingat <br></td><td>  F <sub>1</sub> <br></td></tr><tr><td>  Mnb <br></td><td>  0,7577 <br></td><td>  0,7564 <br></td><td>  0,7560 <br></td></tr><tr><td>  CNN <br></td><td>  <b>0,78142</b> <br></td><td>  <b>0,78142</b> <br></td><td>  <b>0,78142</b> <br></td></tr></tbody></table></div><br>  Seperti yang Anda lihat, kualitas klasifikasi CNN melebihi MNB beberapa persen.  Nilai metrik dapat ditingkatkan lebih banyak lagi jika Anda berupaya mengoptimalkan hyperparameter dan arsitektur jaringan.  Misalnya, Anda dapat mengubah jumlah era pelatihan, memeriksa efektivitas penggunaan berbagai representasi vektor kata dan kombinasinya, pilih jumlah filter dan ketinggiannya, menerapkan pemrosesan teks yang lebih efektif (koreksi kesalahan ketik, normalisasi, stamping), sesuaikan jumlah lapisan dan neuron yang terhubung penuh yang tersembunyi di dalamnya . <br><br>  Kode sumber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tersedia di Github</a> , model CNN dan Word2Vec yang terlatih dapat diunduh di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><h2>  Sumber </h2><br><ol><li>  Cliche M. BB_twtr di SemEval-2017 Tugas 4: Analisis Sentimen Twitter dengan CNN dan LSTMs // Prosiding Lokakarya Internasional ke-11 tentang Evaluasi Semantik (SemEval-2017).  - 2017 .-- S. 573-580. <br></li><li>  Zhang Y., Wallace B. Analisis Sensitivitas (dan Panduan Praktisi untuk) Jaringan Syaraf Konvolusional untuk Klasifikasi Kalimat // arXiv preprint arXiv: 1510.03820.  - 2015. <br></li><li>  Rosenthal S., Farra N., Nakov P. SemEval-2017 tugas 4: Analisis Sentimen di Twitter // Prosiding Lokakarya Internasional ke-11 tentang Evaluasi Semantik (SemEval-2017).  - 2017 .-- S. 502-518. <br></li><li>  Yu V. V. Rubtsova.  Membangun kumpulan teks untuk mengatur pengelompokan nada // Produk dan Sistem Perangkat Lunak, 2015, No. 1 (109), —C.72-78. <br></li><li>  Mikolov T. et al.  Representasi Kata-kata dan Frasa Terdistribusi serta Komposisionalitasnya // Kemajuan dalam Sistem Pemrosesan Informasi Saraf Tiruan.  - 2013 .-- S. 3111-3119. <br></li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id417767/">https://habr.com/ru/post/id417767/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id417755/index.html">Studi: 80% dari 2017 ICO dianggap curang</a></li>
<li><a href="../id417757/index.html">Membuat bot untuk berpartisipasi dalam cangkir mini AI. Pengalaman GPU</a></li>
<li><a href="../id417759/index.html">Jadilah bebek karet saya</a></li>
<li><a href="../id417761/index.html">GitLab bergerak dari Azure ke Google Cloud Platform. Berita Relokasi dan Tanggal Pemeliharaan</a></li>
<li><a href="../id417763/index.html">MVIDroid: tinjauan perpustakaan MVI baru (Model-View-Intent)</a></li>
<li><a href="../id417769/index.html">Desain Memori Pengguna: Cara Desain Untuk Usia</a></li>
<li><a href="../id417771/index.html">Paket ICANN: Perusahaan Menawarkan Model Manajemen DNS Root Server Baru</a></li>
<li><a href="../id417773/index.html">Pemasang Komponen OpenPnP buatan sendiri</a></li>
<li><a href="../id417775/index.html">Mekanisme komisi Bitcoin dan mengapa berteman dengan penambang</a></li>
<li><a href="../id417777/index.html">Weekend Reading: 25 bahan untuk penggemar vinyl pemula</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>