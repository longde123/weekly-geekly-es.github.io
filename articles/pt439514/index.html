<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçü§ù‚Äçüë®üèª üëß üî£ Replica√ß√£o do Tarantool: configura√ß√£o e uso ü§∂üèº üóÑÔ∏è üì∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entro na equipe do Tarantool Core e participo do desenvolvimento de um mecanismo de banco de dados, comunica√ß√£o interna dos componentes do servidor e ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Replica√ß√£o do Tarantool: configura√ß√£o e uso</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/439514/"><img src="https://habrastorage.org/webt/ec/a3/bt/eca3bttl2uu6rg8sz7e9sj3o3us.jpeg"><br><br>  Entro na equipe do Tarantool Core e participo do desenvolvimento de um mecanismo de banco de dados, comunica√ß√£o interna dos componentes do servidor e replica√ß√£o.  E hoje eu vou lhe dizer como a replica√ß√£o funciona. <br><a name="habracut"></a><br><h2>  Sobre replica√ß√£o </h2><br>  Replica√ß√£o √© o processo de fazer c√≥pias de dados de um armazenamento para outro.  Cada c√≥pia √© chamada de r√©plica.  A replica√ß√£o pode ser usada se voc√™ precisar obter um backup, implementar o hot standby ou escalar o sistema horizontalmente.  E para isso √© necess√°rio poder usar os mesmos dados em diferentes n√≥s da rede de computadores do cluster. <br><br>  Classificamos a replica√ß√£o de duas maneiras principais: <br><br><ul><li> <b>Dire√ß√£o: mestre-mestre ou mestre-escravo</b> .  A replica√ß√£o mestre-escravo √© a op√ß√£o mais f√°cil.  Voc√™ tem um n√≥ no qual est√° alterando dados.  Voc√™ traduz essas altera√ß√µes para os outros n√≥s em que s√£o aplicadas.  Com a replica√ß√£o mestre-mestre, s√£o feitas altera√ß√µes em v√°rios n√≥s de uma s√≥ vez.  Nesse caso, cada n√≥ altera seus dados e aplica as altera√ß√µes feitas em outros n√≥s. </li><li>  <b>Modo de opera√ß√£o: ass√≠ncrono ou s√≠ncrono</b> .  A replica√ß√£o s√≠ncrona implica que os dados n√£o ser√£o confirmados e a replica√ß√£o n√£o ser√° confirmada para o usu√°rio at√© que as altera√ß√µes sejam propagadas por pelo menos o n√∫mero m√≠nimo de n√≥s do cluster.  Na replica√ß√£o ass√≠ncrona, confirmar uma transa√ß√£o (confirmar) e interagir com um usu√°rio s√£o dois processos independentes.  Para confirmar dados, √© necess√°rio apenas que eles caiam no log local e somente ent√£o essas altera√ß√µes s√£o transmitidas de alguma forma para outros n√≥s.  Obviamente, a replica√ß√£o ass√≠ncrona tem v√°rios efeitos colaterais por causa disso. </li></ul><br><h2>  Como funciona a replica√ß√£o no Tarantool? </h2><br>  A replica√ß√£o no Tarantool possui v√°rios recursos: <br><br><ul><li>  Ele √© constru√≠do a partir de tijolos b√°sicos, com os quais voc√™ pode criar um cluster de qualquer topologia.  Cada um desses itens de configura√ß√£o b√°sica √© unidirecional, ou seja, voc√™ sempre tem mestre e escravo.  O mestre executa algumas a√ß√µes e gera um log de opera√ß√µes, que √© usado na r√©plica. </li><li>  A replica√ß√£o do Tarantool √© ass√≠ncrona.  Ou seja, o sistema confirma a confirma√ß√£o para voc√™, independentemente de quantas r√©plicas essa transa√ß√£o viu, quanto foi aplicada a si mesma e se acabou sendo feita. </li><li>  Outra propriedade de replica√ß√£o no Tarantool √© que ela √© baseada em linhas.  Tarantool mant√©m um log de opera√ß√µes (WAL).  A opera√ß√£o chega l√° linha por linha, ou seja, quando algumas altera√ß√µes do espa√ßo s√£o alteradas, essa opera√ß√£o √© gravada no log como uma linha.  Depois disso, o processo em segundo plano l√™ essa linha do log e a envia para a r√©plica.  Quantas r√©plicas o mestre possui, tantos processos em segundo plano.  Ou seja, cada processo de replica√ß√£o para diferentes n√≥s do cluster √© executado de forma ass√≠ncrona a partir de outros. </li><li>  Cada n√≥ do cluster possui seu pr√≥prio identificador exclusivo, que √© gerado quando o n√≥ √© criado.  Al√©m disso, o n√≥ tamb√©m possui um identificador no cluster (n√∫mero do membro).  Essa √© uma constante num√©rica atribu√≠da a uma r√©plica quando conectada a um cluster e permanece com a r√©plica durante toda a sua vida √∫til no cluster. </li></ul><br>  Devido √† assincronia, os dados s√£o entregues √†s r√©plicas atrasadas.  Ou seja, voc√™ fez alguma altera√ß√£o, o sistema confirmou a confirma√ß√£o, a opera√ß√£o j√° foi aplicada no mestre, mas nas r√©plicas ela ser√° aplicada com algum atraso, que √© determinado pela velocidade com que o processo de replica√ß√£o em segundo plano l√™ a opera√ß√£o, a envia para a r√©plica e aplica-a . <br><br>  Por esse motivo, h√° uma chance de dados fora de sincronia.  Suponha que tenhamos v√°rios mestres que alteram dados interconectados.  Pode acontecer que as opera√ß√µes usadas n√£o sejam comutativas e se refiram aos mesmos dados, ent√£o dois membros diferentes do cluster ter√£o vers√µes diferentes dos dados. <br><br>  <b>Se a replica√ß√£o no Tarantool √© mestre-escravo unidirecional, como fazer mestre-mestre?</b>  Muito simples: crie outro canal de replica√ß√£o, mas na outra dire√ß√£o.  Voc√™ precisa entender que, no Tarantool, a replica√ß√£o mestre-mestre √© apenas uma combina√ß√£o de dois fluxos de dados que s√£o independentes um do outro. <br><br>  Usando o mesmo princ√≠pio, podemos conectar o terceiro mestre e, como resultado, criar uma rede de malha completa na qual cada r√©plica √© mestre e escravo para todas as outras r√©plicas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nx/z5/b-/nxz5b-f9rzqtmhi7ga4fhq8pdq8.png" width="500"></div><br>  Observe que n√£o apenas as opera√ß√µes iniciadas localmente neste mestre s√£o replicadas, mas tamb√©m aquelas que ele recebeu externamente por meio de protocolos de replica√ß√£o.  Nesse caso, as altera√ß√µes criadas na r√©plica n√∫mero 1 chegar√£o √† r√©plica n√∫mero 3 duas vezes: diretamente e atrav√©s da r√©plica n√∫mero 2. Essa propriedade nos permite criar topologias mais complexas sem usar uma malha completa.  Digamos este. <br><br><img src="https://habrastorage.org/webt/k3/wy/r4/k3wyr4imeqfncmfar65mohmcj24.png"><br><br>  Todos os tr√™s mestres, que juntos formam o n√∫cleo de malha completa do cluster, t√™m uma r√©plica individual anexada.  Como o proxy dos logs √© executado em cada um dos mestres, todos os tr√™s escravos ‚Äúlimpos‚Äù conter√£o todas as opera√ß√µes que foram executadas em qualquer um dos n√≥s do cluster. <br><br>  Essa configura√ß√£o pode ser usada para uma variedade de tarefas.  Voc√™ n√£o pode criar links redundantes entre todos os n√≥s do cluster e, se as r√©plicas forem colocadas nas proximidades, elas ter√£o uma c√≥pia exata do mestre com um atraso m√≠nimo.  E tudo isso √© feito usando o elemento b√°sico de replica√ß√£o mestre-escravo. <br><br><h2>  Opera√ß√µes de cluster de rotulagem </h2><br>  Surge a pergunta: <b>se as opera√ß√µes tiverem proxy entre todos os membros do cluster e chegarem a cada r√©plica v√°rias vezes, como entenderemos qual opera√ß√£o precisa ser executada e quais n√£o?</b>  Isso requer um mecanismo de filtragem.  Cada opera√ß√£o lida no log recebe dois atributos: <br><br><ul><li>  O identificador do servidor no qual esta opera√ß√£o foi iniciada. </li><li>  O n√∫mero de sequ√™ncia da opera√ß√£o no servidor, lsn, que √© seu iniciador.  Cada servidor, ao executar uma opera√ß√£o, atribui um n√∫mero crescente a cada linha de log recebida: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ... Portanto, se sabemos que para um servidor com um determinado identificador, aplicamos a opera√ß√£o com LS 10, as opera√ß√µes com LS 9, 8, 7, 10 que vieram atrav√©s de outros canais de replica√ß√£o n√£o s√£o necess√°rias.  Em vez disso, aplicamos o seguinte: 11, 12 e assim por diante. </li></ul><br><h2>  Status da r√©plica </h2><br>  <b>E como o Tarantool armazena informa√ß√µes sobre as opera√ß√µes que j√° foram aplicadas?</b>  Para fazer isso, h√° um rel√≥gio Vclock - este √© o vetor do √∫ltimo lsn aplicado a cada n√≥ no cluster. <br><br> <code>[lsn <sub>1</sub> , lsn <sub>2</sub> , lsn <sub>n</sub> ]</code> <br> <br>  onde <code>lsn <sub>i</sub></code> √© o n√∫mero da √∫ltima opera√ß√£o conhecida do servidor com o identificador i. <br><br>  O Vclock tamb√©m pode ser chamado de um determinado instant√¢neo de todo o estado do cluster conhecido por esta r√©plica.  Conhecendo o ID do servidor da opera√ß√£o que chegou, isolamos o componente do Vclock local necess√°rio, comparamos o lsn recebido com a opera√ß√£o lsn e decidimos se deve usar essa opera√ß√£o.  Como resultado, as opera√ß√µes iniciadas por um mestre espec√≠fico ser√£o enviadas e aplicadas sequencialmente.  Ao mesmo tempo, os fluxos de trabalho criados em diferentes mestres podem ser misturados entre si devido √† replica√ß√£o ass√≠ncrona. <br><br><h2>  Cria√ß√£o de Cluster </h2><br>  Suponha que tenhamos um cluster composto por dois elementos mestre e escravo, e desejemos conectar uma terceira inst√¢ncia a ele.  Ele possui um UUID exclusivo, mas ainda n√£o h√° identificador de cluster.  Se o Tarantool, que ainda n√£o foi inicializado, deseja ingressar no cluster, ele deve enviar uma opera√ß√£o JOIN para um dos mestres que pode execut√°-lo, ou seja, est√° no modo de leitura / grava√ß√£o.  Em resposta a JOIN, o mestre envia seu instant√¢neo local para a r√©plica de conex√£o.  A r√©plica rola em casa, embora ainda n√£o tenha um identificador.  Agora a r√©plica com um ligeiro atraso est√° sincronizada com o cluster.  Depois disso, o mestre no qual o JOIN foi executado atribui um identificador a essa r√©plica, que √© registrada e enviada √† r√©plica.  Quando um identificador √© atribu√≠do a uma r√©plica, ele se torna um n√≥ completo e, depois disso, pode iniciar a replica√ß√£o de log para o lado. <br><br>  As linhas do di√°rio s√£o enviadas a partir do estado desta r√©plica no momento da solicita√ß√£o do log de replica√ß√£o do mestre - ou seja, do vclock recebido durante o processo JOIN ou do local em que a r√©plica parou anteriormente.  Se a r√©plica cair por algum motivo, na pr√≥xima vez em que se conectar ao cluster, ela n√£o executar√° JOIN, porque j√° possui uma captura instant√¢nea local.  Ela apenas pede todas as opera√ß√µes que ocorreram durante sua aus√™ncia no cluster. <br><br><h2>  Registrar uma r√©plica em um cluster </h2><br>  Para armazenar o estado sobre a estrutura do cluster, √© usado um espa√ßo especial - cluster.  Ele cont√©m os identificadores do servidor no cluster, seus n√∫meros de s√©rie e identificadores exclusivos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/af/0f/hr/af0fhra2ln0s2xcq-qsmeim2xtc.png" width="500"></div><br><br> <code>[1, 'c35b285c-c5b1-4bbe-83b1-b825eb594aa4'] <br> [2, '37b12cb7-d324-4d75-b428-cde92c18e708'] <br> [3, 'b72b1aa6-42a0-4d73-a611-900e44cdd465']</code> <br> <br>  Os identificadores n√£o precisam seguir em ordem, porque os n√≥s podem ser eliminados e adicionados. <br><br>  Aqui est√° a primeira armadilha.  Como regra, os clusters n√£o s√£o coletados por um n√≥: voc√™ executa um determinado aplicativo e ele implementa todo o cluster de uma s√≥ vez.  Mas a replica√ß√£o no Tarantool √© ass√≠ncrona.  E se dois mestres conectarem simultaneamente novos n√≥s e atribu√≠rem identificadores id√™nticos a eles?  Haver√° um conflito. <br><br>  Aqui est√° um exemplo de um JOIN errado e correto: <br><br><img src="https://habrastorage.org/webt/o2/9_/bc/o29_bcjs3dhyllneqlgaczljxys.png"><br><br>  Temos dois mestres e duas r√©plicas que desejam se conectar.  Eles fazem JOINs em diferentes mestres.  Suponha que as r√©plicas obtenham os mesmos identificadores.  Em seguida, a replica√ß√£o entre os mestres e aqueles que conseguem replicar seus logs desmoronar√°, o cluster desmoronar√°. <br><br>  Para impedir que isso aconte√ßa, voc√™ precisa iniciar r√©plicas estritamente em um mestre a qualquer momento.  Para esse fim, Tarantool introduziu esse conceito como l√≠der de inicializa√ß√£o e implementou um algoritmo para escolher esse l√≠der.  Uma r√©plica que deseja se conectar ao cluster primeiro estabelece uma conex√£o com todos os mestres conhecidos na configura√ß√£o transferida.  Em seguida, a r√©plica seleciona aqueles que j√° foram iniciados (ao implantar o cluster, nem todos os n√≥s conseguem ganhar dinheiro cheio).  E a partir deles s√£o selecionados os mestres dispon√≠veis para grava√ß√£o.  No Tarantool, h√° leitura e grava√ß√£o e somente leitura, n√£o podemos registrar no n√≥ somente leitura.  Depois disso, na lista de n√≥s filtrados, selecionamos o que possui o UUID mais baixo. <br><br>  Se usarmos a mesma configura√ß√£o e a mesma lista de servidores em inst√¢ncias n√£o inicializadas conectadas ao cluster, eles selecionar√£o o mesmo mestre, o que significa que JOIN provavelmente ter√° √™xito. <br><br>  A partir daqui, derivamos uma regra: ao conectar r√©plicas a um cluster em paralelo, todas essas r√©plicas devem ter a mesma configura√ß√£o de replica√ß√£o.  Se omitirmos algo em algum lugar, existe a possibilidade de que inst√¢ncias com uma configura√ß√£o diferente sejam iniciadas em mestres diferentes e o cluster n√£o consiga montar. <br><br>  Suponha que estiv√©ssemos enganados, ou o administrador esqueceu de corrigir a configura√ß√£o, ou o Ansible quebrou, e o cluster ainda se desfez.  O que pode testemunhar isso?  Primeiro, as r√©plicas conect√°veis ‚Äã‚Äãn√£o poder√£o criar seus instant√¢neos locais: as r√©plicas n√£o iniciam e relatam erros.  Em segundo lugar, nos mestres nos logs, veremos erros relacionados a conflitos no cluster de espa√ßo. <br><br>  Como resolvemos essa situa√ß√£o?  √â simples: <br><br><ul><li>  Antes de tudo, precisamos validar a configura√ß√£o que definimos para as r√©plicas de conex√£o, porque se n√£o a corrigirmos, todo o resto ser√° in√∫til. </li><li>  Depois disso, limpamos os conflitos no cluster e tiramos uma foto. </li></ul><br>  Agora voc√™ pode tentar inicializar as r√©plicas novamente. <br><br><h2>  Resolu√ß√£o de Conflitos </h2><br>  Ent√£o, criamos um cluster e nos conectamos.  Todos os n√≥s funcionam no modo de assinatura, ou seja, recebem as altera√ß√µes geradas por diferentes mestres.  Como a replica√ß√£o √© ass√≠ncrona, s√£o poss√≠veis conflitos.  Quando voc√™ altera dados simultaneamente em diferentes mestres, r√©plicas diferentes obt√™m c√≥pias diferentes dos dados, porque as opera√ß√µes podem ser aplicadas em uma ordem diferente. <br><br>  Aqui est√° um exemplo de cluster ap√≥s a execu√ß√£o de JOIN: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-o/2j/k2/-o2jk2hjv2dqndshyqta_blcmo8.png" width="500"></div><br>  Temos tr√™s escravos-mestre, os logs s√£o transmitidos entre eles, que s√£o procurados em dire√ß√µes diferentes e s√£o aplicados nos escravos.  Dados fora de sincronia significa que cada r√©plica ter√° seu pr√≥prio hist√≥rico de altera√ß√µes no vclock, porque fluxos de diferentes mestres podem ser misturados.  Por√©m, a ordem das opera√ß√µes nas inst√¢ncias pode variar.  Se nossas opera√ß√µes n√£o forem comutativas, como a opera√ß√£o REPLACE, os dados que recebermos nessas r√©plicas ser√£o diferentes. <br><br>  Um pequeno exemplo.  Suponha que tenhamos dois mestres com vclock = {0,0}.  E ambos realizar√£o duas opera√ß√µes, designadas como op1,1, op1,2, op2,1, op2,2.  Este √© o segundo intervalo de tempo em que cada um dos mestres executou uma opera√ß√£o local: <br><br><img src="https://habrastorage.org/webt/-y/z7/cy/-yz7cyhaozyxkdltf7pptl37oqa.png"><br><br>  Verde indica uma altera√ß√£o no componente vclock correspondente.  Primeiro, os dois mestres alteram seu vclock e, em seguida, o segundo mestre executa outra opera√ß√£o local e aumenta novamente o vclock.  O primeiro mestre recebe a opera√ß√£o de replica√ß√£o do segundo mestre, isso √© indicado pelo n√∫mero vermelho 1 no vclock do primeiro n√≥ do cluster. <br><br><img src="https://habrastorage.org/webt/if/j8/8f/ifj88f4rat3litvsyedit09shw0.png"><br><br>  Ent√£o o segundo mestre recebe a opera√ß√£o do primeiro e do primeiro - a segunda opera√ß√£o do segundo.  E no final, o primeiro mestre executa sua √∫ltima opera√ß√£o e o segundo mestre a recebe. <br><br><img src="https://habrastorage.org/webt/gh/3z/jo/gh3zjoicvoxqd772_pt1m92wfem.png"><br><br>  Vclock no quantum de tempo zero, temos o mesmo - {0,0}.  No √∫ltimo quantum de tempo, tamb√©m temos o mesmo vclock {2,2}, parece que os dados devem ser os mesmos.  Mas a ordem das opera√ß√µes executadas em cada mestre √© diferente.  E se esta √© uma opera√ß√£o REPLACE com valores diferentes para as mesmas chaves?  Ent√£o, apesar do mesmo vclock no final, obteremos vers√µes diferentes dos dados nas duas r√©plicas. <br><br>  Tamb√©m somos capazes de resolver esta situa√ß√£o. <br><br><ul><li>  <b>Registros de sharding</b> .  Primeiro, podemos executar opera√ß√µes de grava√ß√£o n√£o em r√©plicas selecionadas aleatoriamente, mas de alguma forma fragment√°-las.  Eles simplesmente quebraram as opera√ß√µes de grava√ß√£o em diferentes mestres e obtiveram um sistema de consist√™ncia eventual.  Por exemplo, as chaves foram alteradas de 1 para 10 em um mestre e de 11 para 20 em outro - os n√≥s trocar√£o seus logs e obter√£o exatamente os mesmos dados. <br><br>  Sharding implica que temos um certo roteador.  Ele n√£o precisa ser uma entidade separada, o roteador pode fazer parte do aplicativo.  Pode ser um shard que aplica opera√ß√µes de grava√ß√£o a si mesmo ou as transfere para outro mestre de uma maneira ou de outra.  Mas passa de tal maneira que as altera√ß√µes nos valores associados v√£o para algum mestre espec√≠fico: um bloco de valor foi para um mestre, outro bloco para outro mestre.  Nesse caso, as opera√ß√µes de leitura podem ser enviadas para qualquer n√≥ no cluster.  E n√£o se esque√ßa da replica√ß√£o ass√≠ncrona: se voc√™ gravou no mesmo mestre, tamb√©m poder√° precisar ler a mesma. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/de/cq/wv/decqwvuzuaz-yn6t7fbyq2o2sv4.png" width="500"></div></li><li>  <b>Ordena√ß√£o l√≥gica de opera√ß√µes</b> .  Suponha que, de acordo com as condi√ß√µes do problema, voc√™ possa determinar de alguma forma a prioridade da opera√ß√£o.  Digamos, coloque um carimbo de data / hora, ou vers√£o ou algum outro r√≥tulo que nos permita entender qual opera√ß√£o ocorreu fisicamente anteriormente.  Ou seja, estamos falando de uma fonte externa de pedidos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/h8/pt/zl/h8ptzlm1ieiugdnjr66ubi9pxwq.png" width="500"></div><br>  O Tarantool possui um gatilho <code>before_replace</code> que pode ser executado durante a replica√ß√£o.  Nesse caso, n√£o estamos limitados pela necessidade de rotear solicita√ß√µes, podemos envi√°-las para onde quisermos.  Mas, ao executar a replica√ß√£o na entrada do fluxo de dados, temos um gatilho.  Ele l√™ a linha enviada, a compara com a linha que j√° est√° armazenada e decide qual das linhas tem maior prioridade.  Ou seja, o gatilho ignora a solicita√ß√£o de replica√ß√£o ou a aplica, possivelmente com as modifica√ß√µes necess√°rias.  J√° aplicamos essa abordagem, embora ela tamb√©m tenha suas desvantagens.  Primeiro, voc√™ precisa de uma fonte de rel√≥gio externa.  Suponha que uma operadora de um sal√£o de telefonia m√≥vel fa√ßa altera√ß√µes em um assinante.  Para essas opera√ß√µes, voc√™ pode usar o tempo no computador do operador, pois √© improv√°vel que v√°rios operadores fa√ßam altera√ß√µes em um assinante ao mesmo tempo.  As opera√ß√µes podem vir de maneiras diferentes, mas se cada uma delas puder ser atribu√≠da a uma determinada vers√£o, ao passar pelos gatilhos, apenas os relevantes permanecer√£o. <br><br>  A segunda desvantagem do m√©todo: como o gatilho √© aplicado a cada delta que veio pela replica√ß√£o para cada solicita√ß√£o, isso cria uma carga computacional extra.  Mas teremos uma c√≥pia consistente dos dados em uma escala de cluster. </li></ul><br><h2>  Sincronizar </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jr/1n/db/jr1ndb23coit6rz1qeblip_tpy0.png" width="500"></div><br>  Nossa replica√ß√£o √© ass√≠ncrona, ou seja, pela execu√ß√£o da confirma√ß√£o, voc√™ n√£o sabe se esses dados j√° est√£o em algum outro n√≥ do cluster.  Se voc√™ fez uma confirma√ß√£o no mestre, ela foi confirmada e, por algum motivo, o mestre parou imediatamente de funcionar, ent√£o voc√™ n√£o pode ter certeza de que os dados foram salvos em outro lugar.  Para resolver esse problema, o protocolo de replica√ß√£o do Tarantool possui um ACK.  Cada mestre det√©m o conhecimento de qual √∫ltimo ACK veio de cada escravo. <br><br>  O que √© um ACK?  Quando o slave recebe o delta, marcado pelo mestre lsn e seu identificador, em resposta, envia um pacote ACK especial, no qual empacota seu vclock local ap√≥s aplicar esta opera√ß√£o.  Vamos ver como isso pode funcionar. <br><br>  Temos um mestre que realizou 4 opera√ß√µes em si mesmo.  Suponha que em algum momento o escravo escravo tenha recebido as tr√™s primeiras linhas e seu vclock aumentado para {3,0}. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oy/iw/y1/oyiwy1eifo7_jfsb9ylvi1wkija.png" width="500"></div><br>  O ACK ainda n√£o chegou.  Ap√≥s receber essas tr√™s linhas, o slave envia o pacote ACK ao qual costurou seu vclock no momento em que o pacote foi enviado.  Deixe o mestre escravo enviar outra linha no mesmo intervalo de tempo, ou seja, o vclock do escravo aumentou.  Com base nisso, o mestre n ¬∞ 1 sabe com certeza que as tr√™s primeiras opera√ß√µes que ele executou j√° foram aplicadas a esse escravo.  Esses estados s√£o armazenados para todos os escravos com os quais o mestre trabalha; eles s√£o completamente independentes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u1/rx/va/u1rxvazmmssfoizpfecbwf3ceas.png" width="500"></div><br>  E, no final, o escravo responde com um quarto pacote ACK.  Depois disso, o mestre sabe que o escravo est√° sincronizado com ele. <br><br>  Este mecanismo pode ser usado no c√≥digo do aplicativo.  Quando voc√™ confirma uma opera√ß√£o, n√£o reconhece imediatamente o usu√°rio, mas primeiro chama uma fun√ß√£o especial.  Espera que o escravo lsn conhecido pelo mestre seja igual ao lsn do seu mestre no momento em que a confirma√ß√£o √© conclu√≠da.  Portanto, voc√™ n√£o precisa esperar pela sincroniza√ß√£o completa, apenas pelo momento mencionado. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1-/oe/93/1-oe930isjznmsurqj314c2-gue.png" width="500"></div><br>  Suponha que a nossa primeira chamada tenha mudado tr√™s linhas e a segunda tenha mudado uma.  Ap√≥s a primeira chamada, voc√™ deseja garantir que os dados estejam sincronizados.  O estado mostrado acima j√° significa que a primeira chamada foi sincronizada em pelo menos um escravo. <br><br>  Onde exatamente procurar informa√ß√µes sobre isso, consideraremos na pr√≥xima se√ß√£o. <br><br><h2>  Monitoramento </h2><br>  Quando a replica√ß√£o √© s√≠ncrona, o monitoramento √© muito simples: se desmoronar, ser√£o emitidos erros nas suas opera√ß√µes.  E se a replica√ß√£o √© ass√≠ncrona, a situa√ß√£o se torna confusa.  O Mestre responde que tudo est√° bem, funciona, √© aceito, anotado.  Mas, ao mesmo tempo, todas as r√©plicas est√£o inoperantes, os dados n√£o t√™m redund√¢ncia e, se voc√™ perder o mestre, os dados ser√£o perdidos.  Portanto, eu realmente quero monitorar o cluster, entender o que est√° acontecendo com a replica√ß√£o ass√≠ncrona, onde est√£o as r√©plicas e em que estado elas est√£o. <br><br>  Para monitoramento b√°sico, o Tarantool possui uma entidade box.info.  Vale a pena cham√°-lo no console, pois voc√™ ver√° dados interessantes. <br><br><pre> <code class="plaintext hljs">id: 1 uuid: c35b285c-c5b1-4bbe-83b1-b825eb594aa4 lsn : 5 vclock : {2: 1, 1: 5} replication : 1: id: 1 uuid : c35b285c -c5b1 -4 bbe -83b1 - b825eb594aa4 lsn : 5 2: id: 2 uuid : 37 b12cb7 -d324 -4 d75 -b428 - cde92c18e708 lsn : 1 upstream : status : follow idle : 0.30358312401222 peer : lag: 3.6001205444336 e -05 downstream : vclock : {2: 1, 1: 5}</code> </pre> <br>  A m√©trica mais importante √© o ID do <code>id</code> .  Nesse caso, 1 significa que o lsn desse mestre ser√° armazenado na primeira posi√ß√£o em todo o vclock.  Uma coisa muito √∫til.  Se voc√™ tiver um conflito com JOIN, poder√° distinguir um mestre do outro apenas por identificadores exclusivos.  Al√©m disso, quantidades locais incluem quantidades como lsn.  Este √© o n√∫mero da √∫ltima linha que esse mestre executou e gravou em seu log.  No nosso exemplo, o primeiro mestre executou cinco opera√ß√µes.  Vclock √© o estado de opera√ß√µes que ele sabe que se aplicou a si mesmo.  E, finalmente, para o mestre n√∫mero 2, ele executou uma de suas opera√ß√µes de replica√ß√£o. <br><br>  Ap√≥s os indicadores do estado local, voc√™ pode ver o que esta inst√¢ncia sabe sobre o estado da replica√ß√£o de cluster; para isso, h√° uma se√ß√£o de <code>replication</code> .  Ele lista todos os n√≥s do cluster conhecidos pela inst√¢ncia, incluindo ele pr√≥prio.  O primeiro n√≥ possui o identificador 1, id corresponde √† inst√¢ncia atual.  O segundo n√≥ tem o identificador 2, seu lsn 1 corresponde ao lsn que √© gravado no vclock.  Nesse caso, consideramos a replica√ß√£o mestre-mestre, quando o mestre n¬∫ 1 √© o mestre do segundo n√≥ do cluster e seu escravo, ou seja, segue-o. <br><br><ul><li>  A ess√™ncia do <code>upstream</code> .  O atributo <code>status follow</code> significa que o mestre 1 segue o mestre 2. Ocioso √© o tempo que passou localmente desde a √∫ltima intera√ß√£o com esse mestre.  N√£o enviamos um fluxo continuamente, o mestre envia um delta somente quando ocorrem altera√ß√µes nele.  Quando enviamos algum tipo de confirma√ß√£o, tamb√©m nos comunicamos.  Obviamente, se o tempo ocioso aumentar (segundos, minutos, horas), algo est√° errado. </li><li>  Atributo <code>lag</code> .  N√≥s conversamos sobre lag.  Al√©m do lsn e da <code>server id</code> cada opera√ß√£o no log tamb√©m √© marcada com um carimbo de data / hora - hor√°rio local durante o qual essa opera√ß√£o foi registrada no vclock no mestre que a executou.  Ao mesmo tempo, Slave compara seu registro de data e hora local com o registro de data e hora do delta que ele recebeu.  O √∫ltimo registro de data e hora atual recebido para a √∫ltima linha, o escravo √© exibido no monitoramento. </li><li>  Atributo a <code>downstream</code> .  Mostra o que o mestre sabe sobre seu escravo em particular.  Este √© o ACK que o escravo envia para ele.  A <code>downstream</code> apresentada acima significa que a √∫ltima vez que seu escravo, tamb√©m conhecido como mestre no n√∫mero 2, enviou a ele seu vclock, que era 5,1.  Esse mestre sabe que todas as cinco linhas, que ele completou em seu lugar, foram para outro n√≥. </li></ul><br><h2>  Perda de XLOG </h2><br>  Considere a situa√ß√£o com a queda do mestre. <br><br><pre> <code class="plaintext hljs">lsn : 0 id: 3 replication : 1: &lt;...&gt; upstream : status: disconnected peer : lag: 3.9100646972656 e -05 idle: 1602.836148153 message: connect, called on fd 13, aka [::1]:37960 2: &lt;...&gt; upstream : status : follow idle : 0.65611373598222 peer : lag: 1.9550323486328 e -05 3: &lt;...&gt; vclock : {2: 2, 1: 5}</code> </pre> <br>  Primeiro de tudo, o status mudar√°.  <code>Lag</code> n√£o muda porque a linha que aplicamos permanece a mesma, n√£o recebemos nenhuma nova.  Ao mesmo tempo, <code>idle</code> crescendo, nesse caso j√° √© igual a 1602 segundos, tanto tempo que o mestre estava morto.  E vemos uma mensagem de erro: n√£o h√° conex√£o de rede. <br><br>  O que fazer em uma situa√ß√£o semelhante?  N√≥s descobrimos o que aconteceu com o nosso mestre, atra√≠mos o administrador, reiniciamos o servidor, aumentamos o n√≥.  A replica√ß√£o repetida √© executada e, quando o mestre entra no sistema, nos conectamos a ele, assinamos seu XLOG, obtemos para n√≥s mesmos e o cluster se estabiliza. <br><br>  Mas h√° um pequeno problema.  Imagine que t√≠nhamos um escravo, que por algum motivo se desligou e ficou ausente por um longo tempo.  Durante esse per√≠odo, o mestre que o serviu excluiu o XLOG.  Por exemplo, o disco est√° cheio, o coletor de lixo coletou logs.  Como um escravo que retorna pode continuar?  De jeito nenhum.  Como os logs que ele precisa aplicar para se sincronizar com o cluster desapareceram e n√£o h√° para onde retir√°-los.  Nesse caso, veremos um erro interessante: o status n√£o √© mais <code>disconnected</code> , mas <code>stopped</code> .  E uma mensagem espec√≠fica: n√£o h√° arquivo de log correspondente a esse lsn. <br><br><pre> <code class="plaintext hljs">id: 3 replication : 1: &lt;...&gt; upstream : peer : status: stopped lag : 0.0001683235168457 idle : 9.4331328970147 message: 'Missing .xlog file between LSN 7 1: 5, 2: 2 and 8 1: 6, 2: 2' 2: &lt;...&gt; 3: &lt;...&gt; vclock : {2: 2, 1: 5}</code> </pre> <br>  De fato, a situa√ß√£o nem sempre √© fatal.  Suponha que temos mais de dois mestres e, em alguns deles, esses logs ainda s√£o preservados.  N√≥s os derramamos a todos os mestres de uma s√≥ vez, e n√£o os armazenamos em apenas um.  Acontece que essa r√©plica, conectando-se a todos os mestres que ela conhece, em alguns deles encontrar√° os logs de que ela precisa.  Ela executar√° todas essas opera√ß√µes em casa, seu vclock aumentar√° e alcan√ßar√° o estado atual do cluster.  Depois disso, voc√™ pode tentar se reconectar. <br><br>  Se n√£o houver nenhum registro, n√£o podemos continuar a r√©plica.  Resta apenas reinicializ√°-lo.  Lembre-se de seu identificador exclusivo, voc√™ pode escrev√™-lo em um peda√ßo de papel ou em um arquivo.  Em seguida, limpamos a r√©plica localmente: exclua suas imagens, logs e assim por diante.  Depois disso, reconecte a r√©plica com o mesmo UUID que ela possu√≠a. <br><br>  Remova o cluster ou reutilize o UUID para a nova r√©plica: <code>box.cfg{instance_uuid =  uuid}</code> . <br><br>   ,   .   UUID    space cluster,    .       ,    .    UUID,  master,     JOIN,     ,       UUID,   ,    . <br><br>   ,   UUID ,     space cluster      ,    .       .  ,  ,          . <br><br><h2>  </h2><br> ,  -           .   ,        .    ,   ,      . <br><br>  Tarantool   . <br><br> <code>replication_connect_quorum: 2 <br> replication_connect_timeout: 30 <br> replication_sync_lag: 0.1</code> <br> <br> ,   , ,            ,   ,  ,  master'     0,1 .    30 .     ,   .   0,1 .  ,      . <br><br><h2> Keep alive </h2><br>  ,      ip tables drop.  ,    -  30   30 ,    ,      .     ,   keep alive-. <br><br>  keep alive-  : <code>box.cfg.replication_timeout</code> . <br><br>      master'      ,    keep alive-, ,   .    4  master  slave   keep alive-   ,         .             master'. <br><br><h2>    </h2><br>  ,      .    6 ,      5 .     10 ,    9 .     . <br><br>   ,    ,     .       ,         master',   .  -          .   . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pp/wv/qo/ppwvqoys4enyzstnxbfp0mtkug4.png" width="500"></div><br>     6 ,       3.     ,    .  ,     5 ,      3 . <br><br><h2>     </h2><br>   ,       : <br><br><ul><li>  . </li><li>  ,       space cluster,        .          . </li></ul><br>   ,    Telegram-,  .          ,     GitHub,   . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt439514/">https://habr.com/ru/post/pt439514/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt439504/index.html">Sua equipe precisa de um engenheiro de dados?</a></li>
<li><a href="../pt439506/index.html">9 alternativas para uma equipe ruim (padr√£o de design)</a></li>
<li><a href="../pt439508/index.html">Mitap sobre desenvolvimento de c√≥digo aberto em Moscou</a></li>
<li><a href="../pt439510/index.html">Sistema de controle distribu√≠do altamente carregado de uma moderna usina nuclear</a></li>
<li><a href="../pt439512/index.html">A idade dos dinossauros ou resseguro legalmente verificado?</a></li>
<li><a href="../pt439516/index.html">00110001 00110100 00101110 00110000 00110010</a></li>
<li><a href="../pt439518/index.html">GeekUniversity Atualizado Programa de Treinamento em Desenvolvimento Web: Mais Pr√°ticas e Casos de Clubes de Entrega</a></li>
<li><a href="../pt439520/index.html">Prova de jogo: novo modelo de neg√≥cios em 2019?</a></li>
<li><a href="../pt439522/index.html">DNS reencaminhar em 2k19, ou como realmente suar, visitando um site porn√¥</a></li>
<li><a href="../pt439524/index.html">Fortnite √© o futuro, mas por raz√µes inesperadas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>