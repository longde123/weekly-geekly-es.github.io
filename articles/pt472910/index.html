<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òπÔ∏è üñåÔ∏è üèñÔ∏è Encontre texto em placas e pacotes usando um smartphone üë®üèø‚Äçüîß üë®üèº‚Äçüç≥ üë¶üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O problema da busca autom√°tica de texto em imagens existe h√° muito tempo, pelo menos desde o in√≠cio dos anos noventa do s√©culo passado. Eles poderiam ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Encontre texto em placas e pacotes usando um smartphone</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/472910/">  O problema da busca autom√°tica de texto em imagens existe h√° muito tempo, pelo menos desde o in√≠cio dos anos noventa do s√©culo passado.  Eles poderiam ser lembrados pelos veteranos pela ampla distribui√ß√£o do ABBYY FineReader, que pode traduzir as digitaliza√ß√µes de documentos em suas vers√µes edit√°veis. <br><br>  Os scanners conectados a computadores pessoais funcionam muito bem nas empresas, mas o progresso n√£o p√°ra e os dispositivos m√≥veis dominam o mundo.  O intervalo de tarefas para trabalhar com texto tamb√©m mudou.  Agora voc√™ precisa procurar o texto n√£o em folhas A4 perfeitamente retas com texto em preto sobre fundo branco, mas em v√°rios cart√µes de visita, menus coloridos, letreiros de lojas e muito mais sobre o que uma pessoa pode encontrar na selva de uma cidade moderna. <br><br> <a href=""><img src="https://habrastorage.org/webt/br/xk/fe/brxkfes7mckw4fwiswow98w4ajy.png"></a> <br>  <i>Um exemplo real do trabalho de nossa rede neural.</i>  <i>A imagem √© clic√°vel.</i> <br><br><h2>  Requisitos e limita√ß√µes b√°sicas </h2><br>  Com uma variedade de condi√ß√µes para a apresenta√ß√£o de texto, algoritmos manuscritos n√£o podem mais lidar.  Aqui, redes neurais com sua capacidade de generalizar v√™m em socorro.  Neste post, falaremos sobre nossa abordagem para criar uma arquitetura de rede neural que detecta texto em imagens complexas com boa qualidade e alta velocidade. <br><a name="habracut"></a><br>  Os dispositivos m√≥veis imp√µem restri√ß√µes adicionais √† escolha da abordagem: <br><br><ol><li>  Os usu√°rios nem sempre t√™m a oportunidade de usar uma rede m√≥vel para se comunicar com o servidor devido a tr√°fego de roaming caro ou problemas de privacidade.  Portanto, solu√ß√µes como o Google Lens n√£o ajudar√£o aqui. </li><li>  Como nos concentramos no processamento de dados local, seria bom para a nossa solu√ß√£o: <br><ul><li>  Isso levou pouca mem√≥ria; </li><li>  Ele trabalhou rapidamente usando os recursos t√©cnicos do smartphone. </li></ul></li><li>  O texto pode ser girado e ter um fundo aleat√≥rio. </li><li>  As palavras podem ser muito longas.  Nas redes neurais convolucionais, o escopo do kernel de convolu√ß√£o geralmente n√£o cobre toda a palavra estendida; portanto, √© necess√°rio algum truque para contornar essa restri√ß√£o. <br><br><img src="https://habrastorage.org/webt/s5/qw/6h/s5qw6h1fmawausqg-sm_o-7fwm8.png" alt="imagem"><br></li><li> O tamanho do texto em uma foto pode ser diferente: <br><br><img src="https://habrastorage.org/webt/ia/a-/om/iaa-omk2j8qxwl-sbqmg1_v1rlc.png" alt="imagem"><br></li></ol><br><h2>  Solu√ß√£o </h2><br>  A solu√ß√£o mais simples para o problema de pesquisa de texto que vem √† mente √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">usar</a> a melhor rede das competi√ß√µes do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICDAR</a> (Confer√™ncia Internacional sobre An√°lise e Reconhecimento de Documentos), especializadas nesta tarefa e neg√≥cio!  Infelizmente, essas redes alcan√ßam qualidade devido ao seu volume e complexidade computacional e s√£o adequadas apenas como uma solu√ß√£o em nuvem, que n√£o atende aos par√°grafos 1 e 2 de nossos requisitos.  Mas e se pegarmos uma rede grande que funcione bem nos cen√°rios que precisamos cobrir e tentar reduzi-la?  Essa abordagem j√° √© mais interessante. <br><br>  Baoguang Shi et al. Em sua rede neural, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SegLink</a> [1] prop√¥s o seguinte: <br><br><ol><li>  Encontrar n√£o palavras inteiras de uma s√≥ vez (√°reas verdes na imagem <b>a</b> ), mas suas partes, chamadas segmentos, com a previs√£o de rota√ß√£o, inclina√ß√£o e deslocamento.  Vamos emprestar essa ideia. </li><li>  Voc√™ precisa procurar segmentos de palavras em v√°rias escalas ao mesmo tempo para atender ao requisito 5. Os segmentos s√£o mostrados por ret√¢ngulos verdes na imagem <b>b</b> . </li><li>  Para evitar que uma pessoa invente como combinar esses segmentos, simplesmente fazemos a rede neural prever conex√µes (links) entre segmentos relacionados √† mesma palavra <br><br>  a.  dentro da mesma escala (linhas vermelhas na imagem <b>c</b> ) <br><br>  b.  e entre escalas (linhas vermelhas na imagem <b>d</b> ), resolvendo o problema da cl√°usula 4 dos requisitos. </li></ol><br>  Os quadrados azuis na imagem abaixo mostram as √°reas de visibilidade dos pixels das camadas de sa√≠da da rede neural de diferentes escalas, que "veem" pelo menos parte da palavra. <br><br><img src="https://habrastorage.org/webt/qx/eu/zs/qxeuzsrcszz4pxafddjyma3cwqq.png" alt="imagem"><br>  <i>Exemplos de segmentos e links</i> <br><br>  O SegLink usa a conhecida arquitetura VGG-16 como base.  A previs√£o de segmentos e links nele √© realizada em 6 escalas.  Como o primeiro experimento, come√ßamos com a implementa√ß√£o da arquitetura original.  Aconteceu que a rede cont√©m 23 milh√µes de par√¢metros (pesos) que precisam ser armazenados em um arquivo de 88 megabytes de tamanho.  Se voc√™ criar um aplicativo baseado no VGG, ele ser√° um dos primeiros candidatos a remo√ß√£o se n√£o houver espa√ßo suficiente e a pesquisa de texto funcionar√° muito lentamente, portanto a rede precisa perder peso com urg√™ncia. <br><br><img src="https://habrastorage.org/webt/oj/lp/xo/ojlpxo224mtzrerf6o4urty-gfk.png" alt="imagem"><br>  <i>Arquitetura de rede SegLink</i> <br><br><h2>  O segredo da nossa dieta </h2><br>  Voc√™ pode reduzir o tamanho da rede simplesmente alterando o n√∫mero de camadas e canais, ou alterando a convolu√ß√£o em si e as conex√µes entre elas.  Mark Sandler e associados, a tempo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">adotaram</a> a arquitetura em sua rede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNetV2</a> [2], para que ela funcione rapidamente em dispositivos m√≥veis, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ocupe</a> pouco espa√ßo e ainda n√£o fique para tr√°s na qualidade do trabalho do mesmo VGG.  O segredo para acelerar e reduzir o consumo de mem√≥ria est√° em tr√™s etapas principais: <br><br><ol><li>  O n√∫mero de canais com mapas de caracter√≠sticas na entrada do bloco √© reduzido pela convolu√ß√£o do ponto em toda a profundidade (o chamado gargalo) sem uma fun√ß√£o de ativa√ß√£o. </li><li>  A convolu√ß√£o cl√°ssica √© substitu√≠da por uma convolu√ß√£o separ√°vel por canal.  Essa convolu√ß√£o requer menos peso e menos computa√ß√£o. </li><li>  As cartas de personagem ap√≥s o gargalo s√£o encaminhadas para a entrada do pr√≥ximo bloco para soma sem complica√ß√£o adicional. </li></ol><br><br><img src="https://habrastorage.org/webt/s8/uq/zb/s8uqzb_r7i6z508gyddsvwztsfc.png" alt="imagem"><br>  <i>Unidade base MobileNetV2</i> <i><br></i> <br><br><h2>  Rede neural resultante </h2><br>  Usando as abordagens acima, chegamos √† seguinte estrutura de rede: <br><br><ul><li>  Usamos segmentos e links do SegLink </li><li>  Substitua o VGG por um MobileNetV2 menos guloso </li><li>  Reduza o n√∫mero de escalas de pesquisa de texto de 6 para 5 para velocidade </li></ul><br><br><img src="https://habrastorage.org/webt/el/t6/8a/elt68a2truovh33paqx1omw1mjg.png" alt="imagem"><br>  <i>Rede de resumo da pesquisa de texto</i> <i><br></i> <br><br><h3>  Descriptografia de valores em blocos de arquitetura de rede </h3><br>  A etapa de passada e o n√∫mero base de canais nos canais s√£o indicados como s &lt;stride&gt; c &lt;channels&gt;, respectivamente.  Por exemplo, s2c32 significa 32 canais com um deslocamento de 2. O n√∫mero real de canais nas camadas de convolu√ß√£o √© obtido multiplicando seu n√∫mero base por um fator de escala Œ±, que permite simular rapidamente diferentes "espessuras" da rede.  Abaixo est√° uma tabela com o n√∫mero de par√¢metros na rede, dependendo de Œ±. <br><br><img src="https://habrastorage.org/webt/ay/k7/ih/ayk7ihjypgcp6vbw609jf9nmtfi.png" alt="imagem"><br><br>  Tipo de bloco: <br><br><ul><li>  Conv2D - uma opera√ß√£o de convolu√ß√£o completa; </li><li>  Conv D-wise - convolu√ß√£o separ√°vel por canal; </li><li>  Blocos - um grupo de blocos MobileNetV2; </li><li>  Sa√≠da - convolu√ß√£o para obter a camada de sa√≠da.  Valores num√©ricos do tipo NxN indicam o tamanho do campo receptivo do pixel. </li></ul><br>  Como uma fun√ß√£o de ativa√ß√£o, os blocos usam ReLU6. <br><br><img src="https://habrastorage.org/webt/a3/yb/ft/a3ybftdxyu7_vuw04wsx-vclcb4.png" alt="imagem" width="400"><br><br>  A camada de sa√≠da possui 31 canais: <br><br><img src="https://habrastorage.org/webt/se/3i/xc/se3ixchyxmnjwp2mdlp0l0czwr4.png" alt="imagem"><br><br>  Os dois primeiros canais da camada de sa√≠da votam para o pixel pertencer ao texto e n√£o ao texto.  Os cinco canais a seguir cont√™m informa√ß√µes para reconstruir com precis√£o a geometria do segmento: mudan√ßas verticais e horizontais em rela√ß√£o √† posi√ß√£o do pixel, fatores de largura e altura (j√° que o segmento geralmente n√£o √© quadrado) e o √¢ngulo de rota√ß√£o.  16 valores de links intra-canais indicam se h√° uma conex√£o entre oito pixels adjacentes na mesma escala.  Os √∫ltimos 8 canais nos informam sobre a presen√ßa de links para quatro pixels da escala anterior (a escala anterior √© sempre 2 vezes maior).  A cada 2 valores de segmentos, os links intra e cross-scale s√£o normalizados pela fun√ß√£o softmax.  O acesso √† primeira escala n√£o possui links entre escalas. <br><br><h2>  Montagem de palavras </h2><br>  A rede prev√™ se um segmento espec√≠fico e seus vizinhos pertencem ao texto.  Resta colet√°-los em palavras. <br><br>  Para come√ßar, combine todos os segmentos vinculados por links.  Para fazer isso, compomos um gr√°fico em que os v√©rtices s√£o todos os segmentos em todas as escalas e as arestas s√£o links.  Ent√£o encontramos os componentes conectados do gr√°fico.  Para cada componente, agora √© poss√≠vel calcular o ret√¢ngulo anexo da palavra da seguinte maneira: <br><br><ol><li>  Calculamos o √¢ngulo de rota√ß√£o da palavra Œ∏ <br><ul><li>  Ou como o valor m√©dio das previs√µes do √¢ngulo de rota√ß√£o dos segmentos, se houver muitos deles, </li><li>  Ou como o √¢ngulo de rota√ß√£o da linha obtido por regress√£o nos pontos dos centros dos segmentos, se houver poucos segmentos. </li></ul></li><li>  O centro da palavra √© selecionado como o centro de massa dos pontos centrais dos segmentos. </li><li>  Expanda todos os segmentos por -Œ∏ para organiz√°-los horizontalmente.  Encontre os limites da palavra. <br><ul><li>  Os limites esquerdo e direito da palavra s√£o selecionados como os limites dos segmentos mais √† esquerda e mais √† direita, respectivamente. </li><li>  Para obter o limite superior da palavra, os segmentos s√£o classificados pela altura da aresta superior, 20% dos mais altos s√£o cortados e o valor do primeiro segmento da lista restante ap√≥s a filtragem √© selecionado. </li><li>  O limite inferior √© obtido dos segmentos mais baixos com um ponto de corte de 20% do mais baixo, por analogia com o limite superior. </li></ul></li><li>  Gire o ret√¢ngulo resultante de volta para Œ∏. </li></ol><br>  A solu√ß√£o final √© chamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><b>FaSTExt</b> : Extrator de texto r√°pido e pequeno</a> [3] <br><br><h2>  Hora da experi√™ncia! </h2><br><h3>  Detalhes do treinamento </h3><br>  A pr√≥pria rede e seus par√¢metros foram selecionados para um bom trabalho em uma grande amostra interna, que reflete o cen√°rio principal do uso do aplicativo no telefone - ele apontou a c√¢mera para um objeto com texto e tirou uma foto.  Verificou-se que uma grande rede com Œ± = 1 ignora em qualidade a vers√£o com Œ± = 0,5 em apenas 2%.  Esta amostra n√£o √© de dom√≠nio p√∫blico, portanto, para maior clareza, tive que treinar a rede na amostra p√∫blica <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICDAR2013</a> , na qual as condi√ß√µes de filmagem s√£o semelhantes √†s nossas.  Como a amostra √© muito pequena, a rede foi treinada anteriormente em uma enorme quantidade de dados sint√©ticos do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SynthText no Wild Dataset</a> .  O processo de pr√©-treinamento levou cerca de 20 dias de c√°lculos para cada experimento no GTX 1080 Ti; portanto, a opera√ß√£o de rede em dados p√∫blicos foi verificada apenas pelas op√ß√µes Œ± = 0,75, 1 e 2. <br><br>  Como otimizador, foi usada a vers√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AMSGrad</a> de Adam. <br><br>  Fun√ß√µes de erro: <br><br><ul><li>  Entropia cruzada para a classifica√ß√£o de segmentos e links; </li><li>  Fun√ß√£o de perda de Huber para geometria de segmento. </li></ul><br><h2>  Resultados </h2><br>  Em termos de qualidade do desempenho da rede no cen√°rio-alvo, podemos dizer que ela n√£o fica muito atr√°s dos concorrentes em termos de qualidade e supera algumas.  A MS √© uma rede pesada de concorrentes em v√°rias escalas. <br><br><img src="https://habrastorage.org/webt/74/3y/h4/743yh4dgvv_4kjz_e0uecgqxtf0.png" alt="imagem" width="450"><br>  <i>* No artigo sobre o LESTE, n√£o houve resultados na amostra de que precis√°vamos, portanto realizamos o experimento.</i> <br><br>  A imagem abaixo mostra um exemplo de como o FaSTExt funciona em imagens do ICDAR2013.  A primeira linha mostra que as letras iluminadas da palavra ESPMOTO n√£o foram marcadas, mas a rede conseguiu encontr√°-las.  A vers√£o menos espa√ßosa com Œ± = 0,75 lidou com texto pequeno pior que as vers√µes mais "grossas".  A linha inferior novamente mostra falhas de marca√ß√£o na amostra com texto perdido na reflex√£o.  O FaSTExt v√™ ao mesmo tempo esse texto. <br><br><img src="https://habrastorage.org/webt/cc/dg/7f/ccdg7fs1wpqejmvk-hmorikeuwq.png" alt="imagem"><br><br>  Portanto, a rede executa suas tarefas.  Resta verificar se ele realmente pode ser usado em telefones?  Os modelos foram lan√ßados em imagens coloridas de 512x512 no Huawei P20 usando a CPU e no iPhone SE e iPhone XS usando a GPU, porque nosso sistema de aprendizado de m√°quina ainda permite que voc√™ use a GPU apenas no iOS.  Valores obtidos com m√©dia de 100 partidas.  No Android, conseguimos atingir uma velocidade de 5 quadros por segundo aceit√°vel para a nossa tarefa.  O iPhone XS mostrou um efeito interessante com uma diminui√ß√£o no tempo m√©dio necess√°rio para os c√°lculos enquanto complicava a rede.  Um iPhone moderno detecta texto com atraso m√≠nimo, o que pode ser chamado de vit√≥ria. <br><br><img src="https://habrastorage.org/webt/mx/zb/kq/mxzbkqrxlnjhqbwfj3hkv_bamby.png" alt="imagem"><br><br><h3>  Refer√™ncias </h3><br>  [1] B. Shi, X. Bai e S. Belongie, ‚ÄúDetectando texto orientado em imagens naturais por segmentos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vinculados</a> ‚Äù, Hava√≠, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  [2] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov e L.-C.  Chen, ‚ÄúMobileNetV2: res√≠duos invertidos e gargalos lineares‚Äù, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  [3] A. Filonenko, K. Gudkov, A. Lebedev, N. Orlov e I. Zagaynov, ‚ÄúFaSTExt: Extrator de texto pequeno e r√°pido‚Äù, no 8¬∫ Workshop Internacional sobre An√°lise e Reconhecimento de Documentos Baseados em C√¢mera, Sydney, 2019 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  [4] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu e X. Bai, ‚ÄúDetec√ß√£o de texto multi-orientada com redes totalmente convolucionais‚Äù, Las Vegas, 2016. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  [5] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. Ele e J. Liang, "ORIENTE: um detector de texto de cena eficiente e preciso", na Confer√™ncia IEEE de Computadores de 2017. Vis√£o e Padr√£o, Honolulu, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  [6] M. Liao, Z. Zhu, B. Shi, G.-s.  Xia e X. Bai, "Regress√£o sens√≠vel √† rota√ß√£o para detec√ß√£o de texto de cena orientada", na Confer√™ncia IEEE / CVF de 2018 sobre vis√£o e padr√£o de computador, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  [7] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao e J. Yan, ‚ÄúFots: localiza√ß√£o r√°pida de texto orientado com uma rede unificada‚Äù, na Confer√™ncia IEEE / CVF de 2018 em Vis√£o Computacional e Padr√£o, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> <br><br>  <i>Grupo de vis√£o computacional</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt472910/">https://habr.com/ru/post/pt472910/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt472892/index.html">Por que antiv√≠rus tradicionais n√£o s√£o adequados para nuvens p√∫blicas. E o que fazer?</a></li>
<li><a href="../pt472894/index.html">DartUP 2019: confer√™ncia sobre Dart and Flutter em S√£o Petersburgo em 23 de novembro</a></li>
<li><a href="../pt472896/index.html">Helic√≥ptero de uma impressora: pela primeira vez, os cientistas ‚Äúimprimiram‚Äù um estojo grande de um motor de helic√≥ptero</a></li>
<li><a href="../pt472902/index.html">Windows para IoT: Suporte aprimorado a hardware e novos recursos de dispositivos inteligentes</a></li>
<li><a href="../pt472908/index.html">Dagaz: Epis√≥dios (Parte 2)</a></li>
<li><a href="../pt472912/index.html">Banco de dados ClickHouse para humanos ou Alien Technology</a></li>
<li><a href="../pt472916/index.html">Back-end, aprendizado de m√°quina e sem servidor s√£o os mais interessantes da confer√™ncia Habr de julho</a></li>
<li><a href="../pt472918/index.html">ZX Spectrum na R√∫ssia e na CEI: como a busca pelo online se transformou offline</a></li>
<li><a href="../pt472922/index.html">Programador do Defender mais forte que a entropia</a></li>
<li><a href="../pt472926/index.html">A lei dos retornos acelerados (parte 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>