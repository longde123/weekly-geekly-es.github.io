<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙄 👩🏾‍💻 🕷️ Impala vs Hive vs Spark SQL: Auswahl der richtigen SQL-Engine für die ordnungsgemäße Funktion im Cloudera Data Warehouse 👩🏿‍🔧 🕴🏼 👩🏾‍⚖️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Uns fehlen immer Daten. Und wir wollen nicht nur mehr Daten ... wir wollen neue Arten von Daten, mit denen wir unsere Produkte, Kunden und Märkte bess...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Impala vs Hive vs Spark SQL: Auswahl der richtigen SQL-Engine für die ordnungsgemäße Funktion im Cloudera Data Warehouse</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486124/"><img src="https://habrastorage.org/webt/bu/1w/sb/bu1wsbowqiicmki0x1nqsraewjw.jpeg"><br><br>  Uns fehlen immer Daten.  Und wir wollen nicht nur mehr Daten ... wir wollen neue Arten von Daten, mit denen wir unsere Produkte, Kunden und Märkte besser verstehen können.  Wir sind immer auf der Suche nach neuen Daten, Daten in allen Formen und Größen, strukturiert und nicht sehr.  Wir möchten unsere Türen für eine neue Generation von Geschäftsfachleuten und technischen Spezialisten öffnen, die mit Begeisterung neue Datenbanken und Technologien mit uns eröffnen, die anschließend die Art und Weise verändern, wie wir mit Daten interagieren und welche Auswirkungen sie auf unser Leben haben. <br><a name="habracut"></a><br>  Ich werde ein Beispiel aus dem Leben geben, damit Sie besser verstehen, was ich meine.  Vor ungefähr zwei Jahren retteten die Daten das Leben der Tochter meines Freundes.  Bei ihrer Geburt wurden bei ihr sieben Herzfehler diagnostiziert.  Dank neuer Technologien wie interaktiver 3D-Grafik, virtueller Modellierung, intelligenterer EKG-Analyse, moderner Lösungen zur Überwachung von Patienten, die sich in Bettruhe befinden, und dank anderer fortschrittlicher medizinischer Verfahren, die auf Daten basierten, gelang es ihr, zwei Operationen am offenen Herzen zu überstehen und nun ein gesundes Leben zu führen .  Die Daten retteten ihr Leben.  Aus diesem Grund bin ich jeden Tag auf der Suche nach neuen innovativen Lösungen und nach Möglichkeiten, Daten schneller an diejenigen zu übertragen, die sie mehr benötigen als andere. <br><br>  Ich bin stolz darauf, Teil des Cloudera Data Warehouse (CDW) -Teams zu sein, das von der Cloudera Data Platform (CDP) unterstützt wird.  CDP wurde von Grund auf als Enterprise Data Cloud oder Enterprise Data Cloud (EDC) erstellt.  EDC ist ein multifunktionales Tool zur Implementierung vieler Aufgaben auf einer Plattform.  Dank der Verwendung von Hybrid- und Multi-Cloud-Systemen kann CDP überall eingesetzt werden - sowohl auf einer Plattform ohne Betriebssystem als auch in einer privaten und öffentlichen Cloud.  Da im Rahmen unseres digitalen Entwicklungsplans immer mehr Cloud-Lösungen eingeführt werden, werden Hybrid- und Multi-Cloud-Lösungen zur neuen Norm.  Diese kombinierten Lösungen verursachen jedoch Probleme bei der Verwaltung, was wiederum neue Sicherheitsrisiken, die Wahrscheinlichkeit der Überwachung durch den Benutzer und anschließend Gesetzesverstöße zur Folge hat.  Um diese Probleme zu lösen, verfügt CDP über erweiterte Sicherheits- und Kontrollfunktionen, die den Zugriff auf Daten ermöglichen, ohne das Risiko einzugehen, gegen die Sicherheitsrichtlinien oder sogar gegen Gesetze zu verstoßen. <br><br>  CDW on CDP ist ein neuer Service, mit dem Sie ein Self-Service-Data-Warehouse für BI-Analyseteams erstellen können.  Sie können schnell neue Data Warehouses erstellen und diese selbst verwenden oder ihnen Zugriff auf eine Gruppe von Personen gewähren und mit ihnen eine einzige Datenbank verwenden.  Erinnern Sie sich an die Zeiten, als Sie Ihr Data Warehouse selbst verwalten konnten?  Ohne die Teilnahme von Plattformen und der für den Betrieb erforderlichen Infrastruktur zu verwalten?  Das ist noch nie passiert.  CDW hat dies möglich gemacht. <br><br>  Dank CDW sind verschiedene SQL-Engines verfügbar, aber die Auswahl ist groß.  Schauen wir uns die in CDW auf CDP verfügbaren SQL-Engines an und diskutieren, welche SQL-Option für eine bestimmte Aufgabe besser geeignet ist. <br><br>  So eine gute Wahl!  Impala?  Hive LLAP?  Funken?  Was zu verwenden und wann?  Lass es uns herausfinden. <br><br><h2>  Impala SQL Engine </h2><br>  Impala ist eine beliebte Open-Source-MPP-Engine mit einer Vielzahl von Funktionen in Cloudera Distribution Hadoop (CDH) und CDP.  Impala hat durch seine hochinteraktiven SQL-Abfragen mit geringer Latenz das Vertrauen des Marktes gewonnen.  Die Funktionen von Impala sind sehr umfangreich. Impala unterstützt nicht nur das verteilte Hadoop-Dateisystem (HDFS - Hadoop Distributed File System) mit Parkett, optimierter Zeilenspalte (ORC - Optimized Storage Node), JavaScript-Objektnotation (JSON), Avro und Textformaten verfügt über integrierte Unterstützung für Kudu, Microsoft Azure Data Lake-Speicher (ADLS) und Amazon Simple Storage Service (S3).  Impala bietet ein hohes Maß an Sicherheit, da es entweder Wachposten oder Ranger verwendet. Wie Sie wissen, kann Impala Tausende von Benutzern mit Clustern von Hunderten von Knoten in Datasets mit mehreren Petabyte unterstützen.  Schauen wir uns die gesamte Impala-Architektur an. <br><br><img src="https://habrastorage.org/webt/on/f3/uk/onf3ukvs8cr_4lorbm6tkholx9c.png"><br><br>  Impala verwendet den StateStore, um den Zustand des Clusters zu überprüfen.  Wenn der Impala-Knoten aus irgendeinem Grund offline geschaltet wird, sendet der StateStore eine entsprechende Nachricht an alle Knoten und überspringt den Knoten, auf den nicht zugegriffen werden kann.  Der Impala-Verzeichnisdienst verwaltet Metadaten für alle SQL-Anweisungen für alle Knoten im Cluster.  Der StateStore und der Verzeichnisdienst tauschen Daten mit dem Hive MetaStore aus, um Blöcke und Dateien zu speichern und die Metadaten dann auf die Arbeitsknoten zu übertragen.  Wenn eine Anforderung eintrifft, wird sie an eines der vielen übereinstimmenden Programme übergeben, in denen die Kompilierung durchgeführt und die Planung eingeleitet wird.  Fragmente des Plans werden zurückgesandt, und das Koordinierungsprogramm organisiert seine Umsetzung.  Zwischenergebnisse werden zwischen den Impala-Diensten ausgetauscht und anschließend zurückgegeben. <br><br>  Diese Architektur ist ideal für Fälle, in denen Data Marts für Business Intelligence erforderlich sind, um Antworten auf Anfragen mit geringer Latenz zu erhalten, wie dies normalerweise bei Ad-hoc-, Self-Service- und Discovery-Typen der Fall ist.  In diesem Szenario haben wir Kunden, die uns Antworten auf komplexe Fragen von weniger als einer Sekunde bis fünf Sekunden geben. <br><br>  Für IoT-Daten (Internet of Things) und verwandte Szenarien kann Impala zusammen mit Streaming-Lösungen wie NiFi, Kafka oder Spark Streaming und verwandten Data Warehouses wie Kudu ein kontinuierliches Pipelining mit einer Verzögerungszeit von weniger als zehn Sekunden bereitstellen .  Mit integrierten Lese- / Schreibfunktionen für S3, ADLS, HDFS, Hive, HBase und mehr ist Impala eine hervorragende SQL-Engine zum Starten eines Clusters mit bis zu 1000 Knoten und mehr als 100 Billionen Zeilen in Tabellen oder Datensätzen mit 50 BP oder mehr. <br><br><h2>  Hive LLAP </h2><br>  Live Long And Process oder Long Delay Analytics Processing, auch als LLAP bezeichnet, ist ein Hive-basiertes Ausführungsmodul, das Prozesse mit langer Laufzeit unter Verwendung derselben Cache- und Verarbeitungsressourcen unterstützt.  Dieser Verarbeitungsmechanismus gibt uns eine Antwort von SQL mit einer sehr geringen Latenz, da wir keine Zeit haben, die angeforderten Ressourcen zu starten. <br><br><img src="https://habrastorage.org/webt/pq/uw/fc/pquwfcyzx5gn55c8d8k1otqvica.png"><br><br>  Darüber hinaus bietet LLAP die Kontrolle über die Ausführung von Sicherheitsrichtlinien, sodass alle LLAP-Arbeiten für den Benutzer transparent sind. Dadurch kann Hive auch mit den gängigsten und traditionell verwendeten Speichermedien im Hinblick auf die Workload-Leistung mithalten. <br><br>  Hive LLAP bietet die fortschrittlichste SQL-Engine im Big-Data-Ökosystem.  Hive LLAP wurde für eine große Datenmenge erstellt und bietet Benutzern die umfangreichen Funktionen des Enterprise Data Warehouse (EDW), das die Konvertierung großer Datenmengen, die Ausführung langer Abfragen oder schwerer SQL-Abfragen mit Hunderten von Verknüpfungen unterstützt.  Hive unterstützt materialisierte Ansichten, Ersatzschlüssel und verschiedene Einschränkungen, die herkömmlichen relationalen Datenbankverwaltungssystemen ähneln, einschließlich integriertem Caching zum Abfragen von Ergebnissen und Daten.  Hive LLAP kann die Belastung durch wiederholte Anforderungen verringern, indem die Antwortzeit auf den Bruchteil einer Sekunde reduziert wird.  Hive LLAP kann Verbundanforderungen für HDFS (Hadoop Distributed File System) und Objektspeicher sowie Echtzeit-Streaming für Kafka und Druid unterstützen. <br><br>  Daher eignet sich Hive LLAP ideal als Enterprise Data Warehouse (EDW) -Lösung, bei der eine große Anzahl langer Abfragen erforderlich ist, die umfangreiche Transformationen oder mehrere Verknüpfungen zwischen Tabellen und großen Datasets erfordern.  Dank der in Hive LLAP enthaltenen Caching-Technologie haben wir jetzt Kunden, die 330 Milliarden Datensätze mit 92 Milliarden anderen Datensätzen mit oder ohne Partitionsschlüssel zusammenführen und in Sekundenschnelle Ergebnisse erzielen können. <br><br><h2>  Spark sq </h2><br><br>  Spark ist eine leistungsstarke, universelle Datenverarbeitungs-Engine, die die Datenverarbeitung und -verteilung unterstützt und eine breite Palette von Anwendungen bietet.  Es gibt viele Spark-Datenbibliotheken für Data Science- und Machine Learning-Experten, die das übergeordnete Programmiermodell für eine schnelle Entwicklung unterstützen.  Über Spark stehen vor allem Spark SQL, MLlib, Spark Streaming und GrapX. <br><br><img src="https://habrastorage.org/webt/u2/2n/gh/u22nghp80uvyrlof4kwul2pzmgu.png"><br><br>  Spark SQL ist ein mit verschiedenen Datenquellen kompatibles Modul für die strukturierte Datenverarbeitung mit Unterstützung für Hive, Avro, Parquet, ORC, JSON und JDBC.  Spark SQL ist effizient für semistrukturierte Datasets und lässt sich in Hive MetaStore- und NoSQL-Repositorys wie HBase integrieren.  Spark wird häufig mit verschiedenen Software-APIs in unseren bevorzugten Programmiersprachen wie Java, Python, R und Scala verwendet. <br><br>  Spark kann sehr nützlich sein, wenn Sie SQL-Abfragen in Spark-Programme einbetten müssen, wenn es mit großen Datenmengen und hoher Last funktioniert.  Spark hilft vielen unserer Benutzer, die in Global 100-Unternehmen arbeiten, die Verarbeitung von Streaming-Daten zu reduzieren.  In Kombination mit MLlib sehen wir, wie viele unserer Kunden Spark als hervorragendes System für maschinelles Lernen bei der Arbeit mit Data-Warehouse-Anwendungen positiv bewerten.  Mit hoher Leistung, geringer Latenz und hervorragender Integration von Tools von Drittanbietern bietet Spark SQL die besten Voraussetzungen für den Wechsel zwischen Programmierung und SQL. <br><br><h3>  Welche SQL-Engine soll ich verwenden? </h3><br><br>  Da Sie dieselben Daten in CDW mit CDP kombinieren können, können Sie für jede Art von Workload die richtige Engine auswählen, z. B. Data Engineering, traditionelle EDW, Ad-hoc-Analyse, BI-Dashboards, Online Analytical Processing (OLAP) oder Online Transaktionsverarbeitung (OLTP).  Das folgende Diagramm zeigt einige Prinzipien zur Vereinfachung der Auswahl, nach denen die Motoren und ihre Mechanismen für jedes der angegebenen Ziele gut geeignet sind. <br><br><img src="https://habrastorage.org/webt/xw/st/ff/xwstffkvxlp9ubso_swhiinvdda.png"><br><br><h2>  Fazit </h2><br>  Wenn Sie BI-Dashboards mit EDW-Unterstützung verwenden, erzielt Hive LLAP die besten Ergebnisse.  Wenn Sie Ad-hoc-, Self-Service- und Research-Data-Warehousing benötigen, wenden Sie sich an die Vorteile von Impala.  Wenn Sie Data Engineering mit langen Abfragen und ohne hohe Nebenläufigkeit betrachten, ist Spark SQL eine gute Wahl.  Wenn Sie Unterstützung für hohe Parallelität benötigen, können Sie sich Hive on Tez ansehen.  Suchen Sie nach OLAP-Unterstützung für Zeitreihendaten, fügen Sie Druid hinzu, und wenn Sie OLTP mit geringer Latenz und hoher Nebenläufigkeit suchen, sollten Sie möglicherweise Phoenix hinzufügen. <br><br>  Insgesamt - es gibt viele SQL-Engines in CDW zu CDP, und dies geschieht absichtlich.  Das Treffen von Entscheidungen vor einer Entscheidung ist der beste Weg, um Prozesse für Hochleistungsanwendungen mit Multithread-Verarbeitung in großen Data Warehouses zu optimieren.  CDW in CDP bietet Datenaustausch und -freigabe unter einem einzigen System aus Sicherheit, Verwaltung, Datenverfolgung und Metadaten, mit dem Sie SQL-Komponenten in optimierten Repositorys kombinieren können.  Auf diese Weise kann der Benutzer die beste SQL-Engine in Abhängigkeit von seiner Arbeitslast auswählen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de486124/">https://habr.com/ru/post/de486124/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486106/index.html">Adaptive Hintergrundbeleuchtung für Raspberry Pi TV - Ambilight Analog</a></li>
<li><a href="../de486114/index.html">Führende Wissenschaftler auf dem Gebiet der Neurowissenschaften treffen sich auf dem jährlichen Kongress der Neuronet Industry Union</a></li>
<li><a href="../de486116/index.html">Fermat- und Miller-Rabin-Einfachheitstests</a></li>
<li><a href="../de486120/index.html">Normalisierung der Abweichung. Wie falsche Praktiken in unserer Branche zur Norm werden</a></li>
<li><a href="../de486122/index.html">Child ReactJS mit 135 Codezeilen</a></li>
<li><a href="../de486128/index.html">Test Solution Architect: Wer ist es und wann wird es benötigt?</a></li>
<li><a href="../de486144/index.html">Warum sterben Altcoins und was kann in naher Zukunft mit Kryptowährung passieren?</a></li>
<li><a href="../de486150/index.html">Entwicklung der IT-Sphäre in der Slowakei. Arbeitsnutzen für junge Berufstätige</a></li>
<li><a href="../de486156/index.html">Da habe ich unterrichtet und dann ein Trainingshandbuch in Python geschrieben</a></li>
<li><a href="../de486158/index.html">Visualisierung der neuronalen maschinellen Übersetzung (seq2seq-Modelle mit Aufmerksamkeitsmechanismus)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>