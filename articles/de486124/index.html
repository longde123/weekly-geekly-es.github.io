<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÑ üë©üèæ‚Äçüíª üï∑Ô∏è Impala vs Hive vs Spark SQL: Auswahl der richtigen SQL-Engine f√ºr die ordnungsgem√§√üe Funktion im Cloudera Data Warehouse üë©üèø‚Äçüîß üï¥üèº üë©üèæ‚Äç‚öñÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Uns fehlen immer Daten. Und wir wollen nicht nur mehr Daten ... wir wollen neue Arten von Daten, mit denen wir unsere Produkte, Kunden und M√§rkte bess...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Impala vs Hive vs Spark SQL: Auswahl der richtigen SQL-Engine f√ºr die ordnungsgem√§√üe Funktion im Cloudera Data Warehouse</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486124/"><img src="https://habrastorage.org/webt/bu/1w/sb/bu1wsbowqiicmki0x1nqsraewjw.jpeg"><br><br>  Uns fehlen immer Daten.  Und wir wollen nicht nur mehr Daten ... wir wollen neue Arten von Daten, mit denen wir unsere Produkte, Kunden und M√§rkte besser verstehen k√∂nnen.  Wir sind immer auf der Suche nach neuen Daten, Daten in allen Formen und Gr√∂√üen, strukturiert und nicht sehr.  Wir m√∂chten unsere T√ºren f√ºr eine neue Generation von Gesch√§ftsfachleuten und technischen Spezialisten √∂ffnen, die mit Begeisterung neue Datenbanken und Technologien mit uns er√∂ffnen, die anschlie√üend die Art und Weise ver√§ndern, wie wir mit Daten interagieren und welche Auswirkungen sie auf unser Leben haben. <br><a name="habracut"></a><br>  Ich werde ein Beispiel aus dem Leben geben, damit Sie besser verstehen, was ich meine.  Vor ungef√§hr zwei Jahren retteten die Daten das Leben der Tochter meines Freundes.  Bei ihrer Geburt wurden bei ihr sieben Herzfehler diagnostiziert.  Dank neuer Technologien wie interaktiver 3D-Grafik, virtueller Modellierung, intelligenterer EKG-Analyse, moderner L√∂sungen zur √úberwachung von Patienten, die sich in Bettruhe befinden, und dank anderer fortschrittlicher medizinischer Verfahren, die auf Daten basierten, gelang es ihr, zwei Operationen am offenen Herzen zu √ºberstehen und nun ein gesundes Leben zu f√ºhren .  Die Daten retteten ihr Leben.  Aus diesem Grund bin ich jeden Tag auf der Suche nach neuen innovativen L√∂sungen und nach M√∂glichkeiten, Daten schneller an diejenigen zu √ºbertragen, die sie mehr ben√∂tigen als andere. <br><br>  Ich bin stolz darauf, Teil des Cloudera Data Warehouse (CDW) -Teams zu sein, das von der Cloudera Data Platform (CDP) unterst√ºtzt wird.  CDP wurde von Grund auf als Enterprise Data Cloud oder Enterprise Data Cloud (EDC) erstellt.  EDC ist ein multifunktionales Tool zur Implementierung vieler Aufgaben auf einer Plattform.  Dank der Verwendung von Hybrid- und Multi-Cloud-Systemen kann CDP √ºberall eingesetzt werden - sowohl auf einer Plattform ohne Betriebssystem als auch in einer privaten und √∂ffentlichen Cloud.  Da im Rahmen unseres digitalen Entwicklungsplans immer mehr Cloud-L√∂sungen eingef√ºhrt werden, werden Hybrid- und Multi-Cloud-L√∂sungen zur neuen Norm.  Diese kombinierten L√∂sungen verursachen jedoch Probleme bei der Verwaltung, was wiederum neue Sicherheitsrisiken, die Wahrscheinlichkeit der √úberwachung durch den Benutzer und anschlie√üend Gesetzesverst√∂√üe zur Folge hat.  Um diese Probleme zu l√∂sen, verf√ºgt CDP √ºber erweiterte Sicherheits- und Kontrollfunktionen, die den Zugriff auf Daten erm√∂glichen, ohne das Risiko einzugehen, gegen die Sicherheitsrichtlinien oder sogar gegen Gesetze zu versto√üen. <br><br>  CDW on CDP ist ein neuer Service, mit dem Sie ein Self-Service-Data-Warehouse f√ºr BI-Analyseteams erstellen k√∂nnen.  Sie k√∂nnen schnell neue Data Warehouses erstellen und diese selbst verwenden oder ihnen Zugriff auf eine Gruppe von Personen gew√§hren und mit ihnen eine einzige Datenbank verwenden.  Erinnern Sie sich an die Zeiten, als Sie Ihr Data Warehouse selbst verwalten konnten?  Ohne die Teilnahme von Plattformen und der f√ºr den Betrieb erforderlichen Infrastruktur zu verwalten?  Das ist noch nie passiert.  CDW hat dies m√∂glich gemacht. <br><br>  Dank CDW sind verschiedene SQL-Engines verf√ºgbar, aber die Auswahl ist gro√ü.  Schauen wir uns die in CDW auf CDP verf√ºgbaren SQL-Engines an und diskutieren, welche SQL-Option f√ºr eine bestimmte Aufgabe besser geeignet ist. <br><br>  So eine gute Wahl!  Impala?  Hive LLAP?  Funken?  Was zu verwenden und wann?  Lass es uns herausfinden. <br><br><h2>  Impala SQL Engine </h2><br>  Impala ist eine beliebte Open-Source-MPP-Engine mit einer Vielzahl von Funktionen in Cloudera Distribution Hadoop (CDH) und CDP.  Impala hat durch seine hochinteraktiven SQL-Abfragen mit geringer Latenz das Vertrauen des Marktes gewonnen.  Die Funktionen von Impala sind sehr umfangreich. Impala unterst√ºtzt nicht nur das verteilte Hadoop-Dateisystem (HDFS - Hadoop Distributed File System) mit Parkett, optimierter Zeilenspalte (ORC - Optimized Storage Node), JavaScript-Objektnotation (JSON), Avro und Textformaten verf√ºgt √ºber integrierte Unterst√ºtzung f√ºr Kudu, Microsoft Azure Data Lake-Speicher (ADLS) und Amazon Simple Storage Service (S3).  Impala bietet ein hohes Ma√ü an Sicherheit, da es entweder Wachposten oder Ranger verwendet. Wie Sie wissen, kann Impala Tausende von Benutzern mit Clustern von Hunderten von Knoten in Datasets mit mehreren Petabyte unterst√ºtzen.  Schauen wir uns die gesamte Impala-Architektur an. <br><br><img src="https://habrastorage.org/webt/on/f3/uk/onf3ukvs8cr_4lorbm6tkholx9c.png"><br><br>  Impala verwendet den StateStore, um den Zustand des Clusters zu √ºberpr√ºfen.  Wenn der Impala-Knoten aus irgendeinem Grund offline geschaltet wird, sendet der StateStore eine entsprechende Nachricht an alle Knoten und √ºberspringt den Knoten, auf den nicht zugegriffen werden kann.  Der Impala-Verzeichnisdienst verwaltet Metadaten f√ºr alle SQL-Anweisungen f√ºr alle Knoten im Cluster.  Der StateStore und der Verzeichnisdienst tauschen Daten mit dem Hive MetaStore aus, um Bl√∂cke und Dateien zu speichern und die Metadaten dann auf die Arbeitsknoten zu √ºbertragen.  Wenn eine Anforderung eintrifft, wird sie an eines der vielen √ºbereinstimmenden Programme √ºbergeben, in denen die Kompilierung durchgef√ºhrt und die Planung eingeleitet wird.  Fragmente des Plans werden zur√ºckgesandt, und das Koordinierungsprogramm organisiert seine Umsetzung.  Zwischenergebnisse werden zwischen den Impala-Diensten ausgetauscht und anschlie√üend zur√ºckgegeben. <br><br>  Diese Architektur ist ideal f√ºr F√§lle, in denen Data Marts f√ºr Business Intelligence erforderlich sind, um Antworten auf Anfragen mit geringer Latenz zu erhalten, wie dies normalerweise bei Ad-hoc-, Self-Service- und Discovery-Typen der Fall ist.  In diesem Szenario haben wir Kunden, die uns Antworten auf komplexe Fragen von weniger als einer Sekunde bis f√ºnf Sekunden geben. <br><br>  F√ºr IoT-Daten (Internet of Things) und verwandte Szenarien kann Impala zusammen mit Streaming-L√∂sungen wie NiFi, Kafka oder Spark Streaming und verwandten Data Warehouses wie Kudu ein kontinuierliches Pipelining mit einer Verz√∂gerungszeit von weniger als zehn Sekunden bereitstellen .  Mit integrierten Lese- / Schreibfunktionen f√ºr S3, ADLS, HDFS, Hive, HBase und mehr ist Impala eine hervorragende SQL-Engine zum Starten eines Clusters mit bis zu 1000 Knoten und mehr als 100 Billionen Zeilen in Tabellen oder Datens√§tzen mit 50 BP oder mehr. <br><br><h2>  Hive LLAP </h2><br>  Live Long And Process oder Long Delay Analytics Processing, auch als LLAP bezeichnet, ist ein Hive-basiertes Ausf√ºhrungsmodul, das Prozesse mit langer Laufzeit unter Verwendung derselben Cache- und Verarbeitungsressourcen unterst√ºtzt.  Dieser Verarbeitungsmechanismus gibt uns eine Antwort von SQL mit einer sehr geringen Latenz, da wir keine Zeit haben, die angeforderten Ressourcen zu starten. <br><br><img src="https://habrastorage.org/webt/pq/uw/fc/pquwfcyzx5gn55c8d8k1otqvica.png"><br><br>  Dar√ºber hinaus bietet LLAP die Kontrolle √ºber die Ausf√ºhrung von Sicherheitsrichtlinien, sodass alle LLAP-Arbeiten f√ºr den Benutzer transparent sind. Dadurch kann Hive auch mit den g√§ngigsten und traditionell verwendeten Speichermedien im Hinblick auf die Workload-Leistung mithalten. <br><br>  Hive LLAP bietet die fortschrittlichste SQL-Engine im Big-Data-√ñkosystem.  Hive LLAP wurde f√ºr eine gro√üe Datenmenge erstellt und bietet Benutzern die umfangreichen Funktionen des Enterprise Data Warehouse (EDW), das die Konvertierung gro√üer Datenmengen, die Ausf√ºhrung langer Abfragen oder schwerer SQL-Abfragen mit Hunderten von Verkn√ºpfungen unterst√ºtzt.  Hive unterst√ºtzt materialisierte Ansichten, Ersatzschl√ºssel und verschiedene Einschr√§nkungen, die herk√∂mmlichen relationalen Datenbankverwaltungssystemen √§hneln, einschlie√ülich integriertem Caching zum Abfragen von Ergebnissen und Daten.  Hive LLAP kann die Belastung durch wiederholte Anforderungen verringern, indem die Antwortzeit auf den Bruchteil einer Sekunde reduziert wird.  Hive LLAP kann Verbundanforderungen f√ºr HDFS (Hadoop Distributed File System) und Objektspeicher sowie Echtzeit-Streaming f√ºr Kafka und Druid unterst√ºtzen. <br><br>  Daher eignet sich Hive LLAP ideal als Enterprise Data Warehouse (EDW) -L√∂sung, bei der eine gro√üe Anzahl langer Abfragen erforderlich ist, die umfangreiche Transformationen oder mehrere Verkn√ºpfungen zwischen Tabellen und gro√üen Datasets erfordern.  Dank der in Hive LLAP enthaltenen Caching-Technologie haben wir jetzt Kunden, die 330 Milliarden Datens√§tze mit 92 Milliarden anderen Datens√§tzen mit oder ohne Partitionsschl√ºssel zusammenf√ºhren und in Sekundenschnelle Ergebnisse erzielen k√∂nnen. <br><br><h2>  Spark sq </h2><br><br>  Spark ist eine leistungsstarke, universelle Datenverarbeitungs-Engine, die die Datenverarbeitung und -verteilung unterst√ºtzt und eine breite Palette von Anwendungen bietet.  Es gibt viele Spark-Datenbibliotheken f√ºr Data Science- und Machine Learning-Experten, die das √ºbergeordnete Programmiermodell f√ºr eine schnelle Entwicklung unterst√ºtzen.  √úber Spark stehen vor allem Spark SQL, MLlib, Spark Streaming und GrapX. <br><br><img src="https://habrastorage.org/webt/u2/2n/gh/u22nghp80uvyrlof4kwul2pzmgu.png"><br><br>  Spark SQL ist ein mit verschiedenen Datenquellen kompatibles Modul f√ºr die strukturierte Datenverarbeitung mit Unterst√ºtzung f√ºr Hive, Avro, Parquet, ORC, JSON und JDBC.  Spark SQL ist effizient f√ºr semistrukturierte Datasets und l√§sst sich in Hive MetaStore- und NoSQL-Repositorys wie HBase integrieren.  Spark wird h√§ufig mit verschiedenen Software-APIs in unseren bevorzugten Programmiersprachen wie Java, Python, R und Scala verwendet. <br><br>  Spark kann sehr n√ºtzlich sein, wenn Sie SQL-Abfragen in Spark-Programme einbetten m√ºssen, wenn es mit gro√üen Datenmengen und hoher Last funktioniert.  Spark hilft vielen unserer Benutzer, die in Global 100-Unternehmen arbeiten, die Verarbeitung von Streaming-Daten zu reduzieren.  In Kombination mit MLlib sehen wir, wie viele unserer Kunden Spark als hervorragendes System f√ºr maschinelles Lernen bei der Arbeit mit Data-Warehouse-Anwendungen positiv bewerten.  Mit hoher Leistung, geringer Latenz und hervorragender Integration von Tools von Drittanbietern bietet Spark SQL die besten Voraussetzungen f√ºr den Wechsel zwischen Programmierung und SQL. <br><br><h3>  Welche SQL-Engine soll ich verwenden? </h3><br><br>  Da Sie dieselben Daten in CDW mit CDP kombinieren k√∂nnen, k√∂nnen Sie f√ºr jede Art von Workload die richtige Engine ausw√§hlen, z. B. Data Engineering, traditionelle EDW, Ad-hoc-Analyse, BI-Dashboards, Online Analytical Processing (OLAP) oder Online Transaktionsverarbeitung (OLTP).  Das folgende Diagramm zeigt einige Prinzipien zur Vereinfachung der Auswahl, nach denen die Motoren und ihre Mechanismen f√ºr jedes der angegebenen Ziele gut geeignet sind. <br><br><img src="https://habrastorage.org/webt/xw/st/ff/xwstffkvxlp9ubso_swhiinvdda.png"><br><br><h2>  Fazit </h2><br>  Wenn Sie BI-Dashboards mit EDW-Unterst√ºtzung verwenden, erzielt Hive LLAP die besten Ergebnisse.  Wenn Sie Ad-hoc-, Self-Service- und Research-Data-Warehousing ben√∂tigen, wenden Sie sich an die Vorteile von Impala.  Wenn Sie Data Engineering mit langen Abfragen und ohne hohe Nebenl√§ufigkeit betrachten, ist Spark SQL eine gute Wahl.  Wenn Sie Unterst√ºtzung f√ºr hohe Parallelit√§t ben√∂tigen, k√∂nnen Sie sich Hive on Tez ansehen.  Suchen Sie nach OLAP-Unterst√ºtzung f√ºr Zeitreihendaten, f√ºgen Sie Druid hinzu, und wenn Sie OLTP mit geringer Latenz und hoher Nebenl√§ufigkeit suchen, sollten Sie m√∂glicherweise Phoenix hinzuf√ºgen. <br><br>  Insgesamt - es gibt viele SQL-Engines in CDW zu CDP, und dies geschieht absichtlich.  Das Treffen von Entscheidungen vor einer Entscheidung ist der beste Weg, um Prozesse f√ºr Hochleistungsanwendungen mit Multithread-Verarbeitung in gro√üen Data Warehouses zu optimieren.  CDW in CDP bietet Datenaustausch und -freigabe unter einem einzigen System aus Sicherheit, Verwaltung, Datenverfolgung und Metadaten, mit dem Sie SQL-Komponenten in optimierten Repositorys kombinieren k√∂nnen.  Auf diese Weise kann der Benutzer die beste SQL-Engine in Abh√§ngigkeit von seiner Arbeitslast ausw√§hlen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de486124/">https://habr.com/ru/post/de486124/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486106/index.html">Adaptive Hintergrundbeleuchtung f√ºr Raspberry Pi TV - Ambilight Analog</a></li>
<li><a href="../de486114/index.html">F√ºhrende Wissenschaftler auf dem Gebiet der Neurowissenschaften treffen sich auf dem j√§hrlichen Kongress der Neuronet Industry Union</a></li>
<li><a href="../de486116/index.html">Fermat- und Miller-Rabin-Einfachheitstests</a></li>
<li><a href="../de486120/index.html">Normalisierung der Abweichung. Wie falsche Praktiken in unserer Branche zur Norm werden</a></li>
<li><a href="../de486122/index.html">Child ReactJS mit 135 Codezeilen</a></li>
<li><a href="../de486128/index.html">Test Solution Architect: Wer ist es und wann wird es ben√∂tigt?</a></li>
<li><a href="../de486144/index.html">Warum sterben Altcoins und was kann in naher Zukunft mit Kryptow√§hrung passieren?</a></li>
<li><a href="../de486150/index.html">Entwicklung der IT-Sph√§re in der Slowakei. Arbeitsnutzen f√ºr junge Berufst√§tige</a></li>
<li><a href="../de486156/index.html">Da habe ich unterrichtet und dann ein Trainingshandbuch in Python geschrieben</a></li>
<li><a href="../de486158/index.html">Visualisierung der neuronalen maschinellen √úbersetzung (seq2seq-Modelle mit Aufmerksamkeitsmechanismus)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>