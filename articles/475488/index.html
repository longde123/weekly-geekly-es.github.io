<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø ü§ï üíµ Presentaci√≥n de PyTorch: aprendizaje profundo en el procesamiento del lenguaje natural üé∫ üò∂ üè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola habrozhiteli! El procesamiento del lenguaje natural (PNL) es una tarea extremadamente importante en el campo de la inteligencia artificial. La im...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Presentaci√≥n de PyTorch: aprendizaje profundo en el procesamiento del lenguaje natural</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/475488/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/gj/ib/ak/gjibakead8idlzldl2vxqq3gmuc.jpeg" align="left" alt="imagen"></a>  Hola habrozhiteli!  El procesamiento del lenguaje natural (PNL) es una tarea extremadamente importante en el campo de la inteligencia artificial.  La implementaci√≥n exitosa permite productos como Amazon Amazon y Google Translate.  Este libro lo ayudar√° a aprender PyTorch, una biblioteca de aprendizaje profundo para Python, una de las herramientas l√≠deres para cient√≠ficos de datos y desarrolladores de software de PNL.  Delip Rao y Brian McMahan lo pondr√°n al d√≠a con NLP y algoritmos de aprendizaje profundo.  Y muestre c√≥mo PyTorch le permite implementar aplicaciones que usan an√°lisis de texto. <br><br>  En este libro ‚Ä¢ Gr√°ficos computacionales y el paradigma de aprendizaje con un maestro.  ‚Ä¢ Conceptos b√°sicos de la biblioteca PyTorch optimizada para trabajar con tensores.  ‚Ä¢ Una visi√≥n general de los conceptos y m√©todos tradicionales de PNL.  ‚Ä¢ Redes neuronales proactivas (perceptr√≥n multicapa y otros).  ‚Ä¢ Mejora de RNN con memoria a corto plazo a largo plazo (LSTM) y bloques de recurrencia controlados ‚Ä¢ Modelos de predicci√≥n y transformaci√≥n de secuencias.  ‚Ä¢ Patrones de dise√±o de sistemas de PNL utilizados en producci√≥n. <br><a name="habracut"></a><br><h3>  Extracto  Palabras anidadas y otros tipos </h3><br>  Al resolver problemas de procesamiento de textos en lenguajes naturales, uno tiene que lidiar con varios tipos de tipos de datos discretos.  El ejemplo m√°s obvio son las palabras.  Muchas palabras (diccionario), por supuesto.  Entre otros ejemplos, s√≠mbolos, etiquetas de partes del discurso, entidades con nombre, tipos de entidades con nombre, atributos asociados con el an√°lisis, posiciones en el cat√°logo de productos, etc. De hecho, cualquier caracter√≠stica de entrada tomada de un finito (o infinito, pero contables) conjuntos. <br><br>  La base de la aplicaci√≥n exitosa del aprendizaje profundo en PNL es la representaci√≥n de tipos de datos discretos (por ejemplo, palabras) en forma de vectores densos.  Los t√©rminos "aprendizaje de representaci√≥n" e "incrustaci√≥n" significan aprender a mostrar / representar desde un tipo de datos discreto hasta un punto en un espacio vectorial.  Si los tipos discretos son palabras, una representaci√≥n vectorial densa se denomina incrustaci√≥n de palabras.  Ya hemos visto ejemplos de m√©todos de anidaci√≥n basados ‚Äã‚Äãen el n√∫mero de ocurrencias, por ejemplo, TF-IDF ("frecuencia de t√©rmino es la frecuencia inversa de un documento") en el cap√≠tulo 2. En este cap√≠tulo, nos centraremos en m√©todos de anidaci√≥n basados ‚Äã‚Äãen entrenamiento y m√©todos de anidaci√≥n basados ‚Äã‚Äãen predicciones (ver art√≠culo de Baroni et al. (Baroni et al., 2014]), en el que el entrenamiento de desempe√±o se realiza maximizando la funci√≥n objetivo para una tarea de aprendizaje espec√≠fica;  por ejemplo, prediciendo una palabra por contexto.  Los m√©todos de inversi√≥n basados ‚Äã‚Äãen la capacitaci√≥n son actualmente el est√°ndar debido a su amplia aplicabilidad y alta eficiencia.  De hecho, la incrustaci√≥n de palabras en tareas de PNL est√° tan extendida que se les llama "sriracha de PNL", ya que se puede esperar que su uso en cualquier tarea aumente la eficiencia de la soluci√≥n.  Pero este apodo es un poco enga√±oso, porque, a diferencia de Syraci, los archivos adjuntos generalmente no se agregan al modelo despu√©s del hecho, sino que son su componente b√°sico. <br><br>  En este cap√≠tulo, discutiremos las representaciones de vectores en relaci√≥n con la inserci√≥n de palabras: m√©todos de inclusi√≥n de palabras, m√©todos de optimizaci√≥n de inclusi√≥n de palabras para ense√±ar tareas con y sin un maestro, m√©todos de visualizaci√≥n de inclusi√≥n visual y tambi√©n m√©todos de combinaci√≥n de inclusi√≥n de palabras para oraciones y documentos.  Sin embargo, no olvide que los m√©todos descritos aqu√≠ se aplican a cualquier tipo discreto. <br><br><h3>  ¬øPor qu√© capacitaci√≥n en inversiones? </h3><br>  En los cap√≠tulos anteriores, le mostramos los m√©todos habituales para crear representaciones vectoriales de palabras.  Es decir, aprendi√≥ a usar representaciones unitarias: vectores con una longitud que coincide con el tama√±o del diccionario, con ceros en todas las posiciones, excepto uno que contiene el valor 1 correspondiente a una palabra espec√≠fica.  Adem√°s, se encontr√≥ con representaciones del n√∫mero de ocurrencias: vectores de longitud igual al n√∫mero de palabras √∫nicas en el modelo, que contienen el n√∫mero de ocurrencias de palabras en la oraci√≥n en las posiciones correspondientes.  Dichas representaciones tambi√©n se denominan representaciones distributivas, ya que su contenido / significado significativo se refleja en varias dimensiones del vector.  La historia de las representaciones distributivas ha estado ocurriendo durante muchas d√©cadas (ver el art√≠culo de Firth [Firth, 1935]); son excelentes para muchos modelos de aprendizaje autom√°tico y redes neuronales.  Estas representaciones se construyen de forma heur√≠stica1 y no est√°n formadas en datos. <br><br>  La representaci√≥n distribuida obtuvo su nombre porque las palabras en ellas est√°n representadas por un vector denso de una dimensi√≥n mucho m√°s peque√±a (por ejemplo, d = 100 en lugar del tama√±o de todo el diccionario, que puede ser del orden <img src="https://habrastorage.org/webt/vr/yx/0r/vryx0rlwodoo2krhnfdldvleqaq.png" alt="imagen">  ), y el significado y otras propiedades de la palabra se distribuyen en varias dimensiones de este vector denso. <br><br>  Las representaciones densas de baja dimensi√≥n obtenidas como resultado del entrenamiento tienen varias ventajas en comparaci√≥n con los vectores unitarios que contienen el n√∫mero de ocurrencias que encontramos en los cap√≠tulos anteriores.  Primero, la reducci√≥n de dimensionalidad es computacionalmente eficiente.  En segundo lugar, las representaciones basadas en el n√∫mero de ocurrencias conducen a vectores de alta dimensi√≥n con codificaci√≥n excesiva de la misma informaci√≥n en diferentes dimensiones, y su poder estad√≠stico no es demasiado grande.  En tercer lugar, demasiada dimensionalidad de los datos de entrada puede conducir a problemas en el aprendizaje autom√°tico y la optimizaci√≥n, un fen√≥meno a menudo llamado la maldici√≥n de la dimensionalidad ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://bit.ly/2CrhQXm</a> ).  Para resolver este problema con la dimensionalidad, se utilizan varios m√©todos para reducir la dimensi√≥n, por ejemplo, la descomposici√≥n de valores singulares (SVD) y el m√©todo de an√°lisis de componentes principales (PCA), pero, ir√≥nicamente, estos enfoques no se adaptan bien a las dimensiones del orden de millones ( caso t√≠pico en PNL).  En cuarto lugar, las representaciones aprendidas de (o ajustadas en base a) datos espec√≠ficos de un problema son ideales para esta tarea en particular.  En el caso de algoritmos heur√≠sticos como TF-IDF y m√©todos de reducci√≥n dimensional como SVD, no est√° claro si la funci√≥n de optimizaci√≥n objetiva es adecuada para una tarea particular con este m√©todo de inclusi√≥n. <br><br><h3>  Eficiencia de la inversi√≥n </h3><br>  Para comprender c√≥mo funcionan las incrustaciones, considere un ejemplo de un vector unitario por el cual la matriz de peso en una capa lineal se multiplica, como se muestra en la Fig.  5.1.  En los cap√≠tulos 3 y 4, el tama√±o de los vectores unitarios coincidi√≥ con el tama√±o del diccionario.  Un vector se llama unitario porque contiene 1 en la posici√≥n correspondiente a una palabra en particular, lo que indica su presencia. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4q/ag/or4qagtvn_dh4sxqok-q0tcsfmi.png" alt="imagen"></div><br>  Fig.  5.1.  Un ejemplo de multiplicaci√≥n matricial para el caso de un vector unitario y una matriz de pesos de una capa lineal.  Como el vector unitario contiene todos los ceros y solo una unidad, la posici√≥n de esta unidad desempe√±a el papel del operador de elecci√≥n al multiplicar la matriz.  Esto se muestra en la figura como un oscurecimiento de las c√©lulas de la matriz de peso y el vector resultante.  Aunque este m√©todo de b√∫squeda funciona, requiere un gran consumo de recursos inform√°ticos y es ineficiente, ya que el vector unitario se multiplica por cada uno de los n√∫meros en la matriz de peso y la suma se calcula en filas <br><br>  Por definici√≥n, el n√∫mero de filas de la matriz de pesos de una capa lineal que recibe un vector unitario en la entrada debe ser igual al tama√±o de este vector unitario.  Al multiplicar la matriz, como se muestra en la Fig.  5.1, el vector resultante es en realidad una cadena correspondiente a un elemento distinto de cero de un vector unitario.  Seg√∫n esta observaci√≥n, puede omitir el paso de multiplicaci√≥n y usar un valor entero como √≠ndice para extraer la fila deseada. <br><br>  Una √∫ltima nota con respecto al rendimiento de la inversi√≥n: a pesar del ejemplo en la Figura  5.1, donde la dimensi√≥n de la matriz de peso coincide con la dimensi√≥n del vector unitario de entrada, esto est√° lejos de ser siempre el caso.  De hecho, los archivos adjuntos a menudo se usan para representar palabras de un espacio de menor dimensi√≥n de lo que ser√≠a necesario si se usa un vector unitario o representa el n√∫mero de ocurrencias.  Un tama√±o de inversi√≥n t√≠pico en art√≠culos cient√≠ficos es de 25 a 500 mediciones, y la elecci√≥n de un valor espec√≠fico se reduce a la cantidad de memoria de GPU disponible. <br><br><h3>  Enfoques de aprendizaje de apego </h3><br>  El prop√≥sito de este cap√≠tulo no es ense√±arle t√©cnicas espec√≠ficas para invertir palabras, sino ayudarlo a descubrir qu√© son las inversiones, c√≥mo y d√≥nde se pueden aplicar, cu√°l es la mejor manera de usarlas en los modelos y cu√°les son sus limitaciones.  El hecho es que, en la pr√°ctica, uno rara vez tiene que escribir nuevos algoritmos de aprendizaje para la inserci√≥n de palabras.  Sin embargo, en esta subsecci√≥n daremos una breve descripci√≥n de los enfoques modernos de dicha capacitaci√≥n.  El aprendizaje en todos los m√©todos de anidaci√≥n de palabras se realiza utilizando solo palabras (es decir, datos sin etiquetar), pero con un maestro.  Esto es posible debido a la creaci√≥n de tareas de ense√±anza auxiliar con el maestro, en las que los datos se marcan impl√≠citamente, por las razones por las cuales la representaci√≥n optimizada para resolver la tarea auxiliar debe capturar muchas propiedades estad√≠sticas y ling√º√≠sticas del corpus de texto con el fin de traer al menos alg√∫n beneficio.  Aqu√≠ hay algunos ejemplos de tales tareas auxiliares. <br><br><ul><li>  Predecir la siguiente palabra en una secuencia de palabras dada.  Tambi√©n lleva el nombre del problema de modelado de idiomas. </li><li>  Predecir una palabra que falta por palabras ubicadas antes y despu√©s. </li><li>  Predecir palabras dentro de una ventana espec√≠fica, independientemente de la posici√≥n, para una palabra determinada. </li></ul><br>  Por supuesto, esta lista no est√° completa y la elecci√≥n de un problema auxiliar depende de la intuici√≥n del desarrollador del algoritmo y los costos computacionales.  Los ejemplos incluyen GloVe, Bolsa de palabras continua (CBOW), Skipgrams, etc. Los detalles se pueden encontrar en el Cap√≠tulo 10 del libro de Goldberg (Goldberg, 2017), pero discutiremos brevemente el modelo CBOW aqu√≠.  Sin embargo, en la mayor√≠a de los casos, es suficiente usar archivos adjuntos de palabras previamente entrenados y ajustarlos a la tarea existente. <br><br><h3>  Aplicaci√≥n pr√°ctica de archivos adjuntos pre-entrenados </h3><br>  La mayor parte de este cap√≠tulo, as√≠ como el resto del libro, trata sobre el uso de archivos adjuntos de palabras previamente entrenados.  Entrenado previamente usando uno de los muchos m√©todos descritos anteriormente en un cuerpo grande, por ejemplo, el cuerpo completo de Google News, Wikipedia o Common Crawl1, los archivos adjuntos de palabras se pueden descargar y usar libremente.  M√°s adelante en el cap√≠tulo, mostraremos c√≥mo encontrar y cargar correctamente estos archivos adjuntos, estudiar algunas propiedades de las incrustaciones de palabras y dar ejemplos de uso de incrustaciones de palabras previamente capacitadas en tareas de PNL. <br><br><h3>  Descargar archivos adjuntos </h3><br>  Los archivos adjuntos de palabras se han vuelto tan populares y extendidos que muchas opciones diferentes est√°n disponibles para descargar, desde Word2Vec2 original a Stanford GloVe ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://stanford.io/2PSIvPZ</a> ), incluido FastText3 de Facebook ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://fasttext.cc /</a> ) y muchos otros.  Por lo general, los archivos adjuntos se entregan en el siguiente formato: cada l√≠nea comienza con una palabra / tipo seguido de una secuencia de n√∫meros (es decir, una representaci√≥n vectorial).  La longitud de esta secuencia es igual a la dimensi√≥n de la presentaci√≥n (dimensi√≥n del archivo adjunto).  La dimensi√≥n de las inversiones suele ser del orden de cientos.  El n√∫mero de tipos de tokens suele ser igual al tama√±o del diccionario y asciende a aproximadamente un mill√≥n.  Por ejemplo, aqu√≠ est√°n las primeras siete dimensiones de los vectores de perros y gatos de GloVe. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n5/g7/x0/n5g7x0mos74bxchmiuexmcl7yw8.png" alt="imagen"></div><br>  Para una carga y manejo eficiente de los archivos adjuntos, describimos la clase auxiliar PreTrainedEmbeddings (Ejemplo 5.1).  Crea un √≠ndice de todos los vectores de palabras almacenados en la RAM para simplificar la b√∫squeda r√°pida y las consultas de los vecinos m√°s cercanos con la ayuda del paquete de c√°lculo de vecino m√°s cercano, molesto. <br><br>  Ejemplo 5.1.  Uso de archivos adjuntos de palabras previamente entrenados <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nx/hh/hh/nxhhhhz9wsdjl1tzms5ur8e4bvk.png" alt="imagen"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y1/8u/vb/y18uvbmsjpbvwno_wp5dsui8_38.png" alt="imagen"></div><br>  En estos ejemplos, usamos la incrustaci√≥n de las palabras GloVe.  Debe descargarlos y crear una instancia de la clase PreTrainedEmbeddings, como se muestra en la Entrada [1] del Ejemplo 5.1. <br><br><h3>  Relaciones entre archivos adjuntos de palabras </h3><br>  La propiedad clave de las incrustaciones de palabras es la codificaci√≥n de relaciones sint√°cticas y sem√°nticas, manifestadas en forma de patrones de uso de palabras.  Por ejemplo, generalmente se habla de gatos y perros de manera muy similar (discuten sobre sus mascotas, h√°bitos alimenticios, etc.).  Como resultado, los apegos por las palabras gatos y perros est√°n mucho m√°s cerca uno del otro que los apegos por los nombres de otros animales, digamos patos y elefantes. <br><br>  Hay muchas formas de estudiar las relaciones sem√°nticas codificadas en archivos adjuntos de palabras.  Uno de los m√©todos m√°s populares es utilizar la tarea de analog√≠a (uno de los tipos comunes de tareas de pensamiento l√≥gico en ex√°menes como SAT): <br><br>  Palabra1: Palabra2 :: Palabra3: ______ <br><br>  En esta tarea, es necesario determinar el cuarto, dada la conexi√≥n entre los dos primeros, por las tres palabras dadas.  Con la ayuda de palabras anidadas, este problema puede codificarse espacialmente.  Primero, reste Word2 de Word1.  El vector de diferencia entre ellos codifica la relaci√≥n entre Word1 y Word2.  Esta diferencia se puede agregar a Slovo3 y el resultado ser√° el vector m√°s cercano a la cuarta palabra que falta.  Para resolver el problema de analog√≠a, es suficiente consultar a los vecinos m√°s cercanos por √≠ndice utilizando este vector obtenido.  La funci√≥n correspondiente que se muestra en el ejemplo 5.2 hace exactamente lo que se describi√≥ anteriormente: utiliza la aritm√©tica vectorial y un √≠ndice aproximado de los vecinos m√°s cercanos para encontrar el elemento que falta en la analog√≠a. <br><br>  Ejemplo 5.2.  Resolver un problema de analog√≠a usando incrustaciones de palabras <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q3/qv/sl/q3qvslovjualenm1jkjn6sftbtk.png" alt="imagen"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1u/i_/hi/1ui_hizmvkfh5xqy1aqkegqqjfc.png" alt="imagen"></div><br><br>  Curiosamente, usando una analog√≠a verbal simple, se puede demostrar c√≥mo las incrustaciones de palabras pueden capturar una variedad de relaciones sem√°nticas y sint√°cticas (Ejemplo 5.3). <br><br>  Ejemplo 5.3  Codificaci√≥n con la ayuda de incrustaciones de palabras de muchas conexiones ling√º√≠sticas en el ejemplo de tareas en la analog√≠a de SAT <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/rf/v0/q1rfv0qixjwl9mob64rfru-2vgw.png" alt="imagen"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hk/wt/j0/hkwtj0_ea88pxvm-2q3zelcjlfq.png" alt="imagen"></div><br>  Aunque parezca que las conexiones reflejan claramente el funcionamiento del lenguaje, no todo es tan simple.  Como lo demuestra el ejemplo 5.4, las conexiones se pueden definir incorrectamente porque los vectores de palabras se determinan en funci√≥n de su aparici√≥n conjunta. <br><br>  Ejemplo 5.4.  Un ejemplo que ilustra el peligro de codificar el significado de las palabras en funci√≥n de la coincidencia. ¬°A veces no funciona! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b1/xf/t3/b1xft37n3jlu6lwivpjpi_shnmm.png" alt="imagen"></div><br>  El ejemplo 5.5 ilustra una de las combinaciones m√°s comunes al codificar roles de g√©nero. <br><br>  Ejemplo 5.5  Tenga cuidado con los atributos protegidos, como el g√©nero, codificados por archivos adjuntos de palabras.  Pueden conducir a sesgos no deseados en modelos futuros. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ga/5y/cl/ga5yclk4wb32h5rpwujbl96yry0.png" alt="imagen"></div><br>  Resulta que es bastante dif√≠cil distinguir entre patrones de lenguaje y prejuicios culturales profundamente arraigados.  Por ejemplo, los m√©dicos no son siempre hombres y las enfermeras no siempre son mujeres, pero tales prejuicios son tan persistentes que se reflejan en el lenguaje y, como resultado, en los vectores de palabras, como se muestra en el ejemplo 5.6. <br><br>  Ejemplo 5.6.  Prejuicios culturales "cosidos" en los vectores de palabras <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oz/ap/xz/ozapxzj5jxnwwxdcq33sc_dig9q.png" alt="imagen"></div><br>  No debemos olvidar los posibles errores sistem√°ticos en las inversiones, teniendo en cuenta el crecimiento de su popularidad y prevalencia en las aplicaciones de PNL.  La erradicaci√≥n de errores sistem√°ticos en la inclusi√≥n de palabras es un campo nuevo y muy interesante de investigaci√≥n cient√≠fica (ver el art√≠culo de Bolukbashi et al. [Bolukbasi et al., 2016]).  Le recomendamos que consulte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ethicsinnlp.org</a> donde puede encontrar informaci√≥n actualizada sobre √©tica <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">transversal</a> y PNL. <br><br><h3>  Sobre los autores </h3><br>  <b>Delip Rao</b> es el fundador de la firma de consultor√≠a Joostware con sede en San Francisco, especializada en aprendizaje autom√°tico e investigaci√≥n en PNL.  Uno de los cofundadores del Fake News Challenge, una iniciativa dise√±ada para reunir a hackers e investigadores en el campo de la IA sobre las tareas de verificaci√≥n de hechos en los medios.  Delip trabaj√≥ anteriormente en investigaci√≥n y productos de software relacionados con la PNL en Twitter y Amazon (Alexa). <br><br>  <b>Brian McMahan</b> es investigador en Wells Fargo y se centra principalmente en PNL.  Anteriormente trabaj√≥ en Joostware. <br><br>  ¬ªSe puede encontrar m√°s informaci√≥n sobre el libro en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web del editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Contenidos</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Extracto</a> <br><br>  <b>Cup√≥n de</b> 25% de descuento para <b>vendedores ambulantes</b> - <b>PyTorch</b> <br><br>  Tras el pago de la versi√≥n en papel del libro, se env√≠a un libro electr√≥nico por correo electr√≥nico. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/475488/">https://habr.com/ru/post/475488/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../475476/index.html">Malla de datos: c√≥mo trabajar con datos sin un monolito</a></li>
<li><a href="../475478/index.html">Experiencia de Netflix: Netflix Inside</a></li>
<li><a href="../475480/index.html">Que eres C√≥mo distinguimos la parodia de los humanos, e incluso ganamos</a></li>
<li><a href="../475482/index.html">¬øC√≥mo se convirti√≥ la tarea de prueba en una biblioteca de producci√≥n?</a></li>
<li><a href="../475486/index.html">AR-creadores: el surgimiento de una nueva profesi√≥n</a></li>
<li><a href="../475490/index.html">Trabajar bajo presi√≥n</a></li>
<li><a href="../475494/index.html">‚Äú¬øHay vida despu√©s de Signor?‚Äù ¬øO de qu√© hablaremos en SECR-2019?</a></li>
<li><a href="../475496/index.html">C√≥mo determinar la direcci√≥n de un contrato inteligente antes de la implementaci√≥n: usando CREATE2 para el intercambio de cifrado</a></li>
<li><a href="../475498/index.html">Windows Server Core vs. GUI y compatibilidad de software</a></li>
<li><a href="../475506/index.html">Entrevista con Mikhail Chinkov sobre el trabajo y la vida en Berl√≠n.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>