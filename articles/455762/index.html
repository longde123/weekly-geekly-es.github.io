<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëàüèº üß† ‚õΩÔ∏è Una breve introducci√≥n a las cadenas de Markov. üí® üë± üïü</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En 1998, Lawrence Page, Sergey Brin, Rajiv Motwani y Terry Vinograd publicaron el art√≠culo "The PageRank Citation Ranking: Bringing Order to the Web",...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Una breve introducci√≥n a las cadenas de Markov.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455762/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif" alt="imagen"></div><br>  En 1998, Lawrence Page, Sergey Brin, Rajiv Motwani y Terry Vinograd publicaron el art√≠culo "The PageRank Citation Ranking: Bringing Order to the Web", que describ√≠a el ahora famoso algoritmo PageRank, que se convirti√≥ en la base de Google.  Despu√©s de un poco menos de dos d√©cadas, Google se convirti√≥ en un gigante, y aunque su algoritmo ha evolucionado mucho, PageRank sigue siendo el "s√≠mbolo" de los algoritmos de clasificaci√≥n de Google (aunque solo unas pocas personas realmente pueden decir cu√°nto peso tiene el algoritmo hoy) . <br><br>  Desde un punto de vista te√≥rico, es interesante observar que una de las interpretaciones est√°ndar del algoritmo PageRank se basa en un concepto simple pero fundamental de las cadenas de Markov.  Del art√≠culo veremos que las cadenas de Markov son herramientas poderosas para el modelado estoc√°stico que pueden ser √∫tiles para cualquier cient√≠fico de datos.  En particular, responderemos preguntas tan b√°sicas: ¬øqu√© son las cadenas de Markov, qu√© buenas propiedades poseen y qu√© se puede hacer con su ayuda? <br><a name="habracut"></a><br><h4>  Breve rese√±a </h4><br>  En la primera secci√≥n, damos las definiciones b√°sicas necesarias para comprender las cadenas de Markov.  En la segunda secci√≥n, consideramos el caso especial de las cadenas de Markov en un espacio de estado finito.  En la tercera secci√≥n, consideramos algunas de las propiedades elementales de las cadenas de Markov e ilustramos estas propiedades con muchos ejemplos peque√±os.  Finalmente, en la cuarta secci√≥n, asociamos las cadenas de Markov con el algoritmo PageRank y vemos con un ejemplo artificial c√≥mo se pueden usar las cadenas de Markov para clasificar los nodos de un gr√°fico. <br><br><blockquote>  <strong>Nota</strong>  Comprender esta publicaci√≥n requiere conocer los conceptos b√°sicos de probabilidad y √°lgebra lineal.  En particular, se utilizar√°n los siguientes conceptos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener">probabilidad condicional</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener">vector propio</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener">f√≥rmula de probabilidad completa</a> . </blockquote><br><hr><br><h3>  ¬øQu√© son las cadenas de Markov? </h3><br><h4>  Variables aleatorias y procesos aleatorios. </h4><br>  Antes de presentar el concepto de cadenas de Markov, recordemos brevemente los conceptos b√°sicos, pero importantes, de la teor√≠a de la probabilidad. <br><br>  Primero, fuera del lenguaje matem√°tico, una <strong>variable aleatoria</strong> X es una cantidad determinada por el resultado de un fen√≥meno aleatorio.  Su resultado puede ser un n√∫mero (o "similitud de un n√∫mero", por ejemplo, vectores) u otra cosa.  Por ejemplo, podemos definir una variable aleatoria como el resultado de una tirada de un dado (n√∫mero) o como el resultado de un lanzamiento de moneda (no un n√∫mero, a menos que designemos, por ejemplo, "√°guila" como 0, pero "colas" como 1).  Tambi√©n mencionamos que el espacio de posibles resultados de una variable aleatoria puede ser discreto o continuo: por ejemplo, una variable aleatoria normal es continua y una variable aleatoria de Poisson es discreta. <br><br>  Adem√°s, podemos definir un <strong>proceso aleatorio</strong> (tambi√©n llamado estoc√°stico) como un conjunto de variables aleatorias indexadas por el conjunto T, que a menudo denota diferentes puntos en el tiempo (en lo que sigue asumiremos esto).  Los dos casos m√°s comunes: T puede ser un conjunto de n√∫meros naturales (proceso aleatorio con tiempo discreto) o un conjunto de n√∫meros reales (proceso aleatorio con tiempo continuo).  Por ejemplo, si lanzamos una moneda todos los d√≠as, estableceremos un proceso aleatorio con tiempo discreto, y el valor siempre cambiante de una opci√≥n en el intercambio establecer√° un proceso aleatorio con tiempo continuo.  Las variables aleatorias en diferentes momentos pueden ser independientes entre s√≠ (un ejemplo con un lanzamiento de moneda) o tener alg√∫n tipo de dependencia (un ejemplo con el valor de la opci√≥n);  Adem√°s, pueden tener un espacio de estado continuo o discreto (el espacio de resultados posibles en cada momento). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31e/ada/2f8/31eada2f80d66f0df4c007ec8da11579.jpg"></div><br>  <i>Diferentes tipos de procesos aleatorios (discretos / continuos en espacio / tiempo).</i> <br><br><h4>  Propiedad de Markov y cadena de Markov </h4><br>  Existen familias bien conocidas de procesos aleatorios: procesos gaussianos, procesos de Poisson, modelos autorregresivos, modelos de promedio m√≥vil, cadenas de Markov y otros.  Cada uno de estos casos individuales tiene ciertas propiedades que nos permiten explorarlos y comprenderlos mejor. <br><br>  Una de las propiedades que simplifica enormemente el estudio de un proceso aleatorio es la propiedad de Markov.  Si lo explicamos en un lenguaje muy informal, la propiedad Markov nos dice que si conocemos el valor obtenido por alg√∫n proceso aleatorio en un momento dado, no recibiremos ninguna informaci√≥n adicional sobre el comportamiento futuro del proceso, recolectando otra informaci√≥n sobre su pasado.  En un lenguaje m√°s matem√°tico: en cualquier momento, la distribuci√≥n condicional de estados futuros de un proceso con estados actuales y pasados ‚Äã‚Äãdepende solo del estado actual, y no de estados pasados ‚Äã‚Äã(la <strong>propiedad de la falta de memoria</strong> ).  Un proceso aleatorio con una propiedad de Markov se llama <strong>proceso de Markov</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44f/1ba/f48/44f1baf48ceb669e0416489eea2bae35.png"></div><br>  <i>La propiedad de Markov significa que si conocemos el estado actual en un momento dado, no necesitamos ninguna informaci√≥n adicional sobre el futuro, recopilada del pasado.</i> <br><br>  En base a esta definici√≥n, podemos formular la definici√≥n de "cadenas de Markov homog√©neas con tiempo discreto" (en adelante, por simplicidad las llamaremos "cadenas de Markov").  <strong>La cadena de Markov</strong> es un proceso de Markov con tiempo discreto y un espacio de estado discreto.  Entonces, una cadena de Markov es una secuencia discreta de estados, cada uno de los cuales se toma de un espacio de estado discreto (finito o infinito), satisfaciendo la propiedad de Markov. <br><br>  Matem√°ticamente, podemos denotar la cadena de Markov de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/114/3b7/fc3/1143b7fc371f3142534c2b886bf3e69c.png"></div><br>  donde en cada momento el proceso toma sus valores de un conjunto discreto E, de modo que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/886/a22/d76/886a22d7671798102ee3d94fe9868b81.png"></div><br>  Entonces la propiedad de Markov implica que tenemos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/edc/8bf/384/edc8bf38422705e72c9dd7d094b249db.png"></div><br>  Tenga en cuenta nuevamente que esta √∫ltima f√≥rmula refleja el hecho de que, para la cronolog√≠a (d√≥nde estoy ahora y d√≥nde estaba antes), la distribuci√≥n de probabilidad del siguiente estado (donde ser√© el pr√≥ximo) depende del estado actual, pero no de los estados pasados. <br><br><blockquote>  <strong>Nota</strong>  En esta publicaci√≥n introductoria, decidimos hablar solo sobre cadenas de Markov homog√©neas simples con tiempo discreto.  Sin embargo, tambi√©n hay cadenas de Markov no homog√©neas (dependientes del tiempo) y / o cadenas de tiempo continuo.  En este art√≠culo no consideraremos tales variaciones del modelo.  Tambi√©n vale la pena se√±alar que la definici√≥n anterior de una propiedad de Markov est√° extremadamente simplificada: la verdadera definici√≥n matem√°tica utiliza el concepto de filtrado, que va mucho m√°s all√° de nuestro conocimiento introductorio del modelo. </blockquote><br><h4>  Caracterizamos la din√°mica de aleatoriedad de una cadena de Markov </h4><br>  En la subsecci√≥n anterior, nos familiarizamos con la estructura general correspondiente a cualquier cadena de Markov.  Veamos qu√© necesitamos para establecer una "instancia" espec√≠fica de un proceso tan aleatorio. <br><br>  Primero, observamos que la determinaci√≥n completa de las caracter√≠sticas de un proceso aleatorio con un tiempo discreto que no satisface la propiedad de Markov puede ser dif√≠cil: la distribuci√≥n de probabilidad en un momento dado puede depender de uno o m√°s momentos en el pasado y / o en el futuro.  Todas estas posibles dependencias de tiempo pueden potencialmente complicar la creaci√≥n de una definici√≥n de proceso. <br><br>  Sin embargo, debido a la propiedad de Markov, la din√°mica de la cadena de Markov es bastante simple de determinar.  Y de hecho.  solo necesitamos determinar dos aspectos: la <strong>distribuci√≥n de probabilidad inicial</strong> (es decir, la distribuci√≥n de probabilidad en el tiempo n = 0), denotada por <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/595/90e/140/59590e140cdb348c943d9dcab0ea011d.png"></div><br>  y <strong>la matriz de probabilidad de transici√≥n</strong> (que nos da las probabilidades de que el estado en el tiempo n + 1 sea el siguiente para otro estado en el tiempo n para cualquier par de estados), denotado por <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/011/aee/574/011aee5747fe7e42fa09bf044c421e26.png"></div><br>  Si se conocen estos dos aspectos, entonces la din√°mica completa (probabil√≠stica) del proceso est√° claramente definida.  Y, de hecho, la probabilidad de cualquier resultado del proceso puede calcularse c√≠clicamente. <br><br>  Ejemplo: supongamos que queremos saber la probabilidad de que los primeros 3 estados del proceso tengan valores (s0, s1, s2).  Es decir, queremos calcular la probabilidad <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c5/991/81b/6c599181b1fd3892391711f311878b72.png"></div><br>  Aqu√≠ aplicamos la f√≥rmula para la probabilidad total, que establece que la probabilidad de obtener (s0, s1, s2) es igual a la probabilidad de obtener el primer s0 multiplicado por la probabilidad de obtener s1, dado que anteriormente recibimos s0 multiplicado por la probabilidad de obtener s2 teniendo en cuenta el hecho de que llegamos antes en el orden s0 y s1.  Matem√°ticamente, esto se puede escribir como <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a46/da3/64f/a46da364f28d5b69055700759db02663.png"></div><br>  Y luego se revela la simplificaci√≥n, determinada por el supuesto de Markov.  Y, de hecho, en el caso de las cadenas largas, obtenemos probabilidades fuertemente condicionales para los √∫ltimos estados.  Sin embargo, en el caso de las cadenas de Markov, podemos simplificar esta expresi√≥n aprovechando el hecho de que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ab/7f6/568/0ab7f6568e28bb562ebb287252422d51.png"></div><br>  de esta manera <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b6c/700/30a/b6c70030a8627d87c39dab96c147513a.png"></div><br>  Dado que caracterizan completamente la din√°mica probabil√≠stica del proceso, muchos eventos complejos pueden calcularse solo sobre la base de la distribuci√≥n de probabilidad inicial q0 y la matriz de probabilidad de transici√≥n p.  Tambi√©n vale la pena mencionar una conexi√≥n b√°sica m√°s: la expresi√≥n de la distribuci√≥n de probabilidad en el tiempo n + 1, expresada con respecto a la distribuci√≥n de probabilidad en el tiempo n <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/f31/a8a/5aef31a8ae120a25e5b43d6534dc20ff.png"></div><br><h3>  Cadenas de Markov en espacios de estados finitos </h3><br><h4>  Representaci√≥n matricial y gr√°fica </h4><br>  Aqu√≠ suponemos que el conjunto E tiene un n√∫mero finito de estados posibles N: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d57/788/f81/d57788f81d0a92aa1d94ef572bdf25e3.png"></div><br>  Entonces, la distribuci√≥n de probabilidad inicial se puede describir como <strong>un vector de fila</strong> q0 de tama√±o N, y las probabilidades de transici√≥n se pueden describir como una matriz p de tama√±o N por N, de modo que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/100/6f8/6ae/1006f86aeff6699711058bd890190917.png"></div><br>  La ventaja de esta notaci√≥n es que si denotamos la distribuci√≥n de probabilidad en el paso n por el vector de fila qn de modo que se especifiquen sus componentes <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/19b/4ae/00619b4ae1601eacdb8fd7ce248f1738.png"></div><br>  entonces se preservan las relaciones simples de la matriz <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a12/cdc/a7b/a12cdca7b75ef09ceaacef33a3667549.png"></div><br>  (aqu√≠ no consideraremos la prueba, pero es muy simple reproducirla). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23c/3c6/a10/23c3c6a102bd7aa079c36a75f60a5e42.png"></div><br>  <i>Si multiplicamos el vector de fila de la derecha, que describe la distribuci√≥n de probabilidad en una etapa de tiempo dada, por la matriz de probabilidad de transici√≥n, entonces obtenemos la distribuci√≥n de probabilidad en la siguiente etapa de tiempo.</i> <br><br>  Entonces, como vemos, la transici√≥n de la distribuci√≥n de probabilidad de una etapa dada a la siguiente simplemente se define como la multiplicaci√≥n correcta del vector de filas de probabilidades del paso inicial por la matriz p.  Adem√°s, esto implica que tenemos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/35b/072/e0c/35b072e0c74397094e5bf6a9dab11417.png"></div><br>  La din√°mica aleatoria de una cadena de Markov en un espacio de estado finito se puede representar f√°cilmente como un gr√°fico orientado normalizado de modo que cada nodo del gr√°fico sea un estado, y para cada par de estados (ei, ej) existe un borde que va de ei a ej si p (ei, ej )&gt; 0.  Entonces el valor de borde ser√° la misma probabilidad p (ei, ej). <br><br><h4>  Ejemplo: un lector de nuestro sitio </h4><br>  Vamos a ilustrar todo esto con un simple ejemplo.  Considere el comportamiento cotidiano de un visitante ficticio de un sitio.  Todos los d√≠as tiene 3 condiciones posibles: el lector no visita el sitio ese d√≠a (N), el lector visita el sitio, pero no lee la publicaci√≥n completa (V), y el lector visita el sitio y lee una publicaci√≥n completa (R).  Entonces, tenemos el siguiente espacio de estado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/242/b1c/627/242b1c62745a4a13f2a24b8f919c6820.png"></div><br>  Supongamos que, el primer d√≠a, este lector tiene un 50% de posibilidades de acceder solo al sitio y un 50% de posibilidades de visitar el sitio y leer al menos un art√≠culo.  El vector que describe la distribuci√≥n de probabilidad inicial (n = 0) se ve as√≠: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/362/a5a/045/362a5a0455c939f9d855d65605945c90.png"></div><br>  Tambi√©n imagine que se observan las siguientes probabilidades: <br><br><ul><li>  cuando el lector no lo visita un d√≠a, existe una probabilidad del 25% de no visitarlo al d√≠a siguiente, una probabilidad del 50% solo de visitarlo y del 25% de visitar y leer el art√≠culo </li><li>  cuando el lector visita el sitio un d√≠a, pero no lee, tiene un 50% de posibilidades de volver a visitarlo al d√≠a siguiente y no leer el art√≠culo, y un 50% de posibilidades de visitar y leer </li><li>  cuando un lector visita y lee un art√≠culo el mismo d√≠a, tiene un 33% de posibilidades de no iniciar sesi√≥n al d√≠a siguiente <em>(¬°espero que esta publicaci√≥n no tenga ese efecto!)</em> , un 33% de posibilidades de iniciar sesi√≥n solo en el sitio y 34% de visitar y leer el art√≠culo nuevamente </li></ul><br>  Luego tenemos la siguiente matriz de transici√≥n: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cf5/7e6/ba5/cf57e6ba5303d4e13cbb736e6115306d.png"></div><br>  De la subsecci√≥n anterior, sabemos c√≥mo calcular para este lector la probabilidad de cada estado al d√≠a siguiente (n = 1) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c64/a77/76c/c64a7776cfc56a5af1a0ccf495469ef7.png"></div><br>  La din√°mica probabil√≠stica de esta cadena de Markov se puede representar gr√°ficamente de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/797/9a1/8327979a1aa6edd9d462a0b40a4c072d.png"></div><br>  <i>Presentaci√≥n en forma de gr√°fico de la cadena de Markov, que modela el comportamiento de nuestro visitante inventado en el sitio.</i> <br><br><h3>  Propiedades de las cadenas de Markov </h3><br>  En esta secci√≥n, hablaremos solo sobre algunas de las propiedades o caracter√≠sticas m√°s b√°sicas de las cadenas de Markov.  No entraremos en detalles matem√°ticos, pero brindaremos una breve descripci√≥n de los puntos interesantes que deben estudiarse para usar las cadenas de Markov.  Como hemos visto, en el caso de un espacio de estado finito, la cadena de Markov se puede representar como un gr√°fico.  En el futuro, utilizaremos la representaci√≥n gr√°fica para explicar algunas propiedades.  Sin embargo, no olvide que estas propiedades no se limitan necesariamente al caso de un espacio de estado finito. <br><br><h4>  Descomponibilidad, periodicidad, irrevocabilidad y recuperabilidad. </h4><br>  En esta subsecci√≥n, comencemos con varias formas cl√°sicas de caracterizar un estado o una cadena completa de Markov. <br><br>  Primero, mencionamos que la cadena de Markov es <strong>indescomponible</strong> si es posible alcanzar cualquier estado desde cualquier otro estado (no es necesario que en un solo paso del tiempo).  Si el espacio de estado es finito y la cadena se puede representar como un gr√°fico, entonces podemos decir que el gr√°fico de una cadena de Markov no descomponible est√° fuertemente conectado (teor√≠a de los gr√°ficos). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cef/a39/05c/cefa3905cced27b4e27a4e9547fbe846.png"></div><br>  <i>Ilustraci√≥n de la propiedad de la descomposici√≥n (irreductibilidad).</i>  <i>La cadena de la izquierda no se puede acortar: desde 3 o 4 no podemos entrar en 1 o 2. La cadena de la derecha (se agrega un borde) se puede acortar: se puede alcanzar cada estado desde cualquier otro.</i> <br><br>  Un estado tiene un per√≠odo k si, al abandonarlo, para cualquier retorno a este estado, el n√∫mero de pasos de tiempo es un m√∫ltiplo de k (k es el divisor com√∫n m√°s grande de todas las longitudes posibles de rutas de retorno).  Si k = 1, entonces dicen que el estado es aperi√≥dico, y toda la cadena de Markov es <strong>aperi√≥dica</strong> si todos sus estados son aperi√≥dicos.  En el caso de una cadena de Markov irreducible, tambi√©n podemos mencionar que si un estado es aperi√≥dico, todos los dem√°s tambi√©n lo son. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cbe/65f/a07/cbe65fa07e7b816d385408824ba0ff39.png"></div><br>  <i>Ilustraci√≥n de la propiedad de periodicidad.</i>  <i>La cadena de la izquierda es peri√≥dica con k = 2: al salir de cualquier estado, volver a ella siempre requiere el n√∫mero de pasos m√∫ltiplo de 2. La cadena de la derecha tiene un per√≠odo de 3.</i> <br><br>  Un estado es <strong>irrevocable</strong> si, al salir del estado, existe una probabilidad distinta de cero de que nunca volvamos a √©l.  Por el contrario, un estado se considera <strong>retornable</strong> si sabemos que despu√©s de abandonar el estado podemos volver a √©l en el futuro con probabilidad 1 (si no es irrevocable). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6a0/1ad/7b0/6a01ad7b05e96f5a6606a8a48d127233.png"></div><br>  <i>Ilustraci√≥n de la propiedad de devoluci√≥n / irrevocabilidad.</i>  <i>La cadena de la izquierda tiene las siguientes propiedades: 1, 2 y 3 son irrevocables (al abandonar estos puntos no podemos estar absolutamente seguros de que volveremos a ellos) y tienen un per√≠odo de 3, y 4 y 5 son retornables (al abandonar estos puntos estamos absolutamente seguros que alg√∫n d√≠a volveremos a ellos) y tendremos un per√≠odo de 2. La cadena de la derecha tiene otra costilla, lo que hace que toda la cadena sea retornable y aperi√≥dica.</i> <br><br>  Para el estado de retorno, podemos calcular el tiempo de retorno promedio, que es el <strong>tiempo de retorno esperado</strong> al salir del estado.  Tenga en cuenta que incluso la probabilidad de un retorno es 1, esto no significa que el tiempo de retorno esperado sea finito.  Por lo tanto, entre todos los estados de retorno, podemos distinguir entre <strong>estados de retorno positivos</strong> (con un tiempo de retorno esperado finito) y <strong>estados de retorno cero</strong> (con un tiempo de retorno esperado infinito). <br><br><h4>  Distribuci√≥n estacionaria, comportamiento marginal y ergodicidad. </h4><br>  En esta subsecci√≥n, consideramos las propiedades que caracterizan algunos aspectos de la din√°mica (aleatoria) descrita por la cadena de Markov. <br><br>  La distribuci√≥n de probabilidad œÄ sobre el espacio de estado E se llama <strong>distribuci√≥n estacionaria</strong> si satisface la expresi√≥n <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/736/d70/e5d/736d70e5ddfc5aeb788d29ebfa79f9ec.png"></div><br>  Ya que tenemos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b98/c4e/e6c/b98c4ee6c3b8032b074c7543db816c7e.png"></div><br>  Entonces la distribuci√≥n estacionaria satisface la expresi√≥n <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b8/df4/0a8/2b8df40a8a9a2ae90eb49a7c60dafb55.png"></div><br>  Por definici√≥n, la distribuci√≥n de probabilidad estacionaria no cambia con el tiempo.  Es decir, si la distribuci√≥n inicial q es estacionaria, ser√° la misma en todas las etapas posteriores del tiempo.  Si el espacio de estados es finito, entonces p puede representarse como una matriz y œÄ como un vector de fila, y luego obtenemos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee0/565/9fc/ee05659fc4ed268ee3189342a76d9311.png"></div><br>  Esto nuevamente expresa el hecho de que la distribuci√≥n de probabilidad estacionaria no cambia con el tiempo (como vemos, multiplicar la distribuci√≥n de probabilidad a la derecha por p nos permite calcular la distribuci√≥n de probabilidad en la siguiente etapa del tiempo).  Tenga en cuenta que una cadena de Markov indescomponible tiene una distribuci√≥n de probabilidad estacionaria si y solo si uno de sus estados es un retorno positivo. <br><br>  Otra propiedad interesante relacionada con la distribuci√≥n de probabilidad estacionaria es la siguiente.  Si la cadena es de retorno positivo (es decir, tiene una distribuci√≥n estacionaria) y aperi√≥dica, entonces, cualesquiera que sean las probabilidades iniciales, la distribuci√≥n de probabilidad de la cadena converge a medida que los intervalos de tiempo tienden al infinito: dicen que la cadena tiene una <strong>distribuci√≥n limitante</strong> , que no es otra cosa, como una distribuci√≥n estacionaria.  En general, se puede escribir as√≠: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95b/fb9/1c6/95bfb91c67d024e2df40b0e6dcdaf747.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hacemos hincapi√© una vez m√°s en el hecho de que no hacemos suposiciones sobre la distribuci√≥n de probabilidad inicial: la distribuci√≥n de probabilidad de la cadena se reduce a una distribuci√≥n estacionaria (distribuci√≥n de equilibrio de la cadena) independientemente de los par√°metros iniciales. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, la </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ergodicidad</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es otra propiedad interesante relacionada con el comportamiento de la cadena de Markov. Si la cadena de Markov es indescomponible, entonces tambi√©n se dice que es "erg√≥dica" porque satisface el siguiente teorema erg√≥dico. Supongamos que tenemos una funci√≥n f (.) Que va del espacio de estado E al eje (esto puede ser, por ejemplo, el precio de estar en cada estado). Podemos determinar el valor promedio que mueve esta funci√≥n a lo largo de una trayectoria dada (promedio temporal). Para los en√©simos primeros t√©rminos, esto se denota como</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af1/8c5/4a3/af18c54a3d7dc1e1ad4a4015ab7ad64c.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tambi√©n podemos calcular el valor promedio de la funci√≥n f en el conjunto E, ponderado por la distribuci√≥n estacionaria (promedio espacial), que se denota </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04a/352/c77/04a352c77b0687ef3cc89f3b7e0edf38.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces el teorema erg√≥dico nos dice que cuando la trayectoria se vuelve infinitamente larga, el promedio de tiempo es igual al promedio espacial (ponderado por la distribuci√≥n estacionaria). </font><font style="vertical-align: inherit;">La propiedad de ergodicidad se puede escribir de la siguiente manera:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cb/37d/c4d/6cb37dc4dcf0a3e53cc8e6baec8f4b1a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En otras palabras, significa que en el l√≠mite anterior, el comportamiento de la trayectoria se vuelve insignificante y solo el comportamiento estacionario a largo plazo es importante al calcular el promedio temporal. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Volvamos al ejemplo con el lector del sitio. </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nuevamente, considere el ejemplo del lector del sitio. </font><font style="vertical-align: inherit;">En este simple ejemplo, es obvio que la cadena es indescomponible, aperi√≥dica y todos sus estados son positivamente retornables. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para mostrar qu√© resultados interesantes se pueden calcular utilizando las cadenas de Markov, consideramos el tiempo promedio de retorno al estado R (el estado "visita el sitio y lee el art√≠culo"). </font><font style="vertical-align: inherit;">En otras palabras, queremos responder la siguiente pregunta: si nuestro lector visita el sitio un d√≠a y lee un art√≠culo, ¬øcu√°ntos d√≠as tendremos que esperar en promedio para que regrese y lea el art√≠culo? </font><font style="vertical-align: inherit;">Intentemos obtener un concepto intuitivo de c√≥mo se calcula este valor. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Primero denotamos</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/57c/c74/2a257cc74db27e5ac89ffc1e06bd9ed9.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces queremos calcular m (R, R). </font><font style="vertical-align: inherit;">Hablando sobre el primer intervalo alcanzado despu√©s de salir de R, obtenemos</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f9/e1a/a6e/4f9e1aa6e04e736fde182693398a4dca.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, esta expresi√≥n requiere que para el c√°lculo de m (R, R) sepamos m (N, R) ym (V, R). </font><font style="vertical-align: inherit;">Estas dos cantidades se pueden expresar de manera similar:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e23/7cb/f2d/e237cbf2d81597544f800d38b5a59e91.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, obtuvimos 3 ecuaciones con 3 inc√≥gnitas y despu√©s de resolverlas obtenemos m (N, R) = 2.67, m (V, R) = 2.00 ym (R, R) = 2.54. </font><font style="vertical-align: inherit;">El tiempo promedio para volver al estado R es entonces 2.54. </font><font style="vertical-align: inherit;">Es decir, usando √°lgebra lineal, pudimos calcular el tiempo promedio de retorno al estado R (as√≠ como el tiempo promedio de transici√≥n de N a R y el tiempo promedio de transici√≥n de V a R). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para terminar con este ejemplo, veamos cu√°l ser√° la distribuci√≥n estacionaria de la cadena de Markov. </font><font style="vertical-align: inherit;">Para determinar la distribuci√≥n estacionaria, necesitamos resolver la siguiente ecuaci√≥n de √°lgebra lineal:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb3/73d/068/bb373d068a04d681c0501d8276731c0a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Es decir, necesitamos encontrar el vector propio izquierdo p asociado con el vector propio 1. Al resolver este problema, obtenemos la siguiente distribuci√≥n estacionaria: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0c6/abe/19e/0c6abe19e37b67af5f380eb3e5c0beb9.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distribuci√≥n estacionaria en el ejemplo con el lector del sitio. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambi√©n puede notar que œÄ (R) = 1 / m (R, R), y si un poco de reflexi√≥n, esta identidad es bastante l√≥gica (pero no hablaremos de esto en detalle).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dado que la cadena es indescomponible y aperi√≥dica, esto significa que a la larga la distribuci√≥n de probabilidad converge a una distribuci√≥n estacionaria (para cualquier par√°metro inicial). En otras palabras, cualquiera que sea el estado inicial del lector del sitio, si esperamos lo suficiente y seleccionamos un d√≠a al azar, obtendremos la probabilidad œÄ (N) de que el lector no visite el sitio ese d√≠a, la probabilidad œÄ (V) de que el lector se detendr√° pero no leer√° el art√≠culo, y la probabilidad es œÄ¬Æ de que el lector se detenga y lea el art√≠culo. Para comprender mejor la propiedad de la convergencia, echemos un vistazo al siguiente gr√°fico que muestra la evoluci√≥n de las distribuciones de probabilidad a partir de diferentes puntos de partida y (r√°pidamente) convergiendo a una distribuci√≥n estacionaria:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/128/58e/a88/12858ea88a0e3bd05950b9d30096b776.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualizaci√≥n de la convergencia de 3 distribuciones de probabilidad con diferentes par√°metros iniciales (azul, naranja y verde) a la distribuci√≥n estacionaria (rojo).</font></font></i> <br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejemplo cl√°sico: Algoritmo de PageRank </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Es hora de volver al PageRank! </font><font style="vertical-align: inherit;">Pero antes de continuar, vale la pena mencionar que la interpretaci√≥n de PageRank dada en este art√≠culo no es la √∫nica posible, y los autores del art√≠culo original no necesariamente se basaron en el uso de cadenas de Markov al desarrollar la metodolog√≠a. </font><font style="vertical-align: inherit;">Sin embargo, nuestra interpretaci√≥n es buena porque es muy clara.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Usuario web arbitrario </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PageRank est√° tratando de resolver el siguiente problema: ¬øc√≥mo podemos clasificar un conjunto existente (podemos suponer que este conjunto ya ha sido filtrado, por ejemplo, por alguna consulta) utilizando enlaces que ya existen entre las p√°ginas? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para resolver este problema y poder clasificar las p√°ginas, PageRank realiza aproximadamente el siguiente proceso. Creemos que un usuario arbitrario de Internet en el momento inicial est√° en una de las p√°ginas. Luego, este usuario comienza a moverse aleatoriamente, haciendo clic en cada p√°gina en uno de los enlaces que conducen a otra p√°gina del conjunto en cuesti√≥n (se supone que todos los enlaces que salen de estas p√°ginas est√°n prohibidos). En cualquier p√°gina, todos los enlaces v√°lidos tienen la misma probabilidad de hacer clic.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As√≠ es como definimos la cadena de Markov: las p√°ginas son estados posibles, las probabilidades de transici√≥n se establecen mediante enlaces de p√°gina a p√°gina (ponderadas de tal manera que en cada p√°gina todas las p√°ginas enlazadas tienen la misma probabilidad de selecci√≥n), y las propiedades de falta de memoria est√°n claramente determinadas por el comportamiento del usuario. Si tambi√©n suponemos que la cadena dada es positivamente retornable y aperi√≥dica (se utilizan peque√±os trucos para cumplir con estos requisitos), entonces a la larga la distribuci√≥n de probabilidad de la "p√°gina actual" converge a una distribuci√≥n estacionaria. Es decir, sea cual sea la p√°gina inicial, despu√©s de mucho tiempo, cada p√°gina tiene una probabilidad (casi fija) de actualizarse si elegimos un momento aleatorio en el tiempo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PageRank se basa en la siguiente hip√≥tesis: las p√°ginas m√°s probables en una distribuci√≥n estacionaria tambi√©n deber√≠an ser las m√°s importantes (visitamos estas p√°ginas a menudo porque obtienen enlaces de p√°ginas que tambi√©n se visitan con frecuencia durante las transiciones). </font><font style="vertical-align: inherit;">Luego, la distribuci√≥n de probabilidad estacionaria determina el valor de PageRank para cada estado.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejemplo Artificial </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para hacer esto mucho m√°s claro, veamos un ejemplo artificial. </font><font style="vertical-align: inherit;">Supongamos que tenemos un peque√±o sitio web con 7 p√°ginas, etiquetadas del 1 al 7, y los enlaces entre estas p√°ginas corresponden a la siguiente columna.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En aras de la claridad, no se muestran las probabilidades de cada transici√≥n en la animaci√≥n que se muestra arriba. </font><font style="vertical-align: inherit;">Sin embargo, dado que se supone que la "navegaci√≥n" debe ser exclusivamente aleatoria (esto se denomina "caminata aleatoria"), los valores se pueden reproducir f√°cilmente a partir de la siguiente regla simple: para un sitio con K enlaces salientes (una p√°gina con K enlaces a otras p√°ginas), la probabilidad de cada enlace saliente igual a 1 / K. </font><font style="vertical-align: inherit;">Es decir, la matriz de probabilidad de transici√≥n tiene la forma:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b30/3ec/a66/b303eca66763a4187d027842214ff529.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde los valores de 0.0 se reemplazan por conveniencia por ".". </font><font style="vertical-align: inherit;">Antes de realizar m√°s c√°lculos, podemos notar que esta cadena de Markov es indescomponible y aperi√≥dica, es decir, a la larga, el sistema converge a una distribuci√≥n estacionaria. </font><font style="vertical-align: inherit;">Como hemos visto, esta distribuci√≥n estacionaria se puede calcular resolviendo el siguiente problema del vector propio izquierdo</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2f/8da/687/e2f8da6879f6f19fdc921803c8c7e371.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Al hacerlo, obtenemos los siguientes valores de PageRank (valores de distribuci√≥n estacionarios) para cada p√°gina </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a8/b69/a5b/0a8b69a5b2916bca1f5fa45955af1b4b.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Valores de PageRank calculados para nuestro ejemplo artificial de 7 p√°ginas. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces el ranking de PageRank de este peque√±o sitio web es 1&gt; 7&gt; 4&gt; 2&gt; 5 = 6&gt; 3.</font></font><br><br><hr><br><h3>  Conclusiones </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hallazgos clave de este art√≠culo: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Los procesos aleatorios son conjuntos de variables aleatorias que a menudo se indexan por tiempo (los √≠ndices a menudo indican tiempo discreto o continuo) </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para un proceso aleatorio, la propiedad de Markov significa que para una corriente dada, la probabilidad del futuro no depende del pasado (esta propiedad tambi√©n se llama "falta de memoria") </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> la cadena de Markov de tiempo discreto es procesos aleatorios con √≠ndices de tiempo discreto que satisfacen la propiedad de Markov </font></font></li><li>                 (  ,  ‚Ä¶) </li><li>     PageRank ( )    -,       ;          ( ,             ,  ,      ) </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En conclusi√≥n, enfatizamos una vez m√°s cu√°n poderosas son las herramientas de las cadenas de Markov para modelar problemas asociados con din√°micas aleatorias. Debido a sus buenas propiedades, se usan en varios campos, por ejemplo, en la teor√≠a de colas (optimizando el rendimiento de las redes de telecomunicaciones, en las que los mensajes a menudo tienen que competir por recursos limitados y se ponen en cola cuando ya se han tomado todos los recursos), en estad√≠sticas (conocido Monte Carlo seg√∫n el esquema de la cadena de Markov para generar variables aleatorias se basa en las cadenas de Markov), en biolog√≠a (modelando la evoluci√≥n de las poblaciones biol√≥gicas), en inform√°tica (los modelos ocultos de Markov son herramientas importantes umentami en teor√≠a de la informaci√≥n, y el reconocimiento de voz), as√≠ como en otras √°reas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por supuesto, las enormes oportunidades que ofrecen las cadenas de Markov desde el punto de vista del modelado y la computaci√≥n son mucho m√°s amplias que las consideradas en esta modesta revisi√≥n. </font><font style="vertical-align: inherit;">Por lo tanto, esperamos haber podido despertar el inter√©s del lector en seguir estudiando estas herramientas, que ocupan un lugar importante en el arsenal de un cient√≠fico y experto en datos.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455762/">https://habr.com/ru/post/455762/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455746/index.html">Celesta 7.x: ORM, migraci√≥n y pruebas "en un paquete"</a></li>
<li><a href="../455754/index.html">Pruebas de un estratostato a la deriva. Lanzamiento de Rogozin y LoRa en la estratosfera.</a></li>
<li><a href="../455756/index.html">Es [favor] th</a></li>
<li><a href="../455758/index.html">Hacking de crecimiento en el cohete minorista: de la b√∫squeda de hip√≥tesis a las t√©cnicas de prueba</a></li>
<li><a href="../455760/index.html">La magia de SwiftUI o sobre los creadores de funciones</a></li>
<li><a href="../455764/index.html">B√∫squeda de c√≥digos de barras burlonamente precisa, r√°pida y ligera a trav√©s de la segmentaci√≥n sem√°ntica</a></li>
<li><a href="../455768/index.html">Factores esenciales de SEO en el sitio</a></li>
<li><a href="../455770/index.html">AERODISCO: esperando vs. realidad</a></li>
<li><a href="../455774/index.html">Motores de turbina de gas para aeronaves</a></li>
<li><a href="../455784/index.html">Debido a que el gris oscuro es m√°s claro que el gris en CSS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>